Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 289?293,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Mining Equivalent Relations from Linked Data 
 
 
Ziqi Zhang1 Anna Lisa Gentile1 Isabelle Augenstein1 
Eva Blomqvist2 Fabio Ciravegna1 
1 Department of Computer Science, 
University of Sheffield, UK 
2 Department of Computer and Information 
Science, Link?ping University, Sweden 
{z.zhang, a.l.gentile, i.augenstein, 
f.ciravegna}@dcs.shef.ac.uk, eva.blomqvist@liu.se
 
 
Abstract 
Linking heterogeneous resources is a major re-
search challenge in the Semantic Web. This 
paper studies the task of mining equivalent re-
lations from Linked Data, which was insuffi-
ciently addressed before. We introduce an un-
supervised method to measure equivalency of 
relation pairs and cluster equivalent relations. 
Early experiments have shown encouraging 
results with an average of 0.75~0.87 precision 
in predicting relation pair equivalency and 
0.78~0.98 precision in relation clustering. 
1 Introduction 
Linked Data defines best practices for exposing, 
sharing, and connecting data on the Semantic 
Web using uniform means such as URIs and 
RDF. It constitutes the conjunction between the 
Web and the Semantic Web, balancing the rich-
ness of semantics offered by Semantic Web with 
the easiness of data publishing. For the last few 
years Linked Open Data has grown to a gigantic 
knowledge base, which, as of 2013, comprised 
31 billion triples in 295 datasets1.  
A major research question concerning Linked 
Data is linking heterogeneous resources, the fact 
that publishers may describe analogous infor-
mation using different vocabulary, or may assign 
different identifiers to the same referents. Among 
such work, many study mappings between ontol-
ogy concepts and data instances (e.g., Isaac et al 
2007; Mi et al, 2009; Le et al, 2010; Duan et al, 
2012). An insufficiently addressed problem is 
linking heterogeneous relations, which is also 
widely found in data and can cause problems in 
information retrieval (Fu et al, 2012). Existing 
work in linking relations typically employ string 
similarity metrics or semantic similarity mea-
                                                 
1 http://lod-cloud.net/state/ 
sures that require a-priori domain knowledge and 
are limited in different ways (Zhong et al, 2002; 
Volz et al, 2009; Han et al, 2011; Zhao and 
Ichise, 2011; Zhao and Ichise, 2012).  
This paper introduces a novel method to dis-
cover equivalent groups of relations for Linked 
Data concepts. It consists of two components: 1) 
a measure of equivalency between pairs of rela-
tions of a concept and 2) a clustering process to 
group equivalent relations. The method is unsu-
pervised; completely data-driven requiring no a-
priori domain knowledge; and also language in-
dependent. Two types of experiments have been 
carried out using two major Linked Data sets: 1) 
evaluating the precision of predicting equivalen-
cy of relation pairs and 2) evaluating the preci-
sion of clustering equivalent relations. Prelimi-
nary results have shown encouraging results as 
the method achieves between 0.75~0.85 preci-
sion in the first set of experiments while 
0.78~0.98 in the latter. 
2 Related Work  
Research on linking heterogeneous ontological 
resources mostly addresses mapping classes (or 
concepts) and instances (Isaac et al 2007; Mi et 
al., 2009; Le et al, 2010; Duan et al, 2012; 
Schopman et al, 2012), typically based on the 
notions of similarity. This is often evaluated by 
string similarity (e.g. string edit distance), se-
mantic similarity (Budanitsky and Hirst, 2006), 
and distributional similarity based on the overlap 
in data usage (Duan et al, 2012; Schopman et 
al., 2012). There have been insufficient studies 
on mapping relations (or properties) across on-
tologies. Typical methods make use of a combi-
nation of string similarity and semantic similarity 
metrics (Zhong et al, 2002; Volz et al, 2009; 
Han et al, 2011; Zhao and Ichise, 2012). While 
string similarity fails to identify equivalent rela-
tions if their lexicalizations are distinct, semantic 
similarity often depends on taxonomic structures 
289
in existing ontologies (Budanitsky and Hirst, 
2006). Unfortunately many Linked Data instanc-
es use relations that are invented arbitrarily or 
originate in rudimentary ontologies (Parundekar 
et al, 2012). Distributional similarity has also 
been used to discover equivalent or similar rela-
tions. Mauge et al (2012) extract product proper-
ties from an e-commerce website and align 
equivalent properties using a supervised maxi-
mum entropy classification method. We study 
linking relations on Linked Data and propose an 
unsupervised method. Fu et al (2012) identify 
similar relations using the overlap of the subjects 
of two relations and the overlap of their objects. 
On the contrary, we aim at identifying strictly 
equivalent relations rather than similarity in gen-
eral. Additionally, the techniques introduced our 
work is also related to work on aligning multilin-
gual Wikipedia resources (Adar et al, 2009; 
Bouma et al, 2009) and semantic relatedness 
(Budanitsky and Hirst, 2006). 
3 Method 
Let t denote a 3-tuple (triple) consisting of a sub-
ject (ts), predicate (tp) and object (to). Linked Da-
ta resources are typed and its type is called class. 
We write type (ts) = c meaning that ts is of class c. 
p denotes a relation and rp is a set of triples 
whose tp=p, i.e., rp={t | tp = p}. 
Given a specific class c, and its pairs of rela-
tions (p, p?) such that rp={t|tp=p, type(ts)=c} and 
rp?={t|tp=p?, type (ts)=c}, we measure the equiv-
alency of p and p? and then cluster equivalent 
relations. The equivalency is calculated locally 
(within same class c) rather than globally (across 
all classes) because two relations can have iden-
tical meaning in specific class context but not 
necessarily so in general. For example, for the 
class Book, the relations dbpp:title and foaf:name 
are used with the same meaning, however for 
Actor, dbpp:title is used interchangeably with 
awards dbpp:awards (e.g., Oscar best actor). 
In practice, given a class c, our method starts 
with retrieving all t from a Linked Data set 
where type(ts)=c, using the universal query lan-
guage SPARQL with any SPARQL data end-
point. This data is then used to measure equiva-
lency for each pair of relations (Section 3.1). The 
equivalence scores are then used to group rela-
tions in equivalent clusters (Section 3.2). 
3.1 Measure of equivalence 
The equivalence for each distinct pair of rela-
tions depends on three components. 
Triple overlap evaluates the degree of over-
lap2 in terms of the usage of relations in triples. 
Let SO(p) be the collection of subject-object 
pairs from rp and SOint the intersection 
)r(SO)r(SO)'p,p(SO 'ppint ??
           [1] 
then the triple overlap TO(p, p?) is calculated as 
}|r|
|)r,r(SO|,|r|
|)r,r(SO|{MAX
'p
'ppint
p
'ppint
        [2] 
Intuitively, if two relations p and p? have a 
large overlap of subject-object pairs in their data 
instances, they are likely to have identical mean-
ing. The MAX function allows addressing infre-
quently used, but still equivalent relations (i.e., 
where the overlap covers most triples of an in-
frequently used relation but only a very small 
proportion of a much more frequently used).  
Subject agreement While triple overlap looks 
at the data in general, subject agreement looks at 
the overlap of subjects of two relations, and the 
degree to which these subjects have overlapping 
objects. Let S(p) return the set of subjects of rela-
tion p, and O(p|s) returns the set of objects of 
relation p whose subjects are s, i.e.: 
}st,pt|t{)s|r(O)s|p(O spop ????
        [3] 
we define: 
)r(S)r(S)'p,p(S 'ppint ??
         [4] 
|)'p,p(S|
otherwise,
|)s|'p(O)s|p(O|if,
int
)'p,p(Ss int
?
?
??
??
0
01
         [5] 
|)'p(S)p(S|/|)'p,p(S| int ???
        [6] 
then the agreement AG(p, p?) is  
????)'p,p(AG            [7] 
Equation [5] counts the number of overlapping 
subjects whose objects have at least one overlap. 
The higher the value of ?, the more the two rela-
tions ?agree? in terms of their shared subjects. 
For each shared subject of p and p? we count 1 if 
they have at least 1 overlapping object and 0 oth-
erwise. This is because both p and p? can be 
1:many relations and a low overlap value could 
mean that one is densely populated while the 
other is not, which does not necessarily mean 
they do not ?agree?. Equation [6] evaluates the 
degree to which two relations share the same set 
of subjects. The agreement AG(p, p?) balances 
the two factors by taking the product. As a result, 
                                                 
2 In this paper overlap is based on ?exact? match. 
290
relations that have high level of agreement will 
have more subjects in common, and higher pro-
portion of shared subjects with shared objects. 
Cardinality ratio is a ratio between cardinali-
ty of the two relations. Cardinality of a relation 
CD(p) is calculated based on data: 
|)r(S|
|r|)p(CD
p
p?
         [8] 
and the cardinality ratio is calculated as 
)}'p(CD),p(CD{MAX
)}'p(CD),p(CD{IN)'p,p(CDR ?
       [9] 
The final equivalency measure integrates all 
the three components to return a value in [0, 2]: 
)'p,p(CDR
)'p,p(AG)'p,p(TO)'p,p(E ??
              [10] 
The measure will favor two relations that have 
similar cardinality.  
3.2 Clustering 
We apply the measure to every pair of relations 
of a concept, and keep those with a non-zero 
equivalence score. The goal of clustering is to 
create groups of equivalent relations based on the 
pair-wise equivalence scores. We use a simple 
rule-based agglomerative clustering algorithm 
for this purpose. First, we rank all relation pairs 
by their equivalence score, then we keep a pair if 
(i) its score and (ii) the number of triples covered 
by each relation are above a certain threshold, 
TminEqvl and TminTP respectively. Each pair forms 
an initial cluster. To merge clusters, given an 
existing cluster c and a new pair (p, p?) where 
either p?c or p??c, the pair is added to c if E(p, 
p?) is close (as a fractional number above the 
threshold TminEqvlRel) to the average scores of all 
connected pairs in c. This preserves the strong 
connectivity in a cluster. This is repeated until no 
merge action is taken. Adjusting these thresholds 
allows balancing between precision and recall. 
4 Experiment Design 
To our knowledge, there is no publically availa-
ble gold standard for relation equivalency using 
Linked Data. We randomly selected 21 concepts 
(Figure 1) from the DBpedia ontology (v3.8): 
Actor, Aircraft, Airline, Airport, Automobile, 
Band, BasketballPlayer, Book, Bridge, Comedian, 
Film, Hospital, Magazine, Museum, Restaurant, 
Scientist, TelevisionShow, TennisPlayer, Theatre, 
University, Writer 
Figure 1. Concepts selected for evaluation. 
We apply our method to each concept to dis-
cover clusters of equivalent relations, using as 
SPARQL endpoint both DBpedia3 and Sindice4 
and report results separately. This is to study 
how the method performs in different conditions: 
on one hand on a smaller and cleaner dataset 
(DBpedia); on the other hand on a larger and 
multi-lingual dataset (Sindice) to also test cross-
lingual capability of our method. We chose rela-
tively low thresholds, i.e. TminEqvl=0.1, TminTP= 
0.01% and TminEqvlRel=0.6, in order to ensure high 
recall without sacrificing much precision.  
Four human annotators manually annotated 
the output for each concept. For this preliminary 
evaluation, we have limited the amount of anno-
tations to a maximum of 100 top scoring pairs of 
relations per concept, resulting in 16~100 pairs 
per concept (avg. 40) for DBpedia experiment 
and 29~100 pairs for Sindice (avg. 91). The an-
notators were asked to rate each edge in each 
cluster with -1 (wrong), 1 (correct) or 0 (cannot 
decide). Pairs with 0 are ignored in the evalua-
tion (about 12% for DBpedia; and 17% for Sin-
dice mainly due to unreadable encoded URLs for 
certain languages). To evaluate cross-lingual 
pairs, we asked annotators to use translation 
tools. Inter-Annotator-Agreement (observed 
IAA) is shown in Table 1. Also using this data, 
we derived a gold standard for clustering based 
on edge connectivity and we evaluate (i) the pre-
cision of top n% (p@n%) ranked equivalent rela-
tion pairs and (ii) the precision of clustering for 
each concept.  
 Mean High Low 
DBpedia 0.79 0.89 0.72 
Sindice 0.75 0.82 0.63 
Table 1. IAA on annotating pair equivalency 
So far the output of 13 concepts has been an-
notated. This dataset 5  contains ?1800 relation 
pairs and is larger than the one by Fu et al 
(2012). Annotation process shows that over 75% 
of relation pairs in the Sindice experiment con-
tain non-English relations and mostly are cross-
lingual. We used this data to report performance, 
although the method has been applied to all the 
21 concepts, and the complete results can be vis-
ualized at our demo website link. Some examples 
are shown in Figure 2.  
                                                 
3 http://dbpedia.org/sparql 
4 http://sparql.sindice.com/ 
5 http://staffwww.dcs.shef.ac.uk/people/Z.Zhang/ re-
sources/paper/acl2013short/web/ 
291
 Figure 2. Examples of visualized clusters 
5 Result and Discussion 
Figure 3 shows p@n% for pair equivalency6 and 
Figure 4 shows clustering precision.  
 
Figure 3. p@n%. The box plots show the ranges of 
precision at each n%; the lines show the average. 
 
Figure 4. Clustering precision  
As it is shown in Figure 2, Linked Data rela-
tions are often heterogeneous. Therefore, finding 
equivalent relations to improve coverage is im-
portant. Results in Figure 3 show that in most 
cases the method identifies equivalent relations 
with high precision. It is effective for both sin-
gle- and cross-language relation pairs. The worst 
performing case for DBpedia is Aircraft (for all 
n%), mostly due to duplicating numeric valued 
objects of different relations (e.g., weight, length, 
capacity). The decreasing precision with respect 
to n% suggests the measure effectively ranks 
correct pairs to the top. This is a useful feature 
from IR point of view. Figure 4 shows that the 
method effectively clusters equivalent relations 
with very high precision: 0.8~0.98 in most cases. 
                                                 
6 Per-concept results are available on our website. 
Overall we believe the results of this early proof-
of-concept are encouraging. As a concrete exam-
ple to compare against Fu et al (2012), for Bas-
ketballPlayer, our method creates separate clus-
ters for relations meaning ?draft team? and ?for-
mer team? because although they are ?similar? 
they are not ?equivalent?. 
We noticed that annotating equivalent rela-
tions is a non-trivial task. Sometimes relations 
and their corresponding schemata (if any) are 
poorly documented and it is impossible to under-
stand the meaning of relations (e.g., due to acro-
nyms) and even very difficult to reason based on 
data. Analyses of the evaluation output show that 
errors are typically found between highly similar 
relations, or whose object values are numeric 
types. In both cases, there is a very high proba-
bility of having a high overlap of subject-object 
pairs between relations. For example, for Air-
craft, the relations dbpp:heightIn and dbpp: 
weight are predicted to be equivalent because 
many instances have the same numeric value for 
the properties. Another example are the Airport 
properties dbpp:runwaySurface, dbpp:r1Surface, 
dbpp:r2Surface etc., which according to the data 
seem to describe the construction material (e.g., 
concrete, asphalt) of airport runways. The rela-
tions are semantically highly similar and the ob-
ject values have a high overlap. A potential solu-
tion to such issues is incorporating ontological 
knowledge if available. For example, if an ontol-
ogy defines the two distinct properties of Airport 
without explicitly defining an ?equivalence? re-
lation between them, they are unlikely to be 
equivalent even if the data suggests the opposite.  
6 Conclusion 
This paper introduced a data-driven, unsuper-
vised and domain and language independent 
method to learn equivalent relations for Linked 
Data concepts. Preliminary experiments show 
encouraging results as it effectively discovers 
equivalent relations in both single- and multi-
lingual settings. In future, we will revise the 
equivalence measure and also experiment with 
clustering algorithms such as (Beeferman et al, 
2000). We will also study the contribution of 
individual components of the measure in such 
task. Large scale comparative evaluations (incl. 
recall) are planned and this work will be extend-
ed to address other tasks such as ontology map-
ping and ontology pattern mining (Nuzzolese et 
al., 2011).  
 
292
Acknowledgement 
Part of this research has been sponsored by the 
EPSRC funded project LODIE: Linked Open 
Data for Information Extraction, EP/J019488/1. 
Additionally, we also thank the reviewers for 
their valuable comments given for this work. 
 
References  
Eytan Adar, Michael Skinner, Daniel Weld. 
2009. Information Arbitrage across Multi-
lingual Wikipedia. Proceedings of the Second 
ACM International Conference on Web 
Search and Data Mining, pp. 94 ? 103. 
Gosse Bouma, Sergio Duarte, Zahurul Islam. 
2009. Cross-lingual Alignment and Comple-
tion of Wikipedia Templates. Proceedings of 
the Third International Workshop on Cross 
Lingual Information Access: Addressing the 
Information Need of Multilingual Societies, 
pp. 61 ? 69   
Doug Beeferman, Adam Berger. 2000. Agglom-
erative clustering of a search engine query log. 
Proceedings of the sixth ACM SIGKDD inter-
national conference on Knowledge discovery 
and data mining, pp. 407-416. 
Alexander Budanitsky and Graeme Hirst. 2006. 
Evaluating WordNet-based Measures of Se-
mantic Distance. Computational Linguistics, 
32(1), pp.13-47. 
Songyun Duan, Achille Fokoue, Oktie Has-
sanzadeh, Anastasios Kementsietsidis, Kavitha 
Srinivas, and Michael J. Ward. 2012. In-
stance-Based Matching of Large Ontologies 
Using Locality-Sensitive Hashing. ISWC 
2012, pp. 46 ? 64 
Linyun Fu, Haofen Wang, Wei Jin, Yong Yu. 
2012. Towards better understanding and uti-
lizing relations in DBpedia. Web Intelligence 
and Agent Systems , Volume 10 (3) 
Andrea Nuzzolese, Aldo Gangemi, Valentina 
Presutti, Paolo Ciancarini. 2011. Encyclopedic 
Knowledge Patterns from Wikipedia Links. 
Proceedings of the 10th International Semantic 
Web Conference, pp. 520-536 
Lushan Han, Tim Finin and Anupam Joshi. 2011. 
GoRelations: An Intuitive Query System for 
DBpedia. Proceedings of the Joint Internation-
al Semantic Technology Conference 
Antoine Isaac, Lourens van der Meij, Stefan 
Schlobach, Shenghui Wang. 2007. An empiri-
cal study of instance-based ontology match-
ing. Proceedings of the 6th International Se-
mantic Web Conference and the 2nd Asian 
conference on Asian Semantic Web Confer-
ence, pp. 253-266 
Ngoc-Thanh Le, Ryutaro Ichise, Hoai-Bac Le. 
2010. Detecting hidden relations in geograph-
ic data. Proceedings of the 4th International 
Conference on Advances in Semantic Pro-
cessing, pp. 61 ? 68 
Karin Mauge, Khash Rohanimanesh, Jean-David 
Ruvini. 2012. Structuring E-Commerce Inven-
tory. Proceedings of ACL2012, pp. 805-814 
Jinhua Mi, Huajun Chen, Bin Lu, Tong Yu, 
Gang Pan. 2009. Deriving similarity graphs 
from open linked data on semantic web. Pro-
ceedings of the 10th IEEE International Con-
ference on Information Reuse and Integration, 
pp. 157?162. 
Rahul Parundekar, Craig Knoblock,  Jos? Luis. 
Ambite. 2012. Discovering Concept Cover-
ings in Ontologies of Linked Data Sources. 
Proceedings of ISWC2012, pp. 427?443. 
Balthasar Schopman, Shenghui Wang, Antoine 
Isaac, Stefan Schlobach. 2012. Instance-Based 
Ontology Matching by Instance Enrichment. 
Journal on Data Semantics, 1(4), pp 219-236 
Julius Volz, Christian Bizer, Martin Gaedke, 
Georgi Kobilarov. 2009. Silk ? A Link Discov-
ery Framework for the Web of Data. Proceed-
ings of the 2nd Workshop on Linked Data on 
the Web 
Lihua Zhao, Ryutaro Ichise. 2011. Mid-ontology 
learning from linked data. Proceedings of the 
Joint International Semantic Technology Con-
ference, pp. 112 ? 127. 
Lihua Zhao, Ryutaro Ichise. 2012. Graph-based 
ontology analysis in the linked open data. Pro-
ceedings of the 8th International Conference 
on Semantic Systems, pp. 56 ? 63 
Jiwei Zhong, Haiping Zhu, Jianming Li and 
Yong Yu. 2002. Conceptual Graph Matching 
for Semantic Search. The 2002 International 
Conference on Computational Science. 
293
Proceedings of Third Workshop on Semantic Web and Information Extraction, pages 17?24,
Dublin, Ireland, 24 August, 2014.
Seed Selection for Distantly Supervised Web-Based Relation Extraction
Isabelle Augenstein
Department of Computer Science
The University of Sheffield
United Kingdom
i.augenstein@dcs.shef.ac.uk
Abstract
In this paper we consider the problem of distant supervision to extract relations (e.g. origin(musical
artist, location)) for entities (e.g. ?The Beatles?) of certain classes (e.g. musical artist) from Web
pages by using background information from the Linking Open Data cloud to automatically label
Web documents which are then used as training data for relation classifiers. Distant supervision
approaches typically suffer from the problem of ambiguity when automatically labelling text,
as well as the problem of incompleteness of background data to judge whether a mention is a
true relation mention. This paper explores the hypothesis that simple statistical methods based
on background data can help to filter unreliable training data and thus improve the precision of
relation extractors. Experiments on a Web corpus show that an error reduction of 35% can be
achieved by strategically selecting seed data.
1 Introduction
One important aspect to every relation extraction approach is how to annotate training and test data for
learning classifiers. In the past, four different types of approaches for this have been proposed.
For supervised approaches, training and test data is annotated manually by one or several annotators.
While this approach results in a high-quality corpus, it is very expensive and time-consuming. As a
consequence, the corpora used tend to be small and biased towards a certain domain or type of text.
Unsupervised approaches do not need annotated data for training; they instead cluster similar word
sequences and generalise them to relations. Although unsupervised aproaches can process very large
amounts of data, resulting relations are hard to map to particular schemas. In addition, Fader et al. (2011)
observe that these approaches often produce uninformative or incoherent extractions.
Semi-supervised methods are methods that only require a small number of seed instances. Hand-crafted
seeds are used to extract patterns from a corpus, which are then used to extract more instances and those
again to extract new patterns in an iterative way. However, since many iterations are needed, these methods
are prone to semantic drift, i.e. an unwanted shift of meaning. As a consequence these methods require a
certain amount of human effort - to create seeds initially and also to help keep systems ?on track?.
A fourth group of approaches, distant supervision or self-supervised approaches, exploit big knowledge
bases such as Freebase (2008) to automatically label entities in text and use the annotated text to extract
features and train a classifier (Wu and Weld, 2007; Mintz et al., 2009). Unlike supervised systems, they
do not require manual effort to label data and can be applied to large corpora. Since they extract relations
which are defined by schemas, these approaches also do not produce informative or incoherent relations.
Distant supervision approaches are based on the following assumption (Mintz et al., 2009):
?If two entities participate in a relation, any sentence that contains those two entities might express that
relation.? In practice, if the information that two entities participate in a relation is contained in the
knowledge base, whenever they appear in the same sentence, that sentence is used as positive training data
for that relation. This heuristic causes problems if different entities have the same surface form. Consider
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
17
the following example:
?Let It Be is the twelfth album by The Beatles which contains their hit single ?Let It Be?.?
In that sentence, the first mention of Let It Be is an example of the album relation, whereas the second
mention is an example of the track relation. If both mentions are used as positive training examples for
both relations, this impairs the learning of weights of the relation classifiers. We therefore argue for the
careful selection of training data for distant supervision by using measures to discard highly ambiguous
training examples. One further aspect that can be problematic when automatically creating negative
training data is incompleteness. What Riedel et al. (2010) point out, and our observations also confirm,
is that about 20%, or even more, of all true relation mentions in a corpus are not contained in Freebase,
although it is a very big knowledge base.
The main contributions of this paper are: to propose and evaluate several measures for detecting and
discarding unreliable seeds; and to document a distant supervision system for fine-grained class-based
relation extraction on noisy data from the Web.
2 Distantly Supervised Relation Extraction
Distant supervision is defined as the automatic labelling of a corpus with properties, P and entities, E
from a knowledge base, KB to train a classifier to learn to predict relations. Following previous distant
supervision approaches, we only consider binary relations of the form (s, p, o), consisting of a subject, a
predicate and an object (Mintz et al., 2009). We use the established Semantic Web formalisation, rather
than unary and binary first order predicates, to reflect our special and consistent treatment of subjects
versus objects. Each subject and object entity e ? E has a set of lexicalisations, L
e
? L. Furthermore, we
consider only those subjects which have a particular Freebase class C.
3 Seed Selection
Before using the automatically labelled corpus to train a classifier, we include a seed selection step, which
consist of several measures to discard unreliable seeds.
Ambiguity Within An Entity
Unam: Our first approach is to discard lexicalisations of objects if they are ambiguous for the subject
entity, i.e. if a subject is related to two different objects which have the same lexicalisation, and express
two different relations. To illustrate this, let us consider the problem outlined in the introduction again:
Let It Be can be both an album and a track of the subject entity The Beatles, therefore we would like to
discard Let It Be as a seed for the class Musical Artist. We measure the degree to which a lexicalisation
l ? L
o
of an object o is ambiguous by the number of senses the lexicalisation has. For a given subject s, if
we discover a lexicalisation for a related entity, i.e. (s, p, o) ? KB and l ? L
o
, then, since it may be the
case that l ? L
r
for some R 3 r , o, where also (s, q, r) ? KB for some q ? P, we say in this case that l
has a ?sense? o and r, giving rise to ambiguity. We then define A
s
l
, the ambiguity of a lexicalisation with
respect to the subject as follows: A
s
l
= |{e | l ? L
o
? L
w
? (s, p, o) ? KB ? (s, v, w) ? KB ? w , o}|.
Ambiguity Across Classes
In addition to being ambiguous for a subject of a specific class, lexicalisations of objects can be ambiguous
across classes. Our assumption is that the more senses an object lexicalisation has, the more likely it is
that that object occurence is confused with an object lexicalisation of a different property of any class.
An example for this are common names of book authors or common genres as in the sentence ?Jack
mentioned that he read On the Road?, in which Jack is falsely recognised as the author Jack Kerouac.
Stop: One type of very ambiguous words with many senses are stop words. Since some objects of relations
in our training set might have lexicalisations which are stop words, we discard those lexicalisations if they
appear in a stop word list (we use the one described in Lewis et al. (2004)).
Stat: For other highly ambiguous lexicalisations of object entities our approach is to estimate cross-class
ambiguity, i.e. to estimate how ambiguous a lexicalisation of an object is compared to other lexicalisations
of objects of the same relation. If its ambiguity is comparatively low, we consider it a reliable seed,
18
otherwise we want to discard it. For the set of classes under consideration, we know the set of properties
that apply, D ? P and can retrieve the set {o | (s, p, o) ? KB? p ? D}, and retrieve the set of lexicalisations
for each member, L
o
. We then compute A
o
, the number of senses for every lexicalisation of an object L
o
,
where A
o
= |{o | ? L
o
}|.
We view the number of senses of each lexicalisation of an object per relation as a frequency distribution.
We then compute min, max, median (Q2), the lower (Q1) and the upper quartile (Q3) of those frequency
distributions and compare it to the number of senses of each lexicalisation of an object. If A
l
> Q, where
Q is either Q1, Q2 or Q3 depending on the model, we discard the lexicalisation of the object.
Incompletess
One further aspect of knowledge bases that can be problematic when automatically creating negative
training data is incompleteness. Our method for creating negative training data is to assume that all
entities which appear in a sentence with the subject s, but are not in a relation with it according to the
knowledge base, can be used as negative training data. Other distant supervision approaches (Mintz et al.,
2009) follow a similar approach, but only use a random sample of unrelated entities pairs.
Incomp: Our approach is to discard negative training examples which are likely to be true
relation mentions, but missing from the knowledge base. If we find a lexicalisation l where
@ o, p ? l ? L
o
? (s, p, o) ? KB, then before we consider this a negative example we check if
? t ? C ? (t, q, r) ? KB and l ? L
r
, i.e. if any of the properties of the class we examine has an object
lexicalisation l.
4 System
4.1 Corpus
To create a corpus for Web relation extraction using Linked Data, three Freebase classes and their six to
seven most prominent properties (see Table 1) are selected and their values retrieved using the Freebase
API. To avoid noisy training data, entities which only have values for some of those relations were not
used. This resulted in 1800 to 2200 entities per class which were split equally for training and test. For
each entity, at most 10 Web pages were retrieved via the Google Search API using the search pattern
??subject_entity? class property_name?, e.g. ??The Beatles? Musical Artist Origin? resulting in a total
of 450,000 pages
1
. By adding the class, we expect the retrieved Web pages to be more relevant to our
extraction task. For entities, Freebase distinguishes between the most prominant lexicalisation (the entity
name) and other lexicalisations (entity aliases). We use the entity name for all of the search patterns.
Class Property Class Property Class Property
Book author Musical Artist album Politician birthdate
characters active (start) birthplace
publication date active (end) educational institution
genre genre nationality
ISBN record label party
original language origin religion
track spouses
Table 1: Freebase classes and properties we use for our evaluation
4.2 NLP Pipeline
Text content is extracted from HTML pages using the jsoup API
2
, which strips text from each element
recurvisely. Each paragraph is then processed with Stanford CoreNLP
3
to split the text into sentences,
1
URLs of those Web pages are available via http://staffwww.dcs.shef.ac.uk/people/I.Augenstein/SWAIE2014/
2
http://jsoup.org/
3
http://nlp.stanford.edu/software/corenlp.shtml
19
tokenise, POS tag it and normalise time expressions. Named entities are classified using the 7 class (time,
location, organisation, person, money, percent, date) named entity model.
4.3 Relation candidate identification
Some of the objects of relations cannot be categorised according to the 7 named entity (NE) classes
detected by the Stanford named entity classifier (NERC) and are therefore not recognised, for example
MusicalArtist:album or Book:genre. Therefore, in addition to recognising entities with Stanford NERC,
we also implement our own named entity recogniser (NER), which only recognises entity boundaries, but
does not classify them. To detect entity boundaries, we recognise sequences of nouns and sequences of
capitalised words and apply both greedy and non-greedy matching. For greedy matching, we consider
whole noun phrases and for non-greedy matching all subsequences starting with the first word of the those
phrases, i.e. for ?science fiction book?, we would consider ?science fiction book?, ?science fiction? and
?book? as candidates. The reason to do greedy as well as non-greedy matching is because the lexicalisation
of an object does not always span a whole noun phrase, e.g. while ?science fiction? is a lexicalisation of
an object of Book:genre, ?science fiction book? is not. However, for MusicalArtist:genre, ?pop music?
would be a valid lexicalisation of an object. We also recognise short sequences of words in quotes. This is
because lexicalisation of objects of MusicalArtist:track and MusicalArtist:album often appear in quotes,
but are not necessarily noun phrases.
4.4 Identifying Relation Candidates and Selecting Seeds
The next step is to identify which sentences potentially express relations. We only use sentences from
Web pages which were retrieved using a query which contains the subject of the relation. We then select,
or rather discard seeds for training according to the different methods outlined in Section 3. Our baseline
model does not discard any training seeds.
4.5 Features
Our system uses some of the features described in Mintz et al. (2009), and other standard lexical features
and named entity features:
? The object occurrence
? The bag of words of the occurrence
? The number of words of the occurrence
? The named entity class of the occurrence assigned by the 7-class Stanford NERC
? A flag indicating if the object or the subject entity came first in the sentence
? The sequence of part of speech (POS) tags of the words between the subject and the occurrence
? The bag of words between the subject and the occurrence
? The pattern of words between the subject entity and the occurrence (all words except for nouns,
verbs, adjectives and adverbs are replaced with their POS tag, nouns are replaced with their named
entity class if a named entity class is available)
? Any nouns, verbs, adjectives, adverbs or NEs in a 3-word window to the left of the occurrence
? Any nouns, verbs, adjectives, adverbs or NEs in a 3-word window to the right of the occurrence
4.6 Classifier and Models
As a classifier, we choose a first-order conditional random field model (Lafferty et al., 2001). We use the
software CRFSuite
4
and L-BFGS (Nocedal, 1980) for training our classifiers. We train one classifier per
Freebase class and model. Our models only differ in the way training data is selected (see Section 3). The
models are then used to classify each object candidate into one of the relations of the Freebase class or
NONE (no relation).
4
http://www.chokkan.org/software/crfsuite/
20
4.7 Merging and Ranking Results
We understand relation extraction as the task of predicting the relations which can be found in a corpus.
While some approaches aim at correctly predicting every single mention of a relation seperately, we
instead choose to aggregate predictions of relation mentions. For every Freebase class, we get all relation
mentions from the corpus and the classifier?s confidence values for Freebase classes assigned to object
occurences. There are usually several different predictions, e.g. the same occurence could be predicted to
be MusicalArtist:album, MusicalArtist:origin and MusicalArtist:NONE. By aggregating relation mentions
across documents we have increased chances of choosing the right relation, since some contexts of
occurences are inconclusive or ambiguous and thus the classifier chooses the wrong property wfor those.
For a given lexicalisation l, representing an object to which the subject is related, the classifier gives each
object occurence a prediction which is the combination of a predicted relation and a confidence. We
collect these across the chosen documents to form a set of confidence values, for each predicted relation,
per lexicalisation E
l
p
. For instance if the lexicalisation l occurs three times across the documents and is
predicted to represent an object to relation p
1
once with confidence 0.2, and in other cases to represent
the object to relation p
2
with confidence 0.1 and 0.5 respectively, then E
l
p
1
= 0.2 and E
l
p
2
= {0.1, 0.5}. In
order to form an aggregated confidence for each relation with respect to the lexicalisation, g
p
l
, we calculate
the mean average for each such set and normalise across relations, as follows: g
l
p
= E
l
p
?
|E
l
p
|
?
q?P
|E
l
q
|
5 Evaluation
5.1 Corpus
Although we automatically annotate the training and test part of the Web corpus with properties, we
hand-annotate a portion of the test corpus. The portion of the corpus we manually annotate is the one
which has NONE predictions for object occurrences, i.e. for which occurences do not have a representation
in Freebase. They could either get NONE predictions because they are not relation mentions, or because
they are missing from Freebase. We find that on average 45% of the occurences which are predicted by
our models are true relation mentions, but missing from Freebase. Note that some of the automatically
evaluated positive predictions could in fact be false positives.
5.2 Results
Figures 1 and 2 show the precision with respect to confidence and precision@K for our self-supervised
relation extraction models which only differ in the way training data is selected, as described in Section 3.
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
unam_stop_stat25unam_stop_stat50unam_stop_stat75unam_stopstopbaselineincompl
Figure 1: Precision / confidence graph
 0.88
 0.9
 0.92
 0.94
 0.96
 0.98
 1
 0  200  400  600  800  1000  1200  1400  1600  1800  2000
unam_stop_stat50unam_stop_stat75unam_stopstopunam_stop_stat25baselineincompl
Figure 2: Precision@K
Figure 1 shows the precision of our models on the y-axis with respect to a cutoff at a minimum
confidence displayed on the x-axis. The precision at a minimum confidence of 0 is equivalent to the
precision over the whole test corpus. For all of the models, the precision rises with increasing confidence,
which means our confidence measure succeeds at ranking results by precision. With respect to the baseline
21
which does not filter seeds, our best-performing model increases the total precision from 0.825 to 0.896,
which is a total error reduction of 35%. We achieve the best results in terms of total precision with
the model unam_stop_stat25, which filters lexicalisations which are ambiguous for a subject, filters
lexicalisations which are stop words and filters lexicalisations with an ambiguity value higher than the
lower quartile of the distribution for the relation in question. The worst-performing model is, surprisingly,
incompl, the model we built to discard negative training data which are likely to be true relation mentions,
but missing from the knowledge base. We discuss this further in Section 8. Figures 2 shows the precision,
sorted by confidence, for the K highest ranked documents. We decided to use the precision@K measure
instead of computing recall because it is not feasible to manually annotate 450,000 Web pages with
relation mentions. Note, however, that distant supervision is not aimed at high recall anyway - because
only sentences which contain both the subject and the object entity explicitely are used, many relation
mentions will be missed out on. The highest value on the x-axis is the number of predicted relations of the
model with the smallest number of predicted relations. The models which filter seeds improve all above
the baseline in terms of precision@K for 0% to about 65% of the maximum K, from 65% to 100%, only
stop and unam_stop improve on the baseline.
6 Discussion
Although we cannot directly compare our results to that of other distantly supervised relation extraction
models because we use different evaluation data and a different set of relations, our baseline model, which
has a total precision of 0.825, as well as our best-performing model, which has a total precision of 0.896
seem to be perform as well as, if not better than previous systems. Overall, our seed selection methods
seem to perform well at removing unreliable training data to improve precision.
What is still unsuccessful is our incompl model. The idea behind it was that relations which, for a given
subject, have more than one object (e.g. Book:genre) are prone to be ?incomplete? - the objects in the
knowledge base are often just the most prominent ones and other objects, which could be discovered from
text, are missing. When annotating training data for distant supervision, those missing objects would
be considered negative training data, which could potentially be harmful for training. However, just
assuming that all negative training examples could potentially be false negatives if they match one of
the objects does not lead to improved results. One of the reasons for this could be that most of those
potential false negatives are instead objects of relations which expect the same kinds of values - and thus
crucial for training the models. Some relations for which we observed this are are Book:originalLanguage
and Book:translations, as well as Book:firstPublicationDate and Book:dateWritten. Interestingly, neither
Book:originalLanguage nor Book:firstPublicationDate are n:n relations.
7 Related Work
While lots of approaches in the past have focused on supervised, unsupervised (Yates et al., 2007; Fader et
al., 2011) or semi-supervised relation extraction (Hearst, 1992; Carlson et al., 2010), there have also been
some distantly supervised relation extraction approaches in the past few years, which aim at exploiting
background knowledge for relation extraction, most of them for extracting relations from Wikipedia.
Mintz et al. (2009) describe one of the first distant supervision approaches which aims at extracting
relations between entities in Wikipedia for the most frequent relations in Freebase. They report precision
of about 0.68 for their highest ranked 10% of results depending what features they used. In contrast
to our approach, Mintz et al. do not experiment with changing the distance supervision assumption or
removing ambiguous training data, they also do not use fine-grained relations and their approach is not
class-based. Nguyen et al. (2011)?s approach is very similar to that of Mintz et al. (2009), except that
they use a different knowledge base, YAGO (Suchanek et al., 2008). They use a Wikipedia-based NERC,
which, like the Stanford NERC classifies entities into persons, relations and organisations. They report a
precision of 0.914 for their whole test set, however, those results might be skewed by the fact that YAGO
is a knowledge based derived from Wikipedia.
A few strategies for seed selection for distant supervision have already been investigated: At-least-one
models (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Yao et al., 2010; Min et al., 2013),
22
hierarchical topic models (Alfonseca et al., 2012; Roth and Klakow, 2013), pattern correlations (Takamatsu
et al., 2012), and an information retrieval approach (Xu et al., 2013). At-least-one models (Hoffmann
et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Yao et al., 2010; Min et al., 2013) are based
on the idea that ?if two entities particpate in a relation, at least one sentence that mentions these two
entities might express that relation?. While positive results have been reported for those models, Riedel et
al. (Riedel et al., 2010) argues that it is challenging to train those models because they are quite complex.
Hierarchical topic models (Alfonseca et al., 2012; Roth and Klakow, 2013) assume that the context of
a relation is either specific for the pair of entities, the relation, or neither. Min et al. (Min et al., 2013)
further propose a 4-layer hierarchical model to only learn from positive examples to address the problem
of incomplete negative training data. Pattern correlations (Takamatsu et al., 2012) are also based on the
idea of examining the context of pairs of entities, but instead of using a topic model as a pre-processing
step for learning extraction patterns, they first learn patterns and then use a probabilistic graphical model
to group extraction patterns. Xu et al. (Xu et al., 2013) propose a two-step model based on the idea of
pseudo-relevance feedback which first ranks extractions, then only uses the highest ranked ones to re-train
their model. Our research is based on a different assumption: Instead of trying to address the problem of
noisy training data by using more complicated multi-stage machine learning models, we want to examine
how background data can be even further exploited by testing if simple statistical methods based on data
already present in the knowledge base can help to filter unreliable training data.
8 Future Work
In this paper, we have documented and evaluated an approach to discard unreliable seed data for distantly
supervised relation extraction. Our two hypotheses were that discarding highly ambiguous relation
mentions and discarding unreliable negative training seeds could help to improve precision of self-
supervised relation extraction models. While our evaluation indicates that discarding highly ambiguous
relation mentions based on simple statistical methods helps to improve the precision of distantly supervised
relation extraction systems, discarding negative training data does not. We have also described our distantly
supervised relation extraction system, which, unlike other previous systems learns to extract from Web
pages and also learns to extract fine-grained relations for specific classes instead of relations which are
applicable to several broad classes.
In future work, we want to work on increasing the number of extractions for distant supervision systems:
The distant supervision assumption requires sentences to contain both the subject and the object of a
relation. While this ensures high precision and is acceptable for creating training data, most sentences - at
least those in Web documents - do not mention the subject of relations explicitly and we thus miss out
on a lot of data to extract from. We further want to extend our distant supervision approach to extract
information not only from free text, but also from lists and relational tables from Web pages. Finally, we
would like to train distantly supervised models for entity classification to assist relation extraction. A
more detailed description of future work can also be found in Augenstein (2014).
Acknowledgements
We thank Barry Norton, Diana Maynard, as well as the anonymous reviewers for their valuable feed-
back. This research was partly supported by the EPSRC funded project LODIE: Linked Open Data for
Information Extraction, EP/J019488/1.
References
Enrique Alfonseca, Katja Filippova, Jean-Yves Delort, and Guillermo Garrido. 2012. Pattern Learning for Rela-
tion Extraction with a Hierarchical Topic Model. In Proceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Short Papers, volume 2, pages 54?59.
Isabelle Augenstein. 2014. Joint Information Extraction from the Web using Linked Data. Doctoral Consortium
Proceedings of the 13th International Semantic Web Conference. to appear.
23
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A Collabora-
tively Created Graph Database For Structuring Human Knowledge. In Proceedings of the 2008 ACM SIGMOD
international conference on Management of data, pages 1247?1250.
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E.R. Hruschka Jr., and T.M. Mitchell. 2010. Toward an Architecture
for Never-Ending Language Learning. In Proceedings of the Conference on Artificial Intelligence, pages 1306?
1313.
Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1535?1545.
Marti A. Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. In Proceedings of the 14th
International Conference on Computational Linguistics, pages 539?545.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke S. Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
Based Weak Supervision for Information Extraction of Overlapping Relations. In Proceedings of the 49th
Annual Meeting of the Association for Computational Linguistics, pages 541?550.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine
Learning, pages 282?289.
D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. 2004. RCV1: A New Benchmark Collection for Text Categorization
Research. Journal of Machine Learning Research, 5:361?397.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. 2013. Distant Supervision for Relation
Extraction with an Incomplete Knowledge Base. In Proceedings of HLT-NAACL, pages 777?782.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without
labeled data. In Proceedings of ACL-IJCNLP, volume 2, pages 1003?1011.
Truc-Vien T. Nguyen and Alessandro Moschitti. 2011. End-to-End Relation Extraction Using Distant Supervi-
sion from External Semantic Repositories. In Proceedings of the 50th Annual Meeting of the Association for
Computational Linguistics: Short Papers, volume 2, pages 277?282.
Jorge Nocedal. 1980. Updating Quasi-Newton Matrices with Limited Storage. Mathematics of Computation,
35(151):773?782.
Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling Relations and Their Mentions without
Labeled Text. In Proceedings of the 2010 European conference on Machine learning and knowledge discovery
in databases: Part III, pages 148?163.
Benjamin Roth and Dietrich Klakow. 2013. Combining Generative and Discriminative Model Scores for Distant
Supervision. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,
pages 24?29.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. YAGO: A Large Ontology from Wikipedia
and WordNet. Web Semantics: Science, Services and Agents on the World Wide Web, 6(3):203?217.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D. Manning. 2012. Multi-instance Multi-
label Learning for Relation Extraction. In Proceedings of EMNLP-CoNLL, pages 455?465.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa. 2012. Reducing Wrong Labels in Distant Supervision
for Relation Extraction. In Proceedings of the 50th Annual Meeting of the Association for Computational
Linguistics, pages 721?729.
Fei Wu and Daniel S. Weld. 2007. Autonomously Semantifying Wikipedia. In Proceedings of the Sixteenth ACM
Conference on Information and Knowledge Management, pages 41?50.
Wei Xu, Raphael Hoffmann, Le Zhao, and Ralph Grishman. 2013. Filling Knowledge Base Gaps for Distant
Supervision of Relation Extraction. In Proceedings of the 51st Annual Meeting of the Association for Computa-
tional Linguistics, pages 665?670.
Limin Yao, Sebastian Riedel, and Andrew McCallum. 2010. Collective Cross-document Relation Extraction
Without Labelled Data. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing, pages 1013?1023.
Alexander Yates, Michael Cafarella, Michele Banko, Oren Etzioni, Matthew Broadhead, and Stephen Soderland.
2007. TextRunner: Open Information Extraction on the Web. In Proceedings of HLT-NAACL: Demonstrations,
pages 25?26.
24
