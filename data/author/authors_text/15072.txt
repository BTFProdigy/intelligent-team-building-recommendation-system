Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 85?90,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Question Difficulty Estimation in Community Question Answering Services?
Jing Liu? Quan Wang? Chin-Yew Lin? Hsiao-Wuen Hon?
?Harbin Institute of Technology, Harbin 150001, P.R.China
?Peking University, Beijing 100871, P.R.China
?Microsoft Research Asia, Beijing 100080, P.R.China
jliu@ir.hit.edu.cn quanwang1012@gmail.com {cyl,hon}@microsoft.com
Abstract
In this paper, we address the problem of
estimating question difficulty in community
question answering services. We propose a
competition-based model for estimating ques-
tion difficulty by leveraging pairwise compar-
isons between questions and users. Our ex-
perimental results show that our model sig-
nificantly outperforms a PageRank-based ap-
proach. Most importantly, our analysis shows
that the text of question descriptions reflects
the question difficulty. This implies the pos-
sibility of predicting question difficulty from
the text of question descriptions.
1 Introduction
In recent years, community question answering (C-
QA) services such as Stackoverflow1 and Yahoo!
Answers2 have seen rapid growth. A great deal
of research effort has been conducted on CQA, in-
cluding: (1) question search (Xue et al, 2008; Du-
an et al, 2008; Suryanto et al, 2009; Zhou et al,
2011; Cao et al, 2010; Zhang et al, 2012; Ji et
al., 2012); (2) answer quality estimation (Jeon et al,
2006; Agichtein et al, 2008; Bian et al, 2009; Liu
et al, 2008); (3) user expertise estimation (Jurczyk
and Agichtein, 2007; Zhang et al, 2007; Bouguessa
et al, 2008; Pal and Konstan, 2010; Liu et al, 2011);
and (4) question routing (Zhou et al, 2009; Li and
King, 2010; Li et al, 2011).
?This work was done when Jing Liu and Quan Wang were
visiting students at Microsoft Research Asia. Quan Wang is
currently affiliated with Institute of Information Engineering,
Chinese Academy of Sciences.
1http://stackoverflow.com
2http://answers.yahoo.com
However, less attention has been paid to question
difficulty estimation in CQA. Question difficulty es-
timation can benefit many applications: (1) Experts
are usually under time constraints. We do not want
to bore experts by routing every question (including
both easy and hard ones) to them. Assigning ques-
tions to experts by matching question difficulty with
expertise level, not just question topic, will make
better use of the experts? time and expertise (Ack-
erman and McDonald, 1996). (2) Nam et al (2009)
found that winning the point awards offered by the
reputation system is a driving factor in user partici-
pation in CQA. Question difficulty estimation would
be helpful in designing a better incentive mechanis-
m by assigning higher point awards to more diffi-
cult questions. (3) Question difficulty estimation can
help analyze user behavior in CQA, since users may
make strategic choices when encountering questions
of different difficulty levels.
To the best of our knowledge, not much research
has been conducted on the problem of estimating
question difficulty in CQA. The most relevant work
is a PageRank-based approach proposed by Yang et
al. (2008) to estimate task difficulty in crowdsourc-
ing contest services. Their key idea is to construct
a graph of tasks: creating an edge from a task t1 to
a task t2 when a user u wins task t1 but loses task
t2, implying that task t2 is likely to be more diffi-
cult than task t1. Then the standard PageRank al-
gorithm is employed on the task graph to estimate
PageRank score (i.e., difficulty score) of each task.
This approach implicitly assumes that task difficulty
is the only factor affecting the outcomes of competi-
tions (i.e. the best answer). However, the outcomes
of competitions depend on both the difficulty levels
of tasks and the expertise levels of competitors (i.e.
85
other answerers).
Inspired by Liu et al (2011), we propose a
competition-based approach which jointly models
question difficulty and user expertise level. Our ap-
proach is based on two intuitive assumptions: (1)
given a question answering thread, the difficulty s-
core of the question is higher than the expertise score
of the asker, but lower than that of the best answerer;
(2) the expertise score of the best answerer is higher
than that of the asker as well as all other answer-
ers. Given the two assumptions, we can determine
the question difficulty score and user expertise score
through pairwise comparisons between (1) a ques-
tion and an asker, (2) a question and a best answerer,
(3) a best answerer and an asker, and (4) a best an-
swerer and all other non-best answerers.
The main contributions of this paper are:
?We propose a competition-based approach to es-
timate question difficulty (Sec. 2). Our model signif-
icantly outperforms the PageRank-based approach
(Yang et al, 2008) for estimating question difficulty
on the data of Stack Overflow (Sec. 3.2).
?Additionally, we calibrate question difficulty s-
cores across two CQA services to verify the effec-
tiveness of our model (Sec. 3.3).
?Most importantly, we demonstrate that different
words or tags in the question descriptions indicate
question difficulty levels. This implies the possibil-
ity of predicting question difficulty purely from the
text of question descriptions (Sec. 3.4).
2 Competition based Question Difficulty
Estimation
CQA is a virtual community where people can ask
questions and seek opinions from others. Formally,
when an asker ua posts a question q, there will be
several answerers to answer her question. One an-
swer among the received ones will be selected as the
best answer by the asker ua or voted by the com-
munity. The user who provides the best answer is
called the best answerer ub, and we denote the set of
all non-best answerers as S = {uo1 , ? ? ? , uoM}. As-
suming that question difficulty scores and user ex-
pertise scores are expressed on the same scale, we
make the following two assumptions:
?The difficulty score of question q is higher than
the expertise score of asker ua, but lower than that
of the best answerer ub. This is intuitive since the
best answer ub correctly responds to question q that
asker ua does not know.
?The expertise score of the best answerer ub is
higher than that of asker ua and all answerers in S.
This is straightforward since the best answerer ub
solves question q better than asker ua and all non-
best answerers in S.
Let?s view question q as a pseudo user uq. Tak-
ing a competitive viewpoint, each pairwise compar-
ison can be viewed as a two-player competition with
one winner and one loser, including (1) one compe-
tition between pseudo user uq and asker ua, (2) one
competition between pseudo user uq and the best
answerer ub, (3) one competition between the best
answerer ub and asker ua, and (4) |S| competitions
between the best answerer ub and all non-best an-
swers in S. Additionally, pseudo user uq wins the
first competition and the best answerer ub wins all
remaining (|S| + 2) competitions.
Hence, the problem of estimating the question d-
ifficulty score (and the user expertise score) is cast
as a problem of learning the relative skills of play-
ers from the win-loss results of the generated two-
player competitions. Formally, let Q denote the set
of all questions in one category (or topic), andRq de-
note the set of all two-player competitions generated
from question q ? Q, i.e., Rq = {(ua ? uq), (uq ?
ub), (ua ? ub), (uo1 ? ub), ? ? ? , (uo|S| ? ub)},
where j ? i means that user i beats user j in the
competition. Define
R =
?
q?Q
Rq (1)
as the set of all two-player competitions. Our prob-
lem is then to learn the relative skills of players from
R. The learned skills of the pseudo question users
are question difficulty scores, and the learned skills
of all other users are their expertise scores.
TrueSkill In this paper, we follow (Liu et al,
2011) and apply TrueSkill to learn the relative skill-
s of players from the set of generated competitions
R (Equ. 1). TrueSkill (Herbrich et al, 2007) is a
Bayesian skill rating model that is developed for es-
timating the relative skill levels of players in games.
In this paper, we present a two-player version of
TrueSkill with no-draw.
TrueSkill assumes that the practical performance
of each player in a game follows a normal distribu-
86
tion N(?, ?2), where ? means the skill level of the
player and ? means the uncertainty of the estimated
skill level. Basically, TrueSkill learns the skill lev-
els of players by leveraging Bayes? theorem. Giv-
en the current estimated skill levels of two players
(priori probability) and the outcome of a new game
between them (likelihood), TrueSkill model updates
its estimation of player skill levels (posterior prob-
ability). TrueSkill updates the skill level ? and the
uncertainty ? intuitively: (a) if the outcome of a new
competition is expected, i.e. the player with higher
skill level wins the game, it will cause small updates
in skill level ? and uncertainty ?; (b) if the outcome
of a new competition is unexpected, i.e. the player
with lower skill level wins the game, it will cause
large updates in skill level ? and uncertainty ?. Ac-
cording to these intuitions, the equations to update
the skill level ? and uncertainty ? are as follows:
?winner = ?winner +
?2winner
c ? v
(
t
c ,
?
c
)
, (2)
?loser = ?loser ?
?2loser
c
? v
(
t
c
, ?
c
)
, (3)
?2winner = ?2winner ?
[
1 ? ?
2
winner
c2
? w
(
t
c
, ?
c
)]
,
(4)
?2loser = ?2loser ?
[
1 ?
?2loser
c2
? w
(
t
c
, ?
c
)]
, (5)
where t = ?winner ? ?loser and c2 = 2?2 +
?2winner+?2loser. Here, ? is a parameter representing
the probability of a draw in one game, and v(t, ?)
and w(t, ?) are weighting factors for skill level ?
and standard deviation ? respectively. Please refer
to (Herbrich et al, 2007) for more details. In this
paper, we set the initial values of the skill level ?
and the standard deviation ? of each player the same
as the default values used in (Herbrich et al, 2007).
3 Experiments
3.1 Data Set
In this paper, we use Stack Overflow (SO) for our
experiments. We obtained a publicly available da-
ta set3 of SO between July 31, 2008 and August 1,
2012. SO contains questions with various topics,
such as programming, mathematics, and English. In
this paper, we use SO C++ programming (SO/CPP)
3http://blog.stackoverflow.com/category/
cc-wiki-dump/
and mathematics4 (SO/Math) questions for our main
experiments. Additionally, we use the data of Math
Overflow5 (MO) for calibrating question difficulty
scores across communities (Sec. 3.3). The statistics
of these data sets are shown in Table 1.
SO/CPP SO/Math MO
# of questions 122, 012 51, 174 27, 333
# of answers 357, 632 94, 488 65, 966
# of users 67, 819 16, 961 12, 064
Table 1: The statistics of the data sets.
To evaluate the effectiveness of our proposed
model for estimating question difficulty scores, we
randomly sampled 300 question pairs from both
SO/CPP and SO/Math, and we asked experts to
compare the difficulty of every pair. We had two
graduate students majoring in computer science an-
notate the SO/CPP question pairs, and two gradu-
ate students majoring in mathematics annotate the
SO/Math question pairs. When annotating each
question pair, only the titles, descriptions, and tags
of the questions were shown, and other information
(e.g. users, answers, etc.) was excluded. Given each
pair of questions (q1 and q2), the annotators were
asked to give one of four labels: (1) q1 ? q2, which
means that the difficulty of q1 was higher than q2;
(2) q1 ? q2, which means that the difficulty of q1
was lower than q2; (3) q1 = q2, which means that
the difficulty of q1 was equal to q2; (4) Unknown,
which means that the annotator could not make a
decision. The agreements between annotators on
both SO/CPP (kappa value = 0.741) and SO/Math
(kappa value = 0.873) were substantial. When eval-
uating models, we only kept the pairs that annotators
had given the same labels. There were 260 SO/CPP
question pairs and 280 SO/Math question pairs re-
maining.
3.2 Accuracy of Question Difficulty Estimation
We employ a standard evaluation metric for infor-
mation retrieval: accuracy (Acc), defined as follows:
Acc = the number of correct pairwise comparisons
the total number of pairwise comparisons
.
We use the PageRank-based approach proposed
by Yang et al (2008) as a baseline. As described in
4http://math.stackexchange.com
5http://mathoverflow.net
87



	


 	     	     	 	 	 
  	






Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1521?1532,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Hierarchical Entity-based Approach to Structuralize User Generated
Content in Social Media: A Case of Yahoo! Answers
Baichuan Li1,2?, Jing Liu3?, Chin-Yew Lin4, Irwin King1,2, and Michael R. Lyu1,2
1Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China
2Department of Computer Science and Engineering
The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
3Harbin Institute of Technology, Harbin 150001, P.R. China
4Microsoft Research Asia, Beijing 100080, P.R. China
bcli@cse.cuhk.edu.hk jliu@ir.hit.edu.cn cyl@microsoft.com
{king,lyu}@cse.cuhk.edu.hk
Abstract
Social media like forums and microblogs have
accumulated a huge amount of user generated
content (UGC) containing human knowledge.
Currently, most of UGC is listed as a whole
or in pre-defined categories. This ?list-based?
approach is simple, but hinders users from
browsing and learning knowledge of certain
topics effectively. To address this problem, we
propose a hierarchical entity-based approach
for structuralizing UGC in social media. By
using a large-scale entity repository, we design
a three-step framework to organize UGC in
a novel hierarchical structure called ?cluster
entity tree (CET)?. With Yahoo! Answers as
a test case, we conduct experiments and the
results show the effectiveness of our frame-
work in constructing CET. We further evaluate
the performance of CET on UGC organiza-
tion in both user and system aspects. From
a user aspect, our user study demonstrates
that, with CET-based structure, users perform
significantly better in knowledge learning than
using traditional list-based approach. From
a system aspect, CET substantially boosts
the performance of two information retrieval
models (i.e., vector space model and query
likelihood language model).
1 Introduction
With the development of Web 2.0, social
media websites?such as online forums, blogs,
microblogs, social networks, and community
?This work was done when the first two authors were on
internship at MSRA.
Table 1: Sample questions about Edinburgh
1. Where can i buy a hamburger in Edinburgh?
2. Where can I get a shawarma in Edinburgh?
3. How long does it take to drive between Glasgow
and Edinburgh?
4. Whats the difference between Glasgow and Edinburgh?
5. Good hotels in London and Edinburgh?
6. Looking for nice , clean cheap hotel in Edinburgh?
7. Does anyone know of a reasonably cheap hotel in
Edinburgh that is near to Niddry Street South ?
8. Who can recommend a affordable hotel in
Edinburgh City Center?
question answering (CQA) portals?have become
the mainstream of web, where users create, share,
and exchange information with each other. As a
result, more and more UGC is accumulated, with
social media websites retaining a huge amount of
human knowledge and user experience. At present,
most of UGC is organized in a list structure with
extra information (e.g., category hierarchies in
online forums), or without any other information.
This ?list-of-content? (list-based approach) is
simple and straightforward, but ineffective for
browsing and knowledge learning. Consider
the following case: a user wants to spend his
vacation in Edinburgh. He visits a CQA website
to explore which aspects are mostly asked. In this
scenario, he may browse some relevant categories
like ?Travel:United Kingdom:Edinburgh? to get
useful information. He may also issue a query like
?travel in Edinburgh? to search relevant questions.
However, both the browsing and the searching give
the user a list of relevant contents (e.g., questions
shown in Table 1), not the direct knowledge. Thus,
the user has to read these contents, understand them,
classify them into various topics, and gain valuable
1521
HGLQEXUJK
^Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1115?1126,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Regularized Competition Model for Question Difficulty Estimation in
Community Question Answering Services
Quan Wang
?
Jing Liu
?
Bin Wang
?
Li Guo
?
?
Institute of Information Engineering, Chinese Academy of Sciences, Beijing, P. R. China
{wangquan,wangbin,guoli}@iie.ac.cn
?
Harbin Institute of Technology, Harbin, P. R. China
jliu@ir.hit.edu.cn
Abstract
Estimating questions? difficulty levels is
an important task in community question
answering (CQA) services. Previous stud-
ies propose to solve this problem based
on the question-user comparisons extract-
ed from the question answering threads.
However, they suffer from data sparseness
problem as each question only gets a lim-
ited number of comparisons. Moreover,
they cannot handle newly posted question-
s which get no comparisons. In this pa-
per, we propose a novel question difficul-
ty estimation approach called Regularized
Competition Model (RCM), which natu-
rally combines question-user comparisons
and questions? textual descriptions into a
unified framework. By incorporating tex-
tual information, RCM can effectively deal
with data sparseness problem. We further
employ a K-Nearest Neighbor approach to
estimate difficulty levels of newly post-
ed questions, again by leveraging textu-
al similarities. Experiments on two pub-
licly available data sets show that for both
well-resolved and newly-posted question-
s, RCM performs the estimation task sig-
nificantly better than existing methods,
demonstrating the advantage of incorpo-
rating textual information. More interest-
ingly, we observe that RCM might provide
an automatic way to quantitatively mea-
sure the knowledge levels of words.
1 Introduction
Recent years have seen rapid growth in communi-
ty question answering (CQA) services. They have
been widely used in various scenarios, including
general information seeking on the web
1
, knowl-
1
http://answers.yahoo.com/
edge exchange in professional communities
2
, and
question answering in massive open online cours-
es (MOOCs)
3
, to name a few.
An important research problem in CQA is
how to automatically estimate the difficulty lev-
els of questions, i.e., question difficulty estima-
tion (QDE). QDE can benefit many applications.
Examples include 1) Question routing. Routing
questions to appropriate answerers can help ob-
tain quick and high-quality answers (Li and K-
ing, 2010; Zhou et al., 2009). Ackerman and
McDonald (1996) have demonstrated that rout-
ing questions by matching question difficulty lev-
el with answerer expertise level will make better
use of answerers? time and expertise. This is even
more important for enterprise question answering
and MOOCs question answering, where human
resources are expensive. 2) Incentive mechanism
design. Nam et al. (2009) have found that win-
ning point awards offered by reputation system-
s is a driving factor for user participation in C-
QA services. Assigning higher point awards to
more difficult questions will significantly improve
user participation and satisfaction. 3) Linguistics
analysis. Researchers in computational linguistics
are always interested in investigating the correla-
tion between language and knowledge, to see how
the language reflects one?s knowledge (Church,
2011). As we will show in Section 5.4, QDE pro-
vides an automatic way to quantitatively measure
the knowledge levels of words.
Liu et al. (2013) have done the pioneer work
on QDE, by leveraging question-user comparison-
s extracted from the question answering threads.
Specifically, they assumed that the difficulty lev-
el of a question is higher than the expertise level
of the asker (i.e. the user who asked the question),
but lower than that of the best answerer (i.e. the us-
er who provided the best answer). A TrueSkill al-
2
http://stackoverflow.com/
3
http://coursera.org/
1115
gorithm (Herbrich et al., 2006) was further adopt-
ed to estimate question difficulty levels as well as
user expertise levels from the pairwise compar-
isons among them. To our knowledge, it is the on-
ly existing work on QDE. Yang et al. (2008) have
proposed a similar idea, but their work focuses on
a different task, i.e., estimating difficulty levels of
tasks in crowdsourcing contest services.
There are two major drawbacks of previous
methods: 1) data sparseness problem and 2) cold-
start problem. By the former, we mean that un-
der the framework of previous work, each question
is compared only twice with the users (once with
the asker and the other with the best answerer),
which might not provide enough information and
contaminate the estimation accuracy. By the latter,
we mean that previous work only deals with well-
resolved questions which have received the best
answers, but cannot handle newly posted question-
s with no answers received. In many real-world
applications such as question routing and incentive
mechanism design, however, it is usually required
that the difficulty level of a question is known in-
stantly after it is posted.
To address the drawbacks, we propose further
exploiting questions? textual descriptions (e.g., ti-
tle, body, and tags) to perform QDE. Preliminary
observations have shown that a question?s difficul-
ty level can be indicated by its textual descrip-
tion (Liu et al., 2013). We take advantage of the
observations, and assume that if two questions are
close in their textual descriptions, they will also
be close in their difficulty levels, i.e., the smooth-
ness assumption. We employ manifold regular-
ization (Belkin et al., 2006) to characterize the
assumption. Manifold regularization is a well-
known technique to preserve local invariance in
manifold learning algorithms, i.e., nearby points
are likely to have similar embeddings (Belkin and
Niyogi, 2001). Then, we propose a novel Reg-
ularized Competition Model (RCM), which for-
malizes QDE as minimizing a loss on question-
user comparisons with manifold regularization on
questions? textual descriptions. As the smoothness
assumption offers extra information for inferring
question difficulty levels, incorporating it will ef-
fectively deal with data sparsity. Finally, we adopt
a K-Nearest Neighbor approach (Cover and Hart,
1967) to perform cold-start estimation, again by
leveraging the smoothness assumption.
Experiments on two publicly available data sets
collected from Stack Overflow show that 1) RCM
performs significantly better than existing meth-
ods in the QDE task for both well-resolved and
cold-start questions. 2) The performance of RCM
is insensitive to the particular choice of the term
weighting schema (determines how a question?s
textual description is represented) and the similar-
ity measure (determines how the textual similarity
between two questions is measured). The results
demonstrate the advantage of incorporating textu-
al information for QDE. Qualitative analysis fur-
ther reveals that RCM might provide an automatic
way to quantitatively measure the knowledge lev-
els of words.
The main contributions of this paper include: 1)
We take fully advantage of questions? textual de-
scriptions to address data sparseness problem and
cold-start problem which previous QDE methods
suffer from. To our knowledge, it is the first time
that textual information is introduced in QDE. 2)
We propose a novel QDE method that natural-
ly combines question-user comparisons and ques-
tions? textual descriptions into a unified frame-
work. The proposed method performs QDE sig-
nificantly better than existing methods. 3) We
demonstrate the practicability of estimating diffi-
culty levels of cold-start questions purely based on
their textual descriptions, making various applica-
tions feasible in practice. As far as we know, it is
the first work that considers cold-start estimation.
4) We explore how a word?s knowledge level can
be automatically measured by RCM.
The rest of the paper is structured as follows.
Section 2 describes the problem formulation and
the motivation of RCM. Section 3 presents the de-
tails of RCM. Section 4 discusses cold-start esti-
mation. Section 5 reports experiments and results.
Section 6 reviews related work. Section 7 con-
cludes the paper and discusses future work.
2 Preliminaries
2.1 Problem Formulation
A CQA service provides a platform where people
can ask questions and seek answers from others.
Given a CQA portal, consider a specific catego-
ry where questions on the same topic are asked
and answered, e.g., the ?C++ programming? cat-
egory of Stack Overflow. When an asker u
a
posts
a question q in the category, there will be sever-
al answerers to answer the question. Among all
the received answers, a best one will be chosen
1116
by the asker or voted by the community. The an-
swerer who provides the best answer is called the
best answerer u
b
. The other answerers are denoted
by O =
{
u
o
1
, u
o
2
, ? ? ? , u
o
M
}
. A question answering
thread (QA thread) is represented as a quadruplet
(
q, u
a
, u
b
,O
)
. Collecting all such QA threads in the
category, we get M users and N questions, denoted
byU = {u
1
, u
2
, ? ? ? , u
M
} and Q = {q
1
, q
2
, ? ? ? , q
N
}
respectively. Each user u
m
is associated with an
expertise score ?
m
, representing his/her expertise
level. A larger ?
m
indicates a higher expertise lev-
el of the user. Each question q
n
is associated with
a difficulty score ?
n
, representing its difficulty lev-
el. A larger ?
n
indicates a higher difficulty level
of the question. Difficulty scores (as well as ex-
pertise scores) are assumed to be comparable with
each other in the specified category. Besides, each
question q
n
has a textual description, and is repre-
sented as a V-dimensional term vector d
n
, where
V is the vocabulary size.
The question difficulty estimation (QDE) task
aims to automatically learn the question difficul-
ty scores (?
n
?s) by utilizing the QA threads T =
{
(
q, u
a
, u
b
,O
)
: q ? Q} as well as the question de-
scriptions D = {d
1
, d
2
, ? ? ? , d
N
} in the specified
category. Note that in Section 2 and Section 3, we
consider estimating difficulty scores of resolved
questions, i.e., questions with the best answers se-
lected or voted. Estimating difficulty scores of un-
resolved questions, e.g., newly posted ones, will
be discussed in Section 4.
2.2 Competition-based Methods
Liu et al. (2013) have proposed a competition-
based method for QDE. The key idea is to 1) ex-
tract pairwise competitions from the QA threads
and 2) estimate question difficulty scores based on
extracted competitions.
To extract pairwise competitions, it is assumed
that question difficulty scores and user expertise
scores are expressed on the same scale. Given a
QA thread
(
q, u
a
, u
b
,O
)
, it is further assumed that:
Assumption 1 (pairwise comparison assumption)
The difficulty score of question q is higher than the
expertise score of the asker u
a
, but lower than that
of the best answerer u
b
. Moreover, the expertise
score of the best answerer u
b
is higher than that
of the asker u
a
, as well as any answerer in O.
4
4
The difficulty score of question q is not assumed to be
lower than the expertise score of any answerer in O, since
such a user may just happen to see the question and respond
to it, rather than knowing the answer well.
Given the assumption, there are
(
|O| + 3
)
pairwise
competitions extracted from the QA thread, in-
cluding 1) one competition between the question
q and the asker u
a
, 2) one competition between
the question q and the best answerer u
b
, 3) one
competition between the best answerer u
b
and the
asker u
a
, and 4) |O| competitions between the best
answerer u
b
and each of the answerers in O. The
question q is the winner of the first competition,
and the best answerer u
b
is the winner of the re-
maining
(
|O| + 2
)
competitions. These pairwise
competitions are denoted by
C
q
=
{
u
a
?q, q?u
b
, u
a
?u
b
, u
o
1
?u
b
, ? ? ? , u
o
M
?u
b
}
,
where i ? j means that competitor j beats com-
petitor i in a competition. Let
C =
?
q?Q
C
q
(1)
be the set containing all the pairwise competitions
extracted from T .
Given the competition set C, Liu et al. (2013)
further adopted a TrueSkill algorithm (Herbrich
et al., 2006) to learn the competitors? skill level-
s (i.e. the question difficulty scores and the us-
er expertise scores). TrueSkill assumes that the
practical skill level of each competitor follows a
normal distribution N
(
?, ?
2
)
, where ? is the aver-
age skill level and ? is the estimation uncertain-
ty. Then it updates the estimations in an online
mode: for a newly observed competition with its
win-loss result, 1) increase the average skill level
of the winner, 2) decrease the average skill level
of the loser, and 3) shrink the uncertainties of both
competitors as more data has been observed. Yang
et al. (2008) have proposed a similar competition-
based method to estimate tasks? difficulty levels
in crowdsourcing contest services, by leveraging
PageRank (Page et al., 1999) algorithm.
2.3 Motivating Discussions
The methods introduced above estimate competi-
tors? skill levels based solely on the pairwise com-
petitions among them. The more competitions a
competitor participates in, the more accurate the
estimation will be. However, according to the
pairwise comparison assumption (Assumption 1),
each question participates in only two competi-
tions, one with the asker and the other with the
best answerer. Hence, there might be no enough
information to accurately infer its difficulty score.
We call this the data sparseness problem.
1117
(a) Low difficulty. (b) Medium difficulty. (c) High difficulty.
Figure 1: Tag clouds of SO/Math questions with different difficulty levels.
Taking advantage of additional metadata has
been demonstrated to be an effective way of deal-
ing with data sparsity in various applications such
as collaborative filtering (Claypool et al., 1999;
Schein et al., 2002) and personalized search (Dou
et al., 2007; Sugiyama et al., 2004). The ratio-
nale behind is to bridge the gap among users/items
by leveraging their similarities based on the meta-
data. As for QDE, preliminary observations have
shown that a question?s difficulty level can be in-
dicated by its textual description (Liu et al., 2013).
As an example, consider the QA threads in the
?mathematics? category of Stack Overflow. Di-
vide the questions into three groups: 1) low dif-
ficulty, 2) medium difficulty, and 3) high difficul-
ty, according to their difficulty scores estimated by
TrueSkill. Figure 1 visualizes the frequency dis-
tribution of tags in each group, where the size of
each tag is in proportion to its frequency in the
group. The results indicate that the tags associ-
ated with the questions do have the ability to re-
flect the questions? difficulty levels, e.g., low dif-
ficulty questions usually have tags such as ?home-
work? and ?calculus?, while high difficulty ones
usually have tags such as ?general topology? and
?number theory?. We further calculate the Pearson
correlation coefficient (Rodgers and Nicewander,
1988) between 1) the gap between the averaged
difficulty scores in each two groups and 2) the
Euclidean distance between the aggregated textu-
al descriptions in each two groups . The result is
r = 0.6424, implying that the difficulty gap is posi-
tively correlated with the textual distance. In other
words, the more similar two questions? textual de-
scriptions are, the more close their difficulty levels
are. Therefore, we take the textual information to
bridge the difficulty gap among questions, by as-
suming that
Assumption 2 (smoothness assumption) If two
questions q
i
and q
j
are close in their textual de-
scriptions d
i
and d
j
, they will also be close in their
difficulty scores ?
i
and ?
j
.
The smoothness assumption brings us additional
information about question difficulty scores by in-
ferring textual similarities. It serves as a supple-
ment to the pairwise competitions, and might help
address the data sparseness problem which previ-
ous methods suffer from.
3 Modeling Text Similarities for QDE
This section presents a novel Regularized Compe-
tition Model (RCM) for QDE, which combines the
pairwise competitions and the textual descriptions
into a unified framework. RCM can alleviate the
data sparseness problem and perform more accu-
rate estimation.
3.1 Regularized Competition Model
We start with several notations. As question dif-
ficulty scores can be directly compared with user
expertise scores, we take questions as pseudo user-
s. Let
?
? ? R
M+N
denote the skill levels (i.e. the
expertise scores and the difficulty scores) of all the
(pseudo) users:
?
?
i
=
{
?
i
, 1 ? i ? M,
?
i?M
, M < i ? M + N,
where
?
?
i
is the i-th entry of
?
?. The first M entries
are the user expertise scores, denoted by
?
?
u
? R
M
.
The last N entries are the question difficulty s-
cores, denoted by
?
?
q
? R
N
. Let
?
?
(u)
i
and
?
?
(q)
i
denote
the i-th entries of
?
?
u
and
?
?
q
respectively.
Exploiting Pairwise Competitions. We define
a loss on each pairwise competition i ? j:
?
(
?
?
i
,
?
?
j
)
= max
(
0, ? ?
(
?
?
j
?
?
?
i
))
p
, (2)
where p is either 1 or 2. The loss is defined on the
skill gap between the two competitors, i.e.,
?
?
j
?
?
?
i
,
1118
measuring the inconsistency between the expect-
ed outcome and the actual outcome. If the gap is
larger than a predefined threshold ?, competitor j
would probably beat competitor i in the compe-
tition, which coincides with the actual outcome.
Then the loss will be zero. Otherwise, there is a
higher chance that competitor j loses the competi-
tion, which goes against the actual outcome. Then
the loss will be greater than zero. The smaller the
gap is, the higher the chance of inconsistency be-
comes, and the greater the loss will be. Note that
the threshold ? can take any positive value since
we do not pose a norm constraint on
?
?.
5
Without
loss of generality we take ? = 1 throughout this
paper. As we will show in Section 3.2, the loss de-
fined in Eq. (2) has some similarity with the SVM
loss (Chapelle, 2007). We name it hinge loss when
p = 1, and quadratic loss when p = 2.
Given the competition set C, estimating skil-
l levels of (pseudo) users then amounts to solving
the following optimization problem:
min
?
?
?
(i? j)?C
?
(
?
?
i
,
?
?
j
)
+
?
1
2
?
?
T
?
?, (3)
where the first term is the empirical loss measur-
ing the total inconsistency; the second term is a
regularizer to prevent overfitting; and ?
1
? 0 is a
trade-off coefficient. It is also a competition-based
QDE method, called Competition Model (CM).
Exploiting Question Descriptions. Manifold
regularization is a well-known technique used in
manifold learning algorithms to preserve local in-
variance, i.e., nearby points are likely to have sim-
ilar embeddings (Belkin and Niyogi, 2001). In
QDE, the smoothness assumption expresses sim-
ilar ?invariance?, i.e., nearby questions (in terms
of textual similarities) are likely to have similar
difficulty scores. Hence, we characterize the as-
sumption with the following manifold regularizer:
R =
1
2
N
?
i=1
N
?
j=1
(
?
?
(q)
i
?
?
?
(q)
j
)
2
w
i j
=
?
?
T
q
D
?
?
q
?
?
?
T
q
W
?
?
q
=
?
?
T
q
L
?
?
q
, (4)
where w
i j
is the textual similarity between ques-
tion i and question j; W ? R
N?N
is the similarity
matrix with the (i, j)-th entry being w
i j
; D ? R
N?N
is a diagonal matrix with the i-th entry on the diag-
onal being d
ii
=
?
N
j=1
w
i j
; and L = D?W ? R
N?N
5
Given any
?
?
i
,
?
?
j
, and ?, there always exists a linear trans-
formation which keeps the sign of
(
? ?
(
?
?
j
?
?
?
i
))
unchanged.
is the graph Laplacian (Chung, 1997). Minimizing
R results in the smoothness assumption: for any
questions i and j, if their textual similarity w
i j
is
high, the difficulty gap
(
?
?
(q)
i
?
?
?
(q)
j
)
2
will be small.
A Hybrid Method. Combining Eq. (3) and
Eq. (4), we obtain RCM, which amounts to the
following optimization problem:
min
?
?
?
(i? j)?C
?
(
?
?
i
,
?
?
j
)
+
?
1
2
?
?
T
?
? +
?
2
2
?
?
T
q
L
?
?
q
. (5)
Here ?
2
? 0 is also a trade-off coefficient. The
advantages of RCM include 1) It naturally formal-
izes QDE as minimizing a manifold regularized
loss function, which seamlessly integrates both the
pairwise competitions and the textual description-
s. 2) By incorporating textual information, it can
address the data sparseness problem which previ-
ous methods suffer from, and perform significantly
better in the QDE task.
3.2 Learning Algorithm
Redefine the k-th pairwise competition (assumed
to be carried out between competitors i and j) as
(
x
k
, y
k
)
. x
k
? R
M+N
indicates the competitors:
x
(k)
i
= 1, x
(k)
j
= ?1, and x
(k)
l
= 0 for any l , i, j,
where x
(k)
l
is the l-th entry of x
k
. y
k
? {1,?1} is
the outcome: if competitor i beats competitor j,
y
k
= 1; otherwise, y
k
= ?1. The objective in Eq.
(5) can then be rewritten as
L
(
?
?
)
=
|C|
?
k=1
max
(
0, 1 ? y
k
(
?
?
T
x
k
))
p
+
1
2
?
?
T
Z
?
?,
where Z =
(
?
1
I
M
0
0 ?
1
I
N
+ ?
2
L
)
is a block matrix; I
M
?
R
M?M
and I
N
? R
N?N
are identity matrices; p =
1 corresponds to the hinge loss, and p = 2 the
quadratic loss. It is clear that the loss defined in
Eq. (2) has the same format as the SVM loss.
The objectiveL is differentiable for the quadrat-
ic loss but non-differentiable for the hinge loss.
We employ a subgradient method (Boyd et al.,
2003) to solve the optimization problem. The al-
gorithm starts at a point
?
?
0
and, as many iterations
as needed, moves from
?
?
t
to
?
?
t+1
in the direction
of the negative subgradient:
?
?
t+1
=
?
?
t
? ?
t
?L
(
?
?
t
)
,
1119
Algorithm 1 Regularized Competition Model
Require: competition set C and description setD
1:
?
?
0
? 1
2: for t = 0 : T ? 1 do
3: K
t
?
{
k : 1 ? y
k
(
?
?
T
t
x
k
)
> 0
}
4: ?L
(
?
?
t
)
? calculated by Eq. (6)
5:
?
?
t+1
?
?
?
t
? ?
t
?L
(
?
?
t
)
6: ?
t+1
?
{
?
?
0
,
?
?
1
, ? ? ? ,
?
?
t+1
}
7:
?
?
t+1
? argmin
?
???
t+1
L
(
?
?
)
8: end for
9: return
?
?
T
where ?
t
> 0 is the learning rate. The subgradient
is calculated as
?L
(
?
?
t
)
=
?
?
?
?
?
?
?
?
?
Z
?
?
t
?
?
k?K
t
y
k
x
k
, p=1,
Z
?
?
t
+ 2
?
k?K
t
x
k
x
T
k
?
?
t
? 2
?
k?K
t
y
k
x
k
, p=2,
(6)
where K
t
=
{
k : 1 ? y
k
(
?
?
T
t
x
k
)
> 0
}
. As it is not
always a descent method, we keep track of the best
point found so far (Boyd et al., 2003):
?
?
t+1
= arg min
?
???
t+1
L
(
?
?
)
,
where?
t+1
=
{
?
?
0
,
?
?
1
, ? ? ? ,
?
?
t+1
}
. The whole proce-
dure is summarized in Algorithm 1.
Convergence. For constant learning rate (i.e.,
?
t
= ?), Algorithm 1 is guaranteed to converge to
within some range of the optimal value, i.e.,
lim
t??
L
(
?
?
t
)
? L
?
< ?,
where L
?
denotes the minimum of L(?), and ? is a
constant defined by the learning rate ?. For more
details, please refer to (Boyd et al., 2003). During
our experiments, we set the iteration number as
T = 1000 and the learning rate as ?
t
= 0.001, and
convergence was observed.
Complexity. For both the hinge loss and the
quadratic loss, the time complexity (per itera-
tion) and the space complexity of RCM are both
O
(
|C| + ?N
2
)
. Here, |C| is the total number of
competitions, M and N are the numbers of user-
s and questions respectively, and ? is the ratio of
non-zero entries in the graph Laplacian L.
6
In the
analysis, we have assumed that M ? ?N
2
and
N ? ?N
2
.
6
Owing to the sparse nature of questions? textual descrip-
tions, the graph Laplacian L is usually sparse, with about
70% entries being zero according to our experiments.
4 Cold-Start Estimation
Previous sections discussed estimating difficulty s-
cores of resolved questions, from which pairwise
competitions could be extracted. However, for
newly posted questions without any answers re-
ceived, no competitions could be extracted and
none of the above methods work. We call it the
cold-start problem.
We heuristically apply a K-Nearest Neighbor
(KNN) approach (Cover and Hart, 1967) to cold-
start estimation, again by leveraging the smooth-
ness assumption. The key idea is to propagate
difficulty scores from well-resolved questions to
cold-start ones according to their textual simi-
larities. Specifically, suppose that there exists
a set of well-resolved questions whose difficul-
ty scores have already been estimated by a QDE
method. Given a cold-start question q
?
, we first
pick K well-resolved questions that are closest to
q
?
in textual descriptions, referred to as the near-
est neighbors. The difficulty score of question q
?
is then predicted as the averaged difficulty scores
of its nearest neighbors. The KNN method bridges
the gap between cold-start and well-resolved ques-
tions by inferring their textual similarities, and
might effectively deal with the cold-start problem.
5 Experiments
We have conducted experiments to test the effec-
tiveness of RCM in estimating difficulty scores of
both well-resolved and cold-start questions. More-
over, we have explored how a word?s difficulty lev-
el can be quantitatively measured by RCM.
5.1 Experimental Settings
Data Sets. We obtained a publicly available da-
ta set of Stack Overflow between July 31, 2008
and August 1, 2012
7
, containing QA threads in
various categories. We considered the categories
of ?C++ programming? and ?mathematics?, and
randomly sampled about 10,000 QA threads from
each category, denoted by SO/CPP and SO/Math
respectively. For each question, we took the title
and body fields as its textual description. For both
data sets, stop words in a standard list
8
and words
whose total frequencies are less than 10 were re-
moved. Table 1 gives the statistics of the data sets.
7
http://blog.stackoverflow.com/category/cc-wiki-dump/
8
http://jmlr.org/papers/volume5/lewis04a/a11-smart-
stop-list/english.stop
1120
# users # questions # competitions # words
SO/CPP 14,884 10,164 50,043 2,208
SO/Math 6,564 10,528 40,396 2,009
Table 1: Statistics of the data sets.
For evaluation, we randomly sampled 600 ques-
tion pairs from each data set, and asked annotators
to compare the difficulty levels of the questions
in each pair. We had two graduate students ma-
joring in computer science annotate the SO/CPP
questions, and two majoring in mathematics an-
notate the SO/Math questions. For each question,
only the title, body, and tags were exposed to the
annotators. Given a question pair
(
q
1
, q
2
)
, the an-
notators were asked to give one of the three labels:
q
1
? q
2
, q
2
? q
1
, or q
1
= q
2
, which respective-
ly means that question q
1
has a higher, lower, or
equal difficulty level compared with question q
2
.
We used Cohen?s kappa coefficient (Cohen, 1960)
to measure the inter-annotator agreement. The re-
sult is ? = 0.7533 on SO/CPP and ? = 0.8017
on SO/Math, indicating that the inter-annotator a-
greement is quite substantial on both data sets. Af-
ter removing the question pairs with inconsisten-
t labels, we got 521 annotated SO/CPP question
pairs and 539 annotated SO/Math question pairs.
We further randomly split the annotated ques-
tion pairs into development/test/cold-start sets,
with the ratio of 2:2:1. The first two sets were used
to evaluate the methods in estimating difficulty s-
cores of resolved questions. Specifically, the de-
velopment set was used for parameter tuning and
the test set was used for evaluation. The last set
was used to evaluate the methods in cold-start esti-
mation, and the questions in this set were excluded
from the learning process of RCM as well as any
baseline method.
Baseline Methods. We considered three base-
line methods: PageRank (PR), TrueSkill (TS), and
CM, which are based solely on the pairwise com-
petitions.
? PR first constructs a competitor graph, by
creating an edge from competitor i to com-
petitor j if j beats i in a competition. A
PageRank algorithm (Page et al., 1999) is
then utilized to estimate the relative impor-
tance of the nodes, i.e., question difficulty s-
cores and user expertise scores. The damping
factor was set from 0.1 to 0.9 in steps of 0.1.
? TS has been applied to QDE by Liu et al.
(2013). We set the model parameters in the
same way as they suggested.
? CM performs QDE by solving Eq. (3). We
set ?
1
in {0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1}.
We compared RCM with the above baseline meth-
ods. In RCM, both parameters ?
1
and ?
2
were set
in {0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1}.
Evaluation Metric. We employed accuracy
(ACC) as the evaluation metric:
ACC =
# correctly judged question pairs
# all question pairs
.
A question pair is regarded as correctly judged if
the relative difficulty ranking given by an estima-
tion method is consistent with that given by the
annotators. The higher the accuracy is, the better
a method performs.
5.2 Estimation for Resolved Questions
The first experiment tested the methods in estimat-
ing difficulty scores of resolved questions.
Estimation Accuracies. We first compared the
estimation accuracies of PR, TS, CM, and RCM
on the test sets of SO/CPP and SO/Math, obtained
with the best parameter settings determined by the
development sets. Table 2 gives the results, where
?H? denotes the hinge loss and ?Q? the quadratic
loss. In RCM, to calculate the graph Laplacian L,
we adopted Boolean term weighting schema and
took Jaccard coefficient as the similarity measure.
From the results, we can see that 1) RCM perform-
s significantly better than the baseline methods on
both data sets (t-test, p-value < 0.05), demonstrat-
ing the advantage of exploiting questions? textu-
al descriptions for QDE. 2) The improvements of
RCM over the baseline methods on SO/Math are
greater than those on SO/CPP, indicating that the
textual descriptions of the SO/Math questions are
more powerful in reflecting their difficulty level-
s. The reason is that the SO/Math questions are
much more heterogeneous, belonging to various
subfields of mathematics. The difficulty gaps a-
mong different subfields are sometimes obvious
(e.g., a question in topology in general has a high-
er difficulty level than a question in linear algebra),
making the textual descriptions more powerful in
distinguishing the difficulty levels.
Graph Laplacian Variants. We further inves-
tigated the performances of different term weight-
ing schemas and similarity measures in the graph
1121
PR TS
CM RCM
H Q H Q
SO/CPP 0.5876 0.6134 0.6340 0.6753 0.7371 0.7268
SO/Math 0.6067 0.6109 0.6527 0.6820 0.7699 0.7699
Table 2: ACC of different methods for well-
resolved questions.
Notation Definition
Boolean v(w, q) =
?
?
?
?
?
?
?
1, if word w occurs in question q
0, otherwise
TF-1 v(w, q) = f (w, q), the number of occurrences
TF-2 v(w, q) = log ( f (w, q) + 1)
TF-3 v(w, q) = 0.5 +
0.5 ? f (w, q)
max { f (w, q) : w ? q}
TFIDF-1 v(w, q) = TF-1 ? log
|Q|
|{q?Q:w?q}|
TFIDF-2 v(w, q) = TF-2 ? log
|Q|
|{q?Q:w?q}|
TFIDF-3 v(w, q) = TF-3 ? log
|Q|
|{q?Q:w?q}|
Cosine Sim (d
1
, d
2
) =
d
T
1
d
2
?d
1
???d
2
?
? [0, 1]
Jaccard Sim
(
d
1
, d
2
)
=
d
T
1
d
2
?d
1
?
2
+?d
2
?
2
??d
1
???d
2
?
? [0, 1]
Table 3: Different term weighting schemas and
similarity measures.
Laplacian. The term weighting schema deter-
mines how a question?s textual description is rep-
resented. We explored a Boolean schema, three
TF schemas, and three TFIDF schemas (Salton
and Buckley, 1988). The similarity measure de-
termines how the textual similarity between two
questions is calculated. We explored the Co-
sine similarity and the Jaccard coefficient (Huang,
2008). Detailed descriptions are given in Table 3.
Figure 2 and Figure 3 show the estimation ac-
curacies of the RCM variants on the test sets of
SO/CPP and SO/Math respectively, again obtained
with the best parameter settings determined by
the development sets. The performance of CM
is also given (the straight lines in the figures).
9
From the results, we can see that 1) All the RCM
variants can improve over CM on both data sets,
and most of the improvements are significant (t-
test, p-value < 0.05). This further demonstrates
that the effectiveness of incorporating textual de-
scriptions is not affected by the particular choice
of the term weighting schema or similarity mea-
sure. 2) Boolean term weighting schema performs
the best, considering different similarity measures,
loss types, and data sets collectively. 3) Jaccard
9
CM performs better than PR and TS on both data sets.
0.55
0.6
0.65
0.7
0.75 Cosine Jaccard CM(H)
(a) Hinge loss.
0.55
0.6
0.65
0.7
0.75 Cosine Jaccard CM(Q)
(b) Quadratic loss.
Figure 2: ACC of RCM variants for well-resolved
questions on SO/CPP.
0.6
0.65
0.7
0.75
0.8 Cosine Jaccard CM(H)
(a) Hinge loss.
0.6
0.65
0.7
0.75
0.8 Cosine Jaccard CM(Q)
(b) Quadratic loss.
Figure 3: ACC of RCM variants for well-resolved
questions on SO/Math.
coefficient performs as well as Cosine similari-
ty on SO/Math, but almost consistently better on
SO/CPP. Throughout the experiments, we adopted
Boolean term weighting schema and Jaccard coef-
ficient to calculate the graph Laplacian.
5.3 Estimation for Cold-Start Questions
The second experiment tested the methods in es-
timating difficulty scores of cold-start questions.
We employed Boolean term weighting schema to
represent a cold-start question, and utilized Jac-
card Coefficient to select its nearest neighbors.
Figure 4 and Figure 5 list the cold-start estima-
tion accuracies of different methods on SO/CPP
and SO/Math respectively, with different K val-
ues (the number of nearest neighbors). As the
accuracy oscillates drastically with a K value s-
maller than 11 on SO/CPP and smaller than 6 on
SO/Math, we report the results with K ? [11, 20]
on SO/CPP and K ? [6, 15] on SO/Math. The av-
eraged (over different K values) cold-start estima-
tion accuracies are further given in Table 4. All the
results are reported on the cold-start sets, with the
optimal parameter settings adopted in Section 5.2.
From the results, we can see that 1) Cold-start es-
timation is possible, and can achieve a consider-
ably high accuracy by choosing a proper method
(e.g. RCM), making applications such as better
question routing and better incentive mechanism
1122
0.45
0.55
0.65
0.75
11 12 13 14 15 16 17 18 19 20
Acc
urac
y
K
PR TS CM(H)CM(Q) RCM(H) RCM(Q)
Figure 4: ACC of different methods for cold-start
questions on SO/CPP.
0.5
0.6
0.7
0.8
6 7 8 9 10 11 12 13 14 15
Acc
urac
y
K
PR TS CM(H)CM(Q) RCM(H) RCM(Q)
Figure 5: ACC of different methods for cold-start
questions on SO/Math.
design feasible in practice. 2) As the value of K
varies, RCM (the red/blue solid line) performs al-
most consistently better than CM with the same
loss type (the red/blue dotted line), as well as PR
and TS (the gray dotted lines), showing the advan-
tages of RCM in the cold-start estimation. 3) The
cold-start estimation accuracies on SO/Math are
higher than those on SO/CPP, again demonstrating
that the textual descriptions of the SO/Math ques-
tions are more powerful in reflecting their difficul-
ty levels. This is consistent with the phenomenon
observed in Section 5.2.
5.4 Difficulty Levels of Words
The third experiment explored how a word?s diffi-
culty level can be measured by RCM automatical-
ly and quantitatively.
On both SO/CPP and SO/Math, we evenly split
the range of question difficulty scores (estimated
by RCM) into 10 buckets, and assigned questions
to the buckets according to their difficulty scores.
A larger bucket ID indicates a higher difficulty lev-
el. Then, given a word w, we calculated its fre-
quency in each bucket as follows:
f
i
(w) =
# questions in bucket i where w occurs
# all questions in bucket i
.
To make the frequency meaningful, buckets with
less than 50 questions were discarded. We picked
PR TS
CM RCM
H Q H Q
SO/CPP 0.5870 0.5413 0.6120 0.6304 0.6380 0.6609
SO/Math 0.6411 0.6305 0.6653 0.7263 0.6958 0.7442
Table 4: Averaged ACC of different methods for
cold-start questions.
0
0.4
0.8
1.2
3 3.5 4 4.5 5 5.5 6 6.5 7
Occ
urre
nce 
freq
uenc
y
Question buckets
array string virtual multithread
Figure 6: Frequencies of different words in the
buckets on SO/CPP.
four words from each data set as examples. Their
normalized frequencies in different buckets are
shown in Figure 6 and Figure 7. On SO/CPP,
we can observe that ?array? and ?string? occur
most frequently in questions with lower difficul-
ty levels, ?virtual? higher, and ?multithread? the
highest. It coincides with the intuition: ?array?
and ?string? are usually related to some basic con-
cepts in programming language, while ?virtual?
and ?multithread? usually discuss more advanced
topics. Similar phenomena can be observed on
SO/Math. The results indicate that RCM might
provide an automatic way to measure the difficul-
ty levels of words.
6 Related Work
QDE is relevant to the problem of estimating task
difficulty levels and user expertise levels in crowd-
sourcing services (Yang et al., 2008; Whitehill et
al., 2009). Studies on this problem fall into two
categories: 1) binary response based and 2) par-
tially ordered response based. In the first cate-
gory, binary responses (i.e. whether the solution
provided by a user is correct or not) are observed,
and techniques based on item response theory are
further employed (Whitehill et al., 2009; Welin-
der et al., 2010; Zhou et al., 2012). In the second
category, partially ordered responses (i.e. which
of the two given solutions is better) are observed,
and pairwise comparison based methods are fur-
ther adopted (Yang et al., 2008; Liu et al., 2013).
QDE belongs to the latter.
1123
00.4
0.8
1.2
4 5 6 7 8 9
Occ
urre
nce 
freq
uenc
y
Question buckets
homework calculus ring topology
Figure 7: Frequencies of different words in the
buckets on SO/Math.
The most relevant work to ours is a pairwise
comparison based approach proposed by Liu et al.
(2013) to estimate question difficulty levels in C-
QA services. They have also demonstrated that
a similar approach can be utilized to estimate us-
er expertise levels (Liu et al., 2011). Yang et al.
(2008) and Chen et al. (2013) have also proposed
pairwise comparison based methods, for task dif-
ficulty estimation and rank aggregation in crowd-
sourcing settings. Our work differs from previous
pairwise comparison based methods in that it fur-
ther utilizes textual information, formalized as a
manifold regularizer.
Manifold regularization is a geometrically mo-
tivated framework for machine learning, enforcing
the learning model to be smooth w.r.t. the geomet-
rical structure of data (Belkin et al., 2006). Within
the framework, dimensionality reduction (Belkin
and Niyogi, 2001; Cai et al., 2008) and semi-
supervised learning (Zhou et al., 2004; Zhu and
Lafferty, 2005) algorithms have been constructed.
In dimensionality reduction, manifold regulariza-
tion is utilized to guarantee that nearby points will
have similar low-dimensional representations (Cai
et al., 2008), while in semi-supervised learning it
is utilized to ensure that nearby points will have
similar labels (Zhou et al., 2004). In our work, we
assume that nearby questions (in terms of textual
similarities) will have similar difficulty levels.
Predicting reading difficulty levels of text is
also a relevant problem (Collins-Thompson and
Callan, 2004; Schwarm and Ostendorf, 2005). It
is a key to automatically finding materials at ap-
preciate reading levels for students, and also helps
in personalized web search (Collins-Thompson et
al., 2011). In the task of predicting reading dif-
ficulty levels, documents targeting different grade
levels are taken as ground truth, which can be eas-
ily obtained from the web. However, there is no
naturally annotated data for our QDE task on the
web. Other related problems include query dif-
ficulty estimation for search engines (Carmel et
al., 2006; Yom-Tov et al., 2005) and question dif-
ficulty estimation for automatic question answer-
ing systems (Lange et al., 2004). In these tasks,
query/question difficulty is system-oriented and ir-
relevant with human knowledge, which is a differ-
ent setting from ours.
7 Conclusion and Future Work
In this paper, we have proposed a novel method for
estimating question difficulty levels in CQA ser-
vices, called Regularized Competition Model (R-
CM). It takes fully advantage of questions? textu-
al descriptions besides question-user comparisons,
and thus can effectively deal with data sparsity and
perform more accurate estimation. A K-Nearest
Neighbor approach is further adopted to estimate
difficulty levels of cold-start questions. Experi-
ments on two publicly available data sets show
that RCM performs significantly better than exist-
ing methods in the estimation task, for both well-
resolved and cold-start questions, demonstrating
the advantage of incorporating textual informa-
tion. It is also observed that RCM might automat-
ically measure the knowledge levels of words.
As future work, we plan to 1) Enhance the ef-
ficiency and scalability of RCM. The complexity
analysis in Section 3.2 indicates that storing and
processing the graph Laplacian is a bottleneck of
RCM. We would like to investigate how to deal
with the bottleneck, e.g., via parallel or distribut-
ed computing. 2) Apply RCM to non-technical
domains. For non-technical domains such as the
?news and events? category of Yahoo! Answer-
s, there might be no strongly distinct notions of
?experts? and ?non-experts?, and it might be more
difficult to distinguish between ?hard questions?
and ?easy questions?. It is worthy investigating
whether RCM still works on such domains.
Acknowledgments
We would like to thank the anonymous review-
ers for their helpful comments. This work is
supported by the Strategic Priority Research Pro-
gram of the Chinese Academy of Sciences (grant
No. XDA06030200), the National Key Technolo-
gy R&D Program (grant No. 2012BAH46B03),
and the National Natural Science Foundation of
China (grant No. 61272427).
1124
References
Mark S. Ackerman and David W. McDonald. 1996.
Answer garden 2: merging organizational memory
with collaborative help. In Proceedings of the 1996
ACM Conference on Computer Supported Coopera-
tive Work, pages 97?105.
Mikhail Belkin and Partha Niyogi. 2001. Laplacian
eigenmaps and spectral techniques for embedding
and clustering. In Advances in Neural Information
Processing Systems, pages 585?591.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
2006. Manifold regularization: a geometric frame-
work for learning from labeled and unlabeled ex-
amples. Journal of Machine Learning Research,
7:2399?2434.
Stephen Boyd, Lin Xiao, and Almir Mutapcic. 2003.
Subgradient methods. Lecture Notes of EE392o, S-
tanford University.
Deng Cai, Xiaofei He, Xiaoyun Wu, and Jiawei Han.
2008. Non-negative matrix factorization on mani-
fold. In Proceedings of the 8th IEEE International
Conference on Data Mining, pages 63?72.
David Carmel, Elad Yom-Tov, Adam Darlow, and Dan
Pelleg. 2006. What makes a query difficult? In
Proceedings of the 29th International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 390?397.
Olivier Chapelle. 2007. Training a support vec-
tor machine in the primal. Neural Computation,
19(5):1155?1178.
Xi Chen, Paul N. Bennett, Kevyn Collins-Thompson,
and Eric Horvitz. 2013. Pairwise ranking aggrega-
tion in a crowdsourced setting. In Proceedings of the
6th ACM International Conference on Web Search
and Data Mining, pages 193?202.
Fan RK. Chung. 1997. Spectral Graph Theory, vol-
ume 92.
Kenneth Church. 2011. How many multiword expres-
sions do people know. In Proceedings of the ACL-
HLT Workshop on Multiword Expressions: from
Parsing and Generation to the Real World, pages
137?144.
Mark Claypool, Anuja Gokhale, Tim Miranda, Pavel
Murnikov, Dmitry Netes, andMatthew Sartin. 1999.
Combining content-based and collaborative filters in
an online newspaper. In Proceedings of the ACM
SIGIR workshop on Recommender Systems.
Jacob Cohen. 1960. A coefficient of agreemen-
t for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
Kevyn Collins-Thompson and James P. Callan. 2004.
A language modeling approach to predicting reading
difficulty. In Proceedings of the 2004 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 193?200.
Kevyn Collins-Thompson, Paul N Bennett, Ryen W
White, Sebastian de la Chica, and David Sontag.
2011. Personalizing web search results by reading
level. In Proceedings of the 20th ACM Internation-
al Conference on Information and Knowledge Man-
agement, pages 403?412.
Thomas Cover and Peter Hart. 1967. Nearest neighbor
pattern classification. IEEE Transactions on Infor-
mation Theory, 13(1):21?27.
Zhicheng Dou, Ruihua Song, and Ji Rong Wen. 2007.
A large-scale evaluation and analysis of personal-
ized search strategies. In Proceedings of the 16th
International Conference on World Wide Web, pages
581?590.
Ralf Herbrich, Tom Minka, and Thore Graepel. 2006.
Trueskill: a bayesian skill rating system. In Ad-
vances in Neural Information Processing Systems,
pages 569?576.
Anna Huang. 2008. Similarity measures for text doc-
ument clustering. In Proceedings of the 6th New
Zealand Computer Science Research Student Con-
ference, pages 49?56.
Rense Lange, Juan Moran, Warren R. Greiff, and Lisa
Ferro. 2004. A probabilistic rasch analysis of ques-
tion answering evaluations. In Proceedings of the
2004 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 65?72.
Baichuan Li and Irwin King. 2010. Routing ques-
tions to appropriate answerers in community ques-
tion answering services. In Proceedings of the 19th
ACM International Conference on Information and
Knowledge Management, pages 1585?1588.
Jing Liu, Young-In Song, and Chin-Yew Lin. 2011.
Competition-based user expertise score estimation.
In Proceedings of the 34th International ACM SI-
GIR Conference on Research and Development in
Information Retrieval, pages 425?434.
Jing Liu, Quan Wang, Chin-Yew Lin, and Hsiao-Wuen
Hon. 2013. Question difficulty estimation in com-
munity question answering services. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 85?90.
Kevin Kyung Nam, Mark S. Ackerman, and Lada A.
Adamic. 2009. Questions in, knowledge in?: a s-
tudy of naver?s question answering community. In
Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems, pages 779?788.
Larry Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
bringing order to the web. Technical Report, Stan-
ford University.
1125
Joseph Lee Rodgers and W. Alan Nicewander. 1988.
Thirteen ways to look at the correlation coefficient.
The American Statistician, 42(1):59?66.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation Processing & Management, 24(5):513?
523.
Andrew I. Schein, Alexandrin Popescul, Lyle H. Ungar,
and David M. Pennock. 2002. Methods and met-
rics for cold-start recommendations. In Proceed-
ings of the 25th International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 253?260.
Sarah E. Schwarm and Mari Ostendorf. 2005. Reading
level assessment using support vector machines and
statistical language models. In Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 523?530.
Kazunari Sugiyama, Kenji Hatano, and Masatoshi
Yoshikawa. 2004. Adaptive web search based on
user profile constructed without any effort from user-
s. In Proceedings of the 13th International Confer-
ence on World Wide Web, pages 675?684.
Peter Welinder, Steve Branson, Serge Belongie, and
Pietro Perona. 2010. The multidimensional wis-
dom of crowds. In Advances in Neural Information
Processing Systems, pages 2424?2432.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier R Movellan. 2009. Whose
vote should count more: optimal integration of la-
bels from labelers of unknown expertise. In Ad-
vances in Neural Information Processing Systems,
pages 2035?2043.
Jiang Yang, Lada Adamic, and Mark Ackerman. 2008.
Competing to share expertise: the taskcn knowledge
sharing community. In Proceedings of the 2nd In-
ternational AAAI Conference on Weblogs and Social
Media.
Elad Yom-Tov, Shai Fine, David Carmel, and Adam
Darlow. 2005. Learning to estimate query difficulty:
including applications to missing content detection
and distributed information retrieval. In Proceed-
ings of the 28th International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 512?519.
Dengyong Zhou, Olivier Bousquet, Thomas Navin
Lal, Jason Weston, and Bernhard Sch?olkopf. 2004.
Learning with local and global consistency. In Ad-
vances in Neural Information Processing Systems,
pages 321?328.
Yanhong Zhou, Gao Cong, Bin Cui, Christian S.
Jensen, and Junjie Yao. 2009. Routing questions to
the right users in online communities. In Proceed-
ings of the 25th IEEE International Conference on
Data Engineering, pages 700?711.
Dengyong Zhou, John C Platt, Sumit Basu, and Y-
i Mao. 2012. Learning from the wisdom of crowds
by minimax entropy. In Advances in Neural Infor-
mation Processing Systems, pages 2204?2212.
Xiaojin Zhu and John Lafferty. 2005. Harmonic mix-
tures: combining mixture models and graph-based
methods for inductive and scalable semi-supervised
learning. In Proceedings of the 22nd Internation-
al Conference on Machine Learning, pages 1052?
1059.
1126
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1159?1168,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Nonlinear Evidence Fusion and Propagation 
for Hyponymy Relation Mining 
 
Fan Zhang2*     Shuming Shi1     Jing Liu2     Shuqi Sun3*     Chin-Yew Lin1 
1Microsoft Research Asia 
2Nankai University, China 
3Harbin Institute of Technology, China 
{shumings, cyl}@microsoft.com 
 
 
 
Abstract 
This paper focuses on mining the hypon-
ymy (or is-a) relation from large-scale, 
open-domain web documents. A nonlinear 
probabilistic model is exploited to model 
the correlation between sentences in the 
aggregation of pattern matching results. 
Based on the model, we design a set of ev-
idence combination and propagation algo-
rithms. These significantly improve the 
result quality of existing approaches.  Ex-
perimental results conducted on 500 mil-
lion web pages and hypernym labels for 
300 terms show over 20% performance 
improvement in terms of P@5, MAP and 
R-Precision. 
1 Introduction1 
An important task in text mining is the automatic 
extraction of entities and their lexical relations; this 
has wide applications in natural language pro-
cessing and web search. This paper focuses on 
mining the hyponymy (or is-a) relation from large-
scale, open-domain web documents. From the 
viewpoint of entity classification, the problem is to 
automatically assign fine-grained class labels to 
terms. 
There have been a number of approaches 
(Hearst 1992; Pantel & Ravichandran 2004; Snow 
et al, 2005; Durme & Pasca, 2008; Talukdar et al, 
2008) to address the problem. These methods typi-
cally exploited manually-designed or automatical-
                                                          
* This work was performed when Fan Zhang and Shuqi Sun 
were interns at Microsoft Research Asia 
ly-learned patterns (e.g., ?NP such as NP?, ?NP 
like NP?, ?NP is a NP?). Although some degree of 
success has been achieved with these efforts, the 
results are still far from perfect, in terms of both 
recall and precision. As will be demonstrated in 
this paper, even by processing a large corpus of 
500 million web pages with the most popular pat-
terns, we are not able to extract correct labels for 
many (especially rare) entities. Even for popular 
terms, incorrect results often appear in their label 
lists. 
The basic philosophy in existing hyponymy ex-
traction approaches (and also many other text-
mining methods) is counting: count the number of 
supporting sentences. Here a supporting sentence 
of a term-label pair is a sentence from which the 
pair can be extracted via an extraction pattern. We 
demonstrate that the specific way of counting has a 
great impact on result quality, and that the state-of-
the-art counting methods are not optimal. Specifi-
cally, we examine the problem from the viewpoint 
of probabilistic evidence combination and find that 
the probabilistic assumption behind simple count-
ing is the statistical independence between the ob-
servations of supporting sentences. By assuming a 
positive correlation between supporting sentence 
observations and adopting properly designed non-
linear combination functions, the results precision 
can be improved. 
It is hard to extract correct labels for rare terms 
from a web corpus due to the data sparseness prob-
lem. To address this issue, we propose an evidence 
propagation algorithm motivated by the observa-
tion that similar terms tend to share common hy-
pernyms. For example, if we already know that 1) 
Helsinki and Tampere are cities, and 2) Porvoo is 
similar to Helsinki and Tampere, then Porvoo is 
1159
very likely also a city. This intuition, however, 
does not mean that the labels of a term can always 
be transferred to its similar terms. For example, 
Mount Vesuvius and Kilimanjaro are volcanoes 
and Lhotse is similar to them, but Lhotse is not a 
volcano. Therefore we should be very conservative 
and careful in hypernym propagation. In our prop-
agation algorithm, we first construct some pseudo 
supporting sentences for a term from the support-
ing sentences of its similar terms. Then we calcu-
late label scores for terms by performing nonlinear 
evidence combination based on the (pseudo and 
real) supporting sentences. Such a nonlinear prop-
agation algorithm is demonstrated to perform bet-
ter than linear propagation. 
Experimental results on a publicly available col-
lection of 500 million web pages with hypernym 
labels annotated for 300 terms show that our non-
linear evidence fusion and propagation significant-
ly improve the precision and coverage of the 
extracted hyponymy data. This is one of the tech-
nologies adopted in our semantic search and min-
ing system NeedleSeek2. 
In the next section, we discuss major related ef-
forts and how they differ from our work. Section 3 
is a brief description of the baseline approach. The 
probabilistic evidence combination model that we 
exploited is introduced in Section 4. Our main ap-
proach is illustrated in Section 5. Section 6 shows 
our experimental settings and results. Finally, Sec-
tion 7 concludes this paper. 
2 Related Work 
Existing efforts for hyponymy relation extraction 
have been conducted upon various types of data 
sources, including plain-text corpora (Hearst 1992; 
Pantel & Ravichandran, 2004; Snow et al, 2005; 
Snow et al, 2006; Banko, et al, 2007; Durme & 
Pasca, 2008; Talukdar et al, 2008), semi-
structured web pages (Cafarella  et al, 2008; Shin-
zato & Torisawa, 2004), web search results (Geraci 
et al, 2006; Kozareva et al, 2008; Wang & Cohen, 
2009), and query logs (Pasca 2010). Our target for 
optimization in this paper is the approaches that 
use lexico-syntactic patterns to extract hyponymy 
relations from plain-text corpora. Our future work 
will study the application of the proposed algo-
rithms on other types of approaches. 
                                                          
2 http://research.microsoft.com/en-us/projects/needleseek/ or 
http://needleseek.msra.cn/  
The probabilistic evidence combination model 
that we exploit here was first proposed in (Shi et 
al., 2009), for combining the page in-link evidence 
in building a nonlinear static-rank computation 
algorithm. We applied it to the hyponymy extrac-
tion problem because the model takes the depend-
ency between supporting sentences into 
consideration and the resultant evidence fusion 
formulas are quite simple. In (Snow et al, 2006), a 
probabilistic model was adopted to combine evi-
dence from heterogeneous relationships to jointly 
optimize the relationships. The independence of 
evidence was assumed in their model. In compari-
son, we show that better results will be obtained if 
the evidence correlation is modeled appropriately. 
Our evidence propagation is basically about us-
ing term similarity information to help instance 
labeling. There have been several approaches 
which improve hyponymy extraction with instance 
clusters built by distributional similarity. In (Pantel 
& Ravichandran, 2004), labels were assigned to 
the committee (i.e., representative members) of a 
semantic class and used as the hypernyms of the 
whole class. Labels generated by their approach 
tend to be rather coarse-grained, excluding the pos-
sibility of a term having its private labels (consid-
ering the case that one meaning of a term is not 
covered by the input semantic classes). In contrast 
to their method, our label scoring and ranking ap-
proach is applied to every single term rather than a 
semantic class. In addition, we also compute label 
scores in a nonlinear way, which improves results 
quality. In Snow et al (2005), a supervised ap-
proach was proposed to improve hypernym classi-
fication using coordinate terms. In comparison, our 
approach is unsupervised. Durme & Pasca (2008) 
cleaned the set of instance-label pairs with a 
TF*IDF like method, by exploiting clusters of se-
mantically related phrases. The core idea is to keep 
a term-label pair (T, L) only if the number of terms 
having the label L in the term T?s cluster is above a 
threshold and if L is not the label of too many clus-
ters (otherwise the pair will be discarded). In con-
trast, we are able to add new (high-quality) labels 
for a term with our evidence propagation method. 
On the other hand, low quality labels get smaller 
score gains via propagation and are ranked lower. 
Label propagation is performed in (Talukdar et 
al., 2008; Talukdar & Pereira, 2010) based on mul-
tiple instance-label graphs. Term similarity infor-
mation was not used in their approach. 
1160
Most existing work tends to utilize small-scale 
or private corpora, whereas the corpus that we used 
is publicly available and much larger than most of 
the existing work. We published our term sets (re-
fer to Section 6.1) and their corresponding user 
judgments so researchers working on similar topics 
can reproduce our results. 
 
Type Pattern 
Hearst-I NPL {,} (such as) {NP,}
* {and|or} NP  
Hearst-II 
NPL {,} (include(s) | including) {NP,}
* 
{and|or} NP 
Hearst-III NPL {,} (e.g.|e.g) {NP,}
* {and|or} NP 
IsA-I NP (is|are|was|were|being) (a|an) NPL 
IsA-II NP (is|are|was|were|being) {the, those} NPL 
IsA-III NP (is|are|was|were|being) {another, any} NPL 
Table 1. Patterns adopted in this paper (NP: named 
phrase representing an entity; NPL: label) 
3 Preliminaries 
The problem addressed in this paper is corpus-
based is-a relation mining: extracting hypernyms 
(as labels) for entities from a large-scale, open-
domain document corpus. The desired output is a 
mapping from terms to their corresponding hyper-
nyms, which can naturally be represented as a 
weighted bipartite graph (term-label graph). Typi-
cally we are only interested in top labels of a term 
in the graph. 
Following existing efforts, we adopt pattern-
matching as a basic way of extracting hyper-
nymy/hyponymy relations. Two types of patterns 
(refer to Table 1) are employed, including the pop-
ular ?Hearst patterns? (Hearst, 1992) and the IsA 
patterns which are exploited less frequently in ex-
isting hyponym mining efforts. One or more term-
label pairs can be extracted if a pattern matches a 
sentence. In the baseline approach, the weight of 
an edge T?L (from term T to hypernym label L) in 
the term-label graph is computed as, 
 w(T?L)      ( )       
   
    ( )
 (3.1) 
where m is the number of times the pair (T, L) is 
extracted from the corpus, DF(L) is the number of 
in-links of L in the graph, N is total number of 
terms in the graph, and IDF means the ?inverse 
document frequency?. 
A term can only keep its top-k neighbors (ac-
cording to the edge weight) in the graph as its final 
labels. 
Our pattern matching algorithm implemented in 
this paper uses part-of-speech (POS) tagging in-
formation, without adopting a parser or a chunker. 
The noun phrase boundaries (for terms and labels) 
are determined by a manually designed POS tag 
list. 
4 Probabilistic Label-Scoring Model 
Here we model the hyponymy extraction problem 
from the probability theory point of view, aiming 
at estimating the score of a term-label pair (i.e., the 
score of a label w.r.t. a term) with probabilistic 
evidence combination. The model was studied in 
(Shi et al, 2009) to combine the page in-link evi-
dence in building a nonlinear static-rank computa-
tion algorithm. 
We represent the score of a term-label pair by 
the probability of the label being a correct hyper-
nym of the term, and define the following events, 
AT,L: Label L is a hypernym of term T (the ab-
breviated form A is used in this paper unless it is 
ambiguous). 
Ei: The observation that (T, L) is extracted from 
a sentence Si via pattern matching (i.e., Si is a sup-
porting sentence of the pair). 
Assuming that we already know m supporting 
sentences (S1~Sm), our problem is to compute 
P(A|E1,E2,..,Em), the posterior probability that L is 
a hypernym of term T, given evidence E1~Em. 
Formally, we need to find a function f to satisfy, 
 P(A|E1,?,Em) = f(P(A), P(A|E1)?, P(A|Em) ) (4.1) 
For simplicity, we first consider the case of 
m=2. The case of m>2 is quite similar. 
We start from the simple case of independent 
supporting sentences. That is, 
  (     )   (  )   (  ) (4.2) 
  (       )   (    )   (    ) (4.3) 
By applying Bayes rule, we get, 
 
 (       )  
 (       )   ( )
 (     )
 
          
 (    )   ( )
 (  )
 
 (    )   ( )
 (  )
 
 
 ( )
 
          
 (    )   (    )
 ( )
 
(4.4) 
Then define 
 (   )     
 (   )
 ( )
     ( (   ))     ( ( )) 
1161
Here G(A|E) represents the log-probability-gain 
of A given E, with the meaning of the gain in the 
log-probability value of A after the evidence E is 
observed (or known). It is a measure of the impact 
of evidence E to the probability of event A. With 
the definition of G(A|E), Formula 4.4 can be trans-
formed to, 
  (       )   (    )   (    ) (4.5) 
Therefore, if E1 and E2 are independent, the log-
probability-gain of A given both pieces of evidence 
will exactly be the sum of the gains of A given eve-
ry single piece of evidence respectively. It is easy 
to prove (by following a similar procedure) that the 
above Formula holds for the case of m>2, as long 
as the pieces of evidence are mutually independent. 
Therefore for a term-label pair with m mutually 
independent supporting sentences, if we set every 
gain G(A|Ei) to be a constant value g, the posterior 
gain score of the pair will be ?         . If the 
value g is the IDF of label L, the posterior gain will 
be, 
 G(AT,L|E1?,Em) ?    ( )
 
         ( ) (4.6) 
This is exactly the Formula 3.1. By this way, we 
provide a probabilistic explanation of scoring the 
candidate labels for a term via simple counting. 
 
 Hearst-I IsA-I 
E1: Hearst-I 
E2: IsA-I 
RA: 
 (      )
 (    ) (    )
  66.87 17.30 24.38 
R: 
 (    )
 (  ) (  )
  5997 1711 802.7 
RA/R 0.011 0.010 0.030 
Table 2. Evidence dependency estimation for intra-
pattern and inter-pattern supporting sentences 
In the above analysis, we assume the statistical 
independence of the supporting sentence observa-
tions, which may not hold in reality. Intuitively, if 
we already know one supporting sentence S1 for a 
term-label pair (T, L), then we have more chance to 
find another supporting sentence than if we do not 
know S1. The reason is that, before we find S1, we 
have to estimate the probability with the chance of 
discovering a supporting sentence for a random 
term-label pair. The probability is quite low be-
cause most term-label pairs do not have hyponymy 
relations. Once we have observed S1, however, the 
chance of (T, L) having a hyponymy relation in-
creases. Therefore the chance of observing another 
supporting sentence becomes larger than before. 
Table 2 shows the rough estimation of 
 (      )
 (    ) (    )
 (denoted as RA), 
 (    )
 (  ) (  )
 (denoted 
as R), and their ratios. The statistics are obtained 
by performing maximal likelihood estimation 
(MLE) upon our corpus and a random selection of 
term-label pairs from our term sets (see Section 
6.1) together with their top labels3. The data veri-
fies our analysis about the correlation between E1 
and E2 (note that R=1 means independent). In addi-
tion, it can be seen that the conditional independ-
ence assumption of Formula 4.3 does not hold 
(because RA>1). It is hence necessary to consider 
the correlation between supporting sentences in the 
model. The estimation of Table 2 also indicates 
that, 
 
 (     )
 (  ) (  )
 
 (       )
 (    ) (    )
 (4.7) 
By following a similar procedure as above, with 
Formulas 4.2 and 4.3 replaced by 4.7, we have, 
  (       )   (    )   (    ) (4.8) 
This formula indicates that when the supporting 
sentences are positively correlated, the posterior 
score of label L w.r.t. term T (given both the sen-
tences) is smaller than the sum of the gains caused 
by one sentence only. In the extreme case that sen-
tence S2 fully depends on E1 (i.e. P(E2|E1)=1), it is 
easy to prove that 
  (       )   (    )  
It is reasonable, since event E2 does not bring in 
more information than E1. 
Formula 4.8 cannot be used directly for compu-
ting the posterior gain. What we really need is a 
function h satisfying 
  (         )   ( (    )    (    )) (4.9) 
and 
  (      )  ?   
 
     (4.10) 
Shi et al (2009) discussed other constraints to h 
and suggested the following nonlinear functions, 
   (      )    (  ? ( 
    )    )  (4.11) 
                                                          
3 RA is estimated from the labels judged as ?Good?; whereas 
the estimation of R is from all judged labels. 
1162
   (      )  ??   
  
   
 
           (p>1) (4.12) 
In the next section, we use the above two h func-
tions as basic building blocks to compute label 
scores for terms. 
5 Our Approach 
Multiple types of patterns (Table 1) can be adopted 
to extract term-label pairs. For two supporting sen-
tences the correlation between them may depend 
on whether they correspond to the same pattern. In 
Section 5.1, our nonlinear evidence fusion formu-
las are constructed by making specific assumptions 
about the correlation between intra-pattern sup-
porting sentences and inter-pattern ones. 
Then in Section 5.2, we introduce our evidence 
propagation technique in which the evidence of a 
(T, L) pair is propagated to the terms similar to T. 
5.1 Nonlinear evidence fusion 
For a term-label pair (T, L), assuming K patterns 
are used for hyponymy extraction and the support-
ing sentences discovered with pattern i are, 
                  (5.1) 
where mi is the number of supporting sentences 
corresponding to pattern i. Also assume the gain 
score of Si,j is xi,j, i.e., xi,j=G(A|Si,j). 
Generally speaking, supporting sentences corre-
sponding to the same pattern typically have a high-
er correlation than the sentences corresponding to 
different patterns. This can be verified by the data 
in Table-2. By ignoring the inter-pattern correla-
tions, we make the following simplified assump-
tion: 
Assumption: Supporting sentences correspond-
ing to the same pattern are correlated, while those 
of different patterns are independent. 
According to this assumption, our label-scoring 
function is, 
      (   )  ? (               )
 
   
 (5.2) 
In the simple case that         ( ) , if the h 
function of Formula 4.12 is adopted, then, 
      (   )  (? ?  
 
 
   
)     ( ) (5.3) 
We use an example to illustrate the above for-
mula. 
Example: For term T and label L1, assume the 
numbers of the supporting sentences corresponding 
to the six pattern types in Table 1 are (4, 4, 4, 4, 4, 
4), which means the number of supporting sen-
tences discovered by each pattern type is 4. Also 
assume the supporting-sentence-count vector of 
label L2 is (25, 0, 0, 0, 0, 0). If we use Formula 5.3 
to compute the scores of L1 and L2, we can have 
the following (ignoring IDF for simplicity), 
Score(L1)   ?    ; Score(L2) ?     
One the other hand, if we simply count the total 
number of supporting sentences, the score of L2 
will be larger. 
The rationale implied in the formula is: For a 
given term T, the labels supported by multiple 
types of patterns tend to be more reliable than 
those supported by a single pattern type, if they 
have the same number of supporting sentences. 
5.2 Evidence propagation 
According to the evidence fusion algorithm de-
scribed above, in order to extract term labels relia-
bly, it is desirable to have many supporting 
sentences of different types. This is a big challenge 
for rare terms, due to their low frequency in sen-
tences (and even lower frequency in supporting 
sentences because not all occurrences can be cov-
ered by patterns). With evidence propagation, we 
aim at discovering more supporting sentences for 
terms (especially rare terms). Evidence propaga-
tion is motivated by the following two observa-
tions: 
(I) Similar entities or coordinate terms tend to 
share some common hypernyms. 
(II) Large term similarity graphs are able to be 
built efficiently with state-of-the-art techniques 
(Agirre et al, 2009; Pantel et al, 2009; Shi et al, 
2010). With the graphs, we can obtain the similari-
ty between two terms without their hypernyms be-
ing available. 
The first observation motivates us to ?borrow? 
the supporting sentences from other terms as auxil-
iary evidence of the term. The second observation 
means that new information is brought with the 
state-of-the-art term similarity graphs (in addition 
to the term-label information discovered with the 
patterns of Table 1). 
1163
Our evidence propagation algorithm contains 
two phases. In phase I, some pseudo supporting 
sentences are constructed for a term from the sup-
porting sentences of its neighbors in the similarity 
graph. Then we calculate the label scores for terms 
based on their (pseudo and real) supporting sen-
tences. 
Phase I: For every supporting sentence S and 
every similar term T1 of the term T, add a pseudo 
supporting sentence S1 for T1, with the gain score, 
  (         )       (    )   (      ) (5.5) 
where         is the propagation factor, and 
   (   ) is the term similarity function taking val-
ues in [0, 1]. The formula reasonably assumes that 
the gain score of the pseudo supporting sentence 
depends on the gain score of the original real sup-
porting sentence, the similarity between the two 
terms, and the propagation factor. 
Phase II: The nonlinear evidence combination 
formulas in the previous subsection are adopted to 
combine the evidence of pseudo supporting sen-
tences. 
Term similarity graphs can be obtained by dis-
tributional similarity or patterns (Agirre et al, 
2009; Pantel et al, 2009; Shi et al, 2010). We call 
the first type of graph DS and the second type PB. 
DS approaches are based on the distributional hy-
pothesis (Harris, 1985), which says that terms ap-
pearing in analogous contexts tend to be similar. In 
a DS approach, a term is represented by a feature 
vector, with each feature corresponding to a con-
text in which the term appears. The similarity be-
tween two terms is computed as the similarity 
between their corresponding feature vectors. In PB 
approaches, a list of carefully-designed (or auto-
matically learned) patterns is exploited and applied 
to a text collection, with the hypothesis that the 
terms extracted by applying each of the patterns to 
a specific piece of text tend to be similar. Two cat-
egories of patterns have been studied in the litera-
ture (Heast 1992; Pasca 2004; Kozareva et al, 
2008; Zhang et al, 2009): sentence lexical patterns, 
and HTML tag patterns. An example of sentence 
lexical patterns is ?T {, T}*{,} (and|or) T?. HTML 
tag patterns include HTML tables, drop-down lists, 
and other tag repeat patterns. In this paper, we 
generate the DS and PB graphs by adopting the 
best-performed methods studied in (Shi et al, 
2010). We will compare, by experiments, the prop-
agation performance of utilizing the two categories 
of graphs, and also investigate the performance of 
utilizing both graphs for evidence propagation. 
6 Experiments 
6.1 Experimental setup 
Corpus We adopt a publicly available dataset in 
our experiments: ClueWeb094. This is a very large 
dataset collected by Carnegie Mellon University in 
early 2009 and has been used by several tracks of 
the Text Retrieval Conference (TREC)5. The whole 
dataset consists of 1.04 billion web pages in ten 
languages while only those in English, about 500 
million pages, are used in our experiments. The 
reason for selecting such a dataset is twofold: First, 
it is a corpus large enough for conducting web-
scale experiments and getting meaningful results. 
Second, since it is publicly available, it is possible 
for other researchers to reproduce the experiments 
in this paper. 
Term sets Approaches are evaluated by using 
two sets of selected terms: Wiki200, and Ext100. 
For every term in the term sets, each approach 
generates a list of hypernym labels, which are 
manually judged by human annotators. Wiki200 is 
constructed by first randomly selecting 400 Wik-
ipedia6 titles as our candidate terms, with the prob-
ability of a title T being selected to be     (  
 ( )), where F(T) is the frequency of T in our data 
corpus. The reason of adopting such a probability 
formula is to balance popular terms and rare ones 
in our term set. Then 200 terms are manually se-
lected from the 400 candidate terms, with the prin-
ciple of maximizing the diversity of terms in terms 
of length (i.e., number of words) and type (person, 
location, organization, software, movie, song, ani-
mal, plant, etc.). Wiki200 is further divided into 
two subsets: Wiki100H and Wiki100L, containing 
respectively the 100 high-frequency and low-
frequency terms. Ext100 is built by first selecting 
200 non-Wikipedia-title terms at random from the 
term-label graph generated by the baseline ap-
proach (Formula 3.1), then manually selecting 100 
terms. 
Some sample terms in the term sets are listed in 
Table 3. 
 
                                                          
4 http://boston.lti.cs.cmu.edu/Data/clueweb09/  
5 http://trec.nist.gov/  
6 http://www.wikipedia.org/  
1164
Term 
Set 
Sample Terms 
Wiki200 
Canon EOS 400D, Disease management, El Sal-
vador, Excellus Blue Cross Blue Shield, F33, 
Glasstron, Indium, Khandala, Kung Fu, Lake 
Greenwood, Le Gris, Liriope, Lionel Barrymore, 
Milk, Mount Alto, Northern Wei, Pink Lady, 
Shawshank, The Dog Island, White flight, World 
War II? 
Ext100 
A2B, Antique gold, GPTEngine, Jinjiang Inn, 
Moyea SWF to Apple TV Converter, Nanny ser-
vice, Outdoor living, Plasmid DNA, Popon, Spam 
detection, Taylor Ho Bynum, Villa Michelle? 
Table 3. Sample terms in our term sets 
 
Annotation For each term in the term set, the 
top-5 results (i.e., hypernym labels) of various 
methods are mixed and judged by human annota-
tors. Each annotator assigns each result item a 
judgment of ?Good?, ?Fair? or ?Bad?. The annota-
tors do not know the method by which a result item 
is generated. Six annotators participated in the la-
beling with a rough speed of 15 minutes per term. 
We also encourage the annotators to add new good 
results which are not discovered by any method. 
The term sets and their corresponding user anno-
tations are available for download at the following 
links (dataset ID=data.queryset.semcat01): 
http://research.microsoft.com/en-us/projects/needleseek/ 
http://needleseek.msra.cn/datasets/ 
Evaluation We adopt the following metrics to 
evaluate the hypernym list of a term generated by 
each method. The evaluation score on a term set is 
the average over all the terms. 
Precision@k: The percentage of relevant (good 
or fair) labels in the top-k results (labels judged as 
?Fair? are counted as 0.5) 
Recall@k: The ratio of relevant labels in the top-
k results to the total number of relevant labels 
R-Precision: Precision@R where R is the total 
number of labels judged as ?Good? 
Mean average precision (MAP): The average of 
precision values at the positions of all good or fair 
results 
Before annotation and evaluation, the hypernym 
list generated by each method for each term is pre-
processed to remove duplicate items. Two hyper-
nyms are called duplicate items if they share the 
same head word (e.g., ?military conflict? and ?con-
flict?). For duplicate hypernyms, only the first (i.e., 
the highest ranked one) in the list is kept. The goal 
with such a preprocessing step is to partially con-
sider results diversity in evaluation and to make a 
more meaningful comparison among different 
methods. Consider two hypernym lists for ?sub-
way?: 
List-1: restaurant; chain restaurant; worldwide chain 
restaurant; franchise; restaurant franchise? 
List-2: restaurant; franchise; transportation; company; 
fast food? 
There are more detailed hypernyms in the first 
list about ?subway? as a restaurant or a franchise; 
while the second list covers a broader range of 
meanings for the term. It is hard to say which is 
better (without considering the upper-layer appli-
cations). With this preprocessing step, we keep our 
focus on short hypernyms rather than detailed ones. 
 
Term Set Method MAP R-Prec P@1 P@5 
Wiki200 
Linear 0.357 0.376 0.783 0.547 
Log 
0.371 
 3.92% 
0.384 
 2.13% 
0.803 
 2.55% 
0.561 
 2.56% 
PNorm 
0.372 
 4.20% 
0.384 
 2.13% 
0.800 
 2.17% 
0.562 
 2.74% 
Wiki100H 
Linear 0.363 0.382 0.805 0.627 
Log 
0.393 
 8.26% 
0.402 
 5.24% 
0.845 
 4.97% 
0.660 
 5.26% 
PNorm 
0.395 
 8.82% 
0.403 
 5.50% 
0.840 
 4.35% 
0.662 
 5.28% 
Table 4. Performance comparison among various 
evidence fusion methods (Term sets: Wiki200 and 
Wiki100H; p=2 for PNorm) 
6.2 Experimental results 
We first compare the evaluation results of different 
evidence fusion methods mentioned in Section 4.1. 
In Table 4, Linear means that Formula 3.1 is used 
to calculate label scores, whereas Log and PNorm 
represent our nonlinear approach with Formulas 
4.11 and 4.12 being utilized. The performance im-
provement numbers shown in the table are based 
on the linear version; and the upward pointing ar-
rows indicate relative percentage improvement 
over the baseline. From the table, we can see that 
the nonlinear methods outperform the linear ones 
on the Wiki200 term set. It is interesting to note 
that the performance improvement is more signifi-
cant on Wiki100H, the set of high frequency terms. 
By examining the labels and supporting sentences 
for the terms in each term set, we find that for 
many low-frequency terms (in Wiki100L), there 
are only a few supporting sentences (corresponding 
1165
to one or two patterns). So the scores computed by 
various fusion algorithms tend to be similar. In 
contrast, more supporting sentences can be discov-
ered for high-frequency terms. Much information 
is contained in the sentences about the hypernyms 
of the high-frequency terms, but the linear function 
of Formula 3.1 fails to make effective use of it. 
The two nonlinear methods achieve better perfor-
mance by appropriately modeling the dependency 
between supporting sentences and computing the 
log-probability gain in a better way. 
The comparison of the linear and nonlinear 
methods on the Ext100 term set is shown in Table 
5. Please note that the terms in Ext100 do not ap-
pear in Wikipedia titles. Thanks to the scale of the 
data corpus we are using, even the baseline ap-
proach achieves reasonably good performance. 
Please note that the terms (refer to Table 3) we are 
using are ?harder? than those adopted for evalua-
tion in many existing papers. Again, the results 
quality is improved with the nonlinear methods, 
although the performance improvement is not big 
due to the reason that most terms in Ext100 are 
rare. Please note that the recall (R@1, R@5) in this 
paper is pseudo-recall, i.e., we treat the number of 
known relevant (Good or Fair) results as the total 
number of relevant ones. 
 
Method MAP R-Prec P@1 P@5 R@1 R@5 
Linear 0.384 0.429 0.665 0.472 0.116 0.385 
Log 
0.395 0.429 0.715 0.472 0.125 0.385 
 2.86%  0%  7.52%  0%  7.76%  0% 
PNorm 
0.390 0.429 0.700 0.472 0.120 0.385 
 1.56%  0%   5.26%  0%  3.45%  0% 
Table 5. Performance comparison among various 
evidence fusion methods (Term set: Ext100; p=2 
for PNorm) 
The parameter p in the PNorm method is related 
to the degree of correlations among supporting 
sentences. The linear method of Formula 3.1 corre-
sponds to the special case of p=1; while p=  rep-
resents the case that other supporting sentences are 
fully correlated to the supporting sentence with the 
maximal log-probability gain. Figure 1 shows that, 
for most of the term sets, the best performance is 
obtained for   [2.0, 4.0]. The reason may be that 
the sentence correlations are better estimated with 
p values in this range. 
 
 
Figure 1. Performance curves of PNorm with dif-
ferent parameter values (Measure: MAP) 
The experimental results of evidence propaga-
tion are shown in Table 6. The methods for com-
parison are, 
Base: The linear function without propagation. 
NL: Nonlinear evidence fusion (PNorm with 
p=2) without propagation. 
LP: Linear propagation, i.e., the linear function 
is used to combine the evidence of pseudo support-
ing sentences. 
NLP: Nonlinear propagation where PNorm 
(p=2) is used to combine the pseudo supporting 
sentences. 
NL+NLP: The nonlinear method is used to 
combine both supporting sentences and pseudo 
supporting sentences. 
 
Method MAP R-Prec P@1 P@5 R@5 
Base 0.357 0.376 0.783 0.547 0.317 
NL 
0.372 0.384 0.800 0.562 0.325 
 4.20%  2.13%  2.17%  2.74%  2.52% 
LP 
0.357 0.376 0.783 0.547 0.317 
 0%  0%  0%  0%  0% 
NLP 
0.396 0.418 0.785 0.605 0.357 
 10.9%  11.2%  0.26%  10.6%  12.6% 
NL+NLP 
0.447 0.461 0.840 0.667 0.404 
 25.2%  22.6%  7.28%  21.9%  27.4% 
Table 6. Evidence propagation results (Term set: 
Wiki200; Similarity graph: PB; Nonlinear formula: 
PNorm) 
In this paper, we generate the DS (distributional 
similarity) and PB (pattern-based) graphs by adopt-
ing the best-performed methods studied in (Shi et 
al., 2010). The performance improvement numbers 
(indicated by the upward pointing arrows) shown 
in tables 6~9 are relative percentage improvement 
1166
over the base approach (i.e., linear function with-
out propagation). The values of parameter   are set 
to maximize the MAP values. 
Several observations can be made from Table 6. 
First, no performance improvement can be ob-
tained with the linear propagation method (LP), 
while the nonlinear propagation algorithm (NLP) 
works quite well in improving both precision and 
recall. The results demonstrate the high correlation 
between pseudo supporting sentences and the great 
potential of using term similarity to improve hy-
pernymy extraction. The second observation is that 
the NL+NLP approach achieves a much larger per-
formance improvement than NL and NLP. Similar 
results (omitted due to space limitation) can be 
observed on the Ext100 term set. 
 
Method MAP R-Prec P@1 P@5 R@5 
Base 0.357 0.376 0.783 0.547 0.317 
NL+NLP 
(PB) 
0.415 0.439 0.830 0.633 0.379 
 16.2%  16.8%  6.00%  15.7%  19.6% 
NL+NLP 
(DS) 
0.456 0.469 0.843 0.673 0.406 
 27.7%  24.7%  7.66%  23.0%  28.1% 
NL+NLP
(PB+DS) 
0.473 0.487 0.860 0.700 0.434 
 32.5%  29.5%  9.83%  28.0%  36.9% 
Table 7. Combination of PB and DS graphs for 
evidence propagation (Term set: Wiki200; Nonlin-
ear formula: Log) 
 
Method MAP R-Prec P@1 P@5 R@5 
Base 0.351 0.370 0.760 0.467 0.317 
NL+NLP 
(PB) 
0.411 0.448 0.770 0.564 0.401 
?17.1% ?21.1% ?1.32% ?20.8% ?26.5% 
NL+NLP 
(DS) 
0.469 0.490 0.815 0.622 0.438 
 33.6%  32.4%  7.24%  33.2%  38.2% 
NL+NLP
(PB+DS) 
0.491 0.513 0.860 0.654 0.479 
 39.9%  38.6%  13.2%  40.0%  51.1% 
Table 8. Combination of PB and DS graphs for 
evidence propagation (Term set: Wiki100L) 
Now let us study whether it is possible to com-
bine the PB and DS graphs to obtain better results. 
As shown in Tables 7, 8, and 9 (for term sets 
Wiki200, Wiki100L, and Ext100 respectively, us-
ing the Log formula for fusion and propagation), 
utilizing both graphs really yields additional per-
formance gains. We explain this by the fact that the 
information in the two term similarity graphs tends 
to be complimentary. The performance improve-
ment over Wiki100L is especially remarkable. This 
is reasonable because rare terms do not have ade-
quate information in their supporting sentences due 
to data sparseness. As a result, they benefit the 
most from the pseudo supporting sentences propa-
gated with the similarity graphs. 
 
Method MAP R-Prec P@1 P@5 R@5 
Base 0.384 0.429 0.665 0.472 0.385 
NL+NLP 
(PB) 
0.454 0.479 0.745 0.550 0.456 
 18.3%  11.7%  12.0%  16.5%  18.4% 
NL+NLP 
(DS) 
0.404 0.441 0.720 0.486 0.402 
 5.18%  2.66%  8.27%  2.97%  4.37% 
NL+NLP(P
B+DS) 
0.483 0.518 0.760 0.586 0.492 
 26.0%  20.6%  14.3%  24.2%  27.6% 
Table 9. Combination of PB and DS graphs for 
evidence propagation (Term set: Ext100) 
7 Conclusion 
We demonstrated that the way of aggregating sup-
porting sentences has considerable impact on re-
sults quality of the hyponym extraction task using 
lexico-syntactic patterns, and the widely-used 
counting method is not optimal. We applied a se-
ries of nonlinear evidence fusion formulas to the 
problem and saw noticeable performance im-
provement. The data quality is improved further 
with the combination of nonlinear evidence fusion 
and evidence propagation. We also introduced a 
new evaluation corpus with annotated hypernym 
labels for 300 terms, which were shared with the 
research community. 
Acknowledgments 
We would like to thank Matt Callcut for reading 
through the paper. Thanks to the annotators for 
their efforts in judging the hypernym labels. 
Thanks to Yueguo Chen, Siyu Lei, and the anony-
mous reviewers for their helpful comments and 
suggestions. The first author is partially supported 
by the NSF of China (60903028,61070014), and 
Key Projects in the Tianjin Science and Technolo-
gy Pillar Program. 
 
 
 
 
1167
References  
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas-
ca, and A. Soroa. 2009. A Study on Similarity and 
Relatedness Using Distributional and WordNet-based 
Approaches. In Proc. of NAACL-HLT?2009. 
M. Banko, M.J. Cafarella, S. Soderland, M. Broadhead, 
and O. Etzioni. 2007. Open Information Extraction 
from the Web. In Proc. of IJCAI?2007. 
M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. 
Zhang. 2008. WebTables: Exploring the Power of 
Tables on the Web. In Proceedings of the 34th Con-
ference on Very Large Data Bases (VLDB?2008), 
pages 538?549, Auckland, New Zealand. 
B. Van Durme and M. Pasca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of 
labeled instances for open-domain information ex-
traction. Twenty-Third AAAI Conference on Artifi-
cial Intelligence. 
F. Geraci, M. Pellegrini, M. Maggini, and F. Sebastiani. 
2006. Cluster Generation and Cluster Labelling for 
Web Snippets: A Fast and Accurate Hierarchical So-
lution. In Proceedings of the 13th Conference on 
String Processing and Information Retrieval 
(SPIRE?2006), pages 25?36, Glasgow, Scotland. 
Z. S. Harris. 1985. Distributional Structure. The Philos-
ophy of Linguistics. New York: Oxford University 
Press. 
M. Hearst. 1992. Automatic Acquisition of Hyponyms 
from Large Text Corpora. In Fourteenth International 
Conference on Computational Linguistics, Nantes, 
France. 
Z. Kozareva, E. Riloff, E.H. Hovy. 2008. Semantic 
Class Learning from the Web with Hyponym Pattern 
Linkage Graphs. In Proc. of ACL'2008. 
P. Pantel, E. Crestan, A. Borkovsky, A.-M. Popescu and 
V. Vyas. 2009. Web-Scale Distributional Similarity 
and Entity Set Expansion. EMNLP?2009. Singapore. 
P. Pantel and D. Ravichandran. 2004. Automatically 
Labeling Semantic Classes. In Proc. of the 2004 Hu-
man Language Technology Conference (HLT-
NAACL?2004), 321?328. 
M. Pasca. 2004. Acquisition of Categorized Named 
Entities for Web Search. In Proc. of CIKM?2004. 
M. Pasca. 2010. The Role of Queries in Ranking La-
beled Instances Extracted from Text. In Proc. of 
COLING?2010, Beijing, China. 
S. Shi, B. Lu, Y. Ma, and J.-R. Wen. 2009. Nonlinear 
Static-Rank Computation. In Proc. of CIKM?2009, 
Kong Kong. 
S. Shi, H. Zhang, X. Yuan, J.-R. Wen. 2010. Corpus-
based Semantic Class Mining: Distributional vs. Pat-
tern-Based Approaches. In Proc. of COLING?2010, 
Beijing, China. 
K. Shinzato and K. Torisawa. 2004. Acquiring Hypon-
ymy Relations from Web Documents. In Proc. of the 
2004 Human Language Technology Conference 
(HLT-NAACL?2004). 
R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning 
Syntactic Patterns for Automatic Hypernym Discov-
ery. In Proceedings of the 19th Conference on Neural 
Information Processing Systems. 
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic 
Taxonomy Induction from Heterogenous Evidence. 
In Proceedings of the 21st International Conference 
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics 
(COLING-ACL-06), 801?808. 
P. P. Talukdar and F. Pereira. 2010. Experiments in 
Graph-based Semi-Supervised Learning Methods for 
Class-Instance Acquisition. In 48th Annual Meeting 
of the Association for Computational Linguistics 
(ACL?2010). 
P. P. Talukdar, J. Reisinger, M. Pasca, D. Ravichandran, 
R. Bhagat, and F. Pereira. 2008. Weakly-Supervised 
Acquisition of Labeled Class Instances using Graph 
Random Walks. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language 
Processing (EMNLP?2008), pages 581?589. 
R.C. Wang. W.W. Cohen. Automatic Set Instance Ex-
traction using the Web. In Proc. of the 47th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL-IJCNLP?2009), pages 441?449, Sin-
gapore. 
H. Zhang, M. Zhu, S. Shi, and J.-R. Wen. 2009. Em-
ploying Topic Models for Pattern-based Semantic 
Class Discovery. In Proc. of the 47th Annual Meet-
ing of the Association for Computational Linguistics 
(ACL-IJCNLP?2009), pages 441?449, Singapore. 
 
1168
