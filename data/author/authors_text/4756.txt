Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 515?522,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Parsing and Subcategorization Data
Jianguo Li and Chris Brew
Department of Linguistics
The Ohio State University
Columbus, OH, USA
{jianguo|cbrew}@ling.ohio-state.edu
Abstract
In this paper, we compare the per-
formance of a state-of-the-art statistical
parser (Bikel, 2004) in parsing written and
spoken language and in generating sub-
categorization cues from written and spo-
ken language. Although Bikel?s parser
achieves a higher accuracy for parsing
written language, it achieves a higher ac-
curacy when extracting subcategorization
cues from spoken language. Our exper-
iments also show that current technology
for extracting subcategorization frames
initially designed for written texts works
equally well for spoken language. Addi-
tionally, we explore the utility of punctu-
ation in helping parsing and extraction of
subcategorization cues. Our experiments
show that punctuation is of little help in
parsing spoken language and extracting
subcategorization cues from spoken lan-
guage. This indicates that there is no need
to add punctuation in transcribing spoken
corpora simply in order to help parsers.
1 Introduction
Robust statistical syntactic parsers, made possi-
ble by new statistical techniques (Collins, 1999;
Charniak, 2000; Bikel, 2004) and by the avail-
ability of large, hand-annotated training corpora
such as WSJ (Marcus et al, 1993) and Switch-
board (Godefrey et al, 1992), have had a major
impact on the field of natural language process-
ing. There are many ways to make use of parsers?
output. One particular form of data that can be ex-
tracted from parses is information about subcate-
gorization. Subcategorization data comes in two
forms: subcategorization frame (SCF) and sub-
categorization cue (SCC). SCFs differ from SCCs
in that SCFs contain only arguments while SCCs
contain both arguments and adjuncts. Both SCFs
and SCCs have been crucial to NLP tasks. For ex-
ample, SCFs have been used for verb disambigua-
tion and classification (Schulte im Walde, 2000;
Merlo and Stevenson, 2001; Lapata and Brew,
2004; Merlo et al, 2005) and SCCs for semantic
role labeling (Xue and Palmer, 2004; Punyakanok
et al, 2005).
Current technology for automatically acquiring
subcategorization data from corpora usually relies
on statistical parsers to generate SCCs. While
great efforts have been made in parsing written
texts and extracting subcategorization data from
written texts, spoken corpora have received little
attention. This is understandable given that spoken
language poses several challenges that are absent
in written texts, including disfluency, uncertainty
about utterance segmentation and lack of punctu-
ation. Roland and Jurafsky (1998) have suggested
that there are substantial subcategorization differ-
ences between written corpora and spoken cor-
pora. For example, while written corpora show a
much higher percentage of passive structures, spo-
ken corpora usually have a higher percentage of
zero-anaphora constructions. We believe that sub-
categorization data derived from spoken language,
if of acceptable quality, would be of more value to
NLP tasks involving a syntactic analysis of spoken
language. We do not show this here.
The goals of this study are as follows:
1. Test the performance of Bikel?s parser in
parsing written and spoken language.
2. Compare the accuracy level of SCCs gen-
erated from parsed written and spoken lan-
515
guage. We hope that such a comparison will
shed some light on the feasibility of acquiring
subcategorization data from spoken language
using the current SCF acquisition technology
initially designed for written language.
3. Apply our SCF extraction system (Li and
Brew, 2005) to spoken and written lan-
guage separately and compare the accuracy
achieved for the acquired SCFs from spoken
and written language.
4. Explore the utility of punctuation1 in pars-
ing and extraction of SCCs. It is gen-
erally recognized that punctuation helps in
parsing written texts. For example, Roark
(2001) finds that removing punctuation from
both training and test data (WSJ) decreases
his parser?s accuracy from 86.4%/86.8%
(LR/LP) to 83.4%/84.1%. However, spo-
ken language does not come with punctua-
tion. Even when punctuation is added in the
process of transcription, its utility in help-
ing parsing is slight. Both Roark (2001)
and Engel et al (2002) report that removing
punctuation from both training and test data
(Switchboard) results in only 1% decrease in
their parser?s accuracy.
2 Experiment Design
Three models will be investigated for parsing and
extracting SCCs from the parser?s output:
1. punc: leaving punctuation in both training
and test data.
2. no-punc: removing punctuation from both
training and test data.
3. punc-no-punc: removing punctuation from
only the test data.
Following the convention in the parsing com-
munity, for written language, we selected sections
02-21 of WSJ as training data and section 23 as
test data (Collins, 1999). For spoken language, we
designated section 2 and 3 of Switchboard as train-
ing data and files of sw4004 to sw4135 of section 4
as test data (Roark, 2001). Since we are also inter-
ested in extracting SCCs from the parser?s output,
1We use punctuation to refer to sentence-internal punctu-
ation unless otherwise specified.
label clause type desired SCCs
gerundive (NP)-GERUND
S small clause NP-NP, (NP)-ADJP
control (NP)-INF-to
control (NP)-INF-wh-to
SBAR with a complementizer (NP)-S-wh, (NP)-S-that
without a complementizer (NP)-S-that
Table 1: SCCs for different clauses
we eliminated from the two test corpora all sen-
tences that do not contain verbs. Our experiments
proceed in the following three steps:
1. Tag test data using the POS-tagger described
in Ratnaparkhi (1996).
2. Parse the POS-tagged data using Bikel?s
parser.
3. Extract SCCs from the parser?s output. The
extractor we built first locates each verb in the
parser?s output and then identifies the syntac-
tic categories of all its sisters and combines
them into an SCC. However, there are cases
where the extractor has more work to do.
? Finite and Infinite Clauses: In the Penn
Treebank, S and SBAR are used to label
different types of clauses, obscuring too
much detail about the internal structure
of each clause. Our extractor is designed
to identify the internal structure of dif-
ferent types of clause, as shown in Table
1.
? Passive Structures: As noted above,
Roland and Jurafsky (Roland and Juraf-
sky, 1998) have noticed that written lan-
guage tends to have a much higher per-
centage of passive structures than spo-
ken language. Our extractor is also
designed to identify passive structures
from the parser?s output.
3 Experiment Results
3.1 Parsing and SCCs
We used EVALB measures Labeled Recall (LR)
and Labeled Precision (LP) to compare the pars-
ing performance of different models. To compare
the accuracy of SCCs proposed from the parser?s
output, we calculated SCC Recall (SR) and SCC
Precision (SP). SR and SP are defined as follows:
SR = number of correct cues from the parser?s output
number of cues from treebank parse (1)
516
WSJ
model LR/LP SR/SP
punc 87.92%/88.29% 76.93%/77.70%
no-punc 86.25%/86.91% 76.96%/76.47%
punc-no-punc 82.31%/83.70% 74.62%/74.88%
Switchboard
model LR/LP SR/SP
punc 83.14%/83.80% 79.04%/78.62%
no-punc 82.42%/83.74% 78.81%/78.37%
punc-no-punc 78.62%/80.68% 75.51%/75.02%
Table 2: Results of parsing and extraction of SCCs
SP = number of correct cues from the parser?s output
number of cues from the parser?s output (2)
SCC Balanced F-measure = 2 ? SR ? SPSR + SP (3)
The results for parsing WSJ and Switchboard
and extracting SCCs are summarized in Table 2.
The LR/LP figures show the following trends:
1. Roark (2001) showed LR/LP of
86.4%/86.8% for punctuated written
language, 83.4%/84.1% for unpunctuated
written language. We achieve a higher
accuracy in both punctuated and unpunctu-
ated written language, and the decrease if
punctuation is removed is less
2. For spoken language, Roark (2001) showed
LR/LP of 85.2%/85.6% for punctuated spo-
ken language, 84.0%/84.6% for unpunctu-
ated spoken language. We achieve a lower
accuracy in both punctuated and unpunctu-
ated spoken language, and the decrease if
punctuation is removed is less. The trends in
(1) and (2) may be due to parser differences,
or to the removal of sentences lacking verbs.
3. Unsurprisingly, if the test data is unpunctu-
ated, but the models have been trained on
punctuated language, performance decreases
sharply.
In terms of the accuracy of extraction of SCCs,
the results follow a similar pattern. However, the
utility of punctuation turns out to be even smaller.
Removing punctuation from both the training and
test data results in a 0.8% drop in the accuracy of
SCC extraction for written language and a 0.3%
drop for spoken language.
Figure 1 exhibits the relation between the ac-
curacy of parsing and that of extracting SCCs.
If we consider WSJ and Switchboard individu-
ally, there seems to exist a positive correlation be-
tween the accuracy of parsing and that of extract-
ing SCCs. In other words, higher LR/LP indicates
punc no?punc punc?no?punc
74
76
78
80
82
84
86
88
90
Models
F?
me
as
ur
e(%
)
WSJ parsing
Switchboard parsing
WSJ SCC
Switchboard SCC
Figure 1: F-measure for parsing and extraction of
SCCs
higher SR/SP. However, Figure 1 also shows that
although the parser achieves a higher F-measure
value for paring WSJ, it achieves a higher F-
measure value for generating SCCs from Switch-
board.
The fact that the parser achieves a higher ac-
curacy of extracting SCCs from Switchboard than
WSJ merits further discussion. Intuitively, it
seems to be true that the shorter an SCC is, the
more likely that the parser is to get it right. This
intuition is confirmed by the data shown in Fig-
ure 2. Figure 2 plots the accuracy level of extract-
ing SCCs by SCC?s length. It is clear from Fig-
ure 2 that as SCCs get longer, the F-measure value
drops progressively for both WSJ and Switch-
board. Again, Roland and Jurafsky (1998) have
suggested that one major subcategorization differ-
ence between written and spoken corpora is that
spoken corpora have a much higher percentage of
the zero-anaphora construction. We then exam-
ined the distribution of SCCs of different length in
WSJ and Switchboard. Figure 3 shows that SCCs
of length 02 account for a much higher percentage
in Switchboard than WSJ, but it is always the other
way around for SCCs of non-zero length. This
observation led us to believe that the better per-
formance that Bikel?s parser achieves in extracting
SCCs from Switchboard may be attributed to the
following two factors:
1. Switchboard has a much higher percentage of
SCCs of length 0.
2. The parser is very accurate in extracting
shorter SCCs.
2Verbs have a length-0 SCC if they are intransitive and
have no modifiers.
517
0 1 2 3 4
10
20
30
40
50
60
70
80
90
Length of SCC
F?
me
as
ur
e(%
)
WSJ
Switchboard
Figure 2: F-measure for SCCs of different length
0 1 2 3 4
0
10
20
30
40
50
60
Length of SCCs
Pe
rc
en
ta
ge
(%
)
WSJ
Switchboard
Figure 3: Distribution of SCCs by length
3.2 Extraction of Dependents
In order to estimate the effects of SCCs of length
0, we examined the parser?s performance in re-
trieving dependents of verbs. Every constituent
(whether an argument or adjunct) in an SCC gen-
erated by the parser is considered a dependent of
that verb. SCCs of length 0 will be discounted be-
cause verbs that do not take any arguments or ad-
juncts have no dependents3 . In addition, this way
of evaluating the extraction of SCCs also matches
the practice in some NLP tasks such as semantic
role labeling (Xue and Palmer, 2004). For the task
of semantic role labeling, the total number of de-
pendents correctly retrieved from the parser?s out-
put affects the accuracy level of the task.
To do this, we calculated the number of depen-
dents shared by between each SCC proposed from
the parser?s output and its corresponding SCC pro-
3We are aware that subjects are typically also consid-
ered dependents, but we did not include subjects in our
experiments
shared-dependents[i.j] = MAX(
shared-dependents[i-1,j],
shared-dependents[i-1,j-1]+1 if target[i] = source[j],
shared-dependents[i-1,j-1] if target[i] != source[j],
shared-dependents[i,j-1])
Table 3: The algorithm for computing shared de-
pendents
INF #5 1 1 2 3
ADVP #4 1 1 2 2
PP-in #3 1 1 2 2
NP #2 1 1 1 1
NP #1 1 1 1 1
#0 #1 #2 #3 #4
NP S-that PP-in INF
Table 4: An example of computing the number of
shared dependents
posed from Penn Treebank. We based our cal-
culation on a modified version of Minimum Edit
Distance Algorithm. Our algorithm works by cre-
ating a shared-dependents matrix with one col-
umn for each constituent in the target sequence
(SCCs proposed from Penn Treebank) and one
row for each constituent in the source sequence
(SCCs proposed from the parser?s output). Each
cell shared-dependent[i,j] contains the number of
constituents shared between the first i constituents
of the target sequence and the first j constituents of
the source sequence. Each cell can then be com-
puted as a simple function of the three possible
paths through the matrix that arrive there. The al-
gorithm is illustrated in Table 3.
Table 4 shows an example of how the algo-
rithm works with NP-S-that-PP-in-INF as the tar-
get sequence and NP-NP-PP-in-ADVP-INF as the
source sequence. The algorithm returns 3 as the
number of dependents shared by two SCCs.
We compared the performance of Bikel?s parser
in retrieving dependents from written and spo-
ken language over all three models using De-
pendency Recall (DR) and Dependency Precision
(DP). These metrics are defined as follows:
DR = number of correct dependents from parser?s output
number of dependents from treebank parse
(4)
DP = number of correct dependents from parser?s output
number of dependents from parser?s output
(5)
Dependency F-measure = 2 ?DR ?DPDR +DP (6)
518
punc no?punc punc?no?punc
78
80
82
84
86
Models
F?
me
as
ur
e(%
)
WSJ
Switchboard
Figure 4: F-measure for extracting dependents
The results of Bikel?s parser in retrieving depen-
dents are summarized in Figure 4. Overall, the
parser achieves a better performance for WSJ over
all three models, just the opposite of what have
been observed for SCC extraction. Interestingly,
removing punctuation from both the training and
test data actually slightly improves the F-measure.
This holds true for both WSJ and Switchboard.
This Dependency F-measure differs in detail from
similar measures in Xue and Palmer (2004). For
present purposes all that matters is the relative
value for WSJ and Switchboard.
4 Extraction of SCFs from Spoken
Language
Our experiments indicate that the SCCs generated
by the parser from spoken language are as accurate
as those generated from written texts. Hence, we
would expect that the current technology for ex-
tracting SCFs, initially designed for written texts,
should work equally well for spoken language.
We previously built a system for automatically ex-
tracting SCFs from spoken BNC, and reported ac-
curacy comparable to previous systems that work
with only written texts (Li and Brew, 2005). How-
ever, Korhonen (2002) has shown that a direct
comparison of different systems is very difficult to
interpret because of the variations in the number
of targeted SCFs, test verbs, gold standards and in
the size of the test data. For this reason, we apply
our SCF acquisition system separately to a written
and spoken corpus of similar size from BNC and
compare the accuracy of acquired SCF sets.
4.1 Overview
As noted above, previous studies on automatic ex-
traction of SCFs from corpora usually proceed in
two steps and we adopt this approach.
1. Hypothesis Generation: Identify all SCCs
from the corpus data.
2. Hypothesis Selection: Determine which SCC
is a valid SCF for a particular verb.
4.2 SCF Extraction System
We briefly outline our SCF extraction system
for automatically extracting SCFs from corpora,
which was based on the design proposed in
Briscoe and Carroll (1997).
1. A Statistical Parser: Bikel?s parser is used
to parse input sentences.
2. An SCF Extractor: An extractor is use to
extract SCCs from the parser?s output.
3. An English Lemmatizer: MORPHA (Min-
nen et al, 2000) is used to lemmatize each
verb.
4. An SCF Evaluator: An evaluator is used
to filter out false SCCs based on their like-
lihood.
An SCC generated by the parser and extractor
may be a correct SCC, or it may contain an ad-
junct, or it may simply be wrong due to tagging or
parsing errors. We therefore need an SCF evalua-
tor capable of filtering out false cues. Our evalu-
ator has two parts: the Binomial Hypothesis Test
(Brent, 1993) and a back-off algorithm (Sarkar and
Zeman, 2000).
1. The Binomial Hypothesis Test (BHT): Let
p be the probability that an scfi occurs with
verbj that is not supposed to take scfi. If a
verb occurs n times and m of those times it
co-occurs with scfi, then the scfi cues are
false cues is estimated by the summation of
the binomial distribution for m ? k ? n:
P (m+, n, p) =
n
X
k=m
n!
k!(n? k)!p
k(1? p)(n?k) (7)
If the value of P (m+, n, p) is less than or
equal to a small threshold value, then the null
hypothesis that verbj does not take scfi is ex-
tremely unlikely to be true. Hence, scfi is
very likely to be a valid SCF for verbj . The
519
SCCs SCFs
NP-PP-before
NP-S-when NP
NP-PP-at-S-before
NP-PP-to-S-when
NP-PP-to-PP-at NP-PP-to
NP-PP-to-S-because-ADVP
Table 5: SCCs and correct SCFs for introduce
corpus WC SC
number of verb tokens 115,524 109,678
number of verb types 5,234 4,789
verb types seen more than 10 times 1,102 998
number of acquired SCFs 2,688 1,984
average number of SCFs per verb 2.43 1.99
Table 6: Training data for WC and SC
value of m and n can be directly computed
from the extractor?s output, but the value of
p is not easy to obtain. Following Manning
(1993), we empirically determined the value
of p. It was between 0.005 to 0.4 depend-
ing on the likelihood of an SCC being a valid
SCF.
2. Back-off Algorithm: Many SCCs generated
by the parser and extractor tend to contain
some adjuncts. However, for many SCCs,
one of its subsets is likely to be the correct
SCF. Table 5 shows some SCCs generated by
the extractor and the corresponding SCFs.
The Back-off Algorithm always starts with
the longest SCC for each verb. Assume that
this SCC fails the BHT. The evaluator then
eliminates the last constituent from the re-
jected cue, transfers its frequency to its suc-
cessor and submits the successor to the BHT
again. In this way, frequency can accumulate
and more valid frames survive the BHT.
4.3 Results and Discussion
We evaluated our SCF extraction system on writ-
ten and spoken BNC. We chose one million word
written corpus (WC) and a comparable spoken
corpus (SC) from BNC. Table 6 provides relevant
information on the two corpora. We only keep the
verbs that occur at least 10 times in our training
data.
To compare the performance of our system on
WC and SC, we calculated the type precision, type
gold standard COMLEX Manually Constructed
corpus WC SC WC SC
type precision 93.1% 92.9% 93.1% 92.9%
type recall 49.2% 47.7% 56.5% 57.6%
F-measure 64.4% 63.1% 70.3% 71.1%
Table 7: Type precision and recall and F-measure
recall and F-measure. Type precision is the per-
centage of SCF types that our system proposes
which are correct according some gold standard
and type recall is the percentage of correct SCF
types proposed by our system that are listed in the
gold standard. We used the 14 verbs 4 selected
by Briscoe and Carroll (1997) and evaluated our
results of these verbs against the SCF entries in
two gold standards: COMLEX (Grishman et al,
1994) and a manually constructed SCF set from
the training data. It makes sense to use a manually
constructed SCF set while calculating type preci-
sion and recall because some of the SCFs in a syn-
tax dictionary such as COMLEX might not occur
in the training data at all. We constructed separate
SCF sets for the written and spoken BNC.
The results are summarized in Table 7. As
shown in Table 7, the accuracy achieved for WC
and SC are very comparable: Our system achieves
a slightly better result for WC when using COM-
LEX as the gold standard and for SC when using
manually constructed SCF set as gold standard,
suggesting that it is feasible to apply the current
technology for automatically extracting SCFs to
spoken language.
5 Conclusions and Future Work
5.1 Use of Parser?s Output
In this paper, we have shown that it is not nec-
essarily true that statistical parsers always per-
form worse when dealing with spoken language.
The conventional accuracy metrics for parsing
(LR/LP) should not be taken as the only metrics
in determining the feasibility of applying statisti-
cal parsers to spoken language. It is necessary to
consider what information we want to extract out
of parsers? output and make use of.
1. Extraction of SCFs from Corpora: This task
takes SCCs generated by the parser and ex-
tractor as input. Our experiments show that
4The 14 verbs used in Briscoe and Carroll (1997) are ask,
begin, believe, cause, expect, find, give, help, like, move, pro-
duce, provide, seem and sway. We replaced sway with show
because sway occurs less than 10 times in our training data.
520
the SCCs generated for spoken language are
as accurate as those generated for written lan-
guage. We have also shown that it is feasible
to apply the current SCF extraction technol-
ogy to spoken language.
2. Semantic Role Labeling: This task usually
operates on parsers? output and the number
of dependents of each verb that are correctly
retrieved by the parser clearly affects the ac-
curacy of the task. Our experiments show
that the parser achieves a much lower accu-
racy in retrieving dependents from the spoken
language than written language. This seems
to suggest that a lower accuracy is likely to
be achieved for a semantic role labeling task
performed on spoken language. We are not
aware that this has yet been tried.
5.2 Punctuation and Speech Transcription
Practice
Both our experiments and Roark?s experiments
show that parsing accuracy measured by LR/LP
experiences a sharper decrease for WSJ than
Switchboard after we removed punctuation from
training and test data. In spoken language, com-
mas are largely used to delimit disfluency ele-
ments. As noted in Engel et al (2002), statis-
tical parsers usually condition the probability of
a constituent on the types of its neighboring con-
stituents. The way that commas are used in speech
transcription seems to have the effect of increasing
the range of neighboring constituents, thus frag-
menting the data and making it less reliable. On
the other hand, in written texts, commas serve as
more reliable cues for parsers to identify phrasal
and clausal boundaries.
In addition, our experiment demonstrates that
punctuation does not help much with extraction of
SCCs from spoken language. Removing punctu-
ation from both the training and test data results
in rougly a 0.3% decrease in SR/SP. Furthermore,
removing punctuation from both training and test
data actually slightly improves the performance
of Bikel?s parser in retrieving dependents from
spoken language. All these results seem to sug-
gest that adding punctuation in speech transcrip-
tion is of little help to statistical parsers includ-
ing at least three state-of-the-art statistical parsers
(Collins, 1999; Charniak, 2000; Bikel, 2004). As a
result, there may be other good reasons why some-
one who wants to build a Switchboard-like corpus
should choose to provide punctuation, but there is
no need to do so simply in order to help parsers.
However, segmenting utterances into individual
units is necessary because statistical parsers re-
quire sentence boundaries to be clearly delimited.
Current statistical parsers are unable to handle an
input string consisting of two sentences. For ex-
ample, when presented with an input string as in
(1) and (2), if the two sentences are separated by a
period (1), Bikel?s parser wrongly treats the sec-
ond sentence as a sentential complement of the
main verb like in the first sentence. As a result, the
extractor generates an SCC NP-S for like, which is
incorrect. The parser returns the same parse after
we removed the period (2) and let the parser parse
it again.
(1) I like the long hair. It was back in high
school.
(2) I like the long hair It was back in high school.
Hence, while adding punctuation in transcribing
a Switchboard-like corpus is not of much help to
statistical parsers, segmenting utterances into in-
dividual units is crucial for statistical parsers. In
future work, we plan to develop a system capa-
ble of automatically segmenting speech utterances
into individual units.
6 Acknowledgments
This study was supported by NSF grant 0347799.
Our thanks go to Eric Fosler-Lussier, Mike White
and three anonymous reviewers for their valuable
comments.
References
D. Bikel. 2004. Intricacies of Collin?s parsing models.
Computational Linguistics, 30(2):479?511.
M. Brent. 1993. From grammar to lexicon: Unsu-
pervised learning of lexical syntax. Computational
Linguistics, 19(3):243?262.
T. Briscoe and J. Carroll. 1997. Automatic extraction
of subcategorization from corpora. In Proceedings
of the 5th ACL Conference on Applied Natural Lan-
guage Processing, pages 356?363.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 2000 Conference of
the North American Chapter of the Association for
Computation Linguistics, pages 132?139.
M. Collins. 1999. Head-driven statistical models for
natural language parsing. Ph.D. thesis, University
of Pennsylvania.
521
D. Engel, E. Charniak, and M. Johnson. 2002. Parsing
and disfluency placement. In Proceedings of 2002
Conference on Empirical Methods of Natural Lan-
guage Processing, pages 49?54.
J. Godefrey, E. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for
research and development. In Proceedings of
ICASSP-92, pages 517?520.
R. Grishman, C. Macleod, and A. Meryers. 1994.
Comlex syntax: Building a computational lexicon.
In Proceedings of the 1994 International Conference
of Computational Linguistics, pages 268?272.
A. Korhonen. 2002. Subcategorization Acquisition.
Ph.D. thesis, Cambridge University.
M. Lapata and C. Brew. 2004. Verb class disambigua-
tion using informative priors. Computational Lin-
guistics, 30(1):45?73.
J. Li and C. Brew. 2005. Automatic extraction of sub-
categorization frames from spoken corpora. In Pro-
ceedings of the Interdisciplinary Workshop on the
Identification and Representation of Verb Features
and Verb Classes, Saarbracken, Germany.
C. Manning. 1993. Automatic extraction of a large
subcategorization dictionary from corpora. In Pro-
ceedings of 31st Annual Meeting of the Association
for Computational Linguistics, pages 235?242.
M. Marcus, G. Kim, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English:
the Penn Treebank. Computational Linguistics,
19(2):313?330.
P. Merlo and S. Stevenson. 2001. Automatic
verb classification based on statistical distribution
of argument structure. Computational Linguistics,
27(3):373?408.
P. Merlo, E. Joanis, and J. Henderson. 2005. Unsuper-
vised verb class disambiguation based on diathesis
alternations. manuscripts.
G. Minnen, J. Carroll, and D. Pearce. 2000. Applied
morphological processing of English. Natural Lan-
guage Engineering, 7(3):207?223.
V. Punyakanok, D. Roth, and W. Yih. 2005. The neces-
sity of syntactic parsing for semantic role labeling.
In Proceedings of the 2nd Midwest Computational
Linguistics Colloquium, pages 15?22.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of the Con-
ference on Empirical Methods of Natural Language
Processing, pages 133?142.
B. Roark. 2001. Robust Probabilistic Predictive
Processing: Motivation, Models, and Applications.
Ph.D. thesis, Brown University.
D. Roland and D. Jurafsky. 1998. How verb sub-
categorization frequency is affected by the corpus
choice. In Proceedings of the 17th International
Conference on Computational Linguistics, pages
1122?1128.
A. Sarkar and D. Zeman. 2000. Automatic extraction
of subcategorization frames for Czech. In Proceed-
ings of the 19th International Conference on Com-
putational Linguistics, pages 691?697.
S. Schulte im Walde. 2000. Clustering verbs semanti-
cally according to alternation behavior. In Proceed-
ings of the 18th International Conference on Com-
putational Linguistics, pages 747?753.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In Proceedings of 2004 Con-
ference on Empirical Methods in Natural Language
Processing, pages 88?94.
522
Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 79?84,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Parsing and Subcategorization Data
Jianguo Li
Department of Linguistics
The Ohio State University
Columbus, OH, USA
jianguo@ling.ohio-state.edu
Abstract
In this paper, we compare the per-
formance of a state-of-the-art statistical
parser (Bikel, 2004) in parsing written and
spoken language and in generating sub-
categorization cues from written and spo-
ken language. Although Bikel?s parser
achieves a higher accuracy for parsing
written language, it achieves a higher ac-
curacy when extracting subcategorization
cues from spoken language. Additionally,
we explore the utility of punctuation in
helping parsing and extraction of subcat-
egorization cues. Our experiments show
that punctuation is of little help in pars-
ing spoken language and extracting sub-
categorization cues from spoken language.
This indicates that there is no need to add
punctuation in transcribing spoken cor-
pora simply in order to help parsers.
1 Introduction
Robust statistical syntactic parsers, made possi-
ble by new statistical techniques (Collins, 1999;
Charniak, 2000; Bikel, 2004) and by the avail-
ability of large, hand-annotated training corpora
such as WSJ (Marcus et al, 1993) and Switch-
board (Godefrey et al, 1992), have had a major
impact on the field of natural language process-
ing. There are many ways to make use of parsers?
output. One particular form of data that can be ex-
tracted from parses is information about subcate-
gorization. Subcategorization data comes in two
forms: subcategorization frame (SCF) and sub-
categorization cue (SCC). SCFs differ from SCCs
in that SCFs contain only arguments while SCCs
contain both arguments and adjuncts. Both SCFs
and SCCs have been crucial to NLP tasks. For ex-
ample, SCFs have been used for verb disambigua-
tion and classification (Schulte im Walde, 2000;
Merlo and Stevenson, 2001; Lapata and Brew,
2004; Merlo et al, 2005) and SCCs for semantic
role labeling (Xue and Palmer, 2004; Punyakanok
et al, 2005).
Current technology for automatically acquiring
subcategorization data from corpora usually relies
on statistical parsers to generate SCCs. While
great efforts have been made in parsing written
texts and extracting subcategorization data from
written texts, spoken corpora have received little
attention. This is understandable given that spoken
language poses several challenges that are absent
in written texts, including disfluency, uncertainty
about utterance segmentation and lack of punctu-
ation. Roland and Jurafsky (1998) have suggested
that there are substantial subcategorization differ-
ences between written corpora and spoken cor-
pora. For example, while written corpora show a
much higher percentage of passive structures, spo-
ken corpora usually have a higher percentage of
zero-anaphora constructions. We believe that sub-
categorization data derived from spoken language,
if of acceptable quality, would be of more value to
NLP tasks involving a syntactic analysis of spoken
language, but we do not pursue it here.
The goals of this study are as follows:
1. Test the performance of Bikel?s parser in
parsing written and spoken language.
2. Compare the accuracy level of SCCs gen-
erated from parsed written and spoken lan-
guage. We hope that such a comparison will
shed some light on the feasibility of acquiring
SCFs from spoken language using the cur-
79
rent SCF acquisition technology initially de-
signed for written language.
3. Explore the utility of punctuation1 in pars-
ing and extraction of SCCs. It is gen-
erally recognized that punctuation helps in
parsing written texts. For example, Roark
(2001) finds that removing punctuation from
both training and test data (WSJ) decreases
his parser?s accuracy from 86.4%/86.8%
(LR/LP) to 83.4%/84.1%. However, spo-
ken language does not come with punctua-
tion. Even when punctuation is added in the
process of transcription, its utility in help-
ing parsing is slight. Both Roark (2001)
and Engel et al (2002) report that removing
punctuation from both training and test data
(Switchboard) results in only 1% decrease in
their parser?s accuracy.
2 Experiment Design
Three models will be investigated for parsing and
extracting SCCs from the parser?s output:
1. punc: leaving punctuation in both training
and test data.
2. no-punc: removing punctuation from both
training and test data.
3. punc-no-punc: removing punctuation from
only test data.
Following the convention in the parsing com-
munity, for written language, we selected sections
02-21 of WSJ as training data and section 23 as
test data (Collins, 1999). For spoken language, we
designated section 2 and 3 of Switchboard as train-
ing data and files of sw4004 to sw4135 of section 4
as test data (Roark, 2001). Since we are also inter-
ested in extracting SCCs from the parser?s output,
we eliminated from the two test corpora all sen-
tences that do not contain verbs. Our experiments
proceed in the following three steps:
1. Tag test data using the POS-tagger described
in Ratnaparkhi (1996).
2. Parse the POS-tagged data using Bikel?s
parser.
1We use punctuation to refer to sentence-internal punctu-
ation unless otherwise specified.
label clause type desired SCCs
gerundive (NP)-GERUND
S small clause NP-NP, (NP)-ADJP
control (NP)-INF-to
control (NP)-INF-wh-to
SBAR with a complementizer (NP)-S-wh, (NP)-S-that
without a complementizer (NP)-S-that
Table 1: SCCs for different clauses
3. Extract SCCs from the parser?s output. The
extractor we built first locates each verb in the
parser?s output and then identifies the syntac-
tic categories of all its sisters and combines
them into an SCC. However, there are cases
where the extractor has more work to do.
? Finite and Infinite Clauses: In the Penn
Treebank, S and SBAR are used to label
different types of clauses, obscuring too
much detail about the internal structure
of each clause. Our extractor is designed
to identify the internal structure of dif-
ferent types of clause, as shown in Table
1.
? Passive Structures: As noted above,
Roland and Jurafsky (Roland and Juraf-
sky, 1998) have noticed that written lan-
guage tends to have a much higher per-
centage of passive structures than spo-
ken language. Our extractor is also
designed to identify passive structures
from the parser?s output.
3 Experiment Results
3.1 Parsing and SCCs
We used EVALB measures Labeled Recall (LR)
and Labeled Precision (LP) to compare the pars-
ing performance of different models. To compare
the accuracy of SCCs proposed from the parser?s
output, we calculated SCC Recall (SR) and SCC
Precision (SP). SR and SP are defined as follows:
SR = number of correct cues from the parser?s output
number of cues from treebank parse (1)
SP = number of correct cues from the parser?s output
number of cues from the parser?s output (2)
SCC Balanced F-measure = 2 ? SR ? SPSR+ SP (3)
The results for parsing WSJ and Switchboard
and extracting SCCs are summarized in Table 2.
The LR/LP figures show the following trends:
80
WSJ
model LR/LP SR/SP
punc 87.92%/88.29% 76.93%/77.70%
no-punc 86.25%/86.91% 76.96%/76.47%
punc-no-punc 82.31%/83.70% 74.62%/74.88%
Switchboard
model LR/LP SR/SP
punc 83.14%/83.80% 79.04%/78.62%
no-punc 82.42%/83.74% 78.81%/78.37%
punc-no-punc 78.62%/80.68% 75.51%/75.02%
Table 2: Results of parsing and extraction of SCCs
1. Roark (2001) showed LR/LP of
86.4%/86.8% for punctuated written
language, 83.4%/84.1% for unpunctuated
written language. We achieve a higher
accuracy in both punctuated and unpunctu-
ated written language, and the decrease if
punctuation is removed is less
2. For spoken language, Roark (2001) showed
LR/LP of 85.2%/85.6% for punctuated spo-
ken language, 84.0%/84.6% for unpunctu-
ated spoken language. We achieve a lower
accuracy in both punctuated and unpunctu-
ated spoken language, and the decrease if
punctuation is removed is less. The trends in
(1) and (2) may be due to parser differences,
or to the removal of sentences lacking verbs.
3. Unsurprisingly, if the test data is unpunctu-
ated, but the models have been trained on
punctuated language, performance decreases
sharply.
In terms of the accuracy of extraction of SCCs,
the results follow a similar pattern. However, the
utility of punctuation turns out to be even smaller.
Removing punctuation from both training and test
data results in a less than 0.3% drop in the accu-
racy of SCC extraction.
Figure 1 exhibits the relation between the ac-
curacy of parsing and that of extracting SCCs.
If we consider WSJ and Switchboard individu-
ally, there seems to exist a positive correlation
between the accuracy of parsing and that of ex-
tracting SCCs. In other words, higher LR/LP
indicates higher SR/SP. However, Figure 1 also
shows that although the parser achieves a higher
F-measure value for paring WSJ, it achieves a
higher F-measure value when generating SCCs
from Switchboard.
The fact that the parser achieves a higher accu-
racy for extracting SCCs from Switchboard than
WSJ merits further discussion. Intuitively, it
punc no?punc punc?no?punc
74
76
78
80
82
84
86
88
90
Models
F?
me
as
ur
e(%
)
WSJ parsing
Switchboard parsing
WSJ SCC
Switchboard SCC
Figure 1: F-measure for parsing and extraction of
SCCs
seems to be true that the shorter an SCC is, the
more likely that the parser is to get it right. This
intuition is confirmed by the data shown in Fig-
ure 2. Figure 2 plots the accuracy level of extract-
ing SCCs by SCC?s length. It is clear from Fig-
ure 2 that as SCCs get longer, the F-measure value
drops progressively for both WSJ and Switch-
board. Again, Roland and Jurafsky (1998) have
suggested that one major subcategorization differ-
ence between written and spoken corpora is that
spoken corpora have a much higher percentage of
the zero-anaphora construction. We then exam-
ined the distribution of SCCs of different length in
WSJ and Switchboard. Figure 3 shows that SCCs
of length 02 account for a much higher percentage
in Switchboard than WSJ, but it is always the other
way around for SCCs of non-zero length. This
observation led us to believe that the better per-
formance that Bikel?s parser achieves in extracting
SCCs from Switchboard may be attributed to the
following two factors:
1. Switchboard has a much higher percentage of
SCCs of length 0.
2. The parser is very accurate in extracting
shorter SCCs.
3.2 Extraction of Dependents
In order to estimate the effects of SCCs of length
0, we examined the parser?s performance in re-
trieving dependents of verbs. Every constituent
(whether an argument or adjunct) in an SCC gen-
erated by the parser is considered a dependent of
2Verbs have a length-0 SCC if they are intransitive and
have no modifiers.
81
0 1 2 3 4
10
20
30
40
50
60
70
80
90
Length of SCC
F?
me
as
ur
e(%
)
WSJ
Switchboard
Figure 2: F-measure for SCCs of different length
0 1 2 3 4
0
10
20
30
40
50
60
Length of SCCs
Pe
rc
en
ta
ge
(%
)
WSJ
Switchboard
Figure 3: Distribution of SCCs by length
that verb. SCCs of length 0 will be discounted be-
cause verbs that do not take any arguments or ad-
juncts have no dependents3 . In addition, this way
of evaluating the extraction of SCCs also matches
the practice in some NLP tasks such as semantic
role labeling (Xue and Palmer, 2004). For the task
of semantic role labeling, the total number of de-
pendents correctly retrieved from the parser?s out-
put affects the accuracy level of the task.
To do this, we calculated the number of depen-
dents shared by between each SCC proposed from
the parser?s output and its corresponding SCC pro-
posed from Penn Treebank. We based our cal-
culation on a modified version of Minimum Edit
Distance Algorithm. Our algorithm works by cre-
ating a shared-dependents matrix with one col-
umn for each constituent in the target sequence
(SCCs proposed from Penn Treebank) and one
3We are aware that subjects are typically also consid-
ered dependents, but we did not include subjects in our
experiments
shared-dependents[i.j] = MAX(
shared-dependents[i-1,j],
shared-dependents[i-1,j-1]+1 if target[i] = source[j],
shared-dependents[i-1,j-1] if target[i] != source[j],
shared-dependents[i,j-1])
Table 3: The algorithm for computing shared de-
pendents
INF #5 1 1 2 3
ADVP #4 1 1 2 2
PP-in #3 1 1 2 2
NP #2 1 1 1 1
NP #1 1 1 1 1
#0 #1 #2 #3 #4
NP S-that PP-in INF
Table 4: An example of computing the number of
shared dependents
row for each constituent in the source sequence
(SCCs proposed from the parser?s output). Each
cell shared-dependent[i,j] contains the number of
constituents shared between the first i constituents
of the target sequence and the first j constituents of
the source sequence. Each cell can then be com-
puted as a simple function of the three possible
paths through the matrix that arrive there. The al-
gorithm is illustrated in Table 3.
Table 4 shows an example of how the algo-
rithm works with NP-S-that-PP-in-INF as the tar-
get sequence and NP-NP-PP-in-ADVP-INF as the
source sequence. The algorithm returns 3 as the
number of dependents shared by two SCCs.
We compared the performance of Bikel?s parser
in retrieving dependents from written and spo-
ken language over all three models using De-
pendency Recall (DR) and Dependency Precision
(DP). These metrics are defined as follows:
DR = number of correct dependents from parser?s output
number of dependents from treebank parse
(4)
DP = number of correct dependents from parser?s output
number of dependents from parser?s output
(5)
Dependency F-measure = 2 ?DR ?DPDR+DP (6)
The results of Bikel?s parser in retrieving depen-
dents are summarized in Figure 4. Overall, the
parser achieves a better performance for WSJ over
all three models, just the opposite of what have
been observed for SCC extraction. Interestingly,
removing punctuation from both the training and
test data actually slightly improves the F-measure.
82
This holds true for both WSJ and Switchboard.
This Dependency F-measure differs in detail from
similar measures in (Xue and Palmer, 2004). For
present purposes all that matters is the relative
value for WSJ and Switchboard.
punc no?punc punc?no?punc
78
80
82
84
86
Models
F?
me
as
ur
e(%
)
WSJ
Switchboard
Figure 4: F-measure for extracting dependents
4 Conclusions and Future Work
4.1 Use of Parser?s Output
In this paper, we have shown that it is not nec-
essarily true that statistical parsers always per-
form worse when dealing with spoken language.
The conventional accuracy metrics for parsing
(LR/LP) should not be taken as the only metrics
in determining the feasibility of applying statisti-
cal parsers to spoken language. It is necessary to
consider what information we want to extract out
of parsers? output and make use of.
1. Extraction of SCFs from Corpora: This task
usually proceeds in two stages: (i) Use sta-
tistical parsers to generate SCCs. (ii) Ap-
ply some statistical tests such as the Bino-
mial Hypothesis Test (Brent, 1993) and log-
likelihood ratio score (Dunning, 1993) to
SCCs to filter out false SCCs on the basis of
their reliability and likelihood. Our experi-
ments show that the SCCs generated for spo-
ken language are as accurate as those gen-
erated for written language, which suggests
that it is feasible to apply the current technol-
ogy for automatically extracting SCFs from
corpora to spoken language.
2. Semantic Role Labeling: This task usually
operates on parsers? output and the number
of dependents of each verb that are correctly
retrieved by the parser clearly affects the ac-
curacy of the task. Our experiments show
that the parser achieves a much lower accu-
racy in retrieving dependents from the spoken
language than written language. This seems
to suggest that a lower accuracy is likely to
be achieved for a semantic role labeling task
performed on spoken language. We are not
aware that this has yet been tried.
4.2 Punctuation and Speech Transcription
Practice
Both our experiments and Roark?s experiments
show that parsing accuracy measured by LR/LP
experiences a sharper decrease for WSJ than
Switchboard after we removed punctuation from
training and test data. In spoken language, com-
mas are largely used to delimit disfluency ele-
ments. As noted in Engel et al (2002), statis-
tical parsers usually condition the probability of
a constituent on the types of its neighboring con-
stituents. The way that commas are used in speech
transcription seems to have the effect of increasing
the range of neighboring constituents, thus frag-
menting the data and making it less reliable. On
the other hand, in written texts, commas serve as
more reliable cues for parsers to identify phrasal
and clausal boundaries.
In addition, our experiment demonstrates that
punctuation does not help much with extraction of
SCCs from spoken language. Removing punctua-
tion from both the training and test data results in a
less than 0.3% decrease in SR/SP. Furthermore, re-
moving punctuation from both the training and test
data actually slightly improves the performance
of Bikel?s parser in retrieving dependents from
spoken language. All these results seem to sug-
gest that adding punctuation in speech transcrip-
tion is of little help to statistical parsers includ-
ing at least three state-of-the-art statistical parsers
(Collins, 1999; Charniak, 2000; Bikel, 2004). As a
result, there may be other good reasons why some-
one who wants to build a Switchboard-like corpus
should choose to provide punctuation, but there is
no need to do so simply in order to help parsers.
However, segmenting utterances into individual
units is necessary because statistical parsers re-
quire sentence boundaries to be clearly delimited.
Current statistical parsers are unable to handle an
input string consisting of two sentences. For ex-
ample, when presented with an input string as in
(1) and (2), if the two sentences are separated by a
period (1), Bikel?s parser wrongly treats the sec-
ond sentence as a sentential complement of the
83
main verb like in the first sentence. As a result, the
extractor generates an SCC NP-S for like, which is
incorrect. The parser returns the same parse after
we removed the period (2) and let the parser parse
it again.
(1) I like the long hair. It was back in high
school.
(2) I like the long hair It was back in high school.
Hence, while adding punctuation in transcribing
a Switchboard-like corpus is not of much help to
statistical parsers, segmenting utterances into in-
dividual units is crucial for statistical parsers. In
future work, we plan to develop a system capa-
ble of automatically segmenting speech utterances
into individual units.
5 Acknowledgments
This study was supported by NSF grant 0347799.
Our thanks go to Chris Brew, Eric Fosler-Lussier,
Mike White and three anonymous reviewers for
their valuable comments.
References
D. Bikel. 2004. Intricacies of Collin?s parsing models.
Computational Linguistics, 30(2):479?511.
M. Brent. 1993. From grammar to lexicon: Unsu-
pervised learning of lexical syntax. Computational
Linguistics, 19(3):243?262.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 2000 Conference of
the North American Chapter of the Association for
Computation Linguistics, pages 132?139.
M. Collins. 1999. Head-driven statistical models for
natural language parsing. Ph.D. thesis, University
of Pennsylvania.
T. Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
D. Engel, E. Charniak, and M. Johnson. 2002. Parsing
and disfluency placement. In Proceedings of 2002
Conference on Empirical Methods of Natural Lan-
guage Processing, pages 49?54.
J. Godefrey, E. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for
research and development. In Proceedings of
ICASSP-92, pages 517?520.
M. Lapata and C. Brew. 2004. Verb class disambigua-
tion using informative priors. Computational Lin-
guistics, 30(1):45?73.
M. Marcus, G. Kim, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English:
the Penn Treebank. Computational Linguistics,
19(2):313?330.
P. Merlo and S. Stevenson. 2001. Automatic
verb classification based on statistical distribution
of argument structure. Computational Linguistics,
27(3):373?408.
P. Merlo, E. Joanis, and J. Henderson. 2005. Unsuper-
vised verb class disambiguation based on diathesis
alternations. manuscripts.
V. Punyakanok, D. Roth, and W. Yih. 2005. The neces-
sity of syntactic parsing for semantic role labeling.
In Proceedings of the 2nd Midwest Computational
Linguistics Colloquium, pages 15?22.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of the Con-
ference on Empirical Methods of Natural Language
Processing, pages 133?142.
B. Roark. 2001. Robust Probabilistic Predictive
Processing: Motivation, Models, and Applications.
Ph.D. thesis, Brown University.
D. Roland and D. Jurafsky. 1998. How verb sub-
categorization frequency is affected by the corpus
choice. In Proceedings of the 17th International
Conference on Computational Linguistics, pages
1122?1128.
S. Schulte im Walde. 2000. Clustering verbs semanti-
cally according to alternation behavior. In Proceed-
ings of the 18th International Conference on Com-
putational Linguistics, pages 747?753.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In Proceedings of 2004 Con-
ference on Empirical Methods in Natural Language
Processing, pages 88?94.
84
Proceedings of ACL-08: HLT, pages 434?442,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Which Are the Best Features for Automatic Verb Classification
Jianguo Li
Department of Linguistics
The Ohio State University
Columbus Ohio, USA
jianguo@ling.ohio-state.edu
Chris Brew
Department of Linguistics
The Ohio State University
Columbus Ohio, USA
cbrew@ling.ohio-state.edu
Abstract
In this work, we develop and evaluate a wide
range of feature spaces for deriving Levin-
style verb classifications (Levin, 1993). We
perform the classification experiments using
Bayesian Multinomial Regression (an effi-
cient log-linear modeling framework which
we found to outperform SVMs for this task)
with the proposed feature spaces. Our exper-
iments suggest that subcategorization frames
are not the most effective features for auto-
matic verb classification. A mixture of syntac-
tic information and lexical information works
best for this task.
1 Introduction
Much research in lexical acquisition of verbs has
concentrated on the relation between verbs and their
argument frames. Many scholars hypothesize that
the behavior of a verb, particularly with respect to
the expression of arguments and the assignment of
semantic roles is to a large extent driven by deep
semantic regularities (Dowty, 1991; Green, 1974;
Goldberg, 1995; Levin, 1993). Thus measurements
of verb frame patterns can perhaps be used to probe
for linguistically relevant aspects of verb meanings.
The correspondence between meaning regularities
and syntax has been extensively studied in Levin
(1993) (hereafter Levin). Levin?s verb classes are
based on the ability of a verb to occur or not occur
in pairs of syntactic frames that are in some sense
meaning preserving (diathesis alternation). The fo-
cus is on verbs for which distribution of syntactic
frames is a useful indicator of class membership,
and, correspondingly, on classes which are relevant
for such verbs. By using Levin?s classification, we
obtain a window on some (but not all) of the poten-
tially useful semantic properties of verbs.
Levin?s verb classification, like others, helps re-
duce redundancy in verb descriptions and enables
generalizations across semantically similar verbs
with respect to their usage. When the information
about a verb type is not available or sufficient for us
to draw firm conclusions about its usage, the infor-
mation about the class to which the verb type be-
longs can compensate for it, addressing the perva-
sive problem of data sparsity in a wide range of NLP
tasks, such as automatic extraction of subcategoriza-
tion frames (Korhonen, 2002), semantic role label-
ing (Swier and Stevenson, 2004; Gildea and Juraf-
sky, 2002), natural language generation for machine
translation (Habash et al, 2003), and deriving pre-
dominant verb senses from unlabeled data (Lapata
and Brew, 2004).
Although there exist several manually-created
verb lexicons or ontologies, including Levin?s verb
taxonomy, VerbNet, and FrameNet, automatic verb
classification (AVC) is still necessary for extend-
ing existing lexicons (Korhonen and Briscoe, 2004),
building and tuning lexical information specific to
different domains (Korhonen et al, 2006), and boot-
strapping verb lexicons for new languages (Tsang
et al, 2002).
AVC helps avoid the expensive hand-coding of
such information, but appropriate features must be
identified and demonstrated to be effective. In this
work, our primary goal is not necessarily to obtain
the optimal classification, but rather to investigate
434
the linguistic conditions which are crucial for lex-
ical semantic classification of verbs. We develop
feature sets that combine syntactic and lexical infor-
mation, which are in principle useful for any Levin-
style verb classification. We test the general ap-
plicability and scalability of each feature set to the
distinctions among 48 verb classes involving 1,300
verbs, which is, to our knowledge, the largest in-
vestigation on English verb classification by far. To
preview our results, a feature set that combines both
syntactic information and lexical information works
much better than either of them used alone. In ad-
dition, mixed feature sets also show potential for
scaling well when dealing with larger number of
verbs and verb classes. In contrast, subcategoriza-
tion frames, at least on their own, are largely inef-
fective for AVC, despite their evident effectiveness
in supporting Levin?s initial intuitions.
2 Related Work
Earlier work on verb classification has generally
adopted one of the two approaches for devising sta-
tistical, corpus-based features.
Subcategorization frame (SCF): Subcategoriza-
tion frames are obviously relevant to alternation
behaviors. It is therefore unsurprising that much
work on verb classification has adopted them as fea-
tures (Schulte im Walde, 2000; Brew and Schulte im
Walde, 2002; Korhonen et al, 2003). However, rely-
ing solely on subcategorization frames also leads to
the loss of semantic distinctions. Consider the frame
NP-V-PPwith. The semantic interpretation of this
frame depends to a large extent on the NP argument
selected by the preposition with. In (1), the same
surface form NP-V-PPwith corresponds to three dif-
ferent underlying meanings. However, such seman-
tic distinctions are totally lost if lexical information
is disregarded.
(1) a. I ate with a fork. [INSTRUMENT]
b. I left with a friend. [ACCOMPANIMENT]
c. I sang with confidence. [MANNER]
This deficiency of unlexicalized subcategoriza-
tion frames leads researchers to make attempts to
incorporate lexical information into the feature rep-
resentation. One possible improvement over subcat-
egorization frames is to enrich them with lexical in-
formation. Lexicalized frames are usually obtained
by augmenting each syntactic slot with its head noun
(2).
(2) a. NP(I)-V-PP(with:fork)
b. NP(I)-V-PP(with:friend)
c. NP(I)-V-PP(with:confidence)
With the potentially improved discriminatory
power also comes increased exposure to sparse data
problems. Trying to overcome the problem of data
sparsity, Schulte im Walde (2000) explores the ad-
ditional use of selectional preference features by
augmenting each syntactic slot with the concept to
which its head noun belongs in an ontology (e.g.
WordNet). Although the problem of data sparsity
is alleviated to certain extent (3), these features
do not generally improve classification performance
(Schulte im Walde, 2000; Joanis, 2002).
(3) a. NP(PERSON)-V-PP(with:ARTIFACT)
b. NP(PERSON)-V-PP(with:PERSON)
c. NP(PERSON)-V-PP(with:FEELING)
JOANIS07: Incorporating lexical information di-
rectly into subcategorization frames has proved in-
adequate for AVC. Other methods for combining
syntactic information with lexical information have
also been attempted (Merlo and Stevenson, 2001;
Joanis et al, 2007). These studies use a small col-
lection of features that require some degree of expert
linguistic analysis to devise. The deeper linguistic
analysis allows their feature set to cover a variety of
indicators of verb semantics, beyond that of frame
information. Joanis et al (2007) reports an experi-
ment that involves 15 Levin verb classes. They de-
fine a general feature space that is supposed to be
applicable to all Levin classes. The features they
use fall into four different groups: syntactic slots,
slot overlaps, tense, voice and aspect, and animacy
of NPs.
? Syntactic slots: They encode the frequency of
the syntactic positions (e.g. SUBJECT, OB-
JECT, PPat). They are considered approxima-
tion to subcategorization frames.
? Slot overlaps: They are supposed to capture
the properties of alternation by identifying if
a given noun can occur in different syntactic
positions relative to a particular verb. For in-
stance, in the alternation The ice melted and
435
The sun melted the ice, ice occurs in the sub-
ject position in the first sentence but in the ob-
ject position in the second sentence. An over-
lap feature records that there is a subject-object
alternation for melt.
? Tense, voice and aspect: Verb meaning and al-
ternations also interact in interesting ways with
tense, voice, and aspect. For example, mid-
dle construction is usually used in present tense
(e.g. The bread cuts easily).
? Animacy of NPs: The animacy of the seman-
tic role corresponding to the head noun in each
syntactic slot can also distinguish classes of
verbs.
Joanis et al (2007) demonstrates that the gen-
eral feature space they devise achieves a rate of
error reduction ranging from 48% to 88% over a
chance baseline accuracy, across classification tasks
of varying difficulty. However, they also show that
their general feature space does not generally im-
prove the classification accuracy over subcategoriza-
tion frames (see table 1).
Experimental Task All Features SCF
Average 2-way 83.2 80.4
Average 3-way 69.6 69.4
Average (? 6)-way 61.1 62.8
Table 1: Results from Joanis et al (2007) (%)
3 Integration of Syntactic and Lexical
Information
In this study, we explore a wider range of features
for AVC, focusing particularly on various ways to
mix syntactic with lexical information.
Dependency relation (DR): Our way to over-
come data sparsity is to break lexicalized frames into
lexicalized slots (a.k.a. dependency relations). De-
pendency relations contain both syntactic and lexical
information (4).
(4) a. SUBJ(I), PP(with:fork)
b. SUBJ(I), PP(with:friend)
c. SUBJ(I), PP(with:confidence)
However, augmenting PP with nouns selected by
the preposition (e.g. PP(with:fork)) still gives rise
to data sparsity. We therefore decide to break it
into two individual dependency relations: PP(with),
PP-fork. Although dependency relations have been
widely used in automatic acquisition of lexical infor-
mation, such as detection of polysemy (Lin, 1998)
and WSD (McCarthy et al, 2004), their utility in
AVC still remains untested.
Co-occurrence (CO): CO features mostly convey
lexical information only and are generally consid-
ered not particularly sensitive to argument structures
(Rohde et al, 2004). Nevertheless, it is worthwhile
testing whether the meaning components that are
brought out by syntactic alternations are also cor-
related to the neighboring words. In other words,
Levin verbs may be distinguished on the dimension
of neighboring words, in addition to argument struc-
tures. A test on this claim can help answer the ques-
tion of whether verbs in the same Levin class also
tend to share their neighboring words.
Adapted co-occurrence (ACO): Conventional
CO features generally adopt a stop list to filter out
function words. However, some of the functions
words, prepositions in particular, are known to carry
great amount of syntactic information that is related
to lexical meanings of verbs (Schulte im Walde,
2003; Brew and Schulte im Walde, 2002; Joanis
et al, 2007). In addition, whereas most verbs tend to
put a strong selectional preference on their nominal
arguments, they do not care much about the iden-
tity of the verbs in their verbal arguments. Based on
these observations, we propose to adapt the conven-
tional CO features by (1) keeping all prepositions
(2) replacing all verbs in the neighboring contexts of
each target verb with their part-of-speech tags. ACO
features integrate at least some degree of syntactic
information into the feature space.
SCF+CO: Another way to mix syntactic informa-
tion with lexical information is to use subcategoriza-
tion frames and co-occurrences together in hope that
they are complementary to each other, and therefore
yield better results for AVC.
4 Experiment Setup
4.1 Corpus
To collect each type of features, we use the Giga-
word Corpus, which consists of samples of recent
newswire text data collected from four distinct in-
436
ternational sources of English newswire.
4.2 Feature Extraction
We evaluate six different feature sets for their effec-
tiveness in AVC: SCF, DR, CO, ACO, SCF+CO,
and JOANIS07. SCF contains mainly syntactic in-
formation, whereas CO lexical information. The
other four feature sets include both syntactic and lex-
ical information.
SCF and DR: These more linguistically informed
features are constructed based on the grammatical
relations generated by the C&C CCG parser (Clark
and Curran, 2007). Take He broke the door with a
hammer as an example. The grammatical relations
generated are given in table 2.
he broke the door with a hammer.
(det door 3 the 2)
(dobj broke 1 door 3)
(det hammer 6 a 5)
(dobj with 4 hammer 6)
(iobj broke 1 with 4)
(ncsubj broke 1 He 0 )
Table 2: grammatical relations generated by the parser
We first build a lexicalized frame for the verb
break: NP1(he)-V-NP2(door)-PP(with:hammer).
This is done by matching each grammatical label
onto one of the traditional syntactic constituents.
The set of syntactic constituents we use is summa-
rized in table 3.
constituent remark
NP1 subject of the verb
NP2 object of the verb
NP3 indirect object of the verb
PPp prepositional phrase
TO infinitival clause
GER gerund
THAT sentential complement headed by that
WH sentential complement headed by a wh-word
ADJP adjective phrase
ADVP adverb phrase
Table 3: Syntactic constituents used for building SCFs
Based on the lexicalized frame, we construct
an SCF NP1-NP2-PPwith for break. The set of
DRs generated for break is [SUBJ(he), OBJ(door),
PP(with), PP-hammer].
CO: These features are collected using a flat 4-
word window, meaning that the 4 words to the
left/right of each target verb are considered poten-
tial CO features. However, we eliminate any CO
features that are in a stopword list, which con-
sists of about 200 closed class words including
mainly prepositions, determiners, complementizers
and punctuation. We also lemmatize each word us-
ing the English lemmatizer as described in Minnen
et al (2000), and use lemmas as features instead of
words.
ACO: As mentioned before, we adapt the conven-
tional CO features by (1) keeping all prepositions
(2) replacing all verbs in the neighboring contexts of
each target verb with their part-of-speech tags. (3)
keeping words in the left window only if they are
tagged as a nominal.
SCF+CO: We combine the SCF and CO features.
JOANIS07: We use the feature set proposed in
Joanis et al (2007), which consists of 224 features.
We extract features on the basis of the output gener-
ated by the C&C CCG parser.
4.3 Verb Classes
Our experiments involve two separate sets of verb
classes:
Joanis15: Joanis et al (2007) manually selects
pairs, or triples of classes to represent a range of
distinctions that exist among the 15 classes they in-
vestigate. For example, some of the pairs/triples are
syntactically dissimilar, while others show little syn-
tactic distinction across the classes.
Levin48: Earlier work has focused only on a
small set of verbs or a small number of verb classes.
For example, Schulte im Walde (2000) uses 153
verbs in 30 classes, and Joanis et al (2007) takes
on 835 verbs and 15 verb classes. Since one of our
primary goals is to identify a general feature space
that is not specific to any class distinctions, it is of
great importance to understand how the classifica-
tion accuracy is affected when attempting to classify
more verbs into a larger number of classes. In our
automatic verb classification, we aim for a larger
scale experiment. We select our experimental verb
classes and verbs as follows: We start with all Levin
197 verb classes. We first remove all verbs that be-
long to at least two Levin classes. Next, we remove
any verb that does not occur at least 100 times in
the English Gigaword Corpus. All classes that are
left with at least 10 verbs are chosen for our experi-
437
ment. This process yields 48 classes involving about
1,300 verbs. In our automatic verb classification ex-
periment, we test the applicability of each feature
set to distinctions among up to 48 classes 1. To our
knowledge, this is, by far, the largest investigation
on English verb classification.
5 Machine Learning Method
5.1 Preprocessing Data
We represent the semantic space for verbs as a ma-
trix of frequencies, where each row corresponds to
a Levin verb and each column represents a given
feature. We construct a semantic space with each
feature set. Except for JONAIS07 which only con-
tains 224 features, all the other feature sets lead to a
very high-dimensional space. For instance, the se-
mantic space with CO features contains over one
million columns, which is too huge and cumber-
some. One way to avoid these high-dimensional
spaces is to assume that most of the features are irrel-
evant, an assumption adopted by many of the previ-
ous studies working with high-dimensional seman-
tic spaces (Burgess and Lund, 1997; Pado and La-
pata, 2007; Rohde et al, 2004). Burgess and Lund
(1997) suggests that the semantic space can be re-
duced by keeping only the k columns (features) with
the highest variance. However, Rohde et al (2004)
have found it is simpler and more effective to dis-
card columns on the basis of feature frequency, with
little degradation in performance, and often some
improvement. Columns representing low-frequency
features tend to be noisier because they only involve
few examples. We therefore apply a simple fre-
quency cutoff for feature selection. We only use fea-
tures that occur with a frequency over some thresh-
old in our data.
In order to reduce undue influence of outlier fea-
tures, we employ the four normalization strategies in
table 4, which help reduce the range of extreme val-
ues while having little effect on others (Rohde et al,
2004). The raw frequency (wv,f ) of a verb v oc-
curring with a feature f is replaced with the normal-
1In our experiment, we only use monosemous verbs from
these 48 verb classes. Due to the space limit, we do not list the
48 verb classes. The size of the most classes falls in the range
between 10 to 30, with a couple of classes having a size over
100.
ized value (w?v,f ), according to each normalization
method. Our experiments show that using correla-
tion for normalization generally renders the best re-
sults. The results reported below are obtained from
using correlation for normalization.
w?v,f =
row
wv,fP
j wv,j
column
wv,fP
i wi,f
length
wv,f
P
j w
2
v,j
1/2
correlation
Twv,f?
P
j wv,j
P
i wi,f
(
P
j wv,j(T?
P
j wv,j)
P
i wi,f (T?
P
i wi,f ))
1/2
T =
P
i
P
j wi,j
Table 4: Normalization techniques
To preprocess data, we first apply a frequency cut-
off to our data set, and then normalize it using the
correlation method. To find the optimal threshold
for frequency cut, we consider each value between 0
and 10,000 at an interval of 500. In our experiments,
results on training data show that performance de-
clines more noticeably when the threshold is lower
than 500 or higher than 10,000. For each task and
feature set, we select the frequency cut that offers
the best accuracy on the preprocessed training set
according to k-fold stratified cross validation 2.
5.2 Classifier
For all of our experiments, we use the software that
implements the Bayesian multinomial logistic re-
gression (a.k.a BMR). The software performs the so-
called 1-of-k classification (Madigan et al, 2005).
BMR is similar to Maximum Entropy. It has been
shown to be very efficient with handling large num-
bers of features and extremely sparsely populated
matrices, which characterize the data we have for
AVC 3. To begin, let x = [x1, ..., xj , ..., xd]T be a
vector of feature values characterizing a verb to be
classified. We encode the fact that a verb belongs
to a class k ? 1, ...,K by a K-dimensional 0/1 val-
ued vector y = (y1, ..., yK)T , where yk = 1 and all
other coordinates are 0. Multinomial logistic regres-
210-fold for Joanis15 and 9-fold for Levin48. We use a bal-
anced training set, which contains 20 verbs from each class in
Joanis15, but only 9 verbs from each class in Levin48.
3We also tried Chang and Lin (2001)?s LIBSVM library for
Support Vector Machines (SVMs), however, BMR generally
outperforms SVMs.
438
sion is a conditional probability model of the form,
parameterized by the matrix ? = [?1, ..., ?K ]. Each
column of ? is a parameter vector corresponding to
one of the classes: ?k = [?k1, ..., ?kd]T .
P (yk = 1|?k, x) = exp(?
T
k x)/
X
ki
exp(?Tkix)
6 Results and Discussion
6.1 Evaluation Metrics
Following Joanis et al (2007), we adopt a single
evaluation measure - macro-averaged recall - for all
of our classification tasks. As discussed below, since
we always use balanced training sets for each indi-
vidual task, it makes sense for our accuracy metric to
give equal weight to each class. Macro-averaged re-
call treats each verb class equally, so that the size of
a class does not affect macro-averaged recall. It usu-
ally gives a better sense of the quality of classifica-
tion across all classes. To calculate macro-averaged
recall, the recall value for each individual verb class
has to be computed first.
recall =
no. of test verbs in class c correctly labeled
no. of test verbs in class c
With a recall value computed for each verb class,
the macro-averaged recall can be defined by:
macro-averaged recall =
1
|C|
X
c?C
recall for class c
C : a set of verb classes
c : an individual verb class
|C| : the number of verb classes
6.2 Joanis15
With those manually-selected 15 classes, Joanis
et al (2007) conducts 11 classification tasks includ-
ing six 2-way classifications, two 3-way classifica-
tions, one 6-way classification, one 8-way classifi-
cation, and one 14-way classification. In our exper-
iments, we replicate these 11 classification tasks us-
ing the proposed six different feature sets. For each
classification task in this task set, we randomly se-
lect 20 verbs from each class as the training set. We
repeat this process 10 times for each task. The re-
sults reported for each task is obtained by averaging
the results of the 10 trials. Note that for each trial,
each feature set is trained and tested on the same
training/test split.
The results for the 11 classification tasks are sum-
marized in table 5. We provide a chance baseline
and the accuracy reported in Joanis et al (2007) 4 for
comparison of our results. A few points are worth
noting:
? Although widely used for AVC, SCF, at least
when used alone, is not the most effective fea-
ture set. Our experiments show that the per-
formance achieved by using SCF is generally
worse than using the feature sets that mix syn-
tactic and lexical information. As a matter of
fact, it even loses to the simplest feature setCO
on 4 tasks, including the 14-way task.
? The two feature sets (DR, SCF+CO) we pro-
pose that combine syntactic and lexical infor-
mation generally perform better than those fea-
ture sets (SCF, CO) that only include syntactic
or lexical information. Although there is not a
clear winner, DR and SCF+CO generally out-
perform other feature sets, indicating that they
are effective ways for combining syntactic and
lexical information. In particular, these two
feature sets perform comparatively well on the
tasks that involve more classes (e.g. 14-way),
exhibiting the tendency to scale well with larger
number of verb classes and verbs. Another fea-
ture set that combines syntactic and lexical in-
formation, ACO, which keeps function words
in the feature space to preserve syntactic infor-
mation, outperforms the conventional CO on
the majority of tasks. All these observations
suggest that how to mix syntactic and lexical
information is one of keys to an improved verb
classification.
? Although JOANIS07 also combines syntactic
and lexical information, its performance is not
comparable to that of other feature sets that mix
syntactic and lexical information. In fact, SCF
4Joanis et al (2007) is different from our experiments in that
they use a chunker for feature extraction and the Support Vector
Machine for classification.
439
Experimental Task
Random As Reported in Feature Set
Baseline Joanis et al (2007) SCF DR CO ACO SCF+CO JOANIS07
1) Benefactive/Recipient 50 86.4 88.6 88.4 88.2 89.1 90.7 88.9
2) Admire/Amuse 50 93.9 96.7 97.5 92.1 90.5 96.4 96.6
3) Run/Sound 50 86.8 85.4 89.6 91.8 90.2 90.5 87.1
4) Light/Sound 50 75.0 74.8 90.8 86.9 89.7 88.8 82.1
5) Cheat/Steal 50 76.5 77.6 80.6 72.1 75.5 77.8 76.4
6) Wipe/Steal 50 80.4 84.8 80.6 79.0 79.4 84.4 83.9
7) Spray/Fill/Putting 33.3 65.6 73.0 72.8 59.6 66.6 73.8 69.6
8) Run/State Change/Object drop 33.3 74.2 74.8 77.2 76.9 77.6 80.5 75.5
9) Cheat/Steal/Wipe/Spray/Fill/Putting 16.7 64.3 64.9 65.1 54.8 59.1 65.0 64.3
10) 9)/Run/Sound 12.5 61.7 62.3 65.8 55.7 60.8 66.9 63.1
11) 14-way (all except Benefactive) 7.1 58.4 56.4 65.7 57.5 59.6 66.3 57.2
Table 5: Experimental results for Joanis15 (%)
and JOANIS07 yield similar accuracy in our
experiments, which agrees with the findings in
Joanis et al (2007) (compare table 1 and 5).
6.3 Levin48
Recall that one of our primary goals is to identify
the feature set that is generally applicable and scales
well while we attempt to classify more verbs into a
larger number of classes. If we could exhaust all the
possible n-way (2 ? n ? 48) classification tasks
with the 48 Levin classes we will investigate, it will
allow us to draw a firmer conclusion about the gen-
eral applicability and scalability of a particular fea-
ture set. However, the number of classification tasks
grows really huge when n takes on certain value (e.g.
n = 20). For our experiments, we set n to be 2, 5,
10, 20, 30, 40, or 48. For the 2-way classification,
we perform all the possible 1,028 tasks. For the 48-
way classification, there is only one possible task.
We randomly select 100 n-way tasks each for n =
5, 10, 20, 30, 40. We believe that this series of tasks
will give us a reasonably good idea of whether a par-
ticular feature set is generally applicable and scales
well.
The smallest classes in Levin48 have only 10
verbs. We therefore reduce the number of training
verbs to 9 for each class. For each n = 2, 5, 10, 20,
30, 40, 48, we will perform certain number of n-way
classification tasks. For each n-way task, we ran-
domly select 9 verbs from each class as training data,
and repeat this process 10 times. The accuracy for
each n-way task is then computed by averaging the
results from these 10 trials. The accuracy reported
for the overall n-way classification for each selected
n, is obtained by averaging the results from each in-
dividual n-way task for that particular n. Again, for
each trial, each feature set is trained and tested on
the same training/test split.
The results for Levin48 are presented in table 6,
which clearly reveals the general applicability and
scalability of each feature set.
? Results from Levin48 reconfirm our finding
that SCF is not the most effective feature set for
AVC. Although it achieves the highest accuracy
on the 2-way classification, its accuracy drops
drastically as n gets bigger, indicating that SCF
does not scale as well as other feature sets when
dealing with larger number of verb classes. On
the other hand, the co-occurrence feature (CO),
which is believed to convey only lexical infor-
mation, outperforms SCF on every n-way clas-
sification when n ? 10, suggesting that verbs
in the same Levin classes tend to share their
neighboring words.
? The three feature sets we propose that com-
bine syntactic and lexical information generally
scale well. Again, DR and SCF+CO gener-
ally outperform all other feature sets on all n-
way classifications, except the 2-way classifica-
tion. In addition, ACO achieves a better perfor-
mance on every n-way classification than CO.
Although SCF and CO are not very effective
when used individually, they tend to yield the
best performance when combined together.
? Again, JOANIS07 does not match the perfor-
mance of other feature sets that combine both
syntactic and lexical information, but yields
similar accuracy as SCF.
440
Experimental Task No of Tasks Random Baseline
Feature Set
SCF DR CO ACO SCF+CO JOANIS07
2-way 1,028 50 84.0 83.4 77.8 80.9 82.9 82.4
5-way 100 20 71.9 76.4 70.4 73.0 77.3 72.2
10-way 100 10 65.8 73.7 68.8 71.2 72.8 65.9
20-way 100 5 51.4 65.1 58.8 60.1 65.8 50.7
30-way 100 3.3 46.7 56.9 48.6 51.8 57.8 47.1
40-way 100 2.5 43.6 54.8 47.3 49.9 55.1 44.2
48-way 1 2.2 39.1 51.6 42.4 46.8 52.8 38.9
Table 6: Experimental results for Levin48 (%)
6.4 Further Discussion
Previous studies on AVC have focused on using
SCFs. Our experiments reveal that SCFs, at least
when used alone, compare poorly to the feature sets
that mix syntactic and lexical information. One ex-
planation for the poor performance could be that we
use all the frames generated by the CCG parser in
our experiment. A better way of doing this would
be to use some expert-selected SCF set. Levin clas-
sifies English verbs on the basis of 78 SCFs, which
should, at least in principle, be good at separating
verb classes. To see if Levin-selected SCFs are
more effective for AVC, we match each SCF gen-
erated by the C&C CCG parser (CCG-SCF) to one
of 78 Levin-defined SCFs, and refer to the resulting
SCF set as unfiltered-Levin-SCF. Following stud-
ies on automatic SCF extraction (Brent, 1993), we
apply a statistical test (Binomial Hypothesis Test) to
the unfiltered-Levin-SCF to filter out noisy SCFs,
and denote the resulting SCF set as filtered-Levin-
SCF. We then perform the 48-way task (one of
Levin48) with these two different SCF sets. Recall
that using CCG-SCF gives us a macro-averaged re-
call of 39.1% on the 48-way task. Our experiments
show that using unfiltered-Levin-SCF and filtered-
Levin-SCF raises the accuracy to 39.7% and 40.3%
respectively. Although a little performance gain has
been obtained by using expert-defined SCFs, the ac-
curacy level is still far below that achieved by using
a feature set that combines syntactic and semantic
information. In fact, even the simple co-occurrence
feature (CO) yields a better performance (42.4%)
than these Levin-selected SCF sets.
7 Conclusion and Future Work
We have performed a wide range of experiments
to identify which features are most informative in
AVC. Our conclusion is that both syntactic and lex-
ical information are useful for verb classification.
Although neither SCF nor CO performs well on its
own, a combination of them proves to be the most in-
formative feature for this task. Other ways of mixing
syntactic and lexical information, such as DR, and
ACO, work relatively well too. What makes these
mixed feature sets even more appealing is that they
tend to scale well in comparison to SCF and CO. In
addition, these feature sets are devised on a general
level without relying on any knowledge about spe-
cific classes, thus potentially applicable to a wider
range of class distinctions. Assuming that Levin?s
analysis is generally applicable across languages in
terms of the linking of semantic arguments to their
syntactic expressions, these mixed feature sets are
potentially useful for building verb classifications
for other languages.
For our future work, we aim to test whether an
automatically created verb classification can be ben-
eficial to other NLP tasks. One potential applica-
tion of our verb classification is parsing. Lexicalized
PCFGs (where head words annotate phrasal nodes)
have proved a key tool for high performance PCFG
parsing, however its performance is hampered by
the sparse lexical dependency exhibited in the Penn
Treebank. Our experiments on verb classification
have offered a class-based approach to alleviate data
sparsity problem in parsing. It is our goal to test
whether this class-based approach will lead to an im-
proved parsing performance.
8 Acknowledgments
This study was supported by NSF grant 0347799.
We are grateful to Eric Fosler-Lussier, Detmar
Meurers, Mike White and Kirk Baker for their valu-
able comments.
441
References
Brent, M. (1993). From grammar to lexicon: Unsuper-
vised learning of lexical syntax. Computational Lin-
guistics, 19(3):243?262.
Brew, C. and Schulte im Walde, S. (2002). Spectral clus-
tering for German verbs. In Proccedings of the 2002
Conference on EMNLP, pages 117?124.
Burgess, C. and Lund, K. (1997). Modelling parsing
constraints with high-dimentional context space. Lan-
guage and Cognitive Processes, 12(3):177?210.
Chang, C. and Lin, C. (2001). LIBSVM:
A library for support vector machines.
http://www.csie.ntu.edu.tw. cjlin/libsvm.
Clark, S. and Curran, J. (2007). Formalism-independent
parser evaluation with CCG and Depbank. In Proceed-
ings of the 45th Annual Meeting of ACL, pages 248?
255.
Dowty, D. (1991). Thematic proto-roles and argument
selection. Language, 67:547?619.
Gildea, D. and Jurafsky, D. (2002). Automatic labeling of
semantic role. Computational Linguistics, 28(3):245?
288.
Goldberg, A. (1995). Constructions. University of
Chicago Press, Chicago, 1st edition.
Green, G. (1974). Semantics and Syntactic Regularity.
Indiana University Press, Bloomington.
Habash, N., Dorr, B., and Traum, D. (2003). Hybrid natu-
ral language generation from lexical conceptual struc-
tures. Machine Translation, 18(2):81?128.
Joanis, E. (2002). Automatic verb classification using a
general feature space. Master?s thesis, University of
Toronto.
Joanis, E., Stevenson, S., and James, D. (2007). A general
feature space for automatic verb classification. Natural
Language Engineering, 1:1?31.
Korhonen, A. (2002). Subcategorization Acquisition.
PhD thesis, Cambridge University.
Korhonen, A. and Briscoe, T. (2004). Extended lexical-
semantic classification of english verbs. In Proceed-
ings of the 2004 HLT/NAACL Workshop on Computa-
tional Lexical Semantics, pages 38?45, Boston, MA.
Korhonen, A., Krymolowski, Y., and Collier, N. (2006).
Automatic classification of verbs in biomedical texts.
In Proceedings of the 21st International Conference
on COLING and 44th Annual Meeting of ACL, pages
345?352, Sydney, Australia.
Korhonen, A., Krymolowski, Y., and Marx, Z. (2003).
Clustering polysemic subcategorization frame distri-
butions semantically. In Proceedings of the 41st An-
nual Meeting of ACL, pages 48?55, Sapparo, Japan.
Lapata, M. and Brew, C. (2004). Verb class disambigua-
tion using informative priors. Computational Linguis-
tics, 30(1):45?73.
Levin, B. (1993). English Verb Classes and Alternations:
A Preliminary Investigation. University of Chicago
Press, Chicago, 1st edition.
Lin, D. (1998). Automatic retrieval and clustering of sim-
ilar words. In Proceedings of the 17th Internation Con-
ference on COLING and 36th Annual Meeting of ACL.
Madigan, D., Genkin, A., Lewis, D., and Fradkin, D.
(2005). Bayesian Multinomial Logistic Regression for
Author Identification. DIMACS Technical Report.
McCarthy, D., Koeling, R., Weeds, J., and Carroll, J.
(2004). Finding predominant senses in untagged text.
In Proceedings of the 42nd Annual Meeting of ACL,
pages 280?287.
Merlo, P. and Stevenson, S. (2001). Automatic verb clas-
sification based on statistical distribution of argument
structure. Computational Linguistics, 27(3):373?408.
Minnen, G., Carroll, J., and Pearce, D. (2000). Applied
morphological processing of English. Natural Lan-
guage Engineering, 7(3):207?223.
Pado, S. and Lapata, M. (2007). Dependency-based con-
struction of semantic space models. Computional Lin-
guistics, 33(2):161?199.
Rohde, D., Gonnerman, L., and Plaut, D. (2004). An im-
proved method for deriving word meaning from lexical
co-occurrence. http://dlt4.mit.edu/ dr/COALS.
Schulte im Walde, S. (2000). Clustering verbs seman-
tically according to alternation behavior. In Proceed-
ings of the 18th International Conference on COLING,
pages 747?753.
Schulte im Walde, S. (2003). Experiments on the choice
of features for learning verb classes. In Proceedings of
the 10th Conference of EACL, pages 315?322.
Swier, R. and Stevenson, S. (2004). Unsupervised se-
mantic role labelling. In Proceedings of the 2004 Con-
ference on EMNLP, pages 95?102.
Tsang, V., Stevenson, S., and Merlo, P. (2002). Crosslin-
guistic transfer in automatic verb classification. In
Proceedings of the 19th International Conference on
COLING, pages 1023?1029, Taiwan, China.
442
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 204?205,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Robust Extraction of Subcategorization Data from Spoken Language 
 
Jianguo Li & Chris Brew Eric Fosler-Lussier 
Department of Linguistics Department of Computer Science & Engineering 
The Ohio State University, USA The Ohio State University, USA 
{jianguo|cbrew}@ling.ohio-state.edu fosler@cse.ohio-state.edu 
 
 
 
 
1 Introduction 
Subcategorization data has been crucial for various 
NLP tasks. Current method for automatic SCF ac-
quisition usually proceeds in two steps: first, gen-
erate all SCF cues from a corpus using a parser, 
and then filter out spurious SCF cues with statisti-
cal tests. Previous studies on SCF acquisition have 
worked mainly with written texts; spoken corpora 
have received little attention. Transcripts of spoken 
language pose two challenges absent in written 
texts: uncertainty about utterance segmentation and 
disfluency. 
     Roland & Jurafsky (1998) suggest that there are 
substantial subcategorization differences between 
spoken and written corpora. For example, spoken 
corpora tend to have fewer passive sentences but 
many more zero-anaphora structures than written 
corpora. In light of such subcategorization differ-
ences, we believe that an SCF set built from spo-
ken language may, if of acceptable quality, be of 
particular value to NLP tasks involving syntactic 
analysis of spoken language.  
2 SCF Acquisition System  
Following the design proposed by Briscoe and 
Carroll (1997), we built an SCF acquisition system 
consisting of the following four components: 
Charniak?s parser (Charniak, 2000); an SCF ex-
tractor; a lemmatizer; and an SCF evaluator. The 
first three components are responsible for generat-
ing SCF cues from the training corpora and the last 
component, consisting of the Binomial Hypothesis 
Test (Brent, 1993) and a back-off algorithm 
(Sarkar & Zeman, 2000), is used to filter SCF cues 
on the basis of their reliability and likelihood.  
We evaluated our system on a million word 
written corpus and a comparable spoken corpus 
from BNC.  For type precision and recall, we used 
14 verbs selected by Briscoe & Carroll (1997) and 
evaluated our results against SCF entries in 
COMLEX (Grishman et al, 1994). We also calcu-
lated token recall and the results are summarized in 
the following table. 
Corpus Written Spoken 
type precision 93.1% 91.2% 
type recall 48.2% 46.4% 
token recall 82.3% 80% 
Table 1: Type precision, recall and token recall 
3 Detecting Incorrect SCF Cues 
We examined the way segmentation errors and 
disfluency affects our acquisition system ? the sta-
tistical parser and the extractor in particular ? in 
proposing SCF cues and explored ways to detect 
incorrect SCF cues. We extracted 500 SCF cues 
from the ViC corpus (Pitt, et al 2005) and identi-
fied four major reasons that seem to have caused 
the extractor to propose incorrect SCF cues: multi-
ple utterances; missing punctuation; disfluency; 
parsing errors.  
Error analysis reveals that segmentation errors 
and disfluencies cause the parser and the extractor 
to tend to make systematic errors in proposing SCF 
cues ? incorrect SCF cues are likely to have an 
extra complement. We therefore proposed the fol-
lowing two sets of linguistic heuristics for auto-
matically detecting incorrect SCF cues: 
Linguistic Heuristic Set 1: The following SCF 
cues are extremely unlikely whatever the verb. Re-
ject an SCF cue as incorrect if it contains the fol-
lowing patterns: 
? [(NP) PP NP]: We reach out [to your friends] [your 
neighbor]. 
? [NP PP-to S]: Would I want them to say [that][to 
me] [would I want them to do that to me]. 
? [NP NP S]: They just beat [Indiana in basketball] 
[the- Saturday] [I think it was um-hum]. 
204
? [PP-p PP-p]: He starts living [with the] [with the 
guys]. 
Linguistic Heuristic Set 2: The following SCF 
cues are all possibly valid SCFs: for SCF cues of 
the following type, check if the given verb takes it 
in COMLEX. If not, reject it: 
? [(NP) S]: When he was dying [what did he say]. 
? [PP-to S]: The same thing happened [to him] [uh 
he had a scholarship]. 
? [(NP) NP]: OU had a heck of time beating [them] 
[uh-hum]. 
? [(NP) INF]: You take [the plate] from the table 
[rinse them off] and put them by the sink. 
Given the utilization of a gold standard in the 
heuristics, it would be improper to build an end-to-
end system and evaluate against COMLEX. In-
stead, we evaluate by seeing how often our heuris-
tics succeed producing results agreeable to a 
human judge. 
To evaluate the robustness of our linguistic heu-
ristics, we conducted a cross-corpora and cross-
parser comparison. We used 1,169 verb tokens 
from the ViC corpus and another 1,169 from the 
Switchboard corpus. 
Cross-corpus Comparison: The purpose of the 
cross-corpus comparison is to show that our lin-
guistic heuristics based on the data from one spo-
ken corpus can be applied to other spoken corpora. 
Therefore, we applied our heuristics to the ViC and 
the Switchboard corpus parsed by Charniak?s 
parser. We calculated the percentage of incorrect 
SCF cues before and after applying our linguistic 
heuristics. The results are shown in Table 2.  
Charniak?s parser ViC Switchboard 
before heuristics 18.8% 9.5% 
after heuristics 6.4% 4.6% 
Table 2: Incorrect SCF cue rate before and after heuristics 
 
Table 2 shows that the incorrect SCF cue rate 
has been reduced to roughly the same level for the 
two spoken corpora after applying our linguistic 
heuristics. 
Cross-parser Comparison: The purpose of the 
cross-parser comparison is to show that our lin-
guistic heuristics based on the data parsed by one 
parser can be applied to other parsers as well. To 
this end, we applied our heuristics to the 
Switchboard corpus parsed by both Charniak?s 
parser and Bikel?s parsing engine (Bikel, 2004). 
Again, we calculated the percentage of incorrect 
SCF cues before and after applying our heuristics. 
The results are displayed in Table 3. 
Although our linguistic heuristics works slightly 
better for data parsed by Charniak? parser, the in-
correct SCF cue rate after applying heuristics re-
mains at about the same level for the two different 
parsers we used. 
Switchboard Charniak Bikel 
before heuristics 9.5% 9.2% 
after heuristics 4.6% 5.4% 
Table 3: Incorrect SCF cue rate before and after heuristics 
4 Conclusion 
We showed that it should not be assumed that stan-
dard statistical parsers will fail on language that is 
very different from what they are trained on. Spe-
cifically, the results of Experiment 1 showed that it 
is feasible to apply current SCF extraction 
technology to spoken language. Experiment 2 
showed that incorrect SCF cues due to segmenta-
tion errors and disfluency can be recognized by our 
linguistic heuristics. We have shown that our SCF 
acquisition system as a whole will work for the 
different demands of spoken language. 
5 Acknowledgements 
This work was supported by NSF grant 0347799 to 
the second author, and by a summer fellowship 
from the Ohio State Center for Cognitive Science 
to the first author. 
References  
Biekl, D. 2004. Intricacies of Collins? Parsing Model. Computational 
Linguistics, 30(4): 470-511 
Brent, M. 1993. From Grammar to Lexicon: Unsupervised Learning 
of Lexical Syntax. Computational Lingusitics: 19(3): 243-262 
Briscoe, E. & Carroll, G. 1997. Automatic Extraction of Subcategori-
zation from Corpora. In Proceedings of the 5th ACL Conference on 
Applied Natural Language Processing, Washington, DC. 356-363 
Chaniak, E. 2000. A Maximum-Entropy-Inspired Parser. In Proceed-
ings of the 2000 Conference of the North American Chapter of 
ACL. 132-139 
Grishman, R., Macleod, C. & Meyers, A. 1994. COMLEX Syntax: 
Building a Computational Lexicon. In Proceedings of the Interna-
tional Conference on Computational Lingusitics, COLING-94, 
Kyoto, Japan. 268-272 
Pitt, M., Johnson, K., Hume, E., Kiesling, S., Raymond, W. 2005. 
They Buckeye Corpus of Conversational Speech: Labeling Con-
ventions and a Test of Transcriber Reliability. Speech Communica-
tion, 45: 89-95 
Roland, D. & Jurafsky, D. 1998. How Verb Subcategorization Fre-
quency Affected by the Corpus Choice. In Proceedings of 17th In-
ternational Conference on Computational Lingusitics, 2: 1122-
1128 
Sarkar, A. & Zeman, D. 2000. Automatic Extraction of Subcategoriza-
tion Frames for Czech. In Proceedings of the 19th International 
Conference on Computational Lingusitics. 691-697 
205
