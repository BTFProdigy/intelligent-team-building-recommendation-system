Proceedings of SPEECHGRAM 2007, pages 41?48,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Bidirectional Grammar-Based Medical Speech Translator
Pierrette Bouillon1, Glenn Flores2, Marianne Starlander1, Nikos Chatzichrisafis1
Marianne Santaholma1, Nikos Tsourakis1, Manny Rayner1,3, Beth Ann Hockey4
1 University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
Pierrette.Bouillon@issco.unige.ch
Marianne.Starlander@eti.unige.ch, Nikos.Chatzichrisafis@vozZup.com
Marianne.Santaholma@eti.unige.ch, Nikolaos.Tsourakis@issco.unige.ch
2 Medical College of Wisconsin, 8701 Watertown Plank Road, Milwaukee, WI 53226
gflores@mcw.edu
3 Powerset, Inc., 475 Brannan Street, San Francisco, CA 94107
manny@powerset.com
4 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035?1000
bahockey@ucsc.edu
Abstract
We describe a bidirectional version of the
grammar-based MedSLT medical speech
system. The system supports simple medi-
cal examination dialogues about throat pain
between an English-speaking physician and
a Spanish-speaking patient. The physician?s
side of the dialogue is assumed to consist
mostly of WH-questions, and the patient?s of
elliptical answers. The paper focusses on the
grammar-based speech processing architec-
ture, the ellipsis resolution mechanism, and
the online help system.
1 Background
There is an urgent need for medical speech trans-
lation systems. The world?s current population
of 6.6 billion speaks more than 6,000 languages
(Graddol, 2004). Language barriers are associated
with a wide variety of deleterious consequences in
healthcare, including impaired health status, a lower
likelihood of having a regular physician, lower rates
of mammograms, pap smears, and other preven-
tive services, non-adherence with medications, a
greater likelihood of a diagnosis of more severe psy-
chopathology and leaving the hospital against med-
ical advice among psychiatric patients, a lower like-
lihood of being given a follow-up appointment af-
ter an emergency department visit, an increased risk
of intubation among children with asthma, a greater
risk of hospital admissions among adults, an in-
creased risk of drug complications, longer medical
visits, higher resource utilization for diagnostic test-
ing, lower patient satisfaction, impaired patient un-
derstanding of diagnoses, medications, and follow-
up, and medical errors and injuries (Flores, 2005;
Flores, 2006). Nevertheless, many patients who
need medical interpreters do not get them. For ex-
ample, in the United States, where 52 million peo-
ple speak a language other than English at home
and 23 million people have limited English profi-
ciency (LEP) (Census, 2007), one study found that
about half of LEP patients presenting to an emer-
gency department were not provided with a medical
interpreter (Baker et al, 1996). There is thus a sub-
stantial gap between the need for and availability of
language services in health care, a gap that could be
bridged through effective medical speech translation
systems.
An ideal system would be able to interpret ac-
curately and flexibly between patients and health
care professionals, using unrestricted language and
a large vocabulary. A system of this kind is, un-
fortunately, beyond the current state of the art.
It is, however, possible, using today?s technol-
ogy, to build speech translation systems for specific
scenarios and language-pairs, which can achieve
acceptable levels of reliability within the bounds
41
of a well-defined controlled language. MedSLT
(Bouillon et al, 2005) is an Open Source system
of this type, which has been under construction at
Geneva University since 2003. The system is built
on top of Regulus (Rayner et al, 2006), an Open
Source platform which supports development of
grammar-based speech-enabled applications. Regu-
lus has also been used to build several other systems,
including NASA?s Clarissa (Rayner et al, 2005b).
The most common architecture for speech trans-
lation today uses statistical methods to perform both
speech recognition and translation, so it is worth
clarifying why we have chosen to use grammar-
based methods. Even though statistical architec-
tures exhibit many desirable properties (purely data-
driven, domain independent), this is not necessar-
ily the best alternative in safety-critical medical ap-
plications. Anecdotally, many physicians express
reluctance to trust a translation device whose out-
put is not readily predictable, and most of the
speech translation systems which have reached the
stage of field testing rely on various types of
grammar-based recognition and rule-based transla-
tion (Phraselator, 2007; Fluential, 2007).
Statistical speech recognisers can achieve impres-
sive levels of accuracy when trained on enough data,
but it is a daunting task to collect training mate-
rial in the requisite quantities (usually, tens of thou-
sands of high-quality utterances) when trying to
build practical systems. Considering that the medi-
cal speech translation applications we are interested
in constructing here need to work for multiple lan-
guages and subdomains, the problem becomes even
more challenging. Our experience is that grammar-
based systems which also incorporate probabilistic
context-free grammar tuning deliver better results
than purely statistical ones when training data are
sparse (Rayner et al, 2005a).
Another common criticism of grammar-based
systems is that out-of-coverage utterances will
neither be recognized nor translated, an objec-
tion that critics have sometimes painted as de-
cisive. It is by no means obvious, however,
that restricted coverage is such a serious prob-
lem. In text processing, work on several gener-
ations of controlled language systems has devel-
oped a range of techniques for keeping users within
the bounds of system coverage (Kittredge, 2003;
Mitamura, 1999), and variants of these methods can
also be adapted for spoken language applications.
Our experiments with MedSLT show that even a
quite simple help system is enough to guide users
quickly towards the intended coverage of a medium-
vocabulary grammar-based speech translation appli-
cation, with most users appearing confident after just
an hour or two of exposure (Starlander et al, 2005;
Chatzichrisafis et al, 2006).
Until recently, the MedSLT system only sup-
ported unidirectional processing in the physician
to patient direction. The assumption was that the
physician would mostly ask yes/no questions, to
which the patient would respond non-verbally, for
example by nodding or shaking their head. A uni-
directional architecture is easier to make habitable
than a bidirectional one. It is reasonable to as-
sume that the physician will use the system regu-
larly enough to learn the coverage, but most patients
will not have used the system before, and it is less
clear that they will be able to acclimatize within the
narrow window at their disposal. These consider-
ations must however be balanced against the fact
that a unidirectional system does not allow for a
patient-centered interaction characterized by mean-
ingful patient-clinician communication or shared de-
cision making. Multiple studies in the medical lit-
erature document that patient-centeredness, effec-
tive patient-clinician communication, and shared de-
cision making are associated with significant im-
provements in patient health outcomes, including
reduced anxiety levels, improved functional sta-
tus, reduced pain, better control of diabetes melli-
tus, blood pressure reduction among hypertensives,
improved adherence, increased patient satisfaction,
and symptom reduction for a variety of conditions
(Stewart, 1995; Michie et al, 2003). A bidirectional
system is considered close to essential from a health-
care perspective, since it appropriately addresses the
key issues of patient centeredness and shared de-
cision making. For these reasons, we have over
the last few months developed a bidirectional ver-
sion of MedSLT, initially focussing on a throat pain
scenario with an English-speaking physician and a
Spanish-speaking patient. The physician uses full
sentences, while the patient answers with short re-
sponses.
One of the strengths of the Regulus approach is
42
that it is very easy to construct parallel versions of
a grammar; generally, all that is required is to vary
the training corpus. (We will have more to say about
this soon). We have exploited these properties of
the platform to create two different configurations
of the bidirectional system, so that we can compare
competing approaches to the problem of accommo-
dating patients unfamiliar with speech technology.
In Version 1 (less restricted), the patient is allowed
to answer using both elliptical utterances and short
sentences, while in Version 2 (more restricted) they
are only permitted to use elliptical utterances. Thus,
for example, if the physician asks the question ?How
long have you had a sore throat??, Version 1 allows
the patient to respond both ?Desde algunos d??as?
(?For several days?) and ?Me ha dolido la garganta
desde algunos d??as? (?I have had a sore throat for
several days?), while Version 2 only allows the first
of these. Both the short and the long versions are
translated uniformly, with the short version resolved
using the context from the preceding question.
In both versions, if the patient finds it too chal-
lenging to use the system to answer WH-questions
directly, it is possible to back off to the earlier di-
alogue architecture in which the physician uses Y-
N questions and the patient responds with simple
yes/no answers, or nonverbally. Continuing the ex-
ample, if the patient is unable to find an appro-
priate way to answer the physician?s question, the
physician could ask ?Have you had a sore throat for
more than three days??; if the patient responds nega-
tively, they could continue with the follow-on ques-
tion ?More than a week??, and so on.
In the rest of the paper, we first describe the
system top-level (Section 2), the way in which
grammar-based processing is used (Section 3), the
ellipsis processing mechanism (Section 4), and the
help system (Section 5). Section 6 presents an ini-
tial evaluation, and the final section concludes.
2 Top-level architecture
The system is operated through the graphical user
interface (GUI) shown in Figures 1 and 2. In
accordance with the basic principles of patient-
centeredness and shared decision-making outlined
in Section 1, the patient and the physician each have
their own headset, use their own mouse, and share
the same view of the screen. This is in sharp contrast
to the majority of the medical speech translation sys-
tems described in the literature (Somers, 2006).
As shown in the screenshots, the main GUI win-
dow is separated into two tabbed panes, marked
?Doctor? and ?Patient?. Initially, the ?Doctor? view
(the one shown in Figure 1) is active. The physician
presses the ?Push to talk? button, and speaks into
the headset microphone. If recognition is success-
ful, the GUI displays four separate results, listed on
the right side of the screen. At the top, immediately
under the heading ?Question?, we can see the actual
words returned by speech recognition. Here, these
words are ?Have you had rapid strep test?. Below,
we have the help pane: this displays similar ques-
tions taken from the help corpus, which are known to
be within system coverage. The pane marked ?Sys-
tem understood? shows a back-translation, produced
by first translating the recognition result into inter-
lingua, and then translating it back into English. In
the present example, this corrects the minor mistake
the recogniser has made, missing the indefinite ar-
ticle ?a?, and confirms that the system has obtained
a correct grammatical analysis and interpretation at
the level of interlingua. At the bottom, we see the
target language translation. The left-hand side of the
screen logs the history of the conversation to date, so
that both sides can refer back to it.
If the physician decides that the system has cor-
rectly understood what they said, they can now press
the ?Play? button. This results in the system produc-
ing a spoken output, using the Vocalizer TTS engine.
Simultaneously with speaking, the GUI shifts to the
?Patient? configuration shown in Figure 2. This dif-
fers from the ?Doctor? configuration in two respects:
all text is in the patient language, and the help pane
presents its suggestions immediately, based on the
preceding physician question. The various process-
ing components used to support these functionalities
are described in the following sections.
3 Grammar-based processing
Grammar-based processing is used for source-
language speech recognition and target-side genera-
tion. (Source-language analysis is part of the recog-
nition process, since grammar-based recognition in-
cludes creating a parse). All of these functionalities
43
Figure 1: Screenshot showing the state of the GUI after the physician has spoken, but before he has pressed
the ?Play? button. The help pane shows similar queries known to be within coverage.
Figure 2: Screenshot showing the state of the GUI after the physician has pressed the ?Play? button. The
help pane shows known valid responses to similar questions.
44
are implemented using the Regulus platform, with
the task-specific grammars compiled out of general
feature grammar resources by the Regulus tools. For
both recognition and generation, the first step is
to extract a domain-specific feature grammar from
the general one, using a version of the Explanation
Based Learning (EBL) algorithm.
The extraction process is driven by a corpus of ex-
amples and a set of ?operationality criteria?, which
define how the rules in the original resource gram-
mar are recombined into domain-specific ones. It is
important to realise that the domain-specific gram-
mar is not merely a subset of the resource grammar;
a typical domain-specific grammar rule is created by
merging two to five resource grammar rules into a
single ?flatter? rule. The result is a feature gram-
mar which is less general than the original one, but
more efficient. For recognition, the grammar is then
processed further into a CFG language model, using
an algorithm which alternates expansion of feature
values and filtering of the partially expanded gram-
mar to remove irrelevant rules. Detailed descrip-
tions of the EBL learning and feature grammar ?
CFG compilation algorithms can be found in Chap-
ters 8 and 10 of (Rayner et al, 2006). Regulus fea-
ture grammars can also be compiled into generators
using a version of the Semantic Head Driven algo-
rithm (Shieber et al, 1990).
The English (physician) side recogniser is com-
piled from the large English resource grammar de-
scribed in Chapter 9 of (Rayner et al, 2006), and
was constructed in the same way as the one de-
scribed in (Rayner et al, 2005a), which was used for
a headache examination task. The operationality cri-
teria are the same, and the only changes are a differ-
ent training corpus and the addition of new entries
to the lexicon. The same resources, with a differ-
ent training corpus, were used to build the English
language generator. It is worth pointing out that, al-
though a uniform method was used to build these
various grammars, the results were all very differ-
ent. For example, the recognition grammar from
(Rayner et al, 2005a) is specialised to cover only
second-person questions (?Do you get headaches
in the mornings??), while the generator grammar
used in the present application covers only first-
person declarative statements (?I visited the doctor
last Monday.?). In terms of structure, each gram-
mar contains several important constructions that the
other lacks. For example, subordinate clauses are
central in the headache domain (?Do the headaches
occur when you are stressed??) but are not present
in the sore throat domain; this is because the stan-
dard headache examination questions mostly focus
on generic conditions, while the sore throat exami-
nation questions only relate to concrete ones. Con-
versely, relative clauses are important in the sore
throat domain (?I have recently been in contact with
someone who has strep throat?), but are not suffi-
ciently important in the headache domain to be cov-
ered there.
On the Spanish (patient) side, there are four
grammars involved. For recognition, we have
two different grammars, corresponding to the two
versions of the system; the grammar for Ver-
sion 2 is essentially a subset of that for Version
1. For generation, there are two separate and
quite different grammars: one is used for trans-
lating the physician?s questions, while the other
produces back-translations of the patient?s ques-
tions. All of these grammars are extracted from
a general shared resource grammar for Romance
languages, which currently combines rules for
French, Spanish and Catalan (Bouillon et al, 2006;
Bouillon et al, to appear 2007b).
One interesting consequence of our methodology
is related to the fact that Spanish is a prodrop lan-
guage, which implies that many sentences are sys-
tematically ambiguous between declarative and Y-N
question readings. For example, ?He consultado un
me?dico? could in principle mean either ?I visited a
doctor? or ?Did I visit a doctor??. When training the
specialised Spanish grammars, it is thus necessary to
specify which readings of the training sentences are
to be used. Continuing the example, if the sentence
occurred in training material for the answer gram-
mar, we would specify that the declarative reading
was the intended one1.
4 Ellipsis processing and contextual
interpretation
In Version 1 of the system, the patient is per-
mitted to answer using elliptical phrases; in Ver-
1The specification can be formulated as a preference that
applies uniformly to all the training examples in a given group.
45
sion 2, she is obliged to do so. Ability to pro-
cess elliptical responses makes it easier to guide the
patient towards the intended coverage of the sys-
tem, without degrading the quality of recognition
(Bouillon et al, to appear 2007a). The downside is
that ellipses are also harder to translate than full sen-
tences. Even in a limited domain like ours, and in a
closely related language-pair, ellipsis can generally
not be translated word for word, and it is necessary
to look at the preceding context if the rules are to
be applied correctly. In examples 1 and 2 below,
the locative phrase ?In your stomach? in the English
source becomes the subject in the Spanish transla-
tion. This implies that the translation of the ellipsis
in the second physician utterance needs to change
syntactic category: ?In your head? (PP) becomes
?La cabeza? (NP).
(1) Doctor: Do you have a pain in your
stomach?
(Trans): Le duele el estomago?
(2) Doctor: In your head?
(Trans): *En la cabeza?
Since examples like this are frequent, our sys-
tem implements a solution in which the patient?s
replies are translated in the context of the preced-
ing utterance. If the patient-side recogniser?s output
is classified as an ellipsis (this can done fairly reli-
ably thanks to use of suitably specialised grammars;
cf. Section 3), we expand the incomplete phrase
into a full sentence structure by adding appropriate
structural elements from the preceding physician-
side question; the expanded semantic structure is the
one which is then translated into interlingual form,
and thence back to the physician-side language.
Since all linguistic representations, including
those of elliptical phrases and their contexts, are rep-
resented as flat attribute-value lists, we are able to
implement the resolution algorithm very simply in
terms of list manipulation. In YN-questions, where
the elliptical answer intuitively adds information to
the question (?Did you visit the doctor??; ?El lunes?
? ?I visited the doctor on Monday?), the repre-
sentations are organised so that resolution mainly
amounts to concatenation of the two lists2. In WH-
questions, where the answer intuitively substitutes
the elliptical answer for the WH-phrase (?What is
2It is also necessary to replace second-person pronouns with
first-person counterparts.
your temperature??; ?Cuarenta grados?? ?My tem-
perature is forty degrees?), resolution substitutes the
representation of the elliptical phrase for that of a
semantically similar element in the question.
The least trivial aspect of this process is provid-
ing a suitable definition of ?semantically similar?.
This is done using a simple example-based method,
in which the grammar developer writes a set of dec-
larations, each of which lists a set of semantically
similar NPs. At compile-time, the grammar is used
to parse each NP, and extract a generalised skele-
ton, in which specific lexical information is stripped
away; at run-time, two NPs are held to be semanti-
cally similar if they can each be unified with skele-
tons in the same equivalence class. This ensures that
the definition of the semantic similarity relation is
stable across most changes to the grammar and lex-
icon. The issues are described in greater detail in
(Bouillon et al, to appear 2007a).
5 Help system
Since the performance of grammar-based speech un-
derstanding is only reliable on in-coverage mate-
rial, systems based on this type of architecture must
necessarily use a controlled language approach, in
which it is assumed that the user is able to learn the
relevant coverage. As previously noted, the Med-
SLT system addresses this problem by incorporat-
ing an online help system (Starlander et al, 2005;
Chatzichrisafis et al, 2006).
On the physician side, the help system offers, af-
ter each recognition event, a list of related ques-
tions; similarly, on the patient side, it provides ex-
amples of known valid answers to the current ques-
tion. In both cases, the help examples are extracted
from a precompiled corpus of question-answer pairs,
which have been judged for correctness by system
developers. The process of selecting the examples
is slightly different on the two sides. For questions
(physician side), the system performs a second par-
allel recognition of the input speech, using a sta-
tistical recogniser. It then compares the recogni-
tion result, using an N-gram based metric, against
the set of known correct in-coverage questions from
the question-answer corpus, to extract the most sim-
ilar ones. For answers (patient side), the help sys-
tem searches the question-answer corpus to find the
46
questions most similar to the current one, and shows
the list of corresponding valid answers, using the
whole list in the case of Version 1 of the system, and
only the subset consisting of elliptical phrases in the
case of Version 2.
6 Evaluation
In previous studies, we have evaluated speech
recognition and speech understanding per-
formance for physician-side questions in
English (Bouillon et al, 2005) and Spanish
(Bouillon et al, to appear 2007b), and investi-
gated the impact on performance of the help system
(Rayner et al, 2005a; Starlander et al, 2005). We
have also carried out recent evaluations designed to
contrast recognition performance on elliptical and
full versions of the same utterance; here, our results
suggest that elliptical forms of (French-language)
MedSLT utterances are slightly easier to recognise
in terms of semantic error rate than full sentential
forms (Bouillon et al, to appear 2007a). Our initial
evaluation studies on the bidirectional system have
focussed on a specific question which has particular
relevance to this new version of MedSLT. Since
we are assuming that the patient will respond
using elliptical utterances, and that these utterances
will be translated in the context of the preceding
physician-side question, how confident can we
be that this context-dependent translation will be
correct?
In order to investigate these issues, we performed
a small data-collection using Version 2 of the sys-
tem, whose results we summarise here. One of the
authors of the paper played the role of an English-
speaking physician, in a simulated medical exam-
ination scenario where the goal was to determine
whether or not the ?patient? was suffering from a
viral throat infection. The six subjects playing the
role of the patient were all native speakers of Span-
ish, and had had no previous exposure to the system,
or indeed any kind of speech technology. They were
given cards describing the symptoms they were sup-
posed to be displaying, on which they were asked
to based their answers. From a total of 92 cor-
rectly recognised patient responses, we obtained 50
yes/no answers and 42 examples of real elliptical ut-
terances. Out of these, 36 were judged to have been
translated completely correctly, and a further 3 were
judged correct in terms of meaning, but less than flu-
ent. Only 3 examples were badly translated: of these
two were caused by problems in a translation rule,
and one by incorrect treatment of ellipsis resolution.
We show representative exchanges below; the last of
these is the one in which ellipsis processing failed to
work correctly.
(3) Doctor: For how long have you
had your sore throat?
Patient: Desde hace ma?s de
una semana
(Trans): I have had a sore
throat for more than one week
(4) Doctor: What were the results?
Patient: Negativo
(Trans): The results were negative
(5) Doctor: Have you seen a doctor
for your sore throat?
Patient: S?? el lunes
(Trans): I visited the doctor
for my sore throat monday
(6) Doctor: Have you been with anyone
recently who has a strep throat?
Patient: Si ma?s de dos semanas
(Trans): I was in contact with someone
more than two weeks recently
who had strep throat
7 Conclusions
We have presented a bidirectional grammar-based
English ? Spanish medical speech translation sys-
tem built using a linguistically motivated archi-
tecture, where all linguistic information is ulti-
mately derived from two resource grammars, one
for each language. We have shown how this en-
ables us to derive the multiple grammars needed,
which differ both with respect to function (recog-
nition/generation) and to domain (physician ques-
tions/patient answers). The system is currently un-
dergoing initial lab testing; we hope to advance to
initial trials on real patients some time towards the
end of the year.
References
[Baker et al1996] D.W. Baker, R.M. Parker, M.V.
Williams, W.C. Coates, and Kathryn Pitkin. 1996.
47
Use and effectiveness of interpreters in an emer-
gency department. Journal of the American Medical
Association, 275:783?8.
[Bouillon et al2005] P. Bouillon, M. Rayner,
N. Chatzichrisafis, B.A. Hockey, M. Santaholma,
M. Starlander, Y. Nakao, K. Kanzaki, and H. Isahara.
2005. A generic multi-lingual open source platform
for limited-domain medical speech translation. In
Proceedings of the 10th Conference of the European
Association for Machine Translation (EAMT), pages
50?58, Budapest, Hungary.
[Bouillon et al2006] P. Bouillon, M. Rayner, B. Novel-
las Vall, Y. Nakao, M. Santaholma, M. Starlander, and
N. Chatzichrisafis. 2006. Une grammaire multilingue
partage?e pour la traduction automatique de la parole.
In Proceedings of TALN 2006, Leuwen, Belgium.
[Bouillon et alto appear 2007a] P. Bouillon, M. Rayner,
M. Santaholma, and M. Starlander. to appear 2007a.
Les ellipses dans un syste`me de traduction automa-
tique de la parole. In Proceedings of TALN 2006,
Toulouse, France.
[Bouillon et alto appear 2007b] P. Bouillon, M. Rayner,
B. Novellas Vall, Y. Nakao, M. Santaholma, M. Star-
lander, and N. Chatzichrisafis. to appear 2007b. Une
grammaire partage?e multi-ta?che pour le traitement de
la parole : application aux langues romanes. Traite-
ment Automatique des Langues.
[Census2007] U.S. Census, 2007. Selected Social Char-
acteristics in the United States: 2005. Data Set: 2005
American Community Survey. Available here.
[Chatzichrisafis et al2006] N. Chatzichrisafis, P. Bouil-
lon, M. Rayner, M. Santaholma, M. Starlander, and
B.A. Hockey. 2006. Evaluating task performance for
a unidirectional controlled language medical speech
translation system. In Proceedings of the HLT-NAACL
International Workshop on Medical Speech Transla-
tion, pages 9?16, New York.
[Flores2005] G. Flores. 2005. The impact of medical in-
terpreter services on the quality of health care: A sys-
tematic review. Medical Care Research and Review,
62:255?299.
[Flores2006] G. Flores. 2006. Language barriers to
health care in the united states. New England Journal
of Medicine, 355:229?231.
[Fluential2007] Fluential, 2007.
http://www.fluentialinc.com. As of 24 March
2007.
[Graddol2004] D. Graddol. 2004. The future of lan-
guage. Science, 303:1329?1331.
[Kittredge2003] R. I. Kittredge. 2003. Sublanguages and
comtrolled languages. In R. Mitkov, editor, The Ox-
ford Handbook of Computational Linguistics, pages
430?447. Oxford University Press.
[Michie et al2003] S. Michie, J. Miles, and J. Weinman.
2003. Patient-centeredness in chronic illness: what is
it and does it matter? Patient Education and Counsel-
ing, 51:197?206.
[Mitamura1999] T. Mitamura. 1999. Controlled lan-
guage for multilingual machine translation. In Pro-
ceedings of Machine Translation Summit VII, Singa-
pore.
[Phraselator2007] Phraselator, 2007.
http://www.voxtec.com/. As of 24 March 2007.
[Rayner et al2005a] M. Rayner, P. Bouillon,
N. Chatzichrisafis, B.A. Hockey, M. Santaholma,
M. Starlander, H. Isahara, K. Kanzaki, and Y. Nakao.
2005a. A methodology for comparing grammar-based
and robust approaches to speech understanding. In
Proceedings of the 9th International Conference
on Spoken Language Processing (ICSLP), pages
1103?1107, Lisboa, Portugal.
[Rayner et al2005b] M. Rayner, B.A. Hockey, J.M. Ren-
ders, N. Chatzichrisafis, and K. Farrell. 2005b. A
voice enabled procedure browser for the International
Space Station. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (interactive poster and demo track), Ann Arbor,
MI.
[Rayner et al2006] M. Rayner, B.A. Hockey, and
P. Bouillon. 2006. Putting Linguistics into Speech
Recognition: The Regulus Grammar Compiler. CSLI
Press, Chicago.
[Shieber et al1990] S. Shieber, G. van Noord, F.C.N.
Pereira, and R.C. Moore. 1990. Semantic-head-driven
generation. Computational Linguistics, 16(1).
[Somers2006] H. Somers. 2006. Language engineering
and the path to healthcare: a user-oriented view. In
Proceedings of the HLT-NAACL International Work-
shop on Medical Speech Translation, pages 32?39,
New York.
[Starlander et al2005] M. Starlander, P. Bouillon,
N. Chatzichrisafis, M. Santaholma, M. Rayner, B.A.
Hockey, H. Isahara, K. Kanzaki, and Y. Nakao. 2005.
Practising controlled language through a help system
integrated into the medical speech translation system
(MedSLT). In Proceedings of MT Summit X, Phuket,
Thailand.
[Stewart1995] M.A. Stewart. 1995. Effective physician-
patient communication and health outcomes: a review.
Canadian Medical Association Journal, 152:1423?
1433.
48
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 32?35
Manchester, August 2008
The 2008 MedSLT System
Manny Rayner1, Pierrette Bouillon1, Jane Brotanek2, Glenn Flores2
Sonia Halimi1, Beth Ann Hockey3, Hitoshi Isahara4, Kyoko Kanzaki4
Elisabeth Kron5, Yukie Nakao6, Marianne Santaholma1
Marianne Starlander1, Nikos Tsourakis1
1 University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
{Emmanuel.Rayner,Pierrette.Bouillon,Nikolaos.Tsourakis}@issco.unige.ch
{Sonia.Halimi,Marianne.Santaholma,Marianne.Starlander}@eti.unige.ch
2 UT Southwestern Medical Center, Children?s Medical Center of Dallas
{Glenn.Flores,Jane.Brotanek}@utsouthwestern.edu
3 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035?1000
bahockey@ucsc.edu
4 NICT, 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0289
{isahara,kanzaki}@nict.go.jp
5 3 St Margarets Road, Cambridge CB3 0LT, England
elisabethkron@yahoo.co.uk
6 University of Nantes, LINA, 2, rue de la Houssinie`re, BP 92208 44322 Nantes Cedex 03
yukie.nakao@univ-nantes.fr
Abstract
MedSLT is a grammar-based medical
speech translation system intended for
use in doctor-patient diagnosis dialogues,
which provides coverage of several dif-
ferent subdomains and multiple language
pairs. Vocabulary ranges from about 350 to
1000 surface words, depending on the lan-
guage and subdomain. We will demo three
different versions of the system: an any-
to-any multilingual version involving the
languages Japanese, English, French and
Arabic, a bidirectional English ? Span-
ish version, and a mobile version run-
ning on a hand-held PDA. We will also
demo the Regulus development environ-
ment, focussing on features which sup-
port rapid prototyping of grammar-based
speech translation systems.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1 Introduction
MedSLT is a medium-vocabulary grammar-based
medical speech translation system built on top of
the Regulus platform (Rayner et al, 2006). It is
intended for use in doctor-patient diagnosis dia-
logues, and provides coverage of several subdo-
mains and a large number of different language-
pairs. Coverage is based on standard examina-
tion questions obtained from physicians, and fo-
cusses primarily on yes/no questions, though there
is also support for WH-questions and elliptical ut-
terances.
Detailed descriptions of MedSLT can be found
in earlier papers (Bouillon et al, 2005; Bouil-
lon et al, 2008)1. In the rest of this note, we
will briefly sketch several versions of the system
that we intend to demo at the workshop, each of
which displays new features developed over the
last year. Section 2 describes an any-language-to-
any-language multilingual version of the system;
Section 3, a bidirectional English ? Spanish ver-
sion; Section 4, a version running on a mobile PDA
1All MedSLT publications are available on-line
at http://www.issco.unige.ch/projects/
medslt/publications.shtml.
32
platform; and Section 5, the Regulus development
environment.
2 A multilingual version
During the last few months, we have reorganised
the MedSLT translation model in several ways2. In
particular, we give a much more central role to the
interlingua; we now treat this as a language in its
own right, defined by a normal Regulus grammar,
and using a syntax which essentially amounts to
a greatly simplified form of English. Making the
interlingua into another language has made it easy
to enforce tight constraints on well-formedness of
interlingual semantic expressions, since checking
well-formedness now just amounts to performing
generation using the interlingua grammar.
Another major advantage of the scheme is that
it is also possible to systematise multilingual de-
velopment, and only work with translation from
source language to interlingua, and from interlin-
gua to target language; here, the important point
is that the human-readable interlingua surface syn-
tax makes it feasible in practice to evaluate transla-
tion between normal languages and the interlingua.
Development of rules for translation to interlingua
is based on appropriate corpora for each source
language. Development of rules for translating
from interlingua uses a corpus which is formed by
merging together the results of translating each of
the individual source-language corpora into inter-
lingua.
We will demonstrate our new capabilities in
interlingua-based translation, using a version of
the system which translates doctor questions in the
headache domain from any language to any lan-
guage in the set {English, French, Japanese, Ara-
bic}. Table 1 gives examples of the coverage of the
English-input headache-domain version, and Ta-
ble 2 summarises recognition performance in this
domain for the three input languages where we
have so far performed serious evaluations. Differ-
ences in the sizes of the recognition vocabularies
are primarily due to differences in use of inflec-
tion.
3 A bidirectional version
The system from the preceding section is unidi-
rectional; all communication is in the doctor-to-
patient direction, the expectation being that the pa-
2The ideas in the section are described at greater length in
(Bouillon et al, 2008).
Language Vocab WER SemER
English 447 6% 11%
French 1025 8% 10%
Japanese 422 3% 4%
Table 2: Recognition performance for English,
French and Japanese headache-domain recognis-
ers. ?Vocab? = number of surface words in source
language recogniser vocabulary; ?WER? = Word
Error Rate for source language recogniser, on in-
coverage material; ?SemER? = semantic error rate
for source language recogniser, on in-coverage
material.
tient will respond non-verbally. Our second demo,
an early version of which is described in (Bouillon
et al, 2007), supports bidirectional translation for
the sore throat domain, in the English ? Spanish
pair. Here, the English-speaking doctor typically
asks WH-questions, and the Spanish-speaking pa-
tient responds with elliptical utterances, which are
translated as full sentence responses. A short ex-
ample dialogue is shown in Table 3.
Doctor: Where is the pain?
?Do?nde le duele?
Patient: En la garganta.
I experience the pain in my throat.
Doctor: How long have you had a pain
in your throat?
?Desde cua?ndo le duele la garganta?
Patient: Ma?s de tres d??as.
I have experienced the pain in my
throat for more than three days.
Table 3: Short dialogue with bidirectional English
? Spanish version. System translations are in ital-
ics.
4 A mobile platform version
When we have shown MedSLT to medical profes-
sionals, one of the most common complaints has
been that a laptop is not an ideal platform for use
in emergency medical situations. Our third demo
shows an experimental version of the system us-
ing a client/server architecture. The client, which
contains the user interface, runs on a Nokia Linux
N800 Internet Tablet; most of the heavy process-
ing, including in particular speech recognition, is
hosted on the remote server, with the nodes com-
municating over a wireless network. A picture of
33
Where? Is the pain above your eye?
When? Have you had the pain for more than a month?
How long? Does the pain typically last a few minutes?
How often? Do you get headaches several times a week?
How? Is it a stabbing pain?
Associated symptoms? Do you vomit when you get the headaches?
Why? Does bright light make the pain worse?
What helps? Does sleep make the pain better?
Background? Do you have a history of sinus disease?
Table 1: Examples of English MedSLT coverage
the tablet, showing the user interface, is presented
in Figure 1. The sentences appearing under the
back-translation at the top are produced by an on-
line help component, and are intended to guide the
user into the grammar?s coverage (Chatzichrisafis
et al, 2006).
The architecture is described further in
(Tsourakis et al, 2008), which also gives perfor-
mance results for another Regulus applications.
These strongly suggest that recognition perfor-
mance in the client/server environment is no
worse than on a laptop, as long as a comparable
microphone is used.
5 The development environment
Our final demo highlights the new Regulus devel-
opment environment (Kron et al, 2007), which has
over the last few months acquired a large amount
of new functionality designed to facilitate rapid
prototyping of spoken language applications3 . The
developer initially constructs and debugs her com-
ponents (grammar, translation rules etc) in a text
view. As soon as they are consistent, she is able
to compile the source-language grammar into a
recogniser, and combine this with other compo-
nents to run a complete speech translation system
within the development environment. Connections
between components are defined by a simple con-
fig file. Figure 2 shows an example.
References
Bouillon, P., M. Rayner, N. Chatzichrisafis, B.A.
Hockey, M. Santaholma, M. Starlander, Y. Nakao,
K. Kanzaki, and H. Isahara. 2005. A generic multi-
lingual open source platform for limited-domain
medical speech translation. In Proceedings of the
10th Conference of the European Association for
3This work is presented in a paper currently under review.
Machine Translation (EAMT), pages 50?58, Bu-
dapest, Hungary.
Bouillon, P., G. Flores, M. Starlander,
N. Chatzichrisafis, M. Santaholma, N. Tsourakis,
M. Rayner, and B.A. Hockey. 2007. A bidirectional
grammar-based medical speech translator. In Pro-
ceedings of the ACL Workshop on Grammar-based
Approaches to Spoken Language Processing, pages
41?48, Prague, Czech Republic.
Bouillon, P., S. Halimi, Y. Nakao, K. Kanzaki, H. Isa-
hara, N. Tsourakis, M. Starlander, B.A. Hockey, and
M. Rayner. 2008. Developing non-european trans-
lation pairs in a medium-vocabulary medical speech
translation system. In Proceedings of LREC 2008,
Marrakesh, Morocco.
Chatzichrisafis, N., P. Bouillon, M. Rayner, M. Santa-
holma, M. Starlander, and B.A. Hockey. 2006. Eval-
uating task performance for a unidirectional con-
trolled language medical speech translation system.
In Proceedings of the HLT-NAACL International
Workshop on Medical Speech Translation, pages 9?
16, New York.
Kron, E., M. Rayner, P. Bouillon, and M. Santa-
holma. 2007. A development environment for build-
ing grammar-based speech-enabled applications. In
Proceedings of the ACL Workshop on Grammar-
based Approaches to Spoken Language Processing,
pages 49?52, Prague, Czech Republic.
Rayner, M., B.A. Hockey, and P. Bouillon. 2006.
Putting Linguistics into Speech Recognition: The
Regulus Grammar Compiler. CSLI Press, Chicago.
Tsourakis, N., M. Georghescul, P. Bouillon, and
M. Rayner. 2008. Building mobile spoken dialogue
applications using regulus. In Proceedings of LREC
2008, Marrakesh, Morocco.
34
Figure 1: Mobile version of the MedSLT system, running on a Nokia tablet.
Figure 2: Speech to speech translation from the development environment, using a Japanese to Arabic
translator built from MedSLT components. The user presses the Recognise button (top right), speaks in
Japanese, and receives a spoken translation in Arabic together with screen display of various processing
results. The application is defined by a config file which combines a Japanese recogniser and analy-
sis grammar, Japanese to Interlingua and Interlingua to Arabic translation rules, an Arabic generation
grammar, and recorded Arabic wavfiles used to construct a spoken result.
35
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 60?63
Manchester, August 2008
A Small-Vocabulary Shared Task for Medical Speech Translation
Manny Rayner1, Pierrette Bouillon1, Glenn Flores2, Farzad Ehsani3
Marianne Starlander1, Beth Ann Hockey4, Jane Brotanek2, Lukas Biewald5
1 University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
{Emmanuel.Rayner,Pierrette.Bouillon}@issco.unige.ch
Marianne.Starlander@eti.unige.ch
2 UT Southwestern Medical Center, Children?s Medical Center of Dallas
{Glenn.Flores,Jane.Brotanek}@utsouthwestern.edu
3 Fluential, Inc, 1153 Bordeaux Drive, Suite 211, Sunnyvale, CA 94089, USA
farzad@fluentialinc.com
4 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035?1000
bahockey@ucsc.edu
5 Dolores Labs
lukeab@gmail.com
Abstract
We outline a possible small-vocabulary
shared task for the emerging medical
speech translation community. Data would
consist of about 2000 recorded and tran-
scribed utterances collected during an eval-
uation of an English ? Spanish version
of the Open Source MedSLT system; the
vocabulary covered consisted of about 450
words in English, and 250 in Spanish. The
key problem in defining the task is to agree
on a scoring system which is acceptable
both to medical professionals and to the
speech and language community. We sug-
gest a framework for defining and admin-
istering a scoring system of this kind.
1 Introduction
In computer science research, a ?shared task? is a
competition between interested teams, where the
goal is to achieve as good performance as possible
on a well-defined problem that everyone agrees to
work on. The shared task has three main compo-
nents: training data, test data, and an evaluation
metric. Both test and training data are divided
up into sets of items, which are to be processed.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
The evaluation metric defines a score for each pro-
cessed item. Competitors are first given the train-
ing data, which they use to construct and/or train
their systems. They are then evaluated on the test
data, which they have not previously seen.
In many areas of speech and language process-
ing, agreement on a shared task has been a major
step forward. Often, it has in effect created a new
subfield, since it allows objective comparison of
results between different groups. For example, it
is very common at speech conference to have spe-
cial sessions devoted to recognition within a par-
ticular shared task database. In fact, a conference
without at least a couple of such sessions would
be an anomaly. A recent success story in language
processing is the Recognizing Textual Entailment
(RTE) task1. Since its inception in 2004, this has
become extremely popular; the yearly RTE work-
shop now attracts around 40 submissions, and error
rates on the task have more than halved.
Automatic medical speech translation would
clearly benefit from a shared task. As was made
apparent at the initial 2006 workshop in New
York2, nearly every group has both a unique ar-
chitecture and a unique set of data, essentially
making comparisons impossible. In this note, we
will suggest an initial small-vocabulary medical
1http://www.pascal-network.org/
Challenges/RTE/
2http://www.issco.unige.ch/pub/
SLT workshop proceedings book.pdf
60
shared task. The aspect of the task that is hard-
est to define is the evaluation metric, since there
unfortunately appears to be considerable tension
between the preferences of medical professionals
and speech system implementers. Medical profes-
sionals would prefer to carry out a ?deep? evalu-
ation, in terms of possible clinical consequences
following from a mistranslation. System evalua-
tors will on the other hand prefer an evaluation
method that can be carried out quickly, enabling
frequent evaluations of evolving systems. The plan
we will sketch out is intended to be a compromise
between these two opposing positions.
The rest of the note is organised as follows.
Section 2 describes the data we propose to use,
and Section 3 discusses our approach to evaluation
metrics. Section 4 concludes.
2 Data
The data we would use in the task is for the English
? Spanish language pair, and was collected us-
ing two different versions of the MedSLT system3.
In each case, the scenario imagines an English-
speaking doctor conducting a verbal examination
of a Spanish-speaking patient, who was assumed
to be have visited the doctor because they were
displaying symptoms which included a sore throat.
The doctor?s task was to use the translation sys-
tem to determine the likely reason for the patient?s
symptoms.
The two versions of the system differed in
terms of the linguistic coverage offered. The
more restricted version supported a minimal range
of English questions (vocabulary size, about 200
words), and only allowed the patient to respond
using short phrases (vocabulary size, 100 words).
Thus for example the doctor could ask ?How long
have you had a sore throat??, and the patient would
respond Hace dos d??as (?for two days?). The
less restricted version supported a broader range
of doctor questions (vocabulary size, about 450
words), and allowed the patient to respond using
both short phrases and complete sentences (vocab-
ulary size, about 225 words). Thus in response
to ?How long have you had a sore throat??, the
patient could say either Hace dos d??as (?for two
days?) or Tengo dolor en la garganta hace dos d??as
(?I have had a sore throat for two days?).
Data was collected in 64 sessions, carried out
3http://www.issco.unige.ch/projects/
medslt/
over two days in February 2008 at the University
of Texas Medical Center, Dallas. In each session,
the part of the ?doctor? was played by a real physi-
cian, and the part of the ?patient? by a Spanish-
speaking interpreter. This resulted in 1005 En-
glish utterances, and 967 Spanish utterances. All
speech data is available in SPHERE-headed form,
and totals about 90 MB. A master file, organised in
spreadsheet form, lists metadata for each recorded
file. This includes a transcription, a possible valid
translation (verified by a bilingual translator), IDs
for the ?doctor?, the ?patient?, the session and the
system version, and the preceding context. Con-
text is primarily required for short answers, and
consists of the most recent preceding doctor ques-
tion.
3 Evaluation metrics
The job of the evaluation component in the shared
task is to assign a score to each translated utter-
ance. Our basic model will be the usual one for
shared tasks in speech and language. Each pro-
cessed utterance will be assigned to a category;
each category will be associated with a specified
score; the score for a complete testset will the sum
of the scores for all of its utterances. We thus have
three sub-problems: deciding what the categories
are, deciding how to assign a category to a pro-
cessing utterance, and deciding what scores to as-
sociate with each category.
3.1 Defining categories
If the system attempts to translate an utterance,
there are a priori three things that can happen:
it can produce a correct translation, an incorrect
translation, or no translation. Medical speech
translation is a safety-critical problem; a mistrans-
lation may have serious consequences, up to and
including the death of the patient. This implies
that the negative score for an incorrect translation
should be high in comparison to the positive score
for a correct translation. So a naive scoring func-
tion might be ?1 point for a correct translation, 0
points for no translation, ?1000 points for an in-
correct translation.?
However, since the high negative score for a
mistranslation is justified by the possible serious
consequences, not all mistranslations are equal;
some are much more likely than others to result in
clinical consequences. For example, consider the
possible consequences of two different mistrans-
61
lations of the Spanish sentence La penicilina me
da alergias. Ideally, we would like the system to
translate this as ?I am allergic to penicillin?. If it
instead says ?I am allergic to the penicillin?, the
translation is slightly imperfect, but it is hard to see
any important misunderstanding arising as a result.
In contrast, the translation ?I am not allergic to
penicillin?, which might be produced as the result
of a mistake in speech recognition, could have very
serious consequences indeed. (Note in passing that
both errors are single-word insertions). Another
type of result is a nonsensical translation, perhaps
due to an internal system error. For instance, sup-
pose the translation of our sample sentence were
?The allergy penicillin does me?. In this case, it
is not clear what will happen. Most users will
probably dismiss the output as meaningless; a few
might be tempted to try and decipher it, with un-
predictable results.
Examples like these show that it is important for
the scoring metric to differentiate between differ-
ent classes of mistranslations, with the differentia-
tion based on possible clinical consequences of the
error. For similar reasons, it is important to think
about the clinical consequences when the system
produces correct translations, or fails to produce
a translation. For example, when the system cor-
rectly translates ?Hello? as Buenas d??as, there are
not likely to be any clinical consequences, so it is
reasonable to reward it with a lower score than the
one assigned to a clinically contentful utterance.
When no translation is produced, it also seems cor-
rect to distinguish the case where the user was able
recover by a suitably rephrasing the utterance from
the one where they simply gave up. For example,
if the system failed to translate ?How long has this
cough been troubling you??, but correctly handled
the simpler formulation ?How long have you had a
cough??, we would give this a small positive score,
rather than a simple zero.
Summarising, we propose to classify transla-
tions into the following seven categories:
1. Perfect translation, useful clinical conse-
quences.
2. Perfect translation, no useful clinical conse-
quences.
3. Imperfect translation, but not dangerous in
terms of clinical consequences.
4. Imperfect translation, potentially dangerous.
5. Nonsense.
6. No translation produced, but later rephrased
in a way the system handled adequately.
7. No translation produced, but not rephrased in
a way the system handled adequately.
3.2 Assigning utterances to categories
At the moment, medical professionals will only
accept the validity of category assignments made
by trained physicians. In the worst case, it is
clearly true that a layman, even one who has re-
ceived some training, will not be able to determine
whether or not a mistranslation has clinical signif-
icance.
Physician time is, however, a scarce and valu-
able resource, and, as usual, typical case and worst
case may be very different. Particularly for routine
testing during system development, it is clearly not
possible to rely on expert physician assessments.
We consequently suggest a compromise strategy.
We will first carry out an evaluation using medical
experts, in order to establish a gold standard. We
will then repeat this evaluation using non-experts,
and determine how large the differential is in prac-
tice.
We initially intend to experiment with two dif-
ferent groups of non-experts. At Geneva Uni-
versity, we will use students from the School of
Translation. These students will be selected for
competence in English and Spanish, and will re-
ceive a few hours of training on determination of
clinical significance in translation, using guide-
lines developed in collaboration with Glenn Flores
and his colleagues at the UT Southwestern Medi-
cal Center, Texas. Given that the corpus material
is simple and sterotypical, we think that this ap-
proach should yield a useful approximation to ex-
pert judgements.
Although translation students are far cheaper
than doctors, they are still quite expensive, and
evaluation turn-around will be slow. For these rea-
sons, we also propose to investigate the idea of per-
forming evaluations using Amazon?s Mechanical
Turk4. This will be done by Dolores Labs, a new
startup specialising in Turk-based crowdsourcing.
3.3 Scores for categories
We have not yet agreed on exact scores for the
different categories, and this is something that is
4http://www.mturk.com/mturk/welcome
62
probably best decided after mutual discussion at
the workshop. Some basic principles will be evi-
dent from the preceding discussion. The scale will
be normalised so that failure to produce a trans-
lation is counted as zero; potentially dangerous
mistranslations will be associated with a negative
score large in comparison to the positive score for
a useful correct translation. Inability to communi-
cate can certainly be dangerous (this is the point of
having a translation system in the first place), but
mistakenly believing that one has communicated
is usually much worse. As Mark Twain put it: ?It
ain?t what you don?t know that gets you into trou-
ble. It?s what you know for sure that just ain?t so?.
3.4 Discarding uncertain responses
Given that both speech recognition and machine
translation are uncertain technologies, a high
penalty for mistranslations means that systems
which attempt to translate everything may eas-
ily end up with an average negative score - in
other words, they would score worse than a system
which did nothing! For the shared task to be in-
teresting, we must address this problem, and in the
doctor to patient direction there is a natural way
to do so. Since the doctor can reasonably be as-
sumed to be a trained professional who has had
time to learn to operate the system, we can say that
he has the option of aborting any translation where
the machine does not appear to have understood
correctly.
We thus relativise the task with respect to a ?fil-
ter?: for each utterance, we produce both a transla-
tion in the target language, and a ?reference trans-
lation? in the source language, which in some way
gives information about what the machine has un-
derstood. The simplest way to produce this ?ref-
erence translation? is to show the words produced
by speech recognition. When scoring, we evaluate
both translations, and ignore all examples where
the reference translation is evaluated as incorrect.
To go back to the ?penicillin? example, suppose
that Spanish source-language speech recognition
has incorrectly recognised La penicilina me da
alergias as La penicilina no me da alergias. Even
if this produces the seriously incorrect translation
?I am not allergic to penicillin?, we can score it
as a zero rather than a negative, on the grounds
that the speech recognition result already shows
the Spanish-speaking doctor that something has
gone wrong before any translation has happened.
The reference translation may also be produced in
a more elaborate way; a common approach is to
translate back from the target language result into
the source language.
Although the ?filtered? version of the medical
speech translation task makes good sense in the
doctor to patient direction, it is less clear how
meaningful it is in the patient to doctor direction.
Most patients will not have used the system before,
and may be distressed or in pain. It is consequently
less reasonable to expect them to be able to pay at-
tention to the reference translation when using the
system.
4 Summary and conclusions
The preceding notes are intended to form a frame-
work which will serve as a basis for discussion at
the workshop. As already indicated, the key chal-
lenge here is to arrive at metrics which are ac-
ceptable to both the medical and the speech and
language community. This will certainly require
more negotiation. We are however encouraged by
the fact that the proposal, as presented here, has
been developed jointly by representatives of both
communities, and that we appear to be fairly near
agreement. Another important parameter which
we have intentionally left blank is the duration of
the task; we think it will be more productive to de-
termine this based on the schedules of interested
parties.
Realistically, the initial definition of the metric
can hardly be more than a rough guess. Experi-
mentation during the course of the shared task will
probably show that some adjustment will be desir-
able, in order to make it conform more closely to
the requirements of the medical community. If we
do this, we will, in the interests of fairness, score
competing systems using all versions of the metric.
63
