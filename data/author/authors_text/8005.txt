371
372
373
374
375
376
377
378
Quality-Sensitive Test Set Selection for a Speech Translation System 
Fumiaki Sugaya1, Keiji Yasuda2, Toshiyuki Takezawa and Seiichi Yamamoto 
ATR Spoken Language Translation Research Laboratories 
2-2-2 Hikari-dai Seika-cho, Soraku-gun, Kyoto, 619-0288, Japan 
{fumiaki.sugaya, keiji.yasuda, toshiyuki.takezawa,
seiichi.yamamoto}@atr.co.jp
 
 
                                                          
1 
2 
1Current affiliation: KDDI R&D Laboratories. Also at Graduate School of Science and Technology, Kobe University. 
2Also at Graduate School of Engineering, Doshisha University. 
  
Abstract 
We propose a test set selection method to 
sensitively evaluate the performance of a 
speech translation system. The proposed 
method chooses the most sensitive test 
sentences by removing insensitive 
sentences iteratively. Experiments are 
conducted on the ATR-MATRIX speech 
translation system, developed at ATR 
Interpreting Telecommunications 
Research Laboratories. The results show 
the effectiveness of the proposed method. 
According to the results, the proposed 
method can reduce the test set size to less 
than 40% of the original size while 
improving evaluation reliability. 
Introduction 
The translation paired comparison method 
precisely measures the capability of a speech 
translation system.  In this method, native speakers 
compare a system?s translation and the translations, 
made by examinees who have various TOEIC 
scores. The method requires two human costs: the 
data collection of examinees? translations and the 
comparison by native speakers.  In this paper, we 
propose a test set size reduction method that 
reduces the number of test set utterances.  The 
method chooses the most sensitive test utterances 
by removing the most insensitive utterances 
iteratively.    
In section 2, the translation paired comparison 
method is described. Section 3 explains the 
proposed method. In section 4, evaluation results 
for ATR-MATRIX are shown. Section 5 discusses 
the experimental results. In section 6, we state our 
conclusions. 
Translation paired comparison method 
The translation paired comparison method  
(Sugaya, 2000) is an effective evaluation method 
for precisely measuring the capability of a speech 
translation system. In this section, a description of 
the method is given. 
2.1 Methodology of the translation paired 
comparison method 
Figure 1 shows a diagram of the translation paired 
comparison method in the case of Japanese to 
English translation. The Japanese native-speaking 
examinees are asked to listen to Japanese text and 
provide an English translation on paper.  The 
Japanese text is spoken twice within one minute, 
with a pause in-between. To measure the English 
capability of the Japanese native speakers, the 
TOEIC score is used. The examinees are requested 
to present an official TOEIC score certificate 
showing that they have taken the test within the 
past six months. A questionnaire is given to them 
and the results show that the answer time is 
moderately difficult for the examinees. 
The test text is the SLTA1 test set, which 
consists of 330 utterances in 23 conversations from 
a bilingual travel conversation database (Morimoto, 
1994; Takezawa, 1999). The SLTA1 test set is 
                                            Association for Computational Linguistics.
                         Algorithms and Systems, Philadelphia, July 2002, pp. 109-116.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
open for both speech recognition and language 
translation. The answers written on paper are typed. 
In the proposed method, the typed translations 
made by the examinees and the outputs of the 
system are merged into evaluation sheets and are 
then compared by an evaluator who is a native 
English speaker. Each utterance information is 
shown on the evaluation sheets as the Japanese test 
text and the two translation results, i.e., translations 
by an examinee and by the system.  The two 
translations are presented in random order to 
eliminate bias by the evaluator.  The evaluator is 
asked to follow the procedure illustrated in Figure 
2. The four ranks in Figure 2 are the same as those 
used in Sumita (1999). The ranks A, B, C, and D 
indicate: (A) Perfect: no problems in both 
information and grammar; (B) Fair: easy-to-
understand with some unimportant information 
missing or flawed grammar; (C) Acceptable: 
broken but understandable with effort; (D) 
Nonsense: important information has been 
translated incorrectly. 
2.2 Evaluation result using the translation 
paired comparison method 
Figure 3 shows the result of a comparison between 
a language translation subsystem (TDMT) and the 
examinees. The input for TDMT included accurate 
transcriptions. The total number of examinees was 
thirty, with five people having scores in every 
hundred-point TOEIC range between the 300s and 
800s. In Figure 3, the horizontal axis represents the 
TOEIC score and the vertical axis the system 
winning rate (SWR) given by following equation: 
Translation 
Result by 
Human 
Evaluation 
Sheet 
Japanese Test 
Text Typing Paired Comparison 
Accurate Text 
 
 
 
 
where NTOTAL denotes the total number of 
utterances in the test set, NTDMT represents the 
number of  "TDMT won" utterances,  and NEVEN, 
indicates the number of  even (non-winner) 
utterances, i.e., no difference between the results of 
the TDMT and humans. The SWR ranges from 0 
to 1.0, signifying the degree of capability of the 
MT system relative to that of the examinee.  An 
SWR of 0.5 means that the TDMT has the same 
capability as the human examinee. 
Figure 3 shows that the SWR of TDMT is 
greater than 0.5 at TOEIC scores of around 300 
and 400, i.e., the TDMT system wins over humans 
with TOEIC scores of 300 and 400. Examinees, in 
contrast, win at scores of around 800. The 
capability balanced area is around a score of 600 to 
(1)                   
0.5
TOTAL
EVENTDMT
N
NNSWR
?+
=
Figure 1: Diagram of translation pair comparison method 
Japanese-to-English 
Language Translation 
(J-E TDMT) 
Japanese Recognition
(Japanese SPREC) 
Choose A, B, C, or D rank 
 
No 
Same rank?
 
Yes 
Consider naturalness 
 
YesNo 
Same? 
 
Select better result 
 
EVEN 
 
Figure 2: Procedure of comparison 
by native speaker 
 300 400 500 600 700 800 900
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
TO EIC score
SW
R
Figure 3: Evaluation results using translation 
paired comparison method 
Under the above condition, the standard deviation 
of the system's TOEIC score is calculated by 
 
(4)          
)(
)(1
2
2
0
2 ? ?
?
+=
XX
XC
n i
t ?
?
?
 
 
 
where n is the number of examinees, C0 is the 
system's TOEIC score, and X  is the average of 
the examinees' TOEIC scores. Equation (4) 
indicates that the minimum error is given when the 
system's TOEIC score equals the average of the 
examinees' TOEIC scores. 
By using a t-distribution, the confidence 
interval (CI) of the system's TOEIC score with 
confidence coefficient 1-?  is given by 
 700. To precisely determine the balanced point, we 
used regression analysis. The straight line in Figure 
3 is the regression line. The capability balanced 
point between the TDMT subsystem and the 
examinees is 0.5 of SWR. In Figure 3, the exact 
point is a TOEIC score of 708. We call this point 
the system's TOEIC score. Consequently, the 
translation capability of the language translation 
system equals that of the examinees at around a 
score of 700 points on the TOEIC.  
 
 
 
[ ]
(5)                                   )2;
2
( 
   , 00
??=
+?=
ntI
ICICCI
t
?
?
 
In the current study, we employ 0.01 for the 
value of ? .  
2.4 Costs for the translation paired comparison 
method 
The experimental result for ATR-MATRIX, 
which consists of a speech recognition subsystem 
and TDMT, has been also reported (Sugaya, 2000). 
This system?s TOEIC score is 548, where the 
number of speech recognition errors is a factor in 
the degradation of the score. 
The translation paired comparison method is an 
effective evaluation method because it can clearly 
express a system?s performance as a TOEIC (Test 
of English for International Communication)   
score. However, this method has excessive 
evaluation costs.    
Roughly speaking, one of these costs is the need 
to collect translations made by examinees of 
various TOEIC scores. As shown in Equations (4) 
and (5), n, the number of examinees, affects the 
confidence interval of the system?s TOEIC score. 
Therefore, a reduction in this number makes it 
difficult to obtain a reliable evaluation result. 
2.3 Error in the system?s TOEIC score 
The SWR (Yi) and TOEIC scores for the examinees 
(Xi) are assumed to satisfy the population 
regression equation:  
 (2)          ),...,2,1(    21 niXY iii =++= ??? 
The other cost is for the evaluation. Compared 
to a conventional evaluation method, such as a 
simple rank evaluation method, the translation 
paired comparison method uses a larger amount of 
labor because the evaluator must work on n 
evaluation sheets. Each sheet consists of 330 pairs 
of translation results to be evaluated. Even for an 
accomplished evaluator, it takes more than two 
weeks to finish the work, following the method 
explained in section 2.2. 
where 1? and 2? are population regression 
coefficients.  The error term ( i? ) is assumed to 
satisfy the following condition: 
 
 
0    (d)
   if     0),(),(    (c)
(3)                     ,...,2,1     ,)(    (b)
0)(    (a)
22
?
?==
==
=
i
jiji
i
i
jiECov
niV
E
?
????
??
?
 
 
 
 
3 Proposed method 
Yes 
No 
?
?
No
Yes
All candidates 
are calculated?
Set the number of iterations 
 
Remove worst utterances from 
candidates 
Is iteration 
achieved? 
Calculate iteration 
 
Update worst sentence, 
which causes maximum 
iteration 
Get next candidate 
 
As explained in the previous section, the 
translation paired comparison method has an 
excessive evaluation cost. Nevertheless, it is an 
effective evaluation method for measuring the 
capability of a speech translation system. 
Therefore, cost reduction for this evaluation 
method is an important subject for study. 
The proposed method reduces the evaluation 
cost by removing insensitive test utterances from 
the test set. In this section, we explain the 
optimization procedure of the proposed method.  
3.1 Optimization basis 
In the proposed method, the basis of test set 
optimization is the minimization of ? . As shown 
in Equations (4) and (5), this value has an 
influence on the confidence interval of the system's 
TOEIC score. Therefore, minimizing ?  brings 
about a reliable evaluation result.  
We introduce ? iteration, which is calculated in 
each iteration step. ? iteration is also calculated by 
using Equations (2) and (3). The difference 
between ? iteration and?  is the test set to be used 
for calculation. ? iteration is calculated using 
residual test utterances in each iteration step. 
However, the values of 1?  and 2?  are fixed, i.e., 
for the calculation of ? iteration, these 1?  and 2?  
are calculated using the original test set consisting 
of 330 test utterances. 
Optimization is conducted iteratively by 
picking up the test utterance that causes maximum 
?  iteration in each iteration step. The details of this 
procedure is explained in the next subsection. 
3.2 Methodology of the proposed method 
Figure 4 shows a diagram of the proposed method. 
In the first step, the number of iterations is set. 
This number is an actual number of removed test 
utterances. During the iterations, test utterances are 
removed one-by-one. To decide which test 
utterance to remove in each iteration, ? iteration is 
calculated for the condition of removing each test 
utterance. This calculation is done for all 
candidates, i.e., all constituents of residual test 
utterances.  
Figure 4: Procedure of proposed method 
 
At the end of each iteration step, the test 
utterance to be removed is decided. The removed 
test utterance is the one that maximizes ? iteration. 
We regard the utterance as maximizing? iteration if 
removing it from the test set gives minimum 
? iteration. 
70
720
740
760
TO
EI
C
 
sco
re
0 50 100 150 200 250 300
660
680
Iteration
   (upper)  C0 opt  +  Iopt 
  C0 opt
   (lower)  C0 opt   -  Iopt
20
30
40
?
t
 
o
p
t
      Random selection (Averaging of 10 trials)
      Optimzed  (Open)
      Optimzed  (Closed)
0 50 100 150 200 250 300
0
10
Iteration
 
Figure 6: Relationship between iteration 
and ? t opt 
Figure 5: Relationship between iteration  
and system?s TOEIC score 
 As shown in the figure, from iteration 1 to 
iteration 250, the value of C0 opt is stable and does 
not deviate from C0, which is 708. Furthermore, 
until around iteration 200, the value of Iopt 
decreases concurrently with the iteration. 
4 Experimental results 
In this section, we show experimental results of the 
proposed method. Here, we introduce the suffix 
?opt? to distinguish a variable calculated with the 
optimized test set from a variable calculated with 
the original test set. All of the above variables are 
calculated with the original test set. By joining the 
suffix ?opt? to these variables, we refer to variables 
calculated with the optimized test set, e.g., ?  opt 3, 
? t opt, Iopt, C0 opt, CI opt, and so on. 
This result suggests that the proposed may 
provide low-cost evaluation with high reliability. 
 
4.2 Experiment opened for examinees 
In the result shown in the previous subsection, the 
optimization and evaluation were conducted on the 
same examinees, i.e., the evaluation is closed for 
examinees. In this subsection, we look into the 
robustness of the proposed method against 
different examinees. We divided the group, 
consisting of 30 examinees, into two groups: a 
group of odd-numbered examinees and a group of 
even-numbered examinees. Individuals were sorted 
by TOEIC score from lowest to highest.  
4.1 Closed experiment 
This  subsection discusses an experimental result    
obtained for the same test set and examinees 
described in Section 2. Namely, the target test set 
for optimization consists of 330 utterances and the 
number of examinees is 30. 
Figure 5 shows the relationship between 
iteration and the system?s TOEIC score (C0 opt). In 
this figure, the horizontal axis represents the 
iteration number and the vertical axis the TOEIC 
score. The solid line represents C0 opt, which is the 
system?s TOEIC score using the optimized test in 
each iteration. The dotted line above the solid line 
represents the value of C0 opt + Iopt, and the dotted 
line below the solid line C0 opt - Iopt. 
One of the groups is used to optimize the test set. 
The other group is used for the translation paired 
comparison method. We use the term 
?optimization group? to refer to the first group and 
?evaluation group? to refer to the second group. 
Figure 6 shows the relationship between 
iteration and ? t opt. In this figure, the horizontal 
axis represents the iteration and the vertical axis 
shows? t opt. Three kinds of experimental results 
are shown in this figure. In each of three 
experiments, the translation paired comparison is 
conducted by the evaluation group. The differences 
                                                          
3 ? opt is different from? iteration. ? opt is calculated based on 
1?  opt and 2?  opt (not 1?  and 2? ) for the optimized test set.  
Figure 8: Relationship between iteration and 
t opt ?
0 50 100 150 200 250 300
0
5
10
15
20
25
30
Iteration
?
t
 
o
p
t
    Random selection (Averaging of 10 trials)
   Optimzed for TDM T
   Optimzed for ATR-M ATRIX
0 50 100 150 200 250 300
550
60
650
70
750
800
850
Iteration
C 0
 
opt
Random selection (Averaging of 10 trials) 
Optimized (Open)
Optimized (Closed)
Figure 7: Relationship between iteration and 
C0 opt 
among the three experiments are in the group to be 
used for optimization of the test set or the method 
used to reduce it. The double line represents the 
closed result using the test set, optimized on the 
evaluation group. The solid line represents the 
open result using the test set, optimized on the 
optimization group. The broken line represents the 
result using the test set, which is reduced by 
randomly removing test utterances one-by-one. 
The actual plotted broken line is averaged over 10 
random trials.  
0 50 100 150 200 250 300
460
480
500
520
540
560
580
Iteration
C 0
 
opt
   Optimized for TDM T
   Optimzed for ATR-M ATRIX
As shown in Figure 6, in the random selection 
result, t opt is on the rise. On the other hand, the 
open result is on the decline. 
?
Figure 7 shows the relationship between 
iteration and the system?s TOEIC score. In this 
figure, the horizontal axis represents the iteration 
and the vertical axis the TOEIC score. The 
denotation of each line is the same as that in Figure 
6. The error bar from the broken line represents 
? random, which is the standard deviation of the 
system?s TOEIC score over 10 random trials. 
Figure 9: Relationship between iteration and 
C0 opt 
In Figure 7, considering ? random, C0 opt of the 
open evaluation is more approximate to C0 than 
that of random selection, whereas C0 opt of the 
closed evaluation is much more approximate to C0. 
4.3 Experiment on ATR-MATRIX 
To be of actual use, the test set optimized for some 
system must be applicable for evaluation of other 
systems. In this subsection, we show the results of 
an experiment aimed at verifying this requirement 
is met. In this experiment, we apply the test set, 
which is optimized for TDMT, to evaluate ATR-
MATRIX. The experimental conditions are the 
same as in Section 4.1, except for the evaluation 
target. The results are shown in Figure 8 and 
Figure 9. 
Figure 8 shows the relationship between 
iteration and ? t opt. In this figure, the horizontal 
axis represents the iteration and the vertical axis 
shows ? t opt. The double line represents the result 
using the test set, optimized for ATR-MATRIX. 
The solid line represents the result using the test 
set, optimized for TDMT. The broken line 
represents the result using the test set, which is 
reduced by randomly removing test utterances one- 
6 Conclusions by-one. The actual plotted broken line is averaged 
over 10 random trials. 
We proposed a test set selection method for 
evaluating a speech translation system.  This 
method optimizes and drastically reduces the test 
set required by the translation paired comparison 
method. 
Figure 9 shows the relationship between 
iteration and the system?s TOEIC score. In this 
figure, the horizontal axis represents the iteration, 
and the vertical axis TOEIC score. The broken line 
and the solid line are plotted using the same 
denotation as that in Figure 8. Translation paired comparison is an effective 
method for measuring a system?s performance as a 
TOEIC score. However, this method has excessive 
evaluation costs. Therefore, cost reduction for this 
evaluation method is an important subject for study. 
In Figure 8, the solid line always lies on a lower 
position than the broken line. In Figure 9, from 
iteration 1 to around iteration 200, the broken line 
does not deviate from the actual system?s TOEIC 
score, which is 548. We applied the proposed method in an evaluation 
of ATR-MATRIX. Experimental results showed 
the effectiveness of the proposed method. This 
method reduced evaluation costs by more than  
60% and also improved the reliability of the 
evaluation result. 
Considering these results, the test set optimized 
for TDMT is shown to be applicable for evaluating 
ATR-MATRIX. 
5 Discussion 
Acknowledgement In this section, we discuss the experimental results 
shown in Section 4.  
The research reported here was supported in part 
by a contract with the Telecommunications 
Advancement Organization of Japan entitled, "A 
study of speech dialogue translation technology 
based on a large corpus." 
Looking at the broken lines in Figure 6 and 
Figure 8, test set reduction using random selection 
always causes an increase of ? t opt i.e., an increase 
in the scale of confidence interval. Therefore, this 
method causes the reliability of the evaluation 
result to deteriorate. Meanwhile, in the case of 
using the proposed method, looking at the solid 
lines on these figures, ? t opt is on the decline until 
around iteration 200. This means that we can 
achieve a more reliable evaluation result with a 
lower evaluation cost than when using the original 
test set. Here, looking at the solid lines in Figure 7 
and Figure 9, the Co opt system?s TOEIC score is 
nearly stable until iteration 200, and it does not 
deviate from Co. As mentioned before, Co for 
Figure 7 is 708 and Co for Figure 9 is 548. 
References 
Morimoto, T., Uratani, N., Takezawa, T., Furuse, 
O., Sobashima, Y., Iida, H., Nakamura, A., 
Sagisaka, Y., Higuchi, N. and Yamazaki, Y.  
1994. A speech and language database for 
speech translation research. In Proceedings of 
ICSLP `94, pages 1791-1794. 
Sugaya, F., Takezawa, T., Yokoo, A., Sagisaka, Y. 
and Yamamoto, S. 2000. Evaluation of the 
ATR-MATRIX Speech Translation System with 
a Pair Comparison Method between the System 
and Humans. In Proceedings of ICSLP 2000, 
pages 1105-1108. 
Considering these results, the proposed method 
can reduce the 330-utterance test set to a 130- 
utterance test set while reducing the scale of 
confidence interval. In other words, the proposed 
method both reduces evaluation costs by 60% and 
improves  reliability of the evaluation result. 
Sumita, E., Yamada, S., Yamamoto K., Paul, M., 
Kashioka, H., Ishikawa, K. and Shirai, S. 1999. 
Solutions to Problems Inherent in Spoken-
language Translation: The ATR-MATRIX 
Approach. In Proceedings of MT Summit `99, 
pages 229-235. 
Looking at Equations (4) and (5), the scale of 
confidence interval is also influenced by n.  When 
we allow the scale of confidence interval obtained 
from the original test set, we can use the proposed 
method?s reduction effect of ? t to compensate the 
? t 's increase by reducing n.  In this case, the 
actual achievable cost reduction will be more than 
60%.  
Takezawa, T. 1999. Building a bilingual travel 
conversation database for speech recognition 
research. In Proceedings of Oriental COCOSDA 
Workshop, pages 17-20. 
Takezawa, T., Morimoto, T., Sagisaka, Y., 
Campbell, N., Iida., H., Sugaya, F., Yokoo, A. 
and Yamamoto, S. 1998. A Japanese-to-English 
speech translation system: ATR-MATRIX. In 
Proceedings of ICSLP 1998, pages 2779-2782. 
Automatic Measuring of English Language Proficiency using MT
Evaluation Technology
Keiji Yasuda
ATR Spoken Language Translation
Research Laboratories
Department of SLR
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
keiji.yasuda@atr.jp
Fumiaki Sugaya
KDDI R&D Laboratories
2-1-15, Ohara, Kamifukuoka-city,
Saitama, 356-8502, Japan
fsugaya@kddilabs.jp
Eiichiro Sumita
ATR Spoken Language Translation
Research Laboratories
Department of NLR
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
eiichiro.sumita@atr.jp
Toshiyuki Takezawa
ATR Spoken Language Translation
Research Laboratories
Department of SLR
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
toshiyuki.takezawa@atr.jp
Genichiro Kikui
ATR Spoken Language Translation
Research Laboratories
Department of SLR
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
genichiro.kikui@atr.jp
Seiichi Yamamoto
ATR Spoken Language Translation
Research Laboratories
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
seiichi.yamamoto@atr.jp
Abstract
Assisting in foreign language learning is one of
the major areas in which natural language pro-
cessing technology can contribute. This paper
proposes a computerized method of measuring
communicative skill in English as a foreign lan-
guage. The proposed method consists of two
parts. The first part involves a test sentence
selection part to achieve precise measurement
with a small test set. The second part is the ac-
tual measurement, which has three steps. Step
one asks proficiency-known human subjects to
translate Japanese sentences into English. Step
two gauges the match between the translations
of the subjects and correct translations based
on the n-gram overlap or the edit distance be-
tween translations. Step three learns the rela-
tionship between proficiency and match. By re-
gression it finds a straight-line fitting for the
scatter plot representing the proficiency and
matches of the subjects. Then, it estimates pro-
ficiency of proficiency-unknown users by using
the line and the match. Based on this approach,
we conducted experiments on estimating the
Test of English for International Communica-
tion (TOEIC) score. We collected two sets of
data consisting of English sentences translated
from Japanese. The first set consists of 330 sen-
tences, each translated to English by 29 subjects
with varied English proficiency. The second set
consists of 510 sentences translated in a similar
manner by a separate group of 18 subjects. We
found that the estimated scores correlated with
the actual scores.
1 Introduction
For effective second language learning, it is ab-
solutely necessary to test proficiency in the sec-
ond language. This testing can help in selecting
educational materials before learning, checking
learners? understanding after learning, and so
on.
To make learning efficient, it is important to
achieve testing with a short turnaround time.
Computer-based testing is one solution for this,
and several kinds of tests have been developed,
including CASEC (CASEC, 2004) and TOEFL-
CBT (TOEFL, 2004). However, these tests are
mainly based on cloze testing or multiple-choice
questions. Consequently, they require labour
costs for expert examination designers to make
the questions and the alternative ?detractor?
answers.
In this paper, we propose a method for the au-
tomatic measurement of English language pro-
ficiency by applying automatic evaluation tech-
niques. The proposed method selects adequate
test sentences from an existing corpus. Then,
it automatically evaluates the translations of
test sentences done by users. The core tech-
nology of the proposed method, i.e., the auto-
matic evaluation of translations, was developed
in research aiming at the efficient development
of Machine Translation (MT) technology (Su et
al., 1992; Papineni et al, 2002; NIST, 2002).
In the proposed method, we apply these MT
evaluation technologies to the measurement of
human English language proficiency. The pro-
posed method focuses on measuring the commu-
nicative skill of structuring sentences, which is
indispensable for writing and speaking. It does
not measure elementary capabilities including
vocabulary or grammar. This method also pro-
poses a test sentence selection scheme to enable
efficient testing.
Section 2 describes several automatic evalua-
tion methods applied to the proposed method.
Section 3 introduces the proposed evaluation
scheme. Section 4 shows the evaluation results
obtained by the proposed method. Section 5
concludes the paper.
2 MT Evaluation Technologies
In this section, we briefly describe automatic
evaluation methods of translation. These meth-
ods were proposed to evaluate MT output, but
they are applicable to translation by humans.
All of these methods are based on the same
idea, that is, to compare the target transla-
tion for evaluation with high-quality reference
translations that are usually done by skilled
translators. Therefore, these methods require a
corpus of high-quality human reference transla-
tions. We call these translations as ?references?.
2.1 DP-based Method
The DP score between a translation output and
references can be calculated by DP matching
(Su et al, 1992; Takezawa et al, 1999). First,
we define the DP score between sentence (i.e.,
word array) Wa and sentence Wb by the follow-
ing formula.
SDP (Wa,Wb) = T ? S ? I ?DT (1)
where T is the total number of words in Wa, S is
the number of substitution words for comparing
Wa to Wb, I is the number of inserted words for
comparing Wa to Wb, and D is the number of
deleted words for comparing Wa to Wb.
Using Equation 1, (Si(j)), that is, the test
sentence unit DP-score of the translation of test
sentence j done by subject i, can be calculated
by the following formula.
SDPi(j) =
max
k=1 to Nref
{
SDP (Wref(k)(j),Wsub(i)(j)), 0
}
(2)
where Nref is the number of references,
Wref(k)(j) is the k-th reference of the test sen-
tence j, and Wsub(i)(j) is the translation of the
test sentence j done by subject i.
Finally, SDPi , which is the test set unit DP-
score of subject i, can be calculated by the fol-
lowing formula.
SDPi =
1
Nsent
Nsent?
j=1
SDPi(j) (3)
where Nsent is the number of test sentences.
2.2 N-gram-based Method
Papineni et al (2002) proposed BLEU, which is
an automatic method for evaluating MT qual-
ity using N -gram matching. The National Insti-
tute of Standards and Technology also proposed
an automatic evaluation method called NIST
(2002), which is a modified method of BLEU.
In this research we use two kinds of units to
apply BLEU and NIST. One is a test sentence
unit and the other is a test set unit. The unit of
utterance corresponds to the unit of ?segment?
in the original BLEU and NIST studies (Pap-
ineni et al, 2002; NIST, 2002).
Equation 4 is the test sentence unit BLEU
score formulation of the translation of test sen-
tence j done by subject i.
SBLEUi (j) =
exp
{ N?
n=1
wn log(pn)?max
(L?ref
Lsys ? 1, 0
)}
(4)
where
pn =?
C?{Candidates}
?
n?gram?{C} Countclip (n?gram)?
C?{Candidates}
?
n?gram?{C} Count(n?gram)
wn = N?1
and
L?ref = the number of words in the reference
translation that is closest in length to the
translation being scored
Lsys = the number of words in the transla-
tion being scored
Equation 5 is the test sentence unit NIST
score formulation of the translation of test sen-
tence j done by subject i.
SNISTi(j) =
?N
n=1
{?
all w1...wn in sys output
info(w1...wn)?
all w1...wn in sys output
(1)
}
?exp
{
? log2
[
min
(
Lsys
Lref , 1
)]}
(5)
where
info(w1 . . . wn) =
log2
( the number of occurence of w1...wn?1
the number of occurence of w1...wn
)
Lref = the average number of words in a ref-
erence translation, averaged over all refer-
ence translations
Lsys = the number of words in the transla-
tion being scored
and ? is chosen to make the brevity penalty fac-
tor=0.5 when the number of words in the sys-
tem translation is 2/3 of the average number
of words in the reference translation. For Equa-
tions 4 and 7, N indicates the maximum n-gram
length. In this research we set N to 4 for BLEU
and to 5 for NIST.
We may consider the unit of the test set cor-
responding to the unit of ?document? or ?sys-
tem? in BLEU and NIST. However, we use for-
mulations for the test set unit scores that are
different from those of the original BLEU and
NIST.
Calculate correlation 
between TOEIC score and 
sentence unit automatic score
References translated
by bilinguals
English writing by 
proficiency-known
human subjects
English sentences
by proficiency
Japanese test set
Automatic evaluation
(sentence unit evaluation)
Corpus
Select test sentences
based on correlation
Figure 1: Flow of Test Set Selection
The test set unit scores of BLEU and NIST
are calculated by Equations 6 and 7.
SBLEUi =
1
Nsent
Nsent?
j=1
SBLEUi(j) (6)
SNISTi =
1
Nsent
Nsent?
j=1
SNISTi(j) (7)
3 The Proposed Method
The proposed method described in this paper
consists of two parts. One is the test set selec-
tion part and the other is the actual measure-
ment part. The measurement part is divided
into two phases: a parameter-estimation phase
and a testing phase. Here, we use the term ?sub-
jects? to refer to the human subjects in the test
set selection part and the parameter-estimation
phase of the measurement part; we use ?users?
to refer to the humans in the testing phase of
the measurement part.
Regression analysis using
proficiency and automatic
scores
References translated
by bilinguals
English writing by 
proficiency-known 
human subjects
English sentences
by proficiency
Japanese test set
Regression 
coefficient
Automatic evaluation
(Test set unit evaluation)
English writing by a user
Automatic evaluation
Estimation of English
proficiency
English sentences
Automatic score
English
proficiency
?Testing phase?
Corpus
?Parameter-estimation phase?
Figure 2: Flow of English Proficiency Measurment
We employ the Test of English for Interna-
tional Communication (TOEIC, 2004) as an ob-
jective measure of English proficiency.
3.1 Test Sentence Selection Method
Figure 1 shows the flow of the test sentence se-
lection. We first calculate the test sentence
unit automatic score by using Equation 2, 4 or
5 for each test sentence and subject. Second,
for each test sentence, we calculate the correla-
tion between the automatic scores and subjects?
TOEIC scores. Finally, using the above results,
we choose the test sentences that give high cor-
relation.
3.2 Method of Measuring English
Proficiency
Figure 2 shows the flow of measuring English
proficiency. In the parameter-estimation phase,
for each subject, we first calculate the test set
unit automatic score by using Equation 3, 6 or
7. Next, we apply regression analysis using the
automatic scores and subjects? TOEIC scores.
In the testing phase, we calculate a user?s
TOEIC score using the automatic score of the
user and the regression line calculated in the
parameter-estimation phase.
4 Experiments
4.1 Experimental Conditions
4.1.1 Test sets
For the experiments, we employ two differ-
ent test sets. One is BTEC (Basic Travel
Expression Corpus) (Takezawa et al, 2002)
and the other is SLTA1 (Takezawa, 1999).
Both BTEC and SLTA1 are parts of bilingual
corpora that have been collected for research
on speech translation systems. However, they
have different features. A detailed analysis
of these corpora was done by Kikui et al
(2003). Here, we briefly explain these test sets.
In this study, we use the Japanese side as a
test set and the English side as a reference for
automatic evaluation.
BTEC
BTEC was designed to cover expressions for
every potential subject in travel conversation.
This test set was collected by investigating
?phrasebooks? that contain Japanese/English
sentence pairs that experts consider useful for
tourists traveling abroad. One sentence con-
tains 8 words on average. The test set for this
experiment consists of 510 sentences from the
BTEC corpus.
The total number of examinees is 18, and
the range of their TOEIC scores is between the
400s and 900s. Every hundred-point range has
3 examinees.
SLTA1
SLTA1 consists of 330 sentences in 23 conver-
sations from the ATR bilingual travel conver-
sation database (Takezawa, 1999). One sen-
tence contains 13 words on average. This corpus
was collected by simulated dialogues between
Japanese and English speakers through a pro-
fessional interpreter. The topics of the conver-
sations are mainly hotel conversations, such as
reservations, enquiries and so on.
The total number of examinees is 29, and the
range of their TOEIC score is between the 300s
and 800s. Excluding the 600s, every hundred-
point range has 5 examinees.
4.1.2 Reference
For the automatic evaluation, we collected 16
references for each test sentence. One of them
is from the English side of the test set, and the
remaining 15 were translated by 5 bilinguals (3
references by 1 bilingual).
4.2 Experimental Results
4.2.1 Experimental Results of Test Set
Selection
Figures 3 and 4 show the correlation between
the test sentence unit automatic score and the
subjects? TOEIC score. Here, the automatic
score is calculated using Equation 2, 4 or 5. Fig-
ure 3 shows the results on BTEC, and Fig. 4
shows the results on SLTA1. In these fig-
ures, the ordinate represents the correlation.
The filled circles indicate the results using the
DP-based automatic evaluation method. The
gray circles indicate the results using BLEU.
The empty circles indicate the results using
NIST. Looking at these figures, we find that
the three automatic evaluation methods show
a similar tendency. Comparing BTEC and
SLTA1, BTEC contains more cumbersome test
sentences. In BTEC, about 20% of the test sen-
tences give a correlation of less than 0. Mean-
while, in the SLTA1, this percentage is about
10%.
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0 30 60 90 120 150 180 210 240 270 300 330 360 390 420 450 480 510
Test sentence (sorted by correlation)
C
o
r
r
e
l
a
t
i
o
n
DP
BLEU
NIST
Figure 3: Correlation between test sentence unit
automatic scores and subjects? TOEIC scores
(BTEC)
Table 1 shows examples of low-correlated test
sentences. As shown in the table, BTEC con-
tains more short and frequently used expres-
sions than does SLTA1. This kind of expres-
sion is thought to be too easy for testing, so
this low-correlation phenomenon is thought to
occur. SLTA1 still contains a few sentences of
this kind (?Example 1? of SLTA1 in the ta-
ble). Additionally, there is another contributing
factor explaining the low correlation in SLTA1.
Looking at ?Example 2? of SLTA1 in the ta-
ble, this expression is not very easy to translate.
For this test sentence, several expressions can
be produced as an English translation. Thus,
automatic evaluation methods cannot evaluate
correctly due to the insufficient variety of ref-
erences. Considering these results, this method
can remove inadequate test sentences due not
only to the easiness of the test sentence but
also to the difficulty of the automatic evalua-
tion. Figures 5 and 6 show the relationship
between the number of test sentences and cor-
relation. This correlation is calculated between
the test set unit automatic scores and the sub-
jects? TOEIC scores. Here, the automatic score
is calculated using Equation 3, 6 or 7. Figure
5 shows the results on BTEC, and Fig. 6 shows
the results on SLTA1.
In these figures, the abscissa represents the
number of test sentences, i.e., Nsent in Equa-
tions 3, 6 and 7, and the ordinate represents
the correlation. Definitions of the circles are
the same as those in the previous figure. Here,
the test sentence selection is based on the cor-
relation shown in Figs. 3 and 4.
Comparing Fig. 5 to Fig. 6, in the case of
Table 1: Example of low-correlated test sentences
Japanese English
Example 1
???????
Good night.
Example 2
????????????
Can I see a menu, please?
Example 1
??????????????????
Yes, with my Mastercard please
Example 2
???????????????????????
????????????????????????
I wish I could take that but we have a limited budget so
how much will that cost?
S
L
T
A
1
B
T
E
C
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0 30 60 90 120 150 180 210 240 270 300 330
Test sentence (sorted by correlation)
C
o
r
r
e
l
a
t
i
o
n
DP
BLEU
NIST
Figure 4: Correlation between test sentence unit
automatic scores and subjects? TOEIC scores
(SLTA1)
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0 30 60 90 120 150 180 210 240 270 300 330 360 390 420 450 480 510
Number of test sentences
C
o
r
r
e
l
a
t
i
o
n
DP
BLEU
NIST
Figure 5: Correlation between test set unit
automatic scores and subjects? TOEIC scores
(BTEC)
using the full test set (510 test sentences for
BTEC, 330 test sentences for SLTA1), the cor-
relation of BTEC is lower than that of SLTA1.
As we mentioned above, the ratio of the low-
correlated test sentences in BTEC is higher than
that of SLTA1 (See Figs. 3 and 4). This issue
is thought to cause a decrease in the correlation
shown in Fig. 5. However, by applying the se-
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0 30 60 90 120 150 180 210 240 270 300 330
Number of test sentences
C
o
r
r
e
l
a
t
i
o
n
DP
BLEU
NIST
Figure 6: Correlation between test set unit
automatic scores and subjects? TOEIC scores
(SLTA1)
50
100
150
200
250
300
350
0 30 60 90 120 150 180 210 240 270 300 330 360 390 420 450 480 510
Number of test sentences
S
t
a
n
d
a
r
d
 
e
r
r
o
r
DP
BLEU
NIST
Figure 7: Standard error (BTEC)
lection based on sentence unit correlation, these
obstructive test sentences can be removed. This
permits the selection of high-correlated small-
sized test sets. In these figures, the highest cor-
relations are around 0.95.
4.2.2 Experimental Results of English
Proficiency Measurement
For the experiments on English proficiency mea-
surement, we carried out a leave-one-out cross
validation test. The leave-one-out cross valida-
50
100
150
200
250
300
350
0 30 60 90 120 150 180 210 240 270 300 330
Number of test sentences
S
t
a
n
d
a
r
d
 
e
r
r
o
r
DP
BLEU
NIST
Figure 8: Standard error (SLTA1)
tion test is conducted not only for the measure-
ment of the English proficiency but also for the
test set selection.
To evaluate the proficiency measurement by
the proposed method, we calculate the standard
error of the results of a leave-one-out cross val-
idation test. The following formula is the defi-
nition of the standard error.
?E =
???? 1
Nuser
Nuser?
i=1
(Ti ?Ai)2 (8)
where Nuser is the number of users, Ti is the
actual TOEIC score of user i, and Ai is user i?s
estimated TOEIC score by using the proposed
method.
Figures 7 and 8 show the relationship between
the number of test sentences and the standard
error.
In these figures, the abscissa represents the
number of test sentences, and the ordinate rep-
resents the standard error. Definitions of the
circles are the same as in the previous figure.
Here, the test sentence selection is based on the
correlation shown in Figs. 3 and 4.
Looking at Figs. 7 and 8, we can observe dif-
ferences between the standard errors of BTEC
and SLTA1. This is thought to be due to the
difference of the number of subjects in the ex-
periments (for the leave-one-out cross valida-
tion test, 17 subjects with BTEC and 28 sub-
jects with SLTA1). Even though these were
closed experiments, the results in Figs. 5 and
6 show an even higher correlation with BTEC
than with SLTA1 at the highest point. There-
fore, there is room for improvement by increas-
ing the number of subjects with BTEC.
In the test using 30 to 60 test sentences in
Figs. 7 and 8, the standard errors are much
smaller than in the test using the full test set
(510 test sentences for BTEC, 330 test sentences
for SLTA1). These results imply that the test
set selection works very well and that it enables
precise testing using a smaller size test set.
5 Conclusion
We proposed an automatic measurement
method for English language proficiency. The
proposed method applies automatic MT evalu-
ation to measure human English language pro-
ficiency. This method focuses on measuring the
communicative skill of structuring sentences,
which is indispensable in writing and speaking.
However, it does not measure elementary capa-
bilities such as vocabulary and grammar. The
method also involves a new test sentence selec-
tion scheme to enable efficient testing.
In the experiments, we used TOEIC as an ob-
jective measure of English language proficiency.
We then applied some currently available auto-
matic evaluation methods: BLEU, NIST and a
DP-based method. We carried out experiments
on two test sets: BTEC and SLTA1. Accord-
ing to the experimental results, the proposed
method gave a good measurement result on a
small-sized test set. The standard error of mea-
surement is around 120 points on the TOEIC
score with BTEC and less than 100 TOEIC
points score with SLTA1. In both cases, the
optimum size of the test set is 30 to 60 test sen-
tences.
The proposed method still needs human
labour to make the references. To obtain higher
portability, we will apply an automatic para-
phrase scheme (Finch et al, 2002; Shimohata
and Sumita, 2002) to make the references auto-
matically.
6 Acknowledgements
The research reported here was supported in
part by a contract with the National Institute
of Information and Communications Technol-
ogy entitled ?A study of speech dialogue trans-
lation technology based on a large corpus?.
References
CASEC. 2004. Computer Assessment
System for English Communication.
http://www.ets.org/toefl/.
A. Finch, T. Watanabe, and E. Sumita. 2002.
?Paraphrasing by Statistical Machine Trans-
lation?. In Proceedings of the 1st Forum on
Information Technology (FIT2002), volume
E-53, pages 187?188.
G. Kikui, E. Sumita, T. Takezawa, and
S. Yamamoto. 2003. ?Creating Corpora for
Speech-to-Speech Translation?. In Proceed-
ings of EUROSPEECH, pages 381?384.
NIST. 2002. Automatic Evaluation
of Machine Translation Quality Us-
ing N-gram Co-Occurence Statistics.
http://www.nist.gov/speech/tests/mt
/mt2001/resource/.
K. Papineni, S. Roukos, T. Ward, and W.-
J. Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In
Proceedings of the 40th Annual Meeting of
the Association for Computational Linguis-
tics (ACL), pages 311?318.
M. Shimohata and E. Sumita. 2002. ?Auto-
matic Paraphrasing Based on Parallel Corpus
for Normalization?. In Proceedings of Inter-
national Conference on Language Resources
and Evaluation (LREC), pages 453?457.
K.-Y. Su, M.-W. Wu, and J.-S. Chang. 1992.
A new quantitative quality measure for ma-
chine translation systems. In Proceedings of
the 14th International Conference on Com-
putational Linguistics(COLING), pages 433?
439.
T. Takezawa, F. Sugaya, A. Yokoo, and S. Ya-
mamoto. 1999. A new evaluation method for
speech translation systems and a case study
on ATR-MATRIX from Japanese to English.
In Proceeding of Machine Translation Summit
(MT Summit), pages 299?307.
T. Takezawa, E. Sumita, F. Sugaya, H. Ya-
mamoto, and S. Yamamoto. 2002. ?Toward a
Broad-Coverage Bilingual Corpus for Speech
Translation of Travel Conversations in the
Real World?. In Proceedings of International
Conference on Language Resources and Eval-
uation (LREC), pages 147?152.
T. Takezawa. 1999. Building a bilingual travel
conversation database for speech translation
research. In Proceedings of the 2nd Inter-
national Workshop on East-Asian Language
Resources and Evaluation ? Oriental CO-
COSDA Workshop ?99 ?, pages 17?20.
TOEFL. 2004. Test of English as a Foreign
Language. http://www.ets.org/toefl/.
TOEIC. 2004. Test of English
for International Communication.
http://www.ets.org/toeic/.
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 205?208,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Automatic Compilation of Travel Information                                                                
from Automatically Identified Travel Blogs 
 
Hidetsugu Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 27?35,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Classification of Research Papers into a Patent Classification System 
Using Two Translation Models 
 
 
Hidetsugu anba 
Hiroshima City University  
3-4-1 Ozukahigashi, Hiroshima 731-
3194 Japan 
nanba@hiroshima-cu.ac.jp 
Toshiyuki Takezawa 
Hiroshima City University 
3-4-1 Ozukahigashi, Hiroshima 731-
3194 Japan 
takezawa@hiroshima-
cu.ac.jp 
 
 
 
Abstract 
Classifying research papers into patent classi-
fication systems enables an exhaustive and ef-
fective invalidity search, prior art search, and 
technical trend analysis. However, it is very 
costly to classify research papers manually. 
Therefore, we have studied automatic classifi-
cation of research papers into a patent classifi-
cation system. To classify research papers into 
patent classification systems, the differences in 
terms used in research papers and patents 
should be taken into account. This is because 
the terms used in patents are often more ab-
stract or creative than those used in research 
papers in order to widen the scope of the 
claims. It is also necessary to do exhaustive 
searches and analyses that focus on classifica-
tion of research papers written in various lan-
guages. To solve these problems, we propose 
some classification methods using two ma-
chine translation models. When translating 
English research papers into Japanese, the per-
formance of a translation model for patents is 
inferior to that for research papers due to the 
differences in terms used in research papers 
and patents. However, the model for patents is 
thought to be useful for our task because trans-
lation results by patent translation models tend 
to contain more patent terms than those for re-
search papers. To confirm the effectiveness of 
our methods, we conducted some experiments 
using the data of the Patent Mining Task in the 
NTCIR-7 Workshop. From the experimental 
results, we found that our method using trans-
lation models for both research papers and pa-
tents was more effective than using a single 
translation model. 
1 Introduction 
Classification of research papers into patent clas-
sification systems makes it possible to conduct 
an exhaustive and effective prior art search, inva-
lidity search, and technical trend analysis. How-
ever, it would be too costly and time-consuming 
to have the research paper's authors or another 
professional classify such documents manually. 
Therefore, we have investigated the classification 
of research papers into a patent classification 
system. 
In previous studies, classification of patents 
was conducted as subtasks in the 5th and 6th 
NTCIR workshops (Iwayama et al, 2005; 
Iwayama et al, 2007). In these subtasks, partici-
pants were asked to classify Japanese patents 
using the File Forming Term (F-term) system, 
which is a classification system for Japanese pa-
tents. Here, we have focused on the classification 
of research papers, and we need to take into ac-
count the differences in terms used in research 
papers and patents because the terms used in pa-
tents are often more abstract or creative than 
those used in research papers in order to widen 
the scope of the claims. For example, the scho-
larly term "machine translation" can be ex-
pressed as "automatic translation" or "language 
conversion" in patent documents. In addition to 
taking the differences of genres into account, it is 
necessary to do exhaustive searches and analyses 
focusing on the classification of research papers 
written in various languages.  
To solve these problems, we propose some 
classification methods using two machine trans-
lation models. When translating English research 
papers into Japanese, the performance of a trans-
lation model for patents is generally inferior to 
that for research papers, because the terms used 
27
in patents are different from those in research 
papers. However, we thought that a translation 
model for patents might be useful for our task, 
because translation results using the patent trans-
lation model tend to contain more patent terms 
than those obtained using the model for research 
papers. In this paper, we confirm the effective-
ness of our methods using the data of the Cross-
genre Subtask (E2J) in the 7th NTCIR Workshop 
(NTCIR-7) Patent Mining Task (Nanba et al, 
2008:b). 
The remainder of this paper is organized as 
follows. Section 2 describes related work. Sec-
tion 3 describes our methods. To investigate the 
effectiveness of our methods, we conducted 
some experiments, and Section 4 reports the ex-
perimental results. We present some conclusions 
in Section 5. 
2 Related Work 
In this section, we describe some related studies 
on "cross-genre information access" and "cross-
lingual information access". 
Cross-genre Information Access 
Much research has been done in the field of 
cross-genre information retrieval and document 
classification. The technical survey task in the 
NTCIR-3 workshop (Iwayama et al, 2002) is an 
example. This task aimed to retrieve patents re-
levant to a given newspaper article. In this task, 
Itoh et al (2002) focused on "Term Distillation". 
The distribution of the frequency of the occur-
rence of words was known to be different be-
tween newspaper articles and patents. For exam-
ple, the word "president" often appears in news-
paper articles, while this word seldom appears in 
patents. As a result, unimportant words such as 
"president" were assigned high scores in patents 
when using tf*idf to weight words. Term Distil-
lation is a technique that can prevent such cases 
by filtering out words that can be assigned incor-
rect weights. This idea was also used to link 
news articles and blog entries (Ikeda et al, 2006). 
Another approach for cross-genre information 
retrieval was that used by Nanba et al (2008:a), 
who proposed a method to integrate a research 
paper database and a patent database by analyz-
ing citation relations between research papers 
and patents. For the integration, they extracted 
bibliographic information of cited literature in 
"prior art" fields in Japanese patent applications. 
Using this integrated database, users can retrieve 
patents that relate to a particular research paper 
by tracing citation relations between research 
papers and patents. However, the number of 
cited papers among patent applications is not 
sufficient to retrieve related papers or patents, 
even though the number of opportunities for cit-
ing papers in patents or for citing patents in pa-
pers has been increasing recently. 
As another approach for cross-genre informa-
tion retrieval, Nanba et al (2009) proposed a 
method to paraphrase scholarly terms into patent 
terms (e.g., paraphrasing "floppy disc" into 
"magnetic recording medium"). They focused on 
citation relationships between research papers 
and patents for the paraphrased terms. Generally, 
a research paper and a patent that have a citation 
relationship tend to be in the same research field. 
Therefore, they paraphrased a scholarly term into 
a patent term in two steps: (1) retrieve research 
papers that contain a given scholarly term in their 
titles, and (2) extract patent terms from patents 
that have citation relations with the retrieved pa-
pers. 
The NTCIR-7 Patent Mining Task (Nanba et 
al., 2008:b) is another example of research done 
on information access using research papers and 
patents. The aim of the Patent Mining Task was 
to classify research papers written in either Japa-
nese or English using the International Patent 
Classification (IPC) system, which is a global 
standard hierarchical patent classification system. 
The following four subtasks were included in 
this task, and 12 groups participated in three of 
them: Japanese, English, and Cross-lingual (J2E) 
subtasks. 
 Japanese subtask: classification of Japa-
nese research papers using patent data writ-
ten in Japanese. 
 English subtask: classification of English 
research papers using patent data written in 
English. 
 Cross-lingual subtask (J2E): classification 
of Japanese research papers using patent da-
ta written in English. 
 Cross-lingual subtask (E2J): classification 
of English research papers using patent data 
written in Japanese. 
Because the number of categories (IPC codes) 
that research papers were classified into was very 
large (30,855), only two participating groups 
employed machine learning, which is the most 
standard approach in the NLP field. The other 
groups used the k-Nearest Neighbor (k-NN) me-
thod. Among all participant groups, only Mase 
and Iwayama's group (2008) coped with the 
problem of the differences in terms between re-
28
search papers and patents. Mase and Iwayama 
used a pseudo-relevance feedback method to col-
lect related patent terms for a given research pa-
per. First, they retrieved patents relevant to a 
given research paper. Next, they extracted patent 
terms from the top n retrieved patents. Then they 
retrieved patents again using the patent terms 
extracted in the second step. Finally, they classi-
fied research papers using the k-NN method. 
However, they reported that a simple k-NN 
based method was superior to the method based 
on the pseudo-relevance feedback method. In 
this paper, we also examined our methods using 
the data of the NTCIR-7 Patent Mining Task.  
TREC Chemistry Track 1  is another related 
study involving research papers and patents. This 
track aims for cross-genre information retrieval 
using research papers and patents in the chemical 
field. This track started in 2009 under the Text 
Retrieval Conference (TREC), and the details 
including experimental results will be reported at 
the final meeting to be held in November 2009. 
Cross-lingual Information Access 
Much research has been done on cross-lingual 
information access using research papers and 
patents. In the NTCIR workshop, cross-lingual 
information retrieval tasks have been carried out 
using research papers (Kando et al, 1999; Kando 
et al, 2001) and patents (Fujii et al, 2004; Fujii 
et al, 2005; Fujii et al, 2007). In the CLEF 
evaluation workshop, the cross-lingual patent 
retrieval task "CLEF-IP" was initiated in 20092. 
The cross-lingual subtask in the NTCIR-7 Patent 
Mining Task (Nanba et al, 2008:b) is another 
cross-lingual information access study. 
Here, we describe two methods used in the 
cross-lingual subtask (J2E) in the Patent Mining 
Task (Bian and Teng, 2008, Clinchant and Rend-
ers, 2008). Bian and Teng (2008) translated Jap-
anese research papers into English using three 
online translation systems (Google, Excite, and 
Yahoo! Babel Fish), and classified them using a 
k-NN-based text classifier. Clinchant and Rend-
ers (2008) automatically obtained a Japanese-
English bilingual dictionary from approximately 
300,000 pairs of titles from Japanese and English 
research papers (Kando et al, 1999) using Giza3, 
a statistical machine translation toolkit. Then 
                                                 
1 https://wiki.ir-
facility.org/index.php/TREC_Chemistry_Track 
2 http://www.ir-facility.org/the_irf/current-
projects/clef-ip09-track/ 
3 http://www.fjoch.com/GIZA++.html 
they classified papers using this dictionary and a 
k-NN-based document classifier. Bian and Clin-
chant also participated in an English subtask and 
obtained almost the same mean average precision 
(MAP) scores as those of the J2E subtask. 
Although the direction of translation of our 
system is different from Bian and Clinchant, we 
also tried our methods using the data of the 
cross-lingual subtask (E2J). We utilized the Giza 
toolkit in the same way as Clinchant, but our ap-
proach was different from Clinchant, because we 
solved the problem of "differences of terms used 
in research papers and patents" by using two 
translation models obtained from both research 
papers and patents parallel corpora. 
3 Classification of Research Papers into 
a Patent Classification System 
3.1 Our Methods 
We explain here the procedure of our cross-genre, 
cross-lingual document classification method 
depicted in Figure 1. The goal of our task is to 
classify document I written in language L1 in 
genre G1 into a classification system (categories) 
using documents written in language L2 in genre 
G2, and classification codes were manually an-
notated to each of these documents. Generally, 
three steps are required for cross-genre, cross-
lingual document classification: (1) translate 
document I into Language L2 using a translation 
model for genre G1 (document O in Figure 1), 
(2) paraphrase terms in document O into terms in 
genre G2 (document O'), and (3) classify O' into 
a classification system. Here, if a translation 
model for genre G2 is available, steps (1) and (2) 
can be resolved using this translation model, be-
cause terms in the translation results using the 
model are more appropriate in genre G2. How-
ever, as it is assumed that the translation model 
translates documents in genre G2, the translation 
results might contain more mistranslations than 
the results obtained by a model for genre G1. We 
therefore combine translation results (O+O') 
produced by translation models for genre G1 and 
for G2. These results can be expected to contain 
terms in genre G2 and to minimize the effects of 
mistranslation by using the translation model for 
genre G1. 
29
 
 
Figure 1: Overview of our method 
3.2 System Configuration 
The goal of our study is to classify English re-
search papers (Language L1=English, Genre 
G1=research papers) into a patent classification 
using a patent data set written in Japanese (Lan-
guage L2=Japanese, Genre G2=patents). Figure 
2 shows the system configuration. Our system is 
comprised of a "Japanese index creating module" 
and a "document classification module". In the 
following, we explain both modules. 
 
Figure 2: System configuration 
Japanese Index Creating Module 
When a title and abstract pair, as shown in Figure 
3, is given, the module creates a Japanese index, 
shown in Figure 44, using translation models for 
research papers and for patents. 
Here, the following two procedures (A) or (B) 
are possible for creating a Japanese index from 
an English paper: (A) translate the English title 
and abstract into Japanese; then create a Japanese 
                                                 
4 Numerical values shown with index terms indicate 
term frequencies. 
index from them by extracting content terms5, or 
(B) create an English index6  from the English 
title and abstract, then translate each index term 
into Japanese. We conducted experiments using 
both procedures. 
As translation tools, we used Giza and Moses7. 
We obtained translation models using a patent 
bilingual corpus containing 1,800,000 pairs of 
sentences (Fujii et al 2008) and a research paper 
bilingual corpus containing 300,000 pairs auto-
matically created from datasets of NTCIR-1 
(Kando et al 1999), and 2 (Kando et al 2001) 
CLIR tasks. 
Title: A Sandblast-Processed Color-PDP Phos-
phor Screen 
Abstract: Barrier ribs in the color PDP have 
usually been fabricated by multiple screen print-
ing. However, the precise rib printing of fine pat-
terns for the high resolution display panel is dif-
ficult to make well in proportion as the panel size 
grow larger. On the other hand, luminance and 
luminous efficiency of reflective phosphor 
screen will be expected to increase when the 
phosphor is deposited on the inner wall of dis-
play cells. Sandblasting technique has been ap-
plied to make barrier ribs for the high resolution 
PDP and nonffat phosphor screens on the inner 
wall of display cells. 
Figure 3: Example of an English title and abstract 
18 ?? (formation) 
18 ??? (PDP) 
18 ???? (type phosphor screen) 
12 ???? (barrier formation) 
12 ?? (barrier) 
12 ?? (phosphor) 
12 ?????? (color PDP) 
12 ????? (reflective phosphor) 
12 ??? (type phosphor) 
12 ???????? (Sandblasting technique) 
9 ??????? (Sandblasting) 
(snip) 
Figure 4: Example of a Japanese index 
                                                 
5 As content terms, we extracted noun phrases (series 
of nouns), adjectives, and verbs using the Japanese 
morphological analyzer MeCab. 
(http://mecab.sourceforge.net) 
6 We used TreeTagger as a POS tagging tool. 
(http://www.ims.uni-
stuttgart.de/projekte/corplex/TreeTagger/) 
7 http://www.statmt.org/moses/ 
Lang.L1 
Lang.L2 
Genre G2 Genre G1 
 
I 
 
O 
 
O' 
s ?
?(1) 
(2) 
Our method 
Japanese Index 
Creating Module 
Document Classifica-
tion Module 
Patent Data written 
in Japanese 
List of IPC Codes 
O
ur system
 
English paper 
Correct data 
Comparison 
and Evaluation 
30
We used two phrase tables for research papers 
and patents when translating English index terms 
into Japanese. For a given English term, we se-
lected the Japanese term with the highest transla-
tion probability from the candidates in each table. 
These tables were automatically obtained in the 
process of constructing translation models for 
research papers and patents using Giza and Mos-
es. However, there are several other ways to 
translate index terms, such as using bilingual 
dictionaries of technical terms or compositional 
semantics (Tonoike et al, 2007), we employed a 
phrase table-based method because the effective-
ness of this method was experimentally con-
firmed by Itakagi et al (2007). In addition to this 
method, we also investigated using bilingual dic-
tionaries of technical terms as baseline methods. 
Details of these methods are in Section 4.2. 
Document Classification Module 
We used Nanba's k-NN-based system (Nanba, 
2008:c) for a Japanese subtask as a document 
classification module in our system. This module 
uses a patent retrieval engine (Nanba, 2007) 
which was developed for the NTCIR-6 Patent 
Retrieval Task (Fujii et al, 2007). This engine 
introduced the Vector Space Model as a retrieval 
model, SMART (Salton, 1971) for term weight-
ing, and noun phrases (sequence of nouns), verbs, 
and adjectives for index terms. The classification 
module obtained a list of IPC codes using the 
following procedure. 
1. Retrieve top 170 results using the patent 
retrieval engine for a given research paper. 
2. Extract IPC codes with relevance scores for 
the query from each retrieved patent in step 
1. 
3. Rank IPC codes using the following equa-
tion. 
                          n 
 Score(X) = ? Relevance score of each patent 
                  i=1 
Here, X and n indicate the IPC code and the 
number of patents that X was assigned to within 
the top 170 retrieved patents, respectively. Nanba 
determined the value of 170 using the dry run 
data and the training data of the NTCIR-7 Patent 
Mining Task. 
3.3 Classification of Research Papers into 
International Patent Classification 
(IPC) 
As a patent classification system for classifica-
tion of research papers, we employed the Interna-
tional Patent Classification (IPC) system. The 
IPC system is a global standard hierarchical pa-
tent classification system. The sixth edition of 
the IPC contains more than 50,000 classes at the 
most detailed level8. The goal of our task was to 
assign one or more of these IPC codes at the 
most detailed level to a given research paper. 
4 Experiments 
To investigate the effectiveness of our method, 
we conducted some experiments. Section 4.1 
describes the experimental procedure. Section 
4.2 explains several methods that were compared 
in the experiments. Section 4.3 reports the expe-
rimental results, and Section 4.4 discusses them. 
4.1 Experimental Method 
We conducted some experiments using the data 
of the cross-lingual subtask (E2J) in the NTCIR-
7 Patent Mining Task. 
Correct data set 
We used a data set for the formal run of the 
cross-lingual subtask in the NTCIR-7 Patent 
Mining Task (Nanba, et al, 2008). In the data set, 
IPC codes were manually assigned to each 879 
topics (research papers). For each topic, an aver-
age of 2.3 IPC codes was manually assigned. 
These correct data were compared with a list of 
IPC codes 9  by systems, and the systems were 
evaluated in terms of MAP (mean average preci-
sion). Here, the 879 topics were divided into two 
groups: group A, in which highly relevant IPC 
codes were assigned to 473 topics, and group B, 
in which relevant IPC codes were assigned to 
406 topics. In our experiment, we evaluated sev-
eral systems in two ways: using group A only 
and using both groups.  
Document Sets 
An overview of document sets used in our expe-
riments is in Table 1. In the unexamined Japa-
nese patent applications, manually assigned IPC 
codes are included together with full text patent 
data. These data were utilised to apply the k-NN 
method in our document classification module. 
NTCIR-1 and 2 CLIR Task test collections were 
used to obtain a translation model for research 
papers, which we mentioned in Section 3.2. 
                                                 
8 Among 50,000 classes, 30,855 classes relevant to 
academic fields were used in the NTCIR-7 Patent 
Mining Task. 
9 The maximum number of IPC codes allowed to be 
output for a single topic was 1,000. 
31
Table 1: Document sets 
4.2 Alternatives 
We conducted examinations using seven baseline 
methods, three proposed methods, and two up-
per-bound methods shown as follows. In the fol-
lowing, "SMT(X)" is a method to create a Japa-
nese index after translating research papers using 
a translation model X. "Index(X)" is a method to 
create an English index, and to translate the in-
dex terms using a phrase table for translation 
model X. 
Baseline methods 
 SMT(Paper): Create a Japanese index after 
translating research papers using a transla-
tion model for research papers. 
 SMT(Patent): Create a Japanese index after 
translating research papers using a model 
for patents. 
 Index(Paper): First create an English index, 
then translate the index terms into Japanese 
using a phrase table for research papers. 
 Index(Patent): First create an English index, 
then translate the index terms into Japanese 
using a phrase table for patents. 
 SMT(Paper)+Hypernym: Paraphrase index 
terms created from "SMT(Paper)" by their 
hypernyms using a hypernym-hyponym the-
saurus. 
 Index(TechDic): Translate English index 
terms using a Japanese-English dictionary 
consisting of 450,000 technical terms10. 
 Index(EIJIRO): Translate English index 
terms using EIJIRO 11 , a Japanese-English 
dictionary consisting of more than 
1,000,000 pairs of terms. 
Our methods 
 Index(Paper)*Index(Patent): Product set of 
"Index(Paper)" and "Index(Patent)". 
 Index(Paper)+Index(Patent): Union of "In-
dex(Paper)" and "Index(Patent)". 
                                                 
10 "Kagakugijutsu 45 mango taiyakujiten" Nichigai 
Associates, Inc., 2001. 
11 http://www.eijiro.jp/ 
 SMT(Paper)+Index(Patent): Union of 
"SMT(Paper)" and "Index(Patent)". 
Upper-bound methods 
 Japanese subtask: This is the same as the 
Japanese subtask in the NTCIR-7 Patent 
Mining Task. For this subtask, Japanese re-
search papers, which are manual (ideal) 
translations of corresponding English papers, 
are input into a system.  
 Japanese subtask+Index(Patent): Union of 
"Japanese subtask" and "Index(Patent)". 
Another reason for using the baseline methods is 
that the terms used in patents are often more ab-
stract or creative than those used in research pa-
pers, as mentioned in Section 1. Therefore, we 
paraphrased index terms in SMT(Paper) by their 
hypernyms using a hypernym/hyponym thesau-
rus (Nanba, 2007). Nanba automatically created 
this thesaurus consisting of 1,800,000 terms from 
10 years of unexamined Japanese patent applica-
tions using a set of patterns, such as "NP0 ya NP1 
nadono NP2 (NP2 such as NP0 and NP1)" (Hearst, 
1992).  
4.3 Experimental Results 
Experimental results are given in Table 2. From 
the results, we can see that "SMT(Paper)" ob-
tained the highest MAP scores when using topics 
in group A+B and in group A. Of the 10 methods 
used (except for the upper-bound methods), our 
method "SMT(Paper)+Index(Patent)" obtained 
the highest MAP score. 
4.4 Discussion 
Difference of terms between research and pa-
tents (Comparison of "Index(Paper)" and 
"Index(Patent)") 
Although the quality of phrase tables for research 
papers ("Index(Paper)") and patents  ("In-
dex(Patent)") was not very different, the MAP 
score of "Index(Paper)" was 0.01 better than that 
of "Index(Patent)". To investigate this gap, we 
compared Japanese indices by "Index(Paper)" 
and "Index(Patent)". There were 69,100 English 
index terms in total, and 47,055 terms 
(47,055/69,100=0.681) were translated by the 
model for research papers, while 40,427 terms 
(40,427/69,100=0.585) were translated by the 
model for patents. Ten percent of this gap indi-
cates that terms used in research papers and in 
patents are different, which causes the gap in 
MAP scores of "Index(Patent)" and "In-
dex(Paper)". 
Data Year Size o.  Lang. 
Unexamined 
Japanese 
patent appli-
cations 
1993
-
2002 
100 
GB 
3.50 
M 
Japanese 
NTCIR-1 
and 2 CLIR 
Task 
1988
-
1999 
1.4 
GB 
0.26 
M 
Japanese 
/English 
32
Combination of "Index(Paper)" and "In-
dex(Patent)" 
When a term translated by the model for research 
papers matches a term translated by the model 
for patents, they seem to be a correct translation. 
Therefore, we examined "In-
dex(Paper)*Index(Patent)". The method uses 
terms as an index when translation results by 
both models match. From the experimental re-
sults, this method obtained 0.1830 and 0.2230 of 
MAP scores when using topics in group A+B 
and in group A, respectively. These results indi-
cate that the overlap of lexicons between re-
search papers and patents is relatively large, and 
terms in this overlap are effective for our task. 
However, the MAP score of "In-
dex(Paper)*Index(Patent)" was 0.02 lower than 
"Index(Paper)" and "Index(Patent)", which indi-
cates that there are not enough terms in the over-
lap for our task. 
In addition to "Index(Paper)*Index(Patent)", 
we also examined "Index(Paper)+Index(Patent)", 
which is a union of "Index(Paper)" and "In-
dex(Patent)". From the experimental results, we 
obtained respective MAP scores of 0.2258 and 
0.2596 when using topics in group A+B and in 
group A. These scores are 0.01 to 0.02 higher 
than the scores of "Index(Paper)" and "In-
dex(Patent)". These encouraging results indicate 
that our method using two translation models is 
effective for a cross-genre document classifica-
tion task. 
Effectiveness of "SMT(Paper) 
+Index(Patent)" 
In addition to "Index(Paper)", "SMT(Paper)" 
also obtained high MAP scores. Therefore, we 
combined "Index(Patent)" with "SMT(Paper)" 
instead of "Index(Paper)". From the experimental 
results, we found that this approach 
("SMT(Paper)+Index(Patent)") produced MAP 
scores of 0.2633 when using topics in group 
A+B and 0.2807 when using topics in group A. 
These scores were the highest of all, almost ap-
proaching the results of upper-bound methods. 
Comparison of "Index(TechDic)", "In-
dex(EIJIRO)", "Index(Paper)", and "In-
dex(Patent)" 
Both "Index(TechDic)" and "Index(EIJIRO)" 
were worse than "Index(Paper)" and "In-
dex(Patent)" by more than 0.05 in the MAP 
scores. These results were due to the lower num-
ber of terms translated by each method. Because 
phrase tables for research papers and patents 
were automatically created, they were not as cor-
rect as "TechDic" and "EIJIRO". However, the 
phrase tables were able to translate more English 
terms into Japanese in comparison with "Tech-
Dic" (30,008/69,100=0.434) and "EIJIRO" 
(37607/69,100=0.544), and these induced the 
difference of MAP scores. 
Comparison of "SMT(Paper)+Hypernym" 
and "SMT(Paper)" 
"SMT(Paper)+Hypernym" impaired  
"SMT(Paper)", because the method paraphrased 
unnecessary terms into their hypernyms. As a 
result, irrelevant patents were contained within 
the top 170 search results, and the k-NN method 
ranked irrelevant IPC codes at higher levels.  Our 
methods using two translation models are differ-
ent from "SMT(Paper)+Hypernym" in this point 
because two translation models translate into the 
same term when a scholarly term need not be 
paraphrased. 
Classification of Japanese research papers 
using "Index(Patent)" 
As we mentioned above, the "In-
dex(Paper)+Index(Patent)" and 
"SMT(Paper)+Index(Patent)" models improved 
the MAP scores of both "Index(Paper)" and 
"SMT(Paper)". We further investigated whether 
"Index(Patent)" could also improve monolingual 
document classification ("Japanese sub-
task+Index(Patent)"). In this method, a Japanese 
index was created from a manually written Japa-
nese research paper, and this was combined with 
"Index(Patent)". The results showed that "Japa-
nese subtask+Index(Patent)" could slightly im-
prove MAP scores when using topics in group 
A+B and in group A. 
Practicality of our method 
Recall values for the top n results by 
"SMT(Paper)+Index(Patent)", which obtained 
the highest MAP score, are in Table 3. In this 
table, the results using all topics (group A+B) 
and the topics in group A are shown. The results 
indicate that almost 40% of the IPC codes were 
found within top 10 results, and 70% were found 
within the top 100. For practical use, we need to 
improve recall at the top 1, but we still believe 
that these results are useful for supporting begin-
ners in patent searches. It is often necessary for 
searchers to use patent classification codes for 
effective patent retrieval, but professional skill 
and much experience are required to select rele-
vant IPC codes. In such cases, our method is use-
ful to look for relevant IPC codes. 
33
5 Conclusion 
We proposed several methods that automatically 
classify research papers into the IPC system us-
ing two translation models. To confirm the effec-
tiveness of our method, we conducted some ex-
aminations using the data of the NTCIR-7 Patent 
Mining Task. The results showed that one of our 
methods "SMT(Paper)+Index(Patent)" obtained 
a MAP score of 0.2897. This score was higher 
than that of "SMT(Paper)", which used transla-
tion results by the translation model for research 
papers, and this indicates that our method is ef-
fective for cross-genre, cross-lingual document 
classification. 
rank group A group A+B 
1 0.117 (131/1115) 0.110 (  226/2051) 
2 0.186 (207/1115) 0.169 (  347/2051) 
3 0.239 (267/1115) 0.215 (  440/2051) 
4 0.278 (310/1115) 0.250 (  512/2051) 
5 0.311 (347/1115) 0.277 (  567/2051) 
10 0.420 (468/1115) 0.377 (  774/2051) 
20 0.524 (584/1115) 0.467 (  958/2051) 
50 0.659 (735/1115) 0.597 (1224/2051) 
100 0.733 (817/1115) 0.673 (1381/2051) 
500 0.775 (864/1115) 0.728 (1494/2051) 
1000 0.775 (864/1115) 0.728 (1494/2051) 
Table 3: Recall for top n results 
(SMT(Paper)+Index(Patent)) 
References  
Guo-Wei Bian and Shun-Yuan Teng. 2008. Integrat-
ing Query Translation and Text Classification in a 
Cross-Language Patent Access System, Proceeding 
of the 7th TCIR Workshop Meeting: 341-346. 
Stephane Clinchant and Jean-Michel Renders. 2008. 
XRCE's Participation to Patent Mining Task at 
NTCIR-7, Proceedings of the 7th TCIR Workshop 
Meeting: 351-353. 
Atsushi Fujii, Makoto Iwayama, and Noriko Kando. 
2004. Overview of Patent Retrieval Task at 
NTCIR-4, Working otes of the 4th TCIR Work-
shop: 225-232. 
Atsushi Fujii, Makoto Iwayama, and Noriko Kando. 
2005. Overview of Patent Retrieval Task at 
NTCIR-5, Proceedings of the 5th TCIR Workshop 
Meeting: 269-277. 
Atsushi Fujii, Makoto Iwayama, and Noriko Kando. 
2007. Overview of the Patent Retrieval Task at 
NTCIR-6 Workshop, Proceedings of the 6th TCIR 
Workshop Meeting: 359-365. 
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and 
Takehito Utsuro. 2008. Overview of the Patent 
Translation Task at the NTCIR-7 Workshop, Pro-
ceedings of the 7th TCIR Workshop Meeting: 389-
400. 
Marti A. Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora, Proceedings of 
the 14th International Conference on Computation-
al Linguistics: 539-545. 
Daisuke Ikeda, Toshiaki Fujiki, and Manabu Okumu-
ra. 2006. Automatically Linking News Articles to 
Blog Entries, Proceedings of AAAI Spring Sympo-
sium Series Computational Approaches to Analyz-
ing Weblogs: 78-82. 
Masaki Itagaki, Takako Aikawa, and Xiaodong He. 
2007. Automatic Validation of Terminology Trans-
lation Consistency with Statistical Method, Pro-
ceedings of MT summit XI: 269-274. 
Hideo Itoh, Hiroko Mano, and Yasushi Ogawa. 2002. 
Term Distillation for Cross-db Retrieval, Working 
otes of the 3rd TCIR Workshop Meeting, Part 
III: Patent Retrieval Task: 11-14. 
Makoto Iwayama, Atsushi, Fujii, Noriko Kando, and 
Akihiko Takano. 2002. Overview of Patent Re-
  Method group 
A+B 
group A  
 Our 
methods 
Index(Paper)*Index(Patent) 0.1830 0.2230  
 Index(Paper)+Index(Patent) 0.2258 0.2596  
 SMT(Paper)+Index(Patent) 0.2633 0.2897  
 Baseline 
methods 
SMT(Paper) 0.2518 0.2777  
 SMT(Patent) 0.2214 0.2507  
 Index(Paper) 0.2169 0.2433  
 Index(Patent) 0.2000 0.2373  
 SMT(Paper)+Hypernym 0.2451 0.2647  
 Index(TechDic) 0.1575 0.1773  
 Index(EIJIRO) 0.1347 0.1347  
 Upper-
bound 
Japanese subtask 0.2958 0.3267  
 Japanese subtask+Index(Patent) 0.3001 0.3277  
Table 2: Evaluation results 
 
34
trieval Task at NTCIR-3, Working otes of the 3rd 
TCIR Workshop Meeting, Part III: Patent Re-
trieval Task: 1-10. 
Makoto Iwayama, Atsushi Fujii, and Noriko Kando.  
2005. Overview of Classification Subtask at 
NTCIR-5 Patent Retrieval Task, Proceedings of the 
5th TCIR Workshop Meeting: 278-286. 
Makoto Iwayama, Atsushi Fujii, and Noriko Kando. 
2007. Overview of Classification Subtask at 
NTCIR-6 Patent Retrieval Task, Proceedings of the 
6th TCIR Workshop Meeting: 366-372. 
Noriko Kando, Kazuko Kuriyama, Toshihiko Nozue, 
Koji Eguchi, Hiroyuki Kato, and Soichiro Hidaka. 
1999. Overview of IR Tasks at the first NTCIR 
Workshop, Proceedings of the 1st TCIR Work-
shop on Research in Japanese Text Retrieval and 
Term Recognition: 11-44. 
Noriko Kando, Kazuko Kuriyama, and Makoto Yo-
shioka. 2001. Overview  of Japanese and English 
Information Retrieval Tasks (JEIR) at the Second 
NTCIR Workshop, Proceedings of the 2nd TCIR 
Workshop Meeting: 4-37 - 4-60. 
Hisao Mase and Makoto Iwayama. 2008. NTCIR-7 
Patent Mining Experiments at Hitachi, Proceedings 
of the 7th TCIR Workshop Meeting: 365-368. 
Hidetsugu Nanba. 2007. Query Expansion using an 
Automatically Constructed Thesaurus, Proceedings 
of the 6th TCIR Workshop Meeting: 414-419. 
Hidetsugu Nanba, Natsumi Anzen, and Manabu 
Okumura:a. 2008. Automatic Extraction of Citation 
Information in Japanese Patent Applications, Inter-
national Journal on Digital Libraries, 9(2): 151-
161.  
Hidetsugu Nanba, Atsushi Fujii, Makoto Iwayama, 
and Taiichi Hashimoto:b. 2008. Overview of the 
Patent Mining Task at the NTCIR-7 Workshop, 
Proceedings of the 7th TCIR Workshop Meeting: 
325-332. 
Hidetsugu Nanba:c. 2008. Hiroshima City University 
at NTCIR-7 Patent Mining Task. Proceedings of 
the 7th TCIR Workshop Meeting: 369-372. 
Hidetsugu Nanba, Hideaki Kamaya, Toshiyuki Take-
zawa, Manabu Okumura, Akihiro Shinmori, and 
Hidekazu Tanigawa. 2009. Automatic Translation 
of Scholarly Terms into Patent Terms, Journal of 
Information Processing Society Japan TOD, 2(1): 
81-92. (in Japanese) 
Gerald Salton. 1971. The SMART Retrieval System - 
Experiments in Automatic Document Processing. 
Prentice-Hall, Inc., Upper Saddle River, NJ. 
Masatsugu Tonoike. Mitsuhiro Kida, Toshihiro Taka-
gi, Yasuhiro Sakai, Takehito Utsuro, and Satoshi 
Sato. 2005. Translation Estimation for Technical 
Terms using Corpus Collected from the Web, Pro-
ceedings of the Pacific Association for Computa-
tional Linguistics: 325-331. 
35
