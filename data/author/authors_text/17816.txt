Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 1?12,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Multi-document multilingual summarization corpus preparation, Part 1:
Arabic, English, Greek, Chinese, Romanian
Lei Li
BUPT, China
leili@bupt.edu.cn
Corina Forascu
RACAI, Romania
UAIC, Romania
corinfor@info.uaic.ro
Mahmoud El-Haj
Lancaster Univ., UK
m.el-haj@lancaster.ac.uk
George Giannakopoulos
NCSR Demokritos, Greece
SciFY NPC, Greece
ggianna@iit.demokritos.gr
Abstract
This document overviews the strategy, ef-
fort and aftermath of the MultiLing 2013
multilingual summarization data collec-
tion. We describe how the Data Contrib-
utors of MultiLing collected and gener-
ated a multilingual multi-document sum-
marization corpus on 10 different lan-
guages: Arabic, Chinese, Czech, English,
French, Greek, Hebrew, Hindi, Romanian
and Spanish. We discuss the rationale be-
hind the main decisions of the collection,
the methodology used to generate the mul-
tilingual corpus, as well as challenges and
problems faced per language. This paper
overviews the work on Arabic, Chinese,
English, Greek, and Romanian languages.
A second part, covering the remaining lan-
guages, is available as a distinct paper in
the MultiLing 2013 proceedings.
1 Introduction
Summarization has recently received the focus
of media attention (Cahan, 2013; Shih, 2013), due
to a set of corporate buy-outs related to summariza-
tion technology companies. This trend of applying
summarization is the result of a long research effort
related to summarization. Previously, especially
within the Text Analysis Conference (TAC) series
of workshops (Dang, 2005; Dang, 2006; Dang and
Owczarzak, 2008), multi-document summariza-
tion has covered aspects of summarization such
as update summarization, guided summarization
and cross-lingual summarization. In TAC 2011
the MultiLing Pilot (Giannakopoulos et al, 2011)
was introduced: a combined community effort to
present and promote multi-document summariza-
tion apporaches that are (fully or partly) language-
neutral. To support this effort an organizing com-
mittee across more than six countries was assigned
to create a multi-lingual corpus on news texts, cov-
ering seven different languages: Arabic, Czech,
English, French, Greek, Hebrew, Hindi.
The Pilot gave birth to an active community of
researchers, who provided the effort and know-
how to realize a continuation of the original ef-
fort: MultiLing 2013. The MultiLing 2013 Work-
shop, taking place within ACL 2013, built upon
the existing corpus of MultiLing 2011 to provide
additional languages and challenges for summa-
rization systems. This year 3 new languages were
added: Chinese, Romanian and Spanish. Further-
more, more texts were added to most existing cor-
pus languages (with the exception of French and
Hindi).
In the following paragraphs we first overview
theMultiLing tasks, for which the corpus was built
(Section 2). We then describe the rationale and
strategy applied for the corpus collection and cre-
ation (Section 3). We continue with special com-
ments for the English, Greek, Chinese and Roma-
nian languages (Section 4). Finally, we summarize
the findings at the end of this paper (Section 5). We
note that a second paper (Elhadad et al, 2013) de-
scribes the language-specific notes related to the
rest of the MultiLing 2013 language contributions
(Czech, Hebrew, Spanish).
2 The MultiLing tasks
There are two main tasks (and a single-
document multilingual summarization pilot de-
scribed in a separate paper) in MultiLing 2013:
Summarization Task This MultiLing task aims
to evaluate the application of (partially or
fully) language-independent summarization
algorithms on a variety of languages. Each
system participating in the task was called
to provide summaries for a range of differ-
ent languages, based on corresponding cor-
pora. In the MultiLing Pilot of 2011 the lan-
1
guages used were 7, while this year systems
were called to summarize texts in 10 differ-
ent languages: Arabic, Chinese, Czech, En-
glish, French, Greek, Hebrew, Hindi, Roma-
nian, Spanish. Participating systems were re-
quired to apply their methods to a minimum
of two languages.
The task was aiming at the real problem of
summarizing news topics, parts of which may
be described or may happen in different mo-
ments in time. We consider, similarly to Mul-
tiLing 2011(Giannakopoulos et al, 2011) that
news topics can be seen as event sequences:
Definition 1 An event sequence is a set of
atomic (self-sufficient) event descriptions, se-
quenced in time, that share main actors, lo-
cation of occurence or some other important
factor. Event sequences may refer to topics
such as a natural disaster, a crime investiga-
tion, a set of negotiations focused on a single
political issue, a sports event.
The summarization task requires to generate
a single, fluent, representative summary from
a set of documents describing an event se-
quence. The language of the document set
will be within the given range of 10 languages
and all documents in a set share the same lan-
guage. The output summary should be of the
same language as its source documents. The
output summary should be between 240 and
250 words.
Evaluation Task This task aims to examine how
well automated systems can evaluate sum-
maries from different languages. This task
takes as input the summaries generated from
automatic systems and humans in the Sum-
marization Task. The output should be a grad-
ing of the summaries. Ideally, we would want
the automatic evaluation to maximally corre-
late to human judgement.
The first task was aiming at the real problem of
summarizing news topics, parts of which may be
described or happen in different moments in time.
The implications of including multiple aspects of
the same event, as well as time relations at a vary-
ing level (from consequtive days to years), are still
difficult to tackle in a summarization context. Fur-
thermore, the requirement for multilingual appli-
cability of the methods, further accentuates the dif-
ficulty of the task.
The second task, summarization evaluation has
come to be a prominent research problem, based on
the difficulty of the summary evaluation process.
While commonly used methods build upon a few
human summaries to be able to judge automatic
summaries (e.g., (Lin, 2004; Hovy et al, 2005)),
there also exist works on fully automatic evalua-
tion of summaries, without human?model? sum-
maries (Louis and Nenkova, 2012; Saggion et al,
2010). The Text Analysis Conference has a sepa-
rate track, named AESOP (Dang and Owczarzak,
2009) aiming to test and evaluate different auto-
matic evaluation methods of summarization sys-
tems.
Given the tasks, a corpus needed to be gener-
ated, that would be able to:
? provide input texts in different languages to
summarization systems.
? provide model summaries in different lan-
guages as gold standard summaries, to also
allow for automatic evaluation using model-
dependent methods.
? provide human grades to automatic and hu-
man summaries in different languages, to
support the testing of summary evaluation
systems.
In the following section we show how these re-
quirements were met in MultiLing 2013.
3 Corpus collection and generation
The overall process of creating the corpus of
MultiLing 2013 was, similarly to MultiLing 2011,
based on a community effort. The main processes
consisting of the generation of the corpus are as
follows:
? Selection of a source corpus in a single lan-
guage (see Section 3.1).
? Translation of the source corpus to different
languages (see Section 3.2).
? Human summarization of corpus topics per
language (see Section 3.3).
? Evaluation of human summaries, as well as of
submitted system runs (see Section 3.4).
2
We should note here that the translation is meant
to provide a parallel corpus of texts across differ-
ent languages. The main ideas behind this first ap-
proach are that:
? the corpus will allow performing secondary
studies, related to the human summarization
effort in different languages. Having a paral-
lel corpus is such cases can prove critical, in
that it provides a common working base.
? we may be able to study topic-related
or domain-related summarization difficulty
across languages.
? the parallel corpus highlights language-
specific problems (such as ambiguity in word
meaning, named entity representation across
languages).
? the parallel corpus fixes the setting in which
methods can show their cross-language ap-
plicability. Examining significantly varying
results in different languages over a parallel
corpus offers some background on how to im-
prove existing methods and may highlight the
need for language-specific resources.
On the other hand, the significant organizational
and implementaion effort required for the transla-
tion (please see per language notes in the corre-
sponding sections) may lead to a comparable (vs.
parallel) corpus in future MultiLing endeavours.
Given the tasks at hand, the Contributors first
performed the selection of the texts that would be
used for the MultiLing tracks, as described below.
3.1 Selecting the corpus
To support the summarization task, we needed
a dataset of freely available news texts (to allow
reuse), covering news topics that would contain
event sequences. Based on the ? apparently good
? decisions of the MultiLing 2011 Pilot, we de-
termined that each event sequence in the corpus
should contain at least three distinct atomic events,
to imply an underlying story.
The dataset created was based on the WikiNews
site1, which covers a variety of news topics,
while allowing the reuse of the texts based on the
Creative Commons Licence. An example topic
with two sample texts derived from the original
WikiNews documents is provided in Figure 1. It
1See http://www.wikinews.org.
can be seen clearly that the event in the example
has significantly different aspects, since an earth-
quake caused a radiation leak, via a series of inter-
actions in the real world. Systems would normally
be expected to express both aspects of the event
with adequate information.
During the selection of the source texts, we
first gathered an English corpus of 15 topics (10
of which were already available from MultiLing
2011), each containing 10 texts. Wemade sure that
each topic contained at least one event sequence.
From the original HTML text we only kept unfor-
matted content text, without any images, tables or
links.
While choosing topics we made sure that there
existed topics:
? with varying time granularity. Some top-
ics happen within days (e.g., sports events),
while others within years (e.g., Iranian nu-
clear policy and international negotiations).
? covering various domains. There existed top-
ics related to international politics, sports,
natural disasters, political campaigns and
elections.
? with a varying number of apparent actors.
Some topics focus on specific individuals
(e.g., campaign of Barack Obama) while oth-
ers refer to numerous participants (e.g., para-
Olympics and participating athletes).
? with numeric aspects, that would change over
time. Such examples are natural disasters
(with the number of estimated victims, or
the estimated magnitude of earthquakes) and
sports events (number of medals per country).
? with an important time dimension. For ex-
ample during the Egyptian riots, the order of
events is non-trivial to determine from text.
Determining the order of events is also very
challenging while following multi-day sports
events. Ignoring the time dimension in such
topics is expected to worsen the performance
of summarization systems.
Given the English texts, we now needed to pro-
vide corresponding texts in all the languages used
in MultiLing. To this end, we organized a transla-
tion process, which is elaborated below.
3
Fukushima reactor suffers multiple fires, radiation leak confirmed
Tuesday, March 15, 2011
Fires broke out at the Fukushima Daiichi plant's No. 4 reactor in Japan on
Tuesday, according to the Tokyo Electric Power Company. The first fire caused
a leak of concentrated radioactive material, according to the Japanese prime
minister, Naoto Kan.
The first fire broke out at 9:40 a.m. local time on Tuesday, and was thought
to have been put out, but another fire was discovered early on Wednesday,
believed to have started because the earlier one had not been fully
extinguished.
In a televised statement, the prime minister told residents near the plant
that "I sincerely ask all citizens within the 20 km distance from the reactor
to leave this zone." He went on to say that "[t]he radiation level has risen
substantially. The risk that radiation will leak from now on has risen."
Kan warned residents to remain indoors and to shut windows and doors to avoid
radiation poisoning.
The French Embassy in Japan reports that the radiation will reach Tokyo in 10
hours, with current wind speeds.
Death toll rises from Japan quake
Sunday, March 13, 2011
The death toll from the earthquake and subsequent tsunami that hit Japan on
Friday has risen to more than a thousand, with many people still missing,
according to reports issued over the weekend.
While Japan's police says that only 637 are confirmed dead, media reports say
that over a thousand people have been killed, with several hundred bodies
still being transported. Thousands more are still unaccounted for; in the town
of Minamisanriku, Miyagi Prefecture alone, up to 10,000 people are missing.
Four trains that were on the coast have yet to be located.
In the aftermath of the disaster, evacuations of around 300,000 people have
taken place; more evacuations are likely in the wake of concerns over a
damaged nuclear power plant. According to Prime Minister Naoto Kan, around
3,000 people have been rescued thus far. 50,000 troops from the Japanese
military have been deployed to assist in rescue efforts.
The tsunami generated by the quake has destroyed communities along Japan's
Pacific coast, with up to 90% of the houses in some towns having been
destroyed; at least 3,400 structures have been destroyed in total. Fires have
also sprung up among the impacted areas.
Figure 1: Topic Sample (Japan Earthquake and Nuclear Threat)
4
3.2 Translating the corpus
The English texts selected in the selection step
were translated using a sentence-by-sentence ap-
proach to each of the other languages: Arabic, Chi-
nese, Czech, French, Greek, Hebrew, Hindi, Ro-
manian, Spanish. This year there was no support
for the Hindi and French languages, which still
contain 10 topics. Also the Chinese language cov-
ers 10 topics. All the remaining languages cover
15 topics.
During the translation process, the guidelines
were minimal:
Given the source language text A,
the translator is requested to translate
each sentence in A, into the target lan-
guage. Each target sentence should keep
the meaning from the source language.
Some additional, optional guidelines (provided
in the Appendix) were provided by the Romanian
language Contributors, proposing ways to react to
date formatting, name translations, etc.
During the translation process, the translators
were also asked to keep track of the time spent on
different stages of the process: first full reading of
the source document, translation and verification.
The whole set of translated documents together
with the original English document set will be re-
ferred to as the Source Document Set. Given the
creation process, the Source Document Set con-
tains a total of 1350 texts (vs. 700 from MultiLing
2011): 7 languages with 15 topics per language, 10
texts per topic for a total of 1050 texts; 3 languages
with 10 topics per language, 10 texts per topic for
a total of 300 texts.
This Source Document Set was provided to par-
ticipating systems as input for their summarization
systems. It was also provided to human summa-
rizers, so that they would provide human, model
summaries on each topic and each language. The
human summarization process is described in the
following section.
3.3 Summarizing topics
In the summarization step of the corpus creation
different summarizers were asked to generate one
summary per topic in each language. The follow-
ing guidelines were provided to help the summa-
rizers:
The summarizer will read the whole
set of texts at least once. Then, the sum-
marizer should compose a summary,
with a minimum size of 240 and a maxi-
mum size of 250 words. The summary
should be in the same language as the
texts in the set. The aim is to create a
summary that covers all the major points
of the document set (what is major is
left to summarizer discretion). The sum-
mary should be written using fluent, eas-
ily readable language. No formatting or
other markup should be included in the
text. The output summary should be a
self-sufficient, clearly written text, pro-
viding no other information than what is
included in the source documents.
After summarization, human evaluation was
performed. The evaluation covered human sum-
maries, but also summarization system submis-
sions. The details are provided in the following
paragraphs.
3.4 Evaluating the summaries
The evaluation of summaries was performed
both automatically and manually. The manual
evaluation was based on the Overall Responsive-
ness (Dang and Owczarzak, 2008) of a text, as de-
scribed below, and the automatic evaluation used
the ROUGE (Lin, 2004) and AutoSummENG-
MeMoG (Giannakopoulos et al, 2008; Gian-
nakopoulos and Karkaletsis, 2011) and NPowER
(Giannakopoulos and Karkaletsis, 2013) methods
to provide a grading of performance.
For the manual evaluation the human evaluators
were provided the following guidelines:
Each summary is to be assigned an
integer grade from 1 to 5, related to the
overall responsiveness of the summary.
We consider a text to be worth a 5, if
it appears to cover all the important as-
pects of the corresponding document set
using fluent, readable language. A text
should be assigned a 1, if it is either un-
readable, nonsensical, or contains only
trivial information from the document
set. We consider the content and the
quality of the language to be equally im-
portant in the grading.
As indicated in the task, the acceptable limits for
the word count of a summary were between 240
5
and 250 words2 (inclusive). In the case of Chi-
nese there was a problem determining the number
of words. Based on the model summaries gathered
we (arbitrarily) set the upper limit of length in bytes
of the UTF8-encoded summary files to 750 bytes.
4 Language specific notes
In the following paragraphs we provide
language-specific overviews related to the corpus
contribution effort. The aim of these overviews is
to provide a reusable pool of knowledge for future
similar efforts.
In this document we elaborate on Arabic, En-
glish, Greek, Chinese and Romanian languages. A
second document (Elhadad et al, 2013) elaborates
on the rest of the languages.
4.1 Arabic language
The preparation of the Arabic corpus for the
2013 MultiLing Summarization tasks was organ-
ised jointly by Lancaster University and the Uni-
versity of Essex in the United Kingdom. 20 people
participated in translating the English corpus into
Arabic, validating the translation and summarising
the set of related Arabic articles. The participants
are studying, or have finished a university degree
in an Arabic speaking country. The participants?
age ranged between 21 and 32 years old.
The participants translated the English dataset
into Arabic. For each translated article another
translator validated the translation and fixed any
errors. For each of the translated articles, three
manual summaries were created by three different
participants (human peers). Amid the summarisa-
tion process the participants evaluated the quality
of the generated summary by assigning a score be-
tween one (unreadable summary) and five (fluent
and readable summary). No self evaluation was
allowed.
The average time for reading the English news
articles by the Arabic native speaker participants
was 5.58minutes. The average time it took them to
translate these articles into Arabic was 42.18 min-
utes and to validate each of the translated Arabic
articles the participants took 5.25 minutes on aver-
age.
For the summarisation task the average time for
reading the set of related articles (10 articles per
2The count of words was provided by thewc -w linux com-
mand.
each set) was 34.44 minutes. The average time for
summarising each set was 25.41 minutes.
4.1.1 Problems and Challenges
Many difficulties arose during the creation of
the gold-standard summaries. Some are language-
dependent and relate to the complexity of the Ara-
bic language. This required a special attention to
be paid while creating the summaries.
One problem concerns the handling of month
names in Arabic. There are twoways of translating
month names into Arabic:
? using the Arabic transliteration of the
Aramic (Syriac) month names (e.g. ?May?,
?PA


K




@?, ?Ayyar?).
? using the Arabic transliteration of the
English month names (e.g. ?May?,
??K


A

??, ?Mayo?).
Some of the participants found it difficult to
translate sentences where they believe they contain
an ambiguous structure. For example: ?She said
Iranian security Chief Saeed Jalili had requested a
meeting in a telephone call?. The translators (who
are Native Arabic speakers) found it a bit hard to
choose between two translations:
? ?Saeed Jalili asked to schedule a telephone
meeting?
? ?Saeed Jalili phoned to request a meeting?.
Arabic sentence structure is highly complex and
therefore great attention must be paid when mov-
ing forward or pushing back phrases within a sen-
tence, as such shifts are likely to change the over-
all meaning. In addition, the use of passive voice,
metaphors and idioms in the original English text
has captured the translators attention, as the mean-
ing in such cases takes precedence over the literal
translation.
During the summarisation process, a sum-
mariser found that ordering a set of related articles
(discussing the same topic) in chronological order
simplifies the summarisation process.
Many participants found it difficult to meet the
250 summary word-limit as they believe 250 is not
enough to cover all the essential information de-
rived from a given set of documents.
Another problem concerns ?proper nouns? when
translating into Arabic. The Arabic electronic dis-
course would sometimes show two variants of one
6
English proper noun, as in the case with the name
?Francois Hollande?. Mostly in such cases, the
variant used in popular websites such as the Arabic
version Wikipedia was adopted.
Finally, there were many questions by the par-
ticipants on whether to create abstractive or extrac-
tive summaries.
4.2 Chinese language
Below we provide an overview of the organiza-
tional effort and comments on a variety of prob-
lems related to the preparation of the Chinese cor-
pus for MultiLing 2013.
4.2.1 Organization
First, the Chinese language team translated two
texts from English to Chinese together in order to
make an original unified example for each trans-
lator, including file format, title format, date for-
mat, named entity translation, etc. Second, we as-
signed different set of news texts as specific task
for each translator. For each news topic, we usu-
ally split the ten texts to two different translators at
least, so as to bring more thoughts from different
viewers and prepare enough for later discussion.
During the process of each translator, they were
asked to note any problems in a ?problem file?, in-
cluding the source English part and the target Chi-
nese part. Third, we summed up a big problem
file from each translator. After a series of discus-
sions, we classified the problems into different cat-
egories and solved some of the problems success-
fully. The remaining problems were noted down in
a detailed report to the organizer of the MultiLing
2013Workshop of ACL 2013, as a knowledge pool
for future efforts. Fourth, we performed the verifi-
cation task. During the process, we made sure that
for each text, the verifier was different from the
translator. Also each verifier was demanded to log
any problems. Fifth, we did another discussion for
new problems coming from the verification phase.
Some problems were solved; others were added to
the detailed report. Sixth, we generated the needed
result files and made sure that they were in the re-
quested format (e.g., UTF8, no-BOM, plain text
files for summaries).
For the process of summarization and human
evaluation, first, we assigned three summarizers,
each of which needed to read all the ten topics and
write a summary for each topic. Second, we as-
signed three evaluators, making sure that for each
summary, the evaluator was different from the
summarizer. Third, we made a discussion about
the process of summarization and evaluation. All
agreed that summarization and evaluation were
much easier than translation.
There were mainly two common problems. One
was about the summary length. So we set a uni-
fied method for length checking. The other prob-
lem was more complex, which was that there
were many different information in the original ten
texts, but the result summary was limited to 250
words, so it was very difficult to choose the most
important information. As a result, some infor-
mation could be lost in final summaries. At the
same time, we also found minor problems regard-
ing the translation, improved the translation files
and updated the detailed report about the problems
we faced.
4.2.2 Problems and proposed solutions
In fact, related problems mainly came up from
the task of translation. Most of them were com-
mon questions of the translators and language-
dependent problems that needed special care. Here
we only list the main categories of problems 3.
First, there were problems with the translation of
person names. There are several sub-problems
here:
? There are some person names which are not
so popular, we could not find a result, so
we finally keep the unknown English words
among Chinese words.
? There is no specific separator between first
name, middle name and family name in En-
glish, only normal space. But in Chinese, we
usually add a separator ?? ? between them.
?
? There is also some ambiguity in person name
to us, since we may be not quite familiar with
some specific knowledge of news related do-
main. ?
? There are also some person names which
seem to contain non-English characters.
These names are more difficult for us, so we
just keep most of them as the original format
in English news. ?
? There are some person names with only one
capitalized character and a dot in the middle
3A more detailed report has been submitted to the orga-
nizer of the Workshop.
7
part. It?s really difficult for us to find a cor-
responding Chinese translation for it, so we
just keep it as the original English format in
the Chinese translations and keep the original
English name in the following brackets.
Second, the translation for the English name of
some websites, companies, organizations, etc, can
cause problems. Since the full name may be too
long for news reports, most of them also have oc-
curred in corresponding simple format of abbre-
viation. Some of them are famous enough that
we have a popular Chinese translation for them,
while others are not so popular. So we decided
that for unknown ones, we just reserve the English
name, but for those known ones, we add the Chi-
nese translation and keep some of the English ab-
breviation.
Third, the translation of time expressions is non-
trivial. In English, the order usually used is: Week-
day, Month Day, Year. But according to Chinese
habit, we mention time usually in the following or-
der: Year Month Day, Weekday.
Fourth, translation of locations names may not
exist. There are many location names in these
news texts. We tried to find their Chinese transla-
tion from many resources, but there are still some
difficult ones left.
Fifth, there are someEnglishwords in the source
texts which seem to be unrelated to other sentences
in the news text (these may be text captions of pho-
tos in the source WikiNews articles). We just left
them as they were.
Sixth, there are some sentences which are diffi-
cult to understand clearly because the context and
structure are ambiguous. In these cases, we made
a Chinese translation which seems best to us.
The above problems conclude the Chinese lan-
guage contribution language-specific notes.
4.3 English and Greek languages
The effort related to the organization of the En-
glish and Greek languages was essentially equiva-
lent to the MultiLing 2011 pilot (Giannakopoulos
et al, 2011). This year 5 new topics were added
to the two languages. The effort for English was
reduced because no translation was needed. In the
following subsections we elaborate on the organi-
zation details and the problems faced during the
different subprocesses of the corpus creation.
4.3.1 Organization
A total of 7 people (being either MSc students,
or researchers, all with fluency in English and
Greek) were recruited for the two languages. An
initial meeting was held to provide the basic guide-
lines and discuss questions on the translation pro-
cess. Subsequently, e-mail communication and
periodic conferences were used to assign the next
tasks, related to summarization and evaluation.
For the purposes of meaningful assignment we
created and used an automatic assignment script,
that allows pre-allocating specific texts to workers
(for any of the required tasks), while it automati-
cally distributes work according to the availability
of workers. The script avoids assigning workers to
texts/tasks more than once.
In the evaluation process, we made sure
(through pre-assignments) that no human would
judge their own summary. It would have increased
efficiency, if we had ascertained that human sum-
marization would occur right after the translation
of the texts.
The average time for reading the English news
articles by the Greek native speaker participants
was around 8 minutes. The average time it took
them to translate these articles into Greek was
around 48 minutes on average (with a couple of
extreme cases exceeding 100 minutes, due to tech-
nical terminology, which was difficult to trans-
late). The summarization time of the new topics
in English was around 24 minutes per topic (plus
an average of 8 minutes allocated to reading the
source texts). For Greek the summary time was
around 50 minutes per topic (we note that the sum-
marizers? groups for English and Greek were only
minimally overlapping). In the Greek case, some
deeper search showed that a single summarizer
heavily biased the distribution of times to higher
values.
To follow the progress of tasks, a generic project
management tool was used. However, the tool
proved insufficient in the micro-planning of the ef-
fort (individual assignments tracking). It would
clearly make sense to use an ad-hoc designed sys-
tem for planning and implementation of the effort.
4.3.2 Problems and proposed solutions
The main problems identified by contributors
for Greek and English translation were related to
well-known translation problems: named entity
translation, date formatting, highly technical or
domain specific terminology, ambiguous terms in
8
the source text. Additional effort from translators
provided solutions to these problems according to
common practice in the translation domain.
The summarization effort indicated a few inter-
esting points. Even though summarizers have their
individual method for summarizing, some com-
mon practices and notes arise:
? A non-thorough glimpse of the source texts
helps determine the overall topic.
? Time ordering is important in several cases,
thus time ordering of the source texts is ap-
plied before the summarization process itself.
The process is non-trivial even for humans.
? An initial summary which may be longer than
the target size is created and several reductive
transformations are applied. The 250 word
limit proved critical and challenging, in that it
forced summarizers to carefully choose infor-
mation, essentially not covering the whole set
of information from the source documents.
? Syntactic compression and rewriting is the
last line of summarization, when it is obvious
that more compression is needed.
As related to the evaluation process, we noted
that there exists an inherent tendency for evalua-
tors to determine whether a human or a machine
performed the summarization. There were cases
where evaluators altered their grading, because
they inferred that not all texts were from humans
or not all were from machines. We had noted this
phenomenon also in MultiLing 2011. There are
several cases where the evaluator also tries to de-
termine the strategy of the system and, when one
understands the underlying strategy, this may bias
the grade. It would be interesting to evaluate this
bias in the future.
Some additional notes are related to problems
with the organization of the effort:
? A distributed work environment that would
help track the progress of individuals and
assignment of new tasks without significant
communication effort, would have been very
helpful.
? The assignment script was really critical in
facilitating the organization of the effort and
we plan to make it publicly available to allow
reuse.
Overall, the collection and generation of the cor-
pus was a very challenging effort, both in terms
of organization and individual questions arising.
However, next steps can build upon the lessons
learnt, if the effort is well documented and the doc-
uments are freely and openly shared.
4.4 Romanian language
AtMultiLing 2013, Romanian was addressed as
a language for the first time. Following the Call
for Contributors launched by the MultiLing orga-
nizers and based on the experience in the QA @
CLEF4 evaluation campaign (Pe?as et al, 2012),
we started the data collection process workingwith
a group of ten MSc students in Computational
Linguistics from our Faculty, later adding another
MSc student to the working group. Below we pro-
vide some notes on the translation and generation
of human summaries processes:
? The translation, including verification, of
the 150 WikiNews text documents from En-
glish into Romanian, was performed in a dis-
tributed context, theoretically based on an ar-
chitecture like the one described in (Alboaie
et al, 2003). Each student received one topic
(10 documents) to be translated, based on a
set of guidelines. We devised guidelines to
tackle any language-dependent problems that
need special care, and they were improved af-
ter each solution received from the students
and based also on their questions. The full
guidelines are provided in the Appendix of
this document.
We started with the following workflow: stu-
dent A receives 10 English documents to be
translated and summarized and sends the re-
sults to the organizer; another student, B, re-
ceives the English documents and the Roma-
nian translations (made by student A) and s/he
verifies the translations and prepares another
summary. Finally, another student, C, re-
ceives from the organizer the 10 Romanian
documents and s/he prepares the third sum-
mary of a given topic.
Since the task proved to be very time-
consuming for the students, all the last five
topics (the ones introduced this year) were
given to one student and then the translations
were verified by the organizer.
4See http://celct.fbk.eu/ResPubliQA/index.
php for more information.
9
? The generation of human summaries was per-
formed immediately after the translation. For
each topic, the aim was to create a summary
that covers all the major points of the topic
(what is major was left to summarizer?s dis-
cretion), being a self-sufficient, clearly writ-
ten text, providing no other information than
what is included in the source documents.
The students were given no specific recom-
mendations regarding the type of summary
they should produce, e.g. an abstract ver-
sus an extract (Mani and Maybury, 1999),
but they were specifically instructed to under-
stand the main aspects of summarization.
5 Conclusions and lessons learnt
The corpus generated throughout the MultiLing
corpus preparation provides a benchmark dataset
for multilingual summarization. It tries to cap-
tured interesting, representative events, covering
a variety of well-known news events around the
world. The recent corporate interest in summa-
rization, in conjunction with the ever-present in-
crease of information flow from the Web and in-
formation redundancy, show that having a scien-
tifically plausible set of evaluation tools for sys-
tems can help bring useful summarization systems
to a wide audience. MultiLing functions as a fo-
cus point for multilingual summarization research
and this document described the methods used to
create a commonly accepted multilingual, multi-
document summarization corpus.
Concerning thoughts on the future work of Mul-
tiLing, there are some points that have been raised
by Contributors that we reproduce in the following
sentences:
? In the translation phase, it would be useful to
have translators for different languages dis-
cuss directly about some difficult cases, such
as some ambiguous words, phrases and sen-
tences, especially when they are expressed in
some language-specific way.
? It would be very interesting to exploit the po-
tential of comparable corpora, and not only
of the parallel ones, especially if we consider
the multilingual setting of MultiLing 2013.
This means that the data should be collected
starting from a given topic and each language
contributor should find 10 documents on that
given topic in his/her language.
? Creating a collaborative platform for build-
ing and improving summarization corpora
could significantly facilitate the corpus build-
ing process for future efforts.
We remind the reader that a second paper (El-
hadad et al, 2013) addresses the problems and
challenges faced in the remaining languages ac-
tively contributed to in MultiLing 2013 (Czech,
Hebrew and Spanish), thus completing the lessons
learnt from theMultiLing 2013 contribution effort.
Extended technical reports recapitulating discus-
sions and findings from the MultiLing Workshop
will be available after the workshop at the Multi-
Ling Community website5, as an addenum to the
proceedings.
Acknowledgments
MultiLing is a community effort and this com-
munity is what keeps it alive and interesting. We
would like to thank Contributors for their organi-
zational effort, which made MultiLing possible in
so many languages and all volunteers, helpers and
researchers that helped realize individual steps of
the process. A more detailed reference of the con-
tributor teams can be found in Appendix A.
The MultiLing 2013 organization has been par-
tially supported by the NOMAD FP7 EU Project
(cf. http://www.nomad-project.eu).
References
[Alboaie et al2003] Lenuta Alboaie, Sabin C Buraga,
and S?nica Alboaie. 2003. tuBiG?a layered infras-
tructure to provide support for grid functionalities.
Omega, 2:3.
[Cahan2013] Adam Cahan. 2013. Yahoo! To Acquire
Summly http://yodel.yahoo.com/blogs/
general/yahoo-acquire-summly-13171.
html, March 25th.
[Dang and Owczarzak2008] H. T. Dang and
K. Owczarzak. 2008. Overview of the TAC
2008 update summarization task. In TAC 2008
Workshop - Notebook papers and results, pages
10?23, Maryland MD, USA, November.
[Dang and Owczarzak2009] Hoa Trang Dang and
K. Owczarzak. 2009. Overview of the tac 2009
summarization track, Nov.
[Dang2005] H. T. Dang. 2005. Overview of DUC
2005. In Proceedings of the Document Under-
standing Conf. Wksp. 2005 (DUC 2005) at the
5See http://multiling.iit.demokritos.gr/
pages/view/1256/proceedings-addenum)
10
Human Language Technology Conf./Conf. on Em-
pirical Methods in Natural Language Processing
(HLT/EMNLP 2005).
[Dang2006] H. T. Dang. 2006. Overview of DUC
2006. In Proceedings of HLT-NAACL 2006.
[Elhadad et al2013] Michael Elhadad, Sabino
Miranda-Jim?nez, Josef Steinberger, and George
Giannakopoulos. 2013. Multi-document multi-
lingual summarization corpus preparation, part 2:
Czech, hebrew and spanish. In MultiLing 2013
Workshop in ACL 2013, Sofia, Bulgaria, August.
[Giannakopoulos and Karkaletsis2011] George Gi-
annakopoulos and Vangelis Karkaletsis. 2011.
Autosummeng and memog in evaluating guided
summaries. In TAC 2011 Workshop, Maryland MD,
USA, November.
[Giannakopoulos and Karkaletsis2013] George Gi-
annakopoulos and Vangelis Karkaletsis. 2013.
Summary evaluation: Together we stand npower-ed.
In Computational Linguistics and Intelligent Text
Processing, pages 436?450. Springer.
[Giannakopoulos et al2008] George Giannakopoulos,
Vangelis Karkaletsis, George Vouros, and Panagio-
tis Stamatopoulos. 2008. Summarization system
evaluation revisited: N-gram graphs. ACM Trans.
Speech Lang. Process., 5(3):1?39.
[Giannakopoulos et al2011] G. Giannakopoulos,
M. El-Haj, B. Favre, M. Litvak, J. Steinberger,
and V. Varma. 2011. TAC 2011 MultiLing pilot
overview. In TAC 2011 Workshop, Maryland MD,
USA, November.
[Hovy et al2005] E. Hovy, C. Y. Lin, L. Zhou, and
J. Fukumoto. 2005. Basic elements.
[Lin2004] C. Y. Lin. 2004. Rouge: A package for
automatic evaluation of summaries. Proceedings of
the Workshop on Text Summarization Branches Out
(WAS 2004), pages 25?26.
[Louis and Nenkova2012] Annie Louis and Ani
Nenkova. 2012. Automatically assessing ma-
chine summary content without a gold standard.
Computational Linguistics, 39(2):267?300, Aug.
[Mani and Maybury1999] Inderjeet Mani and Mark T
Maybury. 1999. Advances in automatic text sum-
marization. the MIT Press.
[Pe?as et al2012] Anselmo Pe?as, Eduard H. Hovy,
Pamela Forner, ?lvaro Rodrigo, Richard F. E. Sut-
cliffe, Caroline Sporleder, Corina Forascu, Yassine
Benajiba, and Petya Osenova. 2012. Overview of
qa4mre at clef 2012: Question answering for ma-
chine reading evaluation. In CLEF (Online Working
Notes/Labs/Workshop).
[Saggion et al2010] H. Saggion, J. M. Torres-Moreno,
I. Cunha, and E. SanJuan. 2010. Multilingual sum-
marization evaluation without human models. In
Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, page 1059?
1067.
[Shih2013] Gerry Shih. 2013. Sound Famil-
iar? After Yahoo Buys Summly, Google
Buys News Summarization App Wavii
http://www.huffingtonpost.com/2013/
04/24/google-wavii_n_3143116.html, April
23rd.
[Tufis et al2004] Dan Tufis, DanCristea, and Sofia Sta-
mou. 2004. Balkanet: Aims, methods, results and
perspectives. a general overview. Romanian Journal
of Information science and technology, 7(1-2):9?43.
Appendix A: Contributor teams
Arabic language team
Team members Mahmoud El-Haj (Lancaster
University, UK); Ans Alghamdi, Maha
Althobaiti (Essex University, UK); Ahmad
Alharthi (King Saud University, Saudi
Arabia)
Contact e-mail m.el-haj@lancaster.ac.uk
Chinese language team
Team members Lei Li, Wei Heng, Jia Yu, Yu Liu,
Qian Li
Team affiliation Center for Intelligence Science
and Technology (CIST), School of Com-
puter Science,Beijing University of Posts and
Telecommunications,
Postal Address P.O.Box 310, Beijing University
of Posts and Telecommunications, Xitucheng
Road 10, Haidian District, Beijing, China
Contact e-mail leili@bupt.edu.cn
English and Greek languages team
Team members Zoe Angelou, Argyro
Mavridakis, Valentini Mellas, Efrosini
Zacharopoulou, George Kiomourtzis,
George Petasis, George Giannakopoulos
Team affiliation NCSR?Demokritos?
Postal Address Institute of Informatics and
Telecommunications, Patriarchou Grigoriou
and Neapoleos Str., Aghia Paraskevi Attikis,
Athens, Greece
Contact e-mail ggianna@iit.demokritos.gr
11
Romanian language team
Team members Corina Forascu, Raluca Moi-
seanu; Ana Maria Timofciuc, Alexandra
Cristea, Alexandrina Sbiera, Bogdan Puiu,
and Tudor Popoiu; other contributors to the
task were Monica Ancu?a, Romic? Iarca,
Claudiu Popa, and Cosmin Vl?du?u
Team affiliation UAIC, Romania
Contact e-mail corinfor@info.uaic.ro
Appendix B: Romanian guidelines
1. Translation equivalents belonging to the same
part of speech should be used. The Romanian
words should be as?closest?as possible to
their English equivalents: If the English word
has as equivalent a cognate in Romanian, this
one should be used. The Romanian wordnet6
(Tufis et al, 2004) should be used for prob-
lematic situations. If the English word doesn?
t have a Romanian cognate, then the transla-
tor should not try to paraphrase it. Example:
The English ?sporadic?will be translated
into?sporadic?, even though the translator
would be tempted to use instead?izolat?or
?rar?. It is not recommended to give trans-
lations such as ?mai pu?in?or ?mai rar?
.
2. English words should not be omitted and
words which are not in the original English
text should not be added because of stylistic
reasons. Example:?The Telegraph?will be
not translated when it refers to the newspa-
per and, moreover, the translators will not in-
troduce an explanation, like?cotidianul The
Telegraph?[English: The Telegraph newspa-
per].
3. The Romanian diacritics have to be used, in
UTF-8 encoding.
4. The translators must preserve as much as pos-
sible the tenses of the English verbs. Any dis-
agreement from the English tense is allowed
for linguistic reasons only (Romanian spe-
cific constructions), and not for stylistic ones.
5. The translators will preserve the format of
dates, times, numbers. For example, for the
issuing date of an article being ?March 25,
6See http://www.racai.ro/wnbrowser/.
2010?, the Romanian translation will be?25
martie 2010?and NOT ?Martie, 25, 2010?
OR?25 Martie, 2010?.
6. The format of the numbers should follow the
Romanian convention with respect to the dec-
imal separator, which is comma (,), and not
the period (.), like in English-speaking coun-
tries.
7. The unclear or unsure situations encountered
by the translators will be separately recorded
in a file, indicating the provenance of the doc-
ument, the ID used for the problematic sen-
tence and the commentaries/suggestions.
12
Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 64?71,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Using a Keyness Metric for Single and Multi Document Summarisation
Mahmoud El-Haj
School of Computing and
Communications
Lancaster University
United Kingdom
m.el-haj@lancaster.ac.uk
Paul Rayson
School of Computing and
Communications
Lancaster University
United Kingdom
p.rayson@lancaster.ac.uk
Abstract
In this paper we show the results of
our participation in the MultiLing 2013
summarisation tasks. We participated
with single-document and multi-document
corpus-based summarisers for both Ara-
bic and English languages. The sum-
marisers used word frequency lists and
log likelihood calculations to generate sin-
gle and multi document summaries. The
single and multi summaries generated by
our systems were evaluated by Arabic
and English native speaker participants
and by different automatic evaluation met-
rics, ROUGE, AutoSummENG, MeMoG
and NPowER. We compare our results to
other systems that participated in the same
tracks on both Arabic and English lan-
guages. Our single-document summaris-
ers performed particularly well in the auto-
matic evaluation with our English single-
document summariser performing better
on average than the results of the other
participants. Our Arabic multi-document
summariser performed well in the human
evaluation ranking second.
1 Introduction
Systems that can automatically summarise docu-
ments are becoming ever more desirable with the
increasing volume of information available on the
Web. Automatic text summarisation is the process
of producing a shortened version of a text by the
use of computers. For example, reducing a text
document or a group of related documents into a
shorter version of sentences or paragraphs using
automated tools and techniques.
The summary should convey the key contri-
butions of the text. In other words, only key
sentences should appear in the summary and the
process of defining those sentences is highly de-
pendent on the summarisation method used. In
automatic summarisation there are two main ap-
proaches that are broadly used, extractive and ab-
stractive. The first method, the extractive sum-
marisation, extracts, up to a certain limit, the
key sentences or paragraphs from the text and or-
ders them in a way that will produce a coherent
summary. The extracted units differ from one
summariser to another. Most summarisers use
sentences rather than larger units such as para-
graphs. Extractive summarisation methods are
the focus method on automatic text summarisa-
tion. The other method, abstractive summarisa-
tion, involves more language dependent tools and
Natural Language Generation (NLG) technology.
In our work we used extractive single and multi-
document Arabic and English summarisers.
A successful summarisation approach needs a
good guide to find the most important sentences
that are relevant to a certain criterion. Therefore,
the proposed methods should work on extracting
the most important sentences from a set of related
articles.
In this paper we present the results of our par-
ticipation to the MultiLing 2013 summarisation
tasks. MultiLing 2013 was built upon the Text
Analysis Conference (TAC) MultiLing Pilot task
of 2011 (Giannakopoulos et al, 2011). MultiL-
ing 2013 this year asked for participants to run
their summarisers on different languages having a
corpus and gold standard summaries in the same
seven languages (Arabic, Czech, English, French,
Greek, Hebrew or Hindi) of TAC 2011 with a
50% increase to the corpora size. It also intro-
duced three new languages (Chinese, Romanian
and Spanish). MultiLing 2013 this year intro-
duced a new single-document summarisation pilot
for 40 languages including the above mentioned
languages (in our case Arabic and English).
In this paper we introduce the results of our
64
single-document and multi-document summaris-
ers at the MultiLing 2013 summarisation tasks.
We used a language independent corpus-based
word frequency technique and the log-likelihood
statistic to extract sentences with the maximum
sum of log likelihood. The output summary is ex-
pected to be no more than 250 words.
2 Related Work
2.1 Automatic Summarisation
Work on automatic summarisation dates back
more than 50 years, with a focus on the English
language (Luhn, 1958). The work on Arabic au-
tomatic summarisation is more recent and still not
on par with the research on English and other Eu-
ropean languages. Early work on Arabic summari-
sation started less than 10 years ago (Conroy et al,
2006; Douzidia and Lapalme, 2004).
Over time, there have been various approaches
to automatic text summarisation. These ap-
proaches include single-document and multi-
document summarisation. Both single-document
and multi-document summarisation use the sum-
marisation methods mentioned earlier, i.e. ex-
tractive or abstractive. Summarising a text could
be dependent on input information such as a user
query or it could be generic where no user query
is used.
The approach of single-document summarisa-
tion relies on the idea of producing a summary
for a single document. The main factor in single-
document summarisation is to identify the most
important (informative) parts of a document. Early
work on single-document summarisation was the
work by Luhn (1958). In his work he looked
for sentences containing keywords that are most
frequent in a text. The sentences with highly
weighted keywords were selected. The work by
Luhn highlighted the need for features that reflect
the importance of a certain sentence in a text. Bax-
endale (1958) showed the importance of sentence-
position in a text, which is understood to be one
of the earliest extracted features in automatic text
summarisation. They took a sample of 200 para-
graphs and found that in 80% of the paragraphs
the most important sentence was the first one.
Multi-document summarisation produces a sin-
gle summary of a set of documents. The docu-
ments are assumed to be about the same genre and
topic. The analysis in this area is performed typi-
cally at either the sentence or document level.
2.2 Corpus-based and Word Frequency in
Summarisation
Corpus-based techniques are mainly used to com-
pare corpora for linguistic analysis (Rayson and
Garside, 2000; Rayson et al, 2004). There are
two main types of corpora comparisons, 1) com-
paring a sample corpus with a larger standard
corpus (Scott, 2000). 2) comparing two corpora
of equal size (Granger, 1998). In our work we
adopted the first approach, where we used a much
larger reference corpus. The first word list is the
frequency list of all the words in the document (or
group of documents) to be summarised which is
compared to the word frequency list of a much
larger standard corpus. We do that for both Ara-
bic and English texts. Word frequency has been
proven as an important feature when determining
a sentence?s importance (Li et al, 2006). Nenkova
and Vanderwende (2005) studies the impact of fre-
quency on summarisation. In their work they in-
vestigated the association between words that ap-
pear frequently in a document (group of related
documents), and the likelihood that they will be
selected by a human summariser to be included in
a summary. Taking the top performing summaris-
ers at the DUC 20031 they computed how many of
the top frequency words from the input documents
appeared in the system summaries. They found the
following: 1) Words with high frequency in the
input documents are very likely to appear in the
human summaries. 2) The automatic summaris-
ers include less of these high frequency words.
These two findings by Nenkova and Vanderwende
(2005) tell us two important facts. Firstly, it con-
firms that word frequency is an important factor
that impacts humans? decisions on which content
to include in the summary. Secondly, the overlap
between human and system summaries can be im-
proved by including more of the high frequency
words in the generated system summaries. Based
on Nenkova?s study we expand the work on word
frequency by comparing word frequency lists of
different corpora in a way to select sentences with
the maximum sum of log likelihood ratio. The log-
likelihood calculation favours words whose fre-
quencies are unexpectedly high in a document.
2.3 Statistical Summarisation
The use of statistical approaches (e.g. log-
likelihood) in text summarisation is a common
1http://duc.nist.gov/duc2003/tasks.html
65
technique, especially when building a language in-
dependent text summariser.
Morita et al (2011) introduced what they called
?query-snowball?, a method for query-oriented
extractive multi-document summarisation. They
worked on closing the gap between the query and
the relevant sentences. They formulated the sum-
marisation problem based on word pairs as a max-
imum cover problem with Knapsack Constraints
(MCKP), which is an optimisation problem that
maximises the total score of words covered by a
summary within a certain length limit.
Knight and Marcu (2000) used the Expectation
Maximisation (EM) algorithm to compress sen-
tences for an abstractive text summarisation sys-
tem. EM is an iterative method for finding Maxi-
mum Likelihood (ML) or Maximum A Posteriori
(MAP) estimates of parameters in statistical mod-
els. In their summariser, EM was used in the sen-
tences compression process to shorten many sen-
tences into one by compressing a syntactic parse
tree of a sentence in order to produce a shorter but
maximally grammatical version. Similarly, Mad-
nani et al (2007) performed multi-document sum-
marisation by generating compressed versions of
source sentences as summary candidates and used
weighted features of these candidates to construct
summaries.
Hennig (2009) introduced a query-based la-
tent Semantic Analysis (LSA) automatic text sum-
mariser. It finds statistical semantic relationships
between the extracted sentences rather than word
by word matching relations (Hofmann, 1999).
The summariser selects sentences with the highest
likelihood score.
In our work we used log-likelihood to select
sentences with the maximum sum of log likeli-
hood scores, unlike the traditional method of mea-
suring cosine similarity overlap between articles
or sentences to indicate importance (Luhn, 1958;
Barzilay et al, 2001; Radev et al, 2004). The
main advantage of our approach is that the auto-
matic summariser does not need to compare sen-
tences in a document with an initial one (e.g. first
sentence or a query). Our approach works by cal-
culating the keyness (or log-likelihood) score for
each token (word) in a sentence, then picks, to a
limit of 250 words, the sentences with the highest
sum of the tokens? log-likelihood scores.
To the best of our knowledge the use of corpus-
based frequency list to calculate the log-likelihood
score for text summarisation has not been reported
for the Arabic language.
3 Dataset and Evaluation Metrics
3.1 Test Collection
The test collection for the MultiLing 2013 is avail-
able in the previously mentioned languages.2 The
dataset is based on WikiNews texts.3 The source
documents contain no meta-data or tags and are
represented as UTF8 plain text les. The multi-
document dataset of each language contains (100-
150) articles divided into 10 or 15 reference sets,
each contains 10 related articles discussing the
same topic. The original language of the dataset
is English. The organisers of the tasks were re-
sponsible for translating the corpus into differ-
ent languages by having native speaker partici-
pants for each of the 10 languages. In addi-
tion to the news articles the dataset alo provides
human-generated multi-document gold standard
summaries. The single-document dataset contains
single documents for 40 language (30 documents
each) discussing various topics and collected from
Wikipedia.4
3.2 Evaluation
Evaluating the quality and consistency of a gen-
erated summary has proven to be a difficult prob-
lem (Fiszman et al, 2009). This is mainly because
there is no obvious ideal, objective summary. Two
classes of metrics have been developed: form met-
rics and content metrics. Form metrics focus on
grammaticality, overall text coherence, and organ-
isation. They are usually measured on a point
scale (Brandow et al, 1995). Content metrics are
more difficult to measure. Typically, system out-
put is compared sentence by sentence or unit by
unit to one or more human-generated ideal sum-
maries. As with information retrieval, the per-
centage of information presented in the system?s
summary (precision) and the percentage of impor-
tant information omitted from the summary (re-
call) can be assessed. There are various mod-
els for system evaluation that may help in solving
this problem. This include automatic evaluations
(e.g. ROUGE and AutoSummENG), and human-
performed evaluations. For the MultiLing 2013
task, the summaries generated by the participants
2http://multiling.iit.demokritos.gr/file/all
3http://www.wikinews.org/
4http://www.wikipedia.org/
66
were evaluated automatically based on human-
generated model summaries provided by fluent
speakers of each corresponding language (native
speakers in the general case). The models used
were, ROUGE variations (ROUGE1, ROUGE2,
ROUGE-SU4) (Lin, 2004), the MeMoG varia-
tion (Giannakopoulos and Karkaletsis, 2011) of
AutoSummENG (Giannakopoulos et al, 2008)
and NPowER (Giannakopoulos and Karkaletsis,
2013). ROUGE was not used to evaluate the
single-document summaries.
The summaries were also evaluated manually
by human participants. For the manual evalua-
tion the human evaluators were provided with the
following guidelines: Each summary is to be as-
signed an integer grade from 1 to 5, related to the
overall responsiveness of the summary. We con-
sider a text to be worth a 5, if it appears to cover
all the important aspects of the corresponding doc-
ument set using fluent, readable language. A text
should be assigned a 1, if it is either unreadable,
nonsensical, or contains only trivial information
from the document set. We consider the content
and the quality of the language to be equally im-
portant in the grading.
Note, the human evaluation results for the En-
glish language are not included in this paper as by
the time of writing the results were not yet pub-
lished. We only report the human evaluation re-
sults of the Arabic multi-document summaries.
4 Corpus-based Summarisation
Our summarisation approach is a corpus-based
where we use word frequency lists to compare cor-
pora and calculate the log likelihood score for each
word in the list. The compared corpora include
standard Arabic and English corpora in addition
to the Arabic and English summarisation datasets
provided by MultiLing 2013 for the single and
multi-document summarisation tasks. The subsec-
tions below describe the creation of the word lists
and the standard corpora we used for the compar-
ison process.
4.1 Word Frequencies
We used a simple methodology to generate the
word frequency lists for the Arabic and English
summarisation datasets provided by MultiLing
2013. The datasets used in our experiments were
single-document and multi-document documents
in English and Arabic. For the multi-document
(a) Arabic Sample (b) English Sample
Figure 1: Arabic and English Word Frequency List
Sample
dataset we counted the word frequency for all the
documents in a reference set (group of related arti-
cles), each set contains on average 10 related arti-
cles. The single-document dataset was straightfor-
ward, we calculated word frequencies for all the
words in each document. Figure 1 shows a sam-
ple of random words and their frequencies for both
Arabic and English languages. The sample was se-
lected from the MultiLing dataset word frequency
lists. As shown in the figure we did not eliminate
the stop-words, we treat them as normal words.
4.2 Standard Corpora
In our work we compared the word frequency list
of the summarisation dataset against the larger
Arabic and English standard corpora. For each
of the standard corpora we had a list of word fre-
quencies (up to 5, 000 words) for both Arabic and
English using the frequency dictionary of Ara-
bic (Buckwalter and Parkinson, 2011) and the Cor-
pus of Contemporary American English (COCA)
top 5,000 words (Davies, 2010).
The frequency dictionary of Arabic provides a
list of the 5,000 most frequently used words in
Modern Standard Arabic (MSA) in addition to
several of the most widely spoken Arabic dialects.
The list was created based on a 30-million-word
corpus of Arabic including written and spoken ma-
terial from all around the Arab world. The Ara-
bic summarisation dataset provided by MultiL-
ing 2013 was also written using MSA. The cor-
pus of contemporary American English COCA is
a freely searchable 450-million-word corpus con-
taining text in American English of different num-
ber of genres. To be consistent with the Arabic
67
word frequency list, we used the top 5000 words
from the 450 million word COCA corpus.
5 Summarisation Methodology
In our experiments we used generic single-
document and multi-document extractive sum-
marisers that have been implemented for both
Arabic and English (using identical processing
pipelines for both languages). Summaries were
created by selecting sentences from a single doc-
ument or set of related documents. The following
subsections show the methods used in our exper-
iments, the actual summarisation process and the
experimental setup.
5.1 Calculating Log-Likelihood
We begin the summarisation process by calculat-
ing the log likelihood score for each word in the
word frequency lists (see Section 4.1) using the
same methodology described in (Rayson and Gar-
side, 2000). This was performed by constructing a
contingency table as in Table 1.
Corpus
One
Corpus
Two
Total
Frequency
of Word
a b a+b
Frequency
of other
words
c-a d-b c+d-a-b
Total c d c+d
Table 1: Contingency Table
The values c and d correspond to the number of
words in corpus one and corpus two respectively.
Where a and b are the observed values (O). For
each corpus we calculated the expected value E
using the following formula:
Ei =
Ni
?
i
Oi
?
i
Ni
Ni is the total frequency in corpus i (i in our
case takes the values 1 (c) and 2 (d) for the Multi-
Ling Arabic Summaries dataset and the frequency
dictionary of Arabic (or MultiLing English Sum-
maries dataset and COCA corpus) respectively.
The log-likelihood can be calculated as follows:
LL = 2 ? ((a ? ln(
a
E1
)) + (b ? ln(
b
E2
)))
5.2 Summarisation Process
We used the same processing pipeline for both the
single-document and multi-document summaris-
ers. For each word in the MultiLing summari-
sation dataset (Arabic and English) we calculated
the log likelihood scores using the calculations de-
scribed in Section 5.1. We summed up the log
likelihood scores for each sentence in the dataset
and we picked the sentences (up to 250 word limit)
with the highest sum of log likelihood scores. The
main difference between the single-document and
multi-document summarisers is that we treat the
set of related documents in the multiling dataset
as one document.
6 Single-Document Summarisation Task
MultiLing 2013 this year introduced a new single-
document summarisation pilot for 40 languages
including (Arabic, Czech, English, French, Greek,
Hebrew, Hindi, Spanish, Chinese, Romanian
...etc). In our case we participated in two lan-
guages only, English and Arabic.
The pilot aim was to measure the ability of au-
tomated systems to apply single document sum-
marisation, in the context of Wikipedia texts.
Given a single encyclopedic entry, with several
sections/subsections, describing a specific subject,
the pilot guidelines asked the participating sys-
tems to provide a summary covering the main
points of the entry (similarly to the lead section of
a Wikipedia page). The MultiLing 2013 single-
document summaries dataset consisted of (non-
parallel) documents in the above mentioned lan-
guages.
For the English language, there were 7 partici-
pants (peers) including a baseline system (ID5).
The Arabic language had 6 participants including
the same baseline system.
7 Multi-Document Summarisation Task
The Multi-document summarisation task required
the participants to generate a single, fluent, rep-
resentative summary from a set of documents de-
scribing an event sequence. The language of the
document set was within a given range of lan-
guages and all documents in a set shared the same
language. The task guidelines required the output
summary to be of the same language as its source
documents. The output summary should be 250
words at most.
68
The set of documents were available in 10 lan-
guages (Arabic, Czech, English, French, Greek,
Hebrew, Hindi, Spanish, Chinese and Romanian).
In our case we participated using the Arabic and
English set of documents only.
For the English language, there were 10 partic-
ipants (peers) including a baseline (ID6) and a
topline (ID61) systems. The Arabic language had
10 participants as well, including the same base-
line and topline systems.
The baseline summariser sorted sentences based
on their cosine similarity to the centroid of a clus-
ter. Then starts adding sentences to the summary,
until it either reaches 250 words, or it hits the end
of the document. In the second case, it continues
with the next document in the sorted list.
The topline summariser used information from
the model summaries (i.e. cheats). First, it split all
source documents into sentences. Then it used a
genetic algorithm to generate summaries that have
a vector with maximal cosine similarity to the cen-
troid vector of the model summary texts.
8 Results and Discussion
Our single-document summarisers, both English
and Arabic, performed particularly well in the au-
tomatic evaluation. Ranking first and second re-
spectively.
Tables 2 and 3 illustrate the AutoSummEng
(AutoSumm), MeMoG and NPowER results and
the ranking of our English and Arabic single-
document summarisers (System ID2).
System AutoSumm MeMoG NPowER
ID2 0.136 0.136 1.685
ID41 0.129 0.129 1.661
ID42 0.127 0.127 1.656
ID3 0.127 0.127 1.654
ID1 0.124 0.124 1.647
ID4 0.123 0.123 1.641
ID5 0.040 0.040 1.367
Table 2: English Automatic Evaluation Scores
(single-document)
The evaluation scores of our single-document
summarisers confirm with (Li et al, 2006) and
(Nenkova and Vanderwende, 2005) findings, were
they found that word frequency is an important
feature when determining sentences importance
and that words with high frequency in the input
System AutoSumm MeMoG NPowER
ID3 0.092 0.092 1.538
ID2 0.087 0.087 1.524
ID41 0.055 0.055 1.418
ID42 0.055 0.055 1.416
ID4 0.053 0.053 1.411
ID5 0.025 0.025 1.317
Table 3: Arabic Automatic Evaluation Scores
(single-document)
System Score
ID6 3.711
ID3 3.578
ID2 3.578
ID4 3.489
ID1 3.467
ID11 3.333
ID21 3.111
ID51 2.778
ID5 2.711
ID61 2.489
Table 4: Arabic Manual Evaluation Scores (multi-
document)
documents are very likely to appear in the hu-
man summaries, which explains the high correla-
tion between our single-document and the human
(model) summaries as illustrated in the evalua-
tion scores (Tables 2 and 3). The single-document
summaries were evaluated automatically only.
Our Arabic multi-document summariser per-
formed well in the human evaluation ranking sec-
ond jointly with System ID2. Table 4 shows the
average scores of the human evaluation process,
our system is referred to as ID3. On the other
hand, we did not perform well in the automatic
evaluation of the multi-document summarisation
task for both English and Arabic. Our systems did
not perform better than the baseline. The auto-
matic evaluation results placed our Arabic and En-
glish summariser further down in the ranked lists
of systems compared to the human assessment.
This is an area for future work as this seems to
suggest that the automatic evaluation metrics are
not necessarily in line with human judgements.
The low automatic evaluation scores are due
to two main reasons. First, we treated the set
of related documents (multi-documents) as a sin-
gle big document (See Section 5.2), this penalised
69
our summaries as selecting the sentences with the
maximum sum of log likelihood score lead to
many important sentences being overlooked. This
can be solved by running the summariser on each
document to suggest candidate sentences and then
selecting the top sentence(s) of each document to
generate the final summary. Second, we did not
work on eliminating redundancies. Finally, the
log-likelihood score might be improved by the in-
clusion of a dispersion score or weighting to exam-
ine the evenness of the spread of each word across
all the documents.
9 Conclusion
In this paper we presented the results of our par-
ticipation in the MultiLing 2013 summarisation
task. We submitted results for single-document
and multi-document summarisation in two lan-
guages, English and Arabic. We applied a corpus-
based summariser that used corpus-based word
frequency lists. We used a list of the 5,000 most
frequently used words in Modern Standard Ara-
bic (MSA) and English. Using the frequency dic-
tionary of Arabic and the corpus of contemporary
American English (COCA).
Based on the automatic evaluation scores, we
found that our approach appears to work very well
for Arabic and English single-document summari-
sation. According to the human evaluation scores
the approach could potentially work for Arabic
multi-document summarisation as well. We be-
lieve that the approach could still work well for
multi-document summarisation following the sug-
gested solutions in Section 8.
References
R. Barzilay, N. Elhadad, and K. McKeown. 2001. Sen-
tence Ordering in Multidocument Summarization.
In Proceedings of the First International Conference
on Human Language Technology Research, HLT?01,
pages 1?7, Stroudsburg, PA, USA. Association for
Computational Linguistics.
P. Baxendale. 1958. Machine-made index for technical
literature: an experiment. IBM Journal of Research
and Development, 2(4):354?361.
R. Brandow, K. Mitze, and Lisa F. Rau. 1995.
Automatic Condensation of Electronic Publications
by Sentence Selection. Inf. Process. Manage.,
31(5):675?685.
T. Buckwalter and D. Parkinson. 2011. A Frequency
Dictionary of Arabic: Core Vocabulary for Learn-
ers. Routledge, London, United Kingdom.
J. Conroy, J. Schlesinger, D. O?Leary, and J. Goldstein.
2006. Back to Basics: CLASSY 2006. In Pro-
ceedings of the 6th Document Understanding Con-
ferences. DUC.
M. Davies. 2010. The Corpus of Contemporary Amer-
ican English as the First Reliable Monitor Corpus
of English. Literary and Linguistic Computing,
25:447?464.
F. Douzidia and G. Lapalme. 2004. Lakhas, an Ara-
bic Summarising System. In Proceedings of the 4th
Document Understanding Conferences , pages 128?
135. DUC.
M. Fiszman, D. Demner-Fushman, H. Kilicoglu, and
T. Rindflesch. 2009. Automatic Summarization
of MEDLINE Citations for Evidence-based Medical
Treatment: A Topic-oriented Evaluation. Jouranl of
Biomedical Informatics, 42(5):801?813.
G. Giannakopoulos and V. Karkaletsis. 2011. Au-
toSummENG and MeMoG in Evaluating Guided
Summaries. In The Proceedings of the Text Analysis
Conference, MD, USA. TAC.
G. Giannakopoulos and V. Karkaletsis. 2013. Sum-
mary evaluation: Together we stand npower-ed. In
Alexander Gelbukh, editor, Computational Linguis-
tics and Intelligent Text Processing, volume 7817 of
Lecture Notes in Computer Science, pages 436?450.
Springer Berlin Heidelberg.
G. Giannakopoulos, V. Karkaletsis, G. Vouros, and
P. Stamatopoulos. 2008. Summarization Sys-
tem Evaluation Revisited: N?Gram Graphs. ACM
Transactions on Speech and Language Processing
(TSLP), 5(3):1?39.
G. Giannakopoulos, M. El-Haj, B. Favre, M. Litvak,
J. Steinberger, and V. Varma. 2011. TAC 2011 Mul-
tiLing Pilot Overview. In Text Analysis Conference
(TAC) 2011, MultiLing Summarisation Pilot, Mary-
land, USA. TAC.
S. Granger. 1998. The computer learner corpus: A
versatile new source of data for SLA research. pages
3?18.
L. Hennig. 2009. Topic-based multi-document sum-
marization with probabilistic latent semantic analy-
sis. In Proceedings of the International Conference
RANLP-2009, pages 144?149, Borovets, Bulgaria,
September. Association for Computational Linguis-
tics.
T. Hofmann. 1999. Probabilistic latent semantic in-
dexing. In Proceedings of the 22nd annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, SIGIR ?99,
pages 50?57, New York, NY, USA. ACM.
K. Knight and D. Marcu. 2000. Statistics-Based Sum-
marization ? Step One: Sentence Compression. In
Proceedings of the Seventeenth National Conference
on Artificial Intelligence and Twelfth Conference
70
on Innovative Applications of Artificial Intelligence,
pages 703?710, Menlo Park, CA. AAAI Press.
W. Li, B. Li, and M. Wu. 2006. Query Focus Guided
Sentence Selection Strategy.
C. Lin. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. In Proceedings of the
Workshop on Text Summarization Branches Out
(WAS 2004), pages 25?26. WAS 2004).
H. Luhn. 1958. The Automatic Creation of Literature
Abstracts. IBM Journal of Research and Develop-
ment, 2(2):159?165.
N. Madnani, D. Zajic, B. Dorr, N. Ayan, and J. Lin.
2007. Multiple Alternative Sentence Compressions
for Automatic Text Summarization. In Proceedings
of the 7th Document Understanding Conference at
NLT/NAACL, page 26. DUC.
H. Morita, T. Sakai, and M. Okumura. 2011. Query
Snowball: A Co-occurrence-based Approach to
Multi-document Summarization for Question An-
swering. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies: short papers
- Volume 2, HLT?11, pages 223?229, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
A. Nenkova and L. Vanderwende. 2005. The impact of
frequency on summarization. Microsoft Research,
Redmond, Washington, Tech. Rep. MSR-TR-2005-
101.
D. Radev, H. Jing, M. Sty, and D. Tam. 2004.
Centroid-based Summarization of Multiple Docu-
ments. Information Processing and Management,
40:919?938.
P. Rayson and R. Garside. 2000. Comparing corpora
using frequency profiling. In Proceedings of the
workshop on Comparing corpora - Volume 9, WCC
?00, pages 1?6, Stroudsburg, PA, USA.
P. Rayson, D. Berridge, and B. Francis. 2004. Ex-
tending the cochran rule for the comparison of word
frequencies between corpora. In Proceedings of the
7th International Conference on Statistical analysis
of textual data (JADT 2004, pages 926?936.
M. Scott. 2000. Focusing on the text and its key
words. In Burnard, L. and McEnery, T. (eds.) Re-
thinking language pedagogy from a corpus perspec-
tive: papers from the third international conference
on teaching and language corpora, pages 103?121.
71
