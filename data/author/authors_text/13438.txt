Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 483?489, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
FBM: Combining lexicon-based ML and heuristics
for Social Media Polarities
Carlos Rodr??guez-Penagos, Jordi Atserias, Joan Codina-Filba`,
David Garc??a-Narbona, Jens Grivolla, Patrik Lambert, Roser Saur??
Barcelona Media
Av. Diagonal 177, Barcelona 08018
Corresponding author: carlos.rodriguez@barcelonamedia.org
Abstract
This paper describes the system implemented
by Fundacio? Barcelona Media (FBM) for clas-
sifying the polarity of opinion expressions in
tweets and SMSs, and which is supported by
a UIMA pipeline for rich linguistic and sen-
timent annotations. FBM participated in the
SEMEVAL 2013 Task 2 on polarity classifi-
cation. It ranked 5th in Task A (constrained
track) using an ensemble system combining
ML algorithms with dictionary-based heuris-
tics, and 7th (Task B, constrained) using an
SVM classifier with features derived from the
linguistic annotations and some heuristics.
1 Introduction
We introduce the FBM system for classifying the
polarity of short user-generated text (tweets and
SMSs), which participated in the two subtasks of
SEMEVAL 2013 Task 2 on Sentiment Analysis in
Twitter. These are: Task A. Contextual Polarity Dis-
ambiguation, and Task B. Message Polarity Classifi-
cation. The former aimed at classifying the polarity
of already identified opinion expressions (or cues),
whereas the latter consisted in classifying the polar-
ity of the whole text (Wilson et al, 2013).
The literature agrees on two main approaches for
classifying opinion expressions: using supervised
learning methods and applying dictionary/rule-
based knowledge (see (Liu, 2012) for an overview).
Each of them on its own has been used in work-
able systems, and a principled combination of both
of them can yield good results on noisy data, since
generally one (dictionaries/rules) offers good preci-
sion while the other (ML) is able to discover unseen
examples and thus enhances recall.
FBM combined both approaches in order to bene-
fit from their respective strengths and compensating
as much as possible their weaknesses. For Task A
we used linguistic (lexical and syntactic) annotations
to implement both types of approaches. On the one
hand, we built machine learning classifiers based on
Support Vector Machines (SVMs) and Conditional
Random Fields (CRFs). On the other, we imple-
mented a basic classification system mainly based
on polarity dictionaries and negation information, as
well as simple decision tree-like heuristics extracted
from the training data. For task B we trained an
SVM classifier using some of the annotations from
Task A.
The paper first presents the process of data com-
pilation and preprocessing (section 2), and then de-
scribes the systems for Tasks A (section 3) and B
(section 4). Results and conclusions are discussed
in the last section.
2 Data Compilation and Processing
2.1 Making data available
The corpus of SMSs was provided to the partici-
pants by the organizers of the task. As for the corpus
of tweets, legal restrictions on twitter data distribu-
tion required the participants to download the tex-
tual contents of the corpus from a list of tweet ids.
We retrieved the tweet text using the official twit-
ter API instead of script provided by the organizers,
but not all the tweets were available for download
483
due to restrictions of different types (e.g. geograph-
ical), or because the twitter account was temporarily
suspended. In total, we managed to retrieve 10,764
tweets out of 11,777 ids provided by the organizers
(91.4%). It is worth pointing out that the restric-
tions on tweets distribution can become an issue for
future users of the dataset, as the amount of avail-
able tweets will diminish over time. By contrast, the
twitter test corpus was distributed with the full text
to avoid those problems.
2.2 Leveraging the data with rich linguistic
information
We applied the same linguistic processing to both
corpora (SMSs and tweets), even though the SMS
test data presents very different characteristics from
the twitter data, not only because of what can be ap-
preciated as genre differences, but also due to the
fact that is apparently written in Singaporean En-
glish, which differs significantly from American or
British English. No efforts were made to adapt
our linguistic processing modules and dictionaries
to this data.
Tweets and SMSs were processed with a UIMA1-
based pipeline consisting of a set of linguistic and
opinion-oriented modules, which includes:
Basic linguistic processing: Sentence segmen-
tation, tokenization, POS-tagging, lemmatiza-
tion.
Syntax: Dependency parsing.
Lexicon-based annotations:
? Basic polarity, distinguishing among: positive,
negative, and neutral, as encoded in Wilson et
al. (2010).
? Polarity strength, using the score for pos-
itive and negative polarity in SentiWordnet
3.0 (Baccianella et al, 2010). Each Sen-
tiWordNet synset has an associated triplet of
numerical scores (positive, negative,
and objective) expressing the intensity of
positive, negative and objective polarity of the
terms it contains. They range from 0.0 to 1.0,
and their sum is 1.0 for each synset (Esuli and
Sebastiani, 2007). We selected only the synset
1http://uima.apache.org/uima-specification.html
with positive or negative scores higher than 0.5,
containing a total of 16,791 words.
? Subjectiviy clues, from Wilson et al (2010),
which are classified as weak or strong depend-
ing on their degree of subjectivity.
? Sentiment expressions, from the Linguistic In-
quiry and Word Count (LIWC) 2001 Dictio-
nary (Pennebaker et al, 2001).
? In-house compiled lexicons of negation mark-
ers (such as ?no?, ?never?, ?none?) and quanti-
fiers (?all?, ?many?, etc.), the latter further clas-
sified into low, medium and high according to
their quantification degree.
The different classifiers employed by FBM con-
structed their vectors from this output to learn global
and contextual polarities.
3 Task A: Ensemble System
Our system combined Machine Learning and rule-
based approaches. The aim was to combine the
strengths of each individual component while avoid-
ing as much as possible their weaknesses. In what
follows we describe each system component as well
as the way the ensemble system worked out the col-
lective decisions.
3.1 Conditional Random Fields
One of the classifiers uses the Conditional Random
Fields implementation of a biomedical Named En-
tity Recognition system (JNET from JulieLab) 2, ex-
ploiting the classification capabilities of the system
(rather than its span detection) by strongly associat-
ing already defined ?marked instances? with a polar-
ity, and exploring a 5-word window. It uses depen-
dency labels, POS tags, polar words, sentiwordnet
and LWIC sentiment annotations, as well as indica-
tions for quantifiers and negation markers.
3.2 Support Vector Machines
This classifier was implemented using an SVM algo-
rithm with a linear kernel and the C parameter set to
0.2 (determined using a 5 fold cross-validation). The
features set includes those that we used in RepLab
2http://www.julielab.de
484
2012 (Chenlo et al, 2012) (including number of:
characters, words, links, hashtags, positive and neg-
ative emoticons, question-exclamation marks, ad-
jectives, nouns, verbs, adverbs, uppercased words,
words with duplicated vowels), plus a set of new
features at tweet level obtained from the linguistic
annotations: number of high/medium/low polarity
quantifiers, number of positive and negative polar
words, sentiwordnet applied to both the cue and the
whole tweet.
Moreover, the RepLab polarity calculation based
on different dictionaries was modified to take into
account negation (in a 3-word window) potentially
inverting the polarity (negPol). This polarity mea-
sure was applied to the cue and to the whole tweet,
thus generating two additional features.
3.3 Heuristic Approach
In task A, in parallel to the supervised learning sys-
tem, we developed a method (named Heur) based
on polarity dictionary lookup and simple heuristics
(see Figure 1) taking into account opinion words
as well as negation markers and quantifiers. These
heuristics were implemented so as to maximize the
number of correct positive and negative labels in the
training data. To this end, we calculated the aggre-
gate polarity of a cue segment as the sum of word
polarities found in the polarity lexicon. The aggre-
gate values in the training set ranged from -3 to +3,
taking respectively 1, 0 and -1 as the polarity of pos-
itive, neutral and negative words. The label distri-
bution of cue segments with an aggregate polarity
value of -1 is shown in Table 1.
Aggregate polarity -1
Negation no yes
negative 1,032 30
neutral 37 4
positive 178 71
Table 1: Cue segment polarity statistics in training data
for an aggregate polarity value of -1.
In this case, if no negation is present in the cue
segment, a majority (1,032) of examples had the
negative label. In case there was at least a negation, a
majority (71) of examples had a positive label. This
behaviour was observed with all negative aggregate
1: if has polar word(CUE) then
2: polarity= lex(P)-0.5*lex(QP)
3: -lex(N)+0.5*lex(QN)
4: if polarity>0 then
5: if has negation(CUE) then negative
6: else positive
7: end if
8: else if polarity<0 then
9: if has negation(CUE) then positive
10: else negative
11: end if
12: else
13: if has negation(CUE) then positive
14: else negative
15: end if
16: end if
17: else if has negation(CUE) then negative
18: else
19: polarity= tlex(P)-0.5*tlex(QP)
20: -tlex(N)+0.5*tlex(QN)
21: if polarity<0 then negative
22: else if tlex(NEU)>0 then neutral
23: else if polarity>0 then positive
24: else if has negemo(CUE) then negative
25: else if has posemo(CUE) then positive
26: else unknwn
27: end if
28: end if
Figure 1: Heuristics used by the lexicon-based system to
classify the polarity of a segment marked up as opinion
cue (Task A).
polarity values in training data, yielding the rule in
lines 8 to 11 of Figure 1. Similar rules were ex-
tracted for the other aggregate polarity values (lines
4 to 16 of Figure 1).
Figure 1 details the complete classification algo-
rithm. Note (lines 1 to 17) that we first rely on the
basic polarity lexicon annotations (described in sec-
tion 2). The final aggregate polarity formula (lines
2-3) was refined to distinguish sentiment words
which act as quantifiers, such as pretty in pretty mad.
The word pretty is both a positive polar word and a
quantifier. We want its polarity to be positive in case
it occurs in isolation, but less than one so that the
sum with a following negative polar word (such as
mad) be negative. We thus give this kind of words
a polarity of 0.5 by substracting 0.5 for each polar
word which is also a quantifier. In the polarity for-
mula of lines 2-3, lex(X) refers to the number of
words annotated as X, P and N refer respectively
to positive and negative polar words, and QP and
485
QN refer to positive and negative polar words which
are also quantifiers. Quantifiers which are not polar
words are not taken into account because they are
not likely to change the opinion polarity.
In case that no annotations from the basic polar-
ity, quantifiers, and negative markers lexicons are
found (lines 18 to 28), we look up in dictionaries
built from the training data (tlex in lines 19-20).
To build these dictionaries, we counted how many
times each word was labeled positive, negative and
neutral. We considered that a word has a given po-
larity if the number of times it was assigned to this
class is greater than the number of times it was as-
signed to any other class by a given threshold. We
calculated the polarity in the same way as before,
but now with the counts from the lexicon automati-
cally compiled from the training data. To improve
the recall of the dictionary lookup, we performed
some text normalization: lowercasing, deletion of
repeated characters (such as gooood) and deletion of
the hashtag ?#? character. Finally, if no polar word
is found in the automatically compiled lexicon, we
look at the sentiment annotations (extracted from the
LIWC dictionary).
3.4 Ensemble Voting Algorithm
As already mentioned, we combined the results from
the described polarity methods to build a collective
decision. Table 2 shows the performance (in terms
of F1 measure) of the different single methods over
the tweet test data.
SVM Heur Heur+ CRF
Test 80.74 83.47 84.62 62.85
Table 2: Twitter Task A results for different methods
Although the heuristic method outperforms the
ML methods, they are not only different in nature
(ML vs. heuristic) but also use different information
(see Table 5). This suggests that the ensemble solu-
tion will be complementary and capable of obtaining
better results than any of the individual methods by
itself.
The development set was used to calculate the en-
semble response given the individual votes of the
different systems in a way similar to the behavior
knowledge space method (Huang and Suen, 1993).
Table 3 shows an example of how the assemble
voting is built. For each method vote combina-
tion (SVM-Heuristics-CRF) the number of positives
/ negatives / neutral is calculated in the development
data. The ensemble (EV) selects the vote that max-
imizes the number of correct votes in the develop-
ment data (in bold).
SVM Heur CRF EV
# Instances
pos neg neu
? + ? ? 0 6 0
? ? + ? 1 23 2
? ? ? ? 3 125 2
? u + + 1 0 0
+ u n ? 0 1 0
+ ? + + 17 13 2
+ + + + 314 18 17
+ ? n + 3 1 0
Table 3: Oracle building example (EV: Ensemble Vote,
+:positive, ?:negative, n:neutral, u:unknown)
The test data contains some combination of votes
that were not seen in the development data. Thus,
in order to deal with these unseen combinations of
votes in the test set we use the following backup
heuristics based on the preformance figures of the
individual methods: Use the vote of the heuristic
method. If this method does not vote (u), then se-
lect the SVM vote.
Table 4 shows the results of the proposed ensem-
ble method, the well-known majority voting and the
upper bound of this ensemble method (calculated
with the same strategy over the test data), over the
development and test tweet data
Ensemble Majority Upper
Voting Voting Bound
Dev 85.48 81.31 85.48
Test 85.50 82.70 89.37
Table 4: Results for different ensemble strategies
In the development corpus, the upper bound and
ensemble results are the same, given that they ap-
ply the same knowledge. The difference is in the
test dataset, where the ensemble voting is calculated
based on the knowledge obtained from the develop-
ment corpus, while the upper bound uses the knowl-
edge that can be derived from the test corpus.
486
Table 5 illustrates the features used by each com-
ponent.
SVM SVM CRF Heur
(task A) (task B)
word ? ? ?
lemma
pos ? ?
deps ?
pol ? ? ? ?
polW ?
sent ? ? ?
sentiwn ? ? ?
quant ? ? ? ?
neg ? ? ? ?
links ?
hashTags ?
Table 5: Information used (pos: part-of-speech; deps: de-
pendencies; pol: basic polarity classification; polW: basic
polarity word; sent: LIWC sentiments; sentwn: Senti-
Wordnet; quant/neg: quantifiers and negation markers.)
4 Task B: A Support Vector
Machine-based System
The system presented for task B is based on ML us-
ing a SVM model. The feature vector used as input
for the SVM component is composed of the annota-
tions provided by the linguistic annotation pipeline,
extended with a feature obtained by applying nega-
tion to the next polar words (window of size 3).
The features used do not include the words (or
their lemmas) because the number of tweets avail-
able for training is small (104) compared to the num-
ber of different words (4 ? 104). A model based on
bag-of-words would suffer from overfitting and thus
be very domain and time-dependent. If the train and
test sets were randomly selected from a bigger set,
the use of words could increase the model?s accu-
racy, but the model would also be too narrowly ap-
plied to this specific dataset.
From the annotation pipeline we extracted as fea-
tures: the polar words (PolW) and their basic po-
larity (Pol); the sentiment annotations from LIWC
(Sent); the negation markers (Neg) and quantifiers
(Quant). The model was trained using Weka (Hall
et al, 2009).
The model used is SVM with the C parameter set
to 1.0 and applying a 10 fold cross-validation. The
option of doing first a model to discriminate polar
and neutral tweets was discarded because Weka al-
ready does that when training classifiers for more
than two training classes, and the combination of the
two classifiers (a first one between polar and opin-
ionated and a second one between positive and neg-
ative) would produce the same results.
5 Results and Discussion
The results of our system in each subcorpus and task
are presented in Table 5 (average of the F1-measure
over the classes positive and negative, constrained
track), with the ranking achieved in the competition
in parentheses.
Tweet Corpus SMS Corpus
Task A 0.86 (5th) 0.73 (11th)
Task B 0.61 (7th) 0.47 (28th)
Table 6: FBM system performance (F1 average over pos-
itive and negative classes, constrained track) and rankings
Given the differences in style and vocabularies be-
tween the SMS and tweet corpora, and the fact that
we made not effort whatsoever to adapt our system
or models to them, the drop in performance from
one to the other is considerable, but to be expected
since domain customization is an important aspect
of opinion mining.
Task A: The confusion matrix in Table 7 shows
an acceptable performance for the most frequent
classes in the corpus (with an error of 7.75% and
19.5% for postive and negative cues, respectively)
and a very poor job for neutral cues (98.1% of er-
ror), clearly a minority class in the training corpus
(5% of the data).
GOLD: Pos Neg Neu
SYSTEM: Pos 2,522 296 126
Neg 206 1,240 31
Neu 6 5 3
Table 7: Task A confusion matrix
Given the skewed distribution of polarity cate-
gories in the test corpus, however, neutral mistakes
amount to only 23% of our system error, and so we
487
focus our analysis on the problems in positive and
negative cues, respectively amounting to 31.7% and
44.8% of the total error. There are 2 main sources of
error:
? Limitations of the dictionaries employed,
which were short in covering somewhat fre-
quent slang words (e.g., wacky, baddest, shit-
loads), expressions (e.g., ouch, yukk, C?MON),
or phrases (e.g., over the top), some of which
express a particular polarity but contain a word
expressing just the opposite (have a blast, to
want something bad/ly).
? Problems in UGC processing, mainly related to
normalization (e.g., fooooool) and tokenization
(Perfect...not sure), which put at risk the cor-
rect identification of lexical elements that are
crucial for polarity classification.
Task B: The average F-score of positive and neg-
ative classes was 0.62 in the development set (that
was included in the training set) and the averaged F-
score for the test set was 0.61 (so they are very simi-
lar). If focusing on precision and recall, the positive
and negative classes have higher precision but lower
recall in the test set. We think that this low degrada-
tion of perfomance indicates the model?s potential
for generalization.
6 Conclusions
From our results, we can conclude that the use of
ensemble combination of orthogonal methods pro-
vides good performance for Task A. Similar results
could be expected for Task B (judging from mix-
ing dictionaries and ML in similar tasks at RepLab
2012 (Chenlo et al, 2012)). The ML methods that
we applied for Task B are essentially additive, and
hence have difficulties in applying features such as
polarity shifters. To overcome this, one of the fea-
tures includes negation of polar words when a polar-
ity shifter is near.
Overall, the SemEval Tasks have make evident the
usual challenges when mining opinions from Social
Media channels: noisy text, irregular grammar and
orthography, highly specific lingo, etc. Moreover,
temporal dependencies can affect the performance if
the training and test data have been gathered at dif-
ferent times, as is the case with text of such a volatile
nature as tweets and SMSs.
0.00%
5.00%
10.00%
15.00%
20.00%
25.00%
30.00%
35.00%
40.00%
45.00%
50.00%
train
dev
test
Figure 2: Distribution of tweets over time
The histogram in Figure 2 shows that this also ap-
plies to the Semeval tweets dataset. It illustrates the
distribution of tweets over time (extrapolated from
the sequential ids) in the 3 subcorpora (train, devel-
opment and test), showing some divergence between
the test corpus on the one hand, and the develop-
ment and training corpora on the other. Neverthe-
less, our system shows little performance degrada-
tion between development and testing results, as at-
tested in Table 4 (ensemble voting column).
Our work here and at other competitions already
cited validate a system that combines stochastic and
symbolic methodologies in a principled, data-driven
approach. Time and domain dependencies of Social
Media data make system and model generalization
highly desirable, and our system hybrid nature also
contribute to this objective.
Acknowledgments
This work has been partially funded by the Spanish
Government project Holopedia, TIN2010-21128-
C02-02, the CENIT program project Social Media,
CEN-20101037, and the Marie Curie Reintegration
Grant PIRG04-GA-2008-239414.
488
References
Baccianella, Stefano, Andrea Esuli and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of the 7th conference on International
Language Resources and Evaluation, Valletta, Malta.
Chenlo, Jose M., Jordi Atserias, Carlos Rodr??guez-
Penagos and Roi Blanco. 2012. FBM-Yahoo!
at RepLab 2012. In: P. Forner, J. Karlgren,
C. Womser-Hacker (eds.) CLEF 2012 Evalua-
tion Labs and Workshop, Online Working Notes.
http://clef2012.org/index.php?page=Pages/procee-
dings.php.
Esuli, Andrea and Fabrizio Sebastiani. 2007. SEN-
TIWORDNET: a high-coverage lexical resource for
opinion mining. Technical Report ISTI-PP-002/2007,
Institute of Information Science and Technologies
(ISTI) of the Italian National Research Council
(CNR).
Hall, Mark, Frank Eibe, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann and Ian H. Witten. 2009.
The WEKA data mining software: an update. In:
ACM SIGKDD Explorations Newsletter, 1: 10?18.
Huang, Y. S. and C. Y. Suen. 1993. Behavior-knowledge
space method for combination of multiple classifiers.
In Proceedings of IEEE Computer Vision and Pattern
Recognition, 347?352.
Liu, Bing. 2012. Sentiment analysis and opinion mining.
Synthesis Lectures on Human Language Technologies,
(5-1), 1?167.
Pennebaker, James W., Martha E. Francis and Roger
J. Booth. 2001. Linguistic inquiry and word count:
LIWC 2001. Mahway: Lawrence Erlbaum Asso-
ciates.
Wilson, Theresa, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov and Alan. Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ?13.
Wilson, Theresa, Janyce Wiebe and Paul Hoffmann.
2010. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analysis.
Computational Linguistics, 35(3), 399?433.
489
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 114?121,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Opinion Mining of Spanish Customer Comments with Non-Expert
Annotations on Mechanical Turk
Bart Mellebeek, Francesc Benavent, Jens Grivolla,
Joan Codina, Marta R. Costa-jussa` and Rafael Banchs
Barcelona Media Innovation Center
Av. Diagonal, 177, planta 9
08018 Barcelona, Spain
{bart.mellebeek|francesc.benavent|jens.grivolla|joan.codina|
marta.ruiz|rafael.banchs}@barcelonamedia.org
Abstract
One of the major bottlenecks in the develop-
ment of data-driven AI Systems is the cost of
reliable human annotations. The recent ad-
vent of several crowdsourcing platforms such
as Amazon?s Mechanical Turk, allowing re-
questers the access to affordable and rapid re-
sults of a global workforce, greatly facilitates
the creation of massive training data. Most
of the available studies on the effectiveness of
crowdsourcing report on English data. We use
Mechanical Turk annotations to train an Opin-
ion Mining System to classify Spanish con-
sumer comments. We design three different
Human Intelligence Task (HIT) strategies and
report high inter-annotator agreement between
non-experts and expert annotators. We evalu-
ate the advantages/drawbacks of each HIT de-
sign and show that, in our case, the use of
non-expert annotations is a viable and cost-
effective alternative to expert annotations.
1 Introduction
Obtaining reliable human annotations to train data-
driven AI systems is often an arduous and expensive
process. For this reason, crowdsourcing platforms
such as Amazon?s Mechanical Turk1, Crowdflower2
and others have recently attracted a lot of attention
from both companies and academia. Crowdsourc-
ing enables requesters to tap from a global pool of
non-experts to obtain rapid and affordable answers
to simple Human Intelligence Tasks (HITs), which
1https://www.mturk.com
2http://crowdflower.com/
can be subsequently used to train data-driven appli-
cations.
A number of recent papers on this subject point
out that non-expert annotations, if produced in a suf-
ficient quantity, can rival and even surpass the qual-
ity of expert annotations, often at a much lower cost
(Snow et al, 2008), (Su et al, 2007). However, this
possible increase in quality depends on the task at
hand and on an adequate HIT design (Kittur et al,
2008).
In this paper, we evaluate the usefulness of MTurk
annotations to train an Opinion Mining System to
detect opinionated contents (Polarity Detection) in
Spanish customer comments on car brands. Cur-
rently, a large majority of MTurk tasks is designed
for English speakers. One of our reasons for partic-
ipating in this shared task was to find out how easy
it is to obtain annotated data for Spanish. In addi-
tion, we want to find out how useful these data are
by comparing them to expert annotations and using
them as training data of an Opinion Mining System
for polarity detection.
This paper is structured as follows. Section 2 con-
tains an explanation of the task outline and our goals.
Section 3 contains a description of three different
HIT designs that we used in this task. In Section
4, we provide a detailed analysis of the retrieved
HITs and focus on geographical information of the
workers, the correlation between the different HIT
designs, the quality of the retrieved answers and on
the cost-effectiveness of the experiment. In Section
5, we evaluate the incidence of MTurk-generated an-
notations on a polarity classification task using two
different experimental settings. Finally, we conclude
114
in Section 6.
2 Task Outline and Goals
We compare different HIT design strategies by eval-
uating the usefulness of resulting Mechanical Turk
(MTurk) annotations to train an Opinion Mining
System on Spanish consumer data. More specifi-
cally, we address the following research questions:
(i) Annotation quality: how do the different
MTurk annotations compare to expert annotations?
(ii) Annotation applicability: how does the per-
formance of an Opinion Mining classifier vary after
training on different (sub)sets of MTurk and expert
annotations?
(iii) Return on Investment: how does the use of
MTurk annotations compare economically against
the use of expert annotations?
(iv) Language barriers: currently, most MTurk
tasks are designed for English speakers. How easy
is it to obtain reliable MTurk results for Spanish?
3 HIT Design
We selected a dataset of 1000 sentences contain-
ing user opinions on cars from the automotive sec-
tion of www.ciao.es (Spanish). This website was
chosen because it contains a large and varied pool
of Spanish customer comments suitable to train an
Opinion Mining System and because opinions in-
clude simultaneously global numeric and specific
ratings over particular attributes of the subject mat-
ter. Section 5.1 contains more detailed information
about the selection of the dataset. An example of a
sentence from the data set can be found in (1):
(1) ?No te lo pienses ma?s, co?mpratelo!?
(= ?Don?t think twice, buy it!?)
The sentences in the dataset were presented to
the MTurk workers in three different HIT designs.
Each HIT design contains a single sentence to be
evaluated. HIT1 is a simple categorization scheme
in which workers are asked to classify the sentence
as being either positive, negative or neutral, as is
shown in Figure 1b. HIT2 is a graded categorization
template in which workers had to assign a score be-
tween -5 (negative) and +5 (positive) to the example
sentence, as is shown in Figure 1c. Finally, HIT3 is
a continuous triangular scoring template that allows
Figure 1: An example sentence (a) and the three HIT
designs used in the experiments: (b) HIT1: a simple
categorization scheme, (c) HIT2: a graded categoriza-
tion scheme, and (d) HIT3: a continuous triangular scor-
ing scheme containing both a horizontal positive-negative
axis and a vertical subjective-objective axis.
workers to use both a horizontal positive-negative
axis and a vertical subjective-objective axis by plac-
ing the example sentence anywhere inside the trian-
gle. The subjective-objective axis expresses the de-
gree to which the sentence contains opinionated con-
tent and was earlier used by (Esuli and Sebastiani,
2006). For example, the sentence ?I think this is a
wonderful car? clearly marks an opinion and should
be positioned towards the subjective end, while the
sentence ?The car has six cilinders? should be lo-
cated towards the objective end. Figure 1d contains
an example of HIT3. In order not to burden the
workers with overly complex instructions, we did
not mention this subjective-objective axis but asked
them instead to place ambiguous sentences towards
the center of the horizontal positive-negative axis
and more objective, non-opinionated sentences to-
wards the lower neutral tip of the triangle.
115
For each of the three HIT designs, we speci-
fied the requirement of three different unique as-
signments per HIT, which led to a total amount of
3 ? 3 ? 1000 = 9000 HIT assignments being up-
loaded on MTurk. Mind that setting the requirement
of unique assigments ensures a number of unique
workers per individual HIT, but does not ensure a
consistency of workers over a single batch of 1000
HITs. This is in the line with the philosophy of
crowdsourcing, which allows many different people
to participate in the same task.
4 Annotation Task Results and Analysis
After designing the HITs, we uploaded 30 random
samples for testing purposes. These HITs were com-
pleted in a matter of seconds, mostly by workers in
India. After a brief inspection of the results, it was
obvious that most answers corresponded to random
clicks. Therefore, we decided to include a small
competence test to ensure that future workers would
possess the necessary linguistic skills to perform the
task. The test consists of six simple categorisation
questions of the type of HIT1 that a skilled worker
would be able to perform in under a minute. In order
to discourage the use of automatic translation tools,
a time limit of two minutes was imposed and most
test sentences contain idiomatic constructions that
are known to pose problems to Machine Translation
Systems.
4.1 HIT Statistics
Table 1 contains statistics on the workers who com-
pleted our HITs. A total of 19 workers passed the
competence test and submitted at least one HIT. Of
those, four workers completed HITs belonging to
two different designs and six submitted HITs in all
three designs. Twelve workers are located in the US
(64%), three in Spain (16%), one in Mexico (5%),
Equador (5%), The Netherlands (5%) and an un-
known location (5%).
As to a comparison of completion times, it took
a worker on average 11 seconds to complete an in-
stance of HIT1, and 9 seconds to complete an in-
stance of HIT2 and HIT3. At first sight, this result
might seem surprising, since conceptually there is an
increase in complexity when moving from HIT1 to
HIT2 and from HIT2 to HIT3. These results might
Overall HIT1 HIT2 HIT3
ID C % # sec. # sec. # sec.
1 mx 29.9 794 11.0 967 8.6 930 11.6
2 us 27.6 980 8.3 507 7.8 994 7.4
3 nl 11.0 85 8.3 573 10.9 333 11.4
4 us 9.5 853 16.8 - - - -
5 es 9.4 - - 579 9.1 265 8.0
6 ec 4.1 151 9.4 14 16.7 200 13.0
7 us 3.6 3 15.7 139 8.5 133 11.6
8 us 2.2 77 8.2 106 7.3 11 10.5
9 us 0.6 - - - - 50 11.2
10 us 0.5 43 5.3 1 5 - -
11 us 0.4 - - 38 25.2 - -
12 us 0.4 - - 10 9.5 27 10.8
13 es 0.4 - - - - 35 15.1
14 es 0.3 - - 30 13.5 - -
15 us 0.3 8 24.7 18 21.5 - -
16 us 0.2 - - - - 22 8.9
17 us 0.2 - - 17 16.5 - -
18 ? 0.1 6 20 - - - -
19 us 0.1 - - 1 33 - -
Table 1: Statistics on MTurk workers for all three HIT
designs: (fictional) worker ID, country code, % of total
number of HITs completed, number of HITs completed
per design and average completion time.
suggest that users find it easier to classify items
on a graded or continuous scale such as HIT2 and
HIT3, which allows for a certain degree of flexibil-
ity, than on a stricter categorical template such as
HIT1, where there is no room for error.
4.2 Annotation Distributions
In order to get an overview of distribution of the re-
sults of each HIT, a histogram was plotted for each
different task. Figure 2a shows a uniform distribu-
tion of the three categories used in the simple cat-
egorization scheme of HIT1, as could be expected
from a balanced dataset.
Figure 2b shows the distribution of the graded cat-
egorization template of HIT2. Compared to the dis-
tribution in 2a, two observations can be made: (i)
the proportion of the zero values is almost identical
to the proportion of the neutral category in Figure
2a, and (ii) the proportion of the sum of the positive
values [+1,+5] and the proportion of the sum of the
negative values [-5,-1] are equally similar to the pro-
portion of the positive and negative categories in 2a.
This suggests that in order to map the graded annota-
tions of HIT2 to the categories of HIT1, an intuitive
partitioning of the graded scale into three equal parts
should be avoided. Instead, a more adequate alterna-
tive would consist of mapping [-5,-1] to negative, 0
116
Figure 2: Overview of HIT results: a) distribution of the three categories used in HIT1, b) distribution of results in the
scaled format of HIT2, c) heat map of the distribution of results in the HIT3 triangle, d) distribution of projection of
triangle data points onto the X-axis (positive/negative).
to neutral and [+1,+5] to positive. This means that
even slightly positive/negative grades correspond to
positive/negative categories.
Figure 2c shows a heat map that plots the distri-
bution of the annotations in the triangle of HIT3. It
appears that worker annotations show a spontaneous
tendency of clustering, despite the continuous nature
of the design. This suggests that this HIT design,
originally conceived as continuous, was transformed
by the workers as a simpler categorization task using
five labels: negative, ambiguous and positive at the
top, neutral at the bottom, and other in the center.
Figure 2d shows the distribution of all data-
points in the triangle of Figure 2c, projected onto
the X-axis (positive/negative). Although similar to
the graded scale in HIT2, the distribution shows a
slightly higher polarization.
These results suggest that, out of all three HIT de-
signs, HIT2 is the one that contains the best balance
between the amount of information that can be ob-
tained and the simplicity of a one-dimensional an-
notation.
4.3 Annotation Quality
The annotation quality of MTurk workers can be
measured by comparing them to expert annotations.
This is usually done by calculating inter-annotator
agreement (ITA) scores. Note that, since a single
HIT can contain more than one assignment and each
assignment is typically performed by more than one
annotator, we can only calculate ITA scores between
batches of assignments, rather than between individ-
ual workers. Therefore, we describe the ITA scores
in terms of batches. In Table 4.4, we present a com-
parison of standard kappa3 calculations (Eugenio
and Glass, 2004) between batches of assignments in
HIT1 and expert annotations.
We found an inter-batch ITA score of 0.598,
which indicates a moderate agreement due to fairly
consistent annotations between workers. When
comparing individual batches with expert annota-
tions, we found similar ITA scores, in the range be-
tween 0.628 and 0.649. This increase with respect
to the inter-batch score suggests a higher variability
among MTurk workers than between workers and
experts. In order to filter out noise in worker annota-
tions, we applied a simple majority voting procedure
in which we selected, for each sentence in HIT1, the
most voted category. This results in an additional
3In reality, we found that fixed and free margin Kappa values
were almost identical, which reflects the balanced distribution
of the dataset.
117
batch of annotations. This batch, refered in Table
4.4 as Majority, produced a considerably higher ITA
score of 0.716, which confirms the validity of the
majority voting scheme to obtain better annotations.
In addition, we calculated ITA scores between
three expert annotators on a separate, 500-sentence
dataset, randomly selected from the same corpus as
described at the start of Section 3. This collection
was later used as test set in the experiments de-
scribed in Section 5. The inter-expert ITA scores
on this separate dataset contains values of 0.725 for
?1 and 0.729 for ?2, only marginally higher than the
Majority ITA scores. Although we are comparing
results on different data sets, these results seem to
indicate that multiple MTurk annotations are able to
produce a similar quality to expert annotations. This
might suggest that a further increase in the number
of HIT assignments would outperform expert ITA
scores, as was previously reported in (Snow et al,
2008).
4.4 Annotation Costs
As explained in Section 3, a total amount of 9000
assignments were uploaded on MTurk. At a reward
of .02$ per assignment, a total sum of 225$ (180$
+ 45$ Amazon fees) was spent on the task. Work-
ers perceived an average hourly rate of 6.5$/hour for
HIT1 and 8$/hour for HIT2 and HIT3. These fig-
ures suggest that, at least for assignments of type
HIT2 and HIT3, a lower reward/assignment might
have been considered. This would also be consis-
tent with the recommendations of (Mason and Watts,
2009), who claim that lower rewards might have an
effect on the speed at which the task will be com-
pleted - more workers will be competing for the task
at any given moment - but not on the quality. Since
we were not certain whether a large enough crowd
existed with the necessary skills to perform our task,
we explicitly decided not to try to offer the lowest
possible price.
An in-house expert annotator (working at approx-
imately 70$/hour, including overhead) finished a
batch of 1000 HIT assignments in approximately
three hours, which leads to a total expert annotator
cost of 210$. By comparing this figure to the cost
of uploading 3 ? 1000 HIT assignments (75$), we
saved 210 ? 75 = 135$, which constitutes almost
65% of the cost of an expert annotator. These figures
do not take into account the costs of preparing the
data and HIT templates, but it can be assumed that
these costs will be marginal when large data sets are
used. Moreover, most of this effort is equally needed
for preparing data for in-house annotation.
?1 ?2
Inter-batch 0.598 0.598
Batch 1 vs. Expert 0.628 0.628
Batch 2 vs. Expert 0.649 0.649
Batch 3 vs. Expert 0.626 0.626
Majority vs. Expert 0.716 0.716
Experts4 0.725 0.729
Table 2: Interannotation Agreement as a measure of qual-
ity of the annotations in HIT1. ?1 = Fixed Margin
Kappa. ?2 = Free Margin Kappa.
5 Incidence of annotations on supervised
polarity classification
This section intends to evaluate the incidence of
MTurk-generated annotations on a polarity classifi-
cation task. We present two different evaluations.
In section 5.2, we compare the results of training
a polarity classification system with noisy available
metadata and with MTurk generated annotations of
HIT1. In section 5.3, we compare the results of
training several polarity classifiers using different
training sets, comparing expert annotations to those
obtained with MTurk.
5.1 Description of datasets
As was mentioned in Section 3, all sentences were
extracted from a corpus of user opinions on cars
from the automotive section of www.ciao.es
(Spanish). For conducting the experimental evalu-
ation, the following datasets were used:
1. Baseline: constitutes the dataset used for train-
ing the baseline or reference classifiers in Ex-
periment 1. Automatic annotation for this
dataset was obtained by using the following
naive approach: those sentences extracted from
comments with ratings5 equal to 5 were as-
signed to category ?positive?, those extracted
5The corpus at www.ciao.es contains consumer opinions
marked with a score between 1 (negative) and 5 (positive).
118
from comments with ratings equal to 3 were
assigned to ?neutral?, and those extracted from
comments with ratings equal to 1 were assigned
to ?negative?. This dataset contains a total of
5570 sentences, with a vocabulary coverage of
11797 words.
2. MTurk Annotated: constitutes the dataset that
was manually annotated by MTurk workers in
HIT1. This dataset is used for training the con-
trastive classifiers which are to be compared
with the baseline system in Experiment 1. It
is also used in various ways in Experiment 2.
The three independent annotations generated
by MTurk workers for each sentence within this
dataset were consolidated into one unique an-
notation by majority voting: if the three pro-
vided annotations happened to be different6,
the sentence was assigned to category ?neutral?;
otherwise, the sentence was assigned to the cat-
egory with at least two annotation agreements.
This dataset contains a total of 1000 sentences,
with a vocabulary coverage of 3022 words.
3. Expert Annotated: this dataset contains the
same sentences as the MTurk Annotated one,
but with annotations produced internally by
known reliable annotators7. Each sentence re-
ceived one annotation, while the dataset was
split between a total of five annotators.
4. Evaluation: constitutes the gold standard used
for evaluating the performance of classifiers.
This dataset was manually annotated by three
experts in an independent manner. The gold
standard annotation was consolidated by using
the same criterion used in the case of the pre-
vious dataset8. This dataset contains a total of
500 sentences, with a vocabulary coverage of
2004 words.
6This kind of total disagreement among annotators occurred
only in 13 sentences out of 1000.
7While annotations of this kind are necessarily somewhat
subjective, these annotations are guaranteed to have been pro-
duced in good faith by competent annotators with an excellent
understanding of the Spanish language (native or near-native
speakers)
8In this case, annotator inter-agreement was above 80%, and
total disagreement among annotators occurred only in 1 sen-
tence out of 500
Baseline Annotated Evaluation
Positive 1882 341 200
Negative 1876 323 137
Neutral 1812 336 161
Totals 5570 1000 500
Table 3: Sentence-per-category distributions for baseline,
annotated and evaluation datasets.
These three datasets were constructed by ran-
domly extracting sample sentences from an origi-
nal corpus of over 25000 user comments contain-
ing more than 1000000 sentences in total. The sam-
pling was conducted with the following constraints
in mind: (i) the three resulting datasets should not
overlap, (ii) only sentences containing more than
3 tokens are considered, and (iii) each resulting
dataset must be balanced, as much as possible, in
terms of the amount of sentences per category. Table
3 presents the distribution of sentences per category
for each of the three considered datasets.
5.2 Experiment one: MTurk annotations vs.
original Ciao annotations
A simple SVM-based supervised classification ap-
proach was considered for the polarity detection task
under consideration. According to this, two dif-
ferent groups of classifiers were used: a baseline
or reference group, and a contrastive group. Clas-
sifiers within these two groups were trained with
data samples extracted from the baseline and anno-
tated datasets, respectively. Within each group of
classifiers, three different binary classification sub-
tasks were considered: positive/not positive, nega-
tive/not negative and neutral/not neutral. All trained
binary classifiers were evaluated by computing pre-
cision and recall for each considered category, as
well as overall classification accuracy, over the eval-
uation dataset.
A feature space model representation of the data
was constructed by considering the standard bag-of-
words approach. In this way, a sparse vector was ob-
tained for each sentence in the datasets. Stop-word
removal was not conducted before computing vec-
tor models, and standard normalization and TF-IDF
weighting schemes were used.
Multiple-fold cross-validation was used in all
conducted experiments to tackle with statistical vari-
119
classifier baseline annotated
positive/not positive 59.63 (3.04) 69.53 (1.70)
negative/not negative 60.09 (2.90) 63.73 (1.60)
neutral/not neutral 51.27 (2.49) 62.57 (2.08)
Table 4: Mean accuracy over 20 independent simula-
tions (with standard deviations provided in parenthesis)
for each classification subtasks trained with either the
baseline or the annotated dataset.
ability of the data. In this sense, twenty independent
realizations were actually conducted for each exper-
iment presented and, instead of individual output re-
sults, mean values and standard deviations of evalu-
ation metrics are reported.
Each binary classifier realization was trained with
a random subsample set of 600 sentences extracted
from the training dataset corresponding to the clas-
sifier group, i.e. baseline dataset for reference sys-
tems, and annotated dataset for contrastive systems.
Training subsample sets were always balanced with
respect to the original three categories: ?positive?,
?negative? and ?neutral?.
Table 4 presents the resulting mean values of
accuracy for each considered subtask in classifiers
trained with either the baseline or the annotated
dataset. As observed in the table, all subtasks ben-
efit from using the annotated dataset for training
the classifiers; however, it is important to mention
that while similar absolute gains are observed for
the ?positive/not positive? and ?neutral/not neutral?
subtasks, this is not the case for the subtask ?neg-
ative/not negative?, which actually gains much less
than the other two subtasks.
After considering all evaluation metrics, the bene-
fit provided by human-annotated data availability for
categories ?neutral? and ?positive? is evident. How-
ever, in the case of category ?negative?, although
some gain is also observed, the benefit of human-
annotated data does not seem to be as much as for
the two other categories. This, along with the fact
that the ?negative/not negative? subtask is actually
the best performing one (in terms of accuracy) when
baseline training data is used, might suggest that
low rating comments contains a better representa-
tion of sentences belonging to category ?negative?
than medium and high rating comments do with re-
spect to classes ?neutral? and ?positive?.
In any case, this experimental work only verifies
the feasibility of constructing training datasets for
opinionated content analysis, as well as it provides
an approximated idea of costs involved in the gener-
ation of this type of resources, by using MTurk.
5.3 Experiment two: MTurk annotations vs.
expert annotations
In this section, we compare the results of training
several polarity classifiers on six different training
sets, each of them generated from the MTurk anno-
tations of HIT1. The different training sets are: (i)
the original dataset of 1000 sentences annotated by
experts (Experts), (ii) the first set of 1000 MTurk re-
sults (Batch1), (iii) the second set of 1000 MTurk
results (Batch2), (iv) the third set of 1000 MTurk
results (Batch3), (v) the batch obtained by major-
ity voting between Batch1, Batch2 and Batch3 (Ma-
jority), and (vi) a batch of 3000 training instances
obtained by aggregating Batch1, Batch2 and Batch3
(All). We used classifiers as implemented in Mal-
let (McCallum, 2002) and Weka (Hall et al, 2009),
based on a simple bag-of-words representation of
the sentences. As the objective was not to obtain
optimum performance but only to evaluate the dif-
ferences between different sets of annotations, all
classifiers were used with their default settings.
Table 5 contains results of four different clas-
sifiers (Maxent, C45, Winnow and SVM), trained
on these six different datasets and evaluated on the
same 500-sentence test set as explained in Section
5.1. Classification using expert annotations usu-
ally outperforms classification using a single batch
(one annotation per sentence) of annotations pro-
duced using MTurk. Using the tree annotations per
sentence available from MTurk, all classifiers reach
similar or better performance compared to the sin-
gle set of expert annotations, at a much lower cost
(as explained in section 4.4).
It is interesting to note that most classifiers bene-
fit from using the full 3000 training examples (1000
sentences with 3 annotations each), which intu-
itively makes sense as the unanimously labeled ex-
amples will have more weight in defining the model
of the corresponding class, whereas ambiguous or
unclear cases will have their impact reduced as their
characteristics are attributed to various classes.
On the contrary, Support Vector Machines show
120
System
E
xp
er
ts
B
at
ch
1
B
at
ch
2
B
at
ch
3
M
aj
or
it
y
A
ll
Winnow 44.2 43.6 40.4 47.6 46.2 50.6
SVM 57.6 53.0 55.4 54.0 57.2 52.8
C45 42.2 33.6 42.0 41.2 41.6 45.0
Maxent 59.2 55.8 57.6 54.0 57.6 58.6
Table 5: Accuracy figures of four different classifiers
(Winnow, SVM, C45 and Maxent) trained on six different
datasets (see text for details).
an important drop in performance when using mul-
tiple annotations, but perform well when using the
majority vote. As a first intuition, this may be due to
the fact that SVMs focus on detecting class bound-
aries (and optimizing the margin between classes)
rather than developing a model of each class. As
such, having the same data point appear several
times with the same label will not aid in finding ap-
propriate support vectors, whereas having the same
data point with conflicting labels may have a nega-
tive impact on the margin maximization.
Having only evaluated each classifier (and train-
ing set) once on a static test set it is unfortunately not
possible to reliably infer the significance of the per-
formance differences (or determine confidence in-
tervals, etc.). For a more in-depth analysis it might
be interesting to use bootstrapping or similar tech-
niques to evaluate the robustness of the results.
6 Conclusions
In this paper we have examined the usefulness of
non-expert annotations on Amazon?s Mechanical
Turk to annotate the polarity of Spanish consumer
comments. We discussed the advantages/drawbacks
of three different HIT designs, ranging from a sim-
ple categorization scheme to a continous scoring
template. We report high inter-annotator agree-
ment scores between non-experts and expert anno-
tators and show that training an Opinion Mining
System with non-expert MTurk annotations outper-
forms original noisy annotations and obtains com-
petitive results when compared to expert annotations
using a variety of classifiers. In conclusion, we
found that, in our case, the use of non-expert anno-
tations through crowdsourcing is a viable and cost-
effective alternative to the use of expert annotations.
In the classification experiments reported in this
paper, we have relied exclusively on MTurk anno-
tations from HIT1. Further work is needed to fully
analyze the impact of each of the HIT designs for
Opinion Mining tasks. We hope that the added rich-
ness of annotation of HIT2 and HIT3 will enable us
to use more sophisticated classification methods.
References
A. Esuli and F. Sebastiani. 2006. SentiWordNet: a pub-
licly available lexical resource for opinion mining. In
Proceedings of LREC, volume 6.
B. D Eugenio and M. Glass. 2004. The kappa statistic: A
second look. Computational linguistics, 30(1):95101.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
A. Kittur, E. H Chi, and B. Suh. 2008. Crowdsourcing
user studies with mechanical turk.
W. Mason and D. J Watts. 2009. Financial incentives
and the performance of crowds. In Proceedings of
the ACM SIGKDD Workshop on Human Computation,
pages 77?85.
A. K. McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
R. Snow, B. O?Connor, D. Jurafsky, and A. Y Ng. 2008.
Cheap and fastbut is it good?: evaluating non-expert
annotations for natural language tasks. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 254?263.
Q. Su, D. Pavlov, J. H Chow, and W. C Baker. 2007.
Internet-scale collection of human-reviewed data. In
Proceedings of the 16th international conference on
World Wide Web, pages 231?240.
121
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 46?52,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
A hybrid framework for scalable Opinion Mining in Social Media: 
detecting polarities and attitude targets 
Carlos Rodr?guez-Penagos 
Barcelona Media Innovaci? 
Av. Diagonal 177  
Barcelona, Spain 
carlos.rodriguez 
@barcelonamedia.org 
Jens Grivolla 
Barcelona Media Innovaci? 
Av. Diagonal 177  
Barcelona, Spain 
jens.grivolla 
@barcelonamedia.org 
Joan Codina Fib? 
Barcelona Media Innovaci? 
Av. Diagonal 177  
Barcelona, Spain 
joan.codina 
@barcelonamedia.org 
 
 
Abstract 
Text mining of massive Social Media 
postings presents interesting challenges for 
NLP applications due to sparse 
interpretation contexts, grammatical and 
orthographical variability as well as its very 
fragmentary nature. No single 
methodological approach can be expected to 
work across such diverse typologies as 
twitter micro-blogging, customer reviews, 
carefully edited blogs, etc. In this paper we 
present a modular and scalable framework 
to Social Media Opinion Mining that 
combines stochastic and symbolic 
techniques to structure a semantic space to 
exploit and interpret efficiently. We 
describe the use of this framework for the 
discovery and clustering of opinion targets 
and topics in user-generated comments for 
the Telecom and Automotive domains. 
1 Introduction 
Social Media (SM) postings constitute a messy 
and highly heterogeneous media that nonetheless 
represent a highly valuable source of information 
about the attitudes, interests and expectations of 
citizens and consumers everywhere. This fact has 
driven a trove of recent research and 
development efforts aimed at managing and 
interpreting such information for a wide 
spectrum of commercial applications, among 
them: reputation management, branding, 
marketing design, etc. A diverse array of 
techniques representing the state of the art run 
the gamut from knowledge-engineered rule-and 
lexicon-base approaches that (when carefully 
crafted) provide high precision in homogeneous 
contexts, to wide-coverage machine learning 
approaches that (when suitable development data 
is available) tackle noisy text with reasonable 
accuracies in some genres. 
 As SM channels are as different from each 
other as, say, spoken text from essay writing, we 
believe that no single technique, powerful as it 
may be, is capable of interpreting all domains, 
genres and channels in the vast universe of SM 
conversations. Faced with an industrial demand 
for simultaneous monitoring of heterogeneous 
opinion sources, our approach has evolved into 
combining diverse NLP technologies into a 
robust semantic analysis framework to create a 
high-granularity representation of user-generated 
commentaries amenable to machine 
interpretation. 
Analysis of Telecom-related social postings 
has shown how a modular and scalable analysis 
framework can combine a veritable arsenal of 
NLP and data mining techniques into a hybrid 
application that adapts well to the unique 
challenges and demands of different Social 
Media genres.  
Section 2 will present the UIMA-Solr 
framework and components used to process 
opinionated text, as well as discuss the 
representational choices made for analysis. 
Section 3 will frame our approach within the 
State-of-the-Art of Sentiment analysis and 
Opinion mining as we interpret it, while Sections 
4 and 5 describe data and results of the 
application of our proposed approach in the 
context of opinion topic detection and clustering 
of SM postings in the Telecoms and Automobile 
domains respectively, and with different textual 
genres. Finally, Section 6 will focus on the 
conclusions and future work that presents to us at 
this point.   
46
2 A modular toolset for SM processing 
For semantic processing of our data we use a 
UIMA 1  (Ferrucci & Lally, 2004) architecture  
plus Solr-based clustering and indexing 
capabilities. Our choice of UIMA is guided in 
part by our wish to achieve good scalability and 
robustness, and that all components can be 
implemented modularly and in a distributed 
manner using UIMA-AS (Asynchronous Scale 
out). Also, UIMA?s data representation as CAS 
objects allows preserving the documents integrity 
since annotations are added as standoff metadata, 
without modifying the original information. 
Under the UIMA architecture, a hybrid NLP 
analysis framework is possible, combining 
powerful Machine Learning modules like 
Maximum Entropy (ME, OpenNLP) 2  or 
Conditional Random Fields (CRF, JulieLab), 3 
with gazetteer and regular expression matchers 
and rule-based Noun Phrase chunkers. The basic 
linguistic processing has a sentence and token 
identifier, a POS tagger, a lemmatizer, a NP 
chunker and a dependency parser. In addition, we 
employ gazetteers to match products, companies, 
and other entities in text, as well as a hand-
crafted lexicon of polar terms created from 
corpus exploration of Telecom domain text, as 
well as a regular expression module to detect 
emoticons when available. Also, two models for 
Named-Entity recognition were applied using 
CRF: one trained on conventional ENAMEX 
Named Entity Recognition and Classification 
entities, and another trained using data from 
customer reviews from various domains (Cars, 
Banking, and Mobile service providers), in order 
to detect opinion targets and cues. One of the 
objectives of this relatively straightforward 
processing (although by no means the only one), 
was to select candidates for classifiers that could 
identify both the specific subject of each opinion 
expressed in text, as well as capture a more 
general topic of the whole conversation (which 
conceivably could coincide or not with one of the 
specific opinion targets). Targets and topics are 
usually expressed as entity names, concepts or 
attributes, and thus can appear in language as 
noun, adjectival, adverbial or even verbal 
phrases. Opinion cues (or Q-elements) are words, 
emoticons and phrases that convey the actual 
attitude of the speaker towards the topics and 
                                                 
1 Unstructured Information Management Architecture 
2 http://maxent.sourceforge.net 
3 http://www.julielab.de 
targets, and a strength and polarity can be 
attributed to them, both a priori and in context. 
Our modular processing approach allows 
customizing the annotation for each domain or 
genre, since, for example, regular expressions to 
detect emoticons will be useful for twitter micro-
blogging, but less so for more conventional blogs 
where such sentiment-expression devices are less 
frequent; Also pre-compiled lists of known 
entities can provide good target precision while 
customised distributional models will help 
discover unlisted names and concepts in text. 
The output of the semantic and syntactic 
processing pipeline is indexed using the Apache 
Solr framework,4 which is based on the Lucene 
engine. This setup allows the implementation of 
clustering and classification algorithms, allowing 
us to obtain reliable statistical correlations 
between documents and entities.  
We also developed or adapted a number of 
visualization components in order to present the 
data stored in Solr in an interactive page that is 
conducive to data exploration and discovery by 
the system?s corporate users. At the same time, 
Carrot2 is connected to Solr and is used to test 
clustering conditions and algorithms, providing a 
nice visualization interface. Carrot2 is an open 
source search results clustering engine (Osi?ski 
& Weiss, 2005). It can automatically organize 
collections of documents into thematic 
categories. 
3 Previous work 
Two good overviews of general Opinion Mining 
and Sentiment Analysis challenges are Pang & 
Lee (2008) and, focused specifically on customer 
reviews, Bhuiyan, Xu & Josang (2009). 
Detecting the subject or targets of opinions is one 
of the main lines of work within Opinion 
Mining, and considerable effort has been put into 
it, since it has been shown to be a highly-domain 
specific task (consumer reviews will focus on 
specific products and features, tweets have 
hashtags to identify topics, blogs can talk almost 
about anything, etc.).  
Outside of user-generated content, Coursey, 
Mihalcea, & Moen (2009) have suggested using 
indirect semantic resources, such as the 
Wikipedia, to identify document topics. For 
Opinion Mining genres, and extending on Hu & 
Liu (2004), Popescu & Etzioni (2005) use a 
combination of Pointwise Mutual Information, 
                                                 
4 http://lucene.apache.org/solr/ 
47
relaxation labeling and dependency analysis to 
extract possible targets and features in product 
reviews. Kim & Hovy (2006), for example, use 
thematic roles to establish a relation between 
candidate opinion holders and opinion topics, 
while exploiting clustering to improve coverage 
in their role-labeling. Recent approaches have 
included adaptation of NER techniques to noisy 
and irregular text, either by using learning 
algorithms or by doing text normalization (Locke 
& Martin, 2009; Ritter, Clark & Etzioni, 2011). 
4 Exploring the semantic space of 
Telecom-related online postings 
We collected close to 200,000 postings from 
various SM sources in a 4 month timeframe, 
including fairly carefully-written product-
oriented forums, blogs, etc., as well as more 
casually-drafted Facebook and twitter micro-
blogging, that discussed Spanish Telecom?s 
services and products. Of these, we randomly 
sub-selected a representative 190-document 
sample that was manually marked-up (for a test 
involving machine learning of cue-polarity-target 
relationships) by two different human annotators 
with a 20-document overlap, using simplified 
annotation guidelines focused on opinion targets, 
topics, cues and polarities. An interesting 
observation about the interannotator agreement 
(but one we can?t discuss in detail here) is that 
with regard to targets one of the human 
annotators tended more towards complete 
syntactic units (noun phrases), while the other 
chose more conceptual and semantic extensions 
as subjects for the opinions. The 20-document 
overlap was meant to help us evaluate this 
guideline development process, but the 
misalignment of guideline interpretation by the 
two human annotators made it very difficult to 
measure any kind of true interannotator 
agreement. Also, single annotation adjudication 
was made difficult due to the fact that both 
interpretations presented valid aspects, and we 
chose to use each set as an independent 
evaluation set to detect any unnoticed patterns 
that could emerge from using one of the other in 
our training and validation, but those results are 
inconclusive and merit further research. Since no 
adjudicator was incorporated in the process to 
resolve disagreements, the final annotated sets do 
not constitute a true Gold Standard, but each 
human-annotated set was used in turn as a 
benchmark against automatic annotators.  
Content elicitation was combined with activity 
and network mining for an enriched overview of 
the social conversation ecosystems, but the 
second aspect won?t be discussed here for the 
sake of brevity. For the same reason, although 
other aspects of sentiment analysis were 
performed on this data (cue and polarity 
detection, for example), we will also restrict the 
scope of these discussions on the detection and 
clustering of specific targets and general topics 
of the opinions expressed in such SM channels. 
Obviously, a deeper and more textured view of 
opinionated text is needed to be of any real use, 
but the overall features, shortcomings and 
advantages of our chosen approach are 
adequately discussed even if we restrict this 
paper to these very specific tasks. 
The first series of experiments about clustering 
using semantics explored the above-mentioned 
corpus of SM posting that discussed a Spanish 
Telecom, one of the aims being detecting and 
aggregating the topics and targets of online 
opinions. Different processing modules geared 
towards topic and target detection were 
compared against each human annotator?s 
choices, but also against each other and to the 
combined output of each. The main modules 
involved were: (A) generic NERC,  (B) a target 
and topic NERC model (StatTarg), (C) a Noun 
Phrase Chunker, and (D) a Gazetteer matcher 
(Taxonomy). Figures 1 through 4 show, 
respectively, recall (1) and precision (2) with 
regard to human annotated topics, and recall (3) 
and precision (4) with regard to human annotated 
targets. 
The results presented here are the overall 
performance across genres and domains, since 
the 190 documents annotated covered the whole 
range from forums to tweets. 
 
 Figure 1. Topic recall 
  
48
 Figure 2. Topic precision 
 
 Figure 3. Target recall 
 
 Figure 4. Target precision 
For this experiment, and as a guideline for the 
human annotators, targets were roughly defined 
as occurrences in the text of objects of opinion, 
whereas topics where to represent the main focus 
of the document or message. The annotators 
usually marked one topic per document, which 
was almost always also one of the targets. 
The customized taxonomy has a good precision 
with regard to target and topic identification, 
while the NERC and NP Chunk approaches 
improve the recall but suffer a bit on precision. 
Generic NER models have a moderately high 
precision (63%) with regard to manually 
annotated targets but rather low recall (specially 
in genres where capitalization is irregular which 
hinders NER detection), while NP Chunks 
present the opposite case: moderately (56%) high 
recall with low precision. This can be explained 
in part by the ?greediness? of each methodology, 
with the chunker annotating extensively while 
the NERC model being much more selective. 
Another noteworthy result is the strong domain 
bias of target annotators trained on a Ciao 
customer reviews for Banking, Automotive and 
Mobile Service markets. The models 
implemented through training from multi-domain 
review sites were found to have medium 
precision, but very low recall. 
The combination of all modules (AllTargets, a 
combination of NERC, Chunker, Taxonomy and 
StatTarget) had a very high recall of around 90%. 
With regard to topic detection, the combination 
of all modules had a recall of 94% and 83%, 
depending on which gold standard it is compared 
to (the one created by one expert human 
annotator or the other), which is an excellent 
recall level. The precision obtained on topic 
detection is very low. This, however, is expected 
as the evaluation is done using all candidates 
given by the different annotation layers, with no 
selection process. Since most of the topics are 
already identified as targets, the key issue here is 
to identify which of the comment targets is the 
main topic. 
It is important to note that merging the Chunker 
output with that of the rest of the modules 
improves the recall of the system but the 
precision becomes low. The main reason is that 
most targets and topics are noun phrases, but not 
all noun phrases are targets or topics.  
It is important to note that combining the output 
of different annotation layers (except for the NP 
chunker) does not reduce overall precision, while 
greatly increasing recall. 
For the clustering experiments, we chose 
Carrot2?s Lingo, a clustering algorithm based on  
Singular Value Decomposition. We envisioned 
the content-based clustering as an interactive 
exploratory tool, rather that providing a single 
?correct? and definitive set of groupings. Cluster 
analysis as such is not an automatic task, but an 
iterative process of knowledge discovery that 
involves trial and failure. It will often be 
necessary to modify the preprocessing and adjust 
parameters until the result achieves the desired 
properties. 
The  query ?problem?, for example, sent to 
some of the telecom forums in May produced 
groupings suggestive of complaints relating to 
rates, internet access, SIM chips, SMS, as well as 
with regard to specific terminal models and 
companies. Even this limited capability can be 
helpful for some of our user?s market analysis 
purposes. 
49
The visualization of query-based clustering 
with detection of target, cues and topics, and the 
possibility of tracking trends over time, provided 
a very powerful overview of how consumer 
attitudes, expectations and complaints about 
products and services are reflected in dynamic 
interchanges in various SM channels. These 
results are available through an online demo 6 
(Figure 5, shown for Facebook postings). 
5 Visualizing the evolution of customer 
opinion 
In addition to exploring SM data for the Telecom 
domain, we performed some experiments using 
clustering without directly using annotated 
semantics, but instead using the semantics only 
for data interpretation. We crawled more than 
10,000 customer reviews in the automotive 
domain in Spanish, along with some metadata 
that included the numerical ratings added by the 
reviewers themselves. Using our modular 
pipeline, we did shallow document clustering 
followed by linguistic processing that included 
lemmatization, POS tagging and Named Entity 
Recognition, in order to allow for analytical 
exploitation of the community-driven discussion 
on automobiles, product features and 
                                                 
6 http://webmining.barcelonamedia.org/Orange/ 
automakers. The most relevant nouns, adjectives, 
bigrams and named entities from a given query, 
are projected into a polarity versus time dynamic 
map. The clustering was performed by the 
combined use of vector space reduction 
techniques and the K-means classification 
paradigm in a completely unsupervised manner. 
Clusters thus obtained were represented by sets 
of words that best described them to obtain a 
view of the emerging terms, trends and features 
contained in the opinions, with the aim of 
providing a representation of their collective 
content. Since evaluating clustering techniques 
per se was not the objective of these 
experiments, and since a gold standard was not 
available, the purpose of the system was (A) to 
validate the coherence of the groupings 
according to the review?s content, and (B) assess 
if those clusters also aggregate as well along 
declared global polarity. Although inconclusive 
from a quantitative point of view, those 
experiments show the feasibility of leveraging 
existing Social Media resources in order to 
develop applications that can visualize and 
explore the semantic ecosystem of consumer 
opinions and attitudes, in a cost-effective and 
efficient manner. A demo of the functionalities 
of the system described here is also publicly 
Figure 5. Facebook's "Iphone" semantic exploration (screenshot) 
50
available. 7 . One cluster, a very positive one 
(based on the average user rating), is represented 
by the terms land-terreno-todoterreno-rover-
campo-4x4 (off-road, field, ground, land, Rover), 
while another one, aceite-garant?a-servicio-
problemas-a?os (oil-warranty-service-problems-
years), in the lower right side might indicate 
unhappy reviewers. 
6 Conclusion and future work 
The results obtained on the Telecom corpus with 
different automatic annotation layers suggest that 
a possible improvement in the system could 
come from researching which combinations of 
automatic annotators can enhance overall 
performance, as one module?s strength might 
complement another weaknesses and vice versa, 
so that what one is missing another one can 
catch. An additional option to increase overall 
recall is to implement a weighted voting scheme 
among the modules, allowing calculation of 
probabilities from the combinations of various 
annotations that overlap a textual segment. 
The fact that combination of annotation layers 
through simple merging of all annotations has 
such a great impact on recall while not reducing 
precision suggests that the different methods are 
very complementary. We expect to be able to 
trade off some of the gained recall for much 
improved precision by applying more 
sophisticated merging methods. 
Another possibility to be explored is using top 
level dependencies (such as SUBJECT, 
SENTENCE, etc.) to rank and select the main 
topic and target candidates using sentence 
structure configuration. This approach would 
also ensure that once a polarity-laden cue is 
identified, the corresponding target could be 
uniquely identified. This linguistics-heavy 
approach is feasible only in texts whose 
characteristics more closely resemble the data 
used to train the parser. 
Our work has helped us focus more clearly many 
of the challenges faced by any NLP system when 
used in a new user-generated content: scarce 
development data, novel pattern and form 
adaptability, tool robustness, and scalability to 
massive and noisy text.  
One of the lessons learned during these 
experiences is that keeping a modular hybrid 
analysis framework can improve matching by 
either customizing the pipeline to each genre and 
                                                 
7 http://webmining.barcelonamedia.org/cometa/index_dates 
task requirements, or by combining the results of 
different approaches to benefit from each one?s 
strengths while minimizing each one?s 
weaknesses. Extracting opinion centered 
information from highly heterogeneous text and 
from multitudes of authors will never be as 
straightforward as, say, doing IE on newswire or 
financial news, but it should be feasible and 
useful by using the right toolset. We are in the 
process of using crowdsourcing to fully annotate 
vast Spanish and English corpora of opinionated 
text, which will allow us to perform a better and 
more fine-grained quantitative analysis of our 
framework in the near future. 
Another lesson learned is that even if high-
precision opinion classification is not available 
(because not enough development data is 
available, or data is noisy, or for whatever other 
reason) doing even superficial semantic 
annotation of the text and unsupervised 
clustering can help industrial consumer of these 
technologies understand better what is being said 
in the Social Media ecosystems. Valuable 
objectives for a useful opinion mining system do 
not need to include all possible analyses or state-
of-the-art performance. 
Going forward, computational exploitation of 
Social Media and of community-based, data-
driven discussions on diverse topics and products 
is definitely an important facet of future market 
and business intelligence competencies, since 
more and more of our activities as citizens, 
friends and consumers take place in an online 
environment, where everything seems possible 
but where also everything we do leaves a trace 
and has a meaning. Extracting the semantics of 
collective action enables us to access that 
meaning. 
References 
 
Ritter A, Clark S, Mausam, and Etzioni O (2011). 
Named Entity Recognition in Tweets: An 
Experimental Study. Proceedings of the 2011 
Conference on Empirical Methods in Natural 
Language Processing (EMNLP 2011) 
Bhuiyan, T., Xu, Y., & Josang, A. (2009). State-of-
the-Art Review on Opinion Mining from Online 
Customers? Feedback. Proceedings of the 9th Asia-
Pacific Complex Systems Conference (pp. 385?
390). 
Coursey, K., Mihalcea, R., & Moen, W. (2009). Using 
encyclopedic knowledge for automatic topic 
identification. Proceedings of the Thirteenth 
Conference on Computational Natural Language 
Learning, CoNLL  ?09 (pp. 210?218). Stroudsburg, 
51
PA, USA: Association for Computational 
Linguistics.  
Ferrucci, D., & Lally, A. (2004). UIMA: an 
architectural approach to unstructured information 
processing in the corporate research environment. 
Natural Language Engineering, 10(3-4), 327?348. 
Hu, M., & Liu, B. (2004). Mining and summarizing 
customer reviews. Proceedings of the tenth ACM 
SIGKDD international conference on Knowledge 
discovery and data mining (pp. 168-177). Seattle, 
WA, USA: ACM. doi:10.1145/1014052.1014073 
Kim, S. M., & Hovy, E. (2006). Extracting opinions, 
opinion holders, and topics expressed in online 
news media text. Proceedings of the Workshop on 
Sentiment and Subjectivity in Text (pp. 1?8). 
Locke, B., & Martin, J. (2009). Named entity 
recognition: Adapting to microblogging. University 
of Colorado.  
Osi?ski and D. Weiss (2005), ?Carrot 2: Design of a 
flexible and efficient web information retrieval 
framework,? Advances in Web Intelligence, pp. 
439?444, 2005. 
Pang, B., & Lee, L. (2008). Opinion mining and 
sentiment analysis. Foundations and Trends in 
Information Retrieval, 2(1-2), 1?135. 
Popescu, A. M., & Etzioni, O. (2005). Extracting 
product features and opinions from reviews. 
Proceedings of HLT/EMNLP (Vol. 5, pp. 339?
346). 
 
 
 
 
52
Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 101?109,
Dublin, Ireland, August 23rd 2014.
EUMSSI: a Platform for Multimodal Analysis and Recommendation
using UIMA
Jens Grivolla
Universitat Pompeu Fabra
Barcelona, Spain
jens.grivolla@upf.edu
Maite Melero
Universitat Pompeu Fabra
Barcelona, Spain
maite.melero@upf.edu
Toni Badia
Universitat Pompeu Fabra
Barcelona, Spain
toni.badia@upf.edu
Cosmin Cabulea
Deutsche Welle
Bonn, Germany
cosmin.cabulea@dw.de
Yannick Est
`
eve
Universit?e du Maine
Le Mans, France
yannick.esteve@
lium.univ-lemans.fr
Eelco Herder
L3S Research Center
Hannover, Germany
herder@l3s.de
Jean-Marc Odobez
IDIAP Research Institute
Martigny, Switzerland
odobez@idiap.ch
Susanne Preu?
Gesellschaft zur F?orderung der
Angewandten Informationsforschung
Saarbr?ucken, Germany
susannep@iai.uni-sb.de
Ra?ul Mar??n
VSN Innovation
and Media Solutions
Alicante, Spain
rmarin@vsn.es
Abstract
The EUMSSI project (Event Understanding through Multimodal Social Stream Interpretation)
aims at developing technologies for aggregating data presented as unstructured information in
sources of very different nature. The multimodal analytics will help organize, classify and clus-
ter cross-media streams, by enriching its associated metadata in an interactive manner, so that
the data resulting from analysing one media helps reinforce the aggregation of information from
other media, in a cross-modal semantic representation framework. Once all the available de-
scriptive information has been collected, an interpretation component will dynamically reason
over the semantic representation in order to derive implicit knowledge. Finally the enriched in-
formation will be fed to a hybrid recommendation system, which will be at the basis of two
well-motivated use-cases. In this paper we give a brief overview of EUMSSI?s main goals and
how we are approaching its implementation using UIMA to integrate and combine various layers
of annotations coming from different sources.
1 Introduction
Nowadays, a multimedia journalist has access to a vast amount of data from a plurality of types of sources
to document a story. In order to put information into context and tell his story from all significant angles,
he needs to go through an enormous amount of records with information of very diverse degrees of
granularity. At the same time, he needs to reduce the noise of irrelevant content. This is extremely
time-consuming, especially when a topic or event is interconnected with multiple entities from different
domains. At a different level, many TV viewers are getting used to navigating with their tablets or iPads
while watching the TV, the tablet effectively functioning as a second screen, often providing background
information on the program or interaction in social networks about what is being watched. Both the
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
101
journalist and the TV viewer would greatly benefit from a system capable of automatically analysing and
interpreting unstructured multimedia data stream and its social background, and, with this understanding,
be able of contextualising the data, and contributing with new, related information.
The FP7-ICT-2013-10 STREP project EUMSSI, which started in December 2013, is developing
methodologies and techniques for identifying and aggregating data presented as unstructured informa-
tion in sources of very different nature (video, image, audio, speech, text and social context), including
both online (e.g., YouTube) and traditional media (e.g. audiovisual repositories), and for dealing with
information of very different degrees of granularity.
This will be accomplished thanks to the integration in a UIMA-based
1
multimodal platform of state-
of-the-art information extraction and analysis techniques from the different fields involved (image, audio,
text and social media analysis). The multimodal interpretation platform, in an optimized process chain,
will analyze a vast amount of multimedia content, aggregate all the resulting information and semanti-
cally enrich it with additional metadata layers. The resulting system will be potentially useful for any
application in need of cross-media data analysis and interpretation, such as intelligent content manage-
ment, recommendation, real time event tracking, content filtering, etc. In particular, the EUMSSI project
will use the semantically enriched information to make personalized content-based recommendation.
2 Multimodal analytics and Semantic Enrichment
For reasoning with and about the multimedia data, the EUMSSI platform needs to recognize entities,
such as actors, places, topics, dates and genres. A core idea is that the process of integrating information
coming from different media sources is carried out in an interactive manner, so that the metadata resulting
from analyzing one media helps reinforce the aggregation of information from other media. For example,
the quality of speech recognition heavily depends on the audio quality and background noise. Existing
text, tags and other metadata will be exploited for disambiguation. Further, OCR on video data, speech
analysis and speaker recognition mutually reinforce one another. The combined and integrated results of
the audio, video and text analysis will significantly enhance the existing metadata, which can be used for
retrieval and recommendation. In addition, the extracted entities and other annotations will be exploited
for identifying specific video fragments in which a particular person speaks, a new topic begins, or an
entity is mentioned. Figure 1 illustrates some of the different layers of analysis that may exist for a video
content item.
Once the entities and concepts have been identified in the different modalities, all the information is ag-
gregated and semantically enriched, using general ontologies or structured knowledge bases. Wikipedia
categories have been successfully exploited with this purpose in different works: e.g. to describe chemi-
cal documents (K?ohncke and Balke, 2010), to identify topics of interest for Twitter users (Michelson and
Macskassy, 2010), and also to improve Web video categorization (Chen et al., 2010). Moreover, (Hahn et
al., 2010) have shown that the structured information gathered from Wikipedia infoboxes can be used to
answer complex questions, like ?Which Rivers flow into the Rhine and are longer than 50 kilometers??
For this purpose, text documents need to be previously annotated using DBpedia Spotlight (Mendes et
al., 2011), which automatically annotates text with links to articles in Wikipedia. The process of se-
mantic enrichment is still largely domain-dependent; therefore, apart from the available general-purpose
knowledge bases and ontologies (DBpedia, FOAF, DublinCore...), the EUMSSI platform needs special-
ized resources for categorizing videos on different dimensions. Linked Data technologies (Heath and
Bizer, 2011) and the Linked Open Data cloud
2
provide access to several of these resources, including
geodata, movie databases and program information.
3 Content-based Recommendation and the Demonstrators
The semantically enriched information is then used by the EUMSSI system to make personalized
content-based recommendation. We propose a novel recommender system that leverages matrix factor-
ization (Koren, 2008) with implicit feedback in order to integrate content-based similarity, usage history
1
Unstructured Information Management Architecture: http://uima.apache.org/
2
http://lod-cloud.net/
102
Figure 1: Video Mining Analysis
(i.e. collaborative filtering), as well as user demographics. This integrated approach reduces the cold-
start problems typical of collaborative filtering, both for new users and for new content. Recommendation
and aggregation of related content in EUMSSI is expected to use varying degrees of personalization, giv-
ing more weight in some cases to the individual user?s interests, based on his viewing history, but being
based primarily on the similarity to the currently shown content in other cases.
On top of the recommender, two demonstrators will be implemented within the EUMSSI project, each
catering to a different use-case: (i) a computer-assisted storytelling tool integrated in the workflow of a
multimedia news editor, empowering the journalist to monitor and gather up-to-date documents related
with his investigation, without the need of reviewing an enormous amount of insufficiently annotated
records; and (ii) a second-screen application for an end-user, able to make relevant suggestions of mul-
timedia content based on what the user is watching, what other people have watched, and what people
are saying about these contents in the social networks. Figure 2 shows how both applications build on a
common base of multimedia analysis and content aggregation/recommendation algorithms.
4 Architecture overview
All new content coming into the system is first normalized to a common metadata schema (based on
schema.org) and stored in a database (MAM/media asset manager, or MongoDB
3
) to make it available
for further processing. Analysis results, as well as the original metadata, are stored in CAS format to
allow integration of different aligned layers of analysis.
The process flow, pictured in Figure 3, can be summarized as follows:
1. new data arrives (or gets imported)
2. preprocessing stage
(a) make content available through unique URI (from central MAM)
(b) create initial CAS with aligned metadata / text content and content URI
3
it will be developed in parallel as an open source MongoDB based solution, as well as integrated into VSN?s proprietary
platform
103
Figure 2: Multimodal platform catering both for the journalist and the end-user?s use-cases
(c) add content to processing queues
3. processing / content analysis
(a) distributed analysis systems query queue when they have processing capacity
(b) retrieve CAS with existing data (or get relevant metadata from wrapper API)
(c) retrieve raw content based on content URI
(d) process
(e) update CAS (possibly through wrapper API)
(f) update queues
i. mark as processed
ii. add to queues for other processes that depend on previous analysis results
4. indexing when processing is complete for a content item (e.g. with Solr)
Note that this architecture design mainly depicts the data analysis part of the EUMSSI system ? the
deployment by Web applications is not visible in the figure. These will be built upon the Solr indexes
created from the CAS.
5 Aligned data representation
Much of the reasoning and cross-modal integration depends on an aligned view of the different annotation
layers, e.g., in order to connect person names detected from OCR with corresponding speakers from the
speaker recognition component, or faces detected by the face recognition.
The Apache UIMA
4
CAS (common analysis structure) representation is a good fit for the needs of the
EUMSSI project as it has a number of interesting characteristics:
? Annotations are stored ?stand-off?, meaning that the original content is not modified in any way by
adding annotations. Rather, the annotations are entirely separate and reference the original content
by offsets
4
http://uima.apache.org/
104
Data Sources
crawlers
DW
feeds
...
extract metadata / 
content
create 
initial CAS
add to / update 
processing queues
video 
analysis
audio 
analysis
text 
analysis
MAM / 
MongoDB
1. get raw content / 
previous CAS
2. process
3. update CAS
Preprocess
Processing
queue 
manager
...
Figure 3: Architecture design
? Annotations can be defined freely by defining a ?type system? that specifies the types of anno-
tations (such as Person, Keyword, Face, etc.) and the corresponding attributes (e.g. dbpediaUrl,
canonicalRepresentation, ...)
? Source content can be included in the CAS (particularly for text content) or referenced as external
content via URIs (e.g. for multimedia content)
? While each CAS represents one ?document? or ?content item?, it can have several Views that rep-
resent different aspects of that item, e.g. the video layer, audio layer, metadata layer, transcribed
text layer, etc., with separate source content (SofA or ?subject of annotation?) and separate sets of
annotations
? CASes can be passed efficiently in-memory between UIMA analysis engines
? CASes can be serialized in a standardised OASIS format
5
for storage and interchange
In the case of the EUMSSI project, the common base for alignment for different annotation layers
referring to multimedia content is timestamps relative to the original content.
Annotations based directly on multimedia content (video and audio) will naturally refer to that content
via timestamps, whereas text analysis modules normally work with character offsets relative to the text
content. It is therefore fundamental that any textual views created from multimedia content (e.g. via ASR
or OCR) refer back to the timestamps in the original content. This will be done by creating annotations,
e.g. tokens, that include the original timestamps as attributes in addition to the character offsets.
As an example, we may have a CAS with an audio view on which we apply automatic speech recogni-
tion (ASR), providing the transcription as a series of tokens/words with a timestamp for each word. The
system then creates a new view in the CAS that has the full plain-text transcription as SofA and a series
of Token annotations with both character offsets relative to the plain-text SofA, and timestamp offsets
relative to the multimedia content.
In this way it is possible to apply standard text analysis modules (that rely on character offsets) on the
textual representation, while maintaining the possibility to later map the resulting annotations back onto
the temporal scale.
Timestamps will be represented in milliseconds in order to avoid floating point values. In this way, all
annotations can be subtypes of the standard UIMA Annotation type
6
, which provides access to a number
5
http://docs.oasis-open.org/uima/v1.0/uima-v1.0.html
6
otherwise annotations would need to derive from the more generic TOP type
105
of utility functions that help find sets of overlapping annotations, retrieve annotations in offset order, etc.
SofA-aware UIMA components are able to work on multiple views, whereas ?normal? analysis en-
gines only see one specific view that is presented to them. This means that e.g. standard text analysis
engines don?t need to be aware that they are being applied to an ASR view or an OCR view; they just
see a regular text document. SofA-aware components, however, can explicitly work on annotations from
different views and can therefore be used to integrate and combine the information coming from different
sources or layers, and create new, integrated views with the output from that integration and reasoning
process.
6 Flow management
UIMA provides a platform for execution of analysis components (Analysis Engines or AEs), as well as
for managing the flow between those components.
CPE or uimaFIT
7
(Ogren and Bethard, 2009) can be used to design and execute pipelines made up of a
sequence of AEs (and potentially some more complex flows), and UIMA-AS
8
(Asynchronous Scaleout)
permits the distribution of the process among various machines or even a cluster (with the help of UIMA
DUCC
9
).
Analysis Engines can either be ?natively? written for UIMA or can be wrappers that translate inputs
and outputs for existing analysis components so they can be integrated in UIMA. All text analysis com-
ponents, as well as the integration and reasoning components, will be available as UIMA AEs and can
therefore be configured and executed directly within the UIMA environment.
There are some components of the EUMSSI platform, however, that do not integrate easily in this
fashion. This is the case of computationally expensive processes that are optimized for batch execution.
A UIMA AE needs to expose a process() method that operates on a single CAS (= document), and is
therefore not compatible with batch processing. This is particularly true for processes that need to be run
on a cluster, with significant startup overhead, such as many video and audio analysis tasks.
It is therefore necessary to have an alternative flow mechanism for offline or batch processes, which
needs to integrate with the processing performed within the UIMA environment.
The main architectural and integration issues revolve around the data flow, rather than the computa-
tion. In fact, the computationally complex and expensive aspects are specific to the individual analysis
components, and should not have an important impact on the design of the overall platform.
As such, the design of the flow management is presented in terms of transformations between data
states, rather than from the procedural point of view. The resulting system should only rely on the
robustness of those data states to ensure the reliability and robustness of the overall system, protecting
against potential problems from server failures or other causes. At any point, the system should be able
to resume its function purely from the state of the persisted data.
To ensure reliability and performance of the data persistence, we expect to use a well-established and
widely used database system such as MongoDB.
Figure 4 shows the general flow of the EUMSSI system, focusing on the data states needed for the
system to function.
In order to avoid synchronization issues, the state of the data processing is stored together with the
data, and the list of pending tasks can be extracted at any point through simple database queries.
For example in order to retrieve the list of content items that have been crawled or received from feeds,
but still need to be converted to the unified EUMSSI schema, it is sufficient to query for items that have
a ?source meta:original? but no ?source meta:eumssi?.
Similarly, the queues for analysis processes can be constructed directly from the ?processing state? of
an item by selecting (for a given queue) all items that have not yet been processed by that queue and that
fulfil all prerequisites (dependencies).
7
https://uima.apache.org/uimafit.html
8
http://uima.apache.org/doc-uimaas-what.html
9
http://uima.apache.org/doc-uimaducc-whatitam.html
106
Figure 4: data flow and transformations
As an illustration, each content item has approximately the following structure:
{
"content_id" : UUID,
"source_meta" : {
"original" : ORIGINAL_SOURCE_METADATA,
"eumssi" : EUMSSI_SOURCE_METADATA
},
"cas" : {
"xmi" : XMI_CAS,
"binary" : BINARY_CAS
},
"processing_state" : {
"queue1" : "done",
"queue2" : "in_process",
...
"queueN" : "pending"
},
"extracted_meta" : METADATA_FROM_CAS
}
where:
107
? UUID is a system-wide unique content id, created when first inserting the content into the system
? ORIGINAL SOURCE METADATA is the metadata as provided from the original content fields
? EUMSSI SOURCE METADATA is the original metadata mapped to the EUMSSI vocabulary /
schema
? XMI CAS is the CAS serialized in XMI format (and possibly compressed)
? BINARY CAS is the CAS serialized in binary format (alternative to XMI CAS)
? METADATA FROM CAS is metadata that is generated by EUMSSI analysis processes, using the
EUMSSI schema
Normally, the CAS will be stored only in one of the available formats, but potentially different serial-
izations could be used. The ?extracted meta? information can be used for analysis results that are used
as inputs to other annotators (such as detected Named Entities as input to speech recognition), to avoid
the overhead of extracting that information from the CAS on demand.
MongoDB allows to stored structured information (corresponding to a JSON structure), so that the
content of fields like ORIGINAL SOURCE METADATA can reflect whatever internal structure the orig-
inal data had.
The final applications are not expected to use the information stored in MongoDB directly, but rather
access Solr indexes created from that information to respond specifically to the types of queries needed
by the applications. Those indexes will typically be created from the CAS when all analysis steps have
been performed.
It is, however, possible to have indexing processes that only depend on a subset of analyses, and thus
make content items (at least partially) accessible to the applications before they have been fully processed
(which may take a relatively long time). The indexing processes can be managed in the same way as any
analysis process, with their own queues that specify the necessary dependencies, and taking the current
state of the CAS as input.
In its simplest form, the processes responsible for the data transitions are fully independent and poll
the database periodically to retrieve pending work. Those processes can then be implemented in any
language that can communicate comfortably with MongoDB. As an efficiency improvement, in order
to reduce the polling load, message queues (such as managed by ActiveMQ
10
) can be used to notify
processes of pending work after performing the preceding steps.
7 Conclusions and future work
In this paper, we have presented the main goals and approaches of the EUMSSI project, which aims
to innovatively integrate state-of-the-art text and A/V analysis technologies, semantic enrichment and
reasoning, social intelligence and collaborative content-based recommendation, in order to build a mul-
timodal, interoperable platform potentially useful for any application in need of automatic cross-media
data analysis and interpretation, such as intelligent content management, personalized recommendation,
real time event tracking, content filtering, etc.
The project is still in an early stage, and many aspects will need to be defined later on. The different
analysis modalities are handled by separate research groups that will each improve the individual types of
analysis in their are of expertise. This paper only reports on the platform that will integrate and combine
the analysis results.
Additionally, possible interactions between modalities will need to be defined as it becomes clearer
what information each analysis can provide or benefit from. We have at this point identified some of the
more obvious interactions, such as doing text analysis on speech recognition output, or adding Named
Entities from surrounding text to the vocabulary known to the ASR system, but many more may become
apparent as the different research groups learn from each other.
10
http://activemq.apache.org/
108
One of the main innovative aspects of the project also lies in the combination of the outputs of different
analysis layers, and the capacity to perform reasoning or inference over this combined view to create a
richer model of the content than can be obtained individually. This is an important research task that
has not started yet, and we hope to report on it in the near future. As such, this article is limited to the
technological foundation that will enable this work by providing a flexible platform with easy access to
all available information layers.
Development of the platform has recently begun and all developments will become publicly available
at https://github.com/EUMSSI/.
Acknowledgements
The work presented in this article is being carried out within the FP7-ICT-2013-10 STREP
project EUMSSI under grant agreement n
?
611057, receiving funding from the European
Union?s Seventh Framework Programme managed by the REA-Research Executive Agency
http://ec.europa.eu/research/rea.
References
Zhineng Chen, Juan Cao, Yicheng Song, Yongdong Zhang, and Jintao Li. 2010. Web video categorization based
on Wikipedia categories and content-duplicated open resources. In Proceedings of the international conference
on Multimedia - MM ?10, page 1107, New York, New York, USA, October. ACM Press.
Rasmus Hahn, Christian Bizer, Christopher Sahnwaldt, Christian Herta, Scott Robinson, Michaela B?urgle, Holger
D?uwiger, and Ulrich Scheel. 2010. Faceted wikipedia search. In Business Information Systems, pages 1?11.
Springer.
Tom Heath and Christian Bizer. 2011. Linked data: Evolving the web into a global data space. Synthesis Lectures
on the Semantic Web: Theory and Technology.
Benjamin K?ohncke and Wolf-Tilo Balke. 2010. Using Wikipedia categories for compact representations of chem-
ical documents. In Proceedings of the 19th ACM international conference on Information and knowledge
management - CIKM ?10, page 1809, New York, New York, USA, October. ACM Press.
Yehuda Koren. 2008. Factorization meets the neighborhood. In Proceeding of the 14th ACM SIGKDD interna-
tional conference on Knowledge discovery and data mining - KDD 08, page 426, New York, New York, USA,
August. ACM Press.
Pablo N. Mendes, Max Jakob, Andr?es Garc??a-Silva, and Christian Bizer. 2011. DBpedia spotlight. In Proceedings
of the 7th International Conference on Semantic Systems - I-Semantics ?11, pages 1?8, New York, New York,
USA, September. ACM Press.
Matthew Michelson and Sofus A. Macskassy. 2010. Discovering users? topics of interest on twitter. In Proceed-
ings of the fourth workshop on Analytics for noisy unstructured text data - AND ?10, page 73, New York, New
York, USA, October. ACM Press.
Philip V. Ogren and Steven J. Bethard. 2009. Building test suites for UIMA components. SETQA-NLP ?09
Proceedings of the Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language
Processing, pages 1?4, June.
109
