Improved-Edit-Distance Kernel for Chinese Relation Extraction
Wanxiang Che
School of Computer Sci. and Tech.
Harbin Institute of Technology
Harbin China, 150001
tliu@ir.hit.edu.cn
Jianmin Jiang, Zhong Su, Yue Pan
IBM CRL
Beijing China, 100085
{jiangjm, suzhong,
panyue}@cn.ibm.com
Ting Liu
School of Computer Sci. and Tech.
Harbin Institute of Technology
Harbin China, 150001
tliu@ir.hit.edu.cn
Abstract
In this paper, a novel kernel-based
method is presented for the problem
of relation extraction between named
entities from Chinese texts. The ker-
nel is defined over the original Chi-
nese string representations around par-
ticular entities. As a kernel func-
tion, the Improved-Edit-Distance (IED)
is used to calculate the similarity be-
tween two Chinese strings. By em-
ploying the Voted Perceptron and Sup-
port Vector Machine (SVM) kernel ma-
chines with the IED kernel as the clas-
sifiers, we tested the method by extract-
ing person-affiliation relation from Chi-
nese texts. By comparing with tradi-
tional feature-based learning methods,
we conclude that our method needs less
manual efforts in feature transformation
and achieves a better performance.
1 Introduction
Relation extraction (RE) is a basic and impor-
tant problem in information extraction field. It
extracts the relations among the named enti-
ties. Examples of relations are person-affiliation,
organization-location, and so on. For example, in
the Chinese sentence ?????IBM????
??? (Gerstner is the chairman of IBM Corpora-
tion.), the named entities are??? (people) and
IBM?? (organization). The relation between
them is person-affiliation.
Usually, we can regard RE as a classification
problem. All particular entity pairs are found
from a text and then decided whether they are a
relation which we need or not.
At the beginning, a number of manually en-
gineered systems were developed for RE prob-
lem (Aone and Ramos-Santacruz, 2000). The
automatic learning methods (Miller et al, 1998;
Soderland, 1999) are not necessary to have some-
one on hand with detailed knowledge of how the
RE system works, or how to write rules for it.
Usually, the machine learning method repre-
sents the NLP objects as feature vectors in the
feature extraction step. The methods are named
feature-based learning methods. But in many
cases, data cannot be easily represented explicitly
via feature vectors. For example, in most NLP
problems, the feature-based representations pro-
duce inherently local representations of objects,
for it is computationally infeasible to generate
features involving long-range dependencies. On
the other hand, finding the suitable features of a
particular problem is a heuristic work. Their ac-
quisition may waste a lot of time.
Different from the feature-based learning
methods, the kernel-based methods do not need
to extract the features from the original text, but
retain the original representation of objects and
use the objects in algorithms only via comput-
ing a kernel (similarity) function between a pair
of objects. Then the kernel-based methods use
existing learning algorithms with dual form, e.g.
the Voted Perceptron (Freund and Schapire, 1998)
or SVM (Cristianini and Shawe-Taylor, 2000), as
kernel machine to do the classification task.
132
Haussler (1999) and Watkins (1999) proposed
a new kernel method based on discrete structures
respectively. Lodhi et al (2002) used string ker-
nels to solve the text classification problem. Ze-
lenko et al (2003) used the kernel methods
for extracting relations from text. They defined
the kernel function over shallow parse represen-
tation of text. And the kernel method is used in
conjunction with the SVM and the Voted Percep-
tron learning algorithms for the task of extracting
person-affiliation and organization-location rela-
tions from text.
As mentioned above, the discrete structure ker-
nel methods are more suitable to RE problems
than the feature-based methods. But the string-
based kernel methods only consider the word
forms without their semantics. Shallow parser
based kernel methods need shallow parser sys-
tems. Because the performance of shallow parser
systems is not high enough until now, especially
for Chinese text, we cannot depend on it com-
pletely.
To cope with these problems, we propose the
Improved-Edit-Distance (IED) algorithm to cal-
culate the kernel (similarity) function. We con-
sider the semantic similarity between two words
in two strings and some structure information of
strings.
The rest of the paper is organized as follows. In
Section 2, we introduce the kernel-based machine
learning algorithms and their application in nat-
ural language processing problems. In Section 3,
we formalize the relation extraction problem as
a machine learning problem. In Section 4, we
give a novel kernel method, named the IED kernel
method. Section 5 describes the experiments and
results on a particular relation extraction problem.
In Section 6, we discuss the reason why the IED
based kernel method yields a better result than
other methods. Finally, in Section 7, we give the
conclusions and comments on the future work.
2 Kernel-based Machine Learning
Most machine learning methods represent an ob-
ject as a feature vector. They are well-known
feature-based learning methods.
Kernel methods (Cristianini and Shawe-Taylor,
2000) are an attractive alternative to feature-based
methods. The kernel methods retain the original
representation of objects and use the object only
via computing a kernel function between a pair
of objects. As we know, a kernel function is a
similarity function satisfying certain properties.
There are a number of learning algorithms that
can operate using only the dot product of exam-
ples. We call them kernel machines. For in-
stance, the Perceptron learning algorithm (Cris-
tianini and Shawe-Taylor, 2000), Support Vector
Machine (SVM) (Vapnik, 1998) and so on.
3 Relation Extraction Problem
We regard the RE problem as a classification
learning problem. We only consider the relation
between two entities in a sentence and no rela-
tions across sentences. For example, the sen-
tence ????????IBM??????
??? (President Bush met Gerstner, the chair-
man of IBM Corporation.) contains three enti-
ties,?? (people), ??? (people) and IBM?
? (organization). The three entities form two
candidate person-affiliation relation pairs: ??-
IBM?? and ???-IBM?? . The con-
texts of the entities pairs produce the examples
for the binary classification problem. Then, from
the context examples, a classifier can decide ??
?-IBM?? is a real person-affiliation relation
but ??-IBM?? is not.
3.1 Feature-based Methods
The feature-based methods have to transform the
context into features. Expert knowledge is re-
quired for deciding which elements or their com-
binations thereof are good features. Usually these
features? values are binary (0 or 1).
The feature-based methods will cost lots of la-
bor to find suitable features for a particular appli-
cation field. Another problem is that we can either
select only the local features with a small win-
dow or we will have to spend much more training
and test time. At the same time, the feature-based
methods will not use the combination of these fea-
tures.
3.2 Kernel-based Methods
Different from the feature-based methods, kernel-
based methods do not require much labor on ex-
tracting the suitable features. As explained in the
introduction to Section 2, we retain the original
133
string form of objects and consider the similarity
function between two objects. For the problem of
the person-affiliation relation extraction, the ob-
jects are the context around people and organiza-
tion with a fixed window size w. It means that
we get w words around each entity as the samples
in the classification problem. Again considering
the example ????????IBM?????
????, with w = 2, the object for the pair ?
?? (people) and IBM?? (organization) can
be written as ??? ? ORG ?? PEO ??
Through the objects transformed from the origi-
nal texts, we can calculate the similarity between
any two objects by using the kernel (similarity)
function.
For the Chinese relation extraction problem,
we must consider the semantic similarity between
words and the structure of strings while comput-
ing similarity. Therefore we must consider the
kernel function which has a good similarity mea-
sure. The methods for computing the similarity
between two strings are: the same-word based
method (Nirenburg et al, 1993), the thesaurus
based method (Qin et al, 2003), the Edit-Distance
method (Ristad and Yianilos, 1998) and the statis-
tical method (Chatterjee, 2001). We know that the
same-word based method cannot solve the prob-
lem of synonyms. The thesaurus based method
can overcome this difficulty but does not con-
sider the structure of the text. Although the Edit-
Distance method uses the structure of the text, it
also has the same problem of the replacement of
synonyms. As for the statistical method, it needs
large corpora of similarity text and thus is difficult
to use for realistic applications.
For the reasons described above, we propose a
novel Improved-Edit-Distance (IED) method for
calculating the similarity between two Chinese
strings.
4 IED Kernel Method
Like normal kernel methods, the new IED ker-
nel method includes two components: the ker-
nel function and the kernel machine. We use the
IED method to calculate the semantic similarity
between two Chinese strings as the kernel func-
tion. As for the kernel machine, we tested the
Voted Perceptron with dual form and SVM with a
customized kernel. In the following subsections,
(a) Edit-Distance (b) Improved-Edit-Distance
Figure 1: The comparison between the Edit-
Distance and the Improved-Edit-Distance
we will introduce the kernel function, the IED
method, and kernel machines.
4.1 Improved-Edit-Distance
Before the introduction to IED, we will give
a brief review of the classical Edit-Distance
method (Ristad and Yianilos, 1998).
The edit distance between two strings is de-
fined as: The minimum number of edit operations
necessary to transform one string into another.
There are three edit operations, Insert, Delete, and
Replace. For example, in Figure 1(a), the edit dis-
tance between ?????(like apples)? and ??
????(like bananas)? is 4, as indicated by the
four dotted lines.
As we see, the method of computing the edit
distance between two Chinese strings cannot re-
flect the actual situation. First, the Edit-Distance
method computes the similarity measured in Chi-
nese character. But in Chinese, most of the char-
acters have no concrete meanings, such as ???,
??? and so on. The single character cannot ex-
press the meanings of words. Second, the cost
of the Replace operation is different for different
words. For example, the operation of ??(love)?
being replace by ???(like)? should have a small
cost, because they are synonyms. At last, if there
are a few words being inserted into a string, the
meaning of it should not be changed too much.
Such as ?????(like apples)? and ?????
?(like sweet apples)? are very similar.
Based on the above idea, we provide the IED
method for computing the similarity between two
Chinese strings. It means that we will use Chinese
words as the basis of our measurement (instead of
characters). By using a thesaurus, the similarity
between two Chinese words can be computed. At
the same time, the cost of the Insert operation is
reduced.
Here, we use the CiLin (Mei et al, 1996) as
134
the thesaurus resource to compute the similarity
between two Chinese words. In CiLin, the se-
mantics of words are divided into High, Middle,
and Low classes to describe a semantic system
from general to special semantic. For example:
???(apple)? is Bh07, ???(banana)? is Bh07,
????(tomato)? is Bh06, and so on.
The semantic distance between word A and
word B can be defined as:
Dist(A, B) = min
a?A,b?B
dist(a, b)
where A and B are the semantic sets of word
A and word B respectively. The distance be-
tween semantic a and b is: dist(a, b) = 2 ?
(3 ? d), where d means that the semantic code
is different from the dth class. If the seman-
tic code is same, then the semantic distance is
0. Therefore, Dist(?????) = 0 and
Dist(??????) = 2.
Table 1 defines the variations of the edit dis-
tance on string ?AB? after doing various edit op-
erations. Where, ??? denotes one to four words,
?A? and ?B? are two words which user inputs. X?
denotes the synonyms of X.
Table 1: The Variations of Edit-Distance with AB
Rank Pattern
1 AB
2 A?B
3 AB?; A?B
4 A?B?; A??B
5 A?; B?
According to Table 1, we can define the cost of
various edit operations in IED. See Table 2, where
??? denotes the delete operation.
Table 2: The Cost of Edit Operation in IED
Edit Operation Cost
A?A 0
Insert 0.1
A?A? Dist(A, A?)/10 + 0.5
Others 1
By the redefinition of the cost of edit opera-
tions, the computation of IED between ????
?? and ??????? is as shown Figure 1(b),
where the Replace cost of ???????? is 0.5
and ????????? is 0.7. Thus the cost of IED
is 1.2. Compared with the cost of classical Edit-
Distance, the cost of IED is much more appropri-
ate in the actual situation.
We use dynamic programming to compute the
IED similar with the computing of edit distance.
In order to compute the similarity between two
strings, we should convert the distance value into
a similarity. Empirically, the maximal similarity
is set to be 10. The similarity is 10 minus the
improved edit distance of two Chinese strings.
4.2 Kernel Machines
We use the Voted Perceptron and SVM algorithms
as the kernel machines here.
The Voted Perceptron algorithm was described
in (Freund and Schapire, 1998). We used
SVMlight (Joachims, 1998) with custom kernel as
the implementation of the SVM method. In our
experiments, we just replaced the custom kernel
with the IED kernel function.
5 Experiments and Results
In this section, we show how to extract the
person-affiliation relation from text and give
some experimental results. It is relatively
straightforward to extend the IED kernel method
to other RE problems.
The corpus for our experiments comes from
Bejing Youth Daily1. We annotated about 500
news with named entities of PEO and ORG. We
selected 4,200 sentences (examples) with both
PEO and ORG pairs as described in Section 3.
There are about 1,200 positive examples and
3,000 negative examples. We took about 2,500
random examples as training data and the rest of
about 1,700 examples as test data.
5.1 Infection of Window Size in Kernel
Methods
The change of the performance of the IED kernel
method varying while the window size w is shown
in Table 3. Here the Voted Perceptron is used as
the kernel machine.
Our experimental results show that the IED
kernel method got the best performance with the
highest F -Score when the window size w =
1http://www.bjyouth.com/
135
2. As w grows, the Precision becomes higher.
With smaller w?s, the Recall becomes higher.
5.2 Comparison between Feature and
Kernel Methods
For the feature-based methods implementation,
we use the words which are around the PEO and
the ORG entities and their POS. The window size
is w (See Section 3). All examples can be trans-
formed into feature vectors. We used the regular-
ized winnow learning algorithm (Zhang, 2001) to
train on the training data and predict the test data.
From the experimental results, we find that when
w = 2, the performance of feature-based method
is highest.
The comparison of the performance between
the feature-based and the kernel-base methods is
shown in Table 4.
Figure 2 displays the change of F -Score for
different methods varying with the training data
size.
Figure 2: The learning curve (of F -Score) for the
person-affiliation relation, comparing IED kernel
with feature-based algorithms
Table 3: The Performance Effected by w
w Precision Recall F -Score
1 66.67% 92.68% 77.55%
2 93.55% 87.80% 90.85%
3 94.23% 74.36% 83.12%
Table 4: The Performance Comparison
Precision Recall F -Score
Regularized Winnow 75.90% 96.92% 85.14%
Voted Perceptron 93.55% 87.80% 90.85%
SVM 94.15% 88.38% 91.17%
From Table 4 and Figure 2, we can see
that the IED kernel methods perform better for
the person-affiliation relation extraction problem
than for the feature-based methods.
Figure 2 shows that the Voted Perceptron
method gets close to, but not as good as, the per-
formance to the SVM method on the RE problem.
But when using the method, we can save signifi-
cantly on computation time and programming ef-
fort.
6 Discussion
Our experimental results show that the kernel-
based and the feature-based methods can get the
best performance with the highest F -Score when
the window size w = 2. This shows that for re-
lation extraction problem, the two words around
entities are the most significant ones. On the other
hand, with w becoming bigger, the Precision be-
comes higher. And with w becoming smaller, the
Recall becomes higher.
From Table 4 and Figure 2, we can see that
the IED kernel methods perform very well for
the person-affiliation relation extraction. Further-
more, it does not need an expensive feature selec-
tion stage like feature-based methods. Because
the IED kernel method uses the semantic similar-
ity between words, it can get a better extension.
We can conclude that the IED kernel method re-
quires much fewer examples than feature-based
methods for achieving the same performance.
For example, there is a test sentence ???
? ?? ?? ? IBM?? ?? ? (Chairman
Hu Jintao met the CEO of IBM Corporation). The
feature-based method judges the ???-IBM?
? as a person-affiliation relation, because the
context around??? and IBM?? is similar
with the context of the person-affiliation relation.
However, the IED kernel method does the correct
judgment based on the structure information. For
this case the IED kernel method gets a higher pre-
cision. At the same time, because the IED kernel
method considers the extension of synonyms, its
recall does not decrease very much.
The speed is a practical problem in apply-
ing kernel-based methods. Kernel-based clas-
sifiers are relatively slow compared to feature-
based classifiers. The main reason is that the com-
puting of kernel (similarity) function takes much
136
time. Therefore, it becomes a key problem to im-
prove the efficiency of the computing of the ker-
nel function.
7 Conclusions
We presented a new approach for using kernel-
based machine learning methods for extracting re-
lations between named entities from Chinese text
sources. We define kernels over the original rep-
resentations of Chinese strings around the partic-
ular entities and use the IED method for comput-
ing the kernel function. The kernel-based meth-
ods need not transform the original expression of
objects into feature vectors, so the methods need
less manual efforts than the feature-based meth-
ods. We applied the Voted Perceptron and the
SVM learning method with custom kernels to ex-
tract the person-affiliation relations. The method
can be extended to extract other relations between
entities, such as organization-location, etc. We
also compared the performance of kernel-based
methods with that of feature-based methods, and
the experimental results show that kernel-based
methods are better than feature-based methods.
Acknowledgements
This research has been supported by National
Natural Science Foundation of China via grant
60435020 and IBM-HIT 2005 joint project.
References
Chinatsu Aone and Mila Ramos-Santacruz. 2000.
Rees: A large-scale relation and event extraction
system. In Proceedings of the 6th Applied Natural
Language Processing Conference, pages 76?83.
Niladri Chatterjee. 2001. A statistical approach
for similarity measurement between sentences for
EBMT. In Proceedings of Symposium on Transla-
tion Support Systems STRANS-2001, Indian Insti-
tute of Technology, Kanpur.
N. Cristianini and J. Shawe-Taylor. 2000. An Intro-
duction to Support Vector Machines. Cambridge
University Press, Cambirdge University.
Yoav Freund and Robert E. Schapire. 1998. Large
margin classification using the perceptron algo-
rithm. In Computational Learning Theory, pages
209?217.
David Haussler. 1999. Convolution kernels on dis-
crete structures. Technical Report UCSC-CRL-99-
10, 7,.
Thorsten Joachims. 1998. Text categorization with
support vector machines: learning with many rele-
vant features. In Proceedings of ECML-98, number
1398, pages 137?142, Chemnitz, DE.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. J. Mach. Learn.
Res., 2:419?444.
Jiaju Mei, Yiming Lan, Yunqi Gao, and Hongxiang
Yin. 1996. Chinese Thesaurus Tongyici Cilin (2nd
Edtion). Shanghai Thesaurus Press, Shanghai.
Scott Miller, Michael Crystal, Heidi Fox, Lance
Ramshaw, Richard Schwartz, Rebecca Stone,
Ralph Weischedel, and the Annotation Group.
1998. Algorithms that learn to extract information?
BBN: Description of the SIFT system as used for
MUC. In Proceedings of the Seventh Message Un-
derstanding Conference (MUC-7).
S. Nirenburg, C. Domashnev, and D.J. Grannes. 1993.
Two approaches to matching in example-based ma-
chine translation. In Proceedings of the Fifth In-
ternational Conference on Theoretical and Method-
ological Issues in Machine Translation, pages 47?
57, Kyoto, Japan.
Bing Qin, Ting Liu, Yang Wang, Shifu Zheng, and
Sheng Li. 2003. Chinese question answering sys-
tem based on frequently asked questions. Jour-
nal of Harbin Institute of Technology, 10(35):1179?
1182.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-edit distance. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 20(5):522?
532.
Stephen Soderland. 1999. Learning information ex-
traction rules for semi-structured and free text. Ma-
chine Learning, 34(1-3):233?272.
Vladimir N. Vapnik. 1998. Statistical Learning The-
ory. Wiley.
Chris Watkins. 1999. Dynamic alignment kernels.
Technical Report CSD-TR-98-11, 1,.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. J. Mach. Learn. Res., 3:1083?1106.
Tong Zhang. 2001. Regularized winnow methods.
In Advances in Neural Information Processing Sys-
tems 13, pages 703?709.
137
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 281?289,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Domain Adaptation with Latent Semantic Association
for Named Entity Recognition
Honglei Guo Huijia Zhu Zhili Guo Xiaoxun Zhang Xian Wu and Zhong Su
IBM China Research Laboratory
Beijing, P. R. China
{guohl, zhuhuiji, guozhili, zhangxx, wuxian, suzhong}@cn.ibm.com
Abstract
Domain adaptation is an important problem in
named entity recognition (NER). NER classi-
fiers usually lose accuracy in the domain trans-
fer due to the different data distribution be-
tween the source and the target domains. The
major reason for performance degrading is
that each entity type often has lots of domain-
specific term representations in the different
domains. The existing approaches usually
need an amount of labeled target domain data
for tuning the original model. However, it
is a labor-intensive and time-consuming task
to build annotated training data set for every
target domain. We present a domain adapta-
tion method with latent semantic association
(LaSA). This method effectively overcomes
the data distribution difference without lever-
aging any labeled target domain data. LaSA
model is constructed to capture latent seman-
tic association among words from the unla-
beled corpus. It groups words into a set of
concepts according to the related context snip-
pets. In the domain transfer, the original term
spaces of both domains are projected to a con-
cept space using LaSA model at first, then the
original NER model is tuned based on the se-
mantic association features. Experimental re-
sults on English and Chinese corpus show that
LaSA-based domain adaptation significantly
enhances the performance of NER.
1 Introduction
Named entities (NE) are phrases that contain names
of persons, organizations, locations, etc. NER is an
important task in information extraction and natu-
ral language processing (NLP) applications. Super-
vised learning methods can effectively solve NER
problem by learning a model from manually labeled
data (Borthwick, 1999; Sang and Meulder, 2003;
Gao et al, 2005; Florian et al, 2003). However, em-
pirical study shows that NE types have different dis-
tribution across domains (Guo et al, 2006). Trained
NER classifiers in the source domain usually lose
accuracy in a new target domain when the data dis-
tribution is different between both domains.
Domain adaptation is a challenge for NER and
other NLP applications. In the domain transfer,
the reason for accuracy loss is that each NE type
often has various specific term representations and
context clues in the different domains. For ex-
ample, {?economist?, ?singer?, ?dancer?, ?athlete?,
?player?, ?philosopher?, ...} are used as context
clues for NER. However, the distribution of these
representations are varied with domains. We expect
to do better domain adaptation for NER by exploit-
ing latent semantic association among words from
different domains. Some approaches have been pro-
posed to group words into ?topics? to capture im-
portant relationships between words, such as Latent
Semantic Indexing (LSI) (Deerwester et al, 1990),
probabilistic Latent Semantic Indexing (pLSI) (Hof-
mann, 1999), Latent Dirichlet Allocation (LDA)
(Blei et al, 2003). These models have been success-
fully employed in topic modeling, dimensionality
reduction for text categorization (Blei et al, 2003),
ad hoc IR (Wei and Croft., 2006), and so on.
In this paper, we present a domain adaptation
method with latent semantic association. We focus
281
on capturing the hidden semantic association among
words in the domain adaptation. We introduce the
LaSA model to overcome the distribution difference
between the source domain and the target domain.
LaSA model is constructed from the unlabeled cor-
pus at first. It learns latent semantic association
among words from their related context snippets.
In the domain transfer, words in the corpus are as-
sociated with a low-dimension concept space using
LaSA model, then the original NER model is tuned
using these generated semantic association features.
The intuition behind our method is that words in one
concept set will have similar semantic features or
latent semantic association, and share syntactic and
semantic context in the corpus. They can be consid-
ered as behaving in the same way for discriminative
learning in the source and target domains. The pro-
posed method associates words from different do-
mains on a semantic level rather than by lexical oc-
currence. It can better bridge the domain distribu-
tion gap without any labeled target domain samples.
Experimental results on English and Chinese corpus
show that LaSA-based adaptation significantly en-
hances NER performance across domains.
The rest of this paper is organized as follows. Sec-
tion 2 briefly describes the related works. Section 3
presents a domain adaptation method based on latent
semantic association. Section 4 illustrates how to
learn LaSA model from the unlabeled corpus. Sec-
tion 5 shows experimental results on large-scale En-
glish and Chinese corpus across domains, respec-
tively. The conclusion is given in Section 6.
2 Related Works
Some domain adaptation techniques have been em-
ployed in NLP in recent years. Some of them
focus on quantifying the generalizability of cer-
tain features across domains. Roark and Bacchiani
(2003) use maximum a posteriori (MAP) estimation
to combine training data from the source and target
domains. Chelba and Acero (2004) use the param-
eters of the source domain maximum entropy clas-
sifier as the means of a Gaussian prior when train-
ing a new model on the target data. Daume III and
Marcu (2006) use an empirical Bayes model to esti-
mate a latent variable model grouping instances into
domain-specific or common across both domains.
Daume III (2007) further augments the feature space
on the instances of both domains. Jiang and Zhai
(2006) exploit the domain structure contained in the
training examples to avoid over-fitting the training
domains. Arnold et al (2008) exploit feature hier-
archy for transfer learning in NER. Instance weight-
ing (Jiang and Zhai, 2007) and active learning (Chan
and Ng, 2007) are also employed in domain adap-
tation. Most of these approaches need the labeled
target domain samples for the model estimation in
the domain transfer. Obviously, they require much
efforts for labeling the target domain samples.
Some approaches exploit the common structure of
related problems. Ando et al (2005) learn pred-
icative structures from multiple tasks and unlabeled
data. Blitzer et al (2006, 2007) employ structural
corresponding learning (SCL) to infer a good fea-
ture representation from unlabeled source and target
data sets in the domain transfer. We present LaSA
model to overcome the data gap across domains by
capturing latent semantic association among words
from unlabeled source and target data.
In addition, Miller et al (2004) and Freitag
(2004) employ distributional and hierarchical clus-
tering methods to improve the performance of NER
within a single domain. Li and McCallum (2005)
present a semi-supervised sequence modeling with
syntactic topic models. In this paper, we focus on
capturing hidden semantic association among words
in the domain adaptation.
3 Domain Adaptation Based on Latent
Semantic Association
The challenge in domain adaptation is how to cap-
ture latent semantic association from the source and
target domain data. We present a LaSA-based do-
main adaptation method in this section.
NER can be considered as a classification prob-
lem. Let X be a feature space to represent the ob-
served word instances, and let Y be the set of class
labels. Let ps(x, y) and pt(x, y) be the true under-
lying distributions for the source and the target do-
mains, respectively. In order to minimize the efforts
required in the domain transfer, we often expect to
use ps(x, y) to approximate pt(x, y).
However, data distribution are often varied with
the domains. For example, in the economics-to-
282
entertainment domain transfer, although many NE
triggers (e.g. ?company? and ?Mr.?) are used in
both domains, some are totally new, like ?dancer?,
?singer?. Moreover, many useful words (e.g.
?economist?) in the economics NER are useless in
the entertainment domain. The above examples
show that features could change behavior across do-
mains. Some useful predictive features from one do-
main are not predictive or do not appear in another
domain. Although some triggers (e.g. ?singer?,
?economist?) are completely distinct for each do-
main, they often appear in the similar syntactic and
semantic context. For example, triggers of per-
son entity often appear as the subject of ?visited?,
?said?, etc, or are modified by ?excellent?, ?popu-
lar?, ?famous? etc. Such latent semantic association
among words provides useful hints for overcoming
the data distribution gap of both domains.
Hence, we present a LaSA model ?s,t to cap-
ture latent semantic association among words in the
domain adaptation. ?s,t is learned from the unla-
beled source and target domain data. Each instance
is characterized by its co-occurred context distribu-
tion in the learning. Semantic association feature
in ?s,t is a hidden random variable that is inferred
from data. In the domain adaptation, we transfer the
problem of semantic association mapping to a pos-
terior inference task using LaSA model. Latent se-
mantic concept association set of a word instance x
(denoted by SA(x)) is generated by ?s,t. Instances
in the same concept set are considered as behaving
in the same way for discriminative learning in both
domains. Even though word instances do not ap-
pear in a training corpus (or appear rarely) but are in
similar context, they still might have relatively high
probability in the same semantic concept set. Obvi-
ously, SA(x) can better bridge the gap between the
two distributions ps(y|x) and pt(y|x). Hence, LaSA
model can enhance the estimate of the source do-
main distribution ps(y|x; ?s,t) to better approximate
the target domain distribution pt(y|x; ?s,t).
4 Learning LaSA Model from Virtual
Context Documents
In the domain adaptation, LaSA model is employed
to find the latent semantic association structures of
?words? in a text corpus. We will illustrate how
to build LaSA model from words and their context
snippets in this section. LaSA model actually can
be considered as a general probabilistic topic model.
It can be learned on the unlabeled corpus using the
popular hidden topic models such as LDA or pLSI.
4.1 Virtual Context Document
The distribution of content words (e.g. nouns, adjec-
tives) is usually varied with domains. Hence, in the
domain adaptation, we focus on capturing the latent
semantic association among content words. In or-
der to learn latent relationships among words from
the unlabeled corpus, each content word is charac-
terized by a virtual context document as follows.
Given a content word xi, the virtual context docu-
ment of xi (denoted by vdxi) consists of all the con-
text units around xi in the corpus. Let n be the total
number of the sentences which contain xi in the cor-
pus. vdxi is constructed as follows.
vdxi = {F (xs1i ), ..., F (xski ), ..., F (xsni )}
where, F (xski ) denotes the context feature set of
xi in the sentence sk, 1 ? k ? n.
Given the context window size {-t, t} (i.e. pre-
vious t words and next t words around xi in sk).
F (xski ) usually consists of the following features.
1. Anchor unit AxiC : the current focused word unit xi.
2. Left adjacent unit AxiL : The nearest left adjacent
unit xi?1 around xi, denoted by AL(xi?1).
3. Right adjacent unit AxiR : The nearest right adjacent
unit xi+1 around xi, denoted by AR(xi+1).
4. Left context set CxiL : the other left adjacent units
{xi?t, ..., xi?j , ..., xi?2} (2 ? j ? t) around xi, de-
noted by {CL(xi?t), ..., CL(xi?j), ..., CL(xi?2)}.
5. Right context set CxiR : the other right adjacent units
{xi+2, ..., xi+j , ..., xi+t} (2 ? j ? t ) around xi, de-
noted by {CR(xi+2), ..., CR(xi+j), ..., CR(xi+t)}.
For example, given xi=?singer?, sk=?This popu-
lar new singer attended the new year party?. Let
the context window size be {-3,3}. F (singer)
= {singer, AL(new), AR(attend(ed)), CL(this),
CL(popular), CR(the), CR(new) }.
vdxi actually describes the semantic and syntac-
tic feature distribution of xi in the domains. We
construct the feature vector of xi with all the ob-
served context features in vdxi . Given vdxi =
283
{f1, ..., fj , ..., fm}, fj denotes jth context feature
around xi, 1 ? j ? m, m denotes the total num-
ber of features in vdxi . The value of fj is calculated
by Mutual Information (Church and Hanks, 1990)
between xi and fj .
Weight(fj , xi) = log2 P (fj , xi)P (fj)P (xi) (1)
where, P (fj , xi) is the joint probability of xi and
fj co-occurred in the corpus, P (fj) is the probabil-
ity of fj occurred in the corpus. P (xi) is the proba-
bility of xi occurred in the corpus.
4.2 Learning LaSA Model
Topic models are statistical models of text that posit
a hidden space of topics in which the corpus is em-
bedded (Blei et al, 2003). LDA (Blei et al, 2003) is
a probabilistic model that can be used to model and
discover underlying topic structures of documents.
LDA assumes that there are K ?topics?, multinomial
distributions over words, which describes a collec-
tion. Each document exhibits multiple topics, and
each word in each document is associated with one
of them. LDA imposes a Dirichlet distribution on
the topic mixture weights corresponding to the doc-
uments in the corpus. The topics derived by LDA
seem to possess semantic coherence. Those words
with similar semantics are likely to occur in the same
topic. Since the number of LDA model parameters
depends only on the number of topic mixtures and
vocabulary size, LDA is less prone to over-fitting
and is capable of estimating the probability of un-
observed test documents. LDA is already success-
fully applied to enhance document representations
in text classification (Blei et al, 2003), information
retrieval (Wei and Croft., 2006).
In the following, we illustrate how to construct
LDA-style LaSA model ?s,t on the virtual con-
text documents. Algorithm 1 describes LaSA
model training method in detail, where, Function
AddTo(data, Set) denotes that data is added to
Set. Given a large-scale unlabeled data set Du
which consists of the source and target domain data,
virtual context document for each candidate content
word is extracted from Du at first, then the value of
each feature in a virtual context document is calcu-
lated using its Mutual Information ( see Equation 1
in Section 4.1) instead of the counts when running
Algorithm 1: LaSA Model Training
Inputs:1
? Unlabeled data set: Du ;2
Outputs:3
?LaSA model: ?s,t;4
Initialization:5
? Virtual context document set: V Ds,t = ?;6
? Candidate content word set: Xs,t = ?;7
Steps:8
begin9
foreach content word xi ?Du do10
if Frequency(xi)? the predefined threshold then11
AddTo(xi, Xs,t);12
foreach xk ?Xs,t do13
foreach sentence Si ?Du do14
if xk ? Si then15
F (xSik ) ??16
{xk, A
xk
L , A
xk
R , C
xk
L , C
xk
R };
AddTo(F (xSik ), vdxk );
AddTo(vdxk , V Ds,t);17
? Generate LaSA model ?s,t with Dirichlet distribution on V Ds,t .18
end19
LDA. LaSA model ?s,t with Dirichlet distribution is
generated on the virtual context document set V Ds,t
using the algorithm presented by Blei et al(2003).
1 2 3 4 5
customer theater company Beijing music
president showplace government Hongkong film
singer courtyard university China arts
manager center community Japan concert
economist city team Singapore party
policeman gymnasium enterprise New York Ballet
reporter airport bank Vienna dance
director square market America song
consumer park organization Korea band
dancer building agency international opera
Table 1: Top 10 nouns from 5 randomly selected topics
computed on the economics and entertainment domains
LaSA model learns the posterior distribution to
decompose words and their corresponding virtual
context documents into topics. Table 1 lists top 10
nouns from a random selection of 5 topics computed
on the unlabeled economics and entertainment do-
main data. As shown, words in the same topic are
representative nouns. They actually are grouped into
broad concept sets. For example, set 1, 3 and 4
correspond to nominal person, nominal organization
and location, respectively. With a large-scale unla-
beled corpus, we will have enough words assigned
to each topic concept to better approximate the un-
derlying semantic association distribution.
In LDA-style LaSA model, the topic mixture
is drawn from a conjugate Dirichlet prior that re-
mains the same for all the virtual context docu-
284
ments. Hence, given a word xi in the corpus, we
may perform posterior inference to determine the
conditional distribution of the hidden topic feature
variables associated with xi. Latent semantic asso-
ciation set of xi (denoted by SA(xi)) is generated
using Algorithm 2. Here, Multinomial(?s,t(vdxi))
refers to sample from the posterior distribution over
topics given a virtual document vdxi . In the domain
adaptation, we do semantic association inference on
the source domain training data using LaSA model
at first, then the original source domain NER model
is tuned on the source domain training data set by
incorporating these generated semantic association
features.
Algorithm 2: Generate Latent Semantic As-
sociation Set of Word xi Using K-topic
LaSA Model
Inputs:1
? ?s,t: LaSA model with multinomial distribution;2
?Dirichlet(?): Dirichlet distribution with parameter ?;3
? xi: Content word;4
Outputs:5
? SA(xi): Latent semantic association set of xi ;6
Steps:7
begin8
? Extract vdxi from the corpus.9
? Draw topic weights ?s,t(vdxi ) from Dirichlet(?);10
? foreach fj in vdxi do11
draw a topic zj?{ 1,...,K} from Multinomial(?s,t(vdxi ));12
AddTo(zj , Topics(vdxi ));13
? Rank all the topics in Topics(vdxi );14
? SA(xi)?? top n topics in Topics(vdxi );15
end16
LaSA model better models latent semantic asso-
ciation distribution in the source and the target do-
mains. By grouping words into concepts, we effec-
tively overcome the data distribution difference of
both domains. Thus, we may reduce the number
of parameters required to model the target domain
data, and improve the quality of the estimated pa-
rameters in the domain transfer. LaSA model ex-
tends the traditional bag-of-words topic models to
context-dependence concept association model. It
has potential use for concept grouping.
5 Experiments
We evaluate LaSA-based domain adaptation method
on both English and Chinese corpus in this section.
In the experiments, we focus on recognizing person
(PER), location (LOC) and organization (ORG) in
the given four domains, including economics (Eco),
entertainment (Ent), politics (Pol) and sports (Spo).
5.1 Experimental setting
In the NER domain adaptation, nouns and adjectives
make a significant impact on the performance. Thus,
we focus on capturing latent semantic association
for high-frequency nouns and adjectives (i.e. occur-
rence count ? 50 ) in the unlabeled corpus. LaSA
models for nouns and adjectives are learned from
the unlabeled corpus using Algorithm 1 (see section
4.2), respectively. Our empirical study shows that
better adaptation is obtained with a 50-topic LaSA
model. Therefore, we set the number of topics N as
50, and define the context view window size as {-
3,3} (i.e. previous 3 words and next 3 words) in the
LaSA model learning. LaSA features for other irre-
spective words (e.g. token unit ?the?) are assigned
with a default topic value N+1.
All the basic NER models are trained on the
domain-specific training data using RRM classifier
(Guo et al, 2005). RRM is a generalization Winnow
learning algorithm (Zhang et al, 2002). We set the
context view window size as {-2,2} in NER. Given a
word instance x, we employ local linguistic features
(e.g. word unit, part of speech) of x and its context
units ( i.e. previous 2 words and next 2 words ) in
NER. All Chinese texts in the experiments are auto-
matically segmented into words using HMM.
In LaSA-based domain adaptation, the semantic
association features of each unit in the observation
window {-2,2} are generated by LaSA model at first,
then the basic source domain NER model is tuned on
the original source domain training data set by incor-
porating the semantic association features. For ex-
ample, given the sentence ?This popular new singer
attended the new year party?, Figure 1 illustrates
various features and views at the current word wi=
?singer? in LaSA-based adaptation.
? Tagging ?
Position wi?2 wi?1 wi wi+1 wi+2
Word popular new singer attend the
POS adj adj noun verb article
SA SA(popular) SA(new) SA(singer) SA(attend) SA(the)
.....
Tag ti?2 ti?1 ti
Figure 1: Feature window in LaSA-based adaptation
In the viewing window at the word ?singer? (see
Figure 1), each word unit around ?singer? is codi-
fied with a set of primitive features (e.g. POS, SA,
Tag), together with its relative position to ?singer?.
285
Here, ?SA? denotes semantic association feature set
which is generated by LaSA model. ?Tag? denotes
NE tags labeled in the data set.
Given the input vector constructed with the above
features, RRM method is then applied to train linear
weight vectors, one for each possible class-label. In
the decoding stage, the class with the maximum con-
fidence is then selected for each token unit.
In our evaluation, only NEs with correct bound-
aries and correct class labels are considered as the
correct recognition. We use the standard Precision
(P), Recall (R), and F-measure (F = 2PRP+R ) to mea-
sure the performance of NER models.
5.2 Data
We built large-scale English and Chinese anno-
tated corpus. English corpus are generated from
wikipedia while Chinese corpus are selected from
Chinese newspapers. Moreover, test data do not
overlap with training data and unlabeled data.
5.2.1 Generate English Annotated Corpus
from Wikipedia
Wikipedia provides a variety of data resources for
NER and other NLP research (Richman and Schone,
2008). We generate all the annotated English corpus
from wikipedia. With the limitation of efforts, only
PER NEs in the corpus are automatically tagged us-
ing an English person gazetteer. We automatically
extract an English Person gazetteer from wikipedia
at first. Then we select the articles from wikipedia
and tag them using this gazetteer.
In order to build the English Person gazetteer
from wikipdedia, we manually selected several key
phrases, including ?births?, ?deaths?, ?surname?,
?given names? and ?human names? at first. For
each article title of interest, we extracted the cate-
gories to which that entry was assigned. The en-
try is considered as a person name if its related
explicit category links contain any one of the key
phrases, such as ?Category: human names?. We to-
tally extracted 25,219 person name candidates from
204,882 wikipedia articles. And we expanded this
gazetteer by adding the other available common
person names. Finally, we obtained a large-scale
gazetteer of 51,253 person names.
All the articles selected from wikipedia are further
tagged using the above large-scale gazetteer. Since
human annotated set were not available, we held out
more than 100,000 words of text from the automat-
ically tagged corpus to as a test set in each domain.
Table 2 shows the data distribution of the training
and test data sets.
Domains Training Data Set Test Data Set
Size PERs Size PERs
Pol 0.45M 9,383 0.23M 6,067
Eco 1.06M 21,023 0.34M 6,951
Spo 0.47M 17,727 0.20M 6,075
Ent 0.36M 12,821 0.15M 5,395
Table 2: English training and test data sets
We also randomly select 17M unlabeled English
data (see Table 3) from Wikipedia. These unlabeled
data are used to build the English LaSA model.
All Domain
Pol Eco Spo Ent
Data Size(M) 17.06 7.36 2.59 3.65 3.46
Table 3: Domain distribution in the unlabeled English
data set
5.2.2 Chinese Data
We built a large-scale high-quality Chinese NE
annotated corpus. All the data are news articles from
several Chinese newspapers in 2001 and 2002. All
the NEs (i.e. PER, LOC and ORG ) in the corpus are
manually tagged. Cross-validation checking is em-
ployed to ensure the quality of the annotated corpus.
Domain Size NEs in the training data set
(M) PER ORG LOC Total
Pol 0.90 11,388 6,618 14,350 32,356
Eco 1.40 6,821 18,827 14,332 39,980
Spo 0.60 11,647 8,105 7,468 27,220
Ent 0.60 12,954 2,823 4,665 20,442
Domain Size NEs in the test data set
(M) PER ORG LOC Total
Pol 0.20 2,470 1,528 2,540 6,538
Eco 0.26 1,098 2,971 2,362 6,431
Spo 0.10 1,802 1,323 1,246 4,371
Ent 0.10 2,458 526 738 3,722
Table 4: Chinese training and test data sets
All the domain-specific training and test data are
selected from this annotated corpus according to the
domain categories (see Table 4). 8.46M unlabeled
Chinese data (see Table 5) are randomly selected
from this corpus to build the Chinese LaSA model.
5.3 Experimental Results
All the experiments are conducted on the above
large-scale English and Chinese corpus. The overall
performance enhancement of NER by LaSA-based
286
All Domain
Pol Eco Spo Ent
Data Size(M) 8.46 2.34 1.99 2.08 2.05
Table 5: Domain distribution in the unlabeled Chinese
data set
domain adaptation is evaluated at first. Since the
distribution of each NE type is different across do-
mains, we also analyze the performance enhance-
ment on each entity type by LaSA-based adaptation.
5.3.1 Performance Enhancement of NER by
LaSA-based Domain Adaptation
Table 6 and 7 show the experimental results for
all pairs of domain adaptation on both English and
Chinese corpus, respectively. In the experiment,
the basic source domain NER model Ms is learned
from the specific domain training data set Ddom
(see Table 2 and 4 in Section 5.2). Here, dom ?
{Eco,Ent, Pol, Spo}. F indom denotes the top-line
F-measure of Ms in the source trained domain dom.
When Ms is directly applied in a new target do-
main, its F-measure in this basic transfer is consid-
ered as baseline (denoted by FBase). FLaSA de-
notes F-measure of Ms achieved in the target do-
main with LaSA-based domain adaptation. ?(F ) =
FLaSA?FBase
FBase , which denotes the relative F-measure
enhancement by LaSA-based domain adaptation.
Source ? Performance in the domain transfer
Target
FBase FLaSA ?(F ) ?(loss) FTop
Eco?Ent 57.61% 59.22% +2.79% 17.87% F inEnt=66.62%
Pol?Ent 57.5 % 59.83% +4.05% 25.55% F inEnt=66.62%
Spo?Ent 58.66% 62.46% +6.48% 47.74% F inEnt=66.62%
Ent?Eco 70.56 % 72.46% +2.69% 19.33% F inEco=80.39%
Pol?Eco 63.62% 68.1% +7.04% 26.71% F inEco=80.39%
Spo?Eco 70.35% 72.85% +3.55% 24.90% F inEco=80.39%
Eco?Pol 50.59% 52.7% +4.17% 15.81% F inPol=63.94%
Ent?Pol 56.12% 59.82% +6.59% 47.31% F inPol=63.94%
Spo?Pol 60.22% 62.6% +3.95% 63.98% F inPol=63.94%
Eco?Spo 60.28% 61.21% +1.54% 9.93% F inSpo=69.65%
Ent?Spo 60.28% 62.68% +3.98% 25.61% F inSpo=69.65%
Pol?Spo 56.94% 60.48% +6.22% 27.85% F inSpo=69.65%
Table 6: Experimental results on English corpus
Experimental results on English and Chinese cor-
pus indicate that the performance of Ms signifi-
cantly degrades in each basic domain transfer with-
out using LaSA model (see Table 6 and 7). For ex-
ample, in the ?Eco?Ent? transfer on Chinese cor-
pus (see Table 7), F ineco of Ms is 82.28% while FBase
of Ms is 60.45% in the entertainment domain. F-
measure of Ms significantly degrades by 21.83 per-
Source ? Performance in the domain transfer
Target
FBase FLaSA ?(F ) ?(loss) FTop
Eco?Ent 60.45% 66.42% +9.88% 26.29% F inEnt=83.16%
Pol?Ent 69.89% 73.07% +4.55% 23.96% F inEnt =83.16%
Spo?Ent 68.66% 70.89% +3.25% 15.38% F inEnt =83.16%
Ent?Eco 58.50% 61.35% + 4.87% 11.98% F inEco=82.28%
Pol?Eco 62.89% 64.93% +3.24% 10.52% F inEco=82.28%
Spo?Eco 60.44% 63.20% + 4.57 % 12.64% F inEco=82.28%
Eco?Pol 67.03% 70.90 % +5.77% 27.78% F inPol=80.96%
Ent?Pol 66.64 % 68.94 % +3.45% 16.06% F inPol=80.96%
Spo?Pol 65.40% 67.20% +2.75% 11.57% F inPol=80.96%
Eco?Spo 67.20% 70.77% +5.31% 15.47% F inSpo=90.24%
Ent?Spo 70.05% 72.20% +3.07% 10.64% F inSpo=90.24%
Pol?Spo 70.99% 73.86% +4.04% 14.91% F inSpo=90.24%
Table 7: Experimental results on Chinese corpus
cent points in this basic transfer. Significant perfor-
mance degrading of Ms is observed in all the basic
transfer. It shows that the data distribution of both
domains is very different in each possible transfer.
Experimental results on English corpus show that
LaSA-based adaptation effectively enhances the per-
formance in each domain transfer (see Table 6).
For example, in the ?Pol?Eco? transfer, FBase is
63.62% while FLaSA achieves 68.10%. Compared
with FBase, LaSA-based method significantly en-
hances F-measure by 7.04%. We perform t-tests on
F-measure of all the comparison experiments on En-
glish corpus. The p-value is 2.44E-06, which shows
that the improvement is statistically significant.
Table 6 also gives the accuracy loss due to transfer
in each domain adaptation on English corpus. The
accuracy loss is defined as loss = 1 ? FF indom . And
the relative reduction in error is defined as ?(loss)=
|1 ? lossLaSAlossBase |. Experimental results indicate that
the relative reduction in error is above 9.93% with
LaSA-based transfer in each test on English cor-
pus. LaSA model significantly decreases the ac-
curacy loss by 29.38% in average. Especially for
?Spo?Pol? transfer, ?(loss) achieves 63.98% with
LaSA-based adaptation. All the above results show
that LaSA-based adaptation significantly reduces the
accuracy loss in the domain transfer for English
NER without any labeled target domain samples.
Experimental results on Chinese corpus also show
that LaSA-based adaptation effectively increases the
accuracy in all the tests (see Table 7). For example,
in the ?Eco?Ent? transfer, compared with FBase,
LaSA-based adaptation significantly increases F-
measure by 9.88%. We also perform t-tests on F-
287
measure of 12 comparison experiments on Chinese
corpus. The p-value is 1.99E-06, which shows that
the enhancement is statistically significant. More-
over, the relative reduction in error is above 10%
with LaSA-based method in each test. LaSA model
decreases the accuracy loss by 16.43% in average.
Especially for the ?Eco?Ent? transfer (see Table 7),
?(loss) achieves 26.29% with LaSA-based method.
All the above experimental results on English and
Chinese corpus show that LaSA-based domain adap-
tation significantly decreases the accuracy loss in the
transfer without any labeled target domain data. Al-
though automatically tagging introduced some er-
rors in English source training data, the relative re-
duction in errors in English NER adaptation seems
comparable to that one in Chinese NER adaptation.
5.3.2 Accuracy Enhancement for Each NE
Type Recognition
Our statistic data (Guo et al, 2006) show that the
distribution of NE types varies with domains. Each
NE type has different domain features. Thus, the
performance stability of each NE type recognition is
very important in the domain transfer.
Figure 2 gives F-measure of each NE type recog-
nition achieved by LaSA-based adaptation on En-
glish and Chinese corpus. Experimental results
show that LaSA-based adaptation effectively in-
creases the accuracy of each NE type recognition in
the most of the domain transfer tests. We perform
t-tests on F-measure of the comparison experiments
on each NE type, respectively. All the p-value is
less than 0.01, which shows that the improvement
on each NE type recognition is statistically signifi-
cant. Especially, the p-value of English and Chinese
PER is 2.44E-06 and 9.43E-05, respectively, which
shows that the improvement on PER recognition is
very significant. For example, in the ?Eco?Pol?
transfer on Chinese corpus, compared with FBase,
LaSA-based adaptation enhances F-measure of PER
recognition by 9.53 percent points. Performance en-
hancement for ORG recognition is less than that one
for PER and LOC recognition using LaSA model
since ORG NEs usually contain much more domain-
specific information than PER and LOC.
The major reason for error reduction is that exter-
nal context and internal units are better semantically
associated using LaSA model. For example, LaSA
Figure 2: PER, LOC and ORG recognition in the transfer
model better groups various titles from different do-
mains (see Table 1 in Section 4.2). Various industry
terms in ORG NEs are also grouped into the seman-
tic sets. These semantic associations provide useful
hints for detecting the boundary of NEs in the new
target domain. All the above results show that LaSA
model better compensates for the feature distribution
difference of each NE type across domains.
6 Conclusion
We present a domain adaptation method with LaSA
model in this paper. LaSA model captures latent se-
mantic association among words from the unlabeled
corpus. It better groups words into a set of concepts
according to the related context snippets. LaSA-
based domain adaptation method projects words to
a low-dimension concept feature space in the trans-
fer. It effectively overcomes the data distribution gap
across domains without using any labeled target do-
main data. Experimental results on English and Chi-
nese corpus show that LaSA-based domain adapta-
tion significantly enhances the performance of NER
across domains. Especially, LaSA model effectively
increases the accuracy of each NE type recogni-
tion in the domain transfer. Moreover, LaSA-based
domain adaptation method works well across lan-
guages. To further reduce the accuracy loss, we will
explore informative sampling to capture fine-grained
data difference in the domain transfer.
References
Rie Ando and Tong Zhang. 2005. A Framework for
Learning Predictive Structures from Multiple Tasks
288
and Unlabeled Data. In Journal of Machine Learning
Research 6 (2005), pages 1817?1853.
Andrew Arnold, Ramesh Nallapati, and William W. Co-
hen. 2008. Exploiting Feature Hierarchy for Trans-
fer Learning in Named Entity Recognition. In Pro-
ceedings of 46th Annual Meeting of the Association of
Computational Linguistics (ACL?08), pages 245-253.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet Allocation. Journal of Machine Learn-
ing Research, 3:993?1022.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain Adaptation with Structural Correspon-
dence Learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2006), pages 120-128.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, Bollywood, Boom-boxes and Blenders:
Domain Adaptation for Sentiment Classification. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics (ACL?07), pages
440-447.
Andrew Borthwick. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
New York University.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain Adap-
tation with Active Learning for Word Sense Disam-
biguation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics
(ACL?07).
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. In Proceedings of the 2004 Conference on Empir-
ical Methods in Natural Language Processing.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information and lexicogra-
phy. Computational Linguistics, 16(1):22?29.
Hal Daume III. 2007. Frustratingly Easy Domain Adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics.
Hal Daume III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26:101?126.
Scott Deerwester, Susan T. Dumais, and Richard Harsh-
man. 1990. Indexing by latent semantic analysis.
Journal of the American Society for Information Sci-
ence, 41(6):391?407.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong
Zhang. 2003. Named entity recogintion through clas-
sifier combination. In Proceedings of the 2003 Confer-
ence on Computational Natural Language Learning.
Freitag. 2004. Trained Named Entity Recognition Using
Distributional Clusters. In Proceedings of the 2004
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004).
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name Tagging with Word Clusters and Discrim-
inative Training. In Proceedings of HLT-NAACL 04.
Jianfeng Gao, Mu Li, Anndy Wu, and Changning Huang.
2005. Chinese Word Segmentation and Named Entity
Recognition: A Pragmatic Approach. Computational
Linguisitc, 31(4):531?574.
Honglei Guo, Jianmin Jiang, Gang Hu, and Tong Zhang.
2005. Chinese Named Entity Recognition Based on
Multilevel Linguistic Features. In Lecture Notes in Ar-
tificial Intelligence, 3248:90?99.
Honglei Guo, Li Zhang, and Zhong Su. 2006. Empirical
Study on the Performance Stability of Named Entity
Recognition Model across Domains. In Proceedings
of the 2006 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2006), pages 509-
516.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22th Annual Inter-
national SIGIR Conference on Research and Develop-
ment in Information Retrieval (SIGIR?99).
Jing Jiang and ChengXiang Zhai. 2006. Exploiting Do-
main Structure for Named Entity Recognition. In Pro-
ceedings of HLT-NAACL 2006, pages 74?81.
Jing Jiang and ChengXiang Zhai. 2007. Instance
Weighting for Domain Adaptation in NLP. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL?07), pages
264?271.
Wei Li and Andrew McCallum. 2005. Semi-supervised
sequence modeling with syntactic topic models. In
Proceedings of Twenty AAAI Conference on Artificial
Intelligence (AAAI-05).
Alexander E. Richman and Patrick Schone. 2008. Min-
ing Wiki Resources for Multilingual Named Entity
Recognition. In Proceedings of the 46th Annual Meet-
ing of the Association of Computational Linguistics.
Brian Roark and Michiel Bacchiani. 2003. Supervised
and unsupervised PCFG adaptation to novel domains.
In Proceedings of the 2003 Human Language Technol-
ogy Conference of the North American Chapter of the
Association for Computational Linguistics.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: Language
independent named entity recognition. In Proceed-
ings of the 2003 Conference on Computational Natural
Language Learning (CoNLL-2003), pages 142?147.
Xing Wei and Bruce Croft. 2006. LDA-based document
models for ad-hoc retrieval. In Proceedings of the 29th
Annual International SIGIR Conference on Research
and Development in Information Retrieval.
Tong Zhang, Fred Damerau, and David Johnson. 2002
Text chunking based on a generalization of Winnow.
Journal of Machine Learning Research, 2:615?637.
289
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 509?516,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Empirical Study on the Performance Stability of Named Entity
Recognition Model across Domains
Hong Lei Guo Li Zhang and Zhong Su
IBM China Research Laboratory
Building 19, Zhongguancun Software Park
8 Dongbeiwang WestRoad, Haidian District, Beijing, 100094, P.R.C.
{guohl, lizhang , suzhong }@cn.ibm.com
Abstract
When a machine learning-based named
entity recognition system is employed in
a new domain, its performance usually de-
grades. In this paper, we provide an em-
pirical study on the impact of training data
size and domain information on the per-
formance stability of named entity recog-
nition models. We present an informative
sample selection method for building high
quality and stable named entity recogni-
tion models across domains. Experimen-
tal results show that the performance of
the named entity recognition model is en-
hanced significantly after being trained
with these informative samples.
1 Introduction
Named entities (NE) are phrases that contain
names of persons, organizations, locations, etc.
Named entity recognition (NER) is an important
task in many natural language processing appli-
cations, such as information extraction and ma-
chine translation. There have been a number of
conferences aimed at evaluating NER systems,
for example, MUC6, MUC7, CoNLL2002 and
CoNLL2003, and ACE (automatic content extrac-
tion) evaluations.
Machine learning approaches are becoming
more attractive for NER in recent years since they
are trainable and adaptable. Recent research on
English NER has focused on the machine learning
approach (Sang and Meulder, 2003). The relevant
algorithms include Maximum Entropy (Borth-
wick, 1999; Klein et al, 2003), Hidden Markov
Model (HMM) (Bikel et al, 1999; Klein et al,
2003), AdaBoost (Carreras et al, 2003), Memory-
based learning (Meulder and Daelemans, 2003),
Support Vector Machine (Isozaki and Kazawa,
2002), Robust Risk Minimization (RRM) Classi-
fication method (Florian et al, 2003), etc.
For Chinese NER, most of the existing ap-
proaches use hand-crafted rules with word (or
character) frequency statistics. Some machine
learning algorithms also have been investigated in
Chinese NER, including HMM (Yu et al, 1998;
Jing et al, 2003), class-based language model
(Gao et al, 2005; Wu et al, 2005), RRM (Guo
et al, 2005; Jing et al, 2003), etc.
However, when a machine learning-based NER
system is directly employed in a new domain, its
performance usually degrades. In order to avoid
the performance degrading, the NER model is of-
ten retrained with domain-specific annotated cor-
pus. This retraining process usually needs more
efforts and costs. In order to enhance the perfor-
mance stability of NER models with less efforts,
some issues have to be considered in practice. For
example, how much training data is enough for
building a stable and applicable NER model? How
does the domain information and training data size
impact the NER performance?
This paper provides an empirical study on the
impact of training data size and domain informa-
tion on NER performance. Some useful observa-
tions are obtained from the experimental results
on a large-scale annotated corpus. Experimental
results show that it is difficult to significantly en-
hance the performance when the training data size
is above a certain threshold. The threshold of the
training data size varies with domains. The perfor-
mance stability of each NE type recognition also
varies with domains. Corpus statistical data show
that NE types have different distribution across do-
mains. Based on the empirical investigations, we
present an informative sample selection method
509
for building high quality and stable NER models.
Experimental results show that the performance of
the NER model is enhanced significantly across
domains after being trained with these informative
samples. In spite of our focus on Chinese, we be-
lieve that some of our observations can be poten-
tially useful to other languages including English.
This paper is organized as follows. Section 2
describes a Chinese NER system using multi-level
linguistic features. Section 3 discusses the impact
of domain information and training data size on
the NER performance. Section 4 presents an in-
formative sample selection method to enhance the
performance of the NER model across domains.
Finally the conclusion is given in Section 5.
2 Chinese NER Based on Multilevel
Linguistic Features
In this paper, we focus on recognizing four types
of NEs: Persons (PER), Locations (LOC), Orga-
nizations (ORG) and miscellaneous named enti-
ties (MISC) which do not belong to the previous
three groups (e.g. products, conferences, events,
brands, etc.). All the NER models in the follow-
ing experiments are trained with a Chinese NER
system. In this section, we simply describe this
Chinese NER system. The Robust Risk Minimiza-
tion (RRM) Classification method and multi-level
linguistic features are used in this system (Guo et
al., 2005).
2.1 Robust Risk Minimization Classifier
We can view the NER task as a sequential classi-
fication problem. If toki (i = 0, 1, ..., n) denotes
the sequence of tokenized text which is the input
to the system, then every token toki should be as-
signed a class-label ti.
The class label value ti associated with each to-
ken toki is predicted by estimating the conditional
probability P (ti = c|xi) for every possible class-
label value c, where xi is a feature vector associ-
ated with token toki.
We assume that P (ti = c|xi) = P (ti =
c|toki, {tj}j?i). The feature vector xi can depend
on previously predicted class labels {tj}j?i, but
the dependency is typically assumed to be local.
In the RRM method, the above conditional proba-
bility model has the following parametric form:
P (ti = c|xi, ti?l, ..., ti?1) = T (wTc xi + bc),
where T (y) = min(1,max(0, y)) is the truncation
of y into the interval [0, 1]. wc is a linear weight
vector and bc is a constant. Parameters wc and bc
can be estimated from the training data. Given
training data (xi, ti) for i = 1, ..., n, the model
is estimated by solving the following optimization
problem for each c (Zhang et al, 2002):
inf
w,b
1
n
n
?
i=1
f(wTc xi + bc, yic),
where yic = 1 when ti = c, and yic = ?1 other-
wise. The function f is defined as:
f(p, y) =
?
?
?
?
?
?2py py < 1
1
2(py ? 1)2 py ? [?1, 1]
0 py > 1
Given the above conditional probability model,
the best possible sequence of ti?s can be estimated
by dynamic programming in the decoding stage
(Zhang et al, 2002).
2.2 Multilevel Linguistic Features
This Chinese NER system uses Chinese charac-
ters (not Chinese words) as the basic token units,
and then maps word-based features that are as-
sociated with each word into corresponding fea-
tures of those characters that are contained in the
word. This approach can effectively incorporate
both character-based features and word-based fea-
tures. In general, we may regard this approach
as information integration from linguistic views at
different abstraction levels.
We integrate a diverse set of local linguistic fea-
tures, including word segmentation information,
Chinese word patterns, complex lexical linguis-
tic features (e.g. part of speech and semantic fea-
tures), aligned at the character level. In additional,
we also use external NE hints and gazetteers, in-
cluding surnames, location suffixes, organization
suffixes, titles, high-frequency Chinese characters
in Chinese names and translation names, and lists
of locations and organizations. In this system, lo-
cal linguistic features of a token unit are derived
from the sentence containing this token unit. All
special linguistic patterns (i.e. date, time, numeral
expression) are encoded into pattern-specific class
labels aligned with the tokens.
3 Impact of Training Data Size And
Domain Information on the NER
Performance
It is very important to keep the performance sta-
bility of NER models across domains in practice.
510
However, the performance usually becomes unsta-
ble when NER models are applied in different do-
mains. We focus on the impact of the training data
size and domain information on the NER perfor-
mance in this section.
3.1 Data
We built a large-scale high-quality Chinese NE an-
notated corpus. The corpus size is 114.25M Chi-
nese characters. All the data are news articles se-
lected from several Chinese newspapers in 2001
and 2002. All the NEs in the corpus are manually
tagged. Documents in the corpus are also man-
ually classified into eight domain categories, in-
cluding politics, sports, science, economics, enter-
tainment, life, society and others. Cross-validation
is employed to ensure the tagging quality.
All the training data and test data in the exper-
iments are selected from this Chinese annotated
corpus. The general training data are randomly se-
lected from the corpus without distinguishing their
domain categories. All the domain-specific train-
ing data are selected from the corpus according to
their domain categories. One general test data set
and seven domain-specific test data sets are used
in our experiments (see Table 1). The size of the
general test data set is 1.34M Chinese characters.
Seven domain-specific test sets are extracted from
the general test data set according to the document
domain categories.
Domain NE distribution in the domain-oriented test data set Test set
PER ORG LOC MISC Total Size
General 11,991 9,820 12,353 1,820 35,984 1.34M
Politics 2,470 1,528 2,540 480 7,018 0.2M
Economics 1,098 2,971 2,362 493 6,924 0.26M
Sports 1,802 1,323 1,246 478 4,849 0.10M
Entertainment 2,458 526 738 542 4,264 0.10M
Society 916 418 823 349 2,506 0.08M
Life 2,331 1,690 3,634 763 8,418 0.39M
Science 1,802 1,323 1,246 478 4,849 0.10M
Table 1: NE distribution in the general and
domain-specific test data sets
In our evaluation, only NEs with correct bound-
aries and correct class labels are considered as the
correct recognition. We use the standard P (i.e.
Precision), R (i.e. Recall), and F-measure (de-
fined as 2PR/(P+R)) to measure the performance
of NER models.
3.2 Impact of Training Data Size on the NER
Performance across Domains
The amount of annotated data is always a bottle-
neck for supervised learning methods in practice.
Figure 1: Performance curves of the general and
specific domain NER models
Thus, we evaluate the impact of training data size
on the NER performance across domains.
In this baseline experiment, an initial general
NER model is trained with 0.1M general data at
first. Then the NER model is incrementally re-
trained by adding 0.1M new general training data
each time till the performance isn?t enhanced sig-
nificantly. The NER performance curve (labelled
with the tag ?General? ) in the whole retraining
process is shown in Figure 1. Experimental results
show that the performance of the general NER
model is significantly enhanced in the first several
retraining cycles since more training data are used.
However, when the general training data set size is
more than 2.4M, the performance enhancement is
very slight.
In order to analyze how the training data size
impacting the performance of NER models in spe-
cific domains, seven domain-specific NER mod-
els are built using the similar retraining process.
Each domain-specific NER model is also trained
with 0.1M domain-specific data at first. Then,
each initial domain-specific NER model is incre-
mentally retrained by adding 0.1M new domain-
specific data each time.
NER F(%) Size NE distribution in the training set
Model thre-
shold
(M) PER ORG LOC MISC Total
General 80.38 2.4 24,960 27,231 21,098 7,439 80,728
Politics 83.09 0.9 11,388 6,618 14,350 1,974 34,330
Econ-
omics 85.46 1.7 7,197 21,113 15,582 3,466 47,358
Sports 90.78 0.6 11,647 8,105 7,468 3,070 30,290
Entert-
ainment 83.31 0.6 12,954 2,823 4,665 3,518 32,860
Society 76.55 0.6 7,099 3,279 6,946 1,909 19,233
Life 81.06 1.7 10,502 5,675 18,980 2,420 37,577
Science 70.02 0.4 1,625 3,010 2,083 902 7,620
Table 2: Performance of NER models, size thresh-
old and NE distribution in the corresponding train-
ing data sets
511
The performance curves of these domain-
specific NER models are also shown in Figure 1
(see the curves labelled with the domain tags). Al-
though the initial performance of each domain-
specific NER model varies with domains, the per-
formance is also significantly enhanced in the first
several retraining cycles. When the size of the
domain-specific training data set is above a certain
threshold, the performance enhancement is very
slight as well.
The final performance of the trained NER mod-
els, and the corresponding training data sets are
shown in Table 2.
From these NER performance curves, we obtain
the following observations.
1. More training data are used, higher NER per-
formance can be achieved. However, it is
difficult to significantly enhance the perfor-
mance when the training data size is above a
certain threshold.
2. The threshold of the training data size and
the final achieved performance vary with do-
mains (see Table 2). For example, in enter-
tainment domain, the threshold is 0.6M and
the final F-measure achieves 83.31%. In eco-
nomic domain, the threshold is 1.7M, and the
corresponding F-measure is 85.46%.
3.3 The Performance Stability of Each NE
Type Recognition across Domains
Statistic data on our large-scale annotated corpus
(shown in Table 3) show that the distribution of NE
types varies with domains. We define ? NE density
? to quantitatively measure the NE distribution in
an annotated data set. NE density is defined as ?the
count of NE instances in one thousand Chinese
characters?. Higher NE density usually indicates
that more NEs are contained in the data set. We
may easily measure the distribution of each NE
type across domains using NE density. In this an-
notated corpus, PER, LOC, and ORG have similar
NE density while MISC has the smallest NE den-
sity. All the NE types also have different NE den-
sity in each domain. For example, the NE density
of ORG and LOC is much higher than that of PER
in economic domain. PER and LOC have higher
NE density than ORG in politics domain. PER
has the highest NE density among these NE types
in both sports and entertainment domains. The
unbalanced NE distribution across domains shows
that news articles on different domains usually fo-
cus on different specific NE types. These NE dis-
tribution features imply that each NE type has dif-
ferent domain dependency feature. The perfor-
mance stability of domain-focused NE type recog-
nition becomes more important in domain-specific
applications. For example, since economic news
articles usually focus on ORG and LOC NEs, the
high-quality LOC and ORG recognition models
will be more valuable in economic domain. In ad-
dition, these distribution features also can be used
to guide training and test data selection.
Domain NE distribution in the specific domain
PER LOC ORG MISC ALL Ratio
(%)
Politics 167,989 180,193 105,936 30,830 484,948 16.43
Econ-
omics 117,459 200,261 352,323 76,320 746,363 25.29
Sports 129,137 73,435 98,618 33,304 334,494 11.33
Entert- 154,193 50,408 40,444 52,460 297,505 10.08
ainment
Life 200,222 234,150 145,138 65,733 645,243 21.86
Society 63,793 53,724 43,657 21,162 182,336 6.18
Science 27,878 30,737 72,413 16,824 147,852 5.00
Others 31,723 40,730 26,666 13,926 113,045 3.83
All 892,394 863,638 885,195 310,559 2,951,786 ?
Domain NE density in the Chinese annotated corpus Size
PER LOC ORG MISC ALL (M)
Politics 10.70 11.48 6.75 1.96 31.21 15.70
Econ-
omics 4.18 7.13 12.55 2.72 26.58 28.08
Sports 16.43 9.34 12.55 4.24 42.57 7.86
Entert-
ainment 16.81 5.05 4.14 5.72 32.44 9.17
Life 5.64 6.59 4.09 1.85 18.17 35.52
Society 8.57 7.22 5.87 2.84 24.51 7.44
Science 4.30 4.74 11.17 2.60 22.82 6.48
Others 7.9 10.18 6.67 3.48 28.26 4.00
All 7.81 7.56 7.75 2.72 25.89 114.25
Table 3: NE distribution in the Chinese annotated
corpus
In this experiment, the performance stability
of NER models across domains is evaluated, es-
pecially the performance stability of each NE
type recognition. The general NER model is
trained with 2.4M general data. Seven domain-
specific models are trained with the corresponding
domain-specific training sets (see Table 2 in Sec-
tion 3.2).
The performance stability of the general NER
model is firstly evaluated on the general and
domain-specific test data sets (see Table 1 in Sec-
tion 3.1 ). The experimental results are shown in
Table 4. The performance curves of the general
model are shown in Figure 2, including the total
F-measure curve of the NER model (labelled with
the tag ?All?) and F-measure curves of each NE
type recognition in the specific domains (labelled
with the NE tags respectively).
The performance stability of the seven domain-
specific NER models are also evaluated. Each
domain-specific NER model is tested on the gen-
512
Domain F(%) of general NER model
PER LOC ORG MISC ALL
General 86.69 85.55 73.59 56.00 80.38
Economic 85.11 88.22 75.91 49.53 80.50
Politics 86.26 87.00 71.31 61.50 81.90
Sports 91.87 89.03 81.67 67.41 86.10
Entertainment 84.24 85.85 68.65 60.96 79.31
Life 86.62 83.54 70.30 58.49 79.73
Society 84.53 76.16 68.89 41.14 74.50
Science 87.74 86.42 65.85 24.10 69.55
Table 4: Performance of the general NER model
in specific domains
Figure 2: Performance curves of the general NER
model in specific domains
eral test data and the other six different domain-
specific test data sets. The experimental results are
shown in Table 5. The performance curves of three
domain-specific NER models are shown in Figure
3, Figure 4 and Figure 5 respectively.
From these experimental results, we have the
following conclusions.
1. The performance stability of all the NER
models is limited across domains. When a
NER model is employed in a new domain, its
performance usually decreases. Moreover, its
performance is usually much lower than the
performance of the corresponding domain-
specific model.
2. The general NER model has better per-
Figure 3: Performance curves of economic do-
main NER model in the other specific domains
NER F(%) in specific domain
Model Gen- Eco- Poli- Spo- Enter- Life Soc- Sci-
eral nomic tics rts tainment iety ence
General 80.38 80.50 81.90 86.10 79.31 79.73 74.50 69.55
Econ-
omic 75.30 85.46 74.32 72.89 68.46 76.23 65.75 68.97
Politics 73.37 66.39 83.09 76.37 71.51 74.83 67.31 53.76
Sports 71.23 62.56 68.99 90.78 73.48 71.18 64.82 53.85
Entert-
ainment 70.82 61.52 72.04 75.34 83.31 71.80 69.10 52.50
Life 73.53 66.92 75.07 73.86 72.68 81.06 69.61 57.36
Society 70.29 62.55 72.70 70.69 72.24 74.10 76.55 53.42
Science 67.26 67.57 69.00 64.32 63.84 69.05 64.85 70.02
Table 5: Performance of NER models in specific
domains
Figure 4: Performance curves of sports domain
NER model in the other specific domains
formance stability than the domain-specific
NER model when they are applied in new do-
mains (see Table 5). Domain-specific mod-
els usually could achieve a higher perfor-
mance in its corresponding domain after be-
ing trained with a smaller amount of domain-
specific annotated data (see Table 2 in Sec-
tion 3.2). However, the performance stability
of domain-specific NER model is poor across
different domains. Thus, it is very popular to
build a general NER model for the general
applications in practice.
3. The performance of PER, LOC and ORG
recognition is better than that of MISC recog-
Figure 5: Performance curves of politics domain
NER model in the other specific domains
513
nition in NER (see Figure 2 ? Figure 5).
The main reason for the poor performance of
MISC recognition is that there are less com-
mon indicative features among various MISC
NEs which we do not distinguish. In addi-
tion, NE density of MISC is much less than
that of PER, LOC, and ORG. There are a
relatively small number of positive training
samples for MISC recognition.
4. NE types have different domain dependency
attribute. The performance stability of each
NE type recognition varies with domains (see
Figure 2 ? Figure 5). The performance of
PER and LOC recognition are more stable
across domains. Thus, few efforts are needed
to adapt the existing high-quality general
PER and LOC recognition models in domain-
specific applications. Since ORG and MISC
NEs usually contain more domain-specific
semantic information, ORG and MISC are
more domain-dependent than PER and LOC.
Thus, more domain-specific features should
be mined for ORG and MISC recognition.
4 Use Informative Training Samples to
Enhance the Performance of NER
Models across Domains
A higher performance system usually requires
more features and a larger number of training data.
This requires larger system memory and more effi-
cient training method, which may not be available.
Within the limitation of available training data and
computational resources, it is necessary for us to
either limit the number of features or select more
informative data which can be efficiently handled
by the training algorithm. Active learning method
is usually employed in text classification (McCal-
lum and Nigam et al, 1998). It is only recently
employed in NER (Shen et al, 2004).
In order to enhance the performance and over-
come the limitation of available training data and
computational resources, we present an informa-
tive sample selection method using a variant of
uncertainty-sampling (Lewis and Catlett, 1994).
The main steps are described as follows.
1. Build an initial NER model (F-
measure=76.24%) using an initial data
set. The initial data set (about 1M Chinese
characters) is randomly selected from the
large-scale candidate data set (about 9M ).
Figure 6: Performance curves of general NER
models after being trained with informative sam-
ples and random samples respectively
2. Refine the training set by adding more infor-
mative samples and removing those redun-
dant samples. In this refinement phase, all of
the data are annotated by the current recogni-
tion model (e.g. the initial model built in Step
1). Each annotation has a confidence score
associated with the prediction. In general, an
annotation with lower confidence score usu-
ally indicates a wrong prediction. The con-
fidence score of the whole sample sentence
is defined as the average of the confidence
scores of all the annotations contained in the
sentence. Thus, we add those sample sen-
tences with lower confidence scores into the
training set. Meanwhile, in order to keep a
reasonable size of the training set, those old
training sample sentences with higher confi-
dence scores are removed from the current
training set. In each retraining phase, all of
the sample sentences are sorted by the con-
fidence score. The top 1000 new sample
sentences with lowest confidence scores are
added into the current training set. The top
500 old training sample sentences with high-
est confidence scores are removed from the
current training set.
3. Retrain a new Chinese NER model with the
newly refined training set
4. Repeat Step 2 and Step 3, until the perfor-
mance doesn?t improve any more.
We apply this informative sample selection
method to incrementally build the general domain
NER model. The size of the final informative
training sample set is 1.05M Chinese characters.
This informative training sample set has higher
NE density than the random training data set (see
Table 6).
514
We denote this general NER model trained with
the informative sample set as ?general informa-
tive model?, and denote the general-domain model
which is trained with 2.4M random general train-
ing data as ?general random model?. The perfor-
mance curves of the general NER models after be-
ing trained with informative samples and random
data respectively are shown in Figure 6. Experi-
ment results (see Table 6) show that there is a sig-
nificant enhancement in F-measure if using infor-
mative training samples. Compared with the ran-
dom model, the informative model can increase F-
measure by 4.21 percent points.
Type Using informative sample set Using random training set
(1.05M) (2.4M)
F(%) NEs NE density F(%) NEs NE density
PER 89.87 18,898 18.00 86.69 24,960 10.38
LOC 89.68 24,862 23.68 85.55 21,089 11.33
ORG 79.22 22,173 21.12 73.59 27,231 8.78
MISC 64.27 8,067 7.68 56.00 7,439 3.10
Total 84.59 74,000 70.48 80.38 80,728 33.58
Table 6: Performance of informative model and
random model in the general domain
Domain F(%) of general informative model
PER LOC ORG MISC ALL
Economic 89.26 90.66 81.24 61.14 84.63
Politics 89.36 89.37 74.76 65.95 84.70
Sports 93.65 90.66 86.00 72.05 88.71
Entertainment 88.38 87.54 73.88 58.32 82.74
Life 89.15 88.35 75.68 72.01 84.66
Society 86.61 82.15 72.99 58.55 79.49
Science 90.91 88.35 71.69 25.16 72.71
Table 7: Performance of the general informative
model in specific domains
This informative model is also evaluated on the
domain-specific test sets. Experimental results are
shown in Table 7. We view the performance of the
domain-specific NER model as the baseline per-
formance in its corresponding domain (see Table
8), denoted as Fbaseline. The performance of in-
formative model in specific domains is very close
to the corresponding Fbaseline (see Figure 7). We
define the domain-specific average F-measure as
the average of all the F-measure of the NER model
in seven specific domains, denote as F . The av-
erage of all the Fbaseline in specific domains is
denoted as F baseline. The average F-measure of
the informative model and the random model in
specific domains is denoted as F informative and
F random respectively. Compared with F baseline
(F =81.47%), the informative model increases F
by 1.05 percent points. However, F decreases by
2.67 percent points if using the random model. Es-
pecially, the performance of the informative model
is better than the corresponding baseline perfor-
Figure 7: Performance comparison of informa-
tive model, random model, and the corresponding
domain-specific models
mance in politics, life, society and science do-
mains. Moreover, the size of the informative sam-
ple set is much less than the life domain training
set (1.7M).
NER F(%) in specific domains
model Eco- Poli- Spo- Entert- Life So- Sci- F
nomic tics rts ainment ciety ence
domain-
specific 85.46 83.09 90.78 83.31 81.06 76.55 70.02 81.47
(baseline)
Infor-
mative 84.63 84.70 88.71 82.74 84.66 79.49 72.71 82.52
Random 80.50 81.90 86.10 79.31 79.73 74.50 69.55 78.80
NER ?(F ) in specific domain
model ?(F ) = (F ? F ) ?
Eco- Poli- Spo- Entert- Life So- Sci-
nomic tics rts ainment ciety ence
Infor-
mative 2.11 2.18 6.19 0.22 2.14 -3.03 -9.81 4.74
Random 1.7 3.1 7.3 0.51 0.93 -4.3 -9.25 4.94
Table 8: Performance comparison of informa-
tive model, random model and the corresponding
domain-specific model in each specific domain
The informative model has much better perfor-
mance than the random model in specific domains
(see Table 8 and Figure 7). F informative is 82.52%
while F random is 78.80%. The informative model
can increase F by 3.72 percent points. The infor-
mative model is also more stable than the random
model in specific domains (see Table 8). Standard
deviation of F-measure for the informative model
is 4.74 while that for the random model is 4.94.
Our experience with the incremental sample se-
lection provides the following hints.
1. The performance of the NER model across
domains can be significantly enhanced after
being trained with informative samples. In
515
order to obtain a high-quality and stable NER
model, it is only necessary to keep the infor-
mative samples. Informative sample selec-
tion can alleviate the problem of obtaining a
large amount of annotated data. It is also an
effective method for overcoming the poten-
tial limitation of computational resources.
2. In learning NER models, annotated results
with lower confidence scores are more use-
ful than those samples with higher confidence
scores. This is consistent with other studies
on active learning.
5 Conclusion
Efficient and robust NER model is very impor-
tant in practice. This paper provides an empirical
study on the impact of training data size and do-
main information on the performance stability of
NER. Experimental results show that it is difficult
to significantly enhance the performance when the
training data size is above a certain threshold. The
threshold of the training data size varies with do-
mains. The performance stability of each NE type
recognition also varies with domains. The large-
scale corpus statistic data also show that NE types
have different distribution across domains. These
empirical investigations provide useful hints for
enhancing the performance stability of NER mod-
els across domains with less efforts. In order to en-
hance the NER performance across domains, we
present an informative training sample selection
method. Experimental results show that the per-
formance is significantly enhanced by using infor-
mative training samples.
In the future, we?d like to focus on further
exploring more effective methods to adapt NER
model to a new domain with much less efforts,
time and performance degrading.
References
Daniel M. Bikel, Richard L. Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Machine Learning, 34(1-3):211?231.
Andrew Borthwick. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
New York University.
Xavier Carreras, Llu??s Ma`rquez, and Llu??s Padro?.
2003. A simple named entity extractor using ad-
aboost. In Proceedings of CoNLL-2003, pages 152?
155.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recogintion
through classifier combination. In Proceedings
CoNLL-2003, pages 168?171.
Jian F. Gao, Mu Li, Anndy Wu, and Chang N., Huang.
2005. Chinese Word Segmentation and Named En-
tity Recognition: A Pragmatic Approach. Computa-
tional Linguisitc,31(4):531-574.
Hong L. Guo, Jian M. Jiang, Gang Hu, and Tong
Zhang. 2005. Chinese Named Entity Recognition
Based on Multilevel Linguistic Features. Lecture
Notes in Artificial Intelligence,3248:90-99,Springer.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
support vector classifiers for named entity recogni-
tion. In Proceedings of Coling-2002, pages 1-7.
Hongyan Jing, Radu Florian, Xiaoqiang Luo, Tong
Zhang, and Abraham Ittycheriah. 2003. Howtoge-
tachinesename (entity) : Segmentation and combi-
nation issues. In EMNLP 2003, pages 200-207.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recogni-
tion with character-level models. In Proceedings of
CoNLL-2003, pages 180?183.
David D. Lewis and Jason Catlett. 1994. Heteroge-
neous uncertainty sampling for supervised learning.
In Proceedings of the Eleventh International Con-
ference on Machine Learning, pages 148?156.
Andrew Kamal McCallum and K. Nigam. 1998. Em-
ploying EM in pool-based active learning for text
classification. Proceedings of 15th International
Conference on Machine Learning, pages 350-358.
Fien De Meulder and Walter Daelemans. 2003.
Memory-based named entity recognition using
unannotated data. In Proceedings of CoNLL-2003,
pages 208?211.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: Lan-
guage independent named entity recognition. In
Walter Daelemans and Miles Osborne, editors, Pro-
ceedings of CoNLL-2003, pages 142?147.
Dan Shen, Jie Zhang, Jian Su, Gou D. Zhou, and Chew
L.Tan, 2004. Multi-Criteria-based Active Learn-
ing for Named Entity Recognition. Proceedings of
ACL04, pages 589-596.
Yu Z. Wu, Jun Zhao, Bo Xu, and Hao Yu. 2005. Chi-
nese Named Entity Recognition Based on Multiple
Features. Proceedings of EMNLP05, pages 427-434
Shi H. Yu, Shuan H. Bai, and Paul Wu. 1998. De-
scription of the kent ridge digital labs system used
for muc-7. In Proceedings of the Seventh Message
Understanding Conference (MUC-7).
Tong Zhang, Fred Damerau, and David E. Johnson.
2002. Text chunking based on a generalization of
Winnow. Journal of Machine Learning Research,
2:615?637.
516
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1360?1369, Dublin, Ireland, August 23-29 2014.
Sentence Compression for Target-Polarity Word Collocation Extraction
Yanyan Zhao
1
, Wanxiang Che
2
, Honglei Guo
3
, Bing Qin
2
, Zhong Su
3
and Ting Liu
2?
1: Department of Media Technology and Art, Harbin Institute of Technology
2: Department of Computer Science and Technology, Harbin Institute of Technology
3: IBM Research-China
{yyzhao, bqin, tliu}@ir.hit.edu.cn, {guohl, suzhong}@cn.ibm.com
Abstract
Target-polarity word (T-P) collocation extraction, a basic sentiment analysis task, relies primarily
on syntactic features to identify the relationships between targets and polarity words. A major
problem of current research is that this task focuses on customer reviews, which are natural or
spontaneous, thus posing a challenge to syntactic parsers. We address this problem by proposing
a framework of adding a sentiment sentence compression (Sent Comp) step before performing
T-P collocation extraction. Sent Comp seeks to remove the unnecessary information for senti-
ment analysis, thereby compressing a complicated sentence into one that is shorter and easier to
parse. We apply a discriminative conditional random field model, with some special sentiment-
related features, in order to automatically compress sentiment sentences. Experiments show that
Sent Comp significantly improves the performance of T-P collocation extraction.
1 Introduction
Sentiment analysis deals with the computational treatment of opinion, sentiment and subjectivity in tex-
t (Pang and Lee, 2008), and has received considerable attention in recent years (Liu, 2012). Target-
Polarity word (T-P) collocation extraction, which aims to extract the collocation of a target and its cor-
responding polarity word in a sentiment sentence, is a basic task in sentiment analysis. For example,
in a sentiment sentence ????????????? (The camera has a novel appearance), ????
(appearance) is the target, and ???? (novel) is the polarity word that modifies ???? (appearance).
According, ???, ??? (?appearance, novel?) is the T-P collocation. Generally, T-P collocation is a
basic and complete sentiment unit, thus is very useful for many sentiment analysis applications.
Features derived from syntactic parse trees are particularly useful for T-P collocation extraction (Ab-
basi et al., 2008; Duric and Song, 2012). For example, the syntactic relation ?Adj
ATT
x Noun?, where the
ATT denotes an attributive syntactic relation, can be used as an important evidence to extract the T-P
collocation ???, ??? (?appearance, novel?) in the above sentiment sentence (Bloom et al., 2007;
Qiu et al., 2011; Xu et al., 2013).
However, one major problem of these approaches is the ?naturalness? of sentiment sentences, that is,
such sentences are more natural or spontaneous compared with normal sentences, thus posing a challenge
to syntactic parsers. Accordingly, many wrong syntactic features have been produced and these can
further result in the poor performance of T-P collocation extraction. Taking the sentence in Figure 1(a)
as an example, because the word ???? (fortunately) is so chatty,1 the parsing result is wrong. Thus,
are unable to extract the T-P collocation ???,?? (?keyboard, good?).
To solve the ?naturalness? problem, we can train a parser on sentiment sentences. Unfortunately, an-
notating such data will cost us a lot of time and effort. Instead, in this paper we produce a sentence
compression model, Sent Comp, which is designed especially to compress complicated sentiment sen-
tences into formal and easier to parse ones, further improving T-P collocation extraction.
?
Correspondence author: tliu@ir.hit.edu.cn
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Note that, in Figure 1, the Chinese word ???? is chatty, although its translated English word ?fortunately? is not. In this
paper, we focus on processing the Chinese data.
1360
?? ?? ?
fortunately keyboard good
SBV
VOB
ROOT
(a) before compression
?? ?
keyboard good
SBV
ROOT
(b) after compression
Figure 1: Parse trees before and after compression.
This idea is motivated by the observation that, current syntactic parsers usually perform accurately
for short, simple and formal sentences, whereas error rates increase for longer, more complex or more
natural and spontaneous sentences (Finkel et al., 2008). Hence, the improvement in syntactic parsing
performance would have a ripple effect over T-P collocation extraction. For example, we can compress
the sentence in Figure 1(a) into a shortened sentence in Figure 1(b) by removing the chatty part ????
(fortunately). We can see that the shortened sentence is now well-formed (in Chinese) and its parse tree
is correct, making it easier to accurately extract T-P collocation.
Traditional sentence compression aims to obtain a shorter grammatical sentence by retaining impor-
tant information (usually important grammar structure) (Jing, 2000). For example, the sentence ?Overall,
this is a great camera.? can be compressed into ?This is a camera.? by removing the adverbial ?overall?
and the modifier ?great?. However, the modifier ?great? is a polarity word and very important for sen-
timent analysis. Therefore, Sent Comp model for sentiment sentences is different from the traditional
compression models, because it needs to retain the important sentiment information, such as the polarity
word. Hence, using Sent Comp, the above sentence can be compressed into ?This is a great camera.?
We regard Sent Comp as a sequence labeling task, which can be solved by a conditional random
fields (CRF) model. Instead of seeking the manual rules on parse trees for compression, as in other
studies (Vickrey and Koller, 2008), this method is an automatic procedure. In this work, we introduce
some sentiment-related features to retain the sentiment information for Sent Comp.
We apply Sent Comp as the first step in the T-P collocation extraction task. First, we compress the
sentiment sentences into easier to parse ones using Sent Comp, after which we employ the state-of-the-
art T-P collocation extraction approach on the compressed sentences. Experimental results on a Chinese
corpus of four product domains show the effectiveness of our approach.
The main contributions of this paper are as follows:
? We present a framework of using sentiment sentence compression preprocessing step to improve T-
P collocation extraction. This framework can better solve the ?over-natural? problem of sentiment
sentences, which poses a challenge to syntactic parsers. More importantly, the idea of this frame-
work can be applied to some other sentiment analysis tasks that rely heavily on syntactic results.
? We develop a simple yet effective compression model Sent Comp for sentiment sentences. To the
best of our knowledge, this is the first sentiment sentence compression model.
2 Background
For our baseline system, we used the state-of-the-art method to extract T-P collocations introduced by
Qiu et al. (2011), who proposed a double propagation method. This idea is based on the observation
that there is a natural syntactic relationship between polarity words and targets owing to the fact that
polarity words are used to modify targets. Furthermore, they also found that polarity words and targets
themselves have relations in some sentiment sentences (Qiu et al., 2011).
Based on this idea, in the double propagation method, we first used an initial seed polarity word lexicon
and the syntactic relations to extract the targets, which can fall into a new target lexicon. Then we used the
target lexicon and the same syntactic relations to extract the polarity words and to subsequently expand
the polarity word lexicon. This is an iterative procedure, because this method can iteratively produce the
new polarity words and targets back and forth using the syntactic relations.
1361
?? ?? ??
function very powerful
SBV
ADV
ROOT
(a) syntactic structure 1
?? ? ??
powerful function
ATT
RAD
ROOT
(b) syntactic structure 2
?? ? ?? ?
function is powerful
SBV VOB RAD
ROOT
(c) syntactic structure 3
?? ? ?? ?? ??
function and service very powerful
COO
LAD ADV
SBV ROOT
(d) syntactic structure 4
?? ?? ?? ? ??
function very powerful and complete
SBV COO
ADV LAD
ROOT
(e) syntactic structure 5
Figure 2: Example of syntactic structure rules for T-P collocation extraction. We showed five examples
from a total of nine syntactic structures. For each kind of syntactic structure (a) to (e), the target is
shown with a red box and the polarity word is shown with a green box. Syntactic structures (a) to (c)
describe the relations between targets and polarity words. Syntactic structure (d), which is extended
from (a), describes the relation between two targets. Syntactic structure (e), which is also extended from
(a), describes the relation between two polarity words. Similarly, we can summarize the other four rules
extended from (b) and (c) to describe the relations between two targets or two polarity words.
We can see that the syntactic relations are important for this method, and Qiu et al. (2011) proposed
eight rules to describe these relations. However, their work only focused on English sentences, whereas
the relations for Chinese sentences are different. Thus, in accordance with Chinese grammar, we pro-
posed nine syntactic structure rules between target t and polarity word p in a Chinese T-P collocation
?t, p?.
2
The three main rules are shown below and some example rules are illustrated in Figure 2.
Rule 1: t
SBV
x p, the ?subject-verb? structure between t and p, such as the example in Figure 2(a).
Rule 2: p
ATT
x t, that p is an attribute for t, such as the example in Figure 2(b).
Rule 3: t
SBV
x ?
VOB
y p, the ?subject-verb-object? structure between t and p, such as the example in
Figure 2(c). The ? denotes any word.
The other six rules can be extended from the three main rules by obtaining the coordination (COO)
relation of t or p, such as t
SBV
x ?
COO
y p in Figure 2(e). Note that the POS for t should be noun and for p
should be adjective.
As described above, the T-P collocation extraction relies heavily on syntactic parsers. Hence, if we
can use the Sent Comp model to improve the performance of parsers, the performance of T-P collocation
extraction can also be improved accordingly.
3 Sentiment Sentence Compression
3.1 Problem Analysis
First, we conducted an error analysis for the results of current T-P collocation extraction, from which we
observed that the ?naturalness? of sentiment sentences is one of the main problems. For examples:
? Chatty form: some sentiment sentences are so chatty, that they bring many difficulties to the parser.
For example, in the sentence ??????? (fortunately the keyboard is good) shown in Figure 1,
the usage of the chatty word ???? (fortunately) affects the accuracy of the syntactic parser.
2
A Chinese natural language processing toolkit, Language Technology Platform (LTP) (Che et al., 2010), was used as our
dependency parser. More information about the syntactic relations can be found in their paper. The state-of-the-art graph-based
dependency parsing model, in the toolkit, was trained on Chinese Dependency Treebank 1.0 (LDC2012T05).
1362
?? ?? ?
besides photo good
ADV
POB
ROOT
comp
?? ?
photo good
SBV
ROOT
(a) parse tree 1 before and after compression
?? ? ? ? ?? ??screen for people feel good
SBV
ATT
POBRAD SBV
ROOT
comp
?? ??screen good
SBV
ROOT
(b) parse tree 2 before and after compression
Figure 3: ?Naturalness? problem of sentiment sentences.
? Conjunction word usage: conjunction words are often used in sentiment sentences to show the dis-
course relations between two sentences. However, there are so many conjunction words in Chinese,
some of which can cause errors among parsers. For example, in Figure 3(a), the parse tree of sen-
tence ???????? (besides the photo is good) is wrong because of the usage of the conjunction
word ???? (besides).
? Feeling words/phrase usage: in sentiment sentences, people often use some feeling words/phrase,
such as ??????? (feel like) in Figure 3(b) or ????? (smell like). Given that the current
syntactic parser cannot handle the feeling words/phrases very well, the T-P collocation ???, ?
?? (?screen, good?) in Figure 3(b) cannot be extracted correctly.
To address the ?naturalness? problem, we compressed the sentiment sentences into one that are shorter
and easier to parse. Similar to the examples in Figure 1 and 3, the compressed sentences can be easily
and correctly parsed. The above analysis can be used as the criteria to guide us in compressing sentiment
sentences when annotating, and can also help us exploit more useful features for automatic sentiment
sentence compression.
3.2 Task Definition
We focus on studying the methods for extractive sentence compression.
3
Formally, extractive sentence
compression aims to shorten a sentence x = x
1
? ? ?x
n
into a substring y = y
1
? ? ? y
m
, where y
i
?
{x
1
, ? ? ? , x
n
}, m ? n.
In this paper, similar to Nomoto (2007), we also treated the sentence compression as a sequence
labeling task which can be solved by a CRF model. We assigned a compression tag t
i
to each word x
i
in
an original sentence x, where t
i
= N if x
i
? y, else t
i
= Y.
A first-order linear-chain CRF is used which defines the following conditional probability:
P (t|x) =
1
Z(x)
?
i
M
i
(t
i
, t
i?1
|x) (1)
where x and t are the input and output sequences respectively, Z(x) is the partition function, and M
i
is
the clique potential for edge clique i. Here, we used the CRFsuite toolkit to train the CRF model.
4
3.3 Features
The features for Sent Comp are listed in Table 1. Aside from the basic word (w), POS tag (t) and
their combination context features (01 ? 04), we introduced some sentiment-related features (05 ? 06)
and latent semantic features (07 ? 08) to better handle sentiment analysis data and generalize word
features. Then we added the syntactic parse features (09), which are commonly used in traditional
sentence compression task.
One sentiment-related feature (feeling(?)) indicates whether a word is a feeling word, which is inspired
by the naturalness problem in Figure 3(b). As discussed above, the current parser often produces wrong
parse trees because of these feeling words. Therefore, the feeling words tend to be removed from a
3
Generally, there are two kinds of sentence compression methods: extractive method and abstractive method. Because
abstractive method needs more resource and is more complicated, in this paper, we only focus on extractive approach.
4
www.chokkan.org/software/crfsuite/
1363
Basic Features
01: w
i+k
,?1 ? k ? 1
02: w
i+k?1
? w
i+k
, 0 ? k ? 1
03: t
i+k
,?2 ? k ? 2
04: t
i+k?1
? t
i+k
,?1 ? k ? 2
Sentiment-related Features
05: feeling(w
i
)
06: polarity(w
i
)
Latent Semantic Features
07: suffix(w
i
) if t(w
i
) == n else prefix(w
i
)
08: cluster(w
i
)
Syntactic Features
09: dependency(w
i
)
Table 1: Features of sentiment sentence compression
sentiment sentence for Sent Comp. We can obtain a feeling word lexicon from HowNet,
5
a popular
Chinese sentiment thesaurus, where a feeling word is defined by DEF={perception|??} tag. Finally,
we collected 38 feeling words, such as?? (realize),?? (find), and?? (think).
The other sentiment-related feature (polarity(?)) indicates whether a word is a polarity word. One
of the main differences between a sentiment sentence and a formal sentence is that the former often
contains polarity words. In contrast to the features of feeling(?), polarity words (e.g., ?great? in the
sentence ?Overall, this is a great camera?) tend to be retained, because they are important and special
to sentiment analysis. In this paper, we treat polarity words as important features, considering that they
are often tagged as modifiers and are easily removed by common sentence compression methods. We
can obtain the polarity feature (polarity(?)) from a polarity lexicon, which can also be obtained from
HowNet.
To generalize the words in sentiment sentences, we proposed two kinds of semantic features. The
first one is a suffix or prefix character feature (prefix(?) or suffix(?)). In contrast to English, the suffix
(for noun) or prefix (for non noun) characters of a Chinese word often carry that word?s core semantic
information. For example, ??? (bicycle), ?? (car), and ?? (train) are all various kinds of ?
(vehicle), which is also the suffix of the three words. Given that all of them may become targets, they
tend to be retained in compressed sentences. The verbs, ?? and ??, can be denoted by their prefix
feel (?), and can be removed from original sentences because they are feeling words.
We used word clustering features (cluster(?)) as the other latent semantic feature to further improve
the generalization over common words. Word clustering features contain some semantic information
and have been successfully used in several natural language processing tasks, including NER (Miller et
al., 2004; Che et al., 2013) and dependency parsing (Koo et al., 2008). For instance, the words ??
and ?? (appearance) belong to the same word cluster, although they have a different suffix or prefix.
Both words are important for T-P collocation extraction and should be retained. We used the Brown
word clustering algorithm (Brown et al., 1992) to obtain the word clusters (Liang, 2005). Raw texts were
obtained from the fifth edition of Chinese Gigaword (LDC2011T13).
Finally, similar to McDonald (2006), we also added the dependency relation between a word and its
parent as the syntactic features. Intuitively, the dependency relations are helpful in carrying out sentence
compression. For example, the ROOT relation typically indicates that the word should not be removed
because it is the main verb of a sentence.
4 Experiments
4.1 Experimental Setup
4.1.1 Corpus
We conducted the experiments on a Chinese corpus of four product domains, which came from the Task3
of the Chinese Opinion Analysis Evaluation (COAE) (Zhao et al., 2008).
6
Table 2 describes the corpus,
5
www.keenage.com
6
www.ir-china.org.cn/coae2008.html
1364
Domain # reviews # sentences # collocations
Camera 138 1,249 1,335
Car 161 1,172 1,312
Notebook 56 623 674
Phone 123 1,350 1,479
All 478 4,394 4,800
Table 2: Corpus statistics for the Chinese corpus of four product domains.
where 4,394 sentiment sentences containing 4,800 T-P collocations are manually found and annotated
from 478 reviews.
We ask annotators to manually compress all the sentiment sentences. Specifically, the annotators
removed some words from a sentiment sentence according to two criteria stated as follows: (1) removing
the word should not change the essential content of the sentence, and (2) removing the word should
not change the sentiment orientation of the sentence. In order to assess the quality of the annotation,
we sampled 500 sentences from this corpus and asked two annotators to perform the annotation. The
resulting word-based Cohen?s kappa (Cohen, 1960) (i.e., a measure of inter-annotator agreement ranging
from zero to one) of 0.7 indicated a good strength of agreement.
4.1.2 Evaluation
Generally, compressions are evaluated using three criteria (McDonald, 2006), namely, grammaticality,
importance, and compression rate. Obviously, the former two are difficult to evaluate objectively. Previ-
ous works used human judgment, which entails a difficult and expensive process. In this paper, similar to
a common sequence labeling task, we simply used the F-score metric of removed words to roughly eval-
uate the performance of sentiment sentence compression. Of course, the final effectiveness of sentence
compression model can be reviewed by the derived T-P collocation extraction task.
For T-P collocation extraction, we applied the traditional P, R and F-score for the final evaluations.
Specially, a fuzzy matching evaluation is adopted for the T-P collocation extraction. That is to say,
given an extracted T-P collocation ?t, p?, whose standard result is ?t
s
, p
s
?, if t is the substring of t
s
, and
meanwhile p is the substring of p
s
, we consider the extracted ?t, p? is a correct T-P collocation.
4.2 Sentiment Sentence Compression Results
Features P(%) R(%) F(%)
Basic (01 ? 04) 76.4 57.4 65.5
+ feeling (05) 75.9 57.6 65.5
+ polarity (06) 76.6 57.6 65.7
+ suffix or prefix (07) 78.4 56.9 66.0
+ cluster (08) 74.9 58.9 65.9
+ dependency (09) 75.3 57.2 65.0
All (01 ? 08) 77.3 59.1 67.0
All - feeling (05) 77.1 58.9 66.8
Table 3: The results of sentiment sentence compression with different features.
Results of Sent Comp with different features are shown in Table 3. All results are reported using five-
fold cross validation. We can see that the performance is improved when we added feeling
7
and polarity
features (05 ? 06) respectively, indicating that the sentiment-related features are useful for sentiment
sentence compression. In addition, the latent semantic features (07 ? 08) are also helpful, especially the
suffix or prefix features, which show better performance than the four other kinds of features.
Nonetheless, the dependency features (09) have a negative on compression performance due to the
specificity of compression for sentiment sentences. That is because the lower dependency parsing per-
formance on sentiment sentences introduces many wrong dependency relations, which counteract the
7
In Table 3, although the performance of adding feeling is comparative to the basic system (Basic (01-04)), the system
without feeling (All - feeling (05), the last line) is worse than the system using all the features (All (01-08)). This can illustrate
the effectiveness of the feeling feature.
1365
Domain Method P(%) R(%) F(%)
no Comp 74.7 58.4 65.6
Camera manual Comp 83.4 62.7 71.6
auto Comp 80.4 62.1 70.1
no Comp 68.2 53.1 59.7
Car manual Comp 76.3 57.7 65.7
auto Comp 72.3 56.1 63.2
no Comp 74.1 56.8 64.3
Notebook manual Comp 82.7 64.5 72.5
auto Comp 79.7 62.8 70.2
no Comp 77.3 60.9 68.1
Phone manual Comp 82.7 65.7 73.2
auto Comp 80.3 63.3 70.8
no Comp 73.7 57.5 64.6
All manual Comp 81.2 62.5 70.6
auto Comp 78.1 60.9 68.4
Table 4: Results on T-P collocation extraction for four product domains.
contribution of the dependency relation features. This is also the reason why we need to compress sen-
timent sentences as the first step for T-P collocation extraction. Finally, when we combine all of useful
features (01 ? 08), the performance achieves the highest score.
It is worth noting that sentiment sentence compression is a new task proposed in this paper. For
simplicity, this paper aims to attempt a simple yet effective sentiment sentence compression model. We
will polish the Sent Comp model in the future work.
4.3 Sent Comp for T-P Collocation Extraction
We designed three comparative systems to demonstrate the effectiveness of Sent Comp for T-P collo-
cation extraction. Note that, Sent Comp is the first step to process the corpus before T-P collocation
extraction. The method for T-P collocation extraction was based on the state-of-the-art method proposed
by Qiu et al. (2011) as described in Section 2.
no Comp - This refers to the system that only uses the T-P collocation extraction method and does not
perform sentence compression as the first step.
manual Comp - This system manually compresses the corpus into a new one as the first step, and then
applies the T-P collocation extraction method on the new compressed corpus.
auto Comp - This system uses Sent Comp as the first step to automatically compress the corpus into a
new one, and then applies the T-P collocation extraction method on the new corpus.
From the descriptions above, we can draw a conclusion that the performance of manual Comp can be
considered as the upper bound for the sentiment sentence compression based T-P collocation extraction
task.
Table 4 shows the experimental results of the three systems on T-P collocation extraction for four prod-
uct domains. Here, manual Comp can significantly (p < 0.01) improved the F-score by approximately
6%,
8
compared with no Comp. This illustrates that the idea of sentiment sentence compression is use-
ful for T-P collocation extraction. Specifically, the proposed method can transform some over-natural
sentences into normal ones, further influencing their final syntactic parsers. Evidently, because the T-P
collocation extraction relies heavily on syntactic features, the more correct syntactic parse trees derived
from the compressed sentences can help to increase the performance of this task.
Compared with no Comp, the auto Comp system also yielded a significantly better results (p < 0.01)
that indicated an improvement of 3.8% in the F-score, despite the fact that the automatic sentence com-
pression model Sent Comp may wrongly compress some sentences. This demonstrates the usefulness
of sentiment sentence compression step in the T-P collocation extraction task and further proves the
effectiveness of our proposed model.
8
We use paired bootstrap resampling significance test (Efron and Tibshirani, 1993).
1366
Moreover, we can observe that the idea of sentence compression and our Sent Comp are useful for
all the four product domains on T-P collocation extraction task, indicating that Sent Comp is domain
adaptive. However, we can find a small gap between auto Comp and manual Comp, which indicates
that the Sent Comp model can still be improved further. In the future, we will explore more effective
sentence compression algorithms to bridge the gap between the two systems.
5 Related Works
5.1 Sentiment Analysis
T-P collocation extraction is a basic task in sentiment analysis. In order to solve this task, most methods
focused on identifying relationships between targets and polarity words. In early studies, researcher-
s recognized the target first, and then chose its polarity word within a window of size k (Hu and Liu,
2004). However, considering that this kind of method is too heuristic, the performance proved to be very
limited. To tackle this problem, many researchers found syntactic patterns that can better describe the
relationships between targets and polarity words. For example, Bloom et al. (2007) constructed a link-
age specification lexicon containing 31 patterns, while Qiu et al. (2011) proposed a double propagation
method that introduced eight heuristic syntactic patterns to extract the collocations. Xu et al. (2013) used
the syntactic patterns to extract the collocation candidates in their two-stage framework.
Based on the above, we can conclude that syntactic features are very important for T-P collocation
extraction. However, the ?naturalness? problem can still seriously affect the performance of syntactic
parser. Once our sentiment sentence compression method can improve the quality of parsing, the perfor-
mance of T-P collocation extraction task can be improved as well. Note that, to date, there is no previous
work using a sentence compression model to improve this task.
5.2 Sentence Compression
Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones,
while preserving the essential content (Jing, 2000). There are many applications that can benefit from
a robust compression system, such as summarization systems (Li et al., 2013), semantic role label-
ing (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on.
Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and
Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos,
2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the
syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus,
the compressed tree (after removing constituents from a bad parse) may not produce a good compressed
sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem
by using discriminative models.
Aside from above extractive sentence compression approaches, there is another research line, namely,
abstractive approach, which compresses an original sentence by reordering, substituting, and inserting,
as well as removing (Cohn and Lapata, 2013). This method needs more resource and is more complicat-
ed. Therefore, in this paper, we only focus on extractive approach.
At present, the current sentence compression methods all focus on formal sentences, and few meth-
ods are being proposed to study sentiment sentences. As discussed in the above sections, the current
compression models cannot be directly utilized to T-P collocation extraction owing to the specificity of
sentiment sentences. Therefore, a new compression model for sentiment sentences should be established.
6 Conclusion and Future Work
In this work, we presented a framework that adopted a CRF based sentiment sentence compression mod-
el Sent Comp, as a preprocessing step, to improve the T-P collocation extraction task. Different from
the existing sentence compression models used for formal sentences, Sent Comp incorporated some
sentiment-related features to retain the sentiment information. Experimental results showed that the sys-
tem with the sentence compression step performed better than that without this step, thus demonstrating
the effectiveness of the framework and the compression model Sent Comp.
1367
Generally, the idea of this framework maybe useful for many sentiment analysis tasks that rely heavily
on syntactic results. Thus in the future, we will try to apply the Sent Comp model for these tasks. Besides,
the simplicity and effectiveness of this framework motivates us to pursue the study further. For example,
we will polish the Sent Comp model by exploring more sentiment-related features and exploring other
types of compression models.
Acknowledgments
We thank the anonymous reviewers for their helpful comments. This work was supported by National
Natural Science Foundation of China (NSFC) via grant 61300113, 61133012 and 61273321, the Ministry
of Education Research of Social Sciences Youth funded projects via grant 12YJCZH304, the Fundamen-
tal Research Funds for the Central Universities via grant No.HIT.NSRIF.2013090 and IBM Research-
China Joint Research Project.
References
Ahmed Abbasi, Hsinchun Chen, and Arab Salem. 2008. Sentiment analysis in multiple languages: Feature
selection for opinion classification in web forums. ACM Trans. Inf. Syst., 26(3):12:1?12:34, June.
Kenneth Bloom, Navendu Garg, and Shlomo Argamon. 2007. Extracting appraisal expressions. In HLT-NAACL
2007, pages 308?315.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Comput. Linguist., 18(4):467?479, December.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp: A chinese language technology platform. In Coling 2010:
Demonstrations, pages 13?16, Beijing, China, August. Coling 2010 Organizing Committee.
Wanxiang Che, Mengqiu Wang, Christopher D. Manning, and Ting Liu. 2013. Named entity recognition with
bilingual constraints. In Proceedings of the 2013 Conference of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Language Technologies, pages 52?62, Atlanta, Georgia, June.
Association for Computational Linguistics.
James Clarke and Mirella Lapata. 2008. Global inference for sentence compression: An integer linear program-
ming approach. J. Artif. Intell. Res. (JAIR), 31:399?429.
Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement,
20(1):37 ? 46.
Trevor Cohn and Mirella Lapata. 2009. Sentence compression as tree transduction. Journal of Artificial Intelli-
gence Research, 34:637?674.
Trevor Cohn and Mirella Lapata. 2013. An abstractive approach to sentence compression. ACM Transactions on
Intelligent Systems and Technology, 4(3):1?35.
Adnan Duric and Fei Song. 2012. Feature selection for sentiment analysis based on content and syntax models.
Decis. Support Syst., 53(4):704?711, November.
B. Efron and R. J. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman & Hall, New York.
Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL-08: HLT, pages 959?967, Columbus, Ohio, June. Association for
Computational Linguistics.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An extractive supervised two-stage method for sentence
compression. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, pages 885?893, Los Angeles, California, June. Association
for Computational Linguistics.
Michel Galley and Kathleen McKeown. 2007. Lexicalized Markov grammars for sentence compression. In
Human Language Technologies 2007: The Conference of the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main Conference, pages 180?187, Rochester, New York, April.
Association for Computational Linguistics.
1368
Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of KDD-2004,
pages 168?177.
Hongyan Jing. 2000. Sentence reduction for automatic text summarization. In IN PROCEEDINGS OF THE 6TH
APPLIED NATURAL LANGUAGE PROCESSING CONFERENCE, pages 310?315.
Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach to
sentence compression. Artif. Intell., 139(1):91?107, July.
Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-08: HLT, pages 595?603, Columbus, Ohio, June. Association for Computational Linguistics.
Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013. Document summarization via guided sentence compression.
In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 490?500,
Seattle, Washington, USA, October. Association for Computational Linguistics.
Percy Liang. 2005. Semi-supervised learning for natural language. Master?s thesis, MIT.
Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.
Morgan & Claypool Publishers.
Ryan McDonald. 2006. Discriminative sentence compression with soft syntactic evidence. In In Proc. EACL.
Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and discriminative
training. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings,
pages 337?342, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.
Makoto Miwa, Rune Saetre, Yusuke Miyao, and Jun?ichi Tsujii. 2010. Entity-focused sentence simplification
for relation extraction. In Proceedings of the 23rd International Conference on Computational Linguistics
(COLING 2010), pages 788?796.
Tadashi Nomoto. 2007. Discriminative sentence compression with conditional random fields. Information Pro-
cessing and Management, 43(6):1571?1587, November.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?135,
January.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through
double propagation. Computational Linguistics, 37(1):9?27.
Kapil Thadani and Kathleen McKeown. 2013. Sentence compression with joint structural inference. In Pro-
ceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 65?74, Sofia,
Bulgaria, August. Association for Computational Linguistics.
Jenine Turner and Eugene Charniak. 2005. Supervised and unsupervised learning for sentence compression. In
Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ?05, pages 290?
297, Stroudsburg, PA, USA. Association for Computational Linguistics.
David Vickrey and Daphne Koller. 2008. Sentence simplification for semantic role labeling. In ACL, pages
344?352. The Association for Computer Linguistics.
Kristian Woodsend and Mirella Lapata. 2011. Learning to simplify sentences with quasi-synchronous grammar
and integer programming. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language
Processing, pages 409?420, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun Zhao. 2013. Mining opinion words and opinion targets
in a two-stage framework. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1764?1773, Sofia, Bulgaria, August. Association for Computational
Linguistics.
Jun Zhao, Hongbo Xu, Xuanjing Huang, Songbo Tan, Kang Liu, and Qi Zhang. 2008. Overview of chinese pinion
analysis evaluation 2008. In The First Chinese Opinion Analysis Evaluation (COAE) 2008.
1369
