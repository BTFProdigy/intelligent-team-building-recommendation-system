Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1740?1751, Dublin, Ireland, August 23-29 2014.
Detecting Learner Errors in the Choice of Content Words
Using Compositional Distributional Semantics
Ekaterina Kochmar
Computer Laboratory
University of Cambridge
ek358@cl.cam.ac.uk
Ted Briscoe
Computer Laboratory
University of Cambridge
ejb@cl.cam.ac.uk
Abstract
We describe a novel approach to error detection in adjective?noun combinations. We present and
release a new dataset of annotated errors where the examples are extracted from learner texts and
annotated with error types. We show how compositional distributional semantic approaches can
be applied to discriminate between correct and incorrect word combinations from learner data.
Finally, we show how the output of the compositional distributional semantic models can be used
as features in a classifier yielding good precision and accuracy.
1 Introduction
The task of error detection and correction (henceforth, EDC) in non-native writing in English has been
a focus of research in recent years. However, usually research in this area focuses on EDC in the use of
function words, such as articles or prepositions (Leacock et al., 2010; Dale et al., 2012), while much less
attention has been paid to errors in the choice of content words.
Errors in function words are some of the most common error types in learner writing (Dalgish, 1985;
Leacock et al., 2010), so it is important for any EDC system to be able to deal with such errors. Certain
properties of these errors facilitate their detection and correction. As function words belong to closed
classes, the set of possible corrections is limited by the size of the function word set. Since errors in
function words are systematic and highly recurrent, in practice, each article or preposition has an even
smaller number of appropriate alternatives. We illustrate this point with the following examples on (1)
article and (2) preposition errors:
(1) I am 0*/a student.
(2) Last October, I came in*/to Tokyo.
In (1) an EDC system would consider {a, an, the} as possible corrections for the missing article. To
correct the preposition in in (2), an EDC system would consider the most frequent prepositions {on,
from, for, of, about, to, at, with, by}, among which at or to would have a higher chance to be appropriate
corrections as these are most often confused with in. Confusion sets can be learnt from learner texts, and
probabilities can be set up according to the distribution of the confusions (Rozovskaya and Roth, 2011).
EDC is usually cast as a multi-class classification task, with the number of classes equal to the number
of target corrections. Detection and correction can occur simultaneously: an error is detected when an
EDC system suggests using a word different from the one originally used by the learner, and the sug-
gested word can be used as a correction. Each occurrence of a function word is represented with a feature
vector, where features are derived from the surrounding context. This is usually highly informative for
function words: for example, a context of I am and student or a similar noun requires the use of an
indefinite article, while the only correct preposition to relate a verb of movement like come to a locative
like Tokyo is to.
In this work, however, we focus on errors in the choice of content words, which have received much
less attention in spite of being the third most frequent error type in learner writing (Leacock et al., 2010).
Errors in content words are more challenging than errors in function words, since the number of possible
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1740
confusions and corrections cannot be reduced to a finite set. For example, consider incorrect choice of
adjectives in the following sentences extracted from learner data:
(3) A big*/great damage has been made to the environment.
(4) I have tried a rock?n?roll dance and a classic*/classical dance already.
The confusion in (3) is caused by semantic similarity of the adjectives big and great, while in (4) it is
due to similarity in form between classic and classical. It is much harder to cast the EDC in content words
as multi-class classification, unless we consider the full set of English adjectives as possible classes. The
surrounding contexts are much sparser and less informative, and in addition to that, often contain further
errors. In this work, we address error detection and focus on adjective-noun combinations (ANs), which
are representative of the more general task of EDC in content word combinations and are a frequent error
type in learner text.
We have created a dataset of ANs, where the combinations are extracted from learner texts and man-
ually error-coded using a novel annotation scheme. This scheme is motivated by observations about
typical learner confusions in the choice of adjectives and nouns ? for example, semantically-related or
form-related confusions. Since errors in content words are related to semantics, we derive semantically-
motivated features through models of compositional distributional semantics and use these features for
error detection. We treat error detection as a binary classification task, following the usual convention in
EDC.
The original contributions of this paper are that we:
? present and release an error-annotated AN dataset extracted from learner data;
? show how compositional distributional semantic models can be applied to detect semantic anomalies
in this dataset;
? demonstrate that the output of these models can be used to derive features for error detection in AN
combinations.
2 Previous work
2.1 Error Detection in Content Words
Previous work on EDC for content words has either focused on correction alone assuming that errors
are already detected (Liu et al., 2009; Dahlmeier and Ng, 2011), or has reformulated the task as writing
improvement (Shei and Pain, 2000; Wible et al., 2003; Chang et al., 2008; Futagi et al., 2008; Park et al.,
2008; Yi et al., 2008;
?
Ostling and Knutsson, 2009).
In the first case, the task is reduced to the search for the most suitable correction among the alternatives
typically composed of synonyms, homophones or L1-related paraphrases (Dahlmeier and Ng, 2011),
while the more challenging error detection step is omitted. In the second case, error detection is integrated
into suggestion of alternatives and their comparison to the originally used word combination according
to some metric of collocational strength. Such approaches aim to improve the fluency of non-native
texts by correcting erroneous idioms or collocations, where low frequency or low collocational strength
clearly signifies an error.
These approaches might be useful for correcting collocations, but they are less suitable for error detec-
tion in free word combinations. As they compare original word combinations to their alternatives using
corpus statistics, they are not applicable to unseen word combinations, while learner texts contain many
previously unseen combinations, not all of which are errors. Moreover, some word combinations may
be correct even though less fluent than some of their alternatives. For example, appropriate concern,
though it is correct, would have lower collocational strength than its alternative proper concern, and
would, according to this approach, be tagged as an error. From the educational point of view, tagging an
acceptable combination as an error is misleading for language learners and should be avoided.
We implement a baseline model inspired by such comparison-based approaches and demonstrate that it
cannot be usefully applied to error detection in content word combinations. Then we present an approach
that is capable of dealing with unseen data and does not rely on direct corpus-based comparison.
1741
2.2 Semantic Anomaly Detection
Learner errors in content words often result from a semantic mismatch between the chosen words. A
similar problem of semantic anomaly detection in content word combinations has been addressed with
compositional distributional semantic models.
These models are based on distributional representations for words which are then composed to derive
phrase representations. They rely on the assumption that a word meaning can be approximated by its
distribution across its contexts of use. Words are represented as vectors in a high-dimensional space with
each dimension encoding a word?s co-occurrence with one of its contextual elements. Distributional
models are less suitable for representing content word combinations directly since these will be very
sparse and will often remain unattested even in an extremely large corpus.
A promising solution is provided by compositional distributional semantic models, which combine
distributional vectors for the component words using some function over such vectors. Compositional
distributional semantic representations have been previously used to detect semantic anomaly in AN
combinations (Vecchi et al., 2011). Vecchi et al. have applied the additive and multiplicative models
of Mitchell and Lapata (2008) and adjective-specific linear maps of Baroni and Zamparelli (2010) to a
set of corpus-unattested ANs. They show that there is a distinguishable difference in the compositional
semantic representations for the semantically acceptable and anomalous combinations, suggesting that
compositional distributional models can be used to detect semantic anomaly without relying directly on
corpus statistics.
Kochmar and Briscoe (2013) have applied the same models of semantic composition to distinguish
between correct and incorrect ANs extracted from learner texts. Their results support the assumption
that there is a distinguishable difference between the composite vectors for the correct and incorrect
ANs, but they did not address the question of how to integrate these semantic models into an error
detection system.
Recent work by Lazaridou et al. (2013) has shown that measures used for quantifying the degree of
semantic anomaly in phrases derived from their compositional distributional semantic representations
can be used as features by a classifier to help resolve syntactic ambiguities.
Our goals are to test, using a new and larger AN dataset, whether semantic models can distinguish
between correct and incorrect AN combinations, which cannot be dealt with using simpler error detection
approaches, and to implement an error detection system using these semantically-based features.
3 Data Annotation
We present and release a dataset of AN combinations which, on the one hand, exemplify the typical
errors committed by language learners in the choice of content words within such combinations, and, on
the other hand, are challenging for an EDC system.
For that, we examined the publicly available CLC-FCE dataset (Yannakoudakis et al., 2011), used
the error annotation (Nicholls, 2003), and analysed the typical errors in AN combinations committed by
language learners. We have compiled a list of 61 adjectives that are most problematic for learners.
Most typically, learners confuse semantically related words: for example, they are unable to distin-
guish between synonyms, near-synonyms or co-hyponyms and choose an appropriate one from the set.
Our list of adjectives contains some frequent ones that are confused with each other due to their similarity
in meaning. For example, the adjectives within the set {big, large, great} are frequently confused with
each other as in:
(5) big*/large quantity (6) big*/great importance
Another common source of error related to the high-frequency adjectives involves using them instead
of more specific ones: in such cases, learners are unable to distinguish between the more specific terms
and they choose the most frequent adjective, usually encompassing a variety of related meanings, to
represent the whole class of similar words. For example, adjectives big and large encompass a variety of
meanings including those of high, wide or broad. As learners often lack intuitions about which of these
1742
more specific adjectives should be chosen, they use the ones with more general meaning. This results in
errors like:
(7) big*/long history
(8) bigger*/wider variety
(9) greatest*/highest revenue
(10) large*/broad knowledge
The reverse of this ? an incorrect selection of a more specific term instead of the more general one ?
also leads to learner errors.
Form-related confusions represent another typical source of learner errors, and we have included pairs
of adjectives such as classic and classical, economic and economical and the like in our dataset:
(11) classic*/classical dance (12) economical*/economic crisis
Using this set of 61 adjectives, we have extracted AN combinations from the Cambridge Learner
Corpus (CLC),
1
a large corpus of texts produced by English language learners, sitting Cambridge As-
sessment?s examinations.
2
We have focused on AN combinations previously unseen in a native English
corpus, as we hypothesise that they would have a higher chance of containing an error. Such combina-
tions are more challenging for EDC algorithms since:
? these ANs cannot be effectively handled with simple comparison-based approaches like the ones
overviewed in section 2.1;
? language learners are creative in their writing, so there is a substantial number of such previously
unseen combinations;
? as no corpus could cover all possible acceptable content word combinations in language, the fact that
these combinations are not seen in the corpus cannot be used as definitive evidence of incorrectness.
To summarise, it is important for an EDC algorithm to handle such combinations, but their absence in
a native corpus of English makes it impossible to rely on simpler approaches and suggests that semantic
analysis of such combinations would be more effective. In our research, we used the British National
Corpus (BNC)
3
to select the corpus-unattested combinations.
We have compiled a set of 798 AN combinations.
4
An annotation scheme has been devised to annotate
these examples as correct or incorrect, and for the incorrect combinations, to identify the locus of error
(adjective, noun or both) and the type of confusion (incorrect synonym, form-related word, or non-related
word). The most appropriate corrections are included in the dataset.
We also distinguish between out-of-context (OOC) and in-context (IC) annotation. The motivation
behind this distinction is as follows: some combinations may appear to be correct when considered
out of their original context of use, because there might be other contexts where the same combination
would be appropriate. For example, classic dance is annotated as correct out of context because one
could imagine using it in a context where it would denote some typical dance like:
(13) They performed a classic Ceilidh dance.
However, in practice, the AN classical dance is used much more frequently, and classic dance is most
often errorful in context, as in (4) above.
Some ANs in our dataset are represented with more than one context of use, and in that case the
in-context annotation can be conditioned on each context, or used to derive the most typical annotation
for the AN. Both types of information are useful, as EDC systems which make use of the surrounding
context should rely on the annotation in each particular context of use and, for example, be able to detect
1
http://www.cup.cam.ac.uk/gb/elt/catalogue/subject/custom/item3646603/
Cambridge-International-Corpus-Cambridge-Learner-Corpus/
2
http://www.cambridgeenglish.org
3
http://www.natcorp.ox.ac.uk/
4
This dataset is released and publicly-available at http://www.ilexir.com/
1743
Type Cor. Incor. LB UB
OOC 633 165 0.7932 0.8650
IC 394 404 0.5063 0.7467
Table 1: Distribution of correct (cor.) and incorrect (incor.) ANs in the dataset.
that classic dance is correct in one specific context, while in others it is incorrect. EDC systems that do
not make use of the context can simply rely on the most frequent in-context annotation and detect that
classic dance is typically an error in learner writing.
To create the two-level annotation, the annotators were first presented with an AN combination and
asked to tag each word as correct or incorrect depending on whether they can think of some appropriate
contexts of use for it. Next, the same combination was presented in its context of use from the CLC and
the annotators were asked to annotate it with respect to its context.
The dataset was primarily annotated by a professional linguist. To ensure that the annotation scheme
is clear and efficient, the dataset was split into 100 and 698 ANs, and the 100 ANs were first annotated
by the same professional annotator and three other annotators. We have measured the inter-annotator
agreement for the two levels of annotation using the mean values for the observed agreement within
each pair of annotators, as well as mean Cohen?s kappa value (Cohen, 1960). In Table 1 we report
the mean inter-annotator agreement for the correct versus incorrect combinations at the two annotation
levels, which represents the upper bound (UB) in our experiments. We have obtained the mean kappa
values of 0.65 and 0.49 at the two levels of annotation, which are interpreted as substantial and medium
agreement between annotators and confirm that the annotation scheme is clear.
5
Table 1 presents the
distribution of ANs and the majority class baseline which we further use as a lower bound (LB).
4 Semantic Models for Error Detection
We replicate the semantic approaches, which have previously shown promising results in detecting se-
mantic anomaly and content word errors (Vecchi et al., 2011; Kochmar and Briscoe, 2013), and test their
performance on our dataset of corpus-unattested correct and incorrect AN combinations.
4.1 Experimental Setting
We use the additive (add) and multiplicative (mult) models of Mitchell and Lapata (2008), and the
adjective-specific linear maps (alm) of Baroni and Zamparelli (2010).
The first two models derive the composite phrase vector through addition and multiplication of the
components of the word vectors. These models have a clear mathematical interpretation and require
no training. Their principal weakness is that they are symmetric, and fail to represent the difference in
grammatical function of the component words. The alm model provides a theoretically more appropriate
way of representing ANs based on this asymmetry: nouns are represented by their distributional vectors,
while attributive adjectives are functions mapping from noun meanings to a composite noun-like vector
for the ANs. Adjectives are represented as weight matrices which are learned from corpus-attested
examples of noun?AN mappings, and composition is defined by matrix-by-vector multiplication.
We use the experimental setting previously described (Vecchi et al., 2011; Kochmar and Briscoe,
2013) and populate the semantic space with the constituent nouns and adjectives from the test ANs,
frequent nouns and adjectives from the BNC and the AN combinations containing these frequent words.
We use about 8K nouns, 4K adjectives and 64K ANs following Kochmar and Briscoe (2013). The
semantic space is represented by a matrix encoding word co-occurrences, where the rows represent the
76K elements mentioned above, and the columns represent a selected set of 10K context elements.
The 10K context elements include the most frequent nouns, adjectives and verbs from the corpus. The
word co-occurrence counts are estimated using the BNC. The corpora have been lemmatized, tagged and
parsed with the RASP system (Briscoe et al., 2006; Andersen et al., 2008; Yannakoudakis et al., 2011),
and all statistics are extracted at the lemma level.
5
Further details of the annotation experiment are described in the dataset release.
1744
We transform the raw sentence-internal co-occurrence counts into Local Mutual Information
scores (Baroni and Zamparelli, 2010; Evert, 2005), and perform dimensionality reduction applying Sin-
gular Value Decomposition to the noun and adjective matrix rows, projecting the AN rows onto the same
reduced space following Baroni and Zamparelli (2010). The original 76K ? 10K matrix is reduced to a
76K ? 300 matrix. This allows us to perform training and other calculations in the semantic space more
efficiently.
The weight coefficients for the alm model are estimated with multivariate partial least squares re-
gression using the RPLS package (Mevik and Wehrens, 2007). The weight matrix is learned for each
adjective separately.
4.2 Semantic Cues
In previous work (Vecchi et al., 2011; Kochmar and Briscoe, 2013) several semantic measures for de-
tecting semantic anomaly have been introduced. We reimplement these measures (1 to 8), but also test
some additional measures (9 to 13) that we hypothesise can also help distinguish between correct and
incorrect word combinations:
1. Vector length (VLen): vectors for correct and incorrect combinations may differ with respect to
their length, and the latter are expected to be shorter;
2. Cosine to the input noun (cosN): the distance between the model-generated AN vector and the
input noun vector is expected to be greater for the incorrect combinations, as the noun meaning is
typically ?distorted?;
3. Cosine to the input adjective (cosA): analogical to cosN measure, the adjective meaning might be
?distorted? as well, especially as two of the composition functions are symmetric;
4. Density of the neighbourhood populated by 10 nearest neighbours (dens) is calculated as the
average distance from the model-generated vector to the 10 nearest neighbours in the original se-
mantic space, and is expected to be higher for the correct ANs;
5. Density among the 10 nearest neighbours (densAll) is a modification of dens, which is estimated
as an average for the 11 density values calculated for each member within the set consisting of the
AN vector and its 10 neighbours;
6. Ranked density in close proximity (Rdens) relies on the notion of close proximity, which is defined
as a neighbourhood populated by some very close neighbours (for example, within a distance of
? 0.8). It is calculated as: RDens =
?
N
i=1
rank
i
distance
i
with N being the total number of
close neighbours within close proximity, each with its rank and distance;
7. Number of neighbours within close proximity (num) is used as another measure, and is assumed
to be lower for incorrect combinations, which are expected to be more isolated in the semantic
space;
8. Overlap between the 10 nearest neighbours and constituent noun/adjective (OverAN) assumes
correct ANs should be surrounded by similar words and combinations. It is calculated as the pro-
portion of the 10 nearest neighbours containing the same constituent words as in the tested ANs;
9. Overlap between the 10 nearest neighbours and input noun (OverN) is a variant of the OverAN
with only the noun considered;
10. Overlap between the 10 nearest neighbours and input adjective (OverA) is a variant of the
OverAN with only the adjective considered;
11. Overlap between the 10 nearest neighbours for the AN and constituent noun/adjective
(NOverAN) assumes that correct ANs and their constituent words should be placed in similar neigh-
bourhoods. It is calculated as the proportion of the common neighbours among the 10 nearest
neighbours for the model-generated AN and the constituent words;
1745
Metric add mult alm
VLen 0.7589 0.7690 0.1676
cosN 0.1621 0.0248 0.0227
cosA 0.0029 0.4782 0.0921
dens 0.6731 0.1182 0.1024
densAll 0.4967 0.1026 0.1176
RDens 0.2786 0.8754 0.1970
num 0.3132 0.4673 0.3765
OverAN 0.8529 0.1622 0.2808
OverA 0.0151 0.6377 0.4886
OverN 0.0138 0.0764 0.4118
NOverAN 0.3941 0.6730 0.0858
NOverA 0.0009 0.3342 0.1575
NOverN 0.0018 0.1463 0.1497
Table 2: p values, out-of-context annotation
Metric add mult alm
VLen 0.6675 0.0027 0.0111
cosN 0.0417 0.0070 0.1845
cosA 0.00003 0.1791 0.1442
dens 0.4756 0.7120 0.1278
densAll 0.2262 0.7139 0.5310
RDens 0.8934 0.8664 0.1985
num 0.7077 0.7415 0.4259
OverAN 0.1962 0.8635 0.5669
OverA 0.00007 0.7271 0.6229
OverN 0.0017 0.9680 0.7733
NOverAN 0.0227 0.3473 0.1587
NOverA 0.000004 0.3749 0.1576
NOverN 0.0001 0.6651 0.2610
Table 3: p values, in-context annotation
12. Overlap between the 10 nearest neighbours for the AN and input noun (NOverN) is a variant
of the NOverAN with only the noun considered;
13. Overlap between the 10 nearest neighbours for the AN and input adjective (NOverA) is a
variant of the NOverAN with only the adjective considered.
4.3 Results
We evaluate the models and report the results following the procedure that has been used before in Vecchi
et al. (2011) and Kochmar and Briscoe (2013). For each model and semantic measure, we report the p
value denoting statistical significance of the difference between the groups of correct and incorrect ANs.
The statistical significance is reported at the p<0.05 level, and if a measure applied to the two groups of
ANs shows statistically significant difference we interpret that as an ability of this measure to distinguish
the correct ANs from the incorrect ones in general. The results for the out-of-context annotation are
reported in Table 2, and those for the in-context annotation in Table 3.
The results show that the difference between the vector representations for the correct and incorrect AN
combinations can be reliably detected with a number of the proposed measures. Measures which show
statistically significant results with at least one model are marked in bold. These results also suggest that
the values for the semantic measures can be used to derive discriminative features for a classifier.
5 Error Detection as Classification Task
5.1 Baseline System
We implement a simple comparison-based baseline system inspired by previous work on error detection
in content words (see section 2.1). For every AN, we create a set of possible alternatives crossing the
confusion set for the adjective with that for the noun, and compare the collocational strength of the
original combination with that for each of the alternatives. If an alternative has higher collocational
strength than the original combination, the original combination is tagged as an error and the alternative
is chosen as a correction. Since semantically related confusions are a rich source of learner errors in
content word combinations, we include adjective synonyms in the confusion set for an adjective, and
noun synonyms and hyponyms in the confusion set for a noun. All synonyms and hyponyms are retrieved
using WordNet 3.0 without word sense disambiguation.
We measure collocational strength using normalized pointwise mutual information (npmi) of the ad-
jective a and noun n, which is defined as:
1746
npmi(a, n) =
pmi(a, n)
?log[p(a, n)]
(1) pmi(a, n) = log
p(a, n)
p(a)p(n)
(2)
All probabilities are estimated from the BNC. This approach performs poorly on the unseen ANs in
our dataset, since any alternative AN seen in the BNC would be preferred by this system over the original
unseen AN. This ensures that less fluent (in this case, unseen) word combinations are substituted with
more fluent (seen) ones. As a result, even though an original AN important conversation in our dataset
is correct, it is still ?corrected? by this system to serious conversation. At the same time, some incorrect
combinations are not recognised if no appropriate alternative is found (e.g., *high shyness). It shows that
this approach lacks deeper semantic analysis and is also too dependent on the set of alternatives found
for a word combination.
We measure accuracy (acc) as the proportion of true positives (TP) and true negatives (TN) to the total
number of test items:
Acc =
TP + TN
TP + FP + TN + FN
(3)
Accuracy reflects how often an error detection system correctly identifies that an AN is correct or
incorrect. We compare the results to the lower and upper bounds set as the majority class distribution
and inter-annotator agreement, respectively (see section 3).
With this approach we get quite low accuracy of 0.3897 on the out-of-context annotation since most
of the test items are correct out of context (LB=0.7932), and the baseline system overcorrects many of
those. Accuracy of the baseline system on the in-context annotation is 0.5147, which is slightly above
the lower bound of 0.5063. These results are used as a baseline and included in Table 4.
Type Accuracy Baseline LB UB
OOC 0.8113 ? 0.0149 0.3897 0.7932 0.8650
IC 0.6535 ? 0.0189 0.5147 0.5063 0.7467
Table 4: Decision Tree classification results
Type P (correct) P (incorrect)
OOC 0.8193 0.7500
IC 0.6241 0.6850
Table 5: Classification precision
5.2 Classification
We implement a supervised classifier which uses output of the semantic models as features. We have
tested a number of classifier models but the best results so far have been obtained with the Decision
Tree classifier using NLTK (Bird et al., 2009). We assume that this classifier effectively learns the
inter-dependencies between the features within the small feature set that we use in our experiments. We
use feature binning where the whole range of feature values is divided into 10 bins according to the
distribution of values for each feature. This feature representation technique combined with the classifier
helps generalise over feature values, reducing feature space dimensionality. The order of the feature
application to the data is determined by the classifier on the basis of the information gain for the features
and their values.
We apply 5-fold cross-validation and report average accuracy over the folds. The 798 ANs are split
into 5 subsets with 80% in each of the splits used for training and 20% for testing. We keep the AN error
rate in the training and test sets, as well as for each adjective, approximately the same across the splits to
avoid any bias. Error detection is cast as a binary classification task. The output of the semantic models
is used to derive numeric features for the classifier. Most values are in the range of [0, 1], and we apply
normalisation to VLen, RDens and num which originally have a different range.
The full feature set contains 14 features, with 13 features derived from the semantic measures, and
1 feature representing adjective identity. We hypothesise that introduction of this feature might help
classifier learn that, for example, an AN containing an adjective classic has a higher chance of being
incorrect, as most of the ANs with this adjective in the learner data are incorrect and involve confusions
with classical. We also hypothesise that it facilitates learning correlations between the adjective and other
1747
feature values: it might be the case that ANs with an adjective adj
1
, on the average, have higher cosN
values than ANs with an adjective adj
2
. This feature helps the classifier establish such dependencies
between the adjective and the values of the semantic measures. For instance, in our data ANs with
the adjective true have significantly higher cosine between AN vectors and vectors for their constituent
nouns than ANs with the adjective false: this is in accordance with an intuition that, for example, true
happiness is more similar to happiness than false happiness is.
The best results in our experiments have been obtained with the mult model. We have performed
ablation tests incrementally removing features that did not improve classifier performance in order to
find an optimal feature set. The best-performing feature set we found for the mult model on the out-
of-context annotation uses adjective, cosN and RDens features, while for the in-context annotation the
best-performing feature set found uses a combination of features including adjective, VLen, densAll,
NOverA, NOverN, RDens and num features.
We note that the sets of best performing features in the classification experiments do not coincide with
the semantic measures that showed the highest statistically significant difference (Tables 2 and 3). We
conclude that although the p values reported in Tables 2 and 3 show that some semantic measures can
distinguish one group of ANs from another on the basis of the statistically significant difference between
the means of the two groups, when the measures are used as features for a classifier the results depend
on how these features interact with each other as well as on their individual discriminativeness across the
test dataset. For example, Figure 1 illustrates a small part of the decision tree constructed using the best
performing feature set on the in-context annotation:
Figure 1: Decision Tree classifier pseudocode.
Figure 1 shows how interaction of feature values for num and VLen in combination with the adjective
identity feature can help classify the two ANs containing adjective large as correct (1) or incorrect (-1).
In Table 4 we report results for the out-of-context (OOC) and in-context (IC) annotation. The accuracy
is reported with its mean ? standard deviation over the 5 data splits. We compare the Decision Tree
classifier results to those obtained with the baseline system, as well as to the lower and upper bounds set
as before (see section 3). The results show that a classifier that uses output of the semantic models as
features outperforms the comparison-based baseline system by a large margin.
6 Discussion
In the previous section, we showed that a classifier that uses output of the semantic models as features
outperforms the comparison-based baseline system and shows good accuracy. In this section, we analyse
the classifier?s performance in more detail.
We note that, from an educational point of view, it is important for an EDC system to have high
precision. For example, it has been shown that grammatical error detection systems with high preci-
sion maximize learning effect, and that systems with high precision but lower recall are more useful
in language learning than systems with high recall and lower precision (Nagata and Nakatani, 2010).
This suggests that learners might be misled and confused if they are frequently notified by a system that
something is an error when it is not.
Since precision is measured as the proportion of true positives (TP) to the sum of true positives and
false positives (FP):
1748
P =
TP
TP + FP
(4)
an EDC system that achieves precision less than 0.5 is, in fact, misleading for language learners: for
example, precision of less than 0.5 on the class of errors means that the system misidentifies correct use
as an error more frequently than it correctly detects an error.
Our classifier achieves good precision values with respect to both out-of-context and in-context anno-
tations, on correct and incorrect examples. Precision (P ) values are reported in Table 5. As precision
figures are higher than 0.5 in each case, it shows that the implemented error detection system would, on
balance, help guide a learner to text regions in need of reformulation.
With respect to the out-of-context annotation, the error detection system has good precision and recall
on correct examples (P = 0.8193, R = 0.9762). Precision on the incorrect examples is also high
(P = 0.7500). This is a very encouraging result, suggesting the system would rarely misidentify an
originally correct AN combination as an error.
For the in-context annotation, both precision and recall on correct and incorrect examples are quite
high: P = 0.6241 and R = 0.7169 on the correct examples, and P = 0.6850 and R = 0.5849 on the
incorrect examples.
Error analysis on the classifier?s output shows that the majority of the incorrect examples misclassified
as correct (missed errors) contain semantically-related confusions. It appears that the classifier relying
on semantically-motivated features misses a number of cases where the original AN and its correction
are semantically similar: for example, it misses the errors in big*/great anger, biggest*/greatest painter
and small*/short speech. Since the ANs in these pairs are semantically similar, the features based on
their semantic representations might not be discriminative enough. In contrast, the classifier is more
effective in detecting errors in cases where the original AN and its correction are only similar in form, or
not related to each other.
7 Conclusion
We have presented and released a dataset of learner errors in ANs, which has been extracted from learner
texts and annotated with error types and corrections. The dataset contains examples not seen in a native
corpus of English, and error annotation shows that a substantial number of such examples are correct.
Error detection in this dataset is a challenging task, since absence of the ANs in a corpus of English
cannot be used as definitive evidence of incorrectness. We have implemented a simple baseline system
inspired by previous work on improving content word combinations and shown that such a system would
not be effective for error detection in our dataset.
We have cast error detection as a binary classification task and implemented a supervised classifier
that uses semantically-motivated features. The features are derived from the compositional distributional
semantic representations of the AN combinations. We use a number of semantic measures that describe
and distinguish between semantic representations for correct and incorrect combinations. We have intro-
duced new semantic measures in addition to the ones used in previous work and show that they can be
effectively applied to this task.
The best results in our experiments are obtained with a Decision Tree classifier, and we show that the
resulting error detection system can identify errors with high precision and accuracy. We aim to extend
this system to perform error correction on ANs, as well as error detection and correction on other types
of content word combinations.
Acknowledgments
We are grateful to Cambridge English Language Assessment and Cambridge University Press for sup-
porting this research and for granting us access to the CLC for research purposes. We would like to
thank ?istein Andersen for providing us with the annotation tool, Diane Nicholls for undertaking the
bulk of the annotation work, and Helen Yannakoudakis and the anonymous reviewers for their valuable
comments.
1749
References
?istein Andersen, Julien Nioche, Ted Briscoe and John Carroll 2008. The BNC parsed with RASP4UIMA. In
Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC), pp. 865?869.
Marco Baroni and Roberto Zamparelli 2010. Nouns are vectors, adjectives are matrices: Representing adjective-
noun constructions in semantic space. In Proceedings of the EMNLP-2010, pp. 1183?1193.
Steven Bird, Ewan Klein, and Edward Loper 2009. Natural Language Processing with Python ? Analyzing Text
with the Natural Language Toolkit. O?Reilly Media.
Ted Briscoe, John Carroll and Rebecca Watson 2006. The Second Release of the RASP System. In Proceedings of
the COLING/ACL-2006 Interactive Presentation Sessions, pp. 59?68.
Yu-Chia Chang, Jason S. Chang, Hao-Jan Chen and Hsien-Chin Liou 2008. An automatic collocation writing
assistant for Taiwanese EFL learners: A case of corpus-based NLP technology. Computer Assisted Language
Learning, 21(3), pp. 283?299.
Jacob Cohen 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement,
20(1), pp. 37?46.
Robert Dale, Ilya Anisimoff and George Narroway 2012. HOO 2012: A Report on the Preposition and Determiner
Error Correction Shared Task. In Proceedings of the 7th Workshop on Innovative Use of NLP for Building
Educational Applications, pp. 54?62.
Daniel Dahlmeier and Hwee Tou Ng 2011. Correcting Semantic Collocation Errors with L1-induced Paraphrases.
In Proceedings of the EMNLP-2011, pp. 107?117.
Gerard M. Dalgish 1985. Computer-assisted ESL research. In CALICO Journal, 2(2), pp. 32?37.
Stefan Evert 2005. The Statistics of Word Cooccurrences. Dissertation, Stuttgart University.
Yoko Futagi, Paul Deane, Martin Chodorow and Joel Tetreault 2009. A computational approach to detecting
collocation errors in the writing of non-native speakers of English. Computer Assisted Language Learning,
21(4), pp. 353?367.
Ekaterina Kochmar and Ted Briscoe 2013. Capturing Anomalies in the Choice of Content Words in Composi-
tional Distributional Semantic Space. In Proceedings of the Recent Advances in Natural Language Processing
(RANLP-2013).
Angeliki Lazaridou, Eva Maria Vecchi and Marco Baroni 2013. Fish transporters and miracle homes: How com-
positional distributional semantics can help NP parsing. In Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pp. 1908?1913.
Claudia Leacock, Martin Chodorow, Michael Gamon and Joel Tetreault 2010. Automated Grammatical Error
Detection for Language Learners. Morgan and Claypool Publishers.
Anne Li-E Liu, David Wible and Nai-Lung Tsao 2009. Automated suggestions for miscollocations. In Proceed-
ings of the 4th Workshop on Innovative Use of NLP for Building Educational Applications, pp. 47?50.
Bj?rn-Helge Mevik and Ron Wehrens 2007. The pls package: Principal component and partial least squares
regression in R. Journal of Statistical Software, 18(2), pp. 1?24.
Jeff Mitchell and Mirella Lapata 2008. Vector-based models of semantic composition. In Proceedings of ACL,
pp. 236?244.
Jeff Mitchell and Mirella Lapata 2010. Composition in distributional models of semantics. Cognitive Science, 34,
pp. 1388?1429.
Ryo Nagata and Kazuhide Nakatani 2010. Evaluating performance of grammatical error detection to maximize
learning effect. In Proceedings of COLING 2010, pp. 894?900.
Diane Nicholls 2003. The Cambridge Learner Corpus: Error coding and analysis for lexicography and ELT. In
Proceedings of the Corpus Linguistics conference, pp. 572?581.
Robert
?
Ostling and Ola Knutsson 2009. A corpus-based tool for helping writers with Swedish collocations. In
Proceedings of the Workshop on Extracting and Using Constructions in NLP, pp. 28?33.
1750
Taehyun Park, Edward Lank, Pascal Poupart, Michael Terry 2008. Is the sky pure today? AwkChecker: an assistive
tool for detecting and correcting collocation errors. In Proceedings of the 21st annual ACM symposium on User
interface software and technology, pp. 121?130.
Alla Rozovskaya and Dan Roth 2011. Algorithm Selection and Model Adaptation for ESL Correction Tasks.
InProceeding of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies ? Volume 1, pp. 924?933.
Chi-Chiang Shei and Helen Pain 2000. An ESL Writer?s Collocation Aid. Computer Assisted Language Learning,
13(2), pp. 167?182.
Eva Maria Vecchi, Marco Baroni and Roberto Zamparelli 2011. (Linear) maps of the impossible: Capturing
semantic anomalies in distributional space. In Proceedings of the DISCO Workshop at ACL-2011, pp. 1?9.
David Wible, Chin-Hwa Kuo, Nai-Lung Tsao, Anne Liu and H.-L. Lin 2003. Bootstrapping in a language-
learning environment. Journal of Computer Assisted Learning, 19(4), pp. 90?102.
Helen Yannakoudakis, Ted Briscoe and Ben Medlock 2011. A New Dataset and Method for Automatically Grad-
ing ESOL Texts. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies, 1, pp. 180?189.
Xing Yi, Jianfeng Gao and William B. Dolan 2008. A Web-based English Proofing System for English as a Second
Language Users. In Proceedings of the third International Joint Conference on Natural Language Processing
(IJCNLP), pp. 619?624.
1751
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 242?250,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
HOO 2012 Error Recognition and Correction Shared Task:
Cambridge University Submission Report
Ekaterina Kochmar
Computer Laboratory
University of Cambridge
ek358@cl.cam.ac.uk
?istein Andersen
iLexIR Ltd
Cambridge
and@ilexir.co.uk
Ted Briscoe
Computer Laboratory
University of Cambridge
ejb@cl.cam.ac.uk
Abstract
Previous work on automated error recognition
and correction of texts written by learners of
English as a Second Language has demon-
strated experimentally that training classifiers
on error-annotated ESL text generally outper-
forms training on native text alone and that
adaptation of error correction models to the
native language (L1) of the writer improves
performance. Nevertheless, most extant mod-
els have poor precision, particularly when at-
tempting error correction, and this limits their
usefulness in practical applications requiring
feedback.
We experiment with various feature types,
varying quantities of error-corrected data, and
generic versus L1-specific adaptation to typi-
cal errors using Na??ve Bayes (NB) classifiers
and develop one model which maximizes pre-
cision. We report and discuss the results for
8 models, 5 trained on the HOO data and
3 (partly) on the full error-coded Cambridge
Learner Corpus, from which the HOO data is
drawn.
1 Introduction
The task of detecting and correcting writing errors
made by learners of English as a Second Language
(ESL) has recently become a focus of research.
The majority of previous papers in this area
have presented machine learning methods with mod-
els being trained on well-formed native English
text (Eeg-Olofsson and Knutsson, 2003; De Felice
and Pulman, 2008; Gamon et al, 2008; Han et al,
2006; Izumi et al, 2003; Tetreault and Chodorow,
2008; Tetreault et al, 2010). However, some recent
approaches have explored ways of using annotated
non-native text either by incorporating error-tagged
data into the training process (Gamon, 2010; Han
et al, 2010), or by using native language-specific
error statistics (Rozovskaya and Roth, 2010b; Ro-
zovskaya and Roth, 2010c; Rozovskaya and Roth,
2011). Both approaches show improvements over
the models trained solely on well-formed native text.
Training a model on error-tagged non-native
text is expensive, as it requires large amounts of
manually-annotated data, not currently publically
available. In contrast, using native language-specific
error statistics to adapt a model to a writer?s first or
native language (L1) is less restricted by the amount
of training data.
Rozovskaya and Roth (2010b; 2010c) show that
adapting error corrections to the writer?s L1 and in-
corporating artificial errors, in a way that mimics
the typical error rates and confusion patterns of non-
native text, improves both precision and recall com-
pared to classifiers trained on native data only. The
approach proposed in Rozovskaya and Roth (2011)
uses L1-specific error correction patterns as a dis-
tribution on priors over the corrections, incorporat-
ing the appropriate priors into a generic Na??ve Bayes
(NB) model. This approach is both cheaper to im-
plement, since it does not require a separate classi-
fier to be trained for every L1, and more effective,
since the priors condition on the writer?s L1 as well
as on the possible confusion sets.
Some extant approaches have achieved good re-
sults on error detection. However, error correction
is much harder and on this task precision remains
242
low. This is a disadvantage for applications such
as self-tutoring or writing assistance, which require
feedback to the user. A high proportion of error-
ful suggestions is likely to further confuse learners
and/or non-native writers rather than improve their
writing or assist learning. Instead a system which
maximizes precision over recall returning accurate
suggestions for a small proportion of errors is likely
to be more helpful (Nagata and Nakatani, 2010).
In section 2 we describe the data used for train-
ing and testing the systems we developed. In sec-
tion 3 we describe the preprocessing of the ESL text
undertaken to provide a source of features for the
classifiers. We also discuss the feature types that
we exploit in our classifiers. In section 4 we de-
scribe and report results for a high precision system
which makes no attempt to generalize from train-
ing data. In section 5 we describe our approach to
adapting multiclass NB classifiers to characteristic
errors and L1s. We also report the performance of
some of these NB classifiers on the training and test
data. In section 6 we report the official results of
all our submitted runs on the test data and also on
the HOO training data, cross-validated where appro-
priate. Finally, we briefly discuss our main results,
further work, and lessons learnt.
2 Cambridge Learner Corpus
The Cambridge Learner Corpus1 (CLC) is a large
corpus of learner English. It has been developed
by Cambridge University Press in collaboration with
Cambridge Assessment, and contains examination
scripts written by learners of English from 86 L1
backgrounds. The scripts have been produced by
language learners taking Cambridge Assessment?s
ESL examinations.2
The linguistic errors committed by the learners
have been manually annotated using a taxonomy of
86 error types (Nicholls, 2003). Each error has been
manually identified and tagged with an appropriate
code, specifying the error type, and a suggested cor-
rection. Additionally, the scripts are linked to meta-
data about examination and learner. This includes
the year of examination, the question prompts, the
1http://www.cup.cam.ac.uk/gb/elt/catalogue/subject/
custom/item3646603/Cambridge-International-Corpus-
Cambridge-Learner-Corpus
2http://www.cambridgeesol.org/
learner?s L1, as well as the grades obtained. The cur-
rent version of the CLC contains about 20M words
of error-annotated scripts from a wide variety of ex-
aminations.
The HOO training and test datasets are drawn
from the CLC. The training dataset is a reformatted
1000-script subset of a publically-available subset of
CLC scripts produced by learners sitting the First
Certficate in English (FCE) examination.3 This ex-
amination assesses English at an upper-intermediate
level, so many learners sitting this exam still man-
ifest a number of errors motivated by the conven-
tions of their L1s. The CLC-FCE subcorpus was ex-
tracted, anonymized, and made available as a set of
XML files by Yannakoudakis et al (2011).4
The HOO training dataset contains scripts from
FCE examinations undertaken in the years 2000 and
2001 written by speakers of 16 L1s. These scripts
can be divided into two broad L1 typological groups,
Asian (Chinese, Thai, Korean, Japanese) and Euro-
pean (French, Spanish, Italian, Portuguese, Catalan,
Greek, Russian, Polish). The latter can be further
subdivided into Slavic (Russian, Polish) and Ro-
mance. In turn, the Romance languages differ in ty-
pological relatedness with, for example, Portuguese
and Spanish being closer than Spanish and French.
Error coding which is not relevant to preposition or
determiner errors has been removed from the train-
ing data so that only six error type annotations are
retained for training: incorrect, missing or unnec-
essary determiners (RD, MD, UD) and prepositions
(RT, MT, UT).
One consequence of this reformatting is that the
contexts of these errors often contain further errors
of different types that are no longer coded. The idea
is that errors should be considered in their natural
habitat, and that correcting and copy-editing the sur-
rounding text would create an artificial task. On the
other hand, not correcting anything makes it difficult
in some cases and nigh impossible in others to de-
termine whether a given determiner or preposition is
correct or not. The error-coding in the CLC in such
cases (provided the writer?s intent is deemed recov-
erable) depends not only on the original text, but also
on the correction of nearby errors.
3http://www.cambridgeesol.org/exams/general-english/fce.
html
4http://ilexir.co.uk/applications/clc- fce-dataset/
243
Certain errors even appear as a direct result of
correcting others: for instance, the phrase to sleep
in tents has been corrected to to sleep in a tent in
the CLC; this ends up as a ?correction? to to sleep
in a tents in the HOO dataset. This issue is diffi-
cult to avoid given that the potential solutions are all
labour-intensive (explicit indication of dependencies
between error annotations, completely separate error
annotation for different types of errors, or manual re-
moval of spurious errors after extraction of the types
of error under consideration), and we mention it here
mainly to explain the origin of some surprising an-
notations in the dataset.
A more HOO-specific problem is the ?[removal
of] elements [from] some of [the] files [...] to
dispose of nested edits and other phenomena that
caused difficulties in the preprocessing of the data?
(Dale et al, 2012). This approach unfortunately
leads to mutilated sentences such as I think if we
wear thistoevery wherespace ships. This mean. re-
placing the original I think if we wear this clothes we
will travel to every where easier than we use cars,
ships, planes and space ships. This mean the engi-
neering will find the way to useless petrol for it, so it
must useful in the future.
The HOO test set consists of 100 responses to
individual prompts from FCE examinations set be-
tween 1993 and 2009, also drawn from the CLC.
As a side effect of removing the test data from the
full CLC, we have discovered that the distribution of
L1s, examination years and exam prompts is differ-
ent from the training data. There are 27 L1s exem-
plified, a superset of the 16 seen in the HOO train-
ing data; about half are Romance, and the rest are
widely distributed with Asian and Slavic languages
less well represented than in the training data.
In the experiments reported below, we make use
of both the HOO training data and the full 20M
words of error-annotated CLC, but with the HOO
test data removed, to train our systems. Whenever
we use the larger training set we refer to this as the
full CLC below.
3 Data Preprocessing
We parsed the training and test data (see Section
2) using the Robust Accurate Statistical Parsing
(RASP) system with the standard tokenization and
My friend was (MD: a) good student
Grammatical Relations (GRs):
(ncsubj be+ed:3 VBDZ friend:2 NN1 )
(xcomp be+ed:3 VBDZ student:6 NN1)
(ncmod student:6 NN1 good:5 JJ)
(det friend:2 NN1 My:1 APP$)
*(det student:6 NN1 a:4 AT1)
Figure 1: RASP GR output
sentence boundary detection modules and the unlex-
icalized version of the parser (Briscoe et al, 2006)
in order to broaden the space of candidate fea-
tures types. The features used in our experiments
are mainly motivated by the fact that lexical and
grammatical features have been shown in previous
work to be effective for error detection and correc-
tion. We believe RASP is an appropriate tool to
use with ESL text because the PoS tagger deploys
a well-developed unknown word handling mecha-
nism, which makes it relatively robust to noisy in-
put such as misspellings, and because the parser de-
ploys a hand-coded grammar which indicates un-
grammaticality of sentences and markedness of con-
structions and is encoded entirely in terms of PoS
tag sequences. We utilize the open-source version
of RASP embedded in an XML-handling pipeline
that allows XML-encoded metadata in the CLC and
HOO training data to be preserved in the output,
but ensures that unannotated text is passed to RASP
(Andersen et al, 2008).
Relevant output of the system is shown in Fig-
ure 1 for a typical errorful example. The grammati-
cal relations (GRs) form a connected, directed graph
of typed bilexical head-dependent relations (where a
non-fragmentary analysis is found). Nodes are lem-
matized word tokens with associated PoS tag and
sentence position number. Directed arcs are labelled
with GR types. In the factored representation shown
here, each line represents a GR type, the head node,
the dependent node, and optional subtype informa-
tion either after the GR type or after the dependent.
In this example, the asterisked GR would be missing
in the errorful version of the sentence. We extract the
most likely analysis for each sentence based on the
most probable tag sequence found by the tagger.
Extraction of the lexical and grammatical infor-
244
mation from the parser output is easier when a deter-
miner or preposition is present than when it is miss-
ing. During training, for all nouns, we checked for a
det relation to a determiner, and whenever no det
GR is present, we checked whether the noun is pre-
ceded by an MD annotation in the XML file. For
missing prepositions, we have only extracted cases
where a noun is governed by a verb with a dobj
relation, and cases where a noun is governed by an-
other noun with an ncmod (non-clausal modifier)
relation. For example, in It?s been a long time since
I last wrote you, in absence of the preposition to the
parser would ?recognize? a dobj relation between
you and wrote, and this case would be used as a
training example for a missing preposition, while I
trusted him with the same dobj relation between
trusted and him would be used as a training exam-
ple to correct unwanted use of a preposition as in I
trusted *to him.
3.1 Feature Types
In all the experiments and system configurations
described below, we used a similar set of features
based on the following feature templates.
For determiner errors:
? Noun lemma: lemma of the noun that gov-
erns the determiner
? Noun PoS: PoS tag of the noun
? Distance from Noun: distance in num-
ber of words to the governed determiner
? Head lemma: head lemma in the shortest
grammatical relation in which the noun is de-
pendent
? Head PoS: as defined above, but with PoS tag
rather than lemma
? Distance from Head: distance in num-
ber of words to the determiner from head, as
defined above (for Head lemma)
? GR type to Noun: a GR between Head
and Noun.
For instance for the example shown in Figure 1, the
noun lemma is student, the noun PoS is NN1, the
distance from the noun is 2, the head lemma is be,
the head PoS is VBDZ, and the distance from the
head is 1, while the GR type to the noun is xcomp.
For preposition errors:
? Preposition (P): target preposition
? Head lemma (H): head lemma of the GR in
which the preposition is dependent
? Dependent lemma (D): dependent
lemma of the GR in which the preposition is
head.
For instance, in I am looking forward to your reply,
P is to, H is look and D is reply.
In contrast to work by Rozovskaya and Roth,
amongst others, we have not used word context fea-
tures, but instead focused on grammatical context in-
formation for detecting and correcting errors. We
also experimented with some other feature types,
such as n-grams consisting of the head, preposition
and dependent lemmas, but these did not improve
performance on the cross-validated HOO training
data, perhaps because they are sparser and the train-
ing set is small. However, there are many other po-
tential feature types, such as PoS n-grams or syn-
tactic rule types, and so forth that we don?t explore
here, despite their probable utility. Our main focus
in these experiments is not on optimal feature engi-
neering but rather on the issues of classifier adaption
to errors and high precision error correction.
4 A Simple High Precision Correction
System
We have experimented with a number of approaches
to maximizing precision and have not outperformed
a simple model that doesn?t generalize from the
training data using machine learning techniques. We
leverage the large amount of error-corrected text in
the full CLC to learn reliable contexts in which er-
rors occur and their associated corrections. For the
HOO shared task, we tested variants of this approach
for missing determiner (MD) and incorrect prepo-
sition (RT) errors. Better performing features and
thresholds used to define contexts were found by
testing variants on the HOO training data. The fea-
ture types from section 3.1 deployed for the MD
system submitted for the official run were Noun
245
lemma, Noun PoS, GR types to Noun and
GR types from Noun (set of GRs which has
the noun as head). For the RT system, all three P, H,
and D features were used to define contexts. A con-
text is considered reliable if it occurs at least twice
in the full CLC and more than 75% of the time it
occurs with an error.
The performance of this system on the training
data was very similar to performance on the test data
(in contrast to our other runs). We also explored L1-
specific and L1-group variants of these systems; for
instance, we split the CLC data into Asian and Eu-
ropean languages, trained separate systems on each,
and then applied them according to the L1 meta-
data supplied with the HOO training data. However,
all these systems performed worse than the best un-
adapted system.
The results for the generic, unadapted MD and RT
systems appear as run 0 in Tables 4?9 below. These
figures are artefactually low as we don?t attempt to
detect or correct UD, UT, RD or MT errors. The
actual results computed from the official runs solely
for MD errors are for detection, recognition and cor-
rection: 83.33 precision and 7.63 recall, which gives
an F-measure of 13.99; the RT system performed at
66.67 precision, 8.05 recall and 14.37 F-measure on
the detection, recognition and correction tasks. De-
spite the low recall, this was our best submitted sys-
tem in terms of official correction F-score.
5 Na??ve Bayes (NB) (Un)Adapted
Multiclass Classifiers
Rozovskaya and Roth (2011) demonstrate on a
different dataset that Na??ve Bayes (NB) can out-
perform discriminative classifiers on preposition
error detection and correction if the prior is adapted
to L1-specific estimates of error-correction pairs.
They compare the performance of an unadapted
NB multiclass classifier, in which the prior for a
preposition is defined as the relative probability
of seeing a specific preposition compared to a
predefined subset of the overall PoS class (which
they call the Conf(usion) Set):
prior(p) =
C(p)
?
q?ConfSet C(q)
,
to the performance of the same NB classfier with
an adapted prior which calculates the probability of
a correct preposition as:
prior(c, p,L1) =
CL1(p, c)
CL1(p)
,
where CL1(p) is the number of times preposition
p is seen in texts written by learners with L1 as
their native language, and CL1(p, c) is the number
of times c is the correct preposition when p is used.
We applied Rozovskaya and Roth?s approach to
determiners as well as prepositions, and experi-
mented with priors calculated in the same way for
L1 groups as well as specific L1s. We also com-
pared L1-adaptation to generic adaption to correc-
tions, calculated as:
prior(c, p) =
C(p, c)
C(p)
,
We have limited the set of determiners and prepo-
sitions that our classifiers aim to detect and correct,
if necessary. Our confusions sets contain:
? Determiners: no determiner, the, a, an;
? Prepositions: no preposition, in, of, for,
to, at, with, on, about, from, by, after.
Therefore, for determiners, our systems were only
aimed at detecting and correcting errors in the use of
articles, and we have not taken into account any er-
rors in the use of possessive pronouns (my, our, etc.),
demonstratives (this, those, etc.), and other types of
determiners (any, some, etc.). For prepositions, it is
well known that a set of about 10 of the most fre-
quent prepositions account for more than 80% of all
prepositional usage (Gamon, 2010).
We have calculated the upper bounds for the train-
ing and test sets when the determiner and preposi-
tion confusion sets are limited this way. The upper
bound recall for recognition (i.e., ability of the clas-
sifier to recognize that there is an error, dependent on
the fact that only the chosen determiners and prepo-
sitions are considered) is calculated as the propor-
tion of cases where the incorrect, missing or unnec-
essary determiner or preposition is contained in our
confusion set. For the training set, it is estimated at
91.95, and for the test at 93.20. Since for correction,
246
the determiner or preposition suggested by the sys-
tem should also be contained in our confusion set,
upper bound recall for correction is slightly lower
than that for recognition, and is estimated at 86.24
for the training set, and at 86.39 for the test set.
These figures show that the chosen candidates dis-
tribute similarly in both datasets, and that a system
aimed at recognition and correction of only these
function words can obtain good performance on the
full task.
The 1000 training scripts were divided into 5 por-
tions pseudo-randomly to ensure that each portion
contained approximately the same number of L1-
specific scripts in order not to introduce any L1-
related bias. The results on the training set pre-
sented below were averaged across 5 runs, where in
each run 4 portions (about 800 scripts) were used
for training, and one portion (about 200 scripts) was
used for testing.
We treated the task as multi-class classification,
where the number of classes equates to the size of
our confusion set, and when the classifier?s decision
is different from the input, it is considered to be er-
rorful. For determiners, we used the full set of fea-
tures described in section 3.1, whereas for preposi-
tions, we have tried two different feature sets: only
head lemma (H), or H with the dependent lemma (D).
We ran the unadapted and L1-adapted NB classi-
fiers on determiners and prepositions using the fea-
tures defined above. The results of these preliminary
experiments are presented below.
5.1 Unadapted and L1-adapted NB classifiers
Tables 1 to 3 below present results averaged over
the 5 runs for the unadapted classifiers. We report
the results in terms of recall, precision and F-score
for detection, recognition and correction of errors as
defined for the HOO shared task.5
We have experimented with two types of L1-
specific classification: classifier1 below is a
combination of 16 separate multiclass NB classi-
fiers, each trained on a specific L1 and applied to
the corresponding parts of the data. Classifier2
is a replication of the classifier presented in Ro-
zovskaya and Roth (2011), which uses the priors
5For precise definitions of these measures see
www.correcttext.org/hoo2012
adapted to the writer?s L1 and to the chosen deter-
miner or preposition at decision time. The priors
used for these runs were estimated from the HOO
training data.
We present only the results of the systems that use
H+D features for prepositions, since these systems
outperform systems using H only. Tables 1, 2 and
3 below show the comparative results of the three
classifiers averaged over 5 runs, with all errors, de-
terminer errors only, and preposition errors only, re-
spectively.
Detection Recognition Correction
R P F R P F R P F
U 60.69 21.32 31.55 50.57 17.73 26.25 34.38 12.05 17.85
C1 64.51 16.17 25.85 50.25 12.56 20.10 30.95 7.74 12.39
C2 33.74 16.51 22.15 28.50 13.96 18.72 16.51 8.10 10.85
Table 1: All errors included. Unadapted classifier (U) vs.
two L1-adapted classifiers (C1 and C2). Results on the
training set.
Detection Recognition Correction
R P F R P F R P F
U 54.42 33.25 41.25 50.09 30.60 30.83 40.70 24.84 30.83
C1 61.19 20.25 30.42 52.20 17.27 25.94 40.57 13.43 20.17
C2 40.56 15.88 22.81 37.24 14.58 20.94 23.20 9.08 13.04
Table 2: Determiner errors. Unadapted classifier (U) vs.
two L1-adapted classifiers (C1 and C2). Results on the
training set.
Detection Recognition Correction
R P F R P F R P F
U 65.71 16.89 26.87 50.90 13.09 20.83 28.95 7.45 11.84
C1 66.96 13.86 22.97 48.51 10.05 16.65 22.70 4.70 7.79
C2 27.45 17.06 21.00 21.00 13.07 16.09 10.79 6.73 8.27
Table 3: Preposition errors. Unadapted classifier (U) vs.
two L1-adapted classifiers (C1 and C2). Results on the
training set.
The results show some improvement with a com-
bination of classifiers trained on L1-subsets in terms
of recall for detection and recognition of errors, and
a slight improvement in precision using L1-specific
priors for preposition errors. However, in general,
unadapted classifiers outperform L1-adapted classi-
fiers with identical feature types. Therefore, we have
not included L1-specific classifiers in the submitted
set of runs.
5.2 Submitted systems
For the official runs, we trained various versions of
the unadapted and generic adapted NB classifiers.
247
We trained all the adapted priors on the full CLC
dataset in the expectation that this would yield more
accurate estimates. We trained the unadapted priors
and the NB features as before on the HOO training
dataset. We also trained the NB features on the full
CLC dataset and tested the impact of the preposi-
tion feature D (dependent lemma of the GR from the
preposition, i.e., the head of the preposition comple-
ment) with the different training set sizes. For all
runs we used the full set of determiner features de-
scribed in section 3.1.
The full set of multiclass NB classifiers submitted
is described below:
? Run1: unadapted, trained on the HOO data. H
feature for prepositions;
? Run2: unadapted, trained on the HOO data. H
and D features for prepositions;
? Run3: a combination of the NB classifiers
trained for each of the used candidate words
separately. H and D features are used for prepo-
sitions;
? Run4: generic adapted, trained on HOO data.
H feature for prepositions;
? Run5: generic adapted, trained on HOO data.
H and D features for prepositions;
? Run6: unadapted, trained on the full CLC. H
feature for prepositions;
? Run7: unadapted, trained on the full CLC. H
and D features for prepositions.
The classifiers used for runs 1 and 2 differ from
the ones used for runs 6 and 7 only in the amount
of training data. None of these classifiers involve
any adaptation. The classifiers used for runs 4 and
5 involve prior adaptation to the input determiner
or preposition, adjusted at decision time. In run
3, a combination of classifiers trained on the input
determiner- or preposition-specific partitions of the
HOO training data are used. At test time, the appro-
priate classifier from this set is applied depending on
the preposition or determiner chosen by the learner.
To limit the number of classes for the classifiers
used in runs 1?3 and 6?7, we have combined the
training cases for determiners a and an in one class
a/an; after classification one of the variants is chosen
depending on the first letter of the next word. How-
ever, for the classifiers used in runs 4?5, we used
priors including confusions between a and an.
The results for these runs on the training data are
shown in Tables 4 to 6 below.
Detection Recognition Correction
R P F R P F R P F
0 5.54 81.08 10.37 5.32 77.95 9.97 4.90 71.70 9.17
1 60.14 18.57 28.37 48.21 14.88 22.74 32.71 10.09 15.43
2 60.69 21.32 31.55 50.57 17.73 26.25 34.38 12.05 17.85
3 50.09 27.54 35.52 45.99 25.23 32.57 28.78 15.80 20.39
4 25.39 25.48 25.39 22.10 22.23 22.13 12.23 12.33 12.26
5 31.17 22.33 25.94 26.28 18.88 21.90 14.50 10.46 12.11
6 62.41 10.73 18.31 49.95 8.57 14.63 32.66 5.60 9.57
7 62.92 11.60 19.59 52.29 9.61 16.24 34.32 6.31 10.66
Table 4: Training set results, all errors
Detection Recognition Correction
R P F R P F R P F
0 5.02 82.98 9.46 5.02 82.98 9.46 4.81 79.57 9.07
1?2 54.42 33.25 41.25 50.09 30.60 30.83 40.70 24.84 30.83
3 58.50 62.22 60.22 57.41 61.07 59.11 46.33 49.25 47.68
4?5 34.93 31.09 32.68 33.66 30.01 31.52 19.74 17.66 18.51
6?7 58.65 8.11 14.24 53.90 7.43 13.06 40.61 5.60 9.84
Table 5: Training set results, determiner errors
Detection Recognition Correction
R P F R P F R P F
0 5.87 78.30 10.93 5.59 74.49 10.40 4.97 66.28 9.25
1 64.71 14.04 23.06 46.54 10.11 16.61 25.86 5.61 9.22
2 65.71 16.89 26.87 50.90 13.09 20.83 28.95 7.45 11.84
3 42.63 16.53 23.81 36.18 14.04 20.22 13.74 5.35 7.70
4 16.85 19.27 17.97 12.24 14.03 13.06 5.81 6.67 6.21
5 27.49 16.89 20.88 19.96 12.30 15.19 10.03 6.20 7.65
6 64.69 14.03 23.06 46.51 10.10 16.60 25.83 5.61 9.22
7 65.68 16.89 26.87 50.87 13.09 20.82 28.92 7.44 11.84
Table 6: Training set results, preposition errors
The results on the training data show that use
of the D feature improves the performance of all
the preposition classifiers. Use of the full CLC for
training improves recall, but does not improve pre-
cision for prepositions, while for determiners pre-
cision of the classifiers trained on the full CLC
is much worse. Adaptation of the classifiers with
determiner/preposition-specific priors slightly im-
proves precision on prepositions, but is damaging
for recall. Therefore, in terms of F-score, unadapted
classifiers outperform adapted ones. The over-
all best-performing system on the cross-validated
training data is Run3, which is trained on the
determiner/preposition-specific data subsets and ap-
248
plies an input-specific classifier to test data. How-
ever, the result is due to improved performance on
determiners, not prepositions.
6 Official Evaluation Results
The results presented below are calculated using the
evaluation tool provided by the organizers, imple-
menting the scheme specified in the HOO shared
task. The results on the test set, presented in Ta-
bles 7?9 are from the final official run after correc-
tion of errors in the annotation and score calculation
scripts.
Detection Recognition Correction
R P F R P F R P F
0 4.86 76.67 9.15 4.65 73.33 8.75 4.65 73.33 8.75
1 34.46 13.04 18.92 22.83 8.64 12.54 13.53 5.12 7.43
2 35.73 14.04 20.16 23.47 9.22 13.24 12.26 4.82 6.92
3 19.24 12.10 14.86 14.59 9.18 11.27 5.71 3.59 4.41
4 9.51 14.95 11.63 7.19 11.30 8.79 5.29 8.31 6.46
5 15.43 14.31 14.85 10.78 10.00 10.38 6.77 6.28 6.51
6 55.60 11.15 18.58 41.86 8.40 13.99 28.54 5.73 9.54
7 56.66 11.59 19.24 42.49 8.69 14.43 27.27 5.58 9.26
Table 7: Test set results, all errors
Detection Recognition Correction
R P F R P F R P F
0 4.37 83.33 8.30 4.37 83.33 8.30 4.37 83.33 8.30
1?2 8.73 7.61 8.13 4.80 4.18 4.47 4.37 3.80 4.07
3 6.11 11.29 7.93 5.24 9.68 6.80 5.24 9.68 6.80
4?5 6.11 9.72 7.51 4.80 7.64 5.90 4.80 7.64 5.90
6?7 51.09 8.53 14.63 44.10 7.37 12.63 35.37 5.91 10.13
Table 8: Test set results, determiner errors
Detection Recognition Correction
R P F R P F R P F
0 5.33 72.22 9.92 4.92 66.67 9.16 4.92 66.67 9.16
1 57.79 14.29 22.91 39.75 9.83 15.76 22.13 5.47 8.77
2 59.43 15.41 24.47 40.98 10.63 16.88 19.67 5.10 8.10
3 29.10 11.31 16.28 23.36 9.08 13.07 6.15 2.39 3.44
4 12.71 19.75 15.46 9.43 14.65 11.47 5.74 8.92 6.98
5 24.18 16.12 19.34 16.39 10.93 13.12 8.61 5.74 6.89
6 57.79 14.29 22.91 39.75 9.83 15.76 22.13 5.47 8.77
7 59.43 15.41 24.47 40.98 10.63 16.88 19.67 5.10 8.10
Table 9: Test set results, preposition errors
The test set results for NB classifiers (Runs 1?
7) are significantly worse than our preliminary re-
sults obtained on the training data partitions, espe-
cially for determiners. Use of additional training
data (Runs 6 and 7) improves recall, but does not im-
prove precision. Adaptation to the input preposition
improves precision as compared to the unadapted
classifier for prepositions (Run 4), whereas training
on the determiner-specific subsets improves preci-
sion for determiners (Run 3). However, generally
these results are worse than the results of the similar
classifiers on the training data subsets.
We calculated the upper bound recall for our clas-
sifiers on the test data. The upper bound recall on
the test data is 93.20 for recognition, and 86.39 for
correction, given our confusion sets for both deter-
miners and prepositions. However, the actual upper
bound recall is 71.82, with upper bound recall on
determiners at 71.74 and on prepositions at 71.90,
because 65 out of 230 determiner errors, and 68 out
of 243 preposition errors are not considered by our
classifiers, primarily because when the parser fails to
find a full analysis, the grammatical context is often
not recovered accurately enough to identify missing
input positions or relevant GRs. This is an inher-
ent weakness of using only parser-extracted features
from noisy and often ungrammatical input. Taking
this into account, some models (Runs 1, 2, 6 and 7)
achieved quite high recall.
We suspect the considerable drop in precision is
explained by the differences in the training and test
data. The training set contains answers from learners
of a smaller group of L1s from one examination year
to a much more restricted set of prompts. The well-
known weaknesses of generative NB classifiers may
prevent effective exploitation of the additional infor-
mation in the full CLC over the HOO training data.
Experimentation with count weighting schemes and
optimized interpolation of adapted priors may well
be beneficial (Rennie et al, 2003).
Acknowledgements
We thank Cambridge ESOL, a division of Cam-
bridge Assessment for a partial grant to the first au-
thor and a research contract with iLexIR Ltd. We
also thank them and Cambridge University Press for
granting us access to the CLC for research purposes.
References
?istein Andersen. 2011 Semi-automatic ESOL error
annotation. English Profile Journal, vol2:e1. DOI:
10.1017/S2041536211000018, Cambridge University
Press.
?istein Andersen, Julian Nioche, Ted Briscoe, and
John Carroll. 2008 The BNC parsed with
249
RASP4UIMA. In 6th Int. Conf. on Language Re-
sources and Evaluation (LREC), Marrakech, Moroc-
cco
Ted Briscoe, John A. Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of COLING/ACL, vol 6.
Robert Dale, Ilya Anisimoff and George Narroway 2012
HOO 2012: A Report on the Preposition and Deter-
miner Error Correction Shared Task. In Proceedings
of the Seventh Workshop on Innovative Use of NLP for
Building Educational Applications Montreal, Canada,
June.
Rachele De Felice and Stephen G. Pulman. 2008.
A classifier-based approach to preposition and deter-
miner error correction in L2 English. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics (COLING 2008), pages 169?176,
Manchester, UK, -August.
Jens Eeg-Olofsson and Ola Knutsson. 2003. Automatic
grammar checking for second language learners - the
use of prepositions. In Nodalida.
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
der Klementiev, William Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using contextual speller
techniques and language modeling for ESL error cor-
rection. In Proceedings of IJCNLP, Hyderabad, India,
January.
Michael Gamon. 2010. Using Mostly Native Data to
Correct Errors in Learners? Writing: A Meta-Classifier
Approach. In Proceedings of NAACL 2010, pages
163?171, Los Angeles, USA, June.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by non-
native speakers. Journal of Natural Language Engi-
neering, 12(2):115?129.
Na-Rae Han, Joel R. Tetreault, Soo-Hwa Lee, and Jin-
Young Ha. 2010. Using an Error-Annotated Learner
Corpus to Develop an ESL/EFL Error Correction
System. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC-10), Valletta, Malta, May.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thep-
chai Supnithi, and Hitoshi Isahara. 2003. Automatic
error detection in the Japanese learners? English spo-
ken data. In The Companion Volume to the Proceed-
ings of 41st Annual Meeting of the Association for
Computational Linguistics, pages 145?148, Sapporo,
Japan, July.
Ekaterina Kochmar. 2011. Identification of a Writer?s
Native Language by Error Analysis University of
Cambridge, MPhil Dissertation.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Morgan &
Claypool Publishers.
John Lee and Stephanie Seneff. 2008. An analysis of
grammatical errors in non-native speech in English. In
Proceedings of the 2008 Spoken Language Technology
Workshop.
Ryo Nagata and Kazuhide Nakatani. 2010 Evaluating
performance of grammatical error detection to max-
imize learning effect. In Proceedings of Int. Conf.
on Computational Linguistics (Coling-10), Poster Ses-
sion, pages 894?900, Beijing, China.
Diane Nicholls. 2003. The Cambridge Learner Corpus:
Error coding and analysis for lexicography and ELT.
In Proceedings of the Corpus Linguistics conference,
pages 572?581.
Jason Rennie, Lawrence Shih, Jaime Teevan, and
David Karger. 2003 Tackling the Poor Assumtions
of Naive Bayes Text Classifiers. 20th Int. Conference
on Machine Learning (ICML-2003) Washington, DC
Alla Rozovskaya and Dan Roth. 2010a. Annotating ESL
Errors: Challenges and Rewards. In Proceedings of
the NAACL Workshop on Innovative Use of NLP for
Building Educational Applications.
Alla Rozovskaya and Dan Roth. 2010b. Generating Con-
fusion Sets for Context-Sensitive Error Correction. In
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Alla Rozovskaya and Dan Roth. 2010c. Training
Paradigms for Correcting Errors in Grammar and Us-
age. In Proceedings of the Annual Meeting of the
North American Association of Computational Lin-
guistics (NAACL).
Alla Rozovskaya and Dan Roth. 2011. Algorithm Selec-
tion and Model Adaptation for ESL Correction Tasks.
In Proceedings of the Annual Meeting of the Associa-
tion of Computational Linguistics (ACL).
Joel R. Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in ESL writ-
ing. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 865?872, Manchester, UK, August.
Joel R. Tetreault, Jennifer Foster, Martin Chodorow.
2010. Using Parse Features for Preposition Selection
and Error Detection. In ACL.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automatically
Grading ESOL Texts. In The 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies.
250
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 15?24,
Baltimore, Maryland, 26-27 July 2014.
c?2014 Association for Computational Linguistics
Grammatical error correction using hybrid systems and type filtering
Mariano Felice Zheng Yuan ?istein E. Andersen
Helen Yannakoudakis Ekaterina Kochmar
Computer Laboratory, University of Cambridge, United Kingdom
{mf501,zy249,oa223,hy260,ek358}@cl.cam.ac.uk
Abstract
This paper describes our submission to the
CoNLL 2014 shared task on grammatical
error correction using a hybrid approach,
which includes both a rule-based and an
SMT system augmented by a large web-
based language model. Furthermore, we
demonstrate that correction type estima-
tion can be used to remove unnecessary
corrections, improving precision without
harming recall. Our best hybrid system
achieves state-of-the-art results, ranking
first on the original test set and second on
the test set with alternative annotations.
1 Introduction
Grammatical error correction has attracted con-
siderable interest in the last few years, especially
through a series of ?shared tasks?. These efforts
have helped to provide a common ground for eval-
uating and comparing systems while encouraging
research in the field. These shared tasks have pri-
marily focused on English as a second or foreign
language and addressed different error types. The
HOO 2011 task (Dale and Kilgarriff, 2011), for
example, included all error types whereas HOO
2012 (Dale et al., 2012) and the CoNLL 2013
shared task (Ng et al., 2013) were restricted to only
two and five types respectively.
In this paper, we describe our submission to the
CoNLL 2014 shared task (Ng et al., 2014), which
involves correcting all the errors in essays writ-
ten in English by students at the National Univer-
sity of Singapore. An all-type task poses a greater
challenge, since correcting open-class types (such
as spelling or collocation errors) requires different
correction strategies than those in closed classes
(such as determiners or prepositions).
In this scenario, hybrid systems or combinations
of correction modules seem more appropriate and
typically produce good results. In fact, most of
the participating teams in previous shared tasks
have used a combination of modules or systems
for their submissions, even for correcting closed-
class types (Dahlmeier et al., 2011; Bhaskar et
al., 2011; Rozovskaya et al., 2011; Ivanova et al.,
2011; Rozovskaya et al., 2013; Yoshimoto et al.,
2013; Xing et al., 2013; Kunchukuttan et al., 2013;
Putra and Szabo, 2013; Xiang et al., 2013).
In line with previous research, we present a hy-
brid approach that employs a rule-based error cor-
rection system and an ad-hoc statistical machine
translation (SMT) system, as well as a large-scale
language model to rank alternative corrections and
an error type filtering technique.
The remainder of this paper is organised as fol-
lows: Section 2 describes our approach and each
component in detail, Section 3 presents our experi-
ments using the CoNLL 2014 shared task develop-
ment set and Section 4 reports our official results
on the test set. Finally, we discuss the performance
of our system and present an error analysis in Sec-
tion 5 and conclude in Section 6.
2 Approach
We tackle the error correction task using a pipeline
of processes that combines results from multiple
systems. Figure 1 shows the interaction of the
components in our final hybrid system, producing
the results submitted to the CoNLL 2014 shared
task. The following sections describe each of these
components in detail.
2.1 Rule-based error correction system
(RBS)
The rule-based system is a component of the Self-
Assessment and Tutoring (SAT) system, a web
service developed at the University of Cambridge
aimed at helping intermediate learners of English
15
Figure 1: Overview of components and interac-
tions in our final hybrid system.
in their writing tasks
1
(Andersen et al., 2013). The
original SAT system provides three main function-
alities: 1) text assessment, producing an overall
score for a piece of text, 2) sentence evaluation,
producing a sentence-level quality score, and 3)
word-level feedback, suggesting specific correc-
tions for frequent errors. Since the focus of the
shared task is on strict correction (as opposed to
detection), we only used the word-level feedback
component of the SAT system.
This module uses rules automatically derived
from the Cambridge Learner Corpus
2
(CLC)
(Nicholls, 2003) that are aimed at detecting error-
ful unigrams, bigrams and trigrams. In order to
ensure high precision, rules are based on n-grams
that have been annotated as incorrect at least five
times and at least ninety per cent of the times
they occur. In addition to these corpus-derived
rules, many cases of incorrect but plausible deriva-
tional and inflectional morphology are detected by
means of rules derived from a machine-readable
dictionary. For further details on specific compo-
nents, we refer the reader to the aforementioned
paper.
Given an input text, the rule-based system pro-
duces an XML file containing a list of suggested
corrections. These corrections can either be ap-
plied to the original text or used to generate mul-
tiple correction candidates, as described in Sec-
tion 2.3.
2.2 SMT system
We follow a similar approach to the one described
by Yuan and Felice (2013) in order to train an SMT
1
The latest version of the system, called ?Write
& Improve?, is available at http://www.cambridge
english.org/writeandimprovebeta/.
2
More information at http://www.cambridge
.org/elt/catalogue/subject/custom/item36
46603/
system that can ?translate? from incorrect into cor-
rect English. Our training data comprises a set of
different parallel corpora, where the original (in-
correct) sentences constitute the source side and
corrected versions based on gold standard anno-
tations constitute the target side. These corpora
include:
? the NUCLE v3.1 corpus (Dahlmeier et al.,
2013), containing around 1,400 essays writ-
ten in English by students at the National
University of Singapore (approx. 1,220,257
tokens in 57,152 sentences),
? phrase alignments involving corrections ex-
tracted automatically from the NUCLE cor-
pus (with up to 7 tokens per side), which are
used to boost the probability of phrase align-
ments that involve corrections so as to im-
prove recall,
? the CoNLL 2014 shared task development
set, containing 50 essays from the previous
year?s test set (approx. 29,207 tokens in 1,382
sentences),
? the First Certificate in English (FCE) cor-
pus (Yannakoudakis et al., 2011), contain-
ing 1,244 exam scripts and 2 essays per
script (approx. 532,033 tokens in 16,068 sen-
tences),
? a subset of the International English Lan-
guage Testing System (IELTS) examination
dataset extracted from the CLC corpus, con-
taining 2,498 exam scripts and 2 essays per
script (approx. 1,361,841 tokens in 64,628
sentences), and
? a set of sentences from the English Vo-
cabulary Profile
3
(EVP), which have been
modified to include artificially generated er-
rors (approx. 351,517 tokens in 18,830 sen-
tences). The original correct sentences are a
subset of the CLC and come from examina-
tions at different proficiency levels. The ar-
tificial error generation method aims at repli-
cating frequent error patterns observed in the
NUCLE corpus on error-free sentences, as
described by Yuan and Felice (2013).
3
Sentences were automatically scraped from http://
www.englishprofile.org/index.php?option=
com_content&view=article&id=4&Itemid=5
16
Word alignment was carried out using pialign
(Neubig et al., 2011), after we found it outper-
formed GIZA++ (Och and Ney, 2000; Och and
Ney, 2003) and Berkeley Aligner (Liang et al.,
2006; DeNero and Klein, 2007) in terms of pre-
cision and F
0.5
on the development set. Instead
of using heuristics to extract phrases from the
word alignments learnt by GIZA++ or Berker-
ley Aligner, pialign created a phrase table directly
from model probabilities.
In addition to the features already defined by pi-
align, we added character-level Levenshtein dis-
tance to each mapping in the phrase table. This
was done to allow for the fact that, in error correc-
tion, most words translate into themselves and er-
rors are often similar to their correct forms. Equal
weights were assigned to these features.
We then built a lexical reordering model using
the alignments created by pialign. The maximum
phrase length was set to 7, as recommended in the
SMT literature (Koehn et al., 2003; Koehn, 2014).
The IRSTLM Toolkit (Federico et al., 2008)
was used to build a 4-gram target language model
with Kneser?Ney smoothing (Kneser and Ney,
1995) on the correct sentences from the NUCLE,
full CLC and EVP corpora.
Decoding was performed with Moses (Koehn et
al., 2007), using the default settings and weights.
No tuning process was applied. The resulting sys-
tem was used to produce the 10 best correction
candidates for each sentence in the dataset, which
were further processed by other modules.
Segmentation, tokenisation and part-of-speech
tagging were performed using NLTK (Bird et
al., 2009) for consistency with the shared task
datasets.
2.3 Candidate generation
In order to integrate corrections from multiple sys-
tems, we developed a method to generate all the
possible corrected versions of a sentence (candi-
dates). Candidates are generated by computing all
possible combinations of corrections (irrespective
of the system from which they originate), includ-
ing the original tokens to allow for a ?no correc-
tion? option. The list of candidates produced for
each sentence always includes the original (un-
modified) sentence plus any other versions derived
from system corrections.
In order for a combination of corrections to gen-
erate a valid candidate, all the corrections must be
Figure 2: An example showing the candidate gen-
eration process.
Model CE ME UE P R F
0.5
SMT IRSTLM 651 2766 1832 0.2621 0.1905 0.2438
Microsoft Web
N-grams
666 2751 1344 0.3313 0.1949 0.2907
Table 1: Performance of language models on the
development set after ranking the SMT system?s
10-best candidates per sentence. CE: correct ed-
its, ME: missed edits, UE: unnecessary edits, P:
precision, R: recall.
compatible; otherwise, the candidate is discarded.
We consider two or more corrections to be com-
patible if they do not overlap, in an attempt to
avoid introducing accidental errors. In addition,
if different correction sets produce the same can-
didate, we only keep one. Figure 2 illustrates the
candidate generation process.
2.4 Language model ranking
Generated candidates are ranked using a language
model (LM), with the most probable candidate be-
ing selected as the final corrected version.
We tried two different alternatives for ranking:
1) using the target LM embedded in our SMT sys-
tem (described in Section 2.2) and 2) using a large
n-gram LM built from web data. In the latter
case, we used Microsoft Web N-gram Services,
which provide access to large smoothed n-gram
language models (with n=2,3,4,5) built from web
documents (Gao et al., 2010). All our experiments
are based on the 5-gram ?bing-body:apr10? model.
The ranking performance of these two models
was evaluated on the 10-best hypotheses generated
by the SMT system for each sentence in the devel-
opment set. Table 1 shows the results from the
M
2
Scorer (Dahlmeier and Ng, 2012), the official
scorer for the shared task that, unlike previous ver-
sions, weights precision twice as much as recall.
Results show that using Microsoft?s Web LM
yields better performance, which is unsurprising
given the vast amounts of data used to build that
17
System CE ME UE P R F
0.5
RBS 95 3322 107 0.4703 0.0278 0.1124
SMT 452 2965 690 0.3958 0.1323 0.2830
Table 2: Results of individual systems on the de-
velopment set.
model. For this reason, we adopt Microsoft?s
model for all further experiments.
We also note that without normalisation, higher
probabilities may be assigned to shorter sentences,
which can introduce a bias towards preferring
deletions or skipping insertions.
2.5 Type filtering
Analysing performance by error type is very valu-
able for system development and tuning. How-
ever, this can only be performed for corrections
in the gold standard (either matched or missed).
To estimate types for unnecessary corrections, we
defined a set of heuristics that analyse differences
in word forms and part-of-speech tags between
the original phrases and their system corrections,
based on common patterns observed in the train-
ing data. We had previously used a similar strat-
egy to classify errors in our CoNLL 2013 shared
task submission (Yuan and Felice, 2013) but have
now included a few improvements and rules for
new types. Estimation accuracy is 50.92% on the
training set and 67.57% on the development set,
which we consider to be acceptable for our pur-
poses given that the final test set is more similar to
the development set.
Identifying types for system corrections is not
only useful during system development but can
also be exploited to filter out and reduce the num-
ber of proposed corrections. More specifically, if
a system proposes a much higher number of un-
necessary corrections than correct suggestions for
a specific error type, we can assume the system is
actually degrading the quality of the original text,
in which case it is preferable to filter out those er-
ror types. Such decisions will lower the total num-
ber of unnecessary edits, thus improving overall
precision. However, they will also harm recall,
unless the number of matched corrections for the
error type is zero (i.e. unless P
type
= 0). To avoid
this, only corrections for types having zero preci-
sion should be removed.
3 Experiments and results
We carried out a series of experiments on the de-
velopment set using different pipelines and com-
binations of systems in order to find an optimal
setting. The following sections describe them in
detail.
3.1 Individual system performance
Our first set of experiments were aimed at inves-
tigating individual system performance on the de-
velopment set, which is reported in Table 2. Re-
sults show that the SMT system has much better
performance, which is expected given that it has
been trained on texts similar to those in the test
set.
3.2 Pipelines
Since corrections from the RBS and SMT systems
are often complementary, we set out to explore
combination schemes that would integrate correc-
tions from both systems. Table 3 shows results for
different combinations, where RBS and SMT in-
dicate all corrections from the respective systems,
subscript ?c? indicates candidates generated from
a system?s individual corrections, subscript ?10-
best? indicates the 10-best list of candidates pro-
duced by the SMT system, ?>? indicates a pipeline
where the output of one system is the input to the
other and ?+? indicates a combination of candi-
dates from different systems. All these pipelines
use the RBS system as the first processing step in
order to perform an initial correction, which is ex-
tremely beneficial for the SMT system.
Results reveal that the differences between
these pipelines are small in terms of F
0.5
, although
there are noticeable variations in precision and re-
call. The best results are achieved when the 10
best hypotheses from the SMT system are ranked
with Microsoft?s LM, which confirms our results
in Table 1 showing that the SMT LM is outper-
formed by a larger web-based model.
A simple pipeline using the RBS system first
and the SMT system second (#3) yields per-
formance that is better than (or comparable to)
pipelines #1, #2 and #4, suggesting that there is no
real benefit in using more sophisticated pipelines
when only the best hypothesis from the SMT sys-
tem is used. However, performance is improved
when the 10 best SMT hypotheses are considered.
The only difference between pipelines #5 and #6
lies in the way corrections from the RBS system
18
# Pipeline CE ME UE P R F
0.5
?
1 RBS > SMT
c
> LM 372 3045 481 0.4361 0.1088 0.2723
2 RBS
c
+ SMT
c
> LM 400 3017 485 0.4520 0.1171 0.2875
3 RBS > SMT 476 2941 738 0.3921 0.1393 0.2877
4 RBS
c
> LM > SMT 471 2946 718 0.3961 0.1378 0.2881
5 RBS > SMT
10-best
> LM 678 2739 1368 0.3314 0.1984 0.2922
6 RBS
c
> LM > SMT
10-best
> LM 681 2736 1366 0.3327 0.1993 0.2934
Table 3: Results for different system pipelines on the development set.
System CE ME UE P R F
0.5
RBS
c
> LM > SMT
10-best
> LM 681 2736 1366 0.3327 0.1993 0.2934
RBS
c
> LM > SMT
10-best
> LM > Filter 681 2736 1350 0.3353 0.1993 0.2950
Table 4: Results for individual systems on the development set.
are handled. In the first case, all corrections are
applied at once whereas in the second, the sug-
gested corrections are used to generate candidates
that are subsequently ranked by our LM, often dis-
carding some of the suggested corrections.
3.3 Filtering
As described in Section 2.5, we can evaluate per-
formance by error type in order to identify and re-
move unnecessary corrections. In particular, we
tried to optimise our best hybrid system (#6) by
filtering out types with zero precision. Table 5
shows type-specific performance for this system,
where three zero-precision types can be identi-
fied: Reordering (a subset of Others that we treat
separately), Srun (run-ons/comma splices) and Wa
(acronyms). Although reordering was explicitly
disabled in our SMT system, a translation table
can still include this type of mappings if they are
observed in the training data (e.g. ?you also can?
? ?you can also?).
In order to remove such undesired corrections,
the following procedure was applied: first, in-
dividual corrections were extracted by compar-
ing the original and corrected sentences; second,
the type of each extracted correction was pre-
dicted, subsequently deleting those that matched
unwanted types (i.e. reordering, Srun or Wa); fi-
nally, the set of remaining corrections was applied
to the original text. This method improves pre-
cision while preserving recall (see Table 4), al-
though the resulting improvement is not statisti-
cally significant (paired t-test, p > 0.05).
4 Official evaluation results
Our submission to the CoNLL 2014 shared task is
the result of our best hybrid system, described in
the previous section and summarised in Figure 1.
The official test set comprised 50 new essays (ap-
prox. 30,144 tokens in 1,312 sentences) written in
response to two prompts, one of which was also
included in the training data.
Systems were evaluated using the M
2
Scorer,
which uses F
0.5
as its overall measure. As in previ-
ous years, there were two evaluation rounds. The
first one was based on the original gold-standard
annotations provided by the shared-task organis-
ers whereas the second was based on a revised
version including alternative annotations submit-
ted by the participating teams. Our submitted sys-
tem achieved the first and second place respec-
tively. The official results of our submission in
both evaluation rounds are reported in Table 6.
5 Discussion and error analysis
In order to assess how our system performed per
error type on the test set, we ran our type estima-
tion script and obtained the results shown in Ta-
ble 7. Although these results are estimated and
therefore not completely accurate,
4
they can still
provide valuable insights, at least at a coarse level.
The following sections discuss our main findings.
5.1 Type performance
According to Table 7, our system achieves the best
performance for types WOadv (adverb/adjective
position) and Wtone (tone), but these results are
4
Estimation accuracy was found to be 57.90% on the test
set.
19
Error type CE ME UE P R F
0.5
ArtOrDet 222 465 225 0.4966 0.3231 0.4485
Cit 0 6 0 ? 0.0000 ?
Mec 31 151 15 0.6739 0.1703 0.4235
Nn 138 256 136 0.5036 0.3503 0.4631
Npos 4 25 45 0.0816 0.1379 0.0889
Others 1 34 12 0.0769 0.0286 0.0575
Pform 1 25 22 0.0435 0.0385 0.0424
Pref 1 38 5 0.1667 0.0256 0.0794
Prep 61 249 177 0.2563 0.1968 0.2417
Reordering 0 1 12 0.0000 0.0000 ?
Rloc- 13 115 80 0.1398 0.1016 0.1300
SVA 32 86 25 0.5614 0.2712 0.4624
Sfrag 0 4 0 ? 0.0000 ?
Smod 0 16 0 ? 0.0000 ?
Spar 4 30 0 1.0000 0.1176 0.4000
Srun 0 55 28 0.0000 0.0000 ?
Ssub 7 64 15 0.3182 0.0986 0.2201
Trans 13 128 36 0.2653 0.0922 0.1929
Um 0 34 0 ? 0.0000 ?
V0 2 16 3 0.4000 0.1111 0.2632
Vform 28 90 68 0.2917 0.2373 0.2789
Vm 9 86 41 0.1800 0.0947 0.1525
Vt 18 137 53 0.2535 0.1161 0.2050
WOadv 0 12 0 ? 0.0000 ?
WOinc 2 35 71 0.0274 0.0541 0.0304
Wa 0 5 2 0.0000 0.0000 ?
Wci 28 400 241 0.1041 0.0654 0.0931
Wform 65 161 54 0.5462 0.2876 0.4630
Wtone 1 12 0 1.0000 0.0769 0.2941
TOTAL 681 2736 1366 0.3327 0.1993 0.2934
Table 5: Type-specific performance of our best hy-
brid system on the development set. Types with
zero precision are marked in bold.
Test set CE ME UE P R F
0.5
Original 772 1793 1172 0.3971 0.3010 0.3733
Revised 913 1749 1042 0.4670 0.3430 0.4355
Table 6: Official results of our system on the orig-
inal and revised test sets.
not truly representative as they only account for a
small fraction of the test data (0.64% and 0.36%
respectively).
The third best performing type is Mec, which
comprises mechanical errors (such as punctuation,
capitalisation and spelling mistakes) and repre-
sents 11.58% of the errors in the data. The remark-
ably high precision obtained for this error type
suggests that our system is especially suitable for
correcting such errors.
We also found that our system was particularly
good at enforcing different types of agreement, as
demonstrated by the results for SVA (subject?verb
agreement), Pref (pronoun reference), Nn (noun
number) and Vform (verb form) types, which add
up to 22.80% of the errors. The following example
shows a successful correction:
Error type CE ME UE P R F
0.5
ArtOrDet 185 192 206 0.4731 0.4907 0.4766
Mec 86 219 16 0.8431 0.2820 0.6031
Nn 122 106 143 0.4604 0.5351 0.4736
Npos 2 13 59 0.0328 0.1333 0.0386
Others 0 30 10 0.0000 0.0000 ?
Pform 8 26 21 0.2759 0.2353 0.2667
Pref 19 77 12 0.6129 0.1979 0.4318
Prep 100 159 144 0.4098 0.3861 0.4049
Reordering 0 0 7 0.0000 ? ?
Rloc- 23 89 116 0.1655 0.2054 0.1722
SVA 38 85 31 0.5507 0.3089 0.4762
Sfrag 0 4 0 ? 0.0000 ?
Smod 0 2 0 ? 0.0000 ?
Spar 0 10 0 ? 0.0000 ?
Srun 0 14 1 0.0000 0.0000 ?
Ssub 8 39 19 0.2963 0.1702 0.2581
Trans 17 54 39 0.3036 0.2394 0.2881
Um 2 21 0 1.0000 0.0870 0.3226
V0 8 20 15 0.3478 0.2857 0.3333
Vform 31 93 46 0.4026 0.2500 0.3588
Vm 7 27 35 0.1667 0.2059 0.1733
Vt 26 108 40 0.3939 0.1940 0.3266
WOadv 10 11 0 1.0000 0.4762 0.8197
WOinc 1 33 37 0.0263 0.0294 0.0269
Wci 33 305 146 0.1844 0.0976 0.1565
Wform 42 49 29 0.5915 0.4615 0.5600
Wtone 4 7 0 1.0000 0.3636 0.7407
TOTAL 772 1793 1172 0.3971 0.3010 0.3733
Table 7: Type-specific performance of our submit-
ted system on the original test set.
ORIGINAL SENTENCE:
He or she has the right not to tell anyone .
SYSTEM HYPOTHESIS:
They have the right not to tell anyone .
GOLD STANDARD:
They have the right not to tell anyone .
In other cases, our system seems to do a good
job despite gold-standard annotations:
ORIGINAL SENTENCE:
This is because his or her relatives have the
right to know about this .
SYSTEM HYPOTHESIS:
This is because their relatives have the right
to know about this .
GOLD STANDARD:
This is because his or her relatives have the
right to know about this . (unchanged)
The worst performance is observed for Others
(including Reordering) and Srun, which only ac-
count for 1.69% of the errors. We also note that
Reordering and Srun errors, which had explicitly
been filtered out, still appear in our final results,
20
which is due to differences in the edit extraction
algorithms used by the M
2
Scorer and our own im-
plementation. According to our estimations, our
system has poor performance on the Wci type (the
second most frequent), suggesting it is not very
successful at correcting idioms and collocations.
Corrections for more complex error types such
as Um (unclear meaning), which are beyond the
scope of this shared task, are inevitably missed.
5.2 Deletions
We have also observed that many mismatches be-
tween our system?s corrections and the gold stan-
dard are caused by unnecessary deletions, as in the
following example:
ORIGINAL SENTENCE:
I could understand the feeling of the carrier .
SYSTEM HYPOTHESIS:
I understand the feeling of the carrier .
GOLD STANDARD:
I could understand the feeling of the carrier .
(unchanged)
This effect is the result of using 10-best hy-
potheses from the SMT system together with LM
ranking. Hypotheses from an SMT system can in-
clude many malformed sentences which are effec-
tively discarded by the embedded target language
model and additional heuristics. However, rank-
ing these raw hypotheses with external systems
can favour deletions, as language models will gen-
erally assign higher probabilities to shorter sen-
tences. A common remedy for this is normali-
sation but we found it made no difference in our
experiments.
In other cases, deletions can be ascribed to dif-
ferences in the domain of the training and test sets,
as observed in this example:
ORIGINAL SENTENCE:
Nowadays , social media are able to dissemi-
nate information faster than any other media .
SYSTEM HYPOTHESIS:
Nowadays , the media are able to disseminate
information faster than any other media .
GOLD STANDARD:
Nowadays , social media are able to dissemi-
nate information faster than any other media .
(unchanged)
5.3 Uncredited corrections
Our analysis also reveals a number of cases where
the system introduces changes that are not in-
cluded in the gold standard but we consider im-
prove the quality of a sentence. For example:
ORIGINAL SENTENCE:
Demon is not easily to be defeated and it is
required much of energy and psychological
support .
SYSTEM HYPOTHESIS:
Demon is not easily defeated and it requires
a lot of energy and psychological support .
GOLD STANDARD:
The demon is not easily defeated and it re-
quires much energy and psychological sup-
port .
Adding alternative corrections to the gold stan-
dard alleviates this problem, although the list of
alternatives will inevitably be incomplete.
There are also a number of cases where the sen-
tences are considered incorrect as part of a longer
text but are acceptable when they are evaluated in
isolation. Consider the following examples:
ORIGINAL SENTENCE:
The opposite is also true .
SYSTEM HYPOTHESIS:
The opposite is true .
GOLD STANDARD:
The opposite is also true . (unchanged)
ORIGINAL SENTENCE:
It has erased the boundaries of distance and
time .
SYSTEM HYPOTHESIS:
It has erased the boundaries of distance and
time . (unchanged)
GOLD STANDARD:
They have erased the boundaries of distance
and time .
In both cases, system hypotheses are perfectly
grammatical but they are considered incorrect
when analysed in context. Such mismatch is the
result of discrepancies between the annotation and
evaluation criteria: while the gold standard is an-
notated taking discourse into account, system cor-
21
rections are proposed in isolation, completely de-
void of discursive context.
Finally, the inability of the M
2
Scorer to com-
bine corrections from different annotators (as op-
posed to selecting only one annotator?s corrections
for the whole sentence) can also result in underes-
timations of performance. However, it is clear that
exploring these combinations during evaluation is
a challenging task itself.
6 Conclusions
We have presented a hybrid approach to error cor-
rection that combines a rule-based and an SMT
error correction system. We have explored dif-
ferent combination strategies, including sequen-
tial pipelines, candidate generation and ranking.
In addition, we have demonstrated that error type
estimations can be used to filter out unnecessary
corrections and improve precision without harm-
ing recall.
Results of our best hybrid system on the offi-
cial CoNLL 2014 test set yield F
0.5
=0.3733 for
the original annotations and F
0.5
=0.4355 for alter-
native corrections, placing our system in the first
and second place respectively.
Error analysis reveals that our system is partic-
ularly good at correcting mechanical errors and
agreement but is often penalised for unnecessary
deletions. However, a thorough inspection shows
that the system tends to produce very fluent sen-
tences, even if they do not match gold standard
annotations.
Acknowledgements
We would like to thank Marek Rei for his valuable
feedback and suggestions as well as Cambridge
English Language Assessment, a division of Cam-
bridge Assessment, for supporting this research.
References
?istein E. Andersen, Helen Yannakoudakis, Fiona
Barker, and Tim Parish. 2013. Developing and test-
ing a self-assessment and tutoring system. In Pro-
ceedings of the Eighth Workshop on Innovative Use
of NLP for Building Educational Applications, BEA
2013, pages 32?41, Atlanta, GA, USA, June. Asso-
ciation for Computational Linguistics.
Pinaki Bhaskar, Aniruddha Ghosh, Santanu Pal, and
Sivaji Bandyopadhyay. 2011. May I check the
English of your paper!!! In Proceedings of the
Generation Challenges Session at the 13th Euro-
pean Workshop on Natural Language Generation,
pages 250?253, Nancy, France, September. Associ-
ation for Computational Linguistics.
Steven Bird, Edward Loper, and Ewan Klein.
2009. Natural Language Processing with Python.
O?Reilly Media Inc.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Bet-
ter evaluation for grammatical error correction. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL 2012, pages 568?572, Montreal, Canada.
Daniel Dahlmeier, Hwee Tou Ng, and Thanh Phu Tran.
2011. NUS at the HOO 2011 Pilot Shared Task. In
Proceedings of the Generation Challenges Session
at the 13th European Workshop on Natural Lan-
guage Generation, pages 257?259, Nancy, France,
September. Association for Computational Linguis-
tics.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a Large Annotated Corpus of
Learner English: The NUS Corpus of Learner En-
glish. In Proceedings of the 8th Workshop on Inno-
vative Use of NLP for Building Educational Appli-
cations, BEA 2013, pages 22?31, Atlanta, Georgia,
USA, June.
Robert Dale and Adam Kilgarriff. 2011. Helping
Our Own: The HOO 2011 Pilot Shared Task. In
Proceedings of the Generation Challenges Session
at the 13th European Workshop on Natural Lan-
guage Generation, pages 242?249, Nancy, France,
September. Association for Computational Linguis-
tics.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A Report on the Preposition
and Determiner Error Correction Shared Task. In
Proceedings of the Seventh Workshop on Building
Educational Applications Using NLP, pages 54?62,
Montr?eal, Canada, June. Association for Computa-
tional Linguistics.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 17?24,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Proceed-
ings of the 9th Annual Conference of the Interna-
tional Speech Communication Association, INTER-
SPEECH 2008, pages 1618?1621, Brisbane, Aus-
tralia, September. ISCA.
Jianfeng Gao, Patrick Nguyen, Xiaolong Li, Chris
Thrasher, Mu Li, and Kuansan Wang. 2010. A
Comparative Study of Bing Web N-gram Language
Models for Web Search and Natural Language Pro-
cessing. In Web N-gram Workshop, Workshop of the
22
33rd Annual International ACM SIGIR Conference
(SIGIR 2010), pages 16?21, Geneva, Switzerland,
July.
Elitza Ivanova, Delphine Bernhard, and Cyril Grouin.
2011. Handling Outlandish Occurrences: Using
Rules and Lexicons for Correcting NLP Articles. In
Proceedings of the Generation Challenges Session
at the 13th European Workshop on Natural Lan-
guage Generation, pages 254?256, Nancy, France,
September. Association for Computational Linguis-
tics.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, volume I,
pages 181?184, Detroit, Michigan, May.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology, vol-
ume 1 of NAACL ?03, pages 48?54, Edmonton,
Canada. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Prague, Czech Republic.
Association for Computational Linguistics.
Philipp Koehn, 2014. Moses: Statistical Ma-
chine Translation System ? User Manual and Code
Guide. University of Edinburgh, April. Available
online at http://www.statmt.org/moses/
manual/manual.pdf.
Anoop Kunchukuttan, Ritesh Shah, and Pushpak Bhat-
tacharyya. 2013. IITB System for CoNLL 2013
Shared Task: A Hybrid Approach to Grammati-
cal Error Correction. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 82?87, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 104?111, New York City,
USA, June. Association for Computational Linguis-
tics.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. 2011. An
unsupervised model for joint phrase alignment and
extraction. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 632?
641, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-
2013 Shared Task on Grammatical Error Correction.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning: Shared
Task, pages 1?12, Sofia, Bulgaria, August. Associa-
tion for Computational Linguistics.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The CoNLL-2014 Shared Task
on Grammatical Error Correction. In Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning: Shared Task (CoNLL-2014
Shared Task), Baltimore, Maryland, USA, June. As-
sociation for Computational Linguistics. To appear.
Diane Nicholls. 2003. The Cambridge Learner Cor-
pus: Error coding and analysis for lexicography and
ELT. In Dawn Archer, Paul Rayson, Andrew Wil-
son, and Tony McEnery, editors, Proceedings of
the Corpus Linguistics 2003 conference, pages 572?
581, Lancaster, UK. University Centre for Computer
Corpus Research on Language, Lancaster Univer-
sity.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?00, pages 440?447, Hong
Kong, October. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Desmond Darma Putra and Lili Szabo. 2013. UdS
at CoNLL 2013 Shared Task. In Proceedings of
the Seventeenth Conference on Computational Natu-
ral Language Learning: Shared Task, pages 88?95,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Alla Rozovskaya, Mark Sammons, Joshua Gioja, and
Dan Roth. 2011. University of Illinois System in
HOO Text Correction Shared Task. In Proceedings
of the Generation Challenges Session at the 13th Eu-
ropean Workshop on Natural Language Generation,
pages 263?266, Nancy, France, September. Associ-
ation for Computational Linguistics.
Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,
and Dan Roth. 2013. The University of Illinois
System in the CoNLL-2013 Shared Task. In Pro-
ceedings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 13?19, Sofia, Bulgaria, August. Association
for Computational Linguistics.
23
Yang Xiang, Bo Yuan, Yaoyun Zhang, Xiaolong Wang,
Wen Zheng, and Chongqiang Wei. 2013. A hy-
brid model for grammatical error correction. In Pro-
ceedings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 115?122, Sofia, Bulgaria, August. Associa-
tion for Computational Linguistics.
Junwen Xing, Longyue Wang, Derek F. Wong, Lidia S.
Chao, and Xiaodong Zeng. 2013. UM-Checker: A
Hybrid System for English Grammatical Error Cor-
rection. In Proceedings of the Seventeenth Confer-
ence on Computational Natural Language Learn-
ing: Shared Task, pages 34?42, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading esol texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
180?189, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuzawa,
Keisuke Sakaguchi, Tomoya Mizumoto, Yuta
Hayashibe, Mamoru Komachi, and Yuji Matsumoto.
2013. NAIST at 2013 CoNLL Grammatical Er-
ror Correction Shared Task. In Proceedings of the
Seventeenth Conference on Computational Natural
Language Learning: Shared Task, pages 26?33,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Zheng Yuan and Mariano Felice. 2013. Constrained
grammatical error correction using statistical ma-
chine translation. In Proceedings of the Seventeenth
Conference on Computational Natural Language
Learning: Shared Task, pages 52?61, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
24
