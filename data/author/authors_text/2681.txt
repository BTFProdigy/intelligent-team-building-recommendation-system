Proceedings of NAACL HLT 2009: Short Papers, pages 109?112,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Using Integer Linear Programming for Detecting Speech Disfluencies
Kallirroi Georgila
Institute for Creative Technologies, University of Southern California
13274 Fiji Way, Marina del Rey, CA 90292, USA
kgeorgila@ict.usc.edu
Abstract
We present a novel two-stage technique for de-
tecting speech disfluencies based on Integer
Linear Programming (ILP). In the first stage
we use state-of-the-art models for speech dis-
fluency detection, in particular, hidden-event
language models, maximum entropy models
and conditional random fields. During testing
each model proposes possible disfluency la-
bels which are then assessed in the presence of
local and global constraints using ILP. Our ex-
perimental results show that by using ILP we
can improve the performance of our models
with negligible cost in processing time. The
less training data is available the larger the im-
provement due to ILP.
1 Introduction
Speech disfluencies (also known as speech repairs)
occur frequently in spontaneous speech and can pose
difficulties to natural language processing (NLP)
since most NLP tools (e.g. parsers, part-of-speech
taggers, information extraction modules) are tradi-
tionally trained on written language. Speech dis-
fluencies can be divided into three intervals, the
reparandum, the editing term and the correction
(Heeman and Allen, 1999; Liu et al, 2006).
(it was) * (you know) it was set
In the above example, ?it was? is the reparandum,
?you know? is the editing term and the remaining
sentence is the correction. The asterisk marks the in-
terruption point at which the speaker halts the origi-
nal utterance in order to start the repair. The edit-
ing term is optional and consists of one or more
filled pauses (e.g. uh, uh-huh) or discourse mark-
ers (e.g. you know, so). Some researchers include
editing terms in the definition of disfluencies. Here
we focus only on detecting repetitions (the speaker
repeats some part of the utterance), revisions (the
speaker modifies the original utterance) or restarts
(the speaker abandons an utterance and starts over).
We also deal with complex disfluencies, i.e. a series
of disfluencies in succession (?I think I think uh I
believe that...?).
In previous work many different approaches to
detecting speech disfluencies have been proposed.
Different types of features have been used, e.g. lexi-
cal features only, acoustic and prosodic features only
or a combination of both (Liu et al, 2006). Fur-
thermore, a number of studies have been conducted
on human transcriptions while other efforts have
focused on detecting disfluencies from the speech
recognition output.
In this paper we propose a novel framework for
speech disfluency detection based on Integer Lin-
ear Programming (ILP). With Linear Programming
(LP) problems the goal is to optimize a linear ob-
jective function subject to linear equality and linear
inequality constraints. When some or all the vari-
ables of the objective function and the constraints
are non-negative integers, LP becomes ILP. ILP has
recently attracted much attention in NLP. It has been
applied to several problems including sentence com-
pression (Clarke and Lapata, 2008) and relation ex-
traction (Roth and Yih, 2004). Some of these meth-
ods (e.g. (Roth and Yih, 2004)) follow the two-stage
approach of first hypothesizing a list of possible an-
swers using a classifier and then selecting the best
answer by applying ILP. We have adopted this two-
stage approach and applied it to speech disfluency
detection.
In the first stage we use state-of-the-art tech-
109
niques for speech disfluency detection, in particular,
Hidden-Event Language Models (HELMs) (Stolcke
and Shriberg, 1996), Maximum Entropy (ME) mod-
els (Ratnaparkhi, 1998) and Conditional Random
Fields (CRFs) (Lafferty et al, 2001). Nevertheless,
any other classification method could be used in-
stead. During testing each classifier proposes pos-
sible labels which are then assessed in the presence
of local and global constraints using ILP. ILP makes
the final decision taking into account both the con-
straints and the output of the classifier.
In the following we use the Switchboard corpus
and only lexical features for training our 3 classi-
fiers. Then we apply ILP to the output of each clas-
sifier. Our goal is not to investigate the best set
of features or achieve the best possible results. In
that case we could also use prosodic features as they
have been shown to improve performance. Our tar-
get is to show that by using ILP we can improve with
negligible cost in processing time the performance
of state-of-the-art techniques, especially when not
much training data is available.
The novelty of our work lies in the two follow-
ing areas: First, we propose a novel approach for
detecting disfluencies with improvements over state-
of-the-art models (HELMs, ME models and CRFs)
that use similar lexical features. Although the two-
stage approach is not unique, as discussed above,
the formulation of the ILP objective function and
constraints for disfluency detection is entirely novel.
Second, we compare our models using the tasks of
both detecting the interruption point and finding the
beginning of the reparandum. In previous work (Liu
et al, 2006) Hidden Markov Models (combination
of decision trees and HELMs) and ME models were
trained to detect the interruption points and then
heuristic rules were applied to find the correct on-
set of the reparandum in contrast to CRFs that were
trained to detect both points at the same time.
The structure of the paper is as follows: In sec-
tion 2 we describe our data set. In section 3 we de-
scribe our approach in detail. Then in section 4 we
present our experiments and provide results. Finally
in section 5 we present our conclusion and propose
future work.
2 Data Set
We use Switchboard (LDC catalog LDC99T42),
which is traditionally used for speech disfluency ex-
periments. We transformed the Switchboard annota-
tions into the following format:
I BE was IE one IP I was right
BE (beginning of edit) is the point where the
reparandum starts and IP is the interruption point
(the point before the repair starts). In the above ex-
ample the beginning of the reparandum is the first
occurrence of ?I?, the interruption point appears af-
ter ?one? and every word between BE and IP is
tagged as IE (inside edit). Sometimes BE and IP
occur at the same point, e.g. ?I BE-IP I think?.
The number of occurrences of BE and IP in our
training set are 34387 and 39031 respectively, in our
development set 3146 and 3499, and in our test set
6394 and 7413.
3 Methodology
In the first stage we train our classifier. Any clas-
sifier can be used as long as it provides more than
one possible answer (i.e. tag) for each word in the
utterance. Valid tags are BE, BE-IP, IP, IE or O. The
O tag indicates that the word is outside the disflu-
ent part of the utterance. ILP will be applied to the
output of the classifier during testing.
Let N be the number of words of each utter-
ance and i the location of the word in the utterance
(i=1,...,N ). Also, let CBE(i) be a binary variable (1
or 0) for the BE tag. Its value will be determined
by ILP. If it is 1 then the word will be tagged as
BE. In the same way, we use CBE?IP (i), CIP (i),
CIE(i), CO(i) for tags BE-IP, IP, IE and O respec-
tively. Let PBE(i) be the probability given by the
classifier that the word is tagged as BE. In the same
way, let PBE?IP (i), PIP (i), PIE(i), PO(i) be the
probabilities for tags BE-IP, IP, IE and O respec-
tively. Given the above definitions, the ILP problem
formulation can be as follows:
max[?Ni=1[PBE(i)CBE(i) + PBE?IP (i)CBE?IP (i)+PIP (i)CIP (i) + PIE(i)CIE(i) + PO(i)CO(i)]]
(1)
subject to:
CBE(i) + CBE?IP (i) + CIP (i) + CIE(i)
+CO(i) = 1 ?i ? (1, ..., N) (2)
CBE(1) + CBE?IP (1) + CO(1) = 1 (3)
CBE?IP (N) + CIP (N) + CO(N) = 1 (4)
CBE(i)? CBE?IP (i? 1)? CIP (i? 1)
?CO(i? 1) ? 0 ?i ? (2, ..., N) (5)
1? CBE(i)? CBE(i? 1) ? 0 ?i ? (2, ..., N) (6)
110
Equation 1 is the linear objective function that we
want to maximize, i.e. the overall probability of the
utterance. Equation 2 says that each word can have
one tag only. Equation 3 denotes that the first word is
either BE, BE-IP or O. Equation 4 says that the last
word is either BE-IP, IP or O. For example the last
word cannot be BE because then we would expect to
see an IP. Equation 5 defines the transitions that are
allowed between tags as described in Table 1 (first
row). Equation 5 says that if we have a word tagged
as BE it means that the previous word was tagged as
BE-IP or IP or O. It could not have been tagged as
IE because IE must be followed by an IP before a
new disfluency starts. Also, it could not have been
BE because then we would expect to see an IP. From
Table 1 we can easily define 4 more equations for the
rest of the tags. Finally, equation 6 denotes that we
cannot transition from BE to BE (we need an IP in
between).
We also formulate some additional rules that
describe common disfluency patterns. First, let us
have an example of a long-context rule. If we have
the sequence of words ?he was the one um you know
she was the one?, we expect this to be tagged as ?he
BE was IE the IE one IP um O you O know O she O
was O the O one O?, if we do not take into account
the context in which this pattern occurs. We incor-
porate this rule into our ILP problem formulation as
follows: Let (w1,...,wN ) be a sequence of N words
where both w2 and wN?7 are personal pronouns,
the word sequence w3,w4,w5 is the same as the
sequence wN?6,wN?5,wN?4 and all the words in
between (w6,...,wN?8) are filled pauses or discourse
markers. Then the probabilities given by the classi-
fier are modified as follows: PBE(2)=PBE(2)+th1,
PIE(3)=PIE(3)+th2, PIE(4)=PIE(4)+th3 and
PIP (5)=PIP (5)+th4, where th1, th2, th3 and th4
are empirically set thresholds (between 0.5 and 1,
using the development set of the corpus).
Now, here is an example of a short-context rule.
If we have the same word appear 3 times in a row
(?do do do?) we expect this to be tagged as ?do BE-
IP do IP do O?. To incorporate this rule into our ILP
problem formulation we can modify the probabili-
ties given by the classifier accordingly.
In total we have used 7 rules that deal with short-
context and 5 rules that deal with long-context de-
pendencies. From now on we will refer to the model
that uses all rules (general ILP formulation and all
pattern-based rules) as ILP and to the model that
From Tag To Tag
BE-IP or IP or O BE
BE-IP or IP or O BE-IP
BE or BE-IP or IP or IE IP
BE or BE-IP or IP or IE IE
BE-IP or IP or O O
Table 1: Possible transitions between tags.
uses only the general ILP constraints and the short-
context pattern-based rules as ILP-. In all rules, we
can skip editing terms (see example above).
4 Experiments
For HELMs we use the SRI Statistical Language
Modeling Toolkit. Each utterance is a sequence of
word and Part-of-Speech (POS) pairs fed into the
toolkit: i/prp BE was/vbd IE one/cd IP
i/prp was/vbd right/jj. We report results
with 4-grams. For ME we use the OpenNLP Max-
Ent toolkit and for CRFs the toolkit CRF++ (both
available from sourceforge). We experimented
with different sets of features and we achieved the
best results with the following setup (i is the loca-
tion of the word or POS in the sentence): Our word
features are ?wi?, ?wi+1?, ?wi?1, wi?, ?wi, wi+1?,
?wi?2, wi?1, wi?, ?wi, wi+1, wi+2?. Our POS fea-
tures have the same structure as the word features.
For ILP we use the lp solve software also avail-
able from sourceforge.
For evaluating the performance of our models we
use standard metrics proposed in the literature, i.e.
F-score and NIST Error Rate. We report results for
BE and IP. F-score is the harmonic mean of preci-
sion and recall (we equally weight precision and re-
call). Precision is computed as the ratio of the cor-
rectly identified tags X to all the tags X detected by
the model (where X is BE or IP). Recall is the ra-
tio of the correctly identified tags X to all the tags
X that appear in the reference utterance. The NIST
Error Rate measures the average number of incor-
rectly identified tags per reference tag, i.e. the sum
of insertions, deletions and substitutions divided by
the total number of reference tags (Liu et al, 2006).
To calculate the level of statistical significance we
always use the Wilcoxon signed-rank test.
Table 2 presents comparative results between our
models. The ILP and ILP- models lead to signif-
icant improvements compared to the plain models
for HELMs and ME (p<10?8, plain models vs. ILP
and ILP-). With CRFs the improvement is smaller,
111
BE IP
F-score Error F-score Error
4gram 60.3 54.8 67.0 50.7
4gram ILP 76.0 38.1 79.0 38.0
4gram ILP- 73.9 39.5 77.9 38.3
ME 63.8 52.6 72.8 44.3
ME ILP 77.9 36.3 80.8 35.4
ME ILP- 75.6 37.2 81.0 33.7
CRF 78.6 34.3 82.0 31.7
CRF ILP 80.1 34.5 82.5 33.3
CRF ILP- 79.8 33.5 83.4 30.5
Table 2: Comparative results between our models.
25% 50% 75% 100%
4gram 59.8 56.6 56.2 54.8
4gram ILP 40.2 38.9 38.2 38.0
4gram ILP- 42.1 40.7 39.8 39.5
ME 61.6 56.9 54.7 52.6
ME ILP 38.5 37.7 36.5 36.3
ME ILP- 39.7 38.7 37.6 37.2
CRF 40.3 37.1 35.5 34.3
CRF ILP 37.1 36.2 35.2 34.5
CRF ILP- 36.6 35.5 34.4 33.5
Table 3: Error rate variation for BE depending on the
training set size.
p<0.03 (CRF vs. CRF with ILP), not significant
(CRF vs. CRF with ILP-), p<0.0008 (CRF with ILP
vs. CRF with ILP-). HELMs and ME models ben-
efit more from the ILP model than the ILP- model
(ME only for the BE tag) whereas ILP- appears to
perform better than ILP for CRFs.
Table 3 shows the effect of the training set size on
the error rates only for BE due to space restrictions.
The trend is similar for IP. The test set is always the
same. Both ILP and ILP- perform better than the
plain models. This is true even when the ILP and
ILP- models are trained with less data (HELMs and
ME models only). Note that HELM (or ME) with
ILP or ILP- trained on 25% of the data performs bet-
ter than plain HELM (or ME) trained on 100% of the
data (p<10?8). This is very important because col-
lecting and annotating data is expensive and time-
consuming. Furthermore, for CRFs in particular the
training process takes long especially for large data
sets. In our experiments CRFs took about 400 iter-
ations to converge (approx. 136 min for the whole
training set) whereas ME models took approx. 48
min for the same number of iterations and training
set size. Also, ME models trained with 100 iter-
ations (approx. 11 min) performed better than ME
models trained with 400 iterations. The cost of ap-
plying ILP is negligible since the process is fast and
applied during testing.
5 Conclusion
We presented a novel two-stage technique for de-
tecting speech disfluencies based on ILP. In the first
stage we trained HELMs, ME models and CRFs.
During testing each classifier proposed possible la-
bels which were then assessed in the presence of lo-
cal and global constraints using ILP. We showed that
ILP can improve the performance of state-of-the-art
classifiers with negligible cost in processing time,
especially when not much training data is available.
The improvement is significant for HELMs and ME
models. In future work we will experiment with
acoustic and prosodic features and detect disfluen-
cies from the speech recognition output.
Acknowledgments
This work was sponsored by the U.S. Army Re-
search, Development, and Engineering Command
(RDECOM). The content does not necessarily re-
flect the position or the policy of the Government,
and no official endorsement should be inferred.
References
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear programming
approach. Journal of Artificial Intelligence Research,
31:399?429.
P. Heeman and J. Allen. 1999. Speech repairs, in-
tonational phrases and discourse markers: Modeling
speakers? utterances in spoken dialogue. Computa-
tional Linguistics, 25:527?571.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
Y. Liu, E. Shriberg, A. Stolcke, D. Hillard, M. Osten-
dorf, and M. Harper. 2006. Enriching speech recogni-
tion with automatic detection of sentence boundaries
and disfluencies. IEEE Trans. Audio, Speech and Lan-
guage Processing, 14(5):1526?1540.
A. Ratnaparkhi. 1998. Maximum Entropy Models for
natural language ambiguity resolution. Ph.D. thesis,
University of Pennsylvania.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Proc. of CoNNL.
A. Stolcke and E. Shriberg. 1996. Statistical language
modeling for speech disfluencies. In Proc. of ICASSP.
112
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 1?10,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Evaluating the Effectiveness of Information Presentation
in a Full End-To-End Dialogue System
Taghi Paksima
Enterprise Search Group
Microsoft
D-81669 Munich, Germany
taghi.paksima@microsoft.com
Kallirroi Georgila
Institute for Creative Technologies
University of Southern California
Marina del Rey, CA 90292, USA
kgeorgila@ict.usc.edu
Johanna D. Moore
School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
j.moore@ed.ac.uk
Abstract
Recent work on information presenta-
tion in dialogue systems combines user
modelling (UM) and stepwise refine-
ment through clustering and summarisa-
tion (SR) in the UMSR approach. An eval-
uation in which participants rated dialogue
transcripts showed that UMSR presents
complex trade-offs understandably, pro-
vides users with a good overview of their
options, and increases users? confidence
that all relevant options have been pre-
sented (Demberg and Moore, 2006). In
this paper, we evaluate the effectiveness
of the UMSR approach in a more realis-
tic setting, by incorporating this informa-
tion presentation technique into a full end-
to-end dialogue system in the city infor-
mation domain, and comparing it with the
traditional approach of presenting infor-
mation sequentially. Our results suggest
that despite complications associated with
a real dialogue system setting, the UMSR
model retains its advantages.
1 Introduction
Spoken dialogue systems (SDS) that help users
find a desired option (e.g., flight, restaurant,
movie) from the set of options satisfying their con-
straints typically present options sequentially, or-
dered along a default dimension (e.g., by price or
departure time). An example is shown in Fig. 1.
The user can then navigate through the options
and refine them by offering new constraints until
a suitable option has been found. However, when
the number of available options is large, this pro-
cess can be painstaking, leading to long dialogues
There are six restaurant options matching your query.
Number 1: Voujon offers a bright, airy and contempo-
rary dining area, with simple floral displays and leather
seating. It serves Indian cuisine. It is located in the city
centre. The average price is ?24 per person.
Number 2: Saffrani?s decor is modern, the dining room
wee, though the menu is enormous, and the atmosphere
charming. It offers new Indian dishes never before seen
in Edinburgh. It serves Indian, seafood cuisine. It is
located in the city centre. The average price is ?28 per
person.
Number 3: Britannia Spice . . .
Figure 1: Example of sequential information pre-
sentation in the city information domain (modi-
fied version of the TownInfo system (Lemon et al,
2006)).
and reduced user satisfaction. Thus a major chal-
lenge in the development of SDS is to improve
information presentation algorithms. This is im-
portant for several reasons: (1) to avoid overbur-
dening the user?s memory by presenting too many
options; (2) to ensure that the user is given an
overview of the available option space so that the
optimal option can be found; and (3) to minimise
the number of dialogue turns (hence dialogue du-
ration) required for the user to find an acceptable
option. As Walker et al (2001) showed, failing to
meet this third goal may reduce overall user satis-
faction.
Recently several approaches have been pro-
posed to overcome the shortcomings of the se-
quential enumeration strategy (Polifroni et al,
2003; Chung, 2004; Demberg and Moore, 2006;
Polifroni and Walker, 2008). Because of the com-
plexity of building a complete end-to-end SDS,
these approaches have been evaluated using an
?overhearer? methodology in which dialogues are
either hand-crafted or simulated and then pre-
sented to subjects, either as textual transcripts
1
(Demberg and Moore, 2006; Polifroni and Walker,
2008) or audio recordings (Walker et al, 2004),
for rating. The general consensus from these stud-
ies is that users significantly prefer approaches
that take their preferences into account. How-
ever, because users were not interacting with these
SDS, the evaluation criteria were limited to users?
perceptions (e.g., informativeness, good overview
of options, confidence in choice, etc.), and met-
rics such as effectiveness (i.e., actual or perceived
task completion) and efficiency (i.e., length of di-
alogue) could not be assessed. To address this
issue, Winterboer and Moore (2007) carried out
a Wizard-of-Oz (WOz) study in which users par-
ticipated in dialogues controlled by two different
information presentation algorithms. They found
that not only did users prefer presentations based
on a user model, dialogues employing the ?user-
model based summarise and refine? (UMSR) ap-
proach led to greater task success and dialogue ef-
ficiency.
In this paper, we take this one step further, and
evaluate the effectiveness of the UMSR approach
in a more realistic setting, incorporating this con-
tent selection and presentation strategy into a full
end-to-end dialogue system, and comparing it to
the traditional sequential enumeration approach.
Our results suggest that despite complications as-
sociated with a real dialogue system setting, the
UMSR model retains its advantages. Our results
also verify the hypothesis that the UMSR model
presents complex trade-offs in a concise, yet un-
derstandable way. Furthermore, as in the WOz
study, the UMSR approach leads to a significant
reduction in the number of dialogue turns.
The structure of the paper is as follows: In
Sec. 2, we discuss related work. In Sec. 3 we
present the full end-to-end SDS used for com-
parison between the standard sequential enumer-
ation approach and the UMSR approach. In Sec. 4
we describe how we implemented the UMSR ap-
proach. Then in Sec. 5 we provide an example. In
Sec. 6 we describe our experimental design and in
Sec. 7 our results. Finally in Sec. 8, we present our
conclusions.
2 Previous Approaches
As noted above, a number of approaches to in-
formation presentation in SDS have recently been
proposed. The user-model based (UM) approach
employs a model of the users preferences and de-
cision theory techniques to identify and present a
small number of options that best match the user?s
preferences (Carenini and Moore, 2001; Walker et
al., 2004; Moore et al, 2004). Fig. 2 shows a sam-
ple presentation generated using the UM approach
for a student user who cares most about price and
flying direct.
There?s a direct flight on BMI with a good price. It
arrives at four ten p.m. and costs a hundred and twelve
pounds. The cheapest flight is on Ryanair. It arrives at
twelve forty-five p.m. and costs just fifty pounds, but it
requires a connection in Dublin.
Figure 2: Example of information presentation
based on the UM approach, from (Moore et al,
2004).
Although the UM approach enables a concise
presentation of a small number of options, high-
lighting the ways in which these options satisfy
user preferences, it does not scale up to presenting
a large number of options. When there are hun-
dreds of potentially relevant options to consider
(e.g., all Italian restaurants in New York City),
there may be a large number of options that fit
the user model, making it difficult to narrow down
the number of options. In addition, users may
not be able to provide constraints until they are
presented with more details concerning the option
space. Moreover, the system does not provide an
overview of the available options, which may lead
to the user?s actual or perceived missing out on
possibly better alternatives.
An alternative approach, which we call the sum-
marise and refine approach (SR) (Polifroni et al,
2003; Chung, 2004) structures large datasets for
summarisation and successive refinement. It sup-
ports the user in narrowing in on a suitable option
by grouping the options in the database into clus-
ters of options with similar features. The system
can then summarise the common attributes of the
options and thus suggest further refinement con-
straints to the user. In this content structuring ap-
proach, the prompts presented to the user and the
order in which they appear, are determined at run-
time based on an algorithm that computes the most
useful set of attributes, as dictated by the set of op-
tions that satisfy the user query. For large datasets,
attributes that partition the data into the minimal
number of clusters are chosen, so that a concise
summary can be presented to the user to refine
(Fig. 3).
In addition to its many advantages, the SR ap-
proach also has several limitations. First, depend-
ing on the size and complexity of the domain,
many turns may be required during the refinement
2
M: What restaurants do you have?
S: I have found 983 restaurants. Most of them are
located in Boston and Cambridge. There are 32
choices for cuisine. I also have information about
price range.
M: Okay tell me about the ones in Boston.
S: I have found 401 restaurants in Boston. There are
29 choices for cuisine.
M: Do you have any that serve seafood?
S: I have found 19 seafood restaurants. They are
predominantly in Back Bay, the North End, South
Boston and the South End.
Figure 3: Sample dialogue between simulator (M)
and SR system (S), from (Polifroni et al, 2003).
process. Because the user?s preferences are not
taken into account, the clusters may contain many
irrelevant entities, which must be filtered out suc-
cessively with each refinement step. Second, if
there is no optimal solution, exploration of trade-
offs between options can be difficult. Finally, the
chosen clusters may be based on attributes that are
irrelevant for the specific user.
In an attempt to combine the benefits of the UM
and SR approaches, Demberg & Moore (2006)
devised the user-model based summarise and re-
fine (UMSR) approach to information presenta-
tion. This approach first clusters the values of each
attribute in order to group them so that the op-
tions can be summarised more easily later, and la-
bels like ?cheap?, ?moderate?, ?expensive? can be
assigned to values of continuous categories such
as ?price?. The system then structures options
into an option tree based on the ranking of at-
tributes in the user model, the options returned
from the database, and the attribute-value clus-
tering. The resulting option tree determines how
different options relate to one another, and which
ones are most attractive for the user. After the tree
structure is built, it is pruned to decide which op-
tions are compelling to the user according to the
user model. This allows the system to save time
by omitting options that are not of any potential
interest to the user. Once pruning is complete,
each branch of the tree describes a possible refine-
ment path, and thus can be used to direct dialogue
flow. Trade-offs between alternative options are
presented explicitly in order to provide the user
with a better overview of the option space. In ad-
dition, to give users confidence that they are being
presented with all of the relevant options, a brief
account of all the remaining (irrelevant) options is
also provided. For a more detailed discussion of
the UMSR approach, see (Demberg and Moore,
2006). In Sec. 4 we describe how we employed
the UMSR approach in our system.
3 The TownInfo System
The TownInfo SDS was developed as part of the
EC project TALK (Lemon et al, 2006). Users
can search for hotels, bars and restaurants in
an artificial town. The system supports two di-
alogue strategies, one hand-crafted and another
learnt using Reinforcement Learning (Henderson
et al, 2008). For the current experiment we used
the hand-crafted strategy. Natural language un-
derstanding is performed using a keyword-based
parser and natural language generation is based
on templates. The information presentation is se-
quential. An example is given in Fig. 1, taken
from the modified version of TownInfo for the cur-
rent experiment. Although the original TownInfo
system supported speech input and speech output,
here we use text input/output to make sure that our
results are not influenced by poor recognition ac-
curacy or intelligibility due to poor speech syn-
thesis. Of course, as we mention in Sec. 8, the
next step would be to perform an experiment with
speech input/output.
For our current experiment we focussed on
restaurant recommendations and the TownInfo
database had to be extended to include a much
wider range of options to provide more realistic
information presentation scenarios. The database
used in our experiments contains a total of 80
restaurants in Edinburgh, UK.
4 The UMSR Algorithm
This section briefly describes our implementation
of the UMSR algorithm; for more details see
(Demberg and Moore, 2006). Sec. 5 provides an
example for clarity.
4.1 The User Model
The user model contains the user?s ranking and
preferred values for the relevant attributes in the
restaurant domain: price, distance, star
rating, service rating, and cuisine
type. Table 1 shows a sample user model. The
Rank field indicates the relative importance of the
attributes for the user, with 1 being most impor-
tant. The Value field indicates the user?s preferred
value for each attribute.1
1If two attributes in a user model have identical ranks, the
order of the preferences is used to decide which has a higher
priority.
3
UserID Attribute Value Rank
1 Price Cheap 1.00
1 Distance Near 2.00
1 Star High 3.00
1 Cuisine Indian 4.00
1 Service Don?t Care 5.00
Table 1: Sample user model for a student.
According to Elzer et al (1994), some prefer-
ences are enough to reject options outright (and
therefore are more like goals) whereas others are
more purely like preferences (to be weighed and
ranked). Here we do not make such a distinction.
4.2 Adapting to Changes to the User Model
In the original design, the user model was cre-
ated at the outset and not modified during the dia-
logue. However, during initial piloting of the sys-
tem, we found that this design did not support ?sit-
uational preferences?. For example, consider the
user model for the student in Table 1. This user
normally prefers to have Indian food if she has the
option to (a ?dispositional preference?). If, how-
ever, in the current situation she is entertaining a
friend from out of town who wishes to try Scottish
food, the user may decide to explore options for
Scottish cuisine (a ?situational preference?). Here,
the user changes her original query for the situa-
tion, thus redefining her preferences. When this
occurs, we must perform a new database query
and rebuild the option tree. To take these dynamic
changes into account during the course of the dia-
logue, at each dialogue turn the user query is com-
pared against the user model, and if any difference
is noted, the user model is updated to reflect the
current preferences, the tree is rebuilt using the
new user model, and the dialogue continues with
a summary of the available options based on this
new tree.
Note that for individual models, i.e. user models
that are designed for individual people and not for
classes of users (student or business person), some
queries could justify situational changes and some
could indicate permanent (or at least less tempo-
rary) changes to the user model (e.g., ?Are there
any nicer restaurants? I got a new job?). In our
experiment we use only class models and we do
not allow permanent changes to the user model.
4.3 The Clustering Algorithm
Following (Polifroni et al, 2003) and (Demberg
and Moore, 2006), we used agglomerative group-
average clustering to automatically group values
for each attribute. The algorithm begins by assign-
ing each unique attribute value to its own bin, and
successively merging bins whose means are most
similar until a stopping criterion (a target of no
more than three clusters, in our implementation)
is met. The bins are then assigned predefined la-
bels, e.g., ?cheap?, ?moderately priced? and ?ex-
pensive? for price. Clustering attribute values
with this algorithm allows for database-dependent
labelling. Therefore, a restaurant with a price of
?35 might be considered as expensive for Edin-
burgh, but inexpensive for London.
4.4 Building the Option Tree
The tree building algorithm is recursive. It begins
at the root node, which contains all entities in the
retrieved dataset, and builds up the tree level by
level based on the ranking of attributes in the user
model. At each node of the tree, it retrieves the
next attribute preference from the user model and
then invokes the clustering algorithm for this at-
tribute?s values. Once the current dataset has been
clustered, the algorithm then adds the resultant
clusters as the children of the current node. Af-
ter each cluster is added, the algorithm is invoked
recursively on the newly created children of the
current node.
As the tree is being constructed, the algorithm
arranges the nodes in the tree such that the children
of each node are ordered from left to right in de-
creasing order of desirability. For example, if the
particular user prefers restaurants that are far from
the city centre, the clusters based on distance
would be ordered such that ?far? is the leftmost
child and ?near? is the rightmost child. Fig. 5 de-
picts an option tree structure for the user model of
Table 1, in the context of the example of Sec. 5.
The numbers in the nodes indicate how many op-
tions are represented by the node.
Given an option tree ordered in this way, to find
the best available options, the system traverses the
tree in a depth-first fashion starting from the root
and selecting the leftmost branch at each node.
4.5 Pruning the Option Tree
The goal of the UMSR algorithm is to present an
overview of the available options, that are most
relevant to the user?s preferences, concisely and
understandably. To determine the relevance of op-
tions, we use the notion of ?dominance? defined
in Demberg & Moore (2006). Dominant options
are those for which there is no other option in the
dataset that is better on all attributes. A domi-
4
nated option is in all respects equal to or worse
than some other option in the relevant subset of
the database; it should not be of interest for any
rational user.
The pruning algorithm follows Demberg &
Moore (2006), and thus we summarise it only
briefly here. The algorithm operates directly on
the ordered option tree, using the tree structure so
that it can efficiently determine dominance rela-
tions without having to compare each pair of op-
tions. The algorithm traverses the tree in depth-
first order, generating constraints during this pro-
cess. These constraints encode the properties that
other options would need to satisfy in order not to
be dominated by the options which have already
been deemed to be dominant. A node must ful-
fil the constraints that apply to it, otherwise it is
pruned from the tree. If an option (or a cluster of
options) satisfies a constraint, the property that sat-
isfied the constraint is marked as the options? jus-
tification. If some, but not all, of the constraints
can be satisfied by an option, the constraints are
propagated to the other nodes (see Fig. 5).
4.6 Natural Language Generation
Once a pruned option tree has been constructed,
the system can generate a presentation to the user.
The natural language generation (NLG) algorithm
includes three steps described below.
4.6.1 Identifying Trade-offs
To identify the trade-offs, the algorithm tra-
verses the tree looking for constraints that were
generated during the pruning process. For each
node that generated a constraint, the algorithm
finds the best sibling, which satisfies the con-
straint. It does this by first checking the siblings
of the current node, and if none satisfy the con-
straint, it moves up the tree and recursively tra-
verses siblings of the ancestor node. Once a trade-
off node is found, it is recorded in the option tree
at that point, and the algorithm then searches up-
ward in the tree to find the lowest common parent
of the trade-off nodes. This is the ?Differentiating
Parent? for the trade-off; the attribute that deter-
mines the children of this node indicates the trade-
off the user has to make. In the example shown
in Fig. 5, the attribute that the user must trade-
off is distance. The system then generates a
natural language description of the trade-off using
templates, e.g., ?For a restaurant that serves Indian
cuisine you would have to go farther from the city
centre?. If a constraint cannot be satisfied, the sys-
tem generates a sentence describing this situation,
e.g., ?However, there are no inexpensive restau-
rants that match your preferences?.
4.6.2 Summarisation
The summarisation process aims to present the
user with an understandable and concise overview
of the available options, so that she can make
informed decisions on how to proceed with her
query. In a SDS, it is important not to present too
much information in a single turn in order to keep
the memory load on the user manageable. Thus,
in our implementation, the system provides a sum-
mary of the first three attributes (corresponding to
the first three levels of the tree), e.g., ?I found 12
restaurants which are inexpensive. Only 7 of them
are near the city centre, out of which only 4 have
a high star rating?. If there are any trade-offs over
the first three attributes, they are also mentioned to
the user, using the algorithm described above.
4.6.3 Turn Generation
The turn generation algorithm first determines
whether the current user?s query is consistent with
the preferences defined in the user model. Since
the option tree, and consequently the generation so
far, have primarily been based on the user model,
if the algorithm notices an inconsistency it will
give priority to the most recent user query as a sit-
uational adjustment (see Sec. 4.2), and then update
the user model to reflect the adjustment. If the user
model is modified, the algorithm rebuilds the op-
tion tree, and generates the next turn using the new
user model and the most recent user query.
To ensure the user that the system has under-
stood her correctly, and to increase her confidence,
the algorithm includes a summary of all attribute
values that are common to the cluster in the option
tree that is being used to generate the summary.
For example, for the second turn in the dialogue of
Fig. 4, although the user?s most recent query was
about cuisine, the system includes a summary
of all the attributes already mentioned to the user:
?All of them are cheap, are near the city centre,
and have a high star rating?.
5 Example
To clarify things further, here we present an exam-
ple. A dialogue between the UMSR system and
a student (user model of Table 1) is depicted in
Fig. 4.
Fig. 5 depicts the tree structure for the stu-
dent user of our example. The user begins by
5
U: Tell me about the restaurants you have.
S: I found 12 restaurants which are inexpensive. Only
7 of them are near the city centre, out of which only
4 have a high star rating. I also have information
about cuisine and service quality.
U: Tell me about the cuisine.
S: There is only 1 restaurant that serves Scottish food,
and 3 restaurants that serve French food. They all
provide good service quality. All of them are cheap,
are near the city centre, and have a high star rating.
For a restaurant that serves Indian cuisine you
would have to go farther from the city centre.
U: Okay, tell me about the Scottish one.
S: The Jam House?s menu includes Scottish dishes
sparked with international flavour, with a decent
choice of vegetarian dishes. It is located in the
centre. The average price is ?22 per person.
U: What about Indian restaurants?
S: I found 2 restaurants that serve Indian food.
Both of them are cheap, within average distance
from the city centre, and have a high star rating.
Only one of them provides good service quality.
U: Okay, tell me about this one.
S: The Raj on the Shore?s theme extends from the
soldier figurines behind the exotic blue bar to the
predominantly Bangladeshi, Indian menu. It serves
Indian and Bangladeshi cuisine. It is located
in the north. The average price is ?20 per person.
Figure 4: Example dialogue with our UMSR ap-
proach (student user of Table 1).
asking generally for a restaurant recommendation,
and the system provides a summary based on the
three top-ranking attributes for this user: price,
distance and star rating, which corre-
spond to the first three levels of the option tree.
Next the user asks about cuisine type, which
generates a trade-off since there are no Indian
restaurants (user?s preference) that are cheap, near
the city centre, and of high star rating. The user
then asks about the Scottish option, before switch-
ing back to her preferred cuisine type (Indian). Be-
cause Indian cuisine was in the user?s initial pref-
erence model, a constraint of cuisine=Indian
was generated when traversing the leftmost branch
of the tree, and this justified not pruning the un-
shaded nodes in the right subtree of Fig. 5, in or-
der to generate the trade-off. However, if the user
had asked about expensive restaurants, then a new
database query would have been made and a new
option tree would have been built. A more com-
plex example is given in the Appendix.
6 Experimental Design
In total 18 subjects interacted with our two sys-
tems. Each participant interacted three times with
the modified TownInfo system, and another three
times with the system that supported our imple-
mentation of the UMSR model (108 dialogues in
Figure 5: A sample option tree structure for the
student user of Table 1. Pruned nodes are shown
as shaded.
total). The order of the dialogues was randomised
among the subjects. Each experiment took be-
tween 40 and 50 minutes on average.
For each task, subjects were provided with the
user profile and the actual scenario for the spe-
cific task in hand. The tasks were carefully con-
structed so that half of them could be solved with-
out making any trade-offs and the other half re-
quired a trade-off to be made. At the end of each
task the subjects had to fill out a questionnaire with
10 questions on a 7-point Likert scale. They were
also asked if they had been able to accomplish the
given task (perceived task completion), i.e., to find
a suitable restaurant for the scenario and user pro-
file in hand. Finally, after each task they had to
provide the name(s) of the restaurants they chose
for the task. The name(s) stated for this task were
then used to compare perceived task completion
with actual task completion. At the end of each
task with the UMSR system, the profiles were re-
set to the default attribute values and ranks.
Both systems had identical software configura-
tions, i.e., they only differed in the information
presentation component. Yet another important
feature was that the UMSR based model did not
accept multiple attributes in a single query. So
for instance the user could not ask ?I am look-
ing for a moderately priced restaurant near the city
centre that serves Italian food?. This seemed to
be a major shortcoming of the UMSR based sys-
tem compared to the TownInfo system with se-
quential information presentation. However, as we
will see in the following, even with this shortcom-
6
System U CC CF A E
UMSR-all 5.04 4.65 3.22 3.66 4.69
TownInfo-all 4.87 4.04 2.93 3.20 3.59
UMSR-with TO 4.74 4.59 2.67 3.26 4.15
TownInfo-with TO 4.59 3.41 2.74 2.33 2.70
UMSR-no TO 5.33 4.70 3.78 4.08 5.22
TownInfo-no TO 5.15 4.67 3.11 4.07 4.48
Table 2: Average scores of the question-
naires for all dialogues, dialogues with trade-
offs (with TO) and dialogues without trade-offs
(no TO) (U=understandability, CC=conciseness,
CF=confidence, A=accessibility, E=efficiency).
ing the UMSR approach retained its advantages
and proved more successful than the traditional se-
quential enumeration approach.
7 Results
The perceived task completion (PTC) for the
UMSR system and the TownInfo system was
90.74% and 85.19% respectively, and the actual
task completion (ATC) 74.07% and 62.96%. Thus
the UMSR approach led to a relatively better user
confidence in having achieved the task.
The average number of turns was 9.24 for
UMSR compared to 17.78 for TownInfo, which
denotes a significant reduction in the number of
dialogue turns required to accomplish a given
task. This reduction becomes even more promi-
nent when there is a trade-off involved. With such
dialogues, the average number of turns for UMSR
remained almost constant at 9.41, whereas Town-
Info showed an increase reaching up to 24.19.
This huge difference is obviously a significant
improvement in system efficiency and user sat-
isfaction. It also supports our hypothesis that
the UMSR approach can present trade-offs under-
standably. For dialogues without a trade-off the
number of turns was 9.07 for UMSR and 11.37
for TownInfo.
Dialogue duration also showed a great improve-
ment in UMSR over TownInfo (4:49 (m:s) vs.
6:11). The duration however was almost the same
for the two systems when a trade-off existed (4:40
vs. 4:49). This could mean that although the num-
ber of turns in this case is smaller for UMSR, the
length of the generated output is longer, and re-
quires more attention to understand. Yet again in
dialogues without a trade-off, UMSR had a con-
siderably shorter duration than TownInfo (4:57 vs.
7:34).
Average scores of the questionnaires are given
in Table 2.
In response to the question ?I thought the way
the system provided information to me was easy
to understand? the average score over all 108 di-
alogues was 5.04 for UMSR and 4.87 for Town-
Info. The preference for UMSR exists for dia-
logues both with and without a trade-off. How-
ever, for all three cases the differences were not
significant (p > 0.05).
Conciseness is the quality of providing a con-
cise overview of all the available options to the
user. The UMSR system was preferred at 4.65
over 4.04 for TownInfo (p = 0.034). The differ-
ence between the two systems is very significant
for dialogues with a trade-off (p < 0.003). How-
ever, for dialogues without a trade-off p = 0.92.
This was predictable as the main innovation in
UMSR is the ability to present trade-offs in a con-
cise and understandable way, hence the significant
difference for the dialogues with trade-offs.
To evaluate their confidence in having heard all
the relevant options, the subjects were asked to
rate the statement ?I thought there were better op-
tions for my request than what the system gave
me?. Because of the negative nature of the ques-
tion, the Likert scale was inverted before analysis.
The average score was 3.22 and 2.93 for UMSR
and TownInfo respectively. This indicates that
the users have slightly more confidence in hav-
ing heard all the relevant options with the UMSR
system, although this difference is not significant
(p > 0.05). For dialogues with a trade-off, the
average confidence score was slightly better for
TownInfo (2.74 vs. 2.67), but not significant (p =
0.8). However, there is a significant difference for
dialogues without a trade-off (p < 0.03). An-
other notable issue is the overall low scores for the
cases with a trade-off. This signifies that perhaps
more information needs to be given to the user
for dialogue turns describing a trade-off. A care-
ful balance needs to be drawn between concise-
ness and comprehensiveness in these cases. This
however, will obviously increase dialogue dura-
tion, and might affect understandability.
By accessibility, we mean ease of use and
communication with the system. The scores for
UMSR and TownInfo were 3.66 and 3.20 respec-
tively (p = 0.18). A more significant difference
in accessibility was noted for dialogues with a
trade-off (p = 0.008). Again it seemed that users
preferred UMSR when it came down to dealing
with trade-offs. However, the accessibility scores
for dialogues without a trade-off were almost the
same (p = 0.92).
7
Efficiency is the quality of enabling users to
find the optimal option quickly. The statement
?In this task, the system allowed me to find the
optimal restaurant quickly?, resulted in an aver-
age score of 4.69 for UMSR vs. 3.59 for Town-
Info (p = 0.002). Once again, a significant dif-
ference was noted for dialogues with a trade-off,
with 4.15 and 2.70 for UMSR and TownInfo re-
spectively (p = 0.004). However, the difference
for dialogues without a trade-off was not signifi-
cant (p = 0.12).
8 Conclusions and Future Work
In this paper, we evaluated the effectiveness of the
UMSR approach in information presentation in a
full end-to-end dialogue system. The UMSR ap-
proach was compared with the traditional sequen-
tial enumeration of options. Our results verified
our hypothesis that the UMSR approach presents a
better overview of the trade-offs within the option
space, and improves user experience and confi-
dence in the system. Furthermore, with the UMSR
approach there is a significant reduction in the
number of dialogue turns required to complete the
task. The results also showed that UMSR specifi-
cally outperforms TownInfo when there is a trade-
off involved. The UMSR results presented statisti-
cally significant improvement for conciseness, ac-
cessibility, and efficiency. Overall, subjects were
more satisfied with the UMSR system. When they
were asked if they would use the system again as
a deployed product the score was 4.74 for UMSR
and 3.70 for TownInfo (p = 0.002), further veri-
fying that the users preferred the UMSR approach
over the sequential enumeration of TownInfo.
In future work we intend to make a number of
improvements. For example in the turn genera-
tion algorithm, we will optimise the generated out-
put in an effort to strike a balance between un-
derstandability and complexity. Another impor-
tant issue is to modify the UMSR algorithm so that
it can accept multiple attributes in a single query.
Moreover, we will perform experiments with both
speech input and output. Finally, we will com-
pare the UMSR approach with the UM and SR ap-
proaches in the same setting, i.e., a full end-to-end
SDS.
Acknowledgements
This paper is based on a research experiment con-
ducted at the University of Edinburgh. Paksima
was funded by the European Commission Eras-
mus Mundus scholarship program. Georgila was
partially funded by the Wellcome Trust VIP Award
and is currently funded by the U.S. Army Re-
search, Development, and Engineering Command
(RDECOM). The content does not necessarily re-
flect the position or the policy of the U.S. Gov-
ernment, and no official endorsement should be
inferred. The authors thank the three anonymous
reviewers.
References
G. Carenini and J.D. Moore. 2001. An empirical study
of the influence of user tailoring on evaluative argu-
ment effectiveness. In Proc. of IJCAI.
G. Chung. 2004. Developing a flexible spoken dialog
system using simulation. In Proc. of ACL.
V. Demberg and J.D. Moore. 2006. Information pre-
sentation in spoken dialogue systems. In Proc. of
EACL.
S. Elzer, J. Chu-Carroll, and S. Carberry. 1994. Recog-
nizing and utilizing user preferences in collaborative
consultation dialogues. In Proc. of UM ?94.
J. Henderson, O. Lemon, and K. Georgila. 2008. Hy-
brid reinforcement/supervised learning of dialogue
policies from fixed datasets. Computational Lin-
guistics, 34(4):487?511.
O. Lemon, K. Georgila, J. Henderson, and M. Stut-
tle. 2006. An isu dialogue system exhibiting re-
inforcement learning of dialogue policies: generic
slot-filling in the talk in-car system. In Proc. of
EACL.
J.D. Moore, M.E. Foster, O. Lemon, and M. White.
2004. Generating tailored, comparative descriptions
in spoken dialogue. In Proc. of FLAIRS.
J. Polifroni and M. Walker. 2008. Intensional sum-
maries as cooperative responses in dialogue: Au-
tomation and evaluation. In Proc. of ACL?HLT.
J. Polifroni, G. Chung, and M. Walker. 2003. To-
wards automatic generation of mixed-initiative di-
alogue systems from web content. In Proc. of Eu-
rospeech.
M.A. Walker, R.J. Passonneau, and J.E. Boland. 2001.
Quantitative and qualitative evaluation of darpa
communicator spoken dialogue systems. In Proc.
of ACL.
M. Walker, S. Whittaker, A. Stent, P. Maloor, J.D.
Moore, M. Johnston, and G. Vasireddy. 2004. Gen-
eration and evaluation of user tailored responses in
dialogue. Cognitive Science, 28:811?840.
A. Winterboer and J.D. Moore. 2007. Evaluating in-
formation presentation strategies for spoken recom-
mendations. In RecSys ?07: Proc. of the 2007 ACM
conference on Recommender systems.
8
Appendix
Option Tree A for User Model A is built.
U: I?d like to get some food tonight.
S: I found 17 restaurants with a high star rating. Only 8 of them provide good service quality, out of which
only 3 serve Scottish food. I also have information about distance and price.
U: Are they close to the city centre?
S: I found 3 restaurants that are located far from the city centre. 1 of them is cheap and 2 are expensive.
They all have a high star rating, provide good service quality, and serve Scottish food. For a restaurant near
the city centre you would have to go for average service quality.
U: How about French cuisine?
The System cannot answer the query because the ?French?cuisine branches of Option Tree A have been
pruned and their nodes are shown as shaded.
User Model A changes to User Model B (situational query).
Option Tree B for User Model B is built.
S: I found 10 restaurants that serve French food. Only 5 of them have a high star rating, out of which only 3
provide good service quality. I also have information about distance and price.
U: Tell me about the distance from the city centre.
S: Only 1 of them is located near the city centre and is inexpensive. It serves French food, has a high star rating,
and provides good service quality.
U: Tell me about it.
S: La Bagatelle?s dishes include a starter of roast pigeon with canelloni of shiitake mushroom, and a main course
of panfried veal with prunes.
U: That?s great. I?ll take it.
. . .
Figure 6: Example dialogue with our UMSR approach for a business person, User Model A (Table 3)
and User Model B (Table 4), Option Tree A (Fig. 7) and Option Tree B (Fig. 8).
UserID Attribute Value Rank
2 Star High 1.00
2 Service Good 2.00
2 Cuisine Scottish 3.00
2 Distance Near 4.00
2 Price Don?t Care 5.00
Table 3: Sample user model for a business person (User Model A).
UserID Attribute Value Rank
2 Cuisine French 0.95
2 Star High 1.00
2 Service Good 2.00
2 Distance Near 4.00
2 Price Don?t Care 5.00
Table 4: Modified user model for a business person after the situational query ?I?d like a French restau-
rant? (User Model B).
9
Figure 7: Option tree structure (Option Tree A) corresponding to the User Model A of Table 3. Pruned
nodes are shown as shaded.
Figure 8: Option tree structure (Option Tree B) corresponding to the User Model B of Table 4. Pruned
nodes are shown as shaded.
10
An ISU Dialogue System Exhibiting Reinforcement Learning of Dialogue
Policies: Generic Slot-filling in the TALK In-car System
Oliver Lemon, Kallirroi Georgila, and James Henderson
School of Informatics
University of Edinburgh
olemon@inf.ed.ac.uk
Matthew Stuttle
Dept. of Engineering
University of Cambridge
mns25@cam.ac.uk
Abstract
We demonstrate a multimodal dialogue system
using reinforcement learning for in-car sce-
narios, developed at Edinburgh University and
Cambridge University for the TALK project1.
This prototype is the first ?Information State
Update? (ISU) dialogue system to exhibit rein-
forcement learning of dialogue strategies, and
also has a fragmentary clarification feature.
This paper describes the main components and
functionality of the system, as well as the pur-
poses and future use of the system, and surveys
the research issues involved in its construction.
Evaluation of this system (i.e. comparing the
baseline system with handcoded vs. learnt dia-
logue policies) is ongoing, and the demonstra-
tion will show both.
1 Introduction
The in-car system described below has been con-
structed primarily in order to be able to collect data
for Reinforcement Learning (RL) approaches to mul-
timodal dialogue management, and also to test and fur-
ther develop learnt dialogue strategies in a realistic ap-
plication scenario. For these reasons we have built a
system which:
  contains an interface to a dialogue strategy learner
module,
  covers a realistic domain of useful ?in-car? con-
versation and a wide range of dialogue phenom-
ena (e.g. confirmation, initiative, clarification, in-
formation presentation),
  can be used to complete measurable tasks (i.e.
there is a measure of successful and unsuccessful
dialogues usable as a reward signal for Reinforce-
ment Learning),
  logs all interactions in the TALK data collection
format (Georgila et al, 2005).
1This research is supported by the TALK project (Euro-
pean Community IST project no. 507802), http://www.talk-
project.org
In this demonstration we will exhibit the software
system that we have developed to meet these require-
ments. First we describe the domain in which the di-
alogue system operates (an ?in-car? information sys-
tem). Then we describe the major components of the
system and give examples of their use. We then discuss
the important features of the system in respect to the
dialogue phenomena that they support.
1.1 A System Exhibiting Reinforcement Learning
The central motivation for building this dialogue sys-
tem is as a platform for Reinforcement Learning (RL)
experiments. The system exhibits RL in 2 ways:
  It can be run in online learning mode with real
users. Here the RL agent is able to learn from suc-
cessful and unsuccessful dialogueswith real users.
Learning will be much slower than with simulated
users, but can start from an already learnt policy,
and slowly improve upon that.
  It can be run using an already learnt policy (e.g.
the one reported in (Henderson et al, 2005;
Lemon et al, 2005), learnt from COMMUNICA-
TOR data (Georgila et al, 2005)). This mode can
be used to test the learnt policies in interactions
with real users.
Please see (Henderson et al, 2005) for an expla-
nation of the techniques developed for Reinforcement
Learning with ISU dialogue systems.
2 System Overview
The baseline dialogue system is built around the DIP-
PER dialogue manager (Bos et al, 2003). This sys-
tem is initially used to conduct information-seeking di-
alogues with a user (e.g. find a particular hotel and
restaurant), using hand-coded dialogue strategies (e.g.
always use implicit confirmation, except when ASR
confidence is below 50%, then use explicit confirma-
tion). We have then modified the DIPPER dialogue
manager so that it can consult learnt strategies (for ex-
ample strategies learnt from the 2000 and 2001 COM-
MUNICATOR data (Lemon et al, 2005)), based on its
119
current information state, and then execute dialogue ac-
tions from those strategies. This allows us to compare
hand-coded against learnt strategies within the same
system (i.e. the other components such as the speech-
synthesiser, recogniser, GUI, etc. all remain fixed).
2.1 Overview of System Features
The following features are currently implemented:
  use of Reinforcement Learning policies or dia-
logue plans,
  multiple tasks: information seeking for hotels,
bars, and restaurants,
  overanswering/ question accommodation/ user-
initiative,
  open speech recognition using n-grams,
  confirmations - explicit and implicit based on
ASR confidence,
  fragmentary clarifications based on word confi-
dence scores,
  multimodal output - highlighting and naming en-
tities on GUI,
  simple user commands (e.g. ?Show me all the in-
dian restaurants?),
  dialogue context logging in ISU format (Georgila
et al, 2005).
3 Research Issues
The work presented here explores a number of research
themes, in particular: using learnt dialogue policies,
learning dialogue policies in online interaction with
users, fragmentary clarification, and reconfigurability.
3.1 Moving between Domains:
COMMUNICATOR and In-car Dialogues
The learnt policies in (Henderson et al, 2005) focussed
on the COMMUNICATOR system for flight-booking di-
alogues. There we reported learning a promising initial
policy for COMMUNICATOR dialogues, but the issue
arises of how we could transfer this policy to new do-
mains ? for example the in-car domain.
In the in-car scenarios the genre of ?information
seeking? is central. For example the SACTI corpora
(Stuttle et al, 2004) have driver information requests
(e.g. searching for hotels) as a major component.
One question we address here is to what extent di-
alogue policies learnt from data gathered for one sys-
tem, or family of systems, can be re-used or adapted
for use in other systems. We conjecture that the slot-
filling policies learnt from our experiments with COM-
MUNICATOR will also be good policies for other slot-
filling tasks ? that is, that we are learning ?generic?
slot-filling or information seeking dialogue policies. In
section 5 we describe how the dialogue policies learnt
for slot filling on the COMMUNICATOR data set can be
abstracted and used in the in-car scenarios.
3.2 Fragmentary Clarifications
Another research issue we have been able to explore
in constructing this system is the issue of generating
fragmentary clarifications. The system can be run with
this feature switched on or off (off for comparison with
COMMUNICATOR systems). Instead of a system sim-
ply saying ?Sorry, please repeat that? or some such sim-
ilar simple clarification request when there is a speech
recognition failure, we were able to use the word con-
fidence scores output by the ATK speech recogniser to
generate more intelligent fragmentary clarification re-
quests such as ?Did you say a cheap chinese restau-
rant??. This works by obtaining an ASR confidence
score for each recognised word. We are then able to
try various techniques for clarifying the user utterance.
Many possibilities arise, for example: explicitly clarify
only the highest scoring content word below the rejec-
tion threshold, or, implicitly clarify all content words
and explicitly clarify the lowest scoring content word.
The current platform enables us to test alternative
strategies, and develop more complex ones.
4 The ?In-car? Scenario
The scenario we have designed the system to cover is
that of information seeking about a town, for example
its hotels, restaurants, and bars. We imagine a driver
who is travelling towards this town, or is already there,
who wishes to accomplish relatively complex tasks,
such as finding an italian restaurant near their hotel,
or finding all the wine bars in town, and so on. The
driver/user should be able to specify queries using nat-
ural dialogue, and will receive system output that is a
mixture of spoken and graphical information (e.g. a de-
scription of an item and a map showing its location).
The example town is taken from the (Stuttle et al,
2004) corpus collection materials, and contains a num-
ber of hotels, bars, restaurants, and tourist attractions.
The user should be able to get information on a range
of locations in the town, and the dialogue system will
be used to specify and refine the user queries, as well
as to present information to the user. See the example
dialogue in table 1.
We now describe the dialogue system components.
5 Component-level Description
This section describes the components of the baseline
in-car dialogue system. Communication between com-
ponents is handled by OAA?s asynchronous hub archi-
tecture (Cheyer and Martin, 2001). The major compo-
nents are:
  DIPPER (Bos et al, 2003) ISU dialogue manager,
  Dialogue Policy Reinforcement Learner Agent2,
  ATK for speech recognition,
  Festival speech synthesiser,
2This is written in Python and has an OAA wrapper in C.
120
Utterance Feature
S How can I help you? Open initiative ini-
tial question
U I?m looking for a cheap chinese
restaurant.
User initiative
S Would you like something cen-
tral?
Intention recogni-
tion
U Yes.
S The Dragon Way is an af-
fordable cantonese restaurant
located on Main Street [dis-
played on map].
Multimodal presen-
tation
S Are you happy with those op-
tions?
U Yes, and I also want a double
room.
S Would you like something ex-
pensive, cheap, or reasonably
priced?
Intention recogni-
tion
U A luxury hotel, in the center of
town.
S There are 2 hotel options meet-
ing your requirements. The
Royal Hotel [displayed on
map] is a renovated nineteenth
century palace     The Hotel
Primus [displayed on map] has
big, bright, clean rooms    
Which hotel option would you
like?
Multimodal presen-
tation
U The second one please.
S The phone number of the Hotel
Primus is 2094-227.
S Would you like any further in-
formation?
U I want to find a jazz bar.
S I?m sorry, what kind of bar are
you looking for?
Intention recogni-
tion, Fragmentary
clarification
U A jazz bar.
S A jazz bar. Would you like
something central?
Implicit confirma-
tion
Table 1: Example dialogue, showing system features
  Multimodal Map interface (a java OAA agent),
  Database agent (java OAA wrapper to MySQL).
5.1 Dialogue Policy Learner Agent
This agent acts as an interface between the DIPPER
dialogue manager and the system simulation based on
RL. In particular it has the following solvable:
callRLsimulation(IS file name,
conversational domain, speech act, task,
result).
The first argument is the name of the file that contains
all information about the current information state,
which is required by the RL algorithm to produce
an action. The action returned by the RL agent is
a combination of conversational domain,
speech act, and task. The last argument shows
whether the learnt policy will continue to produce
more actions or release the turn. When run in online
learning mode the agent not only produces an action
when supplied with a state, but at the end of every
dialogue it uses the reward signal to update its learnt
policy. The reward signal is defined in the RL agent,
and is currently a linear combination of task success
metrics combined with a fixed penalty for dialogue
length (see (Henderson et al, 2005)).
This agent can be called whenever the system has
to decide on the next dialogue move. In the original
hand-coded system this decision is made by way of a
dialogue plan (using the ?deliberate? solvable). The
RL agent can be used to drive the entire dialogue pol-
icy, or can be called only in certain circumstances. This
makes it usable for whole dialogue strategies, but also,
if desired, it can be targetted only on specific dialogue
management decisions (e.g. implicit vs. explicit confir-
mation, as was done by (Litman et al, 2000)).
One important research issue is that of tranferring
learnt strategies between domains. We learnt a strat-
egy for the COMMUNICATOR flight booking dialogues
(Lemon et al, 2005; Henderson et al, 2005), but
this is generated by rather different scenarios than the
in-car dialogues. However, both are ?slot-filling? or
information-seeking applications. We defined a map-
ping (described below) between the states and actions
of both systems, in order to construct an interface be-
tween the learnt policies for COMMUNICATOR and the
in-car baseline system.
5.2 Mapping between COMMUNICATOR and
the In-car Domains
There are 2 main problems to be dealt with here:
  mapping between in-car system information states
and COMMUNICATOR information states,
  mapping between learnt COMMUNICATOR sys-
tem actions and in-car system actions.
The learnt COMMUNICATOR policy tells us, based
on a current IS, what the optimal system action
is (for example request info(dest city) or
acknowledgement). Obviously, in the in-car sce-
nario we have no use for task types such as ?destina-
tion city? and ?departure date?. Our method therefore
is to abstract away from the particular details of the
task type, but to maintain the information about dia-
loguemoves and the slot numbers that are under discus-
sion. That is, we construe the learnt COMMUNICATOR
policy as a policy concerning how to fill up to 4 (or-
dered) informational slots, and then access a database
and present results to the user. We also note that some
slots are more essential than others. For example, in
COMMUNICATOR it is essential to have a destination
city, otherwise no results can be found for the user.
Likewise, for the in-car tasks, we consider the food-
type, bar-type, and hotel-location to be more important
to fill than the other slots. This suggests a partial order-
ing on slots via their importance for an application.
In order to do this we define the mappings shown
in table 2 between COMMUNICATOR dialogue actions
and in-car dialogue actions, for each sub-task type of
the in-car system.
121
COMMUNICATOR action In-car action
dest-city food-type
depart-date food-price
depart-time food-location
dest-city hotel-location
depart-date room-type
depart-time hotel-price
dest-city bar-type
depart-date bar-price
depart-time bar-location
Table 2: Action mappings
Note that we treat each of the 3 in-car sub-tasks (ho-
tels, restaurants, bars) as a separate slot-filling dialogue
thread, governed by COMMUNICATOR actions. This
means that the very top level of the dialogue (?How
may I help you?) is not governed by the learnt policy.
Only when we are in a recognised task do we ask the
COMMUNICATOR policy for the next action. Since the
COMMUNICATOR policy is learnt for 4 slots, we ?pre-
fill? a slot3 in the IS when we send it to the Dialogue
Policy Learner Agent in order to retrieve an action.
As for the state mappings, these follow the same
principles. That is, we abstract from the in-car states to
form states that are usable by COMMUNICATOR . This
means that, for example, an in-car state where food-
type and food-price are filled with high confidence is
mapped to a COMMUNICATOR state where dest-city
and depart-date are filled with high confidence, and
all other state information is identical (modulo the task
names). Note that in a future version of the in-car sys-
tem where task switching is allowed we will have to
maintain a separate view of the state for each task.
In terms of the integration of the learnt policies with
the DIPPER system update rules, we have a system flag
which states whether or not to use a learnt policy. If
this flag is present, a different update rule fires when
the system determines what action to take next. For
example, instead of using the deliberate predicate
to access a dialogue plan, we instead call the Dialogue
Policy Learner Agent via OAA, using the current Infor-
mation State of the system. This will return a dialogue
action to the DIPPER update rule.
In current work we are evaluating howwell the learnt
policies work for real users of the in-car system.
6 Conclusions and Future Work
This report has described work done in the TALK
project in building a software prototype baseline ?In-
formation State Update? (ISU)-based dialogue system
in the in-car domain, with the ability to use dialogue
policies derived from machine learning and also to per-
form online learning through interaction. We described
the scenarios, gave a component level description of
the software, and a feature level description and exam-
3We choose ?orig city? because it is the least important
and is already filled at the start of many COMMUNICATOR
dialogues.
ple dialogue.
Evaluation of this system (i.e. comparing the sys-
tem with hand-coded vs. learnt dialogue policies) is
ongoing. Initial evaluation of learnt dialogue policies
(Lemon et al, 2005; Henderson et al, 2005) suggests
that the learnt policy performs at least as well as a rea-
sonable hand-coded system (the TALK policy learnt for
COMMUNICATOR dialogue management outperforms
all the individual hand-coded COMMUNICATOR sys-
tems).
The main achievements made in designing and con-
structing this baseline system have been:
  Combining learnt dialogue policies with an ISU
dialogue manager. This has been done for online
learning, as well as for strategies learnt offline.
  Mapping learnt policies between domains, i.e.
mapping Information States and system actions
between DARPA COMMUNICATOR and in-car in-
formation seeking tasks.
  Fragmentary clarification strategies: the combina-
tion of ATK word confidence scoring with ISU-
based dialogue management rules allows us to ex-
plore word-based clarification techniques.
References
J. Bos, E. Klein, O. Lemon, and T. Oka. 2003.
DIPPER: Description and Formalisation of an
Information-State Update Dialogue System Archi-
tecture. In 4th SIGdial Workshop on Discourse and
Dialogue, Sapporo.
A. Cheyer and D. Martin. 2001. The open agent archi-
tecture. Journal of Autonomous Agents and Multi-
Agent Systems, 4(1):143?148.
K. Georgila, O. Lemon, and J. Henderson. 2005. Au-
tomatic annotation of COMMUNICATOR dialogue
data for learning dialogue strategies and user sim-
ulations. In Ninth Workshop on the Semantics and
Pragmatics of Dialogue (SEMDIAL), DIALOR?05.
J. Henderson, O. Lemon, and K. Georgila. 2005.
Hybrid Reinforcement/Supervised Learning for Di-
alogue Policies from COMMUNICATOR data. In
IJCAI workshop on Knowledge and Reasoning in
Practical Dialogue Systems.
O. Lemon, K. Georgila, J. Henderson, M. Gabsdil,
I. Meza-Ruiz, and S. Young. 2005. D4.1: Inte-
gration of Learning and Adaptivity with the ISU ap-
proach. Technical report, TALK Project.
D. Litman, M. Kearns, S. Singh, and M. Walker. 2000.
Automatic optimization of dialoguemanagement. In
Proc. COLING.
M. Stuttle, J. Williams, and S. Young. 2004. A frame-
work for dialog systems data collection using a sim-
ulated ASR channel. In ICSLP 2004, Jeju, Korea.
122
Hybrid Reinforcement/Supervised Learning
of Dialogue Policies from Fixed Data Sets
James Henderson?
University of Geneva
Oliver Lemon??
University of Edinburgh
Kallirroi Georgila??
University of Edinburgh
We propose a method for learning dialogue management policies from a fixed data set. The method
addresses the challenges posed by Information State Update (ISU)-based dialogue systems, which
represent the state of a dialogue as a large set of features, resulting in a very large state space
and a huge policy space. To address the problem that any fixed data set will only provide
information about small portions of these state and policy spaces, we propose a hybrid model that
combines reinforcement learning with supervised learning. The reinforcement learning is used
to optimize a measure of dialogue reward, while the supervised learning is used to restrict the
learned policy to the portions of these spaces for which we have data. We also use linear function
approximation to address the need to generalize from a fixed amount of data to large state spaces.
To demonstrate the effectiveness of this method on this challenging task, we trained this model
on the COMMUNICATOR corpus, to which we have added annotations for user actions and In-
formation States. When tested with a user simulation trained on a different part of the same data
set, our hybrid model outperforms a pure supervised learning model and a pure reinforcement
learning model. It also outperforms the hand-crafted systems on the COMMUNICATOR data,
according to automatic evaluation measures, improving over the average COMMUNICATOR
system policy by 10%. The proposed method will improve techniques for bootstrapping and
automatic optimization of dialogue management policies from limited initial data sets.
1. Introduction
In the practical development of dialogue systems it is often the case that an initial
corpus of task-oriented dialogues is collected, either using ?Wizard of Oz? methods
or a prototype system deployment. This data is usually used to motivate and inspire
a new hand-built dialogue system or to modify an existing one. However, given the
? Universite? de Gene`ve, De?partement d?Informatique, Battelle-ba?timent A, 7 route de Drize, 1227 Carouge,
Switzerland. E-mail: james.henderson@cui.unige.ch.
?? University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: {olemon, kgeorgil}@
inf.ed.ac.uk.
Submission received: 18 November 2005; revised submission received: 18 October 2006; accepted for
publication: 21 September 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 4
existence of such data, it should be possible to exploit machine learning methods to
automatically build and optimize a new dialogue system. This objective poses two
questions: what machine learning methods are effective for this problem? and how
can we encode the task in a way which is appropriate for these methods? For the
latter challenge, we exploit the Information State Update (ISU) approach to dialogue
systems (Bohlin et al 1999; Larsson and Traum 2000), which provides the kind of rich
and flexible feature-based representations of context that are used with many recent
machine learning methods, including the linear function approximation method we
use here. For the former challenge, we propose a novel hybrid method that combines
reinforcement learning (RL) with supervised learning (SL).
The focus of this article is to establish effective methods for using fixed corpora
of dialogues to automatically optimize complex dialogue systems. To avoid the need
for extensive hand-crafting, we allow rich representations of context that include all
the features that might be relevant to dialogue management decisions, and we allow
a broad set of dialogue management decisions with very few constraints on when a
decision is applicable. This flexibility simplifies system design, but it leads to a huge
space of possible dialogue management policies, which poses severe difficulties for
existing approaches to machine learning for dialogue systems (see Section 1.1). Our pro-
posed method addresses these difficulties without the use of user simulations, feature
engineering, or further data collections.
We demonstrate the effectiveness of the proposed method on the COMMUNICA-
TOR corpora of flight-booking dialogues. Our method (?hybrid learning? with linear
function approximation) can learn dialogue strategies that are better than those learned
by standard learning methods, and that are better than the (in this case hand-coded)
strategies present in the original corpora, according to a variety of metrics. To evaluate
learned strategies we run them with simulated users that are also trained on (differ-
ent parts of) the COMMUNICATOR corpora, and automatically score the simulated
dialogues based on how many information ?slots? they manage to collect from users
(?filled slots?), whether those slots were confirmed (?confirmed slots?), and how many
dialogue turns were required to do so. Later work has shown these metrics to correlate
strongly with task completion for real users of the different policies (Lemon, Georgila,
and Henderson 2006).
The main contributions of the work are therefore in empirically demonstrating that:
 limited initial data sets can be used to train complex dialogue policies,
using a novel combination of supervised and reinforcement learning; and
 large, feature-based representations of dialogue context can be used
in tractable learning of dialogue policies, using linear function
approximation.
In this article, after a discussion of related work, we outline the annotations we have
added to the COMMUNICATOR data, then present the proposed learning method, and
describe our evaluation method. Finally, we present the evaluation results and discuss
their implications.
1.1 Related Work
As in previous work on learning for dialogue systems, in this article we focus on
learning dialogue management policies. Formally, a dialogue management policy is a
488
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
mapping from a dialogue context (a.k.a. a state) to an action that the system should
take in that context. Because most previous work on dialogue systems has been done in
the context of hand-crafted systems, we use representations of the dialogue context and
the action set based on previous work on hand-crafted dialogue systems. Our main
novel contribution is in the area of learning, where we build on previous work on
automatically learning dialogue management policies, discussed subsequently.
The ISU approach to dialogue (Bohlin et al 1999; Larsson and Traum 2000) employs
rich representations of dialogue context for flexible dialogue management. Information
States are feature structures intended to record all the information about the preceding
portion of the dialogue that is relevant to making dialogue management decisions. An
example of some of the types of information recorded in our Information States is shown
in Figure 1, including filled slots, confirmed slots, and previous speech acts. Previous
work has raised the question of whether dialogue management policies can be learned
(Levin and Pieraccini 1997) for systems that have only a limited view of the dialogue
context, for example, not including prior speech act history (see the following).
One prominent representation of the set of possible system actions is the DATE
scheme (Walker and Passonneau 2001). In particular, this representation is used in the
COMMUNICATOR corpus annotation (Walker, Passonneau, and Boland 2001), discussed
herein. The DATE scheme classifies system actions in terms of their Conversational
Domain, Speech Act, and Task. For example, one possible system action is ?about task,
Figure 1
Example fields from an Information State annotation. User-provided information is in
square brackets.
489
Computational Linguistics Volume 34, Number 4
request info, dest city?, which corresponds to a system utterance such as What is
your destination city? The specific instantiation of this scheme, and our extensions to it,
are discussed in Section 1.2.
Machine-learning approaches to dialogue management attempt to learn optimal
dialogue policies from corpora of simulated or real dialogues, or by generating such
data during automatic trial-and-error exploration of possible policies. Automatic op-
timization is desirable because of the high cost of developing and maintaining hand-
coded dialogue managers, and because there is no guarantee that hand-coded dialogue
management strategies are good. Several research groups have developed reinforce-
ment learning approaches to dialogue management, starting with Levin and Pieraccini
(1997) and Walker, Fromer, and Narayanan (1998). Previous work has been restricted
to limited dialogue context representations and limited sets of actions to choose among
(Walker, Fromer, and Narayanan 1998; Goddeau and Pineau 2000; Levin, Pieraccini, and
Eckert 2000; Roy, Pineau, and Thrun 2000; Scheffler and Young 2002; Singh et al 2002;
Williams and Young 2005; Williams, Poupart, and Young 2005a).
Much of the prior work in RL for dialogue management focuses on the problem
of choosing among a particular limited set of actions (e.g., confirm, don?t confirm)
in specific problematic states (see, e.g., Singh et al 2000a). This approach augments,
rather than replaces, hand-crafted dialogue systems, because the vast majority of deci-
sions, which are not learned, need to be specified by hand. In contrast, we tackle the
problem of learning to choose among any possible dialogue actions for almost every
possible state.
In addition, all prior work has used only a limited representation of the dialogue
context, often consisting only of the states of information slots (e.g., destination city
filled with high confidence) in the application (Goddeau and Pineau 2000; Levin,
Pieraccini, and Eckert 2000; Singh et al 2000a, 2000b, 2002; Young 2000; Scheffler and
Young 2002; Williams, Poupart, and Young 2005a, 2005b; Williams and Young 2005;
Pietquin and Dutoit 2006b), with perhaps some additional low-level information (such
as acoustic features [Pietquin 2004]). Only recently have researchers experimented with
using enriched representations of dialogue context (Gabsdil and Lemon 2004; Lemon
et al 2005; Frampton and Lemon 2006; Rieser and Lemon 2006c), as we do in this
article. From this work it is known that adding context features leads to better dialogue
strategies, compared to, for example, simply using the status of filled or confirmed
information slots as has been studied in all prior work (Frampton and Lemon 2006).
In this article we explore methods for scalable, tractable learning when using all the
available context features.
Reinforcement Learning requires estimating how good different actions will be
in different dialogue contexts. Because most previous work has only differentiated
between a small number of possible dialogue contexts, they have been able to per-
form these estimates for each state independently (e.g., Singh et al 2002; Pietquin
2004). In contrast, we use function approximation to allow generalization to states
that were not in the training data. Function approximation was also applied to RL by
Denecke, Dohsaka, and Nakano (2005), but they still use a relatively small state space
(6 features, 972 possible states). They also only exploit data for the 50 most frequent
states, using what is in effect a Gaussian kernel to compute estimates for the remaining
states from these 50 states. This is a serious limitation to their method, because a large
percentage of the data is likely to be from less frequent states, and thus would be
ignored. In our data set, we found that state frequencies followed a Zipfian (i.e., large-
tailed) distribution, with 61% of the system turns having states that only occurred once
in the data.
490
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
Another source of variation between learning approaches is the extent to which they
train on data from simulated users of different kinds, rather than train on data gathered
from real user interactions (as is done in this article). Simulated users are generally
preferred due to the much smaller development effort involved, and the fact that trial-
and-error training with humans is tedious for the users. However, the issues of how
to construct and then evaluate simulated users are open problems. Clearly there is a
dependency between the accuracy of the simulation used for training and the eventual
dialogue policy that is learned (Schatzmann et al 2005). Current research attempts to
develop metrics for user simulation that are predictive of the overall quality of the
final learned dialogue policy (Schatzmann, Georgila, and Young 2005; Schatzmann
et al 2005; Georgila, Henderson, and Lemon 2005; Georgila, Henderson, and Lemon
2006; Rieser and Lemon 2006a; Schatzmann et al 2006; Williams 2007). Furthermore,
several approaches use simple probabilistic simulations encoded by hand, using intu-
itions about reasonable user behaviors (e.g., Pietquin 2004; Frampton and Lemon 2005;
Pietquin and Dutoit 2006a), whereas other work (e.g., Scheffler and Young 2001, 2002;
Georgila, Henderson, and Lemon 2005; Georgila, Henderson, and Lemon 2006; Rieser
and Lemon 2006a) builds simulated users from dialogue corpora. We use the latter
approach, but only in the evaluation of our learned policies.
No matter which method is chosen for user simulation, a simulated user is still
clearly different from a human user. Therefore, it is important to learn as much as possi-
ble from the data we have from human users. In addition, the huge policy space makes
policy exploration with simulated users intractable, unless we can initialize the system
with a good policy and constrain the policy exploration. This also requires learning as
much as possible from the initial set of data. Therefore, in this article we investigate
using a fixed corpus of dialogues to automatically optimize dialogue systems. No user
simulation is involved in training, thus avoiding the issue of dependency on the quality
and availability of user simulations.
Previous work on RL has made use of policy exploration (Sutton and Barto 1998),
where new data is generated for each policy that is considered during the course
of learning (for example using simulated users). Indeed, this is often considered an
integral part of RL. In contrast, we choose to learn from a fixed data set, without
policy exploration. This is motivated by the fact that real dialogue corpora are very
expensive to produce, and it is often not practical to produce new real dialogues during
the course of learning. Singh et al (2002) manage to perform one iteration of policy
exploration with real data, but most work on RL requires many thousands of iterations.
As discussed previously, this motivates using simulated data for training, but even if
accurate dialogues can be automatically generated with simulated users, training on
simulated dialogues does not replace the need to fully exploit the real data, and does
not solve the sparse data problems that we address here. With a very large state space,
it will never be tractable for policy exploration to test a new policy on even a reasonable
proportion of the states. Thus we will inevitably need to stop policy exploration with
a policy that has not been sufficiently tested. In this sense, we will be in a very similar
situation to learning from a fixed data set, where we don?t have the option of generating
new data for new states. For this reason, the solution we propose for learning from fixed
data sets is also useful for learning with policy exploration.
There have been some proposals in RL for learning a policy that is different from
that used to generate the data (called ?off-policy? learning), but these methods have
been found not to work well with linear function approximation (Sutton and Barto
1998). They also do not solve the problem of straying from the region of state space
that has been observed in the data, discussed subsequently.
491
Computational Linguistics Volume 34, Number 4
1.2 The COMMUNICATOR Domain and Data Annotation
To empirically evaluate our proposed learning method, we apply it to the COMMU-
NICATOR domain using the COMMUNICATOR corpora. The COMMUNICATOR corpora
(2000 [Walker et al 2001] and 2001 [Walker et al 2002b]) consist of human?machine
dialogues (approximately 2,300 dialogues in total). The users always try to book a flight,
but they may also try to select a hotel or car rental. The dialogues are primarily ?slot-
filling? dialogues, with some information being presented to the user after the system
thinks it has filled (or confirmed) the relevant slots. These corpora have been previously
annotated using the DATE scheme, for the Conversational Domain, Speech Act, and
Task of each system utterance (Walker and Passonneau 2001; Walker, Passonneau, and
Boland 2001). In addition, the results of user questionnaires are available, but only for
the 2001 corpus.
Table 1 shows some statistics for the two collections. In the 2000 collection each turn
contains only one utterance but in the 2001 corpus a turn may contain more than one
utterance. More details about the COMMUNICATOR corpora can be found in Walker,
Passonneau, and Boland (2001) and Walker et al (2001, 2002a).
We used a hand-crafted automatic system (Georgila, Lemon, and Henderson 2005;
Georgila et al, submitted) to assign Speech Acts and Tasks to the user utterances, and to
compute state representations for each point in the dialogue (i.e., after every utterance).
Although we annotated the whole 2000 and 2001 corpora, because we need the results
of user questionnaires (as discussed subsequently), we only make use of the 2001 data
for the experiments reported here. The 2001 data has eight systems, 1,683 dialogues, and
125,388 total states, two thirds of which result from system actions and one third from
user actions. The annotation system is implemented using DIPPER (Bos et al 2003) and
OAA (Cheyer and Martin 2001), using several OAA agents (see Georgila, Lemon, and
Henderson, 2005, and Georgila et al, submitted, for more details). Following the ISU
approach, we represented states using Information States, which are feature structures
intended to record all the information about the preceding portion of the dialogue that
is relevant to making dialogue management decisions. An example of some of the types
of information recorded in an Information State is shown in Figure 1, including filled
slots, confirmed slots, and previous speech acts.
Given this corpus, we need to learn a dialogue management policy that maps these
state representations to effective system actions. As the example in Figure 1 illustrates,
there are a large number of features in dialogue states that are potentially relevant to
Table 1
Statistics for the 2000 and 2001 COMMUNICATOR data.
Year
2000 2001 Total
Number of dialogues 648 1683 2331
Number of turns 24,728 78,718 103,446
Number of system turns 13,013 39,419 52,432
Number of user turns 11,715 39,299 51,014
Number of utterances 24,728 89,666 114,394
Number of system utterances 13,013 50,159 63,172
Number of user utterances 11,715 39,507 51,222
Number of system dialogue acts 22,752 85,881 108,633
492
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
dialogue management, and thus should not be excluded from the state representations
we use in learning. This leads to a very large space of possible states (over 10386 states are
theoretically possible in our model), with a very high chance that a state encountered
in testing will not be exactly the same as any state encountered in training. This fact
motivates, if not requires, the use of approximation methods.
The complexity of the COMMUNICATOR domain is alsomanifested in the large num-
ber of system actions that the dialogue management policy needs to choose between.
The DATE scheme representation of system actions implies that each possible triple of
values for the Conversational Domain, Speech Act, and Task is a different action. In
addition, we have added release turn and end dialogue actions. There are a total of
74 system actions that occur in the annotated COMMUNICATOR data.
2. Reinforcement Learning with a Fixed Data Set
We use the annotated COMMUNICATOR data to train a Reinforcement Learning system.
In RL, the objective of the system is to maximize the reward it gets during entire dia-
logues. Rewards are defined to reflect how successful a dialogue was, so by maximizing
the total reward the system optimizes the quality of dialogues. The difficulty is that, at
any point in the dialogue, the system cannot be sure what will happen in the remainder
of the dialogue, and thus cannot be sure what effect its actions will have on the total
reward at the end of the dialogue. Thus the system must choose an action based on the
average reward it has observed previously after it has performed that action in states
similar to the current one. This average is the expected future reward.
The core component of any RL system is the estimation of the expected future
reward (called the Q-function). Given a state and an action that could be taken in that
state, the Q-function tells us what total reward, on average, we can expect between
taking that action and the end of the dialogue.1 Once we have this function, the optimal
dialogue management policy reduces to simply choosing the action that maximizes the
expected future reward for the current state.
Our proposal for RL with fixed data sets uses two main techniques. The first is the
use of function approximation to estimate the expected future reward. We claim that
linear function approximation is an effective way to generalize from a limited data set
to a large space of state?action pairs. The second technique is a novel hybrid learning
method that combines RL with supervised learning (SL). SL is used to characterize how
much data we have for each area of the state?action space (also using linear function
approximation). Our hybrid policy uses SL to avoid state?action pairs for which we do
not have enough data, while using RL to maximize reward within the parts of the space
where we do have enough data. We claim that this is an effective solution to the problem
of learning complex tasks from fixed data sets.
2.1 Defining Dialogue Reward
To apply RL to the COMMUNICATOR data, we first have to define a mapping r(d, i) from
a dialogue d and a position in that dialogue i to a reward value. This reward function is
computed using the reward level of annotation in the COMMUNICATOR data, which was
1 The expected future reward also depends on the dialogue management policy that the system will use in
the future. This self-referential nature of RL is the topic of much RL research, and will be discussed more
in the following.
493
Computational Linguistics Volume 34, Number 4
extracted from user questionnaires and task completion measures. For all states other
than the final state, we provide a reward of ?1 if the state follows a system action, and
0 otherwise. This encodes the idea that, all other things being equal, short dialogues
are better than long ones. For the final state we provide a reward that is the sum of
the rewards for each feature in the reward annotation. ?Actual Task Completion? and
?Perceived Task Completion? are both worth a reward of 100 if they are non-zero (i.e.,
true), and 0 otherwise. The remaining reward features have values ranging from 1 to 5
in the annotation (where 5 is the best), which we rescale to the range 0 to 1 (1 converts
to 0, 5 converts to 1). Their reward is their rescaled value times the weight shown in
Table 2. The relative values of these later weights were determined by the empirical
analysis reported in Walker et al (2001) within the PARADISE evaluation framework
(Walker, Kamm, and Litman 2000).
2.2 Estimating the Expected Future Reward
Given this definition of reward, we want to find an estimate Q(si, a) of the expected
future reward, which is the expected value (?E[ ]? in Equation 1) of the total reward
between taking action a in state si and the end of the dialogue. This expectation is a sum
over all possible future dialogues d, weighted by the probability of the dialogue given
that we have performed action a in state si.
Q(si, a) ? Ed|si,a[
?
j>i
r(d, j)] =
?
d
(P(d|si, a)
?
j>i
r(d, j)) (1)
Given that the number of possible future dialogues d = ?si+1, . . . , snd? is exponential in
the length of the sequences, it is not surprising that estimating the expected reward over
these sequences can be very difficult.
The ISU framework is significantly different from the frameworks used in previous
work on reinforcement learning for dialogue management, in that the rich context
representation makes the number of possible states extremely large. Having a large
number of states is a more realistic scenario for practical, flexible, and generic dialogue
systems, but it also makes many RL approaches intractable. In particular, with a large
number of states it is not possible to learn estimates of the expected future reward for
every state, unless we can exploit commonalities between different states. The feature-
based nature of ISU state representations expresses exactly these commonalities be-
tween states through the features that the states share. There are a number of techniques
that could be used for RL with feature-based representations of states, but the simplest
and most efficient is linear function approximation.
Table 2
The weights used to compute a dialogue?s final reward value, multiplied by values between 0
and 1 computed from user responses.
Actual task completion 100
Perceived task completion 100
Task ease 36
Comprehension ease 28
System behaved as expected 32
Future use 36
494
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
We use linear function approximation to map from a vector of real valued features
f (s) for the state s to a vector of estimates Q(s, a), one estimate for each a. The trained
parameters of the linear function are a vector of weights wa for each action a. Given
weights trained on a given data set, an estimate Qdata(s, a) of the expected future reward
given a state s and an action a is the inner product of the state vector f (s) and the weight
vector wa.
2
Qdata(s, a) = f (s)
Twa =
?
i
fi(s)wai (2)
This approximation method has the effect of treating two states as similar if they
share features. During learning, updating the estimate Qdata(s, a) for one observed state
s will also update the estimate Qdata(s
?, a) for all other states s? to the extent that s?
shares features with s. This updating happens via the weights wa; if s has feature i then
updating the estimate Qdata(s, a) will change wai, which will in turn change Qdata(s
?, a) for
any s? that also has feature i. Thus each feature represents a dimension with respect to
which two states can be similar or different. This similarity measure is known as a linear
kernel.
This is the first time that linear function approximation has been used for learning
dialogue strategies. Denecke, Dohsaka, and Nakano (2005) also use function approxi-
mation, but there the notion of similarity used during learning is Euclidean distance,
rather than shared features. In effect, Denecke, Dohsaka, and Nakano use a Gaussian
kernel, whereas we use a linear kernel.
To train the weights of the linear approximation Qdata(s, a), we employed a standard
RL learning method called SARSA(?) (Sutton and Barto 1998). This method learns
based on two criteria, with a parameter ? used to weight their relative influence. The
first criterion comes from temporal-difference learning: the current estimate for the
Q-function should (on average) equal the reward from the next state plus the estimate
for the expected future reward at the next state. The second criterion comes directly
from the observed reward: The current estimate for the Q-function should (on average)
equal the reward observed for the remainder of the dialogue. The combination of these
two criteria makes learning faster than using either one alone. Gradient descent learning
is applied to the weights; at each step of learning, the weights are updated so as to make
the Q-function better fit this combined criterion.
Whereas the weights wa are learned from data, the mapping f (s) from states to vec-
tors must be specified beforehand. Because each value fi(s) in these vectors represents a
possible commonality between states, it is through the definition of f (s) that we control
the notion of similarity that will be used by the linear function approximation. The
definition of f (s) we are currently using is a straightforward mapping from attribute?
value pairs in the Information State s to values in the vector f (s).
The state vector mapping f (s) was computed using the first four levels of our state
annotations for the COMMUNICATOR data (i.e., the Dialogue, Task, Low, and History
levels shown in Figure 1). The values of the attributes in these annotations were con-
verted to features of three types. For attributes that take numbers as values, we used
a simple function to map these numbers to a real number between 0 and 1, with the
absence of any value being mapped to 0 (resulting in six features, e.g., StateNumber).
2 We will use the notation xTy to denote the inner product between vectors x and y (i.e., ?x transpose
times y?). wai is the ith element of the vector wa.
495
Computational Linguistics Volume 34, Number 4
For attributes that can have arbitrary text as their values, we used 1 to represent the
presence of text and 0 to represent no value (resulting in two features, e.g., AsrInput).
The remaining attributes all have either a finite set of possible values, or a list of such
values.
The vast majority of our features are constructed from this third set of attributes.
First, to reflect the importance of speech act?task pairs (which we use to define both
system and user actions), we construct a new SpeechAct-Task attribute whose value
is the concatenation of the values for the SpeechAct and Task attributes. The same is
done for the SpeechActsHist and TasksHist attributes. Second, attributes with a list
value (i.e., the . . .Hist and . . .Status attributes, plus user actions3) are converted to a
set of attribute?value pairs consisting of the attribute and each value in the list (result-
ing in 509 features, e.g., FilledSlotsStatus:[orig city]). Note that this conversion
loses the ordering between the values in the list. In the case of SpeechAct, Task, and
SpeechAct-Task attributes that have list values (which result from turns in which a
user performs more than one action), we also include the whole list as a value for
the attribute4 (resulting in 364 features, e.g., SpeechAct:[no answer,provide info]).
Finally, attributes with single values are assigned features (which result in 401 features,
e.g., Speaker:user).
From this set of potential features, we only use those that occur in the data at least
five times.5 (Only these features are included in the feature counts given previously.)
Each feature is assigned an element of the vector f (s) that is 1 if that feature is present in
the state and 0 if it is not. In total there are 1,282 features.
One advantage of using linear function approximation is that the learning method
can be kept fairly simple, while still incorporating domain knowledge in the design
of the mapping to feature vectors. One area of future research is to investigate more
complicated mappings to feature vectors f (s). This would involve making use of kernel-
based methods. Kernels are used to compensate for the oversimplicity of linear func-
tions, and can be used to express more complicated notions of commonality between
states (Shawe-Taylor and Cristianini 2004).
2.3 Pure RL and SL Policies
Given the estimate of the expected future reward Qdata(s, a) discussed in the previous
section, one obvious approach would use this estimate to define the dialogue policy.
This ?pure RL? policy simply selects the action a with the highest Qdata(s, a) given the
state s. As demonstrated by the evaluation in Section 3, this policy performs very badly.
Inspection of the actions chosen by the pure RL policy indicates that this policy is
very different from the policy observed in the COMMUNICATOR data; the pure RL policy
almost never chose the same action as was in the data. This means that the actions that
have been learned to have the best future reward for a state are not the ones that were
3 Because in the 2001 COMMUNICATOR data users may perform more than one action in a single turn, a
user?s action is potentially a list of speech act?task pairs. These are annotated as lists of speech acts plus
lists of tasks, to which we add lists of speech act?task pairs. Histories of these lists (i.e., lists of lists) are
first flattened and then treated like other lists.
4 These ?list? values are more accurately described as set values, because we do not encode the ordering
of the values in the list.
5 We also do not include the . . .Value. . . attributes, such as FilledSlotValue, which specify the actual
fillers for slots.
496
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
typically chosen by the COMMUNICATOR systems in that state. This difference results in
two problems:
 such atypical actions then lead to states unlike anything observed in the
data,
 the policy that the system will use for future actions is different from that
observed in the data, and
The first problem makes the Qdata(s, a) estimates for the visited states highly unreliable,
because we don?t have data for these states. Because the future reward depends on the
policy that the systemwill use in the future, the second problemmeans that the estimate
Qdata(s, a) is not even relevant to the expected future reward of the pure RL policy.Wewill
return to these problems when we develop our proposed method in Section 2.4.
These problems are a result of the fact that we are training on a fixed data set,
and therefore cannot generate new data that is appropriate for the new policy. The
solution to these problems that is typically used in RL research is to generate new
data as learning progresses and the policy changes, as discussed in Section 1.1. The RL
system can thus explore the space of possible policies and states, generating new data
that is relevant to each explored policy and its states. The problem with learning with
policy exploration, even when using simulated users, is that it is not tractable with a
large state space and action set. Consider that with 10386 states and 74 actions, there are
7410
386
possible policies. If we were able to explore policies at a rate of 1 policy a second,
after 1 year we would have visited only one policy in every 7410
385.6
policies. Policy
exploration algorithms are only partially random, so to some extent they can make
accurate choices about which parts of the policy space to explore and which to ignore,
but these numbers are indicative of the scale of the problem faced by policy exploration.
In addition, experiments with a random policy achieved an average score of ?66,
showing that the vast majority of policies are very bad. This indicates that starting
policy exploration with a random policy would require an extremely large amount of
exploration to move from there to a policy which is as good as the policy found with
the proposal discussed herein (which achieved a score of 140, out of a maximum 197).
Therefore it is crucial that exploratory learning at least be initialized with a policy that
we already know to be good. The method proposed in this article for learning a policy
from a pre-existing corpus of dialogues can be used to find such an initial policy.
Given these problems with using RL with a fixed data set, an obvious alternative
would be to simply train a policy to mimic the policies of the systems used to generate
the data. One reason for training a policy, rather than using one of the original policies,
is that learning allows us to merge the policies from all the different systems, which
can lead to a better policy than any one system (as we will show in Section 3). Another
reason is that learning results in a policy that generalizes from the original policies in
interesting ways. Most notably, our learning method can be used to define a probabilis-
tic policy, not just the (presumably) deterministic policies used to generate the data. A
third reason could be (as in our case) that we do not have access to any of the original
systems that generated the data. In some sense we can use learning to reverse engineer
the systems.
We train a policy to mimic the policy observed in the data using supervised learning
with linear function approximation. This ?pure SL? policy simply selects the action
a with the highest probability P(a|s) of being chosen given the state s. We estimate
P(a|s) with linear function approximation, just as for Qdata(s, a), except that a normalized
497
Computational Linguistics Volume 34, Number 4
exponential function (a.k.a. ?softmax?) is used so that the result is a probability distri-
bution over actions a.
P(a|s) ? Sdata(s, a) =
exp( f (s)Tw?a)
?
a? exp( f (s)
Tw?
a?
)
(3)
This gives us a log-linear model, also known as a maximum entropy model. The
parameters of this model (thew?a) are trained using supervised learning on the COMMU-
NICATOR data. As with the Q-function, the use of linear function approximation means
that we have estimates for P(a|s) even for states s that have never occurred in the data,
based on similar states that did occur.
2.4 A Hybrid Approach to RL
In this work we focus on solving the first of the two problems we have discussed,
namely, preventing the system from straying into portions of the state space for which
we do not have sufficient data. To do this, we propose a novel hybrid approach that
combines RL with supervised learning. SL is used to model which actions will take the
system into a portion of the state space for which we don?t have sufficient data. RL
is used to choose between the remaining actions. A discriminant function Qhybrid(s, a) is
derived that combines these two criteria in a principled way. The resulting policy can be
adjusted to be as similar as necessary to the policy in the data, thereby also addressing
the second problem discussed previously.
As with the pure SL policy, supervised learning is used to model the policy that
the systems in the data actually use. Because in general multiple policies were used,
we model the data?s policy as a probabilistic policy, using the estimate Sdata(s, a) of P(a|s)
presented in the previous section. Sdata(s, a) is an estimate of the probability that a random
system selected from those that generated the data would choose action a given that it is
in state s. Because we are using function approximation to learn Sdata(s, a) from the data,
it will generalize (or ?smooth?) the policies actually used to generate the data so that
similar states will allow similar sets of actions.6
The hybrid approach we have investigated is based on the assumption that the Q-
function trained on the data is a poor model of the expected future reward for states
in the portion of the state space not covered by the data. Thus we need an alternative
method for estimating the future reward for these unobserved states. We have exper-
imented with two such methods. The first method simply specifies a fixed reward U
for these states. By setting this fixed reward to a low value, it amounts to a penalty for
straying from the observed portion of the state space.
The second method estimated the reward for unobserved states by adding a fixed
reward offset UO to the reward estimates for ending the dialogue immediately. This
method compensates for the use of a dialogue-final reward scheme, where many things
that the dialogue has already accomplished aren?t reflected in the reward given so far.
For example, in our reward scheme, filling a slot does not result in immediate reward,
but instead results in reward at the end of the dialogue if it leads to a successful
dialogue. The estimated reward for ending the dialogue immediately reflects howmuch
6 For this reason, we will get a probabilistic policy even if only a single deterministic policy is used
to generate the data. This makes this method applicable even for data sets generated with a single
deterministic prototype system.
498
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
reward is stored up in the state in this way. If the fixed reward added to this estimate is
set to negative, then we can be sure that the reward estimated for unobserved states is
always less than that for the best observed state, so this method also results in a penalty
for straying from the observed portion of the state space.
Given an estimated reward u for unobserved states, the expected future reward is
then the average between u for the cases where performing a in s leads to an unobserved
state and the expected reward Qdata(s, a) for the cases where it leads to an observed state.
Formally, this average is a mixture of the estimate u with the estimate Qdata(s, a), where
the mixture coefficient is the probability Pobserved(s, a) that performing a in swill lead to an
observed state.
Ed|si,a[
?
j>i r(d, j)]
? Qdata(s, a)Pobserved(s, a)+ u(1? Pobserved(s, a))
(4)
Because this estimate of the expected future reward is only needed for choosing
the next action given the current state s, we only need to estimate a function that dis-
criminates between different actions in the same way as this estimate. To derive such a
discriminant function, we first approximate Pobserved(s, a) with a first-order approximation
in terms of the probability distribution in the data P(s, a) and the size of the data set N,
under the assumption that the number of possible state?action pairs is much larger than
the size of the data set (so P(s, a)N 	 1).
Pobserved(s, a) = 1? (1? P(s, a))N
? P(s, a)N ? Sdata(s, a)P(s)N
(5)
Given this approximation, the discriminant function needs to order two actions a1, a2 in
the same way as this estimate of the expected future reward.
Qdata(s, a1)Sdata(s, a1)P(s)N + u(1? Sdata(s, a1)P(s)N)
? Qdata(s, a2)Sdata(s, a2)P(s)N + u(1? Sdata(s, a2)P(s)N)
if and only if
Sdata(s, a1)(Qdata(s, a1)? u) ? Sdata(s, a2)(Qdata(s, a2)? u)
(6)
We call this discriminant function Qhybrid(s, a).
Qhybrid(s, a) = Sdata(s, a)(Qdata(s, a)? u) (7)
We use thisQhybrid(s, a) function to choose the actions for our hybrid policy. By adjust-
ing the value of the unobserved state penalty u, we can adjust the extent to which this
model follows the supervised policy defined by Sdata(s, a) or the reinforcement learning
policy defined by Qdata(s, a). In particular, if u is very low, then maximizing Qhybrid(s, a) is
equivalent to maximizing Sdata(s, a). Thus a very low u is equivalent to the policy that
always chooses the most probable action, which we will call the ?SL policy.?
The procedure for trainingQhybrid(s, a) is simply to trainQdata(s, a) with RL and Sdata(s, a)
with SL. These two models are then combined using Equation (7), given a value for u
computed with one of the two methods presented previously. Both of these methods
involve setting a constant that determines the relative importance of RL versus SL. In
the next section we will empirically investigate good values for these constants.
499
Computational Linguistics Volume 34, Number 4
3. Empirical Evaluation
We evaluate the trained dialogue management policies by running them against trained
user simulations. The policies and the user simulations were trained using different
parts of the annotated COMMUNICATOR data (using two-fold and five-fold cross val-
idation). We compare our results against each other and against the performance of
the eight COMMUNICATOR systems, using an evaluation metric discussed subsequently.
The Information States for the simulated dialogues were computed with the same rules
used to compute the Information States for the annotated data.
3.1 The Testing Setup
For these experiments, we restrict our attention to users who only want single-leg
and return flight bookings. This allows us to do the evaluation using only the four
essential slots included in both these types of bookings: origin city, destination city,
departure date, and departure time. To achieve this restriction, we first selected all those
COMMUNICATOR dialogues that consisted only of single-leg or return flight bookings.
This subset contained 217 ATT dialogues, 116 BBN dialogues, 126 CMU dialogues, 159
Colorado dialogues, 77 IBM dialogues, 192 Lucent dialogues, 180 MIT dialogues, and
185 SRI dialogues, for a total of 1,252 dialogues (out of 1,683). This subset was used for
evaluating the COMMUNICATOR systems and for training the user models. The system
models were trained on the full set of dialogues, because they should not know the
user?s goals in advance. So, for each fold of the data, the user model was trained on only
the single-leg and return dialogues from that fold and the system model was trained on
the full set of dialogues from a subset of the remaining folds (one fold for the two-fold
experiments and three folds for the five-fold experiment, as discussed subsequently).
The user models were trained in the same way as the Sdata(s, a) function for the
pure SL model discussed in Section 2.3, using linear function approximation and a
normalized exponential output function. The states that precede user actions are input
as vectors of features virtually identical to those used for the system. However, unlike
the action set for the system, the user only chooses one action per turn, and that action
can include multiple ?Speech Act, Task? pairs. The output of the model is a probability
distribution over these actions. The user simulation selects an action randomly accord-
ing to this distribution. We also trained a user model based on n-grams of user and
system actions, which produced similar results in our testing (Georgila, Henderson,
and Lemon 2006).
In our initial experiments with the hybrid policy, we found that it never closed
the dialogue. We think that this was due to the system action (annotated in DATE)
meta greeting goodbye, which is used both as the first action and as the last action
of a dialogue. The hybrid policy expects this action to be chosen before it will close
the dialogue, but the system never chooses this action at the end of a dialogue because
it is so strongly associated with the beginning of the dialogue. This is an example of
the limitations of linear function approximation, and our dependence on the previous
COMMUNICATOR annotations. We could address this problem by splitting this action
into two actions, one for ?greeting? and one for ?goodbye.? But because we do not want
to embark on the task of feature engineering at this stage, we have instead augmented
the hybrid policy with a rule that closes the dialogue after the system chooses the action
offer, to offer the user a flight. After this first flight offer, the user has one turn to
reply, and then the dialogue is ended. For practical reasons we have also added rules
500
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
that close the dialogue after 100 states (i.e., total of user and system actions), and that
release the turn if the system has done 10 actions in a row without releasing the turn.
3.2 The Evaluation Metrics
To evaluate the success of a dialogue, we take the final state of the dialogue and use
it to compute a scoring function. We want the scoring function to be similar to the
reward we compute from the quality measures provided with the COMMUNICATOR
data (e.g., the user questionnaires), but because we do not have these quality measures
for the simulated dialogues, we cannot use the exact same reward function. When we
compare the hybrid policy against the COMMUNICATOR systems, we apply the same
scoring function to both types of dialogues so that we have a comparable evaluation
metric for both.
Because currently we are only considering users who only want single-leg or return
flight bookings, the scoring function only looks at the four essential slots for these
bookings: origin city, destination city, departure date, and departure time. We give
25 points for each slot that is filled, plus another 25 points for each slot that is also
confirmed. We also deduct 1 point for each action performed by the system, to penalize
longer dialogues. Thus the maximum possible score is 197 (i.e., 200 minus 3 system
actions: ask for all the user information in one action, then confirm all the four slots in
one action and offer a flight).
The motivation behind this evaluation metric is that confirmed slots are more likely
to be correct than slots that are just filled. If we view the score as proportional to the
probability that a slot is filled correctly, then this scoring assumes that confirmed slots
are twice as likely to be correct. Although other scoring metrics are clearly possible, this
one is a simple and reasonable approximation of the relative expected correctness of
confirmed versus non-confirmed information in dialogue systems. On the other hand,
none of our conclusions depend on this exact scoring function, as indicated by results for
the ?no-conf? version of our scoring function (discussed subsequently), which ignores
confirmations.
When combining the scores for different slots, we do not try to model the all-or-
nothing nature of the COMMUNICATOR task-completion quality measures, but instead
sum the scores for the individual slots. This summakes our scoring metric value partial
completions more highly, but inspection of the distributions of scores indicates that
this difference does not favor either the hybrid policy or the original COMMUNICATOR
systems.
Although this evaluation metric could reflect the relative quality of individual
dialogues more accurately, we believe it provides a good measure of the relative quality
of the systems we wish to compare. First, the exact same metric is applied to every
system. Additional information that we have for some systems, but not all, is not used
(e.g., the COMMUNICATOR user questionnaires, which we do not have for simulated
dialogues). Second, the systems are being run against approximately equivalent users.
The user simulation is trained on exactly the same user actions that are used to evaluate
the COMMUNICATOR systems, so the user simulations mimic exactly these users. In
particular, the simulation is able to mimic the effects of speech recognition errors,
because it is just as likely as the real users to disagree with a confirmation or provide a
new value for a previously filled slot. The nature of the simulation model may make it
systematically different from real users in some way, but we know of no argument for
why this would bias our results in favor of one system or another.
501
Computational Linguistics Volume 34, Number 4
One concern about this evaluation metric is that it does not reflect the quality of the
speech recognizer being used by the system. If a system has a good speech recognizer,
then it may not be necessary for it to confirm a slot value, but our scoring function will
still penalize it for not confirming. This would certainly be a problem if this metric were
to be used to compare different systems within the COMMUNICATOR data set. However,
the intention of the metric is simply to facilitate comparisons between different versions
of our proposed system, and between our proposed systems and those in the data.
Because the user simulations are trained on the COMMUNICATOR data, they simulate
speech recognition errors at the same rate as the data, thereby controlling for the quality
of the speech recognizer.
Nonetheless, it is worth considering another evaluation metric that does not penal-
ize for missing confirmations. For this reason we also evaluate the different systems
based on their scores for only filled slots and length, which we call the ?no-conf? score.
3.3 The Influence of Reinforcement Learning
In our first set of experiments, we evaluated the success of our hybrid policy relative
to the performance of the pure reinforcement learning policy and the pure supervised
learning policy. We also investigated how to best set the parameters for combining the
supervised and reinforcement learning policies in a hybrid policy.
We first compared the two proposed hybrid methods using two-fold cross valida-
tion. We trained models of both Qdata(s, a) and Sdata(s, a), and then used them to define
policies. We trained both models for 100 iterations through the training portion of the
data, at which point there was little change in the training error. We trained Qdata(s, a)
using SARSA(?) with ? = 0.9. This training was repeated twice, once for each fold of the
complete data set. The reinforcement learning policy uses only Qdata(s, a), the SL policy
uses only Sdata(s, a), and the hybrid policies combine the two using Equation (7). For the
hybrid policies, we used the two methods for estimating the unobserved state penalty
u and various values for the fixed reward U or reward offset UO.
During testing, each policy was run for 2,000 dialogues against a linear function
approximation user model trained on the opposite half of the data. The final state for
each one of these dialogues was then fed through the scoring function and averaged
across dialogues and across data halves. The results are plotted in Figure 2. To allow
direct comparisons between the different values of U and UO, these scores are plotted
against the proportion of decisions that are different from that which the pure SL policy
would choose. Thus the SL policy (average reward 139.8) is plotted at 0 (which is
Figure 2
Average dialogue score plotted against the proportion of decisions that diverge from the SL
policy, for different values of the unobservable state reward U and reward offset UO. Averages
over two folds, 2,000 dialogues per fold.
502
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
Figure 3
Average dialogue score plotted against the proportion of decisions that diverge from the SL
policy, for different values of the unobservable state reward offset UO. Averages over five folds,
2,000 dialogues per fold.
equivalent to a large negative U or UO). Additional points for the hybrid policies are
shown for (from left to right, respectively) U = 0, 40, 80, and 100, and UO = ?300, ?100,
?60, ?40, ?20, ?10, and 0. The pure reinforcement learning policy is not shown because
its average score falls well below the bottom of the graph, at 44.4.
Figure 2 indicates that, for both hybrid methods, adding some influence from RL
increases performance over pure SL, but too much RL results in degradation. Using a
reward offsetUO for u generally does better than a fixed rewardU, and allows a greater
influence from RL before degradation.
We found that the results for our two folds were very different,7 so we repeated
the experiments using five-fold cross validation, where the dialogues from each system
were split randomly (rather than chronologically).8 For each fold, we trained models
of both Qdata(s, a) and Sdata(s, a) on three of the folds, using a fourth fold to decide when
to stop training. The fifth fold was then used to train a linear function approximation
user model, which was used to generate 2,000 simulated dialogues. Combining the five
folds, this gave us 10,000 dialogues per model. Because, in the previous experiments,
using a reward offset UO performed better than using a fixed reward U, we only tested
models using different values of the reward offset UO.
The validation performance of the trained models for Qdata(s, a) and Sdata(s, a) per-
formed similarly across the different splits. Taken together, the models of Sdata(s, a) had a
perplexity of 4.4. Intuitively, this means that the supervised models were able to narrow
down the list of possible actions from 74 to about 4 choices, on average. This suggests
that the ISU representation of state is doing a good job of representing the information
being used by the systems to make dialogue management decisions, but that there is
still a good amount of uncaptured variability. Presumably most of this variability is due
to differences between the policies for the different systems. The models of Qdata(s, a)
had a mean squared error of 8,242, whose square root is 91. This measure is harder to
interpret because it is dominated by large errors, but suggests that the expected future
reward is rather hard to predict, as is to be expected.
Figure 3 shows the average scores for the pure SL policy (at 0) and for hybrid
policies (from left to right) with UO = ?300, ?100, ?60, ?40, ?20, ?10, ?5 and 0. The
7 For the two-fold experiments, the data were split by putting the first half of the dialogues for each system
in one fold and the second half in the other, under the constraint that no user had dialogues in more than
one fold. It appears that the users that were run at the beginning of the 2001 COMMUNICATOR data
collection were very different from those run at the end.
8 To be more precise, for each system we split the set of users randomly into five groups. Then all the
dialogues for a given group of users were put in the same fold.
503
Computational Linguistics Volume 34, Number 4
hybrid policies perform consistently better than the SL policy. The difference between
the hybrid policy and the SL policy is statistically significant at the 5% level for the
three best hybrid policies tested (p < 0.01 for UO = ?40, p < 0.001 for UO = ?10, and
p < 0.007 for UO = ?5). If we combine all the tested hybrid policies together, then their
average score (139.4) is also significantly better than the SL policy (p < 0.014). All these
results are significantly better than the average score of the pure RL policy (34.9).
3.4 Comparisons with COMMUNICATOR Systems
In our second set of experiments, we evaluated the success of our learned policies
relative to the performance of the COMMUNICATOR systems that they were trained on.
To evaluate the performance of the COMMUNICATOR systems, we extracted final states
from all the dialogues that only contain single-leg or return flight bookings and fed
them through the scoring function. The average scores are shown in Tables 3 and 4,
along with the average scores for the pure SL policy, the pure RL policy, and the best
hybrid policy (UO = ?10). The total score, the score excluding confirmations, and the
three components of the total score are shown.
Table 3 shows the results computed from the complete dialogues. These results
show a clear advantage for the hybrid policy over the average across the COMMUNI-
CATOR systems, as well as over each individual COMMUNICATOR system. In particular,
the hybrid policy uses fewer steps. Because the number of steps is doubtless affected
by the hybrid policy?s built-in strategy of stopping the dialogue after the first flight
offer, we also evaluated the performance of the COMMUNICATOR systems if we also
stopped these dialogues after the first flight offer, shown in Table 4. The COMMUNI-
CATOR systems do better when stopped at the first flight offer, but still their average
(?all COMMUNICATOR?) is not nearly as good as the hybrid or SL policies, under all
measures.
Although the average score of the COMMUNICATOR systems in Table 4 is well below
those of the hybrid and SL policies, under this measure the single best system (BBN)
beats our proposed system. Also, if we ignore confirmations (the ?no-conf? measure),
Table 3
The average scores from the different systems for single-leg and return dialogues, the score
excluding confirmations, and the three components of these scores.
System Total score No-conf Filled Confirmed Length
score slots slots penalty
hybrid RL/SL 140.3 70.3 88.0 70.0 ?17.7
pure SL 138.3 69.2 89.2 69.1 ?20.0
pure RL 34.9 25.6 56.9 8.3 ?31.3
all COMMUNICATOR 103.6 40.6 85.0 63.0 ?44.4
SRI 115.3 50.5 83.4 64.9 ?32.9
MIT 114.3 43.2 87.1 71.1 ?43.9
LUC 110.3 36.1 91.1 74.1 ?55.0
COL 105.9 47.0 90.6 59.0 ?43.6
BBN 102.4 27.1 82.5 75.2 ?55.4
ATT 94.0 38.7 78.3 55.3 ?39.6
CMU 92.1 24.0 81.7 68.1 ?57.7
IBM 77.0 61.8 85.4 15.3 ?23.6
504
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
Table 4
The average scores after the first flight offer for single-leg and return dialogues, the score
excluding confirmations, and the three components of these scores.
System Total score No-conf Filled Confirmed Length
score slots slots penalty
hybrid RL/SL 140.3 70.3 88.0 70.0 ?17.7
pure SL 138.3 69.2 89.2 69.1 ?20.0
pure RL 34.9 25.6 56.9 8.3 ?31.3
all COMMUNICATOR 127.1 63.2 84.5 63.9 ?21.3
BBN 148.9 73.2 88.6 75.6 ?15.4
LUC 138.5 59.1 91.1 79.4 ?32.1
MIT 136.4 66.9 82.8 69.4 ?15.9
COL 132.9 71.4 89.9 61.5 ?18.6
SRI 128.2 61.7 84.2 66.5 ?22.5
CMU 123.8 58.7 77.2 65.1 ?18.5
ATT 109.1 53.6 78.3 55.4 ?24.7
IBM 86.4 71.2 85.1 15.3 ?13.9
then three of the individual systems beat our proposed system by small amounts. How-
ever, as discussed in Section 3.2, our evaluation methodology is not really appropriate
for comparing against individual COMMUNICATOR systems, due to likely differences in
speech recognition performance across systems. To test this explanation, we looked at
the word error rates for the speech recognition outputs for the different systems. BBN
has the highest percentage of user utterances with no speech recognition errors (79%,
versus an average of 66%), and the second lowest average word error rate (12.1 versus
an average of 22.1). Because our simulated users simulate speech recognition errors at
the average rate, the difference in performance between BBN and our systems could
easily be explained simply by differences in the speech recognizers, and not differences
in the dialogue management policies.
3.5 Discussion
The most obvious conclusion to draw from these results is not a surprising one: Pure
reinforcement learning with such a huge state space and such limited data does not
performwell. Given the pure RL policy?s score of 34.9, all the policies in Figure 3 and all
the COMMUNICATOR systems in Tables 3 and 4 perform better by quite a large margin.
Inspection of the dialogues indicates that the pure RL policy does not result in a coherent
sequence of actions. This policy tends to choose actions that are associated with the end
of the dialogue, even at the beginning of the dialogue. Perhaps this is because these
actions are only chosen by the COMMUNICATOR systems during relatively successful
dialogues. This policy also tends to repeat the same actions many times, for example
repeatedly requesting information even after the user has supplied this information.
These phenomena are examples of the problemwe used tomotivate our hybrid learning
method, in that they both involve state?action pairs that the learner would never have
seen in the COMMUNICATOR training data.
Given the disappointing performance of the pure RL policy, it is surprising that our
hybrid policies outperform the pure SL policy, as shown in Figures 2 and 3. Though the
increase in performance is small, it is statistically significant, and consistent across the
505
Computational Linguistics Volume 34, Number 4
two hybrid methods and across a range of degrees of influence from RL.9 This indicates
that our hybrid policies are succeeding in getting useful information from the results of
reinforcement learning, even under these extremely difficult circumstances. Perhaps,
under less severe circumstances for RL, a greater gain can be achieved with hybrid
policies. For the second hybrid policy (unobserved state reward offset), the fact that the
best result was achievedwith aUO value (UO = ?10) that is very close to the theoretical
limit of this method (UO = 0) suggests that future improvements to this method could
result in even more useful information being extracted from the RL policy.
The different components of the scoring function give some indication of how the
hybrid policies differ from the SL policy. As indicated in the top two rows of Table 4,
the hybrid policies mostly improve over the SL policy in dialogue length, with a slight
increase in confirmed slots and a slight decrease in filled slots.
One striking conclusion from the results comparing the learned policies to the poli-
cies of the COMMUNICATOR systems, shown in Tables 3 and 4, is that the learned policies
score better than the policies they were trained on. This is particularly surprising for
the pure SL policy, given that this policy is simply trying to mimic the behavior of
these same systems. This can be explained by the fact that the SL policy is the result
of merging all policies of the COMMUNICATOR systems. Thus it can be thought of as
a form of multi-version system, where decisions are made based on what the majority
of systems would do.10 Multi-version systems are well known to perform better than
their component systems, because the mistakes tend to be different across the different
component systems. They remove errors made by any one system that are not shared
by most of the other systems.
The good performance of the SL policy compared to the COMMUNICATOR systems
makes the better performance of the hybrid policies even more impressive. As shown
on the x axis of Figure 3, the best hybrid systems choose a different action from the
SL policy about one action out of four. Despite the good performance of the action
chosen by the SL policy, RL is able to (on average) find a better action by looking at the
rewards achieved by the systems in the data when they chose those actions in similar
states. By following different systems? choices at different points in the dialogue, the
learned policy can potentially perform better than any individual system. Although our
current evaluation methodology is not fine-grained enough to determine if this is being
achieved, the most promising aspect of applying RL to fixed data sets is in learning to
combine the best aspects of each system in the data set.
Although we believe that these results provide an accurate picture of the relative
strengths of the different types of systems we compare, it should be noted that the
reliance on evaluation with simulated dialogues inevitably leads to some lack of pre-
cision in the evaluation. All these results are computed with users who have the same
goal (booking a return flight) and with an evaluation metric that only looks at dialogue
length and whether the four main slots were filled and (optionally) confirmed. On the
9 We previously reported results that showed that adding influence from reinforcement learning always
degraded performance slightly compared to the pure SL policy (Henderson, Lemon, and Georgila 2005).
However, these results were obtained with a preliminary version of the data annotation, which gave
a less accurate indication of when slots were filled and confirmed. The scores we are achieving with
the new data annotation (Georgila et al submitted) are all higher than those reported in Henderson,
Lemon, and Geogila (2005), including the scores calculated from the data for the COMMUNICATOR
systems themselves.
10 To be more technically accurate, we can think of the SL policy as in effect asking each COMMUNICATOR
system for a probability distribution over state?action pairs for the current state, summing these
probabilities across systems, and choosing the action with the highest probability.
506
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
other hand, all the systems were trained to handle a more complicated task than this,
including multi-leg flights, hotel bookings, and rental-car bookings. They were also
designed or trained to complete the task, rather than to fill the slots. Therefore the
evaluation does not reflect all the capabilities or behaviors of the systems. However,
there is no apparent reason to believe that this fact biases our results towards one type
of system or another. This claim is easiest to support for the comparisons between
the hybrid method and the two trained baselines, pure RL and pure SL. For all these
systems, the same data was used to train the systems, the same user models were
used to generate simulated dialogues, and the same evaluation metric was applied to
these simulated dialogues. For the comparisons between the hybrid method and the
COMMUNICATOR systems, only the evaluation metric is exactly the same, but the user
models used for testing the hybrid method were trained to mimic exactly the users
in the dialogues used to evaluate the COMMUNICATOR systems. Because we know of
no evaluation bias introduced when moving from real users to their simulation, we
conclude that this comparison is also indicative of the relative performance of these two
types of systems (particularly given the size of the improvement).
A more general methodological objection could be raised against any evaluation
that uses simulated users. Despite the substantial amount of dialogue system work that
has relied on simulated users (e.g., Scheffler and Young 2002; Pietquin 2004; Georgila,
Henderson, and Lemon 2006; Schatzmann et al 2006), to date there has not been a
systematic experiment that validates this methodology against results from human
users. However, in related work (Lemon, Georgila, and Henderson 2006), we have
demonstrated that a hybrid policy learned as proposed in this article performs better
than a state-of-the-art hand-coded system in experiments with human users. The exper-
iments were done using the ?Town Information?multimodal dialogue system of Lemon
et at. (2006) and Lemon, Georgila, and Stuttle (2005). The hybrid policy reported here
(trained on the COMMUNICATOR data) was ported to this domain, and then evaluated
with human subjects. The learned policy achieved an average gain in perceived task
completion of 14.2% (from 67.6% to 81.8% at p < 0.03) compared to a state-of-the-art
hand-coded system (Lemon, Georgila, and Henderson 2006). This demonstrates that a
policy that performs well in simulation also performs well in real dialogues.11
These experiments demonstrate improvements given an initial fixed data set which
has been generated from existing systems. For applications where there are no existing
systems, an alternative would be to generate the initial data with a Wizard-of-Oz
experiment, where a human plays the part of the system, as explored by Williams and
Young (2003) and Rieser and Lemon (2006b). The methods proposed in this article can
be used to train a policy from such data without having to first build an initial system.
4. Conclusions
In this article, we have investigated how reinforcement learning can be applied to learn
dialogue management policies with large action sets and very large state spaces given
only a fixed data set of dialogues. Under a variety of metrics, our proposed hybrid re-
inforcement learning method outperforms both a policy trained with standard RL and a
11 Future work is to port the hand-coded policy back to the COMMUNICATOR domain for use in simulation.
This will investigate whether a relative improvement in simulated dialogues translates into a relative
improvement in real dialogues.
507
Computational Linguistics Volume 34, Number 4
policy trained with supervised learning, as well as the COMMUNICATOR systems which
generated the data it was trained on. This performance is achieved despite the extremely
challenging task, with 74 actions to choose between, over 10386 possible states, and
very few hand-coded policy decisions. The two main features of our model that make
this possible are the incorporation of supervised learning into a reinforcement learning
model, and the use of linear function approximation with state features provided by the
Information State Update approach to dialogue management. The supervised learning
is used to avoid states not covered by the data set, and the linear function approximation
is used to handle the very large state spaces.
With such a large space of possible state?action pairs, and therefore a huge policy
space, pure reinforcement learning would require an enormous amount of data to find
good policies. We have succeeded in using RL with fairly small data sets of only around
1,000 dialogues (in the portion used for training). This is achieved by using supervised
learning to model when an action would lead to a state for which we do not have
enough data. We proposed two methods for estimating a default value for these unseen
states, and derived a principled way to combine this value with the value estimated by
RL, using the probability provided by SL to weight this combination. This gave us two
hybrid RL/SL methods, both of which outperform both the RL and SL policies alone.
The best hybrid policy performs 302% better than the standard RL policy, and 1.4%
better than the SL policy, according to our automatic evaluation method. In addition,
according to our automatic evaluation method, the hybrid RL/SL policy outperforms
the systems used to generate the data. The best hybrid policy improves over the average
COMMUNICATOR system policy by 10% on our metric. This good performance has
been corroborated in separate experiments with human subjects (Lemon, Georgila, and
Henderson 2006), where the learned policy outperforms a state-of-the-art hand-coded
system.
The success of the hybrid method (and of pure supervised learning) on this chal-
lenging task indicates that linear function approximation is a viable approach to the very
large state spaces produced by the ISU framework. It also demonstrates the utility of a
feature-based representation of states, such as that used in the ISU approach. Further
improvement should be possible by tailoring the representation of states and actions
based on our experience so far (e.g., by including information about specific sequences
of moves), and by using automatic feature selection techniques. We should also be able
to get some improvement from more sophisticated function approximation methods,
such as kernel-based methods.
The next step is to better exploit the advantages of reinforcement learning. One
promising approach is to apply RL while running the learned policy against simulated
users, thereby allowing RL to explore parts of the policy and state spaces that are
not included in the COMMUNICATOR data. The hybrid policy we have learned on the
COMMUNICATOR data is a good starting point for this exploration. Also, the supervised
component within the hybrid system can be used to constrain the range of policies
that need to be explored when training the RL component. All of these advances will
improve techniques for bootstrapping and automatic optimization of dialogue manage-
ment policies from limited initial data sets.
Acknowledgments
This work was partially supported by the
European Commission under the FP6 project
?TALK: Talk and Look, Tools for Ambient
Linguistic Knowledge? (507802) and the FP7
project ?CLASSIC: Computational
Learning in Adaptive Systems for Spoken
Conversation? (216594), by the EPSRC
under grant EP/E019501/1, and by SHEFC
HR04016?Wellcome Trust VIP Award.
508
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
We thank Johanna Moore for proposing
the use of the COMMUNICATOR data set for
this work.
References
Bohlin, Peter, Robin Cooper, Elisabet
Engdahl, and Staffan Larsson. 1999.
Information states and dialog move
engines. Electronic Transactions in AI,
3(9). Available at www.ep.liu.se/ej/
etai/1999/D/.
Bos, Johan, Ewan Klein, Oliver Lemon, and
Tetsushi Oka. 2003. DIPPER: Description
and formalisation of an information-state
update dialogue system architecture. In
Proceedings of the 4th SIGdial Workshop on
Discourse and Dialogue, pages 115?124,
Sapporo.
Cheyer, Adam and David Martin. 2001.
The open agent architecture. Journal of
Autonomous Agents and Multi-Agent
Systems, 4(1/2):143?148.
Denecke, Matthias, Kohji Dohsaka, and
Mikio Nakano, 2005. Fast reinforcement
learning of dialogue policies using stable
function approximation. In K. Y. Su,
J. Tsujii, J.-H. Lee, and O. Y. Kwong,
Natural Language Processing, IJCNLP 2004.
Springer, Berlin, pages 1?11.
Frampton, Matthew and Oliver Lemon.
2005. Reinforcement learning of dialogue
strategies using the user?s last dialogue
act. In Proceedings of the 4th Workshop on
Knowledge and Reasoning in Practical Dialog
Systems, International Joint Conference on
Artificial Intelligence (IJCAI), pages 83?90,
Edinburgh.
Frampton, Matthew and Oliver Lemon.
2006. Learning more effective dialogue
strategies using limited dialogue move
features. In Proceedings of the 44th Meeting
of the Association for Computational
Linguistics, pages 185?192, Sydney.
Gabsdil, Malte and Oliver Lemon. 2004.
Combining acoustic and pragmatic
features to predict recognition
performance in spoken dialogue systems.
In Proceedings of the 42nd Meeting of the
Association for Computational Linguistics,
pages 344?351, Barcelona.
Georgila, Kallirroi, James Henderson, and
Oliver Lemon. 2005. Learning user
simulations for Information State
Update dialogue systems. In Proceedings
of the 9th European Conference on
Speech Communication and Technology
(Interspeech ? Eurospeech), pages 893?896,
Lisbon.
Georgila, Kallirroi, James Henderson, and
Oliver Lemon. 2006. User simulation
for spoken dialogue systems: Learning
and evaluation. In Proceedings of the
9th International Conference on Spoken
Language Processing (Interspeech ? ICSLP),
pages 1065?1068, Pittsburgh, PA.
Georgila, Kallirroi, Oliver Lemon, and
James Henderson. 2005. Automatic
annotation of COMMUNICATOR
dialogue data for learning dialogue
strategies and user simulations.
In Proceedings of the Ninth Workshop
on the Semantics and Pragmatics of
Dialogue (SEMDIAL), pages 61?68,
Nancy.
Georgila, Kallirroi, Oliver Lemon, James
Henderson, and Johanna Moore.
(submitted). Automatic annotation of
context and speech acts for dialogue
corpora.
Goddeau, D. and J. Pineau. 2000. Fast
reinforcement learning of dialog strategies.
In Proceedings of the IEEE International
Conference on Acoustics Speech and Signal
Processing (ICASSP), pages II?1233?1236,
Istanbul.
Henderson, James, Oliver Lemon, and
Kallirroi Georgila. 2005. Hybrid
reinforcement/supervised learning for
dialogue policies from COMMUNICATOR
data. In Proceedings of the 4th Workshop on
Knowledge and Reasoning in Practical Dialog
Systems, International Joint Conference on
Artificial Intelligence (IJCAI), pages 68?75,
Edinburgh.
Larsson, Staffan and David Traum. 2000.
Information state and dialogue
management in the TRINDI Dialogue
Move Engine Toolkit. Natural Language
Engineering, 6(3?4):323?340.
Lemon, Oliver, Kallirroi Georgila, and James
Henderson. 2006. Evaluating effectiveness
and portability of reinforcement learned
dialogue strategies with real users: the
TALK TownInfo evaluation. In Proceedings
of the IEEE/ACL 2006 Workshop on Spoken
Language Technology, pages 178?181,
Aruba.
Lemon, Oliver, Kallirroi Georgila, James
Henderson, Malte Gabsdil, Ivan
Meza-Ruiz, and Steve Young. 2005.
Integration of learning and adaptivity with
the ISU approach. Technical Report D4.1,
TALK Project.
Lemon, Oliver, Kallirroi Georgila, James
Henderson, and Matthew Stuttle. 2006.
An ISU dialogue system exhibiting
reinforcement learning of dialogue
policies: generic slot-filling in the TALK
509
Computational Linguistics Volume 34, Number 4
in-car system. In Proceedings of the
Demonstrations of EACL, pages 119?122,
Trento.
Lemon, Oliver, Kallirroi Georgila, and
Matthew Stuttle. 2005. Showcase
exhibiting reinforcement learning for
dialogue strategies in the in-car domain.
Technical Report D4.2, TALK Project.
Levin, Esther and Roberto Pieraccini. 1997.
A stochastic model of computer-human
interaction for learning dialogue strategies.
In Proceedings of the 5th European Conference
on Speech Communication and Technology
(Interspeech ? Eurospeech), pages 1883?1886,
Rhodes.
Levin, Esther, Roberto Pieraccini, and
Wieland Eckert. 2000. A stochastic model
of human-machine interaction for learning
dialog strategies. IEEE Transactions on
Speech and Audio Processing, 8(1):11?23.
Pietquin, Olivier. 2004. A Framework for
Unsupervised Learning of Dialogue Strategies.
Presses Universitaires de Louvain,
SIMILAR Collection.
Pietquin, Olivier and Thierry Dutoit. 2006a.
Dynamic Bayesian networks for NLU
simulation with application to dialog
optimal strategy learning. In Proceedings
of the IEEE International Conference on
Acoustics Speech and Signal Processing
(ICASSP), pages 49?52, Toulouse.
Pietquin, Olivier and Thierry Dutoit. 2006b.
A probabilistic framework for dialog
simulation and optimal strategy learning.
IEEE Transactions on Speech and Audio
Processing, 14(2):589?599.
Rieser, Verena and Oliver Lemon. 2006a.
Cluster-based user simulations for
learning dialogue strategies and the
SUPER evaluation metric. In Proceedings
of the 9th International Conference on Spoken
Language Processing (Interspeech ? ICSLP),
pages 1766?1769, Pittsburgh, PA.
Rieser, Verena and Oliver Lemon. 2006b.
Using logistic regression to initialise
reinforcement-learning-based dialogue
systems. In Proceedings of the IEEE/ACL
2006 Workshop on Spoken Language
Technology, pages 190?193, Aruba.
Rieser, Verena and Oliver Lemon. 2006c.
Using machine learning to explore
human multimodal clarification
strategies. In Proceedings of the Poster
Session of the 44th Meeting of the
Association for Computational Linguistics,
pages 659?666, Sydney.
Roy, Nicholas, Joelle Pineau, and Sebastian
Thrun. 2000. Spoken dialog management
for robots. In Proceedings of the 38th Meeting
of the Association for Computational
Linguistics, pages 93?100, Hong Kong.
Schatzmann, Jost, Kallirroi Georgila,
and Steve Young. 2005. Quantitative
evaluation of user simulation techniques
for spoken dialogue systems. In
Proceedings of the 6th SIGdial Workshop
on Discourse and Dialogue, pages 45?54,
Lisbon.
Schatzmann, Jost, Matthew N. Stuttle,
Karl Weilhammer, and Steve Young.
2005. Effects of the user model on
simulation-based learning of dialogue
strategies. In Proceedings of the IEEE
Automatic Speech Recognition and
Understanding Workshop, pages 220?225,
San Juan, Puerto Rico.
Schatzmann, Jost, Karl Weilhammer,
Matthew N. Stuttle, and Steve Young.
2006. A survey of statistical user
simulation techniques for
reinforcement-learning of dialogue
management strategies. The Knowledge
Engineering Review, 21:97?126.
Scheffler, Konrad and Steve Young. 2001.
Corpus-based dialogue simulation
for automatic strategy learning and
evaluation. In Proceedings of the NAACL
Workshop on Adaptation in Dialogue
Systems, pages 64?70, Pittsburgh, PA.
Scheffler, Konrad and Steve Young.
2002. Automatic learning of dialogue
strategy using dialogue simulation
and reinforcement learning. In Proceedings
of the Human Language Technology
Conference, pages 12?19, San Diego, CA.
Shawe-Taylor, John and Nello Cristianini.
2004. Kernel Methods for Pattern Analysis.
Cambridge University Press.
Singh, Satinder, Michael Kearns, Diane
Litman, and Marilyn Walker. 2000a.
Empirical evaluation of a reinforcement
learning dialogue system. In Proceedings
of the AAAI, pages 645?651, Whistler.
Singh, Satinder, Michael Kearns, Diane
Litman, and Marilyn Walker. 2000b.
Reinforcement learning for spoken
dialogue systems. In Advances in Neural
Information Processing Systems, 12:956?962.
Singh, Satinder, Diane Litman, Michael
Kearns, and Marilyn Walker. 2002.
Optimizing dialogue management with
reinforcement learning: Experiments
with the NJFun system. Journal of
Artificial Intelligence Research (JAIR),
16:105?133.
Sutton, Richard and Andrew Barto. 1998.
Reinforcement Learning. MIT Press,
Cambridge, MA.
510
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
Walker, M., J. Aberdeen, J. Boland, E. Bratt,
J. Garofolo, L. Hirschman, A. Le, S. Lee,
S. Narayanan, K. Papineni, B. Pellom,
B. Polifroni, A. Potamianos, P. Prabhu,
A. Rudnicky, G. Sanders, S. Seneff,
D. Stallard, and S. Whittaker. 2001.
DARPA communicator dialog travel
planning systems: The June 2000 data
collection. In Proceedings of the 7th European
Conference on Speech Communication and
Technology (Interspeech ? Eurospeech),
pages 1371?1374, Aalborg.
Walker, M. and R. Passonneau. 2001. DATE:
A dialogue act tagging scheme for
evaluation of spoken dialogue systems.
In Proceedings of the Human Language
Technology Conference, pages 1?8, San
Diego, CA.
Walker, M., A. Rudnicky, J. Aberdeen,
E. Bratt, J. Garofolo, H. Hastie, A. Le,
B. Pellom, A. Potamianos, R. Passonneau,
R. Prasad, S. Roukos, G. Sanders, S. Seneff,
D. Stallard, and S. Whittaker. 2002a.
DARPA Communicator Evaluation:
Progress from 2000 to 2001. In Proceedings
of the 7th International Conference on Spoken
Language Processing (Interspeech ? ICSLP),
pages 273?276, Denver, CO.
Walker, M., A. Rudnicky, R. Prasad,
J. Aberdeen, E. Bratt, J. Garofolo,
H. Hastie, A. Le, B. Pellom, A. Potamianos,
R. Passonneau, S. Roukos, G. Sanders,
S. Seneff, and D. Stallard. 2002b. DARPA
Communicator: Cross-system results
for the 2001 evaluation. In Proceedings of
the 7th International Conference on Spoken
Language Processing (Interspeech ? ICSLP),
pages 269?272, Denver, CO.
Walker, Marilyn A., Jeanne C. Fromer, and
Shrikanth Narayanan. 1998. Learning
optimal dialogue strategies: A case study
of a spoken dialogue agent for email.
In Proceedings of the 17th International
Conference on Computational Linguistics,
pages 1345?1351, Montreal.
Walker, Marilyn A., Candace A. Kamm,
and Diane J. Litman. 2000. Towards
developing general models of usability
with PARADISE. Natural Language
Engineering, 6(3):363?377.
Walker, Marilyn A., Rebecca J. Passonneau,
and Julie E. Boland. 2001. Quantitative
and qualitative evaluation of DARPA
Communicator spoken dialogue systems.
In Proceedings of the 39th Meeting of the
Association for Computational Linguistics,
pages 515?522, Toulouse.
Williams, Jason. 2007. A method for
evaluating and comparing user
simulations: The Cramer-von Mises
divergence. In Proceedings of the IEEE
Automatic Speech Recognition and
Understanding Workshop, pages 508?513,
Kyoto.
Williams, Jason, Pascal Poupart, and
Steve Young. 2005a. Factored partially
observable Markov decision processes for
dialogue management. In Proceedings of the
4th Workshop on Knowledge and Reasoning in
Practical Dialog Systems, International Joint
Conference on Artificial Intelligence (IJCAI),
pages 76?82, Edinburgh.
Williams, Jason, Pascal Poupart, and Steve
Young. 2005b. Partially observable Markov
decision processes with continuous
observations for dialogue management.
In Proceedings of the 6th SIGdial Workshop
on Discourse and Dialogue, pages 25?34,
Lisbon.
Williams, Jason and Steve Young. 2003. Using
Wizard-of-Oz simulations to bootstrap
reinforcement-learning-based dialog
management systems. In Proceedings of the
4th SIGdial Workshop on Discourse and
Dialogue, pages 135?139, Sapporo.
Williams, Jason and Steve Young. 2005.
Scaling up POMDPs for dialog
management: The ?Summary POMDP?
method. In Proceedings of the IEEE
Automatic Speech Recognition and
Understanding Workshop, pages 177?182,
San Juan, Puerto Rico.
Young, Steve. 2000. Probabilistic methods in
spoken dialogue systems. Philosophical
Transactions of the Royal Society (Series A),
358(1769):1389?1402.
511

Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 49?52,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Simulating the Behaviour of Older versus Younger Users
when Interacting with Spoken Dialogue Systems
Kallirroi Georgila, Maria Wolters and Johanna D. Moore
Human Communication Research Centre
University of Edinburgh
kgeorgil|mwolters|jmoore@inf.ed.ac.uk
Abstract
In this paper we build user simulations of
older and younger adults using a corpus of
interactions with a Wizard-of-Oz appointment
scheduling system. We measure the quality of
these models with standard metrics proposed
in the literature. Our results agree with predic-
tions based on statistical analysis of the cor-
pus and previous findings about the diversity
of older people?s behaviour. Furthermore, our
results show that these metrics can be a good
predictor of the behaviour of different types of
users, which provides evidence for the validity
of current user simulation evaluation metrics.
1 Introduction
Using machine learning to induce dialogue man-
agement policies requires large amounts of training
data, and thus it is typically not feasible to build
such models solely with data from real users. In-
stead, data from real users is used to build simulated
users (SUs), who then interact with the system as
often as needed. In order to learn good policies, the
behaviour of the SUs needs to cover the range of
variation seen in real users (Schatzmann et al, 2005;
Georgila et al, 2006). Furthermore, SUs are critical
for evaluating candidate dialogue policies.
To date, several techniques for building SUs have
been investigated and metrics for evaluating their
quality have been proposed (Schatzmann et al,
2005; Georgila et al, 2006). However, to our knowl-
edge, no one has tried to build user simulations
for different populations of real users and measure
whether results from evaluating the quality of those
simulations agree with what is known about those
particular types of real users, extracted from other
studies of those populations. This is presumably due
to the lack of corpora for different types of users.
In this paper we focus on the behaviour of older
vs. younger adults. Most of the work to date on di-
alogue systems focuses on young users. However,
as average life expectancy increases, it becomes in-
creasingly important to design dialogue systems in
such a way that they can accommodate older peo-
ple?s behaviour. Older people are a user group with
distinct needs and abilities (Czaja and Lee, 2007)
that present challenges for user modelling. To our
knowledge no one so far has built statistical user
simulation models for older people. The only sta-
tistical spoken dialogue system for older people we
are aware of is Nursebot, an early application of sta-
tistical methods (POMDPs) within the context of a
medication reminder system (Roy et al, 2000).
In this study, we build SUs for both younger and
older adults using n-grams. Our data comes from a
fully annotated corpus of 447 interactions of older
and younger users with a Wizard-of-Oz (WoZ) ap-
pointment scheduling system (Georgila et al, 2008).
We then evaluate these models using standard met-
rics (Schatzmann et al, 2005; Georgila et al, 2006)
and compare our findings with the results of statisti-
cal corpus analysis.
The novelty of our work lies in two areas. First,
to the best of our knowledge this is the first time that
statistical SUs have been built for the increasingly
important population of older users.
Secondly, a general (but as yet untested) assump-
tion in this field is that current SUs are ?enough like?
real users for training good policies, and that testing
system performance in simulated dialogues is an ac-
curate indication of how a system will perform with
human users. The validity of these assumptions is
49
a critically important open research question. Cur-
rently one of the standard methods for evaluating
the quality of a SU is to run a user simulation on
a real corpus and measure how often the action gen-
erated by the SU agrees with the action observed in
the corpus (Schatzmann et al, 2005; Georgila et al,
2006). This method can certainly give us some in-
sight into how strongly a SU resembles a real user,
but the validity of the metrics used remains an open
research problem. In this paper, we take this a step
further. We measure the quality of user simulation
models for both older and younger users, and show
that these metrics are a good predictor of the be-
haviour of those two user types.
The structure of the paper is as follows: In sec-
tion 2 we describe our data set. In section 3 we
discuss the differences between older and younger
users as measured in our corpus using standard sta-
tistical techniques. Then in section 4 we present our
user simulations. Finally in section 5 we present our
conclusions and propose future work.
2 The Corpus
The dialogue corpus which our simulations are
based on was collected during a controlled experi-
ment where we systematically varied: (1) the num-
ber of options that users were presented with (one
option, two options, four options); (2) the confirma-
tion strategy employed (explicit confirmation, im-
plicit confirmation, no confirmation). The combina-
tion of these 3? 3 design choices yielded 9 different
dialogue systems.
Participants were asked to schedule a health care
appointment with each of the 9 systems, yielding a
total of 9 dialogues per participant. System utter-
ances were generated using a simple template-based
algorithm and synthesised using the speech synthe-
sis system Cerevoice (Aylett et al, 2006), which has
been shown to be intelligible to older users (Wolters
et al, 2007). The human wizard took over the func-
tion of the speech recognition, language understand-
ing, and dialogue management components.
Each dialogue corresponded to a fixed schema:
First, users arranged to see a specific health care pro-
fessional, then they arranged a specific half-day, and
finally, a specific half-hour time slot on that half-day
was agreed. In a final step, the wizard confirmed the
appointment.
The full corpus consists of 447 dialogues; 3 di-
alogues were not recorded. A total of 50 partici-
pants were recruited, of which 26 were older (50?
85) and 24 were younger (20?30). The older users
contributed 232 dialogues, the younger ones 215.
Older and younger users were matched for level of
education and gender.
All dialogues were transcribed orthographically
and annotated with dialogue acts and dialogue con-
text information. Using a unique mapping, we as-
sociate each dialogue act with a ?speech act, task?
pair, where the speech act is task independent and
the task corresponds to the slot in focus (health pro-
fessional, half-day or time slot). For each dialogue,
five measures of dialogue quality were recorded: ob-
jective task completion, perceived task completion,
appointment recall, length (in turns), and detailed
user satisfaction ratings. A detailed description of
the corpus design, statistics, and annotation scheme
is provided in (Georgila et al, 2008).
Our analysis of the corpus shows that there are
clear differences in the way users interact with the
systems. Since it is these differences that good user
simulations need to capture, the most relevant find-
ings for the present study are summarised in the next
section.
3 Older vs. Younger Users
Since the user simulations (see section 4) are based
mainly on dialogue act annotations, we will use
speech act statistics to illustrate some key differ-
ences in behaviour between older and younger users.
User speech acts were grouped into four categories
that are relevant to dialogue management: speech
acts that result in grounding (ground), speech acts
that result in confirmations (confirm) (note, this
category overlaps with ground and occurs after the
system has explicitly or implicitly attempted to con-
firm the user?s response), speech acts that indicate
user initiative (init), and speech acts that indi-
cate social interaction with the system (social).
We also computed the average number of different
speech act types used, the average number of speech
act tokens, and the average token/type ratio per user.
Results are given in Table 1.
There are 28 distinct user speech acts (Georgila et
al., 2008). Older users not only produce more indi-
vidual speech acts, they also use a far richer variety
of speech acts, on average 14 out of 28 as opposed to
9 out of 28. The token/type ratio remains the same,
however. Although the absolute frequency of confir-
mation and grounding speech acts is approximately
50
Variable Older Younger Sig.
# speech act types 14 9 ***
# speech act tokens 126 73 ***
Sp. act tokens/types 8.7 8.5 n.s.
# Confirm 31 30 n.s.
% Confirm 28.3 41.5 ***
# Ground 33 30 n.s.
% Ground 29.4 41.7 ***
# Social 26 5 ***
% Social 17.9 5.3 ***
# Init 15 3 ***
% Init 9.0 3.4 **
Table 1: Behaviour of older vs. younger users. Numbers
are summed over all dialogues and divided by the num-
ber of users. *: p<0.01, **: p<0.005, ***: p<0.001 or
better.
the same for younger and older users, the relative
frequency of these types of speech acts is far lower
for older than for younger users, because older users
are far more likely to take initiative by providing ad-
ditional information to the system and speech acts
indicating social interaction. Based on this analysis
alone, we would predict that user simulations trained
on younger users only will not fare well when tested
on older users, because the behaviour of older users
is richer and more complex.
But do older and younger users constitute two
separate groups, or are there older users that be-
have like younger ones? In the first case, we can-
not use data from older people to create simulations
of younger users? behaviour. In the second case,
data from older users might be sufficient to approx-
imately cover the full range of behaviour we see in
the data. The boxplots given in Fig. 1 indicate that
the latter is in fact true. Even though the means
differ considerably between the two groups, older
users? behaviour shows much greater variation than
that of younger users. For example, for user initia-
tive, the main range of values seen for older users
includes the majority of values observed for younger
users.
4 User Simulations
We performed 5-fold cross validation ensuring that
there was no overlap in speakers between different
folds. Each user utterance corresponds to a user ac-
tion annotated as a list of ?speech act, task? pairs.
For example, the utterance ?I?d like to see the di-
abetes nurse on Thursday morning? could be an-
notated as [(accept info, hp), (provide info, half-
Figure 1: Relative frequency of (a) grounding and (b)
user initiative.
day)] or similarly, depending on the previous sys-
tem prompt. There are 389 distinct actions for older
people and 125 for younger people. The actions of
the younger people are a subset of the actions of the
older people.
We built n-grams of system and user actions with
n varying from 2 to 5. Given a history of system and
user actions (n-1 actions) the SU generates an action
based on a probability distribution learned from the
training data (Georgila et al, 2006). We tested four
values of n, 2, 3, 4, and 5. For reasons of space, we
only report results from 3-grams because they suffer
less from data sparsity than 4- and 5-grams and take
into account larger contexts than 2-grams. However,
results are similar for all values of n.
The actions generated by our SUs were compared
to the actions observed in the corpus using five met-
rics proposed in the literature (Schatzmann et al,
2005; Georgila et al, 2006): perplexity (PP), preci-
sion, recall, expected precision and expected recall.
While precision and recall are calculated based on
the most likely action at a given state, expected pre-
cision and expected recall take into account all pos-
sible user actions at a given state. Details are given
in (Georgila et al, 2006). In our cross-validation
experiments, we used three different sources for the
training and test sets: data from older users (O), data
51
PP Prec Rec ExpPrec ExpRec
O-O 18.1 42.8 39.8 56.0 49.4
Y-O 19.6 34.2 25.1 53.4 40.7
A-O 18.7 41.1 35.9 58.9 49.0
O-Y 5.7 44.8 60.6 66.3 73.4
Y-Y 3.7 50.5 54.1 73.1 70.4
A-Y 3.8 45.8 58.5 70.5 73.0
O-A 10.3 43.7 47.2 60.3 58.0
Y-A 9.3 40.3 33.3 62.0 51.5
A-A 9.3 43.2 43.4 63.9 57.9
Table 2: Results for 3-grams and different combinations
of training and test data. O: older users, Y: younger users,
A: all users.
from younger users (Y), and data from all users (A).
Our results are summarised in Table 2.
We find that models trained on younger users, but
tested on older users (Y-O) perform worse than mod-
els trained on older users / all users and tested on
older users (O-O, A-O). Thus, models of the be-
haviour of younger users cannot be used to simulate
older users. In addition, models which are trained
on older users tend to generalise better to the whole
data set (O-A) than models trained only on younger
users (Y-A). These results are in line with our sta-
tistical analysis, which showed that the behaviour
of younger users appears to be a subset of the be-
haviour of older users. All results are statistically
significant at p<0.05 or better.
5 Conclusions
In this paper we built user simulations for older
and younger adults and evaluated them using stan-
dard metrics. Our results suggest that SUs trained
on older people may also cover the behaviour of
younger users, but not vice versa. This finding
supports the principle of ?inclusive design? (Keates
and Clarkson, 2004): designers should consider a
wide range of users when developing a product for
general use. Furthermore, our results agree with
predictions based on statistical analysis of our cor-
pus. They are also in line with findings of tests of
deployed Interactive Voice Response systems with
younger and older users (Dulude, 2002), which
show the diversity of older people?s behaviour.
Therefore, we have shown that standard metrics for
evaluating SUs are a good predictor of the behaviour
of our two user types. Overall, the metrics we used
yielded a clear and consistent picture. Although our
result needs to be verified on similar corpora, it has
an important implication for corpus design. In order
to yield realistic models of user behaviour, we need
to gather less data from students, and more data from
older and middle-aged users.
In our future work, we will perform more detailed
statistical analyses of user behaviour. In particular,
we will analyse the effect of dialogue strategies on
behaviour, experiment with different Bayesian net-
work structures, and use the resulting user simula-
tions to learn dialogue strategies for both older and
younger users as another way for testing the accu-
racy of our user models and validating our results.
Acknowledgements
This research was supported by the Wellcome Trust VIP
grant and the Scottish Funding Council grant MATCH
(HR04016). We would like to thank Robert Logie and
Sarah MacPherson for contributing to the design of the
original experiment, Neil Mayo and Joe Eddy for coding
the Wizard-of-Oz interface, Vasilis Karaiskos and Matt
Watson for collecting the data, and Melissa Kronenthal
for transcribing the dialogues.
References
M. Aylett, C. Pidcock, and M.E. Fraser. 2006. The
Cerevoice Blizzard Entry 2006: A prototype database
unit selection engine. In Proc. BLIZZARD Challenge.
S. Czaja and C. Lee. 2007. The impact of aging on ac-
cess to technology. Universal Access in the Informa-
tion Society (UAIS), 5:341?349.
L. Dulude. 2002. Automated telephone answering sys-
tems and aging. Behaviour Information Technology,
21:171?184.
K. Georgila, J. Henderson, and O. Lemon. 2006. User
simulation for spoken dialogue systems: Learning and
evaluation. In Proc. Interspeech/ICSLP.
K. Georgila, M. Wolters, V. Karaiskos, M. Kronenthal,
R. Logie, N. Mayo, J. Moore, and M. Watson. 2008.
A fully annotated corpus for studying the effect of cog-
nitive ageing on users? interactions with spoken dia-
logue systems. In Proc. LREC.
S. Keates and J. Clarkson. 2004. Inclusive Design.
Springer, London.
N. Roy, J. Pineau, and S. Thrun. 2000. Spoken dialog
management for robots. In Proc. ACL.
J. Schatzmann, K. Georgila, and S. Young. 2005. Quan-
titative evaluation of user simulation techniques for
spoken dialogue systems. In Proc. SIGdial.
M. Wolters, P. Campbell, C. DePlacido, A. Liddell, and
D. Owens. 2007. Making synthetic speech accessi-
ble to older people. In Proc. Sixth ISCA Workshop on
Speech Synthesis, Bonn, Germany.
52
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 500?510,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Single-Agent vs. Multi-Agent Techniques for Concurrent
Reinforcement Learning of Negotiation Dialogue Policies
Kallirroi Georgila, Claire Nelson, David Traum
University of Southern California Institute for Creative Technologies
12015 Waterfront Drive, Playa Vista, CA 90094, USA
{kgeorgila,traum}@ict.usc.edu
Abstract
We use single-agent and multi-agent Rein-
forcement Learning (RL) for learning dia-
logue policies in a resource allocation ne-
gotiation scenario. Two agents learn con-
currently by interacting with each other
without any need for simulated users
(SUs) to train against or corpora to learn
from. In particular, we compare the Q-
learning, Policy Hill-Climbing (PHC) and
Win or Learn Fast Policy Hill-Climbing
(PHC-WoLF) algorithms, varying the sce-
nario complexity (state space size), the
number of training episodes, the learning
rate, and the exploration rate. Our re-
sults show that generally Q-learning fails
to converge whereas PHC and PHC-WoLF
always converge and perform similarly.
We also show that very high gradually
decreasing exploration rates are required
for convergence. We conclude that multi-
agent RL of dialogue policies is a promis-
ing alternative to using single-agent RL
and SUs or learning directly from corpora.
1 Introduction
The dialogue policy of a dialogue system decides
on which actions the system should perform given
a particular dialogue state (i.e., dialogue context).
Building a dialogue policy can be a challenging
task especially for complex applications. For this
reason, recently much attention has been drawn
to machine learning approaches to dialogue man-
agement and in particular Reinforcement Learning
(RL) of dialogue policies (Williams and Young,
2007; Rieser et al, 2011; Jur?c???cek et al, 2012).
Typically there are three main approaches to
the problem of learning dialogue policies using
RL: (1) learn against a simulated user (SU), i.e.,
a model that simulates the behavior of a real user
(Georgila et al, 2006; Schatzmann et al, 2006);
(2) learn directly from a corpus (Henderson et al,
2008; Li et al, 2009); or (3) learn via live interac-
tion with human users (Singh et al, 2002; Ga?si?c et
al., 2011; Ga?si?c et al, 2013).
We propose a fourth approach: concurrent
learning of the system policy and the SU policy
using multi-agent RL techniques. Both agents are
trained simultaneously and there is no need for
building a SU separately or having access to a cor-
pus.
1
As we discuss below, concurrent learning
could potentially be used for learning via live in-
teraction with human users. Moreover, for negoti-
ation in particular there is one more reason in fa-
vor of concurrent learning as opposed to learning
against a SU. Unlike slot-filling domains, in nego-
tiation the behaviors of the system and the user are
symmetric. They are both negotiators, thus build-
ing a good SU is as difficult as building a good
system policy.
So far research on using RL for dialogue pol-
icy learning has focused on single-agent RL tech-
niques. Single-agent RL methods make the as-
sumption that the system learns by interacting with
a stationary environment, i.e., an environment that
does not change over time. Here the environ-
ment is the user. Generally the assumption that
users do not significantly change their behavior
over time holds for simple information providing
tasks (e.g., reserving a flight). But this is not nec-
essarily the case for other genres of dialogue, in-
cluding negotiation. Imagine a situation where a
negotiator is so uncooperative and arrogant that
the other negotiators decide to completely change
their negotiation strategy in order to punish her.
Therefore it is important to investigate RL ap-
proaches that do not make such assumptions about
the user/environment.
1
Though corpora or SUs may still be useful for bootstrap-
ping the policies and encoding real user behavior (see sec-
tion 6).
500
Multi-agent RL is designed to work for non-
stationary environments. In this case the envi-
ronment of a learning agent is one or more other
agents that can also be learning at the same time.
Therefore, unlike single-agent RL, multi-agent RL
can handle changes in user behavior or in the be-
havior of other agents participating in the inter-
action, and thus potentially lead to more realis-
tic dialogue policies in complex dialogue scenar-
ios. This ability of multi-agent RL can also have
important implications for learning via live inter-
action with human users. Imagine a system that
learns to change its strategy as it realizes that a
particular user is no longer a novice user, or that a
user no longer cares about five star restaurants.
We apply multi-agent RL to a resource alloca-
tion negotiation scenario. Two agents with dif-
ferent preferences negotiate about how to share
resources. We compare Q-learning (a single-
agent RL algorithm) with two multi-agent RL al-
gorithms: Policy Hill-Climbing (PHC) and Win
or Learn Fast Policy Hill-Climbing (PHC-WoLF)
(Bowling and Veloso, 2002). We vary the scenario
complexity (i.e., the quantity of resources to be
shared and consequently the state space size), the
number of training episodes, the learning rate, and
the exploration rate.
Our research contributions are as follows: (1)
we propose concurrent learning using multi-agent
RL as a way to deal with some of the issues of cur-
rent approaches to dialogue policy learning (i.e.,
the need for SUs and corpora), which may also
potentially prove useful for learning via live inter-
action with human users; (2) we show that concur-
rent learning can address changes in user behav-
ior over time, and requires multi-agent RL tech-
niques and variable exploration rates; (3) to our
knowledge this is the first time that PHC and PHC-
WoLF are used for learning dialogue policies; (4)
for the first time, the above techniques are applied
to a negotiation domain; and (5) this is the first
study that compares Q-learning, PHC, and PHC-
WoLF in such a variety of situations (varying a
large number of parameters).
The paper is structured as follows. Section 2
presents related work. Section 3 provides a brief
introduction to single-agent RL and multi-agent
RL. Section 4 describes our negotiation domain
and experimental setup. In section 5 we present
our results. Finally, section 6 concludes and pro-
vides some ideas for future work.
2 Related Work
Most research in RL for dialogue management has
been done in the framework of slot-filling applica-
tions such as restaurant recommendations (Lemon
et al, 2006; Thomson and Young, 2010; Ga?si?c
et al, 2012; Daubigney et al, 2012), flight reser-
vations (Henderson et al, 2008), sightseeing rec-
ommendations (Misu et al, 2010), appointment
scheduling (Georgila et al, 2010), etc. RL has
also been applied to question-answering (Misu et
al., 2012), tutoring domains (Tetreault and Litman,
2008; Chi et al, 2011), and learning negotiation
dialogue policies (Heeman, 2009; Georgila and
Traum, 2011; Georgila, 2013).
As mentioned in section 1, there are three main
approaches to the problem of learning dialogue
policies using RL.
In the first approach, a SU is hand-crafted or
learned from a small corpus of human-human or
human-machine dialogues. Then the dialogue pol-
icy can be learned by having the system interact
with the SU for a large number of dialogues (usu-
ally thousands of dialogues). Depending on the
application, building a realistic SU can be just as
difficult as building a good dialogue policy. Fur-
thermore, it is not clear what constitutes a good
SU for dialogue policy learning. Should the SU
resemble real user behavior as closely as possi-
ble, or should it exhibit some degree of random-
ness to explore a variety of interaction patterns?
Despite much research on the issue, these are still
open questions (Schatzmann et al, 2006; Ai and
Litman, 2008; Pietquin and Hastie, 2013).
In the second approach, no SUs are required.
Instead the dialogue policy is learned directly from
a corpus of human-human or human-machine dia-
logues. For example, Henderson et al (2008) used
a combination of RL and supervised learning to
learn a dialogue policy in a flight reservation do-
main, whereas Li et al (2009) used Least-Squares
Policy Iteration (Lagoudakis and Parr, 2003), an
RL-based technique that can learn directly from
corpora, in a voice dialer application. However,
collecting such corpora is not trivial, especially in
new domains. Typically, data are collected in a
Wizard-of-Oz setup where human users think that
they interact with a system while in fact they inter-
act with a human pretending to be the system, or
by having human users interact with a preliminary
version of the dialogue system. In both cases the
resulting interactions are expected to be quite dif-
501
ferent from the interactions of human users with
the final system. In practice this means that dia-
logue policies learned from such data could be far
from optimal.
The first experiment on learning via live inter-
action with human users (third approach) was re-
ported by Singh et al (2002). They used RL to
help the system with two choices: how much ini-
tiative it should allow the user, and whether or not
to confirm information provided by the user. Re-
cently, learning of ?full? dialogue policies (not just
choices at specific points in the dialogue) via live
interaction with human users has become possi-
ble with the use of Gaussian processes (Engel et
al., 2005; Rasmussen and Williams, 2006). Typi-
cally learning a dialogue policy is a slow process
requiring thousands of dialogues, hence the need
for SUs. Gaussian processes have been shown to
speed up learning. This fact together with easy
access to a large number of human users through
crowd-sourcing has allowed dialogue policy learn-
ing via live interaction with human users (Ga?si?c et
al., 2011; Ga?si?c et al, 2013).
Space constraints prevent us from providing an
exhaustive list of previous work on using RL for
dialogue management. Thus below we focus only
on research that is directly related to our work,
specifically research on concurrent learning of the
policies of multiple agents, and the application of
RL to negotiation domains.
So far research on RL in the dialogue commu-
nity has focused on using single-agent RL tech-
niques where the stationary environment is the
user. Most approaches assume that the user goal
is fixed and that the behavior of the user is ratio-
nal. Other approaches account for changes in user
goals (Ma, 2013). In either case, one can build a
user simulation model that is the average of dif-
ferent user behaviors or learn a policy from a cor-
pus that contains a variety of interaction patterns,
and thus safely assume that single-agent RL tech-
niques will work. However, in the latter case if
the behavior of the user changes significantly over
time then the assumption that the environment is
stationary will no longer hold.
There has been a lot of research on multi-agent
RL in the optimal control and robotics communi-
ties (Littman, 1994; Hu andWellman, 1998; Buso-
niu et al, 2008). Here two or more agents learn si-
multaneously. Thus the environment of an agent is
one or more other agents that continuously change
their behavior because they are also learning at the
same time. Therefore the environment is no longer
stationary and single-agent RL techniques do not
work well or do not work at all. We are particu-
larly interested in the work of Bowling and Veloso
(2002) who proposed the PHC and PHC-WoLF al-
gorithms that we use in this paper. We chose these
two algorithms because, unlike other multi-agent
RL methods (Littman, 1994; Hu and Wellman,
1998), they do not make assumptions that do not
always hold and do not require quadratic or linear
programming that does not always scale.
English and Heeman (2005) were the first in the
dialogue community to explore the idea of con-
current learning of dialogue policies. However,
English and Heeman (2005) did not use multi-
agent RL but only standard single-agent RL, in
particular an on-policy Monte Carlo method (Sut-
ton and Barto, 1998). But single-agent RL tech-
niques are not well suited for concurrent learning
where each agent is trained against a continuously
changing environment. Indeed, English and Hee-
man (2005) reported problems with convergence.
Chandramohan et al (2012) proposed a frame-
work for co-adaptation of the dialogue policy and
the SU using single-agent RL. They applied In-
verse Reinforcement Learning (IRL) (Abbeel and
Ng, 2004) to a corpus in order to learn the reward
functions of both the system and the SU. Further-
more, Cuay?ahuitl and Dethlefs (2012) used hier-
archical multi-agent RL for co-ordinating the ver-
bal and non-verbal actions of a robot. Cuay?ahuitl
and Dethlefs (2012) did not use PHC or PHC-
WoLF and did not compare against single-agent
RL methods.
With regard to using RL for learning negotia-
tion policies, the amount of research that has been
performed is very limited compared to slot-filling.
English and Heeman (2005) learned negotiation
policies for a furniture layout task. Then Hee-
man (2009) extended this work by experiment-
ing with different representations of the RL state
in the same domain (this time learning against
a hand-crafted SU). In both cases, to reduce the
search space, the RL state included only infor-
mation about e.g., whether there was a pending
proposal rather than the actual value of this pro-
posal. Paruchuri et al (2009) performed a theo-
retical study on how Partially Observable Markov
Decision Processes (POMDPs) can be applied to
negotiation domains.
502
Georgila and Traum (2011) built argumentation
dialogue policies for negotiation against users of
different cultural norms in a one-issue negotiation
scenario. To learn these policies they trained SUs
on a spoken dialogue corpus in a florist-grocer
negotiation domain, and then tweaked these SUs
towards a particular cultural norm using hand-
crafted rules. Georgila (2013) learned argumen-
tation dialogue policies from a simulated corpus
in a two-issue negotiation scenario (organizing a
party). Finally, Nouri et al (2012) used IRL to
learn a model for cultural decision-making in a
simple negotiation game (the Ultimatum Game).
3 Single-Agent vs. Multi-Agent
Reinforcement Learning
Reinforcement Learning (RL) is a machine learn-
ing technique used to learn the policy of an agent,
i.e., which action the agent should perform given
its current state (Sutton and Barto, 1998). The goal
of an RL-based agent is to maximize the reward it
gets during an interaction. Because it is very dif-
ficult for the agent to know what will happen in
the rest of the interaction, the agent must select an
action based on the average reward it has previ-
ously observed after having performed that action
in similar contexts. This average reward is called
expected future reward. Single-agent RL is used
in the framework of Markov Decision Processes
(MDPs) (Sutton and Barto, 1998) or Partially Ob-
servable Markov Decision Processes (POMDPs)
(Williams and Young, 2007). Here we focus on
MDPs.
An MDP is defined as a tuple (S, A, T , R, ?)
where S is the set of states (representing different
contexts) which the agent may be in, A is the set
of actions of the agent, T is the transition func-
tion S ? A ? S ? [0, 1] which defines a set of
transition probabilities between states after taking
an action, R is the reward function S ? A ? <
which defines the reward received when taking an
action from the given state, and ? is a factor that
discounts future rewards. Solving the MDP means
finding a policy pi : S ? A. The quality of the
policy pi is measured by the expected discounted
(with discount factor ?) future reward also called
Q-value, Q
pi
: S ? A?<.
A stochastic game is defined as a tuple (n, S,
A
1...n
, T , R
1...n
, ?) where n is the number of
agents, S is the set of states, A
i
is the set of ac-
tions available for agent i (and A is the joint ac-
tion space A
1
? A
2
? ... ? A
n
), T is the transi-
tion function S ? A ? S ? [0, 1] which defines
a set of transition probabilities between states af-
ter taking a joint action, R
i
is the reward function
for the ith agent S ? A ? <, and ? is a factor
that discounts future rewards. The goal is for each
agent i to learn a mixed policy pi
i
: S ? A
i
? [0,
1] that maps states to mixed strategies, which are
probability distributions over the agent?s actions,
so that the agent?s expected discounted (with dis-
count factor ?) future reward is maximized.
Stochastic games are a generalization of MDPs
for multi-agent RL. In stochastic games there are
many agents that select actions and the next state
and rewards depend on the joint action of all these
agents. The agents can have different reward
functions. Partially Observable Stochastic Games
(POSGs) are the equivalent of POMDPs for multi-
agent RL. In POSGs, the agents have different ob-
servations, and uncertainty about the state they are
in and the beliefs of their interlocutors. POSGs
are very hard to solve but new algorithms continu-
ously emerge in the literature.
In this paper we use three algorithms: Q-
learning, Policy Hill-Climbing (PHC), and Win
or Learn Fast Policy Hill-Climbing (PHC-WoLF).
PHC is an extension of Q-learning. For all three
algorithms, Q-values are updated as follows:
Q(s, a)? (1??)Q(s, a)+?
(
r + ?max
a
?
Q(s
?
, a
?
)
)
(1)
In Q-learning, for a given state s, the agent
performs the action with the highest Q-value for
that state. In addition to Q-values, PHC and
PHC-WoLF also maintain the current mixed pol-
icy pi(s, a). In each step the mixed policy is up-
dated by increasing the probability of selecting the
highest valued action according to a learning rate
? (see equations (2), (3), and (4) below).
pi(s, a)? pi(s, a) + ?
sa
(2)
?
sa
=
{
??
sa
if a 6= argmax
a
?
Q(s, a
?
)
?
a
?
6=a
?
sa
?
otherwise
(3)
?
sa
= min
(
pi(s, a),
?
|A
i
| ? 1
)
(4)
The difference between PHC and PHC-WoLF is
that PHC uses a constant learning rate ? whereas
503
PHC-WoLF uses a variable learning rate (see
equation (5) below). The main idea is that when
the agent is ?winning? the learning rate ?
W
should
be low so that the opponents have more time to
adapt to the agent?s policy, which helps with con-
vergence. On the other hand when the agent is
?losing? the learning rate ?
LF
should be high so
that the agent has more time to adapt to the other
agents? policies, which also facilitates conver-
gence. Thus PHC-WoLF uses two learning rates
?
W
and ?
LF
. PHC-WoLF determines whether the
agent is ?winning? or ?losing? by comparing the
current policy?s pi(s, a) expected payoff with that
of the average policy p?i(s, a) over time. If the cur-
rent policy?s expected payoff is greater then the
agent is ?winning?, otherwise it is ?losing?.
? =
?
?
?
?
?
?
?
?
W
if
{
?
?
?
pi(s, ?
?
)Q(s, ?
?
) >
?
?
?
p?i(s, ?
?
)Q(s, ?
?
)
?
LF
otherwise
(5)
More details about Q-learning, PHC, and PHC-
WoLF can be found in (Sutton and Barto, 1998;
Bowling and Veloso, 2002).
As discussed in sections 1 and 2, single-agent
RL techniques, such as Q-learning, are not suit-
able for multi-agent RL. Nevertheless, despite its
shortcomings Q-learning has been used success-
fully for multi-agent RL (Claus and Boutilier,
1998). Indeed, as we see in section 5, Q-learning
can converge to the optimal policy for small state
spaces. However, as the state space size increases
the performance of Q-learning drops (compared to
PHC and PHC-WoLF).
4 Domain and Experimental Setup
Our domain is a resource allocation negotiation
scenario. Two agents negotiate about how to share
resources. For the sake of readability from now on
we will refer to apples and oranges.
The two agents have different goals. Also,
they have human-like constraints of imperfect in-
formation about each other; they do not know
each other?s reward function or degree of rational-
ity (during learning our agents can be irrational).
Thus a Nash equilibrium (if there exists one) can-
not be computed in advance. Agent 1 cares more
about apples and Agent 2 cares more about or-
anges. Table 1 shows the points that Agents 1
and 2 earn for each apple and each orange that they
have at the end of the negotiation.
Agent 1 Agent 2
apple 300 200
orange 200 300
Table 1: Points earned by Agents 1 and 2 for each
apple and each orange that they have at the end of
the negotiation.
Agent 1: offer-2-2 (I offer you 2 A and 2 O)
Agent 2: offer-3-0 (I offer you 3 A and 0 O)
Agent 1: offer-0-3 (I offer you 0 A and 3 O)
Agent 2: offer-4-0 (I offer you 4 A and 0 O)
Agent 1: accept (I accept your offer)
Figure 1: Example interaction between Agents 1
and 2 (A: apples, O: oranges).
We use a simplified dialogue model with two
types of speech acts: offers and acceptances. The
dialogue proceeds as follows: one agent makes an
offer, e.g., ?I give you 3 apples and 1 orange?, and
the other agent may choose to accept it or make a
new offer. The negotiation finishes when one of
the agents accepts the other agent?s offer or time
runs out.
We compare Q-learning with PHC and PHC-
WoLF. For all algorithms and experiments each
agent is rewarded only at the end of the dialogue
based on the negotiation outcome (see Table 1).
Thus the two agents have different reward func-
tions. There is also a penalty of -10 for each agent
action to ensure that dialogues are not too long.
Also, to avoid long dialogues, if none of the agents
accepts the other agent?s offers, the negotiation
finishes after 20 pairs of exchanges between the
two agents (20 offers from Agent 1 and 20 offers
from Agent 2).
An example interaction between the two agents
is shown in Figure 1. As we can see, each agent
can offer any combination of apples and oranges.
So if we haveX apples and Y oranges for sharing,
there can be (X + 1) ? (Y + 1) possible offers.
For example if we have 2 apples and 2 oranges
for sharing, there can be 9 possible offers: ?offer-
0-0?, ?offer-0-1?, ..., ?offer-2-2?. For our exper-
iments we vary the number of fruits to be shared
and choose to keep X equal to Y .
Table 2 shows our state representation, i.e., the
state variables that we keep track of with all the
possible values they can take, whereX is the num-
504
Current offer: (X + 1) ? (Y + 1) possible
values
How many times the current offer has already
been rejected: (0, 1, 2, 3, or 4)
Is the current offer accepted: yes, no
Table 2: State variables.
ber of apples and Y is the number of oranges to be
shared. The third variable is always set to ?no? un-
til one of the agents accepts the other agent?s offer.
Table 3 shows the state and action space sizes
for different numbers of apples and oranges to be
shared used in our experiments below. The num-
ber of actions includes the acceptance of an of-
fer. Table 3 also shows the number of state-action
pairs (Q-values). As we will see in section 5, even
though the number of states for each agent is not
large, it takes many iterations and high exploration
rates for convergence due to the fact that both
agents are learning at the same time and the as-
sumption of interacting with a stationary environ-
ment no longer holds. For comparison, in (English
and Heeman, 2005) the state specification for each
agent included 5 binary variables resulting in 32
possible states. English and Heeman (2005) kept
track of whether there was an offer on the table but
not of the actual value of the offer. For our task it
is essential to keep track of the offer values, which
of course results in much larger state spaces. Also,
in (English and Heeman, 2005) there were 5 possi-
ble actions resulting in 160 state-action pairs. Our
state and action spaces are much larger and fur-
thermore we explore the effect of different state
and action space sizes on convergence.
During learning the two agents interact for
5 epochs. Each epoch contains N number of
episodes. We vary N from 25,000 up to 400,000
with a step of 25,000 episodes. English and Hee-
man (2005) trained their agents for 200 epochs,
where each epoch contained 200 episodes.
We also vary the exploration rate per epoch.
In particular, in the experiments reported in sec-
tion 5.1 the exploration rate is set as follows: 0.95
for epoch 1, 0.8 for epoch 2, 0.5 for epoch 3, 0.3
for epoch 4, and 0.1 for epoch 5. Section 5.2 re-
ports results again with 5 epochs of training but a
constant exploration rate per epoch set to 0.3. An
exploration rate of 0.3 means that 30% of the time
the agent will select an action randomly.
Finally, we vary the learning rate. For PHC-
#States #Actions #State-Action
Pairs
1 A & O 40 5 200
2 A & O 90 10 900
3 A & O 160 17 2720
4 A & O 250 26 6500
5 A & O 360 37 13320
6 A & O 490 50 24500
7 A & O 640 65 41600
Table 3: State space, action space, and state-action
space sizes for different numbers of apples and or-
anges to be shared (A: apples, O: oranges).
WoLF we set ?
W
= 0.05 and ?
LF
= 0.2 (see sec-
tion 3). These values were chosen with exper-
imentation and the basic idea is that the agent
should learn faster when ?losing? and slower when
?winning?. For PHC we explore two cases. In the
first case which from now on will be referred to
as PHC-W, we set ? to be equal to ?
W
(also used
for PHC-WoLF). In the second case which from
now on will be referred to as PHC-LF, we set ?
to be equal to ?
LF
(also used for PHC-WoLF). So
unlike PHC-WoLF, PHC-W and PHC-LF do not
use a variable learning rate. PHC-W always learns
slowly and PHC-LF always learns fast.
In all the above cases, training stops after 5
epochs. Then we test the learned policies against
each other for one more epoch the size of which is
the same as the size of the epochs used for train-
ing. For example, if the policies were learned
for 5 epochs with each epoch containing 25,000
episodes, then for testing the two policies will in-
teract for another 25,000 episodes. For compari-
son, English and Heeman (2005) had their agents
interact for 5,000 dialogues during testing. To en-
sure that the policies do not converge by chance,
we run the training and test sessions 20 times each
and we report averages. Thus all results presented
in section 5 are averages of 20 runs.
5 Results
Given that Agent 1 is more interested in apples
and Agent 2 cares more about oranges, the maxi-
mum total utility solution would be the case where
each agent offers to get al the fruits it cares about
and to give its interlocutor all the fruits it does not
care about, and the other agent accepts this of-
fer. Thus, when converging to the maximum to-
tal utility solution, in the case of 4 fruits (4 ap-
505
ples and 4 oranges), the average reward of the
two agents should be 1200 minus 10 for making
or accepting an offer. For 5 fruits the average re-
ward should be 1500 minus 10, and so forth. We
call 1200 (or 1500) the convergence reward, i.e.,
the reward after converging to the maximum to-
tal utility solution if we do not take into account
the action penalty. For example, in the case of 4
fruits, if Agent 1 starts the negotiation, after con-
verging to the maximum total utility solution the
optimal interaction should be: Agent 1 makes an
offer to Agent 2, namely 0 apples and 4 oranges,
and Agent 2 accepts. Thus the reward for Agent 1
is 1190, the reward for Agent 2 is 1190, and the av-
erage reward of the two agents is also 1190. Also,
the convergence reward for Agent 1 is 1200 and
the convergence reward for Agent 2 is also 1200.
Below, in all the graphs that we provide, we
show the average distance from the convergence
reward. This is to make all graphs comparable
because in all cases the optimal average distance
from the convergence reward of the two agents
should be equal to 10 (make the optimal offer
or accept the optimal offer that the other agent
makes). The formulas for calculating the average
distance from the convergence reward are:
AD
1
=
?
n
r
j=1
|CR
1
?R
1j
|
n
r
(6)
AD
2
=
?
n
r
j=1
|CR
2
?R
2j
|
n
r
(7)
AD =
AD
1
+AD
2
2
(8)
whereCR
1
is the convergence reward for Agent 1,
R
1j
is the reward of Agent 1 for run j, CR
2
is the
convergence reward for Agent 2, and R
2j
is the
reward of Agent 2 for run j. Moreover, AD
1
is
the average distance from the convergence reward
for Agent 1, AD
2
is the average distance from the
convergence reward for Agent 2, and AD is the
average of AD
1
and AD
2
. All graphs of section 5
show AD values. Also, n
r
is the number of runs
(in our case always equal to 20). Thus in the case
of 4 fruits, we will have CR
1
=CR
2
=1200, and if
for all runs R
1j
=R
2j
=1190, then AD=10.
5.1 Variable Exploration Rate
In this section we report results with different ex-
ploration rates per training epoch (see section 4).
Q- PHC- PHC- PHC-
learning LF W WoLF
1 A & O 10.5 10 10 10
2 A & O 10.3 10.3 10 10
3 A & O 11.7 10 10 10
4 A & O 15 11.8 11.7 11.7
5 A & O 45.4 29.5 26.5 22.9
6 A & O 60.8 33.4 46.1 33.9
7 A & O 95 56 187.8 88.6
Table 4: Average distance from convergence re-
ward over 20 runs for 100,000 episodes per epoch
and for different numbers of fruits to be shared (A:
apples, O: oranges). The best possible value is 10.
Table 4 shows the average distance from the con-
vergence reward over 20 runs for 100,000 episodes
per epoch, for different numbers of fruits, and
for all four methods (Q-learning, PHC-LF, PHC-
W, and PHC-WoLF). It is clear that as the state
space becomes larger 100,000 training episodes
per epoch are not enough for convergence. Also,
for 1, 2, and 3 fruits all algorithms converge and
perform comparably. As the number of fruits in-
creases, Q-learning starts performing worse than
the multi-agent RL algorithms. For 7 fruits PHC-
W appears to perform worse than Q-learning but
this is because, as we can see in Figure 5, in this
case more than 400,000 episodes per epoch are re-
quired for convergence. Thus after only 100,000
episodes per epoch all policies still behave some-
what randomly.
Figures 2, 3, 4, and 5 show the average distance
from the convergence reward as a function of the
number of episodes per epoch during training, for
4, 5, 6, and 7 fruits respectively. For 4 fruits it
takes about 125,000 episodes per epoch and for 5
fruits it takes about 225,000 episodes per epoch for
the policies to converge. This number rises to ap-
proximately 350,000 for 6 fruits and becomes even
higher for 7 fruits. Q-learning consistently per-
forms worse than the rest of the algorithms. The
differences between PHC-LF, PHC-W, and PHC-
WoLF are insignificant, which is a bit surprising
given that Bowling and Veloso (2002) showed that
PHC-WoLF performed better than PHC in a series
of benchmark tasks. In Figures 2 and 3, PHC-LF
appears to be reaching convergence slightly faster
than PHC-W and PHC-WoLF but this is not statis-
tically significant.
506
Figure 2: 4 fruits and variable exploration rate:
Average distance from convergence reward during
testing (20 runs). The best possible value is 10.
Figure 3: 5 fruits and variable exploration rate:
Average distance from convergence reward during
testing (20 runs). The best possible value is 10.
5.2 Constant Exploration Rate
In this section we report results with a constant
exploration rate for all training epochs (see sec-
tion 4). Figures 6 and 7 show the average dis-
tance from the convergence reward as a function of
the number of episodes per epoch during training,
for 4 and 5 fruits respectively. Clearly having a
constant exploration rate in all epochs is problem-
atic. For 4 fruits, after 225,000 episodes per epoch
there is still no convergence. For comparison, with
a variable exploration rate it took about 125,000
episodes per epoch for the policies to converge.
Likewise for 5 fruits. After 400,000 episodes per
epoch there is still no convergence. For compari-
son, with a variable exploration rate it took about
225,000 episodes per epoch for convergence.
Figure 4: 6 fruits and variable exploration rate:
Average distance from convergence reward during
testing (20 runs). The best possible value is 10.
Figure 5: 7 fruits and variable exploration rate:
Average distance from convergence reward during
testing (20 runs). The best possible value is 10.
The above results show that, unlike single-agent
RL where having a constant exploration rate is
perfectly acceptable, here a constant exploration
rate does not work.
6 Conclusion and Future Work
We used single-agent RL and multi-agent RL for
learning dialogue policies in a resource allocation
negotiation scenario. Two agents interacted with
each other and both learned at the same time. The
advantage of this approach is that it does not re-
quire SUs to train against or corpora to learn from.
We compared a traditional single-agent RL al-
gorithm (Q-learning) against two multi-agent RL
algorithms (PHC and PHC-WoLF) varying the
scenario complexity (state space size), the number
507
Figure 6: 4 fruits and constant exploration rate:
Average distance from convergence reward during
testing (20 runs). The best possible value is 10.
Figure 7: 5 fruits and constant exploration rate:
Average distance from convergence reward during
testing (20 runs). The best possible value is 10.
of training episodes, and the learning and explo-
ration rates. Our results showed that Q-learning
is not suitable for concurrent learning given that
it is designed for learning against a stationary en-
vironment. Q-learning failed to converge in all
cases, except for very small state space sizes. On
the other hand, both PHC and PHC-WoLF always
converged (or in the case of 7 fruits they needed
more training episodes) and performed similarly.
We also showed that in concurrent learning very
high gradually decreasing exploration rates are re-
quired for convergence. We conclude that multi-
agent RL of dialogue policies is a promising alter-
native to using single-agent RL and SUs or learn-
ing directly from corpora.
The focus of this paper is on comparing single-
agent RL and multi-agent RL for concurrent learn-
ing, and studying the implications for convergence
and exploration/learning rates. Our next step is
testing with human users. We are particularly in-
terested in users whose behavior changes during
the interaction and continuous testing against ex-
pert repeat users, which has never been done be-
fore. Another interesting question is whether cor-
pora or SUs may still be required for designing
the state and action spaces and the reward func-
tions of the interlocutors, bootstrapping the poli-
cies, and ensuring that information about the be-
havior of human users is encoded in the resulting
learned policies. Ga?si?c et al (2013) showed that it
is possible to learn ?full? dialogue policies just via
interaction with human users (without any boot-
strapping using corpora or SUs). Similarly, con-
current learning could be used in an on-line fash-
ion via live interaction with human users. Or al-
ternatively concurrent learning could be used off-
line to bootstrap the policies and then these poli-
cies could be improved via live interaction with
human users (again using concurrent learning to
address possible changes in user behavior). These
are open research questions for future work.
Furthermore, we intend to apply multi-agent RL
to more complex negotiation domains, e.g., exper-
iment with more than two types of resources (not
just apples and oranges) and more types of actions
(not just offers and acceptances). We would also
like to compare policies learned with multi-agent
RL techniques with policies learned with SUs or
from corpora both in simulation and with human
users. Finally, we aim to experiment with differ-
ent feature-based representations of the state and
action spaces. Currently all possible deal combi-
nations are listed as possible actions and as ele-
ments of the state, which can quickly lead to very
large state and action spaces as the application be-
comes more complex (in our case as the number of
fruits increases). However, abstraction is not triv-
ial because the agents have no guarantee that the
value of a deal is a simple function of the value of
its parts, and values may differ for different agents.
Acknowledgments
Claire Nelson sadly died in May 2013. We con-
tinued and completed this work after her pass-
ing away. She is greatly missed. This work was
funded by the NSF grant #1117313.
508
References
Pieter Abbeel and Andrew Y. Ng. 2004. Apprentice-
ship learning via inverse reinforcement learning. In
Proc. of the International Conference on Machine
Learning, Bannf, Alberta, Canada.
Hua Ai and Diane Litman. 2008. Assessing dialog sys-
tem user simulation evaluation measures using hu-
man judges. In Proc. of the Annual Meeting of the
Association for Computational Linguistics, Colum-
bus, Ohio, USA.
Michael Bowling and Manuela Veloso. 2002. Multi-
agent learning using a variable learning rate. Artifi-
cial Intelligence, 136(2):215?250.
L. Busoniu, R. Babuska, and B. De Schutter. 2008.
A comprehensive survey of multiagent reinforce-
ment learning. IEEE Transactions on Systems, Man,
and Cybernetics, Part C: Applications and Reviews,
38(2):156?172.
Senthilkumar Chandramohan, Matthieu Geist, Fabrice
Lef`evre, and Olivier Pietquin. 2012. Co-adaptation
in spoken dialogue systems. In Proc. of the Interna-
tional Workshop on Spoken Dialogue Systems, Paris,
France.
Min Chi, Kurt VanLehn, Diane Litman, and Pamela
Jordan. 2011. Empirically evaluating the ap-
plication of reinforcement learning to the induc-
tion of effective and adaptive pedagogical strategies.
User Modeling and User-Adapted Interaction, 21(1-
2):137?180.
Caroline Claus and Craig Boutilier. 1998. The dynam-
ics of reinforcement learning in cooperative multia-
gent systems. In Proc. of the National Conference
on Artificial Intelligence.
Heriberto Cuay?ahuitl and Nina Dethlefs. 2012. Hier-
archical multiagent reinforcement learning for coor-
dinating verbal and nonverbal actions in robots. In
Proc. of the ECAI Workshop on Machine Learning
for Interactive Systems, Montpellier, France.
Lucie Daubigney, Matthieu Geist, Senthilkumar Chan-
dramohan, and Olivier Pietquin. 2012. A compre-
hensive reinforcement learning framework for dia-
logue management optimization. IEEE Journal of
Selected Topics in Signal Processing, 6(8):891?902.
Yaakov Engel, Shie Mannor, and Ron Meir. 2005. Re-
inforcement learning with Gaussian processes. In
Proc. of the International Conference on Machine
Learning, Bonn, Germany.
Michael S. English and Peter A. Heeman. 2005.
Learning mixed initiative dialogue strategies by us-
ing reinforcement learning on both conversants. In
Proc. of the Conference on Empirical Methods in
Natural Language Processing, Vancouver, Canada.
M. Ga?si?c, Filip Jur?c???cek, Blaise Thomson, Kai Yu, and
Steve Young. 2011. On-line policy optimisation
of spoken dialogue systems via live interaction with
human subjects. In Proc. of the IEEE Automatic
Speech Recognition and Understanding Workshop,
Big Island, Hawaii, USA.
Milica Ga?si?c, Matthew Henderson, Blaise Thomson,
Pirros Tsiakoulis, and Steve Young. 2012. Pol-
icy optimisation of POMDP-based dialogue systems
without state space compression. In Proc. of the
IEEE Workshop on Spoken Language Technology,
Miami, Florida, USA.
M. Ga?si?c, C. Breslin, M. Henderson, D. Kim,
M. Szummer, B. Thomson, P. Tsiakoulis, and
S. Young. 2013. On-line policy optimisation of
Bayesian spoken dialogue systems via human inter-
action. In Proc. of the International Conference on
Acoustics, Speech and Signal Processing, Vancou-
ver, Canada.
Kallirroi Georgila and David Traum. 2011. Reinforce-
ment learning of argumentation dialogue policies in
negotiation. In Proc. of Interspeech, Florence, Italy.
Kallirroi Georgila, James Henderson, and Oliver
Lemon. 2006. User simulation for spoken dialogue
systems: Learning and evaluation. In Proc. of Inter-
speech, Pittsburgh, Pennsylvania, USA.
Kallirroi Georgila, Maria K. Wolters, and Johanna D.
Moore. 2010. Learning dialogue strategies from
older and younger simulated users. In Proc. of
the Annual SIGdial Meeting on Discourse and Di-
alogue, Tokyo, Japan.
Kallirroi Georgila. 2013. Reinforcement learning of
two-issue negotiation dialogue policies. In Proc. of
the Annual SIGdial Meeting on Discourse and Dia-
logue, Metz, France.
Peter A. Heeman. 2009. Representing the reinforce-
ment learning state in a negotiation dialogue. In
Proc. of the IEEE Automatic Speech Recognition
and Understanding Workshop, Merano, Italy.
James Henderson, Oliver Lemon, and Kallirroi
Georgila. 2008. Hybrid reinforcement/supervised
learning of dialogue policies from fixed datasets.
Computational Linguistics, 34(4):487?511.
Junling Hu and Michael P. Wellman. 1998. Multia-
gent reinforcement learning: Theoretical framework
and an algorithm. In Proc. of the International Con-
ference on Machine Learning, Madison, Wisconsin,
USA.
Filip Jur?c???cek, Blaise Thomson, and Steve Young.
2012. Reinforcement learning for parameter esti-
mation in statistical spoken dialogue systems. Com-
puter Speech and Language, 26(3):168?192.
Michail G. Lagoudakis and Ronald Parr. 2003. Least-
squares policy iteration. Journal of Machine Learn-
ing Research, 4:1107?1149.
509
Oliver Lemon, Kallirroi Georgila, and James Hender-
son. 2006. Evaluating effectiveness and portabil-
ity of reinforcement learned dialogue strategies with
real users: The TALK TownInfo evaluation. In Proc.
of the IEEEWorkshop on Spoken Language Technol-
ogy, Palm Beach, Aruba.
Lihong Li, Jason D. Williams, and Suhrid Balakrish-
nan. 2009. Reinforcement learning for dialog man-
agement using least-squares policy iteration and fast
feature selection. In Proc. of Interspeech, Brighton,
United Kingdom.
Michael L. Littman. 1994. Markov games as a frame-
work for multi-agent reinforcement learning. In
Proc. of the International Conference on Machine
Learning, New Brunswick, New Jersey, USA.
Yi Ma. 2013. User goal change model for spoken dia-
log state tracking. In Proc. of the NAACL-HLT Stu-
dent Research Workshop, Atlanta, Georgia, USA.
Teruhisa Misu, Komei Sugiura, Kiyonori Ohtake,
Chiori Hori, Hideki Kashioka, Hisashi Kawai, and
Satoshi Nakamura. 2010. Modeling spoken deci-
sion making dialogue and optimization of its dia-
logue strategy. In Proc. of the Annual SIGdial Meet-
ing on Discourse and Dialogue, Tokyo, Japan.
Teruhisa Misu, Kallirroi Georgila, Anton Leuski, and
David Traum. 2012. Reinforcement learning of
question-answering dialogue policies for virtual mu-
seum guides. In Proc. of the Annual SIGdial Meet-
ing on Discourse and Dialogue, Seoul, South Korea.
Elnaz Nouri, Kallirroi Georgila, and David Traum.
2012. A cultural decision-making model for nego-
tiation based on inverse reinforcement learning. In
Proc. of the Cognitive Science Conference, Sapporo,
Japan.
P. Paruchuri, N. Chakraborty, R. Zivan, K. Sycara,
M. Dudik, and G. Gordon. 2009. POMDP based
negotiation modeling. In IJCAI Workshop on Mod-
eling Intercultural Collaboration and Negotiation,
Pasadena, California, USA.
Olivier Pietquin and Helen Hastie. 2013. A survey
on metrics for the evaluation of user simulations.
Knowledge Engineering Review, 28(1):59?73.
Carl Edward Rasmussen and Christopher K. I.
Williams. 2006. Gaussian Processes for Machine
Learning. MIT Press.
Verena Rieser, Simon Keizer, Xingkun Liu, and Oliver
Lemon. 2011. Adaptive information presentation
for spoken dialogue systems: Evaluation with hu-
man subjects. In Proc. of the European Workshop
on Natural Language Generation, Nancy, France.
Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and
Steve Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of dia-
logue management strategies. Knowledge Engineer-
ing Review, 21(2):97?126.
Satinder Singh, Diane Litman, Michael Kearns, and
Marilyn Walker. 2002. Optimizing dialogue man-
agement with reinforcement learning: Experiments
with the NJFun system. Journal of Artificial Intelli-
gence Research, 16:105?133.
Richard S. Sutton and Andrew G. Barto. 1998. Rein-
forcement Learning: An Introduction. MIT Press.
Joel R. Tetreault and Diane J. Litman. 2008. A rein-
forcement learning approach to evaluating state rep-
resentations in spoken dialogue systems. Speech
Communication, 50(8-9):683?696.
Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A POMDP framework for
spoken dialogue systems. Computer Speech and
Language, 24(4):562?588.
Jason D. Williams and Steve Young. 2007. Scal-
ing POMDPs for spoken dialog management. IEEE
Transactions on Audio, Speech, and Language Pro-
cessing, 15(7):2116?2129.
510
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 103?106,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Learning Dialogue Strategies from Older and Younger Simulated Users
Kallirroi Georgila
Institute for Creative Technologies
University of Southern California
Playa Vista, USA
kgeorgila@ict.usc.edu
Maria K. Wolters
School of Informatics
University of Edinburgh
Edinburgh, UK
maria.wolters@ed.ac.uk
Johanna D. Moore
School of Informatics
University of Edinburgh
Edinburgh, UK
J.Moore@ed.ac.uk
Abstract
Older adults are a challenging user group
because their behaviour can be highly vari-
able. To the best of our knowledge, this
is the first study where dialogue strategies
are learned and evaluated with both sim-
ulated younger users and simulated older
users. The simulated users were derived
from a corpus of interactions with a strict
system-initiative spoken dialogue system
(SDS). Learning from simulated younger
users leads to a policy which is close to
one of the dialogue strategies of the under-
lying SDS, while the simulated older users
allow us to learn more flexible dialogue
strategies that accommodate mixed initia-
tive. We conclude that simulated users are
a useful technique for modelling the be-
haviour of new user groups.
1 Introduction
State-of-the-art statistical approaches to dia-
logue management (Frampton and Lemon, 2006;
Williams and Young, 2007) rely on having ade-
quate training data. Dialogue strategies are typ-
ically inferred from data using Reinforcement
Learning (RL), which requires on the order of
thousands of dialogues to achieve good perfor-
mance. Therefore, it is no longer feasible to rely
on data collected with real users. Instead, training
data is generated through interactions of the sys-
tem with simulated users (SUs) (Georgila et al,
2006). In order to learn good policies, the be-
haviour of the SUs needs to cover the range of
variation seen in real users (Georgila et al, 2006;
Schatzmann et al, 2006). Furthermore, SUs are
critical for evaluating candidate dialogue policies.
To date, SUs have been used to learn dialogue
strategies for specific domains such as flight reser-
vation, restaurant recommendation, etc., and to
learn both how to collect information from the
user (Frampton and Lemon, 2006) as well as how
to present information to the user (Rieser and
Lemon, 2009; Janarthanam and Lemon, 2009).
In addition to covering different domains, SUs
should also be able to model relevant user at-
tributes (Schatzmann et al, 2006), such as coop-
erativeness vs. non-cooperativeness (Lo?pez-Co?zar
et al, 2006; Jung et al, 2009), or age (Georgila et
al., 2008). In this paper, we focus on user age.
As the proportion of older people in the popu-
lation increases, it becomes essential to make spo-
ken dialogue systems (SDS) easy to use for this
group of people. Only very few spoken dialogue
systems have been developed for older people (e.g.
Nursebot (Roy et al, 2000)), and we are aware of
no work on learning specific dialogue policies for
older people using SUs and RL.
Older people present special challenges for di-
alogue systems. While cognitive and perceptual
abilities generally decline with age, the spread of
ability in older people is far larger than in any
other segment of the population (Rabbitt and An-
derson, 2005). Older users may also use differ-
ent strategies for interacting with SDS. In our pre-
vious work on studying the interactions between
older and younger users and a simulated appoint-
ment scheduling SDS (Wolters et al, 2009b), we
found that some older users were very ?social?,
treating the system like a human, and failing to
adapt to the SDS?s system-initiative dialogue strat-
egy. A third of the older users, however, tended
to be more ?factual?, using short commands and
conforming to the system?s dialogue strategy. In
that, they were very similar to the younger users
(Wolters et al, 2009b).
In previous work (Georgila et al, 2008), we
successfully built SUs for both older and younger
103
adults from the corpus used by (Wolters et al,
2009b) and documented in (Georgila et al, 2010).
When we evaluated the SUs using metrics such as
precision and recall (Georgila et al, 2006; Schatz-
mann et al, 2006), we found that SUs trained on
older users? data can cover behaviour patterns typ-
ical of younger users, but not the opposite. The
behaviour of older people is too diverse to be cap-
tured by a SU trained on younger users? data. This
result agrees with the findings of (Wolters et al,
2009b; Georgila et al, 2010).
In this study, we take our work one step
further?we use the SUs developed in (Georgila
et al, 2008) to learn dialogue policies and evalu-
ate the resulting policies with data from both older
and younger users. Our work is important for two
reasons. First, to the best of our knowledge this
is the first time that people have used SUs and
RL to learn dialogue strategies for the increas-
ingly important population of older users. Sec-
ond, despite the fact that SUs are used for learn-
ing dialogue strategies it is not clear whether they
can learn policies that are appropriate for different
user populations. We show that SUs can be suc-
cessfully used to learn policies for older users that
are adapted to their specific patterns of behaviour,
even though these patterns are far more varied than
the behaviour patterns of younger users. This pro-
vides evidence for the validity of the user simula-
tion methodology for learning and evaluating dia-
logue strategies for different user populations.
The structure of the paper is as follows: In sec-
tion 2 we describe our data set, discuss the dif-
ferences between older and younger users as seen
in our corpus, and describe our user simulations.
In section 3, we present the results of our experi-
ments. Finally, in section 4 we present our conclu-
sions and propose future work.
2 The Corpus
In the original dialogue corpus, people were asked
to schedule health care appointments with 9 dif-
ferent simulated SDS in a Wizard-of-Oz setting.
The systems varied in the number of options pre-
sented at each stage of the dialogue (1, 2, 4),
and in the confirmation strategies used (explicit
confirmation, implicit confirmation, no confirma-
tion). System utterances were generated using
a simple template-based algorithm and synthe-
sised using a female Scottish English unit selec-
tion voice. The human Wizard took over the func-
tion of speech recognition (ASR), language under-
standing (NLU), and dialogue management com-
ponents. No ASR or NLU errors were simulated,
because having to deal with ASR and/or NLU er-
rors in addition to task completion would have in-
creased cognitive load (Wolters et al, 2009a).
The system (Wizard) followed a strict policy
which resulted in dialogues with a fixed schema:
First, users arranged to see a specific health care
professional, then they arranged a specific half-
day, and finally, a specific half-hour time slot on
that half-day was agreed. Users were not allowed
to skip any stage of the dialogue. This design en-
sured that all users were presented with the rele-
vant number of options and the relevant confirma-
tion strategy at least three times per dialogue. In a
final step, the Wizard confirmed the appointment.
The full corpus consists of 447 dialogues; 3 di-
alogues were not recorded. A total of 50 partici-
pants were recruited, of which 26 were older, aged
between 50 and 85 years, and 24 were younger,
aged between 18 and 30 years. The older users
contributed 232 dialogues, the younger ones 215.
Older and younger users were matched for level
of education and gender. All dialogues were tran-
scribed orthographically and annotated with dia-
logue acts and dialogue context information. Us-
ing a unique mapping, we associate each dialogue
act with a ?speech act, task? pair, where the speech
act is task independent and the task corresponds to
the slot in focus (health professional, half-day or
time slot). For example, ?confirm pos, hp? cor-
responds to positive explicit confirmation of the
health professional slot. For each dialogue, de-
tailed measures of dialogue quality were recorded:
objective task completion, perceived task comple-
tion, appointment recall, length (in turns), and ex-
tensive user satisfaction ratings. For a detailed dis-
cussion of the corpus, see (Georgila et al, 2010).
The choice of dialogue strategy did not affect
task completion and appointment recall, but had
significant effects on efficiency (Wolters et al,
2009a). Task completion and appointment recall
were the same for older and younger users, but
older users took more turns to complete the task
(Wolters et al, 2009a). Clear differences between
the two user groups emerge when we look at in-
teraction patterns in more detail (Wolters et al,
2009b; Georgila et al, 2010). Older people tend
to ?ground? information (using repetitions) and
take the initiative more than younger people. In
our corpus it was very common that the older per-
son would provide information about the half-day
and the time slot of the appointment before hav-
ing been asked by the system. However, due to the
104
Experiment 1 Experiment 2
slot filled +50 +50
appointment confirmed +200 +200
dialogue length -5 per turn -5 per turn
slot confirmed +100 not used
wrong order -500 not used
Table 1: Reward functions for the experiments.
strict policy of the Wizard, this information would
be ignored and the system would later ask for the
information that had already been provided.
In our SUs, each user utterance corresponds to a
user action described by a list of ?speech act, task?
pairs. There are 31 distinct system actions and 389
distinct actions for older users. Younger people
used a subset of 125 of the older users? actions.
Our SUs do not simulate ASR or NLU errors since
such errors were not simulated in the collection of
the corpus.
We built n-grams of system and user actions
with n varying from 2 to 5. Given a history of n-1
actions from system and user, the SU generates an
action based on a probability distribution learned
from the training data (Georgila et al, 2006). In
the present study, n was set to 3, which means that
each user action is predicted based on the previous
user action and the previous system action.
3 Learning Dialogue Strategies
We performed two experiments. In Experiment 1,
our goal was to learn the policy of the Wizard, i.e.
the strict system-initiative policy of requesting and
confirming information for each slot before mov-
ing to the next slot, in the following order: health
professional, half-day, time slot. In Experiment
2, our goal was to learn a more flexible policy that
could accommodate some degree of user initiative.
The reward functions for both experiments are
specified in Table 1; they are similar to the reward
functions used in the literature, e.g. (Frampton and
Lemon, 2006). Slots that have been filled success-
fully and confirmed appointments are rewarded,
while long dialogues are penalised. For Experi-
ment 1, policies were rewarded that filled slots in
the correct order and that confirmed each slot af-
ter it had been filled. A large penalty was imposed
when the policy deviated from the strict slot order
(health professional, half-day, time slot). For Ex-
periment 2, these constraints were removed. Slots
could be filled in any order. Confirmations were
not required because there was no speech act in
the corpus for confirming more than one slot at a
time.
In both experiments we used the SARSA-? al-
gorithm (Sutton and Barto, 1998) for RL. 30,000
iterations were used for learning the final pol-
icy for each condition. For each experiment,
we learned two policies, Policy-Old, which was
based on simulated older users, and Policy-Young,
which was based on simulated younger users.
The resulting policies were then tested on simu-
lated older users (Test-Old) and simulated younger
users (Test-Young). To have comparable results
between Experiment 1 and Experiment 2, dur-
ing testing we score our policies using the reward
function of Experiment 2. The best possible score
is 190, i.e. the user fills all the slots in one turn
and then confirms the appointment. (Note that +50
points are given when a slot is only filled, not con-
firmed too.) For each test condition, we gener-
ated 10,000 simulated dialogues. Overall scores
for each combination of policy and SU were es-
tablished using 5-fold cross-validation.
Our results are summarised in Figure 1. While
average rewards were not affected by policy
type (ANOVA, F (1, 68)=1, p=0.3) or training
data set (F (1, 185)=3, p=0.09), we found a very
strong interaction between policy type and data
set (F (1, 3098)=51, p=0.000). Learning with
simulated younger users yields better strict poli-
cies than learning with older users (Tukey?s Hon-
est Significant Difference Test, ?=20, 95% CI
= [11, 30], p=0.000), while learning with simu-
lated older users yields better flexible policies than
learning with younger users (?=15, 95% CI =
[6, 24], p=0.001). This is what we would expect
from our corpus analysis, since the interaction be-
haviour of older users is far more variable than that
of younger users (Wolters et al, 2009b; Georgila
et al, 2010).
The strict policy that was learned from sim-
ulated younger users was as follows, with only
slight variations: first request the type of health
professional, then implicitly confirm the health
professional and request the half-day slot, then im-
plicitly confirm the half-day slot and request the
time slot, and then confirm the appointment. The
strict policy learned from simulated older users
was similar, but less successful, because most
older users do not readily conform to the fixed
structure.
The flexible policy learned from simulated older
users takes into account initiative from the user
and does not always confirm. The score for the
flexible policy learned from simulated younger
users was relatively low, even though the resulting
105
Score
140150
160170
180190
Test?Old Test?Young
Policy?OldReward?Flex
Test?Old Test?Young
Policy?YoungReward?Flex
Policy?OldReward?Strict
140150
160170
180190
Policy?YoungReward?Strict
Figure 1: Mean scores for each combination of
reward function, training set, and test set (5-fold
cross-validation).
policy was very similar to the strict policy learned
from younger users (i.e. a sequence of informa-
tion requests and implicit confirmations), and even
though the behaviour of younger users is far more
predictable than the behaviour of older users. It
appears that the explicit penalty for violating the
order of slots is crucial for fully exploiting the pat-
terns in younger users? behaviour.
4 Conclusions
We have shown that SUs can be used to learn ap-
propriate policies for older adults, even though
their interaction behaviour is more complex and
diverse than that of younger adults. Crucially, sim-
ulated older users allowed us to learn a more flex-
ible version of the strict system-initiative dialogue
strategies that were used for creating the original
corpus of interactions. These results are consis-
tent with previous analyses of the original corpus
(Wolters et al, 2009b; Georgila et al, 2010) and
support the validity of the user simulation method-
ology for learning and evaluating dialogue strate-
gies.
In our future work, we will experiment with
more complex SUs, e.g. linear feature combina-
tion models (Georgila et al, 2006), and see if they
can be used to learn similar policies. We also plan
to study the effect of training and testing with dif-
ferent user simulation techniques, such as n-grams
versus linear feature combination models.
Acknowledgements
This research was partially supported by the MATCH project
(SHEFC-HR04016, http://www.match-project.
org.uk). Georgila is supported by the U.S. Army Research,
Development, and Engineering Command (RDECOM). The
content does not necessarily reflect the position or the policy
of the U.S. Government, and no official endorsement should
be inferred.
References
M. Frampton and O. Lemon. 2006. Learning more effective
dialogue strategies using limited dialogue move features.
In Proc. ACL.
K. Georgila, J. Henderson, and O. Lemon. 2006. User simu-
lation for spoken dialogue systems: Learning and evalua-
tion. In Proc. Interspeech.
K. Georgila, M. Wolters, and J. Moore. 2008. Simulating the
behaviour of older versus younger users. In Proc. ACL.
K. Georgila, Maria Wolters, J.D. Moore, and R.H. Logie.
2010. The MATCH corpus: A corpus of older and
younger users? interactions with spoken dialogue systems.
Language Resources and Evaluation, 44(3):221?261.
S. Janarthanam and O. Lemon. 2009. A two-tier user simula-
tion model for reinforcement learning of adaptive referring
expression generation policies. In Proc. SIGdial.
S. Jung, C. Lee, K. Kim, and G.G. Lee. 2009. Hybrid ap-
proach to user intention modeling for dialog simulation.
In Proc. ACL.
R. Lo?pez-Co?zar, Z. Callejas, and M. McTear. 2006. Testing
the performance of spoken dialogue systems by means of
an artificially simulated user. Artificial Intelligence Re-
view, 26(4):291?323.
P. Rabbitt and M.M. Anderson. 2005. The lacunae of
loss? Aging and the differentiation of human abilities.
In F.I. Craik and E. Bialystok, editors, Lifespan Cogni-
tion: Mechanisms of Change, chapter 23. Oxford Univer-
sity Press, New York, NY.
V. Rieser and O. Lemon. 2009. Natural language gener-
ation as planning under uncertainty for spoken dialogue
systems. In Proc. EACL.
N. Roy, J. Pineau, and S. Thrun. 2000. Spoken dialog man-
agement for robots. In Proc. ACL.
J. Schatzmann, K. Weilhammer, M. Stuttle, and S. Young.
2006. A survey of statistical user simulation tech-
niques for reinforcement-learning of dialogue manage-
ment strategies. Knowlege Engineering Review, 21(2):97?
126.
R.S. Sutton and A.G. Barto. 1998. Reinforcement Learning:
An Introduction. MIT Press.
J. Williams and S. Young. 2007. Partially observable Markov
decision processes for spoken dialog systems. Computer
Speech and Language, 21(2):393?422.
M. Wolters, K. Georgila, J.D. Moore, R.H. Logie, S.E.
MacPherson, and M. Watson. 2009a. Reducing work-
ing memory load in spoken dialogue systems. Interacting
with Computers, 21(4):276?287.
M. Wolters, K. Georgila, J.D. Moore, and S.E. MacPherson.
2009b. Being old doesn?t mean acting old: How older
users interact with spoken dialog systems. ACM Trans.
Accessible Computing, 2(1).
106
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 237?240,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Cross-Domain Speech Disfluency Detection
Kallirroi Georgila, Ning Wang, Jonathan Gratch
Institute for Creative Technologies, University of Southern California
12015 Waterfront Drive, Playa Vista, CA 90094, USA
{kgeorgila,nwang,gratch}@ict.usc.edu
Abstract
We build a model for speech disfluency
detection based on conditional random
fields (CRFs) using the Switchboard cor-
pus. This model is then applied to a
new domain without any adaptation. We
show that a technique for detecting speech
disfluencies based on Integer Linear Pro-
gramming (ILP) (Georgila, 2009) signifi-
cantly outperforms CRFs. In particular, in
terms of F-score and NIST Error Rate the
absolute improvement of ILP over CRFs
exceeds 20% and 25% respectively. We
conclude that ILP is an approach with
great potential for speech disfluency detec-
tion when there is a lack or shortage of in-
domain data for training.
1 Introduction
Speech disfluencies (also known as speech re-
pairs) occur frequently in spontaneous speech and
can pose difficulties to natural language process-
ing (NLP) since most NLP tools (e.g. parsers and
part-of-speech taggers) are traditionally trained on
written language. However, speech disfluencies
are not noise. They are an integral part of how
humans speak, may provide valuable information
about the speaker?s cognitive state, and can be crit-
ical for successful turn-taking (Shriberg, 2005).
Speech disfluencies have been the subject of much
research in the field of spoken language process-
ing, e.g. (Ginzburg et al, 2007).
Speech disfluencies can be divided into three
intervals, the reparandum, the editing term, and
the correction (Heeman and Allen, 1999; Liu et
al., 2006). In the example below, ?it left? is the
reparandum (the part that will be repaired), ?I
mean? is the editing term, and ?it came? is the cor-
rection:
(it left) * (I mean) it came
The asterisk marks the interruption point at
which the speaker halts the original utterance in
order to start the repair. The editing term is op-
tional and consists of one or more filled pauses
(e.g. uh, um) or discourse markers (e.g. you know,
well). Our goal here is to automatically detect rep-
etitions (the speaker repeats some part of the ut-
terance), revisions (the speaker modifies the orig-
inal utterance), or restarts (the speaker abandons
an utterance and starts over). We also deal with
complex disfluencies, i.e. a series of disfluencies
in succession (?it it was it is sounds great?).
In previous work many different approaches to
detecting speech disfluencies have been proposed.
Different types of features have been used, e.g.
lexical features only, acoustic and prosodic fea-
tures only, or a combination of both (Liu et al,
2006). Furthermore, a number of studies have
been conducted on human transcriptions while
other efforts have focused on detecting disfluen-
cies from the speech recognition output.
In our previous work (Georgila, 2009), we pro-
posed a novel two-stage technique for speech dis-
fluency detection based on Integer Linear Pro-
gramming (ILP). ILP has been applied success-
fully to several NLP problems, e.g. (Clarke
and Lapata, 2008). In the first stage of our
method, we trained state-of-the-art classifiers for
speech disfluency detection, in particular, Hidden-
Event Language Models (HELMs) (Stolcke and
Shriberg, 1996), Maximum Entropy (ME) mod-
els (Ratnaparkhi, 1998), and Conditional Random
Fields (CRFs) (Lafferty et al, 2001). Then in
the second stage and during testing, each classifier
proposed possible labels which were then assessed
in the presence of local and global constraints us-
ing ILP. These constraints are hand-crafted and en-
code common disfluency patterns. ILP makes the
237
final decision taking into account both the output
of the classifier and the constraints. Our approach
is similar to the work of (Germesin et al, 2008) in
the sense that they also combine machine learning
with hand-crafted rules. However, we use differ-
ent machine learning techniques and ILP.
When we evaluated this approach on the
Switchboard corpus (available from LDC and
manually annotated with disfluencies) using lex-
ical features, we found that ILP significantly im-
proves the performance of HELMs and ME mod-
els with negligible cost in processing time. How-
ever, the improvement of ILP over CRFs was only
marginal. These results were achieved when each
classifier was trained on approx. 35,000 occur-
rences of disfluencies. Then we experimented
with varying training set sizes in Switchboard. As
soon as we started reducing the amount of data for
training the classifiers, the improvement of ILP
over CRFs rose and became very significant, ap-
prox. 4% absolute reduction of error rate with 25%
of the training set (approx. 9,000 occurrences of
disfluencies) (Georgila, 2009). This result showed
that ILP is particularly helpful when there is no
much training data available.
However, Switchboard is a unique corpus be-
cause the amount of disfluencies that it contains
is very large. Thus even 25% of our training set
contains more disfluencies than a typical corpus
of human-human or human-machine interactions.
In this paper, we investigate what happens when
we move to a new domain when there is no in-
domain data annotated with disfluencies to be used
for training. This is usually the case when we start
developing a dialogue system in a new domain,
when the system has not been fully implemented
yet, and thus no data from users interacting with
the system has been collected. Since the improve-
ment of ILP over HELMs and ME models was
very large even when the models were both trained
and tested on Switchboard (approx. 15% and 20%
absolute reduction of error rate when 100% and
25% of the training set was used for training the
classifiers respectively (Georgila, 2009)), in this
paper we focus only on comparing CRFs versus
CRFs+ILP. Our goal is to evaluate if and how
much ILP improves CRFs in the case that no train-
ing data is available at all.
The structure of the paper is as follows: In sec-
tion 2 we describe our data sets. In section 3 we
concisely describe our approach. Then in section 4
we present our experiments. Finally in section 5
we present our conclusion.
2 Data Sets
To train our classifiers we use Switchboard (avail-
able from LDC), which is manually annotated
with disfluencies, and is traditionally used for
speech disfluency experiments. We transformed
the Switchboard annotations into the following
format:
it BE was IE a IP it was good
BE (beginning of edit) is the point where the
reparandum starts and IP is the interruption point
(the point before the repair starts). In the above
example the beginning of the reparandum is the
first occurrence of ?it?, the interruption point ap-
pears after ?a?, and every word between BE and
IP is tagged as IE (inside edit). Sometimes BE
and IP occur at the same point, e.g. ?it BE-IP it
was?. In (Georgila, 2009) we divided Switchboard
into training, development, and test sets. Here we
use the same training and development sets as in
(Georgila, 2009) containing 34,387 occurrences of
BE labels and 39,031 occurrences of IP labels, and
3,146 occurrences of BE labels and 3,499 occur-
rences of IP labels, respectively.
We test our approach on a smaller corpus col-
lected in the framework of the Rapport project
(Gratch et al, 2007). The goal of the Rap-
port project is to study how rapport is achieved
in human-human and human-machine interaction.
By rapport we mean the harmony, fluidity, syn-
chrony and flow that someone feels when they are
engaged in a good conversation.
The Rapport agent is a virtual human designed
to elicit rapport from human participants within
the confines of a dyadic narrative task (Gratch et
al., 2007). In this setting, a speaker narrates some
previously observed series of events, i.e. the events
in a sexual harassment awareness and prevention
video, and the events in a video of the Tweety
cartoon. The central challenge for the Rapport
agent is to provide the non-verbal listening feed-
back associated with rapportful interaction (e.g.
head nods, postural mirroring, gaze shifts, etc.).
Our ultimate goal is to investigate possible cor-
relations between disfluencies and these types of
feedback.
We manually annotated 70 sessions of the Rap-
port corpus with disfluencies using the labels de-
scribed above (BE, IP, IE and BE-IP). In each ses-
sion the speaker narrates the events of one video.
These annotated sessions served as our reference
data set (gold-standard), which contained 738 and
865 occurrences of BE and IP labels respectively.
238
3 Methodology
In the first stage we train our classifier. Any clas-
sifier can be used as long as it provides more than
one possible answer (i.e. tag) for each word in the
utterance. Valid tags are BE, BE-IP, IP, IE or O.
The O tag indicates that the word is outside the
disfluent part of the utterance. ILP will be applied
to the output of the classifier during testing.
Let N be the number of words of each utter-
ance and i the location of the word in the utterance
(i=1,...,N ). Also, let CBE(i) be a binary variable
(1 or 0) for the BE tag. Its value will be determined
by ILP. If it is 1 then the word will be tagged as
BE. In the same way, we use CBE?IP (i), CIP (i),
CIE(i), CO(i) for tags BE-IP, IP, IE and O re-
spectively. Let PBE(i) be the probability given by
the classifier that the word is tagged as BE. In the
same way, let PBE?IP (i), PIP (i), PIE(i), PO(i)
be the probabilities for tags BE-IP, IP, IE and O
respectively. Given the above definitions, the ILP
problem formulation can be as follows:
max[
?N
i=1[PBE(i)CBE(i) + PBE?IP (i)CBE?IP (i)
+PIP (i)CIP (i) + PIE(i)CIE(i) + PO(i)CO(i)]]
(1)
subject to constraints, e.g.:
CBE(i) + CBE?IP (i) + CIP (i) + CIE(i)
+CO(i) = 1 ?i ? (1, ..., N) (2)
Equation 1 is the linear objective function that
we want to maximize, i.e. the overall probability
of the utterance. Equation 2 says that each word
can have one tag only. In the same way, we can
define constraints on which labels are allowed at
the start and end of an utterance. There are also
some constraints that define the transitions that are
allowed between tags. For example, IP cannot
follow an O directly, which means that we can-
not start a disfluency with an IP. There has to be
a BE after O and before IP. Details are given in
(Georgila, 2009).
We also formulate some additional rules that
encode common disfluency patterns. The idea
here is to generalize from these patterns. Be-
low is an example of a long-context rule. If we
have the sequence of words ?she was trying to
well um she was talking to a coworker?, we ex-
pect this to be tagged as ?she BE was IE try-
ing IE to IP well O um O she O was O talk-
ing O to O a O coworker O?, if we do not take
into account the context in which this pattern oc-
curs. Basically the pattern here is that two se-
quences of four words separated by a discourse
marker (?well?) and a filled pause (?um?) differ
only in their third word. That is, ?trying? and
?talking? are different words but have the same
part-of-speech tag (gerund). We incorporate this
rule into our ILP problem formulation as follows:
Let (w1,...,wN ) be a sequence of N words where
both w3 and wN?3 are verbs (gerund), the word
sequence w1,w2,w4 is the same as the sequence
wN?5,wN?4,wN?2, and all the words in between
(w5,...,wN?6) are filled pauses or discourse mark-
ers. Then the probabilities given by the classi-
fier are modified as follows: PBE(1)=PBE(1)+b1,
PIE(2)=PIE(2)+b2, PIE(3)=PIE(3)+b3, and
PIP (4)=PIP (4)+b4, where b1, b2, b3 and b4 are
empirically set boosting paremeters with values
between 0.5 and 1 computed using our Switch-
board development set. We use more complex
rules to cover cases such as ?she makes he doesn?t
make?, and boost the probabilities that this is
tagged as ?she BE makes IP he O doesn?t O make
O?.
In total we apply 17 rules and each rule can have
up to 5 more specific sub-rules. The largest con-
text that we take into account is 10 words, not in-
cluding filled pauses and discourse markers.
4 Experiments
For building the CRF model we use the CRF++
toolkit (available from sourceforge). We
used only lexical features, i.e. words and part-of-
speech (POS) tags. Switchboard includes POS
information but to annotate the Rapport corpus
with POS labels we used the Stanford POS tag-
ger (Toutanova and Manning, 2000). We ex-
perimented with different sets of features and
we achieved the best results with the follow-
ing setup (i is the location of the word or POS
in the sentence): Our word features are ?wi?,
?wi+1?, ?wi?1, wi?, ?wi, wi+1?, ?wi?2, wi?1, wi?,
?wi, wi+1, wi+2?. Our POS features have the
same structure as the word features. For ILP we
use the lp solve software also available from
sourceforge. We train on Switchboard and
test on the Rapport corpus.
For evaluating the performance of our models
we use standard metrics proposed in the litera-
ture, i.e. Precision, Recall, F-score, and NIST Er-
ror Rate. We report results for BE and IP. F-score
is the harmonic mean of Precision and Recall (we
equally weight Precision and Recall). Precision is
the ratio of the correctly identified tags X to all the
tags X detected by the model (where X is BE or
IP). Recall is the ratio of the correctly identified
tags X to all the tags X that appear in the reference
239
BE
Prec Rec F-score Error
CRF 74.52 36.45 48.95 73.44
CRF+ILP 77.44 64.63 70.46 47.56
IP
Prec Rec F-score Error
CRF 86.36 41.73 56.27 64.62
CRF+ILP 88.75 72.95 80.08 35.61
Table 1: Comparative results between our models.
utterance. The NIST Error Rate is the sum of in-
sertions, deletions and substitutions divided by the
total number of reference tags (Liu et al, 2006).
Table 1 presents comparative results between
our models. As we can see, now the improve-
ment of ILP over CRFs is not marginal as in
Switchboard. In fact, in terms of F-score and
NIST Error Rate the absolute improvement of ILP
over CRFs exceeds 20% and 25% respectively.
The results are statistically significant (p<10?8,
Wilcoxon signed-rank test). The main gain of ILP
comes from the large improvement in Recall. This
result shows that using ILP has great potential for
speech disfluency detection when there is a lack of
in-domain data for training, and when we use lex-
ical features and human transcriptions. Further-
more, the cost of applying ILP is negligible since
the process is fast and applied during testing.
Note that the improvement of ILP over CRFs is
significant even though the two corpora, Switch-
board and Rapport, differ in genre (conversation
versus narrative).
The reason for the large improvement of ILP
over CRFs is the fact that as explained above
ILP takes into account common disfluency pat-
terns and generalizes from them. CRFs can po-
tentially learn similar patterns from the data but
do not generalize that well. For example, if the
CRF model learns that ?she she? is a repetition it
will not necessarily infer that any sequence of the
same two words is a repetition (e.g. ?and and?).
Of course here, since we deal with human tran-
scriptions we do not worry about speech recogni-
tion errors. Preliminary results with speech recog-
nition output showed that ILP retains its advan-
tages but more modestly. In this case, when decid-
ing which boosting rules to apply, it makes sense
to consider speech recognition confidence scores
per word. For example, a possible repetition ?to
to? could be the result of a misrecognition of ?to
do?. But these types of problems also affect plain
CRFs, so in the end ILP is expected to continue
outperforming CRFs. This is one of the issues for
future work together with using prosodic features.
5 Conclusion
We built a model for speech disfluency detec-
tion based on CRFs using the Switchboard cor-
pus. This model was then applied to a new do-
main without any adaptation. We showed that a
technique for detecting speech disfluencies based
on ILP significantly outperforms CRFs. In partic-
ular, in terms of F-score and NIST Error Rate the
absolute improvement of ILP over CRFs exceeds
20% and 25% respectively. We conclude that ILP
is an approach with great potential for speech dis-
fluency detection when there is a lack or shortage
of in-domain data for training.
Acknowledgments
This work was sponsored by the U.S. Army Research, Devel-
opment, and Engineering Command (RDECOM). The con-
tent does not necessarily reflect the position or the policy of
the Government, and no official endorsement should be in-
ferred.
References
J. Clarke and M. Lapata. 2008. Global inference for sentence
compression: An integer linear programming approach.
Journal of Artificial Intelligence Research, 31:399?429.
K. Georgila. 2009. Using integer linear programming for
detecting speech disfluencies. In Proc. of NAACL.
S. Germesin, T. Becker, and P. Poller. 2008. Hybrid multi-
step disfluency detection. In Proc. of MLMI.
J. Ginzburg, R. Ferna?ndez, and D. Schlangen. 2007. Unify-
ing self- and other-repair. In Proc. of DECALOG.
J. Gratch, N. Wang, J. Gerten, E. Fast, and R. Duffy. 2007.
Creating rapport with virtual agents. In Proc. of Interna-
tional Conference on Intelligent Virtual Agents (IVA).
P. Heeman and J. Allen. 1999. Speech repairs, intonational
phrases and discourse markers: Modeling speakers? ut-
terances in spoken dialogue. Computational Linguistics,
25:527?571.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and
labeling sequence data. In Proc. of ICML.
Y. Liu, E. Shriberg, A. Stolcke, D. Hillard, M. Ostendorf,
and M. Harper. 2006. Enriching speech recognition with
automatic detection of sentence boundaries and disfluen-
cies. IEEE Trans. Audio, Speech and Language Process-
ing, 14(5):1526?1540.
A. Ratnaparkhi. 1998. Maximum Entropy Models for natural
language ambiguity resolution. Ph.D. thesis, University of
Pennsylvania.
E. Shriberg. 2005. Spontaneous speech: How people really
talk, and why engineers should care. In Proc. of Inter-
speech.
A. Stolcke and E. Shriberg. 1996. Statistical language mod-
eling for speech disfluencies. In Proc. of ICASSP.
K. Toutanova and C.D. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-of-
speech tagger. In Proc. of EMNLP/VLC.
240
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 272?278,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
An Annotation Scheme for Cross-Cultural Argumentation
and Persuasion Dialogues
Kallirroi Georgila?, Ron Artstein?, Angela Nazarian?
Michael Rushforth??, David Traum?, Katia Sycara?
?Institute for Creative Technologies, University of Southern California
?Robotics Institute, Carnegie Mellon University
kgeorgila@ict.usc.edu
Abstract
We present a novel annotation scheme for
cross-cultural argumentation and persuasion
dialogues. This scheme is an adaptation of
existing coding schemes on negotiation, fol-
lowing a review of literature on cross-cultural
differences in negotiation styles. The scheme
has been refined through application to cod-
ing both two-party and multi-party negotia-
tion dialogues in three different domains, and
is general enough to be applicable to differ-
ent domains with few if any extensions. Di-
alogues annotated with the scheme have been
used to successfully learn culture-specific di-
alogue policies for argumentation and persua-
sion.
1 Introduction
In both cooperative and non-cooperative negotiation
the nature of the arguments used can be crucial for
the outcome of the negotiation. Argumentation and
persuasion are basic elements of negotiation. More-
over, different cultures favor different types of argu-
ments (Koch, 1983; Han and Shavitt, 1994; Zaharna,
1995; Brett and Gelfand, 2006). For example, it is
claimed that Western individualistic cultures favor
arguments based on logic over arguments that appeal
to emotions. On the other hand, people from East-
ern collectivistic cultures are more likely to use ar-
guments in which the beneficiary is not themselves.
Furthermore, Arab cultures tend to favor more indi-
rect ways of argumentation and expression (Koch,
1983; Zaharna, 1995).
?Now at the University of Texas at San Antonio.
In order to analyze negotiation in detail, including
aspects such as persuasion, negotiation, and cross-
cultural differences, we have developed a novel
annotation scheme. General purpose annotation
schemes such as DAMSL (Core and Allen, 1997)
and DIT++ (Bunt, 2006) represent moves in the dia-
logue but do not capture enough details of the inter-
action to distinguish between different styles of per-
suasion and argumentation, especially cross-cultural
differences.
Our goal for developing this coding scheme is
two-fold. First, we aim to fill the gap in the litera-
ture of cross-cultural argumentation and persuasion.
To our knowledge this is the first annotation scheme
designed specifically for coding cross-cultural argu-
mentation and persuasion strategies. Previous work
on cross-cultural negotiation, e.g. Brett and Gelfand
(2006), has not focused on argumentation or per-
suasion in particular. Also, previous work on argu-
mentation, e.g. Prakken (2008), has not attempted to
capture cross-cultural differences in argumentation
and persuasion strategies. Second, we use this cod-
ing scheme to annotate negotiation dialogues to au-
tomatically learn argumentation and persuasion di-
alogue policies for different cultures (Georgila and
Traum, 2011).
2 Related Work
2.1 Non-Culture Related Argumentation and
Persuasion
The topic of negotiation has widely been studied
across various fields including social and behavioral
science (Kern et al, 2005), and computer science
(Sidner, 1994; Rose? and Torrey, 2004). Our spe-
cific focus is on the role of argumentation and per-
272
suasion. Sycara (1990) studied the role of argumen-
tation in negotiation with regard to the role of ar-
guments in changing the decision process of the in-
terlocutor. Most attempts have focused on study-
ing the structure of argumentation and persuasion,
often using formal logic (Cohen, 1987; Prakken,
2008). Dung (1995) showed that argumentation can
be viewed as a special form of logic programming
with negation as failure. An argumentation scheme
is defined as a structure or template for forming an
argument. Schemes are necessary for identifying
arguments, finding missing premises, analyzing ar-
guments, and evaluating arguments (Pollock, 1995;
Katzav and Reed, 2004; Walton et al, 2008).
Recently, there has been some work on using ma-
chine learning techniques for automatically inter-
preting (George et al, 2007) and generating argu-
ments (Zukerman, 2001). Note also the work of Pi-
wek (2008) who performed a study on how argu-
ments can be presented as fictive dialogues. Finally,
there are a few persuasive dialogue systems, e.g.
Daphne (Grasso et al, 2000) and BIAS (Bayesian In-
teractive Argumentation System) (Zukerman, 2001).
2.2 Cross-Cultural Argumentation and
Persuasion
There is a vast amount of research on cultural ef-
fects on negotiation. Brett and Gelfand (2006) iden-
tify three aspects in cross-cultural negotiation: indi-
vidualism vs. collectivism, egalitarianism vs. hierar-
chy, and low context vs. high context communica-
tion. Typically Western individuals are individualis-
tic, egalitarian, and use low context communication
while Eastern individuals are collectivistic, hierar-
chical, and use high context communication.1
Although there has been a considerable amount of
work on building agents that can negotiate (Traum
et al, 2003; Rose? and Torrey, 2004), little has been
done towards building agents that can take into ac-
count culture aspects of negotiation (Cassell, 2009;
Paruchuri et al, 2009; Traum, 2009).
Our literature review on cross-cultural argumen-
tation and persuasion showed that there are com-
paratively few papers related to cross-cultural argu-
mentation and persuasion in dialogue. Most work
on cross-cultural studies is based on survey experi-
1In high-context cultures the listener must understand the
contextual cues in order to grasp the full meaning of the mes-
sage. In low-context cultures communication tends to be spe-
cific, explicit, and analytical.
ments rather than dialogue analysis. Below we sum-
marize the works that we were influenced by the
most.
Peng and Nisbett (1999) studied the way Chinese
vs. European-American people reason about con-
tradiction. By contradiction, here, we mean op-
posing pieces of information. Chinese individuals
adopt a dialectical or compromise approach by re-
taining basic elements of the opposing perspectives.
European-American people select one of the per-
spectives as correct and dismiss the opposing ones.
Koch (1983) linguistically analyzed several per-
suasive texts in contemporary Arabic in which there
was both repetition of form and repetition of con-
tent. She found that Arabs use repetition as a means
for persuasion. This strategy is called ?presentation
as proof? or ?argumentation by presentation?. Thus
in Arabic argumentation it is the presentation of an
idea that is persuasive, not the logical structure of
proof which Westerners see behind the words. Za-
harna (1995) examined how the Arab and American
cultures have two distinct perspectives for viewing
the role of language, for structuring persuasive mes-
sages, and for communicating effectively with their
audiences. For Arabs emphasis is on form over func-
tion, affect over accuracy, and image over meaning,
which is in line with the work of Koch (1983).
Finally, Cialdini?s work (1998) identified six prin-
ciples of persuasion: reciprocation (tendency to re-
turn favors), scarcity (associated with high value),
authority (tendency to follow authority figures), so-
cial proof (one is looking to the behavior of other in-
dividuals to determine her own actions), liking (one
tends to do things for people that she likes), and
commitment and consistency (one has difficulty to
reverse her commitments).
3 Our Annotation Scheme
We have developed a novel scheme for coding cross-
cultural argumentation and persuasion strategies.
This scheme is based on the literature review pre-
sented in section 2.2, as well as our own analysis of
three very different kinds of negotiation (section 4).
To develop this annotation scheme, we started by
adapting existing coding schemes on negotiation de-
veloped by Pruitt and Lewis (1975), Carnevale et al
(1981), and Sidner (1994). We were also influenced
by the work of Prakken on argumentation and di-
alogue (2008), and the work of Cialdini (1998) on
persuasion (see section 2.2). Our annotation scheme
273
was further refined by iteratively applying it to three
different negotiation domains.
In our coding scheme, we use three dimensions
for annotating an utterance: speech act, topic, and
response or reference to a previous utterance. We
have divided our codes for speech acts in categories.
Below we can see each category and the codes that
are included in it with explanatory examples, mostly
drawn from the florist-grocer dialogues described in
section 4.1.
3.1 Topic Tracking
start topic Let?s talk about the design.
end topic We are done with the design.
redirect topic We need to get back to the task.
3.2 Information Exchange
This category includes providing and requesting in-
formation, broken down into three kinds of informa-
tion that are about the negotiation (priority, value,
preference) as well as a fourth category (fact) which
can be further subdivided, depending on the issue
being negotiated (e.g. for the toy domain in sec-
tion 4.3, there are specializations for origin, func-
tion, and utility of the toy).
request info.priority Which issue is the most impor-
tant to you?
request info.value How much money will I get if I
give you this?
request info.preference What do you think about
the blue color?
request info.fact What will happen to the flowers if
the temperature gets higher?
provide info.priority I care most about tempera-
ture.
provide info.value You get $50 more if you agree to
lower the temperature by one degree.
provide info.preference I like design A.
provide info.fact (just a simple fact, neither prefer-
ence nor priority nor value) So one of them will
be yours and one mine.
3.3 Information Comparison
note similarities We both need the temperature to
be relatively low.
note differences It seems that you want design A
and I prefer design C.
project othersposition So you want an equal distri-
bution of rent.
3.4 Clarifications/Confirmations
request clarification I am not getting any more
money with more customers coming in?
provide clarification Not necessarily.
request confirmation Did you say 68 degrees?
self clarification (when the speaker tries to expand
on her ideas) Because when I thought temper-
ature, I was thinking temperature for the prod-
ucts, not temperature for the atmosphere.
3.5 Offer
We use the following format for an offer:
offer.?type?.?beneficiary?.?directness?. For a ?re-
quest offer?, generally only the directness field is
used.
Type can take the following values: ?standard?,
?tradeoff?, ?compromise?, ?concession?, and ?re-
traction?. The difference between ?compromise?
and ?concession? is subtle. ?Concession? means
that ?I don?t really want to do this but I?ll do it be-
cause there is no other way?. ?Compromise? is like
splitting the difference and it does not imply that the
speaker does not like the option.
Beneficiary can be ?me?, ?you?, ?both?, ?else?,
or ?null?. By beneficiary we mean who the offer or
argument would be good for (see also section 3.7).
So for example, if one?s argument is ?it will be too
cold for the customers? then ?beneficiary=else?.
Directness can be ?direct? or ?indirect?. An of-
fer or argument is ?indirect? when it needs to be in-
ferred. For example, when the grocer says ?well let?s
say there are lots of other local florists competing for
your prices?, she means that this is why advertising
is important, but this needs some kind of inference,
so the argument is indirect.
Below we can see examples of various types of
offers (the beneficiary and directness dimensions are
omitted for brevity).
offer.standard How about 62 degrees?
offer.tradeoff (between different issues) I?ll agree
on 64 degrees if you agree on design A.
offer.compromise Well should we just say 50/50?
offer.concession There is no other way so I agree
on 64 degrees.
offer.retraction I changed my mind, I don?t want de-
sign A.
request offer What temperature do you suggest?
274
3.6 General Reaction
accept Okay, 62 degrees is fine. or Yes, I said 62
degrees.
reject 62 degrees is too low for me. or No, I didn?t
say that.
acknowledge I see.
Note that ?accept? is used for accepting offers and
confirmation requests but also for agreement, for ex-
ample, when one interlocutor agrees with the argu-
ment of the other interlocutor. ?Reject? is used for
rejecting offers and confirmation requests but also
for disagreement.
3.7 Argumentation
An argument follows the following format:
?role?.?type?.?beneficiary?.?directness?. The role
can be ?provide argument?, ?attack argument?,
?rebut argument?, ?undercut argument?, and ?ac-
cept defeat?. Beneficiary and directness are defined
as in section 3.5. Below we can see examples of dif-
ferent argument roles.
provide argument The temperature must be low for
my flowers to stay fresh.
attack argument (without necessarily providing a
counter-argument) What you say does not make
sense.
rebut argument (provide a counter-argument) Yes,
but my customers wouldn?t want to shop in such
a low temperature.
undercut argument (invalidate an argument) You
don?t need a low temperature in the shop. Your
flowers can be refrigerated to stay fresh.
accept defeat You are right, I could use a refriger-
ator.
We have identified the following argument types:
ideology (what is ?right?), logic, fairness, prece-
dent, God?s will, promise for the future, honor, duty,
identity, authority, refer to relationship, appeal to
feelings, social responsibility, assurance (abstract
promises), stories/metaphors, ordinance, design
(aesthetics and functionality), effect/consequence,
cost/means. These types are mostly inspired by our
literature review (see section 2.2), as well as our ob-
servations in the domains that we used for develop-
ing the annotation scheme.
An example logical argument is ?my flowers need
low temperatures to stay fresh?. An example argu-
ment that appeals to fairness is ?I helped you last
time so it?s fair to help me now?. Arguments that
appeal to logic are more likely to appear in indi-
vidualistic cultures. Arguments that appeal to duty,
honor, social responsibility, ideology, and fairness
are more common in collectivistic cultures. Sto-
ries/metaphors are very common in Arab cultures
(Koch, 1983; Zaharna, 1995).
3.8 Other Speech Acts
repetition I prefer design A. I said design A.
heavy commitment $50 is all I can give, not a cent
more.
weak commitment Let?s assume that we agree on
this and continue.
meta task discussion (try to figure out the task) You
are the grocer and I am the florist.
self contradiction Speaker A: I like design C.
Speaker A (later): Design C is terrible.
show concern I understand that this solution would
not be good for you.
putdown You are stubborn.
show frustration I?m really sick and tired of this.
threat If you don?t accept my offer I won?t do busi-
ness with you again.
miscellaneous Yes, flowers are beautiful.
4 Applications of the Annotation Scheme
on Various Corpora
In order to prove its generality we applied this cod-
ing scheme to three different negotiation domains.
4.1 Florist-Grocer Domain
The first domain was dialogues between American
undergraduates playing the role of a florist and a gro-
cer who share a retail space. The dialogues were
collected by Laurie R. Weingart, Jeanne M. Brett,
and Mary C. Kern at Northwestern University. The
florist and the grocer negotiate on four issues: the
design of the space, the temperature, the rent, and
their advertising policy. Using the above coding
scheme we annotated 21 dialogues. Example anno-
tations of speech acts are given in Figure 1, as well
as the examples in section 3, above.
The final scheme was the result of several cy-
cles of dialogue annotations and revisions of the
coding manual. We used the florist-grocer annota-
tions to measure inter-annotator reliability between
four annotators. In three cycles of annotation, we
275
measured agreement on speech acts only and com-
plex speech acts were unified, for example, all the
?provide argument? are treated as a single category.
Krippendorff?s ? (Krippendorff, 1980) rose from
0.375 to 0.463 to 0.565.2
After analyzing these results we noticed that the
main problems in terms of inter-annotator relia-
bility were the confusion between ?accept? and
?acknowledge? (e.g. the utterance ?yeah? could
be either, depending on the context), and the
confusion between ?provide argument.logic?, ?pro-
vide argument.effect?, and ?provide info?. So we
revised the manual as follows: in order for some-
thing to be annotated as ?accept? vs. ?acknowledge?
we need to look forward in the dialogue; if an ar-
gument?s type is both ?logic? and ?effect? then ?ef-
fect? supersedes; ?provide info? is just provision of
a piece of information with no argumentative role.
4.2 SASO Domain
In this second domain (Traum et al, 2008), we an-
notated role-play dialogues in English between a US
Army captain and a Spanish doctor in Iraq. We have
annotated five dialogues so far. An example is given
in Figure 2.
4.3 Toy-Naming Domain
Finally, in the third domain groups of four people
negotiate in English, Spanish, and Arabic about how
to name a toy. The dialogues were part of the UTEP-
ICT Cross-Cultural dialogue corpus (Herrera et al,
2010). We have annotated five dialogues in English
and three in Arabic so far, and are currently work-
ing on Spanish. An example is given in Figure 3.
The ?redirect topic? act was added based on this do-
main (to cover cases where one person consciously
redirects the group?s attention to the task when they
drift off-topic for an extended period of time). Also,
we added three domain-specific specializations of
?provide info.fact? and ?request info.fact?: ?pro-
vide info.fact.function? (discussion about what one
can do with the toy or things that it does or has, e.g.
a secret compartment); ?provide info.fact.origin?
(where the toy was manufactured or bought); ?re-
quest info.fact.utility? (a person prompts the others
for ideas or examples of how the toy could be used
and marketed).
2Krippendorff?s ? is 0.460 in the first cycle if we exclude
one of the annotators who annotated only 72% of the items.
5 Discussion
We believe that this annotation scheme can be used
for analyzing and modeling the fine differences of
argumentation and negotiation styles, cross-task,
and cross-culture, as well as providing a basis for
artificial agents to engage in differentiated negotia-
tion behavior.
Our first use of the annotated florist-grocer di-
alogues was for learning dialogue policies using
simulated users and Reinforcement Learning (RL)
(Georgila and Traum, 2011). To facilitate RL we
had to make a few simplifications, for example, fo-
cus only on the temperature issue. In particular, we
built policies for individualistic vs. altruistic florists
(and grocers). Our results in simulation were consis-
tent with our reward functions, i.e. the florist individ-
ualist agreed on low temperatures while interacting
with the grocer altruist, the florist altruist agreed on
high temperatures vs. the grocer individualist, etc.
Details are given in (Georgila and Traum, 2011).
6 Conclusion
We presented a novel annotation scheme for cross-
cultural argumentation and persuasion dialogues.
This scheme is based on a review of literature on
cross-cultural argumentation and persuasion, and
adaptation of existing coding schemes on negotia-
tion. Our annotation scheme is also based on our ob-
servations from its application to coding both two-
party and multi-party negotiation dialogues in three
different domains, and is general enough to be ap-
plicable to different domains with minor or no mod-
ifications at all. Furthermore, dialogues annotated
with the scheme have been used to successfully learn
culture-specific dialogue policies for argumentation
and persuasion.
Acknowledgments
This research was funded by a MURI award through
ARO grant number W911NF-08-1-0301. We are
grateful to Laurie R. Weingart, Jeanne M. Brett,
and Mary C. Kern who provided us with the florist-
grocer dialogues, and to David A. Herrera, David G.
Novick, and Dusan Jan who developed the UTEP-
ICT corpus, as well as Hussein Sadek for transcrip-
tions and translations of the Arabic dialogues.
276
References
J.M. Brett and M.J. Gelfand. 2006. A cultural analysis of
the underlying assumptions of negotiation theory. In
Frontiers of Negotiation Research, L. Thompson (Ed),
pages 173?201. Psychology Press.
H. Bunt. 2006. Dimensions in dialogue act annotation.
In Proc. of LREC.
P.J. Carnevale, D.G. Pruitt, and S.D. Seilheimer. 1981.
Looking and competing: Accountability and visual ac-
cess in integrative bargaining. Journal of Personality
and Social Psychology, 40(1):111?120.
J. Cassell. 2009. Culture as social culture: Being en-
culturated in human-computer interaction. In Proc. of
HCI International.
R.B. Cialdini. 1998. Influence: The psychology of per-
suasion, Revised. Collins.
R. Cohen. 1987. Analyzing the structure of argumen-
tative discourse. Computational Linguistics, 13(1-
2):11?24.
M.G. Core and J.F. Allen. 1997. Coding dialogs with the
DAMSL annotation scheme. In Proc. of the AAAI Fall
Symposium on Communicative Actions in Humans and
Machines.
P.M. Dung. 1995. On the acceptability of arguments and
its fundamental role in nonmonotonic reasoning, logic
programming and n-person games. Artificial Intelli-
gence, 77(2):321?357.
S. George, I. Zukerman, and M. Niemann. 2007. In-
ferences, suppositions and explanatory extensions in
argument interpretation. User Modeling and User-
Adapted Interaction, 17(5):439?474.
K. Georgila and D. Traum. 2011. Learning culture-
specific dialogue models from non culture-specific
data. In Proc. of HCI International.
F. Grasso, A. Cawsey, and R. Jones. 2000. Dialectical ar-
gumentation to solve conflicts in advice giving: A case
study in the promotion of healthy nutrition. Interna-
tional Journal of Human-Computer Studies, 53:1077?
1115.
S. Han and S. Shavitt. 1994. Persuasion and culture:
Advertising appeals in individualistic and collectivistic
societies. Journal of Experimental Social Psychology,
30:326?350.
D. Herrera, D. Novick, D. Jan, and D. Traum. 2010. The
UTEP-ICT cross-cultural multiparty multimodal dia-
log corpus. In Proc. of the LREC Multimodal Corpora
Workshop: Advances in Capturing, Coding and Ana-
lyzing Multimodality (MMC).
J. Katzav and C. Reed. 2004. On argumentation schemes
and the natural classification of arguments. Argumen-
tation, 18(2):239?259.
M.C. Kern, J.M. Brett, and L.R. Weingart. 2005. Get-
ting the floor: Motive-consistent strategy and individ-
ual outcomes in multi-party negotiations. Group De-
cision and Negotiation, 14:21?41.
B. Johnstone Koch. 1983. Presentation as proof: The
language of Arabic rhetoric. Anthropological Linguis-
tics, 25(1):47?60.
K. Krippendorff. 1980. Content analysis: An introduc-
tion to its methodology, chapter 12. Sage, Beverly
Hills, CA.
P. Paruchuri, N. Chakraborty, R. Zivan, K. Sycara,
M. Dudik, and G. Gordon. 2009. POMDP based ne-
gotiation modeling. In Proc. of the IJCAI Workshop on
Modeling Intercultural Collaboration and Negotiation
(MICON).
K. Peng and R.E. Nisbett. 1999. Culture, dialectics,
and reasoning about contradiction. American Psychol-
ogist, 54(9):741?754.
P. Piwek. 2008. Presenting arguments as fictive dia-
logue. In Proc. of the ECAI Workshop on Computa-
tional Models of Natural Argument (CMNA).
J.L. Pollock. 1995. Cognitive Carpentry: A blueprint for
how to build a person. Bradford Books, MIT Press.
H. Prakken. 2008. A formal model of adjudication dia-
logues. Artificial Intelligence and Law, 16:305?328.
D.G. Pruitt and S.A. Lewis. 1975. Development of in-
tegrative solutions in bilateral negotiation. Journal of
Personality and Social Psychology, 31(4):621?633.
C. Rose? and C. Torrey. 2004. DReSDeN: Towards a
trainable tutorial dialogue manager to support negoti-
ation dialogues for learning and reflection. In Proc. of
ITS.
C.L. Sidner. 1994. An artificial discourse language for
collaborative negotiation. In Proc. of the National
Conference on Artificial Intelligence.
K. Sycara. 1990. Persuasive argumentation in negotia-
tion. Theory and Decision, 28(3):203?242.
D. Traum, J. Rickel, S. Marsella, and J. Gratch. 2003.
Negotiation over tasks in hybrid human-agent teams
for simulation-based training. In Proc. of AAMAS.
D. Traum, S. Marsella, J. Gratch, J. Lee, and A. Hartholt.
2008. Multi-party, multi-issue, multi-strategy negotia-
tion for multi-modal virtual agents. In Proc. of IVA.
D. Traum. 2009. Models of culture for virtual human
conversation. In Proc. of HCI International.
D. Walton, C. Reed, and F. Macagno. 2008. Argumenta-
tion Schemes. Cambridge University Press.
R.S. Zaharna. 1995. Understanding cultural preferences
of Arab communication partners. Public Relations Re-
view, 21(3):241?255.
I. Zukerman. 2001. An integrated approach for generat-
ing arguments and understanding rejoinders. In Proc.
of the International Conference on User Modeling.
277
Appendix
Florist: How does that work for you? (request info.preference)
Grocer: Well, personally for the grocery I think it is better to have a higher temperature. (pro-
vide argument.logic.me.indirect)
Grocer: Just because I want the customers to feel comfortable. (elaborate)
Florist: Okay. (acknowledge)
Grocer: And also if it is warm, people are more apt to buy cold drinks to keep themselves comfortable and
cool. (elaborate)
Florist: That?s true. (accept)
Florist: But what about your products staying fresh? Don?t they have to stay fresh or otherwise? (re-
but argument.logic.you.direct)
Figure 1: Example annotated dialogue with speech acts in the florist-grocer domain.
Captain: I think if you just made the compromise, we could provide so much for you if you just agreed to
let us move the clinic. (offer.standard.you.direct)
Doctor: Look I need to get back to my patients. They?re dying now. They?re dying. (show frustration)
Captain: They wouldn?t be dying if you let us move the clinic to the US Army base with the additional
medical support. (provide argument.logic.else.direct)
Doctor: Well they wouldn?t be dying if I was there. (rebut argument.logic.else.direct)
Doctor: Why don?t you provide us with additional medical support and get out of our lives? (re-
quest offer.direct)
Figure 2: Example annotated dialogue with speech acts in the SASO domain.
Speaker 3: Blue pal. (offer.standard.null.direct)
Speaker 4: Blue pal. (acknowledge)
Speaker 2: Blue pal. (acknowledge)
Speaker 4: That sounds pretty good. I actually like the idea. (accept)
Speaker 1: What if it?s a different color? (provide argument.logic.null.direct)
Speaker 2: Yeah, what if it?s like pink and purple. . . (elaborate)
Speaker 4: Uh I like blue pal. I think that one?s pretty cool. . . (provide info.preference)
Speaker 2: Something pal like your pal. (offer.standard.null.direct)
Speaker 4: Blue pal the singing singing pal the singing pal the singing and dancing buddy. The beast you
don?t want to get angry. (offer.standard.null.direct)
Speaker 2: That?s too long. (reject)
Speaker 2: It has to be short. (provide argument.logic.null.direct)
Speaker 1: Furball. (offer.standard.null.direct)
Speaker 4: A short name... Actually a good really long name might work because everything out there is
short... (rebut argument.logic.null.direct)
Figure 3: Example annotated dialogue with speech acts in the toy-naming domain.
278
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 84?93,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Reinforcement Learning of Question-Answering Dialogue Policies
for Virtual Museum Guides
Teruhisa Misu1?, Kallirroi Georgila2, Anton Leuski2, David Traum2
1National Institute of Information and Communications Technology (NICT), Kyoto, Japan
2USC Institute for Creative Technologies, Playa Vista, CA, USA
teruhisa.misu@nict.go.jp, {kgeorgila,leuski,traum}@ict.usc.edu
Abstract
We use Reinforcement Learning (RL) to learn
question-answering dialogue policies for a
real-world application. We analyze a corpus
of interactions of museum visitors with two
virtual characters that serve as guides at the
Museum of Science in Boston, in order to
build a realistic model of user behavior when
interacting with these characters. A simulated
user is built based on this model and used
for learning the dialogue policy of the virtual
characters using RL. Our learned policy out-
performs two baselines (including the original
dialogue policy that was used for collecting
the corpus) in a simulation setting.
1 Introduction
In the last 10 years Reinforcement Learning (RL)
has attracted much attention in the dialogue commu-
nity, to the extent that we can now consider RL as the
state-of-the-art in statistical dialogue management.
RL is used in the framework of Markov Decision
Processes (MDPs) or Partially Observable Markov
Decision Processes (POMDPs). In this paradigm
dialogue moves transition between dialogue states
and rewards are given at the end of a successful dia-
logue. The goal of RL is to learn a dialogue policy,
i.e. the optimal action that the system should take at
each possible dialogue state. Typically rewards de-
pend on the domain and can include factors such as
task completion, dialogue length, and user satisfac-
tion. Traditional RL algorithms require on the order
? This work was done when the first author was a visiting
researcher at USC/ICT.
of thousands of dialogues to achieve good perfor-
mance. Because it is very difficult to collect such a
large number of dialogues with real users, instead,
simulated users (SUs), i.e. models that simulate the
behavior of real users, are employed (Georgila et al,
2006). Through the interaction between the system
and the SUs thousands of dialogues can be gener-
ated and used for learning. A good SU should be
able to replicate the behavior of a real user in the
same dialogue context (Ai and Litman, 2008).
Most research in RL for dialogue management
has been done in the framework of slot-filling appli-
cations (Georgila et al, 2010; Thomson and Young,
2010), largely ignoring other types of dialogue. In
this paper we focus on the problem of learning di-
alogue policies for question-answering characters.
With question-answering systems (or characters),
the natural language understanding task is to retrieve
the best response to a user initiative, and the main
dialogue policy decision is whether to provide this
best response or some other kind of move (e.g. a re-
quest for repair, clarification, or topic change), when
the best answer does not seem to be good enough.
Note that often in the literature the term question-
answering is used for slot-filling dialogue systems
as well, in the sense that the user asks some ques-
tions, for example, about restaurants in a particular
area, and the system answers by providing a list of
options, for example, restaurants. We use the term
?question-answering? for systems where user ques-
tions can be independent of one another (follow-
up questions are possible though) and do not have
the objective of reducing the search space and re-
trieving results from a database of e.g. restaurants,
flights, etc. Thus examples of question-answering
84
characters can be virtual interviewees (that can an-
swer questions, e.g. about an incident), virtual scien-
tists (that can answer general science-related ques-
tions), and so forth.
For our experiments we use a corpus (Aggarwal
et al, 2012) of interactions of real users with two
virtual characters, the Twins, that serve as guides at
the Museum of Science in Boston (Swartout et al,
2010). The role of these virtual characters is to en-
tertain and educate the museum visitors. They can
answer queries about themselves and their technol-
ogy, generally about science, as well as questions
related to the exhibits of the museum. An example
interaction between a museum visitor and the Twins
is shown in Figure 1. The dialogue policy of the
Twins was arbitrarily hand-crafted (see section 7 for
details) and many other policies are possible (includ-
ing Baseline 2, presented in section 7, and taking
more advantage of question topics and context). We
propose to use RL for optimizing the system?s re-
sponse generation. This is a real-world application
for which RL appears to be an appropriate method.
Although there are similarities between question-
answering and slot-filling dialogues there are also a
number of differences, such as the reward function
and the behavior of the users. As discussed later in
detail, in question-answering the users have a num-
ber of questions that they are planning to ask (stock
of queries), which can be increased or decreased de-
pending not only on whether they received the in-
formation that they wanted but also on how satisfied
they are with the interaction. The system has to plan
ahead in order to maximize the number of success-
ful responses that it provides to user queries. At the
same time it needs to avoid providing incorrect or
incoherent responses so that the user does not give
up the interaction.
One of the challenges of our task is to define an
appropriate reward function. Unlike slot-filling dia-
logues, it is not clear what makes an interaction with
a question-answering system successful. A second
challenge is that in a museum setting it is not clear
what constitutes a dialogue session. Often two or
more users alternate in asking questions, which fur-
ther complicates the problem of defining a good re-
ward function. A third challenge is that the domain
is not well defined, i.e. users do not know in advance
what the system is capable of (what kind of ques-
tions the characters can answer). Moreover, there
User: What are your names? (ASR: what are
your names)
Ada: My name?s Ada.
Grace: And I?m Grace. We?re your Virtual Mu-
seum Guides. With your help, we can suggest ex-
hibits that will get you thinking! Or answer ques-
tions about things you may have seen here.
Ada: What do you want to learn about?
User: Artificial intelligence. (ASR: is
artificial intelligence)
Grace: One example of AI, or Artificial Intelli-
gence, is 20Q, an online computer activity here at
Computer Place that asks you questions to guess
what you?re thinking.
Ada: I wish we?d been programmed to do that.
Nah. . . on second thought, I prefer just answering
your questions.
Grace: That takes AI too.
Figure 1: Example dialogue between the Twins virtual
characters and a museum visitor.
are many cases of ?junk? user questions (e.g. ?are
you stupid??) or even user prompts in languages
other than English (e.g. ?hola?).
We first analyze our corpus in order to build a re-
alistic model of user behavior when interacting with
the virtual characters. A SU is built based on this
model and used for learning the dialogue policy of
the virtual characters using RL. Then we compare
our learned policy with two baselines, one of which
is the dialogue policy of the original system that was
used for collecting our corpus and that is currently
installed at the Museum of Science in Boston. Our
learned policy outperforms both baselines in a sim-
ulation setting.
To our knowledge this is the first study that uses
RL for learning this type of question-answering dia-
logue policy. Furthermore, unlike most studies that
use data collected by having paid subjects interact
with the system, we use data collected from real
users, in our case museum visitors.1 We also com-
pare our learned dialogue policy with the dialogue
policy of the original system that is currently in-
stalled at the Museum of Science in Boston.
The structure of the paper is as follows. In sec-
1Note that the CMU ?Let?s Go!? corpus is another case of
using real user data for learning dialogue policies for the Spoken
Dialogue Challenge.
85
tion 2 we present related work. Section 3 provides a
brief introduction to RL and section 4 describes our
corpus. Then in section 5 we explain how we built
our SU from the corpus, and in section 6 we describe
our learning methodology. Section 7 presents our
evaluation results. Finally section 8 presents some
discussion and ideas for future work together with
our conclusion.
2 Related Work
To date, RL has mainly been used for learning di-
alogue policies for slot-filling applications such as
restaurant recommendations (Jurc???c?ek et al, 2012),
sightseeing recommendations (Misu et al, 2010),
appointment scheduling (Georgila et al, 2010), etc.,
largely ignoring other types of dialogue. Recently
there have been some experiments on applying RL
to the more difficult problem of learning negotia-
tion policies (Heeman, 2009; Georgila and Traum,
2011a; Georgila and Traum, 2011b). Also, RL has
been applied to tutoring domains (Tetreault and Lit-
man, 2008; Chi et al, 2011).
There has been a lot of work on developing
question-answering systems with dialogue capabil-
ities, e.g. (Jo?nsson et al, 2004; op den Akker et al,
2005; Varges et al, 2009). Most of these systems are
designed for information extraction from structured
or unstructured databases in closed or open domains.
One could think of them as adding dialogue capa-
bilities to standard question-answering systems such
as the ones used in the TREC question-answering
track (Voorhees, 2001). Other work has focused on
a different type of question-answering dialogue, i.e.
question-answering dialogues that follow the form
of an interview and that can be used, for example,
for training purposes (Leuski et al, 2006; Gandhe et
al., 2009). But none of these systems uses RL.
To our knowledge no one has used RL for learning
policies for question-answering systems as defined
in section 1. Note that Rieser and Lemon (2009)
used RL for question-answering, but in their case,
question-answering refers to asking for information
about songs and artists in an mp3 database, which
is very much like a slot-filling task, i.e. the system
has to fill a number of slots (e.g. name of band, etc.)
in order to query a database of songs and present
the right information to the user. As discussed in
section 1 our task is rather different.
3 Reinforcement Learning
A dialogue policy is a function from contexts to
(possibly probabilistic) decisions that the dialogue
system will make in those contexts. Reinforcement
Learning (RL) is a machine learning technique used
to learn the policy of the system. For an RL-based
dialogue system the objective is to maximize the re-
ward it gets during an interaction. RL is used in the
framework of Markov Decision Processes (MDPs)
or Partially Observable Markov Decision Processes
(POMDPs).
In this paper we follow a POMDP-based ap-
proach. A POMDP is defined as a tuple (S, A, P , R,
O, Z, ?, b0) where S is the set of states (representing
different contexts) which the system may be in (the
system?s world),A is the set of actions of the system,
P : S ? A ? P (S, A) is the set of transition prob-
abilities between states after taking an action, R : S
? A ?< is the reward function, O is a set of obser-
vations that the system can receive about the world,
Z is a set of observation probabilities Z : S ? A
? Z(S, A), and ? a discount factor weighting long-
term rewards. At any given time step i the world
is in some unobserved state si ? S. Because si is
not known exactly, we keep a distribution over states
called a belief state b, thus b(si) is the probability of
being in state si, with initial belief state b0. When
the system performs an action ?i ? A based on b,
following a policy pi : S ? A, it receives a reward
ri(si, ?i) ? < and transitions to state si+1 accord-
ing to P (si+1|si, ?i) ? P . The system then receives
an observation oi+1 according to P (oi+1|si+1, ?i).
The quality of the policy pi followed by the agent is
measured by the expected future reward also called
Q-function, Qpi : S ? A ? <.
There are several algorithms for learning the opti-
mal dialogue policy and we use Natural Actor Critic
(NAC) (Peters and Schaal, 2008), which adopts a
natural policy gradient method for policy optimiza-
tion, also used by (Thomson and Young, 2010;
Jurc???c?ek et al, 2012). Policy gradient methods do
not directly update the value of state S orQ-function
(expected future reward). Instead, the policy pi (or
parameter?, see below) is directly updated so as to
increase the reward of dialogue episodes generated
by the previous policy.
A system action asys is sampled based on the fol-
lowing soft-max (Boltzmann) policy:
86
pi(asys = k|?) = Pr(asys = k|?,?)
= exp(
?I
i=1 ?i ? ?ki)?J
j=1 exp(
?I
i=1 ?i ? ?ji)
Here, ? = (?1, ?2, . . . , ?I) is a basis func-
tion, which is a vector function of the belief state.
? = (?11, ?12, . . . ?1I , . . . , ?JI ) consists of J (# ac-
tions) ? I (# features) parameters. The parameter
?ji works as a weight for the i-th feature of the ac-
tion j and determines the likelihood that the action j
is selected. ? is the target of optimization by RL.
During training, RL algorithms require thousands
of interactions between the system and the user
to achieve good performance. For this reason we
need to build a simulated user (SU) (Georgila et al,
2006), that will behave similarly to a real user, and
will interact with the policy for thousands of itera-
tions to generate data in order to explore the search
space and thus facilitate learning.
Topic Example user question/prompt
introduction Hello.
personal Who are you named after?
school Where do you go to school?
technology What is artificial intelligence?
interfaces What is a virtual human?
exhibition What can I do at Robot Park?
Table 1: Topics of user questions/prompts.
4 The Twins Corpus
As mentioned in section 1 the Twins corpus (Aggar-
wal et al, 2012) was collected at the Museum of Sci-
ence in Boston (Swartout et al, 2010). The Twins
can answer a number of user questions/prompts in
several topics, i.e. about themselves and their tech-
nology, about science in general, and about exhibits
in the museum. We have divided these topics in six
categories shown in Table 1 together with an exam-
ple for each category.
An example interaction between a museum vis-
itor and the Twins is shown in Figure 1. We can
also see the output of the speech recognizer. In the
part of the corpus that we use for our experiment
automatic speech recognition (ASR) was performed
by Otosense, an ASR engine developed by the USC
SAIL lab. Natural language understanding and di-
alogue management are both performed as a single
task by the NPCEditor (Leuski and Traum, 2010),
a text classification system that classifies the user?s
query to a system?s answer using cross-language in-
formation retrieval techniques. When the system
fails to understand the user?s query it can prompt her
to do one of the following:
? rephrase her query (from now on referred to
as off-topic response 1, OT1), e.g. ?please
rephrase your question?;
? prompt the user to ask a particular question that
the system knows that it can handle (from now
on referred to as off-topic response 2, OT2),
e.g. ?you may ask us about our hobbies?;
? cease the dialogue and check out the ?behind
the scenes? exhibit which explains how the vir-
tual characters work (from now on referred to
as off-topic response 3, OT3).
The Twins corpus contains about 200,000 spoken
utterances from museum visitors (primarily chil-
dren) and members of staff or volunteers. For the
purposes of this paper we used 1,178 dialogue ses-
sions (11,074 pairs of user and system utterances)
collected during March to May 2011. This subset
of the corpus contains manual transcriptions of user
queries, system responses, and correct responses to
user queries (the responses that the system should
give when ASR is perfect).
5 User Simulation Model
In order to build a model of user behavior we per-
form an analysis of the corpus. One of our chal-
lenges is that the boundaries between dialogue ses-
sions are hard to define, i.e. it is very hard to auto-
matically calculate whether the same or a new user
speaks to the system, unless complex voice iden-
tification techniques are employed. We make the
reasonable assumption that a new dialogue session
starts when there are no questions to the system for
a time interval greater than 120 sec.
From each session we extract 30 features. A full
list is shown in Table 7 in the Appendix. Our goal
is to measure the contribution of each feature to
the user?s decision with respect to two issues: (1)
whether the user will cease the dialogue or not, and
(2) what kind of query the user will make next, based
87
on what has happened in the dialogue so far. To do
that we use the Chi-squared test, which is commonly
used for feature selection.
So to measure the contribution of each feature to
whether the user will cease the dialogue or not, we
give a binary label to each user query in our corpus,
i.e. 1 when the query is the last user query in the di-
alogue session and 0 otherwise. Then we calculate
the contribution of each feature for estimating this
label. In Table 8, column 1, in the Appendix, we can
see the 10 features that contribute the most to pre-
dicting whether the user will cease the dialogue. As
we can see the dominant features are not whether
the system correctly responded to the user?s query,
but mostly features based on the dialogue history
(e.g. the number of the system?s off-topic responses
so far) and user type information. Indeed, a further
analysis of the corpus showed that children tend to
have longer dialogue sessions than adults.
Our next step is the estimation of the contribution
of each feature for predicting the user?s next query.
The label we predict here is the topic of the user?s
utterance (personal, exhibition, etc., see Table 1).
We can see the 10 most predictive features in Ta-
ble 8, column 2, in the Appendix. The contribution
of the most recent user?s utterance (previous topic
category) is larger than that of dialogue history fea-
tures. This tendency is the same when we ignore re-
peated user queries, e.g. when the system makes an
error and the user rephrases her query (see Table 8,
column 3, in the Appendix). The user type is impor-
tant for predicting the next user query. In Figure 2
we can see the percentages of user queries per user
type and topic.
Based on the above analysis we build a simulated
user (SU). The SU simulates the following:
? User type (child, male, female): a child user
is sampled with a probability of 51.1%, a male
with 31.1%, and a female with 17.8%. These
probabilities are estimated from the corpus.
? Number of questions the user is planning to
ask (stock of queries): We assume here that
the user is planning to ask a number of ques-
tions. This number may increase or decrease.
For example, it can increase when the system
prompts the user to ask about a particular topic
(OT2 prompt), and it may decrease when the
user decides to cease the dialogue immediately.
Figure 2: Percentages of user queries per user type and
topic.
The number of questions is sampled from a
user type dependent Zipf distribution (strictly
speaking the continuous version of the distri-
bution; Parato distribution) the parameter of
which is estimated from the corpus using the
maximum likelihood criterion. We chose Zipf
because it is a long-tail distribution that fits our
data (users are not expected to ask a large num-
ber of questions). According to this distribution
a child user is more likely to have a larger stock
of queries than a male or female adult.
? User?s reaction: The user has to decide on
one of the following. Go to the next topic
(Go-on); cease the dialogue if there are no
more questions in the stock of queries (Out-of-
stock); rephrase the previous query (Rephrase);
abandon the dialogue (Give-up) regardless of
the remaining questions in the stock; gener-
ate a query based on a system recommenda-
tion, OT2 prompt (Refill). We calculate the
user type dependent probability for these ac-
tions from the corpus. But the problem here
is that it is not possible to distinguish be-
tween the case in which the user asked all the
questions in the stock of queries (i.e. all the
questions she intended to ask) and left, from
the case in which she gave up and abandoned
the dialogue. We estimate the percentage of
?Give-up? as the difference between the ratio of
?Cease? after an incorrect response and the ra-
88
tio of ?Cease? after a correct response, assum-
ing a similar percentage of ?Out-of-stock? for
both correct and incorrect responses. Likewise,
the difference in ?Go-on? for OT2 and other re-
sponses is attributed to ?Refill?. The probabil-
ity of ?Rephrase? is estimated from the corpus.
For example the probability that a child will
rephrase after an OT1 system prompt is 54%,
after an erroneous system prompt 38%, etc.
? Topic for next user query (e.g. introduction,
personal, etc.): The SU selects a new topic
based on user type dependent topic transition
bigram probabilities estimated from the corpus.
? User utterance: The SU selects a user utter-
ance from the corpus that matches the current
user type and topic. We have split the corpus
in groups of user utterances based on user type
and topic and we sample accordingly.
? Utterance timing: We simulate utterance tim-
ing (duration of pause between system utter-
ance and next user query) per user type and
user change. The utterance timing is sampled
based on a Gaussian distribution the parameters
of which are set based on the corpus statistics.
For example, the average duration of a session
until the user changes is 62.7 sec with a stan-
dard deviation of 71.2 sec.
6 Learning Question-Answering Policies
Our goal is to use RL in order to optimize the sys-
tem?s response generation. As we saw in the previ-
ous section the SU generates a user utterance from
our corpus. We do not currently use ASR error sim-
ulation but instead a real ASR engine. So the au-
dio file that corresponds to the selected user utter-
ance is forwarded to 3 ASR systems, with child,
male, and female acoustic models (AMs) respec-
tively. Then these recognition results are forwarded
to the NPCEditor that produces an N-best list of pos-
sible system responses (retrieval results). That is,
as mentioned in section 4, the NPCEditor classifies
each ASR result to a system answer using cross-
language information retrieval techniques. The pol-
icy can choose one of the NPCEditor retrieval re-
sults or reject them and instead present one of the
three off-topic prompts (OT1, OT2, or OT3). So the
system has 10 possible actions to choose between:
? use the response with the best or the second
best score retrieved from the NPCEditor based
on a child AM (2 actions);
? use the response with the best or the second
best score retrieved from the NPCEditor based
on a male AM (2 actions);
? use the response with the best or the second
best score retrieved from the NPCEditor based
on a female AM (2 actions);
? use the response with the best of the 6 afore-
mentioned scores of the NPCEditor;
? use off-topic prompt OT1;
? use off-topic prompt OT2;
? use off-topic prompt OT3.
We use the following features to optimize our di-
alogue policy (see section 3). We use the 6 retrieval
scores of the NPCEditor (the 2 best scores for each
user type ASR result), the previous system action,
the ASR confidence scores, the voting scores (calcu-
lated by adding the scores of the results that agree),
the system?s belief on the user type and user change,
and the system?s belief on the user?s previous topic.
So we need to learn a POMDP-based policy using
these 42 features.
Unlike slot-filling dialogues, defining the reward
function is not a simple task (e.g. reward the system
for filled and confirmed slots). So in order to define
the reward function and thus measure the quality of
the dialogue we set up a questionnaire. We asked
5 people to rate 10 dialogues in a 5-Likert scale.
Each dialogue session included 5 question-answer
pairs. Then we used regression analysis to set the
reward for each of the question-answer pair cate-
gories shown in Table 2. So for example, responding
correctly to an in-domain user question is rewarded
(+23.2) whereas providing an erroneous response to
a junk question, i.e. treating junk questions as if they
were in-domain questions, is penalized (-14.7).
One limitation of this reward function (Reward
function 1) is that it does not take into account
whether the user has previously experienced an off-
topic system prompt. To account for that we define
Reward function 2. Here we consider the number
of off-topic responses in the two most recent system
prompts. Reward function 2 is shown in Table 3.
89
QA Pair Reward
in-domain ? correct 23.2
in-domain ? error -12.2
in-domain ? OT1 -5.4
in-domain ? OT2 -8.4
in-domain ? OT3 -9.6
junk question ? error -14.7
junk question ? OT1 4.8
junk question ? OT2 10.2
junk question ? OT3 6.1
give up -16.9
Table 2: Reward function 1.
QA Pair Reward
in-domain ? correct 16.9
in-domain ? error -2.0
in-domain ? OT1 13.9
in-domain ? OT1(2) 7.3
in-domain ? OT2 -7.9
in-domain ? OT2(2) 4.2
in-domain ? OT3 -15.8
in-domain ? OT3(2) -8.3
junk question ? error -4.6
junk question ? OT1 4.1
junk question ? OT1(2) 4.1
junk question ? OT2 43.4
junk question ? OT2(2) -33.1
junk question ? OT3 3.1
junk question ? OT3(2) 6.1
give up -19.5
Table 3: Reward function 2.
As we can see, providing an OT2 as the first off-
topic response is a poor action (-7.9); it is preferable
to ask the user to rephrase her question (OT1) as a
first attempt to recover from the error (+13.9). On
the other hand, providing an OT2 prompt, after an
off-topic prompt has occured in the previous system
prompt, is a reasonable action (+4.2).
7 Evaluation
We compare our learned policy with two baselines.
The first baseline, Baseline 1, is the dialogue pol-
icy that is used by our system that is currently in-
stalled at the Museum of Science in Boston. Base-
line 1 selects the best ASR result (i.e. the result
with the highest confidence score) out of the results
with the 3 different AMs (child, male, and female),
and forwards this result to the NPCEditor to retrieve
the system?s response. If the NPCEditor score is
higher than an emprically set pre-defined threshold
(see (Leuski and Traum, 2010) for details), then the
system presents the retrieved response, otherwise it
presents an off-topic prompt. The system presents
these off-topic prompts in a fixed order. First, OT1,
then OT2, and then OT3.
We also have Baseline 2, which forwards all 3
ASR results to the NPCEditor (using child, male,
and female AMs). Then the NPCEditor retrieves 3
results, one for each one of the 3 ASR results, and
selects the retrieved result with the highest score.
Again if this score is higher than a threshold, the sys-
tem will present this result, otherwise it will present
an off-topic prompt.
Each policy interacts with the SU for 10,000 di-
alogue sessions and we calculate the average accu-
mulated reward for each dialogue. In Tables 4 and 5
we can see our results for Reward functions 1 and 2
respectively. In both cases the learned policy outper-
forms both baselines. For both reward functions the
most predictive feature is the ASR confidence score
when combined with the NPCEditor?s retrieval score
and the previous system action. Also, for both re-
ward functions the second best feature is ?voting?
when combined with the retrieval score and the pre-
vious system action.
In Table 6 we can see how often the learned pol-
icy, which is based on Reward function 1 using all
features, selects each one of the 10 system actions
(200,000 system turns in total).
Policy Avg Reward
Baseline 1 24.76 (19.29)
Baseline 2 51.63 (49.84)
Learned Policy - Features
Retrieval score
+ system action (*) 46.74
(*) + ASR confidence score 61.59
(*) + User type probability 47.28
(*) + Estimated previous topic 47.87
(*) + Voting 59.94
All features 60.93
Table 4: Results with reward function 1. The values in
parentheses for Baselines 1 and 2 are the rewards when
the NPCEditor does not use the pre-defined threshold.
90
Policy Avg Reward
Baseline 1 39.40 (38.51)
Baseline 2 55.45 (54.49)
Learned Policy - Features
Retrieval score
+ system action (*) 49.15
(*) + ASR confidence score 69.51
(*) + User type probability 50.15
(*) + Estimated previous topic 49.84
(*) + Voting 69.06
All features 73.59
Table 5: Results with reward function 2. The values in
parentheses for Baselines 1 and 2 are the rewards when
the NPCEditor does not use the pre-defined threshold.
System Action Frequency
Child + 1st best score 10.33%
Child + 2nd best score 2.70%
Male + 1st best score 13.72%
Male + 2nd best score 1.03%
Female + 1st best score 39.73%
Female + 2nd best score 0.79%
Best of scores 1-6 2.38%
OT1 11.01%
OT2 6.86%
OT3 11.45%
Table 6: Frequency of the system actions of the learned
policy that is based on Reward function 1 using all fea-
tures.
8 Discussion and Conclusion
We showed that RL is a promising technique for
learning question-answering policies. Currently we
use the same SU for both training and testing the
policies. One could argue that this favors the learned
policy over the baselines. Because our SU is based
on general corpus statistics (probability that the user
is child or male or female, number of questions the
user is planning to ask, probability of moving to the
next topic or ceasing the dialogue, utterance timing
statistics) rather than sequential information we be-
lieve that this is acceptable. We only use sequential
information when we calculate the next topic that
the user will choose. That is, due to the way the
SU is built and its randomness, we believe that it is
very unlikely that the same patterns that were gener-
ated during training will be generated during testing.
Thus we do not anticipate that our results would be
different if for testing we used a SU trained on a dif-
ferent part of the corpus, or that the learned policy is
favored over the baselines. However, this is some-
thing to verify experimentally in future work.
For future work we would also like to do the fol-
lowing. First of all, currently we are in the process of
analyzing user satisfaction questionnaires from mu-
seum visitors in order to define a better reward func-
tion. Second, we would like to use voice identifi-
cation techniques to automatically estimate from the
corpus the statistics of having more than one user
or alternating users in the same session. Third, and
most important, we would like to incorporate the
learned policy into the system that is currently in-
stalled in the museum and evaluate it with real users.
Fourth, currently our SU is based on only some of
our findings from the analysis of the corpus. We in-
tend to build a more complex and hopefully more
realistic SU based on our full corpus analysis. Fi-
nally, we will also experiment with learning policies
directly from the data (Li et al, 2009).
To conclude, we analyzed a corpus of interactions
of museum visitors with two virtual characters that
serve as guides at the Museum of Science in Boston,
in order to build a realistic model of user behavior
when interacting with these characters. Based on
this analysis, we built a SU and used it for learning
the dialogue policy of the virtual characters using
RL. We compared our learned policy with two base-
lines, one of which was the dialogue policy of the
original system that was used for collecting the cor-
pus and that is currently installed at the Museum of
Science in Boston. Our learned policy outperformed
both baselines which shows that RL is a promising
technique for learning question-answering dialogue
policies.
Acknowledgments
This work was funded by the NSF grant #1117313.
The Twins corpus collection was supported by the
NSF grant #0813541.
References
Priti Aggarwal, Ron Artstein, Jillian Gerten, Athanasios
Katsamanis, Shrikanth Narayanan, Angela Nazarian,
and David Traum. 2012. The Twins corpus of mu-
91
seum visitor questions. In Proc. of the Language
Resources and Evaluation Conference (LREC), pages
2355?2361, Istanbul, Turkey.
Hua Ai and Diane Litman. 2008. Assessing dialog sys-
tem user simulation evaluation measures using human
judges. In Proc. of the Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL-HLT), pages 622?629, Columbus,
OH, USA.
Min Chi, Kurt VanLehn, Diane Litman, and Pamela Jor-
dan. 2011. Empirically evaluating the application
of reinforcement learning to the induction of effective
and adaptive pedagogical strategies. User Modeling
and User-Adapted Interaction, 21(1-2):137?180.
Sudeep Gandhe, Nicolle Whitman, David Traum, and
Ron Artstein. 2009. An integrated authoring tool
for tactical questioning dialogue systems. In Proc. of
the IJCAI Workshop on Knowledge and Reasoning in
Practical Dialogue Systems, Pasadena, CA, USA.
Kallirroi Georgila and David Traum. 2011a. Learn-
ing culture-specific dialogue models from non culture-
specific data. In Proc. of HCI International, Lecture
Notes in Computer Science Vol. 6766, pages 440?449,
Orlando, FL, USA.
Kallirroi Georgila and David Traum. 2011b. Reinforce-
ment learning of argumentation dialogue policies in
negotiation. In Proc. of Interspeech, pages 2073?
2076, Florence, Italy.
Kallirroi Georgila, James Henderson, and Oliver Lemon.
2006. User simulation for spoken dialogue systems:
Learning and evaluation. In Proc. of Interspeech,
pages 1065?1068, Pittsburgh, PA, USA.
Kallirroi Georgila, Maria K. Wolters, and Johanna D.
Moore. 2010. Learning dialogue strategies from older
and younger simulated users. In Proc. of the Annual
SIGdial Meeting on Discourse and Dialogue (SIG-
dial), pages 103?106, Tokyo, Japan.
Peter A. Heeman. 2009. Representing the reinforcement
learning state in a negotiation dialogue. In Proc. of the
IEEE Automatic Speech Recognition and Understand-
ing Workshop (ASRU), Merano, Italy.
Arne Jo?nsson, Frida Ande?n, Lars Degerstedt, Annika
Flycht-Eriksson, Magnus Merkel, and Sara Norberg.
2004. Experiences from combining dialogue system
development with information access techniques. In
New Directions in Question Answering, Mark T. May-
bury (Ed), pages 153?164. AAAI/MIT Press.
Filip Jurc???c?ek, Blaise Thomson, and Steve Young. 2012.
Reinforcement learning for parameter estimation in
statistical spoken dialogue systems. Computer Speech
and Language, 26(3):168?192.
Anton Leuski and David Traum. 2010. Practical lan-
guage processing for virtual humans. In Proc. of the
22nd Annual Conference on Innovative Applications
of Artificial Intelligence (IAAI), Atlanta, GA, USA.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective question
answering characters. In Proc. of the Annual SIGdial
Meeting on Discourse and Dialogue (SIGdial), pages
18?27, Sydney, Australia.
Lihong Li, Jason D. Williams, and Suhrid Balakrishnan.
2009. Reinforcement learning for dialog management
using least-squares policy iteration and fast feature se-
lection. In Proc. of Interspeech, pages 2475?2478,
Brighton, United Kingdom.
Teruhisa Misu, Komei Sugiura, Kiyonori Ohtake, Chiori
Hori, Hideki Kashioka, Hisashi Kawai, and Satoshi
Nakamura. 2010. Modeling spoken decision making
dialogue and optimization of its dialogue strategy. In
Proc. of the Annual SIGdial Meeting on Discourse and
Dialogue (SIGdial), pages 221?224, Tokyo, Japan.
Rieks op den Akker, Harry Bunt, Simon Keizer, and
Boris van Schooten. 2005. From question answering
to spoken dialogue: Towards an information search as-
sistant for interactive multimodal information extrac-
tion. In Proc. of Interspeech, pages 2793?2796, Lis-
bon, Portugal.
Jan Peters and Stefan Schaal. 2008. Natural actor-critic.
Neurocomputing, 71(7-9):1180?1190.
Verena Rieser and Oliver Lemon. 2009. Does this list
contain what you were searching for? Learning adap-
tive dialogue strategies for interactive question an-
swering. Natural Language Engineering, 15(1):55?
72.
William Swartout, David Traum, Ron Artstein, Dan
Noren, Paul Debevec, Kerry Bronnenkant, Josh
Williams, Anton Leuski, Shrikanth Narayanan, Diane
Piepol, Chad Lane, Jacquelyn Morie, Priti Aggarwal,
Matt Liewer, Jen-Yuan Chiang, Jillian Gerten, Selina
Chu, and Kyle White. 2010. Ada and Grace: Toward
realistic and engaging virtual museum guides. In Proc.
of the International Conference on Intelligent Virtual
Agents (IVA), pages 286?300, Philadelphia, PA, USA.
Joel R. Tetreault and Diane J. Litman. 2008. A reinforce-
ment learning approach to evaluating state representa-
tions in spoken dialogue systems. Speech Communi-
cation, 50(8-9):683?696.
Blaise Thomson and Steve Young. 2010. Bayesian up-
date of dialogue state: A POMDP framework for spo-
ken dialogue systems. Computer Speech and Lan-
guage, 24(4):562?588.
Sebastian Varges, Fuliang Weng, and Heather Pon-Barry.
2009. Interactive question answering and constraint
relexation in spoken dialogue systems. Natural Lan-
guage Engineering, 15(1):9?30.
EllenM. Voorhees. 2001. The TREC question answering
track. Natural Language Engineering, 7(4):361?378.
92
Appendix
Features Features
average ASR accuracy of user queries if system correctly answered current user query
# user queries if system responded with off-topic prompt
to current user query
# correct system responses # times user repeated current query
# incorrect system responses # successive incorrect system responses
# off-topic system prompts # successive off-topic system prompts
% correct system responses # user queries for topic ?introduction?
% incorrect system responses # user queries for topic ?personal?
user type (child, male, female) # user queries for topic ?school?
if user asks example query 1 # user queries for topic ?technology?
if user asks example query 2 # user queries for topic ?interfaces?
if user asks example query 3 # user queries for topic ?exhibition?
if user asks example query 4 # user queries for other topics
if system correctly responds to example query 1 if system correctly responds to example query 3
if system correctly responds to example query 2 if system correctly responds to example query 4
# junk user queries previous topic category
Table 7: List of features used in predicting when the user will cease a session (Cease Dialogue), what the user will say
next (Say Next 1), and what the user will say next after removing repeated user queries (Say Next 2). Example query
1 is ?who are you named after??; example query 2 is ?are you a computer??; example query 3 is ?what do you like to
do for fun??; example query 4 is ?what is artificial intelligence??.
Cease Dialogue Say Next 1 Say Next 2
average ASR accuracy of previous topic category previous topic category
user queries
user type (child, male, female) # user queries for topic ?personal? # junk user queries
# off-topic system prompts # user queries # successive incorrect system
responses
# successive off-topic system # junk user queries if system correctly answered
prompts current user query
# incorrect system responses % correct system responses user type (child, male, female)
# user queries % incorrect system responses % incorrect system responses
# junk user queries # incorrect system responses % correct system responses
# user queries for other topics # user queries for other topics # incorrect system responses
if system responded with off-topic # correct system responses # off-topic system prompts
prompt to current user query
% correct system responses user type (child, male, female) # user queries
Table 8: List of the 10 most dominant features (in order of importance) in predicting when the user will cease a session
(Cease Dialogue), what the user will say next (Say Next 1), and what the user will say next after removing repeated
user queries (Say Next 2).
93
Proceedings of the SIGDIAL 2013 Conference, pages 112?116,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Reinforcement Learning of Two-Issue Negotiation Dialogue Policies
Kallirroi Georgila
Institute for Creative Technologies, University of Southern California
12015 Waterfront Drive, Playa Vista, CA 90094, USA
kgeorgila@ict.usc.edu
Abstract
We use hand-crafted simulated negotiators
(SNs) to train and evaluate dialogue poli-
cies for two-issue negotiation between two
agents. These SNs differ in their goals and
in the use of strong and weak arguments
to persuade their counterparts. They may
also make irrational moves, i.e., moves not
consistent with their goals, to generate a
variety of negotiation patterns. Different
versions of these SNs interact with each
other to generate corpora for Reinforce-
ment Learning (RL) of argumentation di-
alogue policies for each of the two agents.
We evaluate the learned policies against
hand-crafted SNs similar to the ones used
for training but with the modification that
these SNs no longer make irrational moves
and thus are harder to beat. The learned
policies generally do as well as, or bet-
ter than the hand-crafted SNs showing that
RL can be successfully used for learning
argumentation dialogue policies in two-
issue negotiation scenarios.
1 Introduction
The dialogue policy of a dialogue system decides
on what dialogue move (also called action) the
system should make given the dialogue context
(also called dialogue state). Building hand-crafted
policies is a hard task, and there is no guarantee
that the resulting policies will be optimal. This is-
sue has motivated the dialogue community to use
statistical methods for automatically learning dia-
logue policies, the most popular of which is Rein-
forcement Learning (RL) (Szepesva?ri, 2010).
To date, RL has been used mainly for learn-
ing dialogue policies for slot-filling applications
such as restaurant recommendations (Williams
and Young, 2007; Chandramohan et al, 2010;
Jurc???c?ek et al, 2012; Gas?ic? et al, 2012), flight
reservations (Henderson et al, 2008), sightsee-
ing recommendations (Misu et al, 2010), appoint-
ment scheduling (Georgila et al, 2010), techni-
cal support (Janarthanam and Lemon, 2010), etc.,
largely ignoring other types of dialogue. RL has
also been applied to question-answering (Misu et
al., 2012) and tutoring domains (Tetreault and Lit-
man, 2008; Chi et al, 2011). There has also been
some work on applying RL to the more difficult
problem of learning negotiation policies (Heeman,
2009; Paruchuri et al, 2009; Georgila and Traum,
2011a; Georgila and Traum, 2011b; Nouri et al,
2012), which is the topic of this paper.
In negotiation dialogue the system and the user
have opinions about the optimal outcomes and try
to reach a joint decision. Dialogue policy deci-
sions are typically whether to present, accept, or
reject a proposal, whether to compromise, etc. Re-
wards may depend on the type of policy that we
want to learn. For example, a cooperative policy
should be rewarded for accepting proposals.
Recently, Georgila and Traum (2011a; 2011b)
learned argumentation dialogue policies for nego-
tiation against users of different cultural norms in
a one-issue negotiation scenario. We extend this
work by learning argumentation policies in a two-
issue negotiation setting. We aim to learn system
(or agent) policies that will persuade their inter-
locutor (a human user or another agent) to agree
on the system?s preferences.
Our research contribution is two-fold: First, to
our knowledge this is the first study that uses RL
for learning argumentation policies in a two-issue
negotiation scenario and one of the few studies on
using RL for negotiation. Second, for the first
time, we learn policies for agents with different
degrees of persuasion skills, i.e., agents that pro-
vide strong or weak arguments.
Section 2 introduces RL, and section 3 de-
scribes our two-issue negotiation domain and our
learning methodology. Section 4 presents our
evaluation results and section 5 concludes.
112
2 Reinforcement Learning
Reinforcement Learning (RL) is a machine learn-
ing technique used to learn the policy of an
agent (Szepesva?ri, 2010). RL is used in
the framework of Markov Decision Processes
(MDPs) (Szepesva?ri, 2010) or Partially Observ-
able Markov Decision Processes (Williams and
Young, 2007). In this paper we use MDPs.
An MDP is defined as a tuple (S, A, P , R, ?)
where S is the set of states that the agent may be
in, A is the set of actions of the agent, P : S ? A
? P (S, A) is the set of transition probabilities be-
tween states after taking an action, R : S ? A ?
< is the reward function, and ? ? [0, 1] a discount
factor weighting long-term rewards. At any given
time step i the agent is in a state si ? S. When the
agent performs an action ?i ? A following a pol-
icy pi : S ? A, it receives a reward ri(si, ?i) ? <
and transitions to state s?i according to P (s?i|si, ?i)
? P . The quality of the policy pi followed by the
agent is measured by the expected future reward
also called Q-function, Qpi : S ? A?<.
To estimate the Q-function we use Least-
Squares Policy Iteration (LSPI) (Lagoudakis and
Parr, 2003; Li et al, 2009). LSPI can learn directly
from a corpus of dialogues and is sample efficient.
We use linear function approximation of the Q-
function. Thus Q(s, ?) = ?ki=1wi?i(s, ?) where
s is the state that the agent is in and ? the action
that it performs in this state, and w? is a vector of
weights wi for the feature functions ?i(s, ?). The
magnitude of a weight wi shows the contribution
of the feature ?i(s, ?) to the Q(s, ?) value.
3 Learning Argumentation Policies
In our experiments, two agents negotiate on two is-
sues that are independent of each other. Each issue
may have three possible outcomes. Our approach
can be applied to any such issues. For the sake
of readability, from now on we will use a negoti-
ation scenario in which Agents 1 and 2 are hav-
ing a party and need to agree on the type of food
that will be served (Thai, Italian, Mexican) and the
day of the week that the party will be held (Friday,
Saturday, Sunday). Agents 1 and 2 have different
goals. Table 1 shows the points that Agents 1 and 2
earn for each negotiation outcome.
We build hand-crafted simulated negotiators
(SNs) for the two agents that interact with each
other to generate simulated corpora. The SNs dif-
fer not only in their goals but also in whether
they use strong or weak arguments to persuade
Agent 1 Agent 2
Food type
Thai 200 0
Italian 100 40
Mexican 0 80
Day of the week
Friday 80 0
Saturday 40 100
Sunday 0 200
Table 1: Rewards for Agents 1 and 2.
their counterparts, and sometimes make irrational
moves, i.e., moves not consistent with their goals.
For example, Agent 1 may reject an offer for
?Thai? food, and Agent 2 may offer or accept ?Fri-
day?. This is to generate a variety of negotiation
patterns. There is also some randomness regard-
ing whether the SN will start the conversation by
a direct offer or by providing an argument.
The SNs for Agents 1 and 2 can choose
among 13 actions: ?offer-Thai?, ?offer-
Italian?, ?offer-Mexican?, ?offer-Friday?,
?offer-Saturday?, ?offer-Sunday?, ?provide-
argument-Thai?, ?provide-argument-Mexican?,
?provide-argument-Friday?, ?provide-argument-
Sunday?, ?accept?, ?reject?, ?release-turn?. In our
setup Agents 1 and 2 do not provide arguments for
?Italian? or ?Saturday? since these are acceptable
options for both agents. Because Agent 1 cares
more about the food type and Agent 2 cares more
about the day there is potential for trade-offs,
i.e., ?I?ll give you the food type that you want if
you agree on the day that I want?. So we have
one more action ?trade-off? which is basically a
combined action ?offer-Thai, offer-Sunday?. The
two agents have to agree on both issues for the
dialogue to end. If there is no agreement in 40
turns then the dialogue stops.
Note that for testing our learned policies (see
section 4) we use a rationalized version of these
SNs. For example, Agent 1 never offers ?Sunday?
and never accepts ?Mexican?. We will refer to the
SNs that exhibit some degree of randomness and
irrationality as ?semi-rational? and the SNs that al-
ways behave rationally as ?rational?.
For training, 4 corpora are generated (50,000 di-
alogues each) using different SNs, each of which
is limited to using either strong or weak argu-
ments: SN for Agent 1 with strong arguments vs.
SN for Agent 2 with strong arguments, SN for
Agent 1 with strong arguments vs. SN for Agent 2
113
with weak arguments, SN for Agent 1 with weak
arguments vs. SN for Agent 2 with strong argu-
ments, and SN for Agent 1 with weak arguments
vs. SN for Agent 2 with weak arguments.
We use LSPI to learn policies directly from the
4 corpora. Each agent is rewarded only at the end
of the dialogue based on the agreement. So if the
outcome is ?Thai? and ?Saturday? Agent 1 will
earn 240 points and Agent 2 100 points. We set a
small reward +1 point for each policy action taken.
Table 2 shows our state representation.
The first 10 state variables are self-explanatory.
Below we explain how the ?counter? variables
work. Initially the counter for ?Thai? arguments
is set to 0 and Agent 2 supports food type ?Mexi-
can?. Every time the policy of Agent 1 provides
an argument in favor of ?Thai?, the counter for
?Thai? arguments is increased by 1 and the counter
for ?Mexican? arguments is decreased by 1 (like
a penalty). Every time the policy of Agent 1 ar-
gues in favor of ?Mexican? the counter for ?Thai?
arguments is decreased by 1 and the counter for
?Mexican? arguments is increased by 1. When
the counter for ?Thai? arguments becomes 3,
then the state variable ?Thai-argument-counter-
reached-threshold? becomes ?yes? and Agent 2 is
ready to yield to the demands of Agent 1. This
threshold of 3 was set empirically after experimen-
tation. Likewise for the rest of the ?counter? vari-
ables. We also account for both strong and weak
arguments. When the arguments of an agent are
weak, even if the corresponding counters exceed
the predefined threshold and the associated state
variables change from ?no? to ?yes?, the behav-
ior of their interlocutor will not change. This is
to simulate the fact that weak arguments cannot be
persuasive. The release action counter works simi-
larly. Initially it is 0 but after 4 consecutive actions
of the same speaker it is set to 1 to ensure that the
turns are not very long.
There are 786,432 possible states and
11,010,048 possible Q-values (state-action
pairs). We use linear function approximation
with 1,680 manually selected features. The
rationale for selecting these features is as follows:
We associate the action ?offer-Thai? with the
state variables ?current-day-accepted?, ?Thai-
rejected?, ?Italian-rejected?, ?Mexican-rejected?,
?Thai-argument-counter-reached-threshold?, and
?Mexican-argument-counter-reached-threshold?.
Thus we assume that the values of the other state
variables are irrelevant. This is an approximation
(to keep the number of features manageable) that
Current offer on the table (null/Thai/Italian/
Mexican/Friday/Saturday/Sunday/trade-off)
By whom is the current offer on the table
(null/Agent1/Agent2)
Currently accepted food type
(null/Thai/Italian/Mexican)
Currently accepted day (null/Friday/
Saturday/Sunday)
Has food type Thai been rejected? (no/yes)
Has food type Italian been rejected? (no/yes)
Has food type Mexican been rejected? (no/yes)
Has day Friday been rejected? (no/yes)
Has day Saturday been rejected? (no/yes)
Has day Sunday been rejected? (no/yes)
Has counter for food type Thai arguments
reached threshold? (no/yes)
Has counter for food type Mexican arguments
reached threshold? (no/yes)
Has counter for day Friday arguments
reached threshold? (no/yes)
Has counter for day Sunday arguments
reached threshold? (no/yes)
Has release action counter reached
threshold (no/yes)
Table 2: State variables that we keep track of and
all the possible values they can take.
has drawbacks, e.g., we may have an ?offer-Thai?
action even though the food type agreed so far is
?Thai? (because there is no feature to associate the
currently accepted food type value with a ?Thai?
offer). With this configuration we end up having
4 ? 25 = 128 binary features just for the action
?offer-Thai?. Similarly, features are selected for
the rest of the actions.
We partition each one of our 4 simulated cor-
pora into 5 subsets of 10,000 dialogues each. Each
partition is processed independently and will be
referred to as trial. We train policies for each
trial of each corpus type (20 policies for each
agent). Thus we end up with the following 4
types of policies for Agent 1 (and likewise for the
policies of Agent 2): Agent 1 with strong argu-
ments trained against Agent 2 with strong argu-
ments (Agent 1 S(S)); Agent 1 with strong argu-
ments trained against Agent 2 with weak argu-
ments (Agent 1 S(W)); Agent 1 with weak argu-
ments trained against Agent 2 with strong argu-
ments (Agent 1 W(S)); and Agent 1 with weak
arguments trained against Agent 2 with weak ar-
guments (Agent 1 W(W)).
114
Policy Opponent Policy Opponent Policy Opponent
Score Score #Actions #Actions #Turns #Turns
Agent 1 S(S) vs. Agent 2 S 214.3 164.3 7.6 6.2 2.0 1.6
Agent 1 S(S) vs. Agent 2 W 214.1 164.5 7.4 6.1 2.0 1.6
Agent 1 S(W) vs. Agent 2 S 213.9 165.1 7.6 6.2 2.0 1.6
Agent 1 S(W) vs. Agent 2 W 214.1 164.7 7.4 6.1 2.0 1.6
Agent 1 W(S) vs. Agent 2 S 192.4 196.5 9.1 8.5 2.5 2.4
Agent 1 W(S) vs. Agent 2 W 197.9 198.9 7.6 7.0 2.1 1.9
Agent 1 W(W) vs. Agent 2 S 195.0 197.9 8.8 8.5 2.5 2.4
Agent 1 W(W) vs. Agent 2 W 198.1 199.0 7.7 7.0 2.2 2.0
Table 3: Results of different training and testing combinations for learned policies of Agent 1 and rational
SNs for Agent 2.
4 Evaluation
Each policy of Agent 1 resulting from a trial
is evaluated against two hand-crafted SNs for
Agent 2, one where Agent 2 provides strong ar-
guments (Agent 2 S) and one where Agent 2 pro-
vides weak arguments (Agent 2 W). So for the
condition ?Agent 1 with strong arguments trained
against Agent 2 with strong arguments (Agent 1
S(S))? we have 5 policies, each of which interacts
with ?Agent 2 S? (or ?Agent 2 W?). We calcu-
late the averages of the earned points for each of
the agents, of the number of actions per dialogue
of each agent, and of the number of turns per di-
alogue of each agent, over 10,000 dialogues per
policy. Likewise for the policies of Agent 2. Note
that the SNs used in the evaluation do not behave
irrationally like the ones used for training, and thus
are harder to beat.
In Table 3 we can see the results for the policy
of Agent 1. Results for the policy of Agent 2 are
similar given that the goals of Agent 2 mirror the
goals of Agent 1. As we can see, the policy of
Agent 1 with strong arguments learned to provide
the appropriate arguments and make Agent 2 agree
on ?Thai? and ?Friday? or ?Saturday?. When the
policy of Agent 1 provides only weak arguments it
cannot get day ?Friday? but it can secure a trade-
off. This is because both the learned policies and
the SNs usually accept trade-off offers (due to the
way the hand-crafted SNs were constructed). We
also performed tests with SNs that did not propose
or accept as many trade-offs. This arrangement fa-
vored the policy of Agent 1 with strong arguments,
and hurt the performance of the policy of Agent 1
with weak arguments playing against Agent 2 with
strong arguments. This shows that trade-offs help
the weaker negotiators.
Furthermore, we experimented with testing on
semi-rational SNs similar to the ones used for
training and the results were better for the policy
of Agent 1 with weak arguments and worse for the
policy of Agent 1 with strong arguments. So like
trade-offs a semi-rational SN favors the weaker
negotiators.
5 Conclusion
We learned argumentation dialogue policies for
two-issue negotiation, using simulated corpora
generated from the interaction of two hand-crafted
SNs that differed in their goals and in the use of
strong and weak arguments to persuade their coun-
terparts. These SNs sometimes made random or
irrational moves to generate a variety of negotia-
tion patterns.
We used these simulated corpora and RL to
learn argumentation dialogue policies for each of
the two agents. Each of the learned policies was
evaluated against hand-crafted SNs similar to the
ones used for training but with the modification
that these SNs no longer made irrational moves
and thus were harder to beat. The policies gener-
ally did as well as, or better than the hand-crafted
SNs showing that RL can be successfully used for
learning argumentation dialogue policies in two-
issue negotiation scenarios.
For future work we would like to use automatic
feature selection (Li et al, 2009; Misu and Kash-
ioka, 2012) and learn policies for more than two
issues and more than three outcomes per issue.
Selecting features manually is a difficult process
that requires a lot of experimentation and trial-
and-error.
Acknowledgments
This work was funded by the NSF grant
#1117313.
115
References
Senthilkumar Chandramohan, Matthieu Geist, and
Olivier Pietquin. 2010. Sparse approximate dy-
namic programming for dialog management. In
Proc. of the Annual SIGdial Meeting on Discourse
and Dialogue, pages 107?115, Tokyo, Japan.
Min Chi, Kurt VanLehn, Diane Litman, and Pamela
Jordan. 2011. Empirically evaluating the ap-
plication of reinforcement learning to the induc-
tion of effective and adaptive pedagogical strategies.
User Modeling and User-Adapted Interaction, 21(1-
2):137?180.
Milica Gas?ic?, Matthew Henderson, Blaise Thomson,
Pirros Tsiakoulis, and Steve Young. 2012. Pol-
icy optimisation of POMDP-based dialogue systems
without state space compression. In Proc. of the
IEEE Workshop on Spoken Language Technology
(SLT), pages 31?36, Miami, FL, USA.
Kallirroi Georgila and David Traum. 2011a. Learning
culture-specific dialogue models from non culture-
specific data. In Proc. of HCI International, Lecture
Notes in Computer Science Vol. 6766, pages 440?
449, Orlando, FL, USA.
Kallirroi Georgila and David Traum. 2011b. Rein-
forcement learning of argumentation dialogue poli-
cies in negotiation. In Proc. of Interspeech, pages
2073?2076, Florence, Italy.
Kallirroi Georgila, Maria K. Wolters, and Johanna D.
Moore. 2010. Learning dialogue strategies from
older and younger simulated users. In Proc. of
the Annual SIGdial Meeting on Discourse and Di-
alogue, pages 103?106, Tokyo, Japan.
Peter A. Heeman. 2009. Representing the rein-
forcement learning state in a negotiation dialogue.
In Proc. of the IEEE Automatic Speech Recogni-
tion and Understanding Workshop (ASRU), Merano,
Italy.
James Henderson, Oliver Lemon, and Kallirroi
Georgila. 2008. Hybrid reinforcement/supervised
learning of dialogue policies from fixed datasets.
Computational Linguistics, 34(4):487?511.
S. Janarthanam and O. Lemon. 2010. Adaptive refer-
ring expression generation in spoken dialogue sys-
tems: Evaluation with real users. In Proc. of the An-
nual SIGdial Meeting on Discourse and Dialogue,
pages 124?131, Tokyo, Japan.
Filip Jurc???c?ek, Blaise Thomson, and Steve Young.
2012. Reinforcement learning for parameter esti-
mation in statistical spoken dialogue systems. Com-
puter Speech and Language, 26(3):168?192.
Michail G. Lagoudakis and Ronald Parr. 2003. Least-
squares policy iteration. Journal of Machine Learn-
ing Research, 4:1107?1149.
Lihong Li, Jason D. Williams, and Suhrid Balakrish-
nan. 2009. Reinforcement learning for dialog man-
agement using least-squares policy iteration and fast
feature selection. In Proc. of Interspeech, pages
2475?2478, Brighton, United Kingdom.
Teruhisa Misu and Hideki Kashioka. 2012. Simul-
taneous feature selection and parameter optimiza-
tion for training of dialogue policy by reinforcement
learning. In Proc. of the IEEE Workshop on Spoken
Language Technology (SLT), pages 1?6, Miami, FL,
USA.
Teruhisa Misu, Komei Sugiura, Kiyonori Ohtake,
Chiori Hori, Hideki Kashioka, Hisashi Kawai, and
Satoshi Nakamura. 2010. Modeling spoken de-
cision making dialogue and optimization of its di-
alogue strategy. In Proc. of the Annual SIGdial
Meeting on Discourse and Dialogue, pages 221?
224, Tokyo, Japan.
Teruhisa Misu, Kallirroi Georgila, Anton Leuski, and
David Traum. 2012. Reinforcement learning of
question-answering dialogue policies for virtual mu-
seum guides. In Proc. of the Annual SIGdial Meet-
ing on Discourse and Dialogue, pages 84?93, Seoul,
South Korea.
Elnaz Nouri, Kallirroi Georgila, and David Traum.
2012. A cultural decision-making model for nego-
tiation based on inverse reinforcement learning. In
Proc. of the Annual Meeting of the Cognitive Sci-
ence Society (CogSci), pages 2097?2102, Sapporo,
Japan.
P. Paruchuri, N. Chakraborty, R. Zivan, K. Sycara,
M. Dudik, and G. Gordon. 2009. POMDP based
negotiation modeling. In Proc. of the IJCAI Work-
shop on Modeling Intercultural Collaboration and
Negotiation (MICON).
Csaba Szepesva?ri. 2010. Algorithms for Reinforce-
ment Learning. Morgan & Claypool Publishers.
Joel R. Tetreault and Diane J. Litman. 2008. A rein-
forcement learning approach to evaluating state rep-
resentations in spoken dialogue systems. Speech
Communication, 50(8-9):683?696.
Jason D. Williams and Steve Young. 2007. Scal-
ing POMDPs for spoken dialog management. IEEE
Trans. on Audio, Speech, and Language Processing,
15(7):2116?2129.
116
Proceedings of the SIGDIAL 2013 Conference, pages 193?202,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Verbal indicators of psychological distress in interactive dialogue with a
virtual human
David DeVault, Kallirroi Georgila, Ron Artstein, Fabrizio Morbini, David Traum,
Stefan Scherer, Albert (Skip) Rizzo, Louis-Philippe Morency
University of Southern California, Institute for Creative Technologies
Playa Vista, CA
devault@ict.usc.edu
Abstract
We explore the presence of indicators of
psychological distress in the linguistic be-
havior of subjects in a corpus of semi-
structured virtual human interviews. At
the level of aggregate dialogue-level fea-
tures, we identify several significant dif-
ferences between subjects with depres-
sion and PTSD when compared to non-
distressed subjects. At a more fine-grained
level, we show that significant differences
can also be found among features that
represent subject behavior during specific
moments in the dialogues. Finally, we
present statistical classification results that
suggest the potential for automatic assess-
ment of psychological distress in individ-
ual interactions with a virtual human dia-
logue system.
1 Introduction
One of the first steps toward dealing with psy-
chological disorders such as depression and PTSD
is diagnosing the problem. However, there is of-
ten a shortage of trained health care professionals,
or of access to those professionals, especially for
certain segments of the population such as mili-
tary personnel and veterans (Johnson et al, 2007).
One possible partial remedy is to use virtual hu-
man characters to do a preliminary triage screen-
ing, so that mental healthcare providers can focus
their attention on those who are most likely to need
help. The virtual human would engage an indi-
vidual in an interview and analyze some of their
behavioral characteristics. In addition to serving
a triage function, this automated interview could
produce valuable information to help the health-
care provider make their expert diagnosis.
In this paper, we investigate whether features
in the linguistic behavior of participants in a con-
versation with a virtual human could be used
for recognizing psychological distress. We focus
specifically on indicators of depression and post-
traumatic stress disorder (PTSD) in the verbal be-
havior of participants in a Wizard-of-Oz corpus.
The results and analysis presented here are part
of a broader effort to create an automated, interac-
tive virtual human dialogue system that can detect
indicators of psychological distress in the multi-
modal communicative behavior of its users. Re-
alizing this vision requires a careful and strate-
gic design of the virtual human?s dialogue behav-
ior, and in concert with the system?s behavior, the
identification of robust ?indicator? features in the
verbal and nonverbal responses of human intervie-
wees. These indicators should be specific behavior
patterns that are empirically correlated with spe-
cific psychological disorders, and that can inform
a triage screening process or facilitate the diagno-
sis or treatment performed by a clinician.
In this paper, we report on several kinds of such
indicators we have observed in a corpus of 43
Wizard-of-Oz interactions collected with our pro-
totype virtual human, Ellie, pictured in Figure 1.
We begin in Section 2 with a brief discussion of
background and related work on the communica-
tive behavior associated with psychological dis-
tress. In Section 3, we describe our Wizard-of-Oz
data set. Section 4 presents an analysis of indicator
features we have explored in this data set, identi-
fying several significant differences between sub-
jects with depression and PTSD when compared
to non-distressed subjects. In Section 5 we present
statistical classification results that suggest the po-
tential for automatic assessment of psychological
distress based on individual interactions with a vir-
tual human dialogue system. We conclude in Sec-
tion 6.
2 Background and Related Work
There has been a range of psychological and clin-
ical research that has identified differences in the
193
Figure 1: Ellie.
communicative behavior of patients with specific
psychological disorders such as depression. In this
section, we briefly summarize some closely re-
lated work.
Most work has observed the behavior of patients
in human-human interactions, such as clinical in-
terviews and doctor-patient interactions. PTSD is
generally less well studied than depression.
Examples of the kinds of differences that have
been observed in non-verbal behavior include dif-
ferences in rates of mutual gaze and other gaze
patterns, downward angling of the head, mouth
movements, frowns, amount of gesturing, fidget-
ing, emotional expressivity, and voice quality; see
Scherer et al (2013) for a recent review.
In terms of verbal behavior, our exploration of
features here is guided by several previous obser-
vations in the literature. Cohn and colleagues have
identified increased speaker-switch durations and
decreased variability of vocal fundamental fre-
quency as indicators of depression, and have ex-
plored the use of these features for classification
(Cohn et al, 2009). That work studied these fea-
tures in human-human clinical interviews, rather
than in virtual human interactions as reported here.
In clinical studies, acute depression has been as-
sociated with decreased speech, slow speech, de-
lays in delivery, and long silent pauses (Hall et al,
1995). Aggregate differences in lexical frequen-
cies have also been observed. For example, in
written essays, Rude et al (2004) observed that
depressed participants used more negatively va-
lenced words and used the first-person pronoun ?I?
more frequently than never-depressed individuals.
Heeman et al (2010) observed differences in chil-
dren with autism in how long they pause before
speaking and in their use of fillers, acknowledg-
ments, and discourse markers. Some of these fea-
tures are similar to those studied here, but looked
at children communicating with clinicians rather
than a virtual human dialogue system.
Recent work on machine classification has
demonstrated the ability to discriminate between
schizophrenic patients and healthy controls based
on transcriptions of spoken narratives (Hong et al,
2012), and to predict patient adherence to med-
ical treatment from word-level features of dia-
logue transcripts (Howes et al, 2012). Automatic
speech recognition and word alignment has also
been shown to give good results in scoring narra-
tive recall tests for identification of cognitive im-
pairment (Prud?hommeaux and Roark, 2011; Lehr
et al, 2012).
3 Data Set
In this section, we introduce the Wizard-of-Oz
data set that forms the basis for this paper. In
this virtual human dialogue system, the charac-
ter Ellie depicted in Figure 1 carries out a semi-
structured interview with a single user. The sys-
tem was designed after a careful analysis of a
set of face-to-face interviews in the same do-
main. The face-to-face interviews make up the
large human-human Distress Assessment Inter-
view Corpus (DAIC) that is described in Scherer
et al (2013). Drawing on observations of inter-
viewer behavior in the face-to-face dialogues, El-
lie was designed to serve as an interviewer who
is also a good listener, providing empathetic re-
sponses, backchannels, and continuation prompts
to elicit more extended replies to specific ques-
tions. The data set used in this paper is the re-
sult of a set of 43 Wizard-of-Oz interactions where
the virtual human interacts verbally and nonver-
bally in a semi-structured manner with a partici-
pant. Excerpts from the transcripts of two interac-
tions in this Wizard-of-Oz data set are provided in
the appendix in Figure 5.1
3.1 Procedure
The participants were recruited via Craigslist and
were recorded at the USC Institute for Creative
1A sample demonstration video of an interaction be-
tween the virtual agent and a human actor can be seen here:
http://www.youtube.com/watch?v=ejczMs6b1Q4
194
Technologies. In total 64 participants interacted
with the virtual human. All participants who met
requirements (i.e. age greater than 18, and ad-
equate eyesight) were accepted. In this paper,
we focus on a subset of 43 of these participants
who were told that they would be interacting with
an automated system. (The other participants,
which we exclude from our analysis, were aware
that they were interacting with a human-controlled
system.) The mean age of the 43 participants in
our data set was 36.6 years, with 23 males and 20
females.
We adhered to the following procedure for data
collection: After a short explanation of the study
and giving consent, participants completed a series
of questionnaires. These questionnaires included
the PTSD Checklist-Civilian version (PCL-C) and
the Patient Health Questionnaire, depression mod-
ule (PHQ-9) (Scherer et al, 2013) along with other
questions. Then participants engage in an inter-
view with the virtual human, Ellie. After the di-
alogue concludes, participants are then debriefed
(i.e. the wizard control is revealed), paid $25 to
$35, and escorted out.
The interaction between the participants and El-
lie was designed as follows: Ellie explains the pur-
pose of the interaction and that she will ask a series
of questions. She then tries to build rapport with
the participant in the beginning of the interaction
with a series of casual questions about Los Ange-
les. Then the main interview begins, including a
range of questions such as:
What would you say are some of your
best qualities?
What are some things that usually put
you in a good mood?
Do you have disturbing thoughts?
What are some things that make you re-
ally mad?
How old were you when you enlisted?
What did you study at school?
Ellie?s behavior was controlled by two human
?wizards? in a separate room, who used a graph-
ical user interface to select Ellie?s nonverbal be-
havior (e.g. head nods, smiles, back-channels)
and verbal utterances (including the interview
questions, verbal back-channels, and empathy re-
sponses). This Wizard-of-Oz setup allows us to
prove the utility of the protocol and collect training
data for the eventual fully automatic interaction.
The speech for each question was pre-recorded us-
ing an amateur voice actress (who was also one of
the wizards). The virtual human?s performance of
these utterances is animated using the SmartBody
animation system (Thiebaux et al, 2008).
3.2 Condition Assessment
The PHQ-9 and PCL-C scales provide researchers
with guidelines on how to assess the participants?
conditions based on the responses. Among the
43 participants, 13 scored above 10 on the PHQ-
9, which corresponds to moderate depression and
above (Kroenke et al, 2001). We consider these
13 participants as positive for depression in this
study. 20 participants scored positive for PTSD,
following the PCL-C classification. The two pos-
itive conditions overlap strongly, as the evalu-
ated measurements PHQ-9 and PCL-C correlate
strongly (Pearson?s r > 0.8, as reported in Scherer
et al (2013)).
4 Feature Analysis
4.1 Transcription and timing of speech
We have a set D = {d1, ..., d43} of 43 dialogues.
The user utterances in each dialogue were tran-
scribed using ELAN (Wittenburg et al, 2006),
with start and end timestamps for each utterance.2
At each pause of 300ms or longer in the user?s
speech, a new transcription segment was started.
The resulting speech segments were subsequently
reviewed and corrected for accuracy.
For each dialogue di ? D, this process resulted
in a sequence of user speech segments. We repre-
sent each segment as a tuple ?s, e, t?, where s and e
are the starting and ending timestamps in seconds,
and t is the manual text transcription of the corre-
sponding audio segment. The system speech seg-
ments, including their starting and ending times-
tamps and verbatim transcripts of system utter-
ances, were recovered from the system log files.
To explore aggregate statistical features based
on user turn-taking behavior in the dialogues, we
employ a simple approach to identifying turns
within the dialogues. First, all user and system
speech segments are sorted in increasing order of
2ELAN is a tool that supports annotation of
video and audio, from the Max Planck Insti-
tute for Psycholinguistics, The Language Archive,
Nijmegen, The Netherlands. It is available at
http://tla.mpi.nl/tools/tla-tools/elan/.
195
Segment level features
(a) mean speaking rate of each user segment
(b) mean onset time of first segment in each user turn
(c) mean onset time of non-first segments in user turns
(d) mean length of user segments
(e) mean minimum valence in user segments
(f) mean mean valence in user segments
(g) mean maximum valence in user segments
(h) mean number of filled pauses in user segments
(i) mean filled pause rate in user segments
Dialogue level features
(j) total number of user segments
(k) total length of all user segments
Figure 2: List of context-independent features.
their starting timestamps. All consecutive seg-
ments with the same speaker are then designated
as constituting a single turn. While this simple
scheme does not provide a detailed treatment of
relevant phenomena such as overlapping speech,
backchannels, and the interactive process of ne-
gotiating the turn in dialogue (Yang and Heeman,
2010), it provides a conceptually simple model for
the definition of features for aggregate statistical
analysis.
4.2 Context-independent feature analysis
We begin by analyzing a set of shallow features
which we describe as context-independent, as they
apply to user speech segments independently of
what the system has recently said. Most of these
are features that apply to many or all user speech
segments. We describe our context-independent
features in Section 4.2.1, and present our results
for these features in Section 4.2.2.
4.2.1 Context-independent features
We summarize our context-independent features
in Figure 2.
Speaking rate and onset times Based on previ-
ous clinical observations related to slowed speech
and increased onset time for depressed individuals
(Section 2), we defined features for speaking rate
and onset time of user speech segments.
We quantify the speaking rate of a user speech
segment ?s, e, t?, where t = ?w1, ..., wN ?, as
N/(e ? s). Feature (a) is the mean value of
this feature across all user speech segments within
each dialogue.
Onset time is calculated using the notion of user
turns. For each user turn, we extracted the first
user speech segment in the turn fu = ?su, eu, tu?,
and the most recent system speech segment ls =
?ss, es, ts?. We define the onset time of such a first
user segment as su ? es, and for each dialogue,
feature (b) is the intra-dialogue mean of these on-
set times.
In order to also quantify pause length between
user speech segments within a turn, we define fea-
ture (c), a similar feature that measures the mean
onset time between non-first user speech segments
within a user turn in relation to the preceding user
speech segment.
Length of user segments As one way to quan-
tify the amount of speech, feature (d) reports the
mean length of all user speech segments within a
dialogue (measured in words).
Valence features for user speech Features (e)-
(g) are meant to explore the idea that distressed
users might use more negative or less positive vo-
cabulary than non-distressed subjects. As an ex-
ploratory approach to this topic, we used Senti-
WordNet 3.0 (Baccianella and Sebastiani, 2010),
a lexical sentiment dictionary, to assign valence
to individual words spoken by users in our study.
The dictionary contains approximately 117,000
entries. In general, each word w may appear in
multiple entries, corresponding to different parts
of speech and word senses. To assign a single va-
lence score v(w) to each word in the dictionary, in
our features we compute the average score across
all parts of speech and word senses:
v(w) =
?
e?E(w) PosScoree(w)?NegScoree(w)
|E(w)|
where E(w) is the set of entries for the word w,
PosScoree(w) is the positive score for w in entry
e, and NegScoree(w) is the negative score for w
in entry e. This is similar to the ?averaging across
senses? method described in Taboada et al (2011).
We use several different measures of the va-
lence of each speech segment with transcript t =
?w1, ..., wn?. We compute the min, mean, and max
valence of each transcript:
minimum valence of t = minwi?t v(wi)
mean valence of t = 1n
?
wi?t v(wi)
maximum valence of t = maxwi?t v(wi)
Features (e)-(f) then are intra-dialogue mean
196
values for these three segment-level valence mea-
sures.
Filled pauses Another feature that we explored
is the presence of filled pauses in user speech seg-
ments. To do so, we counted the number of times
any of the tokens uh, um, uhh, umm, mm, or mmm
appeared in each speech segment. For each dia-
logue, feature (h) is the mean number of these to-
kens per user speech segment. In order to account
for the varying length of speech segments, we also
normalize the raw token counts in each segment
by dividing them by the length of the segment, to
produce a filled pause rate for the segment. Fea-
ture (i) is the mean value of the filled pause rate
for all speech segments in the dialogue.
Dialogue level features We also included two
dialogue level measures of how ?talkative? the
user is. Feature (j) is the total number of user
speech segments throughout the dialogue. Feature
(k) is the total length (in words) of all speech seg-
ments throughout the dialogue.
Standard deviation features For the classifica-
tion experiments reported in Section 5, we also in-
cluded a standard deviation variant of each of the
features (a)-(i) in Figure 2. These variants are de-
fined as the intra-dialogue standard deviation of
the underlying value, rather than the mean. We
discuss examples of standard deviation features
further in Section 5.
4.2.2 Results for context-independent
features
We summarize the observed significant effects in
our Wizard-of-Oz corpus in Table 1.
Onset time We report our findings for individu-
als with and without depression and PTSD for fea-
ture (b) in Table 1 and in Figure 3. The units are
seconds. While an increase in onset time for in-
dividuals with depression has previously been ob-
served in human-human interaction (Cohn et al,
2009; Hall et al, 1995), here we show that this
effect transfers to interactions between individuals
with depression and virtual humans. We find that
mean onset time is significantly increased for indi-
viduals with depression in their interactions with
our virtual human Ellie (p = 0.018, Wilcoxon
rank sum test).
Additionally, while to our knowledge onset time
for individuals with PTSD has not been reported,
we also found a significant increase in onset time
Me
an
on
se
td
ela
yo
ffi
rst
pa
rtic
ipa
nt
se
gm
en
t(s
ec
on
ds
)
0
1
2
3
4
No depr.
??
Depr.
?
Me
an
on
se
td
ela
yo
ffi
rst
pa
rtic
ipa
nt
se
gm
en
t(s
ec
on
ds
)
0
1
2
3
4
?PTSD
?
PTSD
?
Figure 3: Onset time.
for individuals with PTSD (p = 0.019, Wilcoxon
rank sum test).
Filled pauses We report our findings for individ-
uals with and without depression and PTSD under
feature (h) in Table 1 and in Figure 4. We observed
a significant reduction in this feature for both in-
dividuals with depression (p = 0.012, Wilcoxon
rank sum test) and PTSD (p = 0.014, Wilcoxon
rank sum test). We believe this may be related
to the trend we observed toward shorter speech
segments from distressed individuals (though this
trend did not reach significance). There is a pos-
itive correlation, ? = 0.55 (p = 0.0001), be-
tween mean segment length (d) and mean number
of filled pauses in segments (h).
Other features We did not observe significant
differences in the values of the other context-
independent features in Figure 2.
4.3 Context-dependent features
Our data set alows us to zoom in and look at
specific contextual moments in the dialogues, and
observe how users respond to specific Ellie ques-
tions. As an example, one of Ellie?s utterances,
which has system ID happy lasttime, is:
happy lasttime = Tell me about the last
time you felt really happy.
In our data set of 43 dialogues, this question was
asked in 42 dialogues, including 12 users positive
for depression and 19 users positive for PTSD.
197
Feature Depression (13 yes, 30 no) PTSD (20 yes, 23 no)
(b) mean onset time of first
segment in each user turn
?
Depr.: 1.72 (0.89)
No Depr.: 1.08 (0.56)
p = 0.018
?
PTSD.: 1.56 (0.80)
No PTSD.: 1.03 (0.57)
p = 0.019
(h) mean number of filled pauses
in user segments
?
Depr.: 0.32 (0.19)
No Depr.: 0.48 (0.23)
p = 0.012
?
PTSD: 0.36 (0.24)
No PTSD: 0.49 (0.21)
p = 0.014
Table 1: Results for context-independent features. For each feature and condition, we provide the mean
(standard deviation) for individuals with and without the condition. P-values for individual Wilcoxon
rank sum tests are provided. An up arrow (?) indicates a significant trend toward increased feature values
for positive individuals. A down arrow (?) indicates a significant trend toward decreased feature values
for positive individuals.
Me
an
fille
dp
au
se
si
np
art
icip
an
ts
eg
me
nt
(to
ke
ns
)
0
0.2
0.4
0.6
0.8
1.0
1.2
No depr.
?
Depr.
?
Me
an
fille
dp
au
se
si
np
art
icip
an
ts
eg
me
nt
(to
ke
ns
)
0
0.2
0.4
0.6
0.8
1.0
1.2
?PTSD PTSD
?
Figure 4: Number of filled pauses per speech seg-
ment.
This question is one of 95 topic setting utter-
ances in Ellie?s repertoire. (Ellie has additional
utterances that serve as continuation prompts,
backchannels, and empathy responses, which can
be used as a topic is discussed.)
To define context-dependent features, we asso-
ciate with each user segment the most recent of
Ellie?s topic setting utterances that has occurred in
the dialogue. We then focus our analysis on those
user segments and turns that follow specific topic
setting utterances. In Table 2, we present some ex-
amples of our findings for context-dependent fea-
tures for happy lasttime.3
3While we provide significance test results here at the p <
0.05 level, it should be noted that because of the large number
of context-dependent features that may be defined in a small
corpus such as ours, we report these merely as observations in
our corpus. We do not claim that these results transfer beyond
The contextual feature labeled (g?) in Table 2 is
the mean of the maximum valence feature across
all segments for which happy lasttime is the most
recent topic setting system utterance. We provide
a full example of this feature calculation in Fig-
ure 5 in the appendix.
As the figure shows, we find that users with
both PTSD and depression show a sharp reduc-
tion in the mean maximum valence in their speech
segments that respond to this question. This sug-
gests that in these virtual human interactions, this
question plays a useful role in eliciting differen-
tial responses from subjects with these psycholog-
ical disorders. We observed three additional ques-
tions which showed a significant difference in the
mean maximum valence feature. One example is
the question, How would your best friend describe
you?.
With feature (b?) in Table 2, we find an in-
creased onset time in responses to this question for
subjects with depression.4 Feature (d?) shows that
subjects with PTSD exhibit shorter speech seg-
ments in their responses to this question.
We observed a range of findings of this sort for
various combinations of Ellie?s topic setting utter-
ances and specific context-dependent features. In
future work, we would like to study the optimal
combinations of context-dependent questions that
yield the most information about the user?s distress
status.
this data set.
4In comparing Table 2 with Table 1, this question seems
to induce a higher mean onset time for distressed users than
the average system utterance does. This does not seem to be
the case for non-distressed users.
198
Feature Depression (12 yes, 30 no) PTSD (19 yes, 23 no)
(g?) mean maximum valence
in user segments following
happy lasttime
?
Depr.: 0.15 (0.07)
No Depr.: 0.26 (0.12)
p = 0.003
?
PTSD: 0.16 (0.08)
No PTSD: 0.28 (0.11)
p = 0.0003
(b?) mean onset time of first
segments in user turns
following happy lasttime
?
Depr.: 2.64 (2.70)
No Depr.: 0.94 (1.80)
p = 0.030
n.s.
PTSD: 2.18 (2.48)
No PTSD: 0.80 (1.76)
p = 0.077
(d?) mean length of user
segments following
happy lasttime
n.s.
Depr.: 5.95 (1.80)
No Depr.: 10.03 (6.99)
p = 0.077
?
PTSD: 6.82 (5.12)
No PTSD: 10.55 (6.68)
p = 0.012
Table 2: Example results for context-dependent features. For each feature and condition, we provide
the mean (standard deviation) for individuals with and without the condition. P-values for individual
Wilcoxon rank sum tests are provided. An up arrow (?) indicates a significant trend toward increased
feature values for positive individuals. A down arrow (?) indicates a significant trend toward decreased
feature values for positive individuals.
5 Classification Results
In this section, we present some suggestive clas-
sification results for our data set. We construct
three binary classifiers that use the kinds of fea-
tures described in Section 4 to predict the pres-
ence of three conditions: PTSD, depression, and
distress. For the third condition, we define dis-
tress to be present if and only if PTSD, depres-
sion, or both are present. Such a notion of distress
that collapses distinctions between disorders may
be the most appropriate type of classification for a
potential application in which distressed users of
any type are prioritized for access to health care
professionals (who will make a more informed as-
sessment of specific conditions).
For each individual dialogue, each of the three
classifiers emits a single binary label. We train
and evaluate the classifiers in a 10-fold cross-
validation using Weka (Hall et al, 2009).
While our data set of 43 dialogues is perhaps
of a typical size for a study of a research proto-
type dialogue system, it remains very small from
a machine learning perspective. We report here
two kinds of results that help provide perspective
on the prospects for classification of these condi-
tions. The first kind looks at classification based
on all the context-independent features described
in Section 4.2.1. The second looks at classifica-
tion based on individual features from this set.
In the first set of experiments, we trained a
Na??ve Bayes classifier for each condition using
all the context-independent features. We present
our results in Table 3, comparing each classifier to
a baseline that always predicts the majority class
(i.e. no condition for PTSD, no condition for de-
pression, and with condition for distress).
We note first that the trained classifiers all out-
perform the baseline in terms of weighted F-score,
weighted precision, weighted recall, and accuracy.
The accuracy improvement over baseline is sub-
stantial for PTSD (20.9% absolute improvement)
and distress (23.2% absolute improvement). The
accuracy improvement over baseline is more mod-
est for depression (2.3% absolute). We believe
one factor in the relative difficulty of classifying
depression more accurately is the relatively small
number of depressed participants in our study
(13).
While it has been shown in prior work (Cohn et
al., 2009) that depression can be classified above
baseline performance using features observed in
clinical human-human interactions, here we have
shown that classification above baseline perfor-
mance is possible in interactions between human
participants and a virtual human dialogue system.
Further, we have shown classification results for
PTSD and distress as well as depression.
We tried incorporating context-dependent fea-
tures, and also unigram features, but found that
neither improved performance. We believe our
data set is too small for effective training with
these very large extended feature sets.
199
Disorder Model Weighted F-score Weighted Precision Weighted Recall Accuracy
PTSD Na??ve Bayes 0.738 0.754 0.744 74.4%
Majority Baseline 0.373 0.286 0.535 53.5%
Depression Na??ve Bayes 0.721 0.721 0.721 72.1%
Majority Baseline 0.574 0.487 0.698 69.8%
Distress Na??ve Bayes 0.743 0.750 0.744 74.4%
Majority Baseline 0.347 0.262 0.512 51.2%
Table 3: Classification results.
In our second set of experiments, we sought to
gain understanding of which features were pro-
viding the greatest value to classification perfor-
mance. We therefore retrained Na??ve Bayes classi-
fiers using only one feature at a time. We summa-
rize here some of the highest performing features.
For depression, we found that the feature stan-
dard deviation in onset time of first segment in
each user turn yielded very strong performance
by itself. In our corpus, we observed that de-
pressed individuals show a greater standard devia-
tion in the onset time of their responses to Ellie?s
questions (p = 0.024, Wilcoxon rank sum test).
The value of this feature in classification comple-
ments the clinical finding that depressed individu-
als show greater onset times in their responses to
interview questions (Cohn et al, 2009).
For distress, we found that the feature mean
maximum valence in user segments was the most
valuable. We discussed findings for a context-
dependent version of this feature in Section 4.3.
This finding for distress can be related to previ-
ous observations that individuals with depression
use more negatively valenced words (Rude et al,
2004).
For PTSD, we found that the feature mean num-
ber of filled pauses in user segments was among
the most informative.
Based on our observation of classification per-
formance using individual features, we believe
there remains much room for improvement in fea-
ture selection and training. A larger data set would
enable feature selection approaches that use held
out data, and would likely result in both increased
performance and deeper insights into the most
valuable combination of features for classification.
6 Conclusion
In this paper, we have explored the presence of in-
dicators of psychological distress in the linguistic
behavior of subjects in a corpus of semi-structured
virtual human interviews. In our data set, we
have identified several significant differences be-
tween subjects with depression and PTSD when
compared to non-distressed subjects. Drawing on
these features, we have presented statistical classi-
fication results that suggest the potential for auto-
matic assessment of psychological distress in indi-
vidual interactions with a virtual human dialogue
system.
Acknowledgments
This work is supported by DARPA under con-
tract (W911NF-04-D-0005) and the U.S. Army
Research, Development, and Engineering Com-
mand. The content does not necessarily reflect the
position or the policy of the Government, and no
official endorsement should be inferred.
References
Andrea Esuli Stefano Baccianella and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).
Jeffery F. Cohn, Tomas Simon Kruez, Iain Matthews,
Ying Yang, Minh Hoai Nguyen, Margara Tejera
Padilla, Feng Zhou, and Fernando De la Torre.
2009. Detecting depression from facial actions and
vocal prosody. In Affective Computing and Intelli-
gent Interaction (ACII), September.
Judith A. Hall, Jinni A. Harrigan, and Robert Rosen-
thal. 1995. Nonverbal behavior in clinician-patient
interaction. Applied and Preventive Psychology,
4(1):21 ? 37.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
Peter A Heeman, Rebecca Lunsford, Ethan Selfridge,
Lois Black, and Jan Van Santen. 2010. Autism and
200
interactional aspects of dialogue. In Proceedings
of the 11th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, pages 249?252.
Association for Computational Linguistics.
Kai Hong, Christian G. Kohler, Mary E. March, Am-
ber A. Parker, and Ani Nenkova. 2012. Lexi-
cal differences in autobiographical narratives from
schizophrenic patients and healthy controls. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 37?
47, Jeju Island, Korea, July. Association for Compu-
tational Linguistics.
Christine Howes, Matthew Purver, Rose McCabe,
Patrick G. T. Healey, and Mary Lavelle. 2012.
Predicting adherence to treatment for schizophrenia
from dialogue transcripts. In Proceedings of the
13th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, pages 79?83, Seoul,
South Korea, July. Association for Computational
Linguistics.
Shannon J Johnson, Michelle D Sherman, Jeanne S
Hoffman, Larry C James, Patti L Johnson, John E
Lochman, Thomas N Magee, David Riggs, Jes-
sica Henderson Daniel, Ronald S Palomares, et al
2007. The psychological needs of US military ser-
vice members and their families: A preliminary re-
port. American Psychological Association Presi-
dential Task Force on Military Deployment Services
for Youth, Families and Service Members.
Kurt Kroenke, Robert L. Spitzer, and Janet B. W.
Williams. 2001. The phq-9. Journal of General
Internal Medicine, 16(9):606?613.
Maider Lehr, Emily Prud?hommeaux, Izhak Shafran,
and Brian Roark. 2012. Fully automated neuropsy-
chological assessment for detecting mild cognitive
impairment. In Interspeech 2012: 13th Annual Con-
ference of the International Speech Communication
Association, Portland, Oregon, September.
Emily Prud?hommeaux and Brian Roark. 2011. Ex-
traction of narrative recall patterns for neuropsycho-
logical assessment. In Interspeech 2011: 12th An-
nual Conference of the International Speech Com-
munication Association, pages 3021?3024, Flo-
rence, Italy, August.
Stephanie Rude, Eva-Maria Gortner, and James Pen-
nebaker. 2004. Language use of depressed and
depression-vulnerable college students. Cognition
& Emotion, 18(8):1121?1133.
S. Scherer, G. Stratou, M. Mahmoud, J. Boberg,
J. Gratch, A. Rizzo, and L.-P. Morency. 2013. Au-
tomatic behavior descriptors for psychological dis-
order analysis. In IEEE Conference on Automatic
Face and Gesture Recognition.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional Linguistics, 37(2):267?307, June.
Marcus Thiebaux, Stacy Marsella, Andrew N. Mar-
shall, and Marcelo Kallmann. 2008. Smartbody:
behavior realization for embodied conversational
agents. In Proceedings of the 7th international
joint conference on Autonomous agents and multi-
agent systems - Volume 1, AAMAS ?08, pages 151?
158, Richland, SC. International Foundation for Au-
tonomous Agents and Multiagent Systems.
Peter Wittenburg, Hennie Brugman, Albert Russel,
Alex Klassmann, and Han Sloetjes. 2006. Elan: a
professional framework for multimodality research.
In Proceedings of LREC 2006, Fifth International
Conference on Language Resources and Evaluation.
Fan Yang and Peter A. Heeman. 2010. Initiative con-
flicts in task-oriented dialogue. Computer Speech &
Language, 24(2):175 ? 189.
201
Appendix A. Wizard-of-Oz Dialogue Excerpts
Example user with PTSD and depression Example non-distressed user
max
valence
transcript
Ellie (happy lasttime) tell me
about the last time you
felt really happy
User 0.014 when i play with my dog
Ellie when was the last time
that happened?
User 0.000 this morning
Ellie aw
Ellie tell me more about that
User 0.312 i don?t know i just
User 0.010 whenever i get
User 0.312 super bored or i just
need some time to
User 0.312 for myself i just play
with the dog
Ellie that?s good
User 0.000 yeah
0.1371 = mean
maximum valence in
user segments following
happy lasttime
Ellie (ideal weekendC) tell
me how you spend your
ideal weekend
max
valence
transcript
Ellie (happy lasttime) tell me
about the last time you
felt really happy
User 0.562 um the last time i felt
really happy was
User 0.000 hm
User 0.000 today
Ellie tell me more about that
User 0.688 uh just from the moment
i woke up it was a
beautiful sunny day
User -0.062 i
User 0.565 went to see some friends
we had a good time
went to school
User 0.565 had some good grades
on some papers um
wrote a good essay
User 0.292 feel pretty accomplished
and
User 0.312 i feel like my day is just
User 0.565 a good day
Ellie that?s so good to hear
0.3487 = mean
maximum valence in
user segments following
happy lasttime
Ellie (BF describe) how
would your best friend
describe you?
Figure 5: Examples of maximum valence feature.
202
Proceedings of the SIGDIAL 2014 Conference, pages 254?256,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
A Demonstration of Dialogue Processing in SimSensei Kiosk
Fabrizio Morbini, David DeVault, Kallirroi Georgila,
Ron Artstein, David Traum, Louis-Philippe Morency
USC Institute for Creative Technologies
12015 Waterfront Dr., Playa Vista, CA 90094
{morbini,devault,kgeorgila,artstein,traum,morency}@ict.usc.edu
Abstract
This demonstration highlights the dia-
logue processing in SimSensei Kiosk, a
virtual human dialogue system that con-
ducts interviews related to psychologi-
cal distress conditions such as depression,
anxiety, and post-traumatic stress disorder
(PTSD). The dialogue processing in Sim-
Sensei Kiosk allows the system to con-
duct coherent spoken interviews of human
users that are 15-25 minutes in length,
and in which users feel comfortable talk-
ing and openly sharing information. We
present the design of the individual dia-
logue components, and show examples of
natural conversation flow between the sys-
tem and users, including expressions of
empathy, follow-up responses and contin-
uation prompts, and turn-taking.
1 Introduction
This demonstration highlights the dialogue pro-
cessing in SimSensei Kiosk, a virtual human di-
alogue system that conducts interviews related to
psychological distress conditions such as depres-
sion, anxiety, and post-traumatic stress disorder
(PTSD) (DeVault et al., 2014). SimSensei Kiosk
has two main functions ? a virtual human called
Ellie (pictured in Figure 1), who converses with a
user in a spoken, semi-structured interview, and a
multimodal perception system which analyzes the
user?s behavior in real time to identify indicators
of psychological distress.
The system has been designed and devel-
oped over two years using a series of face-to-
face, Wizard-of-Oz, and automated system stud-
ies involving more than 350 human participants
(Scherer et al., 2013; DeVault et al., 2013; DeVault
et al., 2014). Agent design has been guided by
two overarching goals: (1) the agent should make
Figure 1: Ellie, the virtual human interviewer in
SimSensei Kiosk.
the user feel comfortable talking and openly shar-
ing information, and at the same time (2) the agent
should create interactional situations that support
the automatic assessment of verbal and nonver-
bal behaviors correlated with psychological dis-
tress. During an interview, the agent presents a
set of questions which have been shown in user
testing to support these goals. Since the main in-
terview questions and their order are mostly fixed,
dialogue management concentrates on providing
appropriate verbal feedback behaviors to keep the
user engaged, maintain a natural and comfort-
able conversation flow, and elicit continuations
and elaborations from the user.
The agent is implemented using a modular ar-
chitecture (Hartholt et al., 2013). Dialogue pro-
cessing, which is the focus of this demonstration,
is supported by individual modules for speech
recognition, language understanding and dialogue
management (see Section 2). The agent?s lan-
guage and speech are executed by selecting from
pre-recorded audio clips. Additional agent mod-
ules include nonverbal behavior generation, which
matches appropriately timed body movements to
the agent?s speech; character animation in a vir-
tual 3D environment; and rendering in a game en-
254
gine. The perception system analyzes audio and
video in real time to identify features such as head
position, gaze direction, smile intensity, and voice
quality. DeVault et al. (2014) provides details on
all the agent?s modules.
2 Overview of Dialogue Processing
2.1 ASR and NLU components
Unlike many task-oriented dialogue domains, in-
terview dialogues between SimSensei Kiosk and
participants are naturally open-ended. People tend
to respond to interview stimuli such as ?what?s
one of your most memorable experiences?? with
idiosyncratic stories and events from their lives.
The variability in the vocabulary and content of
participants? answers to such questions is so large
that it makes the ASR task very challenging. Fur-
thermore, continuous ASR is employed to ensure
that participants feel comfortable interacting with
the system without being distracted by having to
use a push-to-talk microphone. The use of con-
tinuous ASR necessitates the development of spe-
cific policies for turn-taking (see Section 2.2). In
this demonstration, voice activity detection and
speech recognition are performed using Pocket-
Sphinx (Huggins-Daines et al., 2006).
Because of the open-ended participants? re-
sponses, for NLU, we cannot simply construct a
small semantic ontology and expect to cover the
majority of meanings that will be expressed by
users. Thus, this is an application in which the
dialogue policy needs to be able to create a sense
of engagement, continuation, and empathy despite
relatively shallow and limited understanding of
user speech. SimSensei Kiosk currently uses 4
statistically trained utterance classifiers to capture
different aspects of user utterance meaning.
The first NLU classifier identifies generic di-
alogue act types, including statements, yes-no
questions, wh-questions, yes and no answers, and
several others. This classifier is trained using
the Switchboard DAMSL corpus (Jurafsky et al.,
1997) using a maximum entropy model.
The second NLU classifier assigns positive,
negative, or neutral valence to utterances, in or-
der to guide Ellie?s expression of empathy. We
use SentiWordNet 3.0 (Baccianella et al., 2010), a
lexical sentiment dictionary, to assign valence to
individual words spoken by users (as recognized
by the ASR); the valence assigned to an utterance
is based primarily on the mean valence scores of
Opening Rapport Building Phase
Ellie What are some things you really like about LA?
(top level question)
User I love the weather, I love the palm trees, I love the
beaches, there?s a lot to do here.
Diagnostic Phase
Ellie Have you noticed any changes in your behavior or
thoughts lately? (top level question)
User Yes.
Ellie Can you tell me about that? (continuation prompt)
User I?m having a lot more nightmares now uh can?t
sleep have haven?t really been eating uh trying to
eat. . . I have to force down food um just feeling
like an emotional wreck.
Ellie I?m sorry to hear that. (empathy response)
Ellie What are you like when you don?t sleep well?
(follow-up question)
User Irritable, emotional, it just adds to my overall
stress um [long pause]
Ellie What. . . (Ellie speaks after the participant?s long
pause)
User Can?t concentrate uh I uh. . . (the participant starts
speaking while Ellie is speaking)
Ellie I?m sorry please continue. (Ellie realizes that she
has interrupted the participant and apologizes)
Positive Closing Phase
Ellie How would your best friend describe you? (top
level question)
User As caring, as fun because most of the time when
I?m around my best friends I?m happy and I?m fun
loving. I joke around with them a lot and uh I do
better when I?m around my friends. . .
Figure 2: Examples of Ellie?s interview phases.
the individual words in the utterance.
The third NLU classifier supports domain-
specific small talk by recognizing a handful of
specific anticipated responses to Ellie?s rapport-
building questions. For example, when Ellie asks
users where they are from, this classifier detects
the names of commonly mentioned cities and re-
gions using keyphrase spotting.
The fourth NLU classifier identifies domain-
specific dialogue acts, and supports Ellie?s follow-
up responses to specific questions, such as ?how
close are you to your family??. This maximum
entropy classifier is trained using face-to-face and
Wizard-of-Oz data to detect specific responses
such as assertions of closeness.
2.2 Dialogue Management
Ellie currently uses about 100 fixed utterances in
total in the automated system. She employs 60 top
level interview questions (e.g., ?do you travel a
255
lot??), plus some follow-up questions (e.g., ?what
do you enjoy about traveling??) and a range of
backchannels (e.g., ?uh huh?), empathy responses
(e.g., ?that?s great?, ?I?m sorry?), and continua-
tion prompts (e.g., ?tell me more about that?).
The dialogue policy is implemented using the
FLoReS dialogue manager (Morbini et al., 2012).
The policy groups interview questions into three
phases (opening rapport building, diagnostic, pos-
itive closing ? ensuring that the participant leaves
with positive feelings). Questions are generally
asked in a fixed order, with some branching based
on responses to specific questions.
Rule-based subpolicies determine what Ellie?s
follow-up responses will be for each of her top-
level interview questions. The rules for follow-ups
are defined in relation to the four NLU classifiers
and the duration of user speech (measured in sec-
onds). These rules trigger continuation prompts
and empathy responses under specific conditions.
The turn-taking policy supports our design goal
to encourage users to openly share information
and to speak at length in response to Ellie?s open-
ended questions. In this domain, users often pause
before or during their responses to think about
their answers to Ellie?s personal questions. The
turn-taking policy is designed to provide ample
time for users to consider their responses, and to
let users take and keep the initiative as much as
possible. Ellie?s turn-taking decisions are based
on thresholds for user pause duration, i.e., how
much time the system should wait after the user
has stopped speaking before Ellie starts speaking.
These thresholds are tuned to the face-to-face and
Wizard-of-Oz data to minimize Ellie?s interrup-
tion rate, and are extended dynamically when El-
lie detects that she has interrupted the participant.
This is to take into account the fact that some peo-
ple tend to use longer pauses than others.
Examples of the three interview phases and of
Ellie?s subdialogue policies (top level and follow-
up questions, continuation prompts, empathy re-
sponses, and turn-taking) are given in Figure 2.
3 Demonstration Summary
The demonstration will feature a live interac-
tion between Ellie and a participant, showing El-
lie?s real-time understanding and consequent pol-
icy actions. Live dialogues will highlight Ellie?s
strategies for questioning, follow-up continuation
prompts, displays of empathy, and turn-taking,
similar to the example in Figure 2. The demon-
stration will illustrate how these elements work to-
gether to enable Ellie to carry out extended inter-
views that also provide information relevant to the
automatic assessment of indicators of distress.
Acknowledgments
The effort described here is supported by DARPA
under contract W911NF-04-D-0005 and the U.S.
Army. Any opinion, content or information pre-
sented does not necessarily reflect the position or
the policy of the United States Government, and
no official endorsement should be inferred.
References
S. Baccianella, A. Esuli, and F. Sebastiani. 2010. Sen-
tiWordNet 3.0: An enhanced lexical resource for
sentiment analysis and opinion mining. In Proceed-
ings of LREC.
D. DeVault, K. Georgila, R. Artstein, F. Morbini, D.
Traum, S. Scherer, A. Rizzo, and L.-P. Morency.
2013. Verbal indicators of psychological distress in
interactive dialogue with a virtual human. In Pro-
ceedings of SIGDIAL.
D. DeVault, R. Artstein, G. Benn, T. Dey, E. Fast,
A. Gainer, K. Georgila, J. Gratch, A. Hartholt, M.
Lhommet, G. Lucas, S. Marsella, F. Morbini, A.
Nazarian, S. Scherer, G. Stratou, A. Suri, D. Traum,
R. Wood, Y. Xu, A. Rizzo, and L.-P. Morency. 2014.
SimSensei Kiosk: A virtual human interviewer for
healthcare decision support. In Proceedings of AA-
MAS.
A. Hartholt, D. Traum, S. Marsella, A. Shapiro, G.
Stratou, A. Leuski, L.-P. Morency, and J. Gratch.
2013. All together now, introducing the virtual hu-
man toolkit. In Proceedings of IVA.
D. Huggins-Daines, M. Kumar, A. Chan, A.W. Black,
M. Ravishankar, and A.I. Rudnicky. 2006. Pocket-
Sphinx: A free, real-time continuous speech recog-
nition system for hand-held devices. In Proceedings
of ICASSP.
D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board SWBD-DAMSL Shallow-Discourse-Function
Annotation Coders Manual, Draft 13.
F. Morbini, D. DeVault, K. Sagae, J. Gerten, A. Nazar-
ian, and D. Traum. 2012. FLoReS: A forward look-
ing reward seeking dialogue manager. In Proceed-
ings of IWSDS.
S. Scherer, G. Stratou, M. Mahmoud, J. Boberg,
J. Gratch, A. Rizzo, and L.-P. Morency. 2013. Au-
tomatic behavior descriptors for psychological dis-
order analysis. In Proceedings of IEEE Conference
on Automatic Face and Gesture Recognition.
256
