Proceedings of the 12th Conference of the European Chapter of the ACL, pages 870?878,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Language ID in the Context of Harvesting Language Data off the Web
Fei Xia
University of Washington
Seattle, WA 98195, USA
fxia@u.washington.edu
William D. Lewis
Microsoft Research
Redmond, WA 98052, USA
wilewis@microsoft.com
Hoifung Poon
University of Washington
Seattle, WA 98195, USA
hoifung@cs.washington.edu
Abstract
As the arm of NLP technologies extends
beyond a small core of languages, tech-
niques for working with instances of lan-
guage data across hundreds to thousands
of languages may require revisiting and re-
calibrating the tried and true methods that
are used. Of the NLP techniques that has
been treated as ?solved? is language iden-
tification (language ID) of written text.
However, we argue that language ID is
far from solved when one considers in-
put spanning not dozens of languages, but
rather hundreds to thousands, a number
that one approaches when harvesting lan-
guage data found on the Web. We formu-
late language ID as a coreference resolu-
tion problem and apply it to a Web harvest-
ing task for a specific linguistic data type
and achieve a much higher accuracy than
long accepted language ID approaches.
1 Introduction
A large number of the world?s languages have
been documented by linguists; it is now increas-
ingly common to post current research and data
to the Web, often in the form of language snip-
pets embedded in scholarly papers. A particu-
larly common format for linguistic data posted to
the Web is ?interlinearized text?, a format used
to present language data and analysis relevant to
a particular argument or investigation. Since in-
terlinear examples consist of orthographically or
phonetically encoded language data aligned with
an English translation, the ?corpus? of interlinear
examples found on the Web, when taken together,
constitute a significant multilingual, parallel cor-
pus covering hundreds to thousands of the world?s
languages. Previous work has discussed methods
for harvesting interlinear text off the Web (Lewis,
2006), enriching it via structural projections (Xia
and Lewis, 2007), and even making it available to
typological analyses (Lewis and Xia, 2008) and
search (Xia and Lewis, 2008).
One challenge with harvesting interlinear data
off the Web is language identification of the har-
vested data. There have been extensive studies
on language identification (language ID) of writ-
ten text, and a review of previous research on this
topic can be found in (Hughes et al, 2006). In gen-
eral, a language ID method requires a collection
of text for training, something on the order of a
thousand or more characters. These methods work
well for languages with rich language resources;
for instance, Cavnar and Trenkle?s N-gram-based
algorithm achieved an accuracy as high as 99.8%
when tested on newsgroup articles across eight
languages (Cavnar and Trenkle, 1994). However,
the performance is much worse (with accuracy
dropping to as low as 1.66%) if there is very lit-
tle language data for training and the number of
languages being evaluated reaches a few hundred.
In this paper, we treat the language ID of har-
vested linguistic data as a coreference resolution
problem. Our method, although narrowly focused
on this very specific data type, makes it possible to
collect small snippets of language data across hun-
dreds of languages and use the data for linguistic
search and bootstrapping NLP tools.
2 Background
2.1 Interlinear glossed text (IGT)
In linguistics, the practice of presenting language
data in interlinear form has a long history, go-
ing back at least to the time of the structural-
ists. Interlinear Glossed Text, or IGT, is often
used to present data and analysis on a language
that the reader may not know much about, and
is frequently included in scholarly linguistic doc-
uments. The canonical form of an IGT consists
870
of three lines: a line for the language in question
(i.e., the language line), an English gloss line, and
an English translation. Table 1 shows the begin-
ning of a linguistic document (Baker and Stewart,
1996) which contains two IGTs: one in lines 30-
32, and the other in lines 34-36. The line numbers
are added for the sake of convenience.
1: THE ADJ/VERB DISTINCTION: EDO EVIDENCE
2:
3: Mark C. Baker and Osamuyimen Thompson Stewart
4: McGill University
....
27: The following shows a similar minimal pair from Edo,
28: a Kwa language spoken in Nigeria (Agheyisi 1990).
29:
30: (2) a. E`me`ri? mo`se?.
31: Mary be.beautiful(V)
32: ?Mary is beautiful.?
33:
34: b. E`me`ri? *(ye?) mo`se?.
35: Mary be.beautiful(A)
36: ?Mary is beautiful (A).?
...
Table 1: A linguistic document that contains IGT:
words in boldface are potential language names
2.2 The Online Database of Interlinear text
(ODIN)
ODIN, the Online Database of INterlinear text, is
a resource built from data harvested from schol-
arly documents (Lewis, 2006). It was built in
three steps: (1) crawling the Web to retrieve doc-
uments that may contain IGT, (2) extracting IGT
from the retrieved documents, and (3) identifying
the language codes of the extracted IGTs. The
identified IGTs are then extracted and stored in a
database (the ODIN database), which can be easily
searched with a GUI interface.1
ODIN currently consists about 189,000 IGT in-
stances extracted from three thousand documents,
with close to a thousand languages represented.
In addition, there are another 130,000 additional
IGT-bearing documents that have been crawled
and are waiting for further process. Once these
additional documents are processed, the database
is expected to expand significantly.
ODIN is a valuable resource for linguists, as it
can be searched for IGTs that belong to a partic-
ular language or a language family, or those that
contain a particular linguistic construction (e.g.,
passive, wh-movement). In addition, there have
1http://odin.linguistlist.org
been some preliminary studies that show the bene-
fits of using the resource for NLP. For instance, our
previous work shows that automatically enriched
IGT data can be used to answer typological ques-
tions (e.g., the canonical word order of a language)
with a high accuracy (Lewis and Xia, 2008), and
the information could serve as prototypes for pro-
totype learning (Haghighi and Klein, 2006).
3 The language ID task for ODIN
As the size of ODIN increases dramatically, it is
crucial to have a reliable module that automati-
cally identifies the correct language code for each
new extracted IGT to be added to ODIN. The cur-
rent ODIN system uses two language identifiers:
one is based on simple heuristics, and the other
on Cavnar and Trenkle?s algorithm (1994). How-
ever, because the task here is very different from
a typical language ID task (see below), both algo-
rithms work poorly, with accuracy falling below
55%. The focus of this paper is on building new
language identifiers with a much higher accuracy.
3.1 The data set
A small portion of the IGTs in ODIN have
been assigned the correct language code semi-
automatically. Table 2 shows the size of the data
set. We use it for training and testing, and all re-
sults reported in the paper are the average of run-
ning 10-fold cross validation on the data set unless
specified otherwise.
Table 2: The data set for the language ID task
# of IGT-bearing documents 1160
# of IGT instances 15,239
# of words on the language lines 77,063
# of languages 638
3.2 The special properties of the task
The task in hand is very different from a typical
language ID task in several respects:
? Large number of languages: The number of
languages in our data set is 638 and that of the
current ODIN database is close to a thousand.
As more data is added to ODIN, the number
of languages may reach several thousand as
newly added linguistic documents could refer
to any of approximately eight thousand living
or dead languages.
871
? The use of language code: When dealing
with only a few dozen languages, language
names might be sufficient to identify lan-
guages. This is not true when dealing with
a large number of languages, because some
languages have multiple names, and some
language names refer to multiple languages
(see Section 4.2). To address this problem,
we use language codes, since we can (mostly)
ensure that each language code maps to ex-
actly one language, and each language maps
to exactly one code.
? Unseen languages: In this data set, about
10% of IGT instances in the test data belong
to some languages that have never appeared
in the training data. We call it the unseen
language problem. This problem turns out to
be the major obstacle to existing language ID
methods.
? Extremely limited amount of training data
per language: On average, each language in
the training data has only 23 IGTs (116 word
tokens in the language lines) available, and
45.3% of the languages have no more than
10 word tokens in the training data.
? The length of test instances: The language
lines in IGT are often very short. The aver-
age length in this data set is 5.1 words. About
0.26% of the language lines in the data set are
totally empty due to the errors introduced in
the crawling or IGT extraction steps.
? Encoding issues: For languages that do not
use Roman scripts in their writing system,
the authors of documents often choose to use
Romanized scripts (e.g., pinyin for Chinese),
making the encoding less informative.
? Multilingual documents: About 40% of doc-
uments in the data set contain IGTs from
multiple languages. Therefore, the language
ID prediction should be made for each indi-
vidual IGT, not for the whole document.
? Context information: In this task, IGTs are
part of a document and there are often various
cues in the document (e.g., language names)
that could help predict the language ID of
specific IGT instances.
Hughes and his colleagues (2006) identified
eleven open questions in the domain of language
ID that they believed were not adequately ad-
dressed in published research to date. Interest-
ingly, our task encounters eight out of the eleven
open questions. Because of these properties, ex-
isting language ID algorithms do not perform well
when applied to the task (see Section 6).
4 Using context information
Various cues in the document can help predict the
language ID of IGTs, and they are represented as
features in our systems.
4.1 Feature templates
The following feature templates are used in our ex-
periments.
(F1): The nearest language that precedes the cur-
rent IGT.
(F2): The languages that appear in the neighbor-
hood of the IGT or at the beginning or the
end of a document.2 Another feature checks
the most frequent language occurring in the
document.
(F3): For each language in the training data, we
build three token lists: one for word uni-
grams, one for morph unigrams and the third
for character ngrams (n ? 4). These word
lists are compared with the token lists built
from the language line of the current IGT.
(F4): Similar to (F3), but the comparison is be-
tween the token lists built from the current
IGT with the ones built from other IGTs in
the same document. If some IGTs in the
same document share the same tokens, they
are likely to belong to the same language.
Here, all the features are binary: for features in
F3 and F4, we use thresholds to turn real-valued
features into binary ones. F1-F3 features can
be calculated by looking at the documents only,
whereas F4 features require knowing the language
codes of other IGTs in the same document.
4.2 Language table
To identify language names in a document and
map language names to language codes, we need
a language table that lists all the (language code,
2For the experiments reported here, we use any line within
50 lines of the IGT or the first 50 or the last 50 lines of the
document.
872
language name) pairs. There are three existing lan-
guage tables: (1) ISO 639-3 maintained by SIL
International,3 (2) the 15th edition of the Ethno-
logue,4 and (3) the list of ancient and dead lan-
guages maintained by LinguistList.5 6 We merged
the three tables, as shown in Table 3.
Table 3: Various language name tables
Language table # of lang # of lang
codes (code, name) pairs
(1) ISO 639-3 7702 9312
(2) Ethnologue v15 7299 42789
(3) LinguistList table 231 232
Merged table 7816 47728
The mapping between language names and lan-
guage codes is many-to-many. A language code
often has several alternate names in addition to the
primary name. For instance, the language code
aaa maps to names such as Alumu, Tesu, Arum,
Alumu-Tesu, Alumu, Arum-Cesu, Arum-Chessu,
and Arum-Tesu. While most language names map
to only one language code, there are exceptions.
For instance, the name Edo can map to either bin
or lew. Out of 44,071 unique language names in
the merged language table, 2625 of them (5.95%)
are ambiguous.7
To identify language names in a document, we
implemented a simple language name detector that
scans the document from left to right and finds the
longest string that is a language name according
to the language table. The language name is then
mapped to language codes. If a language name is
ambiguous, all the corresponding language codes
are considered by later stages. In Table 1, the
language names identified by the detector are in
boldface. The detector can produce false positive
(e.g., Thompson) because a language name can
have other meanings. Also, the language table is
by no means complete and the detector is not able
to recognize any language names that are missing
from the table.
3http://www.sil.org/iso639-3/download.asp
4http://www.ethnologue.com/codes/default.asp#using
5http://linguistlist.org/forms/langs/GetListOfAncientLgs.html
6While ISO 639-3 is supposed to include all the language
codes appearing in the other two lists, there is a lag in the
adoption of new codes, which means the ISO 639-3 list con-
tinues to be somewhat out-of-date with the lists from which
it is compiled since these other lists change periodically.
7Among the ambiguous names, 1996 names each map to
two language codes, 407 map to three codes, 130 map to four
codes, and so on. The most ambiguous name is Miao, which
maps to fourteen language codes.
5 Formulating the language ID task
The language ID task here can be treated as two
different learning problems.
5.1 As a classification problem
The language ID task can be treated as a classifica-
tion problem. A classifier is a function that maps
a training/test instance x to a class label y, and y
is a member of a pre-defined label set C . For lan-
guage ID, the training/test instance corresponds to
a document (or an IGT in our case), and C is the
set of language codes. We call this approach the
classification (CL) approach.
Most, if not all, of previous language ID meth-
ods, fall into this category. They differ with re-
spect to the underlying learning algorithms and the
choice of features or similarity functions. When
applying a feature-based algorithm (e.g., Maxi-
mum entropy) and using the features in Section
4.1, the feature vectors for the two IGTs in Ta-
ble 1 are shown in Table 4. Each line has the for-
mat ?instance name true lang code feat name1
feat name2 ...?, where feat names are the names
of features that are present in the instance. Take
the first IGT as an example, its true language code
is bin; the nearest language name (nearLC) is Edo
whose language code is bin or lew; the languages
that appear before the IGT includes Edo (bin or
lew), Thompson (thp), and so on. The presence of
LMw1 bin and LMm1 bin means that the overlap
between the word/morph lists for bin and the ones
built from the current IGT is higher than some
threshold. The feature vector for the second IGT
looks similar, except that it includes a F4 feature
IIw1 bin, which says that the overlap between the
word list built from the other IGTs in the same
document with language code bin and the word
list built from the current IGT is above a thresh-
old. Note that language codes are part of feature
names; therefore, a simple feature template such
as nearest language (nearLC) corresponds to hun-
dreds or even thousands of features (nearLC xxx).
The CL approach has several major limitations.
First, it cannot handle the unseen language prob-
lem: if an IGT in the test data belongs to a lan-
guage that does not appear in the training data, this
approach cannot classify it correctly. Second, the
lack of parameter tying in this approach makes it
unable to generalize between different languages.
For instance, if the word German appears right be-
fore an IGT, the IGT is likely to be German. The
873
igt1 bin nearLC bin nearLC lew prev50 bin prev50 lew prev50 thp ... LMw1 bin LMm1 bin ...
igt2 bin nearLC bin nearLC lew prev50 bin prev50 lew prev50 thp ... LMw1 bin LMm1 bin ... IIw1 bin ...
Table 4: Feature vectors for the IGTs in Table 1 when using the CL approach (Edo: bin/lew, Thompson:
thp, Kwa: etu/fip/kwb)
same is true if the word German is replaced by an-
other language name. But this property cannot be
leveraged easily by the CL approach without mod-
ifying the learning algorithm. This results in a pro-
liferation of parameters, making learning harder
and more prone to overfitting.
5.2 As a coreference resolution problem
A different way of handling the language ID task
is to treat it as a coreference resolution problem: a
mention is an IGT or a language name appearing
in a document, an entity is a language code, and
finding the language code for an IGT is the same as
linking a mention (i.e., an IGT) to an entity (i.e., a
language code).8 We call this approach the CoRef
approach. The major difference between the CL
approach and the CoRef approach is the role of
language code: in the former, language code is a
class label to be used to tag an IGT; and in the lat-
ter, language code is an entity which an IGT can
be linked to.
The language ID task shares many similarities
with a typical coreference resolution task. For
instance, language names are similar to proper
nouns in that they are often unambiguous. IGT
instances are like pronouns in that they often refer
to language names appearing in the neighborhood.
Once the language ID task is framed as a CoRef
problem, all the existing algorithms on CoRef can
be applied to the task, as discussed below.
5.2.1 Sequence labeling using traditional
classifiers
One common approach to the CoRef problem pro-
cesses the mentions sequentially and determine for
each mention whether it should start a new entity
or be linked to an existing mention (e.g., (Soon
et al, 2001; Ng and Cardie, 2002; Luo, 2007));
that is, the approach makes a series of decisions,
8There are minor differences between the language ID and
coreference resolution tasks. For instance, each entity in the
language ID task must be assigned a language code. This
means that ambiguous language names will evoke multiple
entities, each with a different language code. These differ-
ences are reflected in our algorithms.
one decision per (mention, entity) pair. Apply-
ing this to the language ID task, the (mention, en-
tity) pair would correspond to an (IGT, lang code)
pair, and each decision would have two possibili-
ties: Same when the IGT belongs to the language
or Diff when the IGT does not. Once the decisions
are made for all the pairs, a post-processing proce-
dure would check all the pairs for an IGT and link
the IGT to the language code with which the pair
has the highest confidence score.
Using the same kinds of features in Section 4.1,
the feature vectors for the two IGTs in Table 1 are
shown in Table 5. Comparing Table 4 and 5 re-
veals the differences between the CL approach and
the CoRef approach: the CoRef approach has only
two class labels (Same and Diff) where the CL ap-
proach has hundreds of labels (one for each lan-
guage code); the CoRef approach has much fewer
number of features because language code is not
part of feature names; the CoRef approach has
more training instances as each training instance
corresponds to an (IGT, lang code) pair.
igt1-bin same nearLC prev50 LMw1 LMm1 ...
igt1-lew diff nearLC prev50 ...
igt1-thp diff prev50 ...
...
igt2-bin same nearLC prev50 LMw1 LMm1 IIw1 ...
igt2-lew diff nearLC prev50 ...
igt2-thp diff prev50 ...
...
Table 5: Feature vectors for the IGTs in Table 1
when using the CoRef approach with sequence la-
beling methods
5.2.2 Joint Inference Using Markov Logic
Recently, joint inference has become a topic of
keen interests in both the machine learning and
NLP communities (e.g., (Bakir et al, 2007; Sut-
ton et al, 2006; Poon and Domingos, 2007)).
There have been increasing interests in formulat-
ing coreference resolution in a joint model and
conducting joint inference to leverage dependen-
874
cies among the mentions and entities (e.g., (Well-
ner et al, 2004; Denis and Baldridge, 2007; Poon
and Domingos, 2008)). We have built a joint
model for language ID in Markov logic (Richard-
son and Domingos, 2006).
Markov logic is a probabilistic extension of
first-order logic that makes it possible to com-
pactly specify probability distributions over com-
plex relational domains. A Markov logic net-
work (MLN) is a set of weighted first-order
clauses. Together with a set of constants, it de-
fines a Markov network with one node per ground
atom and one feature per ground clause. The
weight of a feature is the weight of the first-order
clause that originated it. The probability of a
state x in such a network is given by P (x) =
(1/Z) exp (?i wifi(x)), where Z is a normaliza-
tion constant, wi is the weight of the ith clause,
fi = 1 if the ith clause is true, and fi = 0
otherwise. Conditional probabilities can be com-
puted using Markov chain Monte Carlo (e.g., MC-
SAT (Poon and Domingos, 2006)). The weights
can be learned using pseudo-likelihood training
with L-BFGS (Richardson and Domingos, 2006).
Markov logic is one of the most powerful rep-
resentations for joint inference with uncertainty,
and an implementation of its existing learning and
inference algorithms is publicly available in the
Alchemy package (Kok et al, 2007).
To use the features defined in Section 4.1, our
MLN includes two evidence predicates: the first
one is HasFeature(i, l, f) where f is a feature in
F1-F3. The predicate is true iff the IGT-language
pair (i, l) has feature f . The second predicate is
HasRelation(i1, i2, r) where r is a relation that
corresponds to a feature in F4; this predicate is
true iff relation r holds between two IGTs i1, i2.
The query predicate is IsSame(i, l), which is true
iff IGT i is in language l. Table 6 shows the pred-
icates instantiated from the two IGTs in Table 1.
The language ID task can be captured in our
MLN with just three formulas:
IsSame(i, l)
HasFeature(i, l,+f) ? IsSame(i, l)
HasRelation(i1, i2,+r)? IsSame(i1, l)
? IsSame(i2, l)
The first formula captures the default probabil-
ity that an IGT belongs to a particular language.
IsSame(igt1, bin)
HasFeature(igt1, bin, nearLC)
HasFeature(igt1, bin, prev50)
HasFeature(igt1, bin, LMw1)
...
HasFeature(igt1, lew, nearLC)
HasFeature(igt1, lew, prev50)
...
IsSame(igt2, bin)
HasFeature(igt2, bin, nearLC)
HasFeature(igt2, bin, prev50)
HasFeature(igt2, bin, LMw1)
...
HasRelation(igt1, igt2, IIw1)
...
Table 6: The predicates instantiated from the IGTs
in Table 1
The second one captures the conditional likeli-
hoods of an IGT being in a language given the fea-
tures. The third formula says that two IGTs prob-
ably belong to the same language if they have a
certain relation r.
The plus sign before f and r in the formulas
signifies that the MLN will learn a separate weight
for each individual feature f and relation r. Note
that there is no plus sign before i and l, allowing
the MLN to achieve parameter tying by sharing the
same weights for different instances or languages.
5.2.3 The advantage of the Coref approach
Both methods of the CoRef approach address the
limitations of the CL approach: both can handle
the unseen language problem, and both do param-
eter tying in a natural way. Not only does parame-
ter tying reduce the number of parameters, it also
makes it possible to accumulate evidence among
different languages and different IGTs.
6 Experiments
In this section, we compare the two approaches
to the language ID task: the CL approach and the
CoRef approach. In our experiments, we run 10-
fold cross validation (90% for training and 10%
for testing) on the data set in Table 2 and report
the average of language ID accuracy.
The two approaches have different upper
bounds. The upper bound of the CL approach is
the percentage of IGTs in the test data that be-
long to a seen language. The upper bound of the
CoRef approach is the percentage of IGTs in the
test data that belong to a language whose language
name appears in the same document. For the data
set in Table 2, the upper bounds are 90.33% and
875
Table 7: The performance of the CL approach (# of classes: about 600, # of training instances=13,723)
Upper bound of TextCat MaxEnt classifier using context information
CL approach F1 F1-F2 F1-F3 F1-F4 (cheating)
# of features N/A N/A 769 5492 8226 8793
w/o the language filter 90.33 51.38 49.74 61.55 64.19 66.47
w/ the language filter 88.95 60.72 56.69 64.95 67.03 69.20
97.31% respectively. When the training data is
much smaller, the upper bound of the CL approach
would decrease tremendously, whereas the upper
bound of the CoRef approach remains the same.
6.1 The CL approach
As mentioned before, most existing language ID
algorithm falls into this category. We chose
TextCat,9 an implementation of Cavnar-Trenkle?s
algorithm (1994), as an example of these algo-
rithms. In order to take advantage of the con-
text information, we trained several classifiers
(e.g., decision tree, Naive Bayes, and maximum
entropy) using the Mallet package (McCallum,
2002) and a SVM classifier using the libSVM
package (Chang and Lin, 2001).
The result is in Table 7. The first column shows
the upper bound of the CL approach; the second
column is the result of running TextCat;10 the rest
of the table lists the result of running a MaxEnt
classifier with different feature sets.11 F4 features
require knowing the language code of other IGTs
in the document. In the F1-F4 cheating exper-
iments, the language codes of other IGTs come
from the gold standard. We did not implement
beam search for this because the difference be-
tween the cheating results and the results without
F4 features is relatively small and both are much
worse than the results in the CoRef approach.
In Table 7, the first row shows the number of
features; the second row shows the accuracy of the
two classifiers; the last row is the accuracy when
a post-processing filter is added: the filter takes
the ranked language list produced by a classifier,
throws away all the languages in the list that do
not appear in the document, and then outputs the
highest ranked language in the remaining list.
There are several observations. First, applying
the post-processing filter improves performance,
9http://odur.let.rug.nl/ vannoord/TextCat/
10We varied the lexicon size (m) ? an important tuned pa-
rameter for the algorithm ? from 100 and 800 and observed
a minor change to accuracy. The numbers reported here are
with lexicon size set to 800.
11The MaxEnt classifier slightly outperforms other classi-
fiers with the same feature set.
albeit it also lowers the upper bound of algorithms
as the correct language names might not appear
in the document. Second, the MaxEnt classifier
has hundreds of classes, thousands of features, and
millions of model parameters. This will cause se-
vere sparse data and overfitting problems.
6.2 The CoRef approach
For the CoRef approach, we built two systems as
described in Section 5: the first system is a Max-
Ent classifier with beam search, and the second
one is a MLN for joint inference.12 The results
are in Table 8.13
In the first system, the values of F4 features
for the test data come from the gold standard
in the F1-F4 cheating experiments, and come
from beam search in the non-cheating experi-
ments.14 In the second system, the predicate
HasRelation(i1, i2, r) instantiated from the test
data is treated as evidence in the F1-F4 cheat-
ing experiments, and as query in the F1-F4 non-
cheating experiments.
The results for the two systems are very similar
since they use same kinds of features. However,
with Markov logic, it is easy to add predicates and
formulas to allow joint inference. Therefore, we
believe that Markov logic offers more potential to
incorporate arbitrary prior knowledge and lever-
age further opportunities in joint inference.
Tables 7-8 show that, with the same kind of fea-
tures and the same amount of training data, the
CoRef approach has higher upper bound, fewer
model parameters, more training instances, and
much higher accuracy than the CL approach. This
study shows that properly formulating a task into
a learning problem is very important.
12For learning and inference, we used the existing im-
plementations of pseudo-likelihood training and MC-SAT in
Alchemy with default parameters.
13No language filter is needed since the approach links an
IGT to only the language names appearing in the document.
14It turns out that for this task the size of beam does not
matter much and simply using the top choice by the Max-
Ent classifier for each IGT almost always produces the best
results, so that is the setting used for this table and Table 9.
876
Table 8: The performance of the CoRef approach (# of classes=2, # of training instances=511,039)
Upper bound of F1 F1-F2 F1-F3 F1-F4 F1-F4
CoRef approach (cheating) (Non-cheating)
# of features N/A 2 12 17 22 22
Sequence labeling 97.31 54.37 66.32 83.49 90.26 85.10
Markov logic model 97.31 54.98 65.94 83.44 90.37 84.70
Table 9: The performance of the CoRef approach with less training data (the upper bound of the Coref
approach remains 97.31%)
% of training F1 F1-F2 F1-F3 F1-F4 F1-F4 Upper bound of
data used (cheating) (non-cheating) the CL approach
0.1% 54.37 54.84 65.28 81.21 70.15 1.66
0.5% 54.37 62.78 76.74 87.17 80.24 21.15
1.0% 54.37 60.58 76.09 87.24 81.20 28.92
10% 54.37 62.13 77.07 87.20 83.08 54.45
6.3 Experiments with much less data
Table 8 shows that the CoRef approach has very
few features and a much larger number of training
instances; therefore, it is likely that the approach
would work well even with much less training
data. To test the idea, we trained the model with
only a small fraction of the original training data
and tested on the same test data. The results with
the first system are in Table 9. Notice that the up-
per bound of the CoRef approach remains the same
as before. In contrast, the upper bound for the CL
model is much lower, as shown in the last column
of the table. The table shows when there is very
little training data, the CoRef approach still per-
forms decently, whereas the CL approach would
totally fail due to the extremely low upper bounds.
6.4 Error analysis
Several factors contribute to the gap between the
best CoRef system and its upper bound. First,
when several language names appear in close
range, the surface positions of the language names
are often insufficient to determine the prominence
of the languages. For instance, in pattern ?Similar
to L1, L2 ...?, L2 is the more prominent than L1;
whereas in pattern ?L1, a L2 language, ...?, L1 is.
The system sometimes chooses a wrong language
in this case.
Second, the language name detector described
in Section 4.2 produces many false negative (due
to the incompleteness of the language table) and
false positive (due to the fact that language names
often have other meanings).
Third, when a language name is ambiguous,
choosing the correct language code often requires
knowledge that might not even be present in the
document. For instance, a language name could
refer to a list of related languages spoken in the
same region, and assigning a correct language
code would require knowledge about the subtle
differences among those languages.
7 Conclusion and future work
In this paper we describe a language identification
methodology that achieves high accuracy with a
very small amount of training data for hundreds
of languages, significantly outperforming existing
language ID algorithms applied to the task. The
gain comes from two sources: by taking advan-
tage of context information in the document, and
by formulating the task as a coreference resolution
problem.
Our method can be adapted to harvest other
kinds of linguistic data from the Web (e.g., lexicon
entries, word lists, transcriptions, etc.) and build
other ODIN-like resources. Providing a means for
rapidly increasing the amount of data in ODIN,
while at the same time automatically increasing
the number of languages, can have a significant
positive impact on the linguistic community, a
community that already benefits from the existing
search facility in ODIN. Likewise, the increased
size of the resulting ODIN database could pro-
vide sufficient data to bootstrap NLP tools (e.g.,
POS taggers and parsers) for a large number of
low-density languages, greatly benefitting both the
fields of linguistics and NLP.
Acknowledgements This work has been sup-
ported, in part, by the NSF grants BCS-0748919
and BCS-0720670 and ONR grant N00014-08-1-
0670. We would also like to thank three anony-
mous reviewers for their valuable comments.
877
References
Mark C. Baker and Osamuyimen Thompson Stewart.
1996. Unaccusativity and the adjective/verb distinc-
tion: Edo evidence. In Proceedings of the Fifth An-
nual Conference on Document Analysis and Infor-
mation Retrieval (SDAIR), Amherst, Mass.
G. Bakir, T. Hofmann, B. Scholkopf, A. Smola,
B. Taskar, and S. Vishwanathan (eds). 2007. Pre-
dicting Structured Data. MIT Press.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Proceedings of
SDAIR-94, 3rd Annual Symposium on Document
Analysis and Information Retrieval, pages 161?175,
Las Vegas, US.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM:
a library for support vector machines. Available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
Pascal Denis and Jason Baldridge. 2007. Joint de-
termination of anaphoricity and coreference reso-
lution using integer programming. In Proc. of
the Conference on Human Language Technologies
(HLT/NAACL 2007), pages 236?243, Rochester,
New York, April.
Aria Haghighi and Dan Klein. 2006. Prototype-
driven grammar induction. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics (COLING/ACL
2006), pages 881?888, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew MacKinlay. 2006. Recon-
sidering language identification for written language
resources. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC2006), pages 485?488, Genoa, Italy.
S. Kok, P. Singla, M. Richardson, P. Domingos,
M. Sumner, H Poon, and D. Lowd. 2007. The
Alchemy system for statistical relational AI. Tech-
nical report, Dept. of CSE, Univ. of Washington.
William Lewis and Fei Xia. 2008. Automatically Iden-
tifying Computationally Relevant Typological Fea-
tures. In Proc. of the Third International Joint Con-
ference on Natural Language Processing (IJCNLP-
2008), Hyderabad, India.
William Lewis. 2006. ODIN: A Model for Adapting
and Enriching Legacy Infrastructure. In Proc. of the
e-Humanities Workshop, held in cooperation with e-
Science 2006: 2nd IEEE International Conference
on e-Science and Grid Computing, Amsterdam.
Xiaoqiang Luo. 2007. Coreference or not: A
twin model for coreference resolution. In Proc. of
the Conference on Human Language Technologies
(HLT/NAACL 2007), pages 73?80, Rochester, New
York.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Vincent Ng and Claire Cardie. 2002. Improving Ma-
chine Learning Approaches to Coreference Reso-
lution. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-2002), pages 104?111, Philadelphia.
H. Poon and P. Domingos. 2006. Sound and effi-
cient inference with probabilistic and deterministic
dependencies. In Proc. of AAAI-06.
Hoifung Poon and Pedro Domingos. 2007. Joint in-
ference in information extraction. In Proceedings
of the Twenty-Second National Conference on Artifi-
cial Intelligence (AAAI), pages 913?918, Vancouver,
Canada. AAAI Press.
H. Poon and P. Domingos. 2008. Joint unsupervised
coreference resolution with markov logic. In Proc.
of the 13th Conf. on Empirical Methods in Natural
Language Processing (EMNLP-2008).
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine Learning, pages 107?136.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4).
Charles Sutton, Andrew McCallum, and Jeff Bilmes
(eds.). 2006. Proc. of the HLT/NAACL-06 Work-
shop on Joint Inference for Natural Language Pro-
cessing.
B. Wellner, A. McCallum, F. Peng, and M. Hay. 2004.
An integrated, conditional model of information ex-
traction and coreference with application to citation
matching. In Proc. of the 20th Conference on Un-
certainty in AI (UAI 2004).
Fei Xia and William Lewis. 2007. Multilingual struc-
tural projection across interlinear text. In Proc. of
the Conference on Human Language Technologies
(HLT/NAACL 2007), pages 452?459, Rochester,
New York.
Fei Xia and William Lewis. 2008. Repurposing
Theoretical Linguistic Data for Tool Development
and Search. In Proc. of the Third International
Joint Conference on Natural Language Processing
(IJCNLP-2008), Hyderabad, India.
878
Repurposing Theoretical Linguistic Data
for Tool Development and Search
Fei Xia
University of Washington
Seattle, WA 98195
fxia@u.washington.edu
William D. Lewis?
Microsoft Research
Redmond, WA 98052-6399
wilewis@microsoft.com
Abstract
For the majority of the world?s languages,
the number of linguistic resources (e.g., an-
notated corpora and parallel data) is very
limited. Consequently, supervised methods,
as well as many unsupervised methods, can-
not be applied directly, leaving these lan-
guages largely untouched and unnoticed. In
this paper, we describe the construction of a
resource that taps the large body of linguisti-
cally analyzed language data that has made
its way to the Web, and propose using this
resource to bootstrap NLP tool development.
1 Introduction
Until fairly recently, most NLP research has focused
on the ten or so majority languages of the world, the
canonical high density languages. Low density, or
resource poor languages (RPLs), have more recently
captured the interest of NLP research, mostly be-
cause of recent advances in computational technolo-
gies and computing power. As indicated by their
name, RPLs suffer from a lack of resources, namely
data. Supervised learning techniques generally re-
quire large amounts of annotated data, something
that is nonexistent or scare for most RPLs. A greater
number of RPLs, however, have raw data that is
available, and the amount and availability of this raw
data is increasing every day as more of it makes its
way to the Web. Likewise, advances in un- and semi-
supervised learning techniques have made raw data
more readily viable for tool development. Still, how-
ever, such techniques often require ?seeds?, or ?pro-
totypes? (c.f., (Haghighi and Klein, 2006)) which are
used to prune search spaces or direct learners.
An important question is how to create such seeds
for the hundreds to thousands of RPLs. We describe
the construction of a resource that taps the large
body of linguistically analyzed language data that
has made its way to the Web, and propose using this
?
The work described in this document was done while
Lewis was faculty at the University of Washington.
resource as a means to bootstrap NLP tool devel-
opment. Interlinear Glossed Text, or IGT, a semi-
structured data type quite common to the field of
linguistics, is used to present data and analysis for a
language and is generally embedded in scholarly lin-
guistic documents as part of a larger analysis. IGT?s
unique structure ? effectively each instance consists
of a bitext between English and some target language
? can be easily enriched through alignment and pro-
jection (e.g., (Yarowsky and Ngai, 2001), (Hwa et al,
2002)). The reader will note that the IGT instance
in Example (1) consists of a bitext between some tar-
get language on the first line, or the target line (in
this case in Welsh), and a third line in English, the
translation line. The canonical IGT form, which this
example is representative of, has intervening linguis-
tic annotations and glosses on a second line, the gloss
line. Because the gloss line aligns with words and
morphemes on the target line, and contains glosses
that are similar to words on the translation line, it
can serve as a bridge between the target and transla-
tion lines; high word alignment accuracy between the
three lines can be achieved without requiring parallel
data or bilingual dictionaries (Xia and Lewis, 2007).
Furthermore, the gloss line provides additional in-
formation about the target language data, such as
a variety of grammatical annotations, including ver-
bal and tense markers (e.g., 3sg), case markers, etc.,
all of which can provide useful knowledge about the
language.
(1) Rhoddodd yr athro lyfr i?r bachgen ddoe
gave-3sg the teacher book to-the boy yesterday
?The teacher gave a book to the boy yesterday?
(Bailyn, 2001)
ODIN, the Online Database of INterlinear text
(Lewis, 2006), is a resource built over the past few
years from data harvested from scholarly documents.
Currently, ODIN has over 41,581 instances of IGT for
944 languages, and the number of IGT instances is
expected to double or triple in the near-term as new
methods for collecting data are brought online. Al-
though the number of instances per language varies,
e.g., the maximum currently is 2,891 instances (for
529
Table 1: The numbers of languages in ODIN
Range of # of # of % of
IGT instances languages instances instances
1000-2891 10 15019 36.11
500-999 11 8111 19.50
250-499 18 6274 15.08
100-249 22 3303 7.94
50-99 38 2812 6.76
25-49 60 2089 5.02
10-24 127 1934 4.65
1-9 658 2039 4.91
Japanese), and the overall number per language may
appear small, it is still possible to harvest significant
value from IGT for targeted RPLs. In this paper,
we present the ODIN database and methods used to
create it. We also present methods we have employed
to enrich IGT in order to make it more readily useful
for bootstrapping NLP tools. Because the canon of
knowledge embodied in the hundred or so years of
linguistic analysis remains virtually untapped by the
NLP community, we provide a bridge between the
communities by providing linguistic data in a way
that NLP researchers will find useful. Likewise, be-
cause IGT is a common linguistic data type, we pro-
vide a search facility over these data, which has al-
ready been found to be quite useful to the theoretical
linguistics community.
2 Building ODIN
ODIN currently has 41,581 IGT instances for 944
languages. Table 1 shows the number of languages
that fall into buckets defined by the number of IGT
instances for each language. For instance, the fourth
row (?bucket?) says that 22 languages each have 100
to 249 IGT instances, and the 3,303 instances in this
bucket account for 7.94% of all instances. ODIN is
built in three steps, as described below.1
2.1 Crawling for IGT documents
Because a large number of instances of IGT exist on
the Web,2 we have focused on searching for these
1
The work of creating ODIN, in some ways, speaks
to the need of standardizing IGT (perhaps along with
other linguistic data types) such that both humans and
machines can more readily consume the data. Some re-
cent efforts to develop standards for encoding IGT (e.g.,
(Hughes et al, 2003), (Bickel et al, 2004)) have met with
limited success, however, since they have not been widely
recognized and even less frequently adopted. Over time
it is our hope that these or other standards will see wider
use thus eliminating the need for much of the work pro-
posed here.
2
Although we have no direct data about the total
number of IGT instances that exist on the Web, we hy-
instances. The major difficulty with locating docu-
ments that contain IGT, however, is reducing the size
of the search space. We decided very early in the de-
velopment of ODIN that unconstrained Web crawl-
ing was too time and resource intensive a process to
be feasible, mostly due to the Web?s massive size.
We discovered that highly focused metacrawls were
far more fruitful. Metacrawling essentially involves
throwing queries against an existing search engine,
such as Google, Yahoo or MSN Live, and crawling
only the pages returned by those queries. We found
that the most successful queries were those that used
strings contained within IGT itself, e.g. grammatical
annotations, or grams, such as 3sg, NOM, ACC, etc.
In addition, we found precision increased when we
included two or more search terms per query, with
the most successful queries being those which com-
bined grams and language names. Thus, for exam-
ple, although NOM alone returned a large number of
linguistic documents, NOM combined with ACC (or
any other high frequency term), or a language name,
returned a far less noisy and far more relevant set of
documents.
Other queries we have developed include: queries
by language names and language codes (drawn from
the Ethnologue database (Gordon, 2005), which con-
tains about 40,000 language names and their vari-
ants), by linguists? names and the languages they
work on (drawn from the Linguist List?s linguist
database), by linguistically relevant terms (drawn
from the SIL linguistic glossary), and by particular
words or morphemes found in IGT and their gram-
matical markup. Table 2 shows the statistics for the
most successful crawls and their related search term
?types?. Calculated from the top 100 queries for each
type, the table presents the most successful query
types, the average number of documents returned for
each, the average number of documents in which IGT
was actually found, and the average number of IGT
instances netted by each query. The most relevant
measure of success is the number of IGT instances
returned (the obvious focus of our crawling); in turn,
the most successful query types are those which con-
tain a combination of grams and language names.3
pothesize that the total supply is at least several hundred
thousand instances. Given that ODIN contains 41,581
instances which have been extracted from approximately
3,000 documents, and given that we have located at least
60,000 more documents that might contain IGT, we feel
our estimate to be reasonable.
3
Note that target documents are often returned by
multiple queries. For instance, the documents returned
by ?NOM+ACC+Icelandic? will also be returned by the
individual query terms ?NOM?, ?ACC?, and ?Icelandic?.
530
Table 2: The Most Successful Query Types
Query Type Avg # Avg # docs Avg #
docs w/ IGT IGTs
Gram(s) 1184 239 50
Language name(s) 1314 259 33
Both grams 1536 289 77
and names
Language words 1159 193 0
2.2 IGT detection
After crawling, the next step is to identify IGT in-
stances in the retrieved documents. This is a dif-
ficult task for which machine learning methods are
well suited.
2.2.1 Difficulty in IGT detection
The canonical form of IGT, as presented in Sec-
tion 1, consists of three parts and each part is on a
single line. However, many IGT instances do not fol-
low the canonical format for several reasons. First,
when IGT examples appear in a group, very often the
translation or glosses are dropped for some examples
in the group because the missing parts can be recov-
ered from the context, resulting in two-part IGT. In
other cases, some IGT examples include multiple tar-
get transcriptions (e.g., one part in the native script,
and another in a latin transliteration) or even, in rare
cases, multiple translations.
Second, dictated by formatting constraints, long
IGT examples may need to be wrapped one or more
times, and there are no conventions on how wrapping
should be done, nor how many times it can be done.
For short IGT examples, sometimes linguists put the
translation to the right of the target line rather than
below it. As a result, each part of IGT examples
may appear on multiple lines and multiple parts can
appear on a single line.
Third, most IGT-bearing documents on the Web
are in PDF, and the PDF-to-text conversion tools
will sometimes corrupt IGT instances (most often
on the target line). In some instances, some words
or morphemes on the target line are inadvertently
dropped in the conversion, or are displaced up or
down a line. Finally, an IGT instance could fall into
multiple categories. For instance, a two-part IGT
instance could have a corrupted target line. All of
this makes the detection task difficult.
2.2.2 Applying machine learning methods
The first system that we designed for IGT detec-
tion used regular expression ?templates?, effectively
looking for text that resembled IGT. An example is
shown in (2), which matches any three-line instance
(e.g., the IGT instance in (1)) such that the first line
starts with an example number (e.g., (1)) and the
third line starts with a quotation mark.
(2) \s*\(\d+\).*\n
\s*.*\n
\s*\[??"].*\n
Unfortunately, this approach tends to over-select
when applied to the documents crawled from the
Web. Further, many true IGT instances do not
match any of hand-written templates due to the is-
sues mentioned in the previous section. As a result,
both precision and recall are quite low (see Table 4).
Given the irregular structure of IGT instances, a
statistical system is likely to outperform a rule-based
system. In our second system, we treat the IGT de-
tection task as a sequence labeling problem, and ap-
ply machine learning methods to the task: first, we
train a learner and use it to tag each line in a doc-
ument with a tag in a pre-defined tag set; then we
convert the best tag sequence into a span sequence.
A span is a (start, end) pair, which indicates the be-
ginning and ending line numbers of an IGT instance.
Among all the tagging schemes we experimented
with (including the standard BIO tagging scheme),
the following 5-tag scheme works the best on the de-
velopment set: The five tags are BL (any blank line),
O (outside IGT that is not a BL), B (the first line
in an IGT), E (the last line in an IGT), I (inside an
IGT that is not a B, E, or BL).
For machine learning, we use four types of features:
F1: The words that appear on the current line.
These are the features typically used in a text
classification task.
F2: Sixteen features that look at various cues for the
presence of an IGT. For example, whether the
line starts with a quotation, whether the line
starts with an example number (e.g., (1)), and
whether the line contains a large portion of hy-
phenated or non-English tokens.
F3: In order to find good tag sequences, we include
features for the tags of the previous two lines.
F4: The same features as in F2, but they are checked
against the neighboring lines. For instance, if a
feature f5 in F2 checks whether the current line
contains a citation, f+15 checks whether the next
line contains a citation.
After the lines in a document are tagged by the
learner, we identify IGT instances by finding all the
spans in the document that match the ?B [I | BL]?
E? pattern; that is, the span starts with a B, ends
with an E, and has zero or more I or BL in between.4
4
Other heuristics for converting tag sequences to span
sequences produce similar results.
531
Table 3: Data sets for the IGT detection experiments
# files # lines # IGTs
Training data 41 39127 1573
Dev data 10 8932 447
Test data 10 14592 843
2.2.3 Experimental results
To evaluate the two detectors, we randomly se-
lected 61 ODIN documents and manually marked
the occurrence of IGT instances. The files were then
split into training, development, and test sets, and
the size of each set is shown in Table 3. The annota-
tion speed was about four thousand lines per hour.
Each file in the development and test sets was an-
notated independently by two annotators, and the
inter-annotator agreement (f-score) on IGT bound-
ary was 93.74% when using exact match (i.e., two
spans match iff they are identical). When partial
match (i.e., two spans match iff they overlap) was
used, the f-score increased to 98.66%.
We used four machine learning algorithms imple-
mented in Mallet (McCallum, 2002): decision tree,
Naive Bayes, maximum entropy (MaxEnt), and con-
ditional random field (CRF).5 Table 4 shows the
MaxEnt model?s performance on the development set
with different combinations of features: the highest
f-score for exact match in each group is marked in
boldface.6 In addition to exact and partial match
results, we also list the number of spans produced
by the system (cf. the span number in the gold stan-
dard is 447) and the classification accuracy (i.e., the
percent of lines receiving correct labels). The results
for CRF are very similar to those for MaxEnt, and
both outperform decision tree and Naive Bayes.
Several observations are in order. First, as ex-
pected, the machine learning approach outperforms
the regular expression approach. Second, although
F2 contains only sixteen features, it works much bet-
ter than F1, which uses all the words occurring in the
training data. Third, F4 works much better than F3
in capturing contextual information, mainly because
F4 allows the learner to take into account the infor-
mation that appears on both the preceding lines and
the succeeding lines.7 Last, adding F1 and F3 to
5
For the first three methods, we implemented beam
search to find the best tag sequences; and for CRF, we
used features in F1, F2, and F4, as the model itself in-
corporates the information about previous tags already.
6F
4
is an extension of F
2
, so every combination with
F
4
should include F
2
as well. Also, F
3
should not be used
alone. Therefore, Table 4 in fact lists all the possible
feature combinations.
7
The window for F
4
is set empirically to [-2,3]; that
is, F
4
uses the information from the preceding two lines
the F2 + F4 system offers a modest but statistically
significant gain.
Table 5 shows the results on the test data. The
performance of MaxEnt on this data set is slightly
worse than on the development set mainly because
the test set contains much more corrupted data (due
to pdf-to-text conversion) than both the training and
development sets.8 Nevertheless, the machine learn-
ing approach outperforms the regex approach signifi-
cantly, reducing the error rate by 52.3%. In addition,
the partial match results are much better than ex-
act match results, indicating that many span errors
could be potentially fixed by postprocessing.
2.3 Manual review and language ID
About 45% of IGT instances in the current ODIN
database were manually checked to verify IGT
boundaries and to identify the language names of
the target lines. Subsequently, we trained several
language ID algorithms with the labeled data, and
used them to label the remaining 55% of the IGT
instances in ODIN automatically.
The language ID task in this context is different
from a typical language ID task in several ways.
First, the number of languages in IGT is close to
a thousand or even more. In contrast, the amount
of training data for many of the languages is very
limited; for instance, hundreds of languages have less
than 10 sentences, as shown in Table 1. Second, some
languages in the test data might never occur in the
training data, a problem that we shall call the un-
known language problem. Third, the target sentences
in IGT are very short (e.g., a few words), making the
task more challenging. Fourth, for languages that do
not use a latin-based writing system, the target sen-
tences are often transliterated, making the character
encoding scheme less informative. Last, the context,
such as the language names occurring in the docu-
ment, provides important cues for the language ID
of IGT instances.
Given these properties, applying common lan-
guage ID algorithms directly will not produce sat-
isfactory results. For instance, Cavnar and Tren-
kle?s N-gram-based algorithm yields an accuracy of
as high as 99.8% when tested on newsgroup arti-
cles in eight languages (Cavnar and Trenkle, 1994).9
and the succeeding three lines.
8
The corruption not only affects the target lines, but
also the layout of IGT (e.g., the indentation of the three
lines). As a result, features in F
2
and F
4
are not as effec-
tive as for the development set. Since the regex template
approach uses fewer layout features, its performance is
not affected as much.
9
The accuracy ranges from 92.9% to 99.8% depending
on the article length and a model parameter called profile
532
Table 4: Performance on the development set (the span number in the gold standard is 447)
Features System Classification Exact match Partial match
span num accuracy prec recall fscore prec recall fscore
Regex templates 269 N/A 68.40 41.16 51.40 99.26 59.73 74.58
F
1
130 81.50 68.46 19.91 30.85 97.69 28.41 44.02
F
2
405 93.28 58.27 52.80 55.40 95.56 86.58 90.85
F
1
+ F
3
180 80.26 61.67 24.83 35.40 81.11 32.66 46.57
F
1
+ F
2
420 94.42 63.09 59.28 61.13 93.81 88.14 90.88
F
2
+ F
3
339 92.68 75.81 57.49 65.39 93.21 70.69 80.40
F
2
+ F
4
456 96.91 80.92 82.55 81.73 93.64 95.53 94.57
F
1
+ F
2
+ F
3
370 93.39 75.14 62.20 68.05 93.51 77.40 84.70
F
1
+ F
2
+ F
4
444 97.00 84.68 84.11 84.40 95.95 95.30 95.62
F
2
+ F
3
+ F
4
431 97.79 86.77 83.67 85.19 97.68 94.18 95.90
F
1
+ F
2
+ F
3
+ F
4
431 98.00 90.02 86.80 88.38 97.22 93.74 95.44
Table 5: Performance on the test set (the span number in the gold standard is 843)
Features System Classification Exact match Partial match
span num accuracy prec recall fscore prec recall fscore
Regex templates 587 N/A 74.95 52.19 61.54 98.64 68.68 80.98
F
2
719 92.45 57.02 48.64 52.50 94.02 80.19 86.56
F
2
+ F
4
849 95.66 75.50 76.04 75.77 93.76 94.42 94.09
F
2
+ F
3
+ F
4
831 95.95 77.14 76.04 76.58 95.19 93.83 94.50
F
1
+ F
2
+ F
3
+ F
4
830 96.83 82.29 81.02 81.65 96.51 95.02 95.76
However, when we ran the same algorithm on the
IGT data, the accuracy was only 50.2%.10 In con-
trast, a heuristic approach that predicts the language
ID according to the language names occurring in the
document yields an accuracy of 65.6%.
Because the language name associated with an
IGT instance almost always appears somewhere in
the document, we propose to treat the language ID
task as a reference resolution problem, where IGT
instances are the mentions and the language names
appearing in the document are the entities. A lan-
guage identifier simply needs to link the mentions
to the entities, allowing us to apply any good res-
olution algorithms such as (Soon et al, 2001; Ng,
2005; Luo, 2007) and to provide an elegant solution
to the unknown language problem. More detail on
this approach will be reported elsewhere.
3 Using ODIN
We see ODIN being used in a number of different
ways. In another study (Lewis and Xia, 2008), we
demonstrated a method for using ODIN to discover
interesting and computationally relevant typological
features for hundreds of the world?s languages auto-
matically. In this section we present two more uses
length.
10
The setting for our preliminary experiments is as fol-
lows: there are 10,415 IGT instances over 549 languages
in the training data, and 3064 instances in the test data.
The language names of about 12.2% of IGT instances in
the test data never appear in the training data.
for ODIN?s data: bootstrapping NLP tools (specif-
ically taggers), and providing search over ODIN?s
data (as a kind of large-scale multi-lingual search).
3.1 IGT for bootstrapping NLP tools
Since the target line in IGT data does not come with
annotations (e.g., POS tags), it is first necessary to
enrich it. Once enriched, the data can be used as a
bootstrap for tools such as taggers.
3.1.1 Enriching IGT
In a previous study (Xia and Lewis, 2007), we pro-
posed a three-step process to enrich IGT data: (1)
parse the English translation with an English parser
and convert English phrase structures (PS) into de-
pendency structures (DS) with a head percolation
table (Magerman, 1995), (2) align the target line and
the English translation using the gloss line, and (3)
project the syntactic structures (both PS and DS)
from English onto the target line. For instance, given
the IGT example in Ex (1), the enrichment algorithm
will produce the word alignment in Figure 1 and the
syntactic structures in Figure 2.
The   t eache r   gave   a   book   t o     t he     boy    yes te rday   
Rhoddodd   y r    a th ro      l y f r      i ? r      bachgen   ddoe     
 G loss  l i ne :
 T r a n s l a t i o n :
T a r g e t  l i n e :
g a v e - 3 s g   t h e   t e a c h e r  b o o k   t o - t h e   b o y    y e s t e r d a y
Figure 1: Aligning the target line and the English
translation with the help of the gloss line
533
gave
(a) Projecting DS
athro
bachgen
lyfr
yr
ddoei?r
 Rhoddodd S
NP1 VP
NN
teacher
VBD
  gave
NP2
DT
a 
NP4PP
NN
the
IN NP3
yesterday
NN
DT
book
NN
boy
DT
to
S
NP
NN
VBD
NP NPPP
NN
IN+DT
NN
NNDT
  rhoddodd
  (gave) yr(the)
   athro
(teacher)
lyfr
(book)
    i?r
(to-the)
bachogen
(boy) ddoe
(yesterday)
teacher
a boy
the
book
the
yesterdayto
The
(b) Projecting PS
Figure 2: Projecting syntactic structure from English to the target language
We evaluated the algorithm on a small set of 538
IGT instances for several languages. On average,
the accuracy of the English DS (i.e., the percentage
of correct dependency links in the DS) is 93.48%;
the f-score of the word alignment links between the
translation and target lines is 94.03%, and the ac-
curacy of the target DS produced by the projection
algorithm is 81.45%. When we replace the automati-
cally generated English DS and word alignment with
the ones in the gold standard, the accuracy of target
DS increases significantly, from 81.45% to 90.64%.
The details on the algorithms and the experiments
can be found in (Xia and Lewis, 2007).
3.1.2 Bootstrapping NLP tools
The enriched data produced by the projection al-
gorithms contains (1) the English DS and PS pro-
duced by an English parser, (2) the word alignment
among the three parts of IGT data, and (3) the tar-
get DS and PS produced by the projection algorithm.
From the enriched data, various kinds of information
can be extracted. For instance, the target syntactic
structures form small monolingual treebanks, from
which grammars in various formalisms can be ex-
tracted (e.g., (Charniak, 1996)). The English and
target syntactic structures form parallel treebanks,
from which transfer rules and translation lexicon can
be extracted and used for machine translation (e.g.,
(Meyers et al, 2000; Menezes, 2002; Xia and Mc-
Cord, 2004)).
There are many ways of using the enriched data
to bootstrap NLP tools. Suppose we want to build a
POS tagger. Previous studies on unsupervised POS
tagging can be divided into several categories accord-
ing to the kind of information available to the learner.
The first category (e.g., (Kupiec, 1992; Merialdo,
1994; Banko and Moore, 2004; Wang and Schuur-
mans, 2005)) assumes there is a lexicon that lists
the allowable tags for each word in the text. The
common approach is to use the lexicon to initialize
the emission probability in a Hidden Markov Model
(HMM), and run the Baum-Welch algorithm (Baum
et al, 1970) on a large amount of unlabeled data
to re-estimate transition and emission probability.
The second category uses unlabeled data only (e.g.,
(Schu?tze, 1995; Clark, 2003; Biemann, 2006; Das-
gupta and Ng, 2007)). The idea is to cluster words
based on morphological and/or distributional cues.
Haghighi and Klein (2006) showed that adding a
small set of prototypes to the unlabeled data can
improve tagging accuracy significantly.
The tagged target lines in the enriched IGT data
can be incorporated in each category of work men-
tioned above. For instance, the frequency collected
from the data can be used to bias initial transi-
tion and emission probabilities in an HMM model;
the tagged words in IGT can be used to label the
resulting clusters produced by the word clustering
approach; the frequent and unambiguous words in
the target lines can serve as prototype examples in
the prototype-driven approach (Haghighi and Klein,
2006). Finally, we can apply semi-supervised learn-
ing algorithms (e.g., self-training (Yarowsky, 1995),
co-training (Blum and Mitchell, 1998), and transduc-
tive support vector machines (Vapnik, 1998)), using
the tagged sentences as seeds.
3.2 Search
One focus of ODIN is and has always been search:
how can linguists find the data that they are inter-
ested in and how can the data be encoded in such a
way as to accommodate the variety of queries that a
linguist might ask. We currently allow four types of
search queries: search by language name and code,
search by language family, search by concept/gram,
and search by linguistic constructions. The first al-
lows the user to specify a language name or ISO code
to search for, and allows the user to view documents
that contain instances of IGT in that language, as
well as the instances themselves. The second al-
lows the user to specify a language family (families
as specified in the Ethnologue), and returns similar
results, except grouped by language. The third al-
lows the user to select from a list of known grams,
all of which have been mapped to a conceptual space
534
used by linguists (the GOLD ontology, (Farrar and
Langendoen, 2003)).11
The final query type, the Construction Search is
the most powerful and most innovative of the query
facilities currently provided by ODIN. Rather than
limiting search to just the content and markup na-
tively contained within IGT, Construction Search
searches over enriched content. For instance, a search
for relative clauses can look for either the POS tag
sequences that contain a noun followed by an ap-
propriate relativizer, or the parse trees that contain
an NP node with an NP child and a clause child.
Currently, 15 construction queries have been imple-
mented, with some 40 additional queries being eval-
uated and built. Note that currently construction
queries are performed on the English translation, not
on the target language data. As syntactic projection
becomes more reliable, we will allow construction
queries on the target language data and even queries
on both the English and the target (e.g., for com-
parative linguistic analyses). For example, a query
could be something like Find examples where the tar-
get line uses imperfective aspect and is in active voice
and the English translation uses passive voice.
4 Conclusion and Future Directions
In this paper, we introduce Interlinear Glossed Text
(IGT), a data type that has been rarely tapped by
the NLP community, and describe the process of cre-
ating ODIN, a database of IGT data. We show that
using machine learning methods can significantly im-
prove the performance of IGT detection. We then
demonstrate how IGT instances can be enriched and
discuss several ways of using enriched data to boot-
strap NLP tools such as POS taggers. Finally, we
review the four types of linguistic search that are cur-
rently implemented in ODIN. All of the above show
the value of ODIN as a resource for both NLP re-
searchers and linguists. In the future, we plan to im-
prove the IGT detection and language ID algorithms
and will apply them to all the crawled documents.
We expect the size of ODIN to grow dramatically.
We also plan to use the enriched data to bootstrap
taggers and parsers, starting with the ideas outlined
in Section 3.1.2.
Acknowledgements This work has been sup-
ported, in part, by the Royalty Research Fund at
the University of Washington. We would also like
to thank Dan Jinguji for providing the preliminary
11
Most gram-to-concept mapping has been done by
hand. We are currently exploring methods to use ma-
chine learning to enhance our ability to identify and map
additional unknown grams (to be discussed elsewhere).
results on language ID expriments, and three anony-
mous reviewers for their valuable comments.
References
John Frederick Bailyn. 2001. Inversion, dislocation
and optionality in Russian. In Gerhild Zybatow,
editor, Current Issues in Formal Slavic Linguis-
tics.
Michele Banko and Robert C. Moore. 2004. Part
of Speech Tagging in Context. In Proc. of the
20th International Conference on Computational
Linguistics (Coling 2004), pages 556?561, Geneva,
Switzerland.
L. E. Baum, T. Petrie, G. Soules, and N. Weiss.
1970. A maximization technique occurring in
the statistical analysis of probabilistic functions of
Markov chains. Ann. Math. Statistics, 41(1):164?
171.
Balthasar Bickel, Bernard Comrie, and Martin
Haspelmath. 2004. The Leipzig Glossing
Rules: Conventions for interlinear morpheme-by-
morpheme glosses (revised version). Technical re-
port, Max Planck Institute for Evolutionary An-
thropology and the Department of Linguistics of
the University of Leipzig.
Chris Biemann. 2006. Unsupervised part-of-speech
tagging employing efficient graph clustering. In
Proceedings of the COLING/ACL 2006 Student
Research Workshop, pages 7?12, Sydney, Aus-
tralia, July.
Avrim Blum and Tom Mitchell. 1998. Combining
Labeled and Unlabeled Data with Co-training. In
Proc. of the Workshop on Computational Learning
Theory (COLT-1998).
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Proceedings of
SDAIR-94, 3rd Annual Symposium on Document
Analysis and Information Retrieval, pages 161?
175, Las Vegas, US.
Eugene Charniak. 1996. Treebank Grammars. In
Proc. of the 13th National Conference on Artificial
Intelligence (AAAI-1996).
Alexander Clark. 2003. Combining distributional
and morphological information for part of speech
induction. In Proc. of the 10th Conference of the
European Chapter of the Association for Compu-
tational Linguistics (EACL-2003).
Sajib Dasgupta and Vincent Ng. 2007. Unsuper-
vised part-of-speech acquisition for resource-scarce
languages. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
535
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 218?227.
Scott Farrar and D. Terence Langendoen. 2003. A
linguistic ontology for the Semantic Web. GLOT
International, 7(3):97?100.
Raymond G. Gordon, editor. 2005. Ethnologue:
Languages of the World. SIL International, Dallas,
TX, fifteenth edition.
Aria Haghighi and Dan Klein. 2006. Prototype-
driven learning for sequence models. In Proceed-
ings of the Human Language Technology Confer-
ence of the NAACL (HLT/NAACL 2006), pages
320?327, New York City, USA.
Baden Hughes, Steven Bird, and Cathy Bow. 2003.
Interlinear text facilities. In E-MELD 2003, Michi-
gan State University.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and
Okan Kolak. 2002. Evaluating translational cor-
respondence using annotation projection. In Pro-
ceedings of the 40th Annual Meeting of the ACL,
Philadelphia, Pennsylvania.
J. Kupiec. 1992. Robust part-of-speech tagging us-
ing a hidden Markov model. Computer Speech and
Language, 6.
William Lewis and Fei Xia. 2008. Automatically
Identifying Computationally Relevant Typologi-
cal Features. In Proc. of the Third International
Joint Conference on Natural Language Processing
(IJCNLP-2008), Hyderabad, India.
William Lewis. 2006. ODIN: A Model for Adapt-
ing and Enriching Legacy Infrastructure. In Proc.
of the e-Humanities Workshop, held in cooperation
with e-Science 2006: 2nd IEEE International Con-
ference on e-Science and Grid Computing, Ams-
terdam.
Xiaoqiang Luo. 2007. Coreference or not: A twin
model for coreference resolution. In Proc. of
the Conference on Human Language Technologies
(HLT/NAACL 2007), pages 73?80, Rochester,
New York.
David M. Magerman. 1995. Statistical Decision-
Tree Models for Parsing. In Proc. of the 33rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-1995), Cambridge, Mas-
sachusetts, USA.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Arul Menezes. 2002. Better contextual translation
using machine learning. In Proc. of the 5th con-
ference of the Association for Machine Translation
in the Americas (AMTA 2002).
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2).
Adam Meyers, Michiko Kosaka, and Ralph Grish-
man. 2000. Chart-based transfer rule application
in machine translation. In Proc. of the 18th Inter-
national Conference on Computational Linguistics
(COLING 2000).
Vincent Ng. 2005. Machine learning for coreference
resolution: From local classification to global rank-
ing. In Proc. of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL
2005), pages 157?164, Ann Arbor, Michigan.
Hinrich Schu?tze. 1995. Distributional part-of-speech
tagging. In Proc. of the EACL, pages 141?148.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4).
V. Vapnik. 1998. Statistical learning theory. Wiley-
Interscience.
Qin Iris Wang and Dale Schuurmans. 2005.
Improved Estimation for Unsupervised Part-of-
Speech Tagging. In Proc. of IEEE International
Conference on Natural Language Processing and
Knowledge Engineering (IEEE NLP-KE 2005).
Fei Xia and William Lewis. 2007. Multilingual struc-
tural projection across interlinear text. In Proc. of
the Conference on Human Language Technologies
(HLT/NAACL 2007), pages 452?459, Rochester,
New York.
Fei Xia and Michael McCord. 2004. Improv-
ing a Statistical MT System with Automatically
Learned Rewrite Patterns. In Proc. of the 20th
International Conference on Computational Lin-
guistics (COLING 2004), Geneva, Switzerland.
David Yarowsky and Grace Ngai. 2001. Induc-
ing Multilingual POS Taggers and NP Bracketers
via Robust Projection across Aligned Corpora. In
Proc. of the 2001 Meeting of the North American
chapter of the Association for Computational Lin-
guistics (NAACL-2001), pages 200?207.
David Yarowsky. 1995. Unsupervised word sense
disambiguation rivaling supervised methods. In
Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics (ACL-
1995), pages 189?196, Cambridge, Massachussets.
536
Automatically Identifying Computationally Relevant Typological Features
William D. Lewis?
Microsoft Research
Redmond, WA 98052-6399
wilewis@microsoft.com
Fei Xia
University of Washington
Seattle, WA 98195
fxia@u.washington.edu
Abstract
In this paper we explore the potential for iden-
tifying computationally relevant typological fea-
tures from a multilingual corpus of language data
built from readily available language data col-
lected off the Web. Our work builds on previous
structural projection work, where we extend the
work of projection to building individual CFGs
for approximately 100 languages. We then use
the CFGs to discover the values of typological
parameters such as word order, the presence or
absence of definite and indefinite determiners,
etc. Our methods have the potential of being
extended to many more languages and parame-
ters, and can have significant effects on current
research focused on tool and resource develop-
ment for low-density languages and grammar in-
duction from raw corpora.
1 Introduction
There is much recent interest in NLP in ?low-density?
languages, languages that typically defy standard NLP
methodologies due to the absence or paucity of relevant
digital resources, such as treebanks, parallel corpora, ma-
chine readable lexicons and grammars. Even when re-
sources such as raw or parallel corpora exist, they often
cannot be found of sufficient size to allow the use of stan-
dard machine learning methods. In some recent gram-
mar induction and MT work (Haghighi and Klein, 2006;
Quirk et al, 2005) it has been shown that even a small
amount of knowledge about a language, in the form of
grammar fragments, treelets or prototypes, can go a long
way in helping with the induction of a grammar from raw
text or with alignment of parallel corpora.
In this paper we present a novel method for discov-
ering knowledge about many of the world?s languages
by tapping readily available language data posted to the
Web. Building upon our work on structural projections
across interlinearized text (Xia and Lewis, 2007), we de-
scribe a means for automatically discovering a number of
computationally salient typological features, such as the
existence of particular constituents in a language (e.g.,
?The work described in this document was done while Lewis
was faculty at the University of Washington.
definite or indefinite determiners) or the canonical or-
der of constituents (e.g., sentential word order, order of
constituents in noun phrases). This knowledge can then
be used for subsequent grammar and tool development
work. We demonstrate that given even a very small sam-
ple of interlinearized data for a language, it is possible to
discover computationally relevant information about the
language, and because of the sheer volume and diversity
of interlinear text on the Web, it is possible to do so for
hundreds to thousands of the world?s languages.
2 Background
2.1 Web-Based Interlinear Data as Resource
In linguistics, the practice of presenting language data in
interlinear form has a long history, going back at least to
the time of the structuralists. Interlinear Glossed Text,
or IGT, is often used to present data and analysis on a
language that the reader may not know much about, and
is frequently included in scholarly linguistic documents.
The canonical form, an example of which is shown in (1),
consists of three lines: a line for the language in question
(often a sentence, which we will refer to here as the target
sentence), an English gloss line, and an English transla-
tion.
(1) Rhoddodd yr athro lyfr i?r bachgen ddoe
gave-3sg the teacher book to-the boy yesterday
?The teacher gave a book to the boy yesterday?
(Bailyn, 2001)
The reader will note that many word forms are shared
between the gloss and translation lines, allowing for the
alignment between these two lines as an intermediate step
in the alignment between the translation and the target.
We use this fact to facilitate projections from the parsed
English data to the target language, and use the result-
ing grammars to discover the values of the typological
parameters that are the focus of this paper.
We use ODIN, the Online Database of INterlinear text
(http://www.csufresno.edu/odin), as our primary source
of IGT data. ODIN is the result of an effort to collect
and database snippets of IGT contained in scholarly doc-
uments posted to the Web (Lewis, 2006). At the time of
this writing, ODIN contains 41,581 instances of interlin-
ear data for 944 languages.
685
2.2 The Structural Projection and CFG Extraction
Algorithms
Our algorithm enriches the original IGT examples by
building phrase structures over the English data and then
projects these onto the target language data via word
alignment. The enrichment process has three steps: (1)
parse the English translation using an English parser, (2)
align the target sentence and the English translation us-
ing the gloss line, and (3) project the phrase structures
onto the target sentence. The specific details of the pro-
jection algorithm are described in (Xia and Lewis, 2007).
Given the projected phrase structures on target sentences,
we then designed algorithms to extract context-free gram-
mars (CFGs) for each of the languages by reading off the
context-free rules from the projected target phrase struc-
ture. Identical rules are collapsed, and a frequency of
occurrence is associated with each rule. CFGs so gen-
erated provide the target grammars we use for work of
typological discovery we describe here.
Since the gloss line provides a means of associating
the English translation with the target language, the pro-
jections from the English translation effectively project
?through? the gloss line. Any annotations associated the
projected words, such as POS tags, can be associated with
words and morphemes on the gloss line during the enrich-
ment process and then can be projected onto the target.
These tags are essential for answering some of the typo-
logical questions, and are generally not provided by the
linguist. This is especially important for associated par-
ticular grammatical concepts, such as number or tense,
with particular word categories, such as verb and noun.
3 The IGT and English Biases
The choice of the IGT as our source data type presents
two causes for concern. First, IGT is typically used by
linguists to illustrate linguistically interesting phenomena
in a language. A linguist often carefully chooses exam-
ples from a language such that they are representative of
the phenomena he or she wishes to discuss, and in no way
can they be seen as being randomly sampled from a ?cor-
pus? of day-to-day usage for the language. It might be
argued, then, that a corpus built over IGT suffers from
this bias, what we call the IGT bias, and results generated
from IGT will be somewhat skewed. Second, since we
enrich IGT using a method of structural projection from
parses made to English translations, the language struc-
tures and the grammars extracted from them might suf-
fer from an English-centrism, what we call English bias:
we cannot assume that all languages will have the same
or similar grammatical features or constructions that En-
glish has, and by projecting structures from English, we
bias the structures we generate to the English source. The
degree to which we overcome these biases will demon-
strate not only the success of our methodology, but also
the viability of a corpus of IGT instances.
4 Experimental Design
4.1 The Typological Parameters
Linguistic typology is the study of the classification of
languages, where a typology is an organization of lan-
guages by an enumerated list of logically possible types,
most often identified by one or more structural features.1
One of the most well known and well studied typolog-
ical types, or parameters2, is that of word order, made
famous by Joseph Greenberg (Greenberg, 1963). In this
seminal work, Greenberg identified six possible order-
ings of Subjects, Objects, and Verbs in the world?s lan-
guages, namely, SVO, SOV, VSO, VOS, OSV and OVS,
and identified correlations between word order and other
constituent orderings, such as the now well known ten-
dency for SVO languages (e.g., English, Spanish) to have
prepositional ordering in adpositional phrases and SOV
(e.g., Japanese, Korean) to have postpositional.
We take inspiration from Greenberg?s work, and that of
succeeding typologists (e.g.(Comrie, 1989; Croft, 1990)).
Using the linguistic typological literature as our base, we
identified a set of typological parameters which we felt
could have the most relevance to NLP, especially to tasks
which might require prototype or structural bootstraps.
All of the parameters we identified enumerate various
constituent orderings, or the presence or absence of par-
ticular constituents. The complete list of typological pa-
rameters is shown in table 1. There are two major cat-
egories of parameters shown: (1) Constituent order pa-
rameters, which are broken down into (a) word order and
(b) morpheme order, and (2) constituent existence. For
each parameter, we enumerate the list of possible values
(what typologists typically call types), which is generally
a permutation of the possible orderings, constraining the
set of possible answers to these values. The value ndo
is reserved to indicate that a particular language exhibits
no dominant order for the parameter in question, that is,
there is no default or canonical order for the language.
The value nr, or not relevant, indicates that a primary
constituent of the parameter does not exist in the language
and therefore no possible values for the parameter can ex-
ist. A good example of this can be seen for the DT+N
parameter: in some languages, definite and indefinite de-
terminers may not exist, therefore making the parameter
irrelevant. In the specific case of determiners, we have
the Def and Indef parameters, which describe the pres-
ence or absence of definite and/or indefinite determiners
1See (Croft, 1990) for a thorough discussion of linguistic
typology and lists of possible types.
2The term typological parameter is in line with common us-
age within the field of linguistic typology.
686
for any given language. Since the parameters Def and
Indef are strictly existence tests, their possible values are
constrained simply to Yes or No.
4.2 Creating the Gold Standards
The gold standards were created by examining grammars
and typological analyses for each language, and in some
cases, consulting with native speakers or language ex-
perts. A principal target was the World Atlas of Lan-
guage Structures, or WALS (Haspelmath et al, 2005),
which contains a typology for hundreds of the world?s
languages. For each of the parameters shown in Table 1,
a WALS # is provided. This was done for the convenience
of the reader, and refers to the specific section numbers in
WALS that can be consulted for a detailed explanation of
the parameter. In some cases, WALS does not discuss
a particular parameter we used, in which case a WALS
section number is not provided (i.e., it is N/A).
5 Finding the Answers
As discussed, a typology consists of a parameter and a
list of possible types, essentially the values this parame-
ter may hold. These values are usually not atomic, and
can be decomposed into their permuted elements, which
themselves are types. For instance, the word order param-
eter is constrained by the types SVO, SOV, etc., whose
atoms are the types S for Subject, V for Verb, and O
for Object. When we talk about the order of words in
a language, we are not talking about the order of certain
words, such as the constituents The teacher, read, and the
book in the sentence The teacher read the book, but rather
the order of the types that each of these words maps to,
S, V, and O. Thus, examining individuals sentences of a
language tell us little about the values for the typological
parameters if the data is not annotated.
The structural projections built over IGT provide the
annotations for specific phrases, words or morphemes
in the target language, and, where necessary, the struc-
tural relationships between the annotations as expressed
in a CFG. There are three broad classes of algorithms for
this discovery process, which correspond directly to each
of the basic categories of parameters shown in Table 1.
For the word order parameters, we use an algorithm that
directly examines the linear relationship of the relative
types in the CFG. For the DT+N variable, for instance,
we look for the relative order of the POS tags DT and N
in the NP rules. For the WOrder variable, we look for
the relative order NPs and Vs in the S (Sentence) and VP
rules. If a language has a dominant rule of S ? NP VP,
it is highly likely that the language is SVO or SOV, and
we can subsequently determine VO or OV by examining
the VP rule: VP ? V NP indicates VO and VP ? NP V
indicates OV.
Table 2: Functional Tags in the CFGs
Tag Meaning Parameters Affected
NP-SBJ Subject NP WOrder, V-OBJ
NP-OBJ Object NP WOrder, V-OBJ
NP-POSS Possessive NP Poss-N
NP-XOBJ Oblique Object NP VP-OBJ
PP-XOBJ Oblique Object PP VP-OBJ
DT1 the DT-N, Def
DT2 a,an DT-N, Indef
DT3 this, that Dem-N, Def
DT4 all other determiners Not used
Determining morpheme order is somewhat simplified
in that the CFGs do not have to be consulted, but rather a
grammar consisting of possible morpheme orders, which
are derived from the tagged constituents on the gloss line.
The source of the tags varies: POS tags, for instance, are
generally not provided by the linguist, and thus must be
projected onto the target line from the English transla-
tion. Other tags, such as case, number, and tense/aspect
are generally represented by the linguist but with a finer
granularity than we need. For example, the linguist will
list the specific case, such as NOM for Nominative or
ACC for Accusative, rather than just the label ?case?. We
use a table from (Lewis, 2006) that has the top 80 mor-
pheme tags used by linguists to map the specific values
to the case, number, and tense/aspect tags that we need.
The existence parameters?in our study constrained to
Definite and Indefinite determiners?require us to test the
existence of particular POS annotations in the set of rel-
evant CFG rules, and also to examine the specific map-
pings of words between the gloss and translation lines.
For instance, if there are no DT tags in any of the CFG
rules for NPs, it is unlikely the language has definite or
indefinite determiners. This can specifically be confirmed
by checking the transfer rules between the and a and con-
stituents on the gloss line. If either or both the or a mostly
map to NULL, then either or both may not exist in the
language.
6 Experiments
We conducted two experiments to test the feasibility of
our methods. For the first experiment, we built a gold
standard for each of the typological parameters shown
in Table 1 for ten languages, namely Welsh, German,
Yaqui, Mandarin Chinese, Hebrew, Hungarian, Icelandic,
Japanese, Russian, and Spanish. These languages were
chosen for their typological diversity (e.g., word order),
for the number of IGT instances available (all had a min-
imum of fifty instances), and for the fact that some lan-
guages were low-density (e.g., Welsh, Yaqui). For the
second experiment, we examined the WOrder parameter
for 97 languages. The gold standard for this experiment
was copied directly from an electronic version of WALS.
687
Table 1: Computationally Salient Typological parameters (ndo=no dominant order, nr=not relevant)
Label WALS # Description Possible Values
Word Order
WOrder 330 Order of Words in a sentence SVO,SOV,VSO,VOS,OVS, OSV,ndo3
V+OBJ 342 Order of the Verb, Object and Oblique Object (e.g., PP) VXO,VOX,OVX,OXV,XVO,XOV,ndo
DT+N N/A Order of Nouns and Determiners (a, the) DT-N, N-DT, ndo, nr
Dem+N 358 Order of Nouns and Demonstrative Determiners (this, that) Dem-N, N-Dem, ndo, nr
JJ+N 354 Order of Adjectives and Nouns JJ-N, N-JJ, ndo
PRP$+N N/A Order of possessive pronouns and nouns PRP$-N, N-PRP$, ndo, nr
Poss+N 350 Order of Possessive NPs and nouns NP-Poss, NP-Poss, ndo, nr
P+NP 346 Order of Adpositions and Nouns P-NP, NP-P, ndo
Morpheme Order
N+num 138 Order of Nouns and Number Inflections (Sing, Plur) N-num, num-N, ndo
N+case 210 Order of Nouns and Case Inflections N-case, case-N, ndo, nr
V+TA 282 Order of Verbs and Tense/Aspect Inflections V-TA, TA-V, ndo, nr
Existence Tests
Def 154 Do definite determiners exist? Yes, No
Indef 158 Do indefinite determiners exist? Yes, No
Table 3: Experiment 1 Results (Accuracy)
WOrder VP DT Dem JJ PRP$ Poss P N N V Def Indef Avg
+OBJ +N +N +N +N +N +NP +num +case +TA
basic CFG 0.8 0.5 0.8 0.8 1.0 0.8 0.6 0.9 0.7 0.8 0.8 1.0 0.9 0.800
sum(CFG) 0.8 0.5 0.8 0.8 0.9 0.7 0.6 0.8 0.6 0.8 0.7 1.0 0.9 0.762
CFG w/ func 0.9 0.6 0.8 0.9 1.0 0.8 0.7 0.9 0.7 0.8 0.8 1.0 0.9 0.831
both 0.9 0.6 0.8 0.8 0.9 0.7 0.5 0.8 0.6 0.8 0.7 1.0 0.9 0.769
Since the number of IGT instances varied greatly, from a
minimum of 1 (Halkomelem, Hatam, Palauan, Itelmen)
to a maximum of 795 (Japanese), as shown in the first
column of Table 4, we were able to examine specifically
the correlation between the number of instances and our
system?s performance (at least for this parameter).
6.1 Experiment 1 - Results for 10 Languages, 14
Parameters
As described, the grammars for any given language con-
sist of a CFG and associated frequencies. Our first in-
tuition was that for any given word order parameter, the
most frequent ordering, as expressed by the most frequent
rule in which it appears, was likely the predominant pat-
tern in the language. Thus, for Hungarian, the order of the
DT+N parameter is DT-N since the most frequent rule,
namely NP ? DT N, occurs much more frequently than
the one rule with the opposing order, by a factor of 33 to
1. Our second intuition was based on the assumption that
noise could cause an anomalous ordering to appear in the
most frequent rule of a targeted type, especially when the
number of IGT examples was limited. We hypothesized
that ?summing? across a set of rules that contained the list
of constituents we were interested in might give more ac-
curate results, giving the predominant patterns a chance
to reveal themselves in the summation process.
An examination of the types of rules in the CFGs and
the parameter values we needed to populate led us to con-
sider enriching the annotations on the English side. For
instance, if a CFG contained the rule S ? NP V, it is im-
possible for us to tell whether the NP is a subject or an
object, a fact that is particularly relevant to the WOrder
parameter. We enriched the annotations with functional
tags, such as SBJ, OBJ, POSS, etc., which we assigned
using heuristics based on our knowledge of English, and
which could then be projected onto the target. The down-
side of such an approach is that it increases the granular-
ity of the grammar rules, which then could weaken the
generalizations that might be relevant to particular typo-
logical discoveries. However, summing across such rules
might alleviate some of this problem. We also divided the
English determiners into four groups in order to distin-
guish their different types, and projected the refined tags
onto the target. The full set of functional tags we used are
shown in Table 2, with the list of typological parameters
that were affected by the inclusion of each.4 The results
for the experiment are shown in Table 3.
4It should be noted some ?summations? were done to the
CFGs in a preprocessing step, thus affecting all subsequent pro-
cessing. All variants of NN (NN, NNS, NNP) were collapsed
into N and all of VB (VB, VBD, VBZ, etc.) into V. Unaligned
words and punctuation were also deleted and the affected rules
collapsed.
688
Table 4: Confusion Matrix for the Word Order Types
Word # of System Prediction
order languages SVO SOV VSO VOS
SVO 46 32 8 0 6
SOV 39 2 33 0 4
VSO 11 2 2 3 4
VOS 1 0 0 0 1
Table 5: Word Order Accuracy for 97 languages
# of IGT instances Average Accuracy
100+ 100%
40-99 99%
10-39 79%
5-9 65%
3-4 44%
1-2 14%
6.2 Experiment 2 Results - Word Order for 97
Languages
The second experiment sought to assign values for the
WOrder parameter for 97 languages. For this experiment,
a CFG with functional tags was built for each language,
and the WOrder algorithm was applied to each language?s
CFG. The confusion matrix in Table 4 shows the number
of correct and incorrect assignments. SVO and SOV were
assigned correctly most of the time, whereas VSO pro-
duced significant error. This is mostly due to the smaller
sample sizes for VSO languages: of the 11 VSO lan-
guages in our survey, over half had sample sizes less than
10 IGT instances; of those with instance counts above 70
(two languages), the answer was correct.
6.3 Error Analysis
There are four main types of errors that affected our sys-
tem?s performance:
? Insufficient data ? Accuracy of the parameters was
affected by the amount of data available. For the
WOrder parameter, for instance, the number of in-
stances is a good predictor of the confidence of the
value returned. The accuracy of the WOrder param-
eter drops off geometrically as the number of in-
stances approaches zero, as shown in Table 5. How-
ever, even with as few as 4-8 instances, one can ac-
curately predict WOrder?s value more than half the
time. For other parameters, the absence of crucial
constituents (e.g., Poss, PRP$) did not allow us to
generate a value.
? Skewed or inaccurate data ? Depending on the num-
ber of examples and source documents, results could
be affected by the IGT bias. For instance, although
Cantonese (YUH) is a strongly SVO language and
ODIN contains 73 IGT instances for the language,
our system determined that Cantonese was VOS.
This resulted from a large number of skewed exam-
ples found in just one paper.
? Projection errors ? In many cases, noise was intro-
duced into the CFGs when the word aligner or pro-
jction algorithm made mistakes, potentially intro-
ducing unaligned constituents. These were subse-
quently collapsed out of the CFGs. The absent con-
stituents sometimes led to spurious results when the
CFGs were later examined.
? Free constituent order ? Some languages have freer
constituent order than others, making calculation of
particular parametric values difficult. For example,
Jingulu (JIG) and German (GER) alternate between
SVO and SOV. In both cases, our grammars directed
us to an order that was opposite our gold standard.
7 Discussion
7.1 Data
In examining Table 5, the reader might question why it
is necessary to have 40 or more sentences of parsed lan-
guage data in order to generalize the word order of a lan-
guage with a high degree of confidence. After all, anyone
could examine just one or two examples of parsed En-
glish data to discern that English is SVO, and be nearly
certain to be right. There are several factors involved.
First, a typological parameter like WOrder is meant to
represent a canonical characteristic of the language; all
languages exhibit varying degrees of flexibility in the or-
dering of constituents, and discovering the canonical or-
der of constituents requires accumulating enough data for
the pattern to emerge. Some languages might require
more instances of data to reach a generalization than oth-
ers precisely because they might have freer word order.
English has a more rigid word order than most, and thus
would require less data.
Second, the data we are relying on is somewhat
skewed, resulting from the IGT bias. We have to collect
sufficient amounts of data and from enough sources to
counteract any linguist-based biases introduced into the
data. It is also the case that not all examples are full
sentences. A linguist might be exploring the structure of
noun phrases for instance, and not provide full sentences.
Third, we are basing our analyses on projected struc-
tures. The word alignment and syntactic projections are
not perfect. Consequently, the trees generated, and the
rules read off of them, may be incomplete or inaccurate.
7.2 Relevance to NLP
Our efforts described here were inspired by some re-
cent work on low-density languages (Yarowksy and
Ngai, 2001; Maxwell and Hughes, 2006; Drabek and
Yarowsky, 2006). Until fairly recently, almost all NLP
work was done on just a dozen or so languages, with the
689
vast majority of the world?s 6,000 languages being ig-
nored. This is understandable, since in order to do seri-
ous NLP work, a certain threshold of corpus size must
be achieved. We provide a means for generating small,
richly annotated corpora for hundreds of languages using
freely available data found on the Web. These corpora
can then be used to generate other electronic resources,
such as annotated corpora and associated NLP tools.
The recent work of (Haghighi and Klein, 2006) and
(Quirk et al, 2005) were also sources of inspiration. In
the former case, the authors showed that it is possible to
improve the results of grammar induction over raw cor-
pora if one knows just a few facts about the target lan-
guage. The ?prototypes? they describe are very similar to
the our constituent order parameters, and we see our work
as an incremental step in applying grammar induction to
raw corpora for a large number of languages.
Quirk et al2005 demonstrates the success of using
fragments of a target language?s grammar, what they call
?treelets?, to improve performance in phrasal translation.
They show that knowing even a little bit about the syntax
of the target language can have significant effects on suc-
cess of phrasal-based MT. Our parameters are in some
ways similar to the treelets or grammar fragments built
by Quirk and colleagues and thus might be applicable to
phrasal-based MT for a larger number of languages.
Although the reader might question the utility of using
enriched IGT for discovering the values of typological
parameters, since the ?one-off? nature of these discover-
ies might argue for using existing grammars (e.g., WALS)
over harvesting and enriching IGT. However, it is impor-
tant to recognize that the parameters that we specify in
this paper are only a sample of the potential parameters
that might be recoverable from enriched IGT. Further,
because we are effectively building PCFGs for the lan-
guages we target, it is possible to provide gradient values
for various parameters, such as the degree of word order
variability in a language (e.g., SVO 90%, SOV 10%), the
potential for which we not explicitly explored in this pa-
per. In addition, IGT exists in one place, namely ODIN,
for hundreds of languages, and the examples that are har-
vested are also readily available for review (not always
the case for grammars).
8 Conclusion
We demonstrate a method for discovering interesting and
computationally relevant typological features for hun-
dreds of the world?s languages automatically using freely
available language data posted to the Web. We demon-
strate that confidence increases as the number of data
points increases, overcoming the IGT and English biases.
Inspired by work that uses prototypes and grammar frag-
ments, we see the work we describe here as being quite
relevant to the growing body of work on languages whose
digital footprint is much smaller than the ten or so major-
ity languages of the world.
References
John Frederick Bailyn. 2001. Inversion, dislocation and option-
ality in russian. In Gerhild Zybatow, editor, Current Issues
in Formal Slavic Linguistics.
B. Comrie. 1989. Language Universals and Linguistic Typol-
ogy: Syntax and Morphology. Blackwell, Oxford.
William Croft. 1990. Typology and Universals. Cambridge
University Press, New York.
Elliott Franco Drabek and David Yarowsky. 2006. Induction
of fine-grained part-of-speech taggers via classifier combi-
nation and crosslingual projection. In Proceedings of COL-
ING/ACL2006 Workshop on Frontiers in Linguistically An-
notated Corpora.
Joseph H. Greenberg. 1963. Some universals of grammar with
particular reference to the order of meaningful elements. In
Joseph H. Greenberg, editor, Universals of Language, pages
73?113. MIT Press, Cambridge, Massachusetts.
Aria Haghighi and Dan Klein. 2006. Protoype-driven sequence
models. In Proceedings of HLT-NAACL, New York City, NY.
Martin Haspelmath, Matthew S. Dryer, David Gil, and Bernard
Comrie. 2005. The World Atlas of Language Structures.
Oxford University Press, Oxford, England.
William D. Lewis. 2006. ODIN: A Model for Adapting and
Enriching Legacy Infrastructure. In Proceedings of the e-
Humanities Workshop, Amsterdam. Held in cooperation
with e-Science 2006: 2nd IEEE International Conference on
e-Science and Grid Computing.
Mike Maxwell and Baden Hughes. 2006. Frontiers in linguistic
annotation for lower-density languages. In Proceedings of
COLING/ACL2006 Workshop on Frontiers in Linguistically
Annotated Corpora.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Depen-
dency tree translation: Syntactically informed phrasal smt.
In Proceedings of ACL 2005.
Fei Xia and William D. Lewis. 2007. Multilingual structural
projection across interlinearized text. In Proceedings of the
North American Association of Computational Linguistics
(NAACL) conference.
David Yarowksy and Grace Ngai. 2001. Inducing multilingual
pos taggers and np bracketers via robust projection across
aligned corpora. In Proceedings of NAACL-2001, pages
377?404.
690
Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage,
Social Sciences, Humanities, and Education ?LaTeCH ? SHELT&R 2009, pages 51?59,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Applying NLP Technologies to the Collection and Enrichment
of Language Data on the Web to Aid Linguistic Research
Fei Xia
University of Washington
Seattle, WA 98195, USA
fxia@u.washington.edu
William D. Lewis
Microsoft Research
Redmond, WA 98052, USA
wilewis@microsoft.com
Abstract
The field of linguistics has always been
reliant on language data, since that is its
principal object of study. One of the major
obstacles that linguists encounter is find-
ing data relevant to their research. In this
paper, we propose a three-stage approach
to help linguists find relevant data. First,
language data embedded in existing lin-
guistic scholarly discourse is collected and
stored in a database. Second, the lan-
guage data is automatically analyzed and
enriched, and language profiles are created
from the enriched data. Third, a search
facility is provided to allow linguists to
search the original data, the enriched data,
and the language profiles in a variety of
ways. This work demonstrates the benefits
of using natural language processing tech-
nology to create resources and tools for
linguistic research, allowing linguists to
have easy access not only to language data
embedded in existing linguistic papers, but
also to automatically generated language
profiles for hundreds of languages.
1 Introduction
Linguistics is the scientific study of language, and
the object of study is language, in particular lan-
guage data. One of the major obstacles that lin-
guists encounter is finding data relevant to their
research. While the strategy of word of mouth
or consulting resources in a library may work for
small amounts of data, it does not scale well. Val-
idating or reputing key components of a linguistic
theory realistically requires analyzing data across
a large sample of languages. For instance, in lin-
guistic typology a well-known implicational uni-
versal states that if the demonstrative follows the
noun, then the relative clause also follows the noun
(Croft, 2003). Although this particular universal
is well-researched and widely accepted, identify-
ing this tendency anew?as an example of what
one must do when researching a new universal?
would require a significant amount of work: in or-
der to be relatively sure that the universal holds,
the linguist would need to identify a substantial
number of true positives (those that support the
universal), and ensure that there are not a sufficient
number of negatives that would act as a refutation.
The only way a linguist could be completely sure
would be to conduct a thorough literature review
on the subject or go through data from a repre-
sentative and significant sample of data from the
approximately seven thousand languages that are
or have been spoken (and for which data exists).
There have been much effort by the linguistic
community to address the issue. For instance,
LinguistList compiles a long list of linguistic re-
sources1, making it easier to find electronically
available resources. Likewise, the Open Language
Archives Community (OLAC) acts as an online
virtual library of language resources, and provides
a search tool that searches several dozen online
linguistic resources. Further, the World Atlas of
Language Structures (WALS), which was recently
made available online, is a large database of struc-
tural (phonological, grammatical, lexical) proper-
ties of languages gathered from descriptive mate-
rials (Haspelmath et al, 2005).2
1http://www.linguistlist.org/langres/index.html
2There are other online resources for searching for lin-
guistic data, in particular typological data. Two of note in-
clude Autotyp (Bickel and Nichols, 2002) and the Typologi-
cal Database System (Dimitriadis et al, forthcoming), among
others. The former has limited online availability (much of
51
We propose a three-stage approach to help lin-
guists in locating relevant data. First, language
data embedded in existing linguistic scholarly dis-
course is collected and stored in a database. Sec-
ond, the language data is automatically analyzed
and enriched and language profiles are created
from the enriched data. Third, a search facility is
provided to allow linguists to search the original
data, the enriched data, and the language profiles.
This is an on-going research project. While the
first stage is completed, the second and third stages
are partially completed and still undergoing devel-
opment. In this paper, we will describe each stage
and report results.
2 Related work
In this section, we briefly discuss a few projects
that are most relevant to our work.
2.1 Ethnologue
The purpose of the Ethnologue is to provide a
comprehensive listing of the known living lan-
guages of the world. The most recent version, ver-
sion 15, covers more than six thousand languages.
Information in the Ethnologue comes from numer-
ous sources and is confirmed by consulting both
reliable published sources and a network of field
correspondents, and has been built to be consistent
with ISO standard 639-3; the information is com-
piled under several specific categories (e.g., coun-
tries where a language is spoken and their popula-
tions) and no effort is made to gather data beyond
those categories (Gordon, 2005).
2.2 WALS
The World Atlas of Language Structures (WALS)
is a large database of structural (phonologi-
cal, grammatical, lexical) properties of languages
gathered from descriptive materials (such as refer-
ence grammars) by a team of more than 40 lin-
guists (Haspelmath et al, 2005). WALS con-
sists of 141 maps with accompanying text on
diverse features (such as vowel inventory size,
noun-genitive order, passive constructions, and
hand/arm polysemy). Each map corresponds to
a feature and the map shows the feature values
for between 120 and 1370 languages. Altogether
there are 2,650 languages and more than 58,000
the data is not directly accessible through query, but requires
submitting requests to the site owners), however, and the lat-
ter is still under development.
data points; each data point is a (language, fea-
ture, feature value) tuple that specifies the value of
the feature in a particular language. For instance,
(English, canonical word order, SVO) means that
the canonical word order of English is SVO.
2.3 OLAC
The Open Languages Archive Community
(OLAC), described in (Bird and Simons, 2003),
is part of the Open Archives Initiative, which
promotes interoperability standards for linguistic
data.3 The focus of OLAC has been to facilitate
the discovery of linguistic resources through a
common metadata structure for describing digital
data and by providing a common means for locat-
ing these data through search interfaces housed at
Linguist List and the Linguistics Data Consortium
(LDC). Our work shares with OLAC the need
for resource discovery, and moves beyond OLAC
by enriching and manipulating the content of
linguistic resources.
3 Building ODIN
The first stage of the three-stage approach is to col-
lect linguistic data and store it in a database. In lin-
guistics, the practice of presenting language data
in interlinear form has a long history, going back
at least to the time of the structuralists. Interlinear
Glossed Text, or IGT, is often used to present data
and analysis on a language that the reader may
not know much about, and is frequently included
in scholarly linguistic documents. The canonical
form of an IGT consists of three lines: a lan-
guage line for the language in question, a gloss
line that contains a word-by-word or morpheme-
by-morpheme gloss, and a translation line, usually
in English. The grammatical markers such as 3sg
on the gloss line are called grams. Table 1 shows
the beginning of a linguistic document (Baker and
Stewart, 1996) which contains two IGTs: one in
lines 30-32, and the other in lines 34-36. The line
numbers are added for the sake of convenience.
ODIN, the Online Database of INterlinear text,
is a resource built from data harvested from schol-
arly documents (Lewis, 2006). ODIN was built in
three main steps:
(1) Crawling: crawling the Web to retrieve docu-
ments that may contain IGTs
3http://www.language-archives.org/
52
1: THE ADJ/VERB DISTINCTION: EDO EVIDENCE
2:
3: Mark C. Baker and Osamuyimen Thompson Stewart
4: McGill University
....
27: The following shows a similar minimal pair from Edo,
28: a Kwa language spoken in Nigeria (Agheyisi 1990).
29:
30: (2) a. E`me`ri? mo`se?.
31: Mary be.beautiful(V)
32: ?Mary is beautiful.?
33:
34: b. E`me`ri? *(ye?) mo`se?.
35: Mary be.beautiful(A)
36: ?Mary is beautiful (A).?
...
Table 1: A linguistic document that contains IGT:
words in boldface are language names
(2) IGT detection: extracting IGTs from the re-
trieved documents
(3) Language ID: identifying the language code
of the extracted IGTs.
The identified IGTs are then extracted and
stored in a database (the ODIN database), which
can be easily searched with a GUI interface.4 In
this section, we briefly describe the procedure, and
more detail about the procedure can be found in
(Xia and Lewis, 2008) and (Xia et al, 2009).
3.1 Crawling
In the first step, linguistic documents that may
contain instances of IGT are harvested from the
Web using metacrawls. Metacrawling involves
throwing queries against an existing search en-
gine, such as Google and Live Search, and crawl-
ing only the pages returned by those queries. We
found that the most successful queries were those
that used strings contained within IGT itself (e.g.
grams such as 3sg). In addition, we found pre-
cision increased when we included two or more
search terms per query, with the most successful
queries being those which combined grams and
language names.
Other queries we have developed include:
queries by language names and language codes
(drawn from the Ethnologue database (Gordon,
2005), which contains about 40,000 language
names and their variants), by linguists names and
the languages they work on (drawn from the Lin-
guist Lists linguist database), by linguistically rel-
4http://odin.linguistlist.org
evant terms (drawn from the SIL linguistic glos-
sary), and by particular words or morphemes
found in IGT and their grammatical markup.
3.2 IGT detection
The canonical form of IGT consists of three parts
and each part is on a single line. However, many
IGT instances, 53.6% of instances in ODIN, do not
follow the canonical form for various reasons. For
instance, some IGTs are missing gloss or trans-
lation lines as they can be recovered from con-
text (e.g., other neighboring examples or the text
surrounding the instance); some IGTs have multi-
ple translations or language lines (e.g., one part in
the native script, and another in a latin translitera-
tion); still others contain additional lines of anno-
tation and analysis, such as phonological alterna-
tions, underlying forms, etc.
We treat IGT detection as a sequence labeling
problem. First, we train a learner and use it to label
each line in a document with a tag in a pre-defined
tagset. The tagset is an extension of the standard
BIO tagging scheme and it has five tags: they are
BL (any blank line), O (outside IGT that is not a
BL), B (the first line in an IGT), E (the last line in
an IGT), and I (inside an IGT that is not a B, E, or
BL). After the lines in a document are tagged by
the learner, we identify IGT instances by finding
all the spans in the document that match the ?B [I
| BL]* E? pattern; that is, the span starts with a B
line, ends with an E line, and has zero or more I or
BL lines in between.
To test the system, we manually annotated 51
documents to mark the positions of the IGTs. We
trained the system on 41 documents (with 1573
IGT instances) and tested it on 10 documents (with
447 instances). The F-score for exact match (i.e.,
two spans match iff they are identical) was 88.4%,
and for partial match (i.e., two spans match iff they
overlap), was 95.4%. The detail of the system can
be found in (Xia and Lewis, 2008).
3.3 Language ID
The language ID task here is very different from a
typical language ID task. For instance, the num-
ber of languages in ODIN is more than a thou-
sand and could potentially reach several thousand
as more data is added. Furthermore, for most lan-
guages in ODIN, our training data contains few
to no instances of IGT. Because of these proper-
ties, applying existing language ID algorithms to
the task does not produce satisfactory results. For
53
instance, Cavnar and Trenkle?s N-gram-based al-
gorithm produced an accuracy of as high as 99.8%
when tested on newsgroup articles in eight lan-
guages (Cavnar and Trenkle, 1994). However,
when we ran the same algorithm on the IGT data,
the accuracy fell as low as 2% when the training
set was very small.
Since IGTs are part of a document, there are of-
ten various cues in the document (e.g., language
names) that can help predict the language ID of
these instances. We treat the language ID task as
a coreference resolution (CoRef) problem: a men-
tion is an IGT or a language name appearing in a
document, an entity is a language code, and find-
ing the language code for an IGT is the same as
linking a mention (e.g., an IGT) to an entity (i.e.,
a language code).5 Once the language ID task is
framed as a CoRef problem, all the existing algo-
rithms on CoRef can be applied to the task.
We built two systems: one uses a maximum en-
tropy classifier with beam search, which for each
(IGT, language code) pair determines whether the
IGT should be linked to the language code; the
other treats the task as a joint inference task and
performs the inference by using Markov Logic
Network (Richardson and Domingos, 2006). Both
systems outperform existing, general-purpose lan-
guage identification algorithms significantly. The
detail of the algorithm and experimental results is
described in (Xia et al, 2009).
3.4 The current ODIN database
We ran the IGT detection and language ID systems
on three thousand IGT-bearing documents crawled
from the Web and the extracted IGTs were stored
in the ODIN database. Table 2 shows the language
distribution of the IGT instances in the database
according to the output of the language ID sys-
tem. For instance, the third row says that 122
languages each have 100 to 999 IGT instances,
and the 40,260 instances in this bin account for
21.27% of all instances in the ODIN database.6
In addition to the IGTs that are already in the
5A language code is a 3-letter code that uniquely identi-
fies a language. In contrast, the mapping between language
name and a language is not always one-to-one: some lan-
guages have multiple names, and some language names map
to multiple languages.
6Some IGTs are marked by the authors as ungrammatical
(usually with an asterisk ?*? at the beginning of the language
line). These IGTs are kept in ODIN because they may contain
information useful to linguists (for the same reason that they
were included in the original linguistic documents).
Table 2: Language distribution of the IGTs in
ODIN
Range of # of # of IGT % of IGT
IGT instances languages instances instances
> 10000 3 36,691 19.39
1000-9999 37 97,158 51.34
100-999 122 40,260 21.27
10-99 326 12,822 6.78
1-9 838 2,313 1.22
total 1326 189,244 100
ODIN database, there are more than 130,000 ad-
ditional IGT-bearing documents that have been
crawled but have not been fully processed. Once
these additional documents have been processed,
the database is expected to expand significantly,
growing to a million or more IGT instances.
4 Analyzing IGT data and creating
language profiles
The second stage of the three-stage approach is
to analyze and enrich IGT data automatically, to
extract information from the enriched data, and
to create so-called language profiles for the many
languages in the database. A language profile de-
scribes the main attributes of a language, such
as its word order, case markers, tense/aspect,
number/person, major syntactic phenomena (e.g.,
scrambling, clitic climbing), etc.7
An example profile is shown below. The pro-
file says that in Yoruba the canonical word or-
der is SVO, determiners appear after nouns, and
the language has Accusative case, Genitive case,
Nominative case, and so on. The concepts such as
AccusativeCase come from the GOLD Ontology
(Farrar, 2003; Farrar and Langendoen, 2003).
<Profile>
<language code="WBP">Yoruba</language>
<ontologyNamespace prefix="gold">
http://linguistic-ontology.org/gold.owl#
</ontologyNamespace>
<feature="word_order"><value>SVO</value></feature>
<feature="det_order"><value>NN-DT</value></feature>
<feature="case">
<value>gold:AccusativeCase</value>
<value>gold:GenitiveCase</value>
<value>gold:NominativeCase</value>
. . .
</Profile>
Given a set of IGT examples for a language, the
procedure for building a profile for the language
has several steps:
(1) Identifying and separating out various fields
7A thorough discussion on the definition and content of
language profiles is beyond the scope of the paper. The reader
is referred to (Farrar and Lewis, 2006) for more discussion on
the topic.
54
(language data, gloss, translation, citation,
construction name, etc.) in an IGT.
(2) Enriching IGT by processing the translation
line and projecting the information onto the
language line.
(3) Identifying grams in the gloss line and map-
ping them to the concepts defined in GOLD
Ontology or the like.
(4) Answering questions in the language profile.
In this section, we explain each step and report
some preliminary results.
4.1 Identifying fields in IGT
In addition to the language data (L), gloss (G), and
translation (T) parts of IGT, an IGT often contains
other information such as language name (-LN),
citation (-AC), construction names (-CN), and so
on. An example is in (1), in which the first line
contains the language name and citation,8 the third
line includes coindexes i and i/j, and the last two
lines show two possible translations of the sen-
tence. Here, the language line is displayed as two
lines due to errors made by the off-the-shelf con-
verter that converted the crawled pdf documents
into text.
(1) Haitian CF (Lefebvre 1998:165)
ak
Jani pale lii/j
John speak with he
(a) ?John speaks with him? (b) ?John
speaks with himself?
The goal of this step is to separate out differ-
ent fields in an IGT, fix display errors caused by
the pdf-to-text converter, and store the results in a
uniform data structure such as the one in Ex (2)
for the example in Ex (1). The task is not trivial
partially because the IGT detector marks only the
span of an instance. For instance, the coindex i in
Jani and lii/j on the third line of Ex (1) could easily
be mistaken as being part of the word.
(2) Language: Haitian CF
Citation: (Lefebvre 1998:165)
L: Jan pale ak li
Coindx: (Jan, i), (li, i/j)
G: John speak with he
T1: ?John speaks with him?
T2: ?John speaks with himself?
There has been much work on extracting
database records from text or semi-structured
sources, and the common approach is breaking
the text into multiple segments and labeling each
segment with a field name (e.g., (Wellner et al,
2004; Grenager et al, 2005; Poon and Domingos,
8CF here stands for French-lexified creole.
2007)). Our task here is slightly different from
their tasks (e.g., extracting author/title/journal
from citations) in that the fields in IGT could over-
lap9 and corrupted lines need to be re-constructed
and re-stored in a particular way (e.g., pasting the
second and third lines in Ex (1) back together).
Due to the differences, we did not create anno-
tated data by segmenting IGT into separate fields
and labeling each field. Instead, we used a refined
tagset to indicate what information is available at
each line of IGT instances. The tagset includes
six main tags (L, G, T, etc.) and nine secondary
tags (e.g., -CR for corruption and -SY for syntac-
tic information). Each line in each IGT instance is
labeled with one main tag and zero or more sec-
ondary tags. The labeled lines in Ex (1) are shown
in (3).
(3) M-LN-AC: Haitian CF (Lefebvre 1998:165)
L-CR: ak
L-SY-CR: Jani pale lii/j
G: John speak with he
T-DB: (a) ?John speaks with him? (b) ?John
C: speaks with himself?
The labeling of the data is done semi-
automatically. We have created a tool that takes
the IGT spans produced by the current IGT detec-
tor and labels IGT lines by using various cues in
an IGT instance, and designed a GUI that allows
annotators to correct the system output easily. The
annotation speed is about 320 IGT instances per
hour on average. We are currently experimenting
with different ways of re-training the IGT detector
with the new data.
We have built a rule-based module that identi-
fies fields in IGT using the enriched tagset (i.e.,
creating Ex (2) from Ex (3)), relying on the knowl-
edge about the conventions that linguists tend
to follow when specifying citations, construction
names, coindexation and the like. The initial re-
sult of field extraction looks promising. We are
also studying whether existing unsupervised sta-
tistical systems for information extraction (e.g.,
(Poon and Domingos, 2007)) could be extended
to handle this task while taking advantage of the
enriched tagset for IGTs. We plan to complete the
study and report the results in the near future.
4.2 Enriching IGT
Since the language line in IGT data typically does
not come with annotations (e.g., POS tags, phrase
9For instance, in some IGTs, a syntactic structure is added
on top of the language line; for instance, the language line in
Ex (1) could become something like [IP Jani [VP pale [PP
ak lii/j]]]
55
structures), we developed a method to enrich IGT
data and then extract syntactic information (e.g.,
context-free rules) to bootstrap NLP tools such
as POS taggers and parsers. The enrichment al-
gorithm first parses the English translation with
an English parser, then aligns the language line
and the English translation via the gloss line, and
finally projects syntactic information (e.g., POS
tags and phrase structures) from English to the lan-
guage line. For instance, given the IGT example in
Ex (4), the enrichment algorithm would produce
the word alignment in Figure 1 and the phrase
structures in Figure 2. The algorithm was tested
on 538 IGTs from seven languages and the word
alignment accuracy was 94.1% and projection ac-
curacy (i.e., the percentage of correct links in the
projected dependency structures) was 81.5%. De-
tails of the algorithm and the experiments are dis-
cussed in (Xia and Lewis, 2007).
(4) Rhoddodd yr athro lyfr i?r bachgen ddoe
gave-3sg the teacher book to-the boy yesterday
??The teacher gave a book to the boy yesterday??
(Bailyn, 2001)
The   t eache r   gave   a   book   t o     t he     boy    yes te rday   
Rhoddodd   y r    a th ro      l y f r      i ? r      bachgen   ddoe     
 G loss  l i ne :
 T r a n s l a t i o n :
T a r g e t  l i n e :
g a v e - 3 s g   t h e   t e a c h e r  b o o k   t o - t h e   b o y    y e s t e r d a y
Figure 1: Aligning the language line and the En-
glish translation with the help of the gloss line
S
N P 1 V P
N N
t e a c h e r
V B D
  g a v e
N P 2
D T
a  
N P 4P P
N N
t h e
I N N P 3
y e s t e r d a y
N N
D T
b o o k
N N
b o y
D T
t o
S
N P
N N
V B D
N P N PP P
N N
I N + D T
N N
N ND T
  r h o d d o d d
  ( g a v e )   y r
( t he )
   a th ro
( t e a c h e r )
ly f r
( b o o k )
    i ?r
( t o - t he )
b a c h o g e n
   (boy )   d d o e
( y e s t e r d a y )
T h e
Figure 2: Projecting phrase structure from the
translation line to the language line
4.3 Identifying and mapping grams
The third step of Stage 2 identifies grams on the
gloss line of an IGT and mapping them to some
common semantic so that they can reliably be
searched. The gloss line of IGT has two types of
glosses: those representing grammatical informa-
tion (grams) such as NOM, 3sg, PERF, and stan-
dard glosses such as book or give. Early work in
ODIN involved significant manual effort to map
grams to GOLD concepts.10
10See (Lewis, 2006) for more background on mapping
grams to GOLD concepts, and (Farrar, 2003) and (Farrar and
The base of several hundred manually mapped
grams has provided a reasonably reliable ?seman-
tic search? facility in ODIN, which allows lin-
guists to find instances with particular kinds of
markup. For example, searching for Perfective
Aspect finds instances of data where the data was
marked up with PERF, PFV, etc., but also excludes
instances that map to ?Perfect Tense?. While
the manually created mapping table covers many
common grams, it is far from complete, especially
since linguists can coin new grams all the time.
We are currently automating the mapping by using
the grams in the table as labeled data or seeds and
classifying new grams using supervised or semi-
supervised methods. This work, however, is still
too preliminary to be included in this paper.
4.4 Answering questions in language profiles
The final step of Stage 2 is answering questions in
language profiles. Some questions are easier to an-
swer than others. For instance, to determine what
grammatical or lexical cases are available in a lan-
guage according to the data in ODIN, we simply
need to look at the grams in the data that map to the
case category in GOLD. Other questions are more
complex; for instance, to determine whether mul-
tiple wh-questions are allowed in a language, we
need to examine the projected syntactic structure
for the language line and look for the positions of
any wh-words that were projected relative to one
another. A case study is reported next.
4.5 A case study: Answering typological
questions
Two biases are prevalent in IGT data, due to the
opportunistic way in which it is harvested and en-
riched: The first is what we call the IGT-bias, that
is, the bias produced by the fact that IGT examples
are used by authors to illustrate a particular fact
about a language, causing the collection of IGT for
the language to suffer from a potential lack of rep-
resentativeness. The second we call the English-
bias, an English-centrism resulting from the fact
that most IGT examples provide an English trans-
lation which is used to enrich the language line:
as discussed in Section 4.2, the enrichment algo-
rithm assigns a parse tree to the English transla-
tion which is then projected onto the langauge line.
Since the original parse is built over English data,
the projected parse suffers from a bias caused by
Langendoen, 2003) for more detailed background on GOLD.
56
the English source. Because of these biases and er-
rors introduced at various stages of processing, au-
tomatically generated language profiles and asso-
ciated examples should be treated as preliminary
and unattested, subject to verification by the lin-
guist. The question is how reliable the profiles are.
To answer the question, we ran a case study in
which we evaluated the accuracy of our system in
answering a number of typological questions, such
as the canonical order of constituents (e.g., sen-
tential word order, order of constituents in noun
phrases) or the existence of particular constituents
in a language (e.g., determiners). The list of ques-
tions and their possible answers are shown in Ta-
ble 3 (the WALS # is a reference number used in
WALS (Haspelmath et al, 2005) which uniquely
identifies each typological parameter).
In one experiment, we automatically found the
answer to the canonical word order question by
looking at the context free rules extracted from
enriched IGT data. When tested on about 100
languages, the accuracy was 99% for all the lan-
guages with at least 40 IGT instances.12 Not sur-
prisingly, the accuracy decreased for languages
with fewer instances (e.g., 65% for languages with
5-9 IGTs). In another experiment, our system an-
swered all the 13 typological questions in Table 3
for 10 languages and the accuracy was 83.1% on
average across the questions.
This study shows that, despite potential biases
and errors, we can automatically discover certain
kinds of linguistic knowledge from IGT with rea-
sonable accuracy and the accuracy increases as
more data becomes available. The language pro-
files built this way could serve as a complement to
manually crafted resources such as WALS.
4.6 Comparison with WALS
The task is similar to the goal of the WALS
project. In fact, the morphological and syntactic
features in WALS form the initial attribute set for
our language profiles.13 The main difference be-
tween WALS and our approach is that the informa-
tion in WALS (including features, feature values,
and data points) was gathered by a team of more
12Some IGT instances are not sentences and therefore are
not useful for answering this question. Further, those in-
stances marked as ungrammatical (usually with an asterisk
?*?) are ignored for this and all typological questions.
13WALS uses the term feature to refer to a property such as
canonical word order. Since feature in NLP has a very differ-
ent meaning, in this paper we use the term attribute instead
to avoid potential confusion.
than 40 linguists, many of them the leading author-
ities in the field. In contrast, the language profiles
in our work are created automatically from oppor-
tunistically harvested and enriched linguistic data
found on the Web (essentially the IGT in ODIN).
Another difference is that our language profiles
also include highly language-specific information
(e.g., lists of language-specific syntactic construc-
tions, such as bei- and ba- constructions in Man-
darin), as discussed in harvested documents. The
information is gathered by checking the construc-
tion names included in and surrounding IGT.
The benefits of our approach are twofold. First,
we can build language profiles for hundreds of
languages with little human effort and the lan-
guage profiles can be updated whenever the ODIN
database is expanded or enriched. Second, each
entry in the language profile in ODIN is linked to
the relevant IGT instances that are used to answer
the question. For instance, a language profile not
only lists the canonical word order of the language
but also IGT instances from which this informa-
tion is derived.
5 Extending the search facility
The last stage of the three-stage approach is to pro-
vide a search facility for linguists to search the
original IGTs, the enriched IGTs and the automat-
ically created language files. The current search
interface for ODIN allows a variety of search op-
tions, including search by language name or code,
language family, and by grams and their related
concepts (e.g., Accusative case). Once data is dis-
covered that fits a particular pattern that a user is
interested in, he/she can either display the data
(where sufficient citation information exists and
where the data is not corrupted by the text-to-
pdf conversion process) or locate documents from
which the data is extracted. Additional search fa-
cilities allow users to search across linguistically
salient structures (?constructions?) and return re-
sults in the form of language data and language
profiles.
The ODIN database also contains thousands
of tree structures for hundreds of languages,
each linked to the English tree structures from
which they were derived. This can provide un-
precedented options for cross-lingual query across
?syntactic structures?.14
14We fully recognize that the projected structures should
be considered highly experimental, due to noise in the pro-
57
Table 3: Thirteen typlogical questions tested in the case study (ndo=no dominant order, nr=not relevant)
Label WALS # Description Possible Values
Word Order
WOrder 330 Order of Words in a sentence SVO,SOV,VSO,VOS,OVS, OSV,ndo11
V+OBJ 342 Order of the Verb, Object and Oblique Object (e.g., PP) VXO,VOX,OVX,OXV,XVO,XOV,ndo
DT+N N/A Order of Nouns and Determiners (a, the) DT-N, N-DT, ndo, nr
Dem+N 358 Order of Nouns and Demonstrative Determiners Dem-N, N-Dem, ndo, nr
JJ+N 354 Order of Adjectives and Nouns JJ-N, N-JJ, ndo
PRP$+N N/A Order of possessive pronouns and nouns PRP$-N, N-PRP$, ndo, nr
Poss+N 350 Order of Possessive NPs and nouns NP-Poss, NP-Poss, ndo, nr
P+NP 346 Order of Adpositions and Nouns P-NP, NP-P, ndo
Morpheme Order
N+num 138 Order of Nouns and Number Inflections (Sing, Plur) N-num, num-N, ndo
N+case 210 Order of Nouns and Case Inflections N-case, case-N, ndo, nr
V+TA 282 Order of Verbs and Tense/Aspect Inflections V-TA, TA-V, ndo, nr
Existence Tests
Def 154 Do definite determiners exist? Yes, No
Indef 158 Do indefinite determiners exist? Yes, No
We plan to extend the current query facility in
three steps to allow these structure-based queries.
The first step is to do a user study and identify the
types of queries that linguists would be interested
in. We have already consulted with a number of
syntacticians and other linguists, and have com-
piled a list of ?constructions? that would be of the
most interest, and plan to consult with more lin-
guists to extend this list.15 Some of the initial con-
struction queries have already been implemented
in ODIN as ?prototypes? for testing purposes. The
second step is to identify tools that would facili-
tate implementing these queries. One such tool is
tgrep2,16 which is widely used to search treebank
style phrase structures. Since the tool is robust and
widely used and supported, we plan to extend it
to handle the rich data structures found in the en-
riched IGT data. The third step is to write a large
set of queries in tgrep2 (or other query languages)
that ?pre-package? the most desirable queries into
a form that can be easily executed as a Web ser-
vice, and design a Web GUI that provides the most
accessibility to these queries.
6 Conclusion
One of the major obstacles that linguists encounter
is finding data relevant to their research. In this
paper, we outline a three-stage procedure to allevi-
ate the problem. First, language data embedded in
jection algorithms, and the resulting structures still need to
be reviewed by the linguist throwing the query. However, our
case study demonstrates the reasonably high accuracy of an-
swering typological questions with even very limited supplies
of data. This supports their utility in spite of noise and error.
15A similar study was discussed in (Soehn et al, 2008).
16http://tedlab.mit.edu/? dr/TGrep2/
existing linguistic scholarly discourse is collected
and stored in the ODIN database. Second, the
language data is automatically analyzed and en-
riched, and language profiles are created from the
enriched data. Our case study shows that knowl-
edge discovery (for the targeted attributes) works
reasonably well with even a small amount of IGT
data. Third, a search facility is provided that al-
lows linguists to search the original data, the en-
riched data, and the language profiles by language
name, language family, and construction names.
There are several directions for future research.
We will improve and thoroughly evaluate the mod-
ule that extracts various fields from IGT. We will
also build more complete language profiles for a
dozen or so languages for which we have suffi-
cient IGT data and linguistic knowledge to ade-
quately evaluate the results. Finally, we are ex-
ploring ways of extending the query facility (e.g.,
using tgrep2) to allow sophisticated search on the
original and enriched IGT data, and plan to pro-
vide a GUI with pre-packaged queries which will
be easy for linguists to use.
Acknowledgements This work has been sup-
ported, in part, by NSF grants BCS-0748919 and
BCS-0720670 and a RRF grant from the Univer-
sity of Washington. We would also like to thank
four anonymous reviewers for their valuable com-
ments.
References
John Frederick Bailyn. 2001. Inversion, Dislocation
and Optionality in Russian. In Gerhild Zybatow, ed-
itor, Current Issues in Formal Slavic Linguistics.
58
Mark C. Baker and Osamuyimen Thompson Stewart.
1996. Unaccusativity and the adjective/verb distinc-
tion: Edo evidence. In Proceedings of the Fifth An-
nual Conference on Document Analysis and Infor-
mation Retrieval (SDAIR), Amherst, Mass.
Balthasar Bickel and Johanna Nichols. 2002. Autoty-
pologizing databases and their use in fieldwork. In
Proceedings of the LREC Workshop on Resources
and Tools in Field Linguistics, Las Palmas, Spain,
Jun.
Steven Bird and Gary Simons. 2003. Extending dublin
core metadata to support the description and discov-
ery of language resources. Computers and the Hu-
manities, 17(4):375?388.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Proceedings of
SDAIR-94, 3rd Annual Symposium on Document
Analysis and Information Retrieval, pages 161?175,
Las Vegas, US.
William Croft. 2003. Typology and Universals. Cam-
bridge University Press, Cambridge, England.
Alexis Dimitriadis, Menzo Windhouwer, Adam
Saulwick, Rob Goedemans, and Tams Br. forth-
coming. How to integrate databases without start-
ing a typology war: the typological database sys-
tem. In Simon Musgrave Martin Everaert and Alexis
Dimitriadis, editors, The Use of Databases in Cross-
Linguistic Studies. Mouton de Gruyter, Berlin.
Scott Farrar and D. Terence Langendoen. 2003. A lin-
guistic ontology for the semantic web. GLOT Inter-
national, 7(3):97?100.
Scott Farrar and William D. Lewis. 2006. The
GOLD Community of Practice: An infras-
tructure for linguistic data on the Web. Lan-
guage Resources and Evaluation. Available at
http://faculty.washington.edu/wlewis2/papers/FarLew-
06.pdf.
Scott Farrar. 2003. An ontology for linguistics on the
Semantic Web. Ph.d., University of Arizona, May.
Raymond G. Gordon, editor. 2005. Ethnologue: Lan-
guages of the World. SIL International, Dallas, 15
edition.
T. Grenager, D. Klein, and D. Manning. 2005. Unsu-
pervised learning of field segmentation models for
information extraction. In In Proc. ACL-05.
Martin Haspelmath, Matthew Dryer David Gil, and
Bernard Comrie, editors. 2005. World Atlas of Lan-
guage Structures. Oxford University Press, Oxford.
William Lewis. 2006. ODIN: A Model for Adapting
and Enriching Legacy Infrastructure. In Proc. of the
e-Humanities Workshop, held in cooperation with e-
Science 2006: 2nd IEEE International Conference
on e-Science and Grid Computing, Amsterdam.
Hoifung Poon and Pedro Domingos. 2007. Joint in-
ference in information extraction. In Proceedings
of the Twenty-Second National Conference on Artifi-
cial Intelligence (AAAI), pages 913?918, Vancouver,
Canada. AAAI Press.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine Learning, pages 107?136.
Jan-Philipp Soehn, Heike Zinsmeister, and Georg
Rehm. 2008. Requirements of a user-friendly,
general-purpose corpus query interface. In Pro-
ceedings of the LREC 2008 Workshop Sustainability
of Language Resources and Tools for Natural Lan-
guage Processing, Marrakech, Morocco, May 31.
B. Wellner, A. McCallum, F. Peng, and M. Hay. 2004.
An integrated, conditional model of information ex-
traction and coreference with application to citation
matching. In Proc. of the 20th Conference on Un-
certainty in AI (UAI 2004).
Fei Xia and William Lewis. 2007. Multilingual struc-
tural projection across interlinear text. In Proc. of
the Conference on Human Language Technologies
(HLT/NAACL 2007), pages 452?459, Rochester,
New York.
Fei Xia and William Lewis. 2008. Repurposing
Theoretical Linguistic Data for Tool Development
and Search. In Proc. of the Third International
Joint Conference on Natural Language Processing
(IJCNLP-2008), Hyderabad, India.
Fei Xia, William D. Lewis, and Hoifung Poon. 2009.
Language ID in the Context of Harvesting Language
Data off the Web. In Proceedings of The 12th Con-
ference of the European Chapter of the Association
of Computational Linguistics (EACL 2009), Athens,
Greece, April.
59
Proceedings of NAACL HLT 2007, pages 452?459,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Multilingual Structural Projection across Interlinear Text
Fei Xia
Department of Linguistics
University of Washington
Seattle, WA 98195
fxia@u.washington.edu
William D. Lewis
Department of Linguistics
University of Washington
Seattle, WA 98195
wlewis2@u.washington.edu
Abstract
This paper explores the potential for an-
notating and enriching data for low-density
languages via the alignment and projec-
tion of syntactic structure from parsed data
for resource-rich languages such as English.
We seek to develop enriched resources for a
large number of the world?s languages, most
of which have no significant digital pres-
ence. We do this by tapping the body of
Web-based linguistic data, most of which
exists in small, analyzed chunks embedded
in scholarly papers, journal articles, Web
pages, and other online documents. By har-
vesting and enriching these data, we can
provide the means for knowledge discovery
across the resulting corpus that can lead
to building computational resources such
as grammars and transfer rules, which, in
turn, can be used as bootstraps for build-
ing additional tools and resources for the
languages represented.1
1 Introduction
Developing natural language applications is generally
dependent on the availability of annotated corpora.
Building annotated resources, however, is a signif-
icantly time consuming process involving consider-
able human effort. Although a number of projects
have been undertaken to develop annotated resources
for non-English languages, e.g., treebanks, the devel-
opment of these resources has been no small feat, and
to date have been limited to a very small number of
1We would like to thank Dan Jinguji for creating the
word alignment and source dependency structure gold
standards. Our thanks also go to three anonymous re-
viewers for their helpful comments and suggestions.
the world?s languages (e.g., Chinese, German, Ara-
bic, Korean, etc.). Some notable efforts have been
undertaken to develop automated means for creating
annotated corpora through the projection of annota-
tions (Yarowksy and Ngai, 2001; Xi and Hwa, 2005).
The resulting methods, however, can only be applied
to a small number of language pairs due mostly to
the need for sizeable parallel corpora. Unfortunately,
most languages do not have parallel corpora of suffi-
cient size, making these methods inapplicable for the
vast majority of the world?s languages.
We describe a method for bootstrapping resource
creation by tapping the wealth of multilingual data
on the Web that has been created by linguists. Of
particular note is the linguistic presentation format
of ?interlinear text?, a common format used for pre-
senting language data and analysis relevant to a par-
ticular argument or investigation. Since interlin-
ear examples consist of orthographically or phoneti-
cally encoded language data aligned with an English
translation, the ?database? of interlinear examples
found on the Web, when taken together, constitute a
significant multilingual, parallel corpus covering hun-
dreds to thousands of the world?s languages.
We do not propose that a database of interlin-
ear text alone is sufficient to create NLP resources
and tools, but rather that it may act as a means for
more rapidly developing such tools using less data.
We contend that such a resource allows one to de-
velop computational artifacts, such as grammars and
transfer rules, which can be used as ?seed? knowledge
for building larger resources. In particular, knowing
a little about the structure of a language can help
in developing annotated corpora and tools, since a
little knowledge can go a long way in inducing accu-
rate structure and annotations (Haghighi and Klein,
2006).
Of particular relevance to MT is the issue of struc-
452
tural divergence (Dorr, 1994). Many MT models im-
plicitly make the so-called direct correspondence as-
sumption (DCA) as defined in (Hwa et al, 2002).
However, to what extent that assumption holds is
tested only on a small number of language pairs us-
ing hand aligned data (Fox, 2002; Hwa et al, 2002;
Wellington et al, 2006). A larger sample of typo-
logically diverse language data can help test the as-
sumption for hundreds of languages.
We contend that the knowledge garnered from
structural projections applied to interlinear text can
bootstrap the development of resources and tools
across parallel corpora, where such corpora could be
of smaller size and the resulting tools more robust,
opening the door to the development of tools and re-
sources for a larger number of the world?s languages.
Given the imminent death of half of the world?s 6,000
languages (Krauss, 1992), the development of any
language specific tools for a larger percentage of the
world?s languages than is currently possible can aid
in both their documentation and preservation.
2 Background
The practice of presenting language data in interlin-
ear form has a long history in the field of linguistics,
going back at least to the time of the structuralists
(see (Swanton, 1912) for early examples). The mod-
ern form of interlinear data presentation started to
gel in the mid-1960s, resulting in the canonical three
line form shown in Ex (1), which we will refer to
as Interlinear Glossed Text, or IGT. The canonical
form consists of three lines: a line for the language
in question (often a sentence, which we will refer to
here as the source sentence), an English gloss line,
and an English translation.2
(1) Rhoddodd yr athro lyfr i?r bachgen ddoe
gave-3sg the teacher book to-the boy yesterday
?The teacher gave a book to the boy yesterday?
(Bailyn, 2001)
Although IGT is usually embedded in linguistics
documents as part of a larger analysis, in and of
itself it contains analysis and interesting informa-
tion about the source language. In particular, the
gloss line, which is word and morpheme aligned with
the source, contains word and morpheme transla-
tions for the source language data, and can even con-
tain grammatically salient annotations (e.g., 3sg for
Third Person Singular). Further, the reader will note
2As pointed out by a reviewer, there is a long tradi-
tion in the classical languages for using interlinear trans-
lations. So, too, in other literature bases. Our focus here
is strictly limited to IGT, the interlinear form used in the
field of linguistics.
that many words are shared between the gloss and
translation lines, allowing for the alignment between
these two lines as a intermediate step in the align-
ment between the translation and the source.
An effort is underway to collect these interlinear
snippets into an online searchable database, the pri-
mary purpose of which is to help linguists find ana-
lyzed data for languages they are interested in. We
use this resource, called ODIN, the Online Database
of INterlinear text (Lewis, 2006)3, as our primary
data source. At the time of this writing, ODIN con-
tains 36,439 instances of interlinear data for 725 of
the world?s languages.
3 The Enrichment Algorithm
Our algorithm enriches the original IGT examples by
building syntactic structures over the English data
and then projects these onto the source language
data via word alignment. The term syntactic struc-
ture in this paper refers to both phrase structure (PS)
and dependency structure (DS). The enrichment pro-
cess has three steps:
1. Parse the English translation using an off-the-
shelf parser.
2. Align the source sentence and English transla-
tion with the help of the gloss line.
3. Project the English syntactic structures to ob-
tain the source syntactic structures using word
alignment.
3.1 Parsing English sentences
There are many English parsers available to the pub-
lic, and in this experiment we used Charniak?s parser
(Charniak, 1997), which was trained on the English
Penn Treebank (Marcus et al, 1994). Figure 1(a)
shows a parse tree (in the Penn Treebank style) for
the English translation in Ex (1). Given a parse tree,
we use a head percolation table (Magerman, 1995)
to create the corresponding dependency structure.
Figure 2(a) shows the dependency structure derived
from the parse tree in Figure 1(a).
3.2 Word alignment
Because most of the 700+ languages in ODIN are
low-density languages with no on-line bilingual dic-
tionaries or large parallel corpora, aligning the source
sentence and its English translation directly would
not work well. To take advantage of the unique lay-
out of IGT examples, we propose using the gloss line
as a bridge between the other two lines; that is, we
first align the source sentence and the gloss line, and
then align the gloss line and the English translation.
The process is illustrated in Figure 3.
3The url of ODIN is http://www.csufresno.edu/odin
453
SNP1 VP
NN
teacher
VBD
  gave
NP2
DT
a 
NP4PP
NN
the
IN NP3
yesterday
(a)  English PS 
NN
DT
book
NN
boy
DT
The
to
(b)  Source PS after Step 2
S
NP1 VP
NN
athro
(teacher)
VBD
  rhoddodd
  (gave)
NP2 NP4PP
NN
i?r
IN NP3
ddoe
(yesterday)
NN
DT
lyfr
(book)
NN
bachogen
(boy)
DT
yr
(the)
    i?r
(to-the)
S
NP
NN
VBD
NP NPPP
NN
IN+DT
NN
NN
DT
(c)  Final source PS
  rhoddodd
  (gave) yr
(the)
   athro
(teacher)
lyfr
(book)
    i?r
(to-the)
bachogen
(boy) ddoe
(yesterday)
Figure 1: English PS produced by Charniak?s parser, and source PS projected from the English PS
teacher
gave
a boy
the
book
(a) English DS
the
yesterdayto athro
bachgen
lyfr
yr
ddoei?r
 Rhoddodd
i?r
(b)  Source DS after Step 2
athro
bachgen
lyfr
yr
ddoei?r
 Rhoddodd
(c)  Final source DS
Figure 2: English DS derived from English PS, and source DS projected from the English DS
The      teacher  gave    a     book  to    the    boy   yesterday  
Rhoddodd     yr    athro       lyfr     i?r    bachgen     ddoe    
 Gloss:
 Transatlion:
Source:
gave-3sg    the    teacher   book   to-the  boy   yesterday
Figure 3: Aligning source sentence and English
translation with help of the gloss line
The alignment between the source sentence and
the gloss line is trivial and our preliminary exper-
iments showed that simply using whitespace and
dashes as delimiters, and assuming a one-to-one
alignment produces almost perfect results. In con-
trast, the alignment between the gloss line and the
English translation is more complicated since align-
ment links can cross and words on one side can link
to zero or more words on the other side. We built
two aligners for this stage, as described below.
3.2.1 Statistical word aligner
We create a parallel corpus by using the gloss lines
and the translation lines of all the IGT examples for
all the languages in ODIN. We then train IBM mod-
els (Brown et al, 1993) using the GIZA++ package
(Och and Ney, 2000). In addition to the common
practice of lowercasing words and combining word
alignments from both directions, we adopt the fol-
lowing strategies to improve word alignment:
Breaking words into morphemes: Since a
multi-morpheme word in a gloss line often corre-
sponds to multiple words in the translation line, we
split each word on the gloss line into morphemes us-
ing the standard IGT morpheme delimiters (e.g., ?-
?). For instance, the seven words in the gloss line of
Ex (1) become nine morphemes.
Adding (x,x) pairs: If a word x appears in the
gloss and the translation lines of the same IGT ex-
ample, it is highly likely that the two copies of the
same word should be aligned to each other. To help
GIZA++ recognize this property, we first identify
and collect all such words and then add single word
pairs (x,x) to the training data. For instance, from
Ex (1), we would add a sentence pair for each mor-
pheme (excepting -3sg which does not appear in the
translation line).
3.2.2 Heuristic word aligner
Our second word aligner is based on the assump-
tion that if two words (one on the gloss line, the other
on the translation line) have the same root form, they
are likely to be aligned to one other.We built a sim-
ple English morphological analyzer and ran it on the
two lines, and then linked the words with the same
454
root form.4
3.3 Tree projection
We designed two projection algorithms: one which
projects PS and the other which projects DS, both
from the English to the source language.5
3.3.1 Projecting dependency structure
Our DS projection algorithm is similar to the pro-
jection algorithms described in (Hwa et al, 2002) and
(Quirk et al, 2005). It has four steps: First, we copy
the English DS, and remove all the unaligned English
words from the DS.6 Second, we replace each English
word in the DS with the corresponding source words.
If an English word x aligns to several source words,
we will make several copies of the node for x, one
copy for each such source word. The copies will all
be siblings in the DS.
If a source word aligns to multiple English words,
after Step 2 the source word will have several copies
in the resulting DS. In the third step, we keep only
the copy that is closest to the root and remove all the
other copies.7 In Step 4, we attach unaligned source
words to the DS using the heuristics described in
(Quirk et al, 2005). Figure 2 shows the English DS,
the source DS after Step 2, and the final DS.
3.3.2 Projecting phrase structure
Our PS projection algorithm also has four steps,
the first two being the same as those for projecting
DS. In the third step, starting from the root of the
current source PS and for each node x with more
than one child, we reorder each pair of x?s children
until they are in the same order as dictated by the
source sentence. Let yi and yj be two children of
x, and their spans be Si = [ai, bi] and Sj = [aj , bj ].
When we reorder yi and yj, there are four possible
scenarios:
(1) Si and Sj don?t overlap: we put yi before yj
if ai < aj or the opposity if ai > aj .
4When a word is repeated in both the gloss and trans-
lation, the individual occurrences are aligned individually
in left-to-right order.
5The DS projection algorithm as described does not
guarantee that the yield of the resulting source DS has
the same word order as the source sentence; however, if
needed, the algorithm can be easily modified (by mak-
ing its Step 3 similar to the Step 3 of the PS projection
algorithm) to ensure the correct word order.
6Every time we remove an internal node x from a DS,
we make x?s children depend on x?s parent directly.
7The heuristic is not as arbitrary as it sounds because
very often when a source word aligns to multiple English
words, one of the English words dominates the rest in
the DS (e.g., the node for to in Figure 2(a) dominates
the node for the). We are using the dominant word to
represent the whole set.
(2) Si is a strict subset of Sj: we remove yj
from the PS and promote its children: yj?s
children will become children of yj ?s parent.
(3) Sj is a strict subset of Si: we remove yi and
promote its children.
(4) Si and Sj overlap but neither is a strict
subset of the other: we remove both yi and
yj and promote their children. If both yi and
yj are leaf nodes with the same span, we will
merge the two nodes.8
The last step is to insert unaligned source words
into the source PS. For each unaligned source word
x, we will find its closest left and right neighbors that
are aligned to some English words, and then attach x
to the lowest common ancestor of the two neighbors.
Figure 1 shows the English PS, the source PS after
Step 2, and the final source PS. The three boxes in
1(b) mark the nodes that are removed in Step 3.
4 Experiments
We tested the feasibility of our approach on a small
set of IGT examples for seven languages: Ger-
man (GER), Korean (KKN), Hausa (HUA), Mala-
gasy (MEX), Welsh (WLS), Irish (GLI), and Yaqui
(YAQ). This set of languages was chosen because of
its typological diversity: GER and HUA are SVO
languages, KKN and YAQ are SOV, GLI and WLS
are VSO, and MEX is VOS. In addition, while Ger-
man and Korean are well-studied and have readily
accessible resources that we could use to test the ef-
fectiveness and accuracy of our methods, Yaqui, with
about 16,000 speakers, is a highly endangered lan-
guage and serves as a demonstration of our methods
for resource-poor and endangered languages.
4.1 Creating the gold standard for the test
set
The number of IGT examples in ODIN varies greatly
across the seven languages, ranging from less than
one hundred for Welsh to over seventeen hundred for
German. For each language, we randomly picked 50-
150 IGT examples from the available examples whose
English translations had at least five words.9 The
examples were manually checked and corrupted ex-
amples were thrown away. The remaining examples
8We will keep one copy and merge the POS tag of
the words. For instance, the tag IN+DT in Figure 1(c)
was created when two copies of i?r in Figure 1(b) were
merged.
9We skipped examples with very short English trans-
lations because they are unlikely to contain much in the
way of syntactic structures.
455
Table 1: The size and average sentence length of the test data
GER KKN HUA MEX WLS GLI YAQ Total
# of IGT examples 104 103 77 87 53 46 68 538
# of src words 739 526 441 498 313 252 404 3173
Ave src sent leng 7.11 5.11 5.73 5.72 5.91 5.48 5.94 5.90
# of Eng words 711 735 520 646 329 278 544 3823
Ave Eng sent leng 7.41 7.14 6.75 7.43 6.21 6.04 8.01 7.11
# of speakers 128M 78M 39M 9.4M 580K 260K 16K 255.3M
formed our test data. Table 1 shows the size and av-
erage sentence lengths of the test data by language.10
The languages are sorted by number of speakers (as
derived from the Ethnologue (Gordon, 2005)).
We ran our algorithm on the test data, and the
system produced the following: an English PS, En-
glish DS, word alignment, projected source PS, and
projected source DS. We asked human annotators to
manually check the output and correct the English
DS, word alignments and projected DS structures
where necessary.11 12 In order to calculate inter-
annotator agreement, the Yaqui data and half of the
German data were each checked by two annotators,
and the disagreement between the annotators was
adjudicated and a gold standard was created. The
inter-annotator agreement (a.k.a. the F-measure of
dependency or alignment links) on English DS, gloss-
translation alignment, and projected source DS are
96.34%, 96.35%, and 91.09%, respectively. The rest
of the data were annotated by one annotator.
4.2 Word alignment results
We tested our word aligners on 70% (374 examples)
of the whole test set (538 examples), while reserving
the remaining 30% for future use.
4.2.1 Statistical word aligner
As indicated earlier, the ODIN database contains
36,439 IGT examples. We removed duplicates13 and
10There are three reasons why the sentences are so
short. First, since IGT is used to present particular lin-
guistically salient morphological or syntactic material,
sentences in IGT are only as long as needed for the
given expose?. Second, space constraints often dictate us-
ing shorter examples (i.e., they must fit on one line).
Third, the IGT extraction algorithm currently used in
ODIN does not search for the less common multi-line
(i.e., greater than three line) examples.
11The English PS and source PS were not corrected;
without a thorough linguistic study of the source lan-
guages, it is impossible to devise appropriate gold stan-
dards for their phrase structures.
12The DS structures for the English and source lan-
guage in the gold standard can be non-isomorphic.
13Duplicates are common since it is standard practice
in linguistics to copy and cite language examples from
other papers.
Table 2: The training data for GIZA++
# of sentences 28,902
# of words in gloss lines 174,765
# of morphemes in gloss lines 251,465
# of words in translation lines 217,022
Size of gloss word vocabulary 16360
Size of gloss morpheme vocabulary 14050
Size of translation word vocabulary 14029
Table 3: The word alignment results when gloss
words are not split into morphemes
Precision Recall F-measure
Gloss ? trans 0.674 0.689 0.681
Trans ? gloss 0.721 0.823 0.769
Intersection 0.948 0.620 0.750
Union 0.590 0.892 0.711
Refined 0.846 0.780 0.812
examples with missing lines, and used the remain-
ing 28,902 examples for GIZA++ training.14 Table
2 shows the statistics of the training data with all
words lowercased. Tables 3?5 show the performance
of the word aligner under three settings:
(1): Not splitting words in the gloss lines into mor-
phemes.
(2): Splitting words in gloss lines into morphemes.
(3): Doing (2) plus adding (x,x) sentence pairs into
the training data, where x is a word that appears
in both the gloss and translation lines of the
same IGT example.
For each setting, we trained in both directions and
combined the two alignments by taking the intersec-
tion, union, and refined as defined in (Och and Ney,
2000). The best F-score for each setting is in bold-
face. From the tables, it is clear that the third set-
ting works the best, and combining the alignments
14Interestingly, although the IGT examples in the
training data come from hundreds of languages in ODIN,
IBM Model 4 performs significantly better than Models
1 and 2 (by at least two percent points for F-measure);
therefore, all the GIZA++ results reported in the paper
are based on Model 4.
456
Table 4: The word alignment results when gloss
words are split into morphemes
Precision Recall F-measure
Gloss ? trans 0.746 0.889 0.811
Trans ? gloss 0.797 0.863 0.829
Intersection 0.958 0.811 0.878
Union 0.659 0.941 0.775
Refined 0.918 0.900 0.909
Table 5: The word alignment results when (x,x) pairs
are added
Precision Recall F-measure
Gloss ? trans 0.759 0.922 0.833
Trans ? gloss 0.801 0.924 0.858
Intersection 0.956 0.885 0.919
Union 0.666 0.961 0.787
Refined 0.908 0.921 0.915
from both directions works better than either direc-
tion alone.15
4.2.2 Heuristic word aligner
The word aligner has two settings. In the first
one, the aligner aligns two words if and only if they
have the same orthographic form. In the second, it
aligns two words if and only if they have the same
root form.16 The results are shown in the first and
second rows of Table 6.
We experimented with various methods of com-
bining the two aligners, and the best one is an aug-
15For languages with hundreds of IGT examples, one
may wonder whether training GIZA++ with the data for
that language alone would outperform the system trained
with IGT examples from all the languages in ODIN. To
answer this question, we ran three experiments on the
German data (for which there are 1757 IGT examples
in ODIN after removing duplicates): (a) trained on the
(gloss, translation) pairs for all IGT data, (b) trained on
the (gloss, translation) pairs of the German data alone,
and (c) trained on the (source, translation) pairs of the
German data. The test was run against 58 IGT examples,
a subset of the German test data in Table 1. It turns out
that (a) performs much better than (b) and (c), which
justifies the approach we proposed in Section 3.2. For
instance, the F-measures for the refined alignment for
(a)-(c) are 92.5%, 90.2%, and 85.6%, respectively.
16For the second setting, we wrote a 90-line Perl appli-
cation that finds the root for each English word by using
a dozen regular expression patterns combined with a list
of 163 irregular verbs with their inflected forms.
Table 6: The performance of heuristic word aligner
Precision Recall F-measure
No morphing 0.983 0.742 0.846
With morphing 0.983 0.854 0.914
Augmented aligner 0.981 0.881 0.928
mented heuristic word aligner which links two words
if and only if they have the same root form or they
are good translations of each other according to the
translation model built by GIZA++.17 The result
is shown in the last row of Table 6. We used this
aligner for the structural projection experiment.
4.3 Projection results
We evaluated the results of the major steps in our al-
gorithm: the English DS derived from the parse trees
produced by the English parser, the word alignment
between the gloss and translation lines, and the pro-
jected source DS. We calculated the precision, recall,
and F-score of the dependency links and word align-
ment links. The F-scores are shown in Table 7.18
Both the English parser and the word aligner work
reasonably well with most F-scores well above 90%.
The F-scores for dependency links in the source DS
are lower partly due to errors in early parts of the
process (e.g., English DS and word alignment), which
propagates to this step. When we replace the auto-
matically generated English DS and word alignment
with the ones in gold standard, the F-measure of
source DS increases significantly, as shown in Table
8.
To identify the causes of the remaining errors in
the oracle results, we manually checked and classified
one third of the errors in the German data. Among
the 43 errors in the source DS, 26 (60.5%) are due
to language divergence (e.g., head switching), eight
(18.6%) are errors made by the projection heuristics,
and nine (20.9%) are due to non-exact translations
such as the one shown in Ex (2). Because language
divergence can reveal interesting typological distinc-
tions between languages, the first type of error may,
in fact, identify examples that could be of great value
to linguists and computational linguists.
(2) der Antrag des oder der Dozenten
the petition of-the.SG or of-the.PL docent.MSC
?the petition of the docent.? (Daniels, 2001)
5 Discussion
5.1 The IGT bias and knowledge discovery
from enriched data
From the enriched data, various kinds of informa-
tion can be extracted, such as grammars and transfer
rules. We extracted CFGs for the seven languages by
reading off the context-free rules from the projected
17We treat a word pair, (e,f), as a good translation if
and only if both P (e|f) and P (f |e) are high.
18The Total word alignment F-measure is higher than
0.928 as mentioned in Table 6 because the test set used
here is the superset of the one used in that section.
457
Table 7: The system performance on the seven languages
GER KKN HUA MEX WLS GLI YAQ Total
English DS 94.25 89.78 96.15 95.51 91.49 93.53 93.57 93.48
Word alignment 94.91 94.20 94.71 94.26 95.65 88.11 93.64 94.03
Source DS 78.14 82.16 84.71 84.22 84.39 78.17 79.36 81.45
Table 8: The F-measure of source dependency links with perfect English DS and/or word alignment
GER KKN HUA MEX WLS GLI YAQ Total
With gold Eng DS 82.21 87.67 88.46 85.23 91.72 80.16 83.81 85.42
With gold alignment 85.77 86.15 86.07 88.44 84.98 82.40 86.27 86.00
With both 91.21 91.67 89.82 89.65 94.25 85.77 90.68 90.64
Table 9: Extracted CFGs and evidence of word order
HUA MEX GLI YAQ
Word order SVO VOS VSO SOV
# of rule types 102 129 86 115
# of rule tokens 384 466 202 295
source PS. The numbers of rule types and rule tokens
for four of the languages are listed in Table 9.
It is important to note that IGT data is somewhat
biased: examples tend to be short and are selected
for the purposes of a particular rhetorical context.
They, therefore, deviate from the ?normal? usage
that one might normally expect to find in a corpus of
language data. As such, one might question whether
the information extracted from IGT would also be
skewed due to these biases.
To test the usefulness of the data for answering
typological questions, we wrote a tool that predicted
the canonical word order (e.g., SOV, SVO) of a lan-
guage using simple heuristics. It was able to pro-
duce the correct answers for all seven languages in
our sample.19 20 We suspect that the number of
IGT instances and their diversity (i.e., from multiple
documents) is crucial to overcoming the IGT bias,
and feel that the same heuristics could be applied
to a much larger sample of languages. These could
be further adapted to additional typological param-
eters beyond word order (e.g., orders of heads and
modifiers in PS). We leave this to future work.
Given syntactically enriched data, it is also possi-
ble to search for patterns that are linguistically in-
teresting. For instance, we wrote a piece of code
that automatically identified examples with crossing
19Our code simply went through all the rules in the
extracted CFGs and checked the position of the verb with
respect to its subject and object. The -SBJ and -OBJ
function tags were added to the English parse trees using
simple heuristics and were carried over to the source PS
via the projection algorithm.
20There is disagreement among linguists about Ger-
man?s underlying word order, being either SVO or SOV.
Our heuristics returned SOV.
dependencies (i.e., the ones whose DS have crossing
links). One such example from the Yaqui data is in
Ex (3), where the coordinated noun phrase kow-ta
into mis-ta ?the pig and the cat? is separated by the
verb bwuise-k ?grasp?. Note that the crossing depen-
dencies can only be discovered in the Yaqui data and
not in the English since none exist in the English.
(3) inepo kow-ta bwuise-k into mis-ta
1SG pig-NNOM.SG grasp-PST and cat-NNOM.SG
?I caught the pig and the cat.? (Mart??nez Fabia?n,
2006)
So far, we have examined linguistically interesting
information in the source. In the future, we plan to
examine structures in both the source and English.
For instance, we plan to extract transfer rules from
the aligned source and English structures and also
calculate head/modifier crossings between languages
similar to those described in (Fox, 2002).
5.2 Tools and resource building
The information that we discover about a language
can help with the development of tools for the lan-
guage. The order of constituents, for instance, can
be used to inform prototype-driven learning strate-
gies (Haghighi and Klein, 2006), which can then
be applied to raw corpora. It is also possible that
small samples of data showing the alignment inter-
actions between source language structures and those
of English can provide essential bootstrap informa-
tion for informing machine translation systems (cf
(Quirk and Corston-Oliver, 2006)).
Proof of the utility of an enriched corpus built over
ODIN will depend crucially on its evaluation, and we
feel that an important part of our future work will be
the development of parsers that have been trained on
projected structures. These parsers can be evaluated
against human built corpora such as treebanks (obvi-
ously, only for those languages that have treebanks).
Proof will also come from linguists who will be able
to use the corpus to search for constructions of in-
terest (e.g., passives, relative clauses, etc.), and will
likely be able to do so using standard tools such as
458
tgrep.21 Crucially, linguists would be able to conduct
such searches over a very large number of languages.
6 Conclusion
In this paper we demonstrate a methodology for pro-
jecting structure from annotated English data onto
source language data. Because each IGT instance
provides an English translation and an intermedi-
ary gloss line, we are able to project full syntac-
tic structures from the automatically parsed trans-
lation. The fact that our basic methodology and
code were applied to a typologically diverse sample
of seven languages without modification suggests the
potential for application to a much larger sample,
perhaps numbering into the hundreds of languages.
The resulting enriched structures could be of great
importance to the fields of linguistics and compu-
tational linguistics. For the former, search facili-
ties could be built over the data that would allow
linguists to find syntactically marked up data for a
large variety of languages, and could even accommo-
date cross-linguistic comparisons and analyses. For
the latter, we could automatically discern grammars
and transfer rules from the aligned and marked up
data, where these computational artifacts could act
as bootstraps for the development of additional tools
and resources.
References
John Frederick Bailyn. 2001. Inversion, dislocation and
optionality in russian. In Gerhild Zybatow, editor,
Current Issues in Formal Slavic Linguistics.
Peter Brown, Vincent Pietra, Stephen Pietra, and Robert
Mercer. 1993. The Mathematics of Statistical Ma-
chine Translation: Parameter Estimation. Computa-
tional Linguistics, 19(2):263?311.
Eugene Charniak. 1997. Statistical Parsing with a
Context-Free Grammar and Word Statistics. In Proc.
of AAAI-1997.
Michael W. Daniels. 2001. On a type-based analysis
of feature neutrality and the coordination of unlikes.
In Proceedings of the 8th International HPSG Confer-
ence. CSLI Publications.
Bonnie J. Dorr. 1994. Machine translation divergences:
a formal description and proposed solution. Computa-
tional Linguistics, 20(4):597?635.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of EMNLP 2002,
Philadelphia, Pennsylvania.
21This kind of search is reminiscent of Resnik?s Lin-
guists Search Engine (http://lse.umiacs.umd.edu), which
allows structural search across text found on the Web.
Raymond G. Gordon, editor. 2005. Ethnologue: Lan-
guages of the World. SIL International, Dallas, TX,
fifteenth edition.
Aria Haghighi and Dan Klein. 2006. Protoype-driven
sequence models. In Proceedings of HLT-NAACL, New
York City, NY.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan
Kolak. 2002. Evaluating translational correspondence
using annotation projection. In Proceedings of the 40th
Annual Meeting of the ACL, Philadelphia, Pennsylva-
nia.
Michael Krauss. 1992. The World?s Languages in Crisis.
Language, 68(1):4?10.
William D. Lewis. 2006. ODIN: A Model for Adapting
and Enriching Legacy Infrastructure. In Proceedings
of the e-Humanities Workshop, Amsterdam. Held in
cooperation with e-Science 2006: 2nd IEEE Interna-
tional Conference on e-Science and Grid Computing.
David M. Magerman. 1995. Statistical Decision-Tree
Models for Parsing. In Proc. of the 33rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-1995), Cambridge, Massachusetts, USA.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
et al 1994. The Penn Treebank: Annotating Pred-
icate Argument Structure. In Proc of ARPA Speech
and Natural Language Workshop.
Constantino Mart??nez Fabia?n. 2006. Yaqui Coordina-
tion. Ph.D. thesis, University of Arizona.
Franz-Josef Och and Hermann Ney. 2000. Improved Sta-
tistical Alignment Models. In the 38th Annual Confer-
ence of the Association for Computational Linguistics
(ACL-2000), pages 440?447.
Chris Quirk and Simon Corston-Oliver. 2006. The im-
pact of parse quality on syntactically-informed statis-
tical machine translation. In Proceedings of EMNLP
2006.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency tree translation: Syntactically informed
phrasal smt. In Proceedings of ACL 2005.
John R. Swanton. 1912. Haida songs. In Franz Boas,
editor, Publications of the American Ethnological So-
ciety, Volume III. E. J. Brill.
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the com-
plexity of translation equivalence. In Proceedings of
ACL 2006.
Chenhai Xi and Rebecca Hwa. 2005. A backoff model
for bootstrapping resources for non-English languages.
In Proceedings of HLT-EMNLP, pages 851?858, Van-
couver, British Columbia, Canada.
David Yarowksy and Grace Ngai. 2001. Inducing Mul-
tilingual POS taggers and NP Bracketers via robust
projection across aligned corpora. In Proceedings of
NAACL-2001, pages 377?404.
459
The semantics of markup: Mapping legacy markup schemas to a common 
semantics 
Gary F. Simons 
SIL International  
7500 W. Camp Wisdom Road 
Dallas TX 75236, USA  
Gary_Simons@sil.org 
 
 
Scott O. Farrar 
Faculty of Linguistics and Literary Sciences 
University of Bremen  
Bibliothekstr. 1 
D-28359 Bremen, Germany  
farrar@uni-bremen.de  
 
Brian Fitzsimons 
Department of Linguistics 
University of Arizona  
P. O. Box 210028 
Tucson AZ 85721, USA  
 fitzsimo@u.arizona.edu  
William D. Lewis 
Department of Linguistics 
California State University, Fresno 
5245 North Backer Avenue 
Fresno CA 93740, USA   
wlewis@csufresno.edu 
 
D. Terence Langendoen 
Department of Linguistics 
University of Arizona  
P. O. Box 210028 
Tucson AZ 85721, USA  
langendt@u.arizona.edu 
 
Hector Gonzalez 
Department of Linguistics 
California State University, Fresno 
5245 North Backer Avenue 
Fresno CA 93740, USA   
hexgonzo@csufresno.edu 
 
Abstract 
A method for mapping linguistic descrip-
tions in plain XML into semantically rich 
RDF/OWL is outlined and demonstrated. 
Starting with Simons?s (2003) original 
proof of concept of this method, we extend 
his Semantic Interpretation Language (SIL) 
for creating metaschemas to carry out the 
mapping, employ the General Ontology for 
Linguistic Description (GOLD) of Farrar 
and Langendoen (2003) as the target se-
mantic schema, and make use of SeRQL, 
an RDF-aware search engine. This data 
migration effort is in keeping with the vi-
sion of a Semantic Web; it is part of an ef-
fort to build a ?community of practice? 
around semantically rich linguistic re-
sources. 
1 Introduction 
Machine-readable structured linguistic docu-
ments (comparative word lists, lexicons, annotated 
texts, audio and audio-video recordings aligned 
with transcriptions (possibly annotated), gram-
matical descriptions, etc.) are being made available 
in a wide variety of formats on the Web. Until re-
cently, the linguistics community has not been par-
ticularly concerned about the ease with which 
those structures can be accessed by other users, nor 
about the comparability of the structures that can 
be accessed. Now that community is beginning to 
realize that XML encoding provides relatively 
straightforward access to the intended structures 
and at the same time insures that the documents 
will continue be accessible for the foreseeable fu-
ture.  
However, XML encoding by itself does not in-
sure comparability. To achieve that goal, the com-
munity must either adopt standards for encoding 
particular structures, or methods need to be devel-
oped for interpreting structures that are differently 
encoded. This paper reports on an effort to do the 
latter: to migrate XML documentation of linguistic 
structure to a semantically interoperable format. 
One of the most compelling reasons to do so is to 
enable intelligent search: the ability to query 
documents based on their semantics, rather than on 
strings of characters that may occur in them or on 
their document syntax. Facilitating intelligent 
searching is also one of the major goals of the Se-
mantic Web. We are making the first steps towards 
a Semantic Web for linguistics by showing how to 
migrate a significant amount of language resources 
to a format that makes them semantically compa-
rable. 
2 Background 
The work reported in this paper was carried out 
as part of the Electronic Metastructure for Endan-
gered Language Data (EMELD) project 
[emeld.org] (NSF grant 0094934) and the Data-
Driven Linguistic Ontology project (NSF grant 
0411348). One of the objectives of the EMELD 
project is the ?formulation and promulgation of 
best practice in linguistic markup of texts and lexi-
con.? Underlying this objective is the goal of en-
suring that the digital language documentation 
produced by linguists will be truly portable in the 
sense of Bird and Simons (2003): that it will tran-
scend computer environments, scholarly communi-
ties, domains of application, and the passage of 
time. The project was undertaken on the basis of 
the following principles: 
1. 
2. 
3. 
1. 
2. 
3. 
4. 
1. 
2. 
3. 
XML markup provides the best format for 
the interchange and archiving of endangered 
language description and documentation. 
No single schema or set of schemas for 
XML markup can be imposed on all lan-
guage resources. 
The resources must nevertheless be compa-
rable for searching, drawing inferences, etc.  
Simons (2003) points out the conflict between 
the second and third principles, and describes the 
following set of actions for reconciling them. 
Develop a community consensus on shared 
ontologies of linguistic concepts that can 
serve as the basis for interoperation. 
Define the semantics of any particular 
markup schema by mapping its elements and 
attributes to the concepts in the shared on-
tology that they represent. 
Map each individual language resource onto 
its (partial) semantic interpretation by apply-
ing the mapping of its markup schema. 
Perform queries and other knowledge-based 
operations across resources over these se-
mantic interpretations rather than the origi-
nal XML documents. 
The EMELD project has already begun work on 
the first of these action items, the creation of a 
sharable ontology for language documentation and 
description, a General Ontology for Linguistic De-
scription (GOLD) [emeld.org/gold] (Farrar and 
Langendoen, 2003), which is intended to be 
grounded in a suitable upper ontology such as 
SUMO (Niles and Pease, 2001) or DOLCE (Ma-
solo et al, 2002). GOLD is itself being written in 
OWL, the Ontology Web Language (McGuinness 
and van Harmelen, 2004), for use in Semantic Web 
applications. Simons (2003, 2004) also provides a 
?proof of concept? for an implementation of the 
remaining three action items as follows. 
Beginning with three dictionaries that used 
similar but distinct markup based on the 
Text Encoding Initiative (TEI) guidelines 
(Sperberg-McQueen and Burnard, 2002), 
Simons created mappings from their differ-
ent markup schemas to a common semantics 
as defined by an RDF Schema (Brickley and 
Guha, 2004). Such a semantic schema pro-
vides a ?formal definition ... of the concepts 
in a particular domain, including types of re-
sources that exist, the properties that can re-
late pairs of resources, and the properties 
that can describe a single resource in terms 
of literal values? (Simons, 2004). This map-
ping he called a metaschema, a formal defi-
nition of how the elements and attributes of 
a markup schema are to be interpreted in 
terms of the concepts of the semantic 
schema. He called the ?language? for writing 
metaschemas (defined via an XML DTD) a 
Semantic Interpretation Language (SIL).  
Simons performed the semantic interpreta-
tion operation in a two-step process using 
XSLT, first to create an interpreter for a par-
ticular metaschema and then to apply it 
against a source document to yield the RDF 
document (repository) that is its semantic in-
terpretation. 
Simons then loaded the RDF repositories 
into a Prolog system to create a merged da-
tabase of RDF triples and used Prolog?s in-
ference engine to query the semantic inter-
pretations. 
Simons (2003) describes this implementation as 
providing a semantics of markup, rather than as 
devising yet another markup language for seman-
tics. As such, it is in the spirit of efforts such as 
Sperberg-McQueen et al (2000), who define the 
meaning of markup as the set of inferences li-
censed by it. However, their model does not pro-
vide for the general comparison of documents. It is 
also in the spirit of the proposal for a Linguistic 
Annotation Framework (LAF) under development 
by Working Group 1-1 of ISO TC 37 SC 4 
[www.tc37sc4.org] (Ide and Romary, 2003; Ide, 
Romary and de la Clergerie, 2003), but differs 
from it in some significant ways. For example, our 
strategy does not require that the source annota-
tions be mapped to an XML ?pivot format?. On the 
other hand, the LAF does not require that the 
source annotations be in XML to begin with. The 
?data categories? of the LAF correspond to the 
concepts in GOLD; however the ?creation of an 
ontology of annotation classes and types? is not yet 
part of the LAF (Ide, Romary and de la Clergerie 
2003). Moreover, the LAF data model is confined 
to feature structures, whereas GOLD plans to offer 
feature structures as one of several data structuring 
alternatives. Finally, through its connection with 
an upper ontology, GOLD will also be related to 
the ?rest of the world?, whereas the LAF ontology 
is apparently intended for linguistic structure only. 
3 Goals of this paper 
In this paper we extend Simons? proof of con-
cept for the use of metaschemas in the following 
ways. 
1. 
2. 
3. 
4. 
GOLD itself is used as the semantic schema. 
SIL is extended to include the ability to map 
the content of designated elements and at-
tributes in source documents to the semantic 
schema, not just the markup itself. 
We devise metaschemas for lexicons that 
use distinct XML markup schemas: one of 
the lexicons that Simons (2003) originally 
used, for Sikaiana (Solomon Islands) with 
about 3000 entries; a Hopi (Arizona) dic-
tionary with about 30,000 entries, for which 
Kenneth Hill?s original encoding using a 
proprietary and no longer supported data-
base program was converted to XML by 
Lewis and Gonzalez; and a Potawatomi 
(Great Lakes region, US and Canada) lexi-
con being created by Laura Buszard-
Welcher using the EMELD FIELD tool. 
The Prolog query engine is replaced by 
SeRQL, an SQL-like query language for 
Sesame, an RDF database program (Broek-
stra, Kampman and van Harmelen 2002; 
User Guide for Sesame 2004). It is our in-
tention to couple Sesame with an inference 
engine that reads OWL documents, such as 
Racer (Haarslev and Moller 2001). 
In carrying out the migration of such language 
resources to the Semantic Web, we are guided by 
the principle of preserving the original analyses as 
much as possible. At the same time, since the mi-
grated resources are to be rendered mutually inter-
operable and transparent to the tools that are de-
signed to work over them, the migration process 
has the potential to greatly increase the precision 
of the original analyses, to reveal inconsistencies 
in them, and ultimately to result in enriched re-
sources. For example, the comparison of two de-
scriptions of the same language that has been made 
possible by migration could reveal errors in one or 
the other. Similarly, a single resource could be 
checked for consistency with accumulated linguis-
tic knowledge represented in an ontology. The mi-
gration process thus provides two sources of new 
knowledge. First is the knowledge brought in from 
the document interpretation process itself, i.e. by 
the linguist, not necessarily the one who performed 
the original analysis. Second when the migrated 
documents are added to the knowledge base, new 
inferences can be automatically generated based on 
the general knowledge of linguistics captured in 
the ontology. The type of new knowledge gener-
ated is however constrained, for example, by the 
type of search to be done over the resulting knowl-
edge base (see section 6). 
However the migration process can also skew or 
misinterpret the intentions underlying the original 
documentation. To minimize this risk, the migra-
tion tools should be as non-intrusive as possible. 
Even so, some steps are necessary to add structure 
where structure is lacking in the original XML 
documentation and to interpret the meaning of the 
original elements where their meanings are unde-
fined or unclear. For the ontology the implication 
is that theory-laden concepts either should be 
avoided or less encumbered alternatives should be 
made available.  
4 GOLD 
An important guiding principle used in the con-
struction of GOLD is to distinguish between those 
concepts that represent the content of linguistic 
data and those that pertain to the structuring of 
those data (cf. Ide and Romary 2003 who also dis-
tinguish between data content and data structure). 
A particular entry in a lexicon, for example, is a 
data structure used to organize lexical data in a 
particular fashion. Entries usually contain actual 
data instances, e.g., the Hopi word nahalayvi?yma 
or its phonological properties. The process of data 
migration is made much easier if a separation be-
tween data and data structure is upheld in the se-
mantic schema. 
4.1 Data content 
Linguistic data content includes linguistic ex-
pressions, the physical manifestations of language, 
also known as ?morphs?, or simply ?forms?, which 
may be written, spoken or signed. In GOLD, writ-
ten linguistic expressions are represented as 
ORTHOGRAPHICEXPRESSION with the subclasses 
ORTHOGRAPHICPART, ORTHOGRAPHICWORD, and 
ORTHOGRAPHICSENTENCE. These are defined as 
special types of strings. In order to analyze linguis-
tic data further, abstract counterparts of linguistic 
expressions are proposed called LINGUISTICUNIT. 
The abstract units are the main objects of interest 
in formal linguistics. In some theories, the various 
subclasses of LINGUISTICUNIT correspond to 
?morphemes?, ?constituents?, or ?constructions?. No 
assumptions are made about whether these have 
any mental significance, e.g. whether they are un-
derlying forms. The class hierarchy for 
LINGUISTICUNIT is presented in Farrar, Lewis and 
Langendoen (2002), and can be viewed in GOLD 
using Prot?g? 2.0 [protege.stanford.edu]. 
The LINGUISTICUNIT hierarchy is organized ac-
cording to how its components are realized as 
forms, and not according to their formal linguistic 
features, which are theory specific. So, for exam-
ple, LEXICALUNIT is simply a formal unit that can 
appear in isolation in its realized form, and not 
necessarily something that can be a constituent of 
larger syntactic constructions. The methodology 
leaves open the question of whether, for example, 
a SUBLEXICALUNIT can also be a phrasal constitu-
ent, as appears to be the case with CLITIC. Yet an-
other alternative would be to organize LINGUISTIC-
UNIT according to semantic features, e.g., a 
SUBLEXICALUNIT would be something which usu-
ally represents a grammaticized notion. But, since 
this varies from language to language, a different 
taxonomy would be needed for every type of lan-
guage encountered. To sum up, adhering to strictly 
formal features necessitates theory-specific tax-
onomies, while adhering to semantic features leads 
to language-specific taxonomies. Instead a neutral 
approach is taken in which LINGUISTICUNIT is or-
ganized according to how instances are realized as 
linguistic expressions. 
ORTHOGRAPHICEXPRESSION is related to LIN-
GUISTICUNIT by the predicate REALIZES. The par-
ticular sort of LINGUISTICUNIT is further defined 
according to what kinds of attributes it can take. 
So, a MORPHOSYNTACTICUNIT has attributes of 
the sort MORPHOSYNTACTICATTRIBUTE. Instances 
of particular attributes are PASTTENSE, SINGULAR-
NUMBER, and PROGRESSIVEASPECT. The class of 
attributes pertaining to linguistic units parallels 
other kinds of non-linguistic attributes such as 
SHAPEATTRIBUTE and PHYSICALSTATE. 
There are several varieties of attributes which 
linguists find useful for language description, in-
cluding phonological and semantic features. Se-
mantic attributes contrast with morphosyntactic 
attributes in that the former correspond to the no-
tional characteristics of linguistic form that have 
some manifestation in the grammar. 
4.2 Data structures 
A linguistic data structure is defined as an ab-
stract information container which provides a way 
to package elements of linguistic data. The two 
main types of data structures contained in GOLD 
at the moment are LEXICALITEM and FEATURE-
STRUCTURE. Our characterization of LEXICALITEM 
extends that of Bell and Bird (2000). At a mini-
mum, a LEXICALITEM should contain an instance 
of LEXICALUNIT or of SUBLEXICALUNIT. Special 
relations are given in GOLD which pertain only to 
data structures, e.g., HASLEXICALUNIT relates a 
LEXICALITEM to a LEXICALUNIT. Instances of 
LEXICALITEM typically include glosses either in 
the same language in the case of a monolingual 
lexicon, or in some other language in the case of a 
bilingual lexicon. Glosses are simply instances of 
ORTHOGRAPHICEXPRESSION related to the entry 
via the relation GLOSS. Entries relate to one an-
other via relations such as SYNONYMOF and 
ANTONYMOF. 
If a LEXICALITEM contains extensive morpho-
logical information, we may represent this in the 
form of a FEATURESTRUCTURE. The FEATURE-
STRUCTURE class is part of a more extensive set of 
data structures known as a FEATURESYSTEM (Lan-
gendoen and Simons, 1995; Maxwell, Simons and 
Hayashi, 2002). A FEATURESPECIFICATION is a 
data structure that contains a subclass and an in-
stance of MORPHOSYNTACTICATTRIBUTE (i.e. an 
ordered pair), for example, [TENSE: PASTTENSE]. 
The implementation of the FEATURESYSTEM con-
struct allows for recursive FEATURESPECIFICA-
TIONs in which, for example, a subclass of 
MORPHOSYNTACTICATTRIBUTE is paired with an 
instance of FEATURESTRUCTURE. 
One criticism that could be raised against the in-
clusion of data structures in a semantic resource 
such as GOLD is that they are superfluous. Why 
not simply leave it up to the source markup to de-
scribe the elements of data structure, e.g., in the 
form of an XML Schema? This is certainly a rea-
sonable criticism, since excluding data structures 
from GOLD would make the ontological model-
ling process much simpler. However, they are in-
cluded because we envision that subsequent appli-
cations will need to be able to reason, not only 
about the data itself, but also about how it is struc-
tured. For example, it might be necessary to com-
pare elements of a LEXICALITEM to that of 
FEATURESTRUCTURE. This is actually an essential 
step in achieving the vision of the Semantic Web, 
namely, constraining the source data in such a way 
as to preserve structure where structure is defined 
and to enrich structure where structure is left un-
specified. 
5 Semantic Interpretation Language 
The Semantic Interpretation Language (SIL) was 
originally created to define the meaning of the 
elements and attributes declared in an XML 
markup schema, as well as the relationships be-
tween them. An SIL metaschema is an XML 
document that formally maps the elements and 
attributes of an XML encoded resource to concepts 
in an OWL ontology or an RDF Schema. Further-
more, the metaschema formally interprets the 
original markup structure by declaring what the 
dominance and linking relations in the XML 
document structure represent. For example, con-
sider the extract from the Hopi lexicon shown in 
Figure 1. 
The dominance relation between the elements 
<MSI> (for ?morphosyntactic information?) and 
<POS> (for ?part of speech?) in the original XML 
is implicitly something like ?has?. This can be 
made more explicit by mapping it to HAS-
MORPHOSYNTACTICPROPERTY, a formally defined 
relation in the ontology. This relation is formally 
defined in the ontology by specifying its signature, 
i.e. what kinds of arguments it can take. Thus, a 
better defined, more exact, relationship between 
elements of markup is achieved. 
<Lexeme id="L3"> 
 <Headword>naheva</Headword> 
 <MSI> 
  <POS> 
<Feature name = "type">vt 
</Feature> 
  </POS> 
Figure 1. Extract from Hopi Lexicon 
SIL has been extended to formalize the resolu-
tion of content in addition to markup. For example, 
the semantics of the gram vt in the XML structure 
<POS>vt</POS> can be specified via a mapping 
to the ontology as an instance of VERB-
TRANSITIVE, in addition to defining the semantics 
of the POS element itself. 
An SIL metaschema, as described in detail in 
Simons (2004), is an XML document built from 
metaschema directives, which are essentially proc-
essing instructions expressed as XML elements. 
Directives like resource, property, lit-
eral and translate generate elements of the 
resulting semantic interpretation. Part of the SIL 
DTD is shown in Figure 2. 
<!ELEMENT metaschema (namespace+, 
(interpret | ignore)+)> 
<!ELEMENT namespace (#PCDATA)> 
<!ATTLIST namespace prefix CDATA 
#REQUIRED> 
<!ELEMENT interpret (resource | 
translate | property | 
literal)*> 
<!ATTLIST interpret markup CDATA 
#REQUIRED> 
<!ELEMENT resource (property | 
translate | literal | embed)*> 
<!ATTLIST resource concept CDATA 
#REQUIRED> 
<!ELEMENT property (resource | 
resourceRef | embed)> 
<!ATTLIST property concept CDATA 
#REQUIRED> 
<!ELEMENT translate EMPTY> 
<!ATTLIST translate concept CDATA 
#REQUIRED mapping CDATA 
#REQUIRED> 
Figure 2. SIL DTD fragment 
The interpret directive performs the pri-
mary mapping function from markup elements of 
the input resource to the enriched output, as dem-
onstrated in Figure 3. The tag <form> is inter-
preted as a LINGUISTICFORM, specifically as an 
ORTHOGRAPHICREPRESENTATION of that form. 
Input document: 
<form>ahali</form> 
Metaschema directive: 
<interpret markup="form"> 
<property concept = 
"gold:form"> 
<resource concept = 
"gold:LinguisticForm"> 
<literal concept = 
"gold:orthographicRepre
sentation"/> 
</resource> 
</property> 
</interpret> 
Interpretation (output): 
<gold:form> 
<gold:LinguisticForm> 
<gold:orthographicRepresen
tation>ahali 
</gold:orthographicRepresen
tation> 
</gold:LinguisticForm> 
</gold:form> 
Figure 3. Example interpretation of an element 
Of primary importance to the interpretation of 
content is the translate directive, as shown in 
Figure 4. In this example, the tag <Feature 
name="type">, embedded within <POS>, is 
interpreted as referencing a morphosyntactic prop-
erty, the value of which is content interpretable by 
the terminology set identified by the reference 
Hopi/Hopi_pos_mapping.xml. A terminol-
ogy set contains a simple mapping between terms 
used in the source document and the names of the 
equivalent concepts in the ontology. SIL can han-
dle both one-to-one terminology mappings (e.g., 
mapping from the tag vt to the concept VERB-
TRANSITIVE) as well as one-to-many mappings 
(e.g. mapping from 1sg to a property bundle of 
FIRSTPERSON and SINGULARNUMBER). 
Input document: 
<POS> 
<Feature name = "type">vt 
</Feature> 
</POS> 
Metaschema directive: 
<interpret markup = "POS/ 
Feature[@name='type']"> 
<translate concept = 
"gold:property" mapping = 
"Hopi/Hopi_pos_mapping.xml"/> 
</interpret> 
Interpretation (output): 
<gold:property rdf:resource = 
"emeld.org/gold#VerbTransitive"/> 
Figure 4. Example interpretation of content  
SIL is designed to allow interoperability be-
tween resources by mapping the different struc-
tures and content of markup in the source docu-
ments onto the same set of ontological concepts. 
This is demonstrated by comparing the trans-
formed output for Hopi shown in Figure 4 with the 
transformed output for Sikaiana in Figure 5. Note 
that the inputs are different but the outputs are the 
same. 
Input document: 
<pos>Verbt</pos> 
Metaschema directive: 
<interpret markup="pos"> 
<translate concept = 
"gold:property" mapping = 
"SKY/SKY_pos_mapping.xml"/> 
</interpret> 
Interpretation (output): 
<gold:property rdf:resource = 
"emeld.org/gold#VerbTransitive"/> 
Figure 5. Transformed Sikaiana <pos> 
The SIL only guarantees interoperability when 
comparable semantic resources are employed in 
the mapping. If an entire group relies on a common 
semantic schema, e.g. GOLD, a ?community of 
practice? is formed. This in turn facilitates intelli-
gent search across converted resources. 
Currently, writing an SIL metaschema is done 
entirely by hand. We are in the process, however, 
of developing two tools to automate the process. 
The first tool will allow the user to define the rela-
tionship between the terminology used within a 
resource with relevant GOLD concepts. The sec-
ond tool will define the structural mapping rela-
tionship between the resource and a given meta-
structure. The first tool, named Alchemy, presents 
the user with a drag-and-drop interface in which 
the user defines the terms used within her resource 
by associating them with one or more GOLD con-
cepts. The relationship between any given term 
and relevant GOLD concepts can be complex, with 
one-to-one or one-to-many relationships being al-
lowed, and the relationships themselves can be of 
any of a number of types: SameAs, KindOf, etc. 
We are in the process of building this tool, embed-
ded within an systems developer toolkit accompa-
nying GOLD. 
The second as of yet unnamed tool is still in the 
early design stages. This tool will allow the user to 
first define the type of resource she is converting 
(lexicon, interlinear text, grammar, etc.), and will 
then lead her through a series of questions that de-
fine the structure by associating it with a meta-type 
definition for the particular resource type. The tool 
will require a precise and well-defined ?semantics 
of linguistic structure?, a conceptual space of lin-
guistic structural types that will be included in 
GOLD, but is still in the process of being defined. 
The final output of this tool, in association with an 
Alchemy-defined terminology set, will be an SIL 
metaschema. 
6 Querying Resources 
In this section, we discuss the general issue of 
searching over linguistic descriptions on the Web, 
and the current state of our effort to do so using 
SeRQL (see section 3 item 4) over the RDF reposi-
tories for Sikaiana, Hopi and Potawatomi gener-
ated by the metaschemas from their XML-encoded 
lexicons. 
6.1 Dimensions of search over linguistic de-
scriptions 
As mentioned in section 1 above, one of the 
most compelling reasons to migrate XML docu-
mentation to a semantically interoperable format is 
to enable intelligent search. For the linguistics 
community, we envision several parameters of 
search over semantically interoperable linguistic 
documentation. Search may be performed accord-
ing to: 
? level of analysis (phonetic, morphosyntactic, 
discourse) 
? typological dimension (including language 
type) 
? intent of search (for exploring some particu-
lar language, or for language comparison) 
? kind of results desired (which data structure 
to return) 
Search also varies according to degree of diffi-
culty, that is, whether search requires the assis-
tance of an inferencing engine or not. Direct 
search is defined as search over explicitly repre-
sented data, i.e. instance data in the knowledge 
space. This includes the simple string matching of 
conventional search engines. But since the search 
will be carried out using the enriched RDF frame-
work, direct search is not limited to string match-
ing in the original XML. An example of direct 
search is to find all data that includes a reference to 
instances of some grammatical category (e.g., 
PASTTENSE). Boolean searching with direct search 
is also possible, e.g., searching for cases of port-
manteau morphemes, expressed in our framework 
as two or more MORPHOSYNTACTICATTRIBUTES 
associated with some LINGUISTICUNIT. 
Indirect search goes beyond direct search by 
making use of inferences based on the structuring 
of the concepts in an ontology. For example the 
concept of PLURALNUMBER means ?two or more?, 
the concept of DUALNUMBER means ?exactly two?, 
and the concept of MULTALNUMBER means ?three 
or more?. A direct search for PLURALNUMBER will 
miss those instances represented as DUALNUMBER 
and MULTALNUMBER, whereas an indirect search 
will find them. 
6.2 Some SeRQL queries 
In Figure 6, we give the SeRQL query (omitting 
using namespace) for the orthographic forms 
for all the lexical items specified as having the 
GOLD concept PROGRESSIVEASPECT in the three 
lexicons. This query returned 1135 results, all from 
Hopi.  
select distinct R 
from {LI} <gold:meaning> {} 
<gold:grammar> {} 
<gold:property> 
{<gold:ProgressiveAspect>}, 
{LI} <gold:form> {} 
<gold:orthographicRepresenta
tion> {R} 
Figure 6. SeRQL query for 
PROGRESSIVEASPECT forms 
Next, the query in Figure 7 returns all the gram-
matical properties of lexical items categorized as 
NOUNs in each of the lexicons. There were 21 re-
sults from Hopi, 3 from Sikaiana and 6 from 
Potawatomi; an example for each language is 
given in Figure 8. The fact that certain items 
categorized as NOUNs in Sikaiana are also 
categorized as VERBs indicates that those items 
have both classifications. In Figure 9, we give the 
SeRQL query for all such items; 61 results were 
obtained. 
select distinct P, LC 
from {LI} <gold:meaning> {} 
<gold:grammar> {MSI} 
<gold:property> 
{<gold:Noun>}; 
<gold:property> {P}, 
{LI} <gold:languageCode> {LC} 
where P != <gold:Noun> 
Figure 7. SeRQL query for attributes of NOUNs 
Hopi: AUGMENTATIVE 
Sikaiana: VERB 
Potawatomi: INANIMATE 
Figure 8. Sample results of query in Figure 7 
select distinct LI 
from {LI} <gold:meaning> {} 
<gold:grammar> {} 
<gold:property> {<gold:Noun>}; 
<gold:property> {<gold:Verb>} 
Figure 9. SeRQL query for all lexical items 
marked as both NOUN and VERB 
Finally in Figure 10, we give a query used to 
find the parts of speech that are common to entries 
in the Hopi and Sikaiana lexicons. Four results 
were returned, NOUN, VERB, ADJECTIVE and 
NUMERAL. 
select distinct P 
from {LI} <gold:meaning> {} 
<gold:grammar> {} 
<gold:property> {P},  
{LI2} <gold:meaning> {} 
<gold:grammar> {} 
<gold:property> {P},  
{LI} <gold:languageCode> {LC}, 
{LI2} <gold:languageCode> 
{LC2} 
where LC = "HOP" AND LC2 = "SKY" 
Figure 10. SeRQL query for common parts of 
speech in two lexicons 
More complex queries that take advantage of the 
structure of the ontology are also possible, for ex-
ample to find all the verbs in the lexicons regard-
less of whether they have been tagged as transitive 
verbs, intransitive verbs, or simply as verbs. With 
further development of the method described here, 
much more elaborate queries over much larger lin-
guistic data repositories will be possible. This re-
sult, we hope, will encourage much more wide-
spread distribution of language resources on the 
Web and the creation of a large community of 
practice that uses those resources for research, 
teaching, and language revitalization efforts. 
References  
J. Bell and S. Bird. 2000. A preliminary study of 
the structure of lexicon entries. In ?Workshop on 
Web-Based Language Documentation and De-
scription?, Philadelphia. 
[www.ldc.upenn.edu/exploration/expl2000/papers
/bell/bell.html] 
S. Bird and G. F. Simons. 2003. Seven dimensions 
of portability for language documentation and 
description. Language 79(3):557-582. 
D. Brickley and R. V. Guha. 2004. RDF Vocabu-
lary Description Language 1.0: RDF Schema. 
W3C Recommendation 10 February 2004, 
World Wide Web Consortium.  
[www.w3.org/TR/rdf-schema] 
J. Broekstra, A. Kampman and F. van Harmelen. 
2002. Sesame: A generic architecture for storing 
and querying RDF and RDF schema. In ?Pro-
ceedings of the First International Semantic Web 
Conference?, I. Horrocks & J. Hendler, ed., 
pages 54-68, Springer-Verlag, Berlin. 
S. O. Farrar and D. T. Langendoen. 2003. A lin-
guistic ontology for the Semantic Web. Glot In-
ternational 7(3):97-100. 
S. O. Farrar, W. D. Lewis and D. T. Langendoen. 
2002. An ontology for linguistic annotation. In 
?Semantic Web Meets Language Resources: Pa-
pers from the AAAI Workshop?, N. Ide & C. 
Welty, ed., pages 11-16, AAAI Press, Menlo 
Park, CA. 
V. Haarslev and R. Moller. 2001. Description of 
the RACER system and its applications. In ?Pro-
ceedings of the Description Logics Workshop 
DL2001?, pages 132-142, Stanford, CA. 
N. Ide and L. Romary. 2003. Outline of the inter-
national standard Linguistic Annotation Frame-
work. In ?Proceedings of ACL?03 Workshop on 
Linguistic Annotation: Getting the Model 
Right?, pages 1-5, Sapporo.  
[www.cs.vassar.edu/~ide/papers/acl2003-ws-
laf.pdf] 
N. Ide, L. Romary and E. de la Clergerie. 2003. 
International standard for a Linguistic Annota-
tion Framework. In ?Proceedings of HLT-
NAACL'03 Workshop on The Software Engi-
neering and Architecture of Language Technol-
ogy?, Edmonton. 
[www.cs.vassar.edu/~ide/papers/ide-romary-
clergerie.pdf] 
D. T. Langendoen and G. F. Simons. 1995. A ra-
tionale for the Text Encoding Initiative recom-
mendations for feature-structure markup. Com-
puters and the Humanities 29:191-205. 
C. Masolo, S. Borgo, A. Gangemi, N. Guarino, A. 
Oltramari and L. Schneider. 2002. WonderWeb 
deliverable D17 version 2.0. In ?The Wonder-
Web Library of Foundational Ontologies and the 
DOLCE ontology.? 
[www.loa-cnr.it/Papers/WonderWebD17V2.0.pdf] 
M. Maxwell, G. F. Simons and L. Hayashi. 2002. 
A morphological glossing assistant. In ?Proceed-
ings of the International Workshop on Resources 
and Tools in Field Linguistics?, Las Palmas, 
Spain. 
[www.mpi.nl/lrec/papers/lrec-pap-25-
MorphologicalGlossingAssistant.pdf] 
D. L. McGuinness and F. van Harmelen, ed. 2004. 
OWL Web Ontology Language overview.  
[www.w3.org/TR/2004/REC-owl-features-
20040210] 
I. Niles and A. Pease. 2001. Toward a standard 
upper ontology. In ?Proceedings of the 2nd In-
ternational conference on Formal Ontology in 
Information Systems?, Ogunquit, ME. 
[projects.teknowledge.com/HPKB/Publications/
FOIS.pdf] 
G. F. Simons. 2003. Developing a metaschema 
language to support interoperation among XML 
resources with different markup schemas. Paper 
presented at the ACH/ALLC conference, Athens, 
GA. 
[www.sil.org/~simonsg/metaschema/ACH%202
003.pdf] 
G. F. Simons. 2004. A metaschema language for 
the semantic interpretation of XML markup in 
documents. Technical report, SIL, Dallas. 
[www.sil.org/~simonsg/metaschema/sil.htm] 
C. M. Sperberg-McQueen and L. Burnard, eds. 
2002. TEI P4: Guidelines for electronic text en-
coding and interchange, XML version, Text En-
coding Initiative Consortium, Oxford etc. 
[www.tei-c.org/P4X] 
C. M. Sperberg-McQueen, C. Huitfeldt, and A. 
Renear. 2000. Meaning and interpretation of 
markup. Markup Languages: Theory and Prac-
tice 2:215-234. 
User Guide for Sesame. 2004.  
[www.openrdf.org/publications/users/index.html] 
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 385?393,
Beijing, August 2010
Comparing Language Similarity across Genetic
and Typologically-Based Groupings
Ryan Georgi
University of Washington
rgeorgi@uw.edu
Fei Xia
University of Washington
fxia@uw.edu
William Lewis
Microsoft Research
wilewis@microsoft.com
Abstract
Recent studies have shown the poten-
tial benefits of leveraging resources for
resource-rich languages to build tools for
similar, but resource-poor languages. We
examine what constitutes ?similarity? by
comparing traditional phylogenetic lan-
guage groups, which are motivated largely
by genetic relationships, with language
groupings formed by clustering methods
using typological features only. Using
data from the World Atlas of Language
Structures (WALS), our preliminary ex-
periments show that typologically-based
clusters look quite different from genetic
groups, but perform as good or better
when used to predict feature values of
member languages.
1 Introduction
While there are more than six thousand languages
in the world, only a small portion of these lan-
guages have received substantial attention in the
field of NLP. With the increase in use of data-
driven methods, languages with few or no elec-
tronic resources have been difficult to process with
current methods. The morphological tagging of
Russian using Czech resources as done by (Hana
et al, 2004) shows the potential benefit for using
the resources of resource-rich languages to boot-
strap NLP tools for related languages. Projecting
syntactic structures across languages (Yarowsky
and Ngai, 2001; Xia and Lewis, 2007) is another
possible way to harness existing tools, though
such projection is more reliable among languages
with similar syntax.
Studies such as these show the possible bene-
fits of working with similar languages. A crucial
question is how we should define similarity be-
tween languages. While genetically related lan-
guages tend to have similar typological features
as they could inherit the features from their com-
mon ancestor, they could also differ a lot due to
language change over time. On the other hand,
languages with no common ancestor could share
many features due to language contact and other
factors.
It is worth noting that the goals of historical lin-
guistics differ from those of language typology in
that while historical linguistics focuses primarily
on diachronic language change, typology is more
focused on a synchronic survey of features found
in the world?s languages: what typological fea-
tures exist, where they are found, and why a lan-
guage has a feature.
These differences between the concepts of ge-
netic relatedness and language similarities lead us
to the following questions:
Q1. If we cluster languages based only on their
typological features, how do the induced
clusters compare to phylogenetic groupings?
Q2. How well do induced clusters and genetic
families perform in predicting values for ty-
pological features?
Q3. What typological features tend to stay the
same within language families, and what fea-
tures are likely to differ?
These questions are the focus of this study,
and for the experiments, we use information from
World Atlas of Language Structures (Haspelmath
et al, 2005), or WALS.
385
ID# Feature Name Category Feature Values
1 Consonant Inventories Phonology (19) {1:Large, 2:Small, 3:Moderately Small, 4:Moderately Large, 5:Average}
23 Locus of Marking in the Clause Morphology (10) {1:Head, 2:None, 3:Dependent, 4:Double, 5:Other}
30 Number of Genders Nominal Categories (28) {1:Three, 2:None, 3:Two, 4:Four, 5:Five or More}
58 Obligatory Possessive Inflection Nominal Syntax (7) {1:Absent, 2:Exists}
66 The Perfect Verbal Categories (16) {1:None, 2:Other, 3:From ?finish? or ?already?, 4:From Possessive}
81 Order of Subject, Object and Verb Word Order (17) {1:SVO, 2:SOV, 3:No Dominant Order, 4:VSO, 5:VOS, 6:OVS, 7:OSV}
121 Comparative Constructions Simple Clauses (24) {1:Conjoined, 2:Locational, 3:Particle, 4:Exceed}
125 Purpose Clauses Complex Sentences (7) {1:Balanced/deranked, 2:Deranked, 3:Balanced}
138 Tea Lexicon (10) {1:Other, 2:Derived from Sinitic ?cha?, 3:Derived from Chinese ?te?}
140 Question Particles in Sign Languages Sign Languages (2) {1:None, 2:One, 3:More than one}
142 Para-Linguistic Usages of Clicks Other (2) {1:Logical meanings, 2:Affective meanings, 3:Other or none}
Table 1: Sample features and their values used in the WALS database. There are eleven feature cate-
gories in WALS, one feature from each is given here. The numbers in parentheses in the ?Category?
column are the total number of features in that category. Feature values are given with both the integers
that represent them in the database and their description in the form {#:description}.
2 WALS
The WALS project consists of a database that cat-
alogs linguistic features for over 2,556 languages
in 208 language families, using 142 features in 11
different categories.1 Table 1 shows a small sam-
ple of features, one feature from each category in
WALS. Listed are the ID number for each exam-
ple, the feature category, and the possible values
for that feature.
WALS as a resource, however, is primarily de-
signed for surveying the distribution of particu-
lar typological features worldwide, not compar-
ing languages. The authors of WALS compiled
their data from a wide array of primary sources,
but these sources do not always cover the same
sets of features or languages.
If we conceive of the WALS database as a two-
dimensional matrix with languages along one di-
mension and features along the other, then only
16% of the cells in that matrix are filled. An empty
cell in the matrix means the feature value for
the (language, feature) pair is not-specified (NS).
Even well-studied languages could have many
empty cells in WALS, and this kind of data spar-
sity presents serious problems to clustering algo-
rithms that cannot handle unknown values. To
address the data sparsity problem, we experiment
with different pruning criteria to create a new ma-
trix that is reasonably dense for our study.
1Our copy of the database was downloaded from http:
//wals.info in June of 2009 and appears to differ
slightly from the statistics given on the website at the time
of writing. Currently, the WALS website reports 2,650 lan-
guages, with 141 features in use.
2.1 Pruning Methods
Answering questions Q1?Q3 is difficult if there
are too many empty cells in the data. Pruning the
data to produce a smaller but denser subset can be
done by one or more of the following methods.
Prune Languages by Minimum Features
Perhaps the most straightforward method of
pruning is to eliminate languages that fail to con-
tain some minimum number of features. Follow-
ing Daume? (2009), we require languages to have a
minimum of 25 features for the whole-world set,
or 10 features for comparing across subfamilies.
This eliminates many languages that simply do
not have enough features to be adequately repre-
sented.
Prune Features by Minimum Coverage
The values for some features, such as those spe-
cific to sign languages, are provided only for a
very small number of languages. Taking this into
account, in addition to removing languages with a
small number of features, it is also helpful to re-
move features that only cover a small portion of
languages. Again we choose the thresholds se-
lected by Daume? (2009) for pruning features that
do not cover more than 10% of the selected lan-
guages in the whole-world set, and 25% in com-
parisons across subfamilies.
Use a Dense Language Family
Finally, using a well-studied family with a num-
ber of subfamilies can produce data sets with less
sparsity. When clustering methods are used with
this data, the groups correspond to subfamilies
386
Data Set Min Features Min Coverage Grouped By # Langs # Groups # Features Density
Unpruned 0 0% Family 2556 208 142 16.0%
Whole-World 25 10% Family 735 121 139 39.7%
Indo-European 10 25% Subfamily 87 10 64 44.9%
Sino-Tibetan 10 25% Subfamily 96 14 64 38.6%
Table 2: Data sets and pruning options used for this paper. Density = |Filled Cells||Total Cells| ? 100
rather than families. In this study, we choose two
families: Indo-European and Sino-Tibetan.
The resulting data sets after various methods of
pruning can be seen in Table 2.
2.2 Features and Feature Values
Besides dealing with the sparsity of the features,
the actual representation of the features in WALS
needs to be taken into account. As can be seen
in Table 1, features are represented with a range
of discrete integer values. Some features, such
as #58?Obligatory Possessive Inflection?are es-
sentially binary features with values ?Absent?
or ?Exists?. Others, such as #1?Consonant
Inventories?appear to be indices along some di-
mension related to size, ranging from small to
large. Features such as these might conceivably
be viewed as on a continuum where closer dis-
tances between values suggests closer relationship
between languages.
Still other features, such as #81?Order of Sub-
ject, Object, and Verb?have multiple values but
cannot be clearly be treated using distance mea-
sures. It?s unclear how such a distance would vary
between an SOV language and either VSO or VOS
languages.
Binarization
Clustering algorithms use similarity functions,
and some functions may simply check whether
two languages have the same value for a feature.
In these cases, no feature binarization is needed.
If a clustering algorithm requires each data point
(a language in this case) to be presented as a fea-
ture vector, features with more than two categori-
cal values should be binarized. We simply treat a
feature with k possible values as k binary features.
There are other ways to binarize features. For in-
stance, Daume? (2009) chose one feature value as
the ?canonical? value and grouped the other val-
ues into the second value (personal communica-
tion). We did not use this approach as it is not
clear to us which values should be selected as the
?canonical? ones.
3 Experimental Setup
To get a picture of how clustering methods com-
pare to genetic groupings, we looked at three el-
ements: cluster similarity, prediction capability,
and feature selection.
3.1 Clustering
Our first experiment is designed to address ques-
tion Q1: how do induced clusters compare to phy-
logenetic groupings?
Clustering Methods
For clustering, two clustering packages were
used. First, we implemented the k-medoids algo-
rithm, a partitional algorithm similar to k-means,
but using median instead of mean distance for
cluster centers (Estivill-Castro and Yang, 2000).
Second, we used a variety of methods from
the CLUTO (Steinbach et al, 2000) clustering
toolkit: repeated-bisection (rb), a k-means im-
plementation (direct), an agglomerative algo-
rithm (agglo) using UPGMA to produce hierar-
chical clusters, and bagglo, a variant of agglo,
which biases the agglomerative algorithm using
partitional clusters.
Similarity Measures
For similarity measures, we used CLUTO?s
default cosine similarity measure (cos), but
also implemented another similarity mea-
sure shared overlap designed to handle
empty cells. Given two languages A and
B, shared overlap(A,B) is defined to be
# Of Features with Same Values
# Features Both Filled Out in WALS . This measurecan handle language pairs with many empty
cells in WALS as it uses only features with cells
387
a is the number of language pairs found in the same set in both clusterings.
b is the number of language pairs found in different sets in C1, and different sets in C2.
c is the number of language pairs found in the same set in C1, but in different sets in C2.
d is the number of language pairs found in different sets in C1, but the same set in C2.
(a) Variables Used In Calculations
R(C1, C2) =
a + b
a + b + c + d(b) Rand Index
Precision(C1, C2) =
a
a + c(c) Cluster precision
Recall(C1, C2) =
a
a + d(d) Cluster recall
Fscore(C1, C2) =
2 ? (Precision ? Recall)
Precision + Recall(e) Cluster f-score
Figure 1: Formulas for calculating the Rand Index, cluster precision, recall, and f-score of two cluster-
ings C1 and C2. C1 is the system output, C2 is the gold standard.
filled out for both languages, and calculates the
percentage of features with the same values.
3.2 Clustering Performance Metrics
To measure clustering performance, we treat the
genetic families specified in WALS as the gold
standard, although we are not strictly aiming to
recreate them.
Rand Index
The Rand Index (Rand, 1971) is one of the
standard metrics for evaluating clustering results.
It compares pairwise assignments of data points
across two clusterings. For every pair of points
there are four possibilities, as given in Figure 1.
The Rand index is calculated by dividing the num-
ber of matching pairs (a+ b) by the number of all
pairs. This results in a number between 0 and 1
where 1 represents an identical clustering. Unfor-
tunately, as noted by (Daume? and Marcu, 2005),
the Rand Index tends to give disproportionately
greater scores to clusterings with a greater num-
ber of clusters. For example, the Rand Index will
always be 1.0 when each data point belongs to its
own cluster. As a result, we have chosen to cal-
culate metrics other than the Rand index: cluster
precision, recall, and f-score.
Cluster Precision, Recall, and F-Score
Extending the notation in Figure 1, precision
is defined as the proportion of same-set pairs in
the target cluster C1 that are correctly identified
as being in the same set in the gold cluster C2,
while recall is the proportion of all same-set pairs
in the gold cluster C2 that are identified in the tar-
get cluster C1. F-score is calculated as the usual
harmonic mean of precision and recall. As it gives
a more accurate representation of cluster similar-
ity across varying amounts of clusters, we will re-
port cluster similarity using cluster F-score.
3.3 Prediction Accuracy
Our second experiment was to answer the ques-
tion posed in Q2: how do induced clusters and
genetic families compare in predicting the values
of features for languages in the same group?
To answer this question, we measure the accu-
racy of the prediction when both types of groups
are used to predict the values of ?empty? cells. We
used 90% of the filled cells to build clusters, and
then predicted the values of the remaining 10% of
filled cells. The missing cells are filled with the
value that occurs the most times among languages
in the same group. If there are no other languages
in the cluster, or the other languages have no val-
ues for this feature, then the cell is filled with
the most common values for that feature across
all languages in the dataset. Finally, the accuracy
is calculated by comparing these predicted values
with the actual values in the gold standard. We run
10-fold cross validation and report the average ac-
curacy.
In addition to the prediction accuracy for each
method of producing groupings, we calculate the
baseline result where an empty cell is filled with
the most frequent value for that feature across all
the languages in the training data.
3.4 Determining Feature Stability
Finally, we look to answer Q3: what typological
features tend to stay the same within related fam-
ilies? To find an answer, we look again to pre-
diction accuracy. While prediction accuracy can
be averaged across all features, it can also be bro-
ken down feature-by-feature to rank features ac-
cording to how accurately they can be predicted
388
by language families. Features that can be pre-
dicted with high accuracy implies that these fea-
tures are more likely to remain stable within a lan-
guage family than others.
Using prediction accuracies based on the ge-
netic families, we rank features according to their
accuracy and then perform clustering using the top
features to determine if the cluster similarity to the
genetic groups increases when using only the sta-
ble features.
4 Results & Analysis
4.1 Cluster Similarity
The graph in Figure 2(a) shows f-scores of clus-
tering methods with the whole-world set. None
achieve an f-score greater than 0.15, and most
perform even worse when the number of clusters
matches the number of genetic families or sub-
families. This indicates that the induced clusters
based on typological features are very different
from genetic groupings.
The question of similarity between these in-
duced clusters and the genetic families is however
a separate one from how those clusters perform in
predicting typological feature values.
4.2 Prediction Accuracy
To determine the amount of similarity between
languages within clusters, we instead look at pre-
diction accuracy across clustering methods and
the genetic groups. These scores are similar to
those given in Daume? (2009), though not directly
comparable due to small discrepancies in the size
of the data set. As can be seen by the numbers
in Table 3 and the graph in 2(b), despite the lack
of similarity between clustering methods and the
genetic groups, the clustering methods produce
as good or better prediction accuracies. Further-
more, the agglo and bagglo hierarchical clus-
tering methods which are favored for producing
phylogenetically motivated clusters do indeed re-
sult in higher f-score similarity to the genetic clus-
ters than the partitional rb and direct methods,
but produce poorer prediction-accuracy results.
In fact, it is not surprising that some induced
clusters outperform the genetic groupings in pre-
diction accuracy, considering that clustering algo-
rithms often want to maximize the similarity be-
tween languages in the same clusters. Now that
we know similarity between languages does not
necessarily mirror language family membership,
the next question is what features tend to stay the
same among languages in the same language fam-
ilies.
4.3 Feature Selection
Our final experiment was to examine the features
in WALS themselves, and look for features that
appear to vary the least within families, and act as
better predictors of family membership.
In order to do this, we again looked at predic-
tion accuracy information on a feature-by-feature
basis. The results from this experiment are shown
in Table 4, which gives a breakdown of how fea-
tures rank both individually and by category.
Since this table is built upon genetic relation-
ships, it is not surprising that the category for
?Lexicon? appears to be the most reliably stable
category. As noted in (McMahon, 1994), lexi-
cal cognates are often used as good evidence for
determining a shared ancestry. We also find that
word order is rather stable within a family.
We ran one further experiment where, using the
agglo clustering method that provided clusters
most similar to the genetic families previously,
only features that showed accuracies above 50%.
This eliminated 28 features, leaving 111 higher-
scoring features for the whole-world set. Pruning
the features to use only these selected for their sta-
bility within the genetic groupings yielded a very
small increase in f-score similarity, as can be seen
in Figure 3. Although this increase is small, it sug-
gests that more advanced feature selection meth-
ods may be able to reveal language features that
are more resistant to language contact and lan-
guage change.
5 Error Analysis
There are two main reasons for the differences be-
tween induced clusters and genetic groupings.
5.1 Language Similarity vs. Genetic
Relatedness
As mentioned before, language similarity and ge-
netic relatedness are two different concepts. Simi-
389
baseline gold rb agglo bagglo direct k-medoids withsimilarity overlap
k-medoids with
cosine similarity
Whole-World-Set (121 Clusters)
F-Score 0.087 ? 0.080 0.140 0.119 0.089 0.081 0.088
Acc (%) 53.72 63.43 64.33 62.86 61.44 65.47 62.11 63.36
Indo-European Subset (10 Clusters)
F-Score 0.319 ? 0.365 0.377 0.391 0.355 0.352 0.331
Acc (%) 64.27 74.1 71.12 72.26 70.62 74.13 73.36 72.12
Sino-Tibetan Subset (14 Clusters)
F-Score 0.305 ? 0.224 0.340 0.333 0.220 0.285 0.251
Acc (%) 58.08 61.71 63.93 63.74 63.06 65.31 64.55 63.94
Table 3: Comparison of clustering algorithms when the number of clusters is set to the same number of
genetic groupings. The highest number in each row is in boldface.
F-Sc
ore
0.04
0.06
0.08
0.10
0.12
0.14
0.16
Number of Clusters20 40 60 80 100 120 140 160 180 200
(a) F-scores of clustering results
Pred
ictio
n Ac
cura
cy
56
58
60
62
64
66
Number of Clusters20 40 60 80 100 120 140 160 180 200
CLUTO-rbCLUTO-agglo CLUTO-baggloCLUTO-directKmedoid-overlap Kmedoid-cosine Gold
(b) Prediction accuracy
Figure 2: Comparison of the performances of different clustering methods using the whole-world data
set. The number of groups in the gold standard (i.e., genetic grouping) is shown as a vertical dashed
line in 2(a) and 2(b), and the prediction accuracy of the gold standard as a horizontal solid line in 2(b).
F-Sc
ore
0.09
0.10
0.11
0.12
0.13
0.14
0.15
0.16
Number of Clusters20 40 60 80 100 120 140 160 180 200
agglo - all featuresagglo - predictive features
Figure 3: F-scores of the agglo clustering
method when using all the features vs. only fea-
tures whose prediction accuracy by the genetic
grouping is higher than 50%.
lar languages might not be genetically related and
dissimilar languages might be genetically related.
An example is given in Table 5. Persian and En-
glish are both Indo-European languages, but look
very different typologically; in contrast, Finnish
and English are not genetically related but they
look more similar typologically. While English
and Persian are related, they have been diverg-
ing in geographically distant areas for thousands
of years. Thus, the fact that English appears to
share more features with a geographically closer
Finnish is expected.
5.2 WALS as the Dataset
Perhaps the biggest challenge we encounter in this
project has been the dataset itself. WALS has cer-
tain properties that complicate the task.
Data Sparsity and Shared Features
While the previous example shows unrelated
languages can be quite similar typologically, our
clustering methods put two closely related lan-
guages, Eastern and Western Armenian, into dif-
390
Breakdown by Feature Category Breakdown By Feature: Top 10 Breakdown by Feature: Bottom 10
Category Accuracy Feature Acc C V Feature Acc C V
Whole-World Set
Lexicon 75.0% (136) M-T Pronouns 94.0% 230 3 (1) Consonant Inventories 32.6% 561 5
Word Order 68.6% (18) Absence of Common Consonants 93.7% 565 6 (133) Number of Basic Color Categories 33.3% 119 7
Phonology 65.9% (11) Front Rounded Vowels 91.1% 560 4 (23) Locus of Marking in the Clause 33.9% 236 5
Complex Sentences 64.0% (73) The Optative 89.6% 319 2 (71) The Prohibitive 34.6% 495 4
Nominal Syntax 63.2% (137) N-M Pronouns 87.9% 230 3 (22) Inflectional Synthesis of the Verb 35.1% 145 7
Verbal Categories 61.9% (6) Uvular Consonants 85.0% 565 4 (56) Conjunctions and Universal Quantifiers 38.2% 116 3
Simple Clauses 60.5% (130) Finger and Hand 84.4% 591 2 (117) Predicative Possession 39.4% 240 5
Nominal Categories 59.1% (115) Negative Indefinite Pronouns 84.2% 206 4 (92) Position of Polar Question Particles 40.0% 775 6
Morphology 53.9% (19) Presence of Uncommon Consonants 83.0% 565 7 (38) Indefinite Articles 40.4% 473 5
Other 41.3% (58) Obligatory Possessive Inflection 81.4% 244 2 (50) Asymmetrical Case-Marking 40.7% 261 6
Indo-European Subset
Lexicon 86.4% (130) Finger and Hand 100.0% 35 2 (3) Consonant-Vowel Ratio 30.6% 31 5
Morphology 83.1% (118) Predicative Adjectives 100.0% 29 3 (92) Position of Polar Question Particles 34.6% 47 6
Word Order 79.6% (18) Absence of Common Consonants 100.0% 31 6 (78) Coding of Evidentiality 36.0% 23 6
Simple Clauses 76.6% (107) Passive Constructions 100.0% 19 2 (1) Consonant Inventories 42.4% 31 5
Nominal Categories 70.4% (88) Order of Demonstrative and Noun 97.2% 66 6 (2) Vowel Quality Inventories 44.4% 31 3
Phonology 66.7% (89) Order of Numeral and Noun 95.7% 64 4 (84) Order of Object, Oblique, and Verb 47.8% 20 6
Verbal Categories 62.1% (27) Reduplication 95.2% 20 3 (16) Weight Factors in Weight-Sensitive
Stress Systems
51.1% 53 7
(7) Glottalized Consonants 93.9% 31 8 (70) The Morphological Imperative 55.3% 53 5
(93) Position of Interrogative Phrases in Con-
tent Questions
93.9% 44 3 (44) Gender Distinctions in Independent Per-
sonal Pronouns
56.5% 19 6
(5) Voicing and Gaps in Plosive Systems 93.8% 31 5 (37) Definite Articles 59.2% 46 5
Sino-Tibetan Subset
Lexicon 100.0% (130) Finger and Hand 100.0% 8 2 (77) Semantic Distinctions of Evidentiality 9.1% 18 3
Word Order 67.7% (82) Order of Subject and Verb 100.0% 99 3 (78) Coding of Evidentiality 17.7% 18 6
Morphology 63.8% (119) Nominal and Locational Predication 100.0% 13 2 (4) Voicing in Plosives and Fricatives 20.7% 26 4
Simple Clauses 60.9% (86) Order of Genitive and Noun 100.0% 73 3 (1) Consonant Inventories 22.2% 26 5
Verbal Categories 60.7% (129) Hand and Arm 100.0% 8 2 (14) Fixed Stress Locations 25.0% 4 7
Nominal Categories 55.8% (18) Absence of Common Consonants 100.0% 26 6 (15) Weight-Sensitive Stress 25.0% 4 8
Phonology 50.7% (93) Pos. of Interr. Phrases in Content Q?s 100.0% 79 3 (38) Indefinite Articles 31.7% 36 5
(85) Order of Adposition and Noun Phrase 97.5% 79 5 (120) Zero Copula for Predicate Nominals 37.5% 13 2
(95) Relationship b/t Object and Verb and Ad-
position and Noun Phrase
96.3% 76 5 (2) Vowel Quality Inventories 42.9% 26 3
(48) Person Marking on Adpositions 93.3% 14 4 (3) Consonant-Vowel Ratio 46.7% 26 5
Table 4: Prediction accuracy figures derived from genetic groupings for each dataset and broken down
by WALS feature category and feature. Ordering is by descending accuracy for the top 10 features,
and by increasing accuracy for the bottom 10 features. The ?C? and ?V? columns give the number
of languages in the set that a feature appears in, and the number of possible values for that feature,
respectively.
ferent clusters. A quick review shows that the rea-
son for this mistake is due to a lack of shared fea-
tures in WALS. Table 6 shows that very few fea-
tures are specified for both languages. The data
sparsity problem and the distribution of empty
cells adversely affect clustering results.
Notice that in this example, the features whose
values are filled for both languages actually have
identical feature values. While using shared over-
lap as a similarity measure can capture the simi-
larity between these two languages, this measure
biases clustering toward features with fewer cells
filled out. The only way out of errors like this, it
seems, is to obtain more data.
There are a few other typological databases
that might be drawn upon to define a more com-
plete set of data: PHOIBLE, (Moran and Wright,
2009), ODIN (Lewis, 2006), and the AUTOTYP
database (Nichols and Bickel, 2009). Using these
databases to fill in the gaps in data may be the only
way to fully address these issues.
The Feature Set in WALS
The features in WALS are not systematically
chosen for full typological coverage; rather, the
contributors to WALS decide what features they
want to work on based on their expertise. Also,
some features in WALS overlap; for example, one
WALS feature looks at the order between subject,
verb, and object, and another feature checks the
order between verb and object. As a result, the
feature set in WALS might not be a good represen-
tative of the properties of the languages covered in
the database.
6 Conclusion & Further Work
By comparing clusters derived from typological
features to genetic groups in the world?s lan-
guages, we have found two interesting results.
First, the induced clusters look very different from
genetic grouping and this is partly due to the de-
sign of WALS. Second, despite the differences, in-
duced clusters show similar, or even greater levels
391
ID: Feature Name English Finnish Persian
2: Vowel Quality Invento-
ries
Large (7-14) Large (7-14) Average (5-6)
6: Uvular Consonants None None Uvular stops only
11: Front Rounded Vow-
els
None High and Mid None
27: Reduplication No productive redupli-
cation
No productive redupli-
cation
Productive full and partial
reduplication
37: Definite Articles Definite word distinct
from demonstrative
No definite or indefinite
article
No definite, but indefinite
article
53: Ordinal Numerals First, second, three-th First, second, three-th First/one-th, two-th,
three-th
81: Order of Subject, Ob-
ject and Verb
SVO SVO SOV
85: Order of Adposition
and Noun Phrase
Prepositions Postpositions Prepositions
87: Order of Adjective
and Noun
Adjective-Noun Adjective-Noun Noun-Adjective
124: ?Want? Complement
Subjects
Subject left implicit Subject left implicit Subject expressed overtly
Number of Features 139 135 128
Cosine Similarity to Eng 1.00 0.56 0.42
Shared Overlap with Eng 1.00 0.56 0.44
Table 5: A selection of ten features from English, Finnish, and Persian. Same feature values in each
row are in boldface. Despite the genetic relation between English and Persian, similarity metrics place
English closer to Finnish than Persian.
ID# Feature Name Armenian (Eastern) Armenian (Western)
1 Consonant Inventories Small ?
27 Reduplication Full Reduplication Only Full Reduplication Only
33 Coding of Nominal Plurality ? Plural suffix
48 Person Marking on Adj. None ?
81 Order of Subj. Obj., and V ? SOV
86 Order of Adposition and Noun Phrase Postpositions Postpositions
100 Alignment of Verbal Person Marking Accusative ?
129 Hand and Arm ? Identical
Number of Features 85 33
Cosine Similarity 0.22
Shared Overlap 1.00
Table 6: Comparison of features between Eastern and Western Armenian. Same feature values in each
row are in boldface. Empty cells are shown as ???.
of typological similarity than genetic grouping as
indicated by the prediction accuracy.
While these initial findings are interesting, us-
ing WALS as a dataset for this purpose leaves a lot
to be desired. Subsequent work that supplements
the typological data in WALS with the databases
mentioned in ?5.2 would help alleviate the data
sparsity and feature selection problems.
Another useful follow-up would be to perform
application-oriented evaluations. For instance,
evaluating the performance of syntactic projection
methods between languages determined to have
similar syntactic patterns, or using similar mor-
phological induction techniques on morphologi-
cally similar languages. With the development
of large typological databases such as WALS, we
hope to see more studies that take advantage of
resources for resource-rich languages when devel-
oping tools for typologically similar, but resource-
poor languages.
Acknowledgment This work is supported by
the National Science Foundation Grant BCS-
0748919. We would also like to thank Emily Ben-
der, Tim Baldwin, and three anonymous reviewers
for helpful comments.
392
References
Daume?, III, Hal and Daniel Marcu. 2005. A Bayesian
Model for Supervised Clustering with the Dirich-
let Process Prior. Journal of Machine Learning Re-
search, 6:1551?1577.
Daume?, III, Hal. 2009. Non-Parametric Bayesian
Areal Linguistics. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (HLT/NAACL), pages
593?601, Boulder, Colorado, June.
Estivill-Castro, Vladimir and Jianhua Yang. 2000.
A fast and robust general purpose clustering algo-
rithm. In Proc. of Pacific Rim International Con-
ference on Artificial Intelligence, pages 208?218.
Springer.
Hana, Jiri, Anna Feldman, and Chris Brew. 2004. A
Resource-light Approach to Russian Morphology:
Tagging Russian using Czech resources. In Pro-
ceedings of EMNLP 2004, Barcelona, Spain.
Haspelmath, Martin, Matthew S. Dryer, David Gil, and
Bernard Comrie. 2005. The World Atlas of Lan-
guage Structures. Oxford University Press, Oxford,
England.
Lewis, William D. 2006. ODIN: A Model for Adapt-
ing and Enriching Legacy Infrastructure. In Pro-
ceedings of the e-Humanities Workshop, held in co-
operation with e-Science 2006: 2nd IEEE Interna-
tional Conference on e-Science and Grid Comput-
ing, Amsterdam.
McMahon, April M. S. 1994. Understanding lan-
guage change. Cambridge University Press, Cam-
bridge; New York, NY, USA.
Moran, Steven and Richard Wright. 2009. Phonetics
Information Base and Lexicon (PHOIBLE). Online:
http://phoible.org.
Nichols, Johanna and Balthasar Bickel. 2009.
The AUTOTYP genealogy and geography database:
2009 release. http://www.uni-leipzig.
de/?autotyp.
Rand, William M. 1971. Objective criteria for the
evaluation of clustering methods. Journal of the
American Statistical Association, 66(336):846?850.
Steinbach, Michael, George Karypis, and Vipin Ku-
mar. 2000. A comparison of document clustering
techniques. In Proceedings of Workshop at KDD
2000 on Text Mining.
Xia, Fei and William D. Lewis. 2007. Multilin-
gual structural projection across interlinear text.
In Proc. of the Conference on Human Language
Technologies (HLT/NAACL 2007), pages 452?459,
Rochester, New York.
Yarowsky, David and Grace Ngai. 2001. Inducing
multilingual pos taggers and np bracketers via ro-
bust projection across aligned corpora. In Proc. of
the Second meeting of the North American Chapter
of the Association for Computational Linguistics on
Language technologies (NAACL-2001), pages 1?8,
Morristown, NJ, USA.
393
Proceedings of the EACL 2009 Demonstrations Session, pages 41?44,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
Parsing, Projecting & Prototypes: Repurposing
Linguistic Data on the Web
William D. Lewis
Microsoft Research
Redmond, WA 98052
wilewis@microsoft.com
Fei Xia
University of Washington
Seattle, WA 98195
fxia@u.washington.edu
1 Introduction
Until very recently, most NLP tasks (e.g., parsing, tag-
ging, etc.) have been confined to a very limited number
of languages, the so-called majority languages. Now,
as the field moves into the era of developing tools for
Resource Poor Languages (RPLs)?a vast majority of
the world?s 7,000 languages are resource poor?the
discipline is confronted not only with the algorithmic
challenges of limited data, but also the sheer difficulty
of locating data in the first place. In this demo, we
present a resource which taps the large body of linguis-
tically annotated data on the Web, data which can be re-
purposed for NLP tasks. Because the field of linguistics
has as its mandate the study of human language?in
fact, the study of all human languages?and has whole-
heartedly embraced the Web as a means for dissemi-
nating linguistic knowledge, the consequence is that a
large quantity of analyzed language data can be found
on the Web. In many cases, the data is richly annotated
and exists for many languages for which there would
otherwise be very limited annotated data. The resource,
the Online Database of INterlinear text (ODIN), makes
this data available and provides additional annotation
and structure, making the resource useful to the Com-
putational Linguistic audience.
In this paper, after a brief discussion of the previous
work on ODIN, we report our recent work on extend-
ing ODIN by applying machine learning methods to
the task of data extraction and language identification,
and on using ODIN to ?discover? linguistic knowledge.
Then we outline a plan for the demo presentation.
2 Background and Previous work on
ODIN
ODIN is a collection of Interlinear Glossed Text (IGT)
harvested from scholarly documents. In this section,
we describe the original ODIN system (Lewis, 2006),
and the IGT enrichment algorithm (Xia and Lewis,
2007). These serve as the starting point for our current
work, which will be discussed in the next section.
2.1 Interlinear Glossed Text (IGT)
In recent years, a large part of linguistic scholarly dis-
course has migrated to the Web, whether it be in the
form of papers informally posted to scholars? websites,
or electronic editions of highly respected journals. In-
cluded in many papers are snippets of language data
that are included as part of this linguistic discourse.
The language data is often represented as Interlinear
Glossed Text (IGT), an example of which is shown in
(1).
(1) Rhoddodd yr athro lyfr i?r bachgen ddoe
gave-3sg the teacher book to-the boy yesterday
?The teacher gave a book to the boy yesterday?
(Bailyn, 2001)
The canonical form of an IGT consists of three lines:
a language line for the language in question, a gloss
line that contains a word-by-word or morpheme-by-
morpheme gloss, and a translation line, usually in En-
glish. The grammatical annotations such as 3sg on the
gloss line are called grams.
2.2 The Original ODIN System
ODIN was built in three steps. First, linguistic docu-
ments that may contain instances of IGT are harvested
from the Web using metacrawls. Metacrawling in-
volves throwing queries against an existing search en-
gine, such as Google and Live Search.
Second, IGT instances in the retrieved documents
are identified using regular expression ?templates?, ef-
fectively looking for text that resembles IGT. An exam-
ple RegEx template is shown in (2), which matches any
three-line instance (e.g., the IGT instance in (1)) such
that the first line starts with an example number (e.g.,
(1)) and the third line starts with a quotation mark.
(2) \s*\(\d+\).*\n
\s*.*\n
\s*\[??"].*\n
The third step is to determine the language of the
language line in an IGT instance. Our original work in
language ID relied on TextCat, an implementation of
(Cavnar and Trenkle, 1994).
As of January 2008 (the time we started our current
work), ODIN had 41,581 instances of IGT for 731 lan-
guages extracted from nearly 3,000 documents.1
1For a thorough discussion about how ODIN was origi-
nally constructed, see (Lewis, 2006).
41
2.3 Enriching IGT data
Since the language line in IGT data does not come with
annotations (e.g., POS tags, phrase structures), Xia and
Lewis (2007) proposed to enrich the original IGT and
then extract syntactic information (e.g., context-free
rules) to bootstrap NLP tools such as POS taggers and
parsers. The enrichment algorithm has three steps: (1)
parse the English translation with an English parser, (2)
align the language line and the English translation via
the gloss line, and (3) project syntactic structure from
English to the language line. The algorithm was tested
on 538 IGTs from seven languages and the word align-
ment accuracy was 94.1% and projection accuracy (i.e.,
the percentage of correct links in the projected depen-
dency structures) was 81.5%.
3 Our recent work
We extend the previous work in three areas: (1) im-
proving IGT detection and language identification, (2)
testing the usefulness of the enriched IGT by answer-
ing typological questions, and (3) enhancing ODIN?s
search facility by allowing structural and ?construc-
tion? searches.2
3.1 IGT detection
The canonical form of IGT, as presented in Section 2.1,
consists of three parts and each part is on a single line.
However, many IGT instances, 53.6% of instances in
ODIN, do not follow the canonical format for various
reasons. For instance, some IGT instances are missing
gloss or translation lines as they can be recovered from
context (e.g., other neighboring examples or the text
surrounding the instance); other IGT instances have
multiple translations or language lines (e.g., one part in
the native script, and another in a Latin transliteration).
Because of the irregular structure of IGT instances,
the regular expression templates used in the original
ODIN system performed poorly. We apply machine
learning methods to the task. In particular, we treat the
IGT detection task as a sequence labeling problem: we
train a classifier to tag each line with a pre-defined tag
set,3 use the learner to tag new documents, and con-
vert the best tag sequence into a span sequence. When
trained on 41 documents (with 1573 IGT instances) and
tested on 10 documents (with 447 instances), the F-
score for exact match (i.e., two spans match iff they
are identical) is 88.4%, and for partial match (i.e., two
spans match iff they overlap) is 95.4%.4 In comparison,
the F-score of the RegEx approach on the same test set
is 51.4% for exact match and 74.6% for partial match.
2By constructions, we mean linguistically salient con-
structions, such as actives, passives, relative clauses, inverted
word orders, etc., in particular those we feel would be of the
most benefit to linguists and computational linguists alike.
3The tagset extends the standard BIO tagging scheme.
4The result is produced by a Maximum Entropy learner.
The results by SVM and CRF learners are similar. The details
were reported in (Xia and Lewis, 2008).
Table 1: The language distribution of the IGTs in
ODIN
Range of # of # of IGT % of IGT
IGT instances languages instances instances
> 10000 3 36,691 19.39
1000-9999 37 97,158 51.34
100-999 122 40,260 21.27
10-99 326 12,822 6.78
1-9 838 2,313 1.22
total 1326 189,244 100
3.2 Language ID
The language ID task here is very different from a typ-
ical language ID task. For instance, the number of lan-
guages in ODIN is more than a thousand and could po-
tentially reach several thousand as more data is added.
Furthermore, for most languages in ODIN, our training
data contains few to no instances of IGT. Because of
these properties, applying existing language ID algo-
rithms to the task does not produce satisfactory results.
As IGTs are part of a document, there are often
various cues in the document (e.g., language names)
that can help predict the language ID of the IGT in-
stances. We designed a new algorithm that treats the
language ID task as a pronoun resolution task, where
IGT instances are ?pronouns?, language names are ?an-
tecedents?, and finding the language name of an IGT
is the same as linking a pronoun (i.e., the IGT) to its
antecedent (i.e., the language name). The algorithm
outperforms existing, general-purpose language iden-
tification algorithms significantly. The detail of the al-
gorithm and experimental results is described in (Xia et
al., 2009).
Running the new IGT detection on the original three
thousand ODIN documents, the number of IGT in-
stances increases from 41,581 to 189,244. We then ran
the new language ID algorithm on the IGTs, and Table
1 shows the language distribution of the IGTs in ODIN
according to the output of the algorithm. For instance,
the third row says that 122 languages each have 100 to
999 IGT instances, and the 40,260 instances in this bin
account for 21.27% of all instances in ODIN.5
3.3 Answering typological questions
Linguistic typology is the study of the classification
of languages, where a typology is an organization of
languages by an enumerated list of logically possible
types, most often identified by one or more structural
features. One of the most well known and well studied
typological types, or parameters, is that of canonical
word order, made famous by Joseph Greenberg (Green-
berg, 1963).
5Some IGTs are marked by the authors of the crawled
documents as ungrammatical (usually with an asterisk ?*?
at the beginning of the language line). Those IGTs are kept
in ODIN too because they could be useful to other linguists,
the same reason that they were included in the original docu-
ments.
42
In (Lewis and Xia, 2008), we described a means
for automatically discovering the answers to a number
of computationally salient typological questions, such
as the canonical order of constituents (e.g., sentential
word order, order of constituents in noun phrases) or
the existence of particular constituents in a language
(e.g., definite or indefinite determiners). In these ex-
periments, we tested not only the potential of IGT to
provide knowledge that could be useful to NLP, but
also for IGT to overcome biases inherent to the op-
portunistic nature of its collection: (1) What we call
the IGT-bias, that is, the bias produced by the fact that
IGT examples are used by authors to demonstrate a par-
ticular fact about a language, causing the collection of
IGT for a language to suffer from a potential lack of
representativeness. (2) What we call the English-bias,
an English-centrism in the examples brought on by the
fact that most IGT examples provide a translation in
English, which can potentially affect subsequent en-
richment of IGT data, such as through structural pro-
jection. In one experiment, we automatically found the
answer to the canonical word order question for about
100 languages, and the accuracy was 99% for all the
languages with at least 40 IGT instances.6 In another
experiment, our system answered 13 typological ques-
tions for 10 languages with an accuracy of 90%. The
discovered knowledge can then be used for subsequent
grammar and tool development work.
The knowledge we capture in IGT instances?both
the native annotations provided by the linguists them-
selves, as well as the answers to a variety of typological
questions discovered in IGT?we use to populate lan-
guage profiles. These profiles are a recent addition to
the ODIN site, and are available for those languages
where sufficient data exists. Following is an example
profile:
<Profile>
<language code="WBP">Warlpiri</language>
<ontologyNamespace prefix="gold">
http://linguistic-ontology.org/gold.owl#
</ontologyNamespace>
<feature="word_order"><value>SVO</value></feature>
<feature="det_order"><value>DT-NN</value></feature>
<feature="case">
<value>gold:DativeCase</value>
<value>gold:ErgativeCase</value>
<value>gold:NominativeCase</value>
. . .
</Profile>
3.4 Enhancing ODIN?s Value to Computational
Linguistics: Search and Language Profiles
ODIN provides a variety of ways to search across its
data, in particular, search by language name or code,
language family, and even by annotations and their re-
lated concepts. Once data is discovered that fits the
particular pattern that a user is interested in, he/she can
6Some IGT instances are not sentences and therefore are
not useful for answering this question. Further, those in-
stances marked as ungrammatical (usually with an asterisk
?*?) are ignored for this and all the typological questions.
either display the data (where sufficient citation infor-
mation exists and where the data is relatively clean) or
locate documents in which the data exists. Additional
search facilities allow users to search across poten-
tially linguistically salient structures and return results
in the form of language profiles. Although language
profiles are by no means complete?they are subject
to the availability of data to fill in the answers within
the profiles?they provide a summary of automatically
available knowledge about that language as found in
IGT (or enriched IGT).
4 The Demo Presentation
Our focus in this demonstration will be on the query
features of ODIN. In addition, however, we will also
give some background on how ODIN was built, show
how we see the data in ODIN being used by both the
linguistic and NLP communities, and present the kind
of information available in language profiles. The fol-
lowing is our plan for the demo:
? Very brief discussion on the methods used to build
ODIN (as discussed in Section 2.2, 3.1, and 3.2)
? An overview of the IGT enrichment algorithm (as
discussed in Section 2.3).
? A presentation of ODIN?s search facility and
the results that can be returned, in partic-
ular language profiles (as discussed in Sec-
tion 3.3-3.4). ODIN?s current website is
http://uakari.ling.washington.edu/odin. Users
can also search ODIN using the OLAC7 search
interfaces at the LDC8 and LinguistList.9 Some
search examples are given below.
4.1 Example 1: Search by Language Name
The opening screen for ODIN allows the user to search
the ODIN database by clicking a specific language
name in the left-hand frame, or by typing all or part
of a name (finding closest matches). Once a language
is selected, our search tool will list all the documents
that have data for the language in question. The user
can then click on any of those documents, and search
tool will return the IGT instances found in those doc-
uments. Following linguistic custom and fair use re-
strictions, only instances of data that have citations are
displayed. An example is shown in Figure 1. Search by
language and name is by far the most popular search in
ODIN, given the hundreds of queries executed per day.
4.2 Example 2: Search by Linguistic
Constructions
This type of query looks either at enriched data in the
English translation, or at the projected structures in the
7Open Language Archives Community
8http://www.language-archives.org/tools/search/
9LinguistList has graciously offered to host ODIN, and it
is being migrated to http://odin.linguistlist.org. Completion
of this migration is expected sometime in April 2009.
43
Figure 1: IGT instances in a document
target language data. Figure 2 shows the list of linguis-
tic constructions that are currently covered.
Suppose the user clicks on ?Word Order: VSO?,
the search tool will retrieve all the languages in ODIN
that have VSO order according to the PCFGs extracted
from the projected phrase structures (Figure 3). The
user can then click on the Data link for any language in
the list to retrieve the IGT instances in that language.
Figure 2: List of linguistic constructions that are cur-
rently supported
5 Conclusion
In this paper, we briefly discussed our work on im-
proving the ODIN system, testing the usefulness of
the ODIN data for linguistic study, and enhancing the
search facility. While IGT data collected off the Web is
inherently noisy, we show that even a sample size of 40
IGT instances is large enough to ensure 99% accuracy
in predicting Word Order. In the future, we plan to con-
tinue our efforts to collect more data for ODIN, in order
to make it a more useful resource to the linguistic and
computational linguistic audiences. Likewise, we will
Figure 3: Languages in ODIN Determined to be VSO
further extend the search interface to allow more so-
phisticated queries that tap the full breadth of languages
that exist in ODIN, and give users greater access to the
enriched annotations and projected structures that can
be found only in ODIN.
References
John Frederick Bailyn. 2001. Inversion, Dislocation and Op-
tionality in Russian. In Gerhild Zybatow, editor, Current
Issues in Formal Slavic Linguistics.
W. B. Cavnar and J. M. Trenkle. 1994. N-gram-based text
categorization. In Proceedings of Third Annual Sympo-
sium on Document Analysis and Information Retrieval,
pages 161?175, Las Vegas, April.
Joseph H. Greenberg. 1963. Some universals of grammar
with particular reference to the order of meaningful el-
ements. In Joseph H. Greenberg, editor, Universals of
Language, pages 73?113. MIT Press, Cambridge, Mas-
sachusetts.
William D. Lewis and Fei Xia. 2008. Automatically Identi-
fying Computationally Relevant Typological Features. In
Proceedings of The Third International Joint Conference
on Natural Language Processing (IJCNLP), Hyderabad,
January.
William D. Lewis. 2006. ODIN: A Model for Adapting and
Enriching Legacy Infrastructure. In Proceedings of the e-
Humanities Workshop, Amsterdam. Held in cooperation
with e-Science 2006: 2nd IEEE International Conference
on e-Science and Grid Computing.
Fei Xia and William D. Lewis. 2007. Multilingual struc-
tural projection across interlinearized text. In Proceedings
of the North American Association of Computational Lin-
guistics (NAACL) conference.
Fei Xia and William D. Lewis. 2008. Repurposing Theoret-
ical Linguistic Data for Tool Development and Search. In
Proceedings of The Third International Joint Conference
on Natural Language Processing (IJCNLP), Hyderabad,
January.
Fei Xia, William D. Lewis, and Hoifung Poon. 2009. Lan-
guage ID in the Context of Harvesting Language Data off
the Web. In Proceedings of The 12th Conference of the Eu-
ropean Chapter of the Association of Computational Lin-
guistics (EACL), Athens, Greece, April.
44
Proceedings of the ACL 2010 Conference Short Papers, pages 220?224,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Intelligent Selection of Language Model Training Data
Robert C. Moore William Lewis
Microsoft Research
Redmond, WA 98052, USA
{bobmoore,wilewis}@microsoft.com
Abstract
We address the problem of selecting non-
domain-specific language model training
data to build auxiliary language models
for use in tasks such as machine transla-
tion. Our approach is based on comparing
the cross-entropy, according to domain-
specific and non-domain-specifc language
models, for each sentence of the text
source used to produce the latter language
model. We show that this produces better
language models, trained on less data, than
both random data selection and two other
previously proposed methods.
1 Introduction
Statistical N-gram language models are widely
used in applications that produce natural-language
text as output, particularly speech recognition and
machine translation. It seems to be a univer-
sal truth that output quality can always be im-
proved by using more language model training
data, but only if the training data is reasonably
well-matched to the desired output. This presents
a problem, because in virtually any particular ap-
plication the amount of in-domain data is limited.
Thus it has become standard practice to com-
bine in-domain data with other data, either by
combining N-gram counts from in-domain and
other data (usually weighting the counts in some
way), or building separate language models from
different data sources, interpolating the language
model probabilities either linearly or log-linearly.
Log-linear interpolation is particularly popular
in statistical machine translation (e.g., Brants et
al., 2007), because the interpolation weights can
easily be discriminatively trained to optimize an
end-to-end translation objective function (such as
BLEU) by making the log probability according to
each language model a separate feature function in
the overall translation model.
The normal practice when using multiple lan-
guages models in machine translation seems to be
to train models on as much data as feasible from
each source, and to depend on feature weight opti-
mization to down-weight the impact of data that is
less well-matched to the translation application. In
this paper, however, we show that for a data source
that is not entirely in-domain, we can improve the
match between the language model from that data
source and the desired application output by intel-
ligently selecting a subset of the available data as
language model training data. This not only pro-
duces a language model better matched to the do-
main of interest (as measured in terms of perplex-
ity on held-out in-domain data), but it reduces the
computational resources needed to exploit a large
amount of non-domain-specific data, since the re-
sources needed to filter a large amount of data are
much less (especially in terms of memory) than
those required to build a language model from all
the data.
2 Approaches to the Problem
Our approach to the problem assumes that we have
enough in-domain data to train a reasonable in-
domain language model, which we then use to
help score text segments from other data sources,
and we select segments based on a score cutoff op-
timized on held-out in-domain data.
We are aware of two comparable previous ap-
proaches. Lin et al (1997) and Gao et al (2002)
both used a method similar to ours, in which the
metric used to score text segments is their perplex-
ity according to the in-domain language model.
The candidate text segments with perplexity less
than some threshold are selected.
The second previous approach does not explic-
itly make use of an in-domain language model, but
is still applicable to our scenario. Klakow (2000)
estimates a unigram language model from the
entire non-domain-specific corpus to be selected
220
from, and scores each candidate text segment from
that corpus by the change in the log likelihood
of the in-domain data according to the unigram
model, if that segment were removed from the cor-
pus used to estimate the unigram model. Those
segments whose removal would decrease the log
likelihood of the in-domain data more than some
threshold are selected.
Our method is a fairly simple variant of scoring
by perplexity according to an in-domain language
model. First, note that selecting segments based
on a perplexity threshold is equivalent to selecting
based on a cross-entropy threshold. Perplexity and
cross-entropy are monotonically related, since the
perplexity of a string s according to a model M is
simply bHM (s), where HM (s) is the cross-entropy
of s according to M and b is the base with re-
spect to which the cross-entropy is measured (e.g.,
bits or nats). However, instead of scoring text seg-
ments by perplexity or cross-entropy according to
the in-domain language model, we score them by
the difference of the cross-entropy of a text seg-
ment according to the in-domain language model
and the cross-entropy of the text segment accord-
ing to a language model trained on a random sam-
ple of the data source from which the text segment
is drawn.
To state this formally, let I be an in-domain data
set and N be a non-domain-specific (or otherwise
not entirely in-domain) data set. Let HI(s) be the
per-word cross-entropy, according to a language
model trained on I , of a text segment s drawn from
N . Let HN (s) be the per-word cross-entropy of s
according to a language model trained on a ran-
dom sample of N . We partition N into text seg-
ments (e.g., sentences), and score the segments ac-
cording to HI(s) ? HN (s), selecting all text seg-
ments whose score is less than a threshold T .
This method can be justified by reasoning sim-
liar to that used to derive methods for training
binary text classifiers without labeled negative
examples (Denis et al, 2002; Elkin and Noto,
2008). Let us imagine that our non-domain-
specific corpus N contains an in-domain subcor-
pus NI , drawn from the same distribution as our
in-domain corpus I . Since NI is statistically just
like our in-domain data I , it would seem to be a
good candidate for the data that we want to extract
from N . By a simple variant of Bayes rule, the
probability P (NI |s,N) of a text segment s, drawn
randomly from N , being in NI is given by
P (NI |s,N) =
P (s|NI , N)P (NI |N)
P (s|N)
Since NI is a subset of N , P (s|NI , N) =
P (s|NI), and by our assumption about the rela-
tionship of I and NI , P (s|NI) = P (s|I). Hence,
P (NI |s,N) =
P (s|I)P (NI |N)
P (s|N)
If we could estimate all the probabilities in the
right-hand side of this equation, we could use it
to select text segments that have a high probability
of being in NI .
We can estimate P (s|I) and P (s|N) by train-
ing language models on I and a sample of N , re-
spectively. That leaves us only P (NI |N), to es-
timate, but we really don?t care what P (NI |N)
is, because knowing that would still leave us won-
dering what threshold to set on P (NI |s,N). We
don?t care about classification accuracy; we care
only about the quality of the resulting language
model, so we might as well just attempt to find
a threshold on P (s|I)/P (s|N) that optimizes the
fit of the resulting language model to held-out in-
domain data.
Equivalently, we can work in the log domain
with the quantity log(P (s|I)) ? log(P (s|N)).
This gets us very close to working with the differ-
ence in cross-entropies, because HI(s)?HN (s) is
just a length-normalized version of log(P (s|I))?
log(P (s|N)), with the sign reversed. The rea-
son that we need to normalize for length is that
the value of log(P (s|I)) ? log(P (s|N)) tends to
correlate very strongly with text segment length.
If the candidate text segments vary greatly in
length?e.g., if we partition N into sentences?
this correlation can be a serious problem.
We estimated this effect on a 1000-sentence
sample of our experimental data described be-
low, and found the correlation between sentence
log probability difference and sentence length to
be r = ?0.92, while the cross-entropy differ-
ence was almost uncorrelated with sentence length
(r = 0.04). Hence, using sentence probability ra-
tios or log probability differences as our scoring
function would result in selecting disproportion-
ately very short sentences. We tested this in an
experiment not described here in detail, and found
it not to be significantly better as a selection crite-
rion than random selection.
221
Corpus Sentence count Token count
Gigaword 133,310,562 3,445,946,266
Europarl train 1,651,392 48,230,859
Europarl test 2,000 55,566
Table 1: Corpus size statistics
3 Experiments
We have empirically evaluated our proposed
method for selecting data from a non-domain-
specific source to model text in a specific domain.
For the in-domain corpus, we chose the English
side of the English-French parallel text from re-
lease v5 of the Europarl corpus (Koehn, 2005).
This consists of proceedings of the European Par-
liament from 1999 through 2009. We used the
text from 1999 through 2008 as in-domain train-
ing data, and we used the first 2000 sentences
from January 2009 as test data. For the non-
domain-specific corpus, we used the LDC Eng-
lish Gigaword Third Edition (LDC Catalog No.:
LDC2007T07).
We used a simple tokenization scheme on all
data, splitting on white space and on boundaries
between alphanumeric and nonalphanumeric (e.g.,
punctuation) characters. With this tokenization,
the sizes of our data sets in terms of sentences and
tokens are shown in Table 1. The token counts in-
clude added end-of-sentence tokens.
To implement our data selection method we re-
quired one language model trained on the Europarl
training data and one trained on the Gigaword
data. To make these language models comparable,
and to show the feasibility of optimizing the fit to
the in-domain data without training a model on the
entire Gigaword corpus, we trained the Gigaword
language model for data selection on a random
sample of the Gigaword corpus of a similar size to
that of the Europarl training data: 1,874,051 sen-
tences, 48,459,945 tokens.
To further increase the comparability of these
Europarl and Gigaword language models, we re-
stricted the vocabulary of both models to the to-
kens appearing at least twice in the Europarl train-
ing data, treating all other tokens as instances of
<UNK>. With this vocabulary, 4-gram language
models were trained on both the Europarl training
data and the Gigaword random sample using back-
off absolute discounting (Ney et al 1994), with a
discount of 0.7 used for all N-gram lengths. The
discounted probability mass at the unigram level
was added to the probability of <UNK>. A count
cutoff of 2 occurrences was applied to the trigrams
and 4-grams in estimating these models.
We computed the cross-entropy of each sen-
tence in the Gigaword corpus according to both
models, and scored each sentence by the differ-
ence in cross-entropy, HEp(s)?HGw(s). We then
selected subsets of the Gigaword data correspond-
ing to 8 cutoff points in the cross-entropy differ-
ence scores, and trained 4-gram models (again us-
ing absolute discounting with a discount of 0.7) on
each of these subsets and on the full Gigaword cor-
pus. These language models were estimated with-
out restricting the vocabulary or applying count
cutoffs, but the only parameters computed were
those needed to determine the perplexity of the
held-out Europarl test set, which saves a substan-
tial amount of computation in determining the op-
timal selection threshold.
We compared our selection method to three
other methods. As a baseline, we trained lan-
guage models on random subsets of the Gigaword
corpus of approximately equal size to the data
sets produced by the cutoffs we selected for the
cross-entropy difference scores. Next, we scored
all the Gigaword sentences by the cross-entropy
according to the Europarl-trained model alone.
As we noted above, this is equivalent to the in-
domain perplexity scoring method used by Lin et
al. (1997) and Gao et al (2002). Finally, we im-
plemented Klakow?s (2000) method, scoring each
Gigaword sentence by removing it from the Giga-
word corpus and computing the difference in the
log likelihood of the Europarl corpus according to
unigram models trained on the Gigaword corpus
with and without that sentence. With the latter two
methods, we chose cutoff points in the resulting
scores to produce data sets approximately equal in
size to those obtained using our selection method.
4 Results
For all four selection methods, plots of test set per-
plexity vs. the number of training data tokens se-
lected are displayed in Figure 1. (Note that the
training data token counts are displayed on a log-
arithmic scale.) The test set perplexity for the lan-
guage model trained on the full Gigaword corpus
is 135. As we might expect, reducing training
data by random sampling always increases per-
plexity. Selecting Gigaword sentences by their
222
100
120
140
160
180
200
220
240
0.01 0.1 1 10
Te
st
-s
et
 p
er
pl
ex
it
y
Billions of words of training data
random selection
in-domain cross-entropy scoring
Klakow's method
cross-entropy difference scoring
Figure 1: Test set perplexity vs. training set size
Selection Method Original LM PPL Modified LM PPL
in-domain cross-entropy scoring 124.4 124.8
Klakow?s method 110.5 110.8
cross-entropy difference scoring 100.7 101.9
Table 2: Results adjusted for vocabulary coverage
cross-entropy according to the Europarl-trained
model is effective in reducing both test set perplex-
ity and training corpus size, with an optimum per-
plexity of 124, obtained with a model built from
36% of the Gigaword corpus. Klakow?s method
is even more effective, with an optimum perplex-
ity of 111, obtained with a model built from 21%
of the Gigaword corpus. The cross-entropy differ-
ence selection method, however, is yet more effec-
tive, with an optimum perplexity of 101, obtained
with a model built from less than 7% of the Giga-
word corpus.
The comparisons implied by Figure 1, how-
ever, are only approximate, because each perplex-
ity (even along the same curve) is computed with
respect to a different vocabulary, resulting in a dif-
ferent out-of-vocabulary (OOV) rate. OOV tokens
in the test data are excluded from the perplexity
computation, so the perplexity measurements are
not strictly comparable.
Out of the 55566 test set tokens, the number
of OOV tokens ranges from 418 (0.75%), for the
smallest training set based on in-domain cross-
entropy scoring, to 20 (0.03%), for training on
the full Gigaword corpus. If we consider only
the training sets that appear to produce the lowest
perplexity for each selection method, however, the
spread of OOV counts is much narrower, ranging
53 (0.10%) for best training set based on cross-
entropy difference scoring, to 20 (0.03%), for ran-
dom selection.
To control for the difference in vocabulary, we
estimated a modified 4-gram language model for
each selection method (other than random se-
lection) using the training set that appeared to
produce the lowest perplexity for that selection
method in our initial experiments. In the modified
language models, the unigram model based on the
selected training set is smoothed by absolute dis-
counting, and backed-off to an unsmoothed uni-
gram model based on the full Gigaword corpus.
This produces language models that are normal-
ized over the same vocabulary as a model trained
on the full Gigaword corpus; thus the test set has
the same OOVs for each model.
Test set perplexity for each of these modifed
language models is compared to that of the orig-
inal version of the model in Table 2. It can be
seen that adjusting the vocabulary in this way, so
that all models are based on the same vocabulary,
223
yields only very small changes in the measured
test-set perplexity, and these differences are much
smaller than the differences between the different
selection methods, whichever way the vocabulary
of the language models is determined.
5 Conclusions
The cross-entropy difference selection method in-
troduced here seems to produce language mod-
els that are both a better match to texts in a re-
stricted domain, and require less data for train-
ing, than any of the other data selection methods
tested. This study is preliminary, however, in that
we have not yet shown improved end-to-end task
performance applying this approach, such as im-
proved BLEU scores in a machine translation task.
However, we believe there is reason to be opti-
mistic about this. When a language model trained
on non-domain-specific data is used in a statisti-
cal translation model as a separate feature func-
tion (as is often the case), lower perplexity on in-
domain target language test data derived from ref-
erence translations corresponds directly to assign-
ing higher language model feature scores to those
reference translations, which should in turn lead to
translation system output that matches reference
translations better.
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz
J. Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings
of the Joint Conference on Empirical Methods
in Natural Language Processing and Computa-
tional Natural Language Learning, June 28?30,
Prague, Czech Republic, 858?867.
Franc?ois Denis, Remi Gilleron, and Marc Tom-
masi. 2002. Text classification from positive
and unlabeled examples. In The 9th Interna-
tional Conference on Information Processing
and Management of Uncertainty in Knowledge-
Based Systems (IPMU 2002), 1927?1934.
Charles Elkin and Keith Noto. 2008. Learn-
ing classifiers from only positive and unlabeled
data. In KDD 2008, August 24?27, Las Vegas,
Nevada, USA, 213?220.
Jianfeng Gao, Joshua Goodman, Mingjing Li, and
Kai-Fu Lee. 2002. Toward a unified approach
to statistical language modeling for Chinese.
ACM Transactions on Asian Language Informa-
tion Processing, 1(1):3?33.
Dietrich Klakow. 2000. Selecting articles from
the language model training corpus. In ICASSP
2000, June 5?9, Istanbul, Turkey, vol. 3, 1695?
1698.
Philipp Koehn. 2005. Europarl: a parallel cor-
pus for statistical machine translation. In MT
Summit X, September 12?16, Phuket, Thailand,
79?86.
Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien,
Ker-Jiann Chen, and Lin-Shan Lee. 1997.
Chinese language model adaptation based on
document classification and multiple domain-
specific language models. In EUROSPEECH-
1997, 1463?1466.
Hermann Ney, Ute Essen, and Reinhard Kneser.
1994. On structuring dependencies in stochas-
tic language modelling. Computer Speech and
Language, 8:1?38.
224
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 501?511,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Crisis MT: Developing A Cookbook for MT in Crisis Situations
William D. Lewis
Microsoft Research
Redmond, WA 98052
wilewis@microsoft.com
Robert Munro
Stanford University
Stanford, CA 94305
rmunro@stanford.edu
Stephan Vogel
Carnegie Mellon University
Pittsburgh, PA 15213
stephan.vogel@cmu.edu
Abstract
In this paper, we propose that MT is an im-
portant technology in crisis events, something
that can and should be an integral part of a
rapid-response infrastructure. By integrating
MT services directly into a messaging infras-
tructure (whatever the type of messages being
serviced, e.g., text messages, Twitter feeds,
blog postings, etc.), MT can be used to pro-
vide first pass translations into a majority lan-
guage, which can be more effectively triaged
and then routed to the appropriate aid agen-
cies. If done right, MT can dramatically in-
crease the speed by which relief can be pro-
vided. To ensure that MT is a standard tool
in the arsenal of tools needed in crisis events,
we propose a preliminary Crisis Cookbook,
the contents of which could be translated into
the relevant language(s) by volunteers imme-
diately after a crisis event occurs. The result-
ing data could then be made available to relief
groups on the ground, as well as to providers
of MT services. We also note that there
are significant contributions that our commu-
nity can make to relief efforts through con-
tinued work on our research, especially that
research which makes MT more viable for
under-resourced languages.
1 Introduction
The connected world contains approximately 5000
languages ? at least that is how many languages you
could find at the other end of your phone right now.
However, the majority of these languages are under-
resourced, and they have few or no digital resources.
In the event of a sudden onset crisis, people will
immediately begin using their communication tech-
nologies ? and their languages ? to report their situ-
ations, request help, and seek out loved ones. Yet,
in the event that such a crisis occurs in a region
of the world where an under-resourced language is
spoken, delivery of support or aid could be affected
due to the inability to communicate. This was felt
most strongly in the wake of the January 12, 2010
earthquake in Haiti. Local emergency response ser-
vices were inoperable, but 70-80% of cell-towers
were quickly restored. With 83% of men and 67% of
women possessing cellphones, the nation remained
largely connected. People within Haiti were texting,
calling, and interacting with social media, primarily
in Haitian Kreyo`l (Munro, 2011). Yet, most of the
aid that was being delivered to the country ? initially,
soley by the American Military ? was being deliv-
ered by groups that did not communicate in Kreyo`l.
It was the first time that the world has seen a large-
scale sudden onset crisis in a region with productive
digital communications in an under-resourced lan-
guage, but it certainly will not be the last.
We strongly believe that MT is an important tech-
nology to facilitate communication in crisis situa-
tions, crucially since it can make content in a lan-
guage spoken or written by a local population ac-
cessible to those that do not know the language, in
particular aid agencies. Multiple groups saw MT
as a grand challenge in the Haitian crisis, and they
set to work to make MT available as soon as pos-
sible after the crisis. Within two weeks of the cri-
sis, the first two MT engines were built and were
available to those who needed them. We believe that
we can make MT available just as quickly in future
crises, and, with the right preparation, tightly inte-
grate MT into the communication infrastructure that
is deployed (e.g., the text messaging infrastructure).
The challenge is doing the work now to make this
vision possible.
In this paper, we describe the technologies that
came to play in the Haitian crisis, how Haitian
Kreyo`l MT was developed, the problems of surprise
languages and low resource MT, and detail the re-
search and technologies, cast as a ?Crisis MT Cook-
book?, that will be essential for MT to form a core
role in future crises. In Sections 2, 3, and 4 we dis-
cuss Mission 4636 and the technologies that came
501
into play in Haiti and other recent crises, and the
role that technologies can and should play in future
crises. In Section 5, we discuss what made Haitian
Kreyo`l a special case of a ?surprise language?, and
how MT was developed for the language. In Sec-
tion 6, we review the NLP and MT research areas
that will likely net big returns for under-resourced
languages. In Section 7, we review the need for an
MT Crisis Cookbook, and what the data and infras-
tructural components of the Cookbook should be.
Finally, in Section 8 we review a sample crisis time-
line, and how a crisis might play out with all the
components of the Cookbook available. Section 9
wraps up the paper.
2 Mission 4636
In Haiti, crowdsourced translation enabled com-
munications between the Kreyo`l-speaking Haitian
population and English-speaking emergency respon-
ders. A small group of international aid workers
established a phone-number,?4636?1 , that people
were able to send text messages to for free within
Haiti. The actual translations were made by about
2000 Kreyo`l2 and French speaking volunteers col-
laborating on an online microtasking platform that
they used to translate, categorize, identify missing
people and geolocate information on a map (Munro,
2010).3 After a month, this work was gradually
transferred to paid workers in Mirebalais, Haiti.
These messages, about 80,000 in total, were used
as part of the shared task for the 2011 Workshop
on Machine Translation. About 3,000 of the mes-
sages had the categories and coordinates refined by a
third workforce working with the Ushahidi platform
out of Boston.4 They published this information on
an online crisis map and worked directly with the
main emergency responder, the American Military,
to identify actionable information.
1See the ?Mission 4636? website at
http://www.mission4636.org for more information about
the organization and its efforts in Haiti.
2We use the term Kreyo`l for the Creole spoken in Haiti to
differentiate it from other Creoles. This is also in concord with
customary usage in Haiti.
3An author of this paper, Robert Munro, coordinated this
process and is a founding member and translation coordinator
for the Standby Task Force, which is discussed later in the paper.
4For more information on Ushahidi, see
http://www.ushahidi.org.
The strategy for translation was extremely effec-
tive - 80,000 messages equates to about 10 novels of
information, translated in real-time, lifting a burden
off people in Haiti. One high-ranking official de-
scribed the translation process as a ?perfect match?
of social media and traditional emergency response
(Anderson, 2010).
To meet the scale of translation needs, machine
translation services were quickly shipped. A mem-
ber of Mission 4636 built a high-precision, low-
coverage dictionary-based system that was used by
a number of translators. A couple of days later,
the world?s first publically accessible Stastical Ma-
chine Translation (SMT) engine for Kreyo`l was de-
veloped by Microsoft Research, with Google Re-
search following several days later with their own
engine.5 Although the statistical translation engines
were not used directly in the SMS translation ef-
fort, there is evidence they were used by those who
were involved in the relief effort, as determined by
blog postings and a review of translation logs show-
ing relief-centric translations. Although Kreyo`l is
not a high traffic language?it was not expected that
it would be?about 5% of the traffic in the weeks
and the months following the earthquake appeared
to be relief-related, suggesting that machine transla-
tion was being used those who needed it most.6 Had
MT been integrated directly into the text messaging
infrastructure used in Haiti, this percentage would
have been significantly larger.
3 Translation and crisis response - a
quickly changing field
To establish a ready-workforce to aid information
processing in relief efforts an organization called the
5A rough timeline of these developments can
be seen in the commentary posted to the Lan-
guage Log website (see specifically the archive at:
http://languagelog.ldc.upenn.edu/nll/?p=2068).
6The logs output by Microsoft Translator?s engine were ex-
amined, and categorized roughly into broad categories describ-
ing the type of content. These categories were: Relief Related
(suspected), Colloquial or Common Expressions (which could,
in fact, have been relief related), Chat, and Unknown. The anal-
ysis was done by hand on a random sample of 200 messages
from the many thousands of messages received within a couple
of months of the quake. There were a large number of strings
that were difficult to categorize, including many partial strings,
and a bias against Relief Related when it was not clear. Thus,
the 5% estimate is likely a conservative one.
502
Standby Task Force was established in late 2010. Its
founding members had worked together in the Haiti
and/or subsequent Pakistan response efforts. It cur-
rently has several hundred members who special-
ize in tasks like report mapping, verification, media
monitoring and translation. Of all the different tasks
that volunteers can perform, translation is the least
transferable from one crisis to the next.
Following from the lessons learned in Haiti,
crowdsourced and machine translation have been
combined for a number of aid efforts: vote monitor-
ing for the referendem in Southern Sudan (Arabic);
a UN-led earthquake simulation in Colombia (Span-
ish); and for crisis mapping following the tsunami in
Japan (Japanese).
When information is immediately translated into
a high resource language it can be quickly triaged by
a greater number of people. The more time-intensive
task of manually correcting any mistranslations can
be performed in parallel. This workflow of combin-
ing machine and crowdsourced translation is largely
a succesful one and is likely to become common
practice in humanitarian information processing.
The combination of manual and machine-
translation was found to be effective across unpre-
dictable input:
?An email came into the Sudan Vote Mon-
itor platform in Indonesian - your plugin
did a good job of translating it into English
and Arabic?
Helena Puig Larrauri, volunteer for Sudan
Vote Monitor (P.C.)
But not without errors, especially across vital
phrases like location names:
?Names of neighborhoods such as Salitre
or Puerta al Llano were not recognized as
such and unnecessarily being translated.?
Marta Poblet, volunteer for Colombia
earthquake simulation (P.C.)
When the uprisings hit Libya in early 2011 the
United Nations did not have the capacity to col-
lect vital ground-truth data in the lead up to their
involvement. Information about refugee numbers
and needs were on web-accessible articles and so-
cial media, as were reports about the movements of
government and rebel troops and vunerable popula-
tions within the country. But there simply wasn?t the
workforce within the UN to aggregate and verify so
much information. This was the first time the United
Nations directly engaged a volunteer workforce for
large-scale information processing, requesting the
Standby Tasks Force?s deployment. It was also
the first time that so much information had come
from social media, a potentially large but unstruc-
tured data source, but it gave the UN a huge head-
start in their efforts (Verity, 2011). Crowdsourced
and machine translation were also combined here,
but in this case by directly engaging Arabic speak-
ers in media monitoring and by using reports from
Meedan.7
In a crisis, it will now be more common than not
that the volume of available digital information will
surpass the volume of information that aid-workers
can collect directly from the ground. This rapid
change is being quickly met by a rapid change in
cloud-based and automated solutions to language
processing, especially machine translation.
4 Translation and low-resource languages
We were fortunate that Arabic, Spanish and
Japanese are high resource languages for which
online machine translation services already exist.
Speakers of low resource languages cannot currently
benefit from this kind of translation service and yet
low resource languages are disproportionally spoken
by the world?s most vunerable populations. Over the
last 12 months many problems have been solved re-
garding the workflow of managing crisis data, but
one of the biggest remaining problems is the abil-
ity to quickly deploy machine-translation systems to
augment relief efforts.
While translation is not widely discussed aspect
of crisis response, it is ?a perennial hidden issue?
(Disaster 2.0, 2011):
?Go and look at any evaluation from the
last ten or fifteen years. ?Recommenda-
tion: make effective information available
7Meedan is an NGO that seeks to create greater understand-
ing between the Arabic and English speaking world by translat-
ing media reports and blogs between the languages, combining
quick machine-translation with corrections by a volunteer com-
munity.
503
to the government and the population in
their own language.? We didn?t do it . . . It
is a consistent thing across emergencies.?
Brendan McDonald, UN OCHA in (Dis-
aster 2.0, 2011)
Beyond the particular use case of small-to-
medium scale emergency information processing,
machine translation can also contribute to aid ef-
forts when the scale of information is beyond any
manual processing. In addition to the Libya deploy-
ment, a recent Red Cross survey (2010) found that
nearly half the respondents would use social media
to report emergencies. It simply would not be possi-
ble to translate all real-time reports when expressed
through social media, but translation into a high re-
source language could aid semi-automated methods
for discovering and prioritizing information.
There is, therefore, a great need to explore meth-
ods for rapid deployment of machine-translation
systems into minority languages. The questions that
we seek to address in this paper is how we as a com-
munity can prepare for the eventuality of the next
crisis, can draw from the lessons we learned in the
Haitian crisis, and might significantly impact the aid
effort in the next and future crises.
5 Surprise Languages: What Made Haiti
Different?
On January 19th, 2010, the Microsoft Research
Translator team received an e-mail from the field re-
questing that they develop an MT engine for Haitian
Kreyo`l to assist in the relief effort. At the time, no
publically available MT engine existed for Kreyo`l.
In less than five days, the Microsoft Translator site
was supporting the language. Given that it can take
weeks to months to develop an MT engine for a new
language, it would not seem possible that an engine
could be developed so quickly, especially for a low-
resource, minority language. The reasons this was
possible are varied, and are in some ways unique to
Kreyo`l.
Haitian Kreyo`l, as it turns out, has proven to be an
exceptional case for a surprise language. Unlike the
languages in Surprise Language Exercises of nearly
a decade ago (Oard, 2003; Oard and Och, 2003),
in which participants were given a month to collect
data and build language technologies for previously
unknown languages, including Machine Translation
systems, there was a surprising amount of data for
Kreyo`l at the start of the Haitian crisis, and it be-
came available relatively quickly. Partly, this is due
to the growth of the Web, which has proven to be
a surpisingly diverse multi-lingual resource. But it
also stems crucially from work that had been done
in the past on Kreyo`l, specifically, the work that was
done in the DIPLOMAT and NESPOLE! projects at
CMU (Frederking et al, 1997). It was possible to
assemble a reasonable sample of data for the lan-
guage in very short order (i.e., days). Further, since
the language itself is fairly reduced morphologically,
it is an easier target for SMT. In contrast, if one
were to sample a language at random from the set
of the 7,000 languages spoken on the earth, one is
more likely to find a language that is morpholog-
ically richer (e.g., fusional, aggutinating, polysyn-
thetic). Morphological richness compounds the data
sparsity problem, reducing the quality of the result-
ing SMT engines.
In other words, a combination of a simple
morphology combined with reasonably accessible
sources of data made the rapid deployment of MT
for Kreyo`l far more likely. That is not to say
that there weren?t problems. First, Kreyo`l is fairly
?young? as a written language8 , and is still in the
early stages of orthographic standardization and nor-
malization (Allen, 1998). This has led to inconsis-
tencies in the orthography that increases data sparse-
ness and noise. Further, Kreyo`l has multiple regis-
ters in its written form: a ?high? register that uses
full forms for pronouns and a set of function words,
and a ?low? register that corresponds more closely
to its spoken form, and is written with many con-
tractions. For example, the Kreyo`l word for the first
person pronoun is mwen. It can be written as mwen
(the high register), or contracted to m? (the low regis-
ter). The form can either be attached to the succeed-
ing word or written with a following space. Like-
wise, the first person possessive is also mwen which
is written following the word that is possessed. This
8Although Haitian Kreyo`l in written form goes back as far as
the late 18th century (see Lefebvre (1998) for material on some
of these texts), Kreyo`l as a written language did not become
more commonplace until the 20th century, not achieving official
status in Haiti until 1961.
504
can be written as ?m, and can be attached to the word
or delimited by a space. Both m? and ?m appear in
some texts as just m. The same patterns hold for all
pronouns, and some function words as well. See Ta-
ble 1 for a list of these reductions.
Table 1: Sample Pronouns and Reductions
Pronoun Gloss Appears as
mwen I, me, mine m, ?m, m?
nou you (pl), us n, ?n, n?
ou you w, w?
li he, she, it l, l?, ?l
Additionally, writers of Kreyo`l use a large num-
ber of abbreviated forms for common expressions, a
kind of shorthand. For example, ave`n can be used to
represent ave`k nou, mandem can be used for mande
mwen, etc. Overall, the number of alternations and
multi-way ambiguities also increases the level of
noise and data sparsity. 9
So, even with a morphologically reduced lan-
guage like Kreyo`l, one has issues with data sparsity
beyond the mere lack of availability of data. This
compounds the low-data aspect of the language.
Adding in a multitude of morphological variants, as
one might encounter in a Turkic language, or worse,
in an Inuit language, would only make the problem
more severe. The big challenge for Crisis MT is not
only to deal with the data availability problem, but
once one has the data in hand, to deal with the re-
duction in the utility of that data caused by noise
and the multiplication of word forms. These pose
major challenges to our community, which can be
countered through additional research, a motivated
and active community, and scores of rapidly applied
heuristics and data repairs.
6 Research Areas to Counter Data
Sparsity
As noted, the major problems with low-resource MT
is the lack of data and various data issues that in-
crease the sparsity of data already in short supply.
What are the research challenges? How can we
make MT viable quickly for low-resource and si-
multaneously morphologically rich languages?
9For more details of the Haitian Kreyo`l translation systems
developed at Microsoft Research, please see Lewis (2010).
The following constitutes a rough list of solu-
tions, many of which map to very interesting re-
search problems:
? Crowdsourcing ? Beyond the use of crowd-
sourcing in the crisis context itself (e.g., to
translate or process text messages, much as
what was done by Mission 4636), novel tech-
niques for tapping the crowd could also be used
to add or repair data:
? Repairing and evaluation ? In this sce-
nario, the crowd would be used to repair
data that is obviously noisy, evaluate prob-
lems with particular data points, or even
make simple determinations as to whether
the data in question is actually in the lan-
guage(s) of interest or too noisy to use.
? Translating content, generating new data ?
Given crowd sourced, micro-tasking plat-
forms such as Amazon?s Mechanical Turk
and Crowdflower, one can now easily tap
the crowd to generate new data. The ma-
jor challenge will be identifying if speak-
ers of the target language(s) are available
on the desired platform, and if not, if they
could be motivated to particpate.10 Like-
wise, infrastructure and resources will be
needed to evaluate the quality of the re-
sulting translations (Zaidan and Callison-
Burch, 2011).
? Active Crowd Translation ? This method
combines active learning with crowd-
sourcing for annotation of parallel data in
comparable resources, and can be used
to increase the amount of data that is
found (Ambati et al, 2011). Active learn-
ing might be applicable to other crowd-
sourcing tasks as well, such as being used
in crowdsourcing for translating content
or repairing translated content.
? Tapping non-traditional sources ? Critical to
traditional approaches of SMT is parallel train-
ing data. Parallel data is difficult to impossible
to come by for a large number of the world?s
10Based on the results of an informal survey, there may be
speakers of a hundred or more languages on Mechanical Turk.
See http://www.junglelightspeed.com/amt language/ for a list
of the languages that may be available on Turk.
505
languages. Tapping non-traditional sources of
data can help increase the supply of ever valu-
able training data for a language:
? Mining comparable sources of data ? min-
ing comparable data for parallel data has
a long history, including mining compara-
ble sources for named entities (Udupa et
al., 2009; Irvine et al, 2010; Hewavitha-
rana and Vogel, 2008; Hewavitharana and
Vogel, 2011), mining Wikipedia for paral-
lel content, including sentences (Smith et
al., 2010), and many more too numerous
to list. There is always room for improve-
ment and hybridization in this space, as
well as tapping additional sources of data,
such as the volumes of noisy comparable
data on the Web.
? Monolingual ? More recent work has fo-
cused on mining monolingual sources of
data, treating MT as a decipherment prob-
lem (Ravi and Knight, 2011), rather than
a source-target mapping problem.
? Dictionary bootstraps and backoffs ? De-
spite the absence of context, dictionar-
ies can be useful, especially for resolving
out-of-vocabulary items (OOVs). Many
bilingual dictionaries also contain exam-
ple sentences, which can be harvested and
used in training.
? Field data from linguists ? Given that lin-
guists have variously studied a large per-
centage of the world?s languages, tapping
the supply of data that they have accu-
mulated could prove quite fruitful. Some
recent work tapping annotated bitexts (at
this time, for over 1,200 languages) pro-
duced by linguists may prove useful in
the future (Lewis and Xia, 2010), if for
nothing more than to provide information
about linguistic structure (e.g., morpho-
logical complexity or divergences, poten-
tial distortion rates, and structural diver-
gence (a la Fox (2002))). Engaging with
the documentary linguistic community
and providing tools to facilitate the col-
lection of data might produce additional
data, especially data where alignment is
assisted through human input (Monson et
al., 2008).
? Novel ways of countering data sparsity
? Systematizing data cleaning heuristics ?
Undoubtedly, the same kinds of filtra-
tion and data cleaning heuristics used for
Kreyo`l could prove useful for speeding
up the processing of data for new lan-
guages. Applying Machine Learning tech-
niques to data filtration and data cleaning
could aid and generalize the process, thus
decreasing overall latency from acquisi-
tion to training.
? Strategies to make the source look more
like the target (or vice versa) ? A corol-
lary to data sparsity is faulty word align-
ment, where low frequency words fail to
get good alignments because there is not
enough data to reinforce fairly weak hy-
potheses, or where source-target distor-
tion is high. Both problems disfavor what
alignments do exist. If the source and tar-
get are reordered so that one side more
closely matches the other, or one side is
?enriched? to be more like the other, one
can reduce distortion related effects, and
might also counter the large number of
forms in morphologically rich languages
(e.g., (Yeniterzi and Oflazer, 2010; Gen-
zel, 2010), and many others).
? Strategies to systematically deal with complex
morphology ? this is one on-going area of re-
search that could still net large returns, since,
even with some relatively high-data languages,
such as Finnish, data is made sparser due to the
multiplication of possible forms. There is too
long a literature to really do justice here, but
some recent work includes discrimitative lexi-
cons (Jeong et al, 2010), sub-word alignment
strategies (Bodrumlu et al, 2009), learning the
morphological variants in a language (Oflazer
and El-kahlout, 2007), using off-the-shelf mor-
phological tools, e.g., Morfessor 11, etc.
? Use syntax or linguistic knowledge in the
translation task ? By reducing the hypothe-
sis space for possible alignments, syntax-based
11http://www.cis.hut.fi/projects/morpho/
506
approaches can do better in lower-data situa-
tions and can handle source-target discontinu-
ities better than straight phrase-based systems
(e.g., (Quirk and Menezes, 2006; Li et al,
2010)).
7 The MT Crisis Cookbook
Given the relatively narrow domain context of Cri-
sis MT?generally the needed vocabulary and data
should be centered on relief work, medical in-
teractions, and communicating with the affected
populations?it may be possible to approach Crisis
MT as we would MT for any domain (e.g., news,
government, etc.). With enough data relevant to a
particular domain or sub-domain (e.g., earthquake,
tsunami, nuclear disaster, flooding, etc.), it would
be possible to build the relevant translation memo-
ries (TMs) and train highly domain-specific MT en-
gines to produce translations of reasonable quality
and utility. Even with highly inflected languages, a
domain-specific approach may get around many of
the data sparsity issues.
It is also crucial that no data be thrown out. Re-
lief specific content that was relevant to an earlier
crisis can certainly contribute to subsequent crises.
Among these data are difficult to replicate sources
of data, such as SMS messages. This data would
constitute a highly domain specific set of data which
would only grow over time.
7.1 Outline of the Cookbook
The recipe for the MT Crisis Cookbook consists of
two parts:
1. The content that would be most useful in crisis
situations. This consists of relief-centric vocab-
ulary, phrases, sentences, and other material. It
should be in some common ?source? language,
likely English (English is a reasonable ?pivot?
in and out of many other languages, given the
ubiquity of English-to-X content).
2. The infrastructure to support relief workers,
aid agencies, and the affected population. As
made obvious in Haiti, an SMS messaging
infrastructure integrated into a crowd-sourced
translation infrastructure, proved to be crucial.
For future crises, this infrastructure should be
streamlined and have public MT APIs inte-
grated directly into it (to support first pass MT).
7.2 Cookbook Data
As noted in Section 5, one way to counter the data
sparsity problem is to build domain specific engines,
with a set of data ready-to-go in the event of a crisis.
This data, which would exist in English and possibly
other languages, would be translated into the target
language (if needed), distributed to to aid organiza-
tions (as needed), and used to train MT engines and
other language processing resources. The following
list constitutes a set of possible sources. It is by no
means complete (for instance, some resources spe-
cific to particular crisis types, e.g., floods, nuclear
disasters, etc. are not included), but it does repre-
sent a good central core of resources that should be
part of any Crisis Cookbook12 :
? Where There is No Doctor ? This is one of the
most recognized and widely used and useful
references in under-resourced regions around
the world. The publisher of the text, the Hespe-
rian Foundation, has already had the text trans-
lated into 75 languages, and it is available in
PDF as a free download from their website.13
? CMU Medical Domain Phrases, Sentences,
and Glossary ? Collected under the jointly
NSF/EU funded NESPOLE! and DIPLOMAT
projects (Frederking et al, 1997), this data con-
sists of common phrases and sentences that
would be useful in a crisis medical scenario,
and would be quite useful for training MT, as
it was for training the Kreyo`l engines. Only the
English side of this data would be relevant to
future crises.
? Anonymized Crisis-related SMS Messages ?
Relief-related SMS messages may be particu-
larly useful in future crises, since those col-
lected in a crisis scenario are likely to contain
content that transfers readily to similar crises.
A selected sample of the 80,000+ messages re-
sulting from the Haitian crisis could constitute
12Some of the resources listed here are under copyright.
There may need to be some negotiation with the copyright own-
ers to ensure that the texts can be used, and how they can be
used (e.g., to train MT, to be used in TMs, to be distributed in
hardcopy form, etc.).
13http://hesperian.org/
507
a reasonable core of SMS messages that could
be added to over time.
? Red Cross Emergency Multilingual Phrase-
book ? A small, but highly focused, set of
phrases and questions useful in an emergency
medical context. Available in multiple lan-
guages.
? Emergency and Crisis Communication Vocab-
ulary ? An example bilingual set was prepared
by the Canadian Government in both French
and English14 , consisting of a small list of
?official? terms needed in crisis situations, and
their associated descriptions. Although the
terms on the Canadian site are translated and
defined only in English and French and have
a bias to the Canadian government nomecla-
ture, having such a list of terms from multi-
ple government agencies and their definitions
could prove useful for relief vocabulary as well
as for vocabulary needed for official announce-
ments.
? High Frequency Wikipedia Disaster Content
? This would consist of vocabulary that re-
curs across multiple related crisis pages on
wikipedia. The idea is to harvest those terms
that repeat across multiple pages of the same
?sub-domain? (e.g., those that cover events
with floods, earthquakes, nuclear disasters,
etc.), but document disasters in different lo-
cales, where cross-page repeated vocabulary is
favored (substracting out high-frequency vo-
cabulary that occurs elsewhere). This vocab-
ulary could be distilled automatically from a
set of relevant pages, and would likely contain
core vocabulary for specific crisis and disas-
ter contexts. For instance, shared vocabulary
between the Japanese, Indonesian, Pakistani,
and Haitian Earthquake pages might contain a
reasonable set of vocabulary relevant to earth-
quake crises as a whole.
7.3 Cookbook Infrastructure
The Cookbook infrastructure draws directly on what
was found to be useful in the Haitian Crisis. Here are
the infrastructural components we see as crucial:
14http://www.btb.gc.ca/publications/documents/crise-
crisis.pdf
? A crowd sourced microtasking infrastructure
to translate and route messages from the field.
This proved to be essential in Haiti. Hav-
ing such an infrastructure ready-to-go for fu-
ture crises would shave days off implementa-
tion and likely have profound effects on the ra-
pidity of the response.
? Integration of the APIs for the publically avail-
able MT services, such as Microsoft Transla-
tor and Google Translate, into the microtask-
ing and messaging infrastructure, enabling pro-
cessing of SMS messages, Twitter feeds, etc.
In this way, when any of these services deploy
MT for a given crisis language, the switch can
be flipped and first-pass can be MT activated at
a moment?s notice.
? A ready-to-go smart phone app that acts as a
crisis Translation Memory, which can be pop-
ulated with Cookbook content as it becomes
available. In this manner, rather than relying
on the distribution of paper copies of Cook-
book materials, relief workers on the ground
could just sync-up their mobile devices to get
the latest content. This is particularly impor-
tant in crisis locales where ?data plan? access
is limited, and phones will thus not necessarily
have online access to cloud based resources on
a regular basis.
8 A Sample Crisis Timeline
The following timeline is only meant to demonstrate
what might be possible with the right infrastructure
in place and the community fully engaged. The
mantra of ?every crisis is different? applies, and this
timeline should not be interpreted as a ?cookbook?
for a future event. All place and entity names are
intended to add realism; there was no intention to
leave anyone in or out.
Day 0 ? A massive earthquake hits the island nation
of Palladi.
Day 1 ? The first aid organizations arrive on the is-
land with food and humanitarian aid, although
only the two major cities are directly accessi-
ble. Thousands of Palladians are not reach-
able by aid organizations, and the exact num-
bers that are affected and their locations are not
known.
508
The native population of Palladians is nearly
80% monolingual. There is a dire need for Pal-
ladian interpreters, but also of translated Pal-
ladian content. Notified of the need for Pal-
ladian translations, MT community volunteers
begin efforts to collect and license data in Pal-
ladian. The relief community responds by ac-
tivating the crowd sourcing infrastructure used
in other relief scenarios. Researchers and disas-
ter response teams are notified at Microsoft Re-
search and Google Research of the critical need
for crisis content to be translated into Palladian.
Native Palladian speakers are being looked for
by all parties.
Day 2 ? As with the Haitian crisis, a text messag-
ing infrastructure is put in place such that text
messages can be received from the population
and routed to a crowd of rapidly assembling
volunteers. Since there is some internet ac-
cess, including via mobile phones, twitter feeds
are monitored. Until messages start arriving, a
small crowd of Palladian speakers begin trans-
lating content into Palladian, focused specifi-
cally on the Cookbook and off-the-shelf SMS
content.
The first text messages start arriving by late af-
ternoon. These text messages are routed di-
rectly to the text messaging and microtasking
infrastructure. The small but growing crowd
of Palladian translators begin translating this
growing tide of messages.
Day 3 ? The humanitarian information processing
community, with the support of many organiza-
tions and volunteers, releases the first sections
of the Crisis Cookbook. The Crisis Cookbook
is transmitted directly to aid organizations on
the ground in Palladi, and soft- and hard-copies
are distributed to aid workers as quickly as fea-
sible.
AT&T puts into place several cell towers with
satellite connectivity for areas that do not have
cell coverage. Within hours, text and twitter
messages from the field increase dramatically.
Day 4 ? Microsoft and Google release the first ver-
sions of their Palladian-English translators,
with ready access via their public APIs. Since
the text messaging infrastructure already has
both APIs integrated directly into the micro-
tasking and message processing infrastructure,
both engines are activated immediately, and all
messages are translated first by one or the other
engine, and the MT?d content along with the
original message are handed to volunteers.15
Translations are repaired, and routed directly to
aid organizations, and to the Google and Mi-
crosoft teams (for retraining models).
Day 5 ? Additional cookbook materials are trans-
lated. Researchers at Johns Hopkins locate a
stash of Palladian data at the Palladian Cen-
tral University. This data is posted at the CMU
site, and is immediately consumed by all par-
ties working on the MT problem.
Day 6 ? Researchers at University of Edinburgh de-
velop a novel algorithm for dealing with Palla-
dian vowel harmony, which has been a major
problem with Palladian MT, since data sparsity
is exacerbated by the problem. The Edinburgh
researchers publish the algorithm immediately
to their Web site, and notify both Microsoft and
Google.
Day 10 ? Armed with algorithmic improvements
and an increasing volume of data, machine
translated content is now achieving sufficient
quality to warrant passing it directly to aid or-
ganizations. Palladian volunteers now work
principally on the hard to translate cases (those
with high OOVs), and on post-response data
clean-up. The fruits of their labor result in iter-
ative improvements on the various MT engines
that have been deployed.
Day 11+ ? The deployment of language technolo-
gies, specifically MT, in the Palladian crisis re-
sults in saving untold thousands of lives. The
lessons learned in the Palladian earthquake will
be applied to future crises, and the translated
content produced by volunteers will be added
to the cookbook for use in the next crisis.
15Determining which engine to send translations to is a prob-
lem that should be resolved in advance. A combination of either
random selection or on-the-fly OOV calculations could be used
to determine routing.
509
9 Conclusion
In this paper, we propose that MT is an important
technology in crisis events, something that can and
should be an integral part of the rapid-response in-
frastructure. By integrating MT services directly
into a messaging infrastructure (whatever the type of
messages being serviced, e.g., text messages, Twit-
ter feeds, blog postings, etc.), MT can be used to
provide first pass translations into a majority lan-
guage, which can assist in triaging messages and
routing them to appropriate aid agencies. If done
right, MT can dramatically increase the speed by
which relief can be provided. To ensure that MT
is a standard tool in the arsenal of tools used in cri-
sis events, we propose a preliminary Crisis Cook-
book, the data contents of which could be translated
into the relevant language(s) by volunteers imme-
diately after a crisis event takes place. The result-
ing data can then be made available to relief groups
on the ground, as well as to providers of MT ser-
vices. We also note that there are significant con-
tributions that our community can make to relief ef-
forts through continued work on our research, espe-
cially that research which makes MT more viable for
under-resourced languages.
Credits
This paper is dedicated to the thousands of volun-
teers who worked selflessly for many, many hours
in aid of the people of Haiti. Without their help,
many hundreds more would have perished. We also
wish to express our deepest appreciation to all those
who have devoted their lives to aid people in need,
especially the first responders in crisis events. It is
our sincerest hope that that the small measures our
community can take to assist in relief efforts will
help make your jobs more effective, and that our ef-
forts will ultimately assist you and those you strive
to help.
References
Jeffrey Allen. 1998. Lexical variation in Haitian Cre-
ole and orthographic issues for Machine Translation
(MT) and Optical Character Recognition (OCR) appli-
cations. In Association for Machine Translation in the
Americas (AMTA) Workshop on Embedded MT Sys-
tems: Design, Construction, and Evaluation of Sys-
tems with an MT Component, Langhorne, Pennsylva-
nia.
Vamshi Ambati, Sanjika Hewavitharana, Stephan Vogel,
and Jaime Carbonell. 2011. Active Learning with
Multiple Annotations for Comparable Data Classifi-
cation Task. In Proceedings of ACL 2011, Portland,
Oregon, June.
Sharon Anderson. 2010. Talking with Adm. James G.
Stavridis Supreme Allied Commander, Europe Com-
mander, U.S. European Command. CHIPS - The De-
partment of the Navy Information Technology Maga-
zine, 28.
Tugba Bodrumlu, Kevin Knight, and Sujith Ravi. 2009.
A New Objective Function for Word Alignment. In
Proceedings of the NAACL/HLT Workshop on Inte-
ger Programming for Natural Language Processing,
Boulder, Colorado.
Disaster 2.0. 2011. Disaster Relief 2.0: The Future
of Information Sharing in Humanitarian Emergencies.
United Nations Foundation, UN Office for the Coor-
dination of Humanitarian Affairs (UN OCHA), Voda-
fone Foundation, Harvard Humanitarian Initiative.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of EMNLP 2002,
Philadelphia, Pennsylvania.
Robert Frederking, Alexander Rudnicky, and Christopher
Hogan. 1997. Interactive speech translation in the
diplomat project. In Workshop on Spoken Language
Translation at ACL-97, Madrid.
Dmitriy Genzel. 2010. Automatically Learning Source-
side Reordering Rules for Large Scale Machine Trans-
lation. In Proceedings of COLING 2010, Beijing, Au-
gust.
Sanjika Hewavitharana and Stephan Vogel. 2008. En-
hancing a Statistical Machine Translation System by
using an Automatically Extracted Parallel Corpus
from Comparable Sources. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC 2008), Workshop on Comparable
Corpora, Marrakech, Morocco, May.
Sanjika Hewavitharana and Stephan Vogel. 2011. Ex-
tracting Parallel Phrases from Comparable Data. In
Proceedings of ACL 2011, Portland, Oregon, June.
Ann Irvine, Chris Callison-Burch, and Alexandre Kle-
mentiev. 2010. Transliterating from all languages. In
510
Proceedings of the Ninth Conference of the Associa-
tion for Machine Translation in the Americas (AMTA
2010), Denver.
Minwoo Jeong, Kristina Toutanova, Hisami Suzuki, and
Chris Quirk. 2010. A Discriminative Lexicon Model
for Complex Morphology. In Proceedings of the Ninth
Conference of the Association for Machine Translation
in the Americas (AMTA 2010), Denver.
Claire Lefebvre. 1998. Creole Genesis and the Acquisi-
tion of Grammar: The case of Haitian Creole. Cam-
bridge University Press, Cambridge, England.
William D. Lewis and Fei Xia. 2010. Developing
ODIN: A Multilingual Repository of Annotated
Language Data for Hundreds of the World?s Lan-
guages. Literary and Linguistic Computing. See:
http://research.microsoft.com/apps/pubs/default.aspx?
id=138757.
William D. Lewis. 2010. Haitian Creole: How to Build
and Ship an MT Engine from Scratch in 4 Days, 17
Hours, & 30 Minutes. In EAMT 2010: Proceedings of
the 14th Annual conference of the European Associa-
tion for Machine Translation, Saint Raphae?l, France,
May.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Lane Schwartz, Wren N. G.
Thornton, Ziyuan Wang, Jonathan Weese, and Omar F.
Zaidan. 2010. Joshua 2.0: A Toolkit for Parsing-
Based Machine Translation with Syntax, Semirings,
Discriminative Training and Other Goodies. In In Pro-
ceedings of Workshop on Statistical Machine Transla-
tion (WMT10), Uppsala, Sweden.
Christian Monson, Ariadna Font Llitjos, Vamshi Ambati,
Lori Levin, Alon Lavie, Alison Alvarez, Robert Fred-
erking Roberto Aranovich, Jaime Carbonell, Erik Pe-
terson, and Katharina Probst. 2008. Linguistic Struc-
ture and Bilingual Informants Help Induce Machine
Translation of Lesser-Resourced Languages. In Pro-
ceedings of the 6th International Conference on Lan-
guage Resources and Evaluation (LREC 2008), Mar-
rakech, Morocco.
Robert Munro. 2010. Crowdsourced translation for
emergency response in Haiti: the global collaboration
of local knowledge. In AMTA Workshop on Collabo-
rative Crowdsourcing for Translation.
Robert Munro. 2011. Subword and spatiotemporal mod-
els for identifying actionable information in Haitian
Kreyol. In Fifteenth Conference on Computational
Natural Language Learning (CoNLL 2011), Portland.
Douglas W. Oard and Franz Josef Och. 2003. Rapid-
Response Machine Translation for Unexpected Lan-
guages. In MT Summit IX, New Orleans.
Douglas W. Oard. 2003. The Surprise Language Exer-
cises. ACM Transactions on Asian Language Informa-
tion Processing - TALIP, 2(2):79?84.
Kemal Oflazer and Ilknur Durgar El-kahlout. 2007. Ex-
ploring Different Representational Units in English-
to-Turkish Statistical Machine Translation. In In Pro-
ceedings of the Statistical Machine Translation Work-
shop, ACL 2007, Prague.
Chris Quirk and Arul Menezes. 2006. Dependency
Treelet Translation: The convergence of statistical and
example-based machine translation? Machine Trans-
lation, 20:43?65.
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of ACL 2011, Portland,
Oregon, June.
RC. 2010. The American Red Cross: Social Media in
Disasters and Emergencies. Presentation.
Jason Smith, Chris Quirk, and Kristina Toutanova. 2010.
Extracting parallel sentences from comparable cor-
pora using document level alignment. In Proceedings
of Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the ACL,
Los Angeles.
Raghavendra Udupa, K Saravanan, A Kumaran, and Ja-
gadeesh Jagarlamudi. 2009. MINT: A Method for Ef-
fective and Scalable Mining of Named Entity Translit-
erations from Large Comparable Corpora. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL 2009), Athens, Greece.
Andrej Verity. 2011. What the UN could not have
done without the Volunteer Technical Community. In
United Nations Dispatch. The Disaster Relief 2.0 Blog
Series.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
Morphology Mapping in Factored Phrase-Based Sta-
tistical Machine Translation from English to Turkish.
In Proceedings of the ACL 2010, Uppsala, Sweden.
Omar Zaidan and Chris Callison-Burch. 2011. Crowd-
sourcing Translation: Professional Quality from Non-
Professionals. In Proceedings of ACL 2011, Portland,
Oregon, June.
511
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 281?291,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Dramatically Reducing Training Data Size Through Vocabulary
Saturation
William D. Lewis
Microsoft Research
One Microsoft Way
Redmond, WA 98052
wilewis@microsoft.com
Sauleh Eetemadi
Microsoft Research
One Microsoft Way, Redmond, WA 98052
Michigan State University, East Lansing, MI 48824
saulehe@microsoft.com
Abstract
Our field has seen significant improve-
ments in the quality of machine translation
systems over the past several years. The
single biggest factor in this improvement
has been the accumulation of ever larger
stores of data. However, we now find our-
selves the victims of our own success, in
that it has become increasingly difficult to
train on such large sets of data, due to
limitations in memory, processing power,
and ultimately, speed (i.e., data to mod-
els takes an inordinate amount of time).
Some teams have dealt with this by focus-
ing on data cleaning to arrive at smaller
data sets (Denkowski et al, 2012a; Rarrick
et al, 2011), ?domain adaptation? to ar-
rive at data more suited to the task at hand
(Moore and Lewis, 2010; Axelrod et al,
2011), or by specifically focusing on data
reduction by keeping only as much data as
is needed for building models e.g., (Eck
et al, 2005). This paper focuses on tech-
niques related to the latter efforts. We have
developed a very simple n-gram counting
method that reduces the size of data sets
dramatically, as much as 90%, and is ap-
plicable independent of specific dev and
test data. At the same time it reduces
model sizes, improves training times, and,
because it attempts to preserve contexts for
all n-grams in a corpus, the cost in quality
is minimal (as measured by BLEU ). Fur-
ther, unlike other methods created specif-
ically for data reduction that have similar
effects on the data, our method scales to
very large data, up to tens to hundreds of
millions of parallel sentences.
1 Introduction
The push to build higher and higher quality Sta-
tistical Machine Translation systems has led the
efforts to collect more and more data. The
English-French (nearly) Gigaword Parallel Corpus
(Callison-Burch et al, 2009), which we will refer
to henceforth as EnFrGW, is the result of one such
effort. The EnFrGW is a publicly available cor-
pus scraped from Canadian, European and inter-
national Web sites, consisting of over 22.5M par-
allel English-French sentences. This corpus has
been used regularly in the WMT competition since
2009.
As the size of data increases, BLEU scores in-
crease, but the increase in BLEU is not linear in re-
lation to data size. The relationship between data
size and BLEU flattens fairly quickly, as demon-
strated in Figure 1. Here we see that BLEU scores
increase rapidly with small amounts of data, but
they taper off and flatten at much larger amounts.
Clearly, as we add more data, the value of the new
data diminishes with each increase, until very little
value is achieved through the addition of each new
sentence. However, given that this figure repre-
sents samples from EnFrGW, can we be more effi-
cient in the samples we take? Can we achieve near
equivalent BLEU scores on much smaller amounts
of data drawn from the same source, most espe-
cially better than what we can achieve through ran-
dom sampling?
The focus of this work is three-fold. First, we
seek to devise a method to reduce the size of train-
ing data, which can be run independently of par-
ticular dev and test data, so as to maintain the in-
dependence of the data, since we are not interested
here in domain adaptation or selective tuning. Sec-
ond, we desire an algorithm that is (mostly) qual-
ity preserving, as measured by BLEU, OOV rates,
and human eval, ultimately resulting in decreased
training times and reduced model sizes. Reduced
281
Figure 1: BLEU score increase as more data is
added (in millions of words), random samples
from EnFrGW
training times provide for greater iterative capac-
ity, since we can make more rapid algorithmic
improvements and do more experimentation on
smaller data than we can on much larger data.
Since we operate in a production environment, de-
ploying smaller models is also desirable. Third,
we require a method that scales to very large data.
We show in the sections below the application of
an algorithm at various settings to the 22.5M sen-
tence EnFrGW corpus. Although large, 22.5M
sentences does not represent the full total of the
English-French data on the Web. We require an
algorithm that can apply to even larger samples of
data, on the order of tens to hundreds of millions
of sentences.
2 Related Work
In statistical machine translation, selection, prepa-
ration and processing of parallel training data is
often done to serve one of the following scenarios:
? Low Resource Languages: In languages with
low parallel data availability, a subset of a
monolingual corpus is selected for human
translation ((Ananthakrishnan et al, 2010),
(Eck et al, 2005) and (Haffari et al, 2009)).
? Mobile device deployment: For many lan-
guages, translation model sizes built on all
available parallel data are too large to be
hosted on mobile devices. In addition to
translation model pruning, a common solu-
tion is selecting a subset of the data to be
trained on ((Ananthakrishnan et al, 2010)
and (Yasuda et al, 2008)).
? Quick turn-around time during development:
A common motivation for training on a sub-
set of a parallel corpus is to reduce training
time during the development cycle of a sta-
tistical machine translation system ((Lin and
Bilmes, 2011) and (Chao and Li, 2011a)).
? Noise reduction: Simple noise reduction
techniques like sentence length and alpha nu-
meric ratio are often used in data preparation.
However, more sophisticated techniques have
been developed to filter out noise from par-
allel data ((Denkowski et al, 2012a) and
(Taghipour et al, 2010)).
? Domain Adaptation: Recently there has been
significant interest in domain adaptation for
statistical machine translation. One of the ap-
proaches to domain adaptation is selecting a
subset of a data that is closer to the target do-
main ((Moore and Lewis, 2010), (Axelrod et
al., 2011)).
? Improve translation quality: An interesting
area of research is selecting a subset of the
training data that is more suitable for sta-
tistical machine translation learning ((Okita,
2009)).
In comparison, the goal of this work is to effi-
ciently reduce very large parallel data sets (in ex-
cess of tens of billions of tokens) to a desired size
in a reasonable amount of time. In the related work
referenced above two primary methods have been
used.
1. Maximizing n-gram coverage with minimal
data.
2. Filtering out noisy data based on sentence-
pair based features.
One of the earliest and most cited works using
the first method is (Eck et al, 2005). In this work,
a greedy algorithm is developed to select a subset
of the entire corpus that covers most n-grams with
minimum number of words. In a later work by the
same author, the algorithm was modified to give
higher weight to more frequent words. Although
this is a greedy algorithm and does not provide the
optimum solution, its complexity is quadratic in
the number of sentences. Hence it is not practical
to run this algorithm over very large data sets.
Recently (Ananthakrishnan et al, 2010) intro-
duced a new algorithm that is an improvement
282
over (Eck et al, 2005). In this work discriminative
training is used to train a maximum entropy pair-
wise comparator with n-gram based features. The
pair-wise comparator is used to select the highest
scoring sentence followed by discounting features
used for the sentence, which are drawn from the
global pool of features. The complexity of this al-
gorithm after training the pairwise comparator is
O (N ?K ? log(F )) where N is the number of
sentences in the entire corpus, K is the number of
sentences to be selected and F is the size of the fea-
ture space. Although this method works well for
a constant K, its complexity is quadratic when K
is a fraction of N . This method is reported to im-
prove the BLEU score close to 1% over the work
done by (Eck et al, 2005).
(Denkowski et al, 2012a) have developed rela-
tively scalable algorithms that fit in the second cat-
egory above. This algorithm automatically filters
out noisy data primarily based on the following
feature functions: normalized source and target
language model scores, word alignment scores and
fraction of aligned words. Sentences that don?t
score above a certain threshold (mean minus one
or two standard deviations) for all their features
are filtered out. In a similar work, (Taghipour
et al, 2010) use an approach where they incor-
porate similar features based on translation table
entries, word alignment models, source and target
language models and length to build a binary clas-
sifier that filters out noisy data.
Our work incorporates both methods listed
above in a scalable fashion where it selects a sub-
set of the data that is less noisy with a reasonable
n-gram representation of the superset parallel cor-
pus. To put the scalability of our work in perspec-
tive we compiled Table 1, which shows the max-
imum size of the data sets reported in each of the
relevant papers on the topic. Despite the public
availability of parallel corpora in excess of tens
of millions of sentence pairs, none of the related
works, using the first method above, exceed cou-
ple of millions of sentences pairs. This demon-
strates the importance of developing a scalable al-
gorithm when addressing the data selection prob-
lem.
The careful reader may observe that an alter-
nate strategy for reducing model sizes (e.g., use-
ful for the Mobile scenario noted above, but also
in any scenario where space concerns are an is-
sue), would be to reduce phrase table size rather
Reference Total Sentences
(Ananthakrishnan et al, 2010) 253K
(Eck et al, 2005) 123K
(Haffari et al, 2009) 1.8M1
(Lin and Bilmes, 2011) 1.2M2
(Chao and Li, 2011b) 2.3M
Table 1: Data Sizes for Related Systems
than reduce training data size. A good example
of work in this space is shown in (Johnson et al,
2007), who describe a method for phrase table re-
duction, sometimes substantial (>90%), with no
impact on the resulting BLEU scores. The prin-
cipal of our work versus theirs is where the data
reductions occur: before or after training. The pri-
mary benefit of manipulating the training data di-
rectly is the impact on training performance. Fur-
ther, given the increasing sizes of training data,
it has become more difficult and more time con-
suming to train on large data, and in the case of
very large data (say tens to hundreds of millions
of sentence pairs), it may not even be possible to
train models at all. Reduced training data sizes in-
creases iterative capacity, and is possible in cases
where phrase table reduction may not be (i.e., with
very big data).
3 Vocabulary Saturation Filter (VSF)
The effects of more data on improving BLEU
scores is clearly discernible from Figure 1: as
more data is added, BLEU scores increase. How-
ever, the relationship between quantity of data and
BLEU is not linear, such that the effects of more
data diminishes with each increase in data size, ef-
fectively approaching some asymptote. One might
say that the vocabulary of the phrase mappings de-
rived from model training ?saturate? as data size
increases, since less and less novel information
can be derived from each succeeding sentence of
data added to training. It is this observation that
led us to develop the Vocabulary Saturation Filter
(VSF).
VSF makes the following very simple assump-
tion: for any given vocabulary item v there is some
point where the contexts for v?that is, the n-gram
1Sentence count was not reported. We estimated it based
on 18M tokens.
2This is a very interesting work, but is only done for se-
lecting speech data. The total number of sentences is not re-
ported. We given a high-end estimate based on 128K selected
tokens.
283
sequences that contain v?approach some level
of saturation, such that each succeeding sentence
containing v contributes few or no additional con-
texts, and thus has little impact on the frequency
distributions over v. In other words, at a point
where the diversity of contexts for v approach a
maximum, there is little value in adding additional
contexts containing v, e.g., to translation models.
The optimal algorithm would then, for each v
? V, identify the number of unique contexts that
contain v up to some threshold and discard all oth-
ers. An exhaustive algorithm which sets thresh-
olds for all n-gram contexts containing v, however,
would take a large amount of time to run (mini-
mally quadratic), and may also overrun memory
limitations on large data sets.
For VSF, we made the following simplifying as-
sumption: we set an arbitrary count threshold t for
all vocabulary items. For any given v, when we
reach t, we no longer need to keep additional sen-
tences containing v. However, since each instance
of v does not exist in isolation, but is rather con-
tained within sentences that also contain other vo-
cabulary items v, which, in turn, also need to be
counted and thresholded, we simplified VSF even
further with the following heuristic: for any given
sentence s, if all v ? V within s have not reached
t, then the sentence is kept. This has the direct
consequence that many vocabulary items will have
frequencies above t in the output corpus.
The implementation of VSF is described in Al-
gorithm 1 below.
VSF clearly makes a number of simplifying as-
sumptions, many of which one might argue would
reduce the value of the resulting data. Although
easy to implement, it may not achieve the most
optimal results. Assuming that VSF might be de-
fective, we then looked into other algorithms at-
tempting to achieve the same or similar results,
such as those described in Section 2, and explored
in-depth the algorithms described in (Eck et al,
2005).
4 An Alternative: (Eck et al, 2005)
In our pursuit of better and generic data reduction
algorithms, we did a number of experiments using
the algorithms described in (Eck et al, 2005). In
the n-gram based method proposed by this work
the weight of each function is calculated using
Equation 1, where j is the n-gram length. In
each iteration of the algorithm, the weight of each
Input: ParallelCorpus, N, L
Output: SelectedCorpus
foreach sp ? ParallelCorpus do
S ? EnumNgrams(sp.src, L);
T ? EnumNgrams(sp.tgt, L);
selected? false;
foreach (s, t) ? (S, T ) do
if SrcCnt [s]<N ? TgtCnt [t]<N
then
selected? true;
end
end
if selected then
SelectedCorpus.Add(sp);
foreach (s, t) ? (S, T ) do
SrcCnt [s]++;
TgtCnt [t]++;
end
end
end
Algorithm 1: Pseudocode for implementing
VSF. L: n-gram length, N: n-gram threshold.
sentence is calculated and the sentence with the
highest weight is selected. Once a sentence is se-
lected, the n-grams in the sentence are marked as
seen and have a zero weight when they appear in
subsequent sentences. Therefore, the weights of
all remaining sentences have to be recalculated be-
fore the next sentence can be selected. We refer to
this algorithm henceforth as the Eck algorithm.
Wj (sentence) =
j?
i=1
?
? ?
unseen
ngrams
Freq(ngram)
?
?
|sentence| (1)
To compare VSF against the Eck algorithm
we selected the English-Lithuanian parallel corpus
from JRC-ACQUIS (Steinberger et al, 2006). We
selected the corpus for the following reasons:
? VSF performance on this particular data set
was at its lowest compared to a number of
other data sets, so there was room for im-
provement by a potentially better algorithm.
? With almost 50 million tokens combined (En-
glish and Lithuanian) we were able to opti-
mize the Eck algorithm and run it on this data
set in a reasonable amount of time. The ex-
periments run by the original paper in 2005
were run on only 800,000 tokens.
284
Using the Eck algorithm with n-gram length set
to one (j ? 1 in Equation 1) only 10% (5,020,194
tokens total) of the data is sorted, since all n-grams
of size one have been observed by that point and
the weight function for the remaining sentences
returns zero. In other words, since there are no
unseen unigrams after 10% of the data has been
sorted, in Equation 1, the numerator becomes zero
there after and therefore the remaining 90% of
sentence pairs are not sorted. This must be taken
into consideration when examining the compari-
son between unigram VSF and the Eck algorithm
with n-gram length set to one in Figure 2. VSF
with its lowest setting, that is threshold t=1, se-
lects 20% of the data, so this chart may not be a
fair comparison between the two algorithms.
Figure 2: Unigram Eck vs. Unigram VSF
In an attempt to do a fairer comparison, we also
tried n-grams of length two in the Eck algorithm,
where 50% of the data can be sorted (since all uni-
grams and bigrams are observed by that point). As
seen in Figure 3, the BLEU scores for the Eck and
VSF systems built on the similar sized data score
very closely on the WMT 2009 test set.3
Further exploring options using Eck, we devel-
oped the following two extensions to the Eck algo-
rithm, none of which resulted in a significant gain
in BLEU score over VSF with n-gram lengths set
up to three.
? Incorporating target sentence n-grams in ad-
dition to source side sentence n-grams.
? Dividing the weight of an n-gram (its fre-
3The careful reader may note that there is no official
WMT09 test set for Lithuanian, since Lithuanian is not (yet)
a language used in the WMT competition. The test set men-
tioned here was created from a 1,000 sentence sample from
the English-side of the WMT09 test sets, which we then man-
ually translated into Lithuanian.
quency) by a constant number each time a
sentence that contains the n-gram is selected,
as opposed to setting the weight of an n-gram
to zero after it has been seen for the first
time.4
In relatively small data sets there is not a signif-
icant difference between the two algorithms. The
Eck algorithm does not scale to larger data sets
and higher n-grams. Since a principal focus of our
work is on scaling to very large data sets, and since
Eck could not scale to even moderately sized data
sets, we decided to continue our focus on VSF and
improvements to that algorithm.
Figure 3: Bigram Eck vs. Unigram VSF
5 Data Order
Unlike the Eck algorithm, VSF is sensitive to the
order of the input data due to the nature of the al-
gorithm. Depending on the order of sentences in
the input parallel corpus, VSF could select differ-
ent subsets of the parallel corpus that would even-
tually (after training and test) result in different
BLEU scores. To address this concern we use
a feature function inspired by (Denkowski et al,
2012a) which is a normalized combined alignment
score. This feature score is obtained by geomet-
ric averaging of the normalized forward and back-
ward alignment scores which in turn are calculated
using the process described in (Denkowski et al,
2012a). To keep the algorithm as scalable as pos-
sible we use radix sort. This ordering of the data
ensures sentences with high normalized alignment
scores appear first and sentences with low normal-
ized alignment appear last. As a result, for each
n-gram, VSF will choose the top-N highest scor-
ing sentence pairs that contain that n-gram.
4Further details of the modifications to the Eck algorithm
are not discussed here as they did not yield improvements
over the baseline algorithm and the focus of our work pre-
sented here was shifted to improvements over VSF.
285
5.1 Data Ordering Complexity
Ordering the data based on normalized combined
alignment score requires two steps. First, the
normalized combined alignment score is com-
puted for each sentence pair using an exist-
ing HMM alignment model. Next, sentence
pairs are sorted based on the calculated score.
The computational complexity of aligning a sin-
gle sentence pair is O (J + I2) where J is the
number of words in the source sentence and I
is the number of words in the target sentence
(Gao and Vogel, 2008). Therefore the com-
plexity of calculating the combined alignment
score would be O (N ? (J2 + I + I2 + J)) or
O
(
N ?max(I, J)2
) after simplification. Since
radix sort is used for sorting the data, the data can
be sorted in O(d ? N) where d is the number of
significant digits used for sorting. Since d is kept
constant5, the overall computational complexity
for data ordering is O (N +N ?max(I, J)2).
6 Experiments
6.1 The Machine Translation and Training
Infrastructure
We used a custom-built tree-to-string (T2S) sys-
tem for training the models for all experiments.
The T2S system that we developed uses tech-
nology described in (Quirk et al, 2005), and re-
quires a source-side dependency parser, which we
have developed for English.6 We trained a 5-
gram French LM over the entire EnFrGW, which
we used in all systems. We used Minimum Error
Rate Training (MERT) (Och, 2003) for tuning the
lambda values for all systems, tuned using the of-
ficial WMT2010 dev data.
6.2 Test and Training Data
In all experiments, we used the EnFrGW cor-
pus, or subsets thereof. 7 We used three test sets
5In experiments described in Section 6 five significant
digits were used for radix sort.
6Further details about the decoders is beyond the scope of
this paper. The reader is encouraged to refer to the sources
provided for additional information.
7Because of some data cleaning filters we applied to the
data, the actual full sized corpus we used consisted of slightly
less data than that used in the WMT competitions. Every
team has its own set of favorite data cleaning heuristics, and
ours is no different. The filters applied to this data are focused
mostly on noise reduction, and consist of a set of filters re-
lated to eliminating content that contains badly encoded char-
acters, removing content that is too long (since there is little
value in training on very long sentences), removing content
where the ratio between numeric versus alphabetic characters
t = Random VSF Ordered VSF
1 1.83 M 1.83 M 1.68 M
2 2.53 M 2.53 M 2.34 M
5 3.62 M 3.62 M 3.35 M
10 4.62 M 4.62 M 4.29 M
20 5.83 M 5.83 M 5.44 M
40 7.26 M 7.26 M 6.83 M
60 8.21 M 8.21 M 7.78 M
100 9.53 M 9.53 M 9.13 M
150 10.67 M 10.67 M 10.33 M
200 11.53 M 11.53 M 11.23 M
250 12.22 M 12.22 M 11.97 M
All 22.5 M
Table 2: English-side Sentence Counts (in mil-
lions) for different thresholds for VSF, VSF after
ordering the data based on normalized combined
alignment score and random baselines.
in all experiments, as well. Two consisted of
the WMT 2009 and 2010 test sets, used in the
WMT competitions in the respective years. The
third consisted of 5,000 parallel English/French
sentences sampled from logs of actual traffic re-
ceived by our production service, Bing Transla-
tor (http://bing.com/translator), which were then
manually translated. The first two test sets are
publicly available, but are somewhat news fo-
cused. The third, which we will call ReqLog, con-
sists of a mix of content and sources, so can be
considered a truly ?general? test set.
To discern the effects of VSF at different de-
grees of ?saturation?, we tried VSF with different
threshold values t, ranging from 1 to 250. For each
t value we actually ran VSF twice. In the first case,
we did no explicit sorting of the data. In the sec-
ond case, we ranked the data using the method de-
scribed in Section 5.
Finally, we created random baselines for each
t, where each random baseline is paired with the
relevant VSF run, controlled for the number of
sentences (since t has no relevance for random
samples). The different t values and the resulting
training data sizes (sentence and word counts) are
shown in Tables 2 and 3.
Since our interest in this study is scaling paral-
lel data, for all trainings we used the same LM,
which was built over all training data (the French
side of the full EnFrGW). Because monolingual
training scales much more readily than parallel,
is excessively large, deleting content where the script of the
content is mostly not in latin1 (relevant for French), and some
additional filters described in (Denkowski et al, 2012b). If
the reader wishes additional material on data filtration, please
see (Denkowski et al, 2012b) and (Lewis and Quirk, 2013).
286
t = Random VSF Ordered VSF
1 46.1 M 64.52 M 65.74 M
2 63.99 M 87.41 M 88.12 M
5 91.55 M 121.3 M 120.86 M
10 116.83 M 151.53 M 149.95 M
20 147.31 M 186.99 M 184.14 M
40 183.46 M 228.14 M 224.29 M
60 207.42 M 254.89 M 250.68 M
100 240.88 M 291.45 M 287.02 M
150 269.77 M 322.5 M 318.33 M
200 291.4 M 345.37 M 341.69 M
250 308.83 M 363.44 M 360.32 M
All 583.97 M
Table 3: English-side Word Counts for different
thresholds for VSF, VSF after ordering the data
based on normalized combined alignment score
and random baselines.
this seemed reasonable. Further, using one LM
controls one parameter that would otherwise fluc-
tuate across trainings. The result is a much more
focused view on parallel training diffs.
6.3 Results
We trained models over each set of data. In ad-
dition to calculating BLEU scores for each result-
ing set of models in (Table 5), we also compared
OOV rates (Table 6) and performance differences
(Table 4). The former is another window into the
?quality? of the resulting models, in that it de-
scribes vocabulary coverage (in other words, how
much vocabulary is recovered from the full data).
The latter gives some indication regarding the time
savings after running VSF at different thresholds.
On the WMT09 data set, both sets of VSF
models outperformed the relevant random base-
lines. On the WMT10 and ReqLog test sets, the
pre-sorted VSF outperformed all random base-
lines, with the unsorted VSF outperforming most
random baselines, except at t=60 and t=200 for
WMT10. For the ReqLog, unsorted VSF drops be-
low random starting at t=200. Clearly, the t=200
results show that there is less value in VSF as we
approach the total data size.
The most instructive baseline, however, is the
one built over all training data. It is quite obvi-
ous that at low threshold values, the sampled data
is not a close approximation of the full data: not
enough vocabulary and contextual information is
preserved for the data to be taken as a proxy for
the full data. However, with t values around 20-
60 we recover enough BLEU and OOVs to make
the datasets reasonable proxies. Further, because
t = Random VSF Ordered VSF
1 1:07 2:17 1:56
2 1:33 2:55 2:39
5 2:15 4:05 3:47
10 2:43 4:49 4:50
20 3:23 5:25 5:14
40 4:12 6:16 5:56
60 4:45 6:41 7:15
100 5:31 7:32 7:55
150 6:07 8:20 8:18
200 6:36 8:31 8:52
250 7:30 9:19 9:11
All 13:12
Table 4: Word alignment times (hh:mm) for dif-
ferent thresholds for VSF, VSF after model score
ordering, and a random baseline
we see a relative reduction in data sizes of 32-
44%, model size reductions of 27-39%, and per-
formance improvements of 41-50% at these t val-
ues further argues for the value of VSF at these set-
tings. Even at t=250, we have training data that is
54% of the full data size, yet fully recovers BLEU.
7 Discussion
VSF is a simple but effective algorithm for reduc-
ing the size of parallel training data, and does so
independently of particular dev or test data. It per-
forms as well as related algorithms, notably (Eck
et al, 2005), but more importantly, it is able to
scale to much larger data sets than other algo-
rithms. In this paper, we showed VSF applied
to the EnFrGW corpus. It should be noted, how-
ever, that we have also been testing VSF on much
larger sets of English-French data. Two notable
tests are one applied to 65.2M English-French sen-
tence pairs and another applied to one consisting
of 162M. In the former case, we were able to re-
duce the corpus size from 65.2M sentences/1.28B
words8 to 26.2M sentences/568M words. The
BLEU score on this test was stable on the three
test sets, as shown in Table 7. When applied to the
162M sentence/2.1B word data set, we were able
to reduce the data size to 40.5M sentences/674M
words. In this case, sorting the data using model
scores produced the most desirable results, actu-
ally increasing BLEU by 0.90 on WMT09, but,
unfortunately, showing a 0.40 drop on WMT10.
The fact that VSF runs in one pass is both an
asset and a liability. It is an asset since the algo-
rithm is able to operate linearly with respect to the
size the data. It is a liability since the algorithm is
8Word counts based on the English-side, unwordbroken.
287
WMT09 WMT10 ReqLog
t = Random VSF S+VSF Random VSF S+VSF Random VSF S+VSF
1 23.76 23.83 23.84 25.69 25.78 25.68 26.34 26.63 26.67
2 23.91 24.04 24.07 25.76 26.21 26.14 26.54 26.99 26.94
5 24.05 24.29 24.40 26.10 26.40 26.32 26.79 27.22 27.12
10 24.15 24.37 24.45 26.21 26.63 26.32 26.98 27.37 27.62
20 24.20 24.40 24.55 26.30 26.46 26.56 27.22 27.38 27.44
40 24.37 24.43 24.65 26.40 26.55 26.53 27.30 27.38 27.62
60 24.32 24.43 24.64 26.56 26.56 26.61 27.38 27.50 27.64
100 24.37 24.49 24.71 26.46 26.75 26.70 27.37 27.52 27.75
150 24.37 24.61 24.71 26.67 26.67 26.70 27.48 27.62 27.75
200 24.48 24.63 24.69 26.56 26.65 26.78 27.57 27.47 27.72
250 24.41 24.57 24.85 26.62 26.74 26.68 27.63 27.45 27.76
All 24.37 26.54 27.63
Table 5: BLEU Score results for VSF, S+VSF (Sorted VSF), and Random Baseline at different thresholds
t.
WMT09 WMT10 ReqLog
t = Random VSF S+VSF Random VSF S+VSF Random VSF S+VSF
1 630 424 450 609 420 445 1299 973 1000
2 588 374 395 559 385 393 1183 906 919
5 520 343 347 492 350 356 1111 856 853
10 494 336 335 458 344 344 1092 837 848
20 453 335 335 432 339 341 1016 831 834
40 423 330 331 403 336 337 986 828 833
60 419 329 330 407 333 336 964 831 832
100 389 330 329 391 333 335 950 830 830
150 397 330 330 384 332 332 930 828 828
200 381 328 330 371 331 332 912 827 826
250 356 329 328 370 333 331 884 823 823
All 325 331 822
Table 6: OOV rates for VSF, S+VSF (Sorted VSF), and Random Baseline at different thresholds t.
Figure 4: Comparative BLEU scores for two VSF implementations, against a randomly sampled baseline.
Figure 5: Comparative OOV rates for two VSF implementations, against a randomly sampled baseline.
288
ReqLog WMT09 WMT10
65.2 snts 32.90 26.77 29.05
VSF 26.2M snts 33.34 26.75 29.07
Table 7: VSF applied to a 65.2M sentence baseline
system.
sensitive to the order of the data. The latter leads
to issues of reproducibility: with poorly ordered
data, one could easily arrive at a much less than
optimal set of data. However, by adding an addi-
tional pass to build model scores, and then ranking
the data by these scores, we address the serious is-
sue of reproducibility. Further, the ranking tends
to arrive at a better selection of data.
In an attempt to better understand the behavior
of VSF and how VSF changes the n-gram distribu-
tions of vocabulary items in a sample as compared
to the full corpus, we created log2-scale scatter
plots, as seen in Figure 6. In these plots, uni-
gram frequencies of unfiltered data (i.e., the full
corpus, EnFrGW) are on the vertical axis, and un-
igram frequencies of the VSF filtered data are on
the horizontal axis. The three plots show three dif-
ferent settings for t. There following observations
can be made about these plots:
1. On the horizontal axis before we reach
log2(t), all data points fall on the x = y line.
2. As the threshold increases the scatter plot
gets closer to the x = y line.
3. VSF has the highest impact on the ?medium?
frequency unigrams, that is, those with a fre-
quency higher than the threshold.
The third point speaks the most to the ef-
fects that VSF has on data: Very low frequency
items, specifically those with frequencies below
the threshold t, are unaffected by the algorithm,
since we guarantee including all contexts in which
they occur. Low frequency items are at the lower
left of the plots, and their frequencies follow the
x = y line (point 1 above). Medium frequency
items, however, specifically those with frequen-
cies immediately above t, are the most affected
by the algorithm. The ?bulge? in the plots shows
where these medium frequency items begin, and
one can see plainly that their distributions are per-
turbed. The ?bulge? dissipates as frequencies in-
crease, until the effects diminish as we approach
much higher frequencies. The latter is a conse-
quence of a simplifying heuristic applied in VSF
(as described in Section 3): t is not a hard ceil-
ing, but rather a soft one. Vocabulary items that
occur very frequently in a corpus will be counted
many more times than t; for very high frequency
items, their sampled distributions may approach
those observed in the full corpus, and converge on
the x = y line. The authors suspect that the BLEU
loss that results from the application of VSF is the
result of the perturbed distributions for medium
frequency items. Adjusting to higher t values de-
creases the degree of the perturbation, as noted in
the second point, which likewise recovers some of
the BLEU loss observed in lower settings.
8 Future Work
There are several future directions we see with
work on VSF. Because one threshold t for all vo-
cabulary items may be too coarse a setting, we first
plan to explore setting t based on frequency, es-
pecially for vocabulary in the most affected mid-
range (at and above t). If we set t based on uni-
grams falling into frequency buckets, rather than
one setting for all unigrams, we may arrive ear-
lier at a more distributionally balanced corpus, one
that may better match the full corpus. That said,
additional passes over the data come at additional
cost.
Second, we plan to explore applying the VSF al-
gorithm to higher order n-grams (all experiments
thus far have been on unigrams). Preliminary
experiments on bigram VSF, however, show that
with even the lowest setting (t=1), we already pre-
serve well over 50% of the data.
In this work we only experimented with sorting
the data based on the normalized combined align-
ment score inspired by (Eck et al, 2005). A third
direction for future work would be to consider or-
dering the data based on other feature functions
presented in Eck, e.g., source and target language
model, alignment ratio, as well as and feature
functions introduced in (Taghipour et al, 2010),
or a combination of all of these feature functions.
In the fourth case, we plan to do more sophis-
ticated statistical analysis of the effects of VSF
on n-gram distributions and phrase-table entropy.
We also plan to explore the interactions between
VSF and data ?diversity?. For instance, VSF may
have a greater positive impact on more narrowly
focused domains than on those that are more gen-
erally focused.
289
(a) VSF t = 1
0 5 10 15 20 25 300
5
10
15
20
25
30
Unfiltered Token Log Frequency
Filtere
d Toke
n Log F
requen
cy
(b) VSF t = 40
0 5 10 15 20 25 300
5
10
15
20
25
30
Unfiltered Token Log Frequency
Filtere
d Toke
n Log F
requen
cy
(c) VSF t = 200
Figure 6: log2-scale Unigram Frequency scatter plot before VSF versus after VSF
References
S. Ananthakrishnan, R. Prasad, D. Stallard, and
P. Natarajan. 2010. Discriminative sample selection
for statistical machine translation. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, page 626635.
A. Axelrod, X. He, and J. Gao. 2011. Domain adap-
tation via pseudo in-domain data selection. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, page 355362.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
W. Chao and Z. Li. 2011a. A graph-based bilingual
corpus selection approach for SMT. In Proceedings
of the 25th Pacific Asia Conference on Language,
Information and Computation.
WenHan Chao and ZhouJun Li. 2011b. Improved
graph-based bilingual corpus selection with sen-
tence pair ranking for statistical machine transla-
tion. In 2011 23rd IEEE International Conference
on Tools with Artificial Intelligence (ICTAI), pages
446 ?451, November.
Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012a. The CMU-Avenue French-English transla-
tion system. In Proceedings of the NAACL 2012
Workshop on Statistical Machine Translation.
Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012b. The CMU-Avenue French-English Transla-
tion System. In Proceedings of the NAACL 2012
Workshop on Statistical Machine Translation.
M. Eck, S. Vogel, and A. Waibel. 2005. Low
cost portability for statistical machine translation
based in n-gram frequency and TF-IDF. In Inter-
national Workshop on Spoken Language Translation
(IWSLT).
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, page 4957.
G. Haffari, M. Roy, and A. Sarkar. 2009. Active learn-
ing for statistical phrase-based machine translation.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, page 415423.
Howard Johnson, Joel D. Martin, George F. Foster, and
Roland Kuhn. 2007. Improving translation quality
by discarding most of the phrasetable. In Proceed-
ings of EMNLP, pages 967?975.
William D. Lewis and Chris Quirk. 2013. Con-
trolled Ascent: Imbuing Statistical MT with Lin-
guistic Knowledge. In Proceedings of the Second
Hytra (Hybrid Approaches to Translation) Work-
shop, Sofia, Bulgaria, August.
H. Lin and J. Bilmes. 2011. Optimal selection of
limited vocabulary speech corpora. In Proc. Inter-
speech.
Robert C. Moore and William D. Lewis. 2010. Intel-
ligent Selection of Language Model Training Data.
In Proceedings of the ACL 2010 Conference Short
Papers, Uppsala, Sweden, July.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st ACL, Sapporo, Japan.
T. Okita. 2009. Data cleaning for word alignment. In
Proceedings of the ACL-IJCNLP 2009 Student Re-
search Workshop, page 7280.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency tree translation: Syntactically informed
phrasal smt. In Proceedings of ACL 2005.
Spencer Rarrick, Chris Quirk, and William D. Lewis.
2011. MT Detection in Web-Scraped Parallel Cor-
pora. In Proceedings of MT Summit XIII, Xiamen,
China, September.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Toma Erjavec, and Dan Tufi. 2006.
290
The JRC-Acquis: a multilingual aligned paral-
lel corpus with 20+ languages. In In Proceed-
ings of the 5th International Conference on Lan-
guage Resources and Evaluation (LREC?2006, page
21422147.
K. Taghipour, N. Afhami, S. Khadivi, and S. Shiry.
2010. A discriminative approach to filter out noisy
sentence pairs from bilingual corpora. In 2010
5th International Symposium on Telecommunica-
tions (IST), pages 537 ?541, December.
K. Yasuda, R. Zhang, H. Yamamoto, and E. Sumita.
2008. Method of selecting training data to build
a compact and efficient translation model. In Pro-
ceedings of the Third International Joint Conference
on Natural Language Processing, volume 2, page
655660.
291
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 51?66,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Controlled Ascent: Imbuing Statistical MT with Linguistic Knowledge
William D. Lewis and Chris Quirk
Microsoft Research
One Microsoft Way
Redmond, WA 98052
{wilewis,chrisq}@microsoft.com
Abstract
We explore the intersection of rule-based and sta-
tistical approaches in machine translation, with a
particular focus on past and current work here at
Microsoft Research. Until about ten years ago,
the only machine translation systems worth using
were rule-based and linguistically-informed. Along
came statistical approaches, which use large cor-
pora to directly guide translations toward expres-
sions people would actually say. Rather than mak-
ing local decisions when writing and conditioning
rules, goodness of translation was modeled numer-
ically and free parameters were selected to opti-
mize that goodness. This led to huge improvements
in translation quality as more and more data was
consumed. By necessity, the pendulum is swing-
ing towards the inclusion of linguistic features in
MT systems. We describe some of our statistical
and non-statistical attempts to incorporate linguis-
tic insights into machine translation systems, show-
ing what is currently working well, and what isn?t.
We also look at trade-offs in using linguistic knowl-
edge (?rules?) in pre- or post-processing by lan-
guage pair, with a particular eye on the return on
investment as training data increases in size.
1 Introduction
Machine translation has undergone several
paradigm shifts since its original conception.
Early work considered the problem as cryptog-
raphy, imagining that a word replacement cipher
could find the word correspondences between two
languages. Clearly Weaver was decades ahead of
his time in terms of both computational power
and availability of data: only now is this approach
gaining some traction (Knight, 2013)1 At the time,
however, this direction did not appear promising,
and work turned toward rule-based approaches.
Effective translation needs to handle a broad
range of phenomena. Word substitution ciphers
may address lexical selection, but there are many
additional complexities: morphological normal-
ization in the source language, morphological in-
flection in the target language, word order differ-
ences, and sentence structure differences, to name
1For the original 1949 Translation memorandum by
Weaver see (Weaver, 1955).
a few. Many of these could be captured, at least
to a first degree of approximation, by rule-based
approaches. A single rule might capture the fact
that English word order is predominantly SVO
and Japanese word order is predominantly SOV.
While many exceptions exist, such rules handle
many of the largest differences between languages
rather effectively. Therefore, rule-based systems
that did a reasonable job of addressing morpho-
logical and syntactic differences between source
and target dominated the marketplace for decades.
With the broader usage of computers, greater
amounts of electronic data became available to
systems. Example-based machine translation
systems, which learn corpus-specific translations
based on data, began to show substantial improve-
ments in the core problem of lexical selection.
This task was always quite difficult for rule-based
approaches: finding the correct translation in con-
text requires a large amount of knowledge. In
practice, nearby words are effective disambigua-
tors once a large amount of data has been captured.
Phrasal statistical machine translation systems
formalized many of the intuitions in example-
based machine translation approaches, replacing
heuristic selection functions with robust statistical
estimators. Effective search techniques developed
originally for speech recognition were strong start-
ing influences in the complicated realm of MT de-
coding. Finally, large quantities of parallel data
and even larger quantities of monolingual data al-
lowed such phrasal methods to shine even in broad
domain translation.
Translations were still far from perfect, though.
Phrasal systems capture local context and local re-
ordering well, but struggle with global reordering.
Over the past decade, statistical machine transla-
tion has begun to be influenced by linguistic infor-
mation once again. Syntactic models have shown
some of the most compelling gains. Many sys-
tems leverage the syntactic structure of either the
51
source or the target sentences to make better deci-
sions about reordering and lexical selection.
Our machine translation group has been an ac-
tive participant in many of these latest develop-
ments. The first MSR MT system used deep lin-
guistic features, often with great positive effect.
Inspired by the successes and failures of this sys-
tem, we invested heavily in syntax-based SMT.
However, our current statistical systems are still
linguistically impoverished in comparison.
This paper attempts to document important
lessons learned, highlight current best practices,
and identify promising future directions for im-
proving machine translation. A brief review of
our earlier generation of machine translation tech-
nology sets the stage; this older system remains
relevant given renewed interest in semantics (e.g.,
http://amr.isi.edu/). Next we describe some of
our statistical and non-statistical attempts to in-
corporate linguistic insights into machine transla-
tion systems, showing what is currently working
well, and what is not. We also look at trade-offs
in using linguistic knowledge (?rules?) in pre- or
post-processing by language pair, with a particu-
lar eye on the return on investment as training data
increases in size. Systems built on different ar-
chitectures, particularly those incorporating some
linguistic information, may have different learn-
ing curves on data. The advent of social media
and big data presents new challenges; we review
some effective research in this area. We conclude
by exploring promising directions for improving
translation quality, especially focusing on areas
that stand to benefit from linguistic information.
2 Logical Form Translation
Machine translation research at Microsoft Re-
search began in 1999. Analysis components had
been developed to parse surface sentences into
deep logical forms: predicate-argument structures
that normalized away many morphological and
syntactic differences. This deep representation
was originally intended for information mining
and question answering, allowing facts to rein-
force one another, and simplifying question and
answer matching. These same normalizations
helped make information more consistent across
languages: machine translation was a clear poten-
tial application. Consider the deep representations
of the sentence pairs in Figure 1: many of the sur-
face differences, such as word order and morpho-
Figure 1: Example logical forms for three distinct
inputs, demonstrating how differences in syntactic
structure may be normalized away. In each case,
the logical form is a graph of nodes such as ?be?
and ?difficult?, and relations such as ?Tobj? (typ-
ical object) and ?Tsub? (typical subject). In addi-
tion, nodes are marked with binary features called
bits, prefixed with a + symbol in the notation, that
capture unstructured pieces of information such as
tense and number.
logical inflection, are normalized away, potentially
easing the translation process.
Substantial differences remained, however.
Many words and phrases have non-compositional
contextually-influenced translations. Commercial
systems of the time relied on complex, hand-
curated dictionaries to make this mapping. Yet
example-based and statistical systems had already
begun to show promise, especially in the case of
domain-specific translations. Microsoft in par-
ticular had large internal demand for ?technical?
translations. With increasing language coverage
and continuing updates to product documentation
and support articles came increasing translation
costs. Producing translations tailored to this do-
main would have been an expensive task for a
rule-based system; a corpus-based approach was
pursed.
This was truly a hybrid system. Source and tar-
get language surface sentences were parsed into
deep logical forms using rule-based analyzers.2
2These parsers were developed with a strong focus on cor-
pora, though. George Heidorn, Karen Jensen, and the NLP
research group developed a toolchain for quickly parsing a
large bank of test sentences and comparing against the last
best result. The improvements and regressions resulting from
a change to the grammar could be manually evaluated, and
the changes refined until the end result. The end result was a
52
Figure 2: The process of learning translation in-
formation from parallel data in the LF system.
Likewise a rule-based target language generation
component could find a surface realization of a
deep logical form. However, the mapping from
source language logical form fragments to target
language logical form fragments was learned from
parallel data.
2.1 Details of the LF-based system
Training started with a parallel corpus. First, the
source and target language sentences were parsed.
Then the logical forms of the source and target
were aligned (Menezes and Richardson, 2001).
These aligned logical forms were partitioned into
minimal non-compositional units, each consisting
of some non-empty subset of the source and tar-
get language nodes and relations. Much like in
example-based or phrasal systems, both minimal
and composed versions of these units were then
stored as possible translations. A schematic of the
this data flow is presented in Figure 2.
At runtime, an input sentence was first parsed
into a logical form. Units whose source sides
matched the logical form were gathered. A heuris-
tic search found a set of fragments that: (a) cov-
ered every input node at least once, and (b) were
consistent in their translation selections. If some
node or relation was not uncovered, it was copied
from source to target. The resulting target lan-
guage logical form was then fed into a genera-
tion component, which produced the final string.
A schematic diagram is presented in Figure 3.
This overview sweeps many fine details un-
der the rug. Many morphological and syntactic
distinctions were represented as binary features
(?bits?) in the LF; mapping bits was difficult. The
data driven but not statistical approach to parser development.
Figure 3: The process of translating a new sen-
tence in the LF system.
logical form was a graph rather than a tree ? in
?John ate and drank?, John is the DSUB (deep sub-
ject) of both eat and drink ? which led to com-
plications in transferring structure. Many such
complications were often handled through rules;
these rules grew more complex over time. Corpus-
based approaches efficiently learned many non-
compositional and domain specific issues.
2.2 Results and lessons learned
The system was quite successful at the time. MSR
used human evaluation heavily, performing both
absolute and relative quality evaluations. In the
absolute case, human judges gave each transla-
tion a score between 1 (terrible translation) and
4 (perfect). For relative evaluations, judges were
presented with two translations in randomized or-
der, and were asked whether they preferred system
A, system B, or neither. In its training domain,
the LF-based system was able to show substantial
improvements over rule-based systems that domi-
nated the market at the time.
Much of these gains were due to domain- and
context-sensitivity of the system. Consider the
Spanish verb ?activar?. A fair gloss into En-
glish is ?activate?, but the most appropriate trans-
lation in context varies (?signal?, ?flag?, etc.). The
example-based approach was able to capture those
contexts very effectively, leading to automatic do-
main customization given only translation mem-
ories. This was a huge improvement over rule-
based systems of the time.
During this same era, however, statistical ap-
proaches (Och and Ney, 2004) were showing great
promise. Therefore, we ran a comparison be-
tween the LF-based system and a statistical system
53
(a) Effecitve LF translation. Note how the LF system is able to translate ?se lleveban a cabo? even though that particular
surface form was not present in the training data.
SRC: La tabla muestra adema?s do?nde se llevaban a cabo esas tareas en Windows NT versio?n 4.0.
REF: The table also shows where these tasks were performed in Windows NT version 4.0.
LF: The table shows where, in addition, those tasks were conducted on Windows NT version 4.0.
STAT: The table also shows where llevaban to Windows NT version 4.0.
(b) Parsing errors may degrade translation quality; the parser interprted ?/? as coordination.
SRC: La sintaxis del operador / tiene las siguientes partes:
REF: The / operator syntax has these parts:
LF: The operator syntax it has the parts:
STAT: The / operator syntax has these parts:
(c) Graph-like structures for situations such as coordination are difficult to transfer (see the parenthesized group in particular);
selecting the correct form at generation time is difficult in the absence of a target language model.
SRC: Debe ser una consulta de seleccio?n (no una consulta de tabla de referencias cruzadas ni una consulta de accio?n).
REF: Must be a select query (not a crosstab query or action query).
LF: You must not be a select query neither not a query in table in cross-references nor not an action query.
STAT: Must be a select query (not a crosstab query or an action query).
Figure 4: Example source Spanish sentences, English reference translations of those sentences, transla-
tions from the LF system, and translations from a statistical translation system without linguistic features.
without linguistic information. Both systems were
trained and tuned on the same data, and translated
the same unseen test set. The linguistic system
had the additional knowledge sources at its dis-
posal: morphological, lexical, syntactic, and se-
mantic information. Regardless, the systems per-
formed nearly equally well on average. Each had
distinct strengths and weaknesses, though.
Often the success or failure of the LF-system
was tied to the accuracy of its deep analysis. When
these representations were accurate, they could
lead to effective generalizations and better trans-
lations of rare phenomena. Since surface words
were lemmatized and syntactic differences nor-
malized, unseen surface forms could still be trans-
lated as long as their lemma was known (see Fig-
ure 4(a)). Yet mistakes in identifying the correct
logical form could lead to major translation errors,
as in Figure 4(b).
Likewise the lack of statistics in the com-
ponents could cause problems. Statistical ap-
proaches found great benefits from the target lan-
guage model. Using a rule-based generation com-
ponent made it difficult to leverage a target lan-
guage model. Often, even if a particular transla-
tion was presented tens, hundreds, or thousands
of times in the data, the LF-based system could
not produce it because the rule-based generation
component would not propose the common sur-
face form, as in Figure 4(c).
We drew several lessons from this system when
developing our next generation of machine trans-
lation systems. It was clear to us that syntactic rep-
resentations can help translation, especially in re-
ordering and lexical selection: appropriate repre-
sentations allows better generalization. However,
over-generalization can lead to translation error, as
can parsing errors.
3 The Next Generation MSR MT
Systems
Research in machine translation at Microsoft has
been strongly influenced by this prior experience
with the LF system. First we must notice that
there is a huge space of possible translations. Con-
sider human reference translations: unless tied to
a specific domain or area, they seldom agree com-
pletely on lexical selection and word order. If our
system is to produce reasonable output, it should
consider a broad range of translation options, pre-
ferring outputs most similar to language used by
humans. Why do we say ?order of magnitude?
rather than ?magnitude order?, or ?master of cer-
emonies? rather than ?ceremonies master?? Many
choices in language are fundamentally arbitrary,
but we need to conform to those arbitrary deci-
sions if we are to produce fluent and understand-
able output. Second, while there is leverage to be
gained from deep features, seldom do we have a
component that identifies these features with per-
54
fect accuracy. In practice it seems that the error
rate increases as the depth of component analy-
sis increases. Finally, we need a representation
of ?good translations? that is understandable by a
computer. When forced to choose between two
translations, the system needs to make a choice:
an ordering.
Therefore, our data-driven systems crucially
rely on several components. First, we must effi-
ciently search a broad range of translations. Sec-
ond, we must rank according to both our linguistic
intuitions and the patterns that emerge from data.
We use a number of different systems based
on the availability of linguistic resources. So-
called phrasal statistic machine translation sys-
tems, which model translations using no more than
sequences of contiguous words, perform surpris-
ingly well and require nothing but tokenization in
both languages. In language pairs for which we
have a source language parser, a parse of the in-
put sentence is used to guide reordering and help
select relevant non-contiguous units; this is the
treelet system (Quirk and Menezes, 2006). Re-
gardless of which system we use, however, tar-
get language models score the fluency of the out-
put, and have a huge positive impact on translation
quality.
We are interested in means of incorporating lin-
guistic intuition deeper into such a system. As in
the case of the treelet system, this may define the
broad structure of the system. However, there are
also more accessible ways of influencing existing
systems. For instance, linguists may author fea-
tures that identify promising or problematic trans-
lations. We describe one such attempt in the fol-
lowing system.
3.1 Like and DontLike
Even in our linguistically-informed treelet sys-
tem (Quirk and Menezes, 2006), which uses syn-
tax in its translation system, many of the individ-
ual mappings are clearly bad, at least to a human.
When working with linguistic experts, one gut re-
sponse is to write rules that inspect the transla-
tion mappings and discard those translation map-
pings that appear dangerous. Perhaps they seem
to delete a verb, perhaps they use a speculative re-
ordering rule ? something makes them look bad to
a linguist. However, even if we are successful in
removing a poor translation choice, the remaining
possibilities may be even worse ? or perhaps no
translation whatsoever remains.
Instead, we can soften this notion. Imagine that
a linguist is able to say that this mapping is not
preferred because of some property. Likewise, a
skilled linguist might be able to identify mappings
that look particularly promising, and prefer those
mappings to others; see Figure 5 for an example.
This begs the question: how much should we
weight such influence? Our answer is a corpus
driven one. Each of these linguistic preferences
should be noted, and the weight of these prefer-
ences should be tuned with all others to optimize
the goodness of translation. Already our statisti-
cal system has a number of signals that attempt to
gauge translation quality: the translation models
attempt to capture fidelity of translation; language
models focus on fluency; etc. We use techniques
such as MERT (Och, 2003) and PRO (Hopkins
and May, 2011) to tune the relative weight of these
signals. Why not tune indicators from linguists in
the same manner?
When our linguists mark a mapping as +Like or
+DontLike, we track that throughout the search.
Each final translation incorporates a count of Like
mappings and a count of DontLike mappings, just
as it accumulates a language model score, trans-
lation model scores, word penalties, and so on.
These weights are tuned to optimize some approx-
imate evaluation metric. In Figure 6, the weight
of Like and DontLike is shown for a number of
systems, demonstrating how optimization may be
used to tune the effect of hand-written rules. Re-
moving these features degrades the performance
of an MT system by at least 0.5 BLEU points,
though the degradations are often even more visi-
ble to humans.
This mechanism has been used to capture a
number of effects in translation commonly missed
by statistical methods. It is crucial yet challenging
to maintain negation during translation, especially
in language pairs where negation is expressed dif-
ferently: some languages use a free morpheme
(Chinese tends to have a separate word), others
use a bound morpheme (English may use pre-
fixes), others require two separated morphemes
(French has negation agreement); getting any of
these wrong can lead to poor translations. Rules
that look at potentially distant words can help
screen away negation errors. Likewise rules can
help ensure that meaning is preserved, by prevent-
ing main verbs mapping to punctuation, or screen-
55
// don?t allow verb to be lost
if (forany(NodeList(rMapping),[Cat=="Verb" & ?Aux(SynNode(InputNode))])) {
list {segrec} bad_target=sublist(keeplist,
[forall(NodeList,[pure_punk(Lemma) | coord_conjunction(foreign_language,Lemma)])]);
if (bad_target) {
segrec rec;
foreach (rec; bad_target) {
+DontLike(rec);
}
}
}
Figure 5: An example rule for marking mappings as ?DontLike?. In this case, the rule searches for
source verbs that are not auxiliaries and that are translated into lemmas or punctuation. Such translations
are marked as DontLike.
Figure 6: A plot of the weights +Like map-
ping count and +DontLike mapping count weights
across language pairs. Generally Like is assigned
a positive weight (sometimes quite positive), and
DontLike is assigned a negative weight. In our
system, weights are L1 normalized (the sum of the
absolute values of the weights is equal to one), so
feature weights greater than 0.1 are very influen-
tial.
ing out mappings that seem unlikely, especially
when those mappings involve unusual tokens.
These two features are a rather coarse means of
introducing linguistic feedback. As our parame-
ter estimation techniques scale to larger features
more effectively, we are considering using finer-
grained feedback from linguists to say not only
that they like or don?t like a particular mapping,
but why. The relative impact of each type of feed-
back can be weighted: perhaps it is critical to pre-
serve verbs, but not so important to handle defi-
niteness. Given recent successes in scaling param-
eter estimation to larger and larger values, this area
shows great promise.
3.2 Linguistic component accuracy
Another crucial issue is the quality of the linguistic
components. We would certainly hope that better
quality of linguistic analysis should lead to bet-
ter quality translations. Indeed, in certain circum-
stances it appears that this correlation holds.
In the case of the treelet system, we hope to de-
rive benefit from linguistic features via a depen-
dency tree. To investigate the impact of the parse
quality, we can degrade a Treebank-trained parser
by limiting the amount of training data made avail-
able. As this decreases, the parser quality should
degrade. If we hold all other information in the
MT system fixed (parallel and monolingual train-
ing data, training regimen, etc.), then all differ-
ences should be due to the changes in parse qual-
ity. Table 1 presents the results of an experiment
of this form (Quirk and Corston-Oliver, 2006). As
the amount of training data increase, we see a sub-
stantial increase in parse quality.
Another way to mitigate parser error is to main-
tain syntactic ambiguity through the translation
process. For syntax directed translation systems,
this can be achieved by translating forests rather
than single trees, ideally including the score of
56
English- English-
System German Japanese
Phrasal 31.7 32.9
Right branching 31.4 28.0
250 instances 32.8 34.1
2,500 instances 33.0 34.6
25,000 instances 33.7 35.7
39,082 instances 33.8 36.0
Table 1: Comparison of BLEU scores as linguistic
information is varied. A phrasal system provides
a baseline free of linguistic information. Next we
consider a treelet system with a very weak base-
line: a right branching tree is always proposed.
This baseline is much worse than a simple phrasal
system. The final four rows evaluate the impact
of a parser trained on increasing amounts of sen-
tences from the English Penn Treebank. Even with
a tiny amount of training data, the system gets
some benefit from syntactic information, and the
returns appear to increase with more training data.
parse as part of the translation derivation. In un-
published results, we found that this made a sub-
stantial improvement in translation quality; the
effect was corroborated in other syntax directed
translation systems (Mi et al, 2008). Alterna-
tively, allowing a neighborhood of trees similar
to some predicted tree can handle ambiguity even
when the original parser does not maintain a for-
est. This also allows translation to handle phenom-
ena that are systematically mis-parsed, as well as
cases where the parser specification is not ideal
for the translation task. Recent work in this area
has show substantial improvements (Zhang et al,
2011).
4 Evaluation
4.1 Fact or Fiction: BLEU is Biased Against
Rule-Based or Linguistically-Informed
Systems?
It has generally been accepted as common wis-
dom that BLEU favors statistical MT systems and
disfavors those that are linguistically informed or
rule-based. Surprisingly, the literature on the topic
is rather sparse, with some notable exceptions
(Riezler and Maxwell, 2005; Farru?s et al, 2012;
Carpuat and Simard, 2012). We too have made
this assumption, and had a few years ago coined
the term treelet penalty to indicate the degree by
which BLEU favored our phrasal systems over our
treelet systems. We had noted on a few occa-
sions that treelet systems had lower BLEU scores
than our phrasal systems over the same data (the
?penalty?), but when compared against one an-
other in human evaluation, there was little dif-
ference, or often, treelet was favored. A notable
case was on German-English, where we noted a
three-point difference in BLEU between equiva-
lent treelet and phrasal systems (favoring phrasal),
and a ship/no-ship decision was dependent on the
resulting human eval. The general consensus of
the team was that the phrasal system was markedly
better, based on the BLEU result, and treelet sys-
tem should be pulled. However, after a human eval
was conducted, we discovered that the treelet sys-
tem was significantly better than the phrasal. From
that point forward, we talked about the treelet
penalty for German being three points, a ?fact?
that has lived in the lore of our team ever since.
What was really missing, however, was sys-
tematic experimental evidence showing the differ-
ences between treelet and phrasal systems. We
talked about the treelet penalty as a given, but
there was slow rumble of counter evidence sug-
gesting that maybe the assumptions behind the
?penalty? were actually unfounded, or minimally,
misinformed.
One piece of evidence was from experiments
done by Xiaodong He and an intern that showed an
interaction in quality differences between treelet
and phrasal gated by the length of the sentence.
Xiaodong was able to show that phrasal systems
tended to do better on longer sentences and treelet
on shorter: for Spanish-English, he showed a dif-
ference in BLEU of 1.29 on ?short? content on a
general domain test set, and 1.77 for short content
on newswire content (the NIST08 test set). The
BLEU difference diminished as the length of the
content increased, until there was very little dif-
ference (less than 1/2 point) for longer content.3
An interaction between decoder type and sentence
length means that there might also be an interac-
3These results were not published, but were provided to
the authors in a personal conversation with Xiaodong. In a
related paper (He et al, 2008), He and colleagues showed
significant improvements in BLEU on a system combination
system, but no diffs in human eval. Upon analysis, the re-
searchers were able to show that the biggest benefit to BLEU
was in short content, but the same preference was not exhib-
ited on the same content by the human evaluators. In other
words, the improvements observed in the short content that
BLEU favored had little impact on the overall impressions of
the human evaluators.
57
tion between decoder type and test set, especially
if particular test sets contain a lot of long-ish sen-
tences, e.g., WMT and Europarl). To the contrary,
most IT text, which is quite common in Microsoft-
specific localization content, tends to be shorter.
The other was based on general impressions
between treelet and phrasal systems. Because
treelet systems are informed by dependency parses
built over the source sentences (a parse can help
constrain a search space of possible translations,
and prune undesirable mappings e.g., constrain to
nominal types when the source is a noun), and,
as noted earlier, because the parses allow linguists
to pre- or post-process content based on observa-
tions in the parse, we have tended to see more
?fluent? output in treelet than phrasal. However,
as the sizes of data have grown steadily over the
years, the quality of translations in our phrasal sys-
tems have grown proportionally with the increase
in data. The question arose: is there also an in-
teraction between the size of our training data and
decoder type? In effect, does the quality of phrasal
systems catch-up to the quality of treelet systems
when trained over very large sets of data?
4.2 Treelet Penalty Experiments
We ran a set of experiments to measure the dif-
ferences between treelet and phrasal systems over
varying sizes of data, in order to measure the size
of the treelet penalty and its interaction with train-
ing data size. Our assumption was that a such
a penalty existed, and that the penalty decreased
as training data size increased, perhaps converg-
ing on zero for very large systems. Likewise,
we wanted to test the interaction between decoder
type and sentence length.
We chose two languages to run these exper-
iments on, Spanish and German, which we ran
in both directions, that is, English-to-target (EX)
and target-to-English (XE). We chose Spanish and
German for several reasons, first among them be-
ing that we have high-quality parsers for both lan-
guages, as we do for English. Further, we have
done significant development work on pre- and
post-processing for both languages over the past
several years. Both of these facts combined meant
that the treelet systems stood a real chance of be-
ing strong contenders in the experiments against
the equivalent phrasal systems. Further, although
the languages are typologically close neighbors
of English, the word order differences and high
distortion rates from English to or from German
might favor a parser-based approach.
We had four baseline systems that were built
over very large sets of data. For Spanish  En-
glish, the baseline systems were trained on over
22M sentence pairs; for German  English, the
baseline systems were trained on over 36M sen-
tence pairs.4 We then created five samples of the
baseline data for each language pair, consisting of
100K, 500K, 1M, 2M, and 5M sentence pairs (the
same samples were used for both EX and XE for
the respective pairs). We then trained both treelet
and phrasal systems in both directions (EX and
XE) over each sample of data. Language mod-
els were trained on all systems over the target-side
data.
For dev data, we used development data from
the 2010 WMT competition (Callison-Burch et al,
2010), and we used MERT (Och, 2003) to tune
each system. We tested each system against three
different test sets: two were from the WMT com-
petitions of 2009 and 2010, and the other was
one locally constructed from 5000 sentences of
content translated by users of our production ser-
vice (http://bing.com/translator), which we subse-
quently had manually translated into the target lan-
guages. The former two test sets are somewhat
news focused; the latter is a random sample of
miscellaneous translations, and is more generally
focused.
The results of the experiments are shown in Ta-
bles 2 and 3, with the relevant graphs in Fig-
ures 9 - 10. The reader will note that in all cases?
Spanish and German, EX and XE?the treelet sys-
tems scored higher than the related phrasal sys-
tems. This result surprised us, since we thought
that treelet systems would score less than phrasal
systems, especially at lower data sizes. That said,
in the Spanish systems, there is a clear conver-
gence as data sizes increased: on the WMT09
test set for English-Spanish, for instance, the diff
starts at 1.46 BLEU (treelet minus phrasal) for
the 100K sentence system, with a steady conver-
gence to near zero (0.12) for the full-data baseline.
The other test sets show the same steady conver-
gence, although they do not approach zero quite
as closely. (One might ask whether they would
converge to zero with more training data.) The
4A sizable portion of the data for each were scraped from
the Web, but there were other sources used as well, such as
Europarl, data from TAUS, MS internal localization data, UN
content, WMT news content, etc.
58
other direction is even more dramatic: on all test
sets the diffs converge on negative values, indi-
cating that phrasal systems surpass the quality of
the associated treelet systems at the largest data
points. This is a nice result since it shows, at least
in the case of Spanish, that there is an interac-
tion between decoder type and the amount of data:
treelet clearly does better at lower data amounts,
but phrasal catches up with, and can even pass, the
quality of equivalent treelet given sufficient data.
With larger data, phrasal may, in fact, be favored
over treelet.
The German systems do not tell quite as nice a
story. While it is still true that treelet has higher
BLEU scores than phrasal throughout, and that
systems trained using both decoders improve in
quality as more data is added (and the trajectory
is similar), there is no observable convergence as
data size increases. For German, then, we can only
say that more data helps either decoder, but we
cannot say that phrasal benefits from larger data
more than treelet. Why the difference between
Spanish and German? We suspect there may be an
interaction with the parsers, in that two separate
teams developed them. Thus, it could be the fact
that the strength of the respective parsers affected
how ?linguistically informed? particular systems
are. There could also be an interaction with the
number of word types vs. tokens in the German
data?given German?s rampant compounding?
which increases data sparsity, dampening effects
until much larger amounts of data are used. We
are still in the process of running additional ex-
periments to see if there are observable effects in
German with much larger data sizes, or at least,
to determine why German does not show the same
effects as Spanish.
Figure 7: English-Spanish BLEU graph across dif-
ferent data sizes, Treelet vs. Phrasal.
Since human evaluation is the gold standard we
Figure 8: Spanish-English BLEU graph across dif-
ferent data sizes, Treelet vs. Phrasal.
Figure 9: English-German BLEU graph across
different data sizes, Treelet vs. Phrasal.
Figure 10: German-English BLEU graph across
different data sizes, Treelet vs. Phrasal.
59
EX Treelet Phrasal Diff - T-P
Req Log WMT 2009 WMT 2010 Req Log WMT 2009 WMT 2010 Req Log WMT 2009 WMT 2010
100K 26.49 21.52 23.69 23.10 20.06 21.19 3.39 1.46 2.50
500K 28.61 22.85 25.20 25.64 21.47 22.86 2.97 1.38 2.34
1M 30.52 24.82 27.74 28.36 24.17 26.28 2.16 0.65 1.46
2M 31.61 25.59 28.54 29.48 24.76 26.91 2.13 0.83 1.63
5M 32.86 26.37 30.14 30.89 25.84 28.56 1.97 0.53 1.58
22M 33.80 27.01 30.61 32.55 26.89 30.12 1.25 0.12 0.49
XE
100K 27.72 21.76 23.21 26.18 20.80 21.78 1.54 0.96 1.43
500K 29.89 22.86 24.89 28.16 22.15 23.44 1.73 0.71 1.45
1M 32.18 24.76 27.14 31.32 24.32 26.02 0.86 0.44 1.12
2M 33.31 25.44 28.09 32.77 25.26 27.38 0.54 0.18 0.71
5M 34.47 26.17 29.10 34.18 26.10 28.74 0.29 0.07 0.36
22M 35.88 27.16 30.20 36.21 27.26 30.48 -0.33 -0.10 -0.28
Table 2: BLEU Score results for the Spanish Treelet Penalty experiments
EX Treelet Phrasal Diff (T-P)
Req Log WMT 2009 WMT 2010 Req Log WMT 2009 WMT 2010 Req Log WMT 2009 WMT 2010
100K 18.98 11.13 12.19 18.22 10.81 11.53 0.76 0.32 0.66
500K 22.13 13.18 14.33 21.09 12.74 13.68 1.04 0.44 0.65
1M 23.23 13.98 15.12 21.89 13.51 14.27 1.34 0.47 0.85
2M 23.72 14.77 15.87 23.11 14.04 15.03 0.61 0.73 0.84
5M 24.82 15.31 16.58 24.35 15.00 16.01 0.47 0.31 0.57
36M 26.72 16.72 18.20 25.83 16.33 17.18 0.89 0.39 1.02
XE
100K 27.42 15.91 16.37 26.75 15.83 16.28 0.67 0.08 0.09
500K 30.98 18.25 19.16 29.80 18.11 19.09 1.18 0.14 0.07
1M 32.30 19.16 20.40 31.26 19.06 20.18 1.04 0.10 0.22
2M 33.40 19.95 21.48 32.25 19.65 21.06 1.15 0.30 0.42
5M 34.86 21.14 22.55 33.91 20.67 22.13 0.95 0.47 0.42
36M 37.31 22.72 24.97 36.08 21.99 23.85 1.23 0.73 1.12
Table 3: BLEU Score results for the German Treelet Penalty experiments
60
seek to achieve with our quality measures, and
since BLEU is only weakly correlated with hu-
man eval (Coughlin, 2003), we ran human evals
against both the English-Spanish and English-
German output. Performing human evaluation
gives us two additional perspectives on the data:
(1) do humans perceive a qualitative difference be-
tween treelet and phrasal, as we see with BLEU,
and (2), if the difference is perceptible, what is its
magnitude relative to BLEU. If the magnitude of
the difference is much larger than that of BLEU,
and especially does not show convergence in the
Spanish cases, then we still have a strong case
for the Treelet Penalty. In fact, if human evalu-
ators perceive a difference Spanish cases on the
full data systems, the case where we show con-
vergence, then the resulting differences could be
described as the penalty value.
Unfortunately, our human evaluation data on
the Treelet Penalty effect was inconclusive. Our
evaluations show a strong correlation between
BLEU and human evaluation, something that is at-
tested to in the literature (e.g., , the first paper on
BLEU (Papineni et al, 2002), and a deeper explo-
ration in (Coughlin, 2003)). However, the effect
we were looking for ? that is, a difference between
human evaluations across decoders ? was not evi-
dent. In fact, the human evaluations followed the
differences we saw in BLEU between the two de-
coders very closely. Figure 11 shows data points
for each data size for each decoder, plotting BLEU
against human evaluation. When we fit a regres-
sion line against the data points for each decoder,
we see complete overlap.5
Figure 11: Scatterplot showing Treelet vs Phrasal
systems across different data sizes, plotting BLEU
(Y) against Human Eval scores (X)
5Clearly, the sample is very small, so the regression line
should be taken with a grain of salt. We would need a lot
more data to be able to draw any strong conclusions.
In summary, we show a strong effect of treelet
systems performing better than phrasal systems
trained on the same data. That difference, how-
ever, generally diminishes as data sizes increase,
and in the case of Spanish (both directions), there
is a convergence in very large data sizes. These
results are not completely surprising, but still are a
nice systematic confirmation that linguistically in-
formed systems really do better in lower-data en-
vironments. Without enough data, statistical sys-
tems cannot learn the generalizations that might
otherwise be provided by a parse, or codified in
rules. What we failed to show, at least with Span-
ish and German, is a confirmation of the existence
of the Treelet Penalty. Given the small number of
samples, a larger study which includes many more
language pairs and data sizes, may once and for all
confirm the Penalty. Thus far, human evaluations
do not show qualitative differences between the
two decoders?at least, not divergent from BLEU.
4.3 Interaction Between Decoder Type and
Sentence Length
When comparing the differences between de-
coders, another area to pay special attention to is
systematic differences in behavior as input content
is varied. For example, we may expect a phrasal
decoder to do better on noisier, less grammatical
data than a parser-informed decoder, since in the
latter case the parser may fail to parse; the failure
could ripple through subsequent processes, and
thus lessen the quality of the output. Likewise, a
parser-informed decoder may do better on content
that is short and easy to parse. If we were to do a
coarse-grained separation of data into length buck-
ets, making the very gross assumption that short
equals easy-to-parse and long not, then we may
see some qualitative differences between the de-
coders across these buckets.
To see length-based effects across decoder
types, we designed a set of experiments on Ger-
man and Spanish in both directions, where we sep-
arated the WMT 2010 test data into length-based
word-count buckets: 0-10, 10-20, 20-30, 30-40,
and 40+ words. We then calculated the BLEU
scores on each of these buckets, the results for
which are shown in Figures 12.
Treelet does better than phrasal in almost all
conditions (except one). That is not surprising,
given the results we observed in Section 4.2. What
is interesting is to see how much stronger treelet
61
Figure 12: Treelet-Phrasal BLEU differences by
bucket across language pair
performs on short content than phrasal: treelet
does the best on the shortest content, with quality
dropping off anywhere between 10-30 words.
One conclusion that can be drawn from these
data is that treelet performs best on short con-
tent precisely because the parser can easily parse
the content, and the parse is effective in inform-
ing subsequent processes. The most sustained
benefit is observable in English-German, with a
bump up at 10-20, and a slow tapering off there-
after. Processing the structural divergence be-
tween the two languages, especially when it comes
to word order, may benefit more from a parse. In
other words, the parser can help inform alignment
where there are long-distance distortion effects; a
phrasal system?s view is too local to catch them.
However, at longer sentence sizes, the absence
of good parses lessen the treelet advantage. In
fact, in English-German (and in Spanish-English)
at 40+, there is no observable benefit of treelet
over phrasal.6
5 The Data Gap
All Statistical Machine Translation work relies on
data, and the manipulation of the data as a pre-
process can often have significant effects down-
stream. ?Data munging?, as we like to call it, is
every team?s ?secret sauce?, something that can
often lead to multi-point differences in BLEU.
For most teams, the heuristics that are applied are
fairly ad hoc, and highly dependent on the kind of
data being consumed. Since data sources are of-
ten quite noisy, e.g., the Web, noise reduction is a
key component of many of the heuristics. Here is
6The bump up at 40+ on English-Spanish and German-
English is inexplicable, but may be attributable to the diffi-
culty that either decoder has in processing such long content.
There is also likely an interaction with statistical noise cause
by such small sample sizes.
a list of common heuristics applied to data. Some
of these are drawn from our own pre-processing,
some are mentioned explicitly in other literature,
in particular, (Denkowski et al, 2012).
? Remove lines containing escape characters,
invalid Unicode, and other non-linguistic
noise.
? Remove content that where the ratio of cer-
tain content passes some threshold, e.g., al-
phabetic/numeric ratio, script ratio (percent-
age of characters in wrong form passes some
threshold, triggering removal).
? Normalize space, hyphens, quotes, etc. to
standard forms.
? Normalize Unicode characters to canonical
forms, e.g., Form C, Form KC.
? In parallel data, measure the degree of ratio
of length imbalance (e.g., character or word
count) between source and target, as a test for
misalignments. Remove sentence pairs that
pass some threshold.
? Remove content where character count for
any token, or token count across a sentence,
exceeds some threshold (the assumption be-
ing that really long content is of little benefit
due to complications it causes in downstream
processing).
The point of data cleaning heuristics is to in-
crease the value of training data. Each data point
that is noisy increases the chance of learning
something that could be distracting or harmful.
Likewise, each data point that is cleaned reduces
the level of data sparsity (e.g., through normaliza-
tions or substitutions) and improves the chances
that the models will be more robust. Although
it has been shown that increasing the amount of
training data for SMT improves results (Brants et
al., 2007), not all data is beneficial, and clean data
is best of all.
Crucially, most data munging is done through
heuristics, or rules, although thresholds or con-
straints can be tuned by data. A more sophis-
ticated example of data cleaning is described in
(Denkowski et al, 2012) where the authors used
machine learning methods for measuring quality
estimation to select the ?best? portions of a cor-
pus. So, rather than training their SMT on an en-
tire corpus, they trained an estimator that selected
62
the best portions, and used only those. In their en-
try in the 2012 WMT competition, they used only
60% of the English-French Gigaword corpus7 and
came in first in the shared translation task for the
pair.
Another important aspect of data as it relates to
SMT is task-dependence: what domain or genre
of data will an SMT engine be applied to? For
instance, will an SMT engine be used to trans-
late IT content, news content, subtitles, or Eu-
roparl proceedings? If the engine itself is trained
on data that is dissimilar to the desired goal, then
results may be less than satisfying. This is a com-
mon problem in the field, and a cottage industry
has been built around customization and domain-
adaptation, e.g., (Moore and Lewis, 2010; Axelrod
et al, 2011; Wang et al, 2012). In general, the so-
lution is to adapt an SMT engine to the desired
domain using a set of seed data in that domain.
A more difficult problem is when there is very
little parallel data in the desired domain, which is
a problem we will look at in the next section.
5.1 Preprocessing Data to Make it Match
A little over a year ago, Facebook activated a
translation feature in their service, which directly
called Bing Translator. This feature has allowed
users to translate pages or posts not in their native
language with a See Translation option. An exam-
ple is shown in Figure 13.
The real problem with translating ?FB-speak?,
or content from virtually any kind of social media,
is the paucity of parallel data in the domain. This
flies in the face of the usual way problems are tack-
led in SMT, that is, locate (lots of) relevant parallel
data, and then train up a decoder. Outside of a few
slang dictionaries, there is almost no FB-like par-
allel content available.
Given the relatively formal nature of the text
that most of our engines are trained on, the mis-
match between FB content and our translation en-
gines often led to very poor translations. Yet,
given the absence of in-domain parallel data, it
was not possible for us to train-up FB-specific
SMT engines. We realized that our only option
was to somehow manipulate the input to make it
look more like the content we trained our engines
on. Effectively, if we treated ?FB-speak? as a di-
alect of the source language, we could use distri-
7The English-French Gigaword corpus is described in
(Callison-Burch et al, 2009)
Regex Output
frnd[sz] friends
plz+ please
yess* yes
be?c[uo][sz] because
nuff enough
wo?u?lda would have
srr+y sorry
Table 5: Some example regexes to ?fix? FaceBook
content
butional queues of dialect-specific content to find
the counterparts in the majority dialect.
Table 4 gives some examples of FB content on
the left, and the more formal representation of the
same on the right. The reader will note some sys-
tematic characteristics of the FB content as com-
pared to the formal content (see also (Hassan and
Menezes, 2013)). Given the absence of parallel
training data, we could ?correct? the FB content
to make it look more like English, and then trans-
late the ?corrected? English through our engines.
Our first inclination was to examine the logs of
the most frequent words being translated by FB
users and use string substitutions or regexes (regu-
lar expressions) to effect repairs. We arrived very
quickly at a large set of simple repairs like those
shown in Table 5. We were able to achieve greater
than 97% precision using a large table of substitu-
tions for the most common translations (against a
held-out set of FB content). However, there were
two problems with the approach: (1) recall was
relatively low, at 52.03%, and (2) the solution was
not easily scalable to additional languages and sce-
narios.
To address these two deficiencies, we sought a
more data-driven approach. But we had to be cre-
ative since our standard ?hammer? of parallel data
did not exist. Our intuition was that there were
distributional regularities in the FB content that
could help discover a mapping for a given target
word, e.g., the distribution of plzzz in the FB con-
tent would allow us to discover that it distributes
similarly to please in our non-FB content. Hany
Hassan developed a TextCorrector tool that is, as
he put it (Hassan and Menezes, 2013), ?based on
constructing a lattice from possible normalization
candidates and finding the best normalization se-
quence according to an n-gram language model
using a Viterbi decoder?, where he developed an
63
Figure 13: Two Facebook posts: the first translated, the second showing the See Translation option
FB Speak English Translation Comment
goooood morniiing good morning Extended characters for emphasis or dramatic effect
wuz up bro What?s up brother ?Phonetic? spelling to reflect local dialect or usage
cm to c my luv Come to see my love Remove vowels in common words, sound-alike sequences
4get, 2morrow forget, tomorrow Sound-alike number substitution
r u 4 real? Are you for real? Sound-alike letter and number substitutions
LMS Like my status Single ?word? abbreviations for
IDK I don?t know multi-word expressions
ROFL Rolling on the floor laughing
Table 4: FB Speak with English references
?unsupervised approach to learn the normalization
candidates from unlabeled text data.? He then used
a Random Walk strategy to walk a contextual sim-
ilarity graph. The two principal benefits of this
approach is that it did not require parallel train-
ing data?two large monolingual corpora are re-
quired, one for the ?noisy? data (i.e., FB content)
and one for the clean data (i.e., our large supply
of language model training data)?nor did it re-
quire labeled data (i.e., , the algorithm is unsu-
pervised). After several iterations over very large
corpora (tens of millions of sentences) he arrived
at a solution that had comparable precision to the
regex method but had much higher recall. The best
iteration achieved 96.51% precision (the regex ap-
proach achieve 97.07% precision) and 72.38% re-
call (regex: 52.03%).8 Crucially, as the size of
the data increases, the TextCorrector continues to
show improvement.
The end result was a much better User Expe-
rience for FB users. Rather than badly mangled
translations, or worse, no translations at all, users
get translations generated by our standard, very
large statistical engines (for English source, no-
tably, our treelet engines). An example English
source string is shown in Table 6, with transla-
8For a complete description of TextCorrector, please
see (Hassan and Menezes, 2013).
tions shown for both the corrected and uncorrected
source.
6 Conclusions and Future Directions
A crucial lesson from the work on the FB correc-
tions described in Section 5.1 is its analog to Ma-
chine Learning as a whole: rule-based approaches
often achieve very high precision, but often at the
sacrifice of recall. The same is true in Machine
Translation: rule-based MT is often more accurate
when it was accurate, resulting in more precise and
grammatical translations. However, it tends to be
somewhat brittle and does not do as well on cases
not explicitly coded for. SMT, on the other hand,
tends to be more malleable and adaptable, but of-
ten less precise. Tapping rule-based approaches
in a statistical framework can really give us the
best of both worlds, giving us higher precision and
higher recall.
Finding an appropriate mix is difficult, though.
As in the case of parsing, we can see how errors
can substantially degrade translation quality, espe-
cially if we only consider the single best analysis.
By making our analysis components as robust as
possible, quantifying our degree of certainty with
scoring mechanisms, and preserving ambiguity of
the analysis, we can achieve a better return on in-
64
Language Unrepaired Repaired
Original English i?l do cuz ma parnts r ma lyf I?ll do because my parents are my life
To Italian i ? l fare cuz ma parnts r ma lyf lo far perch i miei genitori sono la mia vita
To German i ? l tun Cuz Ma Parnts R Ma lyf Ich werde tun, weil meine Eltern mein Leben sind
To Spanish traer hacer cuz ma parnts r ma lyf voy a hacer porque mis padres son mi vida
Table 6: One English FB sentence with and without normalizations, translated to various languages
vestment. Making this linguistic information be
included softly as features is a powerful way of
surfacing linguistic generalizations to the system
while not forcing its hand.
Some of the greatest successes in mixing lin-
guistic and statistical methods have been in syn-
tax. There is much ground to cover still. Mor-
phology is integrated weakly into current SMT
systems, mostly as broad features (Jeong et al,
2010) though sometimes with more sophistica-
tion (Chahuneau et al, 2013). Better integration of
morphological features could have great effect, es-
pecially in agglutinative languages such as Finnish
and Turkish.
Deeper models of semantics present a rich chal-
lenge to the field. As we proceed into deeper mod-
els, picking the correct representation is a signifi-
cant issue. Humans can generally agree on words,
mostly on morphology, and somewhat on syntax.
But semantics touches on issues of meaning repre-
sentation: how should we best represent semantic
information? Should we attempt to faithfully rep-
resent all the information in the source language,
or gather only a simple model that suffices to dis-
ambiguate information? Others are focusing on
lexical semantics using continuous space repre-
sentations (Mikolov et al, 2013), a softer means
of representing meaning.
Regardless of the details, one point is very clear:
future work in MT will require dealing with data.
Systems, whether statistical or rule-based, will
need to work with and learn from the increas-
ing volumes of information available to comput-
ers. Effective hybrid systems will be no exception
? tempering the keen insights of experts with the
noisy wisdom of big data from the crowd holds
great promise.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of EMNLP, pages 355?
362.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 858?867, Prague, Czech Republic,
June. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 17?53, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Marine Carpuat and Michel Simard. 2012. The trouble
with smt consistency. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
442?449, Montre?al, Canada, June. Association for
Computational Linguistics.
Victor Chahuneau, Noah A. Smith, and Chris Dyer.
2013. Knowledge-rich morphological priors for
bayesian language models. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1206?1215, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Deborah A. Coughlin. 2003. Correlating automated
and human assessments of machine translation qual-
ity. In Proceedings of MT Summit IX, New Or-
leans, Louisiana, USA, September. The Association
for Machine Translation in the Americas (AMTA).
Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012. The CMU-Avenue French-English Transla-
tion System. In Proceedings of the NAACL 2012
Workshop on Statistical Machine Translation.
Mireia Farru?s, Marta R. Costa-jussa?, and Maja
Popovic. 2012. Study and correlation analysis of
linguistic, perceptual and automatic machine trans-
lation evaluations. Journal of the American Society
for Information Science and Technology, 63(1):174?
184, January.
65
Hany Hassan and Arul Menezes. 2013. Social text nor-
malization using contextual graph random walks. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-hmm-
based hypothesis alignment for combining outputs
from machine translation systems. In Proceedings
of EMNLP.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Minwoo Jeong, Kristina Toutanova, Hisami Suzuki,
and Chris Quirk. 2010. A discriminative lexicon
model for complex morphology. In The Ninth Con-
ference of the Association for Machine Translation
in the Americas (AMTA-2010).
Kevin Knight. 2013. Tutorial on decipherment. In
ACL 2013, Sofia, Bulgaria, August.
Arul Menezes and Stephen D. Richardson. 2001. A
best-first alignment algorithm for automatic extrac-
tion of transfer mappings from bilingual corpora. In
Association for Computational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT,
pages 192?199, Columbus, Ohio, June. Association
for Computational Linguistics.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746?751, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Robert C. Moore and William D. Lewis. 2010. Intel-
ligent Selection of Language Model Training Data.
In Proceedings of the ACL 2010 Conference Short
Papers, Uppsala, Sweden, July.
Franz Josef Och and Hermann Ney. 2004. The
Alignment Template Approach to Statistical Ma-
chine Translation. Computational Linguisitics,
30(4):417?449, September.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st ACL, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th ACL, Philadelphia, PA.
Chris Quirk and Simon Corston-Oliver. 2006. The im-
pact of parse quality on syntactically-informed sta-
tistical machine translation. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing, pages 62?69, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Chris Quirk and Arul Menezes. 2006. Dependency
Treelet Translation: The convergence of statistical
and example-based Machine Translation? Machine
Translation, 20:43?65.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, pages
57?64, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Wei Wang, Klaus Macherey, Wolfgang Macherey,
Franz Och, and Peng Xu. 2012. Improved do-
main adaptation for statistical machine translation.
In Proceedings of AMTA.
Warren Weaver. 1955. Translation. In William N.
Locke and A. Donald Booth, editors, Machine
Translation of Languages, pages 15?23. MIT Press,
Massachussets.
Hao Zhang, Licheng Fang, Peng Xu, and Xiaoyun Wu.
2011. Binarized forest to string translation. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 835?845, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
66
