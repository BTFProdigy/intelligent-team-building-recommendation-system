2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 11?19,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Acoustic-Prosodic Entrainment and Social Behavior
Rivka Levitan1, Agust??n Gravano2, Laura Willson1,
S?tefan Ben?us?3, Julia Hirschberg1, Ani Nenkova4
1 Dept. of Computer Science, Columbia University, New York, NY 10027, USA
2 Departamento de Computacio?n (FCEyN), Universidad de Buenos Aires, Argentina
3 Constantine the Philosopher University & Institute of Informatics, Slovak Academy of Sciences, Slovakia
4 Dept. of Computer and Information Science, University of Pennsylvania, Philadelphia, PA 19104, USA
rlevitan@cs.columbia.edu, gravano@dc.uba.ar, law2142@barnard.edu,
sbenus@ukf.sk, julia@cs.columbia.edu, nenkova@seas.upenn.edu
Abstract
In conversation, speakers have been shown
to entrain, or become more similar to each
other, in various ways. We measure entrain-
ment on eight acoustic features extracted from
the speech of subjects playing a cooperative
computer game and associate the degree of en-
trainment with a number of manually-labeled
social variables acquired using Amazon Me-
chanical Turk, as well as objective measures
of dialogue success. We find that male-female
pairs entrain on all features, while male-male
pairs entrain only on particular acoustic fea-
tures (intensity mean, intensity maximum and
syllables per second). We further determine
that entrainment is more important to the per-
ception of female-male social behavior than it
is for same-gender pairs, and it is more impor-
tant to the smoothness and flow of male-male
dialogue than it is for female-female or mixed-
gender pairs. Finally, we find that entrainment
is more pronounced when intensity or speak-
ing rate is especially high or low.
1 Introduction
Entrainment, also termed alignment, adaptation,
priming or coordination, is the phenomenon of
conversational partners becoming more similar to
each other in what they say, how they say it,
and other behavioral phenomena. Entrainment has
been shown to occur for numerous aspects of spo-
ken language, including speakers? choice of re-
ferring expressions (Brennan & Clark, 1996); lin-
guistic style (Niederhoffer & Pennebaker, 2002;
Danescu-Niculescu-Mizil et al, 2011); syntactic
structure (Reitter et al, 2006); speaking rate (Lev-
itan & Hirschberg, 2011); acoustic/prosodic fea-
tures such as fundamental frequency, intensity, voice
quality (Levitan & Hirschberg, 2011); and phonet-
ics (Pardo, 2006).
Entrainment in many of these dimensions has also
been associated with different measures of dialogue
success. For example, Chartrand and Bargh (1999)
demonstrated that mimicry of posture and behavior
led to increased liking between the dialogue par-
ticipants as well as a smoother interaction. They
also found that naturally empathetic individuals ex-
hibited a greater degree of mimicry than did oth-
ers. Nenkova et al (2008) found that entrainment
on high-frequency words was correlated with nat-
uralness, task success, and coordinated turn-taking
behavior. Natale (1975) showed that an individ-
ual?s social desirability, or ?propensity to act in
a social manner,? can predict the degree to which
that individual will match her partner?s vocal inten-
sity. Levitan et al (2011) showed that entrainment
on backchannel-preceding cues is correlated with
shorter latency between turns, fewer interruptions,
and a higher degree of task success. In a study of
married couples discussing problems in their rela-
tionships, Lee et al (2010) found that entrainment
measures derived from pitch features were signifi-
cantly higher in positive interactions than in nega-
tive interactions and were predictive of the polarity
of the participants? attitudes.
These studies have been motivated by theoreti-
cal models such as Giles? Communication Accom-
modation Theory (Giles & Coupland, 1991), which
proposes that speakers promote social approval or
11
efficient communication by adapting to their inter-
locutors? communicative behavior. Another theory
informing the association of entrainment and dia-
logue success is the coordination-rapport hypoth-
esis (Tickle-Degnen & Rosenthal, 1990), which
posits that the degree of liking between conversa-
tional partners should be correlated with the degree
of nonverbal coordination between them.
Motivated by such theoretical proposals and em-
pirical findings, we hypothesized that entrainment
on acoustic/prosodic dimensions such as pitch, in-
tensity, voice quality and speaking rate might also
be correlated with positive aspects of perceived
social behaviors as well as other perceived char-
acteristics of efficient, well-coordinated conversa-
tions. In this paper we describe a series of ex-
periments investigating the relationship between ob-
jective acoustic/prosodic dimensions of entrainment
and manually-annotated perception of a set of so-
cial variables designed to capture important as-
pects of conversational partners? social behaviors.
Since prior research on other dimensions of entrain-
ment has sometimes observed differences in degree
of entrainment between female-female, male-male
and mixed gender groups (Bilous & Krauss, 1988;
Pardo, 2006; Namy et al, 2002), we also exam-
ined our data for variation by gender pair, consid-
ering female-female, male-male, and female-male
pairs of speakers separately. If previous findings
extend to acoustic/prosodic entrainment, we would
expect female-female pairs to entrain to a greater
degree than male-male pairs and female partners in
mixed gender pairs to entrain more than their male
counterparts. Since prior findings posit that entrain-
ment leads to smoother and more natural conversa-
tions, we would also expect degree of entrainment
to correlate with perception of other characteristics
descriptive of such conversations.
Below we describe the corpus and annotations
used in this study and how our social annotations
were obtained in Sections 2 and 3. We next discuss
our method and results for the prevalence of entrain-
ment among different gender groups (Section 4). In
Sections 5 and 6, we present the results of correlat-
ing acoustic entrainment with social variables and
objective success measures, respectively. Finally, in
Section 7, we explore entrainment in cases of outlier
feature values.
2 The Columbia Games Corpus
The Columbia Games Corpus (Gravano & Hirsch-
berg, 2011) consists of approximately nine hours
of spontaneous dialogue between pairs of subjects
playing a series of computer games. Six females and
seven males participated in the collection of the cor-
pus; eleven of the subjects returned on a different
day for another session with a new partner.
During the course of each session, a pair of speak-
ers played three Cards games and one Objects game.
The work described here was carried out on the Ob-
jects games. This section of each session took 7m
12s on average. We have a total of 4h 19m of Ob-
jects game speech in the corpus.
For each task in an Objects game, the players
saw identical collections of objects on their screens.
However, one player (the Describer) had an addi-
tional target object positioned among the other ob-
jects, while the other (the Follower) had the same
object at the bottom of her screen. The Describer
was instructed to describe the position of the target
object so that the Follower could place it in exactly
the same location on her screen. Points (up to 100)
were awarded based on how well the Follower?s tar-
get location matched the describers. Each pair of
partners completed 14 such tasks, alternating roles
with each task. The partners were separated by a
curtain to ensure that all communication was oral.
The entire corpus has been orthographically tran-
scribed and words aligned with the speech source. It
has also been ToBI-labeled (Silverman et al, 1992)
for prosodic events, as well as labeled for turn-
taking behaviors.
3 Annotation of Social Variables
In order to study how entrainment in various dimen-
sions correlated with perceived social behaviors of
our subjects, we asked Amazon Mechanical Turk1
annotators to label the 168 Objects games in our cor-
pus for an array of social behaviors perceived for
each of the speakers, which we term here ?social
variables.?
Each Human Intelligence Task (HIT) presented to
the AMT workers for annotation consisted of a sin-
gle Objects game task. To be eligible for our HITs,
1http://www.mturk.com
12
annotators had to have a 95% success rate on pre-
vious AMT HITs and to be located in the United
States. They also had to complete a survey estab-
lishing that they were native English speakers with
no hearing impairments. The annotators were paid
$0.30 for each HIT they completed. Over half of the
annotators completed fewer than five hits, and only
four completed more than twenty.
The annotators listened to an audio clip of the
task, which was accompanied by an animation that
displayed a blue square or a green circle depending
on which speaker was currently talking. They were
then asked to answer a series of questions about each
speaker: Does Person A/B believe s/he is better than
his/her partner? Make it difficult for his/her partner
to speak? Seem engaged in the game? Seem to dis-
like his/her partner? Is s/he bored with the game?
Directing the conversation? Frustrated with his/her
partner? Encouraging his/her partner? Making
him/herself clear? Planning what s/he is going to
say? Polite? Trying to be liked? Trying to domi-
nate the conversation? They were also asked ques-
tions about the dialogue as a whole: Does it flow
naturally? Are the participants having trouble un-
derstanding each other? Which person do you like
more? Who would you rather have as a partner?
A series of check questions with objectively de-
terminable answers (e.g. ?Which speaker is the De-
scriber??) were included among the target questions
to ensure that the annotators were completing the
task with integrity. HITs for which the annotator
failed to answer the check questions correctly were
disqualified.
Each task was rated by five unique annotators who
answered ?yes? or ?no? to each question, yielding
a score ranging from 0 to 5 for each social vari-
able, representing the number of annotators who an-
swered ?yes.? A fuller description of the annotation
for social variables can be found in (Gravano et al,
2011).
In this study, we focus our analysis on annotations
of four social variables:
? Is the speaker trying to be liked?
? Is the speaker trying to dominate the conversa-
tion?
? Is the speaker giving encouragement to his/her
partner?
? Is the conversation awkward?
We correlated annotations of these variables with
an array of acoustic/prosodic features.
4 Acoustic entrainment
We examined entrainment in this study in eight
acoustic/prosodic features:
? Intensity mean
? Intensity max
? Pitch mean
? Pitch max
? Jitter
? Shimmer
? Noise-to-harmonics ratio (NHR)
? Syllables per second
Intensity is an acoustic measure correlated with
perceived loudness. Jitter, shimmer, and noise-to-
harmonics ratios are three measures of voice quality.
Jitter describes varying pitch in the voice, which is
perceived as a rough sound. Shimmer describes fluc-
tuation of loudness in the voice. Noise-to-harmonics
ratio is associated with perceived hoarseness. All
features were speaker-normalized using z-scores.
For each task, we define entrainment between
partners on each feature f as
ENTp = ?|speaker1f ? speaker2f |
where speaker[1,2]f represents the corresponding
speaker?s mean for that feature over the task.
We say that the corpus shows evidence of en-
trainment on feature f if ENTp, the similarities be-
tween partners, are significantly greater than ENTx,
the similarities between non-partners:
ENTx = ?
?
i |speaker1f ?Xi,f |
|X|
where X is the set of speakers of same gender and
role as the speaker?s partner who are not paired with
the speaker in any session. We restrict the compar-
isons to speakers of the same gender and role as the
speaker?s partner to control for the fact that differ-
ences may simply be due to differences in gender or
role. The results of a series of paired t-tests compar-
ing ENTp and ENTx for each feature are summarized
in Table 1.
13
Feature FF MM FM
Intensity mean X X X
Intensity max X X X
Pitch mean X
Pitch max X
Jitter X X
Shimmer X X
NHR X
Syllables per sec X X X
Table 1: Evidence of entrainment for gender pairs. A tick
indicates that the data shows evidence of entrainment on
that row?s feature for that column?s gender pair.
We find that female-female pairs in our corpus
entrain on, in descending order of significance, jitter,
intensity max, intensity mean, syllables per second
and shimmer. They do not entrain on pitch mean
or max or NHR. Male-male pairs show the least
evidence of entrainment, entraining only on inten-
sity mean, intensity max, and syllables per second,
supporting the hypothesis that entrainment is less
prevalent among males. Female-male pairs entrain
on, again in descending order of significance, inten-
sity mean, intensity max, jitter, syllables per second,
pitch mean, NHR, shimmer, and pitch max ? in fact,
on every feature we examine, with significance val-
ues in each case of p<0.01.
To look more closely at the entrainment behavior
of males and females in mixed-gender pairs, we de-
fine ENT2p as follows:
ENT2p = ?
?
i |Pi,f ? Ti,f |
|T|
where T is the set of the pause-free chunks of speech
that begin a speaker?s turns, and P is the correspond-
ing set of pause-free chunks that end the interlocu-
tor?s preceding turns. Unlike ENTp, this measure is
asymmetric, allowing us to consider each member
of a pair separately.
We compare ENT2p for each feature for males and
females of mixed gender pairs. Contrary to our hy-
pothesis that females in mixed-gender pairs would
entrain more, we found no significant differences
in partner gender. Females in mixed-gender pairs
do not match their interlocutor?s previous turn any
more than do males. This may be due to the fact
Feature FM MM F p
Intensity mean ? ? 3.83 0.02
Intensity max ? ? 4.01 0.02
Syllables per sec ? ? 2.56 0.08
Table 2: Effects of gender pair on entrainment. An arrow
pointing up indicates that the group?s normalized entrain-
ment for that feature is greater than that of female-female
pairs; an arrow pointing down indicates that it is smaller.
that, as shown in Table 1, the overall differences be-
tween partners in mixed-gender pairs are quite low,
and so neither partner may be doing much turn-by-
turn matching.
However, as we expected, entrainment is least
prevalent among male-male pairs. Although we ex-
pected female-female pairs to exhibit the highest
prevalence of entrainment, they do not show evi-
dence of entrainment on pitch mean, pitch max or
NHR, while female-male pairs entrain on every fea-
ture. In fact, although ENTp for these features is not
significantly smaller between female-female pairs
than between female-male pairs, ENTx, the overall
similarity among non-partners for these features, is
significantly larger between females than between
females and males. The degree of similarity between
female-female partners is therefore attributable to
the overall similarity between females rather than
the effect of entrainment.
All three types of pairs exhibit entrainment on in-
tensity mean, intensity max, and syllables per sec-
ond. We look more closely into the gender-based
differences in entrainment behavior with an ANOVA
with the ratio of ENTp to ENTx as the dependent
variable and gender pair as the independent variable.
Normalizing ENTp by ENTx allows us to compare
the degree of entrainment across gender pairs. Re-
sults are shown in Table 2. Male-male pairs have
lower entrainment than female-female pairs for ev-
ery feature; female-male pairs have higher entrain-
ment than female-female pairs for intensity mean
and max and lower for syllables per second (p <
0.1). These results are consistent with the general
finding that male-male pairs entrain the least and
female-male pairs entrain the most.
14
5 Entrainment and social behavior
We next correlate each of the social variables de-
scribed in Section 3 with ENTp for our eight acous-
tic features. Based on Communication Accommo-
dation Theory, we would expect gives encourage-
ment, a variable representing a desirable social char-
acteristic, to be positively correlated with entrain-
ment. Conversely, conversation awkward should be
negatively correlated with entrainment. We note that
Trying to be liked is negatively correlated with the
like more variable in our data ? that is, annotators
were less likely to prefer speakers whom they per-
ceived as trying to be liked. This reflects the in-
tuition that someone overly eager to be liked may
be perceived as annoying and socially inept. How-
ever, similarity-attraction theory states that similar-
ity promotes attraction, and someone might there-
fore entrain in order to obtain his partner?s social
approval. This idea is supported by Natale?s find-
ing that the need for social approval is predictive
of the degree of a speaker?s convergence on inten-
sity (Natale, 1975). We can therefore expect trying
to be liked to positively correlate with entrainment.
Speakers who are perceived as trying to dominate
may be overly entraining to their interlocutors in
what is sometimes called ?dependency overaccom-
modation.? Dependency overaccommodation causes
the interlocutor to appear dependent on the speaker
and gives the impression that the speaker is control-
ling the conversation (West & Turner, 2009).
The results of our correlations of social vari-
ables with acoustic/prosodic entrainment are gen-
erally consonant with these intuitions. Although it
is not straightforward to compare correlation coeffi-
cients of groups for which we have varying amounts
of data, for purposes of assessing trends, we will
consider a correlation strong if it is significant at the
p < 0.00001 level, moderate at the p < 0.01 level,
and weak at the p < 0.05 level. The results are sum-
marized in Table 3; we present only the significant
results for space considerations.
For female-female pairs, giving encouragement
is weakly correlated with entrainment on intensity
max and shimmer. Conversation awkward is weakly
correlated with entrainment on jitter. For male-male
pairs, trying to be liked is moderately correlated
with entrainment on intensity mean and weakly cor-
related with entrainment on jitter and NHR. Giv-
ing encouragement is moderately correlated with
entrainment on intensity mean, intensity max, and
NHR. For female-male pairs, trying to be liked
is moderately correlated with entrainment on pitch
mean. Giving encouragement is strongly corre-
lated with entrainment on intensity mean and max
and moderately correlated with entrainment on pitch
mean and shimmer. However, it is negatively cor-
related with entrainment on jitter, although the cor-
relation is weak. Conversation awkward is weakly
correlated with entrainment on jitter.
As we expected, giving encouragement is corre-
lated with entrainment for all three gender groups,
and trying to be liked is correlated with entrainment
for male-male and female-male groups. However,
trying to dominate is not correlated with entrainment
on any feature, and conversation awkward is actu-
ally positively correlated with entrainment on jitter.
Entrainment on jitter is a clear outlier here, with
all of its correlations contrary to our hypotheses. In
addition to being positively correlated with conver-
sation awkward, it is the only feature to be nega-
tively correlated with giving encouragement.
Entrainment is correlated with the most social
variables for female-male pairs; these correlations
are also the strongest. We therefore conclude that
acoustic entrainment is not only most prevalent for
mixed-gender pairs, it is also more important to the
perception of female-male social behavior than it is
for same-gender pairs.
6 Entrainment and objective measures of
dialogue success
We now examine acoustic/prosodic entrainment in
our corpus according to four objective measures of
dialogue success: the mean latency between turns,
the percentage of turns that are interruptions, the
percentage of turns that are overlaps, and the number
of turns in a task.
High latency between turns can be considered a
sign of an unsuccessful conversation, with poor turn-
taking behavior indicating a possible lack of rapport
and difficulty in communication between the part-
ners. A high percentage of interruptions, another ex-
ample of poor turn-taking behavior, may be a symp-
tom of or a reason for hostility or awkwardness be-
15
Social Acoustic df r p
Female-Female
Giving Int. max -0.24 0.03
enc. Shimmer -0.24 0.03
Conv. Jitter -0.23 0.03
awkward
Male-Male
Trying to Int. mean -0.30 0.006
be liked Jitter -0.27 0.01
NHR -0.23 0.03
Giving Int. mean -0.39 0.0003
enc. Int. max -0.31 0.005
NHR -0.30 0.005
Female-Male
Trying to Pitch mean -0.26 0.001
be liked
Giving Int. mean -0.36 2.8e-06
enc. Int. max -0.31 7.7e-05
Pitch mean -0.23 0.003
Jitter 0.19 0.02
Shimmer -0.16 0.04
Conv. Jitter -0.17 0.04
awkward
Table 3: Correlations between entrainment and social
variables.
tween partners. We expect these measures to be neg-
atively correlated with entrainment. Conversely, a
high percentage of overlaps may be a symptom of
a well-coordinated conversation that is flowing eas-
ily. In the guidelines for the turn-taking annotation
of the Games Corpus (Gravano, 2009), overlaps are
defined as cases in which Speaker 2 takes the floor,
overlapping with the completion of Speaker 1?s ut-
terance. Overlaps require the successful reading of
turn-taking cues and by definition preclude awkward
pauses. We expect a high percentage of overlaps to
correlate positively with entrainment.
The number of turns in a task can be interpreted
either positively or negatively. A high number is
negative in that it is the sign of an inefficient dia-
logue, one which takes many turn exchanges to ac-
complish the objective. However, it may also be
the sign of easy, flowing dialogue between the part-
ners. In our domain, it may also be a sign of a high-
achieving pair who are placing the object meticu-
Objective Acoustic df r p
Female-Female
Latency Int. mean 0.22 0.04
Int. max 0.31 0.005
Pitch mean 0.24 0.02
Jitter 0.29 0.007
Shimmer 0.33 0.002
Syllables/sec 0.39 0.0002
# Turns Int. max -0.30 0.006
Shimmer -0.34 0.002
NHR -0.24 0.03
Syllables/sec -0.28 0.01
% Overlaps Int. max -0.23 0.04
Shimmer -0.30 0.005
% Interruptions Shimmer -0.33 0.005
Male-Male
Latency Int. mean 0.57 8.8e-08
Int. max 0.43 0.0001
Pitch mean 0.52 2.4e-06
Pitch max 0.61 5.7e-09
Jitter 0.65 4.5e-10
NHR 0.40 0.0004
# Turns Int. mean -0.29 0.0002
Pitch mean -0.32 0.003
Pitch max -0.29 0.007
NHR -0.47 7.9e-06
Syllables/sec -0.25 0.02
% Overlaps Int. mean -0.39 0.0002
Int. max -0.39 0.0002
% Interruptions NHR -0.33 0.002
Female-Male
# Turns Int. mean -0.24 0.003
Int. max -0.19 0.02
Shimmer -0.16 0.04
% Overlaps Shimmer -0.26 0.001
Table 4: Correlations between entrainment and objective
variables.
lously in order to secure every single point. We
therefore expect the number of turns to be positively
correlated with entrainment. As before, we con-
sider a correlation strong if it is significant at the
p < 0.00001 level, moderate at the p < 0.01 level,
and weak at the p < 0.05 level. The significant cor-
relations are presented in Table 4.
For female-female pairs, mean latency between
16
turns is negatively correlated with entrainment on all
variables except pitch max and NHR. The correla-
tions are weak for intensity mean and pitch mean
and moderate for intensity max, jitter, shimmer, and
syllables per second. The number of turns is moder-
ately correlated with entrainment on intensity max
and shimmer and weakly correlated with entrain-
ment on syllables per second. Contrary to our expec-
tations, the percentage of interruptions is positively
(though moderately) correlated with entrainment on
shimmer; the percentage of overlaps is moderately
correlated with entrainment on shimmer and weakly
correlated with entrainment on intensity max.
Male-male pairs show the most correlations be-
tween entrainment and objective measures of dia-
logue success. The latency between turns is neg-
atively correlated with entrainment on all variables
except shimmer and syllables per second; the corre-
lations are moderate for intensity max and NHR and
strong for the rest. The number of turns in a task
is positively correlated with entrainment on every
variable except intensity mean, jitter and shimmer:
strongly for NHR; moderately for intensity mean,
pitch mean, and pitch max; and weakly for syllables
per second.. The percentage of overlaps is moder-
ately correlated with entrainment on intensity mean
and max. The percentage of interruptions is moder-
ately correlated with entrainment on NHR.
For female-male pairs, the number of turns is
moderately correlated with entrainment on intensity
mean and weakly correlated with entrainment on in-
tensity max and shimmer. The percentage of over-
laps is moderately correlated with entrainment on
shimmer.
For the most part, the directions of the correla-
tions we have found are in accordance with our hy-
potheses. Latency is negatively correlated with en-
trainment and overlaps and the number of turns are
positively correlated. A puzzling exception is the
percentage of interruptions, which is positively cor-
related with entrainment on shimmer (for female-
female pairs) and NHR (for male-male pairs).
While the strongest correlations were for mixed-
gender pairs for the social variables, we find that
the strongest correlations for objective variables are
for male-male pairs, which also have the great-
est number of correlations. It therefore seems that
while entrainment is more important to the percep-
tion of social behavior for mixed-gender pairs than
it is for same-gender pairs, it is more important to
the smoothness and flow of dialogue for male-male
pairs than it is for female-female or female-male
pairs.
7 Entrainment in outliers
Since acoustic entrainment is generally considered
an unconscious phenomenon, it is interesting to con-
sider tasks in which a particular feature of a person?s
speech is particularly salient. This will occur when a
feature differs significantly from the norm ? for ex-
ample, when a person?s voice is unusually loud or
soft. Chartrand and Bargh (1999) suggest that the
psychological mechanism behind the entrainment is
the perception-behavior link, the finding that the act
of observing another?s behavior increases the like-
lihood of the observer?s engaging in that behavior.
Based on this finding, we hypothesize that a part-
ner pair containing one ?outlier? speaker will exhibit
more entrainment on the salient feature, since that
feature is more likely to be observed and therefore
imitated.
We consider values in the 10th or 90th percentile
for a feature ?outliers.? We can consider ENTx, the
similarity between a speaker and the speakers of her
partner?s role and gender with whom she is never
paired, the ?baseline? value for the similarity be-
tween a speaker and her interlocutor when no en-
trainment occurs. ENTp ? ENTx, the difference be-
tween the similarity existing between partners and
the baseline similarity, is then a measure of how
much entrainment exists relative to baseline.
We compare ENTp ? ENTx for ?normal? versus
?outlier? speakers. ENTp should be smaller for out-
lier speakers, since their interlocutors are not likely
to be similarly unusual. However, ENTx should also
be lower for outlier speakers, since by definition they
diverge from the norm, while the normal speakers
by definition represent the norm. It is therefore rea-
sonable to expect ENTp ? ENTx to be the same for
outlier speakers and normal speakers.
If ENTp ? ENTx is higher for outlier speakers,
that means that ENTp is higher than we expect, and
entrainment is greater relative to baseline for pairs
containing an outlier speaker. If ENTp ? ENTx is
lower for outlier speakers, that means that ENTp is
17
Acoustic t df p
Intensity mean 5.66 94.26 1.7e-07
Intensity max 8.29 152.05 5.5e-14
Pitch mean -1.20 76.82 N.S.
Pitch max -0.84 76.76 N.S.
Jitter 0.36 70.23 N.S.
Shimmer 2.64 102.23 0.02
NHR -0.92 137.34 N.S.
Syllables per sec 2.41 72.60 0.02
Table 5: T-tests for relative entrainment for outlier vs.
normal speakers.
lower than we expect, and pairs containing an outlier
speaker entrain less than do pairs of normal speak-
ers, even allowing for the fact that their usual values
should be further apart to begin with.
The results for t-tests comparing ENTp ? ENTx
for ?normal? versus ?outlier? speakers are shown
in Table 5. Outlier pairs have higher relative en-
trainment than do normal pairs for intensity mean
and max, shimmer, and syllables per second. This
means that speakers confronted with an interlocutor
who diverges widely from the norm for those four
features make a larger adjustment to their speech in
order to converge to that interlocutor.
An ANOVA shows that relative entrainment on
intensity max is higher in outlier cases for male-
male pairs than for female-female pairs and even
higher for female-male pairs (F=11.33, p=5.3e-05).
Relative entrainment on NHR in these cases is lower
for male-male pairs than for female-female pairs
and higher for female-male pairs (F=11.41, p=6.5e-
05). Relative entrainment on syllables per second
is lower for male-male pairs and higher for female-
male pairs (F=5.73, p=0.005). These results differ
slightly from the results in Table 2 for differences
in entrainment in the general case among gender
pairs, reinforcing the idea that cases in which fea-
ture values diverge widely from the norm are unique
in terms of entrainment behavior.
8 Conclusion
Our study of entrainment on acoustic/prosodic vari-
ables yields new findings about entrainment be-
havior for female-female, male-male, and mixed-
gender dyads, as well as the association of entrain-
ment with perceived social characteristics and ob-
jective measures of dialogue smoothness and effi-
ciency. We find that entrainment is the most preva-
lent for mixed-gender pairs, followed by female-
female pairs, with male-male pairs entraining the
least. Entrainment is the most important to the per-
ception of social behavior of mixed-gender pairs,
and it is the most important to the efficiency and flow
of male-male dialogues.
For the most part, the directions of the correla-
tions of entrainment with success variables accord
with hypotheses motivated by the relevant literature.
Giving encouragement and trying to be liked are
positively correlated with entrainment, as are per-
centage of overlaps and number of turns. Mean la-
tency, a symptom of a poorly-run conversation, is
negatively associated with entrainment. However,
several exceptions suggest that the associations are
not straightforward and further research must be
done to fully understand the relationship between
entrainment, social characteristics and dialogue suc-
cess. In particular, the explanation behind the as-
sociations of entrainment on certain variables with
certain social and objective measures is an interest-
ing direction for future work.
Finally, we find that in ?outlier? cases where a
particular speaker diverges widely from the norm for
intensity mean, intensity max, or syllables per sec-
ond, entrainment is more pronounced. This supports
the theory that the perception-behavior link is the
mechanism behind entrainment and provides a pos-
sible direction for research into why speakers entrain
on certain features and not others. In future work we
will explore this direction and go more thoroughly
into individual differences in entrainment behavior.
Acknowledgments
This material is based upon work supported in
part by NSF IIS-0307905, NSF IIS-0803148,
UBACYT 20020090300087, ANPCYT PICT-2009-
0026, CONICET, VEGA No. 2/0202/11; and the
EUSF (ITMS 26240220060).
References
Amazon Mechanical Turk, http://www.mturk.com.
Frances R. Bilous and Robert M. Krauss 1988. Dom-
inance and accommodation in the conversational be-
18
haviours of same- and mixed-gender dyads. Language
and Communication, 8(3/4):183?194.
Susan E. Brennan and Herbert H. Clark. 1996. Concep-
tual Pacts and Lexical Choice in Conversation. Jour-
nal of Experimental Psychology: Learning, Memory
and Cognition, 22(6):1482?1493.
Tanya L. Chartrand and John A. Bargh. 1999. The
Chameleon Effect: The Perception-Behavior Link and
Social Interaction. Journal of Personality and Social
Psychology, 76(6):893?910.
Cristian Danescu-Niculescu-Mizil, Michael Gamon, and
Susan Dumais. 2011. Mark My Words! Linguistic
Style Accommodation in Social Media. Proceedings
of WWW 2011.
H. Giles and N. Coupland. 1991. Language: Contexts
and Consequences. Pacific Grove, CA: Brooks/Cole.
Agust??n Gravano. 2009. Turn-Taking and Affirmative
Cue Words in Task-Oriented Dialogue. Ph.D. thesis,
Columbia University, New York.
Agust??n Gravano and Julia Hirschberg. 2011. Turn-
taking cues in task-oriented dialogue. Computer
Speech and Language, 25(3):601?634.
Agust??n Gravano, Rivka Levitan, Laura Willson, S?tefan
Ben?us?, Julia Hirschberg, Ani Nenkova. 2011. Acous-
tic and Prosodic Correlates of Social Behavior. Inter-
speech 2011.
Chi-Chun Lee, Matthew Black, Athanasios Katsama-
nis, Adam Lammert, Brian Baucom, Andrew Chris-
tensen, Panayiotis G. Georgiou, Shrikanth Narayanan.
2010. Quantification of Prosodic Entrainment in Af-
fective Spontaneous Spoken Interactions of Married
Couples. Eleventh Annual Conference of the Interna-
tional Speech Communication Association.
Rivka Levitan, Agust??n Gravano, and Julia Hirschberg.
2011. Entrainment in Speech Preceding Backchan-
nels. Proceedings of ACL/HLT 2011.
Rivka Levitan and Julia Hirschberg. 2011. Measuring
acoustic-prosodic entrainment with respect to multi-
ple levels and dimensions. Proceedings of Interspeech
2011.
Laura L. Namy, Lynne C. Nygaard, Denise Sauerteig.
2002. Gender differences in vocal accommodation:
the role of perception. Journal of Language and So-
cial Psychology, 21(4):422?432.
Michael Natale. 1975. Convergence of Mean Vocal In-
tensity in Dyadic Communication as a Function of So-
cial Desirability. Journal of Personality and Social
Psychology, 32(5):790?804.
Ani Nenkova, Agust??n Gravano, and Julia Hirschberg.
2008. High-frequency word entrainment in spoken di-
alogue. Proceedings of ACL/HLT 2008.
Kate G. Niederhoffer and James W. Pennebaker. 2002.
Linguistic style matching in social interaction. Jour-
nal of Language and Social Psychology, 21(4):337?
360.
Jennifer S. Pardo. 2006. On phonetic convergence dur-
ing conversational interaction. Journal of the Acousti-
cal Society of America, 119(4):2382?2393.
David Reitter, Johanna D. Moore, and Frank Keller.
1996. Priming of Syntactic Rules in Task-Oriented Di-
alogue and Spontaneous Conversation. Proceedings of
the 28th Annual Conference of the Cognitive Science
Society.
Kim Silverman, Mary Beckman, John Pitrelli, Mori Os-
tendorf, Colin Wightman, Patti Price, Janet Pierrehum-
bert, Julia Hirschberg. 1992. TOBI: A Standard for
Labeling English Prosody. ICSLP-1992, 867-870.
Linda Tickle-Degnen and Robert Rosenthal. 1990. The
Nature of Rapport and its Nonverbal Correlates. Psy-
chological Inquiry, 1(4):285?293.
Richard West & Lynn Turner. 2009. Introducing
Communication Theory: Analysis and Application.
McGraw-Hill Humanities/Social Sciences/Languages,
4th edition.
19
Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 84?90,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Entrainment in Spoken Dialogue Systems: Adopting, Predicting and
Influencing User Behavior
Rivka Levitan
Department of Computer Science
Columbia University
New York, NY 10027, USA
rlevitan@cs.columbia.edu
Abstract
Entrainment is the phenomenon of the speech
of conversational partners becoming more
similar to each other. This thesis proposal
presents a comprehensive look at entrainment
in human conversations and how entrainment
may be incorporated into the design of spo-
ken dialogue systems in order to improve sys-
tem performance and user satisfaction. We
compare different kinds of entrainment in both
classic and novel dimensions, provide exper-
imental results on the utility of entrainment,
and show that entrainment can be used to im-
prove a system?s ASR performance and turn-
taking decisions.
1 Introduction
Entrainment is the phenomenon of interlocutors be-
coming more similar to each other in their speech
in the course of a conversation. Entrainment has
been observed in numerous domains and for mul-
tiple levels of communication. In addition, many
studies have shown associations between entrain-
ment and desirable dialogue characteristics. The
proposed work aims to improve spoken dialogue
system performance both qualitatively and quanti-
tatively by exploiting this prevalent and significant
phenomenon. Spoken dialogue systems can signifi-
cantly improve the quality of their user interactions
by incorporating entrainment into their design:
? A spoken dialogue system can entrain to its
users, adjusting its own output to align with
theirs. This should improve the dialogue qual-
ity as perceived by the user.
? It can exploit the concept of entrainment by
changing the parameters of its own output
when it wants the user to speak differently. For
example, when the user is speaking too quickly,
the system can slow down its own output, caus-
ing the user to do the same.
? It can use an entrainment model along with in-
formation about its own behavior to more accu-
rately predict how the user will behave.
Our proposed work explores the role of entrain-
ment in human conversations and looks at how it
can improve interactions with spoken dialogue sys-
tems. In addition to presenting an in-depth study
of the characteristics of human entrainment, we
will demonstrate that spoken dialogue systems can
use this information to predict characteristics of the
user?s speech, improve the user?s impression of the
dialogue quality and system persona by adopting the
user?s speech characteristics, and improve recogni-
tion accuracy by influencing the user to abandon
prosodic characteristics associated with ASR error.
This thesis proposal is organized as follows: Sec-
tion 2 discusses the literature related to the proposed
work. Section 3 describes the corpus used in these
studies. Section 4 addresses the question of how hu-
mans entrain and how this information can be used
to more accurately predict a user?s behavior. Sec-
tion 5 discusses how entrainment affects the per-
ceived quality of human and human-computer con-
versations, and Section 6 explores how entrainment
can be used to influence user behavior. Section 7
describes the main contributions of this work.
84
2 Related work
Entrainment has been shown to occur at almost ev-
ery level of human communication: lexical (Bren-
nan and Clark, 1992), syntactic (Reitter and Moore,
2007; Ward and Litman, 2007), stylistic (Niederhof-
fer and Pennebaker, 2002; Danescu-Niculescu-Mizil
et al, 2011), acoustic-prosodic (Natale, 1975; Coul-
ston et al, 2002; Ward and Litman, 2007) and pho-
netic (Pardo, 2006).
Entrainment in many of these dimensions has
also been associated with measures of dialogue suc-
cess. Chartrand and Bargh (1999), for example,
demonstrated that subjects who interacted with con-
federates who mimicked their posture and behav-
ior reported greater liking for the confederate and a
smoother interaction. Lee et al (2010) found that
entrainment measures derived from pitch features
were significantly higher in positive interactions be-
tween married couples in therapy than in negative
interactions. Looking at more objective measures,
Nenkova et al (2008) found that the degree of en-
trainment on high-frequency words was correlated
with task score and turn-taking features.
These studies have been motivated by theoreti-
cal models such as Giles? Communication Accom-
modation Theory (Giles et al, 1987), which pro-
poses that speakers promote social approval or ef-
ficient communication by adapting to their inter-
locutors? communicative behavior. Another theory
informing the association of entrainment and dia-
logue success is the coordination-rapport hypoth-
esis (Tickle-Degnen and Rosenthal, 1990), which
posits that the degree of liking between conversa-
tional partners should be correlated with the degree
of nonverbal coordination between them. In con-
trast, Chartrand and Bargh (1999) posit that entrain-
ment is a purely automatic process, a product of the
perception-behavior link, which predicts that the act
of observing a behavior makes the observer more
likely to engage in that behavior as well.
3 Columbia Games Corpus
Many of the studies in this work were conducted on
the Columbia Games Corpus (Gravano, 2009), a col-
lection of twelve dyadic conversations elicited from
native speakers of Standard American English. Dur-
ing the collection of the corpus, each pair of partic-
ipants played a set of computer games that required
them to verbally cooperate to achieve a mutual goal.
In the Cards games, one speaker described the cards
she saw on her screen, and her partner attempted to
match them to the cards on his own screen. In the
Objects games, one speaker described the location
of an object on her screen, and her partner attempted
to place the corresponding object in exactly the same
location on his own screen. For both games, the par-
ticipants received points based on how exact a match
was; they later were paid for each point.
The corpus consists of approximately nine hours
of recorded dialogue. It has been orthographically
transcribed and annotated with prosodic and turn-
taking labels. Thirteen subjects participated in the
collection of the corpus, and nine returned on an-
other day for a second session with a different part-
ner. This is useful for our study of entrainment, since
we can compare a single speaker?s behavior with
two different interlocutors. In addition, the corpus
is representative of the kind of speech we are inter-
ested in: task-oriented dialogue between strangers.
4 Entrainment in human conversations
We begin our study of entrainment by looking at en-
trainment in human conversations. Aside from the
interest inherent in advancing our understanding of
this human behavior, research in this area can inform
the design of spoken dialogue systems. A system
that entrains the way a human does will seem more
natural, and a system that knows how humans en-
train can use this information to better predict how a
user will behave, improving its own performance.
4.1 Acoustic-prosodic entrainment
This study, previously presented in (Levitan and
Hirschberg, 2011), creates a cohesive view of en-
trainment by directly comparing entrainment on a
set of acoustic-prosodic features, measured in five
different ways. By comparing these different mea-
sures of entrainment, we bring clarity to three as-
pects of entrainment:
? Is it global or local? Two speakers may fluc-
tuate around similar means, while diverging
widely at any specific point. Conversely, they
may be globally dissimilar, but locally they
may be relatively similar.
? Is it by value or by direction? If a speaker en-
85
trains to her partner?s actual value, if he low-
ers his voice, she may raise her own in order to
match his new intensity. If she matches the di-
rection of the change rather than the new value,
she will lower her voice as well, even if this
results in a value less similar to his.
? Is the degree of entrainment static, or does
it improve? Do speakers converge?become
more similar?as the conversation progresses?
The features we examine are intensity mean and
max, pitch mean and max, jitter, shimmer, noise-
to-harmonics ratio (NHR), and syllables per sec-
ond1. We look for evidence of global entrainment by
comparing the similarities in feature means between
partners with the similarities between speakers who
are not conversational partners.
We see an effect of entrainment for almost all the
features. In addition, the difference between part-
ners for several of the features is smaller in the sec-
ond half of the conversation, constituting evidence
of convergence. We also find a strong effect of lo-
cal entrainment: for every feature, adjacent turns are
significantly (p < 0.001) more similar to each other
than non-adjacent turns. We conclude that entrain-
ment is by value rather than by direction; that global
entrainment exists in addition to local matching for
several features, most notably intensity; and that en-
trainment is dynamic for some features, improving
as the conversation progresses.
4.2 Entrainment on outliers
Since entrainment is generally considered an uncon-
scious phenomenon, it is interesting to consider en-
trainment when a feature is particularly salient. The
theory that the perception-behavior link is the mech-
anism behind entrainment (Chartrand and Bargh,
1999) would predict that the effect of entrainment
would be stronger in this case, since such features
are more likely to be observed and therefore imi-
tated. We test this hypothesis by looking at cases
in which one speaker in a pair has a feature value in
the 90th or 10th percentile. This study was previ-
ously described in (Levitan et al, 2012).
1Intensity mean is an acoustic measure perceived as loud-
ness, and intensity max represents the range of loudness. Jitter,
shimmer and NHR are three measures of voice quality; jitter
and shimmer are perceived as harshness, and NHR as hoarse-
ness. Syllables per second measure speaking rate.
As in our tests for global entrainment (Section
4.1), we compute a partner and non-partner similar-
ity for each speaker. The partner similarity should be
lower for outlier pairs (pairs in which one speaker
has an outlier feature value), and the non-partner
similarity should be lower as well, since the outlier
speaker diverges from the norm. We therefore can
expect the difference between these two values to be
the same for outlier and typical pairs. If this dif-
ference is lower for outlier pairs, we can conclude
that the effect of entrainment is weaker in outlier
cases. We find, in fact, that this difference is greater
for outlier pairs for several features, indicating that
speakers entrain more to outlier values of these fea-
tures. This finding supports the perception-behavior
link. In addition, it has implications for cases in
which it is an objective to induce one?s interlocutor
to entrain, as we will discuss in Section 6.
4.3 Entrainment and backchannel-inviting cues
Backchannels are short, nondisruptive segments of
speech that a speaker utters to let his interlocutor
know that he is keeping up. They are extremely
prevalent in task-oriented conversation. Gravano
and Hirschberg (2009) identified six acoustic and
prosodic features that tend to be different be-
fore backchannels, hypothesizing that these features
serve as cues to one?s interlocutor that a backchan-
nel would be welcome. Individual speakers use dif-
ferent sets of cues, and can differ in their realiza-
tion of a cue. We look for evidence of entrainment
on backchannel-inviting cues. This work, previously
discussed in (Levitan et al, 2011), represents a first
look at entrainment in a pragmatic dimension.
We measure backchannel-inviting cues in three
ways. Firstly, we measure the similarity of the
speaker pairs? cue sets by counting the number of
cues they have in common, and find that partners
have more cues in common than non-partners. Sec-
ondly, we measure the similarity of cue realization,
and show that feature values before backchannels
for pitch, intensity and voice quality are more sim-
ilar between partners. In addition, this measure
shows evidence of convergence for pitch and inten-
sity, which are more similar before backchannels in
the second half of a conversation. Finally, we mea-
sure the local effect of this entrainment by correlat-
ing feature values before consecutive backchannels
86
and find that pitch and intensity before backchannels
are moderately correlated.
4.4 Future work
We have shown that a speaker?s conversational be-
havior is influenced by that of her interlocutor. We
therefore propose to develop a framework for us-
ing entrainment information to label or predict a
speaker?s behavior. An example of such a task is
predicting backchannels. Based on the work of
Gravano and Hirschberg (2009), a system deciding
whether to produce a backchannel or take the floor
should compare the user?s most recent utterance to
a backchannel-preceding model and a turn-yielding
model. Since each speaker uses a different count
of backchannel-preceding cues, a model trained on
other speakers may not be useful. However, data
from the user may not be available and is likely to
be sparse at best.
Since interlocutors use similar backchannel-
inviting cues, we can use information from the in-
terlouctor ? the system ? to build the model. The
influence of this interlocutor information can be
weighted according to the probable strength of the
entrainment effect, which can depend, as we have
shown, on the feature being predicted, the respec-
tive genders of the participants, whether a feature
value is an outlier, and where in the conversation the
speech segment occurs.
5 Entrainment and dialogue quality
This section addresses two main research questions:
1. What kinds of entrainment are most important
to conversational quality?
2. Will the passive benefits of entrainment apply
when it is a computer that is entraining?
To answer the first question, we look at the entrain-
ment correlates of social and objective variables in
the Games Corpus (previously reported in Levitan
et al, 2012). We address the second question with a
Wizard of Oz study that looks at subjects? reactions
to an entraining spoken dialogue system.
5.1 Entrainment correlates of dialogue charac-
teristics
Lexical entrainment has been associated with mea-
sures of smooth turn-taking and task success
(Nenkova et al, 2008). Here, we correlate en-
trainment on intensity mean and max, pitch mean
and max, jitter, shimmer, noise-to-harmonics ratio
(NHR), and syllables per second with four objective
measures of dialogue coordination: number of turns,
mean turn latency, percentage of overlaps, and per-
centage of interruptions. We interpret a high num-
ber of turns and percentage of overlaps (cases in
which one person begins speaking as her interlocu-
tor finishes his turn) as signs of a smoothly flowing,
well-coordinated conversation. We therefore expect
them to be positively associated with entrainment, in
line with previous work and the theory that entrain-
ment facilitates communication. In contrast, high
turn latency (the lag time between turns) and per-
centage of interruptions (cases in which one person
begins speaking before her interlocutor has finished
his turn) are signs of poor turn-taking behavior and
an awkward conversation. We therefore expect them
to be negatively correlated with entrainment mea-
sures.
To look at more perceptual measures of dialogue
quality, we used Amazon Mechanical Turk2 to an-
notate each task (the sub-units of each game) in the
Games Corpus for what we term social variables,
the perceived social characteristics of an interaction
and ints participants. Details on the annotation pro-
cess can be found in (Gravano et al, 2011). In this
study, we focus on four social variables: trying to
be liked, giving encouragement, trying to dominate,
and conversation awkward. Based on Communica-
tion Accommodation Theory (Giles et al, 1987), we
expect the first two social variables, which represent
the desire to minimize social distance, to be posi-
tively correlated with entrainment. Someone who is
trying to dominate, on the other hand, will try to in-
crease social distance, and we therefore expect this
variable to correlate negatively with entrainment, as
should conversation awkward.
We report separate results for female, male and
mixed-gender pairs. In general, we see correlations
in the expected directions: the number of turns, per-
centage of overlaps, and giving encouragement are
positively correlated with entrainment for all gen-
der groups, latency is negatively correlated with en-
trainment for male and female pairs, and trying to
be liked is positively correlated with entrainment for
2http://www.mturk.com
87
male and mixed-gender pairs. We see no correla-
tions for trying to dominate, possibly because an-
notators were confused between the socially weak
position of trying to dominate, and the socially pow-
erful position of actually dominating.
For objective variables, we see the strongest and
most numerous correlations for male pairs, while
for objective variables, this is true for mixed-gender
pairs, leading us to conclude that entrainment is
most important to the coordination of a conversa-
tion for male pairs and to the perceived quality of a
conversation for mixed-gender pairs. We identify in-
tensity as an important entrainment feature, as well
as shimmer for dialogue coordination for female or
mixed-gender pairs. In future work, we plan to cor-
relate these social and objective variables with mea-
sures of local entrainment and convergence (Section
4.1).
5.2 Entrainment and dialogue quality in spoken
dialogue systems
In this study (currently ongoing), we look at whether
subjects will attribute more positive qualities to an
interaction with a system whose voice is more simi-
lar to their own. To answer this question, we create
a Wizard of Oz setup in which a subject interacts
with an entrained voice and a disentrained voice.
We chose to employ a wizard instead of a fully func-
tional dialogue system in order to neutralize possible
intrusions from other components of a dialogue sys-
tem and isolate the entrainment effect.
The subjects are given three tasks modeled on rea-
sons for which someone might call 311, New York
City?s phone number for government information.
In the taxi scenario, for example, the subject is given
a description of an incident in which a taxi drove
unsafely, and is told to report the incident to the sys-
tem, using the given date, time and location. Using
this paradigm, we can collect spontaneous speech
while still being able to use prerecorded prompts:
the content is predetermined, but the sentence form
and word choice is up to the subject.
For the first task, alternate side parking, the ex-
perimenter prints prompts to the subject?s screen us-
ing a chat program, and the subject responds by
speaking into a headset that plays into the experi-
menter?s computer. The purpose of this first task is
to get a sample of the subject?s speech. The sub-
ject then fills out some demographic forms and the
NEO-FFI personality test, while the experimenter
calculates the vocal intensity and speaking rate of
the subject?s speech. A set of prerecorded prompts
is then scaled to match the subject?s vocal parame-
ters, forming an entrained set, and then scaled away
from the subject?s parameters, forming the disen-
trained set. The parameters for the disentrained set
were chosen empirically to result in a voice percepti-
bly different from the entrained set while remaining
undistorted and natural-sounding.
The subject then completes two more tasks, one
with the entrained voice and one with the disen-
trained voice. We vary the order and combination
of tasks and voices so we can test for effects of or-
der and task. After each task, the subject fills out
a survey containing questions like ?I liked the sys-
tem?s personality? or ?I found talking with the sys-
tem annoying.? We hypothesize that they will agree
more with positive statements about the entraining
version of the system.
We also crudely measure each subject?s percep-
tual sensitivity to vocal characteristics by asking
them to describe each voice by choosing from a list
of adjectives like ?high-pitched,? ?fast,? or ?loud.?
We will look at how this sensitivity, as well as gen-
der and personality, interact with the subjects? reac-
tions to the system?s entrainment.
6 Influencing user behavior
In human conversations, it is common for a speaker
to attempt to affect his interlocutor?s behavior by
modeling a desired change. For example, a speaker
may raise his own voice if he is having trouble hear-
ing and wishes his interlocutor to speak more loudly.
Since humans have been shown to entrain to com-
puters (Coulston et al, 2002; Stoyanchev and Stent,
2009; Bell et al, 2003), it is reasonable for a spoken
dialogue system to use this strategy to influence its
user to speak in a way that will optimize the perfor-
mance of its automatic speech recognition (ASR). A
previous study (Lopes et al, 2011) successfully in-
duced users to abandon words prone to ASR error
simply by removing those words from the system?s
prompts. In this work, we attempt to influence users
to abandon prosodic characteristics associated with
ASR failure by modeling the desired change in the
system?s prompts.
88
Hirschberg et al (2004) found that utterances that
followed longer pauses or were louder, longer, or
pitched higher were less likely to be recognized
correctly. Our method looks for these undesirable
prosodic features in utterances with low ASR con-
fidence and attempts to induce the user to abandon
them. We hypothesize that abandoning prosody as-
sociated with ASR failure will result in improved
ASR performance.
Our approach is as follows. When the system?s
ASR returns a hypothesis with low confidence for
an utterance, it finds the utterance?s intensity, pitch
and duration. If any of these features fall within the
range of utterances that tend to be misrecognized,
the system employs one of four strategies. The ex-
plicit strategy is to ask the user to make the desired
change, e.g. ?Please speak more quietly.? The en-
trainment strategy is to model the desired change,
e.g. lowering the intensity of the system?s out-
put. The explicit+entrainment strategy combines
the two, e.g. by saying ?Please speak more quietly?
in a quieter system voice. We hypothesize that one
strategy may increase the efficacy of the other. We
will also try a no strategy condition as a baseline
for how often the user independently abandons the
undesirable prosody.
Each strategy will be embodied in a simple re-
quest for repetition. For each strategy, we will look
at how often the subsequent turn displays the desired
change in prosody. In addition, we will see how
often the ASR performance improves on the subse-
quent turn. A third measure of a strategy?s success
will be the durability of its effect?that is, how likely
the undesirable prosody is to recur later in the con-
versation.
Within the entrainment condition, we will test
how pronounced a change must be in order to in-
duce a corresponding change on the part of the user.
Our research on outlier entrainment suggests that a
more extreme change is more likely to be entrained
to. However, the most attractive feature of the en-
trainment condition is its nondisruptiveness, and this
quality will be lost if the change in the system?s
voice is too extreme. We will therefore begin with
a slight change, and test how much the degree of
change must be increased before the user will imi-
tate it.
Fandrianto and Eskenazi (2012) implemented a
similar approach, lowering the system?s vocal in-
tensity or increasing its speaking rate when its
classifiers detected the speaking styles of shouting
or hyperarticulation. By responding to individual
prosodic features instead of higher-level speaking
styles, we avoid the layer of error introduced by clas-
sifiers. Furthermore, our approach can account for
cases in which ASR error is caused by prosodic fea-
tures that do not comprise an identifiable speaking
style. Finally, our detailed analysis will give more
information about the advantages and limitations of
each strategy.
7 Contributions
The studies of human-human conversations in this
thesis will advance current understanding of how
people entrain. We provide a cohesive picture of en-
trainment by directly comparing different measures
on a single corpus, establishing that entrainment is
both a global and a local phenomenon, that people
entrain by value rather than by direction, and that it
is a dynamic process, improving with the course of
a dialogue. We show that speaker pairs entrain in
a novel dimension, backchannel-inviting cues, and
that this entrainment is associated with task success
and dialogue coordination. We also show that the ef-
fect of entrainment is stronger in outlier cases, lend-
ing experimental support to the perception-behavior
link.
This work provides experimental results on the
utility of entrainment in conversations with both hu-
mans and spoken dialogue systems. In human con-
versations, we show that entrainment is correlated
with positive social characteristics and turn-taking
features. In our Wizard of Oz experiments, we will
show how entrainment affects a user?s perception of
the quality of a spoken dialogue system.
Finally, this work shows how the principles of en-
trainment can be used to actively improve spoken
dialogue systems. We will build a framework for
implementing the results of our studies of entrain-
ment in human conversations into prediction mod-
els, which we hypothesize will improve their accu-
racy and can be used to improve a system?s perfor-
mance. In our influencing experiments, we will at-
tempt to influence a user to speak in a way that will
optimize ASR performance simply by changing the
system?s own voice.
89
References
Linda Bell, Joakim Gustafson, and Mattias Heldner. Prosodic
adaptation in human-computer interaction. In Proceedings
of ICPHS?03, pages 833?836, 2003.
Susan E. Brennan and Herbert H. Clark. Conceptual pacts
and lexical choice in conversation. Journal of Experimen-
tal Psychology: Learning, Memory and Cognition, 22(6):
1482?1493, 1992.
T. L. Chartrand and J. A. Bargh. The chameleon effect: The
perception-behavior link and social interaction. Journal of
Personality and Social Psychology, 76(6):893?910, 1999.
Rachel Coulston, Sharon Oviatt, and Courtney Darves. Am-
plitude convergence in children?s conversational speech with
animated personas. In Proceedings of ICSLP?02, 2002.
Cristian Danescu-Niculescu-Mizil, Michael Gamon, and Susan
Dumais. Mark my words! linguistic style accommodation in
social media. In Proceedings of WWW, 2011.
Andrew Fandrianto and Maxine Eskenazi. Prosodic entrain-
ment in an information-driven dialog system. In Proceedings
of Interspeech, 2012.
H. Giles, A. Mulac, J.J. Bradac, and P. Johnson. Speech accom-
modation theory: the first decade and beyond. Sage, Beverly
Hills, CA, 1987.
Agust??n Gravano. Turn-taking and affirmative cue words in
task-oriented dialogue. PhD thesis, Columbia University,
2009.
Agust??n Gravano and Julia Hirschberg. Backchannel-inviting
cues in task-oriented dialogue. In Proceedings of Inter-
speech, 2009.
Agust??n Gravano, Rivka Levitan, Laura Willson, Stefan Benus,
Julia Hirschberg, and Ani Nenkova. Acoustic and prosodic
correlates of social behavior. In Proceedings of Interspeech,
2011.
Julia Hirschberg, Diane Litman, and Marc Swerts. Prosodic and
other cues to speech recognition failures. Speech Communi-
cation, 43:155?175, 2004.
Chi-Chun Lee, Matthew Black, Athanasios Katsamanis, Adam
Lammert, Brian Baucom, Andrew Christensen, Panayio-
tis G. Georgiou, and Shrikanth Narayanan. Quantification
of prosodic entrainment in affective spontaneous spoken in-
teractions of married couples. In Proceedings of Interspeech,
2010.
Rivka Levitan and Julia Hirschberg. Measuring acoustic-
prosodic entrainment with respect to multiple levels and di-
mensions. In Proceedings of Interspeech, 2011.
Rivka Levitan, Agust??n Gravano, and Julia Hirschberg. Entrain-
ment in speech preceding backchannels. In Proceedings of
the 49th Annual Meeting of the Association for Computa-
tional Linguistics, 2011.
Rivka Levitan, Agust??n Gravano, Laura Willson, Stefan Benus,
Julia Hirschberg, and Ani Nenkova. Acoustic-prosodic
entrainment and social behavior. In Proceedings of the
2012 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Lan-
guage Technologies, pages 11?19, Montre?al, Canada, June
2012. Association for Computational Linguistics. URL
http://www.aclweb.org/anthology/N12-1002.
Jose? Lopes, Maxine Eskenazi, and Isabel Trancoso. To-
wards choosing better primes for spoken dialog systems. In
ASRU?11, pages 306?311, 2011.
Michael Natale. Convergence of mean vocal intensity in dyadic
communication as a function of social desirability. Journal
of Personality and Social Psychology, 32(5):790?804, 1975.
Ani Nenkova, Agust??n Gravano, and Julia Hirschberg. High
frequency word entrainment in spoken dialogue. In Proceed-
ings of ACL/HLT, 2008.
Kate G. Niederhoffer and James W. Pennebaker. Linguistic
style matching in social interaction. Journal of Language
and Social Psychology, 21(4):337?360, 2002.
Jennifer S. Pardo. On phonetic convergence during conversa-
tional interaction. Journal of the Acoustic Society of Amer-
ica, 19(4), 2006.
David Reitter and Johanna D. Moore. Predicting success in di-
alogue. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 808?815,
2007.
Svetlana Stoyanchev and Amanda Stent. Lexical and syntactic
priming and their impact in deployed spoken dialog systems.
In Proceedings of NAACL HLT, 2009.
Linda Tickle-Degnen and Robert Rosenthal. The nature of rap-
port and its nonverbal correlates. Psychological Inquiry, 1
(4):285?293, 1990.
Arthur Ward and Diane Litman. Measuring convergence and
priming in tutorial dialog. Technical report, University of
Pittsburgh, 2007.
90
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 113?117,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Entrainment in Speech Preceding Backchannels
Rivka Levitan
Dept. of Computer Science
Columbia University
New York, NY 10027, USA
rlevitan@cs.columbia.edu
Agust??n Gravano
DC-FCEyN & LIS
Universidad de Buenos Aires
Buenos Aires, Argentina
gravano@dc.uba.ar
Julia Hirschberg
Dept. of Computer Science
Columbia University
New York, NY 10027, USA
julia@cs.columbia.edu
Abstract
In conversation, when speech is followed by
a backchannel, evidence of continued engage-
ment by one?s dialogue partner, that speech
displays a combination of cues that appear to
signal to one?s interlocutor that a backchan-
nel is appropriate. We term these cues back-
channel-preceding cues (BPC)s, and examine
the Columbia Games Corpus for evidence of
entrainment on such cues. Entrainment, the
phenomenon of dialogue partners becoming
more similar to each other, is widely believed
to be crucial to conversation quality and suc-
cess. Our results show that speaking partners
entrain on BPCs; that is, they tend to use simi-
lar sets of BPCs; this similarity increases over
the course of a dialogue; and this similarity is
associated with measures of dialogue coordi-
nation and task success.
1 Introduction
In conversation, dialogue partners often become
more similar to each other. This phenomenon,
known in the literature as entrainment, alignment,
accommodation, or adaptation has been found to
occur along many acoustic, prosodic, syntactic and
lexical dimensions in both human-human interac-
tions (Brennan and Clark, 1996; Coulston et al,
2002; Reitter et al, 2006; Ward and Litman,
2007; Niederhoffer and Pennebaker, 2002; Ward and
Mamidipally, 2008; Buder et al, 2010) and human-
computer interactions (Brennan, 1996; Bell et al,
2000; Stoyanchev and Stent, 2009; Bell et al, 2003)
and has been associated with dialogue success and
naturalness (Pickering and Garrod, 2004; Goleman,
2006; Nenkova et al, 2008). That is, interlocutors
who entrain achieve better communication. How-
ever, the question of how best to measure this phe-
nomenon has not been well established. Most re-
search has examined similarity of behavior over a
conversation, or has compared similarity in early
and later phases of a conversation; more recent work
has proposed new metrics of synchrony and conver-
gence (Edlund et al, 2009) and measures of similar-
ity at a more local level (Heldner et al, 2010).
While a number of dimensions of potential en-
trainment have been studied in the literature, en-
trainment in turn-taking behaviors has received lit-
tle attention. In this paper we examine entrainment
in a novel turn-taking dimension: backchannel-
preceding cues (BPC)s.1 Backchannels are short
segments of speech uttered to signal continued in-
terest and understanding without taking the floor
(Schegloff, 1982). In a study of the Columbia
Games Corpus, Gravano and Hirschberg (2009;
2011) identify five speech phenomena that are
significantly correlated with speech followed by
backchannels. However, they also note that indi-
vidual speakers produced different combinations of
these cues and varied the way cues were expressed.
In our work, we look for evidence that speaker pairs
negotiate the choice of such cues and their realiza-
tions in a conversation ? that is, they entrain to one
another in their choice and production of such cues.
We test for evidence both at the global and at the
local level.
1Prior studies termed cues that precede backchannels, back-
channel-inviting cues. To avoid suggesting that such cues are a
speaker?s conscious decision, we adopt a more neutral term.
113
In Section 2, we describe the Columbia Games
Corpus, on which the current analysis was con-
ducted. In Section 3, we present three measures of
BPC entrainment. In Section 4, we further show that
two of these measures also correlate with dialogue
coordination and task success.
2 The Columbia Games Corpus
The Columbia Games Corpus is a collection of 12
spontaneous dyadic conversations elicited from na-
tive speakers of Standard American English. 13 peo-
ple participated in the collection of the corpus. 11
participated in two sessions, each time with a dif-
ferent partner. Subjects were separated by a curtain
to ensure that all communication was verbal. They
played a series of computer games requiring collab-
oration in order to achieve a high score.
The corpus consists of 9h 8m of speech. It is
orthographically transcribed and annotated for var-
ious types of turn-taking behavior, including smooth
switches (cases in which one speaker completes her
turn and another speaker takes the floor), interrup-
tions (cases in which one speaker breaks in, leaving
the interlocutor?s turn incomplete), and backchan-
nels. There are 5641 exchanges in the corpus; of
these, approximately 58% are smooth switches, 2%
are interruptions, and 11% are backchannels. Other
turn types include overlaps and pause interruptions;
a full description of the Columbia Games Corpus?
annotation for turn-taking behavior can be found in
(Gravano and Hirschberg, 2011).
3 Evidence of entrainment
Gravano and Hirschberg (2009; 2011) identify five
cues that tend to be present in speech preceding
backchannels. These cues, and the features that
model them, are listed in Table 1. The likelihood
that a segment of speech will be followed by a
backchannel increases quadratically with the num-
ber of cues present in the speech. However, they
note that individual speakers may display different
combinations of cues. Furthermore, the realization
of a cue may differ from speaker to speaker. We hy-
pothesize that speaker pairs adopt a common set of
cues to which each will respond with a backchan-
nel. We look for evidence for this hypothesis us-
ing three different measures of entrainment. Two of
Cue Feature
Intonation pitch slope over the IPU-
final 200 and 300 ms
Pitch mean pitch over the final
500 and 1000 ms
Intensity mean intensity over the
final 500 and 1000 ms
Duration IPU duration in seconds
and word count
Voice quality NHR over the final 500
and 1000 ms
Table 1: Features modeling each of the five cues.
these measures capture entrainment globally, over
the course of an entire dialogue, while the third
looks at entrainment on a local level. The unit of
analysis we employ for each experiment is an inter-
pausal unit (IPU), defined as a pause-free segment
of speech from a single speaker, where pause is de-
fined as a silence of 50ms or more from the same
speaker. We term consecutive pairs of IPUs from
a single speaker holds, and contrast hold-preceding
IPUs with backchannel-preceding IPUs to isolate
cues that are significant in preceding backchannels.
That is, when a speaker pauses without giving up
the turn, which IPUs are followed by backchannels
and which are not? We consider a speaker to use
a certain BPC if, for any of the features model-
ing that cue, the difference between backchannel-
preceding IPUs and hold-preceding IPUs is signif-
icant (ANOVA, p < 0.05).
3.1 Entrainment measure 1: Common cues
For our first entrainment metric, we measure the
similarity of two speakers? cue sets by simply count-
ing the number of cues that they have in common
over the entire conversation. We hypothesize that
speaker pairs will use similar sets of cues.
The speakers in our corpus each displayed 0 to 5
of the BPCs described in Table 1 (mean = 2.17). The
number of cues speaker pairs had in common ranged
from 0 to 4 (out of a maximum of 5). Let S1 and S2
be two speakers in a given dialogue, and n1,2 the
number of BPCs they had in common. Let alo n1,?
and n?,2 be the mean number of cues S1 and S2 had
in common with all other speakers in the corpus not
partnered with them in any session. For all 12 dia-
114
logues in the corpus, we pair n1,2 both with n1,? and
with n?,2, and run a paired t-test. The results indi-
cate that, on average, the speakers had significantly
more cues in common with their interlocutors than
with other speakers in the corpus (t = 2.1, df = 23,
p < 0.05).
These findings support our hypothesis that speak-
er pairs negotiate common sets of cues, and suggest
that, like other aspects of conversation, speaker vari-
ation in use of BPCs is not simply an expression of
personal behavior, but is at least partially the result
of coordination with a conversational partner.
3.2 Entrainment measure 2: BPC realization
With our second measure, we look for evidence that
the speakers? actual values for the cue features are
similar: that not only do they alter their production
of similar feature sets when preceding a backchan-
nel, they also alter their productions in similar ways.
We measure how similarly two speakers S1 and
S2 in a conversation realize a BPC as follows:
First, we compute the difference (df1,2) between both
speakers for the mean value of a feature f over
all backchannel-preceding IPUs. Second, we com-
pute the same difference between each of S1 and S2
and the averaged values of all other speakers in the
corpus who are not partnered with that speaker in
any session (df1,? and df?,2). Finally, if for any fea-
ture f modeling a given cue, it holds that df1,2 <
min(df1,?, d
f
?,2), we say that that session exhibits
mutual entrainment on that cue.
Eleven out of 12 sessions exhibit mutual entrain-
ment on pitch and intensity, 9 exhibit mutual entrain-
ment on voice quality, 8 on intonation, and 7 on du-
ration. Interestingly, the only session not entrain-
ing on intensity is the only session not entraining
on pitch, but the relationships between the different
types of entrainment is not readily observable.
For each of the 10 features associated with
backchannel invitation, we compare the differences
between conversational partners (df1,2) and the aver-
aged differences between each speaker and the other
speakers in the corpus (df1,? and df?,2). Paired t-tests
(Table 2) show that the differences in intensity, pitch
and voice quality in backchannel-preceding IPUs
are smaller between conversational partners than be-
tween speakers and their non-partners in the corpus.
Feature t df p-value Sig.
Intensity 500 -4.73 23 9.09e-05 *
Intensity 1000 -2.80 23 0.01 *
Pitch 500 -3.38 23 0.002 *
Pitch 1000 -3.28 23 0.003 *
Pitch slope 200 -1.77 23 0.09 .
Pitch slope 300 -0.93 23 N.S.
Duration 0.50 23 N.S.
# Words 1.39 23 N.S.
NHR 500 -2.00 23 0.06 .
NHR 1000 -2.30 23 0.03 *
Table 2: T -tests between partners and their non-partners
in the corpus.
The differences between interlocutor and their
non-partners in features modeling pitch show that
there is no single ?optimal? value for a pitch level
that precedes a backchannel; this value is coordi-
nated between partners on a pair-by-pair basis. Sim-
ilarly, while varying intensity or voice quality may
be considered a universal cue for a backchannel, the
specific values of the production appear to be a mat-
ter of coordination between individual speaker pairs.
While some views of entrainment hold that coor-
dination takes place at the very beginning of a dia-
logue, others hypothesize that coordination contin-
ues to improve over the course of the conversation.
T -tests for difference of means show that indeed
the differences between conversational partners in
mean pitch and intensity in the final 1000 millisec-
onds of backchannel-preceding IPUs are smaller in
the second half of the conversation than in the first
(t = 3.44, 2.17; df = 23; p < 0.05, 0.01), indicat-
ing that entrainment in this dimension is an ongoing
process that results in closer alignment after the in-
terlocutors have been speaking for some time.
3.3 Measure 3: Local BPC entrainment
Measures 1 and 2 capture global entrainment and
can be used to characterize an entire dialogue with
respect to entrainment. We now look for evidence
to support the hypothesis that a speaker?s realization
of BPCs influences how her interlocutor produces
BPCs. To capture this, we compile a list of pairs
of backchannel-preceding IPUs, in which the second
member of each pair follows the first in the conver-
115
sation and is produced by a different speaker. For
each feature, we calculate the Pearson?s correlation
between acoustic variables extracted from the first
element of each pair and the second.
The correlations for mean pitch and intensity are
significant (r = 0.3, two-sided t-test: p < 0.05, in
both cases). Other correlations are not significant.
These results suggest that entrainment on pitch and
intensity at least is a localized phenomenon. Spoken
dialogue systems may exploit this information, mod-
ifying their output to invite a backchannel similar to
the user?s own previous backchannel invitation.
4 Correlation with dialogue coordination
and task success
Entrainment is widely believed to be crucial to dia-
logue coordination. In the specific case of BPC en-
trainment, it seems intuitive that some consensus on
BPCs should be integral to the successful coordina-
tion of a conversation. Long latencies (periods of si-
lence) before backchannels can be considered a sign
of poor coordination, as when a speaker is waiting
for an indication that his partner is still attending,
and the partner is slow to realize this. Similarly,
interruptions signal poor coordination, as when a
speaker has not finished what he has to say, but his
partner thinks it is her turn to speak. We thus use
mean backchannel latency and proportion of inter-
ruptions as measures of coordination of whole ses-
sions. We use the combined score of the games the
subjects played as a measure of task success. We
correlate all three with our two global entrainment
scores and report correlation coefficients in Table 3.
Entrain. Success/coord. r p-value
measure measure
1 Latency -0.33 0.06
Interruptions -0.50 0.01
Score 0.22 N.S.
2 Latency -0.61 0.002
Interruptions -0.22 N.S.
Score 0.72 6.9e-05
Table 3: Correlations with success and coordination.
Our first metric for identifying entrainment, Mea-
sure 1, the number of cues the speaker pair has in
common, is negatively correlated with mean latency
and proportion of interruptions, our two measures of
poor coordination. Its correlation with score, though
not significant, is positive. So, more entrainment in
BPCs under Measure 1 means smaller latency before
backchannels and fewer interruptions, while there
is a tendency for such entrainment to be associated
with higher scores.
Our second entrainment metric, Measure 2, cap-
tures the similarities between speaker means of the
10 features associated with BPCs. To test correla-
tions of this measure with task success, we collapse
the ten features into a single measure by taking the
negated Euclidean distance between each speaker
pair?s 2 vectors of means; this measure tells us how
close these speakers are across all features exam-
ined. Under this analysis, we find that Measure 2
is negatively correlated with mean latency and pos-
itively correlated with score. Both correlations are
strong and highly significant. Again, the correlation
with interruptions is negative, although not signifi-
cant. Thus, more entrainment defined by this metric
means shorter latency between turns, fewer interrup-
tions, and again and more strongly, higher scores.
We thus find that, the more entrainment at the
global level, the better the coordination between the
partners and the better their performance on their
joint task. These results provide evidence of the im-
portance of BPC entrainment to dialogue.
5 Conclusion
In this paper we discuss the role of entrainment
in turn-taking behavior and its impact on conversa-
tional coordination and task success in the Columbia
Games Corpus. We examine a novel form of en-
trainment, entrainment in BPCs ? characteristics of
speech segments that are followed by backchannels
from the interlocutor. We employ three measures
of entrainment ? two global and one local ? and
find evidence of entrainment in all three. We also
find correlations between our two global entrain-
ment measures and conversational coordination and
task success. In future, we will extend this analysis
to the complementary turn-taking category of turn-
yielding cues and explore how a spoken dialogue
system may take advantage of information about en-
trainment to improve dialogue coordination and the
user experience.
116
6 Acknowledgments
This material is based on work supported in
part by the National Science Foundation under
Grant No. IIS-0803148 and by UBACYT No.
20020090300087.
References
L. Bell, J. Boye, J. Gustafson, and M. Wiren. 2000.
Modality convergence in a multimodal dialogue sys-
tem. In Proceedings of 4th Workshop on the Semantics
and Pragmatics of Dialogue (GOTALOG).
L. Bell, J. Gustafson, and M. Heldner. 2003. Prosodic
adaptation in human-computer interaction. In Pro-
ceedings of the 15th International Congress of Pho-
netic Sciences (ICPhS).
S.E. Brennan and H.H. Clark. 1996. Conceptual pacts
and lexical choice in conversation. Journal of Exper-
imental Psychology: Learning, Memory, and Cogni-
tion, 22(6):1482?1493.
S.E. Brennan. 1996. Lexical entrainment in spontaneous
dialog. In Proceedings of the International Sympo-
sium on Spoken Dialog (ISSD).
E.H. Buder, A.S. Warlaumont, D.K. Oller, and L.B.
Chorna. 2010. Dynamic indicators of Mother-Infant
Prosodic and Illocutionary Coordination. In Proceed-
ings of the 5th International Conference on Speech
Prosody.
R. Coulston, S. Oviatt, and C. Darves. 2002. Amplitude
convergence in children?s conversational speech with
animated personas. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing
(ICSLP).
J. Edlund, M. Heldner, and J. Hirschberg. 2009. Pause
and gap length in face-to-face interaction. In Proceed-
ings of Interspeech.
D. Goleman. 2006. Social Intelligence: The New Sci-
ence of Human Relationships. Bantam.
A. Gravano and J. Hirschberg. 2009. Backchannel-
inviting cues in task-oriented dialogue. In Proceedings
of SigDial.
A. Gravano and J. Hirschberg. 2011. Turn-taking cues
in task-oriented dialogue. Computer Speech and Lan-
guage, 25(33):601?634.
M. Heldner, J. Edlund, and J. Hirschberg. 2010. Pitch
similarity in the vicinity of backchannels. In Proceed-
ings of Interspeech.
A. Nenkova, A. Gravano, and J. Hirschberg. 2008. High
frequency word entrainment in spoken dialogue. In
Proceedings of ACL/HLT.
K. Niederhoffer and J. Pennebaker. 2002. Linguistic
style matching in social interaction. Journal of Lan-
guage and Social Psychology, 21(4):337?360.
M. J. Pickering and S. Garrod. 2004. Toward a mecha-
nistic psychology of dialogue. Behavioral and Brain
Sciences, 27:169?226.
D. Reitter, F. Keller, and J.D. Moore. 2006. Computa-
tional modelling of structural priming in dialogue. In
Proceedings of HLT/NAACL.
E. Schegloff. 1982. Discourse as an interactional
achievement: Some uses of ?uh huh? and other things
that come between sentences. In D. Tannen, editor,
Analyzing Discourse: Text and Talk, pages 71?93.
Georgetown University Press.
S. Stoyanchev and A. Stent. 2009. Lexical and syntactic
priming and their impact in deployed spoken dialogue
systems. In Proceedings of NAACL.
A. Ward and D. Litman. 2007. Automatically measuring
lexical and acoustic/prosodic convergence in tutorial
dialog corpora. In Proceedings of the SLaTE Work-
shop on Speech and Language Technology in Educa-
tion.
N.G. Ward and S.K. Mamidipally. 2008. Factors Affect-
ing Speaking-Rate Adaptation in Task-Oriented Di-
alogs. In Proceedings of the 4th International Con-
ference on Speech Prosody.
117
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 230?235,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Detecting Retries of Voice Search Queries
Rivka Levitan
Columbia University
?
rlevitan@cs.columbia.edu
David Elson
Google Inc.
elson@google.com
Abstract
When a system fails to correctly recog-
nize a voice search query, the user will fre-
quently retry the query, either by repeat-
ing it exactly or rephrasing it in an attempt
to adapt to the system?s failure. It is de-
sirable to be able to identify queries as
retries both offline, as a valuable quality
signal, and online, as contextual informa-
tion that can aid recognition. We present
a method than can identify retries offline
with 81% accuracy using similarity mea-
sures between two subsequent queries as
well as system and user signals of recogni-
tion accuracy. The retry rate predicted by
this method correlates significantly with a
gold standard measure of accuracy, sug-
gesting that it may be useful as an offline
predictor of accuracy.
1 Introduction
With ever more capable smartphones connecting
users to cloud-based computing, voice has been a
rapidly growing modality for searching for infor-
mation online. Our voice search application con-
nects a speech recognition service with a search
engine, providing users with structured answers to
questions, Web results, voice actions such as set-
ting an alarm, and more. In the multimodal smart-
phone interface, users can press a button to ac-
tivate the microphone, and then speak the query
when prompted by a beep; after receiving results,
the microphone button is available if they wish to
follow up with a subsequent voice query.
Traditionally, the evaluation of speech recogni-
tion systems has been carried by preparing a test
set of annotated utterances and comparing the ac-
curacy of a system?s transcripts of those utterances
?
This work was done while the first author was an intern
at Google Inc.
against the annotations. In particular, we seek to
measure and minimize the word error rate (WER)
of a system, with a WER of zero indicating perfect
transcription. For voice search interfaces such as
the present one, though, query-level metrics like
WER only tell part of the story. When a user is-
sues two queries in a row, she might be seeking the
same information for a second time due to a sys-
tem failure the first time. When this happens, from
an evaluation standpoint it is helpful to break down
why the first query was unsuccessful: it might be
a speech recognition issue (in particular, a mis-
taken transcription), a search quality issue (where
a correct transcript is interpreted incorrectly by the
semantic understanding systems), a user interface
issue, or another factor. As a second voice query
may also be a new query or a follow-up query, as
opposed to a retry of the first query, the detection
of voice search retry pairs in the query steam is
non-trivial.
Correctly identifying a retry situation in the
query stream has two main benefits. The first
involves offline evaluation and monitoring. We
would like to know the rate at which users were
forced to retry their voice queries, as a measure of
quality. The second has a more immediate ben-
efit for individual users: if we can detect in real
time that a new voice search is really a retry of a
previous voice search, we can take immediate cor-
rective action, such as reranking transcription hy-
potheses to avoid making the same mistake twice,
or presenting alternative searches in the user inter-
face to indicate that the system acknowledges it is
having difficulty.
In this paper, we describe a method for the clas-
sification of subsequent voice searches as either
retry pairs of a certain type, or non-retry pairs. We
identify four salient types of retry pairs, describe
a test set and identify the features we extracted to
build an automatic classifier. We then describe the
models we used to build the classifier and their rel-
230
ative performance on the task, and leave the issue
of real-time corrective action to future work.
2 Related Work
Previous work in voice-enabled information re-
trieval has investigated the problem of identifying
voice retries, and some has taken the additional
step of taking corrective action in instances where
the user is thought to be retrying an earlier utter-
ance. Zweig (2009) describes a system switching
approach in which the second utterance is recog-
nized by a separate model, one trained differently
than the primary model. The ?backup? system is
found to be quite effective at recognizing those
utterances missed by the primary system. Retry
cases are identified with joint language modeling
across multiple transcripts, with the intuition that
retry pairs tend to be closely related or exact dupli-
cates. They also propose a joint acoustic model in
which portions of both utterances are averaged for
feature extraction. Zweig et al (2008) similarly
create a joint decoding model under the assump-
tion that a discrete sent of entities (names of busi-
nesses with directory information) underlies both
queries. While we follow this work in our usage of
joint language modeling, our application encom-
passes open domain voice searches and voice ac-
tions (such as placing calls), so we cannot use sim-
plifying domain assumptions.
Other approaches include Cevik, Weng and Lee
(2008), who use dynamic time warping to de-
fine pattern boundaries using spectral features, and
then consider the best matching patterns to be re-
peated. Williams (2008) measures the overlap be-
tween the two utterances? n-best lists (alternate hy-
potheses) and upweights hypotheses that are com-
mon to both attempts; similarly, Orlandi, Culy and
Franco (2003) remove hypotheses that are seman-
tically equivalent to a previously rejected hypoth-
esis. Unlike these approaches, we do not assume a
strong notion of dialog state to maintain per-state
models.
Another consequence of the open-domain na-
ture of our service is that users are conditioned
to interact with the system as they would with a
search engine, e.g., if the results of a search do
not satisfy their information need, they rephrase
queries in order to refine their results. This can
happen even if the first transcript was correct and
the rephrased query can be easily confused for a
retry of a utterance where the recognition failed.
Figure 1: Retry annotation decision tree.
For purposes of latently monitoring the accuracy
of the recognizer from usage logs, this is a signifi-
cant complicating factor.
3 Data and Annotation
Our data consists of pairs of queries sampled from
anonymized session logs. We consider a pair of
voice searches (spoken queries) to be a potential
retry pair if they are consecutive; we assume that
a voice search cannot be a retry of another voice
search if a typed search occurs between them. We
also exclude pairs for which either member has no
recognition result. For the purpose of our analy-
sis, we further restricted our data to query pairs
whose second member had been previously ran-
domly selected for transcription. A set of 8,254
query pairs met these requirements and are consid-
ered potential retry pairs. 1,000 randomly selected
pairs from this set were separated out and anno-
tated by the authors, leaving a test set of 7,254 po-
tential retry pairs. Among the annotated develop-
ment set, 18 inaudible or unintelligible pairs were
discarded, for a final development set of 982 pairs.
The problem as we have formulated it requires
a labeling system that identifies repetitions and
rephrases as retries, while excluding query pairs
that are superficially similar but have different
search intents. Our system includes five labels.
Figure 1 shows the guidelines for annotation that
define each category.
The first distinction is between query pairs with
the same search intent (?Is the user looking for
the same information??) and those with different
search intents. We define search intent as the re-
sponse the user wants and expects from the sys-
tem. If the second query?s search intent is differ-
ent, it is by definition no retry.
The second distinction we make is between
cases where the first query was recognized cor-
231
rectly and those where it was not. Although
a query that was recognized correctly may be
retried?for example, the user may want to be
reminded of information she already received
(other)?we are only interested in cases where the
system is in error.
If the search intent is the same for both queries,
and the system incorrectly recognized the first,
we consider the second query a retry. We dis-
tinguish between cases where the user repeated
the query exactly, repetition, and where the user
rephrased the query in an attempt to adapt to the
system?s failure, rephrase. This category includes
many kinds of rephrasings, such as adding or drop-
ping terms, or replacing them with synonyms.
The rephrased query may be significantly differ-
ent from the original, as in the following example:
Q1. Navigate to chaparral ease. (?Navigate to Chiappar-
elli?s.?)
Q2. Chipper rally?s Little Italy Baltimore. (?Chiappar-
elli?s Little Italy Baltimore.?)
The rephrased query dropped a term (?Navigate
to?) and added another (?Little Italy Baltimore?).
This example illustrates another difficulty of the
data: the unreliability of the automatic speech
recognition (ASR) means that terms that are in
fact identical (?Chiapparelli?s?) may be recog-
nized very differently (?chaparral ease? or ?chip-
per rally?s?). In the next example, the recognition
hypotheses of two identical queries have only a
single word in common:
Q1. I get in the house Google. (?I did it Google?)
Q2. I did it crash cool. (?I did it Google?)
Conversely, recognition hypotheses that are
nearly identical are not necessarily retries. Often,
these are ?serial queries,? a series of queries the
user is making of the same form or on the same
topic, often to test the system.
Q1. How tall is George Clooney?
Q2. How old is George Clooney?
Q1. Weather in New York.
Q2. Weather in Los Angeles.
These complementary problems mean that we
cannot use na??ve text similarity features to identify
retries. Instead, we combine features that model
the first query?s likely accuracy to broader similar-
ity features to form a more nuanced picture of a
likely retry.
The five granular retry labels were collapsed
into binary categories: search retry, other, and no
retry were mapped to NO RETRY; and repetition
and rephrase were mapped to RETRY. The label
(a) Granular labels
(b) Collapsed (binary) labels
Figure 2: Retry label distribution.
distribution of the final dataset is shown in Figure
2.
4 Features
The features we consider can be divided into three
main categories. The first group of features, sim-
ilarity, is intended to measure the similarity be-
tween the two queries, as similar queries are (with
the above caveats) more likely to be retries. We
calculate the edit distance between the two tran-
scripts at the character and word level, as well as
the two most similar phonetic rewrites. We include
both raw and normalized values as features. We
also count the number of unigrams the two tran-
scripts have in common and the length, absolute
and relative, of the longest unigram overlap.
As we have shown in the previous section, sim-
ilarity features alone cannot identify a retry, since
ASR errors and user rephrases can result in recog-
nition hypotheses that are significantly different
from the original query, while a nearly identical
pair of queries can have different search intents.
Our second group of features, correctness, goes
up a level in our labeling decision tree (Figure 1)
and attempts to instead answer the question: ?Was
the first query transcribed incorrectly?? We use
the confidence score assigned by the recognizer to
the first recognition hypothesis as a measure of the
system?s opinion of its own performance. Since
this score, while informative, may be inaccurate,
we also consider signals from the user that might
indicate the accuracy of the hypothesis. A boolean
feature indicates whether the user interacted with
any of the results (structured or unstructured) that
were presented by the system in response to the
first query, which should constitute an implicit ac-
ceptance of the system?s recognition hypothesis.
The length of the interval between the two queries
is another feature, since a query that occurs imme-
diately after another is likely to be a retry. We also
include the difference and ratio of the two queries?
speaking rate, roughly calculated as the number
of vowels divided by the audio duration in sec-
232
onds, since a speaker is likely to hyperarticulate
(speak more loudly and slowly) after being misun-
derstood ((Wade et al, 1992; Oviatt et al, 1996;
Levow, 1998; Bell and Gustafson, 1999; Soltau
and Waibel, 1998)).
The third feature group, recognizability, at-
tempts to model the characteristics of a query that
is likely to be misrecognized (for the first query
of the pair) or is likely to be a retry of a previ-
ous query (for the second query). We look at the
language model (LM) score and the number of al-
ternate pronunciations of the first query, predicting
that a misrecognized query will have a lower LM
score and more alternate pronunciations. In ad-
dition, we look at the number of characters and
unigrams and the audio duration of each query,
with the intuition that the length of a query may
be correlated with its likelihood of being retried
(or a retry). This feature group also includes
two heuristic features intended to flag the ?serial
queries? mentioned before: the number of capital-
ized words in each query, and whether each one
begins with a question word (who, what, etc.).
5 Prediction task
5.1 Experimental Results
A logistic regression model was trained on these
features to predict the collapsed binary categories
of NO RETRY (search retry, other, no retry) vs.
RETRY (rephrase, repetition). The results of run-
ning this model with each combination of the fea-
ture groups are shown in Table 1.
Features Precision Recall F1 Accuracy
Similarity 0.54 0.65 0.59 0.72
Correctness 0.53 0.67 0.59 0.73
Recognizability 0.49 0.63 0.55 0.70
Sim. & Corr. 0.67 0.71 0.69 0.77
Sim. & Rec. 0.62 0.70 0.66 0.76
Corr. & Rec. 0.65 0.71 0.68 0.77
All Features 0.70 0.76 0.73 0.81
Table 1: Results of the binary prediction task.
Individually, each feature group peformed sig-
nificantly better than the baseline strategy of al-
ways predicting NO RETRY (62.4%). Each pair
of feature groups performed better than any indi-
vidual group, and the final combination of all three
feature groups had the highest precision, recall,
and accuracy, suggesting that each aspect of the
retry conceptualization provides valuable informa-
tion to the model.
Of the similarity features, the ones that con-
tributed significantly in the final model were char-
acter edit distance (normalized) and phoneme edit
distance (raw and normalized); as expected, re-
tries are associated with more similar query pairs.
Of the correctness features, high recognizer con-
fidence, the presence of a positive reaction from
the user such as a link click, and a long inter-
val between queries were all negatively associated
with retries. The significant recognizability fea-
tures included length of the first query in charac-
ters (longer queries were less likely to be retried)
and the number of capital letters in each query (as
our LM is case-sensitive): queries transcribed with
more capital letters were more likely to be retried,
but less likely to themselves be retries. In addition,
the language model likelihood for the first query
was, as expected, significantly lower for retries.
Interestingly, the score of the second query was
lower for retries as well. This accords with our
finding that retries of misrecognized queries are
themselves misrecognized 60%-70% of the time,
which highlights the potential value of corrective
action informed by the retry context.
Several features, though not significant in the
model, are significantly different between the
RETRY and NO RETRY categories, which affords
us further insight into the characteristics of a retry.
T -tests between the two categories showed that all
edit distance features?character, word, reduced,
and phonetic; raw and normalized?are signifi-
cantly more similar between retry query pairs.
1
Similarly, the number of unigrams the two queries
have in common is significantly higher for retries.
The duration of each member of the query pair,
in seconds and word count, is significantly more
similar between retry pairs, and each member of a
retry pair tends to be shorter than members of a no
retry pair. Finally, members of NO RETRY query
pairs were significantly more similar in speaking
rate, and the relative speaking rate of the second
query was significantly slower for RETRY pairs,
possibly due to hyperarticulation.
5.2 Analysis
Figure 3 shows a breakdown of the true granular
labels versus the predicted binary labels. The pri-
mary source of error is the REPHRASE category,
which is identified as a retry with only 16.5% ac-
1
T -tests reported here use a conservative significance
threshold of p < 0.00125 to control for family-wise type I
error (?data dredging? effects).
233
Figure 3: Performance on each of the granular categories.
curacy. This result reflects the fact that although
rephrases conceptually belong in the retry cate-
gory, their characteristics are materially different.
Most notably, all edit distance features are signif-
icantly greater for rephrases. Differences in du-
ration between the two queries in a pair, in sec-
onds and words, are significantly greater as well.
Rephrases also are significantly longer, in seconds
and words, than strict retries. The model includ-
ing only correctness and recognizability features
does significantly better on rephrases than the full
model, identifying them as retries with 25.6% ac-
curacy, confirming that the similarity features are
the primary culprit. Future work may address this
issue by including features crafted to examine the
similarity between substrings of the two queries,
rather than the query as a whole, and by expand-
ing the similarity definition to include synonyms.
To test the model?s performance with a larger,
unseen dataset, we looked at how many retries
it detected in the test set of potential retry pairs
(n=7,254). We do not have retry annotations for
this larger set, but we have transcriptions for the
first member of each query pair, enabling us to cal-
culate the word error rate (WER) of each query?s
recognition hypothesis, and thus obtain ground
truth for half of our retry definition. A perfect
model should never predict RETRY when the first
query is transcribed correctly (WER==0). As
shown in Figure 4, our model assigns a RETRY
label to approximately 14% of the queries follow-
ing an incorrectly recognized search, and only 2%
of queries following a correctly recognized search.
While this provides us with only a lower bound on
our model?s error, this significant correlation with
an orthogonal accuracy metric shows that we have
modeled at least this aspect of retries correctly, and
suggests a correlation between retry rate and tradi-
tional WER-based evaluation.
Figure 4: Performance on unseen data. A perfect model
would have a predicted retry rate of 0 when WER==0.
6 Conclusion
We have presented a method for characterizing re-
tries in an unrestricted voice interface to a search
system. One particular challenge is the lack of
simplifying assumptions based on domain and
state (as users may consider the system to be
stateless when issuing subsequent queries). We
introduce a labeling scheme for retries that en-
compasses rephrases?cases in which the user re-
worded her query to adapt to the system?s error?
as well as repetitions.
Our model identifies retries with 81% accuracy,
significantly above baseline. Our error analysis
confirms that user rephrasings complicate the bi-
nary class separation; an approach that models
typical typed rephrasings may help overcome this
difficulty. However, our model?s performance to-
day correlates strongly with an orthogonal accu-
racy metric, word error rate, on unseen data. This
suggests that ?retry rate? is a reasonable offline
quality metric, to be considered in context among
other metrics and traditional evaluation based on
word error rate.
Acknowledgments
The authors thank Daisy Stanton and Maryam
Kamvar for their helpful comments on this project.
References
Linda Bell and Joakim Gustafson. 1999. Repetition
and its phonetic realizations: Investigating a swedish
database of spontaneous computer-directed speech.
In Proceedings of ICPhS, volume 99, pages 1221?
1224.
Mert Cevik, Fuliang Weng, and Chin-Hui Lee. 2008.
Detection of repetitions in spontaneous speech in di-
234
alogue sessions. In Proceedings of the 9th Annual
Conference of the International Speech Communi-
cation Association (INTERSPEECH 2008), pages
471?474, Brisbane, Australia.
Gina-Anne Levow. 1998. Characterizing and recog-
nizing spoken corrections in human-computer di-
alogue. In Proceedings of the 17th international
conference on Computational linguistics-Volume 1,
pages 736?742. Association for Computational Lin-
guistics.
Marco Orlandi, Christopher Culy, and Horacio Franco.
2003. Using dialog corrections to improve speech
recognition. In Error Handling in Spoken Language
Dialogue Systems. International Speech Communi-
cation Association.
Sharon Oviatt, G-A Levow, Margaret MacEachern,
and Karen Kuhn. 1996. Modeling hyperarticu-
late speech during human-computer error resolu-
tion. In Spoken Language, 1996. ICSLP 96. Pro-
ceedings., Fourth International Conference on, vol-
ume 2, pages 801?804. IEEE.
Hagen Soltau and Alex Waibel. 1998. On the influ-
ence of hyperarticulated speech on recognition per-
formance. In ICSLP. Citeseer.
Elizabeth Wade, Elizabeth Shriberg, and Patti Price.
1992. User behaviors affecting speech recognition.
In ICSLP.
Jason D. Williams. 2008. Exploiting the asr n-
best by tracking multiple dialog state hypotheses.
In Proceedings of the 9th Annual Conference of
the International Speech Communication Associa-
tion (INTERSPEECH 2008), pages 191?194, Bris-
bane, Australia.
Geoffrey Zweig, Dan Bohus, Xiao Li, and Patrick
Nguyen. 2008. Structured models for joint
decoding of repeated utterances. In Proceed-
ings of the 9th Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH 2008), pages 1157?1160, Brisbane, Aus-
tralia.
Geoffrey Zweig. 2009. New methods for the
analysis of repeated utterances. In Proceed-
ings of the 10th Annual Conference of the Inter-
national Speech Communication Association (IN-
TERSPEECH 2009), pages 2791?2794, Brighton,
United Kingdom.
235
