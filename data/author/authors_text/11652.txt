Proceedings of the 8th International Conference on Computational Semantics, pages 128?139,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Dialogue Modelling and the Remit
of Core Grammar
Eleni Gregoromichelaki
?
, Yo Sato
?
, Ruth Kempson
?
Andrew Gargett
?
, Christine Howes
?
?
King?s College London,
?
Queen Mary University of London
1 Introduction
In confronting the challenge of providing formal models of dialogue, with
its plethora of fragments and rich variation in modes of context-dependent
construal, it might seem that linguists face two types of methodological
choice: either (a) conversation employs dialogue-specific mechanisms, for
which a grammar specific to such activity must be constructed; or (b) vari-
ation arises due to independent parsing/production systems which invoke
a process-neutral grammar. However, as dialogue research continues to de-
velop, there are intermediate possibilities, and in this paper we discuss the
approach developed within Dynamic Syntax (DS, Kempson et al 2001,
Cann et al 2005), a grammar framework within which, not only the parser,
but indeed ?syntax? itself are just a single mechanism allowing the pro-
gressive construction of semantic representations in context. Here we take
as a case study the set of phenomena classifiable as clarifications, reformu-
lations, fragment requests and corrections accompanied by extensions, and
argue that though these may seem to be uniquely constitutive of dialogue,
they are grounded in the mechanisms of apposition equivalently usable in
monologue for presenting reformulations, extensions, self-corrections etc.
2 Background
The data we focus on are non-repetitive fragment forms of acknowledge-
ments, clarifications and corrections (henceforth, A female, B male):
(1) A: Bob left.
B: (Yeah,) the accounts guy.
(2)
A: They X-rayed me, and took a urine sample, took a blood sample.
Er, the doctor
B: Chorlton?
A: Chorlton, mhm, he examined me, erm, he, he said now they
were on about a slight [shadow] on my heart.
128
(3) A: Bob left.
B: Rob?
A: (No,) (Bob,) the accounts guy.
Even though in the literature the fragments in (2)-(3) might be characterised
as illustrating distinct construction-types, in our view, they all illustrate how
speakers and hearers may contribute, in some sense to be made precise, to
the joint enterprise of establishing some shared communicative content, in
what might be loosely called split utterances. Even (1), an acknowledgement,
can be seen this way upon analysis: B?s addition is similar in structure to
an afterthought extension that might have been added by A herself to A?s
fully sentential utterance. It can be seen in (2) that such joint construction
of content can proceed incrementally: the clarification request in the form of
a reformulation is provided by B and resolved by A within the construction
of a single proposition. In (3) the fragment reply can be taken to involve
correction, in the sense that, according to the DS analysis of B?s fragment
question, he has provided content construable as equivalent to that derived
by processing Rob left? (see Kempson et al (2007)). Nevertheless such
corrections can also incorporate extensions in the above sense, enabling a
single conjoined propositional content to be derived in a single step.
It might seem that such illustration of diversity of fragment usage is am-
ple evidence of the need for conversation-specific rules. Indeed, Ferna?ndez
(2006) presents a thorough taxonomy, as well as detailed formal modelling
of Non-Sentential Utterances (NSUs), referring to contributions such as (1)
as repeated acknowledgements involving reformulation. Ferna?ndez models
such constructions via type-specific ?accommodation rules? which make a
constituent of the antecedent utterance ?topical?. The semantic effect of
acknowledgement is then derived by applying an appropriately defined ut-
terance type for such fragments to the newly constructed context. A distinct
form of contextual accommodation is employed to model so-called helpful
rejection fragments, as in (3) (without the reformulation), whereby a wh-
question is accommodated in the context by abstracting over the content
of one of the sub-constituents of the previous utterance. The content of
the correction is derived by applying this wh-question in the context to the
content of the fragment (see also Schlangen (2003) for another classification
and analysis).
In contrast, the alternative explored here is whether phenomena such
as (1)-(2), both of which are non-repetitive next-speaker contributions, can
be handled uniformly using the mechanisms for structure-building made
available in the core grammar, without recourse to construction-specific ex-
tensions of that grammar and contextual accommodation rules. This is
because, in our view, the range of interpretations these fragments receive
in actual dialogue seem to form continua with no well-defined boundaries
and mixing of functions (see also comments in Schlangen (2003)). Thus we
129
propose that the grammar itself simply provides mechanisms for process-
ing/integrating such fragments in the current structure while their precise
contribution to the interaction can be calculated by pragmatic inferencing
if needed (as in e.g. Schlangen (2003)) or, as seems most often to be the
case, be left underspecified without disruption to the dialogue.
One bonus of the stance taken here is the promise it offers for elucidating
the grammar-parser contribution to the disambiguation task. Part of the
challenge of modelling dialogue is the apparent multiplicity of interpretive
and structural options opened up during processing by the recurrent, of-
ten overlapping fragments as seen in (2) above. Thus, it might seem that
the rich array of elliptical fragments available in dialogue adds to its com-
plexity. However, an alternative point of view is to see such phenomena as
providing a window on how interlocutors exploit the incrementality afforded
by the grammar. The reliance of fragments on context for interpretation,
when employed incrementally, enables the hearer to immediately respond to
a previous utterance at any relevant point, in a constrained manner, with-
out ?recovering? a propositional unit. Three features of the Dynamic Syntax
model of dialogue (Purver et al (2006)), presented below, provide the flex-
ible control required for such processing: (a) word-by-word incrementality
(b) interaction with contextually provided information at every step of the
construction process (c) tight coordination of parsing and production.
3 Dynamic Syntax: A Sketch
Dynamic Syntax (DS ) is a parsing-based framework, involving strictly se-
quential interpretation of linguistic strings. The model is implemented via
goal-directed growth of tree structures and their annotations formalised us-
ing LOFT (Blackburn and Meyer-Viol (1994)), with modal operators ???, ???
to define concepts of mother and daughter, and their iterated counterparts,
??
?
?, ??
?
?, to define the notions be dominated by and dominate. Under-
specification and update are core aspects of the grammar itself and involve
strictly monotonic information growth for any dimension of tree structures
and annotations. Underspecification is employed at all levels of tree rela-
tions (mother, daughter etc.), as well as formulae and type values, each
having an associated requirement that drives the goal-directed process of
update. For example, an underspecified subject node of a tree may have a
requirement expressed in DS with the node annotation ?Ty(e), for which
the only legitimate updates are logical expressions of type entity (Ty(e));
but requirements may also take a modal form, e.g. ????Ty(e ? t), a con-
straint that the mother node be annotated with a formula of predicate type.
Requirements are essential to the dynamics informing the DS account: all
requirements must be satisfied if the construction process is to lead to a
successful outcome.
130
Semantic structure is built from lexical and general computational ac-
tions. Computational actions govern general tree-constructional processes,
such as introducing and updating nodes, as well as compiling interpretation
for all non-terminal nodes in the tree. Construction of only weakly spec-
ified tree relations (unfixed nodes) can also be induced, characterised only
as dominance by some current node, with subsequent update required. In-
dividual lexical items also provide procedures for building structure in the
form of lexical actions, inducing both nodes and annotations. Thus partial
trees grow incrementally, driven by procedures associated with particular
words as they are encountered, with a pointer, ?, recording the parser?s
progress (unlike van Leusen and Muskens (2003), partial trees are part of
the model and, unlike in other frameworks, incrementality is word-by-word
rather than sentence-by-sentence).
Complete individual trees are taken to correspond to predicate-argument
structures (with an event term associated with tense, suppressed in this
paper). The epsilon calculus (see e.g. Meyer-Viol (1995)) provides the se-
mantic representation language. Complex structures are obtained via a gen-
eral tree-adjunction operation licensing the construction of so-called linked
trees, hosting information that is eventually transferred onto the tree from
which the link is made (Kempson et al2001). Structures projected as such
paired trees range over restrictive relatives, nonrestrictive relatives, condi-
tionals, topic structures and appositions as here. As the semantic represen-
tations employ the epsilon calculus, eventual compound epsilon terms (e.g.
?, x, P (x)) are constructed incrementally through link-adjunction:
(4) A consultant, a friend of Jo?s, is retiring
Ty(t),Retire
?
((?, x, Consultant
?
(x) ? Friend
?
(Jo
?
)(x)))
Ty(e), (?, x, Consultant
?
(x) ? Friend
?
(Jo
?
)(x)) Ty(e? t), Retire
?
Ty(e), (?, x, Friend
?
(Jo
?
)(x))
Ty(cn), (x,Friend
?
(Jo
?
)(x))
x Friend
?
(Jo
?
)
Jo
?
Friend
?
Ty(cn? e), ?P.?, P
Underspecification of content as well as structure are central to facilitat-
ing successful linguistic interaction, our primary concern here. Pronouns,
the prototypical case, contribute a place-holding metavariable, noted as e.g.
U, plus an associated requirement for update by an appropriate term value:
??x.Fo(x). Equally, definite NPs contribute place-holders plus a constraint
providing a restriction/?presupposition? on the kind of entity picked out,
e.g., the man contributes the annotation U
Man
?
(U)
, T y(e). The subscript
specification is shorthand for a transition to a linked tree whose root node is
131
annotated with a formula Man
?
(U)
1
. The update of metavariables can be
accomplished if the context contains an appropriate term for substitution:
context involves storage of parse states, i.e., storing of partial tree, word se-
quence to date, plus the actions used in building up the partial tree (Purver
et al2006).
Scope dependencies between constructed terms or the index of evalua-
tion (e.g. S) are defined on completed propositional formulae, relative to
incrementally collected scope constraints (of the form x < y for constructed
terms containing variables x and y respectively). Constraints reflect on-line
processing considerations modulo over-riding lexical stipulations. For ex-
ample, proper names contribute as iota terms, i.e, epsilon terms reflecting
uniqueness in the context, ?, x,Bob
?
(x), and these project a scope depen-
dency solely on the index of evaluation reflecting their widest scope property
(cf Kamp and Reyle 1994). The structure projected from A?s utterance in
(1) is thus (5) (note that trees are the result of processing words but do
not encode the structure of strings, word order etc., only semantic content
derived in interaction with context, thus are the equivalents of DRSs):
(5)
S < x Ty(t),Leave
?
((?, x,Bob
?
(x)))
Ty(e), (?, x,Bob
?
(x)) Ty(e? t), Leave
?
The scope evaluation rule reflects the predicate-logic/epsilon-calculus equiv-
alence ?xF (x) ? F (?, x, F (x)) so evaluated terms eventually reflect their
containing structure. Hence, evaluation of (5) yields:
(6) Ty(t), Leave
?
(?, x,Bob
?
(x) ? Leave
?
(x))
A major aspect of the DS dialogue model is that both generation and
parsing are goal-directed and incremental, and hence are governed by es-
sentially the same mechanism. Under this model, a human hearer-parser
builds a succession of partial parse trees based on what (s)he has heard
thus far. Importantly, however, unlike the conventional bottom-up parsing,
the DS model assumes a strong predictive element in parsing: a hearer is
assumed to entertain some goal to be reached eventually at any stage of
parsing. In (1), for example, as soon as the hearer encounters Bob, an un-
derspecified propositional tree is constructed, as in the first simplified and
schematised tree in Figure 1. Then the tree ?grows? monotonically, i.e. such
that at each word input, it is ?updated? to an ?incremented? tree that is
subsumed by the original tree, as depicted in the same Figure. This can be
described as a process of specifying the relevant nodes towards a complete
tree. This predictive element in DS allows a speaker-generator to be mod-
elled as doing exactly the same, i.e. going through monotonically updated
partial trees, the only difference being that (s)he also has available a more
1
These linked structures are suppressed in all diagrams.
132
fully specified goal tree representing what (s)he wishes to say, corresponding
to the rightmost tree in the Figure (with ?0? in the ?generation? row at the
bottom indicating it is entertained before utterance). Each licensed step in
generation, i.e. the utterance of a word, is governed by whatever step is
licensed by the parsing formalism, constrained via a required subsumption
relation of the goal tree. By updating their growing ?parse? tree relative
to the goal tree, speakers are licensed to produce the associated natural
language string.
Parsing: 1 2 3
?Ty(t)
q
q
q
q
q
q
q
M
M
M
M
M
M
M
Ty(e),
Bob
?
,?
?Ty(e ? t)
7?
?Ty(t)
q
q
q
q
q
q
q
M
M
M
M
M
M
M
Ty(e),
Bob?
Ty(e ? t),
Leave
?
,?
7?
Ty(t),
Leave?(Bob?),?
q
q
q
q
q
q
q
M
M
M
M
M
M
M
Ty(e),
Bob?
Ty(e ? t),
Leave?
Generation: 1 2 0,3
Figure 1: Parallel parsing and generation in DS
This architecture allows for a dialogue model in which generation (what
a speaker does) and parsing (what a hearer does) function in parallel. The
speaker goes through partial trees subsuming a specified goal tree, while
the hearer attempts to ?mirror? the same series of partial trees, albeit not
knowing what the content of the unspecified nodes will be. For the dialogues
in (1)-(3), therefore, B as the hearer will have the partial representation of
what he has successfully parsed, required also for generation. This provides
him with the ability at any stage to become the speaker, interrupting to
ask for clarification, reformulating, or providing a correction, as and when
necessary. As we shall see, B?s parse tree reveals where need of clarifica-
tion or miscommunication occurs, as it will be at that node from which a
sub-routine extending it takes place
2
. According to our model of dialogue,
repeating or extending a constituent of A?s utterance by B is licensed only
if B, the hearer of A turned now a speaker, entertains currently a goal tree
that matches or extends the parse tree of what he has heard in a monotonic
fashion, although he only utters the relevant subpart of A?s utterance. In-
deed, this update is what B is seeking to clarify, correct or acknowledge. In
DS, B can reuse the already constructed (partial) parse tree in his context,
rather than having to rebuild an entire propositional tree or subtree
3
.
2
The account extends the implementation reported in Purver et al (2006)
3
Given the DS concept of linked trees projecting propositional content, we anticipate
that this mechanism will be extendable to fragment construal involving inference (see e.g.
Schlangen (2003), Schlangen and Lascarides (2003))
133
4 NSU fragments in Dynamic Syntax
4.1 Non-repetitive Acknowledgement
From a DS perspective, phenomena like reformulations as in (1), or exten-
sions to what one understands of the other speaker?s utterance, (2), can
be handled with exactly the same mechanisms as the sentence-internal phe-
nomenon independently identifiable as apposition, as in (4), and equally
usable by a single individual as a means of incrementally reformulating, cor-
recting or extending what they have just uttered. The update rule for such
structures, applicable to all terms, takes the two type e terms so formed and
yields a new term whose compound content is a combination of both.
We now have the basis for analysing extensions potentially functioning
as acknowledgements which build on what has been previously said as a way
of confirming the previous utterance. Recall (1), (2). There are two ways for
fragments which reformulate an interlocutor A?s utterance to occur: (a) as
interruptions of A?s utterance with immediate confirmation of identification
of the individual concerned, see (2); (b) as confirmations/extensions of A?s
utterance after the whole of her utterance has been integrated, see (1). Both
are modelled by DS as incremental additions.
Turning to (1), B?s response (Yeah,) the accounts guy constitutes a re-
formulation of A?s utterance and an extension of A?s referring expression,
yielding a similar content as that of an appositive expression Bob, the ac-
counts guy in this case jointly constructed. B?s reformulation/extension
counts in effect as an acknowledgement in virtue of signalling successful
processing of A?s utterance without objection raised. Thus there is no need
for a separate grammatical mechanism to process these structures. In DS
terms, after processing A?s utterance, B?s context consists of the following
tree:
(7) B?s Context for producing ?Yeah?
Ty(t), Leave
?
(?, x,Bob
?
(x) ? Leave
?
(x)),?
(?, x,Bob
?
(x)) Leave
?
B, as a speaker, can now re-use this representation as point of departure
for generating the expression the accounts guy. In this case his goal tree,
the message to be expressed, will now be annotated with a composite term
made up from both the term recovered from parsing A?s utterance and the
new addition. This requires attaching a linked tree to the correct node
and an appropriate update of the context tree (for reasons of space, the
exact structure of the linked tree is condensed below, with subscripting as
shorthand):
134
(8) B?s goal tree:
Ty(t), Leave
?
((?, x,Bob
?
(x) ?Acc.guy
?
(x))) ? Leave
?
(x))
(?, x,Bob
?
(x) ?Acc.guy
?
(x))
Leave
?
(?, x,Bob
?
(x))
Acc.guy
?
(x)
In order to license generation of the expression the accounts guy, B now
needs to verify that processing these words in the context provided by the
tree in (7) will produce a tree that matches this goal tree in (8). To achieve
this, starting from (7), a series of simulated ?parse? trees are generated
which indeed result in the requisite matching. Steps include shifting the
pointer to the appropriate node, projection of a linked tree from that node
and test-processing the words the accounts guy, each step checking against
the goal tree that a subsumption relation between the current ?parse? tree
and the goal tree is always maintained:
(9) B?s parse tree licensing production of the accounts guy: link adjunction
Ty(t), Leave
?
(?, x,Bob
?
(x) ? Leave
?
(x))
(?, x,Bob
?
(x)) Leave
?
U
Acc.guy
?
(U)
,?
The only way to update this representation relative to both the restriction
on the metavariable and monotonicity of growth on any one node in a tree
involves replacing the metavariable with (?, x,Bob
?
(x)), as this is commen-
surate with an extension of the term annotating the node from which the
link transition was constructed:
(10) Updating B?s parse tree licensing production of the accounts guy
Ty(t), Leave
?
(?, x,Bob
?
(x) ?Acc.guy
?
(x) ? Leave
?
(x)),?
(?, x,Bob
?
(x) ?Acc.guy
?
(x)) Leave
?
(?, x,Bob
?
(x))
Acc.guy
?
(x)
Finally, the information is passed up to the top node of the main tree,
completing the parse tree to match B?s goal tree, (8), thus licensing the
utterance of the expression the accounts guy.
4.2 Non-repetitive Clarification
In the acknowledgement case above, the proposition relative to which the
linked structure is built is completed (with an already extended epsilon
term); but the same mechanism can be used when the interlocutor needs
clarification, prior to any such completion of the tree. In (2), B again, as
the speaker, takes as his goal tree a tree annotated with an expansion of the
term constructed from parsing A?s utterance but nevertheless picking out
the same individual. Using the very same mechanism as in (1) of building a
linked structure, B, interrupting A, provides a distinct expression, the name
Chorlton, this time before he has completed the parse tree for A?s utterance.
All that has been achieved at this point is the definite?s contribution of a
meta-variable with the restriction that the individual picked out must be a
doctor:
135
(11) A/B?s parse tree for Chorlton:
?Ty(t)
U
Doctor
?
(U)
,?
?Ty(e ? t)
(?, x, Chorlton
?
(x))
As in the acknowledgement case, but this time at the node initiating the
link transition, the only possible value to provide for the metavariable U
compatible both with its restriction and the monotoniticity constraint is
the composite term (?, x,Doctor
?
(x) ? Chorlton
?
(x)). The mechanism of
constructing paired structures involving type e terms across linked trees
is identical to that employed in B?s utterance in (1), though to a rather
different effect at this intermediate stage in the interpretation process. This
extension of the term is confirmed by A, this time replicating the composite
term which processing B?s utterance has led to. The eventual effect of the
process of inducing linked structures to be annotated by coreferential type e
terms may thus vary across monologue and different dialogue applications,
yielding different interpretations, but the mechanism is the same.
4.3 Correction
It might be argued nonetheless that correction is intrinsically a dialogue
phenomenon. Consider (3), for example. One of the possible interpretations
of (3), according to the DS analysis, is that B has offered the equivalent of the
content derived by processing Rob left?. That is, let?s assume here that B has
misheard and requests confirmation of what he has perceived A as saying.
A in turn rejects B?s understanding of her utterance and provides more
information. Presuming rejection as simple disagreement (i.e. the utterance
has been understood, but judged as incorrect), in DS terms, this means that
A has in mind a goal tree that licensed what she had produced, which is
distinct from the one derived by processing B?s clarification. As shown in
Kempson et al (2007), this means that A has been unable to process B?s
clarification request as an extension of her own context. Instead, she has to
parse the clarification by exploiting the potential for introducing an initially
structurally underspecified tree-node to accommodate the contribution of
the word Rob. Subsequently, by re-running the actions stored in context
previously by processing her own utterance of the word left, she is able to
complete the integration of the fragment in a new propositional structure.
Now, in order for A to produce the following correction, what is required
is for A to establish as the current most recent representation in context her
original goal tree. This can be monotonically achieved by recovering and
copying this original goal tree to serve as the current most immediate con-
text
4
. An option available to A at this point is to introduce, in addition
or exclusively, a reformulation of her original utterance in order to facilitate
4
Mistaken representations must be maintained in the context as they can provide an-
tecedents for subsequent anaphoric expressions.
136
identification of the named individual which proved problematic for B previ-
ously. She can answer B?s utterance of Rob? with (No,) Bob, the accounts
guy, as in (3) or simply with (No,) the accounts guy. Both are licensed
by the DS parsing mechanism without more ado. For both, the goal tree
will be as follows and it will always be the point of reference for checking
the subsumption relation relative to the simulated parsing steps described
further below:
(12) A?s goal tree
Ty(t), Leave
?
((?, x,Bob
?
(x) ?Acc.guy
?
(x)))
(?, x,Bob
?
(x) ?Acc.guy
?
(x)) Leave
?
(?, x,Bob
?
(x))
Acc.guy
?
(x)
Under these circumstances, given the DS grammar-as-parser perspective,
several strategies are now available for the licensing of generation of the
fragment. A is licensed to repeat the name Bob by locally extending the
node in the context tree where the representation of the individual referred to
is located by using the rule of late*adjunction, a process which involves
building a node of type e from a dominating node of that type (illustrated in
Kempson et al 2007). An alternative way of licensing repetition of the word
Bob is to employ one of the strategies generally available for the parsing of
long distance dependencies i.e. constructing initial tree nodes as unfixed
(*adjunction). We show here how the latter strategy can be exploited to
license the production of the fragment by A.
(13) Parsing simulation licensing generation of Bob, the accounts guy
Step 1: *Adjunction Step 2: LINK-Adjuction + testing the accounts guy
?Ty(t)
(?, x,Bob
?
(x)),?
?Ty(t)
(?, x,Bob
?
(x)),?
U
Acc.guy
?
(U)
The only way to develop the constructed tree at Step 2 commensurate with
the goal tree (12) is to identify the value of U as (?, x,Bob
?
(x)), so this
is what is entered at the newly constructed linked tree, duly leading to
extension of the term originally given as annotating the unfixed node as
(?, x,Bob
?
(x) ? Acc.guy
?
(x)). The structure
5
derived by processing such an
extension is exactly that of (1) above (compare goal tree in (12) above
and tree in (8)). Now, as mentioned before, context, as defined in DS,
keeps track not only of tree representations and words but also of actions
contributed by the words and utilised in building up the tree representations.
Here, according to DS, production of the correction in (3) is licensed to be
5
Again note that DS trees represent derived content rather than structure over natural
language strings.
137
fragmental only because the original actions for parsing/producing the word
left are available in the context and can be recalled to complete the structure
initiated by processing/producing the name Bob. Now these stored actions
can be retrieved to develop the tree further:
(14) Parsing simulation licensing generation of Bob, the accounts guy
Step 3: test-processing stored actions for left
?Ty(t)
(?, x,Bob
?
(x) ?Acc.guy
?
(x)) ?Ty(e),? Leave?
(?, x,Bob
?
(x))
Acc.guy
?
(?,x,Bob
?
(x))
With this partial tree being commensurate with the goal tree, all actions that
follow are general computational processes for completing the tree: unifying
the unfixed node to determine the subject argument, applying the subject
to the predicate, evaluating the quantified terms. Nothing specific to this
structure is needed. Indeed, all these mechanisms are equally applicable by
an individual speaker, perhaps more familiar as right dislocation phenomena,
but equally available incrementally:
(15) Bob left, (Bob) the accounts guy.
5 Conclusion
As these fragments and their construal have demonstrated, despite serving
distinct functions in dialogue, the mechanisms which make such diversity
possible are general strategies for tree growth. In all cases, the advantage
which use of fragments provides is a ?least effort? means of re-employing
previous content/structure/actions which constitute the minimal local con-
text. As modelled in DS, it is more economical to reuse information from
this local context rather than constructing representations afresh (via costly
processes of lexical retrieval, choice of alternative parsing strategies, etc.).
A further quandary in dialogue construal is that, despite such avenues
for economising their efforts, interlocutors are nevertheless faced with an
increasing set of interpretive options at any point during the construction
of representations. One strategy available to hearers is to delay a disam-
biguating move until further input potentially resolves the uncertainty. How-
ever, as further input is processed and parsing/interpretive options increase
rapidly, the human processor struggles. The incremental definition of the
DS formalism allows for the modelling of an alternative strategy available
to hearers: at any (sub-sentential) point they could opt to intervene imme-
diately, and make a direct appeal to the speaker for more information as
illustrated by the clause-medial fragment interruption (2). It seems clear
that the grammar should allow the resources for modelling this behaviour
without any complications.
138
The phenomena examined here are also cases where speakers? and hear-
ers? representations, despite attempts at coordination, may nevertheless sep-
arate sufficiently for them to have to seek ?repair? (see especially (3)). In
the model presented here, the dynamics of interaction allow fully incremen-
tal generation and integration of fragmental utterances so that interlocutors
can be taken to constantly provide optimal evidence of each other?s represen-
tations with necessary adjuncts being able to be incrementally introduced.
But such mechanisms apply equally within an individual utterance, with self-
correction, extension, elaboration, repetition etc. The effect is that all the de-
vices which seem so characteristic of dialogue involve mechanisms invariably
available within an individual?s core grammar. This suggests a new inverse
methodology: it is the challenge of modelling dialogue that can be used as
a point of departure for modelling grammars for individual speakers, rather
than the other, more familiar, way round (see also Ginzburg (forthcmg)).
This reversibility is, notably, straightforwardly available to grammar for-
malisms in which the incremental dynamics of information growth is the
core structural concept because emergent dialogue structure crucially ex-
hibits and interpretively relies on such incrementality.
Acknowledgements
This work was supported by grants ESRC RES-062-23-0962 and Leverhulme F07-
04OU. We are grateful for comments to: Robin Cooper, Alex Davies, Arash Eshghi,
Jonathan Ginzburg, Pat Healey, Greg Mills. Normal disclaimers apply.
References
Patrick Blackburn and Wilfried Meyer-Viol. Linguistics, logic and finite trees.
Bulletin of the IGPL, 2:3?31, 1994.
Raquel Ferna?ndez. Non-Sentential Utterances in Dialogue: Classification, Resolu-
tion and Use. PhD thesis, King?s College London, University of London, 2006.
Jonathan Ginzburg. Semantics for Conversation. CSLI, forthcmg.
Ruth Kempson, Andrew Gargett, and Eleni Gregoromichelaki. Clarification re-
quests: An incremental account. In Proceedings of the 11th Workshop on the
Semantics and Pragmatics of Dialogue (DECALOG), 2007.
Wilfried Meyer-Viol. Instantial Logic. PhD thesis, University of Utrecht, 1995.
Matthew Purver, Ronnie Cann, and Ruth Kempson. Grammars as parsers: Meeting
the dialogue challenge. Research on Language and Computation, 4(2-3):289?326,
2006.
David Schlangen. A Coherence-Based Approach to the Interpretation of Non-
Sentential Utterances in Dialogue. PhD thesis, University of Edinburgh, 2003.
David Schlangen and Alex Lascarides. The interpretation of non-sentential utter-
ances in dialogue. In Proceedings of the 4th SIGdial Workshop on Discourse and
Dialogue, pages 62?71, Sapporo, Japan, July 2003. Association for Computa-
tional Linguistics.
Noor van Leusen and Reinhard Muskens. Construction by description in discourse
representation. In J. Peregrin, editor, Meaning: The Dynamic Turn, chapter 12,
pages 33?65. 2003.
139
Report on the Second NLG Challenge on
Generating Instructions in Virtual Environments (GIVE-2)
Alexander Koller
Saarland University
koller@mmci.uni-saarland.de
Kristina Striegnitz
Union College
striegnk@union.edu
Andrew Gargett
Saarland University
gargett@mmci.uni-saarland.de
Donna Byron
Northeastern University
dbyron@ccs.neu.edu
Justine Cassell
Northwestern University
justine@northwestern.edu
Robert Dale
Macquarie University
Robert.Dale@mq.edu.au
Johanna Moore
University of Edinburgh
J.Moore@ed.ac.uk
Jon Oberlander
University of Edinburgh
J.Oberlander@ed.ac.uk
Abstract
We describe the second installment of the
Challenge on Generating Instructions in
Virtual Environments (GIVE-2), a shared
task for the NLG community which took
place in 2009-10. We evaluated seven
NLG systems by connecting them to 1825
users over the Internet, and report the re-
sults of this evaluation in terms of objec-
tive and subjective measures.
1 Introduction
This paper reports on the methodology and results
of the Second Challenge on Generating Instruc-
tions in Virtual Environments (GIVE-2), which
we ran from August 2009 to May 2010. GIVE
is a shared task for the NLG community which
we ran for the first time in 2008-09 (Koller et al,
2010). An NLG system in this task must generate
instructions which guide a human user in solving
a treasure-hunt task in a virtual 3D world, in real
time. For the evaluation, we connect these NLG
systems to users over the Internet, which makes
it possible to collect large amounts of evaluation
data cheaply.
While the GIVE-1 challenge was a success, in
that it evaluated five NLG systems on data from
1143 game runs in the virtual environments, it
was limited in that users could only move and
turn in discrete steps in the virtual environments.
This made the NLG task easier than intended; one
of the best-performing GIVE-1 systems generated
instructions of the form ?move three steps for-
ward?. The primary change in GIVE-2 compared
to GIVE-1 is that users could now move and turn
freely, which makes expressions like ?three steps?
meaningless, and makes it hard to predict the pre-
cise effect of instructing a user to ?turn left?.
We evaluated seven NLG systems from six in-
stitutions in GIVE-2 over a period of three months
from February to May 2010. During this time,
we collected 1825 games that were played by
users from 39 countries, which is an increase of
over 50% over the data we collected in GIVE-
1. We evaluated each system both on objec-
tive measures (success rate, completion time, etc.)
and subjective measures which were collected by
asking the users to fill in a questionnaire. We
completely revised the questionnaire for the sec-
ond challenge, which now consists of relatively
fine-grained questions that can be combined into
more high-level groups for reporting. We also in-
troduced several new objective measures, includ-
ing the point in the game in which users lost
or cancelled, and an experimental ?back-to-base?
task intended to measure how much users learned
about the virtual world while interacting with the
NLG system.
Plan of the paper. The paper is structured as fol-
lows. In Section 2, we describe and motivate the
GIVE-2 Challenge. In section 3, we describe the
evaluation method and infrastructure. Section 4
reports on the evaluation results. Finally, we con-
clude and discuss future work in Section 5.
2 The GIVE Challenge
GIVE-2 is the second installment of the GIVE
Challenge (?Generating Instructions in Virtual En-
vironments?), which we ran for the first time in
2008-09. In the GIVE scenario, subjects try to
solve a treasure hunt in a virtual 3Dworld that they
have not seen before. The computer has a com-
plete symbolic representation of the virtual world.
The challenge for the NLG system is to gener-
ate, in real time, natural-language instructions that
will guide the users to the successful completion
of their task.
Users participating in the GIVE evaluation
start the 3D game from our website at www.
give-challenge.org. They then see a 3D
Figure 1: What the user sees when playing with
the GIVE Challenge.
game window as in Fig. 1, which displays instruc-
tions and allows them to move around in the world
and manipulate objects. The first room is a tuto-
rial room where users learn how to interact with
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system. Users can
either finish a game successfully, lose it by trig-
gering an alarm, or cancel the game. This result is
stored in a database for later analysis, along with a
complete log of the game.
In each game world we used in GIVE-2, players
must pick up a trophy, which is in a wall safe be-
hind a picture. In order to access the trophy, they
must first push a button to move the picture to the
side, and then push another sequence of buttons to
open the safe. One floor tile is alarmed, and play-
ers lose the game if they step on this tile without
deactivating the alarm first. There are also a num-
ber of distractor buttons which either do nothing
when pressed or set off an alarm. These distractor
buttons are intended to make the game harder and,
more importantly, to require appropriate reference
to objects in the game world. Finally, game worlds
contained a number of objects such as chairs and
flowers that did not bear on the task, but were
available for use as landmarks in spatial descrip-
tions generated by the NLG systems.
The crucial difference between this task and
the (very similar) GIVE-1 task was that in GIVE-
2, players could move and turn freely in the vir-
tual world. This is in contrast to GIVE-1, where
players could only turn by 90 degree increments,
and jump forward and backward by discrete steps.
This feature of the way the game controls were set
up made it possible for some systems to do very
well in GIVE-1 with only minimal intelligence,
using exclusively instructions such as ?turn right?
and ?move three steps forward?. Such instructions
are unrealistic ? they could not be carried over to
instruction-giving in the real world ?, and our aim
was to make GIVE harder for systems that relied
on them.
3 Method
Following the approach from the GIVE-1 Chal-
lenge (Koller et al, 2010), we connected the NLG
systems to users over the Internet. In each game
run, one user and one NLG system were paired up,
with the system trying to guide the user to success
in a specific game world.
3.1 Software infrastructure
We adapted the GIVE-1 software to the GIVE-2
setting. The GIVE software infrastructure (Koller
et al, 2009a) consists of three different mod-
ules: The client, which is the program which the
user runs on their machine to interact with the
virtual world (see Fig. 1); a collection of NLG
servers, which generate instructions in real-time
and send them to the client; and a matchmaker,
which chooses a random NLG server and virtual
world for each incoming connection from a client
and stores the game results in a database.
The most visible change compared to GIVE-1
was to modify the client so it permitted free move-
ment in the virtual world. This change further ne-
cessitated a number of modifications to the inter-
nal representation of the world. To support the de-
velopment of virtual worlds for GIVE, we changed
the file format for world descriptions to be much
more readable, and provided an automatic tool
for displaying virtual worlds graphically (see the
screenshots in Fig. 2).
3.2 Recruiting subjects
Participants were recruited using email distribu-
tion lists and press releases posted on the Internet
and in traditional newspapers. We further adver-
tised GIVE at the Cebit computer expo as part of
the Saarland University booth. Recruiting anony-
mous experimental subjects over the Internet car-
ries known risks (Gosling et al, 2004), but we
showed in GIVE-1 that the results obtained for
the GIVE Challenge are comparable and more in-
formative than those obtained from a laboratory-
World 1 World 2 World 3
Figure 2: The three GIVE-2 evaluation worlds.
based experiment (Koller et al, 2009b).
We also tried to leverage social networks for re-
cruiting participants by implementing and adver-
tising a Facebook application. Because of a soft-
ware bug, only about 50 participants could be re-
cruited in this way. Thus tapping the true poten-
tial of social networks for recruiting participants
remains a task for the next installment of GIVE.
3.3 Evaluation worlds
Fig. 2 shows the three virtual worlds we used in the
GIVE-2 evaluation. Overall, the worlds were more
difficult than the worlds used in GIVE-1, where
some NLG-systems had success rates around 80%
in some of the worlds. As for GIVE-1, the three
worlds were designed to pose different challenges
to the NLG systems. World 1 was intended to be
more similar to the development world and last
year?s worlds. It did have rooms with more than
one button of the same color, however, these but-
tons were not located close together. World 2 con-
tained several situations which required more so-
phisticated referring expressions, such as rooms
with several buttons of the same color (some of
them close together) and a grid of buttons. Fi-
nally, World 3 was designed to exercise the sys-
tems? navigation instructions: one room contained
a ?maze? of alarm tiles, and another room two
long rows of buttons hidden in ?booths? so that
they were not all visible at the same time.
3.4 Timeline
After the GIVE-2 Challenge was publicized in
June 2009, fifteen researchers and research teams
declared their interest in participating. We dis-
tributed a first version of the software to these
teams in August 2009. In the end, six teams sub-
mitted NLG systems (two more than in GIVE-1);
one team submitted two independent NLG sys-
tems, bringing the total number of NLG systems
up to seven (two more than in GIVE-1). These
were connected to a central matchmaker that ran
for a bit under three months, from 23 February to
17 May 2010.
3.5 NLG systems
Seven NLG systems were evaluated in GIVE-2:
? one system from the Dublin Institute of Tech-
nology (?D? in the discussion below);
? one system from Trinity College Dublin
(?T?);
? one system from the Universidad Com-
plutense de Madrid (?M?);
? one system from the University of Heidelberg
(?H?);
? one system from Saarland University (?S?);
? and two systems from INRIA Grand-Est in
Nancy (?NA? and ?NM?).
Detailed descriptions of these systems as well
as each team?s own analysis of the evalua-
tion results can be found at http://www.
give-challenge.org/research.
4 Results
We now report the results of GIVE-2. We start
with some basic demographics; then we discuss
objective and subjective evaluation measures. The
data for the objective measures are extracted from
the logs of the interactions; whereas the data for
the subjective measures are obtained from a ques-
tionnaire which asked subjects to rate various as-
pects of the NLG system they interacted with.
Notice that some of our evaluation measures are
in tension with each other: For instance, a sys-
tem which gives very low-level instructions may
allow the user to complete the task more quickly
(there is less chance of user errors), but it will re-
quire more instructions than a system that aggre-
gates these. This is intentional, and emphasizes
our desire to make GIVE a friendly comparative
challenge rather than a competition with a clear
winner.
4.1 Demographics
Over the course of three months, we collected
1825 valid games. This is an increase of almost
60% over the number of valid games we collected
in GIVE-1. A game counted as valid if the game
client did not crash, the game was not marked as a
test game by the developers, and the player com-
pleted the tutorial.
Of these games, 79.0% were played by males
and 9.6% by females; a further 11.4% did not
specify their gender. These numbers are compa-
rable to GIVE-1. About 42% of users connected
from an IP address in Germany; 12% from the US,
8% from France, 6% from Great Britain, and the
rest from 35 further countries. About 91% of the
participants who answered the question self-rated
their English language proficiency as ?good? or
better. About 65% of users connected from vari-
ous versions of Windows, the rest were split about
evenly between Linux and MacOS.
4.2 Objective measures
The objective measures are summarize in Fig. 3.
In addition to calculating the percentage of games
users completed successfully when being guided
by the different systems, we measured the time
until task completion, the distance traveled until
task completion, and the number of actions (such
as pushing a button to open a door) executed. Fur-
thermore, we counted howmany instructions users
received from each system, and how many words
these instructions contained on average. All objec-
tive measures were collected completely unobtru-
sively, without requiring any action on the user?s
part. To ensure comparability, we only counted
successfully completed games.
task success: Did the player get the trophy?
duration: Time in seconds from the end of the tu-
torial until the retrieval of the trophy.
distance: Distance traveled (measured in distance
units of the virtual environment).
actions: Number of object manipulation actions.
instructions: Number of instructions produced
by the NLG system.
words per instruction: Average number of
words the NLG system used per instruction.
Figure 3: Objective measures.
Fig. 4 shows the results of these objective mea-
sures. Task success is reported as the percent-
age of successfully completed games. The other
measures are reported as the mean number of sec-
onds/distance units/actions/instructions/words per
instruction, respectively. The figure also assigns
systems to groups A, B, etc. for each evaluation
measure. For example, users interacting with sys-
tems in group A had a higher task success rate,
needed less time, etc. than users interacting with
systems in group B. If two systems do not share
the same letter, the difference between these two
systems is significant with p < 0.05. Significance
was tested using a ?2-test for task success and
ANOVAs for the other objective measures. These
were followed by post-hoc tests (pairwise ?2 and
Tukey) to compare the NLG systems pairwise.
In terms of task success, the systems fall pretty
neatly into four groups. Note that systems D and
T had very low task success rates. That means
that, for these systems, the results for the other ob-
jective measures may not be reliable because they
are based on just a handful of games. Another
aspect in which systems clearly differed is how
many words they used per instruction. Interest-
ingly, the three systems with the best task success
rates also produced the most succinct instructions.
The distinctions between systems in terms of the
other measures is less clear.
4.3 Subjective measures
The subjective measures were obtained from re-
sponses to a questionnaire that was presented to
users after each game. The questionnaire asked
users to rate different statements about the NLG
D H M NA NM S T
task
success
9% 11% 13% 47% 30% 40% 3%
A A
B
C C C
D D
duration
888 470 407 344 435 467 266
A A A A A
B B B B B
C
distance
231 164 126 162 167 150 89
A A A A A A
B B B B B
actions
25 22 17 17 18 17 14
A A A A A A A
instructions
349 209 463 224 244 244 78
A A A A A A
B B
words per
instruction
15 11 16 6 10 6 18
A A
B
C
D
E E
Figure 4: Results for the objective measures.
system using a continuous slider. The slider posi-
tion was translated to a number between -100 and
100. Figs. 7 and 6 show the statements that users
were asked to rate as well as the results. These
results are based on all games, independent of the
success. We report the mean rating for each item,
and, as before, systems that do not share a letter,
were found to be significantly different (p< 0.05).
We used ANOVAs and post-hoc Tukey tests to test
for significance. Note that some items make a pos-
itive statement about the NLG system (e.g., Q1)
and some make a negative statement (e.g., Q2).
For negative statements, we report the reversed
scores, so that in Figs. 7 and 6 greater numbers are
always better, and systems in group A are always
better than systems in group B.
In addition to the items Q1?Q22, the ques-
tionnaire contained a statement about the over-
all instruction quality: ?Overall, the system gave
me good directions.? Furthermore notice that the
other items fall into two categories: items that as-
sess the quality of the instructions (Q1?Q15) and
items that assess the emotional affect of the in-
teraction (Q16?Q22). The ratings in these cate-
D H M NA NM S T
overall
quality
question
-33 -18 -12 36 18 19 -25
A
B B
C C C C
quality
measures
(summed)
-183 -148 -18 373 239 206 -44
A A A
B B B B
emotional
affect
measures
(summed)
-130 -103 -90 20 -5 0 -88
A A A A
B B B B B
C C C C C
Figure 5: Results for item assessing overall in-
struction quality and the aggregated quality and
emotional affect measures.
gories can be aggregated into just two ratings by
summing over them. Fig. 5 shows the results for
the overall question and the aggregated ratings for
quality measures and emotional affect measures.
The three systems with the highest task success
rate get rated highest for overall instruction qual-
ity. The aggregated quality measure also singles
out the same group of three systems.
4.4 Further analysis
In addition to the differences between NLG sys-
tems, some other factors also influence the out-
comes of our objective and subjective measures.
As in GIVE-1, we find that there is a significant
difference in task success rate for different evalua-
tion worlds and between users with different levels
of English proficiency. Fig. 8 illustrates the effect
of the different evaluation worlds on the task suc-
cess rate for different systems, and Fig. 9 shows
the effect that a player?s English skills have on the
task success rate. As in GIVE-1, some systems
seem to be more robust than others with respect to
changes in these factors.
None of the other factors we looked at (gender,
age, and computer expertise) have a significant ef-
fect on the task success rate. With a few excep-
tions the other objective measures were not influ-
enced by these demographic factors either. How-
ever, we do find a significant effect of age on the
time and number of actions a player needs to re-
trieve the trophy: younger players are faster and
need fewer actions. And we find that women travel
a significantly shorter distance than men on their
way to the trophy. Interestingly, we do not find
D H M NA NM S T
Q1: The system used words and phrases
that were easy to understand.
45 26 41 62 54 58 46
A A A A
B B B B
C C C
Q2: I had to re-read instructions to under-
stand what I needed to do.
-26 -9 3 40 8 19 0
A
B B B B
C C C
D D
Q3: The system gave me useful feedback
about my progress.
-17 -30 -31 9 11 -13 -27
A A
B B B B
C C C C
Q4: I was confused about what to do next.
-35 -27 -18 29 9 5 -31
A
B B
C C C C
Q5: I was confused about which direction
to go in.
-32 -20 -16 21 8 3 -25
A A
B B
C C C C
Q6: I had no difficulty with identifying
the objects the system described for me.
-21 -11 -5 18 13 20 -21
A A A
B B
C C C C
Q7: The system gave me a lot of unnec-
essary information.
-22 -9 6 15 10 10 -6
A A A A
B B B B
C C C
D D D
D H M NA NM S T
Q8: The system gave me too much infor-
mation all at once.
-28 -8 9 31 8 21 15
A A A
B B B B
C C
Q9: The system immediately offered help
when I was in trouble.
-15 -13 -13 32 3 -5 -23
A
B B B B B
C C C C
Q10: The system sent instructions too
late.
15 15 9 38 39 14 8
A A
B B B B B
Q11: The system?s instructions were de-
livered too early.
15 5 21 39 12 30 28
A A A
B B B B
C C C C
D D D D
Q12: The system?s instructions were vis-
ible long enough for me to read them.
-67 -21 -19 6 -14 0 -18
A A
B B B
C C C C
D
Q13: The system?s instructions were
clearly worded.
-20 -9 1 32 23 26 6
A A A
B B B
C C C
D D
Q14: The system?s instructions sounded
robotic.
16 -6 8 -4 -1 5 1
A A A A A A
B B B B B B
Q15: The system?s instructions were
repetitive.
-28 -26 -11 -31 -28 -26 -23
A A A A A
B B B B B B
Figure 7: Results for the subjective measures assessing the quality of the instructions.
D H M NA NM S T
Q16: I really wanted to find that trophy.
-10 -13 -9 -11 -8 -7 -12
A A A A A A A
Q17: I lost track of time while solving the
overall task.
-13 -18 -21 -16 -18 -11 -20
A A A A A A A
Q18: I enjoyed solving the overall task.
-21 -23 -20 -8 -4 -5 -21
A A A A A A
B B B B B
Q19: Interacting with the system was re-
ally annoying.
-14 -20 -12 8 -2 -2 -14
A A A
B B B B B
C C C C
Q20: I would recommend this game to a
friend.
-36 -39 -31 -30 -25 -24 -31
A A A A A A A
Q21: The system was very friendly.
0 -1 5 30 20 19 5
A A A
B B B B
C C C C
D D D D
Q22: I felt I could trust the system?s in-
structions.
-21 -6 -3 37 23 21 -13
A A A
B B B B
Figure 6: Results for the subjective measures as-
sessing the emotional affect of the instructions.
Figure 8: Effect of the evaluation worlds on the
success rate of the NLG systems.
Figure 9: Effect of the players? English skills on
the success rate of the NLG systems.
a significant effect of gender on the time players
need to retrieve the trophy as in GIVE-1 (although
the mean duration is somewhat higher for female
than for male players; 481 vs. 438 seconds).
5 Conclusion
In this paper, we have described the setup and re-
sults of the Second GIVE Challenge. Altogether,
we collected 1825 valid games for seven NLG sys-
tems over a period of three months. Given that this
is a 50% increase over GIVE-1, we feel that this
further justifies our basic experimental methodol-
ogy. As we are writing this, we are preparing de-
tailed results and analyses for each participating
team, which we hope will help them understand
and improve the performance of their systems.
The success rate is substantially worse in GIVE-
2 than in GIVE-1. This is probably due to the
Figure 10: Points at which players lose/cancel.
harder task (free movement) explained in Sec-
tion 2 and to the more complex evaluation worlds
(see Section 3.3). It was our intention to make
GIVE-2 more difficult, although we did not antic-
ipate such a dramatic drop in performance. GIVE-
2.5 next year will use the same task as GIVE-2 and
we hope to see an increase in task success as the
participating research teams learn from this year?s
results.
It is also noticeable that players gave mostly
negative ratings in response to statements about
immersion and engagement (Q16-Q20). We dis-
cussed last year how to make the task more engag-
ing on the one hand and how to manage expecta-
tions on the other hand, but none of the suggested
solutions ended up being implemented. It seems
that we need to revisit this issue.
Another indication that the task may not be able
to capture participants is that the vast majority of
cancelled and lost games end in the very begin-
ning. To analyze at what point players lose or give
up, we divide the game into phases demarcated
by manipulations of buttons that belong to the 6-
button safe sequence. Fig. 10 illustrates in which
phase of the game players lose or cancel.
We are currently preparing the GIVE-2.5 Chal-
lenge, which will take place in 2010-11. GIVE-2.5
will be very similar to GIVE-2, so that GIVE-2
systems will be able to participate with only mi-
nor changes. In order to support the development
of GIVE-2.5 systems, we have collected a multi-
lingual corpus of written English and German in-
structions in the GIVE-2 environment (Gargett et
al., 2010). We expect that GIVE-3 will then extend
the GIVE task substantially, perhaps in the direc-
tion of full dialogue or of multimodal interaction.
Acknowledgments. GIVE-2 was only possible
through the support and hard work of a number of
colleagues, especially Konstantina Garoufi (who
handled the website and other publicity-related is-
sues), Ielka van der Sluis (who contributed to the
design of the GIVE-2 questionnaire), and several
student assistants who programmed parts of the
GIVE-2 system. We thank the press offices of
Saarland University, the University of Edinburgh,
and Macquarie University for their helpful press
releases. We also thank the organizers of Gener-
ation Challenges 2010 and INLG 2010 for their
support and the opportunity to present our results,
and the seven participating research teams for their
contributions.
References
Andrew Gargett, Konstantina Garoufi, Alexander
Koller, and Kristina Striegnitz. 2010. The GIVE-
2 corpus of giving instructions in virtual environ-
ments. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC), Malta.
S. D. Gosling, S. Vazire, S. Srivastava, and O. P. John.
2004. Should we trust Web-based studies? A com-
parative analysis of six preconceptions about Inter-
net questionnaires. American Psychologist, 59:93?
104.
A. Koller, D. Byron, J. Cassell, R. Dale, J. Moore,
J. Oberlander, and K. Striegnitz. 2009a. The soft-
ware architecture for the first challenge on generat-
ing instructions in virtual environments. In Proceed-
ings of the EACL-09 Demo Session.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, Sara Dalzel-Job, Jo-
hanna Moore, and Jon Oberlander. 2009b. Validat-
ing the web-based evaluation of NLG systems. In
Proceedings of ACL-IJCNLP 2009 (Short Papers),
Singapore.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, Johanna Moore, and
Jon Oberlander. 2010. The first challenge on
generating instructions in virtual environments. In
E. Krahmer and M. Theune, editors, Empirical
Methods in Natural Language Generation, volume
5790 of LNCS, pages 337?361. Springer.
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 166?173,
Dublin, Ireland, August 23, 2014.
Dimensions of Metaphorical Meaning
Andrew Gargett
?
, Josef Ruppenhofer
?
and John Barnden
?
?
University of Birmingham
United Kingdom
{a.d.gargett|j.a.barnden}@cs.bham.ac.uk
?
Hildesheim University
Germany
ruppenho@uni-hildesheim.de
Abstract
Recent work suggests that concreteness and imageability play an important role in the mean-
ings of figurative expressions. We investigate this idea in several ways. First, we try to define
more precisely the context within which a figurative expression may occur, by parsing a corpus
annotated for metaphor. Next, we add both concreteness and imageability as ?features? to the
parsed metaphor corpus, by marking up words in this corpus using a psycholinguistic database of
scores for concreteness and imageability. Finally, we carry out detailed statistical analyses of the
augmented version of the original metaphor corpus, cross-matching the features of concreteness
and imageability with others in the corpus such as parts of speech and dependency relations, in
order to investigate in detail the use of such features in predicting whether a given expression is
metaphorical or not.
1 Introduction
Figurative language plays an important role in ?grounding? our communication in the world around us.
Being able to talk metaphorically about ?the journey of life?, ?getting into a relationship?, whether there
are ?strings attached? to a contract, or even just ?surfing the internet?, are important and useful aspects of
everyday discourse. Recent work on such phenomena has pursued this kind of grounding in interesting
directions, in particular, treating it as a way of injecting meanings that are somehow more ?concrete?
into daily discourse (Neuman et al., 2013; Turney et al., 2011; Tsvetkov et al., 2013), or else as a way
of expressing abstract ideas in terms of concepts that are more ?imageable?, where imageability can be
defined as how easily a word can evoke mental imagery, (Cacciari and Glucksberg, 1995; Gibbs, 2006;
Urena and Faber, 2010). It should be noted that while it is generally accepted that imageability and
concreteness are highly correlated, recent work has shown they are contrastive, in particular, in their
interaction with additional cognitive dimensions such as affective states, so that they ?can no longer be
considered interchangeable constructs? (Dellantonio et al., 2014).
When someone describes love as a journey, or life as a test, one possible way of thinking about what
they are doing is that they are trying to cast a fairly abstract idea or concept, such as love or life, in
terms of more concrete or imageable experiences or concepts, such as a journey or a test. More formally,
metaphor can be characterized as the mapping of properties from a ?source? domain concept (typically
more concrete) on to a ?target? domain concept (typically more abstract). However, despite the ease
with which people understand both established metaphors such as these, or even more novel ones
1
,
and despite well-established findings about the ubiquity of metaphor in everyday discourse (Lakoff and
Johnson, 1980), explicit and testable proposals for the mechanisms underlying such forms of expression
remain elusive.
When looking for such mechanisms, it seems natural to start with the patterns of language that so
effectively convey metaphorical meanings. Along these lines, Deignan (2006) argues that:
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
Consider how readily one can make sense of a novel, yet metaphorical utterance, such as ?life is a box of chocolates? (from
a recent film), despite never having heard it before.
166
[M]etaphorical uses of words show differences in their grammatical behavior, or even their
word class, when compared to their literal use. In addition, it shows that metaphorical uses of
a word commonly appear in distinctive and relatively fixed syntactic patterns.
Focusing on word class of figurative expressions, so-called content words, such as nouns, adjectives
and verbs, have long been considered to more strongly convey figurative meanings than so-called func-
tion words, such as prepositions (Neuman et al., 2013; Tsvetkov et al., 2013). Yet, Steen et al. (2010)
find prepositions within figurative expressions to be as prevalent as content words such as nouns and
verbs, and indeed, for particular genres (such as academic texts) prepositions are the most frequently
attested part of speech for figurative expressions.
Further, there has been work on the interaction between metaphorical expressions and syntactically
defined contexts (e.g. phrase, clause, sentence). For example, Neuman et al. (2013) investigate how
metaphorical expressions apparently pattern by syntactically definable types, specifically: Type I, where
?a subject noun is associated with an object noun via a form of the copula verb to be? (e.g. ?God is a
king?), Type II having the verb as ?the focus of the metaphorical use representing the act of a subject
noun on an object noun? (e.g. ?The war absorbed his energy?), and Type III ?involve an adjective-
noun phrase? (e.g. ?sweet girl?). While such work yields a useful typology of figurative expressions,
such investigations into the syntactic patterns of figurative forms of expression is far from exhaustive. It
would be useful to take this further somewhat, with a more rigorous, syntactically precise definition of
the context of occurrence of figurative language.
Motivated by the above considerations, we have begun investigating the interaction of concreteness
and imageability with figurative meanings in several ways. This paper reports the initial stages of this
ongoing work into the dimensions of meaning of figurative language such as metaphor. As part of this
work, we have attempted to define more precisely the context within which a figurative expression may
occur, by parsing a corpus annotated for metaphor, the Vrije University Amsterdam Metaphor Corpus
(VUAMC) (Steen et al., 2010), using an off the shelf dependency parser, the Mate parser (Bohnet, 2010).
In addition, we add both concreteness and imageability as ?features? to the dependency parsed metaphor
corpus, by marking up words in this corpus using a psycholinguistic database of scores for concreteness
and imageability, the MRC Psycholinguistic Database (Wilson, 1988). In this paper, we report detailed
statistical analyses we have carried out of the resulting data set, cross-matching the features of concrete-
ness and imageability with others in the corpus such as parts of speech (PsOS) and dependency relations,
in order to investigate in detail the use of such features in determining whether a given expression is
metaphorical or not.
2 Method
2.1 Data
Our data comes from the Vrije University Amsterdam Metaphor Corpus (VUAMC), consisting of ap-
proximately 188,000 words selected from the British National Corpus-Baby (BNC-Baby), and annotated
for metaphor using the Metaphor Identification Procedure (MIP) (Steen et al., 2010). The corpus has four
registers, of between 44,000 and 50,000 words each: academic texts, news texts, fiction, and conversa-
tions. We have chosen this corpus because of its broad coverage and its rich metaphorical annotation.
2.2 Procedure
PRE-PROCESSING. We have enriched the VUAMC in several ways. First, we have parsed the corpus
using the graph-based version of the Mate tools dependency parser (Bohnet, 2010), adding rich syntactic
information.
2
Second, we have incorporated the MRC Psycholinguistic Database
3
(Wilson, 1988), a
dictionary of 150,837 words, with different subsets of these words having been rated by human subjects
in psycholinguistic experiments. Of special note, the database includes 4,295 words rated with degrees of
abstractness, these ratings ranging from 158 (meaning highly abstract) to 670 (meaning highly concrete),
2
https://code.google.com/p/mate-tools/
3
http://ota.oucs.ox.ac.uk/headers/1054.xml
167
and also 9,240 words rated for degrees of imageability, which can be defined as how easily a word can
evoke mental imagery, these ratings also ranging between 100 and 700 (a higher score indicating greater
imageability). It should be noted that it has long been known that the concreteness and imageability
scores are highly correlated (Paivio et al., 1968), however, there are interesting differences between
these sets of scores (Dellantonio et al., 2014), and we are currently investigating these differences in
further studies (see Section (4) below). These scores have been used extensively for work that is similar
to ours, e.g. (Neuman et al., 2013; Turney et al., 2011; Tsvetkov et al., 2013), and while our work is also
largely computational in approach, a significant component of our research is devoted to investigating in
some detail the cognitive aspects of figurative meanings.
EXPERIMENTAL DESIGN. We carried out five studies, all beginning with pre-processing tasks to pre-
pare the data (additional to those listed immediately above, undertaken to prepare the entire corpus for
these studies). We list the aims, details of pre-processing, and hypotheses below.
Study 1. This study initiated the investigation, and guided the setting up of the computational frame-
work for our broader research activities. The VUAMC was extended with dependency information from
the Mate dependency parser, enabling extraction of both dependency information and metaphorical anno-
tation for each VUAMC word.
4
Hypotheses: H
1
= nouns are more prevalent in metaphorical expressions
than verbs, verbs more than adjectives, adjectives more than prepositions; H
2
= metaphorical expressions
are more likely to occur in sentences in which other metaphorical expressions occur.
Study 2. This study aimed to evaluate claims about syntactically-defined metaphor types (Neuman et
al., 2013), and search for other types. The structure of a sentence revealed by a dependency parse is
based on the relation between a word, known as a head, and its dependents. This extended VUAMC data
provided variables for metaphor types I, II and III, respectively, Noun-BE-Noun, Noun-ActiveVerb-Noun,
and Adjective-Noun, as well as the discovery of additional metaphor types.
Study 3. Going further than Studies 1 and 2, this study extended the VUAMC data with MRC con-
creteness and imageability scores, plus further processing of the VUAMC corpus, assigning MRC scores
to each item in this corpus. Note here that the VUAMC data was examined word-by-word (rather than
sentence-by-sentence, as for Study 2). However, the VUAMC data set is much larger than the MRC
data set, so that many VUAMC words have no MRC scores. To smooth this discrepancy, for this initial
stage of our investigations, we have implemented the fairly rudimentary approach of calculating global
MRC scores by: first, from VUAMC words with MRC scores, a global average MRC score for each part
of speech of the VUAMC data was calculated, and second, those VUAMC words without MRC scores
(i.e. missing from the MRC database) were assigned a global score based on their part of speech. Of
course, a range of possible smoothing strategies are available, and while at this stage we are employing
a rather crude averaging of the score, this is an area we intend to investigate further in follow-up studies,
inspired by the more sophisticated methods that have been implemented by others, e.g. (Feng et al.,
2011; Tsvetkov et al., 2013).
5
For this study, we sought to answer the following two questions: Do
concreteness and imageability scores correlate with metaphoricity of expressions? Do concreteness and
imageability scores correlate with parts of speech of metaphorical expressions?
Study 4. This study replicated Study 3, but also considered the data sentence-by-sentence (cf. Study
2), to integrate syntactic information and MRC score. Examining MRC scores across syntactically fine-
grained contexts, enabled collecting information about heads, their dependent/s, as well as the depen-
dency relation/s, and this information could then be examined to see if it helped to distinguish between
literal and nonliteral items. This approach enables us to investigate in detail the contexts in which con-
creteness and imageability with figurative meanings, a key aim of our work, as pointed out in Section (1).
Hypotheses: H
3
= metaphorical expressions are more likely to occur in sentences where the head is more
4
For more details on the VUAMC categories, see: http://www.natcorp.ox.ac.uk/docs.
5
This work is part of a larger project, http://www.cs.bham.ac.uk/
?
gargetad/genmeta-about.html,
which aims to annotate larger web-based corpora of discourse on illness and political conflict.
168
Figure 1: Plots of concreteness vs. imageability scores for literal vs. nonliteral words in the VUAMC
(Conc=concreteness, Imag=imageability, NL=nonliteral, L=literal)
concrete than the dependent/s; H
4
= metaphorical expressions are more likely to occur in sentences where
the head is more imageable than the dependent/s.
Study 5. Finally, this study finished by examining the relative importance of the variables identified
so far, for predicting literal vs. nonliteral expressions, another key aim of our work (as mentioned in
Section (1)). We implemented this study through building and evaluating a series of logistic regression
models.
3 Results
3.1 Study 1
The first hypothesis listed for this study above has not been refuted, with the percentage of all non-
literal sentences in our collection having only one nonliteral item being 27%, while the percentage
of all nonliteral sentences having more than one nonliteral item is 73%: so after finding one nonlit-
eral item in a sentence, we can expect to find more. Regarding the second hypothesis, our data set
had the following proportions of occurrence of nonliteral items according to parts of speech: Adjec-
tives=10.8%, Prepositions=28%, Nouns=22.5%, Verbs=27%, Adverbs=5%, Pronouns=0.2%, Conjunc-
tions=0.5%, Other=6%. Consistent with Steen et al. (2010), that function words can occur more fre-
quently than content words in metaphorical expressions, we found prepositions to be far more prevalent
than adjectives in such expressions, and occur about as frequently as verbs.
3.2 Study 2
We found the following percentages of metaphor types (across all metaphors): Type I = 3.06%, Type
II = 33.53%, Type III = 7.56% (note the reversal for Type II vs. Type III, contrary to (Neuman et al.,
2013)). Such differences may be due to differences in data sets, as well as different syntactic models.
6
Additionally, we found a pattern of expression we have dubbed ?Type IV? metaphors, consisting of
preposition as head, together with noun phrase dependents (e.g. ?at the end of the decade?, ?after the
break-up?): these account for 35.53% of the total occurrence of metaphors.
3.3 Study 3
The boxplots in Figure (1) compare concreteness and imageability scores for nonliteral vs. literal items,
suggesting nonliteral and literal items are indistinguishable from one another with respect to their con-
creteness and imageability scores. Next, we further categorise our data according to parts of speech, the
boxplots in Figure (2) showing results for concreteness, and the boxplots Figure (3) presenting results for
imageability ? these figures suggest literal and nonliteral items can be better distinguished, with respect
to their concreteness and imageability scores, by increasing the granularity of annotation of the context
(e.g. by including parts of speech). Note that imageability scores for prepositions seem to show the
6
Neuman et al. (2013) used the Stanford Dependency Parser (De Marneffe and Manning, 2008).
169
Figure 2: Plots of concreteness scores for literal vs. nonliteral/metaphorical words in the VUAMC,
grouped by parts of speech (L=literal, NL=nonliteral, N=noun, V=verb, P=preposition)
Figure 3: Plots of imageability scores for literal vs. nonliteral/metaphorical words in the VUAMC,
grouped by parts of speech (L=literal, NL=nonliteral, N=noun, V=verb, P=preposition)
clearest distinction between literal vs. nonliteral items. But can we do better? What further categories
in the data should we focus on in order to achieve even clearer distinctions between literal vs. nonliteral
items?
3.4 Study 4
Figures (4) and (5) show the variation that can be achieved by making a more fine-grained distinction
within our data set between heads and their dependents, plus MRC scores of each. Figure (4) shows that
concreteness scores enable distinguishing between literal and nonliteral items for some parts of speech,
such as nouns, where nonliteral heads have higher MRC scores than their dependents, distinct from
literal head nouns (verbs appear to make no such a distinction). While literal and nonliteral head prepo-
sitions both seem indistinguishable from their dependents in terms of concreteness scores, nonliteral
head prepositions seem to have imageability scores quite distinct from their dependents.
3.5 Study 5
Based on our previous studies, we here examine the following 5 independent variables: POS = part
of speech of the head, C Head = concreteness score of the head, I Head = imageability score of the
head, C Dep = average concreteness score of the dependents, I Dep = average imageability score of
the dependents. Table (1) sets out the results for 7 logistic regression models we tested, and formulas
representing these models M1 to M7 are as follows (Nonliteral of course being the dependent variable,
its values being either ?yes, this is nonliteral? or ?no, this is not nonliteral?):
170
Figure 4: Plots of concreteness scores for literal vs. nonliteral/metaphorical heads vs. their dependents,
in the VUAMC, grouped by parts of speech (L=literal, NL=nonliteral, N=noun, V=verb, P=preposition,
h=head, d=dependents)
Figure 5: Plots of imageability scores for literal vs. nonliteral/metaphorical heads vs. their dependents,
in the VUAMC, grouped by parts of speech (L=literal, NL=nonliteral, N=noun, V=verb, P=preposition,
h=head, d=dependents)
M1: Nonliteral ? POS + C Head + I Head + C Dep + I Dep
M2: Nonliteral ? C Head + I Head
M3: Nonliteral ? POS + C Head + I Head
M4: Nonliteral ? POS + C Head + C Dep + I Dep
M5: Nonliteral ? POS + I Head + C Dep + I Dep
M6: Nonliteral ? POS + C Head + C Dep
M7: Nonliteral ? POS + I Head + I Dep
In Table (1), p-values have three categories, p < .0001, p < .001, or p < .01: this value represents a
test of the null hypothesis that the coefficient of the variable being considered is zero, i.e., the variable
has no effect on the model (a lower p-value is stronger evidence for rejecting the null hypothesis). Where
variables have significantly low p-values, Table (1) in effect presents optimal combinations of variables
for specific models, with low p-values indicating variables likely to have a greater effect on the model
and so more directly reflecting changes in the independent variable. For example, Table (1) shows that
models selecting MRC scores for heads (e.g. C Head) with the same kinds of scores for their dependents
(e.g.C Dep) seem most successful, which is perhaps to be expected, in light of studies 3 and 4.
It should be noted that no single variable models are reported here, since (1) while models such as
Nonliteral ? I Head and Nonliteral ? C Head indeed achieve significant p-values, others such
as Nonliteral ? I Dep and Nonliteral ? C Dep do not, (2) single variable models do not explain
Figure (1), nor indeed the variation for multiple variable contexts as exhibited by Figures (4) and (5).
We are currently comparing single vs. multiple variables, and early machine learning results suggest
multiple variable models are superior compared to single variable models as predictive tools.
171
Variables M1 M2 M3 M4 M5 M6 M7
Intercept -7.534*** -2.609* -9.088*** -7.836*** -7.522*** -7.816*** -7.614***
POS 9.265*** 8.884*** 9.330*** 9.163*** 9.316*** 9.082***
C Head 1.555 0.288 1.382 4.844*** 4.876***
I Head 0.459 -1.312 0.513 4.611*** 4.660***
C Dep -1.964 -1.982 -1.919 -3.799***
I Dep 0.682 0.699 0.660 -3.325**
Table 1: Results (t scores) of logistic regression model for predicting non/literal items
from the VUAMC, n=1855 (nb. p-values are shown by asterisks, ***=p<.0001, **=p<.001, *=p<.01)
4 Discussion
This paper reports results from ongoing work we are carrying out toward building a tool for identi-
fying metaphorical expressions in everyday discourse, through fine-grained analysis of the dimensions
of meaning of such expressions. We have presented evidence that detecting metaphor can usefully be
pursued as the problem of modeling how conceptual meanings such as concreteness and imageability,
interact with syntactically definable linguistic contexts. We increase the granularity of our analyses by
incorporating detailed syntactic information about the context in which metaphorical expressions occur.
By increasing the granularity of context, we were able to distinguish between metaphorical expressions
according to different parts of speech, and further, according to heads and their dependents.
We were able to show that for the purpose of determining whether a specific linguistic expression is
metaphorical or not, the most successful approach seems to be to combine information about parts of
speech with either concreteness scores for both heads and their dependents, or else with imageability
scores for both heads and their dependents. Note that this result is in part a direct consequence of the
high correlation between concreteness and imageability, whereby their combination will typically not
result in an optimal regression model. Such high correlation between concreteness and imageability has
been understood for some time (Paivio et al., 1968), yet, of course, there is good reason to think that
concreteness and imageability do not in fact pattern identically, and that they are at some level distinct
phenomena. Indeed, concreteness and imageability are likely related to distinct cognitive systems, and
we are currently undertaking further investigations in this direction.
Finally, we should note that while our results are likely to be language-specific, it is reasonable to
assume the general approach could be replicated across languages. We are currently planning such
cross-linguistic research for future work.
Acknowledgements
We acknowledge financial support through a Marie Curie International Incoming Fellowship (project
330569) awarded to two of the authors (Gargett as fellow, Barnden as P.I.). We would also like to
sincerely thank the reviewers for many very useful comments; of course, we assume full responsibility
for the final version.
References
Bernd Bohnet. 2010. Very high accuracy and fast dependency parsing is not a contradiction. In The 23rd Interna-
tional Conference on Computational Linguistics (COLING 2010), Beijing, China.
Christina Cacciari and Sam Glucksberg. 1995. Imaging idiomatic expressions: literal or figurative meanings. In
Martin Everaert, Erik-Jan van der Linden, Andr Schenk, and Rober Schreuder, editors, Idioms: Structural and
psychological perspectives, pages 43?56. Lawrence Erlbaum.
Marie-Catherine De Marneffe and Christopher D Manning. 2008. The stanford typed dependencies representation.
In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages
1?8. Association for Computational Linguistics.
172
Alice Deignan. 2006. The grammar of linguistic metaphors. In Anatol Stefanowitsch and Stefan Gries, editors,
Corpus-based approaches to metaphor and metonymy, pages 106?122. Walter de Gruyter.
Sara Dellantonio, Claudio Mulatti, Luigi Pastore, and Remo Job. 2014. Measuring inconsistencies can lead you
forward: The case of imageability and concreteness ratings. Frontiers in Psychology, 5(708).
Shi Feng, Zhiqiang Cai, Scott A Crossley, and Danielle S McNamara. 2011. Simulating human ratings on word
concreteness. In FLAIRS Conference.
Raymond W Gibbs. 2006. Metaphor interpretation as embodied simulation. Mind & Language, 21(3):434?458.
George Lakoff and Mark Johnson. 1980. Metaphors We Live By. University of Chicago.
Yair Neuman, Dan Assaf, Yohai Cohen, Mark Last, Shlomo Argamon, Newton Howard, and Ophir Frieder. 2013.
Metaphor identification in large texts corpora. PloS one, 8(4):e62343.
Allan Paivio, John C Yuille, and Stephen A Madigan. 1968. Concreteness, imagery, and meaningfulness values
for 925 nouns. Journal of experimental psychology, 76(1, pt.2):1?25.
G.J. Steen, A.G. Dorst, J.B. Herrmann, A.A. Kaal, and T. Krennmayr. 2010. A Method for Linguistic Metaphor
Identification: From MIP to MIPVU. Converging Evidence in Language and Communication Research. John
Benjamins Publishing Company.
Yulia Tsvetkov, Elena Mukomel, and Anatole Gershman. 2013. Cross-lingual metaphor detection using common
semantic features. In Proceedings of the First Workshop on Metaphor in NLP, pages 45?51, Atlanta, Georgia,
June. Association for Computational Linguistics.
Peter D Turney, Yair Neuman, Dan Assaf, and Yohai Cohen. 2011. Literal and metaphorical sense identification
through concrete and abstract context. In Proceedings of the 2011 Conference on the Empirical Methods in
Natural Language Processing, pages 680?690.
Jose Manuel Urena and Pamela Faber. 2010. Reviewing imagery in resemblance and non-resemblance metaphors.
Cognitive Linguistics, 21(1):123?149.
Michael Wilson. 1988. MRC psycholinguistic database: Machine-usable dictionary, version 2.00. Behavior
Research Methods, Instruments, & Computers, 20(1):6?10.
173
