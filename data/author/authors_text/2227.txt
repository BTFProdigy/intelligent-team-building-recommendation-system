The Syntactically Annotated ICE Corpus
and the Automatic Induction of a Formal Grammar
Alex Chengyu Fang
Department of Chinese, Translation and Linguistics
City University of Hong Kong
acfang@cityu.edu.hk
Abstract
The International Corpus of English is
a corpus of national and regional
varieties of English. The mega-word
British component has been con-
structed, grammatically tagged, and
syntactically parsed. This article is a
description of work that aims at the
automatic induction of a wide-coverage
grammar from this corpus as well as an
empirical evaluation of the grammar. It
first of all describes the corpus and its
annotation schemes and then presents
empirical statistics for the grammar. I
will then evaluate the coverage and the
accuracy of such a grammar when
applied automatically in a parsing
system. Results show that the grammar
enabled the parser to achieve 86.1%
recall rate and 83.5% precision rate.
1 Introduction
The International Corpus of English (ICE) is a
project that aims at the construction of a
collection of corpora of English in countries and
regions where English is used either as a first or
as an official language (Greenbaum 1992). Each
component corpus comprises one million words
of both written and transcribed spoken samples
that are then annotated at grammatical and
syntactic levels. The British component of the
ICE corpus was used to automatically induce a
large formal grammar, which was subsequently
used in a robust parsing system. In what follows,
this article will first of all describe the
annotation schemes for the corpus and the
evaluation of a formal grammar automatically
induced from the corpus in terms of its potential
coverage when tested with empirical data.
Finally, this article will present an evaluation of
the grammar through its application in a robust
parsing system in terms of labelling and
bracketing accuracies.
1.1 The ICE wordclass annotation scheme
There are altogether 22 head tags and 71 features
in the ICE wordclass tagging scheme, resulting in
about 270 grammatically possible combinations.
Compared with 134 tags for LOB, 61 for BNC,
and 36 for Penn Treebank, the ICE tagset is
perhaps the most detailed in automatic appli-
cations. They cover all the major English word
classes and provide morphological, grammatical,
collocational, and sometimes syntactic inform-
ation. A typical ICE tag has two components: the
head tag and its features that bring out the
grammatical features of the associated word. For
instance, N(com,sing) indicates that the lexical
item associated with this tag is a common (com)
singular (sing) noun (N).
Tags that indicate phrasal collocations
include PREP(phras) and ADV(phras), pre-
positions (as in [1]) and adverbs (as in [2]) that are
frequently used in collocation with certain verbs
and adjectives:
[1] Thus the dogs? behaviour had been changed
because they associated the bell with the food.
[2] I had been filming The Paras at the time, and
Brian had had to come down to Wales with the
records.
Some tags, such as PROFM(so,cl) (pronominal
so representing a clause as in [3]) and
PRTCL(with) (particle with as in [4]), indicate
the presence of a clause; so in [3] signals an
49
abbreviated clause while with in [4] a non-finite
clause:
[3] If so, I?ll come and meet you at the station.
[4] The number by the arrows represents the
order of the pathway causing emotion, with the
cortex lastly having the emotion.
Examples [5]-[7] illustrate tags that note special
sentence structures. There in [5] is tagged as
EXTHERE, existential there that indicates a
marked sentence order. [6] is an example of the
cleft sentence (which explicitly marks the focus),
where it is tagged as CLEFTIT. [7] exemplifies
anticipatory it, which is tagged as ANTIT:
[5] There were two reasons for the secrecy.
[6] It is from this point onwards that Roman
Britain ceases to exist and the history of sub-
Roman Britain begins.
[7] Before trying to answer the question it is
worthwhile highlighting briefly some of the
differences between current historians.
The verb class is divided into auxiliaries and
lexical verbs. The auxiliary class notes modals,
perfect auxiliaries, passive auxiliaries, semi-
auxiliaries, and semip-auxiliaries (those followed
by -ing verbs). The lexical verbs are further
annotated according to their complementation
types. There are altogether seven types: complex
transitive, complex ditransitive, copular, dimono-
transitive, ditransitive, intransitive, mono-
transitive, and TRANS. Figure 1 shows the sub-
categorisations of the verb class.
Transitive
Intransitive
Lexical Verb
Mono-
transitive
Copula
Di-transitive
Di-mono-
transtive
Trans
Complex-
transitive
Figure 1: The ICE subcategorisation for verbs
The notation TRANS of the transitive verb class is
used in the ICE project to tag those transitive
verbs followed by a noun phrase that may be the
subject of the following non-finite clause. This
type of verb can be analysed differently according
to various tests into, for instance, monotransitives,
ditransitives and complex transitives. To avoid
arbitrary decisions, the complementing non-finite
clause is assigned a catch-all term ?transitive
complement? in parsing, and its preceding verb is
accordingly tagged as TRANS in order to avoid
making a decision on its transitivity type. This
verb type is best demonstrated by [8]-[11]:
[8] Just before Christmas, the producer of Going
Places, Irene Mallis, had asked me to make a
documentary on ?warm-up men?.
[9] They make others feel guilty and isolate
them.
[10] I can buy batteries for the tape - but I can see
myself spending a fortune!
[11]The person who booked me in had his
eyebrows shaved and replaced by straight black
painted lines and he had earrings, not only in his
ears but through his nose and lip!
In examples [8]-[11], asked, make, see, and had
are all complemented by non-finite clauses with
overt subjects, the main verbs of these non-finite
clauses being infinitive, present participle and past
participle.
As illustrated by examples [1]-[11], the ICE
tagging scheme has indeed gone beyond the
wordclass to provide some syntactic information
and has thus proved itself to be an expressive
and powerful means of pre-processing for
subsequent parsing.
1.2 The ICE parsing scheme
The ICE parsing scheme recognises five basic
syntactic phrases. They are adjective phrase
(AJP), adverb phrase (AVP), noun phrase (NP),
prepositional phrase (PP), and verb phrase (VP).
Each tree in the ICE parsing scheme is re-
presented as a functionally labelled hierarchy,
with features describing the characteristics of each
constituent, which is represented as a pair of
function-category labels. In the case of a terminal
node, the function-category descriptive labels are
appended by the lexical item itself in curly
brackets. Figure 2 is such a structure for [12].
[12]We will be introducing new exam systems for
both schools and universities.
50
According to Figure 2, we know that [12] is a
parsing unit (PU) realised by a clause (CL), which
governs three daughter nodes: SU NP (NP as
subject), VB VP (VP as verbal), and OD NP (NP
as direct object). Each of the three daughter nodes
are sub-branched until the leaves nodes with the
input tokens in curly brackets. The direct object
node, for example, has three immediate
constituents: NPPR AJP (AJP as NP pre-
modifier), NPHD N(com,plu) (plural common
noun as the NP head), and NPPO PP (PP as NP
post-modifier).
Figure 2: A parse tree for [12]
Note that in the same example, the head of the
complementing NP of the prepositional phrase is
initially analysed as a coordinated construct
(COORD), with two plural nouns as the conjoins
(CJ) and a coordinating conjunction as co-
ordinator (COOR).
In all, there are 58 non-terminal parsing
symbols in the ICE parsing scheme, compared
with 20 defined in the Penn Treebank project. The
Suzanne Treebank has 43 function/category
symbols, discounting those that are represented as
features in the ICE system.
2 The generation of a formal grammar
The British component of the ICE corpus,
annotated in fashions described above, has been
used to automatically generate a formal
grammar that has been subsequently applied in
an automatic parsing system to annotate the rest
of the corpus (Fang 1995, 1996, 1999). The
grammar consists of two sets of rules. The first
set describes the five canonical phrases (AJP,
AVP, NP, PP, VP) as sequences of grammatical
tags terminating at the head of the phrase. For
example, the sequence AUX(modal,pres)
AUX(prog,infin) V(montr,ingp) is a VP
rule describing instantiations such as will be
introducing in [12]. The second set describes the
clause as sequences of phrase types. The string
in [12], for instance, is described by a sequence
NP VP NP PP in the set of clausal rules.
To empirically characterise the grammar,
the syntactically parsed ICE corpus was divided
into ten equal parts according to the number of
component texts. One part was set aside for
testing, which was further divided into five test
sets. The remaining nine parts were used as
training data in a leave-one-out fashion. In this
way, the training data was used to generate 9
consecutive training sets, each increased by one
part over the previous set, with Set 1 formed of
one training set, Set 2 two training sets, and Set
3 three training sets, etc. The evaluation thus not
only aims to establish the potential coverage of
the grammar but also to indicate the function
between the coverage of the grammar and the
training data size.
Figure 3 shows the growth of the number of
phrase structure rules as a function of the growth
of training data size. The Y-axis indicates the
number of rules generated from the training data
and the X-axis the gradual increase of the
training data size.
0
2500
5000
7500
10000
12500
15000
1 2 3 4 5 6 7 8 9
AVP
AJP
NP
PP
VP
Figure 3: The number of phrase structure rules as a
function of growing training data size
It can be observed that AJP and AVP show only
a marginal increase in the number of different
rules with the increase of training data size,
therefore demonstrating a relatively small core
set. In comparison, VPs are more varied but still
exhibit a visible plateau of growth. The other
two phrases, NP and PP, show a much more
varied set of rules not only through their large
numbers (9,184 for NPs and 13,736 for PPs) but
also the sharp learning curve. There are many
reasons for the potentially large set of rules for
PPs since they structurally subsume the clause
as well as all the phrase types. Their large
number is therefore more or less expected. The
51
large set of NP rules is however a bit surprising
since they are often characterised, perhaps too
simplistically, as comprising a determiner group,
a premodifier group, and the noun head but the
grammar has 9,184 different rules for this phrase
type. While this phenomenon calls for further
investigations, we are concerned with only the
coverage issue for the moment in the current
article.
3 The coverage of the formal grammar
The coverage of the formal grammar is
evaluated through individual rule sets for the
five canonical phrase types separately. The
coverage by the clausal rules will also be report-
ed towards the end of this section.
3.1 The coverage of AJP rules
As Figure 4 suggests, the coverage of the
grammar, when tested with the five samples, is
consistently high ? all above 99% even when the
grammar was trained from only one ninth of the
training set. The increase of the size of the train-
ing set does not show significant enhancement
of the coverage.
99
99.2
99.4
99.6
99.8
100
1 2 3 4 5 6 7 8 9
Set 1
Set 2
Set 3
Set 4
Set 5
Figure 4: The coverage of AJP rules
3.2 The coverage of AVP rules
Like AJP rules, high coverage can be achieved
with a small training set since when trained with
only one ninth of the training data, the AVP
rules already showed a high coverage of above
99.4% and quickly approaching 100%. See
Figure 5.
99.4
99.5
99.6
99.7
99.8
99.9
100
1 2 3 4 5 6 7 8 9
Set 1
Set 2
Set 3
Set 4
Set 5
Figure 5: The coverage of AVP rules
3.3 The coverage of NP rules
Although lower than AVP and AJP discussed
above, the NP rules show a satisfactorily high
coverage when tested by the five samples. As
can be seen from Figure 6, the initial coverage
when trained with one ninth of the training data
is generally above 97%, rising proportionally as
the training data size increases, to about 99%.
This seems to suggest a mildly complex nature
of NP structures.
97
97.5
98
98.5
99
99.5
100
1 2 3 4 5 6 7 8 9
Set 1
Set 2
Set 3
Set 4
Set 5
Figure 6: The coverage of NP rules
3.4 The coverage of VP rules
VPs do not seem to pose significant challenge to
the parser. As Figure 7 indicates, the initial
coverage is all satisfactorily above 97.5%. Set 1
even achieved a coverage of over 98.5% when
the grammar was trained with only one ninth of
the training data. As the graph seems to suggest,
the learning curve arrives at a plateau when
trained with about half of the total training data,
suggesting a centralised use of the rules.
97.5
98
98.5
99
99.5
100
1 2 3 4 5 6 7 8 9
Set 1
Set 2
Set 3
Set 4
Set 5
Figure 7: The coverage of VP rules
3.5 The coverage of PP rules
As is obvious from Figure 8, PPs are perhaps the
most complex of the five phrases with an initial
coverage of just over 70%. The learning curve is
sharp, culminating between 85% and 90% with
the full training data set. As far as parser
construction is concerned, this phrase alone
deserves special attention since it explains much
of the structural complexity of the clause. Based
52
on this observation, a separate study was carried
out to automatically identify the syntactic
functions of PPs.
70
75
80
85
90
95
1 2 3 4 5 6 7 8 9
Set 1
Set 2
Set 3
Set 4
Set 5
Figure 8: The coverage of PP rules
3.6 The coverage of clausal rules
Clausal rules present the most challenging
problem since, as Figure 9 clearly indicates,
their coverage is all under 67% even when
trained with all of the training data. This
observation seems to reaffirm the usefulness of
rules at phrase level but the inadequacy of
clause structural rules. Indeed, it is intuitively
clear that the complexity of the sentence is
mainly the result of the combination of clauses
of various kinds.
55
57
59
61
63
65
67
1 2 3 4 5 6 7 8 9
Set 1
Set 2
Set 3
Set 4
Set 5
Figure 9: The coverage of CL rules
3.7 Discussion
This section presented an evaluation of the
grammar in terms of its coverage as a function
of growing training data size. As is shown, the
parsed corpus resulted in excellent grammar sets
for the canonical phrases, AJP, AVP, NP, PP,
and VP: except for PPs, all the phrase structure
rules achieved a wide coverage of about 99%.
The more varied set for PPs demonstrated a
coverage of nearly 90%, not as high as what is
achieved for the other phrases but still highly
satisfactory.
The coverage of clause structure rules, on
the other hand, showed a considerably poorer
performance compared with the phrases. When
all of the training data was used, these rules
covered just over 65% of the testing data.
In view of these empirical observations, it
can be reliably concluded that the corpus-based
grammar construction holds a promising
approach in that the phrase structure rules
generally have a high coverage when tested with
unseen data. The same approach has also raised
two questions at this stage: Does the high-
coverage grammar also demonstrate a high
precision of analysis? Is it possible to enhance
the coverage of the clause structure rules within
the current framework?
4 Evaluating the accuracy of analysis
The ICE project used two major annotation
tools: AUTASYS and the Survey Parser.
AUTASYS is an automatic wordclass tagging
system that applies the ICE tags to words in the
input text with an accuracy rate of about 94%
(Fang 1996a). The tagged text is then fed into
the Survey Parser for automated syntactic
analysis. The parsing model is one that tries to
identify an analogy between the input string and
a sentence is that already syntactically analysed
and stored in a database (Fang 1996b and 2000).
This parser is driven by the previously described
formal grammar for both phrasal and clausal
analysis. In this section, the formal grammar is
characterised through an empirical evaluation of
the accuracy of analysis by the Survey Parser.
4.1 The NIST evaluation scheme
The National Institute of Science and Tech-
nology (NIST) proposed an evaluation scheme
that looks at the following properties when
comparing recognition results with the correct
answer:
C
P
T
T
intsconstituenofnumber
intsconstituencorrectofnumberRateMatchCorrect =
C
P
T
T
intsconstituenofnumber
intsconstituendsubstituteofnumberRateonSubstituti =
C
P
T
T
intsconstituenofnumber
intsconstituendeletedofnumberRateDeletion =
intsconstitueninsertedofnumberInsertion PT=
C
P
T
T
intsconstituenofnumber
insertionsofnumberintsconstituencorrectofnumberRateCombined !=
Notably, the correct match rate is identical to the
labelled or bracketed recall rate. The commonly
used precision score is calculated as the total
number of correct nodes over the sum of correct,
substituted, and inserted nodes. The insertion
53
score, arguably, subsumes crossing brackets
errors since crossing brackets errors are caused
by the insertion of constituents even though not
every insertion causes an instance of crossing
brackets violation by definition. In this respect,
the crossing brackets score only implicitly hints
at the insertion problem while the insertion rate
of the NIST scheme explicitly addresses this
issue.
Because of the considerations above, the
evaluations to be reported in the next section
were conducted using the NIST scheme. To
objectively present the two sides of the same
coin, the NIST scheme was used to evaluate the
Survey Parser in terms of constituent labelling
and constituent bracketing before the two are
finally combined to yield performance scores.
In order to conduct a precise evaluation of
the performance of the parser, the experiments
look at the two aspects of the parse tree:
labelling accuracy and bracketing accuracy.
Labelling accuracy expresses how many
correctly labelled constituents there are per
hundred constituents and is intended to measure
how well the parser labels the constituents when
compared to the correct tree. Bracketing ac-
curacy attempts to measure the similarity of the
parser tree to that of the correct one by
expressing how many correctly bracketed
constituents there are per hundred constituents.
In this section, the NIST metric scheme will be
applied to the two properties separately before
an attempt is made to combine the two to assess
the overall performance of the Survey Parser.
The same set of test data described in the
previous section was used to create four test sets
of 1000 trees each to evaluate the performance
of the grammar induced from the training sets
described earlier.
4.2 Labelling Accuracy
To evaluate labelling accuracy with the NIST
scheme, the method is to view the labelled
constituents as a linear string with attachment
bracketing removed. For [12], as an example,
Figure 10 is a correct tree and Figure 11 is a
parser-produced tree.
[12] It was probably used in the Southern
States as well.
Figure 10: A correct tree for [12]
After removing the bracketed structure, we then
have two flattened sequences of constituent
labels and compare them using the NIST
scheme, which will yield the following statistics:
Total # sentences evaluated : 1
Total # constituent labels : 42
Total # correct matches : 37 (88.1%)
Total # labels substituted : 5 (11.9%)
Total # labels deleted : 0 ( 0.0%)
Total # labels inserted : 6
Overall labelling accuracy : 73.8%
Figure 11: A parser-produced tree for [12]
Accordingly, we may concretely claim that there
are 42 constituent labels according to the correct
tree, of which 37 (88.1%) are correctly labelled
by the parser, with 5 substitutions (11.9%), 0
deletion, and 6 insertions. The overall labelling
accuracy is then calculated as 73.8%.
A total of 4,000 trees, divided into four sets
of 1,000 each, were selected from the test data to
evaluate the labelling accuracy of the parser.
Empirical results show that the parser achieved
an overall labelling precision of over 80%.
54
Test Set 1 Test Set 2 Test Set 3 Test Set 4
# % # % # % # %
Tree 1000 1000 1000 1000
Node 31676 34095 31563 30140
Correct 27329 86.3 29263 85.8 27224 86.3 26048 86.4
Subs 3214 10.1 3630 10.6 3253 10.3 3084 10.2
Del 1133 3.6 1202 3.5 1086 3.4 1008 3.3
Ins 2021 2316 1923 1839
Prec. 83.9 83.1 84.1 84.1
Overall 79.9 79.0 80.2 80.3
Table 1: Labelling accuracy
Table 1 shows that the Survey Parser scored
86% or better in terms of correct match (labelled
recall) and nearly 84% in terms of labelled
precision rate for the four sets. About 10% of
the constituent labels are wrong (Subs) with a
deletion rate (Del) of about 3.5%. Counting
insertions (Ins), the overall labelling accuracy by
the parser is around 80%.
4.3 Bracketing Accuracy
A second aspect of the evaluation of the
grammar through the use of the Survey Parser
involves the measuring of its attachment
precision, an attempt to characterise the
similarity of the parser-produced hierarchical
structure to that of the correct parse tree. To
estimate the precision of constituent attachment
of a tree, a linear representation of the
hierarchical structure of the parse tree is design-
ed which ensures that wrongly attached non-
terminal nodes are penalised only once if their
sister and daughter nodes are correctly aligned.
Table 2 shows that the parser achieved
nearly 86% for the bracketed correct match and
82.8% for bracketing precision. Considering
insertions and deletions, the overall accuracy
according to the NIST scheme is about 77%.
This indicates that for every 100 bracket pairs
77 are correct, with 23 substituted, deleted, or
inserted. In other words, for a tree of 100
constituents, 23 edits are needed to conform to
the correct tree structure.
Test Set 1 Test Set 2 Test Set 3 Test Set 4
# % # % # % # %
Tree 1000 1000 1000 1000
Node 12451 13390 12411 11858
Correct 10679 85.8 11402 85.2 10620 85.6 10271 86.6
Subs 1088 8.7 1249 9.3 1115 9.0 968 8.2
Del 648 5.5 739 5.5 676 5.4 619 5.2
Ins 1127 1297 1092 1029
Prec 82.8 81.7 82.8 83.7
Overall 76.7 75.5 76.8 77.9
Table 2: Bracketing accuracy
4.4 Combined accuracy
The combined score for both labelling and
bracketing accuracy is achieved through re-
presenting both constituent labelling and un-
labelled bracketing in a linear string described in
the previous sections.
Table 3 gives the total number of trees in
the four test sets and the total number of
constituents. The number of correct matches,
substitutions, insertions and deletions are in-
dicated and combined scores computed ac-
cordingly. The table shows that the parser
scored 86% and 83.5% respectively for labelled
recall and precision. It is also shown that the
parser achieved an overall performance of about
79%. Considering that the scoring program
tends to underestimate the success rate, it is
reasonable to assume a real overall combined
performance of 80%.
Test Set 1 Test Set 2 Test Set 3 Test Set 4
# % # % # % # %
Tree 1000 1000 1000 1000
Node 44127 47485 43974 41998
Correct 38008 86.1 40665 85.6 37844 86.1 36319 86.5
Subs 4302 9.7 4879 10.3 4368 9.9 4052 9.6
Del 1781 4.0 1941 4.1 1762 4.0 1627 3.9
Ins 3148 3613 3015 2868
Prec 83.6 82.7 83.7 83.9
Overall 79.0 78.0 79.2 79.6
Table 3: Combined accuracy
4.5 Discussion
Although the scores for the grammar and the
parser look both encouraging and promising, it
is difficult to draw straightforward comparisons
with other systems. Charniak (2000) reports a
maximum entropy inspired parser that scored
90.1% average precision/recall when trained and
tested with sentences from the Wall Street
Journal corpus (WSJ). While the difference in
precision/recall between the two parsers may
indicate the difference in terms of performance
between the two parsing approaches, there
nevertheless remain two issues to be investigat-
ed. Firstly, there is the issue of how text types
may influence the performance of the grammar
and indeed the parsing system as a whole.
Charniak (2000) uses WSJ as both training and
testing data and it is reasonable to expect a fairly
good overlap in terms of lexical co-occurrences
and linguistic structures and hence good
performance scores. Indeed, Gildea (2001)
suggests that the standard WSJ task seems to be
simplified by its homogenous style. It is thus yet
55
to be verified how well the same system will
perform when trained and tested on a more
?balanced? corpus such as ICE. Secondly, it is
not clear what the performance will be for
Charniak?s parsing model when dealing with a
much more complex grammar such as ICE,
which has almost three times as many non-
terminal parsing symbols. The performance of
the Survey Parser is very close to that of an
unlexicalised PCFG parser reported in Klain and
Manning (2003) but again WSJ was used for
training and testing and it is not clear how well
their system will scale up to a typologically
more varied corpus.
5 Conclusion
This article described a corpus of contemporary
English that is linguistically annotated at both
grammatical and syntactic levels. It then
described a formal grammar that is auto-
matically generated from the corpus and
presented statistics outlining the learning curve
of the grammar as a function of training data
size. Coverage by the grammar was presented
through empirical tests. It then reported the use
of the NIST evaluation metric for the evaluation
of the grammar when applied by the Survey
Parser on test sets totalling 4,000 trees.
Through the size of the grammar in terms of
the five canonical phrases as a function of
growth in training data size, it was observed that
the learning curves for AJP, AVP, and VP
culminated fairly rapidly with growing training
data size. In contrast, NPs and PPs demonstrate
a sharp learning curve, which may have
suggested that there would be a lack of
sufficient coverage by the grammar for these
two phrase types. Experiments show that such a
grammar still had a satisfactory coverage for
these two with a near total coverage for the other
three phrase types.
The NIST scheme was used to evaluate the
performance of the grammar when applied in the
Survey Parser. An especially advantageous
feature of the metric is the calculation of an
overall parser performance rate that takes into
account the total number of insertions in the
parse tree, an important structural distortion
factor when calculating the similarity between
two trees. A total of 4,000 trees were used to
evaluate the labelling and bracketing accuracies
of the parse trees automatically produced by the
parser. It is shown that the LR rate is over 86%
and LP is about 84%. The bracketed recall is
85.8% with a bracketed precision of 82.8%.
Finally, an attempt was made to estimate the
combined performance score for both labelling
and bracketing accuracies. The combined recall
is 86.1% and the combined precision is 83.5.
These results show both encouraging and
promising performance by the grammar in terms
of coverage and accuracy and therefore argue
strongly for the case of inducing formal
grammars from linguistically annotated corpora.
A future research topic is the enhancement of
the recall rate for clausal rules, which now
stands at just over 65%. It is of great benefit to
the parsing community to verify the impact the
size of the grammar has on the performance of
the parsing system and also to use a typo-
logically more balanced corpus than WSJ as a
workbench for grammar/parser development.
References
Charniak, E. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st
Annual Meeting of the North American
Chapter of the ACL, Seattle, Washington.
Fang, A.C. 1996a. Grammatical tagging and
cross-tagset mapping. In S. Greenbaum
(ed).
Fang, A.C. 1996b. The Survey Parser: Design
and development. In S. Greenbaum (ed).
Fang, A.C. 2000. From Cases to Rules and Vice
Versa: Robust Practical Parsing with
Analogy. In Proceedings of the Sixth Inter-
national Workshop on Parsing Tech-
nologies, 23-25 February 2000, Trento,
Italy. pp 77-88.
Gildea, D. 2001. Corpus variation and parser
performance. In Proceedings of the Con-
ference on Empirical Methods in Natural
Language Processing, 2001.
Greenbaum, S. 1992. A new corpus of English:
ICE. In Directions in Corpus Linguistics:
Proceedings of Nobel Symposium 82,
Stockholm 4-8 August 1991, ed. by Jan
Svartvik. Berlin: Mouton de Gruyter. pp
171-179.
Greenbaum, S. 1996. The International Corpus
of English. Oxford: Oxford University
Press.
Klein, D. and C. Manning, 2003. Accurate un-
lexicalized parsing. In Proceeding of the
41st Annual Meeting of the Association for
Computational Linguistics, July 2003. pp
423-430.
56
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 61?68,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
Collaborative Annotation of Dialogue Acts:  
Application of a New ISO Standard to the Switchboard Corpus 
Alex C. Fang1, Harry Bunt2, Jing Cao3, and Xiaoyue Liu4 
1,3,4The Dialogue Systems Group, Department of Chinese, Translation and Linguistics 
City University of Hong Kong, Hong Kong, SAR  
2Tilburg Centre for Cognition and Communication  
Tilburg University, The Netherlands 
3School of Foreign Languages, Zhongnan University of Economics and Law, China 
E-mail: {1acfang, 3cjing3, 4xyliu0}@cityu.edu.hk, 2harry.bunt@uvt.nl  
Abstract 
This article reports some initial results from the collaborative work on converting SWBD-DAMSL annotation scheme used in the 
Switchboard Dialogue Act Corpus to ISO DA annotation framework, as part of our on-going research on the interoperability of 
standardized linguistic annotations. A qualitative assessment of the conversion between the two annotation schemes was performed to 
verify the applicability of the new ISO standard using authentic transcribed speech. The results show that in addition to a major part of 
the SWBD-DAMSL tag set that can be converted to the ISO DA scheme automatically, some problematic SWBD-DAMSL tags still 
need to be handled manually. We shall report the evaluation of such an application based on the preliminary results from automatic 
mapping via machine learning techniques. The paper will also describe a user-friendly graphical interface that was designed for manual 
manipulation. The paper concludes with discussions and suggestions for future work. 
 
 
 
1. Introduction 
This article describes the collaborative work on applying 
the newly proposed ISO standard for dialogue act 
annotation to the Switchboard Dialogue Act (SWBD-DA) 
Corpus, as part of our on-going effort to promote 
interoperability of standardized linguistic annotations 
with the ultimate goal of developing shared and open 
language resources.  
Dialogue acts (DA) play a key role in the 
interpretation of the communicative behaviour of 
dialogue participants and offer valuable insight into the 
design of human-machine dialogue systems (Bunt et al, 
2010). More recently, the emerging ISO DIS 24617-2 
(2010) standard for dialogue act annotation defines 
dialogue acts as the ?communicative activity of a 
participant in dialogue interpreted as having a certain 
communicative function and semantic content, and 
possibly also having certain functional dependence 
relations, rhetorical relations and feedback dependence 
relations? (p. 3). The semantic content specifies the 
objects, relations, events, etc. that the dialogue act is 
about; the communicative function can be viewed as a 
specification of the way an addressee uses the semantic 
content to update his or her information state when he or 
she understands the corresponding stretch of dialogue. 
Continuing efforts have been made to identify and 
classify the dialogue acts expressed in dialogue utterances 
taking into account the empirically proven 
multifunctionality of utterances, i.e., the fact that 
utterances often express more than one dialogue act (see 
Bunt, 2009 and 2011). In other words, an utterance in 
dialogue typically serves several functions. See Example 
(1) taken from the SWBD-DA Corpus 
(sw_0097_3798.utt). 
 
(1) A: Well, Michael, what do you think about, uh, 
funding for AIDS research? Do you? 
B:   Well, uh, uh, that?s something I?ve thought a lot 
about.  
 
With the first utterance, Speaker A performs two 
dialogue acts: he (a) assigns the next turn to the 
participant Michael, and (b) formulates an open question. 
Speaker B, in his response, (a) accepts the turn, (b) stalls 
for time, and (c) answers the question by making a 
statement.  
Our concern in this paper is to explore the 
applicability of the new ISO Standard to the existing 
Switchboard corpus with joint efforts of automatic and 
manual mapping. In the rest of the paper, we shall first 
describe the Switchboard Dialogue Act (SWBD-DA) 
Corpus and its annotation scheme (i.e. SWBD-DAMSL). 
We shall then describe the new ISO Standard and explain 
our mapping of SWBD-DAMSL to the ISO DIS 24617-2 
DA tag set. In addition, machine learning techniques are 
employed for automatic DA classification on the basis of 
lexical features to evaluate the application of the new ISO 
DA scheme using authentic transcribed speech. We shall 
then introduce the user interface designed for manual 
mapping and explain the annotation guidelines. Finally, 
the paper will conclude with discussions and suggestions 
for future work.  
2. Corpus Resource 
This study uses the Switchboard Dialog Act (SWBD-DA) 
Corpus as the corpus resource, which is available online 
from the Linguistic Data Consortium 1 . The corpus 
                                                          
1 http://www.ldc.upenn.edu/ 
61
contains 1,155 5-minute conversations2, orthographically 
transcribed in about 1.5 million word tokens. It should be 
noted that the minimal unit of utterances for DA 
annotation in the SWBD Corpus is the so called ?slash 
unit? (Meteer and Taylor, 1995), defined as ?maximally a 
sentence but can be smaller unit? (p. 16), and ?slash-units 
below the sentence level correspond to those parts of the 
narrative which are not sentential but which the annotator 
interprets as complete? (p. 16). See Table 1 for the basic 
statistics of the SWBD-DA Corpus. 
 
Table 1: Basic Statistics of the SWBD-DA Corpus 
 
Altogether, the corpus comprises 223,606 slash-units and 
each is annotated for its communicative function 
according to a set of dialogue acts specified in the 
SWBD-DAMSL scheme (Jurafsky et al, 1997) and 
assigned a DA tag. See Example (2) taken from 
sw_0002_4330.utt, where qy is the DA tag for yes/no 
questions.  
 
(2) qy   A.1 utt1: {D Well, } {F uh, } does the company 
you work for test for drugs? /   
 
A total of 303 different DA tags are identified throughout 
the corpus, which is different from the total number of 
220 tags mentioned in Jurafsky et al (1997: 3). To ensure 
enough instances for the different DA tags, we also 
conflated the DA tags together with their secondary 
carat-dimensions, and yet we did not use the seven special 
groupings by Jurafsky et al (1997) as we kept them as 
separate DA types (see Section 4 for further explanations). 
In the end, the 303 tags were clustered into 60 different 
individual communicative functions. See Table 2 for the 
basic statistics of the 60 DA clusters.  
According to Table 2, we observe that the 60 DA 
clusters range from 780,570 word tokens for the 
top-ranking statement-non-opinion to only 4 word 
                                                          
2 Past studies (e.g. Stolcke et al, 2000; Jurafsky et al, 
1997; Jurafsky et al, 1998a; Jurafsky et al, 1998b) have 
been focused on only 1115 conversations in the 
SWBD-DA Corpus as the training set. As there is no clear 
description which 40 conversations have been used as the 
testing set or for future use, we use all the 1155 
conversations.   
tokens for you?re-welcome. In Table 2, the Token % 
column lists the relative importance of DA types 
measured as the proportion of the word tokens in the 
SWBD-DA corpus as whole. It can be observed that, as 
yet another example to illustrate the uneven use of DA 
types, statement-opinion accounts for 21.04% of the 
total number of word tokens in the corpus.  
 
60 DAs Tokens Token % Cum % 
Statement-non-opinion 780,570 51.79 51.79 
Statement-opinion 317,021 21.04 72.83 
Segment-(multi-utterance) 135,632 9.00 81.83 
Acknowledge-(backchannel) 40,696 2.70 84.53 
Abandoned 35,214 2.34 86.87 
Yes-no-question 34,817 2.31 89.18 
Accept 20,670 1.37 90.55 
Statement-expanding-y/n-answer 14,479 0.96 91.51 
Wh-question 14,207 0.94 92.45 
Appreciation 13,957 0.93 93.38 
Declarative-yes-no-question 10,062 0.67 94.05 
Conventional-closing 9,017 0.60 94.65 
Quoted-material 7,591 0.50 95.15 
Summarize/reformulate 6,750 0.45 95.60 
Action-directive 5,860 0.39 95.99 
Rhetorical-questions 5,759 0.38 96.37 
Hedge 5,636 0.37 96.74 
Open-question 4,884 0.32 97.06 
Affirmative-non-yes-answers 4,199 0.28 97.34 
Uninterpretable 4,138 0.27 97.61 
Yes-answers 3,512 0.23 97.84 
Completion 2,906 0.19 98.03 
Hold-before-answer/agreement 2,860 0.19 98.22 
Or-question 2,589 0.17 98.39 
Backchannel-in-question-form 2,384 0.16 98.55 
Acknowledge-answer 2,038 0.14 98.69 
Negative-non-no-answers 1,828 0.12 98.81 
Other-answers 1,727 0.11 98.92 
No-answers 1,632 0.11 99.03 
Or-clause 1,623 0.11 99.14 
Other 1,578 0.10 99.24 
Dispreferred-answers 1,531 0.10 99.34 
Repeat-phrase 1,410 0.09 99.43 
Reject 891 0.06 99.49 
Transcription-errors:-slash-units 873 0.06 99.55 
Declarative-wh-question 855 0.06 99.61 
Signal-non-understanding 770 0.05 99.66 
Self-talk 605 0.04 99.70 
Offer 522 0.03 99.73 
Conventional-opening 521 0.03 99.76 
3rd-party-talk 458 0.03 99.79 
Accept-part 399 0.03 99.82 
Downplayer 341 0.02 99.84 
Apology 316 0.02 99.86 
Exclamation 274 0.02 99.88 
Commit 267 0.02 99.90 
Thanking 213 0.01 99.91 
Double-quote 183 0.01 99.92 
Reject-part 164 0.01 99.93 
Tag-question 143 0.01 99.94 
Maybe 140 0.01 99.95 
Sympathy 80 0.01 99.96 
Explicit-performative 78 0.01 99.97 
Open-option 76 0.01 99.98 
Other-forward-function 42 0.00 99.98 
Correct-misspeaking 37 0.00 99.98 
No-plus-expansion 26 0.00 99.98 
Yes-plus-expansion 22 0.00 99.98 
You?re-welcome 4 0.00 99.98 
Double-labels 2 0.00 100.00 
 Total 1,507,079 100.00 100.00 
Table 2: Basic Statistics of the 60 DAs 
 
If the cumulative proportion (Cum%) is considered, we 
Folder 
# of 
Conversations 
# of 
Slash-units 
# of 
Tokens 
sw00  99 14,277 103,045 
sw01 100 17,430 119,864 
sw02 100 20,032 132,889 
sw03 100 18,514 127,050 
sw04 100 19,592 132,553 
sw05 100 20,056 131,783 
sw06 100 19,696 135,588 
sw07 100 20,345 136,630 
sw08 100 19,970 134,802 
sw09 100 20,159 133,676 
sw10 100 22,230 143,205 
sw11  16   3,213   20,493 
sw12  11   2,773   18,164 
sw13  29   5,319   37,337 
Total      1,155   223,606 1,507,079 
62
see that the top 10 DA types alone account for 93.38% of 
the whole corpus, suggesting again the uneven occurrence 
of DA types in the corpus and hence the disproportional 
use of communication functions in conversational 
discourse.  
It is particularly worth mentioning that 
segment-(multi-utterance) is not really a DA type 
indicating communicative function and yet it is the third 
most frequent DA tag in SWBD-DAMSL.  As a matter of 
fact, the SWBD-DAMSL annotation scheme contains 
quite a number of such non-communicative DA tags, such 
as abandoned, and quoted-material. 
3. ISO DIS 24617-2 (2010) 
A basic premise of the emerging ISO standard for 
dialogue act annotation, i.e., ISO DIS 24617-2 (2010), is 
that utterances in dialogue are often multifunctional; 
hence the standard supports so-called ?multidimensional 
tagging?, i.e., the tagging of utterances with multiple DA 
tags. It does so in two ways: First of all, it defines nine 
dimensions to which a dialogue act can belong: 
? Task 
? Auto-Feedback 
? Allo-Feedback 
? Turn Management 
? Time Management 
? Discourse Structuring 
? Social Obligations Management 
? Own Communication Management 
? Partner Communication Management 
Secondly, it takes a so-called ?functional segment? as 
the unit in dialogue to be tagged with DA information, 
defined as a ?minimal stretch of communicative behavior 
that has one or more communicative functions? (Bunt et 
al., 2010). A functional segment is allowed to be 
discontinuous, and to overlap with or be included in 
another functional segment. A functional segment may be 
tagged with at most one DA tag for each dimension. 
Another important feature is that an ISO DA tag 
consists not only of a communicative function encoding, 
but also of a dimension indication, with optional attributes 
for representing certainty, conditionality, sentiment, and 
links to other dialogue units expressing semantic, 
rhetorical and feedback relations. 
Thus, two broad differences can be observed between 
SWBD-DAMSL and ISO. The first concerns the 
treatment of the basic unit of analysis. While in 
SWBD-DAMSL this is the slash-unit, ISO DIS 24617-2 
(2010) employs the functional segment, which serves well 
to emphasise the multifunctionality of dialogue utterances. 
An important difference here is that the ISO scheme 
identifies multiple DAs per segment and assigns multiple 
tags via the stand-off annotation mechanism. 
The second difference is that each slash-unit (or 
utterance) in the SWBD-DA Corpus is annotated with one 
SWBD-DAMSL label, while each DA tag in the ISO 
scheme is additionally associated with a dimension tag 
and, when appropriate, with function qualifiers and 
relations to other dialogue units. See the following 
example taken from the Schiphol Corpus. 
 
(3) A: I?m most grateful for your help 
 
While the utterance in Example (3) would be annotated 
with only a functional tag in SWBD-DAMSL, it is 
annotated to contain the communicative function ?inform? 
and in addition the dimension of social obligation 
management:  
 
    communicativeFunction = ?inform? 
  dimension = ?socialObligationManagement? 
4. Mapping SWBD-DAMSL to ISO  
4.1 Data Pre-processing 
For the benefit of the current study and potential 
follow-up work, the banners between folders were 
removed and each slash-unit was extracted to create a set 
of files. See Example (4), the tenth slash-unit taken from 
the file sw_0052_4378.utt in the folder sw00.    
 
(4) sd     B.7 utt1: {C And,} {F uh,} <inhaling> we?ve  
                             done <sigh> lots to it. /  
 
The following set of files is created: 
 
sw00-0052-0010-B007-01.txt  the original utterance 
sw00-0052-0010-B007-01-S.da  SWBD-DAMSL tag 
 
In the .txt file, there is the original utterance:  
 
     {C And,} {F uh,} <inhaling> we?ve                             
done <sigh> lots to it. /  
 
While the *-S.da file only contains the DA label: sd^t. 
Still another one or more files (depending on the number 
of dimensions) will be added to this set after converting 
the SWBD-DAMSL to the ISO tag sets.  Take Example (4) 
for instance. Two more files will be created, namely,   
 
sw00-0052-0010-B007-01-ISO-0.da  ISO DA tag 
sw00-0052-0010-B007-01-ISO-1.da  ISO DA tag 
 
The *-ISO-0.da file will contain in this case:  
 
   communicativeFunction = ?inform? 
   dimension = ?task?3 
 
and the *-ISO-1.da file will contain4:  
 
   communicativeFunction = ?stalling? 
   dimension = ?timeManagement? 
                                                          
3 The same function Inform have been observed to occur 
in different dimensions. See ISO DIS 24617-2 (2010) for 
detailed description.  
4 See Section 4.2 for more explanation of the multi-layer 
annotations in ISO standard.  
63
4.2 Assessment of the Conversion 
When mapping SWBD-DAMSL tags to functional ISO 
tags, it is achieved in terms of semantic contents rather 
than the surface labels. To be more exact, four situations 
were identified in the matching process.  
The first is what is named as ?exact matches?. It is 
worth mentioning that since we are not matching the 
labels in the two annotation schemes, even for the exact 
matches, the naming in SWBD-DAMSL is not always the 
same as that in the ISO scheme, but they have the same or 
very similar meaning. Table 3 lists the exact matches. 
 
SWBD-DAMSL ISO 
Open-question Question  
Dispreferred answers Disconfirm 
Offer Offer 
Commit Promise 
Open-option Suggest 
Hold before answer/ agreement Stalling 
Completion Completion 
Correct-misspeaking CorrectMisspeaking 
Apology Apology 
Downplayer AcceptApology 
Thanking Thanking 
You?re-welcome AcceptThanking 
Signal-non-understanding AutoNegative 
Conventional-closing InitialGoodbye 
Table 3: Exact Matches 
It can also be noted that in the previous study on the 42 
DA types in SWBD-DAMSL, open-option (oo), 
offer (co), commit (cc) are treated as one DA type. In 
the current study, they are treated as individual DA types, 
which makes more sense especially when mapping to the 
ISO DA tag sets since each of them corresponds to a 
different ISO tag, suggest, offer, and promise 
respectively.   The same is also true for the 
you?re-welcome (fw) and correct-misspeaking 
(bc), which are combined together in SWBD-DAMSL 
and correspond to different ISO DA label.  
 
SWBD-DAMSL ISO 
Wh-question; Declarative wh-question SetQuestion 
Or-question; Or-clause ChoiceQuestion 
Yes-no-question;  
Backchannel in question form PropositionalQuestion 
Tag-question;  
Declarative Yes-no-question CheckQuestion 
Statement-non-opinion;  
Statement-opinion;  
Rhetorical-question;  
Statement expanding y/n answer; Hedge 
Inform 
Maybe; Yes-answer;  
Affirmative non-yes answers;  
Yes plus expansion; No-answer;  
Negative non-no answers;  
No plus expansion 
Answer 
Acknowledge (backchannel); 
Acknowledge answer; Appreciation; 
Sympathy; Summarize/reformulate;  
Repeat-phrase 
AutoPositive 
Accept-part; Reject-part Correction 
Table 4: Many-to-one Matches 
The second situation is where more than one 
SWBD-DAMSL tags can be matched to the one ISO DA 
type, as defined as many-to-one matches. Table 4 shows 
the many-to-one matches. Such matches occur because 
semantically identical functions are sometimes given 
different names in SWBD-DAMSL in order to distinguish 
differences in lexical or syntactic form. For example, an 
affirmative non-yes answer is defined as an 
affirmative answer that does not contain the word yes or 
one of its variants (like yeah and yep). 
 The most complex issue is with the one-to-many 
matches, where a DA function in SWBD-DAMSL is too 
general and corresponds to a set of different DAs in the 
ISO scheme. Consider the DA type of accept in 
SWBD-DAMSL. It is a broad function applicable to a 
range of different situations. For instance, accept 
annotated as aa in Example (5) taken from 
sw_0005_4646.utt corresponds to Agreement in ISO 
DIS 24617-2 (2010). 
 
(5) sd    A.25 utt1: {C Or } people send you there as a  
                                  last resort. / 
     aa     B.26 utt1: Right,  / 
 
However, accept (aa) in Example (6) taken from 
sw_0098_3830.utt actually corresponds to 
acceptOffer in ISO/DIS 24617-2 (2010).  
 
(6) co    B.26 utt1: I can tell you my last job or --/ 
      aa    A.27 utt1: Okay,  / 
 
As a matter of fact, accept in SWBD-DAMSL may 
correspond to several different DAs in the ISO tag set 
such as: 
? Agreement  
? AcceptRequest (addressRequest) 
? AccpetSuggestion (addressSuggestion) 
? AcceptOffer (addressOffer) 
? etc. 
 
Other cases include reject, action-directive and 
other answers.  
Finally, the remaining tags are unique to 
SWBD-DAMSL, including  
 
? quoted material 
? uninterpretable 
? abandoned 
? self-talk 
? 3rd-party-talk 
? double labels  
? explicit-performative  
? exclamation 
? other-forward-function 
 
It is not difficult to notice that 6 out of the 9 DA types 
mainly concern the marking up of other phenomena than 
dialogue acts. The last three unique DA types only 
account for a marginal portion of the whole set, about 
0.03% all together (See Table 2).  
64
In addition, multi-layer annotations of ISO can be 
added to the original markup of SWBD (Meteer and 
Taylor 1995), especially in cases such as Stalling and 
Self-Correction. See Example (7) taken from 
sw_0052_4378.utt. 
 
(7) sd   A.12  utt2 : [ I, + {F uh, } two months ago I ]  
                               went to Massachusetts -- /  
 
According to Meteer and Taylor (1995), the {F ?} is 
used to mark up ?filler? in utterances, which corresponds 
to Stalling in ISO DIS 24617-2 (2010). In addition, the 
markup of [ ? + ?] indicates the repairs (Meteer and 
Taylor, 1995), which suits well the definition of 
Self-correction in the ISO standard. As a result, the 
utterance in Example (7) is thus annotated in three 
dimensions:  
communicativeFunction = ?inform? 
dimension = ?task? 
 
communicativeFunction = ?stalling? 
dimension = ?timeManagement? 
 
communicativeFunction = ?self-correction? 
dimension = ?ownCommManagement? 
4.3 Mapping Principles 
Given the four setting of the matching, there major 
principles were made:  
1) Cases in both ?exact matches? and ?many-to-one 
matches? can be automatically mapped to ISO tags by 
programming. 
2) Tags that are unique to SWBD-DAMSL would not 
be considered at the current stage due to the absence of 
ISO counterparts and their marginal proportion. 
3) Cases in ?one-to-many matches? are more complex 
and call for manual mapping, which will be further 
discussed in Section 6.  
4) Different DA dimensions will be also automatically 
added accordingly to each utterance in the format of 
stand-off annotation.  
5. Application Verification 
To evaluate the applicability of mapping SWBD-DAMSL 
tag set to the new ISO standard (ISO DIS 24617-2, 2010), 
machine learning techniques are employed, based on the 
preliminary results from the automatic mapping, to see 
how well the SWBD-ISO DA tags can be automatically 
identified and classified based on lexical features. The 
result is also compared with that obtained from the 
Top-15 SWBD-DAMSL tags. It will be particularly 
interesting to find out whether the emerging ISO DA 
annotation standard will produce better automatic 
prediction accuracy. In this paper, we evaluate the 
performance of automatic DA classification in the two DA 
annotation schemes by employing the unigrams as the 
feature set.  
Two classification tasks were then identified 
according to the two DA annotation schemes. Task 1 is to 
automatically classify the DA types in the 
SWBD-DAMSL. Based on the observations mentioned 
above, it was decided to use the top 15 DA types to 
investigate the distribution of word types in order to 
ascertain the lexical characteristics of DAs. Furthermore, 
since segment-(multi-utterance), abandoned, and 
quoted-material do not relate to dialogue acts per se, 
these three were replaced with rhetorical-questions, 
open-question and 
affirmative-non-yes-answers. We thus derive 
Table 6 below, showing that the revised list of top 15 DA 
types account for 85.13% of the SWBD corpus. The DA 
types are arranged according to Token% in descending 
order.  
 
Top-15 SWBD-DAMSL DAs Tokens Token % Cum % 
Statement-non-opinion 780,570 51.79 51.79 
Statement-opinion 317,021 21.04 72.83 
Acknowledge-(backchannel) 40,696 2.70 75.53 
Yes-no-question 34,817 2.31 77.84 
Accept 20,670 1.37 79.21 
Statement-expanding-y/n-answer 14,479 0.96 80.17 
Wh-question 14,207 0.94 81.11 
Appreciation 13,957 0.93 82.04 
Declarative-yes-no-question 10,062 0.67 82.71 
Conventional-closing 9,017 0.60 83.31 
Summarize/reformulate 6,750 0.45 83.76 
Action-directive 5,860 0.39 84.15 
Rhetorical-questions 5,759 0.38 84.53 
Open-question 4,884 0.32 84.85 
Affirmative-non-yes-answers 4,199 0.28 85.13 
Total 1,282,948 85.13  
Table 6: Top-15 SWBD-DAMSL DA types 
Next, accordingly, task 2 is to classify the top 15 ISO 
DAs based on the results from the automatic mapping. It 
should be pointed out that only one layer of annotation in 
the ISO DA tags is considered in order to make the result 
comparable to that from SWBD-DAMSL, and the 
dimension of task is the priority when it comes to 
multi-layer annotations.  
 
Top-15 SWBD-ISO DAs Tokens Token % Cum % 
Inform 1,117,829   74.17 74.17 
AutoPositive 64,851 4.30 78.47 
PropositionalQuestion 37,201 2.47 80.94 
SetQuestion 15,062 1.00 81.94 
Answer 11,171 0.74 82.68 
CheckQuestion 10,062 0.67 83.35 
InitialGoodbye 9,017 0.60 83.95 
Question 4,884 0.32 84.27 
ChoiceQuestion 4,212 0.28 84.55 
Completion 2,906 0.19 84.75 
Stalling 2,860 0.19 84.94 
Disconfirm 1,531 0.10 85.04 
AutoNegative 770 0.05 85.09 
Offer 522 0.03 85.12 
AcceptApology 341 0.02 85.15 
Total 1,283,219   85.15  
Table 7: Top-15 SWBD-ISO DA types 
The Na?ve Bayes Multinomial classifier was 
employed, which is available from Waikato Environment 
for Knowledge Analysis, known as Weka (Hall et al, 
2009). 10-fold cross validation was performed and the 
65
results evaluated in terms of precision, recall and F-score 
(F1). 
Table 8 presents the results for classification task 1. 
The SWBD-DAMSL DAs are arranged according to 
F-score in descending order. 
 
Top 15 SWBD-DAMSL DAs Precision Recall F1 
Acknowledge-(backchannel) 0.821 0.968 0.888 
Statement-non-opinion 0.732 0.862 0.792 
Appreciation 0.859 0.541 0.664 
Statement-opinion 0.538 0.584 0.560 
Conventional-closing 0.980 0.384 0.552 
Accept 0.717 0.246 0.367 
Yes-no-question 0.644 0.204 0.309 
Wh-question 0.760 0.189 0.303 
Open-question 0.932 0.084 0.154 
Action-directive 1.000 0.007 0.013 
Statement-expanding-y/n-answer 0.017 0 0.001 
Declarative-yes-no-question 0 0 0 
Summarize/reformulate 0 0 0 
Rhetorical-questions 0 0 0 
Affirmative-non-yes-answers 0 0 0 
Weighted Average 0.704 0.725 0.692 
Table 8: Results from Task 1 
As can be noted, the weighted average F-score is 69.2%. 
To be more specific, acknowledge-(backchannel) 
achieves the best F-score of 0.888, followed by 
statement-non-opinion with an F-score of 0.792. 
Surprisingly, the action-directive has the highest 
precision of 100%, but has the second lowest recall of 
over 0.7%. It can also be noted that the last four types of 
DAs cannot be classified with the F-score of 0%.  
 
Top 15 SWBD-ISO DAs Precision Recall F1 
Inform 0.879 0.987 0.930 
Answer 0.782 0.767 0.775 
AutoPositive 0.711 0.507 0.592 
InitialGoodbye 0.972 0.351 0.516 
PropositionalQuestion 0.521 0.143 0.224 
SetQuestion 0.668 0.120 0.203 
Question 0.854 0.051 0.097 
AutoNegative 0.889 0.026 0.051 
ChoiceQuestion 0.286 0.008 0.015 
Stalling 0.400 0.003 0.007 
CheckQuestion 0.042 0.001 0.001 
AcceptApology 0 0 0 
Completion 0 0 0 
Disconfirm 0 0 0 
Offer 0 0 0 
Weighted Average 0.832 0.865 0.831 
Table 9: Results from Task 2 
Table 9 presents the results for classification task 2. 
The DAs are arranged according to F-score in descending 
order. As can be noted, the weighted average F-score is 
83.1%, over 10% higher than task 1. To be more specific, 
Inform achieves the best F-score of 0.93, followed by 
Answer with an F-score of 0.775. The DA 
InitialGoodbye has the highest precision, of about 
97%, whereas Inform has the highest recall of over 98%. 
Similar to the results obtained in Task 1, the last four types 
of DAs in Task 2 also cannot be classified with the 
F-score of 0%. 
Meanwhile, as mentioned earlier, when the data size 
for each DA type is taken into consideration, Task 2 may 
be more challenging than Task 1 in that 6 out of the 15 
SWBD-ISO DA types has a total number of word tokens 
fewer than 4,000 whereas all the 15 SWBD-DAMSL DA 
types has a total number of over 4,000. Therefore, the 
much higher average F-score suggests that the application 
of ISO standard DA scheme could lead to better 
classification performance, suggesting that the ISO DA 
standard represents a better option for automatic DA 
classification. 
To sum up, with a comparable version of the 
SWBD-DA Corpus, results from the automatic DA 
classification tasks show that the ISO DA annotation 
scheme produces better automatic prediction accuracy, 
which encourages the completion of the manual mapping. 
6. Manual Mapping 
6.1 Analysis of Problematic DA Types 
As mentioned earlier, there are mainly four problematic 
SWBD-DAMSL tags, namely, accept (aa), reject 
(ar), action-directive (ad) and other answers 
(no). They are problematic in that they carry a broad 
function applicable to a range of different situations 
according to the new ISO standard, as evidenced in the 
case of accept discussed in Section 4.2. Consequently, to 
map the problematic SWBD-DAMSL tags to the ISO tags 
calls for manual manipulation. 
A close look into those four types shows that the 
mapping could be further divided into two setting. Again, 
take accept (aa) for example. In the first setting, a 
sub-division of accept (aa) can also be automatically 
matched according to the previous utterance by the other 
speaker in the adjacent pair. See Example (8) taken from 
sw_0001_4325.utt.  
 
(8) sv     A.49 utt3: take a long time to find the right  
                                 place / 
      x      A.49 utt4: <laughter>. 
      aa     B.50 utt1: Yeah,  / 
 
Here accept (aa) corresponds to Agreement because of 
the DA type in A.49 utt3 but not the immediate previous 
DA as in A.49 utt4. With this principle, the particular 
sub-groups for automatic mapping were identified for 
accept (aa). See Table 10. 
 
SWBD-DAMSL 
ISO 
Previous DA Current DA 
Statement-non-opinion; 
Statement-opinion; Hedge 
Rhetorical-question;  
Statement expanding y/n answer,  
accept 
Agreement 
Offer AcceptOffer 
Open-option AcceptRequest 
Thanking AcceptThanking 
Apology AcceptApology 
Table 10: Sub-groups of accept for Auto Mapping 
The remaining cases, in the second setting, call for 
manual annotation. For instance, when the previous DA 
type is also a problematic one, annotators need to decide 
66
the corresponding ISO DA tag for the previous 
SWBD-DAMSL one before converting the accept (aa).  
See Example (9) taken from sw_0423_3325.utt.  
 
(9) ad    B.128 utt2: {C so } we'll just wait. / 
      aa    A.129 utt1: Okay,  / 
 
Here, action-directive (ad) is first decided as a 
suggestion, and therefore accept (aa) turns out to 
actually correspond to acceptSuggestion 
(addressSuggestion) in ISO/DIS 24617-2 (2010).  
6.2 Design of a User Interface 
Given the analysis of those four DA tags, a user-friendly 
interface was then designed to assist annotators to 
maximize the inter-annotator agreement.  See Figure 1.  
 
Figure 1: User Interface 
 
Figure 1 shows the screenshot when the targeted 
SWBD-DAMSL type is accept (aa). As can be noted 
above, the basic functional bars have been designed, 
including: 
? Input: the path of the input 
? Automatch: to filter out the sub-groups that can be 
automatically matched 
? DA Tag: the targeted problematic DAs, namely, 
? aa (accept) 
? ar (reject) 
? ad (action-directive) and 
? no (other answers) 
? Previous: to go back to the previous instance of the 
targeted DA type 
? Next: to move on to the next instance of the targeted 
DA type 
? Current: the extraction of the adjacent turns 
? Previous5T: the extraction of the previous five turns 
when necessary 
? PreviousAll: the extraction of all the previous turns 
when necessary 
? MatchInfo: Bars for mapping information with five 
options: 
? Four pre-defined ISO DA types 
? Other: a user-defined mapping with a 
two-fold function: for user defined ISO DA 
type and for extra pre-defined ISO DA types 
(since the pre-defined DA types differ for 
the four targeted SWBD-DAMSL types).  
? Output: the path of the output 
? Result: export the results to the chosen path 
 
With this computer-aided interface, three annotators are 
invited to carry out the manual mapping. They are all 
postgraduates with linguistic background. After a month 
of training on the understanding of the two annotation 
schemes (in process), they will work on the 
SWBD-DAMSL DA instances from 115 randomly chosen 
files, and map them into ISO DA tags independently. The 
kappa value will be calculated to measure the 
inter-annotator agreement.  
 
7. Conclusion 
 
In this paper, we reported our efforts in applying the 
ISO-standardized dialogue act annotations to the 
Switchboard Dialogue Act (SWBD-DA) Corpus. In 
particular, the SWBD-DAMSL tags employed in the 
SWBD-DA Corpus were analyzed and mapped onto the 
ISO DA tag set (ISO DIS 24617-2 2010) according to 
their communicative functions and semantic contents. 
Such a conversion is a collaborative process involving 
both automatic mapping and manual manipulation.  With 
the results from the automatic mapping, machine learning 
techniques were employed to evaluate the applicability of 
the new ISO standard for dialogue act annotation in 
practice. With the encouraging results from the evaluation, 
the manual mapping was carried out. A user-friendly 
interface was designed to assist annotators. The 
immediate future work would be finish the manual 
mapping and thus to  produce a comparable version of the 
SWBD-DA Corpus was produced so that the two 
annotation schemes (i.e. SWBD-DAMSL vs. SWBD-ISO) 
can be effectively compared on the basis of empirical data. 
Furthermore, with the newly built resource, i.e., 
SWBD-ISO, we plan to examine the effect of 
grammatical and syntactic cues on the performance of DA 
classification, with a specific view on whether dialogue 
acts exhibit differentiating preferences for grammatical 
and syntactic constructions that have been overlooked 
before.  
 
 
8. Acknowledgements 
Research described in this article was supported in part by 
grants received from City University of Hong Kong 
(Project Nos 7008002, 9610188, 7008062 and 6454005). 
It was also partially supported by the General Research 
Fund of the Research Grants Council of Hong Kong 
(Project No 142711). 
9. References 
 
Bunt, H. (2009). Multifunctionality and multidimensional 
dialogue semantics. In Proceedings of DiaHolmia 
Workshop on the Semantics and Pragmatics of 
67
Dialogue, Stockholm, 2009.  
Bunt, H. (2011). Multifunctionality in dialogue and its 
interpretation. Computer, Speech and Language, 25 (2), 
pp. 225--245.  
Bunt, H., Alexandersson, J., Carletta, J., Choe, J.-W., 
Fang, A.C., Hasida, K., Lee, K., Petukhova, V., 
Popescu-Belis, A., Romary, L., Soria, C. and Traum, D. 
(2010). Towards an ISO standard for dialogue act 
annotation. In Proceedings of the Seventh International 
Conference on Language Resources and Evaluation. 
Valletta, MALTA, 17-23 May 2010. 
Hall, M., Frank, E., Holmes, G., Pfahringer, B., 
Reutemann, P. and Witten, I. H. (2009). The WEKA 
data mining software: an update. SIGKDD 
Explorations, 11 (1), pp. 10--18. 
ISO DIS 24617-2. (2010). Language resource 
management ? Semantic annotation framework 
(SemAF), Part 2: Dialogue acts. ISO, Geneva, January 
2010. 
Jurafsky, D., Shriberg, E. and Biasca, D. (1997). 
Switchboard SWBD-DAMSL 
shallow-discourse-function annotation coders manual, 
Draft 13.  University of Colorado, Boulder Institute of 
Cognitive Science Technical Report 97-02. 
Jurafsky, D., Bates, R., Coccaro, N., Martin, R., Meteer, 
M., Ries, K., Shriberg, E., Stolcke, A., Taylor,  P. and 
Ess-Dykema, C. V. (1998a). Switchbaod Discourse 
Language Modeling Project and Report. Research Note 
30, Center for Language and Speech Processing, Johns 
Hopkins University, Baltimore, MD, January. 
Jurafsky, D., Shriberg, E., Fox B. and Curl, T. (1998b).  
Lexical, prosodic, and syntactic cues for dialog acts. 
ACL/COLING-98 Workshop on Discourse Relations 
and Discourse Markers.  
Meeter, M., Taylor, A. (1995). Dysfluency annotation 
stylebook for the Switchboard Corpus. Available at 
ftp://ftp.cis.upenn.edu/pub/treebank/swbd/doc/DFL-bo
ok.ps. 
Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., 
Jurfsky, D., Taylor, P., Martin, R., Ess-Dykema, C.V. 
and Meteer, M.  (2000). Dialogue Act Modeling for 
Automatic Tagging and Recognition of Conversational 
Speech. Computational Linguistics, 26 (3), pp. 
339--373.  
68
