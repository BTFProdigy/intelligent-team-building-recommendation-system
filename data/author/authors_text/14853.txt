Proceedings of BioNLP Shared Task 2011 Workshop, pages 151?152,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
An Incremental Model for the Coreference Resolution Task of BioNLP 2011
Don Tuggener, Manfred Klenner, Gerold Schneider, Simon Clematide, Fabio Rinaldi
Institute of Computational Linguistics, University of Zurich, Switzerland
{tuggener,klenner,gschneid,siclemat,rinaldi}@cl.uzh.ch
Abstract
We introduce our incremental coreference res-
olution system for the BioNLP 2011 Shared
Task on Protein/Gene interaction. The benefits
of an incremental architecture over a mention-
pair model are: a reduction of the number
of candidate pairs, a means to overcome the
problem of underspecified items in pair-wise
classification and the natural integration of
global constraints such as transitivity. A fil-
tering system takes into account specific fea-
tures of different anaphora types. We do not
apply Machine Learning, instead the system
classifies with an empirically derived salience
measure based on the dependency labels of the
true mentions. The OntoGene pipeline is used
for preprocessing.
1 Introduction
The Coreference Resolution task of BioNLP fo-
cused on finding anaphoric references to proteins
and genes. Only antecedent-anaphora pairs are con-
sidered in evaluation and not full coreference sets.
Although it might not seem to be necessary to gen-
erate full coreference sets, anaphora resolution still
benefits from their establishment. Our incremental
approach (Klenner et al, 2010) naturally enforces
transitivity constraints and thereby reduces the num-
ber of potential antecedent candidates. The system
achieved good results in the BioNLP 2011 shared
task (Fig. 1)
Team R P F1
A 22.18 73.26 34.05
Our model 21.48 55.45 30.96
B 19.37 63.22 29.65
C 14.44 67.21 23.77
D 3.17 3.47 3.31
E 0.70 0.25 0.37
Figure 1: Protein/Gene Coreference Task
2 Preprocessing: The OntoGene Pipeline
OntoGene?s text mining system is based on an
internally-developed fast, broad-coverage, deep-
syntactic parsing system (Schneider, 2008). The
parser is wrapped into a pipeline which uses a num-
ber of other NLP tools. The parser is a key compo-
nent in a pipeline of NLP tools (Rinaldi et al, 2010),
used to process input documents. First, in a pre-
processing stage, the input text is transformed into
a custom XML format, and sentences and tokens
boundaries are identified. The OntoGene pipeline
also includes a step of term annotation and disam-
biguation, which are not used for the BioNLP shared
task, since relevant terms are already provided in
both the training and test corpora. The pipeline also
includes part-of-speech taggers, a lemmatizer and a
syntactic chunker.
When the pipeline finishes, each input sentence
has been annotated with additional information,
which can be briefly summarized as follows: sen-
tences are tokenized and their borders are detected;
each sentence and each token has been assigned an
ID; each token is lemmatized; tokens which be-
long to terms are grouped; each term is assigned a
normal-form and a semantic type; tokens and terms
are then grouped into chunks; each chunk has a
type (NP or VP) and a head token; each sentence
is described as a syntactic dependency structure. All
this information is represented as a set of predicates
and stored into the Knowledge Base of the system,
which can then be used by different applications,
such as the OntoGene Relation Miner (Rinaldi et al,
2006) and the OntoGene Protein-Protein Interaction
discovery tool (Rinaldi et al, 2008).
3 Our Incremental Model for Coreference
Resolution
1 for i=1 to length(I)
2 for j=1 to length(C)
3 rj := virtual prototype of coreference set Cj
4 Cand := Cand ? rj if compatible(rj ,mi)
5 for k= length(B) to 1
6 bk:= the k-th licensed buffer element
7 Cand := Cand ? bk if compatible(bk,mi)
8 if Cand = {} then B := B ?mi
9 if Cand 6= {} then
10 antei := most salient element of Cand
11 C := augment(C,antei,mi)
Figure 2: Incremental model: base algorithm
151
Fig. 2 shows the base algorithm. Let I be the
chronologically ordered list of NPs, C be the set
of coreference sets and B a buffer, where NPs are
stored, if they are not anaphoric (but might be valid
antecedents). Furthermore mi is the current NP and
? means concatenation of a list and a single item.
The algorithm proceeds as follows: a set of an-
tecedent candidates is determined for each NP mi
(steps 1 to 7) from the coreference sets (rj) and the
buffer (bk). A valid candidate rj or bk must be com-
patible with mi. The definition of compatibility de-
pends on the POS tags of the anaphor-antecedent
pair. The most salient available candidate is selected
as antecedent for mi.
3.1 Restricted Accessibility of Antecedent
Candidates
In order to reduce underspecification, mi is com-
pared to a virtual prototype of each coreference set
(similar to e.g. (Luo et al, 2004; Yang et al, 2004;
Rahman and Ng, 2009)). The virtual prototype bears
morphologic and semantic information accumulated
from all elements of the coreference set. Access to
coreference sets is restricted to the virtual prototype.
This reduces the number of considered pairs (from
the cardinality of a set to 1).
3.2 Filtering based on Anaphora Type
Potentionally co-refering NPs are extracted from the
OntoGene pipeline based on POS tags. We then ap-
ply filtering based on anaphora type: Reflexive pro-
nouns must be bound to a NP that is governed by the
same verb. Relative pronouns are bound to the clos-
est NP in the left context. Personal and possessive
pronouns are licensed to bind to morphologically
compatible antecedent candidates within a window
of two sentences. Demonstrative NPs containing the
lemmata ?protein? or ?gene? are licensed to bind to
name containing mentions. Demonstrative NPs not
containing the trigger lemmata can be resolved to
string matching NPs preceding them1.
3.3 Binding Theory as a Filter
We know through binding theory that ?modulator?
and ?it? cannot be coreferent in the sentence ?Over-
expression of protein inhibited stimulus-mediated
transcription, whereas modulator enhanced it?.
Thus, the pair ?modulator?-?it? need not be consid-
ered at all. We have not yet implemented a full-
1As we do not perform anaphoricity determination of nom-
inal NPs, we do not consider bridging anaphora (anaphoric
nouns that are connected to their antecedents through seman-
tic relations and cannot be identified by string matching).
blown binding theory. Instead, we check if the an-
tecedent and the anaphor are governed by the same
verb.
4 An Empirically-based Salience Measure
Our salience measure is a partial adaption of the
measure from (Lappin and Leass, 1994). The
salience of a NP is solely defined by the salience
of the dependency label it bears. The salience of a
dependency label, D, is estimated by the number of
true mentions (i.e. co-refering NPs) that bear D (i.e.
are connected to their heads with D), divided by the
total number of true mentions (bearing any D). The
salience of the label subject is thus calculated by:
Number of truementions bearing subject
Total number of truementions
We get a hierarchical ordering of the dependency la-
bels (subject > object > pobject > ...) according to
which antecedents are ranked and selected.
References
Manfred Klenner, Don Tuggener, and Angela Fahrni. 2010. Inkre-
mentelle koreferenzanalyse fu?r das deutsche. In Proceedings der
10. Konferenz zur Verarbeitung Natu?rlicher Sprache.
Shalom Lappin and Herbert J Leass. 1994. An algorithm for pronomi-
nal anaphora resolution. Computational Linguistics, 20:P. 535?561.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kambhatla, and
Salim Roukos. 2004. A mention-synchronous coreference resolu-
tion algorithm based on the bell tree. In Proceedings of the 42nd
Annual Meeting on Association for Computational Linguistics.
Altaf Rahman and Vincent Ng. 2009. Supervised models for corefer-
ence resolution. In Proceedings of the 2009 Conference on Empir-
ical Methods in Natural Language Processing: Volume 2 - Volume
2, EMNLP ?09, pages 968?977, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand, Michael Hess, and
Martin Romacker. 2006. An Environment for Relation Mining over
Richly Annotated Corpora: the case of GENIA. BMC Bioinformat-
ics, 7(Suppl 3):S3.
Fabio Rinaldi, Thomas Kappeler, Kaarel Kaljurand, Gerold Schnei-
der, Manfred Klenner, Simon Clematide, Michael Hess, Jean-Marc
von Allmen, Pierre Parisot, Martin Romacker, and Therese Vachon.
2008. OntoGene in BioCreative II. Genome Biology, 9(Suppl
2):S13.
Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand, Simon Clematide,
Therese Vachon, and Martin Romacker. 2010. OntoGene in
BioCreative II.5. IEEE/ACM Transactions on Computational Bi-
ology and Bioinformatics, 7(3):472?480.
Gerold Schneider. 2008. Hybrid Long-Distance Functional Depen-
dency Parsing. Doctoral Thesis, Institute of Computational Linguis-
tics, University of Zurich.
Xiaofeng Yang, Jian Su, Guodong Zhou, and Chew Lim Tan. 2004. An
np-cluster based approach to coreference resolution. In Proceedings
of the 20th international conference on Computational Linguistics.
152
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 116?120,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
UZH in the BioNLP 2013 GENIA Shared Task
Gerold Schneider, Simon Clematide, Tilia Ellendorff, Don Tuggener, Fabio Rinaldi,
{rinaldi,gschneid,siclemat,ellendorff,tuggener}@cl.uzh.ch
Institute of Computational Linguistics, University of Zurich, Switzerland
Gintare? Grigonyte?
Stockholm University, Department of Linguistics, Section for Computational Linguistics
gintare@ling.su.se
Abstract
We describe a biological event detec-
tion method implemented for the Genia
Event Extraction task of BioNLP 2013.
The method relies on syntactic depen-
dency relations provided by a general NLP
pipeline, supported by statistics derived
from Maximum Entropy models for can-
didate trigger words, for potential argu-
ments, and for argument frames.
1 Introduction
The OntoGene team at the University of Zurich
has developed text mining applications based on
a combination of deep-linguistic analysis and ma-
chine learning techniques (Rinaldi et al, 2012b;
Clematide and Rinaldi, 2012; Rinaldi et al, 2010).
Our approaches have proven competitive in sev-
eral shared task evaluations (Rinaldi et al, 2013;
Clematide et al, 2011; Rinaldi et al, 2008). Addi-
tionally, we have developed advanced systems for
the curation of the biomedical literature (Rinaldi
et al, 2012a).
Our participation in the Genia Event Extraction
task of BioNLP 2013 (Kim et al, 2013) was moti-
vated by the desire of testing our technologies on
a more linguistically motivated task. In the course
of our participation we revised several modules of
our document processing pipeline, however we did
not have sufficient resources to completely revise
the final module which generates the event struc-
tures, and we still relied on a module which we
had developed for our previous participation to the
BioNLP shared task.
The final submission was composed by our
standard preprocessing module (described briefly
in section 2) and novel probability models (section
3), combined within the old event generator (sec-
tion 4).
2 Preprocessing
The OntoGene environment is based on a pipeline
of several NLP tools which all operate on a com-
mon XML representation of the original docu-
ment.
Briefly, the pipeline includes modules for
sentence-splitting, tokenization, part-of-speech
tagging, lemmatization, stemming, term-
recognition (not used for the BioNLP shared
task), chunking, dependency-parsing and event
generation. Different variants of those modules
have been used in different instantiations of the
pipeline. For the BioNLP 2013 participation,
lingpipe was used for sentence splitting, tok-
enization and PoS tagging, morpha (Minnen et
al., 2001) was used for lemmatization, a python
implementation of the Porter stemmer for stem-
ming, LTTT (Grover et al, 2000), was used for
chunking, and the Pro3Gres parser (Schneider,
2008) for dependency analysis.
As we have made good experiences with a
rule based system for anaphora resolution in the
BioNLP 2011 shared task (Tuggener et al, 2011),
we implemented a similar approach that resolves
anaphors to terms identified during preprocessing.
Rules contain patterns like ?X such as Y? or ?X
is a Y?, and pronouns are resolved to the nearest
grammatical subject or object. Anaphora resolu-
tion led to an improvement of 0.2% recall on the
development set, while precision was hardly af-
fected.
3 Probability models
Several probability models have been computed
from the training data in order to be used to score
and filter candidate events generated by the sys-
tem. The following models played a role in the
final submission:
P (eventType | trigger candidate) (1)
116
P (frame ? eventType | trigger candidate) (2)
P (role ? eventType | protein) (3)
P (role(t, d) | synpath(t, d)) (4)
For all of them we computed global Maximum
Likelihood Estimations (MLE), using the training
and development datasets from the 2013 and 2011
challenges. For all of the models above, except
for the last one, we also estimated the probabili-
ties by a Maximum Entropy (ME) approach. The
MegaM tool (Daume? III, 2004) allows for a super-
vised training of binary classifiers where the class
probability is optimized by adjusting the feature
weights and not just the binary classification deci-
sion itself. This helps to deal with the imbalanced
classes such as the distribution of true or false trig-
gerword candidates.
For the classification of trigger candidates
(Equation 1), a binary ME classifier for each event
type is separately trained, based on local and
global features as described below. The trigger-
word candidates are collected from the training
data using their stemmed representation as a selec-
tion criterion. We generally exclude triggerword
candidates that occur in less than 1% as true trig-
gers in the training set. Within the data, we found
that triggers that consist of more than one word are
rather rare (less than 5% of all triggers, most of
them occurring once). However, we transformed
these multiword triggers to singleword triggers,
replacing them by their first content word.
The choice of ME features, partly inspired by
(Ekbal et al, 2013), can be grouped into features
derived from the triggerword itself (word), fea-
tures from the sentence of the triggerword (con-
text), and features from article-wide information
(global).
Word features: (1) The text, lemma, part of
speech (PoS), stem and local syntactic dependency
of the triggerword candidate as computed by the
Pro3Gres parser. (2) Information whether a trig-
gerword candidate is head of a chunk as well as
whether the chunk is nominal or verbal
Context features: Unigrams and bigrams in a
window of variable size to left and right of the trig-
gerword candidate; three types of uni- and bigrams
are used: PoS, lemmas and stems; for unigrams we
also include the lower-cased words; for bigrams,
the triggerword candidate itself is included in the
first bigram to either side.
Global features: (1) Presence or absence of a
protein in a window of a given size around the
triggerword candidate (Boolean feature); only the
most frequent proteins of an article are considered.
(2) The zone in an article where the triggerword
candidate appears, e.g. Title/Abstract, Introduc-
tion, Background, Material and Methods, Results
and Discussion, Caption and Conclusion.
Feature engineering was done by testing differ-
ent combinations of settings (window size, thresh-
olds) with the aim of finding an optimal overall
ME model which reaches the lowest error rates for
all event types. The error rate of the candidate set
was measured as the cumulative error mass com-
puted from the assigned class probability as fol-
lows: if the trigger candidate is a true positive, the
error is 1 minus the probability assigned by the
classifier. If the candidate is a false positive, the
error is the probability assigned by the classifier.
Our approach does not allow us to compute an er-
ror rate for false negatives, because we simply rely
on the set of trigger words seen in the training data
as possible candidates.
In these experiments, we discovered that for
most event types an optimal setting for the context
features considers a wide span of about 20 tokens
to the left and right of the triggerword. Includ-
ing bigrams of lemmas, stems and PoS delivered
the best results compared to including only one or
two of these bigram types. Context features can be
parameterized according to how much positional
information they contain: the distance of a word
to the right and left of the trigger, only the direc-
tion (left or right) or no position information at all
(bag of unigrams/bigrams). We found that the ex-
act positional information is only important for the
first word to the left and right (adjacent to the trig-
gerword), whereas for all words that are further
away it is favorable to only use the direction in re-
lation to the trigger. A window size of 10 words
within which proteins are found in the context of
a triggerword gave the best results. The optimal
number of the most frequent proteins considered
within this window was found to be the 10 most
frequent proteins within an article.
The second type of ME classifier (Equation 2)
has the purpose of calculating the probabilities of
event frames for all event types given a trigger
word. We use the term frame for a combination of
arguments that an event is able to accept as theme
and cause and whether these arguments are real-
117
ized as proteins or subevents.
For the classification of proteins (Equation 3),
again separate binary ME classifiers were built in
order to estimate the probability that a protein has
a role (theme or cause) in an event of a given type.
4 Event Generation
We tested two independent event generation mod-
ules, one based on a revision of our previous 2009
submission (Kaljurand et al, 2009) and one which
is a totally new implementation. We could do only
preliminary tests with the second module, which
however showed promising results, in particular
with much better recall than the older module (up
to 65.23%), despite the very little time that we
could invest in its development. The best F-score
that we could reach was still slightly inferior to the
one of the old module at the deadline for submis-
sion of results. In the rest of this paper we will
describe only the module which was used in the
official submission.
The event extraction process consists of three
phases. First, event candidates are generated,
based on trigger words and their context, using the
ME and MLE probabilities pT (equation 1).
Second, individual arguments of an event are
generated. We calculate the MLE probability pR
of an argument role (e.g. Theme) to occur as part
of a given event type, as follows:
pR(Role |EventType) =
f(Role ? EventType)
f(EventType)
(5)
We obtained the best results on the development
corpus when combining the probabilities as:
pA =
pT ? pT ? pR
pT + pT + pR
(6)
We generate arguments, using an MLE syntac-
tic path and an ME argument model, as follows.
The syntactic path between the trigger word and
every term (protein or subordinate event) is con-
sidered. If they are syntactically connected, and
if the probability of a syntactic path to express an
event is above a threshold, it is selected. As this is
a filtering step, it negatively affects recall.
We calculate the MLE probability ppath that
a syntactic configuration fills an argument slot.
Syntactic configurations consist of the head word
(trigger) HWord, the head event type HType, the
dependent word DWord, the dependent event type
DType, and the syntactic path Path between them.
In order to deal with sparse data, we use a
smoothed model.
ppath(Arg |HWord, HType, DWord, DType, Path) =
1
w1+w2+w3
? (
w1 ?
f(HWord, HType, DWord, DType, Path?Arg)
f(HWord, HType, DWord, DType, Path) +
w2 ?
f(HType, DType, Path?Arg)
f(HType, DType, Path) +
w3 ?
f(HType, DType?Arg)
f(HType, DType) ) (7)
The weights were emprically set as w1 = 4,
w2 = 2 and w3 = 1.5. The fact that the weights
decrease approximates a back-off model. The final
probability had to be larger than 0.2.
We have also used an ME model which delivers
the probability parg that a term is the argument of
a specific event, see formula 3. If this ME model
predicts with a probability of above 80% that the
term is not an argument, the search fails. Other-
wise, the probabilities are combined. On the de-
velopment corpus, we achieved best results when
using the harmonic mean:
pargument = 2 ?
ppath ? parg
ppath + parg
(8)
As a last step, the several arguments of an event
are combined into a frame. We have tested mod-
els predicting an entire frame directly, and models
combining the individual arguments generated in
the previous step. The latter approach performed
better. Any permutation of the argument candi-
dates could constitute a frame. Only frames seen
in the training corpus for a given event type are
considered. We have again used an ME and an
MLE model for predicting frames.
The ME model predicts pframe.ME , see for-
mula 2. We have also used two MLE models:
the first one delivers the probability pframe.MLE
based on the event type only, the second one
pframeword.MLE also considers the trigger word
and is much sparser (a low default is thus used for
unseen words). The probability of the individual
arguments also needs to be taken into considera-
tion. We used the mean of the individual argu-
ments? probabilities (pargs?mean).
5 Evaluation
In our analysis of errors, we noticed that frames
with more than one argument are created ex-
tremely rarely. The problem is that frames with
several arguments are rarer because the context
often does not offer the possibility to attach sev-
eral arguments. Therefore, we consistently un-
dergenerated with pargs?mean as outlined above.
118
Event Class gold (match) answer (match) recall prec. fscore
SVT-TOTAL 1117 ( 619) 851 ( 619) 55.42 72.74 62.91
EVT-TOTAL 1490 ( 698) 1103 ( 698) 46.85 63.28 53.84
REG-TOTAL 1694 ( 168) 618 ( 168) 9.92 27.18 14.53
All events total 3184 ( 866) 1721 ( 866) 27.20 50.32 35.31
Table 1: Results on the development set, measured using ?strict equality?.
Event Class gold (match) answer (match) recall prec. fscore
Gene expression 619 (400) 497 (400) 64.62 80.48 71.68
Transcription 101 (26) 100 (26) 25.74 26.00 25.87
Protein catabolism 14 (10) 15 (10) 71.43 66.67 68.97
Localization 99 (34) 39 (34) 34.34 87.18 49.28
=[SIMPLE ALL]= 833 (470) 651 (470) 56.42 72.20 63.34
Binding 333 (74) 264 (74) 22.22 28.03 24.79
Protein modification 1 (0) 0 (0) 0.00 0.00 0.00
Phosphorylation 160 (119) 168 (119) 74.38 70.83 72.56
Ubiquitination 30 (0) 0 (0) 0.00 0.00 0.00
Acetylation 0 (0) 0 (0) 0.00 0.00 0.00
Deacetylation 0 (0) 0 (0) 0.00 0.00 0.00
=[PROT-MOD ALL]= 191 (119) 168 (119) 62.30 70.83 66.30
Regulation 288 (23) 84 (23) 7.99 27.38 12.37
Positive regulation 1130 (129) 444 (129) 11.42 29.05 16.39
Negative regulation 526 (54) 166 (54) 10.27 32.53 15.61
=[REGULATION ALL]= 1944 (206) 694 (206) 10.60 29.68 15.62
==[EVENT TOTAL]== 3301 (869) 1777 (869) 26.33 48.90 34.23
Table 2: Results on the test data, measured using ?strict equality?.
We have added a number of heuristics to boost
multi-argument frames. Multiplying the probabil-
ity of a frame by its cubed length (giving two-
argument slots 9 times higher probability), and
giving Cause-slots 50% higher scores globally led
to best results.
We mainly trained and evaluated using the
?strict equality? evaluation criteria as our refer-
ence. The results on the development data are
shown in table 1. With more relaxed equality def-
initions, the results were always a few percentage
points better. Our results in the official test run are
shown in table 2. In sum, our submitted system
has good performance for simple events, bad per-
formance for Binding events, and a bias towards
precision due to a syntactic-based filtering step.
6 Conclusions and Future work
Our participation in the 2013 BioNLP shared task
was a useful opportunity to revise components of
the OntoGene pipeline and begin the implemen-
tation of a novel event generator. Due to lack of
time, it was not completed in time for the official
submission. We will continue its development and
use the BioNLP datasets.
Acknowledgments
This research is partially funded by the
Swiss National Science Foundation (grant
105315 130558/1).
References
[Clematide and Rinaldi2012] Simon Clematide and
Fabio Rinaldi. 2012. Ranking relations between
diseases, drugs and genes for a curation task.
Journal of Biomedical Semantics, 3(Suppl 3):S5.
[Clematide et al2011] Simon Clematide, Fabio Ri-
naldi, and Gerold Schneider. 2011. Ontogene at
calbc ii and some thoughts on the need of document-
wide harmonization. In Proceedings of the CALBC
II workshop, EBI, Cambridge, UK, 16-18 March.
[Daume? III2004] Hal Daume? III. 2004. Notes on
CG and LM-BFGS optimization of logistic regres-
sion. Paper available at http://pub.hal3.
name#daume04cg-bfgs, implementation avail-
able at http://hal3.name/megam/, August.
[Ekbal et al2013] Asif Ekbal, Sriparna Saha, and
Sachin Girdhar. 2013. Evolutionary approach for
classifier ensemble: An application to bio-molecular
event extraction. In Ajith Abraham and Sabu M
Thampi, editors, Intelligent Informatics, volume 182
of Advances in Intelligent Systems and Computing,
pages 9?15. Springer Berlin Heidelberg.
119
[Grover et al2000] Claire Grover, Colin Matheson, An-
drei Mikheev, and Marc Moens. 2000. Lt ttt - a flex-
ible tokenisation tool. In Proceedings of Second In-
ternational Conference on Language Resources and
Evaluation (LREC 2000).
[Kaljurand et al2009] Kaarel Kaljurand, Gerold
Schneider, and Fabio Rinaldi. 2009. UZurich in the
BioNLP 2009 Shared Task. In Proceedings of the
BioNLP workshop, Boulder, Colorado.
[Kim et al2013] Jin-Dong Kim, Yue Wang, and Ya-
mamoto Yasunori. 2013. The genia event extraction
shared task, 2013 edition - overview. In Proceedings
of BioNLP Shared Task 2013 Workshop, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
[Minnen et al2001] Guido Minnen, John Carroll, and
Darren Pearce. 2001. Applied morphological pro-
cessing of English. Natural Language Engineering,
7(3):207?223.
[Rinaldi et al2008] Fabio Rinaldi, Thomas Kappeler,
Kaarel Kaljurand, Gerold Schneider, Manfred Klen-
ner, Simon Clematide, Michael Hess, Jean-Marc
von Allmen, Pierre Parisot, Martin Romacker, and
Therese Vachon. 2008. OntoGene in BioCreative
II. Genome Biology, 9(Suppl 2):S13.
[Rinaldi et al2010] Fabio Rinaldi, Gerold Schneider,
Kaarel Kaljurand, Simon Clematide, Therese Va-
chon, and Martin Romacker. 2010. OntoGene in
BioCreative II.5. IEEE/ACM Transactions on Com-
putational Biology and Bioinformatics, 7(3):472?
480.
[Rinaldi et al2012a] Fabio Rinaldi, Simon Clematide,
Yael Garten, Michelle Whirl-Carrillo, Li Gong,
Joan M. Hebert, Katrin Sangkuhl, Caroline F. Thorn,
Teri E. Klein, and Russ B. Altman. 2012a. Using
ODIN for a PharmGKB re-validation experiment.
Database: The Journal of Biological Databases and
Curation.
[Rinaldi et al2012b] Fabio Rinaldi, Gerold Schneider,
and Simon Clematide. 2012b. Relation mining ex-
periments in the pharmacogenomics domain. Jour-
nal of Biomedical Informatics, 45(5):851?861.
[Rinaldi et al2013] Fabio Rinaldi, Simon Clematide,
Simon Hafner, Gerold Schneider, Gintare
Grigonyte, Martin Romacker, and Therese Va-
chon. 2013. Using the ontogene pipeline for
the triage task of biocreative 2012. The Journal
of Biological Databases and Curation, Oxford
Journals.
[Schneider2008] Gerold Schneider. 2008. Hy-
brid Long-Distance Functional Dependency Pars-
ing. Doctoral Thesis, Institute of Computational
Linguistics, University of Zurich.
[Tuggener et al2011] D Tuggener, M Klenner,
G Schneider, S Clematide, and F Rinaldi. 2011. An
incremental model for the coreference resolution
task of bionlp 2011. In BioNLP 2011, pages 151?
152. Association for Computational Linguistics
(ACL), June.
120
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 24?33,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Detecting Code-Switching in a Multilingual Alpine Heritage Corpus
Martin Volk and Simon Clematide
University of Zurich
Institute of Computational Linguistics
volk|siclemat@cl.uzh.ch
Abstract
This paper describes experiments in de-
tecting and annotating code-switching in
a large multilingual diachronic corpus of
Swiss Alpine texts. The texts are in En-
glish, French, German, Italian, Romansh
and Swiss German. Because of the mul-
tilingual authors (mountaineers, scientists)
and the assumed multilingual readers, the
texts contain numerous code-switching
elements. When building and annotating
the corpus, we faced issues of language
identification on the sentence and sub-
sentential level. We present our strategy
for language identification and for the an-
notation of foreign language fragments
within sentences. We report 78% precision
on detecting a subset of code-switches
with correct language labels and 92% un-
labeled precision.
1 Introduction
In the Text+Berg project we have digitized the
yearbooks of the Swiss Alpine Club (SAC) from
its first edition in 1864 until today. They contain
articles about mountain expeditions, the flora and
fauna of the Alpes and other mountain regions,
glacier and climate observations, geology and his-
tory papers, book reviews, accident and security
reports, as well as the protocols of the annual
club gatherings. The texts are in the four official
languages of Switzerland French, German, Italian
and Romansh
1
plus a few in English and Swiss
German dialects.
Because of the multilinguality of the authors
and readers, many articles are mixed-language
texts with inter-sentential and intra-sentential
1. Romansh is the 4th official language in Switzerland. It
is spoken by around 25,000 people in the mountainous South-
Eastern canton of Graub?unden.
code-switching. This poses a challenge for auto-
matically processing the texts. When we apply
Part-of-Speech (PoS) tagging, named entity recog-
nition or parsing, our systems need to know the
language that they are dealing with. Therefore we
had used a language identifier from the start of the
project to mark the language of each sentence. We
report on our experiences with sentence-based lan-
guage identification in section 3. Figure 1 shows
an example of a French text with an English ap-
pendix title plus an English quote from this book.
Lately we discovered that our corpus also
contains many intra-sentential code-switches. For
example, we find sentences like
... und ich finde es

very nice and de-
lightful

einen Vortrag halten zu d?urfen.
(Die Alpen, 1925) (EN : ... and I find it
very nice and delightful to be allowed to
give a talk.)
where the German sentence contains an English
phrase in quotation marks. Obviously, a German
PoS tagger will produce nonsense tags for the En-
glish phrase as the words will be unknown to it.
PoS taggers are good at tagging single unknown
words based on the surrounding context, but most
taggers fail miserably when a sequence of two
or more words is unknown. The upper half of fi-
gure 2 shows the PoS tagger output for the above
example. The words very, nice, delightful are sen-
selessly tagged as proper names (NE), only and is
tagged as foreign word (FM).
Our goal is to detect all intra-sentential code-
switches and to annotate them as exemplified in
the lower half of figure 2. They shall be framed
with the TEI-conformant tag <foreign> which
also shall specify the language of the foreign lan-
guage segment. All tokens in the segment shall be
tagged as foreign words (e.g. FM in the German
STTS tag set, ET in the French Le Monde tag set
(Abeill?e et al., 2003)), and each lemma shall get
24
the special symbol @fn@ to set it apart from lem-
mas of the surrounding sentence. In this paper we
report on our experiments towards this goal and
suggest an algorithm for detecting code-switching.
We adopt a wide definition of code-switching.
We are interested in detecting all instances where
a text is in a dominant language and contains
words, phrases and sentences in another language.
Though our definition is broad, it is clearly more
restricted than others, as e.g. the definition by
Kracht and Klein (2014) which includes special
purpose codes like bank account numbers or shoe
sizes.
In this paper we will give an overview of the
language mix in the yearbooks of the Swiss Al-
pine Club over the 150 years, and we will illus-
trate how we identified inter-sentential and intra-
sentential code-switching. We will give a quanti-
tative overview of the number of code-switching
candidates that we automatically located.
2 The Text+Berg Corpus
The Text+Berg corpus comprises the annual pu-
blications of the Swiss Alpine Club (SAC) from
its first edition in 1864 until 2013. From the start
until 1923 the official yearbook was called ?Jahr-
buch des Schweizer Alpen-Club? (EN : yearbook
of the Swiss Alpine Club), and it typically consis-
ted of 500 to 700 pages. The articles of these first
60 years were mostly in German (with 86% of
the words), but some also in French (13% of the
words) and few in Italian and Romansh (Volk et
al., 2010).
Interestingly, the German articles contained
passages in French and sometimes other languages
(e.g. English, Swiss German, Latin) without trans-
lations, and vice versa. Obviously, the article au-
thors and yearbook editors assumed that the rea-
ders of the yearbook were polyglott at least in En-
glish, French, German and Latin during that time.
In fact, the members of the SAC in the 19th cen-
tury came from an academic elite. Mountain ex-
ploration was a past-time of the rich and educated.
Still, during that same time the French-speaking
sections of the Swiss Alpine Club published their
own yearbook in parallel to the official yearbook
and called it ?Echo des Alpes?. It started shortly
after the official yearbook in the late 1860s and
continued until 1923. Each ?Echo des Alpes? year-
book contained between 300 to 600 pages adding
up to a total of 22,582 pages with 7.4 million to-
kens, almost all in French with rare quotes in Ger-
man.
As of 1925 the official SAC yearbook and the
?Echo des Alpes? were merged into a new publi-
cation called ?Die Alpen. Les Alpes. Le Alpi? (in
German, French, Italian) which has been publi-
shed ever since. Over the years it sometimes ap-
peared as quarterly and sometimes as monthly ma-
gazine. Today it appears 12 times per year in ma-
gazine format. For the sake of simplicity we conti-
nue to call each annual volume a yearbook.
The merger in 1925 resulted in a higher per-
centage of French texts in the new yearbook. For
example, the 1925 yearbook had around 143,000
words in German and 112,000 in French (56% to
44%). The ratio varied somewhat but was still at
64% to 36% in 1956.
From 1957 onwards, the SAC has published pa-
rallel (i.e. translated) French and German versions
of the yearbooks. At the start of this new era only
half of the articles were translated, the rest was
printed in the original language in identical ver-
sions in the two language copies.
Over the next decade the number of translations
increased and as of 1983 the yearbooks were com-
pletely translated between German and French.
Few Italian articles were still published verbatim
in both the French and German yearbooks. As of
2012 the SAC has launched an Italian language
version of its monthly magazine so that now it pro-
duces French, German and Italian parallel texts.
In its latest release the Text+Berg corpus (com-
prising the SAC yearbooks, the ALPEN maga-
zine and the Echo des Alpes) contains around
45.8 million tokens (after tokenization). French
and German account for around 22 million tokens
each, Italian accounts for 0.8 million tokens. The
remainder goes to English, Latin, Romansh and
Swiss German. The corpus is freely available for
research purposes upon request.
3 Language Identification in the
Text+Berg Corpus
We compiled the Text+Berg corpus by scanning
all SAC yearbooks from 1864 until 2000 (around
100,000+ pages). Afterwards we employed com-
mercial OCR software to convert the scan images
into electronic text. We developed and applied
techniques to automatically reduce the number of
OCR errors (Volk et al., 2011).
We obtained the yearbooks from 2001 to
25
FIGURE 1 ? Example of an English title and an English quote in a French text (Die Alpen, 1955)
FIGURE 2 ? Example of an annotated German sentence with English segment, before and after code-
switch detection (Die Alpen, 1925)
26
2009 as PDF documents which we automatically
converted to text. The subsequent yearbooks from
2010 until 2013 we received as XML files from
the SAC.
We have turned the whole corpus into a uniform
XML format. For this, the OCR output texts as
well as the texts converted from PDF and XML
are structured and annotated by automatically mar-
king article boundaries, by tokenization, language
identification, Part-of-Speech tagging and lemma-
tization. Our processing pipeline also includes to-
ponym recognition and geo-coding of mountains,
glaciers, cabins, valleys, lakes and towns. Further-
more we recognize and co-reference person names
(Ebling et al., 2011), and we annotate temporal
expressions (date, time, duration and set) with a
variant of HeidelTime (Rettich, 2013). Finally we
analyze the parallel parts of our corpus and pro-
vide sentence alignment information that is com-
puted via BLEUalign (Sennrich and Volk, 2011).
In order to process our texts with language-
specific tools (e.g. PoS tagging and person name
recognition) we employed automatic language
identification on the sentence level. We used
Lingua-Ident
2
(developed by Michael Piotrowski)
to determine for each sentence in our corpus whe-
ther it is in English, French, German, Italian or Ro-
mansh. Lingua-Ident is a statistical language iden-
tifier based on letter n-gram frequencies. For long
sentences it reliably distinguishes between the lan-
guages. Unfortunately it often misclassifies short
sentences. Therefore we decided to use it only for
sentences with more than 40 characters. Shorter
sentences are assigned the language of the article.
This can be problematic for mixed language ar-
ticles. An alternative strategy would be to assign
the language of the previous sentence to short sen-
tences.
For sentences that Lingua-Ident judges as Ger-
man we run a second classifier that distinguishes
between Standard German and Swiss German dia-
lect text. Since there are no writing rules for Swiss
German dialects, they come in a variety of spel-
lings. We have compiled a list of typical Swiss
German words (e.g. Swiss-German : chli, chlii,
chlini, chline = German : klein, kleine = English :
small) that are not used in Standard German in or-
der to identify Swiss German sentences.
3
2. http ://search.cpan.org/dist/Lingua-Ident/
3. We are aware that the Text+Berg corpus contains also
occasional sentences (or sentence fragments) in other Ger-
man dialects (e.g. Austrian German, Bavarian German) and
Based on the language tag of each sentence
we are able to investigate coarse-grained code-
switching. Whenever the language of a sentence
deviates from the language of the article, we have a
candidate for code-switching. For example, in the
yearbook 1867 we find a German text (describing
the activities of the club) with a French quote :
Der Berichterstatter bemerkt dar?uber :
?On peut remarquer `a cette occasion
qu?il est rare que par un effort de l?es-
prit on puisse mettre du brouillard en
bouteille, et . . . ? Die etwas ?altere Sek-
tion Diablerets, deren Steuer Herr Au-
gust Bernus mit kundiger Hand . . .
Most code-switching occurs with direct speech,
quotes and book titles. The communicative goal is
obviously to make the text more authentic.
4 Related Work on Detection of
Code-Switching
Most previous work on automatically detecting
code-switching focused on the switches between
two known languages (whereas we have to deal
with a mix of 6 languages).
Solorio and Liu (2008) worked on real-time
prediction of code-switching points in Spanish-
English conversations. This means that the judge-
ment whether the current word is in a different lan-
guage than the language of the matrix clause can
only be based on the previous words. They use the
PoS tag and its probability plus the lemma as pro-
vided by both the Spanish and the English Tree-
Tagger as well as the position of the word in the
Beginning-Inside-Outside scheme as features for
making the decision. In order to keep the number
of experiments manageable they restricted their
history to one or two preceding words. As an inter-
esting experiment they generated code-switching
sentences Spanish-English based on their different
predictors and asked human judges to rate the na-
turalness of the resulting sentences. This helped
them to identify the most useful code-switching
predictor.
Vu et al. (2013) and Adel et al. (2013) consi-
der English-Mandarin code-switching in speech
recognition. They investigate recurrent neural net-
work language models and factored language mo-
dels to the task in an attempt to integrate syntac-
tic features. For the experiments they use SEAME,
in old German spellings. Since these varieties are rare in the
corpus, we do not deal with them explicitly.
27
the South East Asia Mandarin-English speech cor-
pus compiled from Singaporean and Malaysian
speakers. It consists of spontaneous interviews
and conversations. The transcriptions were clea-
ned and each word was manually tagged as En-
glish, Mandarin or other. The data consists of an
intensive mix of the two languages with the ave-
rage duration of both English and Mandarin seg-
ments to be less than a second ( !). In order to as-
sign PoS tags to this mixed language corpus, the
authors applied two monolingual taggers and com-
bined the results.
Huang and Yates (2014) also work on the de-
tection of English-Chinese code-switching but not
on speech but rather on web forum texts produ-
ced by Chinese speakers living in the US. They
use statistical word alignment and a Chinese lan-
guage model to substitute English words in Chi-
nese sentences with suitable Chinese words. Pre-
paring the data in this way significantly improved
Machine Translation quality. Their approach is li-
mited to two known languages and to very short
code-switching phrases (typically only one word).
Tim Baldwin and his group (Hughes et al.,
2006) have surveyed the approaches to language
identification at the time. They found a number of
missing issues, such as language identification for
minority languages, open class language identifi-
cation (in contrast to identification within a fixed
set of languages), sparse training data, varying
encodings, and multilingual documents. Subse-
quently they (Lui and Baldwin, 2011) introduced a
system for language identification of 97 languages
trained on a mixture of corpora from different
domains. They claim that their system Langid is
particularly well suited for classifying short input
strings (as in Twitter messages). We therefore tes-
ted Langid in our experiments for code-switching
detection.
5 Exploratory Experiments with the
SAC Yearbook 1925
In order to assess the performance of Langid
for the detection of code-switching we performed
an exploratory experiment with the SAC yearbook
1925. We extracted all word sequences between
pairs of quotation marks where at least one token
had been assigned the ?unknown? lemma by our
PoS tagger. The ?unknown? lemma indicates that
this word sequence may come from a different lan-
guage.
The word sequence had to be at least 4 cha-
racters long, thus skipping single letters and ab-
breviations. In this way we obtained 333 word
sequences that are potential candidates for intra-
sentential code-switching. We then ran these word
sequences through the Langid language identifica-
tion system with the restriction that we expect the
word sequences only to be either English, French,
German, Italian or Latin (Romansh and Swiss Ger-
man are not included in Langid). For a given string
Langid delivers the most likely language together
with a confidence score.
We then compared the language predicted by
the Langid system with the (automatically) com-
puted language of the complete sentence. In 189
out of the 333 sentences the Langid output pre-
dicted a code-switch. We then manually graded all
Langid judgements and found that 225 language
judgements (67.5%) were correct. But only 89 of
the 189 predicted code-switches came with the
correct language. 40 of the 100 incorrect judge-
ments were actually code-switches but with a dif-
ferent language. The remaining ones should have
been classified with the same language as the sur-
rounding sentence and are thus no examples of
code-switching.
A closer inspection of the results revealed that
the book contained not only code-switches in the
expected 5 languages, but also into Romansh (6),
Spanish (4) and Swiss-German (13). Obviously all
of these were incorrectly classified. Most (8) of
the Swiss-German word sequences were classified
as German which could count as half correct, but
the others were misclassified as English (among
them a variant of the popular Swiss German fare-
well phrase uf Wiederluege spelled as uf?s Wieder-
luege).
The Langid system has a tendency to classify
word sequences as English. Many of the short, in-
correctly classified word sequences were judged
as English. It turns out that Langid judges even the
empty string as English with a score of 9.06. The-
refore all judgements with this score are dubious.
We found that 56 short word sequences were clas-
sified as English with this score, out of which 35
were erroneously judged as English. Only strings
with a length of 15 and more characters that are
classified as English should be trusted. All others
need to be discarded.
In general, if precision is the most important as-
pect, then Langid should only be used for strings
28
SAC yearbooks candidates predicted code-sw correct wrong lang no code-sw
1868 to 1878 388 121 88 33 13
1926 to 1935 792 335 266 69 23
Total 1180 456 354 102 36
TABLE 1 ? Recognition of code-switches in the Text+Berg corpus
with 20 or more characters. In our test set only 4
strings that were longer than 20 characters were
incorrectly classified within the selected language
set. Among the errors was the famous Latin phrase
conditio sine qua non (length : 21 characters inclu-
ding blanks) which Langid incorrectly classified
as Italian.
Another reason for the considerable number of
misclassifications can be repeated occurrences of
a word sequence. Our error count is a token-based
count and thus prone to misclassified recurring
phrases. In our experiment, Langid misclassified
the French book name Echo des Alpes as Italian.
Unfortunately this name occurs 18 times in our
test set and thus accounts for 18 errors. We suspect
that an -o at the end of a word is a strong indicator
for Italian. In a short string like Echo des Alpes (14
characters), this can make the difference.
Another interesting observation is that hyphens
speak for German. Our test set contains the hy-
phenated French string vesse-de-neige which Lan-
gid misclassifies as German with a clear margin
over French. When the same string is analyzed
without hyphens, then Langid correctly computes
a preference for French over German. A similar
observation comes from the Swiss German phrase
uf?s Wiederluege being classified as English when
spelled with the apostrophe (which is less frequent
in German than in English). Without the apos-
trophe Langid would count the string as German.
With short strings like this, special symbols have a
visible impact on the language identification.
We also observed that Langid is sensitive to
all-caps capitalization. For example, AUS DEM
LEBEN DER GEBIRGSMUNDARTEN (EN : The
Lives of Mountain Dialects) is misclassified as En-
glish (with the default score) while Aus dem Le-
ben der Gebirgsmundarten is correctly classified
as German.
Overall, we found that code-switching within
the same article rarely targets different languages.
For example, if the article is in German and
contains code-switches into English, then it hardly
ever contains code-switches into other languages.
In analogy to the one-sense-per-discourse hypo-
thesis we might call this the one-code-switch-
language-per-discourse hypothesis.
6 Detecting Intra-sentential
Code-Switching
Based on exploratory studies and observations
we decided on the following algorithm for detec-
ting and annotating intra-sentential foreign lan-
guage segments in the Text+Berg corpus. We
search for sub-sentential token sequences (possi-
bly of length 1) that are framed by a pair of quota-
tion marks and that contain at least one ?unknown?
lemma. There must be at least two tokens outside
of the quotation marks in the same sentence. As
a compromise we restrict our detection to strings
longer than 15 characters so that we get relati-
vely reliable language judgements by Langid. The
strings may consist of one token that is longer than
15 characters (e.g. Matterhornhochtourist) or a se-
quence of tokens whose sum of characters inclu-
ding blanks is more than 15. We feed these can-
didate strings to Langid for language identifica-
tion and compare the output language with the lan-
guage attribute of the surrounding sentence. If the
languages are different, then we regard the token
sequence as code-switch and mark it accordingly
in XML as shown in figure 2.
In order to determine the precision of this al-
gorithm, we checked 10 yearbooks from 1868 to
1878 (there was no yearbook in 1870) and from
1926 to 1935. The results are in table 1. From
the 1180 code-switch candidates that we compu-
ted based on the above restrictions, Langid predic-
ted 456 code-switches (39%). This means that in
39% of the cases Langid predicted a language that
was different from the language of the surrounding
sentence.
We manually evaluated all 456 predicted code-
switches and found that 354 of them (78%) were
correctly classified and labeled. These segments
were indeed in a different language than the sur-
rounding sentence and their language was cor-
rectly determined. For example, the French seg-
29
SAC yearbooks
> 15 characters
without unknowns
? 15 characters
all sample : TN/FN all sample : TN/FP
1868 to 1878 322 20/1 404 15/8
1926 to 1935 1944 78/1 1136 54/23
Total 2266 (2%) 98/2 1540 (31%) 69/31
TABLE 2 ? Estimation of the loss of recall due to the filtering approach based on a random sample of 100
quotations for each filtering category (TN : true negatives, FN : false negatives)
ment in the following German sentence is cor-
rectly detected and classified :
Anschliessend f?uhrte Ambros dasselbe
Bergsteigertrio

dans des circonstances
tr`es d?efavorables

auf den Monte Rosa
... (Die Alpen, 1935) (EN : After-
wards Ambros led the same 3 mountai-
neers

under very unfavorable condi-
tions

onto Monte Rosa.)
Out of the 102 segments whose language
was wrongly classified, only 36 were no code-
switches. For example, the Latin segment cum
grano salis africani is indeed a code-switch in
a German sentence although Langid incorrectly
classifies it as English. In fact, our evaluation sho-
wed that Langid is ?reluctant? to classify strings as
Latin. Latin strings are often misclassified as En-
glish or Italian.
Overall this means that only 8% of the predicted
code-switches are no code-switches. Therefore we
can safely add the module for code-switch detec-
tion into our processing and annotation pipeline.
In order to estimate the recall of our quota-
tion filtering approach we manually evaluated a
sample of the quotations that our algorithm exclu-
ded. Table 2 presents the numbers for the two time
periods for two cases : first for sequences that are
longer than 15 characters and contain only known
lemmas, second for sequences that are shorter than
16 characters and contain at least one ?unknown?
lemma. For both cases we checked 100 instances.
The evaluation for the quotations with more
than 15 characters but with all known lemmas (no
?unknown? lemma) shows only 2 false negatives.
Therefore, we can conclude safely that most of the
code-switches with more than 15 characters were
included in our candidate set.
Table 2 also shows that there were 1540 quota-
tions with 15 or less characters. The manual ins-
pection of 100 randomly selected quotations re-
vealed that 31 indeed include foreign material.
Some of these quotations are geographic names,
e.g. the valley Bergell (EN/IT : Val Bregaglia),
where it is difficult to decide whether this should
be regarded as a code-switch. For this evaluation,
we sticked to the principle that a foreign geogra-
phic name in quotation marks counts as a code-
switch. The number of missed code-switches is
high (31%). However, due to the limited preci-
sion of Langid (and other character-based lan-
guage identifiers) for short character sequences,
we still consider our length threshold appropriate.
A different approach to language identification is
needed to reliably classify these short quotes.
7 Discussion
The correctly marked code-switches in our test
periods can be split by language of the matrix sen-
tence and the language of the sub-sentential seg-
ment (= the code-switch segment). Table 3 gives
an overview of the types of code-switches for the
two periods under investigation. We see clearly
that code-switches from German to English were
rare in the 19th century (8 out of 89 = 9%) but be-
came much more popular in the 1920s and 1930s
(61 out of 265 = 23%). This came at the cost of
French which lost ground from 54% (48 out of 89)
to 40% (106 out of 265).
One can only compare the code-switch num-
bers from German with the corresponding num-
bers from French after normalizing the numbers
in relation to the overall amount of text in Ger-
man and French. During the first period (1868 to
1878) we count roughly 200,000 tokens in French
and 1.4 million tokens in German, whereas in the
second period (1926 to 1935) we have around 1
million tokens in French and again 1.4 million
tokens in German. For the first period we find
87 code-switches (triggered by quotation marks)
in the 1.4 million German tokens compared to
189 code-switches in the second period. The num-
30
sent
lang
segm
lang
1868 to
1878
1926 to
1935
de en 8 61
de fr 48 106
de it 24 19
de la 7 3
fr de 2 35
fr en - 20
fr it - 11
fr la - 2
it de - 3
it en - 2
it fr - 3
Total 89 265
TABLE 3 ? Correctly detected code-switches in the
Text+Berg corpus
sent
lang
segm
lang
1868 to
1878
1926 to
1935
de en 13 23
de fr 9 5
de it 8 12
de la 2 1
fr de - 7
fr en 1 10
fr it - 8
fr la - 1
it en - 2
Total 33 69
TABLE 4 ? Incorrectly labeled code-switches in
the Text+Berg corpus
ber of code-switches have clearly increased. For
French we observe the same trend with 2 code-
switches in 200?000 words in the first period com-
pared to 68 code-switches in the 1 million tokens
in the second period.
There is also a striking difference between
French and German with many more code-
switches in German than in French. For instance,
for German we find 135 code-switches per 1 mil-
lion tokens in the second period vs. 68 code-
switches per 1 million tokens for French.
One surprising finding were the code-switches
into Latin. We had not noticed them before, since
our corpus does not contain longer passages of La-
tin text. But this study shows that code-switches
correct
segm Langid prediction
lang en it fr la de Total
la 15 12 3 1 31
de 7 5 5 1 18
fr 7 3 10
it 6 6
es 3 1 2 6
rm 1 2 3
ru 1 1
id 1 1
Total 40 22 10 3 1 76
TABLE 5 ? Confusion matrix for incorrectly labe-
led code-switches in the periods 1868 to 1878 and
1926 to 1935
into Latin persisted into the 1920s (3 out of Ger-
man and 2 out of French).
On the negative side (cf. table 4), misclassi-
fying segments as English is the most frequent
cause for a wrong language assignment in both
periods. Table 5 shows the confusion matrix which
contrasts the manually determined segment lan-
guage with the incorrect language predicted by
Langid. This confirms that Langid has a tendency
to classify short text segments as English. But
there are also a number of errors for Latin being
mistaken for Italian, and German being mistaken
for Italian or French.
As a general remark, it should be noted that an
n-gram-based language identifier has advantages
over a lexicon-based language identifier in the face
of OCR errors. In the yearbook 1926 we observed
the rare case of a whole English sentence having
been contracted to one token Ilovetobemothered.
Still, our code-switch detector recognizes this as
an English string.
4
8 Conclusions
We have described our efforts in language iden-
tification in a multilingual corpus of Alpine texts.
As part of corpus annotation we have identified
the language of each corpus sentence amongst En-
glish, French, Standard German, Swiss German,
4. The complete sentence is : Un long Anglais, avec le-
quel, dans le hall familial, je m?essaie `a ?echanger laborieu-
sement quelques impressions `a ce sujet, me dit :

I love to be
mothered.

31
Italian and Romansh. Furthermore we have de-
veloped an algorithm to identify intra-sentential
code-switching by analyzing sentence parts in
quotation marks that contain ?unknown? lemmas.
We have shown that token sequences that
amount to 15 or more characters can be judged by
a state-of-the-art language identifier and will result
in 78% correctly labeled code-switches. Another
14% are code-switches but with a language dif-
ferent from the auto-assigned language. Only 8%
are not code-switches at all.
There are many ways to continue and extend
this research. We have not included language iden-
tification for Swiss German nor for Romansh in
the intra-sentential code-switch experiments re-
ported in this paper. We will train language models
for these two languages and add them to Langid
to check the impact on the recognition accuracy.
Since code-switches into Romansh are rare, and
since Romansh can easily be confused with Ita-
lian, it is questionable whether the addition of this
language model will have a positive influence.
We have used the ?general-purpose? language
identifier Langid in these experiments. It will be
interesting to investigate language identifiers that
are optimized for short text fragments as discus-
sed by Vatanen et al. (2010). Given the relati-
vely high number of short quotations (31%) that
contain code-switches, recall could improve consi-
derably.
In this paper we have focused solely on code-
switching candidates that are triggered by pairs of
quotation marks. In order to increase the recall we
will certainly enlarge the set of triggers to other in-
dicators such as parentheses or commas. We have
briefly looked at parentheses as trigger symbols
and found them clearly less productive than quo-
tation marks. To also find code-switches that have
no overt marker remains the ultimate goal.
Finally, we will exploit the parallel parts of our
corpus. If a sentence in German contains a French
segment, then it is likely that this French segment
occurs verbatim in the parallel French sentence.
Based on sentence and word alignment we will
search for identical phrases in both language ver-
sions. We hope that this will lead to high accuracy
code-switch data that we can use as training mate-
rial for machine learning experiments.
Acknowledgments
We would like to thank Michi Amsler and
Don Tuggener for useful comments on literature
and tools for language identification and code-
switching, as well as Patricia Scheurer for com-
ments and suggestions on the language use in
the SAC corpus. This research was supported
by the Swiss National Science Foundation under
grant CRSII2 147653/1 through the project ?MO-
DERN : Modelling discourse entities and relations
for coherent machine translation?.
References
Anne Abeill?e, Lionel Cl?ement, and Francois Tousse-
nel. 2003. Building a Treebank for French. In Anne
Abeill?e, editor, Building and Using Parsed Corpora,
volume 20 of Text, Speech and Language Tech-
nology, chapter 10, pages 165?187. Kluwer, Dor-
drecht.
Heike Adel, Ngoc Thang Vu, and Tanja Schultz. 2013.
Combination of recurrent neural networks and fac-
tored language models for code-switching language
modeling. In Proceedings of the 51st Annual Mee-
ting of the Association for Computational Linguis-
tics (ACL), Sofia.
Sarah Ebling, Rico Sennrich, David Klaper, and Martin
Volk. 2011. Digging for names in the mountains :
Combined person name recognition and reference
resolution for German alpine texts. In Proceedings
of The 5th Language & Technology Conference :
Human Language Technologies as a Challenge for
Computer Science and Linguistics, Poznan.
Fei Huang and Alexander Yates. 2014. Improving
word alignment using linguistic code switching data.
In Proceedings of the 14th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 1?9, G?oteborg.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew Mackinlay. 2006. Reconsi-
dering language identification for written language
resources. In Proceedings of LREC 2006, pages
485?488, Genoa.
Marcus Kracht and Udo Klein. 2014. The grammar
of code switching. Journal of Logic, Language and
Information, 23(3) :313?329.
Marco Lui and Timothy Baldwin. 2011. Cross-
domain feature selection for language identification.
In Proceedings of 5th International Joint Conference
on Natural Language Processing, pages 553?561,
Chiang Mai, Thailand. Asian Federation of Natural
Language Processing.
Katrin Rettich. 2013. Automatische Annotation
von deutschen und franz?osischen temporalen Aus-
dr?ucken im Text+Berg-Korpus. Master thesis, Uni-
versit?at Z?urich, Institut f?ur Computerlinguistik.
Rico Sennrich and Martin Volk. 2011. Itera-
tive, MT-based sentence alignment of parallel texts.
32
In Proceedings of The 18th International Nordic
Conference of Computational Linguistics (Noda-
lida), Riga.
Thamar Solorio and Yang Liu. 2008. Learning to pre-
dict code-switching points. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 973?981, Honolulu. Asso-
ciation for Computational Linguistics.
Tommi Vatanen, Jaakko J. V?ayrynen, and Sami Vir-
pioja. 2010. Language identification of short text
segments with n-gram models. In Proceedings of
LREC, pages 3423?3430, Malta.
Martin Volk, Noah Bubenhofer, Adrian Althaus, Maya
Bangerter, Lenz Furrer, and Beni Ruef. 2010. Chal-
lenges in building a multilingual alpine heritage cor-
pus. In Proceedings of LREC, Valletta, Malta.
Martin Volk, Lenz Furrer, and Rico Sennrich. 2011.
Strategies for reducing and correcting OCR errors.
In C. Sporleder, A. van den Bosch, and K. Zerva-
nou, editors, Language Technology for Cultural He-
ritage : Selected Papers from the LaTeCH Work-
shop Series, Theory and Applications of Natural
Language Processing, pages 3?22. Springer-Verlag,
Berlin.
Ngoc Thang Vu, Heike Adel, and Tanja Schultz.
2013. An investigation of code-switching atti-
tude dependent language modeling. In Statistical
Language and Speech Processing, pages 297?308.
Springer.
33
