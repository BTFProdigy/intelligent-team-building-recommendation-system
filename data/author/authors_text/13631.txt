Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1002?1010,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Reformulating Discourse Connectives for Non-Expert Readers
Advaith Siddharthan Napoleon Katsos
Department of Computing Science Research Centre for English and Applied Linguistics
University of Aberdeen University of Cambridge
advaith@abdn.ac.uk nk248@cam.ac.uk
Abstract
In this paper we report a behavioural ex-
periment documenting that different lexico-
syntactic formulations of the discourse rela-
tion of causation are deemed more or less ac-
ceptable by different categories of readers. We
further report promising results for automati-
cally selecting the formulation that is most ap-
propriate for a given category of reader using
supervised learning. This investigation is em-
bedded within a longer term research agenda
aimed at summarising scientific writing for lay
readers using appropriate paraphrasing.
1 Introduction
There are many reasons why a speaker/writer would
want to choose one formulation of a discourse rela-
tion over another; for example, maintaining thread
of discourse, avoiding shifts in focus and issues of
salience and end weight. There are also reasons to
use different formulations for different audiences;
for example, to account for differences in reading
skills and domain knowledge. In this paper, we
present a psycholinguistic experiment designed to il-
luminate the factors that determine the appropriate-
ness of particular realisations of discourse relations
for different audiences. The second part of this pa-
per focuses on training a natural language generation
system to predict which realisation choices are more
felicitous than others for a given audience. Our para-
phrases include eight different constructions. Con-
sider 1a.?d. below:
(1) a. Tom ate because he was hungry.
b. Tom ate because of his hunger.
c. Tom?s hunger caused him to eat.
d. The cause of Tom?s eating was his hunger.
These differ in terms of the lexico-syntactic prop-
erties of the discourse marker (shown in bold font).
Indeed the discourse markers here are conjunctions,
prepositions, verbs and nouns. As a consequence
the propositional content is expressed either as a
clause or a noun phrase (?he was hungry? vs ?his
hunger?, etc.). Additionally, the order of presenta-
tion of propositional content can be varied to give
four more lexico-syntactic paraphrases:
(1) e. Because Tom was hungry, he ate.
f. Because of his hunger, Tom ate.
g. Tom?s eating was caused by his hunger.
h. Tom?s hunger was the cause of his eating.
It is clear that some formulations of this propo-
sitional content are more felicitous than others; for
example, 1a. seems preferable to 1d., but for a
different propositional content, other formulations
might be more felicitous (for instance, example 4,
section 3.1, where the passive seems in fact prefer-
able). While discourse level choices based on infor-
mation ordering play a role in choosing a formula-
tion, it is of particular interest to us that some de-
contextualised information orderings within a sen-
tence are deemed unacceptable. Any summarisation
task that considers discourse coherence should not
introduce sentence-level unacceptability.
We now summarise our main research questions:
1. Are some formulations of a discourse relation more
felicitous than others, given the same propositional
content?
2. Does the reader?s level of domain expertise affect
their preferred formulation?
3. What linguistic features determine which formula-
tions are acceptable?
4. How well can a natural language generator be
trained to predict the most appropriate formulation
for a given category of reader?
In this paper, we focus on causal relations because
these are pervasive in science writing and are inte-
gral to how humans conceptualise the world. The
8 formulations selected are 2 information orderings
1002
of 4 different syntactic constructs; thus we explore a
fairly broad range of constructions.
With regard to genre, we have a particular in-
terest in scientific writing, specifically biomedical
texts. Reformulating such texts for lay audiences is
a highly relevant task today and many news agen-
cies perform this service; e.g., Reuters Health sum-
marises medical literature for lay audiences and
BBC online has a Science/Nature section that re-
ports on science. These services rely either on press
releases by scientists and universities or on special-
ist scientific reporters, thus limiting coverage of a
growing volume of scientific literature in a digital
economy. Thus, reformulating technical writing for
lay audiences is a research area of direct relevance to
information retrieval, information access and sum-
marisation systems.
At the same time, while there are numerous stud-
ies about the effect of text reformulation on people
with different literacy levels or language deficits (see
section 2), the issue of expert vs lay audiences has
received less attention. Further, most studies focus
on narrative texts such as news or history. However,
as Linderholm et al (2000) note, results from studies
of causality in narrative texts might not carry over to
scientific writing, because inferences are made more
spontaneously during the reading of narrative than
expository texts. Thus comparing expert vs lay read-
ers on the comprehension of causal relations in sci-
entific writing is a most timely investigation.
In section 2, we relate our research to the exist-
ing linguistic, psycholinguistic and computational
literature. Then in section 3, we describe our psy-
cholinguistic experiment that addresses our first two
research questions and in section 4 we present a
computational approach to learning felicitous para-
phrases that addresses the final two questions.
2 Background and related work
2.1 Expressing causation
Linguists generally consider five different compo-
nents of meaning (Wolff et al, 2005) in causal ex-
pressions: (a) occurrence of change in patient, (b)
specification of endstate, (c) tendency and concor-
dance, (d) directness and (e) mechanism. The ex-
pressions we consider in this paper, ?because? (con-
junction), ?because of? (preposition) and ?cause?
as noun or verb (periphrastic causatives) express
(a), (b) and in some instances, (c). This is in contrast
to affect verbs that only express (a), link verbs that
express (a?b), lexical causatives that express (a?d)
and resultatives that express (a?e). These distinc-
tions are illustrated by the sentences in example 2:
(2) a. Sara kicked the door. (affect verb ? end
state not specified)
b. The door?s breaking was linked to Sara.
(link verb ? end state specified, but un-
clear that door has a tendency to break)
c. Sara caused the door to break. / The
door broke because of Sara. (periphrastic
/ preposition ? indirect; the door might
have a tendency to break)
d. Sara broke the door. (lexical causative ?
directness of action is specified)
e. Sara broke the door open. (resultative ?
end state is ?open?)
There is much literature on how people prefer
one type of causative over the other based on these
five components of meaning (e.g. see Wolff et al
(2005)). What is less understood is how one selects
between various expressions that carry similar se-
mantic content. In this paper we consider four con-
structs ?because of?, ?because?, and ?cause? as a
verb and a noun. These express the components of
meaning (a?c) using different syntactic structures.
By considering only these four lexically similar con-
structs, we can focus on the role of the lexis and of
syntax in determining the most felicitous expression
of causation for a given propositional content.
2.2 Discourse connectives and comprehension
Previous work has shown that when texts have been
manually rewritten to make discourse relations such
as causation explicit, reading comprehension is sig-
nificantly improved in middle/high school students
(Beck et al, 1991). Further, connectives that permit
pre-posed adverbial clauses have been found to be
difficult for third to fifth grade readers, even when
the order of mention coincides with the causal (and
temporal) order; for instance, 3b. is more accessible
than 3a. (e.g. from Anderson and Davison (1988)).
(3) a. Because Mexico allowed slavery, many
Americans and their slaves moved to
Mexico during that time.
b. Many Americans and their slaves moved
to Mexico during that time, because
Mexico allowed slavery.
1003
Such studies show that comprehension can be im-
proved by reformulating text; e.g., making causal
relations explicit had a facilitatory effect for read-
ers with low reading skills (Linderholm et al, 2000;
Beck et al, 1991) and for readers with low levels of
domain expertise (Noordman and Vonk, 1992). Fur-
ther, specific information orderings were found to be
facilitatory by Anderson and Davison (1988).
However, it has not been investigated whether
readers with different levels of domain expertise are
facilitated by any specific lexico-syntactic formula-
tion among the many possible explicit realisations of
a relation. This is a novel question in the linguistics
literature, and we address it in section 3.
2.3 Connectives and automatic (re)generation
Much of the work regarding (re)generation of text
based on discourse connectives aims to simplify
text in certain ways, to make it more accessible
to particular classes of readers. The PSET project
(Carroll et al, 1998) considered simplifying news
reports for aphasics. The PSET project focused
mainly on lexical simplification (replacing difficult
words with easier ones), but more recently, there
has been work on syntactic simplification and, in
particular, the way syntactic rewrites interact with
discourse structure and text cohesion (Siddharthan,
2006). Elsewhere, there has been renewed interest in
paraphrasing, including the replacement of words
(especially verbs) with their dictionary definitions
(Kaji et al, 2002) and the replacement of idiomatic
or otherwise troublesome expressions with simpler
ones. The current research emphasis is on auto-
matically learning paraphrases from comparable or
aligned corpora (Barzilay and Lee, 2003; Ibrahim et
al., 2003). The text simplification and paraphrasing
literature does not address paraphrasing that requires
syntactic alterations such as those in example 1 or
the question of appropriateness of different formula-
tions of a discourse relation.
Some natural language generation systems in-
corporate results from psycholinguistic studies to
make principled choices between alternative formu-
lations. For example, SkillSum (Williams and Re-
iter, 2008) and ICONOCLAST (Power et al, 2003)
are two contemporary generation systems that allow
for specifying aspects of style such as choice of dis-
course marker, clause order, repetition and sentence
and paragraph lengths in the form of constraints that
can be optimised. However, to date, these systems
do not consider syntactic reformulations of the type
we are interested in. Our research is directly rele-
vant to such generation systems as it can help such
systems make decisions in a principled manner.
2.4 Corpus studies and treebanking
There are two major corpora that mark up discourse
relations ? the RST Discourse Treebank based on
Rhetorical Structure Theory (Mann and Thompson,
1988), and the Penn Discourse Treebank (Webber et
al., 2005). Neither is suitable for studies on the fe-
licity of specific formulations of a discourse relation.
As part of this research, we have created a corpus of
144 real text examples, reformulated in 8 ways, giv-
ing 1152 sentences in total.
There have been numerous corpus studies of dis-
course connectives, such as studies on the discourse-
role disambiguation of individual cue-phrases in
spoken and written corpora (e.g., Hirschberg and
Litman (1993)), the substitutability of discourse
connectives (e.g., Hutchinson (2005)), and indeed
corpus studies as a means of informing the choice
of discourse relations to consider in a theory (e.g.,
Knott and Dale (1994); Knott (1996)). A distin-
guishing feature of our approach relative to previ-
ous ones is an in-depth study of syntactic variations;
in contrast, for example, Knott?s taxonomy of dis-
course relations is based on the use of a substitution
text that precludes variants of the same relation hav-
ing different syntax.
3 Linguistic acceptability study
3.1 Dataset creation
We have constructed a dataset that can be used to
gain insights into differences between different real-
isations of discourse relations. In the following, we
will illustrate such rewriting situations using an ex-
ample from a medical article. As mentioned previ-
ously, we are particularly interested in complex syn-
tactic reformulations; in example 4 below, a. is from
the original text and b.?h. are reformulations. There
are two examples each of formulations using ?be-
cause?, ?because of ?, the verb ?cause? and the noun
?cause? with different ordering of propositional con-
tent. This provides us with 8 formulations per exam-
ple sentence; for example:
1004
(4) a. Fructose-induced hypertension is caused
by increased salt absorption by the intes-
tine and kidney. [cause p]
b. Increased salt absorption by the intestine
and kidney causes fructose-induced hy-
pertension. [cause a]
c. Fructose-induced hypertension occurs
because of increased salt absorption by
the intestine and kidney. [a becof b]
d. Because of increased salt absorption by
the intestine and kidney, fructose-induced
hypertension occurs. [becof ba]
e. Fructose-induced hypertension occurs
because there is increased salt absorption
by the intestine and kidney. [a bec b]
f. Because there is increased salt absorp-
tion by the intestine and kidney, fructose-
induced hypertension occurs. [bec ba]
g. Increased salt absorption by the intes-
tine and kidney is the cause of fructose-
induced hypertension. [b causeof a]
h. The cause of fructose-induced hyperten-
sion is increased salt absorption by the in-
testine and kidney. [causeof ab]
Our corpus contains 144 such examples from three
genres (see below), giving 1152 sentences in total.
These 144 examples contain equal numbers of orig-
inal sentences (18) of each of the 8 types. The man-
ual reformulation is formulaic, and it is part of our
broader research effort to automate the process using
transfer rules and a bi-directional grammar. The ex-
ample above is indicative of the process. To make a
clause out of a noun phrase (examples 4c.?f.), we in-
troduce either the copula or the verb ?occur?, based
on a subjective judgement of whether this is an event
or a continuous phenomenon. Conversely, to create
a noun phrase from a clause, we use a possessive and
a gerund; for example (simplified for illustration):
(5) a. Irwin had triumphed because he was so
good a man.
b. The cause of Irwin?s having triumphed
was his being so good a man.
Clearly, there are many different possibilities for
this reformulation; for example:
(5) b?. The cause of Irwin?s triumph was his be-
ing so good a man.
b?. The cause of Irwin?s triumph was his ex-
ceptional goodness as a man.
As part of our wider research agenda, we are ex-
ploring automatic reformulation using transfer rules
and a bi-directional grammar. In this context, given
our immediate interest is in the discourse markers,
we restrict our reformulation method to only gener-
ate sentences such as 5b. This not only makes au-
tomation easier, but also standardises data for our
experiment by removing an aspect of subjectivity
from the manual reformulation.
We used equal numbers of sentences from three
different genres1:
? PubMed Abstracts: Technical writing from the
Biomedical domain
? BNC World: Article from the British National Cor-
pus tagged as World News
? BNC Natural Science: Article from the British Na-
tional Corpus tagged as Natural Science. This cov-
ers popular science writing in the mainstream media
There were 48 example sentences chosen ran-
domly from each genre, such that there were 6 ex-
amples of each of the 8 types of formulation)
3.2 Experimental setup
Human judgements for acceptability for each of the
1152 sentences in our corpus were obtained using
the WebExp package (Keller et al, 2008 to appear).2
We investigated acceptability because it is a measure
which reflects both ease of comprehension and sur-
face well-formedness.
The propositional content of 144 sentences was
presented in 8 formulations. Eight participant
groups (A?H) consisting of 6 people each were pre-
sented with exactly one of the eight formulations
of each of 144 different sentences, as per a Latin
square design. Thus, while each participant read
an equal number of sentences in each formulation
type, they never read more than one formulation of
the same propositional content. Each group saw 18
original and 126 reformulated sentences in total, 48
from each genre. This experimental design allows
all statistical comparisons between the eight types
of causal formulations to be within-participants.
Acceptability judgements were elicited on the
sentences without presenting the preceding context
1PubMed URL: http://www.ncbi.nlm.nih.gov/pubmed/
The British National Corpus, version 3 (BNC XML Edition).
2007. Distributed by Oxford University Computing Services on
behalf of the BNC Consortium. http://www.natcorp.ox.ac.uk
2Note that the reformulations are, strictly speaking, gram-
matical according to the authors? judgement. We are testing
violations of acceptability, rather than grammaticality per se.
1005
from the original text. The participants were Univer-
sity of Cambridge students (all native English speak-
ers with different academic backgrounds). Post ex-
perimentally we divided participants in two groups
based on having a Science or a non-Science back-
ground3. Rather than giving participants a fixed
scale (e.g. 1?7), we used the magnitude estimation
paradigm, which is more suitable to capture robust
or subtle differences between the relative strength of
acceptability or grammaticality violations (see Bard
et al (1996); Cowart (1997); Keller (2000)).
3.3 Magnitude estimation
Participants were asked to score how acceptable a
modulus sentence was, using any positive number.
They were then asked to score other sentences rel-
ative to this modulus, using any positive number,
even decimals, so that higher scores were assigned
to more acceptable sentences. The advantage of
Magnitude estimation is that the researcher does
not make any assumptions about the number of lin-
guistic distinctions allowed. Each subject makes as
many distinctions as they feel comfortable. Scores
were normalised to allow comparison across partic-
ipants, following standard practice in the literature
by using the z-score: For each participant, each sen-
tence score was normalised so that the mean score is
0 and the standard deviation is 1:
zih =
xih ? ?h
?h
where zih is participant h?s z-score for the sentence
i when participant h gave a magnitude estimation
score of xih to that sentence. ?h is the mean and
?h the standard deviation of the set of magnitude
estimation scores for user h.
3.4 Results
42 out of 48 participants (19 science students and
23 non-science students) completed the experiment,
giving us 3?6 ratings for each of the 1152 sentences.
Figure 1 shows the average z-scores with standard
3Participants provided subject of study prior to participa-
tion in the experiment. Our classification of Science con-
sists of Life Sciences(Genetics/Biology/etc), Chemistry, Envi-
ronmental Science, Engineering, Geology, Physics, Medicine,
Pharmacology, Veterinary Science and Zoology. Non-Science
consists of Archaeology, Business, Classics, Education, Liter-
ature&Languages, International Relations, Linguistics, Maths,
Music, Politics and Theology.
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
Original Reformulated All
Non-S Science
Non-S Science
Non-S Science
BNC-World
BNC-NatSci
Pubmed
Figure 1: Preferences by Field of Study ? Science or
Non-Science.
error bars for Science and non-Science students for
each of the three genres. The first six columns
show the scores for only the 144 Original Sentences.
Note that science students find PubMed sentences
most acceptable (significantly more than BNC Nat-
ural Science; t-test, p < .005), while among non-
science students there is a numerical tendency to find
the world news sentences most acceptable. Both cat-
egories of participants disprefer sentences from the
popular science genre. Columns 7?12 show the av-
erage z-scores for the 1008 reformulated sentences.
Let us note that these are significantly lower than for
the originals (t-test, p < .001).
Some of these results are as expected. With regard
to genre preferences, scientists might find the style
of technical writing acceptable because of familiar-
ity with that style of writing. Second, with regard
to the average score for original and reformulated
sentences, some reformulations just don?t work for
a given propositional content. This pulls the aver-
age for reformulated sentences down. However, on
average 2 out of 7 reformulations score quite high.
It is interesting that the popular science genre is
least preferred by both groups. This suggests that
reformulating technical writing for lay readers is not
a trivial endeavour, even for journalists.
Now consider Figure 2, which shows the aver-
age z-scores for only PubMed sentences for science
and non-science students as a function of sentence
type. For non-science students reading PubMed sen-
tences, three formulations are strongly dispreferred
? ?a is caused by b?, ?because b, a? and ?b is the
1006
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
Science Not-Science
a-bec-b
a-becof-b
b-causeof-a
causeof-ab
bec-ba
becof-ba
cause-a
cause-p
Figure 2: PubMed type preferences
cause of a?. The last two are significantly lower than
?a because b?, ?a because of b? and ?because of b, a?
(t-test, .005 < p < .01). On the contrary, there are
no strong preferences among the science students
and all the error bars overlap. Let us now look at
some specific differences between science and non-
science students:
1. Science students prefer sentences in the passive
voice, while these are strongly dispreferred by non-
science students. While active voice is the canon-
ical form in English, much of science is written in
the passive by convention. This difference can thus
be explained by different levels of exposure.
2. Non-science students disprefer the use of ?cause?
as a noun while science students don?t (columns 3?
4 and 11?12).
3. Non-science students prefer ?because of b, a? to
?because b, a? while science students show the op-
posite preference.
The lack of strongly dispreferred formulations in
the Science students is most likely due to two fac-
tors: (a) the group?s familiarity with this genre and
(b) their expert knowledge compensates for accept-
ability even for relatively odd formulations. In the
absence of exposure and background knowledge, the
non-Science students display clear preferences.4
Note that these preferences are not surprising.
The preference for canonical constructs such as ac-
tive voice and conjunction in infix position are well
documented. Our claim however, is that blindly
4While we only show the averages for all sentences, the dis-
tributions for original and reformulated sentences look remark-
ably similar.
Selection Method Av. z
Always select original sentence .61
Replace cause-p, b-causeof-a and causeof-ab
with cause-a & bec-ba with a-bec-b
.48
Replace cause-p with cause-a, b-causeof-a
with causeof-ab & bec-ba with a-bec-b
.47
Always select most preferred type (a-becof-b) .27
Table 1: Selecting a formulation of PubMed sentences for
non-science students using their global preferences.
rewriting all instances of globally dispreferred con-
structs with globally preferred constructs is counter-
productive because not all formulations are accept-
able for any given propositional content. This claim
is easily verified. Table 1 shows the average z-scores
of non-science students when one formulation of
each of the PubMed sentences is selected based only
on the global preferences in Figure 2. Such rewriting
invariably makes matters worse. In the next section
we present a more intelligent approach.
4 Machine learning experiment
The first question we address is: for a given propo-
sitional content, which formulations are acceptable
and which are not? This is a useful question for mul-
tiple reasons. In this paper, our interest stems from
our desire to selectively rewrite causation based on
the properties of the sentence as well as global pref-
erences of categories of users. More generally, this
information is important for summarisation tasks,
where sentences might appear in different contexts
and different information orderings might be de-
sirable for reasons of coherence. Knowing which
formulations are acceptable in isolation for a given
propositional content is thus important.
Since Magnitude estimation scores are freescale,
we first need to determine how high a score needs to
be for that formulation to be considered acceptable.
Our solution is to (a) treat the original formulation
as acceptable and (b) treat any reformulations with
a higher average z-score than the original as also ac-
ceptable. We find that roughly 3 formulations (the
original and another two) out of 8 are acceptable on
average. Our data is summarised below:
? 1152 Sentences in total (144 originals, 1008 refor-
mulations)
? 361 labelled as acceptable (31%; 144 origi-
nals, 217 reformulations)
1007
? 791 labelled as unacceptable (69%; 791 refor-
mulations)
4.1 Features
We use shallow features derived from the sentence,
as well as the textual genre. Sentences were parsed
using the RASP parser (Briscoe and Carroll, 2002).
The features we extract are as follows:
1. Type (8 values: cause a, cause p, a bec b, bec ba,
a becof b, becof ba, a causeof b, causeof ba)
2. Genre (3 values: pubmed, bnc-world, bnc-natsci)
3. Complexity: As an indication of the complexity of
the propositional content, we use the following:
(a) Length Features
? length (in words) of the sentence and each
clause
? length (as proportion of total length) of
each clause
(b) Whether the causative is embedded in a rela-
tive clause or other construct
(c) The presence or absence of copula in each
clause (e.g., ?because there is...?)
(d) Whether the causation is quantified (e.g., ?a
major cause of...?)
The only feature that varies between the eight for-
mulations of the same sentence is the ?type? feature;
the ?genre? and ?complexity? features are constant
across reformulations. The reason for using 3(c?d)
as features is that expressions such as ?because there
is? might be better formulated as ?because of? and
that it is hard to find an exact reformulation when
quantifiers are present (e.g., ?a major cause of? is
not equivalent to ?often because of?).
Machine performance on this task is not very
good (First Run, Table 2). The problem is that some
propositional content is harder to formulate than oth-
ers. Therefore good formulations of some proposi-
tional content might have much lower scores than
even mediocre formulations of other propositional
content. This makes it hard to learn a function that
distinguishes good from bad formulations for any
particular propositional content. To overcome this,
we run the classifier twice. Given 8 formulations
of 144 sentences Si=1..144,j=1..8, the first run gives
us 1152 probabilities pweka1(Sij) for the acceptabil-
ity of each sentence, independent of propositional
content (these are test-set probabilities using 10-fold
cross-validation). We then run the machine learner
again, with this new feature relative:
Classifier Accuracy Kappa
Baseline .69 0
First Run .72 .23
Second Run .85 .65
Only PubMed .89 .73
Table 2: Accuracy and Agreement of classifier relative to
human judgement.
Genre Class P R F
All Genres Good .72 .78 .75
Bad .91 .89 .90
Only PubMed Good .89 .89 .89
Bad .89 .97 .92
Table 3: Precision, Recall and F-measure of classifier
(second run) relative to human judgement.
? The ratio of the test-set probability (from the first
run) to the highest of the 8 test-set probabilities for
the different formulations of that sentence:
relativei=a,j=b =
pweka1(Si=a,j=b)
maxi=a,j=1..8(pweka1(Si=a,j))
Thus probabilities for acceptability are nor-
malised such that the best score for a given proposi-
tional content is 1 and the other 7 formulations score
less than or equal to 1. The second classifier uses
these relative probabilities as an extra feature.
4.2 Results
Our results are summarised in Table 2 (accu-
racy and agreement) and Table 3 (f-measure).
We experimented with the Weka toolkit (Wit-
ten and Frank, 2000) and report results using
?weka.classifiers.trees.J48 -C 0.3 -M 3? and 10-fold
cross-validation for both runs.5
Table 2 shows that the first run performs at around
baseline levels, but the second run performs signifi-
cantly better (using z-test, p=0.01 on % Accuracy),
with acceptable agreement of ? = 0.656. This in-
creases to 89% (? = .73) when we only consider
technical writing (PubMed genre). Table 3 shows
that precision, recall and f-measure are also around
.90 for PubMed sentences.
5J48 outperformed other Weka classifiers for this task.
6Following Carletta (1996), we measure agreement in ?,
which follows the formula K = P (A)?P (E)1?P (E) where P(A) is ob-
served, and P(E) expected agreement. ? ranges between -1 and
1. ?=0 means agreement is only as expected by chance. Gener-
ally, ? of 0.8 are considered stable, and ? of 0.69 as marginally
stable, according to the strictest scheme applied in the field.
1008
Left out feature First Run Second Run
Acc ? Acc ?
Length .71 -.01 .78 .33
Quantified .71 .20 .75 .36
Embedded .69 .15 .78 .37
Copula Present .72 .20 .79 .44
Table 4: Accuracy and Kappa of classifier when com-
plexity features are left out.
All our context features proved useful for the clas-
sification task, with the length features being the
most useful. Table 4 shows the performance of the
classifier when we leave out individual features.
It thus appears that we can determine the accept-
able formulations of a sentence with high accuracy.
The next question is how this information might be
used to benefit a text regeneration system. To evalu-
ate this, we combined our predictions with the user
preferences visible in figure 2 as follows:
? We calculate a prior prior j for each formulation of
type j using the z-score distribution for non-science
students in Figure 2.
? We calculate prior j=b.pweka2(Si=a,j=b) for each
formulation Si=a,j=b of sentence a and type b,
where pweka2(Si=a,j=b) is the probability returned
by the classifier (second run) for formulation b of
sentence a.
? Selectively Reformulate: We reformulate only
the four dispreferred constructs (cause p, bec ba,
causeof ab, b causeof a) using the formulation for
which the prior times the classifier probability
is the maximum; i.e, for sentence a, we select
max i=a,j=1..8(prior j .pweka2(Si=a,j)).
Table 5 shows the impact this reformulation has
on the acceptability of the sentences. Our algorithm
selects one formulation of each PubMed sentence
based on our prior knowledge of the preferences of
non-science students, and the Weka-probabilities for
acceptability of each formulation of a sentence. Our
selective reformulation increases the average z-score
from .613 to .713. This is now comparable with the
acceptability ratings of non-scientists for sentences
from the world news genre. Note that reformulation
only using priors resulted in worse results (Table 1).
However there remains scope for improvement. If
we had an oracle that selected the best formulation
of each sentence (as scored by non-scientists), this
would result in an average score of 1.04.
Genre Version z-score
PubMed Randomly Selected ?.17
PubMed Original Sentences .61
PubMed Selectively Reformulate .71
PubMed Selected by Oracle 1.04
BNC World Original Sentences .70
Table 5: Average z-scores for non-science students. Se-
lective reformulation increases the acceptability scores of
sentences drawn from technical writing to levels com-
parable to acceptability scores of sentences drawn from
news reports on world news (their most preferred genre).
5 Conclusions and future work
In this investigation we report that science and
non-science university students have different global
preferences regarding which formulations of causa-
tion are acceptable. Using surface features that re-
flect propositional complexity, a machine classifier
can learn which of 8 formulations of a discourse
relation are acceptable (with Accuracy = .89 and
Kappa = .73 for sentences from the PubMed genre).
Using the global preferences of non-science students
as priors, and combining these with machine clas-
sifier predictions of acceptability, we have demon-
strated that it is possible to selectively rewrite sen-
tences from PubMed in a manner that is personalised
for non-science students. This boosts the average z-
score for acceptability from .613 to .713 on PubMed
sentences, a level similar to scores of non-scientists
for sentences from their most preferred World News
genre. We have thus shown that there is potential for
reformulating technical writing for a lay audience ?
differences in preferences for expressing a discourse
relation do exist between lay and expert audiences,
and these can be learnt.
While in this paper we focus on the discourse re-
lation of causation, other discourse relations com-
monly used in scientific writing can also be realised
using markers with different syntactic properties; for
instance, contrast can be expressed using markers
such as ?while?, ?unlike?, ?but?, ?compared to?, ?in
contrast to? or ?the difference between?. As part of
our wider goals, we are in the process of extending
the number of discourse relations considered. We
are also in the process of developing a framework
within which we can use transfer rules and a bi-
directional grammar to automate such complex syn-
tactic reformulation.
1009
Acknowledgements
This work was supported by the Economic and So-
cial Research Council (Grant Number RES-000-22-
3272). We would also like to thank Donia Scott,
Simone Teufel and Ann Copestake for many dis-
cussions that influenced the scope of this work, and
John Williams and Theodora Alexopoulou for their
suggestions on experimental design.
References
R.C. Anderson and A. Davison. 1988. Conceptual and
empirical bases of readibility formulas. In Alice Davi-
son and G. M. Green, editors, Linguistic Complexity
and Text Comprehension: Readability Issues Recon-
sidered. Lawrence Erlbaum Associates, Hillsdale, NJ.
E.G. Bard, D. Robertson, and A. Sorace. 1996. Magni-
tude estimation for linguistic acceptability. Language,
72(1):32?68.
R. Barzilay and L. Lee. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence
alignment. In HLT-NAACL 2003, pp 16?23.
I.L. Beck, M.G. McKeown, G.M. Sinatra, and J.A. Lox-
terman. 1991. Revising social studies text from a text-
processing perspective: Evidence of improved com-
prehensibility. Reading Research Quarterly, pp 251?
276.
E.J. Briscoe and J. Carroll. 2002. Robust accurate sta-
tistical annotation of general text. In Proc. of the 3rd
International Conference on Language Resources and
Evaluation, pp 1499?1504, Gran Canaria.
J. Carletta. 1996. Assessing agreement on classification
tasks: The kappa statistic. Computational Linguistics,
22(2):249?254.
J. Carroll, G. Minnen, Y. Canning, S. Devlin, and J. Tait.
1998. Practical simplification of English newspaper
text to assist aphasic readers. In Proc. of AAAI98
Workshop on Integrating Artificial Intelligence and As-
sistive Technology, pp 7?10, Madison, WI.
W. Cowart. 1997. Experimental Syntax: applying objec-
tive methods to sentence judgement. Thousand Oaks,
CA: Sage Publications.
J. Hirschberg and D. Litman. 1993. Empirical studies
on the disambiguation of cue phrases. Computational
Linguistics, 19(3):501?530.
B. Hutchinson. 2005. Modelling the substitutability
of discourse connectives. In ACL ?05: Proc. of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pp 149?156, Morristown, NJ, USA.
Association for Computational Linguistics.
A. Ibrahim, B. Katz, and J. Lin. 2003. Extracting para-
phrases from aligned corpora. In Proc. of The Second
International Workshop on Paraphrasing.
N. Kaji, D. Kawahara, S. Kurohash, and S. Sato. 2002.
Verb paraphrase based on case frame alignment. In
Proc. of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL?02), pp 215?222,
Philadelphia, USA.
F. Keller, S. Gunasekharan, N. Mayo, and M. Corley.
2008, to appear. Timing accuracy of web experiments:
A case study using the webexp software package. Be-
havior Research Methods.
F. Keller. 2000. Gradience in Grammar: Experimental
and Computational Aspects of Degrees of Grammati-
cality. Ph.D. thesis, University of Edinburgh.
A. Knott and R. Dale. 1994. Using linguistic phenom-
ena to motivate a set of coherence relations. Discourse
Processes, 18(1):35?62.
A. Knott. 1996. A Data-Driven Methodology for Moti-
vating a Set of Discourse Relations. Ph.D. thesis, Ph.
D. thesis, Centre for Cognitive Science, University of
Edinburgh, Edinburgh, UK.
T. Linderholm, M.G. Everson, P. van den Broek,
M. Mischinski, A. Crittenden, and J. Samuels. 2000.
Effects of Causal Text Revisions on More-and Less-
Skilled Readers? Comprehension of Easy and Difficult
Texts. Cognition and Instruction, 18(4):525?556.
W. C. Mann and S. A. Thompson. 1988. Rhetorical
Structure Theory: Towards a functional theory of text
organization. Text, 8(3):243?281.
L. G. M. Noordman and W. Vonk. 1992. Reader?s knowl-
edge and the control of inferences in reading. Lan-
guage and Cognitive Processes, 7:373?391.
R. Power, D. Scott, and N. Bouayad-Agha. 2003. Gen-
erating texts with style. Proc. of the 4 thInternational
Conference on Intelligent Texts Processing and Com-
putational Linguistics.
A. Siddharthan. 2006. Syntactic simplification and text
cohesion. Research on Language and Computation,
4(1):77?109.
B. Webber, A. Joshi, E. Miltsakaki, R. Prasad, N. Di-
nesh, A. Lee, and K. Forbes. 2005. A Short Intro-
duction to the Penn Discourse TreeBank. Treebanking
for discourse and speech: proceedings of the NODAL-
IDA 2005 special session on Treebanks for spoken lan-
guage and discourse.
S. Williams and E. Reiter. 2008. Generating basic skills
reports for low-skilled readers. Natural Language En-
gineering, 14(04):495?525.
I. Witten and E. Frank. 2000. Data Mining: Practical
Machine Learning Tools and Techniques with Java Im-
plementations. Morgan Kaufmann.
P. Wolff, B. Klettke, T. Ventura, and G. Song. 2005.
Expressing causation in English and other languages.
Categorization inside and outside the laboratory: Es-
says in honor of Douglas L. Medin, pp 29?48.
1010
NAACL-HLT 2012 Workshop on Predicting and Improving Text Readability for target reader populations (PITR 2012)., pages 17?24,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Offline Sentence Processing Measures for testing Readability with Users
Advaith Siddharthan Napoleon Katsos
Department of Computing Science Department of Theoretical and Applied Linguistics
University of Aberdeen University of Cambridge
advaith@abdn.ac.uk nk248@cam.ac.uk
Abstract
While there has been much work on compu-
tational models to predict readability based
on the lexical, syntactic and discourse prop-
erties of a text, there are also interesting open
questions about how computer generated text
should be evaluated with target populations.
In this paper, we compare two offline methods
for evaluating sentence quality, magnitude es-
timation of acceptability judgements and sen-
tence recall. These methods differ in the ex-
tent to which they can differentiate between
surface level fluency and deeper comprehen-
sion issues. We find, most importantly, that
the two correlate. Magnitude estimation can
be run on the web without supervision, and
the results can be analysed automatically. The
sentence recall methodology is more resource
intensive, but allows us to tease apart the flu-
ency and comprehension issues that arise.
1 Introduction
In Natural Language Generation, recent approaches
to evaluation tend to consider either ?naturalness? or
?usefulness?. Following evaluation methodologies
commonly used for machine translation and sum-
marisation, there have been attempts to measure nat-
uralness in NLG by comparison to human generated
gold standards. This has particularly been the case
in evaluating referring expressions, where the gen-
erated expression can be treated as a set of attributes
and compared with human generated expressions
(Gatt et al, 2009; Viethen and Dale, 2006), but there
have also been attempts at evaluating sentences this
way. For instance, Langkilde-Geary (2002) gener-
ate sentences from a parsed analysis of an existing
sentence, and evaluate by comparison to the origi-
nal. However, this approach has been criticised at
many levels (see for example, Gatt et al (2009) or
Sripada et al (2003)); for instance, because there are
many good ways to realise a sentence, because typi-
cal NLG tasks do not come with reference sentences,
and because fluency judgements in the monolingual
case are more subtle than for machine translation.
Readability metrics, by comparison, do not rely
on reference texts, and try to model the linguistic
quality of a text based on features derived from the
text. This body of work ranges from the Flesch Met-
ric (Flesch, 1951), which is based on average word
and sentence length, to more systematic evaluations
of various lexical, syntactic and discourse charac-
teristics of a text (cf. Pitler et al (2010), who as-
sess readability of textual summaries). Some re-
searchers have also suggested measuring edit dis-
tance by using a human to revise a system generated
text and quantifying the revisions made (Sripada et
al., 2003). This does away with the need for ref-
erence texts and is quite suited to expert domains
such as medicine or weather forecasting, where a do-
main expert can easily correct system output. Anal-
ysis of these corrections can provide feedback on
problematic content and style. We have previously
evaluated text reformulation applications by asking
readers which version they prefer (Siddharthan et
al., 2011), or through the use of Likert scales (Lik-
ert, 1932) for measuring meaning preservation and
grammaticality (Siddharthan, 2006). However, none
of these approaches tell us very much about the com-
prehensibility of a text for an end reader.
To address this, there has been recent interest in
task based evaluations. Task based evaluations di-
rectly evaluate generated utterances for their utility
17
to the hearer. However, while for some generation
areas like reference (Gatt et al, 2009), the real world
evaluation task is obvious, it is less so for other gen-
eration tasks such as surface realisation or text-to-
text regeneration or paraphrase. We are thus keen
to investigate psycholinguistic methods for investi-
gating sentence processing as an alternative to task
based evaluations.
In the psycholinguistics literature, various offline
and online techniques have been used to investigate
sentence processing by readers. Online techniques
(eye-tracking (Duchowski, 2007), neurophysiologi-
cal (Friederici, 1995), etc.) offer many advantages
in studying how readers process a sentence. But
as these are difficult to set up and also resource in-
tensive, we would prefer to evaluate NLG using of-
fline techniques. Some offline techniques, such as
Cloze tests (Taylor, 1953) or question answering, re-
quire careful preparation of material (choice of texts
and questions, and for Cloze, the words to leave
out). Other methods, such as magnitude estima-
tion and sentence recall (cf. Sec 3 for details), are
more straightforward to implement. In this paper,
we investigate magnitude estimation of acceptabil-
ity judgements and delayed sentence recall in the
context of an experiment investigating generation
choices when realising causal relations. Our goal
is to study how useful these methods are for evaluat-
ing surface level fluency and deeper comprehensibil-
ity. We are interested in whether they can distinguish
between similar sentences, and whether they can be
used to test hypotheses regarding the effect of com-
mon generation decisions such as information order
and choice of discourse marker. We briefly discuss
the data in Section 2, before describing our experi-
ments (Sections 3.1 and 3.2). We finish with a dis-
cussion of their suitability for more general evalua-
tion of NLG with target readers.
2 Data
We use a dataset created to explore generation
choices in the context of expressing causal re-
lations; specifically, the choice of periphrastic
causative (Wolff et al, 2005) and information order.
The dataset considers four periphrastic causatives
(henceforth referred to as discourse markers): ?be-
cause?, ?because of ?, the verb ?cause? and the noun
?cause? with different lexico-syntactic properties.
We present an example from this dataset below (cf.
Siddharthan and Katsos (2010) for details):
(1) a. Fructose-induced hypertension is caused
by increased salt absorption by the intestine
and kidney. [b caused-by a]
b. Increased salt absorption by the intestine
and kidney causes fructose-induced hyper-
tension. [a caused b]
c. Fructose-induced hypertension occurs be-
cause of increased salt absorption by the in-
testine and kidney. [b because-of a]
d. Because of increased salt absorption by the
intestine and kidney, fructose-induced hy-
pertension occurs. [because-of ab]
e. Fructose-induced hypertension occurs be-
cause there is increased salt absorption by
the intestine and kidney. [b because a]
f. Because there is increased salt absorp-
tion by the intestine and kidney, fructose-
induced hypertension occurs. [because ab]
g. Increased salt absorption by the intestine
and kidney is the cause of fructose-induced
hypertension. [a cause-of b]
h. The cause of fructose-induced hypertension
is increased salt absorption by the intestine
and kidney. [cause-of ba]
In this notation, ?a? represents the cause,?b? rep-
resents the effect and the remaining string indi-
cates the discourse marker; their ordering reflects
the information order in the sentence, for exam-
ple, ?a cause-of b? indicates a cause-effect informa-
tion order using ?cause of? as the discourse marker.
The dataset consists of 144 sentences extracted from
corpora (18 sentences in each condition (discourse
marker + information order), reformulated manually
to generated the other seven conditions, resulting in
1152 sentences in total.
Clearly, different formulations have different lev-
els of fluency. In this paper we explore what two of-
fline sentence processing measures can tell us about
their acceptability and ease of comprehension.
3 Method
3.1 Magnitude estimation of acceptability
Human judgements for acceptability for each of the
1152 sentences in the dataset were obtained using
the WebExp package (Keller et al, 2009). Note that
18
the reformulations are, strictly speaking, grammati-
cal according to the authors? judgement. We are test-
ing violations of acceptability, rather than grammat-
icality per se. This mirrors the case of NLG, where
a grammar is often used for surface realisation, en-
suring grammaticality.
Acceptability is a measure which reflects
both ease of comprehension and surface well-
formedness. We later compare this experiment
with a more qualitative comprehension experiment
based on sentence recall (cf. Section 3.2). Rather
than giving participants a fixed scale, we used the
magnitude estimation paradigm, which is more
suitable to capture robust or subtle differences
between the relative strength of acceptability or
grammaticality violations (see, for example, Bard
et al (1996); Cowart (1997); Keller (2000)). One
advantage of magnitude estimation is that the
researcher does not make any assumptions about
the number of linguistic distinctions allowed. Each
participant makes as many distinctions as they feel
comfortable. Participants were given the following
instructions (omitting those that relate to the web
interface):
1. Judge acceptability of construction, not of
meaning;
2. There is no limit to the set of numbers you can
use, but they must all be positive - the lowest
score you can assign is 0. In other words, make
as many distinctions as you feel comfortable;
3. Always score the new sentence relative to the
score you gave the modulus sentence, which
you will see on the top of the screen;
4. Acceptability is a continuum, do not just make
yes/no judgements on grammaticality;
5. Try not to use a fixed scale, such as 1?5, which
you might have used for other linguistic tasks
previously.
Design: The propositional content of 144 sen-
tences was presented in eight conditions. Eight par-
ticipant groups (A?H) consisting of 6 people each
were presented with exactly one of the eight formu-
lations of each of 144 different sentences, as per a
Latin square design. This experimental design al-
lows all statistical comparisons between the eight
types of causal formulations and the three genres to
be within-participant. The participants were Uni-
versity of Cambridge students (all native English
speakers). Participants were asked to score how ac-
ceptable a modulus sentence was, using any positive
number. They were then asked to score other sen-
tences relative to this modulus, so that higher scores
were assigned to more acceptable sentences. Scores
were normalised to allow comparison across partic-
ipants, following standard practice in the literature,
by using the z-score: For each participant, each sen-
tence score was normalised so that the mean score
is 0 and the standard deviation is 1 (zih =
xih??h
?h
),
where zih is participant h?s z-score for the sentence
i when participant h gave a magnitude estimation
score of xih to that sentence. ?h is the mean and ?h
the standard deviation of the set of magnitude esti-
mation scores for user h.
3.2 Sentence Recall
Acceptability ratings are regarded as a useful mea-
sure because they combine surface judgements of
grammaticality with deeper judgements about how
easy a sentence is to understand. However, one
might want to know whether an inappropriate for-
mulation can cause a breakdown in comprehension
of the content of a sentence, which would go beyond
the (perhaps) non-detrimental effect of a form that is
dispreferred at the surface level. To try and learn
more about this, we conducted a second behavioural
experiment using a sentence recall methodology. As
these experiments are harder to conduct and have to
be supervised in a lab (to ensure that participants
have similar conditions of attention and motivation,
and to prevent ?cheating? using cut-and-paste or
note taking techniques), we selected a subset of 32
pairs of items from the previous experiment. Each
pair consisted of two formulations of the same sen-
tence. The pairs were selected in a manner that ex-
hibited a variation in the within-pair difference of
acceptability. In other words, we wanted to explore
whether two formulations of a sentences with sim-
ilar acceptability ratings were recalled equally well
and whether two formulations of a sentence with dif-
ferent acceptability ratings were recalled differently.
Design: 32 students at the University of Cam-
bridge were recruited (these are different partici-
19
pants from those in the acceptability experiment in
Section 3.1, but were also all native speakers). We
created four groups A?D, each with eight partici-
pants. Each Group saw 16 sentences in exactly one
of the two formulation types, such that groups A?B
formed one Latin square and C?D formed another
Latin square. These 16 sentences were interleaved
with 9 filler sentences that did not express causal-
ity. For each item, a participant was first shown
a sentence on the screen at the rate of 0.5 seconds
per word. Then, the sentence was removed from the
screen, and the participant was asked to do two arith-
metic tasks (addition and subtraction of numbers be-
tween 10 and 99). The purpose of these tasks was to
add a load between target sentence and recall so that
the recall of the target sentence could not rely on
internal rehearsal of the sentence. Instead, research
suggests that in such conditions recall is heavily de-
pendent on whether the content and form was ac-
tually comprehended (Lombardi and Potter, 1992;
Potter and Lombardi, 1990). Participants then typed
what they recalled of the sentence into a box on the
screen.
We manually coded the recalled sentences for six
error types (1?6) or perfect recall (0) as shown in
Table 1. Further, we scored the sentences based
on our judgements of how bad each error-type was.
The table also shows the weight for each error type.
For any recalled sentences, only one of (0,1,5,6) is
coded, i.e., these codes are mutually exclusive, but
if none of the positive scores (0,1,5,6) have been
coded, any combination of error types (2,3,4) can
be coded for the same sentence.
4 Results
4.1 Correlation between the two methods
The correlation between the differences in accept-
ability (using average z-scores for each formula-
tion from the magnitude estimation experiment) and
recall scores (scored as described above) for the
32 pairs of sentences was found to be significant
(Spearman?s rho=.43; p=.01). A manual inspec-
tion of the data showed up one major issue regard-
ing the methodologies: our participants appear to
penalise perceived ungrammaticalities in short sen-
tences quite harshly when rating acceptability, but
they have no trouble recalling such sentences ac-
curately. For example, sentence a. in Example 2
below had an average acceptability score of 1.41,
while sentence b. only scored .13, but both sentences
were recalled perfectly by all participants in the re-
call study:
(2) a. It is hard to imagine that it was the cause of
much sadness.
b. It is hard to imagine that because of it there
was much sadness.
Indeed the sentence recall test failed to discrimi-
nate at all for sentences under 14 words in length.
When we removed pairs with sentences under 14
words (there were eight such pairs), the correlation
between the differences in magest and recall scores
for the 24 remaining pairs of sentences was even
stronger (Spearman?s rho=.64; p<.001).
Summary: The two methodologies give very dif-
ferent results for short sentences. This is because
comprehension is rarely an issue for short sentences,
while surface level disfluencies are more jarring to
participants in such short sentences. For longer sen-
tences, the two methods correlate strongly; for such
sentences, magnitude estimations of acceptability
better reflect ease of comprehension. In retrospect,
this suggests that the design of an appropriate load
(we used two arithmetic sums) is an important con-
sideration that can affect the usefulness of recall
measures. One could argue that acceptability is a
more useful metric for evaluating NLG as it com-
bines surface level fluency judgements with ease of
comprehension issues. In Siddharthan and Katsos
(2010), we described how this data could be used to
train an NLG component to select the most accept-
able formulation of a sentence expressing a causal
relation. We now enumerate other characteristics
of magnitude estimation of acceptability that make
them useful for evaluating sentences. Then, in Sec-
tion 4.3, we discuss what further information can be
gleaned from sentence recall studies.
4.2 Results of magnitude estimation study
Distinguishing between sentences: We found
that magnitude estimation judgements are very good
at distinguishing sentences expressing the same con-
tent. Consider Table 2, which shows the average ac-
ceptability for the n-best formulation of each of the
20
Weight Error Code Error Description
+0.5 0 Recalled accurately (clauses A and B can be valid paraphrases, but the discourse con-
nective (TYPE) is the same)
+0.4 1 Clauses A and B are recalled accurately but the relation is reformulated using a differ-
ent but valid discourse marker
-0.25 2 The discourse marker has been changed in a manner that modifies the original causal
relation
-0.5 3 Clause B (effect) recall error (clause is garbled)
-0.5 4 Clause A (cause) recall error (clause is garbled)
+0.25 5 Causal relation and A and B are recalled well, but some external modifying clause is
not recalled properly
+0.25 6 Causality is quantified (e.g., ?major cause?) and this modifier is lost or changed in
recall (valid paraphrases are not counted here)
Table 1: Weighting function for error types.
144 sentences (n=1?8). We see that the best formu-
lation averages .89, the second best .57 and the worst
formulation -.90. Note that it is not always the same
formulation types that are deemed acceptable ? if we
always select the most preferred type (a caused b)
for each of the 144 sentences, the average accept-
ability is only .12.
n = 1 2 3 4 5 6 7 8
Av. Z = .89 .57 .33 .13 -.12 -.33 -.58 -.90
Table 2: Average acceptability for the nth best formula-
tion of each of the 144 sentences.
Testing hypotheses: In addition to distinguishing
between different formulations of a sentence, vary-
ing generation choices systematically allows us to
test any hypotheses we might have about their ef-
fect on acceptability. Indeed, hypothesis testing was
an important consideration in the design of this ex-
periment. For instance, various studies (Clark and
Clark, 1968; Katz and Brent, 1968; Irwin, 1980)
suggest that for older school children, college stu-
dents and adults, comprehension is better for the
cause-effect presentation, both when the relation is
implicit (no discourse marker) and explicit (with a
discourse marker). We can then test specific predic-
tions about which formulations are likely to be more
acceptable.
H1 We expect the cause-effect information order
to be deemed more acceptable than the corre-
sponding effect-cause information order.
H2 As all four discourse markers are commonly
used in language, we do not expect any par-
ticular marker to be globally preferred to the
others.
We ran a 4 (discourse marker) x 2 (information or-
der) repeated measures ANOVA. We found a main
effect of information order (F1(1, 49) = 5.19, p =
.017) and discourse marker (F1(3, 147) = 3.48, p
= .027). Further, we found a strong interaction be-
tween information order and formulation type, F1(3,
147) = 19.17, p<.001. We now discuss what these
results mean.
Understanding generation decisions: The main
effect of discourse marker was not predicted (Hy-
pothesis H2). We could try and explain this em-
pirically. For instance, in the BNC ?because? as
a conjunction occurs 741 times per million words,
while ?cause? as a verb occurs 180 times per mil-
lion words, ?because of? 140 per million words and
?cause? as a noun 86 per million words. We might
expect the more common markers to be judged more
acceptable. However, there was no significant corre-
lation between participants? preference for discourse
marker and the BNC corpus frequencies of the mark-
ers (Spearman?s rho=0.4, p>0.75). This suggests
that corpus frequencies need not be a reliable indica-
tor of reader preferences, at least for discourse con-
nectives. The mean z-scores for the four discourse
markers are presented in Table 3
To explore the interaction between discourse
marker and information order, a post-ANOVA
Tukey HSD analysis was performed. The significant
21
Discourse Marker Average Z-score
Cause (verb) 0.036
Because of 0.028
Because -0.011
Cause (noun) -0.028
Table 3: Av. z-scores for the four discourse markers
effects are listed in Table 4. There is a significant
preference for using ?because? and ?because of? in
the effect-cause order (infix) over the cause-effect
order (prefix) and for using ?cause? as a verb in
the cause-effect order (active voice) over the effect-
cause order (passive voice). Thus, hypothesis H1
is not valid for ?because? and ?because of?, where
the canonical infix order is preferred, and though
there are numerical preferences for the cause-effect
order for ?cause? as a noun we found support for
hypothesis H1 to be significant only for ?cause? as a
verb. Table 4 also tells us that if the formulation is in
cause-effect order, there is a preference for ?cause?
as a verb over ?because? and ?because of?. On the
other hand, if the formulation is in the reverse effect-
cause order, there is a preference for ?because? or
?because of? over ?cause? as a verb or as a noun.
Summary: This evaluation provides us with some
insights into how generation decisions interact,
which can be used prescriptively to, for example, se-
lect a discourse marker, given a required information
order.
4.3 Results of sentence recall study
While magnitude estimation assessments of accept-
ability can be used to test some hypotheses about the
effect of generation decisions, it cannot really tease
apart cases where there are surface level disfluencies
from those that result in a breakdown in comprehen-
sion. To test such hypotheses, we use the sentence
recall study.
Testing hypotheses: Previous research (e.g., En-
gelkamp and Rummer (2002)) suggests that recall
for the second clause is worse when clauses are
combined through coordination (such as ?therefore?
or ?and?) than through subordination such as ?be-
cause?. The explanation is that subordination bet-
ter unifies the two clauses in immediate memory.
We would expect this unification to be even greater
when the cause and effect are arguments to a verb.
Thus, compared to ?because?, we would expect re-
call of the second clause to be higher for ?cause? as
a verb or a noun, due to the tighter syntactic binding
to the discourse marker (object of a verb). Likewise,
compared to ?cause?, we would expect to see more
recall errors for the second clause when using ?be-
cause? as a conjunction. Our hypotheses are listed
below:
H3 For ?cause? as a verb or a noun, there will be
fewer recall errors in ?a? and ?b? compared
to ?because? or ?because of?, because of the
tighter syntactic binding.
H4 For ?because? as a conjunction, there will be
more recall errors in the second clause than in
the first clause; i.e., for ?b because a?, clause
?a? will have more recall errors than ?b? and for
?because ab?, clause ?a? will have fewer recall
errors than ?b?.
Table 5 shows the average incidence of each error
type per sentences in that formulation (cf. Table 1).
Note that the totals per row might add up to slightly
more than 1 because multiple errors can be coded
for the same sentence.
Table 5 shows that ?because? and ?because of?
constructs result in more type 3 and 4 recall errors
in clauses ?a? and/or ?b? compared with ?cause? as
either a noun or a verb. This difference is significant
(z-test; p<.001), thus supporting hypothesis H3.
Further, for ?because?, the recall errors for the
first clause are significantly fewer than for the sec-
ond clause (z-test; p<.01), thus supporting hypoth-
esis H4. In contrast, for the cases with ?cause? as a
verb or noun, both A and B are arguments to a verb
(either ?cause? or a copula), and the tighter syntactic
binding helps unify them in immediate memory, re-
sulting in fewer recall errors that are also distributed
more evenly between the first and the second argu-
ment to the verb.
We make one further observation: passive voice
sentences appear to be reformulated at substantial
levels (19%), but in a valid manner (type 1 errors).
This suggests that the dispreference for passives in
the acceptability study is about surface level form
rather than deeper comprehension. This would be a
22
(a) Ordering Effects
Marker Preference p-value
because effect-cause (.12) is preferred over cause-effect (-.14) p<.001
because of effect-cause (.13) is preferred over cause-effect (-.11) p<.001
cause (verb) cause-effect (.12) is preferred over effect-cause (-.05) p=.0145
cause (noun) cause-effect (.01) is preferred over effect-cause (-.11) p=.302
(b) Discourse Marker Effects
Order Preference p-value
effect-cause ?because? (.12) is preferred over ?cause (noun)? (-.11) p<.001
effect-cause ?because-of? (.13) is preferred over ?cause (noun)? (-.11) p<.001
effect-cause ?because-of? (.13) is preferred over ?cause (verb)? (-.05) p=.001
effect-cause ?because? (.12) is preferred over ?cause (verb)? (-.05) p=.002
effect-cause ?cause (verb)? (-.05) is preferred over ?cause (noun)? (-.11) p=.839
effect-cause ?because? (.12) is preferred over ?because-of? (.13) p=.999
cause-effect ?cause (verb)? (.13) is preferred over ?because? (-.14) p<.001
cause-effect ? cause (verb)? (.13) is preferred over ?because-of? (-.06) p=.006
cause-effect ?cause (verb)? (.13) is preferred over ?cause (noun)? (.01) p=.165
cause-effect ?cause (noun)? (.01) is preferred over ?because? (-.14) p=.237
cause-effect ?because-of? (-.06) is preferred over ?because? (-.14) p=.883
cause-effect ?cause (noun)? (.01) is preferred over ?because-of? (-.06) p=.961
Table 4: Interaction effects between information order and discourse marker (mean z-scores in parentheses; significant
effects in bold face).
reasonable conclusion, given that all our participants
are university students.
Summary: Overall we conclude that sentence re-
call studies provide insights into the nature of the
comprehension problems encountered, and they cor-
roborate acceptability ratings in general, and partic-
ularly so for longer sentences.
5 Conclusions
In this paper, we have tried to separate out surface
form aspects of acceptability from breakdowns in
comprehension, using two offline psycholinguistic
methods.
We believe that sentence recall methodologies can
substitute for task based evaluations and highlight
breakdowns in comprehension at the sentence level.
However, like most task based evaluations, recall ex-
periments are time consuming as they need to be
conducted in a supervised setting. Additionally, they
require manual annotation of error types, though
perhaps this could be automated.
Acceptability ratings on the other hand are easy to
acquire. Based on our experiments, we believe that
acceptability ratings are reliable indicators of com-
prehension for longer sentences and, particularly for
shorter sentences, combine surface form judgements
with ease of comprehension in a manner that is very
relevant for evaluating sentence generation or regen-
eration, including simplification.
Both methods are considerably easier to set up
and interpret than online methods such as self paced
reading, eye tracking or neurophysiological meth-
ods.
Acknowledgements
This work was supported by the Economic and So-
cial Research Council (Grant Number RES-000-22-
3272).
References
E.G. Bard, D. Robertson, and A. Sorace. 1996. Magni-
tude estimation for linguistic acceptability. Language,
72(1):32?68.
H.H. Clark and E.V. Clark. 1968. Semantic distinctions
and memory for complex sentences. The Quarterly
Journal of Experimental Psychology, 20(2):129?138.
23
type err0 err1 err2 err3 err4 err5 err6
b because a 0.62 0.18 0.02 0.10 0.18 0.00 0.05
because ab 0.80 0.03 0.03 0.20 0.10 0.00 0.00
b because-of a 0.78 0.11 0.02 0.06 0.07 0.00 0.06
because-of ab 0.73 0.00 0.00 0.17 0.17 0.10 0.00
a cause-of b 0.89 0.04 0.06 0.00 0.00 0.00 0.07
cause-of ba 0.75 0.06 0.04 0.06 0.08 0.00 0.06
a caused b 0.83 0.05 0.02 0.03 0.03 0.07 0.00
b caused-by a 0.77 0.19 0.00 0.00 0.04 0.00 0.02
Table 5: Table of recall errors per type.
W. Cowart. 1997. Experimental Syntax: applying objec-
tive methods to sentence judgement. Thousand Oaks,
CA: Sage Publications.
A.T. Duchowski. 2007. Eye tracking methodology: The-
ory and practice. Springer-Verlag New York Inc.
J. Engelkamp and R. Rummer. 2002. Subordinat-
ing conjunctions as devices for unifying sentences in
memory. European Journal of Cognitive Psychology,
14(3):353?369.
Rudolf Flesch. 1951. How to test readability. Harper
and Brothers, New York.
A.D. Friederici. 1995. The time course of syntactic ac-
tivation during language processing: A model based
on neuropsychological and neurophysiological data.
Brain and language, 50(3):259?281.
A. Gatt, A. Belz, and E. Kow. 2009. The tuna-reg chal-
lenge 2009: Overview and evaluation results. In Pro-
ceedings of the 12th European workshop on natural
language generation, pages 174?182. Association for
Computational Linguistics.
J.W. Irwin. 1980. The effects of explicitness and clause
order on the comprehension of reversible causal re-
lationships. Reading Research Quarterly, 15(4):477?
488.
E.W. Katz and S.B. Brent. 1968. Understanding connec-
tives. Journal of Verbal Learning & Verbal Behavior.
F. Keller, S. Gunasekharan, N. Mayo, and M. Corley.
2009. Timing accuracy of web experiments: A case
study using the WebExp software package. Behavior
Research Methods, 41(1):1.
Frank Keller. 2000. Gradience in Grammar: Experimen-
tal and Computational Aspects of Degrees of Gram-
maticality. Ph.D. thesis, University of Edinburgh.
I. Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sen-
tence generator. In Proceedings of the 12th Interna-
tional Natural Language Generation Workshop, pages
17?24. Citeseer.
R. Likert. 1932. A technique for the measurement of
attitudes. Archives of psychology.
L. Lombardi and M.C. Potter. 1992. The regeneration of
syntax in short term memory* 1. Journal of Memory
and Language, 31(6):713?733.
E. Pitler, A. Louis, and A. Nenkova. 2010. Automatic
evaluation of linguistic quality in multi-document
summarization. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 544?554. Association for Computational
Linguistics.
M.C. Potter and L. Lombardi. 1990. Regeneration in the
short-term recall of sentences* 1. Journal of Memory
and Language, 29(6):633?654.
Advaith Siddharthan and Napoleon Katsos. 2010. Refor-
mulating discourse connectives for non-expert readers.
In Proceedings of the 11th Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL-HLT 2010), Los Ange-
les, CA.
A. Siddharthan, A. Nenkova, and K. McKeown. 2011.
Information status distinctions and referring expres-
sions: An empirical study of references to peo-
ple in news summaries. Computational Linguistics,
37(4):811?842.
Advaith Siddharthan. 2006. Syntactic simplification and
text cohesion. Research on Language and Computa-
tion, 4(1):77?109.
S. Sripada, E. Reiter, and I. Davy. 2003. SumTime-
Mousam: Configurable marine weather forecast gen-
erator. Expert Update, 6(3):4?10.
W.L. Taylor. 1953. ? cloze procedure?: a new tool for
measuring readability. Journalism Quarterly; Jour-
nalism Quarterly.
Jette Viethen and Robert Dale. 2006. Algorithms for
generating referring expressions: do they do what peo-
ple do? In Proceedings of the Fourth International
Natural Language Generation Conference, pages 63?
70.
P. Wolff, B. Klettke, T. Ventura, and G. Song. 2005.
Expressing causation in English and other languages.
Categorization inside and outside the laboratory: Es-
says in honor of Douglas L. Medin, pages 29?48.
24
