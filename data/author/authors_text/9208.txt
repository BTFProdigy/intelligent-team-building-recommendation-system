Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 150?153,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Boosting for Chinese Named Entity Recognition
Xiaofeng YU Marine CARPUAT Dekai WU*
Human Language Technology Center
HKUST
Department of Computer Science and Engineering
University of Science and Technology
Clear Water Bay, Hong Kong
{xfyu,marine,dekai}@cs.ust.hk
Abstract
We report an experiment in which a high-
performance boosting based NER model
originally designed for multiple European
languages is instead applied to the Chi-
nese named entity recognition task of the
third SIGHAN Chinese language process-
ing bakeoff. Using a simple character-
based model along with a set of features
that are easily obtained from the Chi-
nese input strings, the system described
employs boosting, a promising and the-
oretically well-founded machine learning
method to combine a set of weak classi-
fiers together into a final system. Even
though we did no other Chinese-specific
tuning, and used only one-third of the
MSRA and CityU corpora to train the
system, reasonable results are obtained.
Our evaluation results show that 75.07 and
80.51 overall F-measures were obtained
on MSRA and CityU test sets respectively.
1 Introduction
Named entity recognition (NER), which includes
the identification and classification of certain
proper nouns, such as person names, organiza-
tions, locations, temporal, numerical and mon-
etary phrases, plays an important part in many
natural language processing applications, such
as machine translation, information retrieval, in-
formation extraction and question answering.
Much of the NER research was pioneered in
the MUC/DUC and Multilingual Entity Task
(MET) evaluations, as a result of which signif-
icant progress has been made and many NER
?This work was supported in part by DARPA GALE
contract HR0011-06-C-0023, and by the Hong Kong Re-
search Grants Council (RGC) research grants RGC6083/99E,
RGC6256/00E, and DAG03/04.EG09.
systems of fairly high accuracy have been con-
structed. In addition, the shared tasks of CoNLL-
2002 and CoNLL-2003 helped spur the devel-
opment toward more language-independent NER
systems, by evaluating four types of entities (peo-
ple, locations, organizations and names of miscel-
laneous entities) in English, German, Dutch and
Spanish.
However, these are all European languages, and
Chinese NER appears to be significantly more
challenging in a number of important respects.
We believe some of the main reasons to be as
follows: (1) Unlike European languages, Chi-
nese lacks capitalization information which plays
a very important role in identifying named enti-
ties. (2) There is no space between words in Chi-
nese, so ambiguous segmentation interacts with
NER decisions. Consequently, segmentation er-
rors will affect the NER performance, and vice
versa. (3) Unlike European languages, Chinese al-
lows an open vocabulary for proper names of per-
sons, eliminating another major source of explicit
clues used by European language NER models.
This paper presents a system that introduces
boosting to Chinese named entity identification
and classification. Our primary aim was to con-
duct a controlled experiment to test how well
the boosting based models we designed for Eu-
ropean languages would fare on Chinese, without
major modeling alterations to accommodate Chi-
nese. We evaluated the system using data from
the third SIGHAN Chinese language processing
bakeoff, the goal of which was to perform NER
on three types of named entities: PERSON, LO-
CATION and ORGANIZATION.1 Three training
corpora from MSRA, CityU and LDC were given.
TheMSRA and LDC corpora were simplified Chi-
nese texts while the CityU corpus was traditional
1Except in the LDC corpus, which contains four types
of entities: PERSON, LOCATION, ORGANIZATION and
GEOPOLITICAL.
150
Chinese. In addition, the competition also spec-
ified open and closed tests. In the open test, the
participants may use any other material including
material from other training corpora, proprietary
dictionaries, and material from the Web besides
the given training corpora. In the closed test, the
participants can only use the three training cor-
pora. No other material or knowledge is allowed,
including part-of-speech (POS) information, ex-
ternally generated word-frequency counts, Arabic
and Chinese numbers, feature characters for place
names, common Chinese surnames, and so on.
The approach we used is based on selecting a
number of features, which are used to train several
weak classifiers. Using boosting, which has been
shown to perform well on other NLP problems and
is a theoretically well-founded method, the weak
classifiers are then combined to perform a strong
classifier.
2 Boosting
The main idea behind the boosting algorithm is
that a set of many simple and moderately accu-
rate weak classifiers (also called weak hypothe-
ses) can be effectively combined to yield a sin-
gle strong classifier (also called the final hypoth-
esis). The algorithm works by training weak clas-
sifiers sequentially whose classification accuracy
is slightly better than random guessing and finally
combining them into a highly accurate classifier.
Each weak classifier searches for the hypothesis in
the hypotheses space that can best classify the cur-
rent set of training examples. Based on the eval-
uation of each iteration, the algorithm reweights
the training examples, forcing the newly generated
weak classifier to give higher weights to the exam-
ples that are misclassified in the previous iteration.
The boosting algorithm was originally created to
deal with binary classification in supervised learn-
ing. The boosting algorithm is simple to imple-
ment, does feature selection resulting in a rela-
tively simple classifier, and has fairly good gen-
eralization.
Based on the boosting framework, our system
uses the AdaBoost.MH algorithm (Schapire and
Singer, 1999) as shown in Figure 1, an n-ary clas-
sification variant of the original well-known bi-
nary AdaBoost algorithm (Freund and Schapire,
1997). The original AdaBoost algorithm was de-
signed for the binary classification problem but did
not fulfill the requirements of the Chinese NER
Input: A training set Tr = {< d1, C1 >, . . . , < dg, Cg >}
where Cj ? C = {c1, ..., cm} for all j = 1, . . . , g.
Output: A final hypothesis ?(d, c) =
?S
s=1 ?s?s(d, c).
Algorithm: LetD1(dj , ci) = 1mg for all j = 1, . . . , g and
for all i = 1, . . . ,m. For s = 1, . . . , S do:
? pass distribution Ds(dj , ci)to the weak classifier;
? derive the weak hypothesis ?s from the weak
classifier;
? choose ?s ? R;
? set Ds+1(dj , ci) =
Ds(dj ,ci)exp(??sCj [ci]?s(dj ,ci))
Zs
where
Zs =?m
i=1
?g
j=1 Ds(dj , ci )exp( ? ?sCj [ci] ?s(dj , ci))
is a normalization factor chosen so that?m
i=1
?g
j=1 Ds+1(dj , ci) = 1.
Figure 1: The AdaBoost.MH algorithm.
task. AdaBoost.MH has shown its usefulness on
standard machine learning tasks through exten-
sive theoretical and empirical studies, where dif-
ferent standard machine learning methods have
been used as the weak classifier (e.g., Bauer and
Kohavi (1999), Opitz and Maclin (1999), Schapire
(2002)). It also performs well on a number of nat-
ural language processing problems, including text
categorization (e.g., Schapire and Singer (2000),
Sebastiani et al (2000)) and word sense disam-
biguation (e.g., Escudero et al (2000)). In partic-
ular, it has also been demonstrated that boosting
can be used to build language-independent NER
models that perform exceptionally well (Wu et al
(2002), Wu et al (2004), Carreras et al (2002)).
The weak classifiers used in the boosting algo-
rithm come from a wide range of machine learning
methods. We have chosen to use a simple classifier
called a decision stump in the algorithm. A deci-
sion stump is basically a one-level decision tree
where the split at the root level is based on a spe-
cific attribute/value pair. For example, a possible
attribute/value pair could beW2 =?/.
3 Experiment Details
In order to implement the boosting/decision
stumps, we used the publicly available software
AT&T BoosTexter (Schapire and Singer, 2000),
which implements boosting on top of decision
stumps. For preprocessing we used an off-the-
shelf Chinese lexical analysis system, the open
source ICTCLAS (Zhang et al, 2003), to segment
and POS tag the training and test corpora.
151
3.1 Data Preprocessing
The training corpora provided by the SIGHAN
bakeoff organizers were in the CoNLL two col-
umn format, with one Chinese character per line
and hand-annotated named entity chunks in the
second column.
In order to provide basic features for training
the decision stumps, the training corpora were seg-
mented and POS tagged by ICTCLAS, which la-
bels Chinese words using a set of 39 tags. This
module employs a hierarchical hidden Markov
model (HHMM) and provides word segmentation,
POS tagging and unknown word recognition. It
performs reasonably well, with segmentation pre-
cision recently evaluated at 97.58%.2 The recall
rate of unknownwords using role tagging was over
90%.
We note that about 200 words in each train-
ing corpora remained untagged. For these words
we simply assigned the most frequently occurring
tags in each training corpora.
3.2 Feature Set
The boosting/decision stumps were able to accom-
modate a large number of features. The primitive
features we used were:
? The current character and its POS tag.
? The characters within a window of 2 charac-
ters before and after the current character.
? The POS tags within a window of 2 charac-
ters before and after the current character.
? The chunk tags (gold standard named entity
label during the training) of the previous two
characters.
The chunk tag is the BIO representation, which
was employed in the CoNLL-2002 and CoNLL-
2003 evaluations. In this representation, each
character is tagged as either the beginning of a
named entity (B tag), a character inside a named
entity (I tag), or a character outside a named entity
(O tag).
When we used conjunction features, we found
that they helped the NER performance signifi-
cantly. The conjunction features used are basi-
cally conjunctions of 2 consecutive characters and
2 consecutive POS tags. We also found that a
2Results from the recent official evaluation in the national
973 project.
Table 1: Dev set results on MSRA and CityU.
Precision Recall F?=1
MSRA
LOC 82.00% 85.93% 83.92
ORG 76.99% 61.44% 68.34
PER 89.33% 74.47% 81.22
Overall 82.62% 76.45% 79.41
CityU
LOC 88.62% 81.69% 85.02
ORG 82.50% 66.44% 73.61
PER 84.05% 84.58% 84.31
Overall 86.46% 79.26% 82.71
Table 2: Test set results on MSRA, CityU, LDC.
Precision Recall F?=1
MSRA
LOC 84.98% 80.94% 82.91
ORG 72.82% 57.78% 64.43
PER 82.89% 59.91% 69.55
Overall 81.95% 69.26% 75.07
CityU
LOC 88.65% 83.58% 86.04
ORG 83.75% 57.25% 68.01
PER 86.11% 76.42% 80.98
Overall 86.92% 74.98% 80.51
LDC
LOC 65.84% 76.51% 70.78
ORG 53.69% 39.52% 45.53
PER 80.29% 68.97% 74.20
Overall 67.20% 65.54% 66.36
LDC (w/GPE)
GPE 0.00% 0.00% 0.00
LOC 1.94% 37.74% 3.70
ORG 53.69% 39.52% 45.53
PER 80.29% 68.97% 74.20
Overall 30.58% 29.82% 30.19
larger context window (3 characters instead of 2
before and after the current character) to be quite
helpful to performance.
Apart from the training and test corpora, we
considered the gazetteers from LDC which con-
tain about 540K persons, 242K locations and 98K
organization names. Named entities in the train-
ing corpora which appeared in the gazetteers were
identified lexically or by using a maximum for-
ward match algorithm. Once named entities have
been identified, each character can then be anno-
tated with an NE chunk tag. The boosting learner
152
can view the NE chunk tag as an additional fea-
ture. Here we used binary gazetteer features. If
the character was annotated with an NE chunk
tag, its gazetteer feature was set to 1; otherwise
it was set to 0. However we found that adding bi-
nary gazetteer features does not significantly help
the performance when conjunction features were
used. In fact, it actually hurt the performance
slightly.
The features used in the final experiments were:
? The current character and its POS tag.
? The characters within a window of 3 charac-
ters before and after the current character.
? The POS tags within a window of 3 charac-
ters before and after the current character.
? A small set of conjunctions of POS tags and
characters within a window of 3 characters of
the current character.
? The BIO chunk tags of the previous 3 charac-
ters.
4 Results
Table 1 presents the results obtained on the MSRA
and CityU development test set. Table 2 presents
the results obtained on theMSRA, CityU and LDC
test sets. These numbers greatly underrepresent
what could be expected from the boosting model,
since we only used one-third of MSRA and CityU
training corpora due to limitations of the boost-
ing software. Another problem for the LDC cor-
pus was training/testing mismatch: we did not
train any models at all with the LDC training cor-
pus, which was the only training set annontated
with geopolitical entities (GPE). Instead, for the
LDC test set, we simply used the system trained
on the MSRA corpus. Thus, when we consider
the geopolitical entity (GPE), our low overall F-
measure on the LDC test set cannot be interpreted
meaningfully.3 Even so, using only one-third of
the training data, the results on the MSRA and
CityU test sets are reasonable: 75.07 and 80.51
overall F-measures were obtained on the MSRA
and CityU test sets, respectively.
5 Conclusion
We have described an experiment applying a
boosting based NER model originally designed
3Our LDC test result was scored twice by the organizer.
for multiple European languages instead to the
Chinese named entity recognition task. Even
though we only used one-third of the MSRA and
CityU corpora to train the system, the model
produced reasonable results, obtaining 75.07 and
80.51 overall F-measures on MSRA and CityU
test sets respectively.
Having established this baseline for compari-
son against our multilingual European language
boosting based NER models, our next step will be
to incorporate Chinese-specific attributes into the
model to compare with.
References
Eric Bauer and Ron Kohavi. An empirical comparison of
voting classification algorithms: Bagging, boosting, and
variants. Machine Learning, 36:105?142, 1999.
Xavier Carreras, Llu??s Ma`rquez, and Llu??s Padro?. Named en-
tity extraction using AdaBoost. In Computational Natural
Language Learning (CoNLL-2002), at COLING-2002,
pages 171?174, Taipei, Sep 2002.
Gerard Escudero, Llu??s Ma`rquez, and German Rigau. Boost-
ing applied to word sense disambiguation. In 11th Euro-
pean Conference on Machine Learning (ECML-00), pages
129?141, 2000.
Yoav Freund and Robert E. Schapire. A decision-theoretic
generalization of on-line learning and an application to
boosting. Computer and System Sciences, 55(1):119?139,
1997.
David Opitz and Richard Maclin. Popular ensemble meth-
ods: An empirical study. Journal of Artificial Intelligence
Research, 11:169?198, 1999.
Robert E. Schapire and Yoram Singer. Improved boosting
algorithms using confidence-rated predictions. Machine
Learning, 37(3):297?336, 1999.
Robert E. Schapire and Yoram Singer. Boostexter: A
boosting-based system for text categorization. Machine
Learning, 39(2-3):135?168, 2000.
Robert E. Schapire. The boosting approach to machine learn-
ing: An overview. In MSRI workshop on Nonlinear Esti-
mation and Classification, 2002.
Fabrizio Sebastiani, Alessandro Sperduti, and Nicola Val-
dambrini. An improved boosting algorithm and its appli-
cation to automated text categorization. In Proceedings
of 9th ACM International Conference on Information and
Knowledge Management, pages 78?85, 2000.
Dekai Wu, Grace Ngai, Marine Carpuat, Jeppe Larsen, and
Yongsheng Yang. Boosting for named entity recognition.
In Computational Natural Language Learning (CoNLL-
2002), at COLING-2002, pages 195?198, Taipei, Sep
2002.
Dekai Wu, Grace Ngai, and Marine Carpuat. Why nitpicking
works: Evidence for Occam?s razor in error correctors. In
20th International Conference on Computational Linguis-
tics (COLING-2004), Geneva, 2004.
Hua Ping Zhang, Qun Liu, Xue-Qi Cheng, Hao Zhang, and
Hong Kui Yu. Chinese lexical analysis using Hierarchi-
cal Hidden Markov Model. In Proceedings of the second
SIGHAN workshop on Chinese language processing, vol-
ume 17, pages 63?70, 2003.
153
University of Colorado Dialog Systems for 
Travel and Navigation 
B. Pellom, W. Ward, J. Hansen, R. Cole, K. Hacioglu, J. Zhang, X. Yu, S. Pradhan 
Center for Spoken Language Research, University of Colorado 
Boulder, Colorado 80303, USA 
{pellom, whw, jhlh, cole, hacioglu, zjp, xiu, spradhan}@cslr.colorado.edu 
 
ABSTRACT 
This paper presents recent improvements in the development of 
the University of Colorado ?CU Communicator? and ?CU-
Move? spoken dialog systems. First, we describe the CU 
Communicator system that integrates speech recognition, 
synthesis and natural language understanding technologies using 
the DARPA Hub Architecture. Users are able to converse with an 
automated travel agent over the phone to retrieve up-to-date 
travel information such as flight schedules, pricing, along with 
hotel and rental car availability.  The CU Communicator has 
been under development since April of 1999 and represents our 
test-bed system for developing robust human-computer 
interactions where reusability and dialogue system portability 
serve as two main goals of our work.  Next, we describe our more 
recent work on the CU Move dialog system for in-vehicle route 
planning and guidance.  This work is in joint collaboration with 
HRL and is sponsored as part of the DARPA Communicator 
program.  Specifically, we will provide an overview of the task, 
describe the data collection environment for in-vehicle systems 
development, and describe our initial dialog system constructed 
for route planning. 
1. CU COMMUNICATOR 
1.1 Overview  
The Travel Planning Task 
The CU Communicator system [1,2] is a Hub compliant 
implementation of the DARPA Communicator task [3].  The 
system combines continuous speech recognition, natural 
language understanding and flexible dialogue control to enable 
natural conversational interaction by telephone callers to access 
information from the Internet pertaining to airline flights, hotels 
and rental cars.  Specifically, users can describe a desired airline 
flight itinerary to the Communicator and use natural dialog to 
negotiate a flight plan.  Users can also inquire about hotel 
availability and pricing as well as obtain rental car reservation 
information.   
System Overview 
The dialog system is composed of a Hub and several servers as 
shown in Fig. 1.  The Hub is used as a centralized message router 
through which servers can communicate with one another [4].  
Frames containing keys and values are emitted by each server, 
routed by the hub, and received by a secondary server based on 
rules defined in a ?Hub script?.   
 
 
 
 
 
 
 
 
 
Figure 1.  Block diagram of the functional components that 
comprise the CU Communicator system1. 
1.2 Audio Server 
The audio server is responsible for answering the incoming call, 
playing prompts and recording user input.  Currently, our system 
uses the MIT/MITRE audio server that was provided to DARPA 
Communicator program participants.  The telephony hardware 
consists of an external serial modem device that connects to the 
microphone input and speaker output terminals on the host 
computer.  The record process is pipelined to the speech 
recognition server and the play process is pipelined the text-to-
speech server.  This audio server does not support barge-in. 
Recently we have developed a new audio server that supports 
barge-in using the Dialogic hardware platform.  The new audio 
server implements a Fast Normalized Least-Mean-Square (LMS) 
algorithm for software-based echo cancellation.  During 
operation, the echo from the system speech is actively cancelled 
from the recorded audio to allow the user to cut through while 
                                                          
1
 This work was supported by DARPA through SPAWAR under 
Grant No. N66001-002-8906.  The ?CU Move? system is 
supported in part through a joint collaboration with HRL 
Laboratories. 
 
 
 
Language
Generator
Language
enerator
Hub
Speech 
Recognizer
Speech 
Recognizer
Speech 
Synthesizer
Speech 
Synthesizer
Semantic
Parser
Se antic
Parser
Dialogue
Manager
Dialogue
anager
Data Base / 
Backend
Data Base / 
Backend
Confidence
Server
Confidence
Server
Audio ServerAudio Server
www
the system is speaking.  The new audio server operates in the 
Linux environment and is currently being field-tested at CSLR.  
Because the server implements software-based echo cancellation, 
it can work on virtually any low-cost Dialogic hardware 
platform.  This server will be made available to the research 
community as a resource in the near future. 
1.3 Speech Recognizer 
We are currently using the Carnegie Mellon University Sphinx-II 
system [5] in our speech recognition server. This is a semi-
continuous Hidden Markov Model recognizer with a class 
trigram language model. The recognition server receives the 
input vectors from the audio server. The recognition server 
produces a word lattice from which a single best hypothesis is 
picked and sent to the hub for processing by the dialog manager. 
Acoustic Modeling 
During dialog interaction with the user, the audio server sends 
the acoustic samples to three Sphinx-II speech recognizers.  
While the language model is the same for each decoder, the 
acoustic models consist of (i) speaker independent analog 
telephone, (ii) female adapted analog telephone, and (iii) cellular 
telephone adapted acoustic model sets.   Each decoder outputs a 
word string hypothesis along with a word-sequence probability 
for the best path.  An intermediate server is used to examine each 
hypothesis and pass the most likely word string onto the natural 
language understanding module.   
Language Modeling 
The Communicator system is designed for end users to get up-to-
date worldwide air travel, hotel and rental car information via the 
telephone. In the task there are word lists for countries, cities, 
states, airlines, etc.  To train a robust language model, names are 
clustered into different classes. An utterance with class tagging is 
shown in Fig.2.  In this example, city, hour_number, and am_pm 
are class names. 
Figure 2.  Examples of class-based and grammar-based 
language modeling  
Each commonly used word takes one class. The probability of 
word Wi given class Ci is estimated from training corpora. After 
the corpora are correctly tagged, a back-off class-based trigram 
language model can be computed from the tagged corpora.  We 
use the CMU-Cambridge Statistical Language Modeling Toolkit 
to compute our language models. 
More recently, we have developed a dialog context dependent 
language model (LM) combining stochastic context free 
grammars (SCFGs) and n-grams [6,7].  Based on a spoken 
language production model in which a user picks a set of 
concepts with respective values and constructs word sequences 
using phrase generators associated with each concept in 
accordance with the dialog context, this LM computes the 
probability of a word, P(W), as 
 
         P(W) = P(W/C) P(C/S)          (1) 
 
where W is the sequence of words, C is the sequence of concepts 
and S is the dialog context. Here, the assumptions are (i) S is 
given, (ii) W is independent of S but C, and (iii) W and C 
associations are unambiguous. This formulation can be 
considered as a general extension of the standard class word 
based statistical language model as seen in Fig. 2. 
 
The first term in (1) is modeled by SCFGs, one for each concept. 
The concepts are classes of phrases with the same meaning. Each 
SCFG is compiled into a stochastic recursive transition network 
(STRN). Our grammar is a semantic grammar since the 
nonterminals correspond to semantic concepts instead of 
syntactic constituents. The set of task specific concepts is 
augmented with a single word, multiple word and a small number 
of broad but unambigious part of speech (POS) classes to 
account for the phrases that are not covered by the grammar. 
These classes are considered as "filler" concepts within a unified 
framework. The second term in (1) is modeled as a pool of 
concept n-gram LMs. That is, we have a separate LM for each 
dialog context. At the moment, the dialog context is selected as 
the last question prompted by the system, as it is very simple and 
yet strongly predictive and constraining. SCFG and n-gram 
probabilities are learned by simple counting and smoothing. Our 
semantic grammars have a low degree of ambiguity and therefore 
do not require computationally intensive stochastic training and 
parsing techniques. 
 
Experimental results with N-best list rescoring were found 
promising (5-6% relative improvement in WER).  In addition, we 
have shown that a dynamic combining of our new LM and the 
standard class word n-gram (the LM currently in use in our 
system) should result in further improvements. At the present, we 
are interfacing the grammar LM to the speech recognizer using a 
word graph. 
1.4 Confidence Server 
Our prior work on confidence assessment has considered 
detection and rejection of word-level speech recognition errors 
and out-of-domain phrases using language model features [8].  
More recently [9], we have considered detection and rejection of 
misrecognized units at the concept level.  Because concepts are 
used to update the state of the dialog system, we believe that 
concept level confidence is vitally important to ensuring a 
graceful human-computer interaction.  Our current work on 
concept error detection has considered language model features 
(e.g., LM back-off behavior, language model score) as well as 
acoustic features from the speech recognizer (e.g., normalized 
acoustic score, lattice density, phone perplexity).  Confidence 
Original Utterance 
I want to go from Boston to Portland around nine a_m 
Class-Tagged Utterance 
I want to go from [city:Boston] to [city:Portland] 
around [hour_number:nine] [am_pm:a_m] 
Concept-Tagged Utterance 
[I_want: I want to go] [depart_loc: from Boston] 
[arrive_loc: to Portland] [time:around nine a_m] 
features are combined to compute word-level, concept-level, and 
utterance-level confidence scores.  
1.5 Language Understanding 
We use a modified version of the Phoenix [10] parser to map the 
speech recognizer output onto a sequence of semantic frames. A 
Phoenix frame is a named set of slots, where the slots represent 
related pieces of information. Each slot has an associated 
context-free semantic grammar that specifies word string patterns 
that can fill the slot. The grammars are compiled into Recursive 
Transition Networks, which are matched against the recognizer 
output to fill slots. Each filled slot contains a semantic parse tree 
with the slot name as root.  
Phoenix has been modified to also produce an extracted 
representation of the parse that maps directly onto the task 
concept structures. For example, the utterance  
?I want to go from Boston to Denver Tuesday morning?  
would produce the extracted parse: 
Flight_Constraint: Depart_Location.City.Boston 
Flight_Constraint: Arrive_Location.City.Denver 
Flight Constraints:[Date_Time].[Date].[Day_Name].tuesday 
                             [Time_Range].[Period_Of_Day].morning 
1.6 Dialog Management 
The Dialogue Manager controls the system?s interaction with the 
user and the application server. It is responsible for deciding 
what action the system will take at each step in the interaction. 
The Dialogue Manager has several functions. It resolves 
ambiguities in the current interpretation; Estimates confidence in 
the extracted information; Clarifies the interpretation with the 
user if required; Integrates new input with the dialogue context; 
Builds database queries (SQL); Sends information to NL 
generation for presentation to user; and prompts the user for 
missing information. 
We have developed a flexible, event driven dialogue manager in 
which the current context of the system is used to decide what to 
do next. The system does not use a dialogue network or a 
dialogue script, rather a general engine operates on the semantic 
representations and the current context to control the interaction 
flow.  The Dialogue Manager receives the extracted parse. It then 
integrates the parse into the current context. Context consists of a 
set of frames and a set of global variables. As new extracted 
information arrives, it is put into the context frames and 
sometimes used to set global variables. The system provides a 
general-purpose library of routines for manipulating frames. 
This ?event driven? architecture functions similar to a production 
system. An incoming parse causes a set of actions to fire which 
modify the current context. After the parse has been integrated 
into the current context, the DM examines the context to decide 
what action to take next. The DM attempts the following actions 
in the order listed: 
? Clarify if necessary  
? Sign off if all done  
? Retrieve data and present to user  
? Prompt user for required information  
The rules for deciding what to prompt for next are very 
straightforward. The frame in focus is set to be the frame 
produced in response to the user, or to the last system prompt.  
? If there are unfilled required slots in the focus frame, then 
prompt for the highest priority unfilled slot in the frame. 
? If there are no unfilled required slots in the focus frame, 
then prompt for the highest priority missing piece of 
information in the context.  
Our mechanism does not have separate ?user initiative? and 
?system initiative? modes. If the system has enough information 
to act on, then it does it. If it needs information, then it asks for 
it. The system does not require that the user respond to the 
prompt. The user can respond with anything and the system will 
parse the utterance and set the focus to the resulting frame. This 
allows the user to drive the dialog, but doesn?t require it. The 
system prompts are organized locally, at the frame level. The 
dialog manager or user puts a frame in focus, and the system tries 
to fill it. This representation is easy to author, there is no separate 
dialog control specification required. It is also robust in that it 
has a simple control that has no state to lose track of. 
An additional benefit of Dialog Manager mechanism is that it is 
very largely declarative. Most of the work done by a developer 
will be the creation of frames, forms and grammars. The system 
developer creates a task file that specifies the system ontology 
and templates for communicating about nodes in the hierarchy. 
The templates are filled in from the values in the frames to 
generate output in the desired language. This is the way we 
currently generate SQL queries and user prompts. An example 
task frame specification is: 
Frame:Air 
 [Depart_Loc]+ 
    Prompt: "where are you departing from" 
    [City_Name]* 
 Confirm: "You are departing from $([City_Name]).  
    Is that correct?" 
 Sql: "dep_$[leg_num] in (select airport_code from 
 airport_codes where city like '!%' $(and state_province 
like '[Depart_Loc].[State]' ) )" 
    [Airport_Code]* 
 
This example defines a frame with name Air and slot 
[Depart_Loc]. The child nodes of Depart_Loc are are 
[City_Name] and [Airport_Code]. The ?+? after [Depart_Loc] 
indicates that it is a mandatory field. The Prompt string is the 
template for prompting for the node information. The ?*? after 
[City_Name] and [Airport_Code] indicate that if either of them is 
filled, the parent node [Depart_Loc] is filled. The Confirm string 
is a template to prompt the user to confirm the values. The SQL 
string is the template to use the value in an SQL query to the 
database. 
The system will prompt for all mandatory nodes that have 
prompts. Users may specify information in any order, but the 
system will prompt for whatever information is missing until the 
frame is complete.   
1.7 Database & Internet Interface  
The back-end interface consists of an SQL database and domain-
specific Perl scripts for accessing information from the Internet.  
During operation, database requests are transmitted by the Dialog 
Manager to the database server via a formatted frame. 
The back-end consists of a static and dynamic information 
component.  Static tables contain data such as conversions 
between 3-letter airport codes and the city, state, and country of 
the airport (e.g., BOS for Boston Massachusetts).  There are over 
8000 airports in our database, 200 hotel chains, and 50 car rental 
companies.  The dynamic information content consists of 
database tables for car, hotel, and airline flights.   
When a database request is received, the Dialog Manager?s SQL 
command is used to select records in local memory.  If no 
records are found to match, the back-end can submit an HTTP-
based request for the information via the Internet.  Records 
returned from the Internet are then inserted as rows into the local 
SQL database and the SQL statement is once again applied.   
1.8 Language Generation 
The language generation module uses templates to generate text 
based on dialog speech acts.  Example dialog acts include 
?prompt? for prompting the user for needed information, 
?summarize? for summarization of flights, hotels, and rental cars, 
and ?clarify? for clarifying information such as departure and 
arrival cities that share the same name. 
1.9 Text-to-Speech Synthesis 
For audio output, we have developed a domain-dependent 
concatenative speech synthesizer.  Our concatenative synthesizer 
can adjoin units ranging from phonemes, to words, to phrases 
and sentences.   For domain modeling, we use a voice talent to 
record entire task-dependent utterances  (e.g., ?What are your 
travel plans??) as well as short phrases with carefully determined 
break points (e.g., ?United flight?, ?ten?, ?thirty two?, ?departs 
Anchorage at?).    Each utterance is orthographically transcribed 
and phonetically aligned using a HMM-based recognizer.   Our 
research efforts for data collection are currently focused on 
methods for reducing the audible distortion at segment 
boundaries, optimization schemes for prompt generation, as well 
as tools for rapidly correcting boundary misalignments.  In 
general, we find that some degree of hand-correction is always 
required in order to reduce distortions at concatenation points. 
During synthesis, the text is automatically divided into individual 
sentences that are then synthesized and pipelined to the audio 
server.  A text-to-phoneme conversion is applied using a 
phonetic dictionary.  Words that do not appear in the phonetic 
dictionary are automatically pronounced using a multi-layer 
perceptron based pronunciation module.  Here, a 5-letter context 
is extracted from the word to be pronounced.  The letter input is 
fed through the MLP and a phonetic symbol (or possibly epsilon) 
is output by the network.  By sliding the context window, we can 
extract the phonetic pronunciation of the word.   The MLP is 
trained using letter-context and symbol output pairs from a large 
phonetic dictionary. 
The selection of units to concatenate is determined using a hybrid 
search algorithm that operates at the word or phoneme level.  
During synthesis, sections of word-level text that have been 
recorded are automatically concatenated.  Unrecorded words or 
word sequences are synthesized using a Viterbi beam search 
across all available phonetic units.  The cost function includes 
information regarding phonetic context, pitch, duration, and 
signal amplitude.  Audio segments making up the best-path are 
then concatenated to generate the final sentence waveform.   
2. DATA COLLECTION & EVALUATION 
2.1 Data Collection Efforts 
Local Collection Effort 
The Center for Spoken Language Research maintains a dialup 
Communicator system for data collection1. Users wishing to use 
the dialogue system can register at our web site [1] and receive a 
PIN code and system telephone number. To date, our system has 
fielded over 1750 calls totaling over 25,000 utterances from 
nearly 400 registered users.  
NIST Multi-Site Data Collection 
During the months of June and July of 2000, The National 
Institute of Standards (NIST) conducted a multi-site data 
collection effort for the nine DARPA Communicator 
participants.  Participating sites included: AT&T, IBM, BBN, 
SRI, CMU, Colorado, MIT, Lucent, and MITRE.  In this data 
collection, a pool of potential users was selected from various 
parts of the United States by a market research firm.  The 
selected subjects were native speakers of American English who 
were possible frequent travelers.  Users were asked to perform 
nine tasks.  The first seven tasks consisted of fixed scenarios for 
one-way and round-trip flights both within and outside of the 
United States. The final two tasks consisted of users making 
open-ended business or vacation.   
2.2 System Evaluation 
Task Completion 
A total of 72 calls from NIST participants were received by the 
CU Communicator system.  Of these, 44 callers were female and 
28 were male.  Each scenario was inspected by hand and 
compared against the scenario provided by NIST to the subject. 
For the two open-ended tasks, judgment was made based on what 
the user asked for with that of the data provided to the user. In 
total, 53/72 (73.6%) of the tasks were completed successfully.   
A detailed error analysis can be found in [11]. 
Word Error Rate Analysis 
A total of 1327 utterances were recorded from the 72 NIST calls.  
Of these, 1264 contained user speech.  At the time of the June 
2000 NIST evaluation, the CU Communicator system did not 
implement voice-based barge-in.  We noticed that one source of 
error was due to users who spoke before the recording process 
was started.  Even though a tone was presented to the user to 
signify the time to speak, 6.9% of the utterances contained 
instances in which the user spoke before the tone.  Since all users 
were exposed to several other Communicator systems that 
                                                          
2
 The system can be accessed toll-free at 1-866-735-5189 
employed voice barge-in, there may be some effect from 
exposure to those systems. Table 3 summarizes the word error 
rates for the system utilizing the June 2000 NIST data as the test 
set.  Overall, the system had a word error rate (WER) of 26.0% 
when parallel gender-dependent decoders were utilized. Since 
June of 2000, we have collected an additional 15,000 task-
dependent utterances.  With the extra data, we were able to 
remove our dependence on the CMU Communicator training 
data [12].  When the language model was reestimated and 
language model weights reoptimized using only CU 
Communicator data, the WER dropped from 26.0% to 22.5%.  
This amounts to a 13.5% relative reduction in WER. 
Table 1: CU Communicator Word Error Rates for (A) 
Speaker Independent acoustic models and June 2000 
language model, (B) Gender-dependent parallel recognizers 
with June 2000 Language Model, and (C) Language Model 
retrained in December 2000. 
June 2000 NIST Evaluation Data, 1264 
utterances, 72 speakers 
Word Error 
Rate 
(A) Speaker Indep. HMMs (LM#1) 29.8% 
(B) Gender Dependent HMMs (LM#1) 26.0% 
(C) Gender Dependent HMMs (LM#2)  22.5% 
 
Core Metrics 
Sites in the DARPA Communicator program agreed to log a 
common set of metrics for their systems. The proposed set of 
metrics was: Task Completion, Time to Completion, Turns to 
Completion, User Words/Turn, System Words/Turn, User 
Concepts/Turn, Concept Efficiency, State of Itinerary, Error 
Messages, Help Messages, Response Latency, User Words to 
Completion, System Words to Completion, User Repeats, System 
Repeats/Reprompts, Word Error, Mean Length of System 
Utterance, and Mean Length of System Turn. 
Table 2: Dialogue system evaluation metrics 
Item Min Mean Max 
Time to Completion (secs) 120.9 260.3 537.2 
Total Turns to Completion 23 37.6 61 
Response Latency (secs) 1.5 1.9 2.4 
User Words to Task End 19 39.4 105 
System Words to End 173 331.9 914 
Number of Reprompts 0 2.4 15 
 
Table 2 summarizes results obtained from metrics derived 
automatically from the logged timing markers for the calls in 
which the user completed the task assigned to them.  The average 
time to task completion is 260.  During this period there are an 
average of 19 user turns and 19 computer turns (37.6 average 
total turns).  The average response latency was 1.86 seconds.  
The response latency also includes the time required to access the 
data live from the Internet travel information provider. 
3. CU MOVE 
3.1 Task Overview 
The ?CU Move? system represents our work towards achieving 
graceful human-computer interaction in automobile 
environments.  Initially, we have considered the task of vehicle 
route planning and navigation.  As our work progresses, we will 
expand our dialog system to new tasks such as information 
retrieval and summarization and multimedia access. 
The problem of voice dialog within vehicle environments offers 
some important speech research challenges. Speech recognition 
in car environments is in general fragile, with word-error-rates 
(WER) ranging from 30-65% depending on driving conditions. 
These changing environmental conditions include speaker 
changes (task stress, emotion, Lombard effect, etc.) as well as the 
acoustic environment (road/wind noise from windows, air 
conditioning, engine noise, exterior traffic, etc.).   
In developing the CU-Move system [13,14], there are a number 
of research challenges that must be overcome to achieve reliable 
and natural voice interaction within the car environment. Since 
the speaker is performing a task (driving the vehicle), the driver 
will experience a measured level of user task stress and therefore 
this should be included in the speaker-modeling phase. Previous 
studies have clearly shown that the effects of speaker stress and 
Lombard effect can cause speech recognition systems to fail 
rapidly. In addition, microphone type and placement for in-
vehicle speech collection can impact the level of acoustic 
background noise and speech recognition performance.    
3.2 Signal Processing  
Our research for robust recognition in automobile environments 
is concentrated on development of an intelligent microphone 
array.  Here, we employ a Gaussian Mixture Model (GMM) 
based environmental classification scheme to characterize the 
noise conditions in the automobile.  By integrating an 
environmental classification system into the microphone array 
design, decisions can be made as to how best to utilize a noise-
adaptive frequency-partitioned iterative enhancement algorithm 
[15,16] or model-based adaptation algorithms [17,18] during 
decoding to optimize speech recognition accuracy on the beam-
formed signal. 
3.3 Data Collection 
A five-channel microphone array was constructed using Knowles 
microphones and a multi-channel data recorder housing built 
(Fostex) for in-vehicle data collection. An additional reference 
microphone is situated behind the driver?s seat.  Fig. 3 shows the 
constructed microphone array and data recorder housing. 
      
Figure 3: Microphone array and reference microphone (left), 
Fostex multi-channel data recorder (right). 
As part of the CU-Move system formulation, a two phase data 
collection plan has been initiated. Phase I focuses on collecting 
acoustic noise and probe speech from a variety of cars and 
driving conditions. Phase II focuses on a extensive speaker 
collection across multiple U.S. sites. A total of eight vehicles 
have been selected for acoustic noise analysis. These include the 
following: a compact car, minivan, cargo van, sport utility 
vehicle (SUV), compact and full size trucks, sports car, full size 
luxury car.  A fixed 10 mile route through Boulder, CO was used 
for Phase I data collection. The route consisted of city (25 & 
45mph) and highway driving (45 & 65mph). The route included 
stop-and-go traffic, and prescribed locations where 
driver/passenger windows, turn signals, wiper blades, air 
conditioning were operated. Each data collection run per car 
lasted approximately 35-45 minutes.  A detailed acoustic analysis 
of Phase I data can be found in [13]. Our plan is to begin Phase 
II speech/dialogue data collection during spring 2001, which will 
include (i) phonetically balanced utterances, (ii) task-specific 
vocabularies, (iii) natural extemporaneous speech, and (iv) 
human-to-human and Wizard-of-Oz (WOZ) interaction with CU-
Communicator and CU-Move dialog systems. 
3.4 Prototype Dialog System 
Finally, we have developed a prototype dialog system for data 
collection in the car environment.  The dialog system is based on 
the MIT Galaxy-II Hub architecture with base system 
components derived from the CU Communicator system [1].  
Users interacting with the dialog system can enter their origin 
and destination address by voice. Currently, 1107 street names 
for Boulder, CO area are modeled.  The system can resolve street 
addresses by business name via interaction with an Internet 
telephone book.  This allows users to ask more natural route 
queries (e.g., ?I need an auto repair shop?, or ?I need to get to the 
Boulder Marriott?).  The dialog system automatically retrieves 
the driving instructions from the Internet using an online WWW 
route direction provider.  Once downloaded, the driving 
directions are queried locally from an SQL database.  During 
interaction, users mark their location on the route by providing 
spoken odometer readings.  Odometer readings are needed since 
GPS information has not yet been integrated into the prototype 
dialog system. Given the odometer reading of the vehicle as an 
estimate of position, route information such as turn descriptions, 
distances, and summaries can be queried during travel (e.g., 
"What's my next turn", "How far is it", etc.).  
The prototype system uses the CMU Sphinx-II speech recognizer 
with cellular telephone acoustic models along with the Phoenix 
Parser [10] for semantic parsing.  The dialog manager is mixed-
initiative and event driven.  For route guidance, the natural 
language generator formats the driving instructions before 
presentation to the user by the text-to-speech server.   For 
example, the direction,  "Park Ave W. becomes 22nd St." is 
reformatted to, "Park Avenue West becomes Twenty Second  
Street".  Here, knowledge of the task-domain can be used to 
significantly improve the quality of the output text.   For speech 
synthesis, we have developed a Hub-compliant server that 
interfaces to the AT&T NextGen speech synthesizer.   
3.5 Future Work 
We have developed a Hub compliant server that interfaces a 
Garmin GPS-III global positioning device to a mobile computer 
via a serial port link.  The GPS server reports vehicle velocity in 
the X,Y,Z directions as well as real-time updates of  vehicle 
position in latitude and longitude.  HRL Laboratories has 
developed a route server that interfaces to a major navigation 
content provider.  The HRL route server can take GPS 
coordinates as inputs and can describe route maneuvers in terms 
of GPS coordinates.  In the near-term, we will interface our GPS 
server to the HRL route server in order to provide real-time 
updating of vehicle position.  This will eliminate the need for 
periodic location update by the user and also will allow for more 
interesting dialogs to be established (e.g., the computer might 
proactively tell the user about upcoming points of interest, etc.). 
 
4. REFERENCES 
[1] http://communicator.colorado.edu 
[2] W. Ward, B. Pellom, "The CU Communicator System," IEEE 
Workshop on Automatic Speech Recognition and Understanding, 
Keystone Colorado, December, 1999. 
[3] http://fofoca.mitre.org 
[4] Seneff, S., Hurley, E., Lau, R., Pao, C., Schmid, P., Zue,  V., 
?Galaxy-II: A Reference Architecture for Conversational System 
Development,? Proc. ICSLP, Sydney Australia, Vol. 3, pp. 931-
934, 1998. 
[5] Ravishankar, M.K., ?Efficient Algorithms for Speech 
Recognition?. Unpublished Dissertation CMU-CS-96-138, 
Carnegie Mellon University, 1996 
[6] K. Hacioglu, W. Ward, "Dialog-Context Dependent Language 
Modeling Using N-Grams and Stochastic Context-Free Grammars", 
Proc. IEEE ICASSP, Salt Lake City, May 2001. 
[7] K. Hacioglu, W. Ward, "Combining Language Models : Oracle 
Approach", Proc. Human Language Technology Conference, San 
Diego, March 2001. 
[8] R. San-Segundo, B. Pellom, W. Ward, J. M. Pardo, "Confidence 
Measures for Dialogue Management in the CU Communicator 
System," Proc. IEEE ICASSP, Istanbul Turkey, June 2000. 
[9] R. San-Segundo, B. Pellom, K. Hacioglu, W. Ward, J.M. Pardo, 
"Confidence Measures for Dialogue Systems," Proc. IEEE ICASSP, 
Salt Lake City, May 2001.  
[10] Ward, W., ?Extracting Information From Spontaneous Speech?, 
Proc. ICSLP, September 1994. 
[11] B. Pellom, W. Ward, S. Pradhan, "The CU Communicator: An 
Architecture for Dialogue Systems", Proc. ICSLP, Beijing China, 
November 2000. 
[12] Eskenazi,  M., Rudnicky, A., Gregory, K., Constantinides, P.,  
Brennan, R., Bennett, K., Allen, J., ?Data Collection and 
Processing in the Carnegie Mellon Communicator,?   Proc. 
Eurospeech-99, Budapest, Hungary. 
[13] J.H.L. Hansen, J. Plucienkowski, S. Gallant, B.L. Pellom, W. Ward, 
"CU-Move: Robust Speech Processing for In-Vehicle Speech 
Systems," Proc. ICSLP, vol. 1, pp. 524-527, Beijing, China, Oct. 
2000. 
[14] http://cumove.colorado.edu/ 
[15] J.H.L. Hansen, M.A. Clements, ?Constrained Iterative Speech 
Enhancement with Application to Speech Recognition,? IEEE 
Trans. Signal Proc., 39(4):795-805, 1991. 
[16] B. Pellom, J.H.L. Hansen, ?An Improved Constrained Iterative 
Speech Enhancement Algorithm for Colored Noise Environments," 
IEEE Trans. Speech & Audio Proc., 6(6):573-79, 1998. 
[17] R. Sarikaya, J.H.L. Hansen, "Improved Jacobian Adaptation for 
Fast Acoustic Model Adaptation in Noisy Speech Recognition," 
Proc. ICSLP, vol. 3, pp. 702-705, Beijing, China, Oct. 2000. 
[18] R. Sarikaya, J.H.L. Hansen, "PCA-PMC: A novel use of a priori 
knowledge for fast model combination," Proc. ICASSP, vol. II, pp. 
1113-1116, Istanbul, Turkey, June 2000. 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1065?1072
Manchester, August 2008
An Integrated Probabilistic and Logic Approach to Encyclopedia Relation
Extraction with Multiple Features
?
Xiaofeng YU Wai LAM
Information Systems Laboratory
Department of Systems Engineering & Engineering Management
The Chinese University of Hong Kong
Shatin, N.T., Hong Kong
{xfyu,wlam}@se.cuhk.edu.hk
Abstract
We propose a new integrated approach based on
Markov logic networks (MLNs), an effective com-
bination of probabilistic graphical models and first-
order logic for statistical relational learning, to ex-
tracting relations between entities in encyclopedic
articles from Wikipedia. The MLNs model en-
tity relations in a unified undirected graph col-
lectively using multiple features, including contex-
tual, morphological, syntactic, semantic as well as
Wikipedia characteristic features which can cap-
ture the essential characteristics of relation extrac-
tion task. This model makes simultaneous statisti-
cal judgments about the relations for a set of related
entities. More importantly, implicit relations can
also be identified easily. Our experimental results
showed that, this integrated probabilistic and logic
model significantly outperforms the current state-
of-the-art probabilistic model, Conditional Random
Fields (CRFs), for relation extraction from encyclo-
pedic articles.
1 Introduction
Relation extraction is a growing area of research
that discovers various predefined semantic relations
(e.g., visited, associate, and executive) between en-
tity pairs in text. As a subtask in Information Ex-
traction (IE), this problem has generated much in-
terest and has been formulated as part of Message
Understanding Conferences (MUC) and Automatic
Content Extraction (ACE) Evaluation.
Reliably extracting relations between entities in
natural-language documents is still a difficult, un-
solved problem. A large number of engineered
systems were developed for identifying relations
of interest. Recent approaches to this problem in-
?
The work described in this paper is substantially sup-
ported by grants from the Research Grant Council of the Hong
Kong Special Administrative Region, China (Project Nos:
CUHK4193/04E and CUHK4128/07) and the Direct Grant of
the Faculty of Engineering, CUHK (Project Codes: 2050363
and 2050391). This work is also affiliated with the Microsoft-
CUHK Joint Laboratory for Human-centric Computing and
Interface Technologies.
?
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported license
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some
rights reserved.
clude statistical parsing (Miller et al, 2000), lo-
gistic regression (Kambhatla, 2004), feature-based
methods (Zhou et al, 2005; Toru et al, 2007),
and kernel methods (Zelenko et al, 2003; Culotta
and Sorensen, 2004; Bunescu and Mooney, 2005,
2006).
In text, this usually amounts to examining pairs
of entities in a document and determining whether
a relation exists between them. In general, the
above approaches to relation extraction suffer from
the following three difficulties: (1) enumerating
all pairs of entities, even when restricted to pairs
within a sentence, results in a low density of pos-
itive relation examples, (2) these approaches as-
sume that relations only exist within document, and
classify them independently without considering
dependencies between entities. However, this as-
sumption does not hold in practice, and ignoring
dependencies between entities may lead to reduced
performance, and (3) implicit relations can hardly
be discovered in these models since they generally
exist in cross document and they are only implied
by the text. And these are the sorts of relations
on which current extraction models perform most
poorly.
In this paper we propose a new integrated ap-
proach based on Markov logic networks (MLNs)
to extracting relations between entities in English
encyclopedic articles from Wikipedia. We pre-
dict only relations between the principal entity and
each mentioned secondary entity in Wikipedia ar-
ticles. By anchoring one argument of relations
to be the principal entity, we alleviate the diffi-
culty of enumerating all pairs of entities in a doc-
ument. This approach can incorporate rich depen-
dencies between entities by modeling entity rela-
tions in a coherent undirected graph in a collective
manner, and make simultaneous statistical judg-
ments about the relations for a set of related enti-
ties. It can also exploit relational autocorrelation,
a widely observed characteristic of relational data
1065
in which the value of a variable for one instance is
highly correlated with the value of the same vari-
able on another instance. We show how a vari-
ety of well-engineered features can be easily and
concisely formulated as first-order logic and incor-
porated into MLNs, and we show how implicit re-
lations can be easily discovered in this modeling.
We apply Gibbs sampling, a widely used Markov
chain Monte Carlo (MCMC) algorithm, to perform
collective inference in MLNs. Experimental re-
sults showed that this model yields substantially
better results on encyclopedia relation extraction
over the current state-of-the-art probabilistic rela-
tion extraction model, such as Conditional Random
Fields (CRFs).
2 Wikipedia
Wikipedia
1
is the world?s largest free online ency-
clopedia, representing the outcome of a continuous
collaborative effort of a large number of volunteer
contributors. Virtually any Internet user can cre-
ate or edit a Wikipedia webpage, and this ?freedom
of contribution? has a positive impact on both the
quantity (fast-growing number of articles) and the
quality (potential mistakes are quickly corrected
within the collaborative environment) of this online
resource. Currently Wikipedia has approximately
9.25 million articles in more than 200 languages.
We investigate the task of discovering seman-
tic relations between entity pairs from Wikipedia?s
English encyclopedic articles. The basic entry in
Wikipedia is an article, which mainly defines and
describes an entity (also known as principal en-
tity) or an event, and consists of a hypertext docu-
ment with hyperlinks to other pages within or out-
side Wikipedia. This document mentions some
other entities as secondary entities related to the
principal entity (Culotta et al, 2006). All the
entities are hyper-linked within the text, and the
topic of an article usually defines the principal en-
tity. Moreover, Wikipedia has the category hier-
archy structure which is used to classify articles
according to their content. All these characteris-
tics makeWikipedia an appropriate resource for the
task of relation extraction. In this paper, we predict
only relations between the principal entity and each
mentioned secondary entity.
An illustrative example of Wikipedia article is
shown in Figure 2, where the principal entity
Albert Einstein is boxed and in italic font, and sec-
1
http://www.wikipedia.org/
Albert Einstein
Albert Einstein (March 14, 1879 - April 18, 1955) was a the-
oretical physicist. He was born in Germany. His father was
Hermann Einstein, a salesman and engineer, and his mother
was Pauline Einstein. In 1880, the family moved to Munich.
Albert attended a Catholic elementary school and finally he
was enrolled in the mathematics program at ETH Zurich. Ein-
stein received the Nobel Prize in Physics for his services to
Theoretical Physics in 1921.
Figure 1: An example of Wikipedia article for rela-
tion extraction. The principal entity is boxed and in
italic font, and secondary entities are in italic font.
ondary entities are in italic font. Our goal is to
predict what relation, if any, each secondary entity
has to the principal entity. For example, there is a
job title relation between theoretical physicist and
Albert Einstein and a father relation between Her-
mann Einstein and Albert Einstein , but no relation
between salesman and Albert Einstein .
3 Relation Extraction as Sequence
Labeling: A Baseline Approach
Note that our goal is to extract relations between
the principal entity and each mentioned secondary
entity in Wikipedia?s English encyclopedic articles.
This formulation allows us to view relation extrac-
tion as a sequence labeling task such as part-of-
speech tagging. Motivated by this observation, we
therefore apply Conditional Random Fields (CRFs)
(Lafferty et al, 2001), a probabilistic graphical
model that has been successfully employed on se-
quence labeling tasks with state-of-the-art perfor-
mance. By using the CRF model, each secondary
entity?s label is its relation to the principal entity,
and we can capture the dependency between ad-
jacent labels. For example, in the dataset it is
common to see phrases such as ? Albert Einstein
(1879 - 1955) was born in Germany? for which
the labels birth year, death year, and birth place
occur consecutively. Sequence models are specifi-
cally designed to handle these kinds of dependen-
cies. The modeling flexibility of CRFs permits
the feature functions to be complex, arbitrary, non-
independent, and overlapping features of the input
without requiring additional assumptions, allowing
the multiple features described in Section 5 to be
directly exploited. To avoid overfitting, we penal-
ized the log-likelihood by the commonly used zero-
mean Gaussian prior over the parameters. This
gives us a competitive baseline CRF model for re-
lation extraction.
1066
4 Markov Logic Networks for Collective
Relation Extraction
Markov logic networks (MLNs) conduct statisti-
cal relational learning (SRL) by incorporating the
expressiveness of first-order logic into the flexibil-
ity of probabilistic graphical models under a single
coherent framework (Richardson and Domingos,
2006). An MLN consists of a set of weighted for-
mulae and provides a way of softening first-order
logic by making situations, in which not all for-
mulae are satisfied, less likely but not impossible.
More formally, the probability distribution of a par-
ticular truth assignment x to X specified by the
ground Markov network M
L,C
2
is given by
P (X = x) =
1
Z
exp
(
?
w
i
n
i
(x )
)
=
1
Z
?
?
i
(
x
{i}
)
n
i
(x)
(1)
where X is the set of all propositions describing
a world x (i.e. all gliterals formed by grounding
the predicates with the constants in the domain),
F is the set of all clauses in the MLN, w
i
is the
weight associated with clause F
i
? F , n
i
(x) is the
number of true groundings of F
i
in x, x
{i}
is the
true value of the atoms appearing in F
i
, Z is the
normalizing partition function, ?
i
is a real-valued
potential function and ?
i
(
x
{i}
)
= e
w
i
.
MLNs model the relation extraction task in a
collective manner and take into account the rela-
tion types of related entities. Note that this is dif-
ferent from other relation extraction methods that
predict relations independently without consider-
ing the relationship between entities. Attributes can
be represented in MLNs as predicates of the form
A(x, v), where A is an attribute, x is an entity, and
v is the value of A in x. The relation is a desig-
nated attribute C, representable by C(x, v), where
v is x?s relation. The relations of different entities
depend on each other. Classification is now simply
the problem of inferring the truth value of C(x, v)
for all x and v of interest given all known A(x, v).
In this collective modeling, the Markov blanket of
C(x
i
, v) includes other C(x
j
, v), even after condi-
tioning on the known A(x, v). Relations between
entities are represented by predicates of the form
R(x
i
, x
j
).
2
The graphical structure of M
L,C
is that: there is an edge
between two nodes of M
L,C
iff the corresponding ground
atoms appear together in at least one grounding of one first-
order formula.
4.1 Weight Learning
Given a relational database and a set of first-order
logic, the weight of each clause can in principle be
learned very efficiently by maximizing the pseudo-
log-likelihood of this database on the closed world
assumption using the limited-memory BFGS algo-
rithm (Liu and Nocedal, 1989). These weights re-
flect how often the clauses are actually observed in
the training data.
To estimate the weights, we maximize the loga-
rithm of the conditional likelihood of the training
data
?
(x
h
,x
o
)?T
log
(
p(X
h
= x
h
|X
o
= x
o
)
)
(2)
where X
h
is a list of possible variables and x
h
are
the corresponding values in the observation. X
h
contains all variables referring to possible ground
atoms of entity relations. X
o
is the set of variables
corresponding to all possible instantiations of the
predicates. T is the set of training observations
(x
h
, x
o
). For relation extraction, Equation 2 can
be rewritten as
p(X
h
= x
h
|X
o
= x
o
) =
?
Entity pairs(p,q)
p
(
X
e(p,q)
= x
e(p,q)
|X
g(p,q)
= x
g(p,q)
)
(3)
where X
e(p,q)
corresponds to the ground atoms,
and X
g(p,q)
is a list of all variables corresponding
to predicates.
With Equation 3, the conditional likelihood in
Equation 2 simplifies to
?
(x
h
,x
o
)?T
?
Entity pairs(p,q)
log
(
p
(
x
e(p,q)
|x
g(p,q)
)
)
.
(4)
where p(x|y) is the abbreviation for p(X = x|Y =
y). To calculate the conditional likelihood, we have
p
(
x
e(p,q)
|x
g(p,q)
)
=
p
(
x
e(p,q)
, x
g(p,q)
)
p
(
1, x
g
(p, q)
)
+ p
(
0, x
g
(p, q)
)
(5)
During MLN weight learning, each first-order
formula is converted to Conjunctive Normal Form
(CNF). The probabilities of all formulae collec-
tively determine all weights, if we view them
as empirical probabilities and learn the maximum
likelihood weights. Conversely, the weights in a
learned MLN can be viewed as collectively encod-
ing the empirical formula probabilities.
1067
4.2 Inference
In order to perform inference over a given MLN,
one needs to ground it into its corresponding
Markov network (Pearl, 1988). A large number
of efficient inference techniques are applicable and
the most widely used approximate solution to prob-
abilistic inference in MLNs is Markov chain Monte
Carlo (MCMC) (Gilks et al, 1996). One such al-
gorithm to perform collective inference is called
Gibbs sampling. Gibbs sampling starts by assign-
ing a truth value to each query gliteral (a ground
literal, i.e. one that contains only ground terms). It
then proceeds in rounds to re-sample a value for
gliteral X , given the truth values of its Markov
blanket MB
X
(i.e. the nodes with which it par-
ticipates in ground clauses).
5 Feature Set
We describe the features used in our model. These
features have been shown to be very effective for
relation extraction.
Contextual features: Bag-of-words consisting of
4 words to the left and right of the target entity.
Part-of-Speech: Part-of-speech tags are obtained
using the Stanford POS Tagger
3
, which used rich
knowledge sources and features in a log-linear
model. POS tags with a window size of 4 around
the target entity are used.
Morphological features: Such as whether the en-
tity is capitalized or contains digits or punctuation,
whether the entity ends in some suffixes such as
-eer and -ician, etc.
Syntactic features: Syntactic information can lead
to significant improvements in extraction accuracy
(e.g., Culotta and Sorensen (2004), Bunescu and
Mooney (2005)). The POS-tagged corpus is
submitted to the Stanford Lexicalized Dependency
Parser
4
which generates a dependency parse tree
for each sentence and assigns word positions to
each word. This parser can also output grammati-
cal relations (typed dependency). The grammatical
relations are of the form relation(rel
i
, w
i
, w
j
),
where rel
i
is one of the fixed set of relations
assigned by the parser, and w
i
and w
j
are two
words. The dependency paths, which contain the
relevant terms describing the relations between the
entity pairs, can be easily extracted. We design a
set of first-order formulae that captures some of the
most important syntactic phenomena for relation
3
http://nlp.stanford.edu/software/tagger.shtml
4
http://nlp.stanford.edu/software/lex-parser.shtml
Table 1: Representative relation types and corre-
sponding keywords.
Relation Keywords
job title secretary, writer, novelist, captain, cartoon-
ist, actor, actress, physicist, mathematician,
singer, naturalist, architect, musician, physi-
cian, professor, journalist, banker, business-
man, producer, philosopher, worker
visited from, to, in, at, near, along, visited
associate work for, along with, together with, perform
with, work with, colleague, struck with
member of member of, serve in, serve at, serve with, se-
lect to, campaign for, election to, involve in,
captain with, play for, fellow of, enter
opus sitcom, picture, film, teleplay, novel, essay,
comedy, autobiography, show, movie, plot,
drama, painting, book, cartoon, song, music
education university, academy, school, college, insti-
tute
executive lead, head, leader, president, chairman, com-
mittee, executive, officer, mayor, prince,
chair, governor
birth place born in, born at, birth
death place bury in, died in, died at, pass away, inter
nationality American, English, Irish, French, Italian,
Australian, Canadian, Jewish, Russian
award award, medal, fellowship, prize, pennant,
scholarship
participant during, through
extraction.
Entity features: Important entities are hyper-
linked within the text, but they are not classified by
type. Entity type is very helpful for relation extrac-
tion. For instance, the relation between a person
and a location should be visited, birth place,
death place, etc., but cannot be executive, founder,
etc. We identify named entities (person, location
and organization) by applying the Stanford Named
Entity Recognizer
5
, a CRF based sequence label-
ing model coupled with well-engineered features
including additional distributional similarity fea-
tures. The model is trained on data from CoNLL,
MUC-6, MUC-7, and ACE, making it fairly robust
in practice. Types of other entities (e.g., date, year,
and month) can be well classified by rule-based
approach due to their relatively fixed forms.
Keyword features: Some keywords provide
crucial clues for relationships between entity pairs.
Consider the following sentence:
Bill Gates is the founder of the Microsoft Corpo-
ration.
If Bill Gates is the principal entity and Microsoft
is the secondary entity, the keyword founder im-
plies that there is a founder relation between them.
Similarly, the executive relation may be implied by
5
http://nlp.stanford.edu/software/CRF-NER.shtml
1068
keywords lead, head, leader, president, chairman,
executive officer, director, and administrator.
Moreover, it is particularly interesting that some
entities indicate their relation types to correspond-
ing principal entities. Entities containing keywords
such as secretary, writer, novelist or actor show a
job title relation to their principal entities. We ex-
ploit tf-idf approach to co-occurrence (collocation)
analysis for keyword extraction. Tf-idf is used to
measure the relevance of words with a window
size of 8 to each relation between entity pairs. And
then we rank the relevance scores with respect to
each relation and choose keywords with scores
higher than the user-defined threshold.
Semantic features: Due to data sparseness, tf-idf
model might be unsatisfactory to extract sufficient
keywords. We employ WordNet (Fellbaum, 1998),
an online lexical database, to extend and enrich
each keyword candidate to its synonyms (synsets).
For example, the keyword university for relation
education is extended to the set {university,
academy, college, institute}. Table 1 shows some
representative relation types and keywords using
tf-idf method and semantic extension.
Wikipedia characteristic features: Relations
only exist between principal entities and secondary
entities. There is no relation between any two
principal entities p, q or two secondary entities x, y.
6 First-Order Logic Representation
All the features described in Section 5 can be eas-
ily and concisely represented by first-order for-
mulae, which are used during the MLN learning.
First-order formulae are recursively constructed
from atomic formulae using logical connectives
and quantifiers. Atomic formulae are constructed
using constants, variables, functions, and predi-
cates. We give a couple of examples here.
For contextual features, it is common to see
two secondary entities x and y occur consecu-
tively, accompanied by conjunctions such as ?and?
or punctuation such as ?,?, then probably the
two entities may have the same relation to the
principal entity p. This can be written in first-
order logic form as occur conse(x,y) ?
same relation(x,y). For morphological
features, suffixes such as -eer and -ician may
probably show a job title relation to the princi-
pal entity p. We therefore can easily write down
the logic person(p) ? job suffix(x) ?
job title(x,p) to capture this information.
Entity features can be represented using some
first-order formulae such as:
person(p)?location(x) ? visited(x,
p) ? birth place(x,p) ? death place(x
,p)
person(p)?location(x) ? !executive
(x,p)?!founder(x,p). The formula found
er key(x,p) ? founder relation(x,p)
can be used for keyword features. And
Wikipedia characteristic features can be well
and easily expressed by the logic principal(p)
? principal(q) ? no relation(p,q)
and secondary(x) ? secondary(y) ?
no relation(x,y).
It is worth noticing that some features can be
combined in first-order logic formulation. For ex-
ample, person(p) ? organization(x) ? f
ounder key(x,p) ? founder relation(
x,p)means if there is a founder keyword between
a person and an organization, probably there is a
founder relation between them.
7 Implicit Relation Extraction
Implicit relations are those that do not have direct
contextual evidence. Implicit relations generally
exist in different paragraphs, or even across doc-
uments. They require additional knowledge to be
detected. Notably, these are the sorts of relations
that are likely to have significant impact on per-
formance. A system that can accurately discover
knowledge that is implied by the text will effec-
tively provide access to the implications of a cor-
pus. Unfortunately, extracting implicit relations is
challenging even for current state-of-the-art rela-
tion extraction models.
We show that MLNs can enable this technol-
ogy. By employing the first-order logic formalism,
the implicit relations can be easily discovered from
text. Since these formulae will not always hold,
we would like to handle them probabilistically by
estimating the confidence of each formula. One
Table 2: Examples of first-order logic for implicit
relation extraction.
wife(x,y)? husband(y,x)
father(x,y)? son(y,x) ? daughter(y,x)
brother(x,y)? brother(y,x) ? sister(y,x)
husband(x,y) ? daughter(z,x)? mother(y,z)
father(x,y) ? father(y,z)? grandfather(x,z)
founder(x,y) ? superior(x,z)? employer(z,y)
associate(x,y) ? member of(x,z)? member of(y,z)
executive(x,y) ? member of(z,y)? superior(x,z)
1069
of the benefits of the MLN probabilistic extraction
model is that confidence estimates can be straight-
forwardly obtained.
Consider the following 2 sentences in Wikipedia
articles:
1. On November 4, 1842 Abraham Lincoln
married Mary Todd.
2. Abraham Lincoln had a son named Robert
Todd Lincoln and he was born in Springfield,
Illinois on 1 August 1843.
State-of-the-art extraction models may be able to
detect the wife relation between Mary Todd and
Abraham Lincoln , and the son relation between
Robert Todd Lincoln and Abraham Lincoln suc-
cessfully from local contextual clues. However,
in the descriptive article of Robert Todd Lincoln
in Wikipedia, Robert Todd Lincoln becomes the
principal entity, and the mother relation between
Mary Todd and Robert Todd Lincoln is only im-
plied by the text and it is an implicit relation.
First-order formalism allows the representation of
deep and relational knowledge. Using the logic
wife(x,y) ? son(z,y) ? mother(x,z),
the relational knowledge in the above example can
be easily captured to infer the implicit relation.
These formulae are generally simple, and capture
important knowledge for implicit relation extrac-
tion. Examples of first-order logic to infer implicit
relations are listed in Table 2.
8 Experiments
8.1 Data
We use the same dataset as in (Culotta et al, 2006)
to conduct our experiments. This dataset consists
of 1127 paragraphs from 441 pages from the on-
line encyclopedia Wikipedia with 4701 relation in-
stances and 53 relation types labeled. Table 3
shows the relation types and corresponding fre-
quencies of this dataset.
This dataset was split into training and testing
sets (70%-30% split), attempting to separate the en-
tities into connected components. There are still
occasional paths connecting entities in the training
set to those in the testing set, and we believe this
methodology reflects a typical real-world scenario.
8.2 Results and Discussion
We design 38 first-order logic formulae (15 for-
mulae are used for implicit relation extraction) to
Table 3: Statistics of relation types and correspond-
ing frequencies.
Relation Frequency Relation Frequency
job title 379 daughter 35
visited 368 husband 33
birth place 340 religion 32
associate 326 influence 31
birth year 287 underling 27
member of 283 sister 20
birth day 283 grandfather 20
opus 267 ancestor 19
death year 210 grandson 18
death day 199 inventor 15
education 185 cousin 13
nationality 148 descendant 11
executive 127 role 10
employer 111 nephew 9
death place 93 uncle 6
award 86 supported person 6
father 84 granddaughter 6
participant 81 owns 4
brother 71 great grandson 4
son 68 aunt 4
associate competition 58 supported idea 3
wife 57 great grandfather 3
superior 54 gpe competition 3
mother 50 brother in law 2
political affiliation 44 grandmother 1
friend 43 discovered 1
founder 43 Overall 4701
construct the structure of MLNs. Using the fea-
tures described in Section 5, we train MLNs using
a Gaussian prior with zero mean and unit variance
on each weight to penalize the pseudo-likelihood,
and with the weights initialized at the mode of
the prior (zero). The features specify a ground
Markov network (e.g., ground atoms) containing
one feature for each possible grounding of a first-
order formula. Inference is performed for answer-
ing the query predicates, given the evidence pred-
icates and other relations that can be deterministi-
cally derived. We apply Gibbs sampling to predict
relations of entity pairs simultaneously.
Table 4 presents the performance of our rela-
tion extraction system based on MLNs compared
to CRFs for different types of relations. We use
the same set of features for both MLNs and CRFs.
For MLNs, all the features are represented using
first-order logic. It shows that the MLN system per-
forming collective relation prediction and integrat-
ing implicit relation extraction yields substantially
better results, leading to an improvement of up to
1.84% on the overall F-measure over the current
state-of-the-art CRF model. The improvement is
statistically significant (p < 0.05 with a 95% con-
fidence interval) according to McNemar?s paired
tests.
As shown in Table 4, the performance varies
greatly from different relation types. Both of the
two systems perform quite well on 4 relations:
death day, death year, birth day, and birth year.
1070
Table 4: Comparative relation extraction performance. Both CRFs and MLNs are tested on the same set
of features in Section 5.
CRFs MLNs
Relation Precision Recall F
?=1
Precision Recall F
?=1
death day 100.00% 94.74% 97.30 98.85% 96.00% 97.40
death year 98.21% 94.83% 96.49 98.14% 95.18% 96.64
birth year 95.12% 95.12% 95.12 94.59% 95.68% 95.13
birth day 93.90% 95.06% 94.48 93.20% 95.80% 94.48
nationality 88.37% 95.00% 91.57 88.10% 95.02% 91.43
birth place 86.81% 92.94% 89.77 87.78% 93.32% 90.47
job title 87.07% 91.82% 89.38 87.63% 91.55% 89.55
death place 89.47% 80.95% 85.00 91.66% 82.99% 87.11
education 72.41% 89.36% 80.00 75.11% 90.22% 81.97
father 70.97% 88.00% 78.57 71.88% 89.82% 79.85
wife 72.22% 81.25% 76.47 72.30% 81.75% 76.74
award 94.12% 61.54% 74.42 80.88% 66.49% 72.98
mother 81.82% 64.29% 72.00 80.89% 69.33% 74.67
political affiliation 100.00% 53.33% 69.57 85.66% 57.12% 68.54
husband 66.67% 60.00% 63.16 67.39% 62.48% 64.84
visited 66.29% 55.14% 60.20 66.70% 55.83% 60.78
daughter 66.67% 54.55% 60.00 63.67% 59.00% 61.25
founder 81.82% 47.37% 60.00 77.39% 52.63% 62.65
member of 59.32% 49.30% 53.85 60.91% 51.66% 55.90
executive 64.00% 44.44% 52.46 60.20% 48.48% 53.71
superior 66.67% 42.11% 51.61 60.55% 44.23% 51.12
brother 50.00% 46.67% 48.28 48.80% 48.57% 48.68
opus 68.00% 33.33% 44.74 50.55% 44.75% 47.47
son 50.00% 39.13% 43.90 49.30% 41.55% 45.09
associate 42.28% 45.22% 43.70 40.77% 47.89% 44.04
participant 41.67% 23.81% 30.30 31.98% 26.05% 28.71
employer 46.67% 21.21% 29.17 47.78% 27.33% 34.77
associate competition 23.08% 20.00% 21.43 24.38% 20.42% 22.22
religion 100.00% 8.33% 15.38 15.55% 10.23% 12.34
friend 0 0 0 50.38% 42.33% 46.01
sister 0 0 0 34.66% 20.55% 25.80
grandfather 0 0 0 23.74% 16.56% 19.51
grandson 0 0 0 20.01% 13.39% 16.04
cousin 0 0 0 22.00% 7.13% 10.77
other types 0 0 0 0 0 0
Overall 73.57% 64.20% 68.57 74.70% 66.58% 70.41
Since these relations can be easily identified us-
ing the distinct contextual evidence. However,
some relations (e.g., role, owns, etc.) can hardly
be extracted. One possible reason is the lack of
training data (these relations occur rarely in the
dataset). Among all the 53 relation types in the
dataset, MLNs successfully extract 34 relations,
while CRFs can only detect 29. For all the 34 rela-
tions listed in Table 4, MLNs outperform CRFs on
27 types of them. It is particularly interesting that
MLNs can successfully predict relations friend, sis-
ter, grandfather, grandson, and cousin, whereas
CRFs cannot. CRFs perform relation extraction
sequentially without considering connections be-
tween entities. This may lead to the label incon-
sistency problem. For example, CRF sometimes
fails to label the father relation between George H.
W. Bush and George W. Bush . Implicit relations
can hardly be investigated in this sequence label-
ing model. These disadvantages limit the ability of
CRFs for relation extraction to a large extent.
9 Related Work
Only a few research work has attempted relation
extraction from Wikipedia. Culotta et al (2006)
proposed a probabilistic model based on CRFs
to integrate extraction and data mining tasks per-
formed on biographical Wikipedia articles. Rela-
tion extraction was treated as a sequence labeling
problem and relational patterns were discovered to
boost the performance. However, this model ex-
tracts relations without considering dependencies
between entities, and the best reported F-measure is
67.91, which is significantly (by 2.5%) lower than
our MLN system when evaluated on the same train-
ing and testing sets. Nguyen et al (2007b,a) pro-
posed a subtree mining approach to extracting rela-
tions from Wikipedia by incorporating information
1071
from the Wikipedia structure and by the analysis of
Wikipedia text. In this approach, a syntactic tree
that reflects the relation between a given entity pair
was built, and a tree-mining algorithm was used to
identify the basic elements of syntactic structure of
sentences for relations. This approach mainly relies
on syntactic structures to extract relations. Syntac-
tic structures are important for relation extraction,
but insufficient to extract relations accurately. The
obtained F-measure was only 37.76, which shows
that there is a large room for improving. To the
best of our knowledge, our approach is the first at-
tempt at using MLNs for relation extraction from
Wikipedia which achieves state-of-the-art perfor-
mance.
We mention some other related work. Bunescu
and Mooney (2007) presented an approach to ex-
tract relations from the Web using minimal super-
vision. Rosenfeld and Feldman (2007) presented
a method for improving semi-supervised relation
extraction from the Web using corpus statistics on
entities. Our work is different from these research
work. We investigate supervised relation extraction
fromWikipedia based on probabilistic and logic in-
tegrated graphical models.
10 Conclusion
We summarize the contribution of this paper. First,
we propose a new integrated model based on
MLNs, which provide a natural and systematic way
by modeling entity relations in a coherent undi-
rected graph collectively and integrating implicit
relation extraction easily, to extract relations in en-
cyclopedic articles from Wikipedia. Second, we
design multiple features which can be concisely
formulated by first-order logic and exploit the col-
lective inference algorithm (Gibbs sampling) to
predict relations between entity pairs simultane-
ously. Third, our system achieved significantly bet-
ter results compared to the current state-of-the-art
probabilistic model for relation extraction from en-
cyclopedic articles.
Having established this relation extraction
model, our next step will be to evaluate it on larger
datasets, where we expect collective relation ex-
traction and implicit relation discovery to be even
more interesting.
References
Razvan C. Bunescu and Raymond J. Mooney. A shortest path
dependency kernel for relation extraction. In Proceedings
of HLT-EMNLP 2005, pages 724?731, Vancouver, British
Columbia, Canada, 2005.
Razvan C. Bunescu and Raymond J. Mooney. Subsequence
kernels for relation extraction. In Y. Weiss, B. Sch?olkopf,
and J. Platt, editors, Advances in Neural Information Pro-
cessing Systems 18, pages 171?178. MIT Press, Cam-
bridge, MA, 2006.
Razvan C. Bunescu and Raymond J. Mooney. Learning to
extract relations from the Web using minimal supervision.
In Proceedings of ACL-07, pages 576?583, Prague, Czech
Republic, June 2007.
Aron Culotta and Jeffrey Sorensen. Dependency tree ker-
nels for relation extraction. In Proceedings of ACL-04,
Barcelona, Spain, 2004.
Aron Culotta, Andrew McCallum, and Jonathan Betz. Inte-
grating probabilistic extraction models and data mining to
discover relations and patterns in text. In Proceedings of
HLT-NAACL 2006, pages 296?303, New York, 2006.
Christiane Fellbaum, editor. WordNet: An Electronic Lexical
Database. The MIT Press, 1998.
W.R. Gilks, S. Richardson, and D.J. Spiegelhalter. Markov
chain Monte Carlo in practice. Chapman and Hall, Lon-
don, UK, 1996.
Nanda Kambhatla. Combining lexical, syntactic, and semantic
features with maximum entropy models for extracting rela-
tions. In Proceedings of ACL-04, Barcelona, Spain, 2004.
John Lafferty, Andrew McCallum, and Fernando Pereira.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
ICML-01, pages 282?289. Morgan Kaufmann, San Fran-
cisco, CA, 2001.
Dong C. Liu and Jorge Nocedal. On the limited memory
BFGS method for large scale optimization. Mathematical
Programming, 45:503?528, 1989.
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph
Weischedel. A novel use of statistical parsing to extract in-
formation from text. In Proceedings of NAACL-2000, pages
226?233, Seattle, Washington, 2000.
Dat P. T. Nguyen, Yutaka Matsuo, and Mitsuru Ishizuka. Re-
lation extraction from Wikipedia using subtree mining. In
Proceedings of AAAI-07, pages 1414?1420, Vancouver,
British Columbia, Canada, 2007.
Dat P. T. Nguyen, Yutaka Matsuo, and Mitsuru Ishizuka. Sub-
tree mining for relation extraction from Wikipedia. In Pro-
ceedings of HLT-NAACL 2007, pages 125?128, Rochester,
New York, 2007.
Judea Pearl. Probabilistic reasoning in intelligent systems:
Networks of plausible inference. Morgan Kaufmann Pub-
lishers Inc., San Francisco, CA, 1988.
Matthew Richardson and Pedro Domingos. Markov logic net-
works. Machine Learning, 62(1-2):107?136, 2006.
Benjamin Rosenfeld and Ronen Feldman. Using corpus statis-
tics on entities to improve semi-supervised relation extrac-
tion from the Web. In Proceedings of ACL-07, pages 600?
607, Prague, Czech Republic, June 2007.
Hirano Toru, Matsuo Yoshihiro, and Kikui Genichiro. Detect-
ing semantic relations between named entities in text using
contextual features. In Proceedings of ACL-07, pages 157?
160, Prague, Czech Republic, June 2007.
Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella.
Kernel methods for relation extraction. Journal of Machine
Learning Research, 3:1083?1106, 2003.
Guodong Zhou, Jian Su, Jie Zhang, andMin Zhang. Exploring
various knowledge in relation extraction. In Proceedings of
ACL-05, pages 427?434, Ann Arbor, Michigan, 2005.
1072
A Framework Based on Graphical Models with Logic for
Chinese Named Entity Recognition ?
Xiaofeng YU Wai LAM Shing-Kit CHAN
Information Systems Laboratory
Department of Systems Engineering & Engineering Management
The Chinese University of Hong Kong
Shatin, N.T., Hong Kong
{xfyu,wlam,skchan}@se.cuhk.edu.hk
Abstract
Chinese named entity recognition (NER) has re-
cently been viewed as a classification or sequence
labeling problem, and many approaches have been
proposed. However, they tend to address this
problem without considering linguistic informa-
tion in Chinese NEs. We propose a new framework
based on probabilistic graphical models with first-
order logic for Chinese NER. First, we use Condi-
tional Random Fields (CRFs), a standard and the-
oretically well-founded machine learning method
based on undirected graphical models as a base
system. Second, we introduce various types of
domain knowledge into Markov Logic Networks
(MLNs), an effective combination of first-order
logic and probabilistic graphical models for vali-
dation and error correction of entities. Experimen-
tal results show that our framework of probabilis-
tic graphical models with first-order logic signifi-
cantly outperforms the state-of-the-art models for
solving this task.
1 Introduction
Named entity recognition (NER) is the task of identifying
and classifying phrases that denote certain types of named
entities (NEs), such as person names (PERs), locations
(LOCs) and organizations (ORGs) in text documents. It
is a well-established task in the NLP and data mining com-
munities and is regarded as crucial technology for many
higher-level applications, such as information extraction,
question answering, information retrieval and knowledge
management. The NER problem has generated much in-
terest and great progress has been made, as evidenced by
its inclusion as an understanding task to be evaluated in the
?The work described in this paper is substantially supported by
grants from the Research Grant Council of the Hong Kong Special
Administrative Region, China (Project Nos: CUHK 4179/03E
and CUHK4193/04E) and the Direct Grant of the Faculty of En-
gineering, CUHK (Project Codes: 2050363 and 2050391). This
work is also affiliated with the Microsoft-CUHK Joint Laboratory
for Human-centric Computing and Interface Technologies.
Message Understanding Conference (MUC), the Multilin-
gual Entity Task (MET) evaluations, and the Conference on
Computational Natural Language Learning (CoNLL).
Compared to European-language NER, Chinese NER
seems to be more difficult (Yu et al, 2006). Recent ap-
proaches to Chinese NER are a shift away from manu-
ally constructed rules or finite state patterns towards ma-
chine learning or statistical methods. However, rule-
based NER systems lack robustness and portability. Sta-
tistical methods often suffer from the problem of data
sparsity, and machine learning approaches (e.g., Hidden
Markov Models (HMMs) (Bikel et al, 1999; Zhou and
Su, 2002), Support Vector Machines (SVMs) (Isozaki and
Kazawa, 2002), Maximum Entropy (MaxEnt) (Borthwick,
1999; Chieu and Ng, 2003), Transformation-based Learn-
ing (TBL) (Brill, 1995) or variants of them) might be un-
satisfactory to learn linguistic information in Chinese NEs.
Current state-of-the-art models often view Chinese NER as
a classification or sequence labeling problem without con-
sidering the linguistic and structural information in Chinese
NEs. They assume that entities are independent, however
in most cases this assumption does not hold because of
the existing relationships among the entities. They seek
to locate and identify named entities in text by sequentially
classifying tokens (words or characters) as to whether or
not they participate in an NE, which is sometimes prone to
noise and errors.
In fact, Chinese NEs have distinct linguistic character-
istics in their composition and human beings usually use
prior knowledge to recognize NEs. For example, about 365
of the highest frequently used surnames cover 99% Chi-
nese surnames (Sun et al, 1995). Some LOCs contain
location salient words, while some ORGs contain organi-
zation salient words. For the LOC ??lA?/Hong Kong
Special Region?, ??l/Hong Kong? is the name part and
?A?/Special Region? is the salient word. For the ORG
??lA??/Hong Kong Special Region Government?,
??l/Hong Kong? is the LOC name part, ?A?/Special
Region? is the LOC salient word and ??/Government?
is the ORG salient word. Some ORGs contain one or
more PERs, LOCs and ORGs. A more complex exam-
335
ple is the nested ORG ????D??u??O?
??/School of Computer Science, Tsinghua Univer-
sity, Haidian District, Beijing City? which contains two
ORGs ??u??/Tsinghua University? and ?O???
/School of Computer Science? and two LOCs ??
?/Beijing City? and ??D?/Haidian District?. The two
ORGs contain ORG salient words ???/University? and
??/School?, while the two LOCs contain LOC salient
words ??/City? and ??/District? respectively.
Inspired by the above observation, we propose a new
framework based on probabilistic graphical models with
first-order logic which treats Chinese NER 1 as a statisti-
cal relational learning (SRL) problem and makes use of
domain knowledge. First, we employ Conditional Random
Fields (CRFs), a discriminatively trained undirected graph-
ical model which has theoretical justification and has been
shown to be an effective approach to segmenting and label-
ing sequence data, as our base system. We then exploit a
variety of domain knowledge into Markov Logic Networks
(MLNs), a powerful combination of logic and probability,
to validate and correct errors made in the base system. We
show how a variety of domain knowledge can be formu-
lated as first-order logic and incorporated into MLNs. We
use three Markov chain Monte Carlo (MCMC) algorithms,
including Gibbs sampling, Simulated Tempering, as well as
MC-SAT, andMaximum a posteriori/Most Probable Expla-
nation (MAP/MPE) algorithm for probabilistic inference
in MLNs. Experimental results show that our framework
based on graphical models with logic yields substantially
better NER results, leading to a relative error reduction of
up to 23.75% on the F-measure over state-of-the-art mod-
els. McNemar?s tests confirm that the improvements we
obtained are statistically highly significant.
2 State of the Art
2.1 CRF Model for Chinese NER
Conditional Random Fields (CRFs) (Lafferty et al, 2001)
are undirected graphical models trained to maximize the
conditional probability of the desired outputs given the cor-
responding inputs. CRFs have the great flexibility to en-
code a wide variety of arbitrary, non-independent features
and to straightforwardly combine rich domain knowledge.
Furthermore, they are discriminatively trained, and are of-
ten more accurate than generative models, even with the
same features. CRFs have been successfully applied to a
number of real-world tasks, including NP chunking (Sha
and Pereira, 2003), Chinese word segmentation (Peng et
al., 2004), information extraction (Pinto et al, 2003; Peng
and McCallum, 2004), named entity identification (Mc-
Callum and Li, 2003; Settles, 2004), and many others.
1In this paper we only focus on PERs, LOCs and ORGs. Since
temporal, numerical and monetary phrases can be well identified
with rule-based approaches.
Recently, CRFs have been shown to perform excep-
tionally well on Chinese NER shared task on the third
SIGHAN Chinese language processing bakeoff (SIGHAN-
06) (Zhou et al, 2006; Chen et al, 2006b,a). We follow
the state-of-the-art CRF models using features that have
been shown to be very effective in Chinese NER, namely
the current character and its part-of-speech (POS) tag, sev-
eral characters surrounding (both before and after) the cur-
rent character and their POS tags, current word and several
words surrounding the current word.
We also observe some important issues that significantly
influence the performance as follows:
Window size: The primitive window size we use is 5 ( 2
characters preceding the current character and 2 following
the current character). We extend the window size to 7 but
find that it slightly hurts. The reason is that CRFs can deal
with non-independent features. A larger window size may
introduce noisy and irrelevant features.
Feature representation: For character features, we use
character identities. For word features, BIES representa-
tion (each character is beginning of a word, inside of a
word, end of a word, or a single word) is employed.
Labeling scheme: The labeling scheme can be BIO, BIOE
or BIOES representation. In BIO representation, each char-
acter is tagged as either the beginning of a named entity
(B), a character inside a named entity (I), or a character
outside a named entity (O). In BIOE, the last character in
an entity is labeled as E while in BIOES, single-character
entities are labeled as S. In general, BIOES representation
is more informative and yields better results than both BIO
and BIOE.
2.2 Error Analysis
Even though the CRFmodel is able to accommodate a large
number of well-engineered features which can be easily ob-
tained across languages, some NEs, especially LOCs and
ORGs are difficult to identify due to the lack of linguistic
or structural characteristics. Since predictions are made to-
ken by token, some typical and serious tagging errors are
still made, as shown below:
? ORG is incorrectly tagged as LOC: In Chinese, many
ORGs contain location information. The CRF model only
tags the location information (in the ORGs) as LOCs.
For example, ?/?n??/Tangshan Technical Insti-
tute? and ??H???/Hainan Provincial Committee ? are
ORGs and they contain LOCs ?/?/Tangshan? and ??H
?/Hainan Province?, respectively. ?/?/Tangshan? and
??H?/Hainan Province? are only incorrectly tagged as
LOCs. This affects the tagging performance of both ORGs
and LOCs.
? LOC is incorrectly tagged as ORG: The LOCs ?GZy?
/Sydney Opera? and ??N?,/Beijing Gymnasium?
are mistakenly tagged as ORGs by the CRF model with-
out taking into account the location salient words ?y?
/Opera? and ?N?,/Gymnasium?.
336
? The boundary of entity is tagged incorrectly: This mis-
take occurs for all the entities. For example, the PER
?)0???d/Tom Cruise? may be tagged as a PER ?)
0/Tom?; the LOC ??5r/Bremen? may be tagged as
a LOC ?5r/Laimei?, which is a meaningless word; the
ORG ?u??i/Huawei Corporation? may be tagged as an
ORG ?u?/Huawei?. The reasons for these errors are both
complicated and varied. However, some of them are related
to linguistic knowledge.
? Common nouns are incorrectly tagged as entities: For ex-
ample, the two common nouns ?y???/Modern Mathe-
matics? and ??=????/Galanz Microwave Oven? may
be improperly tagged as a LOC and an ORG. Some tagging
errors could be easily rectified. Take the erroneous ORG
???|??/City Committee Organizes,? for example, in-
tuitively it is not an ORG since an entity cannot span any
punctuation.
3 Our Proposed Framework
3.1 Overview
We propose a framework based on probabilistic graphical
models with first-order logic for Chinese NER. As shown
in Figure 1, the framework is composed of three main com-
ponents. The CRF model is used as a base model. Then we
incorporate domain knowledge that can be well formulated
into first-order logic to extract entity candidates from CRF
results. Finally, the Markov Logic Network (MLN), an
undirected graphical model for statistical relational learn-
ing, is used to validate and correct the errors made in the
base model. We begin by briefly reviewing the necessary
background of MLNs, including weight learning and infer-
ence.
3.2 Markov Logic Networks
A Markov Network (also known as Markov Random Field)
is a model for the joint distribution of a set of variables
(Pearl, 1988). It is composed of an undirected graph G =
(V,E) and a set of real-valued potential functions ?k. A
First-Order Knowledge Base (KB) (Genesereth and Nisls-
son, 1987) is a set of sentences or formulas in first-order
logic.
A Markov Logic Network (MLN) (Richardson and
Domingos, 2006) is a KB with a weight attached to each
formula (or clause). Together with a set of constants
representing objects in the domain, it species a ground
Markov Network containing one feature for each possi-
ble grounding of a first-order formula Fi in the KB, with
the corresponding weight wi. The basic idea in MLNs
is that: when a world violates one formula in the KB
it is less probable, but not impossible. The fewer for-
mulas a world violates, the more probable it is. The
weights associated with the formulas in an MLN jointly
determine the probabilities of those formulas (and vice
versa) via a log-linear model. An MLN is a statisti-
cal relational model that defines a probability distribution
over Herbrand interpretations (possible worlds), and can
Figure 1: Framework Overview
be thought of as a template for constructing Markov Net-
works. Given different sets of constants, it will produce
different networks. These networks will have certain reg-
ularities in structure and parameter given by the MLN
and they are called ground Markov Networks. Suppose
Peter(A), Smith(B) and IBM(X) are 3 constants,
a KB and generated features are listed in Table 1. The
formula Employ(x,y)?Person(x),Company(y)
means x is employed by y and Colleague(x,y)?
Employ(x,z)?Employ(y,z) means x and y are col-
leagues if they are employed by the same company. Fig-
ure 2 shows the graph of the ground Markov network
defined by the formulas in Table 1 and the 3 constants
Peter(A), Smith(B) and IBM(X). The probability
distribution over possible worlds x specified by the ground
Markov Network ML,C is given by
P (X = x) =
1
Z
exp(
?
wini(x )) =
1
Z
?
?i
(
x{i}
)ni(x)
(1)
where ni (x) is the number of true groundings of Fi in
x, x{i} is the true value of the atoms appearing in Fi, and
?i
(
x{i}
)
= ewi .
In the case of Chinese NER, a named entity can be con-
nected to another named entity for instance, because they
share the same location salient word. Thus in an undirected
graph, two node types exist, the LOC nodes and the loca-
tion salient word nodes. The links (edges) indicate the rela-
tion (LOCs contain location salient words) between them.
This representation can be well expressed by MLNs.
However, one problem concerning relational data is, how
to extract useful relations for Chinese NER. There are many
kinds of relations between NEs, some relations are critical
to the NER problemwhile others not. Another problem that
we address is whether these relations can be formulated in
first-order logic and combined in MLNs. In Section 3.3,
we exploit domain knowledge. We will show how these
knowledge can capture essential characteristics of Chinese
NEs and can be well and concisely formulated in first-order
logic in Section 3.4.
337
Table 1: Example of a KB and Generated Features
Fist-Order Logic (KB) Generated Features
? x,y Employ(x,y)?Person(x),Company(y) Employ(Peter,IBM)?Person(Peter),Company(IBM)
Employ(Smith,IBM)?Person(Smith),Company(IBM)
? x,y,z Colleague(x,y)? Employ(x,z)?Employ(y,z) Colleague(Peter,Smith)? Employ(Peter,IBM)
?Employ(Smith,IBM)
3.2.1 Learning Weights
Given a relational database, MLN weights can in princi-
ple be learned generatively by maximizing the likelihood of
this database on the closed world assumption. The gradient
of the log-likelihood with respect to the weights is
?
?wi
logPw(X = x) = ni (x) ?
?
Pw(X = x
?)ni(x
?)
(2)
where the sum is over all possible databases x? , and
Pw(X = x?) is P (X = x?) computed using the cur-
rent weight vector w = (w1, ..., wi, ...). Unfortunately,
computing these expectations can be very expensive. In-
stead, we can maximize the pseudo-log-likelihood of the
data more efficiently. If x is a possible database and xl is
the lth ground atom?s truth value, the pseudo-log-likelihood
of x given weights w is
logP ?w(X = x) =
n?
l=1
logPw(Xl=xl | MBx(Xl )) (3)
where MBx (Xl) is the state of Xl?s Markov blanket 2
in the data. Computing Equation 3 and its gradient does
not require inference over the model, and is therefore much
faster. We can optimize the pseudo-log-likelihood using
the limited-memory BFGS algorithm (Liu and Nocedal,
1989).
3.2.2 Inference
If F1 and F2 are two formulas in first-order logic, C is
a finite set of constants including any constants that appear
in F1 or F2, and L is an MLN, then
P (F1 | F2, L, C) = P (F1 | F2,ML,C)
=
P (F1 ? F2 | ML,C)
P (F2 | ML,C)
=
?
x??F1??F2
P (X = x | ML,C)
?
x??F2
P (X = x | ML,C)
(4)
where ?Fi is the set of worlds where Fi holds, and P (x |
ML,C) is given by Equation 1. The question of whether a
knowledge base entails a formula F in first-order logic is
the question of whether P (F | LKB, CKB,F ) = 1, where
LKB is the MLN obtained by assigning infinite weight to
2 The Markov blanket of a node is the minimal set of nodes
that renders it independent of the remaining network; in a MLN,
this is simply the node?s neighbors in the graph.
Figure 2: A Ground Markov network defined by the formu-
las in Table 1 and the constants Peter(A), Smith(B)
and IBM(X).
all the formulas in KB, andCKB,F is the set of all constants
appearing in KB or F .
A large number of efficient inference techniques are ap-
plicable to MLNs. The most widely used approximate so-
lution to probabilistic inference in MLNs is Markov chain
Monte Carlo (MCMC) (Gilks et al, 1996). In this frame-
work, the Gibbs sampling algorithm is to generate an in-
stance from the distribution of each variable in turn, con-
ditional on the current values of the other variables. The
key to the Gibbs sampler is that one only considers uni-
variate conditional distributions-the distribution when all
of the random variables but one are assigned fixed values.
One way to speed up Gibbs sampling is by Simulated Tem-
pering (Marinari and Parisi, 1992), which performs simu-
lation in a generalized ensemble, and can rapidly achieve
an equilibrium state. Poon and Domingos (2006) pro-
posedMC-SAT, an inference algorithm that combines ideas
from MCMC and satisfiability. MC-SAT works well and is
guaranteed to be sound, even when deterministic or near-
deterministic dependencies are present in real-world rea-
soning.
Besides MCMC framework, maximum a posteriori
(MAP) inference can be carried out using a weighted sat-
isfiability solver like MaxWalkSAT. It is closely related to
maximum likelihood (ML), but employs an augmented op-
timization objective which incorporates a prior distribution
over the quantity one wants to estimate. MAP estimation
can therefore be seen as a regularization of ML estimation.
3.3 Domain Knowledge
We incorporate various kinds of domain knowledge via
MLNs to predict the newly extracted NE candidates from
338
CRF hypotheses. We extract 165 location salient words
and 843 organization salient words from Wikipedia3 and
the LDC Chinese-English bi-directional NE lists compiled
from Xinhua News database, as shown in Table 2. We also
make a punctuation list which contains 18 items and some
stopwords which Chinese NEs cannot contain. The stop-
words are mainly conjunctions, auxiliary and functional
words. We extract new NE candidates from the CRF re-
sults according to the following consideration:
? Definitely, if a chunk (a series of continuous characters) oc-
curs in the training data as a PER or a LOC or an ORG, then
this chunk should be a PER or a LOC or an ORG in the test-
ing data. In general, a unique string is defined as a PER, it
cannot be a LOC somewhere else.
? Obviously, if a tagged entity ends with a location salient
word, it is a LOC. If a tagged entity ends with an organi-
zation salient word, it is an ORG.
? If a tagged entity is close to a subsequent location salient
word, probably they should be combined together as a LOC.
The closer they are, the more likely that they should be com-
bined.
? If a series of consecutive tagged entities are close to a sub-
sequent organization salient word, they should probably be
combined together as an ORG because an ORG may contain
multiple PERs, LOCs and ORGs.
? Similarly, if there exists a series of consecutive tagged enti-
ties and the last one is tagged as an ORG, it is likely that all
of them should be combined as an ORG.
? Entity length restriction: all kinds of tagged entities cannot
exceed 25 Chinese characters.
? Stopword restriction: intuitively, all tagged entities cannot
comprise any stopword.
? Punctuation restriction: in general, all tagged entities cannot
span any punctuation.
? Since all NEs are proper nouns, the tagged entities should
end with noun words.
? The CRF model tags each token (Chinese character) with
a conditional probability. A low probability implies a
low-confidence prediction. For a chunk with low condi-
tional probabilities, all the above assumptions are adopted
(The marginal probabilities are normalized, and probabili-
ties lower than the user-defined threshold are regarded as
low conditional probabilities).
All the above domain knowledge can be formulated as
first-order logic to construct the structure of MLNs. And
all the extracted chunks are accepted as new NE candidates
(or common nouns). We train an MLN to recognize them.
3http://en.wikipedia.org/wiki/.
Table 2: Domain Knowledge for Chinese NER
Location Salient Word Organization Salient Word
g??/Municipality z??i/Department Store
???/Railway Station n??/Technical Institute
U,/Hotel ?1/Travel Agency
?	/Park ??/Press
p/Plateau <??/Personnel Department
?/Province ?1/Bank
	/Town ??/University
?/City ??/City Committee
Stopword Punctuation
E,/still "
?/but ?
?~/very ?
ff/of ;
/and so on ?
@/that ?
3.4 First-Order Logic Representation
We declared 14 predicates (person(candidate), lo
cation(candidate), organization(candidat
e), endwith(candidate, salientword), clos
eto(candidate, salientword), containstop
word(candidate), containpunctuation(cand
idate), etc) and specified 15 first-order formulas (See
Table 3 for some examples) according to the domain
knowledge described in Section 3.3. For example, we
used person(candidate) to specify whether a candi-
date is a PER. Formulas are recursively constructed from
atomic formulas using logical connectives and quantifiers.
They are constructed using four types of symbols: con-
stants, variables, functions, and predicates. Constant sym-
bols represent objects in the domain of interest (e.g., ?
?/Beijing? and ???/Shanghai? are LOCs). Variable
symbols (e.g., r and p) range over the objects in the do-
main. To reduce the size of ground Markov Network,
variables and constants are typed; for example, the vari-
able r may range over candidates, and the constant ?
?/Beijing? may represent a LOC. Function symbols repre-
sent mappings from tuples of objects to objects. Predicate
symbols represent relations among objects (e.g., person)
in the domain or attributes of objects (e.g., endwith). A
ground atom is an atomic formula all of whose arguments
are ground terms (terms containing no variables). For ex-
ample, the ground atom location(??) conveys
that ???/Beijing City? is a LOC.
For example in Table 3, ???/Wu City? is mis-tagged
as an ORG by the CRF model, but it contains the location
salient word ??/City?. So it is extracted as a new entity
candidate, and the corresponding formula endwith(r,
p)?locsalientword(p)?location(r) means if
r ends with a location salient word p, then it is a LOC.
Besides the formulas listed in Table 3, we also speci-
fied logic such as person(p)?!(location(p) v
organization(p)), which means a candidate p can
339
Table 3: Examples of NE Candidates and First-Order Formulas
Mis-tagged NEs New NE Candidates First-Order Logic
F.p[common noun] F.p occurperson(p)?person(p)
?m[PER] ?m occurlocation(p)?location(p)
??8?[common noun] ??8? occurorganization(p)?organization(p)
??[ORG] ?? endwith(r,p)?locsalientword(p)?location(r)
=?[LOC] =? endwith(r,p)?orgsalientword(p)?organization(r)
?[LOC]s	 ?s	 closeto(r,p)?locsalientword(p)?location(r)
a?[LOC]?? a??? closeto(r,p)?orgsalientword(p)?organization(r)
??ff?A[LOC] ??ff?A containstopword(p)?!(person(p) v location(p) v
organization(p))
?z?????%[ORG] ?z?????% containpunctuation(p)?!(person(p) v location(p)
v organization(p))
only belong to one class.
We assume that the relational database contains only bi-
nary relations. Each extracted NE candidate is represented
by one or more strings appearing as arguments of ground
atoms in the database. The goal of NE prediction is to de-
termine whether the candidates are entities and the types of
entities (query predicates), given the evidence predicates
and other relations that can be deterministically derived
from the database. As we will see, despite their simplic-
ity and consistency, these first-order formulas incorporate
the essential features for NE prediction.
4 Experiments
4.1 Dataset
We used People?s Daily corpus (January-Jun, 1998) in
our experiments, which contains approximately 357K sen-
tences, 156K PERs, 219K LOCs and 87K ORGs, respec-
tively. We did some modifications on the original data to
make it cleaner. We enriched some tags so that the abbre-
viation proper nouns are well labeled. We preprocessed
some nested names to make them in better form. We also
processed some person names. We enriched tags for differ-
ent kinds of person names (e.g., Chinese and transliterated
names) and separated consecutive person names.
4.2 The Baseline NER System
We use CRFs to build a character-based Chinese NER sys-
tem, with features described in Section 2.1. To avoid over-
fitting, we penalized the log-likelihood by the commonly
used zero-mean Gaussian prior over the parameters. In
addition, we exploit clue word features which can capture
non-local dependencies. This gives us a competitive base-
line CRF model using both local and non-local information
for Chinese NER.
For clue word features, we employ 412 career titles (e.g.,
o?/President,?/Professor,?	/Police), 59 family ti-
tles (e.g.,ww/Father,~~/Sister), 33 personal pronouns
(e.g., \?/Your, ??/We) and 109 direction words (e.g.,
?/North, H?/South) to represent non-local informa-
tion. Career titles, family titles and personal pronouns may
Figure 3: An Example of Non-local Dependency. The Ca-
reer Title ??? Indicates a PER ??^?
imply a nearby PER and direction words may indicate a
LOC or an ORG. Figure 3 illustrates an example of non-
local dependency.
We do not take the advantage of using the golden-
standard word segmentation and POS tagging provided in
the original corpus, since such information is hardly avail-
able in real text. Instead, we use an off-the-shelf Chi-
nese lexical analysis system, the open source ICTCLAS
(Zhang et al, 2003), to segment and POS tag the corpus.
This module employs a hierarchical Hidden Markov Model
(HHMM) and provides word segmentation, POS tagging
(labels Chinese words using a set of 39 tags) and unknown
word recognition. It performs reasonably well, with seg-
mentation precision recently evaluated at 97.58%. The re-
call of unknown words using role tagging is over 90%.
We use one-month corpus for training and 9-day corpus
for testing. Table 4 shows the experimental results.
4.3 NER System Based on Graphical Models with
Logic
To test the effectiveness of our proposed model, we extract
all the NEs (19,879 PERs, 25,661 LOCs and 11,590 ORGs)
from the training corpus. An MLN training database,
which consists of 14 predicates, 16,620 constants and
97,992 ground atoms was built.
The MLNs were trained using a Gaussian prior with
zero mean and unit variance on each weight to penalize
the pseudo-likelihood, and with the weights initialized at
the mode of the prior (zero). During MLN learning, each
formula is converted to Conjunctive Normal Form (CNF),
and a weight is learned for each of its clauses. The weight
340
Table 4: Chinese NER by CRF Model
Precision Recall F?=1
Character features
PER 92.88% 79.42% 85.62
LOC 90.95% 82.88% 86.73
ORG 88.16% 83.86% 85.96
Overall 90.92% 82.07% 86.27
Character+Word
PER 93.27% 82.99% 87.83
LOC 91.49% 85.16% 88.21
ORG 88.94% 84.79% 86.82
Overall 91.48% 84.46% 87.83
Character+Word+POS
PER 92.17% 90.64% 91.40
LOC 90.56% 89.74% 90.15
ORG 89.15% 85.19% 87.12
Overall 90.76% 89.13% 89.94
All features
PER 92.12% 90.57% 91.34
LOC 90.62% 89.74% 90.18
ORG 89.72% 85.44% 87.53
Overall 90.89% 89.16% 90.02
Table 5: Chinese NER by Graphical Models with Logic
Precision Recall F?=1 RER
CRF Baseline
PER 92.12% 90.57% 91.34
LOC 90.62% 89.74% 90.18
ORG 89.72% 85.44% 87.53
Overall 90.89% 89.16% 90.02
Graphical Models (GS Inference)
PER 93.52% 93.32% 93.42
LOC 93.19% 91.91% 92.55
ORG 90.16% 90.71% 90.43
Overall 92.70% 92.09% 92.39 23.75%
Graphical Models (ST Inference)
PER 93.52% 93.32% 93.42
LOC 93.19% 91.91% 92.55
ORG 90.16% 90.71% 90.43
Overall 92.70% 92.09% 92.39 23.75%
Graphical Models (MC-SAT Inference)
PER 93.52% 93.32% 93.42
LOC 93.19% 91.91% 92.55
ORG 90.16% 90.71% 90.43
Overall 92.70% 92.09% 92.39 23.75%
Graphical Models (MAP/MPE Inference)
PER 92.87% 93.15% 93.01
LOC 93.15% 91.61% 92.37
ORG 90.56% 89.10% 89.82
Overall 92.57% 91.58% 92.07 20.54%
of a clause is used as the mean of a Gaussian prior for the
learned weight. These weights reflect how often the clauses
are actually observed in the training data.
We extract 529 entity candidates to construct the MLN
testing database, which contains 2,543 entries and these en-
tries are used as evidence for inference. Inference is per-
formed by grounding the minimal subset of the network re-
quired for answering the query predicates. We employed 3
MCMC algorithms: Gibbs sampling (GS), Simulated Tem-
pering (ST) as well as MC-SAT, and the MAP/MPE algo-
rithm for inference and the comparative NER results are
shown. The probabilistic graphical models greatly outper-
form the CRF model stand-alone by a large margin. It can
be seen from Table 5, the probabilistic graphical models
integrating first-order logic improve the precision and re-
call for all kinds of entities, thus boosting the overall F-
measure. We achieve a 23.75% relative error reduction
(RER) on F-measure by using 3 MCMC algorithms and
a 20.54% RER by using MAP/MPE algorithm, over an al-
ready competitive CRF baseline. We obtained the same
results using GS, ST and MC-SAT algorithms. MCMC al-
gorithms yields slightly better results than the MAP/MPE
algorithm.
4.4 Significance Test
Ideally, comparisons among NER systems would control
for feature sets, data preparation, training and test proce-
dures, parameter tuning, and estimate the statistical sig-
nificance of performance differences. Unfortunately, re-
ported results sometimes leave out details needed for ac-
curate comparisons.
We give statistical significance estimates using McNe-
mar?s paired tests 4 (Gillick and Cox, 1989) on labeling
disagreements for CRF model and graphical probabilistic
models that we evaluated directly.
Table 6 summarizes the correctness of the labeling de-
cisions between the models with a 95% confidence inter-
val (CI). These tests suggest that the graphical probabilistic
models are significantly more accurate and confirm that the
gains we obtained are statistically highly significant.
Table 6: McNemar?s Tests on Labeling Disagreements
Null Hypothesis 95% CI p-value
Proposed Model (GS) vs. CRFs 5.71-9.52 < 1 ? 10?6
Proposed Model (ST) vs. CRFs 5.71-9.52 < 1 ? 10?6
Proposed Model (MC-SAT) vs. CRFs 5.71-9.52 < 1 ? 10?6
Proposed Model (MAP/MPE) vs. CRFs 4.50-7.37 < 1 ? 10?6
5 Related Work
As a well-established task, Chinese NER has been studied
extensively and a number of techniques for this task have
been reported in the literature. Most recently, the trend
in Chinese NER is to use improved machine learning ap-
proaches, or to integrate various kinds of useful evidences,
features, or resources.
Fu and Luke (2005) presented a lexicalized HMM-
based approach to unifying unknown word identification
4Most researchers refer to statistically significant as p < 0.05
and statistically highly significant as p < 0.001.
341
and NER as a single tagging task on a sequence of known
words. Although lexicalized HMMs was shown to be su-
perior to standard HMMs, this approach has some disad-
vantages: it is a purely statistical model and it suffers from
the problem of data sparseness. And the model fails to tag
some complicated NEs (e.g., nested ORGs) correctly due
to lack of domain adaptive techniques. The F-measures of
LOCs and ORGs are only 87.13 and 83.60, which show
that there is still a room for improving.
A method of incorporating heuristic human knowledge
into a statistical model was proposed in (Wu et al, 2005).
Here Chinese NER was regarded as a probabilistic tagging
problem and the heuristic human knowledge was used to
reduce the searching space. However, this method assumes
that POS tags are golden-standard in the training data and
heuristic human knowledge is often ad hoc. These draw-
backs make the method unstable and highly sensitive to
POS errors; and when golden-standard POS tags are not
available (this is often the case), it may degrade the perfor-
mance.
Cohen and Sarawagi (2004) proposed a semi-Markov
model which combines a Markovian, HMM-like extrac-
tion process and a dictionary component. This process is
based on sequentially classifying segments of several ad-
jacent words. However, this technique requires that entire
segments have the same class label, while our technique
does not. Moreover, compared to a large-scale dictionary,
our domain knowledge is much easier to obtain.
However, all the above models treat NER as classifi-
cation or sequence labeling problem. To the best of our
knowledge, MLNs have not been previously used for NER
problem. To our knowledge, we first view Chinese NER
as a statistical relational learning problem and exploit do-
main knowledge which can be concisely formulated in
MLNs, allowing the training and inference algorithms to
be directly applied to them.
6 Conclusion and Future Work
The contribution of this paper is three-fold. First, we for-
mulate Chinese NER as a statistical relational learning
problem and propose a new framework incorporating prob-
abilistic graphical models and first-order logic for Chinese
NER which achieves state-of-the-art performance. Second,
We incorporate domain knowledge to capture the essen-
tial features of the NER task via MLNs, a unified frame-
work for SRL which produces a set of weighted first-
order clauses to predict new NE candidates. To the best
of our knowledge, this is the first attempt at using MLNs
for the NER problem in the NLP community. Third,
our proposed framework can be extendable to language-
independent NER, due to the simplicity of the domain
knowledge we could access. Directions for future work
include learning the structure of MLNs automatically and
using MLNs for information extraction (e.g., entity relation
extraction).
References
Daniel M. Bikel, Richard Schwartz, and Ralph M. Weischedel. An algorithm that learns what?s in
a name. Machine Learning, 34(1-3):211?231, February 1999.
Andrew Borthwick. A Maximum Entropy Approach to Named Entity Recognition. PhD thesis,
New York University, September 1999.
Eric Brill. Transformation-based error-driven learning and natural language processing: A case
study in part-of-speech tagging. Computational Linguistics, 21(4):543?565, 1995.
Aitao Chen, Fuchun Peng, Roy Shan, and Gordon Sun. Chinese named entity recognition with
conditional probabilistic models. In 5th SIGHAN Workshop on Chinese Language Processing,
Australia, July 2006.
Wenliang Chen, Yujie Zhang, and Hitoshi Isahara. Chinese named entity recognition with condi-
tional random fields. In 5th SIGHAN Workshop on Chinese Language Processing, Australia,
July 2006.
Hai Leong Chieu and Hwee Tou Ng. Named entity recognition with a maximum entropy approach.
In Proceedings of CoNLL-03, 2003.
William W. Cohen and Sunita Sarawagi. Exploiting dictionaries in named entity extraction: Com-
bining semi-Markov extraction processes and data integration methods. In Proceedings of
ACM-SIGKDD 2004, 2004.
Guohong Fu and Kang-Kwong Luke. Chinese named entity recognition using lexicalized HMMs.
ACM SIGKDD Explorations Newsletter, 7:19?25, June 2005.
Michael R. Genesereth and Nils J. Nislsson. Logical foundations of artificial intelligence. Morgan
Kaufmann Publishers Inc., San Mateo, CA, 1987.
W.R. Gilks, S. Richardson, and D.J. Spiegelhalter. Markov chain Monte Carlo in practice. Chap-
man and Hall, London, UK, 1996.
L. Gillick and Stephen Cox. Some statistical issues in the comparison of speech recognition algo-
rithms. In Proceedings of ICASSP-89, pages 532?535, 1989.
Hideki Isozaki and Hideto Kazawa. Efficient support vector classifiers for named entity recogni-
tion. In Proceedings of COLING-02, pages 1?7, Taipei, Taiwan, 2002.
John Lafferty, Andrew McCallum, and Fernando Pereira. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In Proceedings of ICML-01, pages 282?
289. Morgan Kaufmann, San Francisco, CA, 2001.
Dong C. Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimiza-
tion. Mathematical Programming, 45:503?528, 1989.
EnzoMarinari and Giorgio Parisi. Simulated Tempering: A newMonte Carlo scheme. Europhysics
Letters, 19:451?458, 1992.
AndrewMcCallum andWei Li. Early results for named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons. In Proceedings of CoNLL-03, 2003.
Judea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Mor-
gan Kaufmann Publishers Inc., San Francisco, CA, 1988.
Fuchun Peng and Andrew McCallum. Accurate information extraction from research papers using
conditional random fields. In Proceedings of HLT-NAACL 2004, pages 329?336, 2004.
Fuchun Peng, Fangfang Feng, and Andrew McCallum. Chinese segmentation and new word de-
tection using conditional random fields. In Proceedings of COLING-04, pages 562?568, 2004.
David Pinto, AndrewMcCallum, XingWei, andW. Bruce Croft. Table extraction using conditional
random fields. In Proceedings of ACM SIGIR-03, 2003.
Hoifung Poon and Pedro Domingos. Sound and efficient inference with probabilistic and deter-
ministic dependencies. In Proceedings of AAAI-06, Boston, Massachusetts, July 2006. The
AAAI Press.
Matthew Richardson and Pedro Domingos. Markov logic networks. Machine Learning, 62(1-
2):107?136, 2006.
Burr Settles. Biomedical named entity recognition using conditional random fields and rich feature
sets. In Proceedings of the COLING 2004 International Joint Workshop on Natural Language
Processing in Biomedicine and its Applications, Geneva, Switzerland, 2004.
Fei Sha and Fernando Pereira. Shallow parsing with conditional random fields. In Proceedings of
HLT-NAACL 2003, pages 213?220, 2003.
Maosong Sun, Changning Huang, Haiyan Gao, and Jie Fang. Identifying Chinese names in unre-
stricted texts. Journal of Chinese Information Processing, 1995.
Youzheng Wu, Jun Zhao, Bo Xu, and Hao Yu. Chinese named entity recognition based on multiple
features. In Proceedings of HLT-EMNLP 2005, 2005.
Xiaofeng Yu, Marine Carpuat, and Dekai Wu. Boosting for Chinese named entity recognition. In
5th SIGHAN Workshop on Chinese Language Processing, Australia, July 2006.
Hua Ping Zhang, Qun Liu, Xue-Qi Cheng, Hao Zhang, and Hong Kui Yu. Chinese lexical analysis
using Hierarchical Hidden Markov Model. In 2nd SIGHAN Workshop on Chinese Language
Processing, volume 17, pages 63?70, 2003.
Guodong Zhou and Jian Su. Named entity recognition using an HMM-based chunk tagger. In
Proceedings of ACL-02, pages 473?480, Philadelphia, USA, 2002.
Junsheng Zhou, Liang He, Xinyu Dai, and Jiajun Chen. Chinese named entity recognition with
a multi-phase model. In 5th SIGHAN Workshop on Chinese Language Processing, Australia,
July 2006.
342
An Online Cascaded Approach to Biomedical Named Entity Recognition ?
Shing-Kit Chan, Wai Lam, Xiaofeng Yu
Department of Systems Engineering and Engineering Management
The Chinese University of Hong Kong
Shatin, Hong Kong
{skchan, wlam, xfyu}@se.cuhk.edu.hk
Abstract
We present an online cascaded approach to
biomedical named entity recognition. This
approach uses an online training method
to substantially reduce the training time re-
quired and a cascaded framework to relax
the memory requirement. We conduct de-
tailed experiments on the BioNLP dataset
from the JNLPBA shared task and com-
pare the results with other systems and pub-
lished works. Our experimental results show
that our approach achieves comparable per-
formance with great reductions in time and
space requirements.
1 Introduction
In the biomedical domain, the vast amount of data
and the great variety of induced features are two ma-
jor bottlenecks for further natural language process-
ing on the biomedical literature. In this paper, we
investigate the biomedical named entity recognition
(NER) problem. This problem is particularly impor-
tant because it is a necessary pre-processing step in
many applications.
This paper addresses two main issues that arise
from biomedical NER.
?The work described in this paper is substantially supported
by grants from the Research Grant Council of the Hong Kong
Special Administrative Region, China (Project Nos: CUHK
4179/03E and CUHK4193/04E) and the Direct Grant of the
Faculty of Engineering, CUHK (Project Codes: 2050363 and
2050391). This work is also affiliated with the Microsoft-
CUHK Joint Laboratory for Human-centric Computing and In-
terface Technologies.
Long Training Time: Traditional approaches
that depend on the maximum likelihood training
method are slow even with large-scale optimiza-
tion methods such as L-BFGS. This problem wors-
ens with the sheer volume and growth rate of the
biomedical literature. In this paper, we propose the
use of an online training method that greatly reduces
training time.
Large Memory Space: The total number of
features used to extract named entities from docu-
ments is very large. To extract biomedical named
entities, we often need to use extra features in addi-
tion to those used in general-purpose domains, such
as prefix, suffix, punctuation, and more orthographic
features. We need a correspondingly large mem-
ory space for processing, exacerbating the first issue.
We propose to alleviate this problem by employing
a cascaded approach that divides the NER task into
a segmentation task and a classification task.
The overall approach is the online cascaded ap-
proach, which is described in the remaining sections
of this paper: Section 2 describes the general model
that is used to address the above issues. We address
the issue of long training time in Section 3. The is-
sue of large memory space is addressed in Section 4.
Experimental results and analysis are presented in
Section 5. We discuss related work in Section 6 and
conclude with Section 7.
2 Model Descriptions
Our proposed model is similar to a conditional ran-
dom field in a sequence labeling task, but we avoid
directly dealing with the probability distribution. We
use a joint feature representation F(x,y) for each
595
input sequence x and an arbitrary output sequence
y, as follows.
F(x,y) =
|x|
?
i=1
f(x,y, i) (1)
where each f(x,y, i) is a local feature function at
position i. For example, in a segmentation task using
the IOB2 notation, the k-th local feature in f(x,y, i)
can be defined as
fk(x,y, i) =
?
?
?
1 if xi is the word ?boy?,
and yi is the label ?B?
0 otherwise
(2)
With parameter w, the best output sequence y? for
an input sequence x can be found by calculating the
best score:
y? = argmax
y?
w ? F(x,y?) (3)
3 Online Training
We propose to estimate the parameter w in an online
manner. In particular, we use the online passive-
aggressive algorithm (Crammer et al, 2006). Pa-
rameters are estimated by margin-based training,
which chooses the set of parameters that attempts
to make the ?margin? on each training instance
(xt,yt) greater than a predefined value ?,
w ? F(xt,yt) ? w ? F(xt,y?) ? ? ?y? 6= yt
(4)
A hinge loss function ?(w;xt) is defined as
?(w;xt) =
{
0 if ?t ? ?
? ? ?t otherwise
(5)
where ?t is the margin on input xt defined as
?t = w ? F(xt,yt) ? max
y? 6=yt
w ? F(xt,y?) (6)
In online training, the parameter w is updated itera-
tively. Formally speaking, in the t-th iteration with
the parameter wt and the training instance xt, we
try to solve the following optimization problem.
wt+1 = argmin
w
1
2?w ? wt?
2 + C?
(7)
such that ?(w; (xt,yt)) ? ?
where C > 0 is a user-defined aggressiveness pa-
rameter and ? ? 0 is a slack term for the training
data when it is not linearly-separable. C controls
the penalty of the slack term and the aggressiveness
of each update step. A larger C implies a more ag-
gressive update and hence a higher tendency to over-
fit. The solution to Problem (7) is
wt+1 = wt ? ?t[F(xt,yt) ? F(xt, y?t)]
(8)
where ?t = min
{
C, ?(wt; (xt,yt))?F(xt,yt) ? F(xt, y?t)?2
}
(9)
The passiveness of this algorithm comes from the
fact that the parameter wt is not updated when the
hinge loss for xt is zero. It can be proved that the rel-
ative loss bound on the training data (and which also
bounds the number of prediction mistakes on the
training data) cannot be much worse than the best
fixed parameter chosen in hindsight. See (Crammer
et al, 2006) for a detailed proof.
Following most of the work on margin-based
training, in this paper we choose ? to be a function
of the correct output sequence y and the predicted
output sequence y?.
?(y, y?) =
{
0 if y = y?
?|y|
i=1[[yi 6= y?i]] otherwise
(10)
where [[z]] is 1 if z is true, and 0 otherwise.
The major computation difficulty in this online
training comes from Equation (3). Finding the best
output y? is in general an intractable task. We fol-
low the usual first-order independence assumption
made in a linear-chained CRF (Lafferty et al, 2001)
model and calculate the best score using the Viterbi
algorithm.
4 Cascaded Framework
We divide the NER task into a segmentation task
and a classification task. In the segmentation task,
a sentence x is segmented, and possible segments
of biomedical named entities are identified. In the
classification task, the identified segments are clas-
sified into one of the possible named entity types or
rejected.
596
In other words, in the segmentation task, the sen-
tence x are segmented by
y?s = argmax
y?
ws ? Fs(x,y?) (11)
where Fs(?) is the set of segment features, and ws is
the parameter for segmentation.
In the classification task, the segments (which can
be identified by ys) in a sentence x are classified by
y?c = argmax
y?
wc ? Fc(x,ys,y?) (12)
where Fc(?) is the set of classification features, and
wc is the parameter for classification.
In this cascaded framework, the number of possi-
ble labels in the segmentation task is Ns. For exam-
ple, Ns = 3 in the IOB2 notation. In the classifi-
cation task, the number of possible labels is Nc + 1,
which is the number of entity types and one label for
?Other?. Following the first-order independence as-
sumption, the maximum total number of features in
the two tasks is O(max(N2s ,N2c )), which is much
smaller than the single-phase approach in which the
total number of features is O((NsNc)2).
Another potential advantage of dividing the NER
task into two tasks is that it allows greater flexibility
in choosing an appropriate set of features for each
task. In fact, adding more features may not nec-
essarily increase performance. (Settles, 2004) re-
ported that a system using a subset of features out-
performed one using a full set of features.
5 Experiments
We conducted our experiments on the GENIA cor-
pus (Kim et al, 2003) provided in the JNLPBA (Kim
et al, 2004) shared task1. There are 2,000 MED-
LINE abstracts in the GENIA corpus with named
entities tagged in the IOB2 format. There are 18,546
sentences and 492,551 words in the training set, and
3,856 sentences and 101,039 words in the evalua-
tion set. The line indicating the MEDLINE abstract
ID boundary information is not used in our experi-
ments. Each word is tagged with ?B-X?, ?I-X?, or
?O? to indicate that the word is at the ?beginning?
(B) or ?inside? (I) of a named entity of type X, or
1http://research.nii.ac.jp/?collier/
workshops/JNLPBA04st.htm
System F1
(Zhou and Su, 2004) 72.55
Online Cascaded 72.16
(Okanohara et al, 2006) 71.48
(Kim et al, 2005) 71.19
(Finkel et al, 2004) 70.06
(Settles, 2004) 69.80
Table 1: Comparisons with other systems on overall
performance (in percentage).
?outside? (O) of a named entity. The named entity
types are: DNA, RNA, cell line, cell type, and pro-
tein.
5.1 Features
The features used in our experiments mainly fol-
low the work of (Settles, 2004) and (Collins, 2001).
For completeness, we briefly describe the features
here. They include word features, orthographic fea-
tures, parts-of-speech (POS), and two lexicons. The
word features include unigram, bigram, and trigram
(e.g. the previous word, the next word, and the
previous two words), whereas the orthographic fea-
tures include capital letter, dash, punctuation, and
word length. Word class (WC) features are also
added, which replace a capital letter with ?A?, a
lower case letter with ?a?, a digit with ?0?, and all
other characters with ? ?. Similar brief word class
(BWC) features are added by collapsing all of the
consecutive identical characters in the word class
features into one character. For example, for the
word NF-kappa, WC = AA aaaaa, and BWC
= A a. These are listed in Tables 2 and 3. The POS
features are added by the GENIA tagger2.
All of these features except for the prefix/suffix
features are applied to the neighborhood window
[i ? 1, i + 1] for every word. Two lexicons for cell
lines and genes are drawn from two online public
databases: the Cell Line Database3 and the BBID4.
The prefix/suffix and lexicon features are applied to
position i only. All of the above features are com-
2http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/tagger/
3http://www.biotech.ist.unige.it/cldb/
cname-tz.html
4http://bbid.grc.nia.nih.gov/bbidgene.
html
597
Unigram (w?2), (w?1), (w0),
(w1), (w2)
Bigram (w?2 w?1), (w?1 w0),
(w0 w1), (w1 w2)
Trigram (w?2 w?1 w0),
(w?1 w0 w1),
(w0 w1 w2)
Table 2: Word features used in the experiment: w0
is the current word, w?1 is the previous word, etc.
Word features as in Table 2
Prefix/suffix Up to a length of 5
Word Class WC
Brief Word Class BWC
Capital Letter ?[A-Z][a-z]
[A-Z]{2,}
[a-z]+[A-Z]+
Digit [0-9]+
?[?0-9]*[0-9][?0-9]*$
?[?0-9]*[0-9][0-9][?0-9]*$
?[0-9]+$
[0-9]+[,.][0-9,.]+
[A-Za-z]+[0-9]+
[0-9]+[A-Za-z]+
Dash [-]+
?[-]+
[-]+$
Punctuation [,;:?!-+?"\/]+
Word length length of the current word xi
Table 3: Features used in the JNLPBA experiment.
The features for Capital Letter, Digit, Dash, and
Punctuation are represented as regular expressions.
bined with the previous label yi?1 and the current
label yi to form the final set of features.
In the segmentation task, only three labels (i.e. B,
I, O) are needed to represent the segmentation re-
sults. In the classification task, the possible labels
are the five entity types and ?Other?. We also add
the segmentation results as features in the classifica-
tion task.
5.2 Results
We tried different methods to extract the named en-
tities from the JNLPBA dataset for comparisons.
These programs were developed based on the same
basic framework. All of the experiments were run
on a Unix machine with a 2.8 GHz CPU and 16 GB
RAM. In particular, the CRF trained by maximum-
likelihood uses the L-BFGS algorithm (Liu and No-
cedal, 1989), which converges quickly and gives
a good performance on maximum entropy mod-
els (Malouf, 2002; Sha and Pereira, 2003). We com-
pare our experimental results in several dimensions.
Training Time: Referring to Table 4, the train-
ing time of the online cascaded approach is substan-
tially shorter than that of all of the other approaches.
In the single-phase approach, training a CRF by
maximum likelihood (ML) using the L-BFGS algo-
rithm is the slowest and requires around 28 hours.
The online method greatly reduces the training time
to around two hours, which is 14 times faster. By
employing a two-phase approach, the training time
is further reduced to half an hour.
Memory Requirement: Table 4 shows the num-
ber of features that are required by the different
methods. For methods that use the single-phase ap-
proach, because the full set of features (See Sec-
tion 4) is too big for practical experiments on our
machine, we need to set a higher cutoff value to re-
duce the number of features. With a cutoff of 20
(i.e. only features that occur more than 20 times are
used), the number of features can still go up to about
8 million. However, in the two-phase approach, even
with a smaller cutoff of 5, the number of features can
still remain at about 8 million.
F1-measure: Table 4 shows the F1-measure in
our experiments, and Table 1 compares our results
with different systems in the JNLPBA shared tasks
and other published works5. Our performance of the
single-phase CRF with maximum likelihood train-
ing is 69.44%, which agrees with (Settles, 2004)
who also uses similar settings. The single-phase on-
line method increases the performance to 71.17%.
By employing a cascaded framework, the perfor-
mance is further increased to 72.16%, which can be
regarded as comparable with the best system in the
JNLPBA shared task.
6 Related Work
The online training approach used in this paper
is based on the concept of ?margin? (Cristianini,
2001). A pioneer work in online training is the
perceptron-like algorithm used in training a hidden
Markov model (HMM) (Collins, 2002). (McDonald
5We are aware of the high F1 in (Vishwanathan et al, 2006).
We contacted the author and found that their published result
may be incomplete.
598
Experiments no. of features training time F1 rel. err.
red. on F1
single-phase CRF + ML 8,004,392 1699 mins 69.44 ?
CRF + Online 8,004,392 116 mins 71.17 5.66%
two-phase Online seg: 2,356,590 14 + 15 72.16 8.90%
+ Cascaded class: 8,278,794 = 29 mins
Table 4: The number of features, training time, and F1 that are used in our experiments. The cutoff thresh-
olds for the single-phase CRFs are set to 20, whereas that of the online cascaded approach is set to 5 in both
segmentation and classification. The last column shows the relative error reductions on F1 (compared to
CRF+ML).
Experiments R P F1
Segmentation 80.13 73.68 76.77
Classification 92.75 92.76 92.76
Table 5: Results of the individual task in the online
cascaded approach. The F1 of the classification task
is 92.76% (which is based on the fully correct seg-
mented testing data).
et al, 2005) also proposed an online margin-based
training method for parsing. This type of training
method is fast and has the advantage that it does
not need to form the dual problem as in SVMs. A
detailed description of the online passive-aggressive
algorithm used in this paper and its variants can
be found in (Crammer et al, 2006). The Margin
Infused Relaxed Algorithm (MIRA), which is the
ancestor of the online passive-aggressive algorithm
and mainly for the linearly-separable case, can be
found in (Crammer and Singer, 2003).
(Kim et al, 2005) uses a similar two-phase
approach but they need to use rule-based post-
processing to correct the final results. Their CRFs
are trained on a different dataset that contains all of
the other named entities such as lipid, multi cell, and
other organic compound. Table 1 shows the com-
parisons of the final results.
In the JNLPBA shared task, eight NER systems
were used to extract five types of biomedical named
entities. The best system (Zhou and Su, 2004) uses
?deep knowledge?, such as name alias resolution,
cascaded entity name resolution, abbreviation res-
olution, and in-domain POS. Our approach is rela-
tively simpler and uses a unified model to accom-
plish the cascaded tasks. It also allows other post-
processing tasks to enhance performance.
7 Conclusion
We have presented an online cascaded approach to
biomedical named entity recognition. This approach
substantially reduces the training time required and
relaxes the memory requirement. The experimen-
tal results show that our approach achieves perfor-
mance comparable to the state-of-the-art system.
References
Michael Collins. 2001. Ranking algorithms for named-
entity extraction: boosting and the voted perceptron.
In ACL ?02: Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
489?496.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: theory and experiments
with perceptron algorithms. In EMNLP ?02: Proceed-
ings of the ACL-02 conference on Empirical methods
in natural language processing, pages 1?8.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951?991.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Nello Cristianini. 2001. Support vector and ker-
nel machines. ICML tutorial. Available at
http://www.support-vector.net/icml-tutorial.pdf.
J. Finkel, S. Dingare, H. Nguyen, M. Nissim, C. Man-
ning, and G. Sinclair. 2004. Exploiting context for
biomedical entity recognition: from syntax to the web.
In Proceedings of the International Joint Workshop on
599
Natural Language Processing in Biomedicine and its
Applications (NLPBA), pages 88?91.
J.d. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. Ge-
nia corpus - a semantically annotated corpus for bio-
textmining. Bioinformatics (Supplement: Eleventh
International Conference on Intelligent Systems for
Molecular Biology), 19:180?182.
J. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Collier.
2004. Introduction to the bio-entity recognition task at
JNLPBA. In N. Collier, P. Ruch, and A. Nazarenko,
editors, Proceedings of the International Joint Work-
shop on Natural Language Processing in Biomedicine
and its Applications (JNLPBA), Geneva, Switzerland,
pages 70?75, August 28?29. held in conjunction with
COLING?2004.
Seonho Kim, Juntae Yoon, Kyung-Mi Park, and Hae-
Chang Rim. 2005. Two-phase biomedical named en-
tity recognition using a hybrid method. In Proceedings
of The Second International Joint Conference on Nat-
ural Language Processing (IJCNLP-05), pages 646?
657.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory bfgs method for large scale optimization. Math.
Program., 45(3):503?528.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proceed-
ings of CoNLL-2002, pages 49?55.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In ACL ?05: Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 91?98.
Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun?ichi Tsujii. 2006. Improving the scal-
ability of semi-markov conditional random fields for
named entity recognition. In ACL ?06: Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the ACL,
pages 465?472.
B. Settles. 2004. Biomedical named entity recognition
using conditional random fields and rich feature sets.
In Proceedings of the International Joint Workshop on
Natural Language Processing in Biomedicine and its
Applications (NLPBA), pages 104?107.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In NAACL ?03: Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages 134?
141.
S. V. N. Vishwanathan, Nicol N. Schraudolph, Mark W.
Schmidt, and Kevin P. Murphy. 2006. Accelerated
training of conditional random fields with stochastic
gradient methods. In ICML ?06: Proceedings of the
23rd international conference on Machine learning,
pages 969?976.
GuoDong Zhou and Jian Su. 2004. Exploring deep
knowledge resources in biomedical name recognition.
In COLING 2004 International Joint workshop on
Natural Language Processing in Biomedicine and its
Applications (NLPBA/BioNLP) 2004, pages 99?102.
600
Chinese NER Using CRFs and Logic for the Fourth SIGHAN Bakeoff ?
Xiaofeng YU Wai LAM Shing-Kit CHAN Yiu Kei WU Bo CHEN
Information Systems Laboratory
Department of Systems Engineering & Engineering Management
The Chinese University of Hong Kong
Shatin, N.T., Hong Kong
{xfyu,wlam,skchan,ykwu,bchen}@se.cuhk.edu.hk
Abstract
We report a high-performance Chinese NER
system that incorporates Conditional Random
Fields (CRFs) and first-order logic for the fourth
SIGHAN Chinese language processing bake-
off (SIGHAN-6). Using current state-of-the-
art CRFs along with a set of well-engineered
features for Chinese NER as the base model,
we consider distinct linguistic characteristics in
Chinese named entities by introducing various
types of domain knowledge into Markov Logic
Networks (MLNs), an effective combination of
first-order logic and probabilistic graphical mod-
els for validation and error correction of enti-
ties. Our submitted results achieved consistently
high performance, including the first place on the
CityU open track and fourth place on the MSRA
open track respectively, which show both the at-
tractiveness and effectiveness of our proposed
model.
1 Introduction
We participated in the Chinese named entity recognition
(NER) task for the fourth SIGHAN Chinese language
processing bakeoff (SIGHAN-6). We submitted results
for the open track of the NER task. Our official re-
sults achieved consistently high performance, including
the first place on the CityU open track and fourth place on
the MSRA open track. This paper presents an overview
of our system due to space limit. A more detailed de-
scription of our model is presented in (Yu et al, 2008).
Our Chinese NER system combines the strength of two
graphical discriminative models, Conditional Random
?The work described in this paper is substantially supported
by grants from the Research Grant Council of the Hong Kong
Special Administrative Region, China (Project Nos: CUHK
4179/03E, CUHK4193/04E, and CUHK4128/07) and the Di-
rect Grant of the Faculty of Engineering, CUHK (Project Codes:
2050363 and 2050391). This work is also affiliated with the
Microsoft-CUHK Joint Laboratory for Human-centric Comput-
ing and Interface Technologies.
Fields (CRFs) and Markov Logic Networks (MLNs).
First, we employ CRFs, a discriminatively trained undi-
rected graphical model which has been shown to be an
effective approach to segmenting and labeling sequence
data, as our base system. Second, we model the linguis-
tic and structural information in Chinese named entity
composition. We exploit a variety of domain knowledge
which can capture essential characteristics of Chinese
named entities into Markov Logic Networks (MLNs), a
powerful combination of first-order logic and probability,
to (1) validate and correct errors made in the base sys-
tem and (2) find and extract new entity candidates. These
domain knowledge is easy to obtain and can be well and
concisely formulated in first-order logic and incorporated
into MLNs.
2 Conditional Random Fields as Base
Model
Conditional Random Fields (CRFs) (Lafferty et al, 2001)
are undirected graphical models trained to maximize the
conditional probability of the desired outputs given the
corresponding inputs. CRFs have been shown to perform
well on Chinese NER shared task on SIGHAN-4 (Zhou et
al. (2006), Chen et al (2006a), Chen et al (2006b)). We
employ CRFs as the base model in our framework. In this
base model, we design features similar to the state-of-the-
art CRF models for Chinese NER. We use character fea-
tures, word segmentation features, part-of-speech (POS)
features, and dictionary features, as described below.
Character features: These features are the current char-
acter, 2 characters preceding the current character and 2
following the current character. We extend the window
size to 7 but find that it slightly hurts. The reason is that
CRFs can deal with non-independent features. A larger
window size may introduce noisy and irrelevant features.
Word segmentation and POS features: We train our
own model for conducting Chinese word segmentation
and POS tagging. We employ a unified framework to
integrate cascaded Chinese word segmentation and POS
tagging tasks by joint decoding that guards against vi-
102
Sixth SIGHAN Workshop on Chinese Language Processing
olations of those hard-constraints imposed by segmenta-
tion task based on dual-layer CRFs introduced by Shi and
Wang (2007).
We separately train the Chinese word segmentation
and POS tagging CRF models using 8-month and 2-
month PKU 2000 corpus, respectively. The original PKU
2000 corpus contains more than 100 different POS tags.
To reduce the training time for POS tagging experiment,
we merge some similar tags and obtain only 42 tags fi-
nally. For example, {ia, ib, id, in, iv}?i. We use
the same features as described in (Shi and Wang, 2007),
except that we do not use the HowNet features for word
segmentation. Instead, we use max-matching segmenta-
tion features based on a word dictionary. This dictionary
contains 445456 words which are extracted from People?s
Daily corpus (January-June, 1998), CityU, MSRA, and
PKU word segmentation training corpora in SIGHAN-6.
For decoding, we first perform individual decoding for
each task. We then set 10-best segmentation and POS
tagging results for reranking and joint decoding in order
to find the most probable joint decodings for both tasks.
Dictionary features: We obtain a named entity dictio-
nary extracted from People?s Daily 1998 corpus and PKU
2000 corpus, which contains 68305 PERs, 28408 LOCs
and 55596 ORGs. We use the max-matching algorithm
to search whether a string exists in this dictionary.
In summary, we list the features used for our CRF base
model in Table 1. Besides the unigram feature template,
CRFs also allow bigram feature template. With this tem-
plate, a combination of the current output token and pre-
vious output token (bigram) is automatically generated.
We use CRF++ toolkit (version 0.48) (Kudo, 2005) in
our experiments. We find that setting the cut-off threshold
f for the features not only decreases the training time, but
improves the NER performance. CRFs can use the fea-
tures that occurs no less than f times in the given training
data. We set f = 5 in our system.
We extend the BIO representation for the chunk tag
which was employed in the CoNLL-2002 and CoNLL-
2003 evaluations. We use the BIOES representation in
which each character is tagged as either the beginning of
a named entity (B tag), a character inside a named en-
tity (I tag), the last character in an entity (E tag), single-
character entities (S tag), or a character outside a named
entity (O tag). We find that BIOES representation is
more informative and yields better results than BIO rep-
resentation.
3 Markov Logic Networks as Error
Correction Model
Even though the CRF model is able to accommodate a
large number of well-engineered features which can be
easily obtained across languages, some NEs, especially
Table 1: Feature template for CRF model.
Character features (1.1) Cn, n ? [?2, 2]
(1.2) CnCn+1, n ? [?2, 1]
Word features (1.3) Wn, n ? [?3, 3]
(1.4) WnWn+1, n ? [?3, 2]
POS features (1.5) Pn, n ? [?3, 3]
(1.6) PnPn+1, n ? [?3, 2]
Dictionary features (1.7) Dn, n ? [?2, 2]
(1.8) DnDn+1, n ? [?2, 1]
(1.9) D?1D+1
LOCs and ORGs are difficult to identify due to the lack
of linguistic or structural characteristics.
We incorporate domain knowledge that can be well
formulated into first-order logic to extract entity candi-
dates from CRF results. Then, the Markov Logic Net-
works (MLNs), an undirected graphical model for statis-
tical relational learning, is used to validate and correct
the errors made in the base model.
MLNs conduct relational learning by incorporating
first-order logic into probabilistic graphical models under
a single coherent framework (Richardson and Domingos,
2006). Traditional first-order logic is a set of hard con-
straints in which a world violates even one formula has
zero probability. The advantage of MLNs is to soften
these constraints so that when the fewer formulae a world
violates, the more probable it is. MLNs have been applied
to tackle the problems of gene interaction discovery from
biomedical texts and citation entity resolution from cita-
tion texts with state-of-the-art performance (Riedel and
Klein (2005), Singla and Domingos (2006)).
We use the Alchemy system (Beta version) (Kok et al,
2005) in our experiment, which is a software package
providing a series of algorithms for statistical relational
learning and probabilistic logic inference, based on the
Markov logic representation.
3.1 Domain Knowledge
We extract 165 location salient words and 843 organiza-
tion salient words from Wikipedia and the LDC Chinese-
English bi-directional NE lists compiled from Xinhua
News database. We also make a punctuation list which
contains 18 items and some stopwords which Chinese
NEs cannot contain. We extract new NE candidates from
the CRF results according to the following consideration:
? If a chunk (a series of continuous characters) occurs in the
training data as a PER or a LOC or an ORG, then this
chunk should be a PER or a LOC or an ORG in the testing
data. In general, a unique string is defined as a PER, it
cannot be a LOC somewhere else.
? If a tagged entity ends with a location salient word, it is a
LOC. If a tagged entity ends with an organization salient
word, it is an ORG.
103
Sixth SIGHAN Workshop on Chinese Language Processing
Table 2: Statistics of NER training and testing corpora.
Corpus Training NEs PERs/LOCs/ORGs Testing NEs PERs/LOCs/ORGs
CityU 66255 16552/36213/13490 13014 4940/4847/3227
MSRA 37811 9028/18522/10261 7707 1864/3658/2185
NEs: Number of named entities; PERs: Number of person names;
LOCs: Number of location names; ORGs: Number of organization names.
Table 3: OOV Rate of NER testing corpora.
Corpus Overall (IVs/OOVs/OOV-Rate) PER (IVs/OOVs/OOV-Rate) LOC (IVs/OOVs/OOV-Rate) ORG (IVs/OOVs/OOV-Rate)
CityU 6660/6354/0.4882 1062/3878/0.7850 3947/900/0.1857 1651/1576/0.4884
MSRA 6056/1651/0.2142 1300/564/0.3026 3343/315/0.0861 1413/772/0.3533
IVs: number of IV (named entities in vocabulary); OOVs: number of OOV
(named entities out of vocabulary); OOV-Rate: ratio of named entities out of vocabulary.
? If a tagged entity is close to a subsequent location salient
word, probably they should be combined together as a
LOC. The closer they are, the more likely that they should
be combined.
? If a series of consecutive tagged entities are close to a sub-
sequent organization salient word, they should probably
be combined together as an ORG because an ORG may
contain multiple PERs, LOCs and ORGs.
? Similarly, if there exists a series of consecutive tagged en-
tities and the last one is tagged as an ORG, it is likely that
all of them should be combined as an ORG.
? Entity length restriction: all kinds of tagged entities can-
not exceed 25 Chinese characters.
? Stopword restriction: intuitively, all tagged entities cannot
comprise any stopword.
? Punctuation restriction: in general, all tagged entities can-
not span any punctuation.
? Since all NEs are proper nouns, the tagged entities should
end with noun words.
? For a chunk with low conditional probabilities, all the
above assumptions are adopted.
3.2 First-Order Logic Construction
All the above domain knowledge can also be formulated
as first-order logic to construct the structure of MLNs.
First-order formulae are recursively constructed from
atomic formulae using logical connectives and quanti-
fiers. Atomic formulae are constructed using constants,
variables, functions, and predicates.
For example, we use the predicate organization(
candidate) to specify whether a candidate is an ORG.
If ??I?/China Government? is mis-tagged as a
LOC by the CRF model, but it contains the organization
salient word ??/Government?. The corresponding
formula endwith(r, p)?orgsalientword(p)
?organization(r) means if a tagged entity r ends
with an organization salient word p, then it is extracted as
a new ORG entity. Typically only a small number (e.g.,
10-20) of formulae are needed. We declare 14 predi-
cates and 15 first-order formulae according to the domain
knowledge mentioned in Section 3.1.
3.3 Training and Inference for Named Entity
Correction
Each extracted new NE candidate is represented by one
or more strings appearing as arguments of ground atoms
in the database. The goal of NE prediction is to deter-
mine whether the candidates are entities and the types of
entities (query predicates), given the evidence predicates
and other relations that can be deterministically derived
from the database.
We extract all the NEs from the official training cor-
pora, and then convert them to the first-order logic repre-
sentation according to the domain knowledge. The MLN
training database that consists of predicates, constants,
and ground atoms was built automatically. We also ex-
tract new entity candidates from CRF results and con-
struct MLN testing database in the same way.
During MLN learning, each formula is converted to
Conjunctive Normal Form (CNF), and a weight is learned
for each of its clauses. These weights reflect how often
the clauses are actually observed in the training data. In-
ference is performed by grounding the minimal subset
of the network required for answering the query pred-
icates. Conducting maximum a posteriori (MAP) in-
ference which finds the most likely values of a set of
variables given the values of observed variables can be
performed via approximate solution using Markov chain
Monte Carlo (MCMC) algorithms. Gibbs sampling can
be adopted by sampling each non-evidence variable in
turn given its Markov blanket, and counting the fraction
of samples that each variable is in each state.
4 Experiment Details
4.1 Data and Preprocessing
The training corpora provided by the SIGHAN bakeoff
organizers were in the CoNLL two column format, with
one Chinese character per line and hand-annotated named
entity chunks in the second column. The CityU corpus
was traditional Chinese. We converted this corpus to sim-
plified Chinese and we used UTF-8 encoding in all the
experiments so that all the resources (e.g., word dictio-
nary and named entity dictionary) are compatible in our
104
Sixth SIGHAN Workshop on Chinese Language Processing
Table 4: Official results on CityU andMSRA open tracks.
Precision Recall F?=1
CityU
PER 97.21% 95.26% 96.23
LOC 92.35% 93.42% 92.88
ORG 88.05% 66.44% 75.73
Overall 93.42% 87.43% 90.33
MSRA
PER 98.33% 94.58% 96.42
LOC 93.97% 93.36% 93.66
ORG 92.80% 84.39% 88.40
Overall 94.71% 91.11% 92.88
system.
Table 2 shows the statistics of NER training and testing
corpora and Table 3 shows the OOV (Out of Vocabulary)
rate of NER testing corpora 1. The number of NEs in
CityU corpus is almost twice as many as that in MSRA
corpus. The OOV rate in CityU corpus is much higher
than in MSRA corpus for PERs, LOCs and ORGs. These
numbers indicate that NER on CityU corpus is much
more difficult to handle.
4.2 Model Development
We performed holdout methodology to develop our
model. We randomly selected 5000 sentences fromCityU
training corpus for development testing and the rest for
training. We did the same thing for MSRA training cor-
pus.
To avoid overfitting for CRF model, we penalized
the log-likelihood by the commonly used zero-mean
Gaussian prior over the parameters. Also, the MLNs
were trained using a Gaussian prior with zero mean and
unit variance on each weight to penalize the pseudo-
likelihood, and with the weights initialized at the mode
of the prior (zero).
We found an optimal value for the parameter c 2 for
CRFs. Using held-out data, we tested all c values, c ?
[0.2, 2.2], with an incremental step of 0.4. Finally, we set
c = 1.8 for CityU corpus and c = 1.0 for MSRA corpus.
5 Official Results
Table 4 presents the results obtained on the official CityU
and MSRA test sets. Our results are consistently good:
we obtained the first place on the CityU open track (90.33
overall F-measure) and fourth place on the MSRA open
track (92.88 overall F-measure) respectively. The lower
1The NER on the PKU corpus was cancelled by the orga-
nizer due to the tagging inconsistency of this corpus.
2This parameter trades the balance between overfitting and
underfitting. With larger c value, CRF tends to overfit to the give
training corpus. The results will significantly be influenced by
this parameter
F-measure obtained on CityU corpus can be attributed to
the higher OOV rate of this corpus.
6 Conclusion
We have described a Chinese NER system incorporating
probabilistic graphical models and first-order logic which
achieves state-of-the-art performance on the open track of
SIGHAN-6. We exploited domain knowledge which can
capture the essential features of Chinese NER and can
be concisely formulated in MLNs, allowing the training
and inference algorithms to be directly applied to them.
Our proposed framework can also be extendable to NER
for other languages, due to the simplicity of the domain
knowledge we could access.
References
Aitao Chen, Fuchun Peng, Roy Shan, and Gordon Sun. Chi-
nese named entity recognition with conditional probabilistic
models. In 5th SIGHAN Workshop on Chinese Language
Processing, Australia, July 2006.
Wenliang Chen, Yujie Zhang, and Hitoshi Isahara. Chinese
named entity recognition with conditional random fields. In
5th SIGHAN Workshop on Chinese Language Processing,
Australia, July 2006.
Stanley Kok, Parag Singla, Matthew Richardson, and Pedro
Domingos. The Alchemy system for statistical relational
AI. Technical report, Department of Computer Science and
Engineering, University of Washington, Seattle, WA, 2005.
http://www.cs.washington.edu/ai/alchemy.
Taku Kudo. CRF++: Yet another CRF tool kit.
http://crfpp.sourceforge.net/, 2005.
John Lafferty, Andrew McCallum, and Fernando Pereira. Con-
ditional random fields: Probabilistic models for segment-
ing and labeling sequence data. In Proceedings of ICML-
01, pages 282?289. Morgan Kaufmann, San Francisco, CA,
2001.
Matthew Richardson and Pedro Domingos. Markov logic net-
works. Machine Learning, 62(1-2):107?136, 2006.
Sebastian Riedel and Ewan Klein. Genic interaction extraction
with semantic and syntactic chains. In Proceedings of the
Learning Language in Logic Workshop (LLL-05), pages 69?
74, 2005.
Yanxin Shi and Mengqiu Wang. A dual-layer CRFs based
joint decoding method for cascaded segmentation and label-
ing tasks. In Proceedings of IJCAI-07, pages 1707?1712,
Hyderabad, India, 2007.
Parag Singla and Pedro Domingos. Entity resolution with
Markov logic. In Proceedings of ICDM-06, pages 572?582,
Hong Kong, 2006.
Xiaofeng Yu, Wai Lam, and Shing-Kit Chan. A framework
based on graphical models with logic for Chinese named en-
tity recognition. In Proceedings of IJCNLP-08, Hyderabad,
India, 2008. To appear.
Junsheng Zhou, Liang He, Xinyu Dai, and Jiajun Chen. Chinese
named entity recognition with a multi-phase model. In 5th
SIGHAN Workshop on Chinese Language Processing, Aus-
tralia, July 2006.
105
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of NAACL HLT 2007, Companion Volume, pages 197?200,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Chinese Named Entity Recognition with Cascaded Hybrid Model
Xiaofeng YU
Information Systems Laboratory
Department of Systems Engineering & Engineering Management
The Chinese University of Hong Kong
Shatin, N.T., Hong Kong
xfyu@se.cuhk.edu.hk
Abstract
We propose a high-performance cascaded hy-
brid model for Chinese NER. Firstly, we use
Boosting, a standard and theoretically well-
founded machine learning method to combine a
set of weak classifiers together into a base sys-
tem. Secondly, we introduce various types of
heuristic human knowledge into Markov Logic
Networks (MLNs), an effective combination
of first-order logic and probabilistic graphi-
cal models to validate Boosting NER hypothe-
ses. Experimental results show that the cas-
caded hybrid model significantly outperforms
the state-of-the-art Boosting model.
1 Introduction
Named entity recognition (NER) involves the identifica-
tion and classification of certain proper nouns in text,
such as person names (PERs), locations (LOCs), orga-
nizations (ORGs), miscellaneous names (MISCs), tem-
poral, numerical and monetary phrases. It is a well-
established task in the NLP community and is regarded
as crucial technology for many NLP applications, such as
information extraction, question answering, information
retrieval and machine translation.
Compared to European-language NER, Chinese NER
seems to be more difficult (Yu et al, 2006). Recent ap-
proaches to Chinese NER are a shift away from man-
ually constructed rules or finite state patterns towards
machine learning or statistical methods. However, rule-
based NER systems lack robustness and portability, and
machine learning approaches might be unsatisfactory to
learn linguistic information in Chinese NEs. In fact,
Chinese NEs have distinct linguistic characteristics in
their composition and human beings usually use prior
knowledge to recognize NEs. For example, about 365
of the highest frequently used surnames cover 99% Chi-
nese surnames (Sun et al, 1995). For the LOC ??
/Beijing City?, ??/Beijing? is the name part and
?/City? is the salient word. For the ORG ???
?/Beijing City Government?, ??/Beijing? is the LOC
name part, ?/City? is the LOC salient word and ??
?/Government? is the ORG salient word. Some ORGs
contain one or more PERs, LOCs, MISCs and ORGs. A
more complex example is the nested ORG ?VfI
fI'f??:fb/School of Computer Science,
Wuhan University, Wuhan City, Hubei Province? which
contains two ORGs ?fI'f/Wuhan University? and
???:fb/School of Computer Science? and two
LOCs ?V/Hubei Province? and ?fI/Wuhan
City?. The two ORGs contain ORG salient words ?'
f/University? and ?fb/School?, while the two LOCs
contain LOC salient words ?/Province? and ?/City?
respectively.
Inspired by the above observation, we propose a cas-
caded hybrid model for Chinese NER 1. First, we em-
ploy Boosting, which has theoretical justification and has
been shown to perform well on other NLP problems,
to combine weak classifiers into a strong classifier. We
then exploit a variety of heuristic human knowledge into
MLNs, a powerful combination of logic and probabil-
ity, to validate Boosting NER hypotheses. We also use
three Markov chain Monte Carlo (MCMC) algorithms
for probabilistic inference in MLNs. Experimental re-
sults show that the cascaded hybrid model yields better
NER results than the stand-alone Boosting model by a
large margin.
2 Boosting
The main idea behind the Boosting algorithm is that a set
of many simple and moderately accurate weak classifiers
(also called weak hypotheses) can be effectively com-
bined to yield a single strong classifier (also called the
final hypothesis). The algorithm works by training weak
classifiers sequentially whose classification accuracy is
slightly better than random guessing and finally combin-
1In this paper we only focus on PERs, LOCs, ORGs and
MISCs. Since temporal, numerical and monetary phrases can
be well identified with rule-based approaches.
197
ing them into a highly accurate classifier. Each weak clas-
sifier searches for the hypothesis in the hypotheses space
that can best classify the current set of training examples.
Based on the evaluation of each iteration, the algorithm
re-weights the training examples, forcing the newly gen-
erated weak classifier to give higher weights to the exam-
ples that are misclassified in the previous iteration.
We use the AdaBoost.MH algorithm (Schapire and
Singer, 1999) as shown in Figure 1, an n-ary classi-
fication variant of the original well-known binary Ad-
aBoost algorithm (Freund and Schapire, 1997). It has
been demonstrated that Boosting can be used to build
language-independent NER models that perform excep-
tionally well (Wu et al (2002), Wu et al (2004), Carreras
et al (2002)). In particular, reasonable Chinese NER
results were still obtained using Boosting, even though
there was no Chinese-specific tuning and the model was
only trained on one-third of the provided corpora in
SIGHAN bakeoff-3 (Yu et al, 2006).
3 Markov Logic Networks
A Markov Network (also known as Markov Random
Field) is a model for the joint distribution of a set of
variables (Pearl, 1988). It is composed of an undirected
graph and a set of potential functions. A First-Order
Knowledge Base (KB) (Genesereth and Nislsson, 1987)
is a set of sentences or formulas in first-order logic. A
Markov Logic Network (MLN) (Richardson and Domin-
gos, 2006) is a KB with a weight attached to each formula
(or clause). Together with a set of constants representing
objects in the domain, it species a ground Markov Net-
work containing one feature for each possible grounding
of a first-order formula in the KB, with the correspond-
ing weight. The weights associated with the formulas in
an MLN jointly determine the probabilities of those for-
mulas (and vice versa) via a log-linear model. An MLN
defines a probability distribution over Herbrand interpre-
tations (possible worlds), and can be thought of as a tem-
plate for constructing Markov Networks. The probabil-
ity distribution over possible worlds x specified by the
ground Markov Network ML,C is given by
P (X = x) =
1
Z
exp(
?
wini(x )) =
1
Z
?
?i
(
x{i}
)ni(x)
(1)
where Fi is the formula in first-order logic, wi is a real
number, ni (x) is the number of true groundings of Fi in
x, x{i} is the true value of the atoms appearing in Fi, and
?i
(
x{i}
)
= ewi .
3.1 Learning Weights
Given a relational database, MLN weights can in princi-
ple be learned generatively by maximizing the likelihood
of this database. The gradient of the log-likelihood with
Input: A training set Tr = {< d1, C1 >, . . . , < dg, Cg >}
where Cj ? C = {c1, ..., cm} for all j = 1, . . . , g.
Output: A final hypothesis ?(d, c) =
?S
s=1 ?s?s(d, c).
Algorithm: LetD1(dj , ci) = 1mg for all j = 1, . . . , g and
for all i = 1, . . . ,m. For s = 1, . . . , S do:
? pass distribution Ds(dj , ci)to the weak classifier;
? derive the weak hypothesis ?s from the weak
classifier;
? choose ?s ? R;
? set Ds+1(dj , ci) =
Ds(dj ,ci)exp(??sCj [ci]?s(dj ,ci))
Zs
where
Zs =?m
i=1
?g
j=1 Ds(dj , ci )exp( ? ?sCj [ci] ?s(dj , ci))
is a normalization factor chosen so that?m
i=1
?g
j=1 Ds+1(dj , ci) = 1.
Figure 1: The AdaBoost.MH algorithm.
respect to the weights is
?
?wi
logPw(X = x) = ni (x) ?
?
Pw(X = x
?)ni(x
?)
(2)
where the sum is over all possible databases x? , and
Pw(X = x?) is P (X = x?) computed using the current
weight vector w = (w1, ..., wi, ...). Unfortunately, com-
puting these expectations can be very expensive. Instead,
we can maximize the pseudo-likelihood of the data more
efficiently. If x is a possible database and xl is the lth
ground atom?s truth value, the pseudo-log-likelihood of x
given weights w is
logP ?w(X = x) =
n?
l=1
logPw(Xl=xl | MBx(Xl )) (3)
where MBx (Xl) is the state of Xl?s Markov blan-
ket in the data. Computing Equation 3 and its gradient
does not require inference over the model, and is there-
fore much faster. We optimize the pseudo-log-likelihood
using the limited-memory BFGS algorithm (Liu and No-
cedal, 1989).
3.2 Inference
If F1 and F2 are two formulas in first-order logic, C is a
finite set of constants including any constants that appear
in F1 or F2, and L is an MLN, then
P (F1 | F2, L, C) = P (F1 | F2,ML,C) (4)
=
P (F1 ? F2 | ML,C)
P (F2 | ML,C)
(5)
=
?
x??F1??F2
P (X = x | ML,C)
?
x??F2
P (X = x | ML,C)
(6)
198
where ?Fi is the set of worlds where Fi holds, and
P (x | ML,C) is given by Equation 1. The ques-
tion of whether a knowledge base entails a formula F
in first-order logic is the question of whether P (F |
LKB, CKB,F ) = 1, where LKB is the MLN obtained by
assigning infinite weight to all the formulas in KB, and
CKB,F is the set of all constants appearing in KB or F .
The most widely used approximate solution to proba-
bilistic inference in MLNs is Markov chain Monte Carlo
(MCMC) (Gilks et al, 1996). In this framework, the
Gibbs sampling algorithm is to generate an instance from
the distribution of each variable in turn, conditional on the
current values of the other variables. One way to speed
up Gibbs sampling is by Simulated Tempering (Marinari
and Parisi, 1992), which performs simulation in a gener-
alized ensemble, and can rapidly achieve an equilibrium
state. Poon and Domingos (2006) proposed MC-SAT,
an inference algorithm that combines ideas from MCMC
and satisfiability.
4 Heuristic Human Knowledge
Even though the Boosting model is able to accommodate
a large number of features, some NEs, especially LOCs,
ORGs and MISCs are difficult to identify due to lack
of linguistic knowledge. For example, some ORGs are
possibly mistagged as LOCs and/or MISCs. We incor-
porate heuristic human knowledge via MLNs to validate
the Boosting NER hypotheses. We extract 151 location
salient words and 783 organization salient words from the
LDC Chinese-English bi-directional NE lists compiled
from Xinhua News database. We also make a punctua-
tion list which contains 19 items. We make the following
assumptions to validate the Boosting results:
? Obviously, if a tagged entity ends with a location
salient word, it is a LOC. If a tagged entity ends with
an organization salient word, it is an ORG.
? If a tagged entity is close to a subsequent location
salient word, probably they should be combined to-
gether as a LOC. The closer they are, the more likely
that they should be combined.
? Heuristically, if a series of consecutive tagged en-
tities are close to a subsequent organization salient
word, they should probably be combined together
as an ORG because an ORG may contain multiple
PERs, LOCs, MISCs and ORGs.
? Similarly, if there exists a series of consecutive
tagged entities and the last one is tagged as an ORG,
it is likely that all of them should be combined as an
ORG.
? Entity length restriction: all kinds of tagged entities
cannot exceed 25 Chinese characters.
? Punctuation restriction: in general, all tagged enti-
ties cannot span any punctuation.
? Since all NEs are proper nouns, the tagged entities
should end with noun words.
All the above human knowledge can be formulized as
first-order logic to construct the structure of MLNs. And
all the validated Boosting results are accepted as new NE
candidates (or common nouns). We train an MLN to rec-
ognize them.
5 Experiments
We randomly selected 15,000 and 3,000 sentences from
the People?s Daily corpus as training and test sets, respec-
tively. We used the decision stump2 as the weak classifier
in Boosting to construct a character-based Chinese NER
baseline system.
The features used were as follows:
? The current character and its POS tag.
? The characters within a window of 3 characters be-
fore and after the current character.
? The POS tags within a window of 3 characters be-
fore and after the current character.
? A small set of conjunctions of POS tags and charac-
ters within a window of 3 characters of the current
character.
? The BIO 3 chunk tags of the previous 3 characters.
We declared 10 predicates and specified 9 first-
order formulas according to the heuristic human
knowledge in Section 4. For example, we used
person(candidate) to predicate whether a candi-
date is a PER. Formulas are recursively constructed from
atomic formulas using logical connectives and quanti-
fiers. They are constructed using four types of sym-
bols: constants, variables, functions, and predicates.
Constant symbols represent objects in the domain of
interest (e.g., ??/Beijing? and ?
w/Shanghai? are
LOCs). Variable symbols range over the objects in the
domain. Function symbols represent mappings from tu-
ples of objects to objects. Predicate symbols repre-
sent relations among objects in the domain or attributes
of objects. For example, the formula endwith(r,
p)?locsalientword(p)=>location(r) means
if r ends with a location salient word p, then it is a LOC.
2A decision stump is basically a one-level decision tree
where the split at the root level is based on a specific at-
tribute/value pair.
3In this representation, each character is tagged as either the
beginning of a named entity (B tag), a character inside a named
entity (I tag), or a character outside a named entity (O tag).
199
We extracted all the distinct NEs (4,475 PERs, 2,170
LOCs, 2,823 ORGs and 614 MISCs) from the 15,000-
sentence training corpus. An MLN training database,
which consists of 10 predicates and 44,810 ground
atoms was built. A ground atom is an atomic formula
all of whose arguments are ground terms (terms con-
taining no variables). For example, the ground atom
location(?) conveys that ??/Beijing
City? is a LOC.
During MLN learning, each formula is converted to
Conjunctive Normal Form (CNF), and a weight is learned
for each of its clauses. The weight of a clause is used
as the mean of a Gaussian prior for the learned weight.
These weights reflect how often the clauses are actually
observed in the training data.
We validated 352 Boosting results to construct the
MLN testing database, which contains 1,285 entries and
these entries are used as evidence for inference. Infer-
ence is performed by grounding the minimal subset of the
network required for answering the query predicates. We
applied 3 MCMC algorithms: Gibbs sampling (GS), MC-
SAT and Simulated Tempering (ST) for inference and
the comparative NER results are shown in Table 1. The
cascaded hybrid model greatly outperforms the Boosting
model. We obtained the same results using GS and ST
algorithms. And GS (or ST) yields slightly better results
than the MC-SAT algorithm.
6 Conclusion
In this paper we propose a cascaded hybrid model
for Chinese NER. We incorporate human heuristics via
MLNs, which produce a set of weighted first-order
clauses to validate Boosting NER hypotheses. To the best
of our knowledge, this is the first attempt at using MLNs
for the NER problem in the NLP community. Experi-
ments on People?s Daily corpus illustrate the promise of
our approach. Directions for future work include learning
the structure of MLNs automatically and using MLNs for
information extraction and statistical relational learning
(e.g., entity relation identification).
References
Xavier Carreras, Llu??s Ma`rquez, and Llu??s Padro?. Named en-
tity extraction using AdaBoost. In Computational Natural
Language Learning (CoNLL-2002), at COLING-2002, pages
171?174, Taipei, Sep 2002.
Yoav Freund and Robert E. Schapire. A decision-theoretic gen-
eralization of on-line learning and an application to boosting.
Computer and System Sciences, 55(1):119?139, 1997.
Michael R. Genesereth and Nils J. Nislsson. Logical founda-
tions of artificial intelligence. Morgan Kaufmann Publishers
Inc., San Mateo, CA, 1987.
W.R. Gilks, S. Richardson, and D.J. Spiegelhalter. Markov
chain Monte Carlo in practice. Chapman and Hall, London,
UK, 1996.
Table 1: Comparative Chinese NER Results
Precision Recall F?=1
Boosting
PER 99.39% 99.06% 99.22
LOC 87.55% 91.81% 89.63
ORG 82.15% 66.61% 73.57
MISC 80.00% 87.84% 83.74
Overall 90.26% 89.42% 89.84
Hybrid (MC-SAT)
PER 99.39% 99.06% 99.22
LOC 94.83% 91.81% 93.30
ORG 87.82% 85.69% 86.74
MISC 93.53% 85.10% 89.12
Overall 95.01% 92.78% 93.88
Hybrid (GS/ST)
PER 99.39% 99.06% 99.22
LOC 94.80% 91.91% 93.34
ORG 87.82% 86.28% 87.04
MISC 93.53% 85.10% 89.12
Overall 94.99% 92.93% 93.95
Dong C. Liu and Jorge Nocedal. On the limited memory BFGS
method for large scale optimization. Mathematical Program-
ming, 45:503?528, 1989.
Enzo Marinari and Giorgio Parisi. Simulated Tempering: A
new Monte Carlo scheme. Europhysics Letters, 19:451?458,
1992.
Judea Pearl. Probabilistic reasoning in intelligent systems: net-
works of plausible inference. Morgan Kaufmann Publishers
Inc., San Francisco, CA, 1988.
Hoifung Poon and Pedro Domingos. Sound and efficient infer-
ence with probabilistic and deterministic dependencies. In
21st National Conference on Artificial Intelligence (AAAI-
06), Boston, Massachusetts, July 2006. The AAAI Press.
Matthew Richardson and Pedro Domingos. Markov logic net-
works. Machine Learning, 62(1-2):107?136, 2006.
Robert E. Schapire and Yoram Singer. Improved boosting algo-
rithms using confidence-rated predictions. Machine Learn-
ing, 37(3):297?336, 1999.
Maosong Sun, Changning Huang, Haiyan Gao, and Jie Fang.
Identifying Chinese names in unrestricted texts. Journal of
Chinese Information Processing, 1995.
Dekai Wu, Grace Ngai, Marine Carpuat, Jeppe Larsen, and
Yongsheng Yang. Boosting for named entity recognition. In
Computational Natural Language Learning (CoNLL-2002),
at COLING-2002, pages 195?198, Taipei, Sep 2002.
Dekai Wu, Grace Ngai, and Marine Carpuat. Why nitpicking
works: Evidence for Occam?s razor in error correctors. In
20th International Conference on Computational Linguistics
(COLING-2004), Geneva, 2004.
Xiaofeng Yu, Marine Carpuat, and Dekai Wu. Boosting for
Chinese named entity recognition. In 5th SIGHAN Workshop
on Chinese Language Processing, Australia, July 2006.
200
Coling 2010: Poster Volume, pages 1399?1407,
Beijing, August 2010
Jointly Identifying Entities and Extracting Relations in Encyclopedia
Text via A Graphical Model Approach?
Xiaofeng YU Wai LAM
Information Systems Laboratory
Department of Systems Engineering & Engineering Management
The Chinese University of Hong Kong
{xfyu,wlam}@se.cuhk.edu.hk
Abstract
In this paper, we investigate the problem of en-
tity identification and relation extraction from en-
cyclopedia articles, and we propose a joint discrim-
inative probabilistic model with arbitrary graphical
structure to optimize all relevant subtasks simulta-
neously. This modeling offers a natural formalism
for exploiting rich dependencies and interactions
between relevant subtasks to capture mutual ben-
efits, as well as a great flexibility to incorporate a
large collection of arbitrary, overlapping and non-
independent features. We show the parameter es-
timation algorithm of this model. Moreover, we
propose a new inference method, namely collec-
tive iterative classification (CIC), to find the most
likely assignments for both entities and relations.
We evaluate our model on real-world data from
Wikipedia for this task, and compare with current
state-of-the-art pipeline and joint models, demon-
strating the effectiveness and feasibility of our ap-
proach.
1 Introduction
We investigate a compound information extrac-
tion (IE) problem from encyclopedia articles,
which consists of two subtasks ? recognizing
structured information about entities and extract-
ing the relationships between entities. The most
common approach to this problem is a pipeline
architecture: attempting to perform different sub-
tasks, namely, named entity recognition and rela-
tion extraction between recognized entities in sev-
eral separate, and independent stages. Such kind
of design is widely adopted in NLP.
?The work described in this paper is substantially sup-
ported by grants from the Research Grant Council of the
Hong Kong Special Administrative Region, China (Project
No: CUHK4128/07) and the Direct Grant of the Fac-
ulty of Engineering, CUHK (Project Codes: 2050442 and
2050476). This work is also affiliated with the Microsoft-
CUHK Joint Laboratory for Human-centric Computing and
Interface Technologies.
The most common and simplest approach to
performing compound NLP tasks is the 1-best
pipeline architecture, which only takes the 1-best
hypothesis of each stage and pass it to the next
one. Although it is comparatively easy to build
and efficient to run, this pipeline approach is
highly ineffective and suffers from serious prob-
lems such as error propagation (Finkel et al,
2006; Yu, 2007; Yu et al, 2008). It is not sur-
prising that, the end-to-end performance will be
restricted and upper-bounded.
Usually, one can pass N-best lists between dif-
ferent stages in pipeline architectures, and this of-
ten gives useful improvements (Hollingshead and
Roark, 2007). However, effectively making use of
N-best lists often requires lots of engineering and
human effort (Toutanova, 2005). On the other
hand, one can record the complete distribution at
each stage in a pipeline, to compute or approxi-
mate the complete distribution at the next stage.
Doing this is generally infeasible, and this solu-
tion is rarely adopted in practice.
One promising way to tackle the problem of er-
ror propagation is to explore joint learning which
integrates evidences from multiple sources and
captures mutual benefits across multiple compo-
nents of a pipeline for all relevant subtasks simul-
taneously (e.g., (Toutanova et al, 2005), (Poon
and Domingos, 2007), (Singh et al, 2009)). Joint
learning aims to handle multiple hypotheses and
uncertainty information and predict many vari-
ables at once such that subtasks can aid each other
to boost the performance, and thus usually leads
to complex model structure. However, it is typ-
ically intractable to run a joint model and they
sometimes can hurt the performance, since they
1399
increase the number of paths to propagate errors.
Due to these difficulties, research on building joint
approaches is still in the beginning stage.
A significant amount of recent work has shown
the power of discriminatively-trained probabilistic
graphical models for NLP tasks (Lafferty et al,
2001; Sutton and McCallum, 2007; Wainwright
and Jordan, 2008). The superiority of graphical
model is its ability to represent a large number of
random variables as a family of probability dis-
tributions that factorize according to an underly-
ing graph, and capture complex dependencies be-
tween variables. And this progress has begun to
make the joint learning approach possible.
In this paper we study and formally define the
joint problem of entity identification and relation
extraction from encyclopedia text, and we propose
a joint paradigm in a single coherent framework
to perform both subtasks simultaneously. This
framework is based on undirected probabilistic
graphical models with arbitrary graphical struc-
ture. We show how the parameters in this model
can be estimated efficiently. More importantly, we
propose a new inference method ? collective it-
erative classification (CIC), to find the maximum
a posteriori (MAP) assignments for both entities
and relations. We perform extensive experiments
on real-world data from Wikipedia for this task,
and substantial gains are obtained over state-of-
the-art probabilistic pipeline and joint models, il-
lustrating the promise of our approach.
2 Problem Formulation
2.1 Problem Description
This problem involves identifying entities and dis-
covering semantic relationships between entity
pairs from English encyclopedic articles. The ba-
sic document is an article, which mainly defines
and describes an entity (known as principal en-
tity). This document mentions some other entities
as secondary entities related to the principal en-
tity. Clearly, our task consists of two subtasks ?
first, for entity identification, we need to recog-
nize the secondary entities (both the boundaries
and types of them) in the document 1. Second,
1Since the topic/title of an article usually defines a princi-
pal entity (e.g., a famous person) and it is easy to identify, in
after all the secondary entities are identified, our
goal for relation extraction is to predict what rela-
tion, if any, each secondary entity has to the prin-
cipal entity. We assume that there is no relation-
ship between any two secondary entities in one
document.
As an illustrative example, Figure 1 shows the
task of entity identification and relationship ex-
traction from encyclopedic documents. Here,
Abraham Lincoln is the principal entity. Our
task consists of assigning a set of pre-defined en-
tity types (e.g., PER, DATE, YEAR, and ORG)
to segmentations in encyclopedic documents and
assigning a set of pre-defined relations (e.g.,
birth day, birth year, and member of) for each
identified secondary entity to the principal entity.
2.2 Problem Formulation
Let x be an observation sequence of tokens in
encyclopedic text and x = {x1, ? ? ? , xN}. Let
sp be the principal entity (we assume that it is
known or can be easily recognized), and let s =
{s1, ? ? ? , sL} be a segmentation assignment of ob-
servation sequence x. Each segment si is a triple
si = {?i, ?i, yi}, where ?i is a start position, ?i
is an end position, and yi is the label assigned to
all tokens of this segment. The segment si satis-
fies 0 ? ?i < ?i ? |x| and ?i+1 = ?i + 1. Let
rpn be the relation assignment between principal
entity sp and secondary entity candidate sn from
the segmentation s, and r be the set of relation as-
signments for sequence x.
Let y = {r, s} be the pair of segmentation s and
segment relations r for an observation sequence
x. A valid assignment y must satisfy the condi-
tion that the assignments of the segments and the
assignments of the relations of segments are max-
imized simultaneously. We now formally define
this joint optimization problem as follows:
Definition 1 (Joint Optimization of Entity Iden-tification and Relation Extraction): Given an ob-servation sequence x, the goal of joint optimiza-tion of entity identification and relation extraction
is to find the assignment y? = {r?, s?} that has the
maximum a posteriori (MAP) probability
y? = argmax
y
P (y|x), (1)
this paper we only focus on secondary entity identification.
1400
	

  
 
    
Coling 2010: Poster Volume, pages 1408?1416,
Beijing, August 2010
Accelerated Training of Maximum Margin Markov Models for Sequence
Labeling: A Case Study of NP Chunking?
Xiaofeng YU Wai LAM
Information Systems Laboratory
Department of Systems Engineering & Engineering Management
The Chinese University of Hong Kong
{xfyu,wlam}@se.cuhk.edu.hk
Abstract
We present the first known empirical results
on sequence labeling based on maximum mar-
gin Markov networks (M3N ), which incorpo-
rate both kernel methods to efficiently deal with
high-dimensional feature spaces, and probabilistic
graphical models to capture correlations in struc-
tured data. We provide an efficient algorithm, the
stochastic gradient descent (SGD), to speedup the
training procedure of M3N . Using official dataset
for noun phrase (NP) chunking as a case study,
the resulting optimizer converges to the same qual-
ity of solution over an order of magnitude faster
than the structured sequential minimal optimization
(structured SMO). Our model compares favorably
with current state-of-the-art sequence labeling ap-
proaches. More importantly, our model can be eas-
ily applied to other sequence labeling tasks.
1 Introduction
The problem of annotating or labeling observation
sequences arises in many applications across a va-
riety of scientific disciplines, most prominently in
natural language processing, speech recognition,
information extraction, and bioinformatics. Re-
cently, the predominant formalism for modeling
and predicting label sequences has been based on
discriminative graphical models and variants.
Among such models, maximum margin
Markov networks (M3N ) and variants ( Taskar
et al (2003); Taskar (2004); Taskar et al (2005))
have recently gained popularity in the machine
learning community. While theM3N framework
makes extensive use of many theoretical results
?The work described in this paper is substantially sup-
ported by grants from the Research Grant Council of the
Hong Kong Special Administrative Region, China (Project
No: CUHK4128/07) and the Direct Grant of the Fac-
ulty of Engineering, CUHK (Project Codes: 2050442 and
2050476). This work is also affiliated with the Microsoft-
CUHK Joint Laboratory for Human-centric Computing and
Interface Technologies.
available for Markov networks, it largely dis-
penses with the probabilistic interpretation. M3N
thus combines the advantages of both worlds, the
possibility to have a concise model of the relation-
ships present in the data via log-linear Markov
networks over a set of label variables and the
highly accurate predictions based on maximum
margin estimation of the model parameters.
Traditionally, M3N can be trained using
the structured sequential minimal optimization
(structured SMO), a coordinate descent method
for solving quadratic programming (QP) prob-
lems (Taskar et al, 2003). Clearly, however, the
polynomial number of constraints in the QP prob-
lem associated with the M3N can still be very
large, making the structured SMO algorithm slow
to converge over the training data. This currently
limits the scalability and applicability ofM3N to
large-scale real world problems.
Stochastic gradient methods (e.g., Lecun et
al. (1998); Bottou (2004)), on the other hand,
are online and scale sub-linearly with the amount
of training data, making them very attractive for
large-scale datasets. In stochastic (or online) gra-
dient descent (SGD), the true gradient is approx-
imated by the gradient of the cost function only
evaluated on a single training example. The pa-
rameters are then adjusted by an amount propor-
tional to this approximate gradient. Therefore, the
parameters of the model are updated after each
training example. For large-scale datasets, online
gradient descent can be much faster than standard
(or batch) gradient descent.
In this paper, we marry the above two tech-
niques and show how SGD can be used to signif-
icantly accelerate the training of M3N . And we
1408
then apply our model to the well-established se-
quence labeling task: noun phrase (NP) chunking.
Experimental results show the validity and effec-
tiveness of our approach. We now summarize the
primary contributions of this paper as follows:
? We exploit M3N to NP chunking on the
standard evaluation dataset, achieving fa-
vorable performance against recent top-
performing systems. The M3N framework
allows arbitrary features of observation se-
quence, as well as the important benefits of
kernels. To the best of our knowledge, this is
the first known empirical study on NP chunk-
ing usingM3N in the NLP community.
? We provide the efficient SGD algorithm to
accelerate the training procedure of M3N ,
and experimental results show that it con-
verges over an order of magnitude faster than
the structured SMO without sacrificing per-
formance.
? Our model is easily extendable to other se-
quence labeling tasks, such as part-of-speech
tagging and named entity recognition. Based
on the promising results on NP chunking,
we believe that our model will significantly
further the applicability of margin-based ap-
proaches to large-scale sequence labeling
tasks.
2 Maximum Margin Markov Networks
for Sequence Labeling
In sequence labeling, the output is a sequence of
labels y = (y1, . . . , yT ) which corresponds to an
observation sequence x = (x1, . . . , xT ). Suppose
each individual label can take values from set ?,
then the problem can be considered as a multiclass
classification problem with |?|T different classes.
InM3N , a pairwise Markov network is defined
as a graph G = (Y,E). Each edge (i, j) ? E is
associated with a potential function
?ij(x, yi, yj) = exp(
l?
k=1
wk?k(x, yi, yj))
= exp(w>?(x, yi, yj)) (1)
where ?(x, yi, yj) is a pairwise basis function. All
edges in the graph denote the same type of in-
teraction, so that we can define a feature map
?k(x, y) = ?(i,j)?E ?k(x, yi, yj). The network
encodes the following conditional probability dis-
tribution (Taskar et al, 2003):
P (y|x) ? ?
(i,j)?E
?ij(x, yi, yj) = exp(w>?(x, y))
(2)
where ?(x, y) = [?1?2 . . . ?|?|?trans]> is
used to learn a weight vector w. ?k =?n
i=1 ?i(x)I(yi = k),?k ? {1, 2, . . . , |?|} and
?trans = [c11c12 . . . cTT ]> where cij is the num-
ber of observed transitions from the ith alphabet
to the jth alphabet in ?.
Similar to SVMs (Vapnik, 1995),M3N tries to
find a projection to maximize the margin ?. On
the other hand, M3N also attempts to minimize
?w? to minimize the generalization error. Sup-
pose ?tx(y) = ?ni=1 ?tx(yi) =
?n
i=1 I(yi 6=
(t(x))i) where t((x))i is the true label of the ith
sequence xi, and ??x(y) = ?(x, t(x)) ? ?(x, y)
where t(x) is the true label of the observation se-
quence x. We can get a quadratic program (QP)
using a standard transformation to eliminate ? as
follows:
min 12?w?
2; (3)
s.t. w>??x(y) ? ?tx(y),?x ? S,?y ? ?.
However, the sequence data is often not separa-
ble by the defined hyperplane. In such cases, we
can introduce slack variables ?x which are guaran-
teed to be non-negative to allow some constraints.
Thus the complete primal form of the optimiza-
tion problem can be formulated by:
min 12?w?
2 + C
?
x
?x; (4)
s.t. w>??x(y) ? ?tx(y)? ?x,?x ? S,?y ? ?.
where C is called the capacity in the support vector
literature and presents a way to trade-off the train-
ing error and margin size. One should note that the
number of constraints is?Ti=1 |?i|, an extremely
large number. And the corresponding dual formu-
1409
lation can be defined as:
max
?
x,y
?x(y)?tx(y)? 12?
?
x,y
?x(y)??x(y)?2;
s.t.
?
y
?x(y) = C,?x;?x(y) ? 0,?x, y. (5)
where ?x(y) is a dual variable.
As well as loss functions, kernels might have
substantial influence on the performance of a clas-
sification system. M3N is capable of incorpo-
rating many different kinds of kernel functions to
reduce computations in the high-dimensional fea-
ture space H. This is sometimes referred to as
the ?kernel trick? (Scho?lkopf and Smola, 2002;
Shawe-Taylor and Cristianini, 2004). A linear
kernel can be defined as
?((x, y), (x?, y?)) = ??(x, y), ?(x?, y?)?H (6)
For a polynomial kernel,
?((x, y), (x?, y?))
= (s ? ??(x, y), ?(x?, y?)?H + r)d, (7)
and for a neural kernel,
?((x, y), (x?, y?))
= tanh(s ? ??(x, y), ?(x?, y?)?H + r), (8)
where s, d, and r are coefficients in kernel func-
tions.
3 Stochastic Gradient Descent
For M3N optimization, Taskar et al (2003) has
proposed a reparametrization of the dual variables
to take advantage of the network structure of the
labeling sequence problem. The dual QP is then
solved using the structured sequential minimal
optimization (structured SMO) analogous to the
SMO used for SVMs (Platt, 1998). However, the
resulting number of constraints in the QP make
the structured SMO algorithm slow to converge,
or even prohibitively expensive for large-scale real
world problems. In this section we will present
stochastic gradient descent (SGD) method, and
show SGD can significantly speedup the training
ofM3N .
3.1 Regularized Loss Minimization
Recall that for M3N , the goal is to find
a linear hypothesis hw such that hw(x) =
argmaxy?? w>?(x, y). The parameters w are
learned by minimizing a regularized loss
L(w; {(xi, yi)}Ti=1, C) =
m?
i=1
`(w, xi, yi)+C2 ?w?
2.
(9)
The function `measures the loss incurred in us-
ing w to predict the label of xi. Following (Taskar
et al, 2003), `(w, xi, yi) is a variant of the hinge
loss, and can be defined as follows:
`(w, xi, yi) = maxy?? [e(xi, yi, y)
? w ? (?(xi, yi)? ?(xi, y))], (10)
where e(xi, yi, y) is some non-negative measure
of the error incurred in predicting y instead of yi
as the label of xi. We assume that e(xi, yi, y) = 0
for all i, so that no loss is incurred for correct
prediction, and therefore `(w, xi, yi) is always
non-negative. This loss function corresponds to
the M3N approach, which explicitly penalizes
training examples for which, for some y 6= yi,
w ? (?(xi, yi) ? ?(xi, y)) < e(xi, yi, y). And the
function L is convex in w for `(w, xi, yi). There-
fore, minimization of L can be re-cast as opti-
mization of the following dual convex problem:
w? = argmin
w
?
i
maxy?? [e(xi, yi, y)
? w ? (?(xi, yi)? ?(xi, y))] + C2 ?w?
2. (11)
3.2 The SGD Algorithm
To perform parameter estimation, we need to min-
imize L(w; {(xi, yi)}Ti=1, C). For this purpose we
compute its gradient G(w):
G(w) = ??w(L(w; {(xi, yi)}
T
i=1, C))
= ??w(
m?
i=1
`(w, xi, yi) + C2 ?w?
2) (12)
In addition to the gradient, second-order meth-
ods based on Newton steps also require computa-
tion and inversion of the Hessian H(w). Taking
1410
the gradient of Equation 12 wrt. w yields:
H(w) = ??wG(w) =
?2
?w2L (13)
Explicitly computing the full Hessian is time
consuming. Instead we can make use of the dif-
ferential
dG(w) = H(w)dw (14)
to efficiently compute the product of the Hessian
with a chosen vector v =: dw by forward-mode
algorithmic differentiation (Pearlmutter, 1994).
These Hessian-vector products can be computed
along with the gradient at only 2-3 times the
cost of the gradient computation alone. We de-
note G(w) = ?wL, and each iteration of the
SGD algorithm consists in drawing an example
(xi, yi) at random and applying the parameter up-
date rule (Robbins and Monroe, 1951):
wt+1 ? wt ? ? ? ?wL (15)
where ? is the learning rate in the algorithm.
The SGD algorithm has been shown to be fast,
reliable, and less prone to reach bad local minima.
In this algorithm, the weights are updated after
the presentation of each example, according to the
gradient of the loss function (Lecun et al, 1998).
The convergence is very fast when the training ex-
amples are redundant since only a few examples
are needed to perform. This algorithm can get a
good estimation after considerably few iterations.
3.3 Choosing Learning Rate ?
The learning rate ? is crucial to the speed of
SGD algorithm. Ideally, each parameter weight
wi should have its own learning rate ?i. Because
of possible correlations between input variables,
the learning rate of a unit should be inversely pro-
portional to the square root of the number of in-
puts to the unit. If shared weights are used, the
learning rate of a weight should be inversely pro-
portional to the square root of the number of con-
nection sharing that weight.
For one-dimensional sequence labeling task,
the optimal learning rate yields the fastest conver-
gence in the direction of highest curvature is (Bot-
tou, 2004):
?opt = (
?2L
?w2 )
?1 = (H(w))?1, (16)
and the maximum learning rate is ?max = 2?opt.
The simple SGD update offers lots of engineer-
ing opportunities. In practice, however, at any mo-
ment during the training procedure, we can select
a small subset of training examples and try vari-
ous learning rates on the subset, then pick the one
that most reduces the cost and use it on the full
dataset.
3.4 The SGD Convergence
The convergence of stochastic algorithms actually
has been studied for a long time in adaptive signal
processing. Given a suitable choice of the learn-
ing rate ?t, the standard (batch) gradient descent
algorithm is known to converge to a local mini-
mum of the cost function. However, the random
noise introduced by SGD disrupts this determinis-
tic picture and the specific study of SGD conver-
gence usually is fairly complex (Benveniste et al,
1987).
It is reported that for the convex case, if sev-
eral assumptions and conditions are valid, then
the SGD algorithm converges almost surely to the
optimum w? 1. For the general case where the
cost function is non-convex and has both local
and global minima, if four assumptions and two
learning rate assumptions hold, it is guaranteed
that the gradient ?wL converges almost surely to
zero (Bottou, 2004). We omit the details of the
convergence theorem and corresponding proofs
due to space limitation.
3.5 SGD Speedup
Unfortunately, many of sophisticated gradient
methods are not robust to noise, and scale badly
with the number of parameters. The plain SGD
algorithm can be very slow to converge. Inspired
by stochastic meta-descent (SMD) (Schraudolph,
1999), the convergence speed of SGD can be fur-
ther improved with gradient step size adaptation
by using second-order information. SMD is a
highly scalable local optimizer. It shines when
gradients are stochastically approximated.
In SMD, the learning rate ? is simultaneously
1One may argue that SGD on many architectures does
not result in a global optima. However, our goal is to obtain
good performance on future examples in learning rather than
achieving a global optima on the training set.
1411
INPUT: training set S {(x1, y1), . . . , (xT, yT)};
factor ?; number of iterations N .
INITIALIZE: w0, v0 = 0, ?0.
FOR t = 1, 2, . . . , N
Choose a random example (xi, yi) ? S
Compute the gradient ?t = Gt and Htvt
Set vt+1 = ?vt ? ?t ? (Gt + ?Htvt)
Update the parameter vector:
wt+1 ? wt ? ?t ? ?t
Adapt the gradient step size:
?t+1 = ?t ?max(12 , 1? ?Gt+1 ? vt+1)OUTPUT: wN+1
Figure 1: Pseudo-code for the SGD algorithm.
adapted via a multiplicative update with ?:
?t+1 = ?t ?max(
1
2 , 1? ?Gt+1 ? vt+1), (17)
where the vector v (v =: dw) captures the long-
term dependencies of parameters. v can be com-
puted by the simple iterative update:
vt+1 = ?vt ? ?t ? (Gt + ?Htvt), (18)
where the factor 0 ? ? ? 1 governs the time scale
over which long-term dependencies are taken into
account, and Htvt can be calculated efficiently
alongside the gradient by forward-mode algorith-
mic differentiation via Equation 14. This Hessian-
vector product is computed implicitly and it is the
key to SMD?s efficiency. The pseudo-code for the
SGD algorithm is shown in Figure 1.
4 Experiments: A Case Study of NP
Chunking
4.1 Data
Our data comes from the CoNLL 2000 shared task
(Sang and Buchholz, 2000). The dataset is di-
vided into a standard training set of 8,936 sen-
tences and a testing set of 2,012 sentences. This
data consists of the same partitions of the Wall
Street Journal corpus (WSJ) as the widely used
data for NP chunking: sections 15-18 as training
data (211,727 tokens) and section 20 as test data
(47,377 tokens). And the annotation of the data
has been derived from the WSJ corpus.
wt?? = w
wt matches [A-Z]
wt matches [A-Z]+
wt matches [A-Z][a-z]+
wt matches [A-Z]+[a-z]+[A-Z]+[a-z]
wt matches .*[0-9].*
wt contains dash ?-? or dash-based ?-based?
wt is capitalized, all-caps, single capital letter,
or mixed capitalization
wt contains years, year-spans or fractions
wt is contained in a lexicon of words with POS
p (from the Brill tagger)
pt = p
qk(x, t+ ?) for all k and ? ? [?3, 3]
Table 1: Input feature template qk(x, t) for NP
chunking. In this table wt is the token (word) at
position t, pt is the POS tag at position t, w ranges
over all words in the training data, and p ranges
over all POS tags.
4.2 Features
We follow some top-performing NP chunking sys-
tems and perform holdout methodology to design
features for our model, resulting in a rich feature
set including POS features provided in the official
CoNLL 2000 dataset (generated by the Brill tag-
ger (Brill, 1995), with labeling accuracy of around
95-97%), some contextual and morphological fea-
tures. Table 1 lists our feature set for NP chunk-
ing.
4.3 Experimental Results
We trained linear-chain conditional random fields
(CRFs) (Lafferty et al, 2001) as the baseline. The
well known limited memory quasi-Newton BFGS
algorithm (L-BFGS) (Liu and Nocedal, 1989) was
applied to learn the parameters for CRFs. To
avoid over-fitting, we penalized the log-likelihood
by the commonly used zero-mean Gaussian prior
over the parameters. This gives us a competitive
baseline CRF model for NP chunking. To make
fair and accurate comparison, we used the same
set of features listed in Table 1 for bothM3N and
CRFs. All experiments were performed on the
Linux platform, with a 3.2GHz Pentium 4 CPU
and 4 GB of memory.
1412
Model Training Method Kernel Function Iteration Training Time(s) P(%) R(%) F?=1
M3N structured SMO linear kernel: ?a, b?H 100 1176 94.59 94.22 94.40
M3N structured SMO polynomial(quadratic): (?a, b?H + 1)2 100 30792 94.88 94.49 94.68
M3N structured SMO polynomial(cubic): (?a, b?H + 1)3 100 30889 94.47 94.01 94.24
M3N structured SMO polynomial(biquadratic): (?a, b?H + 1)4 100 31556 93.90 93.77 93.83
M3N structured SMO neural kernel: tanh(0.1 ? ?a, b?H) 20 7395 94.42 94.02 94.22
CRFs L-BFGS ? 100 352 94.55 94.09 94.32
Table 2: M3N vs. CRFs: Performance and training time comparison for NP chunking on the CoNLL
2000 official dataset. M3N was trained using the structured SMO algorithm.
Model Training Method Kernel Function Iteration Training Time(s) P(%) R(%) F?=1
M3N SGD linear kernel: ?a, b?H 100 89 94.58 94.21 94.39
M3N SGD polynomial(quadratic): (?a, b?H + 1)2 100 1820 94.89 94.50 94.69
M3N SGD polynomial(cubic): (?a, b?H + 1)3 100 1831 94.47 94.01 94.24
M3N SGD polynomial(biquadratic): (?a, b?H + 1)4 100 1857 93.91 93.76 93.83
M3N SGD neural kernel: tanh(0.1 ? ?a, b?H) 20 477 94.40 94.01 94.20
CRFs L-BFGS ? 100 352 94.55 94.09 94.32
Table 3: M3N vs. CRFs: Performance and training time comparison for NP chunking on the CoNLL
2000 official dataset. M3N was trained using the SGD algorithm.
System F?=1
SVMs (polynomial kernel) (Kudo and Mat-
sumoto, 2000)
93.79
SVM combination (Kudo and Matsumoto,
2001)
94.39
Generalized winnow (Zhang et al, 2002) 94.38
Voted perceptron (Collins, 2002) 94.09
CRFs (Sha and Pereira, 2003) 94.38
Second order CRFs (McDonald et al, 2005) 94.29
Chunks from the Charniak Parser (Holling-
shead et al, 2005)
94.20
Second order latent-dynamic CRFs + improved
A* search based inference (Sun et al, 2008)
94.34
Our approach 94.69
Table 4: NP chunking: Comparison with some ex-
isting state-of-the-art systems.
Similar to other discriminative graphical mod-
els such as CRFs, the modeling flexibility of
M3N permits the feature functions to be com-
plex, arbitrary, nonindependent, and overlapping
features, allowing the multiple features described
in Table 1 to be directly exploited. Moreover,
M3N is capable of incorporating multiple kernel
functions (see Section 2) which allow the efficient
use of high-dimensional feature spaces during the
experiments.
The resulting number of features is 7,835,439,
and both M3N and CRFs were trained to predict
47,366 tokens with 12,422 noun phrases in the
testing set. For simplicity, we denote a = ?(x, y),
and b = ?(x?, y?), and the linear kernel can be
rewritten as ?(a, b) = ?a, b?H. We performed
holdout methodology to find optimal values for
coefficients s, d, and r in M3N kernel functions.
For polynomial kernels, we varied d from 2 to 4,
resulting in quadratic, cubic, and biquadratic ker-
nels, respectively. Finally, we chose optimized
values: s = 1, r = 1 for polynomial kernels, and
s = 0.1, r = 0 for neural kernels. The capacity C
forM3N was set to 1 in our experiments.
Table 2 shows comparative performance and
training time for M3N (trained with structured
SMO) and CRFs, while Table 3 shows compar-
ative performance and training time for M3N
(trained with SGD) and CRFs 2. ForM3N , when
trained with quadratic kernel and structured SMO,
the best F-measure of 94.68 was achieved, leading
to an improvement of 0.36 compared to the CRF
baseline. What follows is the linear kernel that
obtained 94.40 F-measure. The cubic and neu-
ral kernels obtained close performance, while the
biquadratic kernel led to the worst performance.
However, the structured SMO is very computa-
tionally intensive, especially for polynomial ker-
nels. For example, CRFs converged in 352 sec-
2We used Taku Kudo?s CRF++ toolkit (available at
http://crfpp.sourceforge.net/) in our experiments. The M3N
model, and the structured SMO and SGD training algorithms
were also implemented using C++.
1413
-10 0 10 20 30 40 50 60 70 80 90 100
-240
-220
-200
-180
-160
-140
-120
-100
-80
O
b
j
e
c
t
i
v
e
 
f
u
n
c
t
i
o
n
 
v
a
l
u
e
Iteration
(a)
 M
3
N, structured SMO
 M
3
N, SGD
-10 0 10 20 30 40 50 60 70 80 90 100
-045
-040
-035
-030
-025
O
b
j
e
c
t
i
v
e
 
f
u
n
c
t
i
o
n
 
v
a
l
u
e
Iteration
(E)
 M
3
N, structured SMO
 M
3
N, SGD
-2 0 2 4 6 8 10 12 14 16 18 20
-220
-200
-180
-160
-140
-120
-100
O
b
j
e
c
t
i
v
e
 
f
u
n
c
t
i
o
n
 
v
a
l
u
e
Iteration
(F)
 M
3
N, structured SMO
 M
3
N, SGD
Figure 2: Convergence speed comparison for structured SMO and SGD algorithms. The X axis shows
number of training iterations, and the Y axis shows objective function value. (a) TheM3N model was
trained using linear kernel. (b) The M3N model was trained using polynomial(quadratic) kernel. (c)
TheM3N model was trained using neural kernel.
onds, whileM3N (polynomial kernels) took more
than 8.5 hours to finish training.
As can be seen in Table 3, the SGD algorithm
significantly accelerated the training procedure of
M3N without sacrificing performance. When the
linear kernel was used, M3N finished training in
89 seconds, more than 13 times faster than the
model trained with structured SMO. And it is even
much faster than the CRF model trained with L-
BFGS. More importantly, SGD obtained almost
the same performance as structured SMO with all
M3N kernel functions.
Table 4 gives some representative NP chunking
results for previous work and for our best model
on the same dataset. These results showed that our
model compares favorably with existing state-of-
the-art systems 3.
Figure 2 compares the convergence speed of
structured SMO and SGD algorithms for the
M3N model. Linear (Figure 2 (a)), polyno-
mial(quadratic) (Figure 2 (b)) and neural kernels
(Figure 2 (c)) were used 4. We calculated objec-
tive function values during effective training iter-
ations. It can be seen that both structured SMO
and SGD algorithms converge to the same objec-
tive function value for different kernels, but SGD
converges considerably faster than the structured
SMO.
Figure 3 (a) demonstrates the effect of training
set size on performance for NP chunking. We
3Note that it is difficult to compare strictly, since reported
results sometimes leave out details (e.g., feature sets, signifi-
cance tests, etc) needed for accurate comparison.
4For cubic and biquadratic kernels, the curves are very
similar to that of quadratic kernel, and we omitted them for
space.
increased the training set size from 1,000 sen-
tences to 8,000 sentences, with an incremental
step of 1,000. And the testing set was fixed to
be 2,012 sentences. The M3N models (with dif-
ferent kernels) were trained using the SGD algo-
rithm. It is particularly interesting to know that
the performance boosted for all the models when
increasing the training set size. Using linear and
quadratic kernels, M3N model significantly and
consistently outperforms the CRF model for dif-
ferent training set sizes. The cubic and neural
kernels lead to almost the same performance for
M3N , which is slightly lower than the CRF base-
line. As illustrated by the curves, M3N (trained
with quadratic kernel) achieved the best perfor-
mance and larger training set size leads to better
improvement for this model when compared to the
CRFmodel, whileM3N (trained with biquadratic
kernel) obtained the worst performance among all
the models.
Accordingly, Figure 3 (b) shows the impact of
increasing the training set size on training time for
NP chunking. Increasing training set size leads
to an increase in the computational complexity of
training procedure for all models. For the M3N
model, it is faster when trained with linear kernel
than the CRF model. And the three polynomial
kernels (quadratic, cubic and biquadratic) have
roughly the same training time. For CRFs and
(M3N , neural kernel), the training time is close
to each other. For example, when the training
set contains 1,000 sentences, the training time for
CRFs, (M3N , linear kernel), (M3N , quadratic
kernel), (M3N , cubic kernel), (M3N , biquadratic
kernel), and (M3N , neural kernel) is 24s, 7s, 72s,
1414
1N 2N 3N 4N 5N 6N 7N 8N 9N
905
910
915
920
925
930
935
940
945
950
)
-
P
e
a
s
u
r
e
NuPber of traininJ sentences
(a)
 &5)s
 M
3
N, linear Nernel 
 M
3
N, Tuadratic Nernel
 M
3
N, cubic Nernel
 M
3
N, biTuadratic Nernel
 M
3
N, neural Nernel
1N 2N 3N 4N 5N 6N 7N 8N 9N
0
200
400
600
800
1000
1200
1400
1600
1800
2000
7
r
a
i
n
i
n
J
 
t
i
P
e
 

s

NuPber of traininJ sentences
(E)
 &5)s
 M
3
N, linear Nernel 
 M
3
N, Tuadratic Nernel
 M
3
N, cubic Nernel
 M
3
N, biTuadratic Nernel
 M
3
N, neural Nernel
Figure 3: (a) Effect of training set size on performance for NP chunking. The training set size was
increased from 1,000 sentences to 8,000 sentences, with an incremental step of 1,000. The testing set
contains 2,012 sentences. All the M3N models (with different kernels) were trained using the SGD
algorithm. (b) Effect of training set size on training time for NP chunking.
72s, 74s, and 30s. When trained on 8,000 sen-
tences, the numbers become 336s, 79s, 1679s,
1689s, 1712s, and 411s, respectively.
5 Related Work
The M3N framework and its variants have gen-
erated much interest and great progress has been
made, as evidenced by their promising results
evaluated in handwritten character recognition,
collective hypertext classification (Taskar et al,
2003), parsing (Taskar et al, 2004), and XML
tag relabeling (Spengler, 2005). However, all the
above mentioned research work used structured
SMO algorithm for parameter learning, which can
be computationally intensive, especially for very
large datasets.
Recently, similar stochastic gradient methods
have been applied to train log-linear models such
as CRFs (Vishwanathan et al, 2006). However,
the maximum margin loss has a discontinuity in
its derivative, making optimization of such models
somewhat more involved than log-linear ones. We
first exploit SGD method for fast parameter learn-
ing of M3N and achieve state-of-the-art perfor-
mance on the NP chunking task in the NLP com-
munity.
Several algorithms have been proposed to
train max-margin models, including cutting plane
SMO (Tsochantaridis et al, 2005), exponenti-
ated gradient (Bartlett et al, 2004; Collins et al,
2008), extragradient (Taskar et al, 2006), and
subgradient (Shalev-Shwartz et al, 2007). Some
methods are similar to SGD in that they all pro-
cess a single training example at a time. The
SGD methods directly optimize the primal prob-
lem, and at each update use a single example to
approximate the gradient of the primal objective
function. Some of the proposed algorithms, such
as exponentiated gradient corresponds to block-
coordinate descent in the dual, and uses the exact
gradient with respect to the block being updated.
We plan to implement and compare some of these
algorithms with SGD forM3N .
6 Conclusion and Future Work
We have presented the first known empirical study
on sequence labeling based on M3N . We have
also provided the efficient SGD algorithm and
shown how it can be applied to significantly
speedup the training procedure of M3N . As a
case study, we performed extensive experiments
on standard dataset for NP chunking, showing the
promising and competitiveness of our approach.
Several interesting issues, such as the convergence
speed of the SGD algorithm, the effect of train-
ing set size on performance for NP chunking, and
the effect of training set size on training time,
were also investigated in our experiments. For
the future work, we plan to further the scalability
and applicability of our approach and evaluate it
on other large-scale real world sequence labeling
tasks, such as POS tagging and NER.
1415
References
Peter L. Bartlett, Ben Taskar, Michael Collins, and David
Mcallester. Exponentiated gradient algorithms for large-
margin structured classification. In Proceedings of NIPS-
04, pages 113?120. MIT Press, 2004.
A. Benveniste, M. Metivier, and P. Priouret. Algorithmes
adaptatifs et approximations stochastiques. Masson,
1987.
Le?on Bottou. Stochastic learning. In Olivier Bousquet and
Ulrike von Luxburg, editors, Advanced Lectures on Ma-
chine Learning, Lecture Notes in Artificial Intelligence,
LNAI 3176, pages 146?168. Springer Verlag, Berlin,
2004.
Eric Brill. Transformation-based error-driven learning and
natural language processing: A case study in part-of-
speech tagging. Computational Linguistics, 21(4):543?
565, 1995.
Michael Collins, Amir Globerson, Terry Koo, Xavier Car-
reras, and Peter L. Bartlett. Exponentiated gradient al-
gorithms for conditional random fields and Max-margin
Markov networks. Journal of Machine Learning Re-
search, 9:1775?1822, 2008.
Michael Collins. Discriminative training methods for hidden
Markov models: Theory and experiments with perceptron
algorithms. In Proceedings of HLT/EMNLP-02, pages 1?
8, 2002.
Kristy Hollingshead, Seeger Fisher, and Brian Roark.
Comparing and combining finite-state and context-free
parsers. In Proceedings of HLT/EMNLP-05, pages 787?
794, Vancouver, British Columbia, Canada, 2005.
Taku Kudo and Yuji Matsumoto. Use of support vector learn-
ing for chunk identification. In Proceedings of CoNLL-
2000 and LLL-2000, pages 142?144, Lisbon, Portugal,
2000.
Taku Kudo and Yuji Matsumoto. Chunking with support vec-
tor machines. In Proceedings of HLT/NAACL-01, pages
1?8, 2001.
John Lafferty, Andrew McCallum, and Fernando Pereira.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
ICML-01, pages 282?289, 2001.
Yann Lecun, Le?on Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278?2324,
Nov 1998.
Dong C. Liu and Jorge Nocedal. On the limited memory
BFGS method for large scale optimization. Mathematical
Programming, 45:503?528, 1989.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
Flexible text segmentation with structured multilabel clas-
sification. In Proceedings of HLT/EMNLP-05, pages 987?
994, Vancouver, British Columbia, Canada, 2005.
Barak A. Pearlmutter. Fast exact multiplication by the Hes-
sian. Neural Computation, 6(1):147?160, 1994.
John C. Platt. Fast training of support vector machines us-
ing sequential minimal optimization. Advances in Kernel
Methods: Support Vector Learning, pages 41?64, 1998.
H. Robbins and S. Monroe. A stochastic approximation
method. Annals of Mathematical Statistics, 22:400?407,
1951.
Erik Tjong Kim Sang and Sabine Buchholz. Introduction to
the CoNLL-2000 shared task: Chunking. In Proceedings
of CoNLL-2000, pages 127?132, Lisbon, Portugal, 2000.
Bernhard Scho?lkopf and Alexander J. Smola. Learning with
Kernels. MIT Press, Cambridge, MA, 2002.
Nicol N. Schraudolph. Local gain adaptation in stochas-
tic gradient descent. In Proceedings of the 9th Interna-
tional Conference on Artificial Neural Networks, pages
569?574, 1999.
Fei Sha and Fernando Pereira. Shallow parsing with condi-
tional random fields. In Proceedings of HLT/NAACL-03,
pages 213?220, 2003.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pe-
gasos: Primal estimated sub-GrAdient SOlver for SVM.
In Proceedings of ICML-07, pages 807?814, New York,
NY, USA, 2007.
John Shawe-Taylor and Nello Cristianini. Kernel Methods
for Pattern Analysis. Cambridge University Press, Cam-
bridge, UK, 2004.
Alex Spengler. Maximum margin Markov networks for
XML tag relabelling. Master?s thesis, University of Karl-
sruhe, 2005.
Xu Sun, Louis-Philippe Morency, Daisuke Okanohara, and
Jun?ichi Tsujii. Modeling latent-dynamic in shallow pars-
ing: A latent conditional model with improved inference.
In Proceedings of COLING-08, pages 841?848, Manch-
ester, UK, 2008.
Ben Taskar, Carlos Guestrin, and Daphne Koller. Max-
margin Markov networks. In Proceedings of NIPS-03.
MIT Press, 2003.
Ben Taskar, Dan Klein, Michael Collins, Daphne Koller, and
Christopher Manning. Max-margin parsing. In Proceed-
ings of HLT/EMNLP-04, pages 1?8, 2004.
Ben Taskar, Vassil Chatalbashev, Daphne Koller, and Carlos
Guestrin. Learning structured prediction models: A large
margin approach. In Proceedings of ICML-05, pages 896?
903, Bonn, Germany, 2005.
Ben Taskar, Simon Lacoste-Julien, and Michael I. Jordan.
Structured prediction via the extragradient method. In
Proceedings of NIPS-06. MIT Press, 2006.
Ben Taskar. Learning Structured PredictionModels: A Large
Margin Approach. PhD thesis, Stanford University, De-
cember 2004.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, and Yasemin Altun. Large margin methods for
structured and interdependent output variables. Journal
of Machine Learning Research, 6:1453?1484, 2005.
Vladimir N. Vapnik. The Nature of Statistical Learning The-
ory. Springer-Verlag, Inc., New York, USA, 1995.
S. V. N. Vishwanathan, Nicol N. Schraudolph, Mark W.
Schmidt, and Kevin P. Murphy. Accelerated training of
conditional random fields with stochastic gradient meth-
ods. In Proceedings of ICML-06, pages 969?976, Pitts-
burgh, Pennsylvania, 2006.
Tong Zhang, Fred Damerau, and David Johnson. Text chunk-
ing based on a generalization of winnow. Journal of Ma-
chine Learning Research, 2:615?637, 2002.
1416
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 848?859, Dublin, Ireland, August 23-29 2014.
Modeling Mutual Influence Between Social Actions and Social Ties
Xiaofeng YU Junqing XIE
HP Labs China
Universal Business Park
10 Jiu XianQiao Road, Chaoyang District, Beijing, China
{xiaofeng.yu,jun-qing.xie}@hp.com
Abstract
In online social media, social action prediction and social tie discovery are two fundamental tasks
for social network analysis. Traditionally, they were considered as separate tasks and solved inde-
pendently. In this paper, we investigate the high correlation and mutual influence between social
actions (i.e. user-behavior interactions) and social ties (i.e. user-user connections). We propose a
unified coherent framework, namely mutual latent random graphs (MLRGs), to flexibly encode
evidences from both social actions and social ties. We introduce latent, or hidden factors and
coupled models with users, users? behaviors and users? relations to exploit mutual influence and
mutual benefits between social actions and social ties. We propose a gradient based optimization
algorithm to efficiently learn the model parameters. Experimental results show the validity and
competitiveness of our model, compared to several state-of-the-art alternative models.
1 Introduction
With the dramatically rapid growth and great success of many large-scale online social networking ser-
vices, social media bridge our daily physical life and the virtual Web space. Popular social media sites
(e.g., Facebook and Twitter) and mobile social networks (e.g., Foursquare) have gathered billions of act-
ing users and are still attracting millions of newbies everyday. Modeling social actions and social ties
are two fundamental tasks in online social media. Social actions are the users? activities or behaviors
in socially connected networks. For example, a social action can be ?posting a tweet? on Twitter or the
?check-in? behavior on Foursquare. A social tie or social relation is referred to any relationship between
two or more individual users in a social network, such as the friend and colleague relationships. By
understanding a user?s behaviors and accordingly exploiting potentially interesting services to her/him,
one can improve the user?s experience and boost the revenue of social media sites. Also, precise social
tie prediction will help people tap into the wisdom of crowds, to aid in making more informed decisions.
Since individual users are socially connected, social influence occurs through information diffusion
in social networks. Social influence happens when one?s opinions or behaviors are affected by others.
It is well known that different types of social ties have essentially different influence on social actions.
Intuitively, a user?s trusted friends on the web affect that user?s online behavior. Ma et al. (2009) and Ma
et al. (2011) claimed that one user?s final behavior decision is the balance between his/her own taste and
her/his trusted friends? favors. On the other hand, social actions also have important influence on social
ties. Obviously, users with similar preferences or behaviors are more likely to be friends than others in
social media. Users with momentous activities will attract many other users to be connected with. On
the contrary, no body will be interested in users with trivial or insignificant behaviors.
Consequently, we face some very interesting questions: Is there any dynamics or mutual influence
between social actions and social ties? To what extent do they influence each other? A fundamental
mechanism that drives the dynamics of networks is the underlying social phenomenon of homophily
(McPherson et al., 2001): people tend to follow the behaviors of their friends, and people tend to create
relationships with other people who are already similar to them. This suggests that both actions and ties
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
848
are bi-directionally correlated and mutually influenced in social media, they could be mutually reinforced
if modeled jointly.
Inspired by this mechanism, we propose a single unified framework based on exponential-family ran-
dom graph models ( (Frank and Strauss, 1986), (Wasserman and Pattison, 1996)) to exploit homophily
for simultaneous social action prediction and social tie discovery. This mutual latent random graph
(MLRG) framework incorporates shared latent factors with users, users? behaviors and users? relations,
and defines coupled models to encode both social action and social tie information, to capture dynamics
and mutual influence between them. We propose a gradient based algorithm for learning and optimiza-
tion. During the learning procedure, social actions (i.e. user-behavior interactions), social ties (i.e.
user-user connections), and deep dependencies and interactions between them could be efficiently ex-
plored. Experimental results demonstrate that social actions and social ties are highly correlated and
mutually helpful. By coupling actions with ties jointly in a single coherent framework, MLRG achieves
significantly better performance on both social action prediction and social tie inference, compared to
state-of-the-art systems modeling them independently.
2 Related Work
Social network analysis has attracted much interest in both academia and industry recently. Consider-
able research and engineering has been conducted for social media modeling, analytics and optimization,
including social community detection (Fortunato, 2010), user behavior modeling and prediction ( (Ben-
evenuto et al., 2009), (Kwak et al., 2010), (Ma et al., 2009), (Ma et al., 2011)), social tie analysis
( (Tang et al., 2011), (Tang et al., 2012)), social sentiment analysis ( (Wasserman et al., 1994), (Pang
and Lee, 2008)), etc.
Social action prediction and social tie discovery are two fundamental tasks for social media and social
network analysis. Traditionally, they were considered as separate tasks and solved independently without
considering the bidirectional interactions and interdependencies between them. Social action investiga-
tion is essentially important in online social media. Users behaviors could be affected by various kinds of
complex factors, such as users? attributes, users?s historical behaviors, social influence and social network
structures. Based on this motivation, Tan et al. (2010) proposed a noise tolerant time-varying model to
track social actions. Aiming at modeling user actions more accurately and realistically, Ma et al. (2009)
and Ma et al. (2011) considered connections among users and proposed social trust ensemble to fuse
the users? tastes and their trusted friends? favors together. Gao et al. (2013) investigated users? social
behaviors from a spatio-temporal-social aspect in location-based mobile social networks. In particular,
Gao et al. (2013) focused on temporal effects in terms of temporal preferences and temporal correlations,
and modeled temporal cyclic patterns to capture a user?s mobile behavior to investigate correlations to
the spatial context and social context in location-based social networks.
Social tie is the most basic unit to form the network structure. Tang et al. (2011) proposed a semi-
supervised framework, the partially labeled factor graph model to infer the type of social relationships.
The task was formulated as a relationship mining problem to detect the relationship semantics in real-
world networks. Tang et al. (2012) further incorporated social theories and leveraged features based
on those social theories to infer social ties across heterogeneous networks via a transfer learning frame-
work. As can be seen, predicting social actions and inferring social ties were modeled as separate and
independent tasks in the above-mentioned approaches, deep interactions and mutual influence between
them were not taken into consideration. In social media users interact with one another to share the con-
tent they both create and consume. According to the homophily phenomenon, exploring bi-directional
information and mutual influence between them is intuitively appealing.
We are also aware of several research work attempting to explore joint models to capture mutual
benefits and deep dependencies between different tasks in NLP, data mining and information extraction
research communities ( (Ko et al., 2007), (Yu and Lam, 2008), (Yu et al., 2009), (Liu et al., 2009),
(Yu and Lam, 2010b), (Yu and Lam, 2010a), (Yu et al., 2011), (Yu and Lam, 2012), (Zeng et al.,
2013)). In general, joint models aim to handle multiple hypotheses and uncertainty information and to
predict many variables simultaneously such that subtasks can aid each other to boost the performance.
849
Ko et al. (2007) proposed a joint answer ranking framework based on probabilistic graphical models
for question answering. Yu and Lam (2008) proposed an integrated probabilistic and logic approach
based on Markov Logic Networks (MLNs) (Richardson and Domingos, 2006) to encyclopedia relation
extraction. However, this modeling only captures single relation extraction task. Liu et al. (2009)
developed a Bayesian hierarchical approach, the topic-link LDA, to perform topic modeling and author
community discovery for large-scale linked documents in one unified framework. Yu et al. (2009)
integrated two sub-models in a unified framework via Markov chain Monte Carlo (MCMC) sampling
based inference algorithms. This is a loosely coupled model since parameter estimation is performed
separately for the two sub-models. Yu and Lam (2012) further proposed a joint model incorporating
probabilistic graphical models and first-order logic for information extraction. This joint model exploits
structured variational approximation for tractable parameter learning. Zeng et al. (2013) presented a
semi-supervised graph-based approach to joint Chinese word segmentation and POS tagging. However,
none of these models has been investigated or applied to social media and social network analysis. We
believe that one major reason could be the problem of high computational complexity, such as Yu et al.
(2009) and Yu and Lam (2012). Since many social network sites contain millions of users, exploiting
such models could be very challenging. Currently, research on building joint approaches is still in the
infancy stage. To the best of our knowledge, there is few systematically study on building joint models
to explore mutual influence for social actions and social ties.
3 Model
In this section we consider both social action prediction and social tie inference in the context of social
media, where evidences for both actions and ties are available. We begin by necessary description of
preliminaries and notations, we then present the mutual latent random graphs (MLRGs) model, upon
which both sources of evidence could be exploited simultaneously to capture their mutual influence. We
also discuss the major difference and superiority of this model against several alternative models.
3.1 Preliminaries and Notations
Let G = (V,E) be a social network graph, where V = {v
1
, v
2
, . . . , v
N
} is the set of |V| = N
users and E = {e
11
, e
12
, . . . , e
M
} ? V ? V is the set of |E| = M connections between users.
Let y = {y
1
, y
2
, . . . , y
N
}(y
i
? Y) be the set of actions associated with N users, and s =
{s
11
, s
12
, . . . , s
M
}(s
ij
? S) be the set of corresponding social tie labels associated with M connec-
tions. The connection e
ij
(1 ? i, j ? N, i ?= j) between v
i
and v
j
might be directed or undirected. To
be consistent, both s
ij
?= s
ji
and s
ij
= s
ji
are valid settings. Given the observed social network data D
constructing the graph G, our goal is to simultaneously detect the most likely types of actions y
?
and ties
s
?
such that both of them are optimized.
The exponential-family random graph models (ERGMs) ( (Frank and Strauss, 1986), (Wasserman and
Pattison, 1996)) take the form of an exponential family as P
y
i
|G
=
?
y
i
?Y
?(y
i
) =
exp{
?
y
i
?Y
??(y
i
)}
?
?
for
the social action y
i
in the social network graph G, where ?(?) is a factor, ? is a vector of parameters, ?(?)
is a p-vector of sufficient statistics, which captures network features of interest, its postulated dependence
structure, or both. Lastly, ?
?
is a normalization function to make all probabilities sum to one. The class
of ERGMs is a popular framework for social network modeling to capture global network characteristics.
3.2 Modeling Social Actions
To characterize the user action y
i
, we assume that for the user v
i
there exist observable attributes or
properties m
i
, such as the user?s registered information and historical actions. Without loss of generality,
we further assume that there exist some hidden, or latent properties x
ij
for v
i
. These properties are
implicit and cannot be observed directly, such as the influence from social ties. Consequently, we denote
the observable factor ?(y
i
, v
i
,m
i
) for observable properties and latent factor ?
h
(y
i
, s
ij
, x
ij
) for hidden
properties, respectively. Given the graph G, the probability distribution of y
i
depends on both observable
and latent factors as:
P
y
i
|G
? ?(y
i
, v
i
,m
i
), P
y
i
|G
? ?
h
(y
i
, s
ij
, x
ij
), P
y
i
|G
? ?(y
i
, v
i
,m
i
)?
h
(y
i
, s
ij
, x
ij
). (1)
850
Figure 1: (a) A social network containing 6 users and 5 social ties. The social action can be active or idle,
and the social tie can be friend, colleague, or family. (b) The three-dimensional graphical representation
of the corresponding MLRG model. We use different lines to represent functions f(?), g(?), and h(?).
The Mutual Latent Random Graph (MLRG) model
?y
i
? Y P
y
i
|G
? ?(y
i
, v
i
,m
i
)?
h
(y
i
, s
ij
, x
ij
)
?s
ij
? S P
s
ij
|(y
i
,G)
? ?
?
(s
ij
, v
i
, v
j
,w
ij
)?
h
(y
i
, s
ij
, x
ij
)
?y
i
? Y, ?s
ij
? S P
(y
i
,s
ij
)|G
? ?(y
i
, v
i
,m
i
)?
h
(y
i
, s
ij
, x
ij
)?
?
(s
ij
, v
i
, v
j
,w
ij
)
This modeling integrates two types of factors for both observable and latent properties. It captures not
only the user-behavior dependencies, but also the influence from social ties, for exploring social actions.
3.3 Modeling Social Ties
To characterize the social tie s
ij
between user pair (v
i
, v
j
), we also assume that there exist observable
properties w
ij
, such as the posterior probability of the social tie s
ij
assigned to (v
i
, v
j
). We denote the
observable factor ?
?
(s
ij
, v
i
, v
j
,w
ij
) for w
ij
. Similarly, we further assume that there exist some latent
properties to incorporate the social action influence on social ties. To be consistent, we still use the
vector x
ij
to represent the latent properties and the latent factor ?
h
(y
i
, s
ij
, x
ij
) to capture the social
action influence on social ties. Note that both x
ij
and ?
h
(y
i
, s
ij
, x
ij
) now play double duties in encoding
social action dependency and social tie connection simultaneously. On the one hand, ?
h
(y
i
, s
ij
, x
ij
)
exploits influence from social ties for modeling social actions. On the other hand, this factor exploits
influence from social actions for modeling social ties. By doing so, the latent factor ?
h
(y
i
, s
ij
, x
ij
) is bi-
directionally coupled, encoding both sources of evidence and exploring mutual influence and dynamics
between social actions and social ties. Such mutual influence and dynamics are crucial and modeling
them often leads to improved performance. Given the user action y
i
and the graph G, we devise the
following model for the probability distribution of s
ij
depending on both observable and latent factors
as:
P
s
ij
|(y
i
,G)
? ?
?
(s
ij
, v
i
, v
j
,w
ij
), P
s
ij
|(y
i
,G)
? ?
h
(y
i
, s
ij
, x
ij
), P
s
ij
|(y
i
,G)
? ?
?
(s
ij
, v
i
, v
j
,w
ij
)?
h
(y
i
, s
ij
, x
ij
). (2)
3.4 Modeling Mutual Influence
The mutual correlation between social actions and social ties advocates joint modeling of both sources
of evidence in a single unified framework. Based on the above descriptions, we define our mutual la-
tent random graph (MLRG) based on exponential-family random graph models (ERGMs) ( (Frank and
Strauss, 1986), (Wasserman and Pattison, 1996)), which have gained tremendous successes in social
network analysis and have even become the current state-of-the-art (Robins et al., 2007). To design
a concrete model, one needs to specify distributions for the dependencies for MLRGs. According to
the celebrated Hammersley-Clifford theory, the joint conditional distribution P
(y
i
,s
ij
)|G
is factorized as a
product of potential functions over all cliques in the graph G and we summarize the MLRG in the above
table. In summary, our model consists of three factors: the factor ?(y
i
, v
i
,m
i
) measuring dependencies
851
of the social action y
i
conditioned on G, the factor ?
?
(s
ij
, v
i
, v
j
,w
ij
) measuring the social tie s
ij
be-
tween two arbitrary users v
i
and v
j
in G, and the latent factor ?
h
(y
i
, s
ij
, x
ij
) exploiting mutual influence
between the social action y
i
and social tie s
ij
.
The three factors ?(?), ?
h
(?), and ?
?
(?) can be instantiated in different ways. In this paper, each factor
is defined as the exponential family of an inner product over sufficient statistics (feature functions) and
corresponding parameters. Each factor is a clique template whose parameters are tied. More specifically,
we define these factors as
?(y
i
, v
i
,m
i
) =
1
Z
?
exp{
?
y
i
?Y
?f(y
i
, v
i
,m
i
)}, ?
h
(y
i
, s
ij
, x
ij
) =
1
Z
?
exp{
?
y
i
?Y,s
ij
?S
?g(y
i
, s
ij
, x
ij
)},
?
?
(s
ij
, v
i
, v
j
,w
ij
) =
1
Z
?
exp{
?
s
ij
?S
?h(s
ij
, v
i
, v
j
,w
ij
)}, (3)
where ?, ?, and ? are real-valued weighting vectors and f(?), g(?), and h(?) are corresponding vectors of
feature functions.
We denote ? = {?, ?, ?} as the set of model?s parameters, and concatenate all factor functions as
?q(y
i
, s
ij
) = ?f(y
i
, v
i
,m
i
) + ?g(y
i
, s
ij
, x
ij
) + ?h(s
ij
, v
i
, v
j
,w
ij
), the joint probability distribution
shown in the above table can be rewritten as
P
(y,s)|G
=
?
y
i
?Y,s
ij
?S
?(y
i
, s
ij
) =
1
Z
exp{
?
y
i
?Y,s
ij
?S
?q(y
i
, s
ij
)}, (4)
where Z = Z
?
Z
?
Z
?
is the partition function of our MLRG model.
Figure 1 shows an example social network (V = {v
1
, v
2
, v
3
, v
4
, v
5
, v
6
}, Y = {active, idle}, S =
{friend, colleague, family}) and the corresponding 3D graphical representation of the MLRG model.
The functions f(?) model dependencies of social actions in the bottom part, and the functions h(?) model
dependencies of social ties in the upper part. More importantly, the functions g(?) capture mutual in-
fluence and dependencies between social actions and social ties. As we will see, this modeling offers a
natural formalism for exploiting bi-directional dependencies and interactions between social actions and
social ties to capture their mutual influence, as well as a great flexibility to incorporate a large collection
of arbitrary, overlapping and nonindependent features.
3.5 Discussion
Noticeably, our proposed MLRG model is essentially different from the standard exponential-family
random graph models (ERGMs) and the prior models discussed in Section 2 mainly in two aspects.
Firstly, compared to the standard ERGMs, the MLRG model defines latent factors to assume mutual and
dynamical interaction between social ties and social actions. Secondly, compared to the prior models
such as (Ma et al., 2009) and (Tang et al., 2011), MLRG provides a single unified framework to address
both social action prediction and social tie inference simultaneously while enjoying the benefits of both
sources of evidence.
Importantly, we give an analytical explanation on the mutual nature of our model in terms of a random
walk (Lov?asz, 1996) perspective. A random walk on the graph G is a reversible Markov chain on the
vertexes V. The social influence propagation procedure occurs through information diffusion in the
social graph G. More specifically, a user v
i
will propagate his/her influence to other related users, and
will propagate more to the user which has a stronger relation (e.g., friendship) with v
i
. The influence
propagation will stop when the social graph reaches an equilibrium state, in which both social actions
and social ties are mutually reinforced. Interestingly, this process is consistent with the homophily
phenomenon that a user in the social network tends to be similar to his/her connected neighbors.
4 Learning and Inference
4.1 Mutual Optimization
The goal of learning MLRG model is to estimate a parameter configuration ? = {?, ?, ?} such that the
log-likelihood of observation is maximized. We define the log-likelihood objective function O(?) of the
852
Algorithm 1: The Mutual Gradient Descent (MGD) algorithm
Input: The social graph G, number of iterations n, and the learning rate ?.
Output: Optimized parameters ?
?
= {?
?
, ?
?
, ?
?
}.
while equilibrium states or a threshold number of iterations are not reached do
repeat
Choose a random example (y
i
, s
ij
) ? G as a sample;
Optimize social action parameters ? and ?:
Compute the approximated gradients
?O
?
??
and
?O
?
??
according to Eq. (6), Eq. (7) and stochastic
approximation;
Update ? and ? with learning rate ?: ?? ?? ? ?
?O
?
??
, ? ? ? ? ? ?
?O
?
??
.
// Explore social tie influence
Optimize social tie parameters ? and ?:
Compute the approximated gradients
?O
?
??
and
?O
?
??
according to Eq. (8), Eq. (7) and stochastic
approximation;
Update ? and ? with learning rate ?: ? ? ? ? ? ?
?O
?
??
, ? ? ? ? ? ?
?O
?
??
.
// Explore social action influence
until converge;
end
return ?
?
, ?
?
, and ?
?
observation given the graph G as
O(?) = logP
(y,s)|G
? log ?(?) = log[exp{
?
y
i
?Y,s
ij
?S
?q(y
i
, s
ij
)}]? logZ ? log ?(?), (5)
where ?(?) is regularization to reduce over-fitting and a common choice is a spherical Gaussian prior
with mean 0 and covariance ?
2
I . ?(?) =
?
y
i
?Y
?
2
2?
2
+
?
y
i
?Y,s
ij
?S
?
2
2?
2
+
?
s
ij
?S
?
2
2?
2
.
We propose a mutual gradient descent (MGD) algorithm based on the stochastic gradient descent
(SGD) ( (Lecun et al., 1998), (Bottou, 2004)) framework, for estimating the parameters efficiently in
a mutual and collaborative manner. Once we have optimized the social action parameters ? and ?, the
influence and hypotheses of social action can aid the learning of the social tie parameters ? and ?, and
vice versa. As shown in Algorithm 1, ? is coupled parameter vector for both actions and ties, and is
updated twice in each iteration of MGD. By doing so, MGD not only allows learning of social action
parameters to capture social tie influence, but it also optimizes social tie parameters to alleviate social
action influence. This training procedure runs iteratively until converge to boost both the optimization of
social actions and social ties.
Each iteration of the MGD algorithm consists of drawing an example at random and applying param-
eter updates by moving in the direction defined by the stochastically approximated gradient of the loss
function (e.g.,
?O
?
??
). We update each parameter with a learning rate ?. Ideally, each parameter should
have its own learning rate. If shared parameter weights are used, the best learning rate of a weight should
be inversely proportional to the square root of the number of connection sharing that weight (Bottou,
2004). In our MGD implementation, for simplicity we use the same learning rate for all the parameters.
We select a small subset of training data and try various learning rates on the subset, then pick the one
that most reduces the loss and use it on the full dataset. We summarize the partial derivatives of the
log-likelihood function O with respect to the parameter vectors ?, ? and ? as follows:
?O
??
=
?
y
i
?Y
f(y
i
, v
i
,m
i
)?
?
y
i
?Y
f(y
i
, v
i
,m
i
)? P
(y,s)|G
?
?
y
i
?Y
?
?
2
, (6)
?O
??
=
?
y
i
?Y,s
ij
?S
g(y
i
, s
ij
, x
ij
)?
?
y
i
?Y,s
ij
?S
g(y
i
, s
ij
, x
ij
)? P
(y,s)|G
?
?
y
i
?Y,s
ij
?S
?
?
2
, (7)
?O
??
=
?
s
ij
?S
h(s
ij
, v
i
, v
j
,w
ij
)?
?
s
ij
?S
h(s
ij
, v
i
, v
j
,w
ij
)? P
(y,s)|G
?
?
s
ij
?S
?
?
2
. (8)
It is worth noting that the MGD algorithm computes approximations of the gradients, due to the
intractability of the normalizing constant Z in the log-likelihood of our MLRG model. Our proposed
853
MGD algorithm is a generalized extension and it distinguishes from the standard SGD algorithm in two
aspects: (1) MGD optimizes three types of parameters simultaneously, thus MGD is much more general
than SGD, and it is more scalable and applicable to real-world problems. (2) MGD performs mutual
and collaborative optimization to enable mutual influence between social actions and social ties, whereas
SGD does not take such influence into account.
4.2 Complexity Analysis
Given several conditions including a suitable choice of the learning rate and a convex or pseudo-convex
objective function, the MGD algorithm converges almost surely to a global optimum, otherwise it con-
verges almost surely to a local optimum. In our experiments, this algorithm has good performance even
if it does not reach the global optimum. Let D be the number of samples in the social graph G, n be
the number of iterations, and p be the average number of non-zero attributes (features) per sample, the
computational complexity of our MGD algorithm takes O(nDp). As can be seen, this algorithm is com-
putationally efficient, and convergence is very fast when the training examples are redundant since only
a few examples are needed to sample. Furthermore, this algorithm is online and scale sub-linearly with
the amount of training data, making it very attractive for large-scale datasets.
4.3 Inference
The objective of inference is to find the most likely types of actions y
?
and corresponding social tie labels
s
?
, that is, to find (y
?
, s
?
) = argmax
(y,s)
P
(y,s|G)
. The inference procedure is straightforward. Based on
the learned parameters ?
?
= {?
?
, ?
?
, ?
?
}, we firstly predict the label of each social action y
i
by finding
a labeling assignment that maximizes P
y
i
|G
as y
?
i
= argmax
y
i
?Y
P
y
i
|G
. We then infer the social tie label
s
ij
such that s
?
ij
= argmax
s
ij
?S
P
s
ij
|(y
i
,G)
.
5 Experiments
5.1 Foursquare Data
We crawled one dataset from Foursquare
1
, a popular location-based mobile social networking site for
mobile devices (e.g., smartphones) for our experimental evaluation. Foursquare allows a user to check
in at a physical location via his cellphone, and then let his online friends know where he is by publishing
such check-in action online. Users check-in at venues using a mobile website, text messaging or a device-
specific application by selecting from a list of venues the application locates nearby. Location is based
on GPS hardware in the mobile device or network location provided by the application, and the map is
based on data from the OpenStreetMap project. Each check-in awards the user points and sometimes
badges. Figure 2 illustrates a snapshot of the Foursquare application interface on smartphones.
To alleviate the data sparsity problem for better evaluation, we selected check-in venues which have
been visited by at least two distinct users, and users who have checked in at least 10 distinct venues.
The resulting dataset contains 12,368 distinct users, 186,745 venues, 1,425,664 check-in behaviors and
56,395 social connections from January 2012 to December 2012. Table 1 lists the more detailed statistical
information on our dataset, where the ?Avg. Num. of Check-ins? is the average number of check-ins per
user, and ?Max. Num. of Check-ins? is the maximal number of check-ins among users (similarly for
?Avg. Num. of Friendships? and ?Max. Num. of Friendships?). The ?Average Clustering Coefficient
(ACC)? is a measure of the degree to which users in the Foursquare network tend to cluster together,
and ?Diameter? is the longest shortest path in the network. All user and venue information has been
anonymized. Each check-in has a unique id as well as the user id and the venue id, and each social
connection consists of two users represented by two unique ids.
5.2 Task
Mobile phones have become an important tool for communication and they are an ideal platform for
understanding social influence and social dynamics. Using our Foursquare dataset, we can investigate
the mutual influence between social actions and social ties. More specifically, we can investigate how
1
https://foursquare.com/
854
Figure 2: A snapshot of the Foursquare application interface on smartphones.
Duration Jan 2012 to Dec 2012 Num. of Users 12,368
Num. of Check-ins 1,425,664 Num. of Friendships 56,395
Avg. Num. of Check-ins 115.27 Max. Num. of Check-ins 657
Avg. Num. of Friendships 4.56 Max. Num. of Friendships 265
Average Clustering Coefficient (ACC) 0.42 Diameter 12
Table 1: Statistical information of our Foursquare dataset.
the friendship relations affect users? check-in behaviors, and how users? check-in behaviors affect their
friendships. Figure 3 gives an illustrative example of social action prediction and social tie discovery
tasks in our Foursquare dataset. Given an unseen Foursquare social network dataset, our objective is to
predict whether the users have check-in behaviors and whether there are friendship relations between
these users. In the right figure, we list the predicted check-in behaviors (in red color) and the inferred
friendship relations between users (in green color). The probabilities associated with the predictions
represent corresponding confidence scores.
5.3 Evaluation Methodology
We exploited a wide range of important features to define the factors ?(?), and ?
?
(?), including temporal
and social features such as the number of check-ins and number of new check-ins in a user?s history,
number of friends of a user, the check-in information from a user?s friends, etc. For the coupled latent
factor ?
h
(?), we incorporated social tie evidences and hypotheses as features to capture social actions,
and we also incorporated social action evidences and hypotheses as features to leverage social ties.
For quantitative performance evaluation, we used the standard measures of Precision (P), Recall (R),
and F-measure (the harmonic mean of P and R:
2PR
P+R
) for both social action prediction and social tie
inference. We performed four-fold cross-validation on this dataset, and took the average performance.
We compared our approach with the following alternative methods for predicting social actions and
inferring social ties:
?SVM: This model views social action prediction and social tie inference as two separate classification
problems, and solves them independently. We used the SVM-light
2
package for this model.
?ERGM: This is the traditional exponential-family random graph model without the latent factor ?
h
(?)
incorporated for social action prediction and social tie inference. Similar to SVM, this model also
performs them separately.
?DCRF: This model is a dynamical and factorial CRF (Sutton et al., 2007) used to jointly solve the two
tasks. This model was originally proposed for labeling and segmenting sequence data, and we directly
2
http://svmlight.joachims.org/
855
Figure 3: An illustrative example of social action prediction and social tie discovery tasks in our
Foursquare dataset. The left is the input of our problem, and the right is the output of the two tasks.
Models Precision Recall F-measure
SVM 73.75 64.54 68.84
ERGM 80.69 79.70 80.19
DCRF 89.45 82.32 85.74
MLRG 89.03 87.89 88.46
Table 2: Comparative performance of different
models for social action prediction. The best
results are printed in boldface.
Models Precision Recall F-measure
SVM 70.75 61.57 65.84
ERGM 78.85 77.39 78.11
DCRF 82.45 76.56 79.40
MLRG 84.33 83.89 84.11
Table 3: Comparative performance of different
models for social tie inference. The best results
are printed in boldface.
applied it for our tasks in social network analysis.
All these models exploited standard parameter learning and inference algorithms in our experiments.
To avoid over-fitting, penalization techniques on likelihood were also performed. All experiments were
performed on the Linux workstation, with 24 2.5GHz Intel Xeon E5-2640 CPUs and 16 GB of memory.
5.4 Performance
Table 2 shows the performance on social action prediction and Table 3 shows the performance on social
tie inference of different models, respectively. The best Precision, Recall and F1-measure of these results
are highlighted. Our method consistently outperforms other comparative methods on the F-measure. The
improvement is statistically and significantly better according to McNemar?s paired tests. These results
not only imply that there exists high correlation and mutual influence between social actions and social
ties, but also demonstrate the feasibility and effectiveness of our model for exploring them.
The SVM model solves social action prediction and social tie inference independently without consid-
ering mutual influence and benefits between them, thus leading to the worst performance. The ERGM
outperforms SVM by capturing social network structures. However, the performance of this model is
still limited and there is a large room for improving. The DCRF model easily outperforms both SVM
and ERGM by modeling social actions and social ties jointly in a single framework. However, com-
pared to our MLRG model, there are still some shortcomings of DCRF. DCRF was proposed to label
and segment sequence data, such as POS tagging and NP chunking (Sutton et al., 2007). The graphical
structure of DCRF is not well suited for social networks to capture mutual influence. The merits of our
proposed MLRG model over other models principally come from (1) appropriate graphical structure for
social network modeling, especially the coupled latent factor to exploit mutual influence simultaneously,
and (2) the mutual and collaborative learning algorithm MGD to reinforce the optimization of both social
actions and social ties.
5.5 Effect of Mutual Influence and Analysis
We also examined the nature and effectiveness of the associated latent factors on the mutual influence,
and Figure 4 demonstrates their feasibility in our modeling. Note that if we do not incorporate the
latent factors, our MLRG model becomes the traditional ERGM baseline approach. It shows that the
856
latent factors consistently enhance Precision, Recall, and F-measure for both social action prediction and
social tie inference tasks. For example, the latent factors significantly improve the F-measure by 8.27%
(from 80.19 to 88.46) for social action prediction, and improve the F-measure by 6.0% (from 78.11 to
84.11) for social tie discovery, respectively. These results not only illustrate that social actions and social
ties influence each other to a large extent, but also demonstrate the feasibility and effectiveness of our
latent factors for exploring them.
We performed an in-depth error analysis to provide gains of our MLRGmodel and some insights on the
influence between users? check-in behaviors and users? friendship relations. By carefully investigating
our Foursquare dataset, we found that approximately 75% users tend to cluster together to create tightly
knit groups characterized by a relatively high density of friendship relations or ties, and the remaining
25% users loosely or seldom connect with each other through the friendship relations. In other words,
75% users form high density of relationship ties and the average clustering coefficient (ACC) is high
(0.61). However, the tie density of the remaining 25% users is much lower, since the ACC of these
users is only 0.18. Compared to the baseline methods (especially the SVM and ERGM methods), the
performance improvement of our MLRG model mainly comes from 75% users with high density of
friendship ties. In particular, about 20% prediction errors (including social action and social tie prediction
errors) of such users made by the SVM model can be corrected by our MLRG model. This finding shows
that, the mutual influence between users? check-in behaviors and users? friendship relations increases
with the density growth of the friendship relations of these users. This finding is intuitively correct and is
consistent with the homophily theory. More interestingly, this finding also implies the gains and merits of
our MLRG model for exploiting mutual influence, especially when the users in the Foursquare network
cluster together tightly with high density of ties.
5.6 Efficiency
A number of learning algorithms can be applied for parameter optimization of our MLRG model. Table
4 summarizes the efficiency of several alternative optimization algorithms for learning our model?s pa-
rameters. We compared the learning time (hr.) and inference time (sec.) of the MGD algorithm to loopy
belief propagation (LBP), Markov chain Monte Carlo(MCMC) Gibbs sampling (Geman and Geman,
1984), and variational mean-field (VMF) approximation algorithms (Wainwright and Jordan, 2008).
Both Sutton et al. (2007) and Tang et al. (2011) used LBP for parameter estimation. LBP is inherently
unstable and may cause convergence problems. When the graph has large tree-width as in our case, the
LBP algorithm is inefficient, and is slow to converge. In Gibbs sampling, the candidate sample is always
accepted with the probability of 1, lacking the capability of measuring quality of samples and elimi-
nating low grade samples. The VMF approach aims to minimize the Kullback-Leibler (KL) divergence
between an approximated distribution Q and the target distribution P by finding the best distribution Q
from some family of distributions for which an inference is feasible. The MGD algorithm we proposed
is very efficient. It is particularly notable that our MGD algorithm takes much less time than other three
algorithms for learning. In particular, our proposed algorithm is over orders of magnitude faster than the
LBP for running.
6 Conclusions and Future Work
Finally, we answer the questions in Section 1 to draw the conclusions of this paper as follows:
Is there any dynamics or mutual influence between social actions and social ties? Doubtlessly, so-
cial actions and social ties are highly correlated and mutually reinforced. We propose a single unified
framework, mutual latent random graph (MLRG), to exploit homophily for simultaneous social action
prediction and social tie discovery. The MLRG model incorporates coupled latent factors to capture
dynamics and mutual influence between social actions and social ties. Moreover, we propose the mu-
tual gradient descent (MGD) algorithm to perform mutual and collaborative optimization to reinforce
both social actions and social ties. By coupling actions with ties jointly in a single coherent framework,
MLRG achieves significantly better performance on both social action prediction and social tie inference
on our collected Foursquare dataset, compared to several state-of-the-art existing models.
857
To what extent do they influence each other? We perform an in-depth analysis to show the gains and
merits of our MLRG model, as well as some insights on the influence between users? check-in behaviors
and users? friendship relations. The finding on our real-world Foursquare data demonstrates that social
actions (users? check-in behaviors) and social ties (users? friendship relations) influence each other to a
considerable degree when the users connect each other tightly with high density of ties in the network.
Experimental results also illustrate the feasibility and effectiveness of our latent factors for exploring
the mutual influence. In particular, the latent factors in our model significantly improve the F-measure
by 8.27% (from 80.19 to 88.46) for social action prediction, and improve the F-measure by 6.0% (from
78.11 to 84.11) for social tie discovery, respectively.
Two directions of future work appear attractive: Inferring fine-grained and multiple relationships be-
tween users (such as friendship, family, colleague, and advisor-adviser, etc.) on complex social networks
and extending our established optimization algorithms for parallel and distributed learning based on the
Hadoop MapReduce framework to handle large scale social networks involving billions of users.
Figure 4: Contribution of latent factors on social action predic-
tion (left) and social tie inference (right).
Algorithms Learning Inference
LBP 8.67 8
MCMC 3.45 124
VMF 2.39 7
MGD 0.45 6
Table 4: Efficiency comparison of differ-
ent optimization algorithms on learning
time (hr.) and inference time (sec.).
References
Fabr??cio Benevenuto, Tiago Rodrigues, Meeyoung Cha, and Virg??lio A. F. Almeida. Characterizing user
behavior in online social networks. In Proceedings of Internet Measurement Conference, pages 49?62,
2009.
L?eon Bottou. Stochastic learning. In Olivier Bousquet and Ulrike von Luxburg, editors, Advanced
Lectures on Machine Learning, Lecture Notes in Artificial Intelligence, LNAI 3176, pages 146?168.
Springer Verlag, Berlin, 2004.
Santo Fortunato. Community detection in graphs. Physics Reports, 486(3-5):75?174, 2010.
Ove Frank and David Strauss. Markov graphs. Journal of the American Statistical Association (JASA),
81(395):832?842, 1986.
Huiji Gao, Jiliang Tang, Xia Hu, and Huan Liu. Modeling temporal effects of human mobile behavior on
location-based social networks. In Proceedings of CIKM-13, pages 1673?1678, San Francisco, CA,
USA, 2013.
Stuart Geman and Donald Geman. Stochastic relaxation, Gibbs distributions, and the Bayesian restora-
tion of images. IEEE Transitions on Pattern Analysis and Machine Intelligence, 6:721?741, 1984.
Jeongwoo Ko, Luo Si, and Eric Nyberg. A probabilistic graphical model for joint answer ranking in
question answering. In Proceedings of SIGIR-07, pages 343?350, Amsterdam, The Netherlands, 2007.
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue Moon. What is Twitter, a social network or a
news media? In Proceedings of WWW-10, pages 591?600, Raleigh, North Carolina, USA, 2010.
Yann Lecun, L?eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278?2324, Nov 1998.
Yan Liu, Alexandru Niculescu-Mizil, and Wojciech Gryc. Topic-link LDA: joint models of topic and
author community. In Proceedings of ICML-09, pages 665?672, Montreal, Canada, 2009.
L. Lov?asz. Random walks on graphs: A survey. Combinatorics, Paul Erd?os is Eighty, 2:353?398, 1996.
858
Hao Ma, Irwin King, and Michael R. Lyu. Learning to recommend with social trust ensemble. In
Proceedings of SIGIR-09, pages 203?210, Boston, MA, USA, 2009.
Hao Ma, Irwin King, and Michael R. Lyu. Learning to recommend with explicit and implicit social
relations. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):1?19, 2011.
Miller McPherson, Lynn Smith-Lovin, and James M. Cook. Birds of a feather: Homophily in social
networks. Annual Review of Sociology, 27(1):415?444, 2001.
Bo Pang and Lillian Lee. Opinion mining and sentiment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1?135, 2008.
Matthew Richardson and Pedro Domingos. Markov logic networks. Machine Learning, 62(1-2):107?
136, 2006.
Garry Robins, Tom A. B. Snijders, Peng Wang, Mark Handcock, and Philippa Pattison. Recent develop-
ments in exponential random graph (p
?
) models for social networks. Social Networks, 29(2):192?215,
2007.
Charles Sutton, Andrew McCallum, and Khashayar Rohanimanesh. Dynamic conditional random fields:
Factorized probabilistic models for labeling and segmenting sequence data. Journal of Machine Learn-
ing Research, 8:693?723, 2007.
Chenhao Tan, Jie Tang, Jimeng Sun, Quan Lin, and Fengjiao Wang. Social action tracking via noise
tolerant time-varying factor graphs. In Proceedings of KDD-10, pages 1049?1058, Washington, DC,
USA, 2010.
Wenbin Tang, Honglei Zhuang, and Jie Tang. Learning to infer social ties in large networks. In Proceed-
ings of ECML/PKDD-11, pages 381?397, Athens, Greece, 2011.
Jie Tang, Tiancheng Lou, and Jon Kleinberg. Inferring social ties across heterogeneous networks. In
Proceedings of WSDM-12, pages 743?752, Seattle, WA, USA, 2012.
Martin J. Wainwright and Michael I. Jordan. Graphical models, exponential families, and variational
inference. Foundations and Trends in Machine Learning, 1:1?305, 2008.
Stanley Wasserman and Philippa Pattison. Logit models and logistic regressions for social networks: I.
An introduction to Markov graphs and p
?
. Psychometrika, 61(3):401?425, 1996.
Stanley Wasserman, Katherine Faust, Dawn Iacobucci, and Mark Granovetter. Social Network Analysis:
Methods and Applications. Cambridge University Press, 1994.
Xiaofeng Yu and Wai Lam. An integrated probabilistic and logic approach to encyclopedia relation
extraction with multiple features. In Proceedings of COLING-08, pages 1065?1072, Manchester,
United Kingdom, 2008.
Xiaofeng Yu and Wai Lam. Bidirectional integration of pipeline models. In Proceedings of AAAI-10,
pages 1045?1050, Atlanta, Georgia, USA, 2010.
Xiaofeng Yu and Wai Lam. Jointly identifying entities and extracting relations in encyclopedia text via
a graphical model approach. In Proceedings of COLING-10, pages 1399?1407, Beijing, China, 2010.
Xiaofeng Yu and Wai Lam. Probabilistic joint models incorporating logic and learning via structured
variational approximation for information extraction. Knowledge and Information Systems (KAIS),
32:415?444, 2012.
Xiaofeng Yu, Wai Lam, and Bo Chen. An integrated discriminative probabilistic approach to information
extraction. In Proceedings of CIKM-09, pages 325?334, Hong Kong, China, 2009.
Xiaofeng Yu, Irwin King, and Michael R. Lyu. Towards a top-down and bottom-up bidirectional ap-
proach to joint information extraction. In Proceedings of CIKM-11, pages 847?856, Glasgow, Scot-
land, UK, 2011.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and Isabel Trancoso. Graph-based semi-supervised
model for joint chinese word segmentation and part-of-speech tagging. In Proceedings of ACL-13,
pages 770?779, Sofia, Bulgaria, 2013.
859
