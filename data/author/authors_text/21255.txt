A Design Methodology for a Biomedical Literature Indexing Tool
Using the Rhetoric of Science
Robert E. Mercer
University of Western Ontario,
London, Ontario, N6A 5B7
mercer@csd.uwo.ca
Chrysanne Di Marco
University of Waterloo,
Waterloo, Ontario, N2L 3G1
cdimarco@uwaterloo.ca
Abstract
Literature indexing tools provide re-
searchers with a means to navigate
through the network of scholarly scientific
articles in a subject domain. We propose
that more effective indexing tools may be
designed using the links between articles
provided by citations.
With the explosion in the amount of sci-
entific literature and with the advent of ar-
tifacts requiring more sophisticated index-
ing, a means to provide more information
about the citation relation in order to give
more intelligent control to the navigation
process is warranted. In order to navigate
a citation index in this more-sophisticated
manner, the citation index must provide
not only the citation-link information, but
also must indicate the function of the cita-
tion. The design methodology of an in-
dexing tool for scholarly biomedical lit-
erature which uses the rhetorical context
surrounding the citation to provide the ci-
tation function is presented. In particular,
we discuss how the scientific method is re-
flected in scientific writing and how this
knowledge can be used to decide the pur-
pose of a citation.
1 Introduction
1.1 The aim of citation indexing
Indexing tools, such as CiteSeer (Bollacker et al, 1999),
play an important role in the scientific endeavour by
providing researchers with a means to navigate through
the network of scholarly scientific papers using the con-
nections provided by citations. Citations relate articles
within a research field by linking together works whose
methods and results are in some way mutually relevant.
Customarily, authors include citations in their papers to
indicate works that are foundational in their field, back-
ground for their own work, or representative of comple-
mentary or contradictory research. Another researcher
may then use the presence of citations to locate articles
she needs to know about when entering a new field or to
read in order to keep track of progress in a field where she
is already well-established. But, with the explosion in the
amount of scientific literature, a means to provide more
information in order to give more intelligent control to the
navigation process is warranted. A user normally wants
to navigate more purposefully than ?Find all articles cit-
ing a source article?. Rather, the user may wish to know
whether other experiments have used similar techniques
to those used in the source article, or whether other works
have reported conflicting experimental results. In order to
navigate a citation index in this more-sophisticated man-
ner, the citation index must contain not only the citation-
link information, but also must indicate the function of
the citation in the citing article.
The goal of our research project is the design and im-
plementation of an indexing tool for scholarly biomedical
literature which uses the text surrounding the citation to
provide information about the binary relation between the
two papers connected by a citation. In particular, we are
interested in how the scientific method structures the way
in which ideas, results, theories, etc. are presented in sci-
entific writing and how the style of presentation indicates
the purpose of citations, that is, what the relationship is
between the cited and citing papers.
Our interest in the connection between scientific lit-
erature (our focus), ontologies, and databases is that the
content and structure of each of these three repositories
of scientific knowledge has its foundations in the method
of science. Our purpose here is twofold: to make explicit
our design methodology for an indexing tool that uses
                                            Association for Computational Linguistics.
                   Linking Biological Literature, Ontologies and Databases, pp. 77-84.
                                                HLT-NAACL 2004 Workshop: Biolink 2004,
the rhetoric of science as its foundation to see whether the
ideas that underly our methodology can cross-fertilize the
enquiry into the other two areas, and to discuss the tool
itself with the purpose of making known that there exists
a working tool which can assist the development of other
projects.
A citation may be formally defined as a portion of a
sentence in a citing document which references another
document or a set of other documents collectively. For
example, in sentence 1 below, there are two citations:
the first citation is Although the 3-D structure. . . progress,
with the set of references (Eger et al, 1994; Kelly, 1994);
the second citation is it was shown. . . submasses with the
single reference (Coughlan et al, 1986).
(1) Although the 3-D structure analysis by x-ray
crystallography is still in progress (Eger et al,
1994; Kelly, 1994), it was shown by electron
microscopy that XO consists of three submasses
(Coughlan et al, 1986).
A citation index enables efficient retrieval of docu-
ments from a large collection?a citation index consists
of source items and their corresponding lists of biblio-
graphic descriptions of citing works. The use of citation
indexing of scientific articles was invented by Dr. Eugene
Garfield in the 1950s as a result of studies on problems
of medical information retrieval and indexing of biomed-
ical literature. Dr. Garfield later founded the Institute
for Scientific Information (ISI), whose Science Citation
Index (Garfield, no date) is now one of the most popu-
lar citation indexes. Recently, with the advent of digi-
tal libraries, Web-based indexing systems have begun to
appear (e.g., ISI?s ?Web of Knowledge?, CiteSeer (Bol-
lacker et al, 1999)).
Authors of scientific papers normally include citations
in their papers to indicate works that are connected in an
important way to their paper. Thus, a citation connect-
ing the source document and a citing document serves
one of many functions. For example, one function is that
the citing work gives some form of credit to the work
reported in the source article. Another function is to
criticize previous work. Other functions include: foun-
dational works in their field, background for their own
work, works which are representative of complementary
or contradictory research.
The aim of citation analysis studies has been to cate-
gorize and, ultimately, to classify the function of scien-
tific citations automatically. Many citation classification
schemes have been developed, with great variance in the
number and nature of categories used. Garfield (1965)
was the first to define a classification scheme, while
Finney (1979) was the first to suggest that a citation clas-
sifier could be automated. Other classification schemes
include those by Cole (1975), Duncan, Anderson, and
McAleese (1981), Frost (1979), Lipetz (1965), Moravc-
sik and Murugesan (1975), Peritz (1983), Small (1978),
Spiegel-Ro?sing (1977), and Weinstock (1971). Within
this representative group of classification schemes, the
number of categories ranges from four to 26. Examples
of these categories include a contrastive, supportive, or
corrective relationship between citing and cited works.
But, the author?s purpose for including a citation is not
apparent in the citation per se. Determining the nature
of the exact relationship between a citing and cited paper
often requires some level of understanding of the text in
which the citation is embedded.
1.2 Citation indexing in biomedical literature
analysis
In the biomedical field, we believe that the usefulness
of automated citation classification in literature indexing
can be found in both the larger context of managing entire
databases of scientific articles or for specific information-
extraction problems. On the larger scale, database cura-
tors need accurate and efficient methods for building new
collections by retrieving articles on the same topic from
huge general databases. Simple systems (e.g., (Andrade
and Valencia, 1998), (Marcotte et al, 2001)) consider
only keyword frequencies in measuring article similarity.
More-sophisticated systems, such as the Neighbors utility
(Wilbur and Coffee, 1994), may be able to locate articles
that appear to be related in some way (e.g., finding related
Medline abstracts for a set of protein names (Blaschke et
al., 1999)), but the lack of specific information about the
nature and validity of the relationship between articles
may still make the resulting collection a less-than-ideal
resource for subsequent analysis. Citation classification
to indicate the nature of the relationships between articles
in a database would make the task of building collections
of related articles both easier and more accurate. And, the
existence of additional knowledge about the nature of the
linkages between articles would greatly enhance naviga-
tion among a space of documents to retrieve meaningful
information about the related content.
A specific problem in information extraction that may
benefit from the use of citation categorization involves
mining the literature for protein-protein interactions (e.g.,
(Blaschke et al, 1999), (Marcotte et al, 2001), (Thomas
et al, 2000)). Currently, even the most-sophisticated sys-
tems are not yet capable of dealing with all the difficult
problems of resolving ambiguities and detecting hidden
knowledge. For example, Blaschke et al?s system (1999)
is able to handle fairly complex problems in detecting
protein-protein interactions, including constructing the
network of protein interactions in cell-cycle control, but
important implicit knowledge is not recognized. In the
case of cell-cycle analysis for Drosophila, their system is
able to determine that relationships exist between Cak,
Cdk7, CycH, and Cdk2: Cak inhibits/phosphorylates
Cdk7, Cak activates/phosphorylates Cdk2, Cdk7 phos-
phorylates Cdk2, CycH phosphorylates Cak and CycH
phosphorylates Cdk2. However, the system is not able
to detect that Cak is actually a complex formed by Cdk7
and CycH, and that the Cak complex regulates Cdk2.
While the earlier literature describes inter-relationships
among these proteins, the recognition of the generaliza-
tion in their structure, i.e., that these proteins are part
of a complex, is contained only in more-recent articles:
?There is an element of generalization implicit in later
publications, embodying previous, more dispersed find-
ings. A clear improvement here would be the generation
of associated weights for texts according to their level
of generality? (Blaschke et al, 1999). Citation catego-
rization could provide just these kind of ?ancestral? re-
lationships between articles?whether an article is foun-
dational in the field or builds directly on closely related
work?and, if automated, could be used in forming col-
lections of articles for study that are labelled with ex-
plicit semantic and rhetorical links to one another. Such
collections of semantically linked articles might then be
used as ?thematic? document clusters (cf. Wilbur (2002))
to elicit much more meaningful information from docu-
ments known to be closely related.
An added benefit of having citation categories avail-
able in text corpora used for studies such as extract-
ing protein-protein interactions is that more, and more-
meaningful, information may be obtained. In a potential
application for our research, Blaschke et al (1999) noted
that they were able to discover many more protein-protein
interactions when including in the corpus those articles
found to be related by the Neighbors facility (Wilbur and
Coffee, 1994) (285 versus only 28 when relevant protein
names alone were used in building the corpus). Lastly,
very difficult problems in scientific and biomedical infor-
mation extraction that involve aspects of deep-linguistic
meaning may be resolved through the availability of cita-
tion categorization in curated texts: synonym detection,
for example, may be enhanced if different names for the
same entity occur in articles that can be recognized as
being closely related in the scientific research process.
2 Our Guiding Principles
2.1 Scientific writing and the rhetoric of science
The automated labelling of citations with a specific ci-
tation function requires an analysis of the linguistic fea-
tures in the text surrounding the citation, coupled with
a knowledge of the author?s pragmatic intent in placing
the citation at that point in the text. The author?s pur-
pose for including citations in a research article reflects
the fact that researchers wish to communicate their results
to their scientific community in such a way that their re-
sults, or knowledge claims, become accepted as part of
the body of scientific knowledge. This persuasive na-
ture of the scientific research article, how it contributes to
making and justifying a knowledge claim, is recognized
as the defining property of scientific writing by rhetori-
cians of science, e.g., (Gross, 1996), (Gross et al, 2002),
(Hyland, 1998), (Myers, 1991). Style (lexical and syntac-
tic choice), presentation (organization of the text and dis-
play of the data), and argumentation structure are noted as
the rhetorical means by which authors build a convincing
case for their results.
Our approach to automated citation classification is
based on the detection of fine-grained linguistics cues in
scientific articles that help to communicate these rhetori-
cal stances and thereby map to the pragmatic purpose of
citations. As part of our overall research methodology,
our goal is to map the various types of pragmatic cues
in scientific articles to rhetorical meaning. Our previous
work has described the importance of discourse cues in
enhancing inter-article cohesion signalled by citation us-
age (Mercer and Di Marco, 2003), (Di Marco and Mercer,
2003). We have also been investigating another class of
pragmatic cues, hedging cues, (Mercer, Di Marco, and
Kroon, 2004), that are deeply involved in creating the
pragmatic effects that contribute to the author?s knowl-
edge claim by linking together a mutually supportive net-
work of researchers within a scientific community.
2.2 Results of our previous studies
In our preliminary study (Mercer and Di Marco, 2003),
we analyzed the frequency of the cue phrases from
(Marcu, 1997) in a set of scholarly scientific articles. We
reported strong evidence that these cue phrases are used
in the citation sentences and the surrounding text with
the same frequency as in the article as a whole. In sub-
sequent work (Di Marco and Mercer, 2003), we analyzed
the same dataset of articles to begin to catalogue the fine-
grained discourse cues that exist in citation contexts. This
study confirmed that authors do indeed have a rich set
of linguistic and non-linguistic methods to establish dis-
course cues in citation contexts.
Another type of linguistic cue that we are studying is
related to hedging effects in scientific writing that are
used by an author to modify the affect of a ?knowledge
claim?. Hedging in scientific writing has been exten-
sively studied by Hyland (1998), including cataloging the
pragmatic functions of the various types of hedging cues.
As Hyland (1998) explains, ?[Hedging] has subsequently
been applied to the linguistic devices used to qualify a
speaker?s confidence in the truth of a proposition, the kind
of caveats like I think, perhaps, might, and maybe which
we routinely add to our statements to avoid commitment
to categorical assertions. Hedges therefore express tenta-
tiveness and possibility in communication, and their ap-
propriate use in scientific discourse is critical (p. 1)?.
The following examples illustrate some of the ways in
which hedging may be used to deliberately convey an atti-
tude of uncertainty or qualifification. In the first example,
the use of the verb suggested hints at the author?s hesi-
tancy to declare the absolute certainty of the claim:
(2) The functional significance of this modulation
is suggested by the reported inhibition of MeSo-
induced differentiation in mouse erythroleukemia
cells constitutively expressing c-myb.
In the second example, the syntactic structure of the sen-
tence, a fronted adverbial clause, emphasizes the effect
of qualification through the rhetorical cue Although. The
subsequent phrase, a certain degree, is a lexical modifier
that also serves to limit the scope of the result:
(3) Although many neuroblastoma cell lines show
a certain degree of heterogeneity in terms of neu-
rotransmitter expression and differentiative po-
tential, each cell has a prevalent behavior in re-
sponse to differentiation inducers.
In Mercer (2004), we showed that the hedging cues pro-
posed by Hyland occur more frequently in citation con-
texts than in the text as a whole. With this information
we conjecture that hedging cues are an important aspect
of the rhetorical relations found in citation contexts and
that the pragmatics of hedges may help in determining
the purpose of citations.
We investigated this hypothesis by doing a frequency
analysis of hedging cues in citation contexts in a corpus
of 985 biology articles. We obtained statistically signifi-
cant results (summarized in Table 1 indicating that hedg-
ing is used more frequently in citation contexts than the
text as a whole. Given the presumption that writers make
stylistic and rhetorical choices purposefully, we propose
that we have further evidence that connections between
fine-grained linguistic cues and rhetorical relations exist
in citation contexts.
Table 1 shows the proportions of the various types
of sentences that contain hedging cues, broken down by
hedging-cue category (verb or nonverb cues), according
to the different sections in the articles (background, meth-
ods, results and discussion, conclusions). For all but one
combination, citation sentences are more likely to contain
hedging cues than would be expected from the overall fre-
quency of hedge sentences (  ). Citation ?window?
sentences (i.e., sentences in the text close to a citation)
generally are also significantly ( 	
  ) more likely to
contain hedging cues than expected, though for certain
combinations (methods, verbs and nonverbs; res+disc,
verbs) the difference was not significant.
Tables 2, 3, and 4 summarize the occurrence of hedg-
ing cues in citation ?contexts? (a citation sentence and the
surrounding citation window). Table 5 shows the propor-
tion of hedge sentences that either contain a citation, or
fall within a citation window; Table 5 suggests (last 3-
column column) that the proportion of hedge sentences
containing citations or being part of citation windows is
at least as great as what would be expected just by the
distribution of citation sentences and citation windows.
Table 1 indicates (statistically significant) that in most
cases the proportion of hedge sentences in the cita-
tion contexts is greater than what would be expected
by the distribution of hedge sentences. Taken together,
these conditional probabilities support the conjecture that
hedging cues and citation contexts correlate strongly. Hy-
land (1998) has catalogued a variety of pragmatic uses of
hedging cues, so it is reasonable to speculate that these
uses can be mapped to the rhetorical meaning of the text
surrounding a citation, and from thence to the function of
the citation.
3 Our Design Methodology
3.1 The Tool
The indexing tool that we are designing enhances a stan-
dard citation index by labelling each citation with the
function of that citation. That is, given an agreed-upon
set of citation functions, our tool will categorize a cita-
tion automatically into one of these functional categories.
To accomplish this automatic categorization we are using
a decision tree: given a set of features, which combina-
tions of features map to which citation function. Our cur-
rent focus is the biomedical literature, but we are certain
that our tool can be used for the experimental sciences.
We are not certain whether the tool can be generalized
beyond this corpus (Frost, 1979).
In the following we describe in more detail the three as-
pects of our design methodology: the research program,
the tool implementation, and its evaluation. Our basic
assumption is that citations form links to other articles
for much the same purpose and in much the same way as
links to other parts of the same article. These intra-textual
and inter-textual linkages are made to create a coherent
presentation to convince the reader that the content of the
article is of value. The presentation is made cohesive by
use of linguistic and stylistic devices that have been cata-
logued by rhetoricians and which we believe may be de-
tected by automated means.
The research program will
 develop a catalogue of linguistic and non-
linguistic cues that capture both the linguistic
and stylistic techniques as well as the extensive
body of knowledge that has accumulated about
the rhetoric of science and how science is written
about;
Table 1: Proportion of sentences containing hedging cues, by type of sentence and hedging cue category.
Verb Cues Nonverb Cues All Cues
Cite Wind All Cite Wind All Cite Wind All
background 0.15 0.11 0.13 0.13 0.13 0.12 0.25 0.22 0.24
methods 0.09 0.06 0.06 0.05 0.04 0.04 0.14 0.10 0.09
res+disc 0.22 0.16 0.16 0.15 0.14 0.14 0.32 0.27 0.27
conclusions 0.29 0.22 0.20 0.18 0.19 0.15 0.42 0.36 0.32
Table 2: Number and proportion of citation contexts containing a hedging cue, by section and location of hedging cue.
Contexts Sentences Windows
# % # % # %
background 3361 0.33 2575 0.25 2679 0.26
methods 1089 0.18 801 0.14 545 0.09
res+disc 7257 0.44 5366 0.32 4660 0.28
conclusions 338 0.58 245 0.42 221 0.38
 develop computationally realizable methods to
detect these cues;
 connect these cues to rhetorical relations; and
 organize the knowledge that these rhetorical rela-
tions represent as features in a decision tree that
produces the intended function of the citation.
Our purpose in using a decision tree is three-fold.
Firstly, the decision tree gives us ready access to the
citation-function decision rules. Secondly, we aim to
have a working indexing tool whenever we add more
knowledge to the categorization process. This goal ap-
pears very feasible given our design choice to use a
decision tree: adding more knowledge only refines the
decision-making procedure of the previous version. And
thirdly, as we gain more experience (currently, we are
building the decision tree by hand), we intend to use ma-
chine learning techniques to enhance our tool by inducing
a decision tree.
3.2 The Research Program
Our basic assumption is that the rhetorical relations that
will provide the information that will allow the tool to cat-
egorize the citations in a biomedical article are evident to
the reader through the use of surface linguistic cues, cues
which are linguistically-based but require some knowl-
edge that is not directly derivable from the text, and some
cues which are known to the culture of scientific readers-
writers because of the practice of science and how this
practice influences communication through the writing.
We rely on the notion that rhetorical information is
realized in linguistic ?cues? in the text, some of which,
although not all, are evident in surface features (cf. Hy-
land (1998) on surface hedging cues in scientific writing).
Since we anticipate that many such cues will map to the
same rhetorical features that give evidence of the text?s
argumentative and pragmatic meaning, and that the inter-
action of these cues will likely influence the text?s overall
rhetorical effect, the formal rhetorical relation (cf. (Mann
and Thompson, 1988)) appears to be the appropriate fea-
ture for the basis of the decision tree. So, our long-term
goal is to map between the textual cues and rhetorical re-
lations. Having noted that many of the cue words in the
prototype are discourse cues, and with two recent impor-
tant works linking discourse cues and rhetorical relations
((Knott, 1996; Marcu, 1997)), we began our investigation
of this mapping with discourse cues. We have some early
results that show that discourse cues are used extensively
with citations and that some cues appear much more fre-
quently in the citation context than in the full text (Mercer
and Di Marco, 2003). Another textual device is the hedg-
ing cue, which we are currently investigating (Mercer, Di
Marco, and Kroon, 2004).
Although our current efforts focus on cue words which
are connected to organizational effects (discourse cues),
and writer intent (hedging cues), we are also interested
in other types of cues that are associated more closely
to the purpose and method of science. For example, the
scientific method is, more or less, to establish a link to
previous work, set up an experiment to test an hypothe-
sis, perform the experiment, make observations, then fi-
nally compile and discuss the importance of the results of
the experiment. Scientific writing reflects this scientific
method and its purpose: one may find evidence even at
the coarsest granularity of the IMRaD structure in scien-
tific articles. At a finer granularity, we have many target-
Table 3: Proportion of citation contexts containing a verbal hedging cue, by section and location of hedging cue.
Contexts Sentences Windows
# % # % # %
background 1967 0.19 1511 0.15 1479 0.15
methods 726 0.12 541 0.09 369 0.06
res+disc 4858 0.29 3572 0.22 2881 0.17
conclusions 227 0.39 168 0.29 139 0.24
Table 4: Proportion of citation contexts containing a nonverb hedging cue, by section and location of hedging cue.
Contexts Sentences Windows
# % # % # %
background 1862 0.18 1302 0.13 1486 0.15
methods 432 0.07 295 0.05 198 0.03
res+disc 3751 0.23 2484 0.15 2353 0.14
conclusions 186 0.32 107 0.18 111 0.19
ted words to convey the notions of procedure, observa-
tion, reporting, supporting, explaining, refining, contra-
dicting, etc. More specifically, science categorizes into
taxonomies or creates polarities. Scientific writing then
tends to compare and contrast or refine. Not surpris-
ingly, the morphology of scientific terminology exhibits
comparison and contrasting features, for example, exo-
and endo-. Science needs to measure, so scientific writ-
ing contains measurement cues by referring to scales (0?
100), or using comparatives (larger, brighter, etc.). Ex-
periments are described as a sequence of steps, so this is
an implicit method cue.
Since the inception of the formal scientific article in
the seventeenth century, the process of scientific discov-
ery has been inextricably linked with the actions of writ-
ing and publishing the results of research. Rhetoricians
of science have gradually moved from a purely descrip-
tive characterization of the science genre to full-fledged
field studies detailing the evolution of the scientific arti-
cle. During the first generation of rhetoricians of science,
e.g., (Myers, 1991), (Gross, 1996), (Fahnestock, 1999),
the persuasive nature of the scientific article, how it con-
tributes to making and justifying a knowledge claim, was
recognized as the defining property of scientific writing.
Style (lexical and syntactic choice), presentation (orga-
nization of the text and display of the data), and argu-
mentation structure were noted as the rhetorical means
by which authors build a convincing case for their results.
Recently, second-generation rhetoricians of science (e.g.,
(Hyland, 1998), (Gross et al, 2002)) have begun to me-
thodically analyze large corpora of scientific texts with
the purpose of cataloguing specific stylistic and rhetorical
features that are used to create the pragmatic effects that
contribute to the author?s knowledge claim. One particu-
lar type of pragmatic effect, hedging, is especially com-
mon in scientific writing and can be realized through a
wide variety of linguistic choices.
To catalogue these cues and to propose a mapping from
these cues to rhetorical relations, we suggest a research
program that consists of two phases. One phase is theory-
based: we are applying our knowledge from computa-
tional linguistics and the rhetoric of science to develop a
set of principles that guide the development of rules. An-
other phase is data-driven. This phase will use machine-
learning techniques to induce a decision tree.
Our two approaches are guided by a number of factors.
Firstly, the initial set of 35 categories ((Garzone, 1996),
(Garzone and Mercer, 2000)) were developed by combin-
ing and adding to the previous work from the information
science community with a preliminary manual study of
citations in biochemistry and physics articles. Secondly,
our next stages, cataloguing linguistic cues, will require
manual work by rhetoricians. Thirdly, and perhaps most
importantly, one group of cues is not found in the text,
but is rather a set of cultural guidelines that are accepted
by the scientific community for which the article is being
written. Lastly, we are interested not in the connection
between the citation functions and these cues per se, but
rather the citation functions and the rhetorical relations
that are signalled by the cues.
3.3 The Tool Implementation
Concerning the features on which the decision tree makes
its decisions, we have started with a simple, yet fully
automatic prototype (Garzone, 1996) which takes jour-
nal articles as input and classifies every citation found
therein. Its decision tree is very shallow, using only sets
of cue-words and polarity switching words (not, however,
Table 5: Proportion of hedge sentences that contain citations or are part of a citation window, by section and hedging
cue category.
Verb Cues Nonverb Cues All Cues
Cite Wind None Cite Wind None Cite Wind None
background 0.52 0.23 0.25 0.47 0.28 0.25 0.49 0.26 0.26
methods 0.25 0.16 0.59 0.20 0.15 0.65 0.23 0.16 0.61
res+disc 0.26 0.19 0.55 0.21 0.19 0.60 0.23 0.19 0.58
conclusions 0.16 0.14 0.70 0.14 0.16 0.70 0.15 0.14 0.71
etc.), some simple knowledge about the IMRaD struc-
ture1 of the article together with some simple syntactic
structure of the citation-containing sentence. The proto-
type uses 35 citation categories. In addition to having
a design which allows for easy incorporation of more-
sophisticated knowledge, it also gives flexibility to the
tool: categories can be easily coalesced to give users a
tool that can be tailored to a variety of uses.
Although we anticipate some small changes to the
number of categories due to category refinement, the ma-
jor modifications to the decision tree will be driven by
a more-sophisticated set of features associated with each
citation. When investigating a finer granularity of the IM-
RaD structure, we came to realize that the structure of
scientific writing at all levels of granularity was founded
on rhetoric, which involves both argumentation structure
as well as stylistic choices of words and syntax. This was
the motivation for choosing the rhetoric of science as our
guiding principle.
3.4 Evaluation of the Tool
Finally, as for our prototype system, at each stage of de-
velopment the tool will be evaluated:
 A test set of citations will be developed and
will be initially manually categorized by humans
knowledgeable in the scientific field that the arti-
cles represent.
 Of most essential interest, the classification accu-
racy of the citation-indexing tool will be evalu-
ated: we propose to use a combination of statisti-
cal testing and validation by human experts.
 In addition, we would like to assess the tool?s util-
ity in real-world applications such as database cu-
ration for studies in biomedical literature analy-
sis. We have suggested earlier that there may be
many uses of this tool, so a significant aspect of
the value of our tool will be its ability to enhance
other research projects.
1The corpus of biomedical papers all have the standard In-
troduction, Methods, Results, and Discussion or a slightly mod-
ified version in which Results and Discussion are merged.
4 Conclusions and Future Work
The purposeful nature of citation function is a feature of
scientific writing which can be exploited in a variety of
ways. We anticipate more-informative citation indexes as
well as more-intelligent database curation. Additionally,
sophisticated information extraction may be enhanced
when better selection of the dataset is enabled. For ex-
ample, synonym detection in a corpus of papers may be
made more tractable when the corpus is comprised of re-
lated papers derived from navigating a space of linked
citations.
In this paper we have motivated our approach to devel-
oping a literature-indexing tool that computes the func-
tions of citations. We have proposed that the function of a
citation may be determined by analyzing the rhetorical in-
tent of the text that surrounds it. This analysis is founded
on the guiding principle that the scientific method is in-
trinsic to scientific writing.
Our early investigations have determined that linguis-
tic cues and citations are related in important ways. Our
future work will be to map these linguistic cues to rhetor-
ical relations and other pragmatic functions so that this
information can then be used to determine the purpose of
citations.
Acknowledgements
We thank Mark Garzone and Fred Kroon for their par-
ticipation in this project. Our research has been finan-
cially supported by the Natural Sciences and Engineering
Research Council of Canada and by the Universities of
Western Ontario and Waterloo.
References
Miguel A. Andrade and Alfonso Valencia. 1998. Au-
tomatic Extraction of Keywords from Scientific Text:
Application to the Knowledge Domain of Protein Fam-
ilies. Bioinformatics, 14(7):600?607.
Christian Blaschke, Miguel A. Andrade, Christos Ouzou-
nis, and Alfonso Valencia. 1999. Automatic Extrac-
tion of Biological Information from Scientific Text:
Protein-Protein Interactions. International Conference
on Intelligent Systems for Molecular Biology (ISMB
1999), 60?67.
B. Bollacker, S. Lawrence, and C.L. Giles. 1999. A Sys-
tem for Automatic Personalized Tracking of Scientific
Literature on the Web. In Digital Libraries 99?The
Fourth ACM Conference on Digital Libraries, 105?
113. ACM Press, New York.
S. Cole. 1975. The Growth of Scientific Knowledge:
Theories of Deviance as a Case Study. In The Idea of
Social Structure: Papers in Honor of Robert K. Mer-
ton, 175?220. Harcourt Brace Jovanovich, New York.
Chrysanne Di Marco and Robert E. Mercer. 2003. To-
ward a Catalogue of Citation-related Rhetorical Cues
in Scientific Texts. In Proceedings of the Pacific
Association for Computational Linguistics (PACLING
2003) Conference. Halifax, Canada, August 2003.
E.B. Duncan, F.D. Anderson, and R. McAleese. 1981.
Qualified Citation Indexing: Its Relevance to Educa-
tional Technology. In Information Retrieval in Edu-
cational Technology: Proceedings of the First Sympo-
sium on Information Retrieval in Educational Technol-
ogy, 70?79. University of Aberdeen.
Jeanne Fahnestock. 1999. Rhetorical Figures in Science.
Oxford University Press.
B. Finney. 1979. The Reference Characteristics of Sci-
entific Texts. Master?s thesis, The City University of
London.
C. Frost. 1979. The Use of Citations in Literary Re-
search: A Preliminary Classification of Citation Func-
tions. Library Quarterly, 49:399?414.
Eugene Garfield. 1965. Can Citation Indexing Be au-
tomated? In M.E. Stevens et al, editors, Statistical
Association Methods for Mechanical Documentation
(NBS Misc. Pub. 269). National Bureau of Standards,
Washington, DC.
Eugene Garfield. Information, Power, and the Science
Citation Index. In Essays of an Information Scientist,
Volume 1, 1962?1973, Institute for Scientific Infor-
mation.
Mark Garzone. 1996. Automated Classification of Ci-
tations using Linguistic Semantic Grammars. M.Sc.
Thesis, The University of Western Ontario.
Mark Garzone and Robert E. Mercer. 2000. To-
wards an Automated Citation Classifier. In AI?2000,
Proceedings of the 13th Biennial Conference of the
CSCSI/SCEIO, Lecture Notes in Artificial Intelligence,
1822:337?346, H.J. Hamilton (ed.). Springer-Verlag.
A.G. Gross. 1996. The Rhetoric of Science. Harvard
University Press.
A.G. Gross, J.E. Harmon, and M. Reidy. 2002. Commu-
nicating Science: The Scientific Article from the 17th
Century to the Present. Oxford University Press.
M.A.K. Halliday and Ruqaiya Hasan. 1976. Cohesion in
English. Longman Group Limited.
Ken Hyland. 1998. Hedging in Scientific Research Arti-
cles. John Benjamins Publishing Company.
Alistair Knott. 1996. A Data-driven Methodology for
Motivating a Set of Coherence Relations. Ph.D. thesis,
University of Edinburgh.
B.A. Lipetz. 1965. Problems of Citation Analysis: Criti-
cal Review. American Documentation, 16:381?390.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a Functional
Theory of Text Organization. Text, 8(3):243?281.
Edward M. Marcotte, Ioannis Xenarios, and David Eisen-
berg. 2001. Mining Literature for Protein-Protein In-
teractions. Bioinformatics, 17(4):359?363.
Daniel Marcu. 1997. The Rhetorical Parsing, Summa-
rization, and Generation of Natural Language Texts.
Ph.D. thesis, University of Toronto.
Robert E. Mercer and Chrysanne Di Marco. 2003. The
Importance of Fine-grained Cue Phrases in Scientific
Citations. In AI?2003, Proceedings of the 16th Confer-
ence of the CSCSI/SCEIO, 550?556. Edmonton, Al-
berta, 11?13 June 2003.
Robert E. Mercer, Chrysanne Di Marco, and Frederick
Kroon. 2004. The Frequency of Hedging Cues in Cita-
tion Contexts in Scientific Writing. Submitted to Con-
ference of the Canadian Society for the Computational
Studies of Intelligence (CSCSI 2004).
M.J. Moravscik and P. Murugesan. 1975. Some Results
on the Function and Quality of Citations. Social Stud-
ies of Science, 5:86?92.
Greg Myers. 1991. Writing Biology. University of Wis-
consin Press.
B.C. Peritz. 1983. A Classification of Citation Roles for
the Social Sciences and Related Fields. Scientomet-
rics, 5:303?312.
H. Small. 1978. Cited Documents as Concept Symbols.
Social Studies of Science, 8(3):327?340.
I. Spiegel-Ro?sing. 1977. Science Studies: Bibliometric
and Content Analysis. Social Studies of Science, 7:97?
113.
James Thomas, David Milward, Christos Ouzounis,
Stephen Pulman, and Mark Carroll. 2000. Automatic
Extraction of Protein Interactions from Scientific Ab-
stracts. In Proceedings of the 5th Pacific Symposium
on Biocomputing (PSB 2000), 538-549.
M. Weinstock. 1971. Citation Indexes. In Encyclopae-
dia of Library and Information Science, 5:16?40. Mar-
cel Dekkar, New York.
W. John Wilbur. 2002. A Thematic Analysis of the AIDS
Literature. In Proceedings of the 7th Pacific Sympo-
sium on Biocomputing (PSB 2004), 386-397.
W.J. Wilbur and L. Coffee. 1994. The Effectiveness of
Document Neighboring in Search Enhancement. In-
formation Processing Management, 30:253?266.
Book Reviews
The Structure of Scientific Articles: Applications to Citation Indexing
? 2012 Association for Computational Linguistics
and Summarization
Simone Teufel
(University of Cambridge)
Stanford, CA: CSLI Publications (CSLI Studies in Computational Linguistics), 2010,
xii+518 pp; hardbound, ISBN 978-1-57586-555-3, $70.00; paperbound,
ISBN 978-1-57586-556-0, $32.50
Reviewed by
Robert E. Mercer
University of Western Ontario
Discourse models have received significant attention in the computational linguistics
community with some important connections to the non-computational discourse com-
munity. More recently, the importance of discourse annotation has increased as models
generated with supervised machine learning techniques are being used to annotate text
automatically. A primary area for annotation is science. The theme of Teufel?s book is
an important contribution in these areas: discourse models, annotation schemes, and
applications.
The book is a substantial work, approximately 450 pages of text and appendices.
It extends Teufel?s Ph.D. thesis (Teufel 2000) with a decade of new work and updated
references. The book is content-rich and meticulously written. In addition to presenting
Teufel?s discourse model, it also works as a good entry point into discourse models and
annotation. Because each chapter is structured with background, new material, and a
summary, each chapter can be read somewhat independently. Cross-references to other
parts of the book are carefully included where warranted. This structure lends itself
to using the book as a reference for each of the subtopics or as an introduction to the
subject area as a whole, suitable as a textbook.
Chapter 1 sets the stage for the rest of the book. The author sets out her funda-
mental assumptions and hypotheses. The fundamental assumptions arise from three
observations that she has made regarding the literature. Scientific discourse contains
descriptions of positive and negative states, contains references to others? contributions,
and is the result of a rhetorical game intended to promote one?s contribution. Chap-
ter 2, on information retrieval and citation indexes, and Chapter 3, on summarization,
provide the motivation for the main theme of the book: These two information-based
endeavors can be enhanced with automated tools that incorporate an understanding of
the rhetorical aspects of science writing.
Whereas Chapters 2 and 3 give an overview of current methodologies, Chapter 4,
?New Types of Information Access,? introduces two new techniques, rhetorical extracts
and citation maps, that are suggested as information navigation methods enhanced by
knowledge of the discourse that contains the information being accessed. Rhetorical
extracts are snippets that can be tailored to user expertise and navigation task. Citation
maps are interactive citation indexes that have their citation links augmented with
rhetorical or sentiment information.
Chapter 5 gives a detailed description of the five scientific text corpora that
are used in the research described throughout the book: computational linguistics,
Computational Linguistics Volume 38, Number 2
chemistry, genetics, cardiology, and agriculture. The chapter focuses primarily on the
computational linguistics corpus, on which most of the results in the book are based.
SCIXML, Teufel?s markup language for science articles, is described.
Chapter 6 contains an in-depth description of the Knowledge Claim Discourse
Model (KCDM). Teufel gives reasons why the traditional discourse models are aban-
doned in favor of her new model. In addition to it being a shallow method, she points
out the important aspects of KCDM (compared to Rhetorical Structure Theory): It is
text-type-specific (scientific articles); no world knowledge is required; it has global (top-
down) not local (bottom-up) relations; it is non-hierarchical (citation and summarization
applications do not require a rich hierarchical structure).
Chapter 7 presents three annotation schemes based on the KCDM: Knowledge
Claim Attribution (KCA), Citation Function Classification (CFC), and Argumentative
Zoning (AZ). The background and purpose of the schemes are carefully laid out. The
annotation guidelines (coding manuals) are given in Appendix C.
Chapter 8 reports on the reliability studies that use human annotators and gauge
the quality of the annotation scheme using agreement among the annotators as a proxy
for this measure. A good discussion of the measures of annotator agreement opens the
chapter, followed by a detailed analysis of the four studies. Three of the four studies
used three annotators, the other used 18 annotators. All studies used the computational
linguistics corpus.
Chapters 9 and 10 discuss the features that will be used by the machine implemen-
tations of AZ, KCA, and CFC that are described in Chapter 11. Chapter 9 provides a
comprehensive discussion of the various embodiments of meta-discourse, the text that
concerns itself with the dialogue between the author and the reader rather than content-
bearing text. Chapter 10 discusses the computable surface features that capture the
important aspects of meta-discourse that are used by the automatic annotation methods.
Chapter 11 then introduces the reader to the standard supervised machine-learning
methodology used to generate the statistical models that implement the automatic
AZ, KCA, and CFC annotators. Chapter 12 presents gold-standard, and extrinsic and
subjective evaluations of these automatic methods. The gold standard is the human-
annotated computational linguistics articles and the extrinsic task is rhetorical extracts.
Chapter 13 investigates the universality of the KCDM. The earlier chapters? re-
sults were based on the computational linguistics corpus. This chapter considers the
disciplines of chemistry, computer science, biology, astrophysics, and legal texts. Two
issues surface: the need to modify the original KCDM slightly, and the move from an
absolutely domain-knowledge-free annotation to one which includes some high-level
facts about research practices in the discipline.
Chapter 14 pushes the frontiers of potential uses of the KCDM methodology:
support tools for scientific writing, automatic review generation, scientific summary
generation that moves beyond simple sentence extraction methods and summaries of
multiple scientific documents, as well as integration of automatic AZ into a large-scale
digital library. Chapter 15 provides the conclusion. In the first section it recapitulates the
main themes of the book. This section also nicely serves as an introduction to the book,
if so desired. Section 2 lists a number of areas that could lead to an improved automatic
system.
The four appendices contain a list of the CmpLG-D articles, the DTD for SCIXML,
the annotation guidelines, and a catalog of lexical items and patterns useful in the
discourse setting.
The book makes an important and powerful statement in the field of discourse
modeling and annotation, and provides an important body of work to which other
444
Book Reviews
researchers can add or compare their work. I think it is important to keep in mind
the following few points while reading the book: First, Teufel comments that she is
interested in a discourse model for the experimental sciences, yet her focus for much of
the book is a corpus of computational linguistics papers. Also, the discourse model pro-
posed is based on knowledge claims and rhetorical moves. This catholic view of what
is science and the narrow view of structure may surprise some readers given the title
of the book. Next, some of the fundamental decisions regarding the discourse model
are heavily influenced by the requirements of the two motivational topics, leading one
to question the full generality of the discourse model. As well, the range of rhetoric in
science writing may be broader than anticipated by Teufel?s model?for example, the
style found in the geology discipline is more cumulative than critical (Heather Graves,
personal communication). And finally, some researchers (White 2010) argue that the
domain-knowledge-free annotation dictum, although loosened slightly by Teufel, may
need to be further relaxed in order to produce a more accurate gold standard, regardless
of the automatic system?s access to the same domain knowledge.
References
Teufel, Simone. 2000. Argumentative
Zoning: Information Extraction from
Scientific Text. Ph.D. thesis, University
of Edinburgh.
White, Barbara. 2010. Identifying sources of
inter-annotator variation: Evaluating two
models of argument analysis. In Proceedings
of the Fourth Linguistic Annotation Workshop
(LAW IV), Uppsala, pages 132?136.
Robert E. Mercer is a Professor of Computer Science at the University of Western Ontario. His
research interests include argumentation in science writing and annotation. Mercer?s e-mail
address is mercer@csd.uwo.ca.
445
Proceedings of the First Workshop on Argumentation Mining, pages 19?23,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
An automated method to build a corpus of rhetorically-classified
sentences in biomedical texts
Hospice Houngbo
Department of Computer Science
The University of Western Ontario
hhoungbo@uwo.ca
Robert E. Mercer
Department of Computer Science
The University of Western Ontario
mercer@csd.uwo.ca
Abstract
The rhetorical classification of sentences
in biomedical texts is an important task
in the recognition of the components of
a scientific argument. Generating super-
vised machine learned models to do this
recognition requires corpora annotated for
the rhetorical categories Introduction (or
Background), Method, Result, Discus-
sion (or Conclusion). Currently, a few,
small annotated corpora exist. We use
a straightforward feature of co-referring
text using the word ?this? to build a self-
annotating corpus extracted from a large
biomedical research paper dataset. The
corpus is annotated for all of the rhetori-
cal categories except Introduction with-
out involving domain experts. In a 10-fold
cross-validation, we report an overall F-
score of 97% with Na??ve Bayes and 98.7%
with SVM, far above those previously re-
ported.
1 Introduction
Sentence classification is an important pre-
processing task in the recognition of the compo-
nents of an argument in scientific text. For in-
stance, sentences that are deemed as conclusions
of a research paper can be used to validate or re-
fute an hypothesis presented in background or in-
troduction sentences in that paper. Therefore, in
order to understand the argumentation flow in sci-
entific publications, we need to understand how
different sentences fit into the complete rhetorical
structure of scientific writing.
To perform sentence classification using su-
pervised machine learning techniques requires a
large training corpus annotated with the appropri-
ate classification tags. In the biomedical domain,
some corpora already exist, but many of these cor-
pora are still limited and cannot be generalized to
every context. The task of sentence classification
in various rhetorical categories is often performed
on ad hoc corpora derived from a limited num-
ber of papers that don?t necessarily represent all
of the text in the biomedical domain. For instance,
the corpus used by Agarwal and Yu (2009) for the
task of sentence classification into the IMRaD cat-
egories, is composed of only 1131 sentences.
In this study, we hypothesize that using a simple
linguistically-based heuristic, we can build a sig-
nificantly larger corpus comprising sentences that
belong to specific categories of the IMRaD rhetor-
ical structure of the biomedical research text, that
will not need domain experts to annotate them,
and will represent a wider range of publications in
the biomedical literature. We have collected pairs
of sequential sentences where the second sentence
begins with ?This method. . . ?, ?This result. . . ?,
?This conclusion. . . ?. Our hypothesis is that the
first sentence in each pair is a sentence that can be
categorized respectively as Method, Result and
Conclusion sentences.
We have a number of motivations for this work.
First, sentences are the basis for most text min-
ing and extraction systems. The second motiva-
tion is that biomedical texts are the reports of sci-
entific investigations and their discourse structures
should represent the scientific method that drives
these investigations. The third and last motivation
is that categorizing sentences into the IMRaD cat-
egories can help in the task of extracting knowl-
edge discovery elements from scientific papers.
The contribution of our work is twofold. First,
we have used a simple linguistic filter to automati-
cally select thousands of sentences that have a high
probability of being correctly categorized in the
IMRAD scheme, and second, we have used ma-
chine learning techniques to classify sentences in
order to validate our hypothesis that this linguis-
tic filter works. The rest of this paper is organized
as follows. The next section reviews some related
19
work. In Section 3, a detailed methodology of cor-
pus construction and sentence classification tech-
niques is presented. In Section 4, the results are
described.
2 Related Work
The classification of sentences from scientific re-
search papers into different categories has been in-
vestigated in previous works. Many schemes have
been used and currently no standard classification
scheme has been agreed upon. Teufel et al. (1999)
use a classification scheme termed Argumentative
Zoning (AZ) to model the rhetorical and argumen-
tative aspects of scientific writing in order to easily
detect the different claims that are mentioned in a
scientific research paper. AZ has been modified
for the annotation of biology articles (Yoko et al.,
2006) and chemistry articles (Teufel et al., 2009).
Scientific discourse has also been studied in
terms of speculation and modality by Kilicoglu
and Bergler (2008) and Medlock and Briscoe
(2007). Also, Shatkay et al. (2008) and Wilbur
et al. (2006) have proposed an annotation scheme
that categorizes sentences according to various di-
mensions such as focus, polarity and certainty.
Many annotation units have also be proposed in
previous studies. Sentence level annotation is used
in Teufel et al. (1999) whereas de Waard et al.
(2009) used a multi-dimensional scheme for the
annotation of biomedical events (bio-events) in
texts.
Liakata et al. (2012) attempt to classify sen-
tences into the Core Scientific Concept (CoreSC)
scheme. This classification scheme consists of a
number of categories distributed into hierarchical
layers. The first layer consists of 11 categories,
which describe the main components of a sci-
entific investigation, the second layer consists of
properties of those categories (e.g. Novelty, Ad-
vantage), and the third layer provides identifiers
that link together instances of the same concept.
Some other recent works have focussed on the
classification of sentences from biomedical arti-
cles into the IMRaD (Introduction, Methods, Re-
search, and, Discussion) categories. Agarwal and
Yu (2009) use a corpus of 1131 sentences to clas-
sify sentences from biomedical research papers
into these categories. In this study, sentence level
annotation is used and multinomial Na??ve Bayes
machine learning has proved to perform better
than simple Na??ve Bayes. The authors report an
overall F-measure score of 91.55% with a mu-
tual information feature selection technique. The
present study provides an alternative way to build
a larger IMRaD annotated corpus, which com-
bined with existing corpora achieves a better per-
formance.
Methods for training supervised machine-
learning systems on non-annotated data, were pre-
sented in (Yu and Hatzivassiloglou, 2003), which
assumed that in a full-text, IMRaD-structured ar-
ticle, the majority of sentences in each section
will be classified into their respective IMRaD cate-
gory. Also, Agarwal and Yu (2009) used the same
method to build a baseline classifier that achieved
about 77.81% accuracy on their corpus.
3 Methodology
3.1 Constructing a self-annotating corpus
from a biomedical dataset
The goal of this study is to show that the classi-
fication of sentences from scientific research pa-
pers to match the IMRaD rhetorical structure with
supervised machine learning can be enhanced us-
ing a self-annotating corpus. The first task con-
sists of the curation of a corpus that contains sen-
tences representative of the defined categorization
scheme. We have chosen to build the corpus by ex-
tracting sentences from a large repository of full-
text scientific research papers, a publicly available
full-text subset of the PubMed repository.
Since most demonstrative pronouns are co-
referential, a sentence that begins with the demon-
strative noun phrase ?This method. . . ? or ?This re-
sult. . . ? or ?This conclusion. . . ? is co-referential
and its antecedents are likely to be found in previ-
ous sentences. Torii and Vijay-Shanker (2005) re-
ported that nearly all antecedents of such demon-
strative phrases can be found within two sen-
tences. As well, Hunston (2008) reported that
interpreting recurring phrases in a large corpus
enables us to capture the consistency in mean-
ing as well as the role of specific words in such
phrases. So, the recurring semantic sequences
?This method. . . ? or ?This result. . . ? or ?This
conclusion. . . ? in the Pubmed corpus can help
us to capture valuable information in the context
of their usage. A similar technique was used in
(Houngbo and Mercer, 2012), to build a corpus
for method mention extraction from biomedical
research papers.
Our assumption is that a sentence that appears
20
in the co-referential context of the co-referencing
phrase ?This method. . . ?, will likely talk about
a methodology used in a research experiment or
analysis. Similarly, a sentence that starts with the
expression ?This result. . . ? is likely to refer to
a result. And, similarly, for sentences that be-
gin with ?This conclusion. . . ?. The Introduction
(Background) rhetorical category does not have a
similar co-referential structure. We have chosen to
only consider the immediately preceding sentence
to the ?This? referencing sentence. Some exam-
ples are shown below.
Category # of Sentences Proportion
Method 3163 31.9%
Result 6288 62.7%
Conclusion 534 5.4%
Total 9985 100%
Table 1: Initial Self-annotated Corpus Statistics
1. We have developed a DNA microarray-based
method for measuring transcript length . . .
This method, called the Virtual Northern, is
a complementary approach . . .
2. Interestingly, Drice the downstream caspase
activated . . . was not affected by inhibition of
Dronc and Dredd.
This result, . . . suggests that some other
mechanism activates Drice.
3. We obtained a long-range PCR product from
the latter interval, that appeared to encom-
pass the breakpoint on chromosome 2 . . .
This conclusion, however , was regarded
with caution , since . . .
Table 1 shows the number of sentences per cate-
gory in this initial self-annotated corpus.
3.1.1 Feature Extraction
We have used the set of features extracted from
the Agarwal and Yu (2009) IMRaD corpus. The
reason for this choice is to be able to validate our
claim against this previous work. Agarwal and
Yu (2009) experimented with mutual information
and chi-squared for feature selection and obtained
their best performance using the top 2500 features
comprised of a combination of individual words
as well as bigrams and trigrams. A feature that
indicates the presence of a citation in a sentence
is also used as it can be an important feature for
(a) Classification with Multinomial Na??ve Bayes.
Class Precision Recall F-Measure
Method 0.923 0.661 0.77
Result 0.627 0.813 0.708
Conclusion 0.68 0.821 0.744
Average 0.779 0.74 0.744
(b) Classification with Support Vector Machine
Class Precision Recall F-Measure
Method 0.818 0.521 0.636
Result 0.511 0.908 0.654
Conclusion 0.923 0.226 0.364
Average 0.72 0.621 0.604
Table 2: Precision, Recall, F-measure : Classifier trained
with the initial self-annotated corpus and tested on a reduced
Agarwal and Yu (2009) corpus (Method, Result, Conclusion)
distinguishing some categories; for example, ci-
tations are more frequently used in Introduction
than in Results. All numbers were replaced by a
unique symbol #NuMBeR. Stop words were not
removed since certain stop words are also more
likely to be associated with certain IMRaD cate-
gories. Words that refer to a figure or table are not
removed, since such references are more likely to
occur in sentences indicating the outcome of the
study. We also used verb tense features as some
categories may be associated with the presence of
the present tense or the past tense in the sentence.
We used the Stanford parser (Klein and Manning,
2003) to identify these tenses.
3.1.2 Self-annotation
In our first experiment we trained a model on the
initial self-annotated corpus discussed above and
tested the model on the Agarwal and Yu (2009)
corpus. Table 2 shows F-measures that are below
the baseline classifier levels. We suggest that there
are two causes: many of the important n-grams
in the larger corpus are not present in the 2500
n-gram feature set; and there is noise in the ini-
tial self-annotated corpus. To reduce the noise in
the initial self-annotated corpus and to maintain
the 2500 n-gram feature set we pruned our ini-
tial self-annotated corpus using a semi-supervised
learning step using an initial model based on the
Agarwal and Yu feature set and learned from the
Agarwal and Yu corpus. We describe below the
semi-supervised method to do this pruning of the
initial self-annotated corpus.
21
Our method for categorizing sentences into the
IMRaD categories does not work for the Intro-
duction category, so from the Agarwal and Yu
(2009) IMRaD corpus, we have extracted in-
stances belonging to the Method, Result and
Conclusion categories and have used this corpus
to build a model with a supervised multinomial
Na??ve Bayes method. This model is then used
to classify sentences in the initial self-annotated
corpus. When the model matches the initial self-
annotated corpus category with a confidence level
greater than 98%, this instance is added to what we
will now call the model-validated self-annotated
corpus. The composition of this model-validated
corpus is presented in Table 3.
Category # of Sentences Proportion
Method 878 23.6%
Result 2399 64.5%
Conclusion 443 11.9%
Total 3719 100%
Table 3: Model-validated Self-annotated Corpus Statistics
3.2 Automatic text classification
For all supervised learning, we have used two
popular supervised machine-learning algorithms,
multinomial Na??ve Bayes (NB) and Support Vec-
tor Machine (SVM), provided by the open-source
Java-based machine-learning library Weka 3.7
(Witten and Frank, 2005).
4 Results and Discussion
In the first classification task a classifier is trained
with the model-validated self-annotated corpus us-
ing 10-fold cross-validation. The model achieves
an F-measure score of 97% with NB and 98.7%
with SVM. See Table 4. The average F-measure
that Agarwal and Yu (2009) report for their 10-fold
cross-validation (which includes Introduction) is
91.55. The category F-measures that Agarwal and
Yu (2009) report for their 10-fold cross-validation
with the features that we use are: Method: 91.4
(95.04) (their best scores, in parentheses, require
inclusion of the IMRaD section as a feature), Re-
sult: 88.3 (92.24), and Conclusion: 69.03 (73.77).
In the last classification task, a classifier is
trained with the model-validated self-annotated
corpus and tested on the Agarwal and Yu (2009)
corpus. The F-measures in Table 5 are a substan-
tial improvement over those in Table 2.
(a) Classification with Multinomial Na??ve Bayes.
Class Precision Recall F-Measure
Method 0.981 0.957 0.969
Result 0.966 0.992 0.979
Conclusion 0.98 0.885 0.93
Average 0.971 0.971 0.971
(b) Classification with Support Vector Machine
Class Precision Recall F-Measure
Method 0.986 0.984 0.985
Result 0.988 0.995 0.992
Conclusion 0.986 0.95 0.968
Average 0.987 0.987 0.987
Table 4: Precision, Recall, F-measure : Classifier trained
with the model-validated self-annotated corpus (Method, Re-
sult, Conclusion) using 10-fold cross-validation
(a) Classification with Multinomial Na??ve Bayes.
Class Precision Recall F-Measure
Method 0.937 0.806 0.866
Result 0.763 0.873 0.814
Conclusion 0.836 0.911 0.872
Average 0.858 0.847 0.848
(b) Classification with Support Vector Machine
Class Precision Recall F-Measure
Method 0.893 0.824 0.857
Result 0.763 0.85 0.804
Conclusion 0.835 0.811 0.823
Average 0.837 0.832 0.833
Table 5: Precision, Recall, F-measure : Classifier trained
with the model-validated self-annotated corpus and tested on
a reduced Agarwal and Yu (2009) corpus (Method, Result,
Conclusion)
Sentence classification is important in determin-
ing the different components of argumentation.
We have suggested a method to annotate sentences
from scientific research papers into their IMRaD
categories, excluding Introduction. Our results
show that it is possible to extract a large self-
annotated corpus automatically from a large repos-
itory of scientific research papers that generates
very good supervised machine learned models.
Acknowledgments
This work was partially funded through a Natu-
ral Sciences and Engineering Research Council of
Canada (NSERC) Discovery Grant to R. Mercer.
22
References
Shashank Agarwal and Hong Yu. 2009. Automatically
classifying sentences in full-text biomedical articles
into introduction, methods, results and discussion.
Bioinformatics, 25(23):3174?3180.
Anita de Waard, Paul Buitelaar, and Thomas Eigner.
2009. Identifying the epistemic value of discourse
segments in biology texts. In Proceedings of the
Eighth International Conference on Computational
Semantics, IWCS-8 ?09, pages 351?354. Associa-
tion for Computational Linguistics.
Hospice Houngbo and Robert E. Mercer. 2012.
Method mention extraction from scientific research
papers. In Proceedings of COLING 2012, pages
1211?1222, Mumbai, India.
Susan Hunston. 2008. Starting with the small words.
Patterns, lexis and semantic sequences. Interna-
tional Journal of Corpus Linguistics, 13:271?295.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing speculative language in biomedical research ar-
ticles: A linguistically motivated perspective. BMC
Bioinformatics, 9(S-11):S10.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, ACL ?03, pages 423?430. Asso-
ciation for Computational Linguistics.
Maria Liakata, Shyamasree Saha, Simon Dob-
nik, Colin R. Batchelor, and Dietrich Rebholz-
Schuhmann. 2012. Automatic recognition of con-
ceptualization zones in scientific articles and two life
science applications. Bioinformatics, 28(7):991?
1000.
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
ACL ?07, pages 992?999. Association for Computa-
tional Linguistics.
Hagit Shatkay, Fengxia Pan, Andrey Rzhetsky, and
W. John Wilbur. 2008. Multi-dimensional classifi-
cation of biomedical text: Toward automated, prac-
tical provision of high-utility text to diverse users.
Bioinformatics, 24(18):2086?2093.
Simone Teufel, Jean Carletta, and Marc Moens. 1999.
An annotation scheme for discourse-level argumen-
tation in research articles. In Proceedings of the
Ninth Conference of the European Chapter of the
Association for Computational Linguistics, pages
110?117. Association for Computational Linguis-
tics.
Simone Teufel, Advaith Siddharthan, and Colin Batch-
elor. 2009. Towards discipline-independent ar-
gumentative zoning: Evidence from chemistry and
computational linguistics. In Proceedings of the
2009 Conference on Empirical Methods in Natu-
ral Language Processing: Volume 3, EMNLP ?09,
pages 1493?1502. Association for Computational
Linguistics.
Manabu Torii and K. Vijay-Shanker. 2005. Anaphora
resolution of demonstrative noun phrases in Medline
abstracts. In Proceedings of PACLING 2005, pages
332?339.
W. John Wilbur, Andrey Rzhetsky, and Hagit Shatkay.
2006. New directions in biomedical text annota-
tion: definitions, guidelines and corpus construction.
BMC Bioinformatics, 7:356.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann Series in Data Management Sys-
tems. Morgan Kaufmann Publishers Inc., San Fran-
cisco, CA, USA, 2nd edition.
Mizuta Yoko, Anna Korhonen, Tony Mullen, and Nigel
Collier. 2006. Zone analysis in biology articles as a
basis for information extraction. International Jour-
nal of Medical Informatics, 75:468?487.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opin-
ion sentences. In Proceedings of the 2003 Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ?03, pages 129?136. Associa-
tion for Computational Linguistics.
23
Proceedings of the First Workshop on Argumentation Mining, pages 98?99,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Titles That Announce Argumentative Claims in Biomedical Research
Articles
Heather Graves
Roger Graves
Department of English and Film Studies
The University of Alberta
Edmonton, Alberta, Canada
graves1,hgraves@ualberta.ca
Robert E. Mercer
Mahzereen Akter
Department of Computer Science
The University of Western Ontario
London, Ontario, Canada
mercer@csd.uwo.ca
Abstract
In the experimental sciences authors use
the scientific article to express their find-
ings by making an argumentative claim.
While past studies have located the claim
in the Abstract, the Introduction, and in
the Discussion section, in this paper we fo-
cus on the article title as a potential source
of the claim. Our investigation has sug-
gested that titles which contain a tensed
verb almost certainly announce the argu-
ment claim while titles which do not con-
tain a tensed verb have varied announce-
ments. Another observation that we have
confirmed in our dataset is that the fre-
quency of verbs in titles of experimental
research articles has increased over time.
1 Introduction
In this paper we are interested in determining
what is being claimed in articles in experimen-
tal (not clinical) biomedical literature, in partic-
ular. Claims have been studied in the argumen-
tation literature from many different standpoints
(White, 2009). Rhetorical structure theory was
developed from systemic functional linguistics to
map connections among texts (Mann and Thomp-
son, 1987); Argumentative zoning was developed
from Swales? CARS model of moves made in re-
search articles (Teufel, 1999; Teufel and Moens,
1999; Teufel and Moens, 2002). Toulmin-based
analysis has also been used to map the argumen-
tative structure of articles (Toulmin, 1958 2003;
Jenicek, 2006; Reed and Rowe, 2006; Graves et
al., 2013; Graves, 2013). With these models of
argument in mind, we view the claim of a scien-
tific argument as the conclusion that the authors
infer from known information and new informa-
tion (results from an experiment or other forms of
observations). Past studies locate the claim in the
Abstract (Kanoksilapatham, 2013), at the end of
the Introduction (Swales, 1990; Swales and Najjar,
1987; Kanoksilapatham, 2005; Kanoksilapatham,
2012), and in the Discussion section (Kanoksilap-
atham, 2005). Our observations of changes in the
genre of the research article have led us to per-
form a preliminary investigation of titles with the
outcome being a provisional typology.
2 Method
The Genia Tagger uses the Penn Treebank Tagset.
In the following we mention the verb tags from
this tagset: VB ? base form, VBD ? past tense,
VBG ? gerund, VBN ? past participle, VBP
? present tense non-3rd person singular, VBZ
? present tense 3rd person singular. We ap-
plied these tags to the dataset of biomedical ar-
ticle titles and abstracts used in this preliminary
study has been taken from MEDLINE, the well-
known biomedical bibliographic repository that
contains over 19 million citations and abstracts for
about 81% of these citations from approximately
5600 journals (NLM, 2013 accessed 3 February
2014). We have curated a small database using
biotextEngine and some locally developed tools.
3 Analysis
For each title we collect the following:
? cumulative frequency of all verb categories
? whether the title contains a VBP, VBZ, or
passive verb
? whether the title contains a nominalization
4 Findings
Our analysis so far has identified three typologies.
The articles can be categorized according to genre,
purpose and structure. For titles with verbs the
claim of the title is repeated several times: in the
98
5Our ?analysis ?so ?far ?has ?identified ?three ?typologies. ?The ?articles ?can ?be ?categorized ?
according ?to ?genre, ?purpose ?and ?structure. ?A ?typology ?based ?on ?genre ?distinguishes
among ?review ?articles, ?methodological ?articles, ?and ?experimental ?research ?articles. ?
Experimental ?research ?articles ?are ?divided ?into ?those ?that ?report ?clinical ?advances ?and ?
those ?that ?report ?new ?contributions ?to ?knowledge. ?A ?second ?typology ?divides ?articles ?by ?
purpose: ?some ?articles ?summarize ?the ?state ?of ?knowledge ?in ?a ?specialty ?area ?while ?others ?
present ?an ?argument ?for ?the ?results ?they ?report. ?A ?third ?typology ?distinguishes ?among ?
articles ?based ?on ?the ?structure ?of ?their ?titles. ?Some ?contain ?nouns ?and ?noun ?phrases ?that ?
describe ?the ?paper ?topic;? ?others ?contain ?verbs ?and ?verb-?like ?structures ?to ?indicate ?the ?
authors? ?stance ?towards ?the ?topic. ?
To ?highlight ?connections ?between ?these ?typologies, ?we ?conducted ?some ?theoretical ?
sampling ?(Eisenhardt ?& ?Graebner ?2007, ?Eisenhardt ?1989) ?by ?analyzing ?the ?argument ?
structure ?in ?the ?titles ?and ?abstracts ?of ?about ?10 ?sample ?articles. ?These ?samples ?provided ?
cases ?for ?use ?to ?test ?and ?inductively ?develop ?theoretical ?concepts ?to ?begin ?to ?account ?for ?
the ?relationships ?between ?the ?article ?genres, ?their ?purposes, ?and ?the ?title ?structure. ?This ?
analysis ?suggested ?several ?points ?of ?connection. ?For ?example, ?articles ?that ?summarize ?
knowledge ?use ?nouns ?and ?noun ?phrases;? ?review ?articles ?summarize ?knowledge ?to ?inform ?
readers, ?and ?their ?titles ?describe ?the ?topic. ?Experimental ?research ?articles ?that ?make ?a ?
contribution ?to ?knowledge ?present ?an ?argument;? ?their ?titles ?can ?contain ?verbs ?or ?verb-?like ?
structures ?that ?explicitly ?state ?the ?major ?claim. ?
Figure 1: Genre ty ology
Abstract, Introduc on, and Dis sion sections.
For articles without verbs, the claim does not ap-
pear in the title or introduction (it does appear in
t e b tract and discussion sections). A third find-
i g: the frequency of verbs in titles of experi en-
tal research articles has increased over time.
5 Discussion
We believe that our methods for identifying titles
could lead to better literature search techniques.
If researchers are able to identify the claim of
an article from a search of titles alone, they will
be able to evaluate the relevance of each article
more efficiently. We suspect that the increase in
titles with verbs and claims in them is an emerg-
ing trend, possibly the result of explicit editorial
policy. One side effect of including claims in ti-
tles may be higher quality writing by the authors.
Another result from using verbs in titles could be
the automation of claim extraction. Finally, hav-
ing research scientists use clear language to state
their claim can have the added benefit of making
knowledge translation more effective by lessening
the difficulty of reading scientific texts. This, in
turn, might afford greater access to the research
outcomes by clinical practitioners (one of the main
readerships of biomedical research).
References
Heather Graves, Shahin Moghaddasi, and Azirah
Hashim. 2013. Mathematics is the method: Explor-
ing the macro-organizational structure of research
articles in mathematics. Discourse Studies, 15:421?
438.
Heather Graves. 2013. The trouble with Toulmin for
teaching argument in science. In 11th Annual Tech-
nology for Second Language Learning Conference:
Technology and Teaching Writing for Academic Dis-
ciplines. ms.
Milos Jenicek. 2006. How to read, understand, and
write discussion sections in medical articles: An ex-
ercise in critical thinking. Med. Sci. Monitor, 12.
Budsaba Kanoksilapatham. 2005. Rhetorical structure
of biochemistry research articles. English for Spe-
cific Purposes, 24:269?292.
Budsaba Kanoksilapatham. 2012. Structure of re-
search article introductions in three engineering sub-
disciplines. IEEE Transactions on Professional
Communication, 55:294?309.
Budsaba Kanoksilapatham. 2013. Generic characteri-
sation of civil engineering research article abstracts.
3L: The Southeast Asian Journal of English Lan-
guage Studies, 19:1?10.
William C. Mann and Sandra A. Thompson. 1987.
Rhetorical structure theory: Description and con-
struction of text structures. In G. Kempen, editor,
Natural language generation: New results in artifi-
cial intelligence, psychology and linguistics, pages
85?95. Dordrecht: Martinus Nijhoff.
U.S. National Library of Medicine NLM. 2013 (ac-
cessed 3 February 2014). ?ncbi: Pubmed overview.
http://www.ncbi.nlm.nih.gov/
entrez/query/static/overview.html.
Chris Reed and Glenn Rowe. 2006. Translating Toul-
min diagrams: Theory neutrality in argument repre-
sentation. In David Hitchcock and Bart Verheij, ed-
itors, Arguing on the Toulmin model: New essays in
argument analysis and evaluation, pages 341?358.
Dordrecht: Springer.
John Swales and Hazem Najjar. 1987. The writing of
research article introductions. Written Communica-
tion, 4:175?192.
John Swales. 1990. Genre Analysis: English in Aca-
demic and Research Settings. Cambridge Applied
Linguistics. Cambridge University Press.
Simone Teufel and Mark Moens. 1999. Argumentative
classification of extracted sentences as a first step
towards flexible abstracting. In Inderjeet Mani and
Mark Maybury, editors, Advances in automatic text
summarization, pages 155?171. MIT Press.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: Experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409?445.
Simone Teufel. 1999. Argumentative Zoning : Infor-
mation Extraction from Scientific Text. Ph.D. the-
sis, School of Cognitive Science, University of Ed-
inburgh.
Stephen Toulmin. 1958-2003. The uses of argument.
Cambridge University Press.
Barbara White. 2009. Annotating a corpus of biomed-
ical research texts: Two models of rhetorical analy-
sis. Ph.D. thesis, The University of Western Ontario,
Canada.
99
Proceedings of the First Workshop on Argumentation Mining, pages 100?101,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Extracting Higher Order Relations From Biomedical Text
Syeed Ibn Faiz
Department of Computer Science
The University of Western Ontario
syeedibnfaiz@gmail.com
Robert E. Mercer
Department of Computer Science
The University of Western Ontario
mercer@csd.uwo.ca
Abstract
Argumentation in a scientific article is
composed of unexpressed and explicit
statements of old and new knowledge
combined into a logically coherent tex-
tual argument. Discourse relations, lin-
guistic coherence relations that connect
discourse segments, help to communicate
an argument?s logical steps. A biomedi-
cal relation exhibits a relationship between
biomedical entities. In this paper, we are
primarily concerned with the extraction
of connections between biomedical rela-
tions, a connection that we call a higher
order relation. We combine two methods,
namely biomedical relation extraction and
discourse relation parsing, to extract such
higher order relations from biomedical re-
search articles. Finding and extracting
these relations can assist in automatically
understanding the scientific arguments ex-
pressed by the author in the text.
1 Introduction
We use the term higher order relation to denote
a relation that relates two biomedical relations.
Consider, for example, the following sentence:
(1) Aspirin appeared to prevent VCAM-1 tran-
scription, since it dose-dependently inhibited
induction of VCAM-1 mRNA by TNF.
We can find two biomedical relations involving
Aspirin: Aspirin?prevents?VCAM-1 transcrip-
tion and Aspirin?inhibits?induction of VCAM-1
mRNA. These two relations are connected by the
word since. The higher order relation conveys a
causal sense, which indicates that the latter rela-
tion causes the earlier one. In genetic transcrip-
tion mRNA is generated (a process known by the
reader, so not expressed in the argument). This
piece of the author?s argument is that by observing
inhibition of mRNA induction (the genetic process
that activates transcription) by different doses of
aspirin, the inference that aspirin prevents the tran-
scription can be made. This inference is textually
signalled by the discourse connective since.
Formally, we define a higher order relation as a
binary relation that relates one biomedical relation
with another biomedical relation. In this paper we
propose a method for these extracting higher or-
der relations using discourse relation parsing and
biomedical relation extraction.
2 Extracting Higher Order Relations
There are two stages in our method for extracting
higher order relations from text. In the first stage
we use a discourse relation parser to extract the ex-
plicit discourse relations from text. In the second
stage we analyze each extracted explicit discourse
relation to determine whether it can produce a
higher order relation. We use a biomedical rela-
tion extraction system in this process. For each ar-
gument of an explicit discourse relation we find all
occurrences of biomedical relations in it. Higher
order relations are then constructed by pairing the
biomedical relations or observations found in the
discourse arguments. The sense of the explicit dis-
course relation is used to interpret all the higher
order relations derived from it.
Parsing an explicit discourse relation involves
three steps: identifying the explicit discourse con-
nective, the arguments and the sense. In (Faiz
and Mercer, 2013) we showed how to use syntac-
tic and surface level context to achieve a state-of-
the-art result for identifying discourse connectives
from text. Our work on a complete explicit dis-
course relation parser is presented in (Faiz, 2012).
For identifying the arguments of discourse con-
nectives we use the head-based representation pro-
posed by Wellner and Pustejovsky (Wellner and
Pustejovsky, 2007). We found that this head-based
100
representation is very suitable for the task of ex-
tracting higher order relations. The head of an
argument plays an important role in selecting a
biomedical relation as an argument to a higher or-
der relation.
This observation regarding the heads of the dis-
course arguments has another useful implication.
Since the biomedical relations that we have to con-
sider need to involve the argument head, we only
have to extract the portion of the argument that is
influenced or dominated by the head. One simple
way to do this is to consider the dependents of the
head in the dependency representation. Wellner
(2009) reported that finding the dependents of the
syntactic head of an argument often gives a good
approximation of the argument extent .
3 Evaluation
Our algorithm for extracting higher order relations
depends on discourse parsing and biomedical rela-
tion extraction. We have discussed our implemen-
tation of these components and evaluated their per-
formance in previous work (Faiz, 2012; Faiz and
Mercer, 2013). We have evaluated the algorithm
we present in this paper in terms of how accurately
it can use those components in order to find higher
order relations. More specifically, we will mea-
sure how accurately it can determine the part of
the full argument extent that contains the biomed-
ical entities in it.
For this evaluation we used the AIMed corpus
(Bunescu et al., 2005). This corpus contains an
annotation for protein-protein interactions. From
this corpus we collected 69 discourse relations.
For both ARG1 and ARG2 we performed two
tests. We measured from the argument heads how
many protein mentions occurring within the argu-
ment extent (the True Positives) are found and how
many protein mentions that lie beyond the argu-
ment extent (the False Positives) are found. For
ARG1, we found that our algorithm missed only
one protein mention and incorrectly found three
proteins from outside the argument extent, a pre-
cision of 98% and a recall of 99.32%. For ARG2,
we obtained a 100% precision and a 99% recall.
We conducted another experiment, which is
similar to the previous one except that now instead
of counting only the protein mentions, we counted
all the words that can be reached from an argument
head. In other words, this experiment evaluates
our algorithm in terms of how accurately it can
identify the full argument extent (i.e., the words
in it). For ARG1 and ARG2 we got an F-score of
91.98% and 92.98% respectively.
4 Discussion
Extraction of many higher order relations is de-
pendent on coreference resolution. For exam-
ple, in (1), Aspirin is anaphorically referred to in
ARG2. In our current implementation we lack
coreference resolution. Therefore, augmenting
a coreference resolution module in our pipeline
would be an immediate improvement.
In our implementation, we used a simple but
imperfect method to determine whether a biomed-
ical relation involves the head of a discourse ar-
gument. We checked whether the head appears
between the biomedical entities or within a short
distance from either one in the sentence. How-
ever, this simple rule may produce spurious higher
order relations. One way to improve this method
would be to consider the rules we presented for
rule-based biomedical relation extraction. Most of
the rules give a dependency path corresponding to
the relation they can extract. That path can then
be analyzed to determine whether the relation de-
pends on the head.
Acknowledgments
This work was partially funded by a Natural
Sciences and Engineering Research Council of
Canada (NSERC) Discovery Grant to R. Mercer.
References
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun K.
Ramani, and Yuk W. Wong. 2005. Comparative
experiments on learning information extractors for
proteins and their interactions. Artificial Intelligence
in Medicine, 33(2):139?155, February.
Syeed Ibn Faiz and Robert E. Mercer. 2013. Identify-
ing explicit discourse connectives in text. In Cana-
dian Conference on AI, pages 64?76.
Syeed Ibn Faiz. 2012. Discovering higher order rela-
tions from biomedical text. Master?s thesis, Univer-
sity of Western Ontario, London, ON, Canada.
Ben Wellner and James Pustejovsky. 2007. Automati-
cally identifying the arguments of discourse connec-
tives. In EMNLP-CoNLL, pages 92?101. ACL.
Ben Wellner. 2009. Sequence models and ranking
methods for discourse parsing. Ph.D. thesis, Bran-
deis University, Waltham, MA, USA. AAI3339383.
101
Proceedings of the First Workshop on Argumentation Mining, pages 106?107,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Extracting Imperatives from Wikipedia Article for Deletion Discussions
Fiona Mao
Robert E. Mercer
Department of Computer Science
The University of Western Ontario
London, Ontario, Canada
fiona.wt.mao@gmail.com
mercer@csd.uwo.ca
Lu Xiao
Faculty of Information and Media Studies
Department of Computer Science
The University of Western Ontario
London, Ontario, Canada
lxiao24@uwo.ca
Abstract
Wikipedia contains millions of articles,
collaboratively produced. If an article
is controversial, an online ?Article for
Deletion? (AfD) discussion is held to
determine whether the article should be
deleted. It is open to any user to participate
and make a comment or argue an opin-
ion. Some of these comments and argu-
ments can be counter-arguments, attacks
in Dung?s (1995) argumentation terminol-
ogy. Here, we consider the extraction of
one type of attack, the directive speech act
formed as an imperative.
1 Introduction
A large group of volunteers participate to make
Wikipedia one of the most successful collabora-
tive information repositories. To ensure the quality
of the encyclopedia, deletion of articles happens
continually. If an article is controversial, an on-
line discussion called ?Article for Deletion? (AfD)
is held to determine whether the article should be
deleted. It is open to any user to participate in the
discussion and make a comment or argue an opin-
ion. Some of these comments and arguments can
be counter-arguments, attacks in Dung?s (1995) ar-
gumentation terminology. A common argumenta-
tive attack is a directive speech act suggesting a
potential disagreement and a possible way to rec-
tify the matter. Here, we consider the extraction of
this type of attack when formed as an imperative.
Researchers are becoming increasingly inter-
ested in studying the content of Wikipedia?s Ar-
ticles for Deletion (AfD) forum. Schneider et al.
(2013) investigated the difference in arguments
from novices and experienced users. Xiao and
Askin (2014) examined the types of rationales in
Wikipedia AfD discussions.
2 Speech Acts and Imperatives
A speech act is an utterance that has performative
function in communication (Austin, 1975). Of the
three types of speech acts, Searle (1976) subcate-
gorized the illocutionary act, the act of expressing
the speaker?s intention, into five sub-groups. We
are interested here in the Directives sub-group.
Often, a directive can be viewed as an attack
(Dung, 1995), albeit an indirect one, e.g., ?Could
you provide the source to me??. The user, to
whom this directive is made, undercuts (Pollock,
1992) the attack by responding with some sources.
Ervin-Tripp (1976) lists six types of directives
one being the imperative. Imperatives express a
command. Typically the predicate is an action
verb and the subject, often eliminated, is second-
person (you). As well, there can be words of po-
liteness and adverbial modifiers of the verb:
? Please do this sort of check in the future.
? Just avoid those sorts of comments and per-
haps strike the one above.
Cohortatives (first person plural imperatives) are
normally used in suggestions such as, ?Let?s have
dinner together.? Some directive sentences from
AfD discussions are listed below:
? Add the information, and please give us some
information so we can judge these sources.
? Let?s avoid compounding the BLP issues
caused by the existence of this article, in vi-
olation of notability and blp policies, by hav-
ing it snow-deleted post-haste.
? You must first discuss the matter there, and
you need to be specific.
? Perhaps time would be better spent adding
more and improving the article rather than
just arguing here.
? Instead of complaining, how about finding
such content and improving the article?
106
Viewing the above examples, some users directly
suggest or command other users to do something
(the first one). Cohortatives include the user (the
second example). The third one is obviously com-
manding someone to discuss the matter first and
to be specific. The first three examples are imper-
atives. Some commands include politeness, as il-
lustrated by the last two examples. Since the form
of this kind of utterance varies, it is difficult to de-
fine a rule for recognizing it by computer. In this
paper, we only detect direct imperatives and leave
indirect imperative recognition for future work.
3 Detecting Imperatives
In English, a typical imperative is expressed by us-
ing the base form of a verb, normally without a
subject. To detect this kind of imperative, we need
to analyze the grammatical structure of sentences.
According to our observation, a typical imper-
ative contains a verb in base form without any
subject. Therefore, the basic rule for imperative
recognition is to find those sentences with a verb
(in its base form) as the root in the phrase struc-
ture and this particular verb has no subject child
in the dependency structure. Another form of im-
perative is like the sentence: ?You must first dis-
cuss the matter there, and you need to be specific?.
We have adapted a modal directive rule suggested
by Sinclair et al. (1975): We recognize the use of
a personal pronoun or noun (e.g., ?you?, ?we?,
or a username) followed by a modal verb (e.g.,
?should?, ?must?, ?need?) as an imperative. We
used keywords to detect this kind of imperative.
4 Evaluation
In this section, we evaluate the performance of our
methods to detect imperatives. Two human anno-
tators (undergraduate students at The University of
Western Ontario) extracted imperatives from our
data. Agreed upon imperatives became our gold
standard. Our system had Precision 0.8447, Re-
call 0.7337, and F-measure 0.7874 on this data.
Most false positives have an implicit subject ?I?
(e.g., Agree with most of the rest of this.), a writ-
ing style found in this text genre. Missed impera-
tives (false negatives) resulted from parsing errors
by the parsing tool and sentences with the form
of subject + modal verb, but the subject is a noun
(person or organization) instead of a pronoun. Our
method keyed on pronouns.
5 Related Work
Marsi?s (1997) definition of imperative mood is
too restrictive for our purposes here. A use of
Argumentative Zoning to critique thesis abstracts
(Feltrim et al., 2006) gives no details regarding the
imperative sentence recognition techniques, and
the language of interest is Brazilian Portuguese.
Acknowledgments
This project is partially supported by the Discov-
ery program of The Natural Sciences and Engi-
neering Research Council of Canada (NSERC).
References
John Langshaw Austin. 1975. How To Do Things with
Words. Oxford University Press.
Phan Minh Dung. 1995. On the acceptability of ar-
guments and its fundamental role in nonmonotonic
reasoning, logic programming and n-person games.
Artificial Intelligence, 77(2):321?357.
Susan Ervin-Tripp. 1976. Is Sybil there? The structure
of some American English directives. Language in
Society, 5(01):25?66.
Val?eria Feltrim, Simone Teufel, Maria das Grac?as V.
Nunes, and M. Alu??sio, Sandra. 2006. Argumen-
tative zoning applied to critiquing novices? scien-
tific abstracts. In James G. Shanahan, Yan Qu, and
Janyce Wiebe, editors, Computing Attitude and Af-
fect in Text: Theory and Applications, pages 233?
246. Springer Netherlands.
Erwin Marsi. 1997. A reusable syntactic generator for
Dutch. In Peter-Arno Coppen, Hans van Halteren,
and Lisanne Teunissen, editors, Computational Lin-
guistics in the Netherlands 1997: Selected papers
from the Eighth CLIN Meeting, pages 205?222. Am-
sterdam/Atlanta: Rodopi.
John L. Pollock. 1992. How to reason defeasibly. Ar-
tificial Intelligence, 57:1?42.
Jodi Schneider, Krystian Samp, Alexandre Passant, and
Stefan Decker. 2013. Arguments about deletion:
How experience improves the acceptability of argu-
ments in ad-hoc online task groups. In Proceedings
of the 2013 Conference on Computer Supported Co-
operative Work, pages 1069?1080. ACM.
John R Searle. 1976. A classification of illocutionary
acts. Language in Society, 5(01):1?23.
J.M.H. Sinclair and M. Coulthard. 1975. Towards an
analysis of discourse: The English used by teachers
and pupils. Oxford University Press.
Lu Xiao and Nicole Askin. 2014. What influences on-
line deliberation? A Wikipedia study. J. of the As-
sociation for Information Science and Technology.
107
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 147?153,
Baltimore, Maryland, USA. June 27, 2014. c?2014 Association for Computational Linguistics
The Use of Text Similarity and Sentiment Analysis to Examine Ra-tionales in the Large-Scale Online Deliberations  
  Wanting Mao Department of Computer Science The University of Western Ontario London, ON, Canada fiona.wt.mao@gmail.com  
Lu Xiao Faculty of Information & Media Studies The University of Western Ontario London, ON, Canada lxiao24@uwo.ca 
Robert Mercer Department of Computer Science The University of Western Ontario London, ON, Canada mercer@csd.uwo.ca     
Abstract 
To overcome the increasingly time con-suming and potentially challenging iden-tification of key points and the associated rationales in large-scale online delibera-tions, we propose a computational lin-guistics method that has the potential of facilitating this process of reading and evaluating the text. Our approach is novel in how we determine the sentiment of a rationale at the sentence level and in that it includes a text similarity measure and sentence-level sentiment analysis to achieve this goal. 1 Introduction In an online deliberation situation where users join in and offer their opinions or suggestions, they are expected to provide the rationales that justify their standpoints in the deliberation.  In the final decision making process, one expected-ly needs to read through the content and weigh different key points and related rationales. Wik-ipedia Article for Deletion (AfD) deliberations represent one such example. In the Wikipedia community, any member can propose to delete an existing Wikipedia article. After an article is proposed to delete, a deliberation topic about the article is opened in the AfD forum. The commu-nity members can express their opinions (e.g., to keep or to delete the article) and provide their rationales within the specified time period. After that, a community member (often a Wikipedia administrator) closes the deliberation by making the final decision. Researchers have analyzed the Wikipedia AfD forum and have demonstrated 
that it presents a successful example of large-scale online deliberation by allowing many peo-ple to participate equally, encouraging people to deliberate, and producing rational and meaning-ful rationales (e.g., Schneider et al., 2012; Xiao & Askin, 2014). Wikipedia policy requires that the final decision about the article should be made based on the discussed rationales instead of the count of opinion votes. In practice many Wikipedia members who close the deliberations follow this policy, which implies the potential problem of representing the diverse rationales and identifying the influential ones in this con-text. Generating the final decision of a large scale online deliberation can become a daunting task, as the amount of opinions and rationales in the deliberation content increases significantly. To facilitate this decision making process in large-scale online deliberations, we have developed a method that uses an existing text-to-text similari-ty measure and our developed sentence-level sentiment analysis algorithm to address this issue. Specifically, we first group participants? opinions according to the similarity measure, then we identify the positive, neutral, and negative senti-ments suggested by the participants' rationales in each group, and finally we choose a representa-tive rationale from each sentiment category in a group. With our method the diverse opinions and rationales are presented to the final decision maker through a representative set of the ration-ales, reducing the redundant information from the deliberation content so as to make the process of reading and evaluating the deliberation con-tent more efficient.  
147
2 Related Work 2.1 Text Similarity Recognizing the relation between texts (e.g., sentence to sentence, paragraph to paragraph) could help people better understand the context.  Text similarity can be interpreted as similarity between sentences, paragraphs, documents, etc. It has been used in various aspects in NLP such as information retrieval, text classification, and automatic evaluation. The most fundamental part is word similarity. We consider words to be simi-lar in the following conditions: synonyms, anto-nyms, similar concept (e.g., red, green), similar context (e.g., doctor, hospital), and hypo-nym/hypernym relation (e.g., dog, pet). WordNet, a word-to-word similarity library was developed by Pedersen et al. (2004), and has been widely used to compute the similarity at a coarser granularity (e.g., sentence-to-sentence similarity). Various methods to deal with text similarity have been proposed over the past dec-ades. Mihalcea et al. (2006) proposed a greedy method to calculate the similarity score between two texts T1 and T2. Basically for each word in T1 (T2), the maximum similarity score to any word in T2 (T1) is used. The WordNet similarity can be used for assigning similarity scores be-tween every pair of words in the two texts. Rus and Lintean (2012) proposed an optimal method to compute text similarity based on word-to-word similarity. It is similar to the opti-mal assignment problem. Given a weighted complete bipartite graph (G = X ? Y; X ? Y), with weight w(xy) on edge xy, we need to find a matching from X to Y with a maximum total weight. Their results showed that the optimal method outperformed the greedy method in terms of accuracy and kappa statistics. Other statistics-based algorithms are also de-veloped to measure text similarity, e.g., the use of the Latent Dirichlet Allocation (LDA) model (Rus et al., 2013). 2.2 Sentiment Analysis Sentiment analysis is meant to determine the polarity of a certain text, which can be positive, negative or neutral. Related academia and indus-tries have been extensively investigating senti-ment analysis methods over the last decade. While most of the early work in sentiment analy-sis is aimed at analyzing the polarity of customer reviews (e.g., Kim and Hovy, 2004; Hu and Liu, 2004; Turney, 2002), there is a proliferation in 
analyzing social media text (e.g., Balahur, 2013; Liebrecht et al., 2013; Bakliwal et al., 2012; Montejo-Raez et al., 2012) and online discus-sions (e.g., Sood et al., 2012a, 2012b).  Researchers have used a variety of approach-es to detect the sentiment polarity of the given text. For example, in Kim and Hovy's system (2004) the sentiment region of the opinion is identified based on the extracted opinion holders and topics. The system combines the sentiments of the sentiment region and the polarity of the words to determine the polarity of the given text. In Li and Wu?s (2010) study, they interpreted the article as a sequence of key words and calcu-lated the sentiment score of each key word based on the dictionary and its privative and modifier near it. In the analysis of the tweets, Balahur (2013) replaced the sentiment words and modifi-ers by sentiment labels (positive, negative, high positive and high negative) or modification la-bels (negator, intensifier or diminisher), and then applied Support Vector Machine Sequential Min-imal Optimization (SVM SMO) to classify three different data sets. Online discussions may have inappropriate use of language in some cases, which affects the online community management negatively. Sood et al. (2012a) proposed a multistep classifier by combining valence analysis and a SVM to detect insults and classify the insult object. Researchers have also looked at the use of de-pendency tree-based method for sentiment classi-fication. For instance, Nakagawa et al. (2010) used a probabilistic model of the information garnered from the dependency tree to determine the sentiment of a sentence. Rentoumi et al. (2010) combines word sense disambiguation, a rule-based system, and Hidden Markov Models (HMMs) to deal with figurative language (e.g. record-shattering day) in sentiment analysis. Moilanen and Pulman (2007) presented a com-positional model for three-class (positive, nega-tive, and neutral) phrase-level and sentence-level sentiment analysis. In their algorithm, each bi-nary combination of a Head and Complement had a rule that determined which of the Head and Complement polarities dominated. In exceptional cases the rule inverts the polarity of the subordi-nate.  Socher et al. (2013) developed a Recursive Neural Tensor Network (RNTN) model. The au-thors showed that the accuracy obtained by RNTN outperformed a standard recursive neural network (RNN), matrix-vector RNN (MV-RNN), Naive Bayes (NB) and SVM. The advantage of 
148
RNTN is especially evident when compared with the methods that only use bag of words (NB and SVM). This indicates the importance of using parse trees during sentiment analysis.  3 A Method for Identifying Representa-tive Rationales in Online Delibera-tions Our observation of the Wikipedia AfD forum suggests that one topic (e.g., notability) can ap-pear multiple times in different rationales by dif-ferent users. For example, two users? comments ??Could be redirected to OpenXMA, the content of which isn't all that different from this article? and ?Redirected to OpenXMA as suggested??are considered redundant. The redundant information itself does not add a new perspective to final decision making. On the other hand, sometime the information about the same type of rationale represents different opinions about it. Here is one such example from an article?s deletion discussion: ?redirecting the page to the lead actors future projects section will be cool? and ?I don't think it is wise to redi-rect to the original film?. To make the final decision making process more efficient, compared to human reading of all the deliberation content, we have developed a method that includes a text-to-text similarity measure and a sentence-level sentiment analysis algorithm. Specifically, we use text similarity to 
group the rationales according to the aspects they reflect so we can select some rationales from each aspect group instead of all of them. We note that although the rationales are redundant in showing the same aspect, the redundancy implies the importance of the aspect in the deliberation since they are used multiple times by users in justifying their opinions. So in our method, we record the number of members that proposed the same aspect assuming that this would indicate the level of importance of the aspect to some ex-tent. . With the rationales grouped according to the aspects that they involve (e.g., notability, credi-bility, etc), our method examines the sentiment polarity of each rationale in a group to further examine whether the rationale is positive or neg-ative (e.g., the article is notable or not), or is neu-tral about the aspect. Then we can identify the representative rationales of an opinion by choos-ing those that have the highest similarity score in a group. In sum, the text-to-text similarity measure combined with our sentence-level sen-timent analysis algorithm helps us identify the representative rationales of diverse opinions in an online deliberation. An overview of our meth-od is shown in Figure 1.  We applied our method in analyzing Wikipe-dia Article for Deletion (AfD) deliberation con-tent. Next we discuss how this method is used to analyze the content. 
 Figure	 ?1.	 ?An	 ?overview	 ?of	 ?our	 ?method	 ?for	 ?identifying	 ?representative	 ?rationales	 ?from	 ?large-??scale	 ?online	 ?delibera-??tion3.1 Text-to-Text Similarity Measure In our study, we used SEMILAR, a semantic similarity toolkit (Rus et al., 2013), to measure We tested three similarity approaches provided in SEMILAR: optimum method based on Word-Net, similarity based on Latent Semantic Analy-sis (LSA) and similarity based on Latent Di-richlet Analysis (LDA). We first extracted 80 pairs of sentences from the Wikipedia AfD fo-rum and manually annotated them as similar or 
not. We then used these annotated results in measuring the accuracy of the three SEMILAR approaches. SEMILAR assigns a similarity score to each pair of sentences ranging from 0 to 1. To evaluate the accuracy of the three approaches, we identified a threshold to divide the result into two groups (i.e., similar and not similar). To do so, we computed the accuracy for 101 thresholds ranging from 0.00 to 1.00 with an interval of 0.01 to find the highest accuracy. Through this approach, we identified that the WordNet-based 
149
optimum method achieved the best accuracy of 76.3% at threshold 0.13. The other two methods achieved similar accuracy (76.3% and 75% re-spectively) but took more than double the time to process. Therefore, we chose the WordNet-based optimum method.  With this method, we have a similarity matrix that shows the similarity score between every pair of sentences in the discussion. We transform the similarity matrix to a dissimilarity matrix by transforming the similarity score x for two sen-tences to the distance between the sentences 1/x. Then we used hierarchical clustering (Kaufman and Rousseeuw, 2009) to cluster the sentences into groups. To do so, we set the maximum al-lowed distance between two similar sentences to be 8 (i.e., the similarity score would be 0.125), and used the agglomerative approach to form the clusters. As a consequence, the sentences in the same group are related to a common theme. 3.2 Sentence-Level Sentiment Analysis In our sentiment analysis algorithm, each word in a sentence is assigned a prior polarity based on an adapted MPQA Subjectivity Lexicon (Pedersen et al., 2004). Compared to the original Lexicon, this adapted one includes additional sentiment words that are important for the Wik-ipedia?s AfD discussions (e.g., notable). Then, using the syntactic and dependency trees of the sentence, the algorithm calculates each word?s current polarity score which can be affected by its children?s polarity scores. Through this ap-proach, the root?s current polarity score becomes the sentence?s polarity score.  The children?s polarity scores can affect the parent?s prior polarity score positively or nega-tively. The positive or neutral effect of the chil-dren?s polarity scores is reflected through sum-ming the children?s polarity scores and then ad-ding the sum to the parent?s polarity score. The negative effect is reflected through summing the children?s polarity scores and then multiplying the sum to the parent?s polarity score. Because our algorithm only considers three sentiment sit-uations: negative, positive, and neutral, it is the negation of the parent?s prior polarity that affects the accuracy of our algorithm the most. There-fore, the core of our algorithm is a recursive method that examines different negation situa-tions in the input sentence, starting from the leaf node of the sentence?s dependency tree.  We use this tree structure because it helps us detect the most of the negation situations: 1. I agree that the place is notable. 
2. I don?t agree that the place is notable. (Local Negation) 3. I disagree that the place is notable. (Pred-icate Negation) 4. Neither one of us agrees that the place is notable. (Subject Negation) 5. It is a violation of notability. (Preposition Negation) However, there is one negation situation that cannot be detected from the syntactic structure of the sentence. For example, in the sentence ?the place is of indeterminable notability?, notability is a positive word, but as it is modified by a neg-ative word indeterminable the phrase becomes negative. This negation case is called modifier negation. A negative modifier might also negate a negative word, such as little damage, never fail. However a negative modifier does not always negate the polarity of the phrase determined by the polarity of the related word. Instead, the phrase remains its prior polarity, e.g., terribly allergic.  It is also worth noticing that context affects the phrase polarity. Consider the phrase original research in our study context ? the Wikipedia AfD forum. Because articles reporting original research violate Wikipedia?s neutrality policy, the phrase original research in the deletion dis-cussions should be considered to be negative. As there is no straightforward way of deter-mining whether or not a modifier negates the polarity of the word being modified, we decided to use machine learning methods to help classify the modifier negation cases. We considered the following modifier phrases in the study and at least one word in the phrase has to be a sentiment word: ? Noun modified by adjective ? Noun modified by noun ? Adjective modified by adverb  ? Adverb modified by adverb ? Verb modified by adverb We used six attributes to describe a two-word phrase: first word token, second word token, first word polarity, second word polarity, first word part-of-speech (POS), and second word POS. The machine learning algorithm is expected to predict the polarity of a word pair given these six attributes of the pair. To build our machine learn-ing model, we obtained 961 two-word phrases from the AfD forum and annotated their polari-ties manually. They all follow the modifier nega-tion combinations discussed earlier and at least one of the two words is a sentiment word. The selected phrases are balanced in terms of the 
150
number of positive, negative, and neutral cases represented in the data set. We then used Weka (Hall et al., 2009) to evaluate the performance of three machine learning algorithms with 10-fold cross validation: Naive Bayes, k-nearest neigh-bor (KNN) and decision tree. The results showed that the accuracy produced by KNN is the high-est among the three methods. We further identi-fied that when the k value is 1, the KNN perfor-mance is the best. Thus we selected the KNN method in detecting modifier negation in our method.  Figure 2 shows the calculated polarity score for the sentence ?Neither one of us agrees that the place is notable?.  
 Figure	 ? 2.	 ? Polarity	 ? score	 ? on	 ? every	 ? node	 ? of	 ? the	 ? sen-??tence?	 ?dependency	 ?structure	 ?As shown in the figure, there are two positive words agree and notable and one negative word neither. If we simply use a bag of words ap-proach and add the polarity scores together, we would get a result of positive. However, the neg-ative word neither, being part of the subject, plays a dominant role in this sentence. Our algo-rithm is able to detect that negation influence: the root node is a verb and not neutral, so its current polarity score is the product of its prior polarity +1 multiplied by that of the node notable, which is also +1. Then because of the subject negation, the final polarity score of the root node is the multiplication of its current polarity score by the polarity score of the subject node, which is -1. 4 Evaluation and Discussion To evaluate the performance of our sentiment polarity prediction algorithm, we randomly se-lected 236 sentences from the Wikipedia AfD forum and manually annotated their sentiment polarity. 83 sentences are annotated as positive, 102 as negative and 51 as neutral. With our algo-rithm that includes the machine learning process to detect modifier negations, the accuracy is 
60.2%. In Socher et al.?s (2013) evaluation of their algorithm, 5-class (very negative, negative, neutral, positive, very positive) and 2-class (neg-ative, positive) predictions of sentence-level sen-timent analysis reached an accuracy of 45.7% and 85.4% respectively. We anticipate that the accuracy of their algorithm for 3-class prediction would be around 60%. 	 ?For sentence-level sentiment analysis, Moil-anen and Pulman?s algorithm obtained an accu-racy of 65.6%.	 ?Our algorithm differs from Moil-anen and Pulman in two ways: (1) the node-based computation is more general, i.e. for verbs, prepositions, and subjects it is a simple combina-tion (multiplication or addition) of the subordi-nate nodes' polarities, and for local negation it is an inversion of the subordinate polarity; (2) a trained classifier serves two functions: it fulfills the role of determining the contextual informa-tion and it determines whether a modifier chang-es the polarity of what it modifies. .	 ?	 ?5 Conclusion Deliberation is a method of logical communica-tion that rationalizes the process of reaching a decision. To reach the decision, people often need to weigh different opinions and rationales expressed in the deliberation. Given the prolifer-ation of online platforms and communities for collective decision making and knowledge crea-tion, online deliberation is becoming an increas-ingly important and common approach of engag-ing large numbers of people to participate in the decision making processes. One foreseen issue in such a context is the daunting tasks of reading through all the deliberation content, and identify-ing and evaluating diverse key points and related rationales.  Our study is interested in addressing the issue through a computational linguistic approach. We developed an approach that combines a text-to-text similarity technique with a sentence-level sentiment analysis method. The deliberation con-tent is first divided into groups based on the similarity of texts, then within each group we use a recursive algorithm to examine the sentiment polarity of each sentence according to the identi-fied similar topic to further classify the sentences into three groups: positive, neutral, and negative. Although not discussed in this paper, it is a sim-ple step to identify the representative rationales of diverse opinions by choosing those that have the highest similarity score in each polarity group. 
151
Acknowledgement This project is partially supported by the Discovery program of The Natural Sciences and Engineering Research Council of Canada (NSERC). References Akshat Bakliwal, Piyush Arora, Senthil Madhappan-Nikhil Kapre, Mukesh Singh and Vasudeva Varma, Mining Sentiments from Tweets, Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis. 11?18, Je-ju, Republic of Korea, 2012  Alexandra Balahur. Sentiment Analysis in Social Me-dia Texts. WASSA 2013, page 120. 2013. Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H Witten. The Weka data mining software: an Update. ACM SIGKDD Explorations Newsletter, 11(1):10?18, 2009.  Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge discovery and data mining, pages 168?177. ACM, 2004. Leonard Kaufman and Peter J Rousseeuw. Finding groups in data: an introduction to cluster analysis, volume 344. John Wiley & Sons, 2009. Soo-Min Kim and Eduard Hovy. Determining the sentiment of opinions. In Proceedings of the 20th international conference on Computational Lin-guistics, page 1367. Association for Computational Linguistics, 2004.  Nan Li, and Desheng Dash Wu. Using text mining and sentiment analysis for online forums hotspot detection and forecast. Decision Support Sys-tems 48(2):354-368. 2010. Christine Liebrecht, Florian Kunneman, and Antal van den Bosch (2013). The perfect solution for de-tecting sarcasm in tweets# not, Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, 29?37, Atlanta, Georgia, 2013. Rada Mihalcea, Courtney Corley, and Carlo Strap-parava. Corpus-based and knowledge-based measures of text semantic similarity. In AAAI, volume 6, pages 775?780, 2006.  Karo Moilanen and Stephen Pulman. Sentiment com-position. In In Proceedings of Recent Advances in Natural Language Processing, pages 378 ? 382, 2007. Arturo Montejo-Raez, Eugenio Marti?nez-Camara, M. Teresa Martin-Valdivia and L.Alfonso Urena-Lopez, Random Walk Weighting over SentiWord-Net for Sentiment Polarity Detection on Twitter, 
Proceedings of the 3rd Workshop on Computation-al Approaches to Subjectivity and Sentiment Anal-ysis. 11?18, Jeju, Republic of Korea, 2012. Tetsuji Nakagawa, Kentaro Inui, and Sadao Kuro-hashi. Dependency tree-based sentiment classifica-tion using CRFs with hidden variables, In Human Language Technologies: The 2010 Annual Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics, pages 786-794, 2010. Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. Wordnet:: Similarity: measuring the re-latedness of concepts. In Demonstration Papers at HLT-NAACL 2004, pages 38?41. Association for Computational Linguistics, 2004. Vassiliki Rentoumi, Stefanos Petrakis, Manfred Klen-ner, George A. Vouros, and Vangelis Karkaletsis. United we stand: Improving sentiment analysis by joining machine learning and rule based methods. In Proceedings of the Seventh conference on Inter-national Language Resources and Evaluation (LREC?10), Valletta, Malta. pages 1089 ? 1094, 2010. Vasile Rus and Mihai Lintean. A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity met-rics. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 157?162. Association for Computational Linguistics, 2012. Vasile Rus, Mihai Lintean, Rajendra Banjade, Nobal Niraula, and Dan Stefanescu. Semilar: The seman-tic similarity toolkit. In Proceedings of the 51st Annual Meeting of the Association for Computa-tional Linguistics: System Demonstrations, pages 163?168.  2013. Jodi Schneider, Alexandre Passant, and Stefan Decker. Deletion discussions in Wikipedia: Decision fac-tors and outcomes. In Proceedings of the Eighth Annual International Symposium on Wikis and Open Collaboration, page 17. ACM, 2012. Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment tree-bank. In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing (EMNLP), pages 1631?1642, 2013. Sara Owsley Sood, Elizabeth F. Churchill, and Judd Antin. Automatic identification of personal insults on social news sites. J. Am. Soc. Inf. Sci., 63: 270?285. 2012a. Sara Sood, Judd Antin, and Elizabeth Churchill. 2012. Profanity use in online communities. In Proceed-ings of the SIGCHI Conference on Human Factors 
152
in Computing Systems (CHI '12). ACM, New York, NY, USA, 1481-1490, 2012b. Peter D Turney. Thumbs up or thumbs down?: seman-tic orientation applied to unsupervised classifica-tion of reviews. In Proceedings of the 40th Annual Meeting of the Association for Computational Lin-
guistics, pages 417?424. Association for Computa-tional Linguistics, 2002. Lu Xiao and Nicole Askin. What influences online deliberation? A Wikipedia study. Journal of the Association for Information Science and Technol-ogy, 65, pages 898?910, 2014 
 
153
