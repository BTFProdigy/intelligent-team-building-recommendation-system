Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 673?680
Manchester, August 2008
Recent Advances in a Feature-rich Framework for Treebank Annotation
Petr Pajas
Charles Univ. in Prague, MFF
?
UFAL
Malostransk?e n?am. 25
118 00 Prague 1 ? Czech Rep.
pajas@ufal.ms.mff.cuni.cz
Jan
?
St
?
ep
?
anek
Charles Univ. in Prague, MFF
?
UFAL
Malostransk?e n?am. 25
118 00 Prague 1 ? Czech Rep.
stepanek@ufal.ms.mff.cuni.cz
Abstract
This paper presents recent advances in
an established treebank annotation frame-
work comprising of an abstract XML-
based data format, fully customizable ed-
itor of tree-based annotations, a toolkit
for all kinds of automated data process-
ing with support for cluster computing, and
a work-in-progress database-driven search
engine with a graphical user interface built
into the tree editor.
1 Introduction
Constructing a treebank is a complicated process.
Among other things it requires a good choice of
tools, varying from elementary data conversion
scripts over annotation tools and tools for consis-
tency checking, to tools used for semi-automatic
treebank building (POS taggers, syntactic parsers).
If no existing tool fits the needs, a new one has to
be developed (or some existing tool adapted or ex-
tended, which, however, seldom happens in prac-
tice). The variety of tools that exist and emerged
from various treebanking projects shows that there
is no simple solution that would fit all. It is some-
times a small missing feature or an incompati-
ble data format that disqualifies certain otherwise
well-established tools in the eyes of those who de-
cide which tools to use for their annotation project.
This paper presents an annotation framework
that was from its very beginning designed to be
extensible and independent of any particular anno-
tation schema. While reflecting the feedback from
several treebanking projects, it evolved into a set
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
of generic tools that is open to all kinds of anno-
tations that involve tree structures. By this paper
we would like not only to promote this framework,
but also show that due to its open nature, it may
be easily extended to fit new requirements. The
first three sections describe base components of
the framework, an abstract data format, a versatile
annotation tool for tree-oriented annotations, and
a framework for automatic annotation processing;
some of these components have been mentioned
in earlier publications, but the framework has nei-
ther been published in its integrity nor described
in much detail. The last section describes a query
engine that is a newest addition to the framework,
first presented by this paper.
2 Data format
The base data format selected for the present an-
notation framework, both for data exchange and
as a memory-model reference, is PML (Pajas and
?
St?ep?anek, 2006). PML is an abstract XML-based
format intended to be generally applicable to all
types of annotation purposes, and especially suit-
able for multi-layered treebank annotations fol-
lowing the stand-of principles. A notable fea-
ture that distinguishes PML from other encoding
schemes, like Tiger-XML (Mengel and Lezius,
2000), XCES (Ide and Romary, 2003), or maybe
even SynAF by ISO TC37/SC4 (Declerck, 2006),
is its generic and open nature. Rather than being
targeted to one particular annotation schema or be-
ing a set of specifically targeted encoding conven-
tions, PML is an open system, where a new type
of annotation can be introduced easily by creating
a simple XML file called PML schema, which de-
scribes the annotation by means of declaring the
relevant data types and possibly assigning certain
roles to these data types. The roles in the con-
673
text of PML are just labels from a pre-defined set
that can be used to mark the declarations accord-
ing to their purpose. For instance, the roles indi-
cate which data structures represent the nodes of
the trees, how the node data structures are nested
to form a tree, which field in a data structure car-
ries its unique ID (if any), or which field carries
a link to the annotated data or other layers of an-
notation, and so on. PML schema can define all
kinds of annotations varying from linear annota-
tions through constituency or dependency trees, to
complex graph-oriented annotation systems. The
PML schema provides information for validating
the annotation data as well as for creating a rel-
evant data model for their in-memory representa-
tion.
To give an example, the annotation of the Prague
Dependency Treebank 2.0 (PDT) (Haji?c and oth-
ers, 2006), which was published in the PML for-
mat, consists of four annotation layers, each de-
fined by its own PML schema: a lowest word-form
layer consisting of tokenized text segmented just
into documents and paragraphs; a morphological
layer segmenting the token stream of the previ-
ous layer to sentences and attaching morphological
form, lemma, and tag to each token; an analytical
layer building a morpho-syntactic dependency tree
from the words of each sentence (morphologically
analyzed on the previous layer); and a tectogram-
matical layer consisting of deep-syntactic depen-
dency trees interlinked in a N :M manner with the
analytical layer and a valency lexicon and carrying
further relational annotation, such as coreference
and quotation sets. All these features are formally
described by the respective PML schemas.
The fundamental toolkit for PML comprises of
a validator (based on compiling PML schemas to
RelaxNG grammars accompanied by Schematron
rules), and API, consisting of a Perl library (ba-
sic interfaces for Java and C++ are planned). The
input/output functions of the library are modular
and can work with local files as well as with re-
mote resources accessible via HTTP, FTP or SSH
protocols (with pluggable support for other pro-
tocols). Additionally, the library supports on-
the-fly XSLT-based format conversions that can
be easily plugged in via a simple configuration
file. Consequently, the API can transparently
handle even non-PML data formats. Currently
there are about a dozen input/output conversion fil-
ters available, covering various existing data for-
mats including the TigerXML format, the for-
mats of the Penn Treebank (Marcus et al, 1994),
the CoNLL-X shared task format (Buchholz and
Marsi, 2006), and the formats of the Latin Depen-
dency (Bamman and Crane, 2006), Sinica (Chu-
Ren et al, 2000), Slovene Dependency (D?zeroski
et al, 2006) (SDT), and Alpino (van der Beek et
al., 2002) treebanks. Support for XCES formats
is planned as soon as a final release of XCES is
available.
This basic toolkit is further supplemented by
various auxiliary tools, such as pmlcopy which
allows one to copy, move, rename, or GZip sets
of interconnected PML data files without breaking
the internal URL-based references.
3 Tree Editor
The heart of the annotation framework is a multi-
platform graphical tree editor called TrEd, (Haji?c
et al, 2001).
TrEd was from the beginning designed to be
annotation-schema independent, extensible and
configurable. TrEd can work with any PML data
format whose PML schema correctly defines (via
roles) at least one sequence of trees. Beside PML
format, TrEd can work with many other data for-
mats, either by means of the modular input/output
interface of the PML library or using its own in-
put/output backends.
The basic editing capabilities of TrEd allow the
user to easily modify the tree structure with drag-
and-drop operations and to easily edit the asso-
ciated data. Although this is sufficient for most
annotation tasks, the annotation process can be
greatly accelerated by a set of custom extension
functions, called macros, written in Perl. Macros
are usually created to simplify the most common
tasks done by the annotators. They can be called
either from menu or by keyboard shortcuts.
Although TrEd ensures that the result of the an-
notation is in accord with the related PML schema,
there is still a chance that an annotator errs in some
other aspect of the annotation. For this reason
TrEd offers the possibility to write macros that in-
corporate custom consistency tests into the built-in
editing commands of TrEd. Such tests can prevent
the user from making accidental mistakes (like as-
signing a case to a verb or subordinating a Subject
to a particle). Macros can also completely disable
some dangerous editing commands (for example,
the PDT annotation modes in TrEd disable the pos-
674
sibility to add or delete tokens or trees).
While macros provide means to extend, accel-
erate and control the annotation capabilities of
TrEd, the concept of style-sheets gives users con-
trol over the visual presentation of the annotated
data. Style-sheets, among other, offer the possi-
bility to: visually differentiate nodes and edges by
color, shape, size or line style according to arbi-
trary criteria; assemble the data associated with
nodes and edges to node and edge labels; alter
node positioning and padding; visualize additional
edges and cross-structure relations by means of
arrows or other types of connections; control the
content and styling of the text (usually the an-
notated sentence) displayed in a box above the
tree. TrEd can also balance trees, visualize dis-
connected groups of nodes, zoom the tree view ar-
bitrarily, and display trees in a vertical mode, see
Fig. 1.
Figure 1: The same tree displayed using horizontal
and vertical display mode in TrEd
Due to model/view separation, TrEd provides
means for controlling which nodes are actually dis-
played (it is thus possible to write modes that col-
lapse subtrees, hide auxiliary nodes, completely
skip some levels of the tree, display multiple trees
at once (Fig. 2), or even display additional ?vir-
tual? nodes and edges that are not actually present
in the underlying data structures).
So far, TrEd has been selected as annotation
tool for PDT and several similarly structured tree-
banking projects like Slovene (D?zeroski et al,
2006), Croatian (Tadi?c, 2007), or Greek Depen-
dency Treebanks (Prokopidis et al, 2005), but also
for Penn-style Alpino Treebank (van der Beek et
al., 2002), the semantic annotation in the Dutch
Figure 2: The main window visualizing node-to-
node alignment of trees in the TectoMT project
(
?
Zabokrtsk?y et al, 2008); side-bar shows data as-
sociated with the selected node.
language Corpus Initiative project (Trapman and
Monachesi, 2006), as well as for annotation of
morphology using so-called MorphoTrees (Smr?z
and Pajas, 2004) in the Prague Arabic Depen-
dency Treebank (where it was also used for anno-
tation of the dependency trees in the PDT style).
While most other projects use off-line conversion
Figure 3: One possible way of displaying Penn
Treebank data in TrEd
to some format directly supported by TrEd, for
Alpino Treebank a better approach has been se-
lected: the PML library has been configured to
convert between Alpino XML format and PML
transparently on-the-fly using two simple XSLT
stylesheets created for this purpose.
Like some other annotation tools, for example
DepAnn (Kakkonen, 2006), TrEd provides means
for comparing two (or more) annotations and vi-
675
sually marking the differences. This functionality
is currently provided by macros tailored especially
for the PDT annotations. Modifying these macros
for specific needs of other tree-based annotations
should be easy.
4 Automated processing
The same code-base that runs TrEd (except for the
GUI) is used in a command-line tool for automated
processing of the annotated data called BTrEd.
This tool allows one to search, transform or mod-
ify the data by means of small programs written in
Perl known from TrEd as macros. Given a list of
files, the tool opens the files one after another and
applies a given macro on each of them (or, if one
chooses, to each tree or each node). With the pow-
erful API of TrEd and the expressiveness of the
Perl programming language at hand, one can very
easy prepare scripts that gather information, create
reports, or automate some parts of the annotation
process; in some cases the script can be as short to
fit on the command-line.
It is often the case that one has to process a
large amount of data repeatedly with one or more
scripts. To avoid the need of reading the data
into memory upon each execution, BTrEd is aug-
mented by a client-server interface and a client
tool called NTrEd. NTrEd, given a list of com-
puter hostnames or IP addresses and a list of files,
starts remotely on each of the computers a server-
mode instance of BTrEd and distributes the sup-
plied data among these servers (either equally or
according to CPU load). Since only file-names are
transferred in the communication, some form of
shared data storage is assumed. The BTrEd servers
read their respective share of the treebank, keep it
in their RAM and await client connections. Once
the servers are started in this way, one can pass a
script to NTrEd as one would do with BTrEd; the
tool forwards the script to the servers and collect-
ing the results, outputs them in the same way as
a stand-alone BTrEd would. If the script modifies
the data, the user can send a request to the servers
to either save the changed files or reload them, dis-
carding all modifications. Security of NTrEd is
achieved by means of SHA-based authentication
(completely transparent to the user) and, option-
ally, by SSH tunneling.
Since one machine can run multiple instances of
BTrEd server, each user of a computer cluster can
run its own set of BTrEd servers without interfer-
ing with other users. Concurrent work of several
users on the same data is supported by TrEd and
BTrEd by a simple system of file locks, similar to
that of GNU Emacs.
This kind of processing was exploited heav-
ily during the post-annotation checking phases of
PDT 2.0 production (
?
St?ep?anek, 2006). Employing
a cluster consisting from about ten computers, a
typical consistency-checking script processed the
whole amount of PDT 2.0 (1.5 million analytical
nodes and 700 thousand tectogrammatical nodes)
in just a few seconds. This was particularly help-
ful for rapid prototyping or testing hypotheses and
it accelerated the whole process enormously.
The NTrEd system, keeping the data in RAM of
the servers, is sufficient for small to medium-sized
corpora. For huge corpora in scale of terabytes it
may not be the case. For processing such huge
amounts of data, another tool called JTrEd was re-
cently added to the framework. JTrEd is a wrapper
script that simplifies distribution of BTrEd tasks
over a computer cluster controlled by the Sun Grid
Engine (SGE).
The BTrEd machinery is not intended just for
small scripts. A project building a full MT en-
gine on top of this framework is in progress
(
?
Zabokrtsk?y et al, 2008).
5 Data-base driven query engine
One of the reasons for which tree-banks are cre-
ated is that they cover and capture a representa-
tive number of syntactic constructions of the par-
ticular language. However, to be able to identify
them effectively, one has to employ some query-
ing system, consisting of a sufficiently expressive
query language and an engine that can search the
treebank and present the user with all occurrences
matching the constraints of the query.
While for complex queries the tools described
in the previous section serve well to users with
basic programming skills,?every-day? querying by
linguistic public requires a more accessible user-
interface. In this section we describe a working
prototype of a new query engine and its user inter-
face, based on the data representation and tools de-
scribed in the preceding sections. First, however,
we briefly review some existing solutions.
For searching over PDT a tool called Net-
Graph (M??rovsk?y, 2006) is traditionally used. This
tool?s graphical interface allows the users to for-
mulate their queries in a very natural way, namely
676
as trees whose structures correspond to the struc-
tures of the desired search results (although one
may specify, for example, that an edge in the query
tree should actually match a path in the result tree).
Each node in the query tree can carry a set of at-
tributes that match or otherwise constrain the at-
tributes of the corresponding node in the result
tree. The query can further put some cardinality
constraints on the matching nodes; these are for-
mulated using a special set of labels on the query
tree. A great advantage of NetGraph is its web-
enabled user interface (Java applet). The under-
lying query engine is written in C, and although
relatively simplistic (i.e. no indexing or planning
techniques are used), for PDT-sized corpus it of-
fers reasonable speed for the interactive use. Cer-
tain disadvantages of the NetGraph system in our
view are: lack of support for querying relations
between two or more trees; no support for multi-
layered annotations; limited means of expressing
attribute constraints and their boolean combina-
tions; restriction to a limited legacy data format.
Probably the best-known query languages for
tree structures nowadays are XPath and XQuery,
promoted by (and in case of the latter bound to)
the XML technology. The advantage of these
query languages is that there are several implemen-
tations to choose from. Beside searching, some
tools (e.g. XSH2 (Pajas, 2005)) provide means for
XPath-based data modification. For these reasons,
XPath searches over XML-encoded treebank data
are promoted (Bouma and Kloosterman, 2002).
The disadvantage is, however, that being restricted
to the XML data model, users of such tools have
to query over a particular XML encoding of the
data which often in some way or other obscures
the actual annotation schema and relations the an-
notation represents. Besides, it can be argued that
XPath alone does not provide sufficient expres-
siveness for typical linguistic queries.
As a remedy for the last deficiency, Steven Bird
et al (Bird et al, 2006) proposed a concise query
language named LPath, which, while extending
core XPath, was designed with the needs of lin-
guistic queries in mind. Their query system is
powered by a relational database in which the
queries are translated from LPath to SQL. To en-
able efficient evaluation of constraints on horizon-
tal and vertical relationships between two nodes
of a tree by the relational database, the database
representation of the trees uses a simple labeling
scheme which labels each node with several inte-
gers so that the relationship constraints translate in
SQL to simple comparisons of the respective inte-
ger labels.
It has been shown (Lai and Bird, 2005) that fur-
ther extension to the LPath language, known as
LPath+, is already 1st-order complete. It should,
however, be noted that 1st-order completeness has
little to do with the practical expressiveness of the
language; certain queries, easily expressed in 1st-
order logic, only translate to LPath+ at the cost of
combinatorial explosion in the size of the query.
For example, like XPath, the only way LPath+ of-
fers to match two non-identical sibling nodes is
to reach one by the child axis and the other us-
ing the following-sibling or preceding-sibling axes
from the first one; thus for a query with n sibling
nodes whose constraints do not necessarily imply
inequality and which can appear in the tree in ar-
bitrary order, the LPath+ query must, in general,
enumerate a disjunction of all the n! possible per-
mutations. This may not be a problem when query-
ing over English treebanks, but is a serious handi-
cap for querying over treebanks for languages with
free word-order.
There are several other tools for querying over
treebanks, we are aware at least of TIGERSearch
(Lezius, 2002) for annotations in the TigerXML
format, and TGrep2 (Rohde, 2001) for Penn Tree-
bank and similar, which we shall not describe here
in detail as they are well known.
For the PML-based annotation system presented
in this paper, we have developed a prototype of a
new querying system, referred to, just for the pur-
poses of this paper, as PML Tree Query (PML-
TQ). The new system attempts to equal the qual-
ities of the above mentioned systems and addition-
ally provide
? a query language with sufficient expressive-
ness yet without complex formalisms
? unified treatment of structural and non-
structural, inter- and cross-layer relationships
? basic reporting capabilities (computing num-
ber or distribution of occurrences, etc.)
? a graphical query editor built into TrEd
? a scriptable and extensible interface
At the current stage, PML-TQ provides a proto-
type query language supporting arbitrary logical
677
conditions on attributes of nodes and their inter-
and cross-layer relations, optional nodes, and ba-
sic cardinality constraints. A result of an evalua-
tion of a PML-TQ can be either a set of matches,
each match being a set of nodes in the treebank
corresponding to the nodes in the query, or a re-
port with some information computed from these
node sets. The reporting capabilities of PML-TQ
allow one to perform various aggregations on the
result node sets and compute statistics over the
aggregated groups. Thus, one may easily formu-
late queries such as ?what is the maximum, mini-
mum, and average depth of a tree in the treebank?,
?what preposition forms correspond on the surface
layer to tectogrammatical nodes with the functor
DIR3 and what is their distribution?, ?what is the
most common functor for a child node of a node
with functor PRED?, ?what is the joint distribution
of functors for nodes in the parent-child relation?,
etc.
In the graphical representation of PML-TQ, re-
lations between nodes are represented by arrows.
Each PML-TQ query forms a tree or forest whose
edges represent basic relations between nodes and
possibly nesting of subqueries and whose nodes
can be interconnected by additional arrows repre-
senting further relations.
a-layer t-layer
? ?
xx
?

?

ff
a-layer t-layer
Obj
?
?
a/lex.rf
oo
AuxP
?

?

a/lex.rf
yy
a/aux.rf
oo
Obj
?

Figure 4: Examples of cross-layer queries in PML-
TQ. Query on the left finds dependency that got
reversed on the PDT tectogrammatical layer com-
pared to the analytical layer; the query on the right
finds tectogrammatical nodes corresponding to an
analytical object governing a prepositional object.
Apart from the graphical representation, PML-
TQ queries can be formulated in a textual form
with syntax resembling XPath (but substantially
more expressive). A query is parsed from this syn-
tax into a syntactic tree encoded in PML; in this
representation the queries can be stored, visual-
ized, and graphically manipulated in TrEd.
There are presently two engines that can eval-
uate PML-TQ queries. To utilize the modern
RDBMS technology for performance and scalabil-
ity, we have created a translator of PML-TQ to
SQL. One can thus query over a static treebank
stored in a database (for encoding tree-structures
into database tables, we have adopted a label-
ing system similar to that described in (Bird et
al., 2006)). For querying over data that change
(e.g. a file currently open in TrEd or a bunch of
files with annotation in progress), we have imple-
mented a simple, yet still relatively fast, evaluator
in Perl with a basic planner that can perform PML-
TQ searches over PML data sequentially in either
TrEd, BTrEd, NTrEd, or JTrEd.
Having two engines behind our query interface
in TrEd has several benefits. The annotators will
be able to perform identical queries over a cor-
pus stored in a remote SQL database as well as to
search in their own locally stored data. The devel-
opers of scripts for BTrEd will be able to formulate
parts of their scripts briefly as PML-TQ queries
whose correctness they will be able to verify in-
dependently on a treebank using the SQL backend
using an interactive environment.
The SQL-based execution system has currently
two database backends for feature and perfor-
mance comparison: Oracle Express 10g and Post-
gres SQL. We use Perl DBI modules to intercon-
nect these backends with the TrEd toolkit.
Figure 5: PML-TQ interface in TrEd. Top: the
query as text, bottom left the query tree, bottom
right a matching tree from PDT 2.0.
The search results can be presented in TrEd in
one of several ways. Results of queries that make
use of the report-generating facilities of PML-TQ
are displayed simply as tables in a text window.
Most PML-TQ queries, however, return matching
nodes or trees. To display them, TrEd retrieves
678
corresponding URLs and node positions from the
database and reads the actual data from the PML
files that must currently be accessible locally or re-
motely via HTTP, FTP or SSH. Since there are sit-
uations when the original data cannot be accessed
in this way, we are working on a solution that
would allow TrEd to fetch and reconstruct the trees
directly from the database.
For populating the database system with tree-
banks, we have developed a tool that can transfer
arbitrary PML-encoded data into a set of database
tables suitable for PML-TQ querying. The in-
formation about available inter-layer and cross-
layer relations is automatically extracted from the
PML schema and can be further adjusted by a few
command-line parameters.
We have evaluated our database search engine
using queries obtained by several means. We had
the luck that the developer of NetGraph kindly pro-
vided us with all queries collected from real Net-
Graph users over the past few years in the server
logs. We thus obtained almost 9000 queries for
the analytical layer and about 5000 queries for the
tectogrammatical layer of PDT 2.0. By translat-
ing them to PML-TQ with a simple Perl script,
we obtained a large collection for testing the ba-
sic functionality and performance of our system.
To that we added a set of queries that test more
advanced PML-TQ features and, for comparison,
several queries analogous to the LPath query ex-
amples given in (Bird et al, 2006).
When we first run our complete query collec-
tion on the Oracle database with 1.5 million nodes
and about 88 thousand trees from the analytical
layer of PDT, we were surprised to see that out
of 8188 queries, 8102 computes in a fraction of
second, further 33 in less then 2 seconds, further
36 in less than 10 seconds, 14 in less than 20 sec-
onds and only 5 in more than one minute. Four
of these, however, took extremely long time to
compute (from hours to days). We observed that
all these time-consuming queries were rather simi-
lar: they imposed either no or too week constraints
on the nodes and sometimes the query tree had
a rather large number of auto-morphisms (there
was a query consisting of a node with six iden-
tical child-nodes none of which carried any con-
straints). We then found a tree in our data set
that contained a node with 85 children. This gives
roughly 10
12
solutions to the query with six sib-
lings on this tree alone.
In some cases the queries can be rewritten us-
ing cardinality constraints (?find all nodes with at
least 6 children?), which avoids the combinatorial
explosion. Since we felt this may not always be
possible, we also tried to remove from our data
set al trees with more than 20 siblings (44 trees
from 70K) that turned out to be mostly TV listings
anyway. After that, the performance for the four
of the problematic queries improved dramatically:
first 100 matches were found in a few seconds and
fist 10
6
matches in less than 10 minutes.
Although we have modified the query compiler
to suggest cardinality constraints were it seems
appropriate and to automatically eliminate some
types of automorphisms on the query tree by im-
posing a strict ordering on the permutable query
nodes, we think it is neither possible to completely
secure the query system against time-exhaustive
queries nor to reliably detect such queries auto-
matically. The querying interface therefore gives
the users the option to select a reasonable maxi-
mum number of results and allows them to cancel
the query evaluation at any time.
6 Conclusion
Over last few years our annotation framework
made a considerable leap, from a simple annota-
tion tool to a feature-rich system with several inter-
operating components. The complete framework
is publicly available, either under the General Pub-
lic License License (GPL), the Perl Artistic Li-
cense or other GPL-compatible free license. A
public release of the tree query interface described
in the previous section is scheduled for mid to end
of 2008.
7 Acknowledgment
This paper as well as the development of the
framework is supported by the grant Informa-
tion Society of GA AV
?
CR under contract
1ET101120503.
References
Bamman, David and Gregory Crane. 2006. The design
and use of a Latin dependency treebank. In Proceed-
ings of the Fifth International Workshop on Tree-
banks and Linguistic Theories (TLT 2006), pages
67?78, Prague.
Bird, Steven, Yi Chen, Susan B. Davidson, Haejoong
Lee, and Yifeng Zheng. 2006. Designing and eval-
uating an XPath dialect for linguistic queries. In
679
ICDE ?06: Proceedings of the 22nd International
Conference on Data Engineering, page 52, Washing-
ton, DC, USA. IEEE Computer Society.
Bouma, Gosse and Geert Kloosterman. 2002. Query-
ing dependency treebanks in XML. In Proceedings
of the Third international conference on Language
Resources and Evaluation (LREC), Gran Canaria.
Buchholz, Sabine and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings CoNLL-X.
Chu-Ren, Huang, Keh-Jiann Chen, Feng-Yi Chen, Keh-
Jiann Chen, Zhao-Ming Gao, and Kuang-Yu Chen.
2000. Sinica treebank: Design criteria, annotation
guidelines, and on-line interface. In Proceedings of
2nd Chinese Language Processing Workshop (Held
in conjunction with ACL-2000), pages 29?37, Hong
Kong, October 7.
Declerck, Thierry. 2006. Synaf: Towards a stan-
dard for syntactic annotation. In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation (LREC 2006), pages 209?
232.
D?zeroski, Sa?so, Toma?z Erjavec, Nina Ledinek, Petr Pa-
jas, Zden?ek
?
Zabokrtsk?y, and Andreja
?
Zele. 2006.
Towards a slovene dependency treebank. In Pro-
ceedings of the 5th International Conference on
Language Resources and Evaluation (LREC 2006),
pages 1388?1391.
Haji?c, Jan, Barbora Vidov?a-Hladk?a, and Petr Pajas.
2001. The Prague Dependency Treebank: Anno-
tation Structure and Support. In Proceedings of
the IRCS Workshop on Linguistic Databases, pages
105?114, Philadelphia, USA. University of Pennsyl-
vania.
Haji?c, Jan et al 2006. The Prague Dependency Tree-
bank 2.0. CD-ROM. CAT: LDC2006T01.
Ide, Nancy and R. Romary. 2003. Encoding syntactic
annotation. In Abill?e, A., editor, Building and Using
Parsed Corpora. Kluwer, Dordrecht.
Kakkonen, Tuomo. 2006. Depann - an annotation tool
for dependency treebanks. In Proceedings of the
11th ESSLLI Student Session at the 18th European
Summer School in Logic, Language and Information,
pages 214?225, Malaga, Spain.
Lai, Catherine and Steven Bird. 2005. LPath+: A first-
order complete language for linguistic tree query. In
Proceedings of the 19th Pacific Asia Conference on
Language (PACLIC), Information and Computation,
pages 1?12, Taipei, Taiwan. Academia Sinica.
Lezius, Wolfgang. 2002. Ein Suchwerkzeug f?ur syn-
taktisch annotierte Textkorpora. Ph.D. thesis, IMS,
University of Stuttgart, December. Arbeitspapiere
des Instituts f?ur Maschinelle Sprachverarbeitung
(AIMS), volume 8, number 4.
Marcus, Mitchell P., Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schasberger.
1994. The penn treebank: Annotating predicate
argument structure. In HLT. Morgan Kaufmann.
Mengel, A. and W. Lezius. 2000. An XML-based rep-
resentation format for syntactically annotated cor-
pora.
M??rovsk?y, Ji?r??. 2006. Netgraph: A tool for search-
ing in prague dependency treebank 2.0. In Haji?c,
Jan and Joakim Nivre, editors, Proceedings of the
Fifth Workshop on Treebanks and Linguistic Theo-
ries (TLT), pages 211?222, Prague, Czech Republic.
Pajas, Petr and Jan
?
St?ep?anek. 2006. XML-based repre-
sentation of multi-layered annotation in the PDT 2.0.
In Proceedings of the LREC Workshop on Merging
and Layering Linguistic Information (LREC 2006),
pages 40?47.
Pajas, Petr. 2005. XSH - XML Editing Shell (an intro-
duction). In Proceedings of XMLPrague conference
on XML 2005, pages 69?78, Prague.
Prokopidis, P, E Desypri, M Koutsombogera, H Papa-
georgiou, and S Piperidis. 2005. Theoretical and
practical issues in the construction of a greek depen-
dency treebank. In In Proc. of the 4th Workshop on
Treebanks and Linguistic Theories (TLT, pages 149?
160.
Rohde, D. 2001. TGrep2 the next-generation search
engine for parse trees. http://tedlab.mit.edu/ dr/-
Tgrep2/.
Smr?z, Otakar and Petr Pajas. 2004. MorphoTrees
of Arabic and Their Annotation in the TrEd Envi-
ronment. In Nikkhou, Mahtab, editor, Proceedings
of the NEMLAR International Conference on Ara-
bic Language Resources and Tools, pages 38?41,
Cairo. ELDA.
?
St?ep?anek, Jan. 2006. Post-annotation checking of
prague dependency treebank 2.0 data. In Proceed-
ings of the 9th International Conference, TSD 2006,
number 4188 in Lecture Notes In Computer Science,
pages 277?284. Springer-Verlag Berlin Heidelberg.
Tadi?c, Marko. 2007. Building the croatian dependency
treebank: the initial stages. In Contemporary Lin-
guistics, volume 63, pages 85?92.
Trapman, Jantine and Paola Monachesi. 2006. Manual
for the. annotation of semantic roles in D-Coi. Tech-
nical report, University of Utrecht.
van der Beek, Leonoor, Gosse Bouma, Robert Malouf,
and Gertjan van Noord. 2002. The alpino depen-
dency treebank. In Computational Linguistics in the
Netherlands CLIN 2001, Rodopi.
?
Zabokrtsk?y, Zden?ek, Jan Pt?a?cek, and Petr Pajas. 2008.
TectoMT: Highly modular hybrid MT system with
tectogramatics used as transfer layer. (To appear).
680
Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 33?36,
Suntec, Singapore, 3 August 2009.
c?2009 ACL and AFNLP
System for Querying Syntactically Annotated Corpora
Petr Pajas
Charles Univ. in Prague, MFF
?
UFAL
Malostransk?e n?am. 25
118 00 Prague 1 ? Czech Rep.
pajas@ufal.mff.cuni.cz
Jan
?
St
?
ep
?
anek
Charles Univ. in Prague, MFF
?
UFAL
Malostransk?e n?am. 25
118 00 Prague 1 ? Czech Rep.
stepanek@ufal.mff.cuni.cz
Abstract
This paper presents a system for querying
treebanks. The system consists of a pow-
erful query language with natural support
for cross-layer queries, a client interface
with a graphical query builder and visual-
izer of the results, a command-line client
interface, and two substitutable query en-
gines: a very efficient engine using a re-
lational database (suitable for large static
data), and a slower, but paralel-computing
enabled, engine operating on treebank files
(suitable for ?live? data).
1 Introduction
Syntactically annotated treebanks are a great re-
source of linguistic information that is available
hardly or not at all in flat text corpora. Retrieving
this information requires specialized tools. Some
of the best-known tools for querying treebanks
include TigerSEARCH (Lezius, 2002), TGrep2
(Rohde, 2001), MonaSearch (Maryns and Kepser,
2009), and NetGraph (M??rovsk?y, 2006). All these
tools dispose of great power when querying a sin-
gle annotation layer with nodes labeled by ?flat?
feature records.
However, most of the existing systems are little
equipped for applications on structurally complex
treebanks, involving for example multiple inter-
connected annotation layers, multi-lingual par-
allel annotations with node-to-node alignments,
or annotations where nodes are labeled by at-
tributes with complex values such as lists or nested
attribute-value structures. The Prague Depen-
dency Treebank 2.0 (Haji?c and others, 2006), PDT
2.0 for short, is a good example of a treebank with
multiple annotation layers and richly-structured
attribute values. NetGraph was a tool tradition-
ally used for querying over PDT, but still it does
not directly support cross-layer queries, unless the
layers are merged together at the cost of loosing
some structural information.
The presented system attempts to combine and
extend features of the existing query tools and re-
solve the limitations mentioned above. We are
grateful to an anonymous referee for pointing us
to ANNIS2 (Zeldes and others, 2009) ? another
system that targets annotation on multiple levels.
2 System Overview
Our system, named PML Tree Query (PML-TQ),
consists of three main components (discussed fur-
ther in the following sections):
? an expressive query language supporting
cross-layer queries, arbitrary boolean com-
binations of statements, able to query com-
plex data structures. It also includes a sub-
language for generating listings and non-
trivial statistical reports, which goes far be-
yond statistical features of e.g. TigerSearch.
? client interfaces: a graphical user inter-
face with a graphical query builder, a cus-
tomizable visualization of the results and a
command-line interface.
? two interchangeable engines that evaluate
queries: a very efficient engine that requires
the treebank to be converted into a rela-
tional database, and a somewhat slower en-
gine which operates directly on treebank files
and is useful especially for data in the process
of annotation and experimental data.
The query language applies to a generic data
model associated with an XML-based data format
called Prague Markup Language or PML (Pajas
and
?
St?ep?anek, 2006). Although PML was devel-
oped in connection with PDT 2.0, it was designed
as a universally applicable data format based on
abstract data types, completely independent of a
33
particular annotation schema. It can capture sim-
ple linear annotations as well as annotations with
one or more richly structured interconnected an-
notation layers. A concrete PML-based format for
a specific annotation is defined by describing the
data layout and XML vocabulary in a special file
called PML Schema and referring to this schema
file from individual data files.
It is relatively easy to convert data from other
formats to PML without loss of information. In
fact, PML-TQ is implemented within the TrEd
framework (Pajas and
?
St?ep?anek, 2008), which
uses PML as its native data format and already of-
fers all kinds of tools for work with treebanks in
several formats using on-the-fly transformation to
PML (for XML input via XSLT).
The whole framework is covered by an open-
source license and runs on most current platforms.
It is also language and script independent (operat-
ing internally with Unicode).
The graphical client for PML-TQ is an exten-
sion to the tree editor TrEd that already serves as
the main annotation tool for treebank projects (in-
cluding PDT 2.0) in various countries. The client
and server communicate over the HTTP protocol,
which makes it possible to easily use PML-TQ en-
gine as a service for other applications.
3 Query Language
A PML-TQ query consists of a part that selects
nodes in the treebank, and an optional part that
generates a report from the selected occurrences.
The selective part of the query specifies condi-
tions that a group of nodes must satisfy to match
the query. The conditions can be formulated as
arbitrary boolean combinations of subqueries and
simple statements that can express all kinds of re-
lations between nodes and/or attribute values. This
part of the query can be visualized as a graph with
vertices representing the matching nodes, con-
nected by various types of edges. The edges (vi-
sualized by arrows of different colors and styles)
represent various types of relations between the
nodes. There are four kinds of these relations:
? topological relations (child, descendant
depth-first-precedes, order-precedes, same-
tree-as, same-document-as) and their
reversed counterparts (parent, ancestor,
depth-first-follows, order-follows)
? inter- or cross-layer ID-based references
? user-implemented relations, i.e. relations
whose low-level implementation is provided
by the user as an extension to PML-TQ
1
(for example, we define relations eparent and
echild for PDT 2.0 to distinguish effective de-
pendency from technical dependency).
? transitive closures of the preceding two types
of relations (e.g. if coref text.rf is a re-
lation representing textual coreference, then
coref text.rf{4,} is a relation rep-
resenting chains of textual coreference of
length at least 4).
The query can be accompanied by an optional
part consisting of a chain of output filters that can
be used to extract data from the matching nodes,
compute statistics, and/or format and post-process
the results of a query.
Let us examine these features on an example
of a query over PDT 2.0, which looks for Czech
words that have a patient or effect argument in in-
finitive form:
t-node $t := [
child t-node $s := [
functor in { "PAT", "EFF" },
a/lex.rf $a ] ];
a-node $a := [
m/tag
?
??Vf?,
0x child a-node [ afun = ?AuxV? ] ];
>> for $s.functor,$t.t_lemma
give $1, $2, count()
sort by $3 desc
The square brackets enclose conditions regarding
one node, so t-node $t := [...] is read
?t-node $t with . . . ?. Comma is synonymous with
logical and. See Fig. 3 for the graphical represen-
tation of the query and one match.
This particular query selects occurrences of a
group of three nodes, $t, $s, and $a with the
following properties: $t and $s are both of type
t-node, i.e. nodes from a tectogrammatical tree
(the types are defined in the PML Schema for the
PDT 2.0); $s is a child of $t; the functor at-
tribute of $s has either the value PAT or EFF; the
node $s points to a node of type a-node, named
$a, via an ID-based reference a/lex.rf (this
expression in fact retrieves value of an attribute
lex.rf from an attribute-value structure stored
in the attribute a of $s); $a has an attribute m car-
rying an attribute-value structure with the attribute
1
In some future version, the users will also be able to de-
fine new relations as separate PML-TQ queries.
34
0xt-node $s functor in { "PAT", "EFF" }
t-node $t 
Output filters:  >>  for $s.functor,$t.t_lemma     give $1,$2,count()     sort by $3 desc
a-node afun = 'AuxV'
a-node $a m/tag ~ '^Vf'
a/lex.rfchild
#PersPronACTn.pron.def.pers
zapomenout enuncPREDv
#CorACTqcomplex
d?chatPATv
.
a-lnd94103-087-p1s3AuxS
Zapomn?liPred
jsmeAuxV d?chatObj
.AuxK
Zapomn?eli jsme d?ychat. [We-forgot (aux) to-breathe.]
Figure 1: Graphical representation of a query (left) and a result spanning two annotation layers
tagmatching regular expression ?Vf (in PDT 2.0
tag set this indicates that $a is an infinitive); $a
has no child node that is an auxiliary verb (afun
= ?AuxV?). This last condition is expressed as a
sub-query with zero occurrences (0x).
The selective part of the query is followed by
one output filter (starting with >>). It returns three
values for each match: the functor of $s, the tec-
togrammatical lemma of $t, and for each distinct
pair of these two values the number of occurrences
of this pair counted over the whole matching set.
The output is ordered by the 3rd column in the de-
scending order. It may look like this:
PAT mo?znost 115
PAT schopn?y 110
EFF a 85
PAT #Comma 83
PAT rozhodnout_se 75
In the PML data model, attributes (like a of
$t, m of $a in our example) can carry com-
plex values: attribute-value structures, lists, se-
quences of named elements, which in turn may
contain other complex values. PML-TQ addresses
values nested within complex data types by at-
tribute paths whose notation is somewhat similar
to XPath (e.g. m/tag or a/[2]/aux.rf). An
attribute path evaluated on a given node may re-
turn more than one value. This happens for ex-
ample when there is a list value on the attribute
path: the expression m/w/token=?a? where m
is a list of attribute-value structures reads as some
one value returned by m/w/token equals ?a?.
By prefixing the path with a
*
, we may write
all values returned by m/w/token equal ?a? as
*
m/w/token=?a?.
We can also fix one value returned by an at-
tribute path using the member keyword and query
it the same way we query a node in the treebank:
t-node $n:= [
member bridging [
type = "CONTRAST",
target.rf t-node [ functor="PAT" ]]]
where bridging is an attribute of t-node con-
taining a list of labeled graph edges (attribute-
value structures). We select one that has type
CONTRAST and points to a node with functor PAT.
4 Query Editor and Client
Figure 2: The PML-TQ graphical client in TrEd
The graphical user interface lets the user to
build the query graphically or in the text form; in
both cases it assists the user by offering available
node-types, applicable relations, attribute paths,
and values for enumerated data types. It commu-
nicates with the query engine and displays the re-
sults (matches, reports, number of occurrences).
35
Colors are used to indicate which node in the
query graph corresponds to which node in the re-
sult. Matches from different annotation layers are
displayed in parallel windows. For each result, the
user can browse the complete document for con-
text. Individual results can be saved in the PML
format or printed to PostScript, PDF, or SVG. The
user can also bookmark any tree from the result
set, using the bookmarking features of TrEd. The
queries are stored in a local file.
2
5 Engines
For practical reasons, we have developed two en-
gines that evaluate PML-TQ queries:
The first one is based on a translator of PML-
TQ to SQL. It utilizes the power of modern re-
lational databases
3
and provides excellent perfor-
mance and scalability (answering typical queries
over a 1-million-word treebank in a few seconds).
To use this engine, the treebank must be, simi-
larly to (Bird and others, 2006), converted into
read-only database tables, which makes this en-
gine more suitable for data that do not change too
often (e.g. final versions of treebanks).
For querying over working data or data not
likely to be queried repeatedly, we have devel-
oped an index-less query evaluator written in Perl,
which performs searches over arbitrary data files
sequentially. Although generally slower than the
database implementation (partly due to the cost
of parsing the input PML data format), its perfor-
mance can be boosted up using a built-in support
for parallel execution on a computer cluster.
Both engines are accessible through the identi-
cal client interface. Thus, users can run the same
query over a treebank stored in a database as well
as their local files of the same type.
When implementing the system, we periodi-
cally verify that both engines produce the same
results on a large set of test queries. This testing
proved invaluable not only for maintaining con-
sistency, but also for discovering bugs in the two
implementations and also for performance tuning.
6 Conclusion
We have presented a powerful open-source sys-
tem for querying treebanks extending an estab-
2
The possibility of storing the queries in a user account
on the server is planned.
3
The system supports Oracle Database (version 10g or
newer, the free XE edition is sufficient) and PostgreSQL (ver-
sion at least 8.4 is required for complete functionality).
lished framework. The current version of the sys-
tem is available at http://ufal.mff.cuni.
cz/
?
pajas/pmltq.
Acknowledgments
This paper as well as the development of the sys-
tem is supported by the grant Information Society
of GA AV
?
CR under contract 1ET101120503 and
by the grant GAUK No. 22908.
References
Steven Bird et al 2006. Designing and evaluating an
XPath dialect for linguistic queries. In ICDE ?06:
Proceedings of the 22nd International Conference
on Data Engineering, page 52. IEEE Computer So-
ciety.
Jan Haji?c et al 2006. The Prague Dependency Tree-
bank 2.0. CD-ROM. Linguistic Data Consortium
(CAT: LDC2006T01).
Wolfgang Lezius. 2002. Ein Suchwerkzeug f?ur syn-
taktisch annotierte Textkorpora. Ph.D. thesis, IMS,
University of Stuttgart, December. Arbeitspapiere
des Instituts f?ur Maschinelle Sprachverarbeitung
(AIMS), volume 8, number 4.
Hendrik Maryns and Stephan Kepser. 2009.
Monasearch ? querying linguistic treebanks with
monadic second-order logic. In Proceedings of the
7th International Workshop on Treebanks and Lin-
guistic Theories (TLT 2009).
Ji?r?? M??rovsk?y. 2006. Netgraph: A tool for searching
in Prague Dependency Treebank 2.0. In Proceed-
ings of the 5th Workshop on Treebanks and Linguis-
tic Theories (TLT 2006), pages 211?222.
Petr Pajas and Jan
?
St?ep?anek. 2008. Recent advances
in a feature-rich framework for treebank annotation.
In The 22nd International Conference on Computa-
tional Linguistics - Proceedings of the Conference,
volume 2, pages 673?680. The Coling 2008 Orga-
nizing Committee.
Petr Pajas and Jan
?
St?ep?anek. 2006. XML-based repre-
sentation of multi-layered annotation in the PDT 2.0.
In Proceedings of the LREC Workshop on Merging
and Layering Linguistic Information (LREC 2006),
pages 40?47.
Douglas L.T. Rohde. 2001. TGrep2 the
next-generation search engine for parse trees.
http://tedlab.mit.edu/
?
dr/Tgrep2/.
Amir Zeldes et al 2009. Information structure in
african languages: Corpora and tools. In Proceed-
ings of the Workshop on Language Technologies for
African Languages (AFLAT), 12th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL-09), Athens, Greece, pages
17?24.
36
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 1?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The CoNLL-2009 Shared Task:
Syntactic and Semantic Dependencies in Multiple Languages
Jan Hajic?? Massimiliano Ciaramita? Richard Johansson? Daisuke Kawahara?
Maria Anto`nia Mart???? Llu??s Ma`rquez?? Adam Meyers?? Joakim Nivre?? Sebastian Pado???
Jan ?Ste?pa?nek? Pavel Stran?a?k? Mihai Surdeanu?? Nianwen Xue?? Yi Zhang??
?: Charles University in Prague, {hajic,stepanek,stranak}@ufal.mff.cuni.cz
?: Google Inc., massi@google.com
?: University of Trento, johansson@disi.unitn.it
?: National Institute of Information and Communications Technology, dk@nict.go.jp
??: University of Barcelona, amarti@ub.edu
??: Technical University of Catalonia, Barcelona, lluism@lsi.upc.edu
??: New York University, meyers@cs.nyu.edu
??: Uppsala University and Va?xjo? University, joakim.nivre@lingfil.uu.se
??: Stuttgart University, pado@ims.uni-stuttgart.de
??: Stanford University, mihais@stanford.edu
??: Brandeis University, xuen@brandeis.edu
??: Saarland University, yzhang@coli.uni-sb.de
Abstract
For the 11th straight year, the Conference
on Computational Natural Language Learn-
ing has been accompanied by a shared task
whose purpose is to promote natural language
processing applications and evaluate them in
a standard setting. In 2009, the shared task
was dedicated to the joint parsing of syntac-
tic and semantic dependencies in multiple lan-
guages. This shared task combines the shared
tasks of the previous five years under a unique
dependency-based formalism similar to the
2008 task. In this paper, we define the shared
task, describe how the data sets were created
and show their quantitative properties, report
the results and summarize the approaches of
the participating systems.
1 Introduction
Every year since 1999, the Conference on Com-
putational Natural Language Learning (CoNLL)
launches a competitive, open ?Shared Task?. A
common (?shared?) task is defined and datasets are
provided for its participants. In 2004 and 2005, the
shared tasks were dedicated to semantic role label-
ing (SRL) in a monolingual setting (English). In
2006 and 2007 the shared tasks were devoted to
the parsing of syntactic dependencies, using corpora
from up to 13 languages. In 2008, the shared task
(Surdeanu et al, 2008) used a unified dependency-
based formalism, which modeled both syntactic de-
pendencies and semantic roles for English. The
CoNLL-2009 Shared Task has built on the 2008 re-
sults by providing data for six more languages (Cata-
lan, Chinese, Czech, German, Japanese and Span-
ish) in addition to the original English1. It has thus
naturally extended the path taken by the five most
recent CoNLL shared tasks.
As in 2008, the CoNLL-2009 shared task com-
bined dependency parsing and the task of identify-
ing and labeling semantic arguments of verbs (and
other parts of speech whenever available). Partici-
pants had to choose from two tasks:
? Joint task (syntactic dependency parsing and
semantic role labeling), or
? SRL-only task (syntactic dependency parses
have been provided by the organizers, using
state-of-the art parsers for the individual lan-
guages).
1There are some format changes and deviations from the
2008 task data specification; see Sect. 2.3
1
In contrast to the previous year, the evaluation data
indicated which words were to be dealt with (for the
SRL task). In other words, (predicate) disambigua-
tion was still part of the task, whereas the identi-
fication of argument-bearing words was not. This
decision was made to compensate for the significant
differences between languages and between the an-
notation schemes used.
The ?closed? and ?open? challenges have been
kept from last year as well; participants could have
chosen one or both. In the closed challenge, systems
had to be trained strictly with information contained
in the given training corpus; in the open challenge,
systems could have been developed making use of
any kind of external tools and resources.
This paper is organized as follows. Section 2 de-
fines the task, including the format of the data, the
evaluation metrics, and the two challenges. A sub-
stantial portion of the paper (Section 3) is devoted
to the description of the conversion and develop-
ment of the data sets in the additional languages.
Section 4 shows the main results of the submitted
systems in the Joint and SRL-only tasks. Section 5
summarizes the approaches implemented by partic-
ipants. Section 6 concludes the paper. In all sec-
tions, we will mention some of the differences be-
tween last year?s and this year?s tasks while keeping
the text self-contained whenever possible; for details
and observations on the English data, please refer to
the overview paper of the CoNLL-2008 Shared Task
(Surdeanu et al, 2008) and to the references men-
tioned in the sections describing the other languages.
2 Task Definition
In this section we provide the definition of the shared
task; after introducing the two challenges and the
two tasks the participants were to choose, we con-
tinue with the format of the shared task data, fol-
lowed by a description of the evaluation metrics
used.
For three of the languages (Czech, English and
German), out-of-domain data (OOD) have also been
prepared for the final evaluation, following the same
guidelines and formats.
2.1 Closed and Open Challenges
Similarly to the CoNLL-2005 and CoNLL-2008
shared tasks, this shared task evaluation is separated
into two challenges:
Closed Challenge The aim of this challenge was to
compare performance of the participating systems in
a fair environment. Systems had to be built strictly
with information contained in the given training cor-
pus, and tuned with the development section. In
addition, the lexical frame files (such as the Prop-
Bank and NomBank for English, the valency dictio-
nary PDT-Vallex for Czech etc.) were provided and
may have been used. These restrictions mean that
outside parsers (not trained by the participants? sys-
tems) could not be used. However, we did provide
the output of a single, state-of-the-art dependency
parser for each language so that participants could
build a SRL-only system (using the provided parses
as inputs) within the closed challenge (as opposed to
the 2008 shared task).
Open Challenge Systems could have been devel-
oped making use of any kind of external tools and
resources. The only condition was that such tools or
resources must not have been developed with the an-
notations of the test set, both for the input and output
annotations of the data. In this challenge, we were
interested in learning methods which make use of
any tools or resources that might improve the per-
formance. The comparison of different systems in
this setting may not be fair, and thus ranking of sys-
tems is not necessarily important.
2.2 Joint and SRL-only tasks
In 2008, systems participating in the open challenge
could have used state-of-the-art parsers for the syn-
tactic dependency part of the task. This year, we
have provided the output of these parsers for all the
languages in an uniform way, thus allowing an or-
thogonal combination of the two tasks and the two
challenges. For the SRL-only task, participants in
the closed challenge simply had to use the provided
parses only.
Despite the provisions for the SRL-only task, we
are more interested in the approaches and results of
the Joint task. Therefore, primary system ranking is
provided for the Joint task while additional measures
2
are computed for various combinations of parsers
and SRL methods across the tasks and challenges.
2.3 Data Format
The data format used in this shared task has been
based on the CoNLL-2008 shared task, with some
differences. The data follows these general rules:
? The files contain sentences separated by a blank
line.
? A sentence consists of one or more tokens and
the information for each token is represented on
a separate line.
? A token consists of at least 14 fields. The fields
are separated by one or more whitespace char-
acters (spaces or tabs). Whitespace characters
are not allowed within fields.
The data is thus a large table with whitespace-
separated fields (columns). The fields provided in
the data are described in Table 1. They are identical
for all languages, but they may differ in contents;
for example, some fields might not be filled for all
the languages provided (such as the FEAT or PFEAT
fields).
For the SRL-only task, participants have been
provided will all the data but the PRED and
APREDs, which they were supposed to fill in with
their correct values. However, they did not have
to determine which tokens are predicates (or more
precisely, which are the argument-bearing tokens),
since they were marked by ?Y? in the FILLPRED
field.
For the Joint task, participants could not (in ad-
dition to the PRED and APREDs) see the gold-
standard nor the predicted syntactic dependencies
(HEAD, PHEAD) and their labels (DEPREL, PDE-
PREL). These syntactic dependencies were also to
be filled by participants? systems.
In both tasks, participants have been free to
use any other data (columns) provided, except the
LEMMA, POS and FEAT columns (to get more ?re-
alistic? results using only their automatically pre-
dicted variants PLEMMA, PPOS and PFEAT).
Besides the corpus proper, predicate dictionaries
have been provided to participants in order to be able
to properly match the predicates to the tokens in the
corpus; their contents could have been used e.g. as
features for the PRED/APREDs predictions (or even
for the syntactic dependencies, i.e., for filling in the
PHEAD and PDEPREL fields).
The system of filling-in the APREDs follows
the 2008 pattern; for each argument-bearing token
(predicate), a new APREDn column is created in the
order in which the predicate token is encountered
within the sentence (i.e., based on its ID seen as a
numerical value). Then, for each token in the sen-
tence, the value in the intersection of the APREDn
column and the token row is either left unfilled
(if the token is not an argument), or a predicate-
argument label(s) is(are) filled in.
The differences between the English-only 2008
task and this year?s multilingual task can be briefly
summarized as follows:
? only ?split?2 lemmas and forms have been pro-
vided in the English datasets (for the other lan-
guages, original tokenization from the respec-
tive treebanks has been used);
? rich morphological features have been added
wherever available;
? syntactic dependencies by state-of-the-art
parsers have been provided (for the SRL-only
task);
? multiple semantic labels for a single token have
been allowed (and properly evaluated) in the
APREDs columns;
? predicates have been pre-identified and marked
in both the training and test data;
? some of the fields (e.g. the APREDx) and val-
ues (ARG0? A0 etc.) have been renamed.
2.4 Evaluation Measures
It was required that participants submit results in all
seven languages in the chosen task and in any of (or
both) the challenges. Submission of out-of-domain
data files has been optional.
The main evaluation measure, according to which
systems are primarily compared, is the Joint task,
2Splitting of forms and lemmas in English has been intro-
duced in the 2008 shared task to match the tokenization con-
vention for the arguments in NomBank.
3
Field # Name Description
1 ID Token counter, starting at 1 for each new sentence
2 FORM Form or punctuation symbol (the token; ?split? for English)
3 LEMMA Gold-standard lemma of FORM
4 PLEMMA Automatically predicted lemma of FORM
5 POS Gold-standard POS (major POS only)
6 PPOS Automatically predicted major POS by a language-specific tagger
7 FEAT Gold-standard morphological features (if applicable)
8 PFEAT Automatically predicted morphological features (if applicable)
9 HEAD Gold-standard syntactic head of the current token (ID or 0 if root)
10 PHEAD Automatically predicted syntactic head
11 DEPREL Gold-standard syntactic dependency relation (to HEAD)
12 PDEPREL Automatically predicted dependency relation to PHEAD
13 FILLPRED Contains ?Y? for argument-bearing tokens
14 PRED (sense) identifier of a semantic ?predicate? coming from a current token
15... APREDn Columns with argument labels for each semantic predicate (in the ID order)
Table 1: Description of the fields (columns) in the data provided. The values of columns 9, 11 and 14 and above are
not provided in the evaluation data; for the Joint task, columns 9?12 are also empty in the evaluation data.
closed challenge, Macro F1 score. However, scores
can also be computed for a number of other condi-
tions:
? Task: Joint or SRL-only
? Challenge: open or closed
? Domain: in-domain data (IDD, separated from
training corpus) or out-of-domain data (OOD)
Joint task participants are also evaluated separately
on the syntactic dependency task (labeled attach-
ment score, LAS). Finally, systems competing in
both tasks are compared on semantic role labeling
alone, to assess the impact of the the joint pars-
ing/SRL task compared to an SRL-only task on pre-
parsed data.
Finally, as an explanatory measure, precision and
recall of the semantic labeling task have been com-
puted and tabulated.
We have decided to omit several evaluation fig-
ures that were reported in previous years, such as the
percentage of completely correct sentences (?Exact
Match?), unlabeled scores, etc. With seven lan-
guages, two tasks (plus two challenges, and the
IDD/OOD distinction), there are enough results to
get lost even as it is.
2.4.1 Syntactic Dependency Measures
The LAS score is defined similarly as in the pre-
vious shared tasks, as the percentage of tokens for
which a system has predicted the correct HEAD and
DEPREL columns. The unlabeled attachment score
(UAS), i.e., the percentage of tokens with correct
HEAD regardless if the DEPREL is correct, has not
been officially computed this year. No precision and
recall measures are applicable, since all systems are
supposed to output a single dependency with a single
label (see also below the footnote to the description
of the combined score).
2.4.2 Semantic Labeling Measures
The semantic propositions are evaluated by con-
verting them to semantic dependencies, i.e., we cre-
ate n semantic dependencies from every predicate
to its n arguments. These dependencies are labeled
with the labels of the corresponding arguments. Ad-
ditionally, we create a semantic dependency from
each predicate to a virtual ROOT node. The latter
dependencies are labeled with the predicate senses.
This approach guarantees that the semantic depen-
dency structure conceptually forms a single-rooted,
connected (but not necessarily acyclic) graph. More
importantly, this scoring strategy implies that if a
system assigns the incorrect predicate sense, it still
receives some points for the arguments correctly as-
signed. For example, for the correct proposition:
verb.01: A0, A1, AM-TMP
the system that generates the following output for
the same argument tokens:
4
verb.02: A0, A1, AM-LOC
receives a labeled precision score of 2/4 because two
out of four semantic dependencies are incorrect: the
dependency to ROOT is labeled 02 instead of 01
and the dependency to the AM-TMP is incorrectly la-
beled AM-LOC. Using this strategy we compute pre-
cision, recall, and F1 scores for semantic dependen-
cies (labeled only).
For some languages (Czech, Japanese) there may
be more than one label in a given argument position;
for example, this happens in Czech in special cases
of reciprocity when the same token serves as two or
more arguments to the same predicate. The scorer
takes this into account and considers such cases to
be (as if) multiple predicate-argument relations for
the computation of the evaluation measures.
For example, for the correct proposition:
v1f1: ACT|EFF, ADDR
the system that generates the following output for
the same argument tokens:
v1f1: ACT, ADDR|PAT
receives a labeled precision score of 3/4 because
the PAT is incorrect and labeled recall 3/4 be-
cause the EFF is missing (should the ACT|EFF and
ADDR|PAT be taken as atomic values, the scores
would then be zero).
2.4.3 Combined Syntactic and Semantic Score
We combine the syntactic and semantic measures
into one global measure using macro averaging. We
compute macro precision and recall scores by aver-
aging the labeled precision and recall for semantic
dependencies with the LAS for syntactic dependen-
cies:3
LMP = Wsem ? LPsem + (1?Wsem) ? LAS (1)
LMR = Wsem ? LRsem + (1 ?Wsem) ? LAS (2)
where LMP is the labeled macro precision and
LPsem is the labeled precision for semantic depen-
dencies. Similarly, LMR is the labeled macro re-
call and LRsem is the labeled recall for semantic
dependencies. Wsem is the weight assigned to the
3We can do this because the LAS for syntactic dependen-
cies is a special case of precision and recall, where the predicted
number of dependencies is equal to the number of gold depen-
dencies.
semantic task.4 The macro labeled F1 score, which
was used for the ranking of the participating sys-
tems, is computed as the harmonic mean of LMP
and LMR.
3 Data
The unification of the data formats for the various
languages appeared to be a challenge in itself. We
will briefly describe the processes of the conversion
of the existing treebanks in the seven languages of
the CoNLL-2009 shared task. In many instances,
the original treebanks had to be not only converted
format-wise, but also merged with other resources in
order to generate useful training and testing data that
fit the task description.
3.1 The Input Corpora
The data used as the input for the transformations
aimed at arriving at the data contents and format de-
scribed in Sect. 2.3 are described in (Taule? et al,
2008), (Xue and Palmer, 2009), (Hajic? et al, 2006),
(Surdeanu et al, 2008), (Burchardt et al, 2006) and
(Kawahara et al, 2002).
In the subsequent sections, the procedures for the
data conversion for the individual languages are de-
scribed. The data has been collected by the main
organization site and checked for format errors, and
repackaged for distribution.
There were three packages of the data distributed
to the participants: Trial, Training plus Develop-
ment, and Evaluation. The Trial data were rather
small, just to give the feeling of the format and
languages involved. A visual representation of the
Trial data was also created to make understanding
of the data easier. Any data in the same format
can be transformed and displayed in the Tree Editor
TrEd5 (Pajas and ?Ste?pa?nek, 2008) with the CoNLL
2009 Shared Task extension that can be installed
from within the editor. A sample visualization of an
English sentence after its conversion to the shared
task format (Sect. 2.3) is in Fig. 1.
Due to licensing requirements, every package of
the data had to be split into two portions. One
portion (Catalan, German, Japanese, and Spanish
data) was published on the task?s webpage for down-
4We assign equal weight to the two tasks, i.e., Wsem = 0.5.
5http://ufal.mff.cuni.cz/?pajas/tred
5
$QG
'(3 &&
VRPHWLPHV
703 5%
D
102' '7
UHSXWDEOH
102' --
FKDULW\
6%- 11
ZLWK
102' ,1
D
102' '7
KRXVHKROG
102' 11
QDPH QDPH
302' 11
JHWV JHW
5227 9%=
XVHG XVH
9& 9%1
DQG
&225' &&
GRHV
&21- 9%=
Q
W
$'9 5%
HYHQ
$'9 5%
NQRZ NQRZ
9& 9%
LW
2%- 353

3  
$0703$0703$0703
 

$$$$

 
 

$
 

 

$



$01(*

$0$'9
 


$

Figure 1: Visualisation of the English sentence ?And sometimes a reputable charity with a houshold name gets used
and doesn?t even know it.? (Penn Treebank, wsj 0559) showing jointly the labeled syntactic and semantic depen-
dencies. The basic tree shape comes from the syntactic dependencies; syntactic labels and POS tags are on the 2nd
line at each node. Semantic dependencies which do not follow the syntactic ones use dotted lines. Predicate senses
in parentheses (use:01, ...) follow the word label. SRLs (A0, AM-TMP, ...) are on the last line. Please note that
multiple semantic dependencies (e.g., there are four for charity: A0? know, A1? gets, A1? used, A1? name)
and self-dependencies (name) appear in this sentence.
load, the other portion (Czech, English, and Chinese
data) was invoiced and distributed by the Linguistic
Data Consortium under a special agreement free of
charge.
Distribution of the Evaluation package was a bit
more complicated, because there were two types of
the packages - one for the Joint task and one for the
SRL-only task. Every participant had to subscribe
to one of the two tasks; subsequently, they obtained
the appropriate data (again, from the webpage and
LDC).
Prior to release, each data file was checked to
eliminate errors. The following test were carried
out:
? For every sentence, number of PREDs rows
matches the number of APREDs columns.
? The first line of each file is never empty, while
the last line always is.
? The first character on a non-empty line is al-
ways a digit, the last one is never a whitespace.
? The number of empty lines (i.e. the number
of sentences) equals the number of lines begin-
ning with ?1?.
? The data contain no spaces nor double tabs.
Some statistics on the data can be seen in Ta-
bles 2, 3 and 4. Whereas the training sizes of the
data have not been that different as they were e.g.
for the 2007 shared task on multilingual dependency
parsing (Nivre et al, 2007)6, substantial differences
existed in the distribution of the predicates and ar-
guments, the input features, the out-of-vocabulary
rates, and other statistical characteristics of the data.
Data sizes have been relatively uniform in all the
datasets, with Japanese having the smallest dataset
6http://nextens.uvt.nl/depparse-wiki/
DataOverview
6
containing data for SRL annotation training. To
compensate at least for the dependency parsing part,
an additional, large Japanese corpus with syntactic
dependency annotation has been provided.
The average sentence length, the vocabulary sizes
for FORM and LEMMA fields and the OOV rates
characterize quite naturally the properties of the re-
spective languages (in the domain of the training and
evaluation data). It is no surprise that the FORM
OOV rate is the highest for Czech, a highly inflec-
tional language, and that the LEMMA OOV rate is
the highest for German (as a consequence of keeping
compounds as a single lemma). The other statistics
also reflect (to a large extent) the annotation speci-
fication and conventions used for the original tree-
banks and/or the result of the conversion process to
the unified CoNLL-2009 Shared Task format.
Starting with the POS and FEAT fields, it can be
seen that Catalan, Czech and Spanish use only the
12 major part-of-speech categories as values of the
POS field (with richly populated FEAT field); En-
glish and Chinese are the opposite extreme, disre-
garding the use of the FEAT field completely and
coding everything as a POS value. While for Chi-
nese this is quite understandable, English follows the
PTB tradition in this respect. German and Japanese
use relatively rich set of values in both the POS and
FEAT fields.
For the dependency relations (DEPREL), all
the languages use a similarly-sized set except for
Japanese, which only encodes the distinction be-
tween a root and a dependent node (and some in-
frequent special ones).
Evaluation data are over 10% of the size of the
training data for Catalan, Chinese, Czech, Japanese
and Spanish and roughly 5% for English and Ger-
man.
Table 3 shows the distribution of the five most fre-
quent dependency relations (determined as part of
the subtask of syntactic parsing). With the exception
of Japanese, which essentially does not label depen-
dency relations at this level, all the other languages
show little difference in this distribution. For exam-
ple, the unconditioned probability of ?subjects? is
almost the same for all the six other languages (be-
tween 6 and 8 percent). The probability mass cov-
ered by the first five most frequent DEPRELs is also
almost the same (again, except for Japanese), sug-
gesting that the labeling task might have similar dif-
ficulty7. The most skewed one is for Czech (after
Japanese).
Table 4 shows similar statistics for the argument
labels (PRED/APREDs); it also adds the average
number of arguments per ?predicate? token, since
this is part of the SRL task8. It is apparent from the
comparison of the ?Total? rows in this table and Ta-
ble 3 that the first five argument labels cover more
that their syntactic counterparts. For example, the
arguments A0-A4 account for all but 3% of all ar-
guments labels, whereas Spanish and Catalan have
much more rich set of argument labels, with a high
entropy of the most-frequent-label distribution.
3.2 Catalan and Spanish
The Catalan and Spanish datasets (Taule? et al, 2008)
were generated from the AnCora corpora9 through
an automatic conversion process from a constituent-
based formalism to dependencies (Civit et al, 2006).
AnCora corpora contain about half million words
for Catalan and Spanish annotated with syntactic
and semantic information. Text sources for the Cata-
lan corpus are EFE news agency (?75Kw), ACN
Catalan news agency (?225Kw), and ?El Perio?dico?
newspaper (?200Kw). The Spanish corpus comes
from the Lexesp Spanish balanced corpus (?75Kw),
the EFE Spanish news agency (?225Kw), and the
Spanish version of ?El Perio?dico? (?200Kw). The
subset from ?El Perio?dico? corresponds to the same
news in Catalan and Spanish, spanning from January
to December 2000.
Linguistic annotation is the same in both lan-
guages and includes: PoS tags with morphologi-
cal features (gender, number, person, etc.), lemma-
tization, syntactic dependencies (syntactic func-
tions), semantic dependencies (arguments and the-
matic roles), named entities and predicate semantic
classes (Lexical Semantic Structure, LSS). Tag sets
are shared by the two languages.
If we take into account the complete PoS tags,
7Yes, this is overgeneralization since this distribution does
not condition on the features, dependencies etc. But as a rough
measure, it often correlates well with the results.
8A number below 1 means there are some argument-bearing
words (often nouns) which have no arguments in the particular
sentence in which they appear.
9http://clic.ub.edu/ancora
7
Characteristic Catalan Chinese Czech English German Japanese Spanish
Training data size (sentences) 13200 22277 38727 39279 36020 4393a 14329
Training data size (tokens) 390302 609060 652544 958167 648677 112555a 427442
Avg. sentence length (tokens) 29.6 27.3 16.8 24.4 18.0 25.6 29.8
Tokens with argumentsb (%) 9.6 16.9 63.5 18.7 2.7 22.8 10.3
DEPREL types 50 41 49 69 46 5 49
POS types 12 41 12 48 56 40 12
FEAT types 237 1 1811 1 267 302 264
FORM vocabulary size 33890 40878 86332 39782 72084 36043 40964
LEMMA vocabulary size 24143 40878 37580 28376 51993 30402 26926
Evaluation data size (sent.) 1862 2556 4213 2399 2000 500 1725
Evaluation data size (tokens) 53355 73153 70348 57676 31622 13615 50630
Evaluation FORM OOVc 5.40 3.92 7.98/8.62d 1.58/3.76d 7.93/7.57d 6.07 5.63
Evaluation LEMMA OOVc 4.14 3.92 3.03/4.29d 1.08/2.30d 5.83/7.36d 5.21 3.69
Table 2: Elementary data statistics for the CoNLL-2009 Shared Task languages. The data themselves, the original
treebanks they were derived from and the conversion process are described in more detail in sections 3.2-3.7. All
evaluation data statistics are derived from the in-domain evaluation data.
aThere were additional 33257 sentences (839947 tokens) available for syntactic dependency parsing of Japanese; the type and
vocabulary statistics are computed using this larger dataset.
bPercentage of tokens with FILLPRED=?Y?.
cPercentage of FORM/LEMMA tokens not found in the respective vocabularies derived solely from the training data.
dOOV percentage for in-domain/out-of-domain data.
DEPREL Catalan Chinese Czech English German Japanese Spanish
sn 0.16 COMP 0.21 Atr 0.26 NMOD 0.27 NK 0.31 D 0.93 sn 0.16
spec 0.15 NMOD 0.14 AuxP 0.10 P 0.11 PUNC 0.14 ROOT 0.04 spec 0.15
Labels f 0.11 ADV 0.10 Adv 0.10 PMOD 0.10 MO 0.12 P 0.03 f 0.12
sp 0.09 UNK 0.09 Obj 0.07 SBJ 0.07 SB 0.07 A 0.00 sp 0.08
suj 0.07 SBJ 0.08 Sb 0.06 OBJ 0.06 ROOT 0.06 I 0.00 suj 0.08
Total 0.58 0.62 0.59 0.61 0.70 1.00 0.59
Table 3: Unigram probability for the five most frequent DEPREL labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five dependency labels shown.
APRED Catalan Chinese Czech English German Japanese Spanish
arg1-pat 0.22 A1 0.30 RSTR 0.30 A1 0.37 A0 0.40 GA 0.33 arg1-pat 0.20
arg0-agt 0.18 A0 0.27 PAT 0.18 A0 0.25 A1 0.39 WO 0.15 arg0-agt 0.19
Labels arg1-tem 0.15 ADV 0.20 ACT 0.17 A2 0.12 A2 0.12 NO 0.15 arg1-tem 0.15
argM-tmp 0.08 TMP 0.07 APP 0.06 AM-TMP 0.06 A3 0.06 NI 0.09 arg2-atr 0.08
arg2-atr 0.08 DIS 0.04 LOC 0.04 AM-MNR 0.03 A4 0.01 DE 0.06 argM-tmp 0.08
Total 0.71 0.91 0.75 0.83 0.97 0.78 0.70
Avg. 2.25 2.26 0.88 2.20 1.97 1.71 2.26
Table 4: Unigram probability for the five most frequent APRED labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five argument labels shown. The ?Avg.? line
shows the average number of arguments per predicate or other argument-bearing token (i.e. for those marked by
FILLPRED=?Y?).
8
AnCora has 280 different labels. Considering only
the main syntactic categories, the tag set is reduced
to 47 tags. The syntactic tag set consists of 50 dif-
ferent syntactic functions. Regarding semantic ar-
guments, we distinguish Arg0, Arg1, Arg2, Arg3,
Arg4, ArgM, and ArgL. The first five tags are num-
bered from less to more obliqueness with respect
to the verb, ArgM corresponds to adjuncts. The
list of thematic roles consists of 20 different labels:
AGT (Agent), AGI (Induced Agent), CAU (Cause),
EXP (Experiencer), SCR (Source), PAT (Patient),
TEM (Theme), ATR (Attribute), BEN (Beneficiary),
EXT (Extension), INS (Instrument), LOC (Loca-
tive), TMP (Time), MNR (Manner), ORI (Origin),
DES (Goal), FIN (Purpose), EIN (Initial State), EFI
(Final State), and ADV (Adverbial). Each argument
position can map onto specific thematic roles. By
way of example, Arg1 can be PAT, TEM or EXT. For
Named Entities, we distinguish six types: Organiza-
tion, Person, Location, Date, Number, and Others.
An incremental process guided the annotation of
AnCora, since semantics depends on morphosyntax,
and syntax relies on morphology. This procedure
made it possible to check, correct, and complete
the previous annotations, thus guaranteeing the final
quality of the corpora and minimizing the error rate.
The annotation process was carried out sequentially
from lower to upper layers of linguistic description.
All resulting layers are independent of each other,
thus making easier the data management. The ini-
tial annotation was performed manually for syntax,
semiautomatically in the case of arguments and the-
matic roles, and fully automatically for PoS (Mart??
et al, 2007; Ma`rquez et al, 2007).
The Catalan and Spanish AnCora corpora were
straightforwardly translated into the CoNLL-2009
shared task formatting (information about named
entities was skipped in this process). The resulting
Catalan corpus (including training, development and
test partitions) contains 16,786 sentences with an av-
erage length of 29.59 lexical tokens per sentence.
Long sentences abound in this corpus. For instance,
10.73% of the sentences are longer than 50 tokens,
and 4.42% are longer than 60. The corpus con-
tains 47,537 annotated predicates (2.83 predicates
per sentence, on average) with 107,171 arguments
(2.25 arguments per predicate, on average). From
the latter, 73.89% correspond to core arguments and
26.11% to adjuncts. Numbers for the Spanish cor-
pus are comparable in all aspects: 17,709 sentences
with 29.84 lexical tokens on average (11.58% of the
sentences longer than 50 tokens, 4.07% longer than
60); 54,075 predicates (3.05 per sentence, on aver-
age) and 122,478 arguments (2.26 per predicate, on
average); 73.34% core arguments and 26.66% ad-
juncts.
The following are important features of the Cata-
lan and Spanish corpora in the CoNLL-2009 shared
task setting: (1) all dependency trees are projective;
(2) no word can be the argument of more than one
predicate in a sentence; (3) semantic dependencies
completely match syntactic dependency structures
(i.e., no new edges are introduced by the semantic
structure); (4) only verbal predicates are annotated
(with exceptional cases referring to words that can
be adjectives and past participles); (5) the corpus is
segmented so multi-words, named entities, temporal
expressions, compounds, etc. are grouped together;
and (6) segmentation also accounts for elliptical pro-
nouns (there are marked as empty lexical tokens ?_?
with a pronoun POS tag).
Finally, the predicted columns (PLEMMA,
PPOS, and PFEAT) have been generated with the
FreeLing Open source suite of Language Analyz-
ers10. Accuracy in PLEMMA and PPOS columns
is above 95% for the two languages. PHEAD
and PDEPREL columns have been generated using
MaltParser11. Parsing accuracy (LAS) is above 86%
for the the two languages.
3.3 Chinese
The Chinese Corpus for the 2009 CoNLL Shared
Task was generated by merging the Chinese Tree-
bank (Xue et al, 2005) and the Chinese Proposition
Bank (Xue and Palmer, 2009) and then converting
the constituent structure to a dependency formalism
as specified in the CoNLL Shared Task. The Chi-
nese data used in the shared task is based on Chinese
Treebank 6.0 and the Chinese Proposition Bank 2.0,
both of which are publicly available via the Linguis-
tic Data Consortium.
The Chinese Treebank Project originated at Penn
and was later moved to University of Colorado at
10http://www.lsi.upc.es/?nlp/freeling
11http://w3.msi.vxu.se/?jha/maltparser
9
Boulder. Now it is the process of being to moved
to Brandeis University. The data sources of the Chi-
nese Treebank range from Xinhua newswire (main-
land China), Hong Kong news, and Sinorama Maga-
zine (Taiwan). More recently under DARPA GALE
funding it has been expanded to include broadcast
news, broadcast conversation, news groups and web
log data. It currently has over one million words
and is fully segmented, POS-tagged and annotated
with phrase structure. The version of the Chinese
Treebank used in this shared task, CTB 6.0, includes
newswire, magazine articles, and transcribed broad-
cast news 12. The training set has 609,060 tokens,
the development set has 49,620 tokens, and the test
set has 73,153 tokens.
The Chinese Proposition Bank adds a layer of se-
mantic annotation to the syntactic parses in the Chi-
nese Treebank. This layer of semantic annotation
mainly deals with the predicate-argument structure
of Chinese verbs and their nominalizations. Each
major sense (called frameset) of a predicate takes a
number of core arguments annotated with numeri-
cal labels Arg0 through Arg5 which are defined in
a predicate-specific manner. The Chinese Proposi-
tion Bank also annotates adjunctive arguments such
as locative, temporal and manner modifiers of the
predicate. The version of the Chinese Propbank used
in this CoNLL Shared Task is CPB 2.0, but nominal
predicates are excluded because the annotation is in-
complete.
Since the Chinese Treebank is annotated with
constituent structures, the conversion and merging
procedure converts the constituent structures to de-
pendencies by identifying the head for each con-
stituent in a parse tree and making its sisters its de-
pendents. The Chinese Propbank pointers are then
shifted from the entire constituent to the head of that
constituent. The conversion procedure identifies the
head by first exploiting the structural information
in the syntactic parse and detecting six broad cate-
gories of syntactic relations that hold between the
head and its dependents (predication, modification,
complementation, coordination, auxiliary, and flat)
and then designating the head based on these rela-
tions. In particular, the first conjunct of a coordina-
12A small number of files were taken out of the CoNLL
shared task data due to conversion problems and time con-
straints to fix them.
tion structure is designated as the head and the heads
of the other conjuncts are the conjunctions preced-
ing them. The conjunctions all ?modify? the first
conjunct.
3.4 Czech
For the training, development and evaluation data,
Prague Dependency Treebank 2.0 was used (Hajic?
et al, 2006). For the out-of-domain evaluation data,
part of the Czech side of the Prague Czech-English
Dependency Treebank (version 2, under construc-
tion) was used13, see also ( ?Cmejrek et al, 2004). For
the OOD data, no manual annotation of LEMMA,
POS, and FEAT existed, so the predicted values
were used. The same conversion procedure has been
applied to both sources.
The FORM column was created from the form
element of the morphological layer, not from the
?token? from the word-form layer. Therefore, most
typos, errors in word segmentation and tokenization
are corrected and numerals are normalized.
The LEMMA column was created from the
lemma element of the morphological layer. Only
the initial string of the element was used, so there is
no distinction between homonyms. However, some
components of the detailed lemma explanation were
incorporated into the FEAT column (see below).
The POS column was created form the morpho-
logical tag element, its first character more pre-
cisely.
The FEAT column was created from the remain-
ing characters of the tag element. In addition, the
special feature ?Sem? corresponds to a semantic fea-
ture of the lemma.
For the HEAD and DEPREL columns, the PDT
analytical layer was used. The DEPREL was taken
from the analytic function (the afun node at-
tribtue). There are 27 possible values for afun el-
ement: Pred, Pnom, AuxV, Sb, Obj, Atr, Adv,
Atv, AtvV, Coord, Apos, ExD, and a number
of auxiliary and ?double-function? labels. The first
nine of these are the ?most interesting? from the
point of view of the shared task, since they relate to
semantics more closely than the rest (at least from
the linguistic point of view). The HEAD is a pointer
to its parent, which means the PDT?s ord attribute
13http://ufal.mff.cuni.cz/pedt
10
(within-sentence ID / word position number) of the
parent. If a node is a member of a coordination
or apposition (is_member element), its DEPREL
obtains the _M suffix. The parenthesis annotation
(is_parenthesis_root element) was ignored.
The PRED and APREDs columns were created
from the tectogrammatical layer of PDT 2.0 and the
valency lexicon PDT-Vallex according to the follow-
ing rules:
? Every line corresponding to an analytical node
referenced by a lexical reference (a/lex.rf)
from the tectogrammatical layer has a PRED
value filled. If the referring non-generated
tectogrammatical node (is_generated not
equal to 1) has a valency frame assigned
(val_frame.rf), the value of PRED is the
identifier of the frame. Otherwise, it is set to
the same value as the LEMMA column.
? For every tectogrammatical node, a corre-
sponding analytical node is searched for:
1. If the tectogrammatical node is not
generated and has a lexical reference
(a/lex.rf), the referenced node is
taken.
2. Otherwise, if the tectogrammatical node
has a coreference (coref_text.rf or
coref_gram.rf) or complement refer-
ence (compl.rf) to a node that has an
analytical node assigned (by 1. or 2.), the
assigned node is taken.
APRED columns are filled with respect to the
following correspondence: for a tectogrammatical
node P and its effective child C with functor F, the
column for P?s corresponding analytical node at the
row for C?s corresponding analytical node is filled
with F. Some nodes can thus have several functors
in one APRED column, separated by a vertical bar
(see Sect. 2.4.2).
PLEMMA, PPOS and PFEAT were gener-
ated by the (cross-trained) morphological tagger
MORCE (Spoustova? et al, 2009), which gives full
combined accuracy (PLEMMA+PPOS+PFEAT)
slightly under 96%.
PHEAD and PDEPREL were generated by
the (cross-trained) MST parser for Czech (Chu?
Liu/Edmonds algorithm, (McDonald et al, 2005)),
which has typical dependency accuracy around
85%.
The valency lexicon, converted from (Hajic? et al,
2003), has four columns:
1. lemma (can occur several times in the lexicon,
with different frames)
2. frame identifier (as found in the PRED column)
3. list of space-separated actants and obligatory
members of the frame
4. example(s)
The source of the out-of-domain data uses an
extended valency lexicon (because of out-of-
vocabulary entries). For simplicity, the extended
lexicon was not provided; instead, such words were
not marked as predicates in the OOD data (their
FILLPRED was set to ?_?) and thus not evaluated.
3.5 English
The English corpus is almost identical to the cor-
pus used in the closed challenge in the CoNLL-2008
shared task evaluation (Surdeanu et al, 2008). This
corpus was generated through a process that merges
several input corpora and converts them from the
constituent-based formalism to dependencies. The
following corpora were used as input to the merging
procedure:
? Penn Treebank 3 ? The Penn Treebank 3 cor-
pus (Marcus et al, 1994) consists of hand-
coded parses of the Wall Street Journal (test,
development and training) and a small subset
of the Brown corpus (W. N. Francis and H.
Kucera, 1964) (test only).
? BBN Pronoun Coreference and Entity Type
Corpus ? BBN?s NE annotation of the Wall
Street Journal corpus (Weischedel and Brun-
stein, 2005) takes the form of SGML inline
markup of text, tokenized to be completely
compatible with the Penn Treebank annotation.
For the CoNLL-2008 shared task evaluation,
this corpus was extended by the task organizers
to cover the subset of the Brown corpus used as
a secondary testing dataset. From this corpus
we only used NE boundaries to derive NAME
11
dependencies between NE tokens, e.g., we cre-
ate a NAME dependency from Mary to Smith
given the NE mention Mary Smith.
? Proposition Bank I (PropBank) ? The Prop-
Bank annotation (Palmer et al, 2005) classifies
the arguments of all the main verbs in the Penn
Treebank corpus, other than be. Arguments are
numbered (Arg0, Arg1, . . .) based on lexical
entries or frame files. Different sets of argu-
ments are assumed for different rolesets. De-
pendent constituents that fall into categories in-
dependent of the lexical entries are classified as
various types of adjuncts (ArgM-TMP, -ADV,
etc.).
? NomBank ? NomBank annotation (Meyers et
al., 2004) uses essentially the same framework
as PropBank to annotate arguments of nouns.
Differences between PropBank and NomBank
stem from differences between noun and verb
argument structure; differences in treatment of
nouns and verbs in the Penn Treebank; and dif-
ferences in the sophistication of previous re-
search about noun and verb argument structure.
Only the subset of nouns that take arguments
are annotated in NomBank and only a subset of
the non-argument siblings of nouns are marked
as ArgM.
The complete merging process and the conversion
from the constituent representation to dependencies
is detailed in (Surdeanu et al, 2008).
The main difference between the 2008 and 2009
version of the corpora is the generation of word lem-
mas. In the 2008 version the only lemmas pro-
vided were predicted using the built-in lemmatizer
in WordNet (Fellbaum, 1998) based on the most fre-
quent sense for the form and the predicted part-of-
speech tag. These lemmas are listed in the 2009
corpus under the PLEMMA column. The LEMMA
column in the 2009 version of the corpus contains
lemmas generated using the same algorithm but us-
ing the correct Treebank part-of-speech tags. Addi-
tionally, the PHEAD and PDEPREL columns were
generated using MaltParser14, similarly to the open
challenge corpus in the CoNLL 2008 shared task.
14http://w3.msi.vxu.se/?nivre/research/
MaltParser.html
3.6 German
The German in-domain dataset is based on the an-
notated verb instances of the SALSA corpus (Bur-
chardt et al, 2006), a total of around 40k sen-
tences15. SALSA provides manual semantic role
annotation on top of the syntactically annotated
TIGER newspaper corpus, one of the standard Ger-
man treebanks. The original SALSA corpus uses se-
mantic roles in the FrameNet paradigm. We con-
structed mappings between FrameNet frame ele-
ments and PropBank argument positions at the level
of frame-predicate pairs semi-automatically. For the
frame elements of each frame-predicate pair, we first
identified the semantically defined PropBank Arg-
0 and Arg-1 positions. To do so, we annotated a
small number of very abstract frame elements with
these labels (Agent, Actor, Communicator as Arg-
0, and Theme, Effect, Message as Arg-1) and per-
colated these labels through the FrameNet hierar-
chy, adding further manual labels where necessary.
Then, we used frequency and grammatical realiza-
tion information to map the remaining roles onto
higher-numbered Arg roles. We considerably sim-
plified the annotations provided by SALSA, which
use a rather complex annotation scheme. In partic-
ular, we removed annotation for multi-word expres-
sions (which may be non-contiguous), annotations
involving multiple frames for the same predicate
(metaphors, underspecification), and inter-sentence
roles.
The out-of-domain dataset was taken from a study
on the multi-lingual projection of FrameNet annota-
tion (Pado and Lapata, 2005). It is sampled from
the EUROPARL corpus and was chosen to maxi-
mize the lexical coverage, i.e., it contains of a large
number of infrequent predicates. Both syntactic and
semantic structure were annotated manually, in the
TIGER and SALSA format, respectively. Since it
uses a simplified annotation schemes, we did not
have to discard any annotation.
For both datasets, we converted the syntactic
TIGER (Brants et al, 2002) representations into de-
pendencies with a similar set of head-finding rules
used for the preparation of the CoNLL-X shared task
German dataset. Minor modifications (for the con-
15Note, however, that typically not all predicates in each sen-
tence are annotated (cf. Table 2).
12
version of person names and coordinations) were
made to achieve better consistency with datasets
of other languages. Since the TIGER annotation
allows non-contiguous constituents, the resulting
dependencies can be non-projective. Secondary
edges were discarded in the conversion. As for the
automatically constructed features, we used Tree-
Tagger (Schmid, 1994) to produce the PLEMMA
and PPOS columns, and the Morphisto morphol-
ogy (Zielinski and Simon, 2008) for PFEAT.
3.7 Japanese
For Japanese, we used the Kyoto University Text
Corpus (Kawahara et al, 2002), which consists of
approximately 40k sentences taken from Mainichi
Newspapers. Out of them, approximately 5k sen-
tences are annotated with syntactic and semantic de-
pendencies, and are used the training, development
and test data of this year?s shared task. The remain-
ing sentences, which are annotated with only syntac-
tic dependencies, are provided for the training cor-
pus of syntactic dependency parsers.
This corpus adopts a dependency structure repre-
sentation, and thus the conversion to the CoNLL-
2009 format was relatively straightforward. How-
ever, since the original dependencies are annotated
on the basis of phrases (Japanese bunsetsu), we
needed to automatically convert the original annota-
tions to word-based ones using several criteria. We
used the following basic criteria: the words except
the last word in a phrase depend on the next (right)
word, and the last word in a phrase basically depends
on the head word of the governing phrase.
Semantic dependencies are annotated for both
verbal predicates and nominal predicates. The se-
mantic roles (APRED columns) consist of 41 sur-
face cases, many of which are case-marking post-
positions such as ga (nominative), wo (accusative)
and ni (dative). Semantic frame discrimination is not
annotated, and so the PRED column is the same as
the LEMMA column. The original corpus contains
coreference annotations and inter-sentential seman-
tic dependencies, such as inter-sentential zero pro-
nouns and bridging references, but we did not use
these annotations, which are not the target of this
year?s shared task.
To produce the PLEMMA, PPOS and PFEAT
columns, we used the morphological analyzer JU-
MAN 16 and the dependency and case structure an-
alyzer KNP 17. To produce the PHEAD and PDE-
PREL columns, we used the MSTParser 18.
4 Submissions and Results
Participants uploaded the results through the shared
task website, and the official evaluation was per-
formed centrally. Feedback was provided if any for-
mal problems were encountered (for a list of checks,
see the previous section). One submission had to
be rejected because only English results were pro-
vided. After the evaluation period had passed, the
results were anonymized and published on the web.
A total of 20 systems participated in the closed
challenge; 13 of them in the Joint task and seven in
the SRL-only task. Two systems participated in the
open challenge (Joint task). Moreover, 17 systems
provided output in the out-of-domain part of the task
(11 in the OOD Joint task and six in the OOD SRL-
only task).
The main results for the core task - the Joint task
(dependency syntax and semantic relations) in the
context of the closed challenge - are summarized and
ranked in Table 5.
The largest number of systems can be compared
in the SRL results table (Table 6), where all the sys-
tems have been evaluated solely on the SRL perfor-
mance regardless whether they participated in the
Joint or SRL-only task. However, since the results
might have been influenced by the supplied parser,
separate ranking is provided for both types of the
systems.
Additional breakdown of the results (open chal-
lenge, precision and recall tables for the semantic
labeling task, etc.) are available from the CoNLL-
2009 Shared Task website19.
5 Approaches
Table 7 summarizes the properties of the systems
that participated in the closed the open challenges.
16http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
17http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
18http://sourceforge.net/projects/
mstparser
19http://ufal.mff.cuni.cz/conll2009-st
13
Rank System Average Catalan Chinese Czech English German Japanese Spanish
1 Che 82.64 81.84 76.38 83.27 87.00 82.44 85.65 81.90
2 Chen 82.52 83.01 76.23 80.87 87.69 81.22 85.28 83.31
3 Merlo 82.14 82.66 76.15 83.21 86.03 79.59 84.91 82.43
4 Bohnet 80.85 80.44 75.91 79.57 85.14 81.60 82.51 80.75
5 Asahara 78.43 75.91 73.43 81.43 86.40 69.84 84.86 77.12
6 Brown 77.27 77.40 72.12 75.66 83.98 77.86 76.65 77.21
7 Zhang 76.49 75.00 73.42 76.93 82.88 73.76 78.17 75.25
8 Dai 73.98 72.09 72.72 67.14 81.89 75.00 80.89 68.14
9 Lu Li 73.97 71.32 65.53 75.85 81.92 70.93 80.49 71.72
10 Llu??s 71.49 56.64 66.18 75.95 81.69 72.31 81.76 65.91
11 Vallejo 70.81 73.75 67.16 60.50 78.19 67.51 77.75 70.78
12 Ren 67.81 59.42 75.90 60.18 77.83 65.77 77.63 57.96
13 Zeman 51.07 49.61 43.50 57.95 50.27 49.57 57.69 48.90
Table 5: Official results of the Joint task, closed challenge. Teams are denoted by the last name (first name added
only where needed) of the author who registered for the evaluation data. Results are sorted in descending order of the
language-averaged macro F1 score on the closed challenge Joint task. Bold numbers denote the best result for a given
language.
Rank Rank in task System Average Catalan Chinese Czech English German Japanese Spanish
1 1 (SRLonly) Zhao 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.46
2 2 (SRLonly) Nugues 80.31 80.01 78.60 85.41 85.63 79.71 76.30 76.52
3 1 (Joint) Chen 79.96 80.10 76.77 82.04 86.15 76.19 78.17 80.29
4 2 (Joint) Che 79.94 77.10 77.15 86.51 85.51 78.61 78.26 76.47
5 3 (Joint) Merlo 78.42 77.44 76.05 86.02 83.24 71.78 77.23 77.19
6 3 (SRLonly) Meza-Ruiz 77.46 78.00 77.73 75.75 83.34 73.52 76.00 77.91
7 4 (Joint) Bohnet 76.00 74.53 75.29 79.02 80.39 75.72 72.76 74.31
8 5 (Joint) Asahara 75.65 72.35 74.17 84.69 84.26 63.66 77.93 72.50
9 6 (Joint) Brown 72.85 72.18 72.43 78.02 80.43 73.40 61.57 71.95
10 7 (Joint) Dai 70.78 66.34 71.57 75.50 78.93 67.43 71.02 64.64
11 8 (Joint) Zhang 70.31 67.34 73.20 78.28 77.85 62.95 64.71 67.81
12 9 (Joint) Lu Li 69.72 66.95 67.06 79.08 77.17 61.98 69.58 66.23
13 4 (SRLonly) Baoli Li 69.26 74.06 70.37 57.46 69.63 67.76 72.03 73.54
14 10 (Joint) Vallejo 68.95 70.14 66.71 71.49 75.97 61.01 68.82 68.48
15 5 (SRLonly) Moreau 66.49 65.60 67.37 71.74 72.14 66.50 57.75 64.33
16 11 (Joint) Llu??s 63.06 46.79 59.72 76.90 75.86 62.66 71.60 47.88
17 6 (SRLonly) Ta?ckstro?m 61.27 57.11 63.41 71.05 67.64 53.42 54.74 61.51
18 7 (SRLonly) Lin 57.18 61.70 70.33 60.43 65.66 59.51 23.78 58.87
19 12 (Joint) Ren 56.69 41.00 72.58 62.82 67.56 54.31 58.73 39.80
20 13 (Joint) Zeman 32.14 24.19 34.71 58.13 36.05 16.44 30.13 25.36
Table 6: Official results of the semantic labeling, closed challenge, all systems. Teams are denoted by the last name
(first name added only where needed) of the author who registered for the evaluation data. Results are sorted in
descending order of the semantic labeled F1 score (closed challenge). Bold numbers denote the best result for a given
language. Separate ranking is provided for SRL-only systems.
The second column of the table highlights the over-
all architectures. We used + to indicate that the
components are sequentially connected. The lack of
a + sign indicates that the corresponding tasks are
performed jointly.
It is perhaps not surprising that most of the obser-
vations from the 2008 shared task still hold; namely,
the best systems overall do not use joint learning or
optimization (the best such system was placed third
in the Joint task, and there were only four systems
where the learning methodology can be considered
?joint?).
Therefore, most of the observations and conclu-
sions from 2008 shared task hold as well for the
current results. For details, we will leave it to the
reader to interpret the architectures and methods
14
O
v
er
a
ll
D
D
D
PA
PA
PA
Jo
in
t
M
L
Sy
st
em
a
A
rc
h.
b
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
c
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
Le
a
rn
in
g/
O
pt
.
M
et
ho
ds
Zh
ao
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
/g
lo
ba
l
se
ar
ch
(S
R
L-
o
n
ly
)
M
E
N
u
gu
es
(P
C+
A
I+
A
C)
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
be
am
se
ar
ch
+
re
ra
n
ki
n
g
(S
R
L-
o
n
ly
)
L2
-
re
gu
la
riz
ed
lin
.
re
gr
es
sio
n
Ch
en
P
+
PC
+
A
I+
A
C
gr
ap
h
pa
rt
ia
lly
M
ST
C
L
/E
cl
as
s
n
o
gr
ee
dy
(?)
n
o
M
E
Ch
e
D
+
PC
+
A
IC
gr
ap
h
n
o
M
ST
H
O
E
cl
as
s
n
o
IL
P
n
o
SV
M
,
M
E
M
er
lo
D
PA
IC
+
D
ge
n
er
at
iv
e,
tr
an
s
n
o
be
am
se
ar
ch
tr
an
s
n
o
be
am
se
ar
ch
sy
n
ch
ro
n
iz
ed
de
riv
at
io
n
IS
B
N
M
ez
a-
R
u
iz
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
M
ar
ko
v
LN
n
o
Cu
tti
n
g
Pl
an
e
(S
R
L-
o
n
ly
)
M
IR
A
B
o
hn
et
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
C
+
re
ar
ra
n
ge
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
IR
A
)
A
sa
ha
ra
D
+
PI
C
+
A
IC
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
n
-
be
st
re
la
x
.
n
o
pe
rc
ep
tr
o
n
D
ai
D
+
PC
+
A
C
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
pr
o
b
ite
ra
tiv
e
M
E
Zh
an
g
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
E
cl
as
s
n
o
cl
as
sifi
ca
tio
n
n
o
M
IR
A
,
M
E
Lu
Li
D
+
(P
C
||
A
IC
)
gr
ap
h
fo
r
ea
ch
la
n
g.
M
ST
C
L
/E
,
M
ST
E
cl
as
s
n
o
gr
ee
dy
n
o
M
E
B
ao
li
Li
PC
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
,
kN
N
,
M
E
Va
lle
jod
[D
+
P+
A
]C
+
D
I
cl
as
s
n
o
re
ra
n
ki
n
g
cl
as
s
n
o
re
ra
n
ki
n
g
u
n
ifi
ed
la
be
ls
M
B
L
M
o
re
au
D
+
PI
+
Cl
u
st
er
in
g
+
A
I+
A
C
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
CR
F
(S
R
L-
o
n
ly
)
CR
F
Ll
u
??s
D
+
D
A
IC
+
PC
gr
ap
h
n
o
M
ST
E
gr
ap
h
n
o
M
ST
E
ye
s,
M
ST
E
Av
g.
Pe
rc
ep
tr
o
n
Ta?
ck
st
ro?
m
D
+
PI
+
A
I
+
A
C
+
Co
n
st
ra
in
tS
at
isf
ac
tio
n
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
R
en
D
+
PC
+
A
IC
tr
an
s
n
o
gr
ee
dy
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
al
t),
M
E
Ze
m
an
D
I+
D
C+
PC
+
A
I+
A
C
tr
an
s
n
o
gr
ee
dy
w
ith
he
u
ris
tic
s
cl
as
s
n
o
gr
ee
dy
n
o
co
o
cc
u
rr
en
ce
Ta
bl
e
7:
Su
m
m
ar
y
o
fs
ys
te
m
ar
ch
ite
ct
u
re
s
fo
r
th
e
Co
N
LL
-
20
09
sh
ar
ed
ta
sk
;
al
ls
ys
te
m
s
ar
e
in
cl
u
de
d.
SR
L-
o
n
ly
sy
st
em
s
do
n
o
t
ha
v
e
th
e
D
co
lu
m
n
s
an
d
th
e
Jo
in
t
Le
ar
in
g/
O
pt
.
co
lu
m
n
s
fil
le
d
in
.
Th
e
sy
st
em
s
ar
e
so
rt
ed
by
th
e
se
m
an
tic
la
be
le
d
F 1
sc
o
re
av
er
ag
ed
o
v
er
al
lt
he
la
n
gu
ag
es
(sa
m
e
as
in
Ta
bl
e
6).
O
n
ly
th
e
sy
st
em
s
th
at
ha
v
e
a
co
rr
es
po
n
di
n
g
pa
pe
r
in
th
e
pr
o
ce
ed
in
gs
ar
e
in
cl
u
de
d.
A
cr
o
n
ym
s
u
se
d:
D
-
sy
n
ta
ct
ic
de
pe
n
de
n
ci
es
,
P
-
pr
ed
ic
at
e,
A
-
ar
gu
m
en
t,
I-
id
en
tifi
ca
tio
n
,
C
-
cl
as
sifi
ca
tio
n
.
O
v
er
a
ll
a
rc
h.
st
an
ds
fo
r
th
e
co
m
pl
et
e
sy
st
em
ar
ch
ite
ct
u
re
;D
A
rc
h.
st
an
ds
fo
r
th
e
ar
ch
ite
ct
u
re
o
ft
he
sy
n
ta
ct
ic
pa
rs
er
;D
C
o
m
b.
in
di
ca
te
s
if
th
e
fin
al
pa
rs
er
o
u
tp
u
tw
as
ge
n
er
at
ed
u
sin
g
pa
rs
er
co
m
bi
n
at
io
n
;D
In
fe
re
n
ce
st
an
ds
fo
r
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
sy
n
ta
ct
ic
pa
rs
in
g;
PA
A
rc
h.
st
an
ds
th
e
ty
pe
o
fa
rc
hi
te
ct
u
re
u
se
d
fo
r
PA
IC
;P
A
C
o
m
b.
in
di
ca
te
s
if
th
e
PA
o
u
tp
u
t
w
as
ge
n
er
at
ed
th
ro
u
gh
sy
st
em
co
m
bi
n
at
io
n
;P
A
In
fe
re
n
ce
st
an
ds
fo
r
th
e
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
PA
IC
;J
o
in
tL
ea
rn
in
g/
O
pt
.
in
di
ca
te
s
if
so
m
e
fo
rm
o
fjo
in
tl
ea
rn
in
g
o
r
o
pt
im
iz
at
io
n
w
as
im
pl
em
en
te
d
fo
r
th
e
sy
n
ta
ct
ic
+
se
m
an
tic
gl
o
ba
lt
as
k;
M
L
M
et
ho
ds
lis
ts
th
e
M
L
m
et
ho
ds
u
se
d
th
ro
u
gh
o
u
tt
he
co
m
pl
et
e
sy
st
em
.
a
A
u
th
o
rs
o
ft
w
o
sy
st
em
s:
?
B
ro
w
n
?
an
d
?
Li
n
?
di
dn
?
ts
u
bm
it
a
pa
pe
r,
so
th
ei
r
sy
st
em
s?
ar
ch
ite
ct
u
re
s
ar
e
u
n
kn
ow
n
.
b T
he
sy
m
bo
l+
in
di
ca
te
s
se
qu
en
tia
lp
ro
ce
ss
in
g
(ot
he
rw
ise
,
pa
ra
lle
l/jo
in
t).
Th
e
||
m
ea
n
s
th
at
se
v
er
al
di
ffe
re
n
ta
rc
hi
te
ct
u
re
s
sp
an
n
in
g
m
u
lti
pl
e
su
bt
as
ks
ra
n
in
pa
ra
lle
l.
c
M
ST
C
L
/E
as
u
se
d
by
M
cD
o
n
al
d
(20
05
),
M
ST
C
by
Ca
rr
er
as
(20
07
),M
ST
E
by
Ei
sn
er
(20
00
),
M
ST
H
O
E
=
M
ST
E
w
ith
hi
gh
er
-
o
rd
er
fe
at
u
re
s
(si
bl
in
gs
+
al
lg
ra
n
dc
hi
ld
re
n
).
d T
he
sy
st
em
u
n
ifi
es
th
e
sy
n
ta
ct
ic
an
d
se
m
an
tic
la
be
ls
in
to
o
n
e
la
be
l,
an
d
tr
ai
n
s
cl
as
sifi
er
s
o
v
er
th
em
.
It
is
th
u
s
di
ffi
cu
lt
to
sp
lit
th
e
sy
st
em
ch
ar
ac
te
ris
tic
in
to
a
?
D
?
/?
PA
?
pa
rt
.
15
when comparing Table 7 with the Tables 5 and 6).
6 Conclusion
This year?s task has been demanding in several re-
spects, but certainly the most difficulty came from
the fact that participants had to tackle all seven lan-
guages. It is encouraging that despite this added af-
fort the number of participating systems has been
almost the same as last year (20 vs. 22 in 2008).
There are several positive outcomes from this
year?s enterprise:
? we have prepared a unified format and data for
several very different lanaguages, as a basis
for possible extensions towards other languages
and unified treatment of syntactic depenndecies
and semantic role labeling across natural lan-
guages;
? 20 participants have produced SRL results for
all seven languages, using several different
methods, giving hope for a combined system
with even substantially better performance;
? initial results have been provided for three lan-
guages on out-of-domain data (being in fact
quite close to the in-domain results).
Only four systems tried to apply what can be de-
scribed as joint learning for the syntactic and seman-
tic parts of the task. (Morante et al, 2009) use a true
joint learning formulation that phrases syntactico-
semantic parsing as a series of classification where
the class labels are concatenations of syntactic and
semantic edge labels. They predict (a), the set of
syntactico-semantic edge labels for each pair of to-
kens; (b), the set of incoming syntactico-semantic
edge labels for each individual token; and (c), the
existence of an edge between each pair of tokens.
Subsequently, they combine the (possibly conflict-
ing) output of the three classifiers by a ranking ap-
proach to determine the most likely structure that
meets all well-formedness constraints. (Llu??s et al,
2009) present a joint approach based on an exten-
sion of Eisner?s parser to accommodate also seman-
tic dependency labels. This architecture is similar
to the one presented by the same authors in the past
edition, with the extension to a second-order syn-
tactic parsing and a particular setting for Catalan
and Spanish. (Gesmundo et al, 2009) use an in-
cremental parsing model with synchronous syntac-
tic and semantic derivations and a joint probability
model for syntactic and semantic dependency struc-
tures. The system uses a single input queue but two
separate stacks and synchronizes syntactic and se-
mantic derivations at every word. The synchronous
derivations are modeled with an Incremental Sig-
moid Belief Network that has latent variables for
both syntactic and semantic states and connections
from syntax to semantics and vice versa. (Dai et
al., 2009) designed an iterative system to exploit
the inter-connections between the different subtasks
of the CoNLL shared task. The idea is to decom-
pose the joint learning problem into four subtasks
? syntactic dependency identification, syntactic de-
pendency labeling, semantic dependency identifica-
tion and semantic dependency labeling. The initial
step is to use a pipeline approach to use the input of
one subtask as input to the next, in the order speci-
fied. The iterative steps then use additional features
that are not available in the initial step to improve the
accuracy of the overall system. For example, in the
iterative steps, semantic information becomes avail-
able as features to syntactic parsing, so on and so
forth.
Despite these results, it is still not clear whether
joint learning has a significant advantage over other
approaches (and if yes, then for what languages). It
is thus necessary to carefully plan the next shared
tasks; it might be advantageous to bring up a sim-
ilar task in the future once again, and/or couple it
with selected application(s). There, (we hope) the
benefits of the dependency representation combined
with semantic roles the way we have formulated it
in 2008 and 2009 will really show up.
Acknowledgments
We would like to thank the Linguistic Data Consor-
tium, mainly to Denise DiPersio, Tony Casteletto
and Christopher Cieri for their help and handling
of invoicing and distribution of the data for which
LDC has a license. For all of the trial, training and
evaluation data they had to act a very short notice.
All the data has been at the participants? disposal
(again) free of charge. We are grateful to all of them
for LDC?s continuing support of the CoNLL Shared
16
Tasks.
We would also like to thank organizers of the pre-
vious four shared tasks: Sabine Buchholz, Xavier
Carreras, Ryan McDonald, Amit Dubey, Johan Hall,
Yuval Krymolowski, Sandra Ku?bler, Erwin Marsi,
Jens Nilsson, Sebastian Riedel and Deniz Yuret.
This shared task would not have been possible with-
out their previous effort.
We also acknowledge the support of the M?SMT
of the Czech Republic, projects MSM0021620838
and LC536; the Grant Agency of the Academy of
sciences of the Czech Republic 1ET201120505 (for
Jan Hajic?, Jan ?Ste?pa?nek and Pavel Stran?a?k).
Llu??s Ma`rquez and M. Anto`nia Mart?? partici-
pation was supported by the Spanish Ministry of
Education and Science, through the OpenMT and
TextMess research projects (TIN2006-15307-C03-
02, TIN2006-15265-C06-06).
The following individuals directly contributed to
the Chinese Treebank (in alphabetic order): Meiyu
Chang, Fu-Dong Chiou, Shizhe Huang, Zixin Jiang,
Tony Kroch, Martha Palmer, Mitch Marcus, Fei
Xia, Nianwen Xue. The contributors to the Chi-
nese Proposition Bank include (in alphabetic order):
Meiyu Chang, Gang Chen, Helen Chen, Zixin Jiang,
Martha Palmer, Zhiyi Song, Nianwen Xue, Ping Yu,
Hua Zhong. The Chinese Treebank and the Chinese
Proposition Bank were funded by DOD, NSF and
DARPA.
Adam Meyers? work on the shared task has been
supported by the NSF Grant IIS-0534700 ?Structure
Alignment-based MT.?
We thank the Mainichi Newspapers for the per-
mission of distributing the sentences of the Kyoto
University Text Corpus for this shared task.
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
EMNLP-CoNLL 2007, pages 957?961, June. Prague,
Czech Republic.
Montserrat Civit, M. Anto`nia Mart??, and Nu?ria Buf??.
2006. Cat3LB and Cast3LB: from constituents to
dependencies. In Proceedings of the 5th Interna-
tional Conference on Natural Language Processing,
FinTAL, pages 141?153, Turku, Finland. Springer Ver-
lag, LNAI 4139.
Qifeng Dai, Enhong Chen, and Liu Shi. 2009. An it-
erative approach for joint dependency parsing and se-
mantic role labeling. In Proceedings of the 13th Con-
ference on Computational Natural Language Learning
(CoNLL-2009), June 4-5, Boulder, Colorado, USA.
June 4-5.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Tehcnologies, pages 29?62. Kluwer Academic
Publishers.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press, Cambridge.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the 13th Conference on
Computational Natural Language Learning (CoNLL-
2009), June 4-5, Boulder, Colorado, USA. June 4-5.
Jan Hajic?, Jarmila Panevova?, Zden?ka Ures?ova?, Alevtina
Be?mova?, Veronika Kola?r?ova?- ?Rezn??c?kova??, and Petr
Pajas. 2003. PDT-VALLEX: Creating a Large-
coverage Valency Lexicon for Treebank Annotation.
In J. Nivre and E. Hinrichs, editors, Proceedings of The
Second Workshop on Treebanks and Linguistic Theo-
ries, pages 57?68, Vaxjo, Sweden. Vaxjo University
Press.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan ?Ste?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k ?Zabokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Xavier Llu??s, Stefan Bott, and Llu??s Ma`rquez. 2009.
A second-order joint eisner model for syntactic and
semantic dependency parsing. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA. June 4-5.
17
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
Llu??s Ma`rquez, Luis Villarejo, M. Anto`nia Mart??, and
Mariona Taule?. 2007. SemEval-2007 Task 09: Mul-
tilevel semantic annotation of catalan and spanish.
In Proceedings of the 4th International Workshop on
Semantic Evaluations (SemEval-2007), pages 42?47,
Prague, Czech Republic.
M. Anto`nia Mart??, Mariona Taule?, Llu??s Ma`rquez, and
Manu Bertran. 2007. Anotacio?n semiautoma?tica
con papeles tema?ticos de los corpus CESS-ECE.
Procesamiento del Lenguaje Natural, SEPLN Journal,
38:67?76.
Ryan McDonald, Fernando Pereira, Jan Hajic?, and Kiril
Ribarov. 2005. Non-projective dependency parsing
using spanning tree algortihms. In Proceedings of
NAACL-HLT?05, Vancouver, Canada, pages 523?530.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank Project: An Interim Report. In NAACL/HLT
2004 Workshop Frontiers in Corpus Annotation,
Boston.
Roser Morante, Vincent Van Asch, and Antal van den
Bosch. 2009. A simple generative pipeline approach
to dependency parsing and semantic role labeling. In
Proceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL-2009), Boulder,
Colorado, USA. June 4-5.
Joakim Nivre, Johann Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The conll 2007 shared task on depen-
dency parsing. In Proceedings of the EMNLP-CoNLL
2007 Conference, pages 915?932, Prague, Czech Re-
public.
Sebastian Pado and Mirella Lapata. 2005. Cross-lingual
projection of role-semantic information. In Proceed-
ings of the Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing (HLT/EMNLP-2005), pages 859?
866, Vancouver, BC.
Petr Pajas and Jan ?Ste?pa?nek. 2008. Recent advances in
a feature-rich framework for treebank annotation. In
The 22nd International Conference on Computational
Linguistics - Proceedings of the Conference (COL-
ING?08), pages 673?680, Manchester.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the European ACL Cenference EACL?09,
Athens, Greece.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008), pages 159?177.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Martin ?Cmejrek, Jan Cur???n, Jan Hajic?, Jir??? Havelka,
and Vladislav Kubon?. 2004. Prague Czech-English
Dependency Treebank: Syntactically Anntoated Re-
sources for Machine Translation. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation (LREC-2004), pages 1597?
1600, Lisbon, Portugal.
W. N. Francis and H. Kucera. 1964. Brown Corpus Man-
ual of Information to accompany A Standard Corpus
of Present-Day Edited American English, for use with
Digital Computers. Revised 1971, Revised and Am-
plified 1979, available at www.clarinet/brown.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Technical report,
Lin- guistic Data Consortium.
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Nianwen Xue, Fei Xia, Fu Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Andrea Zielinski and Christian Simon. 2008. Morphisto:
An open-source morphological analyzer for german.
In Proceedings of the Conference on Finite State Meth-
ods in Natural Language Processing.
18
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 517?527,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Coordination Structures in Dependency Treebanks
Martin Popel, David Marec?ek, Jan S?te?pa?nek, Daniel Zeman, Zdene?k Z?abokrtsky?
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics (U?FAL)
Malostranske? na?me?st?? 25, CZ-11800 Praha, Czechia
{popel|marecek|stepanek|zeman|zabokrtsky}@ufal.mff.cuni.cz
Abstract
Paratactic syntactic structures are noto-
riously difficult to represent in depen-
dency formalisms. This has painful con-
sequences such as high frequency of pars-
ing errors related to coordination. In other
words, coordination is a pending prob-
lem in dependency analysis of natural lan-
guages. This paper tries to shed some
light on this area by bringing a system-
atizing view of various formal means de-
veloped for encoding coordination struc-
tures. We introduce a novel taxonomy of
such approaches and apply it to treebanks
across a typologically diverse range of 26
languages. In addition, empirical obser-
vations on convertibility between selected
styles of representations are shown too.
1 Introduction
In the last decade, dependency parsing has grad-
ually been receiving visible attention. One of
the reasons is the increased availability of depen-
dency treebanks, be they results of genuine depen-
dency annotation projects or converted automat-
ically from previously existing phrase-structure
treebanks.
In both cases, a number of decisions have to be
made during the construction or conversion of a
dependency treebank. The traditional notion of
dependency does not always provide unambiguous
solutions, e.g. when it comes to attaching func-
tional words. Worse, dependency representation is
at a loss when it comes to representing paratactic
linguistic phenomena such as coordination, whose
nature is symmetric (two or more conjuncts play
the same role), as opposed to the head-modifier
asymmetry of dependencies.1
1We use the term modifier (or child) for all types of de-
pendent nodes including arguments.
The dominating solution in treebank design is to
introduce artificial rules for the encoding of coor-
dination structures within dependency trees using
the same means that express dependencies, i.e., by
using edges and by labeling of nodes or edges. Ob-
viously, any tree-shaped representation of a coor-
dination structure (CS) must be perceived only as
a ?shortcut? since relations present in coordination
structures form an undirected cycle, as illustrated
already by Tesnie`re (1959). For example, if a noun
is modified by two coordinated adjectives, there
is a (symmetric) coordination relation between the
two conjuncts and two (asymmetric) dependency
relations between the conjuncts and the noun.
However, as there is no obvious linguistic in-
tuition telling us which tree-shaped CS encoding
is better and since the degree of freedom has sev-
eral dimensions, one can find a number of distinct
conventions introduced in particular dependency
treebanks. Variations exist both in topology (tree
shape) and labeling. The main goal of this pa-
per is to give a systematic survey of the solutions
adopted in these treebanks.
Naturally, the interplay of dependency and co-
ordination links in a single tree leads to serious
parsing issues.2 The present study does not try to
decide which coordination style is the best from
the parsing point of view.3 However, we believe
that our survey will substantially facilitate experi-
ments in this direction in the future, at least by ex-
ploring and describing the space of possible can-
didates.
2CSs have been reported to be one of the most frequent
sources of parsing errors (Green and Z?abokrtsky?, 2012; Mc-
Donald and Nivre, 2007; Ku?bler et al, 2009; Collins, 2003).
Their impact on quality of dependency-based machine trans-
lation can also be substantial; as documented on an English-
to-Czech dependency-based translation system (Popel and
Z?abokrtsky?, 2009), 39% of serious translation errors which
are caused by wrong parsing have to do with coordination.
3There might be no such answer, as different CS conven-
tions might serve best for different applications or for differ-
ent parser architectures.
517
The rest of the paper is structured as follows.
Section 2 describes some known problems related
to CS. Section 3 shows possible ?styles? for rep-
resenting CS. Section 4 lists treebanks whose CS
conventions we studied. Section 5 presents empir-
ical observations on CS convertibility. Section 6
concludes the paper.
2 Related work
Let us first recall the basic well-known character-
istics of CSs.
In the simplest case of a CS, a coordinating
conjunction joins two (usually syntactically and
semantically compatible) words or phrases called
conjuncts. Even this simplest case is difficult to
represent within a dependency tree because, in the
words of Lombardo and Lesmo (1998): Depen-
dency paradigms exhibit obvious difficulties with
coordination because, differently from most lin-
guistic structures, it is not possible to characterize
the coordination construct with a general schema
involving a head and some modifiers of it.
Proper formal representation of CSs is further
complicated by the following facts:
? CSs with more than two conjuncts (multi-
conjunct CSs) exist and are frequent.
? Besides ?private? modifiers of individual
conjuncts, there are modifiers shared by
all conjuncts, such as in ?Mary came and
cried?. Shared modifiers may appear along-
side with private modifiers of particular con-
juncts.
? Shared modifiers can be coordinated, too:
?big and cheap apples and oranges?.
? Nested (embedded) coordinations are possi-
ble: ?John and Mary or Sam and Lisa?.
? Punctuation (commas, semicolons, three
dots) is frequently used in CSs, mostly with
multi-conjunct coordinations or juxtaposi-
tions which can be interpreted as CSs with-
out conjunctions (e.g. ?Don?t worry, be
happy!?).
? In many languages, comma or other punctu-
ation mark may play the role of the main co-
ordinating conjunction.
? The coordinating conjunction may be a mul-
tiword expression (?as well as?).
? Deficient CSs with a single conjunct exist.
? Abbreviations like ?etc.? comprise both the
conjunction and the last conjunct.
? Coordination may form very intricate struc-
tures when combined with ellipsis. For ex-
ample, a conjunct can be elided while its ar-
guments remain in the sentence, such as in
the following traditional example: ?I gave
the books to Mary and the records to Sue.?
? The border between paratactic and hypotactic
surface means of expressing coordination re-
lations is fuzzy. Some languages can use en-
clitics instead of conjunctions/prepositions,
e.g. Latin ?Senatus Populusque Romanus?.
Purely hypotactic surface means such as the
preposition in ?John with Mary? occur too.4
? Careful semantic analysis of CSs discloses
additional complications: if a node is mod-
ified by a CS, it might happen that it is
the node itself (and not its modifiers) what
should be semantically considered as a con-
junct. Note the difference between ?red and
white wine? (which is synonymous to ?red
wine and white wine?) and ?red and white
flag of Poland?. Similarly, ?five dogs and
cats? has a different meaning than ?five dogs
and five cats?.
Some of these issues were recognized already
by Tesnie`re (1959). In his solution, conjuncts are
connected by vertical edges directly to the head
and by horizontal edges to the conjunction (which
constitutes a cycle in every CS). Many different
models have been proposed since, out of which the
following are the most frequently used ones:
? MS = Mel?c?uk style used in the Meaning-
Text Theory (MTT): the first conjunct is the
head of the CS, with the second conjunct at-
tached as a dependent of the first one, third
conjunct under the second one, etc. Coor-
dinating conjunction is attached under the
penultimate conjunct, and the last conjunct
is attached under the conjunction (Mel?c?uk,
1988),
? PS = Prague Dependency Treebank (PDT)
style: all conjuncts are attached under the
coordinating conjunction (along with shared
modifiers, which are distinguished by a spe-
cial attribute) (Hajic? et al, 2006),
4As discussed by Stassen (2000), all languages seem to
have some strategy for expressing coordination. Some of
them lack the paratactic surface means (the so called WITH-
languages), but the hypotactic surface means are present al-
most always.
518
? SS = Stanford parser style:5 the first conjunct
is the head and the remaining conjuncts (as
well as conjunctions) are attached under it.
One can find various arguments supporting the
particular choices. MTT possesses a complex
set of linguistic criteria for identifying the gov-
ernor of a relation (see Mazziotta (2011) for an
overview), which lead to MS. MS is preferred in
a rule-based dependency parsing system of Lom-
bardo and Lesmo (1998). PS is advocated by
S?te?pa?nek (2006) who claims that it can represent
shared modifiers using a single additional binary
attribute, while MS would require a more complex
co-indexing attribute. An argumentation of Tratz
and Hovy (2011) follows a similar direction: We
would like to change our [MS] handling of coordi-
nating conjunctions to treat the coordinating con-
junction as the head [PS] because this has fewer
ambiguities than [MS]. . .
We conclude that the influence of the choice of
coordination style is a well-known problem in de-
pendency syntax. Nevertheless, published works
usually focus only on a narrow ad-hoc selection of
few coordination styles, without giving any sys-
tematic perspective.
Choosing a file format presents a different prob-
lem. Despite various efforts to standardize lin-
guistic annotation,6 no commonly accepted stan-
dard exists. The primitive format used for CoNLL
shared tasks is widely used in dependency parsing,
but its weaknesses have already been pointed out
(cf. Stran?a?k and S?te?pa?nek (2010)). Moreover, par-
ticular treebanks vary in their contents even more
than in their format, i.e. each treebank has its own
way of representing prepositions or different gran-
ularity of syntactic labels.
3 Variations in representing
coordination structures
Our analysis of variations in representing coordi-
nation structures is based on observations from a
set of dependency treebanks for 26 languages.7
5We use the already established MS-PS-SS distinction to
facilitate literature overview; as shown in Section 3, the space
of possible coordination styles is much richer.
6For example, TEI (TEI Consortium, 2013), PML (Hana
and S?te?pa?nek, 2012), SynAF (ISO 24615, 2010).
7The primary data sources are the following: Ancient
Greek: Ancient Greek Dependency Treebank (Bamman and
Crane, 2011), Arabic: Prague Arabic Dependency Tree-
bank 1.0 (Smrz? et al, 2008), Basque: Basque Dependency
Treebank (larger version than CoNLL 2007 generously pro-
In accordance with the usual conventions, we as-
sume that each sentence is represented by one de-
pendency tree, in which each node corresponds
to one token (word or punctuation mark). Apart
from that, we deliberately limit ourselves to CS
representations that have shapes of connected sub-
graphs of dependency trees.
We limit our inventory of means of expressing
CSs within dependency trees to (i) tree topology
(presence or absence of a directed edge between
two nodes, Section 3.1), and (ii) node labeling
(additional attributes stored insided nodes, Sec-
tion 3.2).8 Further, we expect that the set of pos-
sible variations can be structured along several di-
mensions, each of which corresponds to a certain
simple characteristic (such as choosing the left-
most conjunct as the CS head, or attaching shared
modifiers below the nearest conjunct). Even if it
does not make sense to create the full Cartesian
product of all dimensions because some values
cannot be combined, it allows to explore the space
of possible CS styles systematically.9
3.1 Topological variations
We distinguish the following dimensions of topo-
logical variations of CS styles (see Figure 1):
Family ? configuration of conjuncts. We di-
vide the topological variations into three main
groups, labeled as Prague (fP), Moscow (fM), and
vided by IXA Group) (Aduriz and others, 2003), Bulgarian:
BulTreeBank (Simov and Osenova, 2005), Czech: Prague
Dependency Treebank 2.0 (Hajic? et al, 2006), Danish: Dan-
ish Dependency Treebank (Kromann et al, 2004), Dutch:
Alpino Treebank (van der Beek and others, 2002), English:
Penn TreeBank 3 (Marcus et al, 1993), Finnish: Turku De-
pendency Treebank (Haverinen et al, 2010), German: Tiger
Treebank (Brants et al, 2002), Greek (modern): Greek De-
pendency Treebank (Prokopidis et al, 2005), Hindi, Ben-
gali and Telugu: Hyderabad Dependency Treebank (Husain
et al, 2010), Hungarian: Szeged Treebank (Csendes et al,
2005), Italian: Italian Syntactic-Semantic Treebank (Mon-
temagni and others, 2003), Latin: Latin Dependency Tree-
bank (Bamman and Crane, 2011), Persian: Persian Depen-
dency Treebank (Rasooli et al, 2011), Portuguese: Floresta
sinta?(c)tica (Afonso et al, 2002), Romanian: Romanian De-
pendency Treebank (Ca?la?cean, 2008), Russian: Syntagrus
(Boguslavsky et al, 2000), Slovene: Slovene Dependency
Treebank (Dz?eroski et al, 2006), Spanish: AnCora (Taule?
et al, 2008), Swedish: Talbanken05 (Nilsson et al, 2005),
Tamil: TamilTB (Ramasamy and Z?abokrtsky?, 2012), Turk-
ish: METU-Sabanci Turkish Treebank (Atalay et al, 2003).
8Edge labeling can be trivially converted to node labeling
in tree structures.
9The full Cartesian product of variants in Figure 1 would
result in topological 216 variants, but only 126 are applicable
(the inapplicable combinations are marked with ??? in Fig-
ure 1). Those 126 topological variants can be further com-
bined with labeling variants defined in Section 3.2.
519
Main family Prague family (code fP)[14 treebanks]
Moscow family (code fM)
[5 treebanks]
Stanford family (code fS)
[6 treebanks]
Choice of head
Head on left (code hL)
[10 treebanks]
dogs
and,  cats rats dooooogsanoos, oocsan
 trn
Head on right (code hR)
[14 treebanks]
Mixed head (code hM) [1 treebank] A mixture of hL and hR
Attachment of shared modifiers
Shared modifier
below the nearest conjunct
(code sN)
[15 treebanks]
Shared modifier below head
(code sH)
[11 treebanks]
dogs
dogsaaan, caaaaataaaaaarocaaaaaoc
on
dogs an, cct do
dogs
an, cct
orao 
o 
dogsaaan, caaaataaaarocaaaon
oc
Attachment of coordinating conjunction
Coordinating conjunction
below previous conjunct (code cP)
[2 treebanks]
?
dogs
and,, acst,,racs dooooogsanoooooooooo,san
 ctn
sr 
Coordinating conjunction
below following conjunct (code cF)
[1 treebank]
?
dogssadn,
g c,
adn,tssrdn,dog dooooogsanoooooooooo,san
 ctn
sr 
Coordinating conjunction
between two conjuncts (code cB)
[8 treebanks]
?
dogs
and,  cats rats dooooogsanoos, oocsan
 trn
Coordinating conjunction as the head (code cH)
is the only applicable style for the Prague family [14 treebanks] ? ?
Placement of punctuation
values pP [7 treebanks], pF [1 treebank] and pB [15 treebanks] are analogous to cP, cF and cB
(but applicable also to the Prague family)
Figure 1: Different coordination styles, variations in tree topology. Example phrase: ?(lazy) dogs, cats
and rats?. Style codes are described in Section 3.1.
Stanford (fS) families.10 This first dimension dis-
tinguishes the configuration of conjuncts: in the
Prague family, all the conjuncts are siblings gov-
erned by one of the conjunctions (or a punctuation
fulfilling its role); in the Moscow family, the con-
juncts form a chain where each node in the chain
depends on the previous (or following) node; in
the Stanford family, the conjuncts are siblings ex-
cept for the first (or last) conjunct, which is the
10Names are chosen purely as a mnemonic device, so that
Prague Dependency Treebank belongs to the Prague family,
Mel?c?uk style belongs to the Moscow family, and Stanford
parser style belongs to the Stanford family.
head.11
Choice of head ? leftmost or rightmost. In
the Prague family, the head can be either the left-
most12 (hL) or the rightmost (hR) conjunction or
punctuation. Similarly, in the Moscow and Stan-
ford families, the head can be either the leftmost
(hL) or the rightmost (hR) conjunct. A third op-
11Note that for CSs with just two conjuncts, fM and fS
may look exactly the same (depending on the attachment of
conjunctions and punctuation as described below).
12For simplicity, we use the terms left and right even if
their meaning is reversed for languages with right-to-left
writing systems such as Arabic or Persian.
520
tion (hM) is to mix hL and hR based on some cri-
terion, e.g. the Persian treebank uses hR for coor-
dination of verbs and hL otherwise. For the exper-
iments in Section 5, we choose the head which is
closer to the parent of the whole CS, with the mo-
tivation to make the edge between CS head and its
parent shorter, which may improve parser training.
Attachment of shared modifiers. Shared mod-
ifiers may appear before the first conjunct or after
the last one. Therefore, it seems reasonable to at-
tach shared modifiers either to the CS head (sH),
or to the nearest (i.e. first or last) conjunct (sN).
Attachment of coordinating conjunctions. In
the Moscow family, conjunctions may be either
part of the chain of conjuncts (cB), or they may be
put outside of the chain and attached to the previ-
ous (cP) or following (cF) conjunct. In the Stan-
ford family, conjunctions may be either attached
to the CS head (and therefore between conjuncts)
(cB), or they may be attached to the previous (cP)
or the following (cF) conjunct. The cB option in
both Moscow and Stanford families, treats con-
junctions in the same way as conjuncts (with re-
spect to topology only). In the Prague family, there
is just one option available (cH) ? one of the con-
junctions is the CS head while the others are at-
tached to it.
Attachment of punctuation. Punctuation to-
kens separating conjuncts (commas, semicolons
etc.) could be treated the same way as conjunc-
tions. However, in most treebanks it is treated
differently, so we consider it as well. The val-
ues pP, pF and pB are analogous to cP, cF and
cB except that punctuation may be also attached
to the conjunction in case of pP and pF (other-
wise, a comma before the conjunction would be
non-projectively attached to the member follow-
ing the conjunction).
The three established styles mentioned in Sec-
tion 2 can be defined in terms of the newly intro-
duced abbreviations: PS = fPhRsHcHpB, MS =
fMhLsNcBp?, and SS = fShLsNcBp?.13
3.2 Labeling variations
Most state-of-the-art dependency parsers can pro-
duce labeled edges. However, the parsers produce
only one label per edge. To fully capture CSs,
we need more than one label, because there are
several aspects involved (see the initial assump-
13The question marks indicate that the original Mel?c?uk
and Stanford parser styles ignore punctuation.
tions in Section 3): We need to identify the co-
ordinating conjunction (its POS tag might not be
enough), conjuncts, shared modifiers, and punctu-
ation that separates conjuncts. Besides that, there
should be a label classifying the dependency rela-
tion between the CS and its parent.
Some of the information can be retrieved from
the topology of the tree and the ?main label? of
each node, but not everything. The additional in-
formation can be attached to the main label, but
such approach obscures the logical structure.
In the Prague family, there are two possible
ways to label a conjunction and conjuncts:
Code dU (?dependency labeled at the upper
level of the CS?). The dependency relation of the
whole CS to its parent is represented by the label
of the conjunction, while the conjuncts are marked
with a special label for conjuncts (e.g. ccof in the
Hyderabad Dependency Treebank).
Code dL (?lower level?). The CS is represented
by a coordinating conjunction (or punctuation if
there is no conjunction) with a special label (e.g.
Coord in PDT). Subsequently, each conjunct has
its own label that reflects the dependency relation
towards the parent of the whole CS, therefore, con-
juncts of the same CS can have different labels,
e.g. ?Who[SUBJ] and why[ADV] did it??
Most Prague family treebanks use sH, i.e.
shared modifiers are attached to the head (coor-
dinating conjunction). Each child of the head has
to belong to one of three sets: conjuncts, shared
modifiers, and punctuation or additional conjunc-
tions. In PDT, conjuncts, punctuation and addi-
tional conjunctions are recognized by specific la-
bels. Any other children of the head are shared
modifiers.
In the Stanford and Moscow families, one of
the conjuncts is the head. In practice, it is never la-
beled as a conjunct explicitly, because the fact that
it is a conjunct can be deduced from the presence
of conjuncts among its children. Usually, the other
conjuncts are labeled as conjuncts; conjunctions
and punctuation also have a special label. This
type of labeling corresponds to the dU type.
Alternatively (as found in the Turkish treebank,
dL), all conjuncts in the Moscow chain have their
own dependency labels and the fact that they are
conjuncts follows from the COORDINATION la-
bels of the conjunction and punctuation nodes be-
tween them.
To represent shared modifiers in the Stan-
521
ford and Moscow families, an additional label
is needed again to distinguish between private
and shared modifiers since they cannot be distin-
guished topologically. Moreover, if nested CSs
are allowed, a binary label is not sufficient (i.e.
?shared? versus ?private?) because it also has to
indicate which conjuncts the shared modifier be-
longs to.14
We use the following binary flag codes for cap-
turing which CS participants are distinguished in
the annotation: m01 = shared modifiers anno-
tated; m10 = conjuncts annotated; m11 = both
annotated; m00 = neither annotated.
4 Coordination Structures in Treebanks
In this section, we identify the CS styles defined
in the previous section as used in the primary tree-
bank data sources; statistical observations (such
as the amount of annotated shared modifiers) pre-
sented here, as well as experiments on CS-style
convertibility presented in Section 5.2, are based
on the normalized shapes of the treebanks as con-
tained in the HamleDT 1.0 treebank collection
(Zeman et al, 2012).15
Some of the treebanks were downloaded indi-
vidually from the web, but most of them came
from previously published collections for depen-
dency parsing campaigns: six languages from
CoNLL-2006 (Buchholz and Marsi, 2006), seven
languages from CoNLL-2007 (Nivre et al, 2007),
two languages from CoNLL-2009 (Hajic? and oth-
ers, 2009), three languages from ICON-2010 (Hu-
sain et al, 2010). Obviously, there is a certain
risk that the CS-related information contained in
the source treebanks was slightly biased by the
properties of the CoNLL format upon conversion.
In addition, many of the treebanks were natively
dependency-based (cf. the 2nd column of Table 1),
but some were originally based on constituents
and thus specific converters to the CoNLL for-
mat had to be created (for instance, the Span-
ish phrase-structure trees were converted to de-
pendencies using a procedure described by Civit
et al (2006); similarly, treebank-specific convert-
ers have been used for other languages). Again,
14This is not needed in Prague family where shared modi-
fiers are attached to the conjunction provided that each shared
modifier is shared by conjuncts that form a full subtree to-
gether with their coordinating conjunctions; no exceptions
were found during the annotation process of the PDT.
15A subset of the treebanks whose license
terms permit redistribution is available directly at
http://ufal.mff.cuni.cz/hamledt/.
Danish Romanian
dogsa
n,  anctttr  attt,

ddog
san,n           cntnrnsn     c,n
Hungarian
dogsadnnnn,nnnn ctrdadnnnnrncgdasd
Figure 2: Annotation styles of a few treebanks do
not fit well into the multidimensional space de-
fined in Section 3.1.
there is some risk that the CS-related information
contained in treebanks resulting from such conver-
sions is slightly different from what was intended
in the very primary annotation.
There are several other languages (e.g. Esto-
nian or Chinese) which are not included in our
study, despite of the fact that constituency tree-
banks do exist for them. The reason is that the
choice of their CS style would be biased, because
no independent converters exist ? we would have
to convert them to dependencies ourselves. We
also know about several more dependency tree-
banks that we have not processed yet.
Table 1 shows 26 languages whose treebanks
we have studied from the viewpoint of their CS
styles. It gives the basic quantitative properties of
the treebanks, their CS style in terms of the tax-
onomy introduced in Section 3, as well as statis-
tics related to CSs: the average number of CSs per
100 tokens, the average number of conjuncts per
one CS, the average number of shared modifiers
per one CS,16 and the percentage of nested CSs
among all CSs. The reader can return to Figure
1 to see the basic statistics on the ?popularity? of
individual design decisions among the developers
of dependency treebanks or constituency treebank
converters.
CS styles of most treebanks are easily classifi-
able using the codes introduced in Section 3, plus
a few additional codes:
? p0 = punctuation was removed from the tree-
bank.
16All non-Prague family treebanks are marked sN and
m00 or m10, (i.e. shared modifiers not marked in the origi-
nal annotation, but attached to the head conjunct) because we
found no counterexamples (modifiers attached to a conjunct,
but not the nearest one). The HamleDT normalization proce-
dure contains a few heuristics to detect shared modifiers, but
it cannot recover the missing distinction reliably, so the num-
bers in the ?SMs/CJ? column are mostly underestimated.
522
Language Orig. Data Sents. Tokens Original CS CSs / CJs / SMs / Nested RT
type set style code 100 tok. CS CS CS[%] UAS
Ancient
Greek dep prim. 31 316 461 782 fP hR sH cH pB dL m11 6.54 2.17 0.16 10.3 97.86
Arabic dep C07 3 043 116 793 fP hL sH cH pB dL m00 3.76 2.42 0.13 10.6 96.69
Basque dep prim. 11 225 151 593 fP hR sN cH pP dU m00 3.37 2.09 0.03 5.1 99.32
Bengali dep I10 1 129 7 252 fP hR sH cH pP dU m11 4.87 1.71 0.05 24.1 99.97
Bulgarian phr C06 13 221 196 151 fS hL sN cB pB dU m10 2.99 2.19 0.00 0.0 99.74
Czech dep C07 25 650 437 020 fP hR sH cH pB dL m11 4.09 2.16 0.20 14.6 99.42
Danish dep C06 5 512 100 238 fS* hL sN cP pB dU m10 3.68 1.93 0.13 7.5 99.76
Dutch phr C06 13 735 200 654 fP hR sN cH pP dU m10 2.06 2.17 0.05 3.3 99.47
English phr C07 40 613 991 535 fP hR sH cH pB dU m10 2.07 2.33 0.05 6.3 99.84
Finnish dep prim. 4 307 58 576 fS hL sN cB pB dU m10 4.06 2.41 0.00 6.4 99.70
German phr C09 38 020 680 710 fM hL sN cP pP dU m10 2.79 2.09 0.01 0.0 99.73
Greek dep C07 2 902 70 223 fP hR sH cH pB dL m11 3.25 2.48 0.18 7.2 99.43
Hindi dep I10 3 515 77 068 fP hR sH cH pP dU m11 2.45 1.97 0.04 10.3 98.35
Hungarian phr C07 6 424 139 143 fT hX sN cX pX dL m00 2.37 1.90 0.01 2.2 99.84
Italian dep C07 3 359 76 295 fS hL sN cB pB dU m10 3.32 2.02 0.03 3.8 99.51
Latin dep prim. 3 473 53 143 fP hR sH cH pB dL m11 6.74 2.24 0.41 12.3 97.45
Persian dep prim. 12 455 189 572 fM*hM sN cB pP dU m00 4.18 2.10 0.18 3.7 99.82
Portuguese phr C06 9 359 212 545 fS hL sN cB pB dU m10 2.51 1.95 0.26 11.1 99.16
Romanian dep prim. 4 042 36 150 fP* hR sN cH p0 dU m10 1.80 2.00 0.00 0.0 100.00
Russian dep prim. 34 895 497 465 fM hL sN cB p0 dU m10 4.02 2.02 0.07 3.9 99.86
Slovene dep C06 1 936 35 140 fP hR sH cH pB dL m00 4.31 2.49 0.00 10.8 98.87
Spanish phr C09 15 984 477 810 fS hL sN cB pB dU m10 2.79 1.98 0.14 12.7 99.24
Swedish phr C06 11 431 197 123 fM hL sN cF pF dU m10 3.94 2.19 0.13 0.7 99.66
Tamil dep prim. 600 9 581 fP hR sH cH pB dL m11 1.66 2.46 0.22 3.8 99.67
Telugu dep I10 1 450 5 722 fP hR sH cH pP dU m11 3.48 1.59 0.06 5.0 100.00
Turkish dep C07 5 935 69 695 fM hR sN cB pB dL m10 3.81 2.04 0.00 34.3 99.23
Table 1: Overview of analyzed treebanks. prim. = primary source; C06?C09 = CoNLL 2006?2009;
I10 = ICON 2010; SM = shared modifier; CJ = conjunct; Nested CS = portion of CSs participating in
nested CSs (both as the inner and outer CS); RT UAS = unlabeled attachment score of the roundtrip
experiment described in Section 5. Style codes are defined in Sections 3 and 4.
? fM* = Persian treebank uses a mix of fM and
fS: fS for coordination of verbs and fM oth-
erwise.
Figure 2 shows three other anomalies:
? fS* = Danish treebank employs a mixture of
fS and fM, where the last conjunct is attached
indirectly via the conjunction.
? fP* = Romanian treebank omits punctuation
tokens and multi-conjunct coordinations get
split.
? fT = Hungarian Szeged treebank uses
?Tesnie`re family? ? disconnected graphs for
CSs where conjuncts (and conjunction and
punctuation) are attached directly to the par-
ent of CS, and so the other style dimensions
are not applicable (hX, cX, pX).
5 Empirical Observations on
Convertibility of Coordination Styles
The various styles cannot represent the CS-related
information to the same extent. For example,
it is not possible to represent nested CSs in the
Moscow and Stanford families without signifi-
cantly changing the number of possible labels.17
The dL style (which is most easily applicable to
the Prague family) can represent coordination of
different dependency relations. This is again not
possible in the other styles without adding e.g. a
special ?prefix? denoting the relations.
We can see that the Prague family has a greater
expressive power than the other two families: it
can represent complex CSs using just one addi-
tional binary label, distinguishing between shared
modifiers and conjuncts. A similar additional label
is needed in the other styles to distinguish between
shared and private modifiers.
Because of the different expressive power, con-
verting a CS from one style to another may
lead to a loss of information. For example, as
17Mel?c?uk uses ?grouping? to nest CSs ? cf. related so-
lutions involving coindexing or bubble trees (Kahane, 1997).
However, these approaches were not used in any of the re-
searched treebanks. To combine grouping with shared modi-
fiers, each group in a tree should have a different identifier.
523
there is no way of representing shared modifiers
in the Moscow family without an additional at-
tribute, converting a CS with shared modifiers
from Prague to Moscow family makes the modi-
fiers private. When converting back, one can use
certain heuristics to handle the most obvious cases,
but sometimes the modifiers will stay private (very
often, the nature of a modifier depends on context
or is debatable even for humans, e.g. ?Young boys
and girls?).
5.1 Transformation algorithm
We developed an algorithm to transform one CS
style to another. Two subtasks must be solved by
the algorithm: identification of individual CSs and
their participants, and transforming of the individ-
ual CSs.
Obviously, the individual CSs cannot be trans-
formed independently because of coordination
nesting. For instance, when transforming a nested
coordination from the Prague style to the Moscow
style (e.g. to fMhL), the leftmost conjunct in the
inner (lower) coordination must climb up to be-
come the head of the inner CS, but then it must
climb up once again to become the head of the
outer (upper) CS too. This shows that inner CSs
must be transformed first.
We tackle this problem by a depth-first recur-
sion. When going down the tree, we only recog-
nize all the participants of the CSs, classify them
and gather them in a separate data structure (one
for each visited CS). The following four types
of CS participants are distinguished: coordinat-
ing conjunctions, conjuncts, shared modifiers, and
punctuations that separate conjuncts.18 No change
of the tree is performed during these descent steps.
When returning back from the recursion (i.e.,
when climbing from a node back up to its par-
ent), we test whether the abandoned node is the
topmost node of some CS. If so, then this CS is
transformed, which means that its participants are
rehanged and relabelled according the the target
CS style.
This procedure naturally guarantees that the in-
18Conjuncts are explicitly marked in most styles. Coordi-
nating conjunctions can be usually identified with the help of
dependency labels and POS tags. Punctuation separating con-
juncts can be detected with high accuracy using simple rules.
If shared modifiers are not annotated (code m00 or m10),
one can imagine rule-based heuristics or special classifiers
trained to distinguish shared modifiers. For the experiments
in this section, we use the HamleDT gold annotation attribute
is shared modifier.
ner CSs are transformed first and that all CSs are
transformed when the recursions returns to the
root.
5.2 Roundtrip experiment
The number of possible conversion directions ob-
viously grows quadratically with the number of
styles. So far, we limited ourselves only to con-
versions from/to the style of the HamleDT tree-
bank collection, which contains all the treebanks
under our study already converted into a com-
mon scheme. The common scheme is based
on the conventions of PDT, whose CS style is
fPhRsHcHpB.19
We selected nine styles (3 families times 3 head
choices) and transformed all the HamleDT scheme
treebanks to these nine styles and back, which we
call a roundtrip. Resulting averaged unlabeled at-
tachment scores (UAS, evaluated against the Ham-
leDT scheme) in the last column of Table 1 indi-
cate that the percentage of transformation errors
(i.e. tokens attached to a different parent after the
roundtrip) is lower than 1% for 20 out of the 26
languages.20 A manual inspection revealed two
main error sources. First, as noted above, the Stan-
ford and Moscow families have lower expressive
power than the Prague family, so naturally, the in-
verse transformation was ambiguous and the trans-
formation heuristics were not capable of identify-
ing the correct variant every time. Second, we also
encountered inconsistencies in the original tree-
banks (which we were not trying to fix in Ham-
leDT for now).
6 Conclusions and Future Work
We described a (theoretically very large) space of
possible representations of CSs within the depen-
dency framework. We pointed out a range of de-
tails that make CSs a really complex phenomenon;
anyone dealing with CSs in treebanking should
take these observations into account.
We proposed a taxonomy of those approaches
19As documented in Zeman et al (2012), the normalization
procedures used in HamleDT embrace many other phenom-
ena as well (not only those related to coordination), and in-
volve both structural transformation and dependency relation
relabeling.
20Table 1 shows that Latin and Ancient Greek treebanks
have on average more than 6 CSs per 100 tokens, more than
2 conjuncts per CS, and Latin has also the highest number of
shared modifiers per CS. Therefore the percentage of nodes
affected by the roundtrip is the highest for these languages
and the lower roundtrip UAS is not surprising.
524
that have been argued for in literature or employed
in real treebanks.
We studied 26 existing treebanks of different
languages. For each value of each dimension in
Figure 1, we found at least one treebank where the
value is used; even so, several treebanks take their
own unique path that cannot be clearly classified
under the taxonomy (the taxonomy could indeed
be extended, for the price of being less clearly ar-
ranged).
We discussed the convertibility between the var-
ious styles and implemented a universal tool that
transforms between any two styles of the taxon-
omy. The tool achieves a roundtrip accuracy close
to 100%. This is important because it opens the
door to easily switching coordination styles for
parsing experiments, phrase-to-dependency con-
version etc.
While the focus of this paper is to explore and
describe the expressive power of various annota-
tion styles, we did not address the learnability of
the styles by parsers. That will be a complemen-
tary point of view, and thus a natural direction of
future work for us.
Acknowledgments
We thank the providers of the primary data re-
sources. The work on this project was sup-
ported by the Czech Science Foundation grants
no. P406/11/1499 and P406/2010/0875, and by
research resources of the Charles University in
Prague (PRVOUK). This work has been using lan-
guage resources developed and/or stored and/or
distributed by the LINDAT-Clarin project of the
Ministry of Education of the Czech Republic
(project LM2010013). Further, we would like to
thank Jan Hajic?, Ondr?ej Dus?ek and four anony-
mous reviewers for many useful comments on the
manuscript of this paper.
References
Itzair Aduriz et al 2003. Construction of a Basque de-
pendency treebank. In Proceedings of the 2nd Work-
shop on Treebanks and Linguistic Theories.
Susana Afonso, Eckhard Bick, Renato Haber, and Di-
ana Santos. 2002. ?Floresta sinta?(c)tica?: a tree-
bank for Portuguese. In LREC, pages 1968?1703.
Nart B. Atalay, Kemal Oflazer, and Bilge Say. 2003.
The annotation process in the Turkish treebank. In
Proceedings of the 4th Intern. Workshop on Linguis-
tically Interpreteted Corpora (LINC).
David Bamman and Gregory Crane. 2011. The An-
cient Greek and Latin dependency treebanks. In
Language Technology for Cultural Heritage, Theory
and Applications of Natural Language Processing,
pages 79?98. Springer Berlin Heidelberg.
Igor Boguslavsky, Svetlana Grigorieva, Nikolai Grig-
oriev, Leonid Kreidlin, and Nadezhda Frid. 2000.
Dependency treebank for Russian: Concept, tools,
types of information. In Proceedings of the 18th
conference on Computational linguistics-Volume 2,
pages 987?991. Association for Computational Lin-
guistics Morristown, NJ, USA.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories, Sozopol.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164.
Montserrat Civit, Maria Anto`nia Mart??, and Nu?ria Buf??.
2006. Cat3LB and Cast3LB: From constituents to
dependencies. In FinTAL, volume 4139 of Lec-
ture Notes in Computer Science, pages 141?152.
Springer.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational lin-
guistics, 29(4):589?637.
Do?ra Csendes, Ja?nos Csirik, Tibor Gyimo?thy, and
Andra?s Kocsor. 2005. The Szeged treebank. In
TSD, volume 3658 of Lecture Notes in Computer
Science, pages 123?131. Springer.
Mihaela Ca?la?cean. 2008. Data-driven dependency
parsing for Romanian. Master?s thesis, Uppsala
University, August.
Sas?o Dz?eroski, Tomaz? Erjavec, Nina Ledinek, Petr Pa-
jas, Zdene?k Z?abokrtsky?, and Andreja Z?ele. 2006.
Towards a Slovene dependency treebank. In LREC
2006, pages 1388?1391, Genova, Italy. European
Language Resources Association (ELRA).
Nathan Green and Zdene?k Z?abokrtsky?. 2012. Hy-
brid combination of constituency and dependency
trees into an ensemble dependency parser. In Pro-
ceedings of the Workshop on Innovative Hybrid Ap-
proaches to the Processing of Textual Data, pages
19?26, Avignon, France. Association for Computa-
tional Linguistics.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka,
Marie Mikulova?, Zdene?k Z?abokrtsky?, and Magda
S?evc???kova?-Raz??mova?. 2006. Prague Dependency
Treebank 2.0. CD-ROM, Linguistic Data Consor-
tium, LDC Catalog No.: LDC2006T01, Philadel-
phia.
525
Jan Hajic? et al 2009. The CoNLL-2009 shared
task: Syntactic and semantic dependencies in mul-
tiple languages. In Proceedings of the 13th Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), June 4-5, Boulder, Colorado, USA.
Jirka Hana and Jan S?te?pa?nek. 2012. Prague
markup language framework. In Proceedings of the
Sixth Linguistic Annotation Workshop, pages 12?
21, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics, Association for Computational
Linguistics.
Katri Haverinen, Timo Viljanen, Veronika Laippala,
Samuel Kohonen, Filip Ginter, and Tapio Salakoski.
2010. Treebanking Finnish. In Proceedings of
the Ninth International Workshop on Treebanks and
Linguistic Theories (TLT9), pages 79?90.
Samar Husain, Prashanth Mannem, Bharat Ambati,
and Phani Gadde. 2010. The ICON-2010 tools
contest on Indian language dependency parsing. In
Proceedings of ICON-2010 Tools Contest on Indian
Language Dependency Parsing, Kharagpur, India.
ISO 24615. 2010. Language resource management ?
Syntactic annotation framework (SynAF).
Sylvain Kahane. 1997. Bubble trees and syntactic
representations. In Proceedings of the 5th Meeting
of the Mathematics of the Language, DFKI, Saar-
brucken.
Matthias T. Kromann, Line Mikkelsen, and Stine Kern
Lynge. 2004. Danish dependency treebank.
Sandra Ku?bler, Erhard Hinrichs, Wolfgang Maier, and
Eva Klett. 2009. Parsing coordinations. In Pro-
ceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 406?414,
Athens, Greece, March. Association for Computa-
tional Linguistics.
Vincenzo Lombardo and Leonardo Lesmo. 1998. Unit
coordination and gapping in dependency theory. In
Processing of Dependency-Based Grammars; pro-
ceedings of the workshop. COLING-ACL, Montreal.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Nicolar Mazziotta. 2011. Coordination of verbal de-
pendents in Old French: Coordination as a specified
juxtaposition or apposition. In Proceedings of In-
ternational Conference on Dependency Linguistics
(DepLing 2011).
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency pars-
ing models. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 122?131.
Igor A. Mel?c?uk. 1988. Dependency Syntax: Theory
and Practice. State University of New York Press.
Simonetta Montemagni et al 2003. Building the Ital-
ian syntactic-semantic treebank. In Building and us-
ing Parsed Corpora, Language and Speech series,
pages 189?210, Dordrecht. Kluwer.
Jens Nilsson, Johan Hall, and Joakim Nivre. 2005.
MAMBA meets TIGER: Reconstructing a Swedish
treebank from antiquity. In Proceedings of the
NODALIDA Special Session on Treebanks.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
2007 Shared Task. EMNLP-CoNLL, June.
Martin Popel and Zdene?k Z?abokrtsky?. 2009.
Improving English-Czech Tectogrammatical MT.
The Prague Bulletin of Mathematical Linguistics,
(92):1?20.
Prokopis Prokopidis, Elina Desipri, Maria Koutsom-
bogera, Harris Papageorgiou, and Stelios Piperidis.
2005. Theoretical and practical issues in the con-
struction of a Greek dependency treebank. In Pro-
ceedings of the 4th Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 149?160.
Loganathan Ramasamy and Zdene?k Z?abokrtsky?. 2012.
Prague dependency style treebank for Tamil. In
Proceedings of LREC 2012, pages 23?25, I?stanbul,
Turkey. European Language Resources Association.
Mohammad Sadegh Rasooli, Amirsaeid Moloodi,
Manouchehr Kouhestani, and Behrouz Minaei-
Bidgoli. 2011. A syntactic valency lexicon for
Persian verbs: The first steps towards Persian de-
pendency treebank. In 5th Language & Technology
Conference (LTC): Human Language Technologies
as a Challenge for Computer Science and Linguis-
tics, pages 227?231, Poznan?, Poland.
Kiril Simov and Petya Osenova. 2005. Extending
the annotation of BulTreeBank: Phase 2. In The
Fourth Workshop on Treebanks and Linguistic Theo-
ries (TLT 2005), pages 173?184, Barcelona, Decem-
ber.
Otakar Smrz?, Viktor Bielicky?, Iveta Kour?ilova?, Jakub
Kra?c?mar, Jan Hajic?, and Petr Zema?nek. 2008.
Prague Arabic dependency treebank: A word on the
million words. In Proceedings of the Workshop on
Arabic and Local Languages (LREC) 2008, pages
16?23, Marrakech, Morocco. European Language
Resources Association.
Leon Stassen. 2000. And-languages and with-
languages. Linguistic Typology, 4(1):1?54.
Jan S?te?pa?nek. 2006. Capturing a Sentence Struc-
ture by a Dependency Relation in an Annotated Syn-
tactical Corpus (Tools Guaranteeing Data Consis-
tence) (in Czech). Ph.D. thesis, Charles Univer-
526
sity in Prague, Faculty of Mathematics and Physics,
Prague, Czech Republic.
Pavel Stran?a?k and Jan S?te?pa?nek. 2010. Represent-
ing layered and structured data in the CoNLL-ST
format. In Alex Fang, Nancy Ide, and Jonathan
Webster, editors, Proceedings of the Second Inter-
national Conference on Global Interoperability for
Language Resources, pages 143?152, Hong Kong,
China. City University of Hong Kong, City Univer-
sity of Hong Kong.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel annotated cor-
pora for Catalan and Spanish. In LREC. European
Language Resources Association.
TEI Consortium. 2013. TEI P5: Guidelines for Elec-
tronic Text Encoding and Interchange.
Lucien Tesnie`re. 1959. Ele?ments de syntaxe struc-
turale. Paris.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of EMNLP, pages 1257?1268, Edin-
burgh, Scotland, UK, July. Association for Compu-
tational Linguistics.
Leonoor van der Beek et al 2002. Chapter 5. The
Alpino dependency treebank. In Algorithms for Lin-
guistic Processing NWO PIONIER Progress Report,
Groningen, The Netherlands.
Daniel Zeman, David Marec?ek, Martin Popel,
Loganathan Ramasamy, Jan S?te?pa?nek, Zdene?k
Z?abokrtsky?, and Jan Hajic?. 2012. HamleDT: To
parse or not to parse? In Proceedings of LREC 2012,
pages 2735?2741, I?stanbul, Turkey. European Lan-
guage Resources Association.
527
