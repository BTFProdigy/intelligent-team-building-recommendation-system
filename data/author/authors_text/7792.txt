Experiments on Sentence Boundary Detection 
Mark  Stevenson  and  Rober t  Ga izauskas  
Depar tment  of  Computer  Science, 
Un ivers i ty  of  Sheff ield 
Regent  Cour t ,  211 Por tobe l lo  St reet ,  
Sheff ield 
S1 4DP Un i ted  K ingdom 
{marks, robertg}@dcs, shef. ac.uk 
Abst ract  
This paper explores the problem of identifying sen- 
tence boundaries in the transcriptions produced by 
automatic speech recognition systems. An experi- 
ment which determines the level of human perform- 
ance for this task is described as well as a memory- 
based computational pproach to the problem. 
1 The  Prob lem 
This paper addresses the problem of identifying sen- 
tence boundaries in the transcriptions produced by 
automatic speech recognition (ASR) systems. This 
is unusual in the field of text processing which has 
generally dealt with well-punctuated text: some of 
the most commonly used texts in NLP are machine 
readable versions of highly edited documents uch 
as newspaper articles or novels. However, there are 
many types of text which are not so-edited and the 
example which we concentrate on in this paper is 
the output from ASR systems. These differ from 
the sort of texts normally used in NLP in a number 
of ways; the text is generally in single case (usually 
upper), unpunctuated and may contain transcrip- 
tion errors. 1 Figure 1 compares a short text in the 
format which would be produced by an ASR system 
with a fully punctuated version which includes case 
information. For the remainder of this paper error- 
free texts such as newspaper articles or novels shall 
be referred to as "standard text" and the output 
from a speech recognition system as "ASR text". 
There are many possible situations in which an 
NLP system may be required to process ASR text. 
The most obvious examples are NLP systems which 
take speech input (eg. Moore et al (1997)). Also, 
dictation software programs do not punctuate or 
capitalise their output but, if this information could 
be added to ASR text, the results would be far more 
usable. One of the most important pieces of inform- 
1 Speech recognition systems are often evaluated in terms 
of word error ate (WER), the percentage oftokens which are 
wrongly transcribed. For large vocabulary tasks and speaker- 
independent systems, WER varies between 7% and 50%, de- 
pending upon the quality of the recording being recognised. 
See, e.g., Cole (1996). 
G00D EVENING GIANNI VERSACE ONE OF THE 
WORLDS LEADING FASHION DESIGNERS HAS 
BEEN MURDERED IN MIAMI POLICE SAY IT WAS 
A PLANNED KILLING CARRIED OUT LIKE AN 
EXECUTION SCHOOLS INSPECTIONS ARE GOING 
TO BE TOUGHER TO FORCE BAD TEACHERS OUT 
AND THE FOUR THOUSAND COUPLES WH0 SHARED 
THE QUEENS GOLDEN DAY 
Good evening. Gi~nni Versace, one of 
the world's leading fashion designers, 
has been murdered in Miami. Police say 
it was a planned killing carried out 
like an execution. Schools inspections 
are going to be tougher to force bad 
teachers out. And the four thousand 
couples who shared the Queen's golden 
day. 
Figure 1: Example text shown in standard and ASR 
format 
ation which is not available in ASR output is sen- 
tence boundary information. However, knowledge of 
sentence boundaries i required by many NLP tech- 
nologies. Part of speech taggers typically require 
input in the format of a single sentence per line (for 
example Brill's tagger (Brill, 1992)) and parsers gen- 
erally aim to produce a tree spanning each sentence. 
Only the most trivial linguistic analysis can be car- 
ried out on text which is not split into sentences. 
It is worth mentioning that not all transcribed 
speech can be sensibly divided into sentences. It has 
been argued by Gotoh and Renals (2000) that the 
main unit in spoken language is the phrase rather 
than the sentence. However, there are situations 
in which it is appropriate to consider spoken lan- 
guage to be made up from sentences. One example 
is broadcast news: radio and television news pro- 
grams. The DARPA HUB4 broadcast news evalu- 
ation (Chinchor et al, 1998) focussed on informa- 
tion extraction from ASR text from news programs. 
Although news programs are scripted there are of- 
ten deviations from the script and they cannot be 
relied upon as accurate transcriptions of the news 
84 
program. The spoken portion of the British National 
Corpus (Burnard, 1995) contains 10 million words 
and was manually marked with sentence boundar- 
ies. A technology which identifies entence boundar- 
ies could be used to speed up the process of creating 
any future corpus of this type. 
It is important o distinguish the problem just 
mentioned and another problem sometimes called 
"sentence splitting". This problem aims to identify 
sentence boundaries in standard text but since this 
includes punctuation the problem is effectively re- 
duced to deciding which of the symbols which poten- 
tially denote sentence boundaries ( . ,  !,  ?) actually 
do. This problem is not trivial since these punc- 
tuation symbols do not always occur at the end of 
sentences. For example in the sentence "Dr. Jones 
l ec tures  at  U.C.L.A." only the final full stop de- 
notes the end of a sentence. For the sake of clarity 
we shall refer to the process of discovering sentence 
boundaries in standard punctuated text as "punc- 
tuation disambiguation" and that of finding them 
in unpunctuated ASR text as "sentence boundary 
detection". 
2 Related Work 
Despite the potential application of technology 
which can carry out the sentence boundary detec- 
tion task, there has been little research into the 
area. However, there has been work in the re- 
lated field of punctuation disambiguation. Palmer 
and Hearst (1994) applied a neural network to the 
problem. They used the Brown Corpus for training 
and evaluation, noting that 90% of the full stops in 
this text indicate sentence boundaries. They used 
the part of speech information for the words sur- 
rounding a punctuation symbol as the input to a 
feed-forward neural network. But, as we mentioned, 
most part of speech taggers require sentence bound- 
aries to be pre-determined and this potential cir- 
cularity is avoided by using the prior probabilities 
for each token, determined from the Brown corpus 
markup. The network was trained on 573 potential 
sentence nding marks from the Wall Street Journal 
and tested on 27,294 items from the same corpus. 
98.5% of punctuation marks were correctly disam- 
biguated. 
Reynar and Ratnaparkhi (1997) applied a max- 
imum entropy approach to the problem. Their 
system considered only the first word to the left 
and right of any potential sentence boundary and 
claimed that examining wider context did not help. 
For both these words the prefix, suffix, presence of 
particular characters in the prefix or suffix, whether 
the candidate is honorific (Mr., Dr. etc.) and 
whether the candidate is a corporate designator (eg. 
Corp.) are features that are considered. This sys- 
tem was tested on the same corpus as Palmer and 
Hearst's system and correctly identified 98.8% of 
sentence boundaries. Mikheev (1998) optimised this 
approach and evaluated it on the same test corpus. 
An accuracy of 99.2477% was reported, to our know- 
ledge this is the highest quoted result for this test 
set. 
These three systems achieve very high results 
for the punctuation disambiguation task. It would 
seem, then, that this problem has largely been 
solved. However, it is not clear that these techniques 
will be as successful for ASR text. We now go on to 
describe a system which attempts a task similar to 
sentence boundary detection of ASR text. 
Beeferman et al (1998) produced a system, "CY- 
BERPUNC", which added intra-sentence punctu- 
ation (i.e. commas) to the output of an ASR system. 
They mention that the comma is the most frequently 
used punctuation symbol and its correct insertion 
can make a text far more legible. CYBERPUNC 
operated by augmenting a standard trigram speech 
recognition model with information about commas; 
it accesses only lexical information. CYBERPUNC 
was tested by separating the trigram model from 
the ASR system and applying it to 2,317 sentences 
from the Wall Street Journal. The system achieved 
a precision of 75.6% and recall of 65.6% compared 
against he original punctuation i  the text. 2 A fur- 
ther qualitative valuation was carried out using 100 
randomly-drawn output sentences from the system 
and 100 from the Wall Street Journal. Six human 
judges blindly marked each sentence as either ac- 
ceptable or unacceptable. It was found that the 
Penn TreeBank sentences were 86% correct and the 
system output 66% correct. It is interesting that the 
human judges do not agree completely on the ac- 
ceptability of many sentences from the Wall Street 
Journal. 
In the next section we go on to describe exper- 
iments which quantify the level of agreement that 
can be expected when humans carry out sentence 
boundary detection. Section 4 goes on to describe a
computational pproach to the problem. 
3 Determining Human Abi l i ty 
Beeferman et. al.'s experiments demonstrated that 
humans do not always agree on the acceptability of 
comma insertion and therefore it may be useful to 
determine how often they agree on the placing of 
sentence boundaries. To do this we carried out ex- 
periments using transcriptions ofnews programmes, 
specifically the transcriptions of two editions of the 
~Precision and recall are complementary  evaluation met- 
rics commonly  used in Information Retrieval (van Rijsbergen, 
1979). In this case precision is the percentage of commas pro- 
posed by the system which are correct while recall is the per- 
centage of the commas occurring in the test corpus which the 
system identified. 
R~ 
BBC television program "The Nine O'Clock News" .3 
The transcriptions consisted of punctuated mixed 
case text with sentences boundaries marked using a 
reserved character ("; "). These texts were produced 
by trained transcribers listening to the original pro- 
gram broadcast. 
Six experimental subjects were recruited. All sub- 
jects were educated to at least Bachelor's degree 
level and are either native English speakers or flu- 
ent second language speakers. Each subject was 
presented with the same text from which the sen- 
tence boundaries had been removed. The texts were 
transcriptions of two editions of the news program 
from 1997, containing 534 sentences and represented 
around 50 minutes of broadcast news. The subjects 
were randomly split into two groups. The subjects 
in the first group (subjects 1-3) were presented with 
the text stripped of punctuation and converted to 
upper case. This text simulated ASR text with no 
errors in the transcription. The remaining three sub- 
jects (4-6) were presented with the same text with 
punctuation removed but case information retained 
(i.e. mixed case text). This simulated unpunctuated 
standard text. All subjects were asked to add sen- 
tence boundaries to the text whenever they thought 
they occurred. 
The process of determining human ability at some 
linguistic task is generally made difficult by the lack 
of an appropriate reference. Often all we have to 
compare one person's judgement with is that of an- 
other. For example, there have been attempts to 
determine the level of performance which can be ex- 
pected when humans perform word sense disambig- 
uation (Fellbaum et al, 1998) but these have simply 
compared some human judgements against others 
with one being chosen as the "expert". We have 
already seen, in Section 2, that there is a signific- 
ant degree of human disagreement over the accept- 
ability of intra-sentential punctuation. The human 
transcribers ofthe "Nine O'Clock News" have access 
to the original news story which contains more in- 
formation than just the transcription. Under these 
conditions it is reasonable to consider their opinion 
as expert. 
Table 1 shows the performance of the human sub- 
jects compared to the reference transcripts. 4 
An algorithm was implemented to provide a 
baseline tagging of the text. The average length of 
sentences in our text is 19 words and the baseline al- 
gorithm randomly assigns a sentence break at each 
word boundary with a probability of ~ .  The two 
annotators labelled "random" show the results when 
this algorithm is applied. This method produced a
3This is a 25 minute long television ews program broad- 
cast in the United Kingdom on Monday to Friday evenings. 
4F-measure (F) is a weighted harmonic ombining preci- 
sion (P) and recall (R) via the formula 2PR 
PTR " 
very low result in comparison to the expert annota- 
tion. 
1 Upper 84 68 76 
2 Upper 93 78 85 
3 Upper 90 76 82 
4 Mixed 97 90 94 
5 Mixed 96 89 92 
6 Mixed 97 67 79 
Random Upper 5 5 5 
Random Mixed 5 5 5 
Table 1: Results from Human Annotation Experi- 
ment 
The performance of the human annotators on the 
upper case text is quite significantly lower than 
the reported performance of the algorithms which 
performed punctuation disambiguation on standard 
text as described in Section 2. This suggests that 
the performance which may be obtained for this task 
may be lower than has been achieved for standard 
text. 
~Sarther insight into the task can be gained from 
determining the degree to which the subjects agreed. 
Carletta (1996) argues that the kappa statistic (a) 
should be adopted to judge annotator consistency 
for classification tasks in the area of discourse and 
dialogue analysis. It is worth noting that the prob- 
lem of sentence boundary detection presented so far 
in this paper has been formulated as a classification 
task in which each token boundary has to be clas- 
sifted as either being a sentence boundary or not. 
Carletta argues that several incompatible measures 
of annotator agreement have been used in discourse 
analysis, making comparison impossible. Her solu- 
tion is to look to the field of content analysis, which 
has already experienced these problems, and adopt 
their solution of using the kappa statistic. This de- 
termines the difference between the observed agree- 
ment for a linguistic task and that which would be 
expected by chance. It is calculated according to for- 
mula 1, where Pr(A) is the proportion of times the 
annotators agree and Pr(E) the proportion which 
would be expected by chance. Detailed instructions 
on calculating these probabilities are described by 
Siegel and Castellan (1988). 
Pr(A) - Pr(E)  
= (1) 
1 - Pr(E) 
The value of the kappa statistic ranges between 
1 (perfect agreement) and 0 (the level which would 
be expected by chance). It has been claimed that 
content analysis researchers usually regard a > .8 to 
demonstrate good reliability and .67 < ~ < .8 al- 
f16 
lows tentative conclusions to be drawn (see Carletta 
(1996)). 
We began to analyse the data by computing the 
kappa statistic for both sets of annotators. Among 
the two annotators who marked the mixed case (sub- 
jects 4 and 5) there was an observed kappa value of 
0.98, while there was a measure of 0.91 for the three 
subjects who annotated the single case text. These 
values are high and suggest a strong level of agree- 
ment between the annotators. However, manual 
analysis of the annotated texts suggested that the 
subjects did not agree on many cases. We then ad- 
ded the texts annotated by the "random" annotation 
algorithm and calculated the new ~ values. It was 
found that the mixed case test produced a kappa 
value of 0.92 and the upper case text 0.91. These 
values would still suggest a high level of agreement 
although the sentences produced by our random al- 
gorithm were nonsensical. 
The problem seems to be that most word bound- 
aries in a text are not sentence boundaries. There- 
fore we could compare the subjects' annotations 
who had not agreed on any sentence boundaries but 
find that they agreed most word boundaries were 
not sentence boundaries. The same problem will 
effect other standard measures of inter-annotator 
agreement such as the Cramer, Phi and Kendall 
coefficients (see Siegel and Castellan (1988)). Car- 
letta mentions this problem, asking what the dif- 
ference would be if the kappa statistic were com- 
puted across "clause boundaries, transcribed word 
boundaries, and transcribed phoneme boundaries" 
(Carletta, 1996, p. 252) rather than the sentence 
boundaries he suggested. It seems likely that more 
meaningful ~ values would be obtained if we restric- 
ted to the boundaries between clauses rather than 
all token boundaries. However, it is difficult to ima- 
gine how clauses could be identified without parsing 
and most parsers require part of speech tagged input 
text. But, as we already mentioned, part of speech 
taggers often require input text split into sentences. 
Consequently, there is a lack of available systems for 
splitting ASR text into grammatical clauses. 
4 A Computat iona l  Approach  to  
Sentence  Boundary  Detect ion  
The remainder of this paper describes an implemen- 
ted program which attempts entence boundary de- 
tection. The approach is based around the Timbl 
memory-based learning algorithm (Daelemans et al, 
1999) which we previously found to be very success- 
ful when applied to the word sense disambiguation 
problem (Stevenson and Wilks, 1999). 
Memory-based learning, also known as case-based 
and lazy learning, operates by memorising a set of 
training examples and categorising new cases by as- 
signing them the class of the most similar learned 
example. We apply this methodology to the sen- 
tence boundary detection task by presenting Timbl 
with examples of word boundaries from a train- 
ing text, each of which is categorised as either 
sentence_boundary or no_boundary. Unseen ex- 
amples are then compared and categorised with the 
class of the most similar example. We shall not 
discuss the method by which Timbl determines the 
most similar training example which is described by 
Daelemans et al (1999). 
Following the work done on punctuation disambig- 
uation and that of Beeferman et. al. on comma in- 
sertion (Section 2), we used the Wall Street Journal 
text for this experiment. These texts are reliably 
part of speech tagged 5 and sentence boundaries can 
be easily derived from the corpus. This text was 
initially altered so as to remove all punctuation and 
map all characters into upper case. 90% of the cor- 
pus, containing 965 sentence breaks, was used as a 
training corpus with the remainder, which contained 
107 sentence breaks, being held-back as unseen test 
data. The first stage was to extract some statistics 
from the training corpus. We examined the training 
corpus and computed, for each word in the text, the 
probability that it started a sentence and the prob- 
ability that it ended a sentence. In addition, for each 
part of speech tag we also computed the probability 
that it is assigned to the first word in a sentence and 
the probability that it is assigned to the last word. 6 
Each word boundary in the corpus was translated to 
a feature-vector representation consisting of 13 ele- 
ments, shown in Table 2. Vectors in the test corpus 
are in a similar format, the difference being that the 
classification (feature 13) is not included. 
The results obtained are shown in the top row of 
Table 3. Both precision and recall are quite prom- 
ising under these conditions. However, this text is 
different from ASR text in one important way: the 
text is mixed case. The experimented was repeated 
with capitalisation information removed; that is, 
features 6 and 12 were removed from the feature- 
vectors. The results form this experiment are shown 
in the bottom row of Table 3. It can be seen that 
the recorded performance is far lower when capital- 
isation information is not used, indicating that this 
is an important feature for the task. 
These experiments have shown that it is much 
easier to add sentence boundary information to 
mixed case test, which is essentially standard text 
with punctuation removed, than ASR text, even as- 
5Applying a priori tag probability distributions could have 
been used rather than the tagging in the corpus as such re- 
liable annotations may not be available for the output of an 
ASR system. Thus, the current experiments should be viewed 
as making an optimistic assumption. 
eWe attempted to smooth these probabilities using Good- 
Turing frequency estimation (Gale and Sampson, 1996) but 
found that it had no effect on the final results. 
87 
Position Feature 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
Preceding word 
Probability preceding word ends a sentence 
Part of speech tag assigned to preceding word 
Probability that part of speech tag (feature 3) is assigned to last word in a sentence 
Flag indicating whether preceding word is a stop word 
Flag indicating whether preceding word is capitalised 
Following word 
Probability following word begins a sentence 
Part of speech tag assigned to following word 
Probability that part of speech (feature 9) is assigned to first word in a sentence 
Flag indicating whether following word is a stop word 
Flag indicating whether following word is capitalised word 
sentence_boundary or no_boundary 
Table 2: Features used in Timbl representation 
Case information \[I P I R I F 
Applied I 78 \[ 75 \[ 76 
Not applied 36 35 35 
Table 3: Results of the sentence boundary detection 
program 
suming a zero word error rate. This result is in 
agreement with the results from the human annota- 
tion experiments described in Section 3. However, 
there is a far greater difference between the auto- 
matic system's performance on standard and ASR 
text than the human annotators. 
Reynar and Ratnaparkhi (1997) (Section 2) ar- 
gued that a context of one word either side is suf- 
ficient for the punctuation disambiguation problem. 
However, the results of our system suggest that this 
may be insufficient for the sentence boundary detec- 
tion problem even assuming reliable part of speech 
tags (cf note 5). 
These experiments do not make use of prosodic in- 
formation which may be included as part of the ASR 
output. Such information includes pause length, 
pre-pausal lengthening and pitch declination. If this 
information was made available in the form of extra 
features to a machine learning algorithm then it is 
possible that the results will improve. 
5 Conc lus ion  
This paper has introduced the problem of sentence 
boundary detection on the text produced by an ASR 
system as an area of application for NLP technology. 
An attempt was made to determine the level of 
human performance which could be expected for the 
task. It was found that there was a noticeable dif- 
ference between the observed performance for mixed 
and upper case text. It was found that the kappa 
statistic, a commonly used method for calculating 
inter-annotator agreement, could not be applied dir- 
ectly in this situation. 
A memory-based system for identifying sentence 
boundaries in ASR text was implemented. There 
was a noticeable difference when the same system 
was applied to text which included case information 
demonstrating that this is an important feature for 
the problem. 
This paper does not propose to offer a solution to 
the sentence boundary detection problem for ASR 
transcripts. However, our aim has been to high- 
light the problem as one worthy of further explor- 
ation within the field of NLP and to establish some 
baselines (human and algorithmic) against which 
further work may be compared. 
Acknowledgements  
The authors would like to thank Steve Renals and 
Yoshihiko Gotoh for providing the data for human 
annotation experiments and for several useful con- 
versations. They are also grateful to the following 
people who took part in the annotation experiment: 
Paul Clough, George Demetriou, Lisa Ferry, Michael 
Oakes and Andrea Setzer. 
References  
D. Beeferman, A. Berger, and J. Lafferty. 1998. CY- 
BERPUNC: A lightweight punctuation annota- 
tion system for speech. In Proceedings of the IEEE 
International Conference on Acoustics, Speech 
and Signal Processing, pages 689-692, Seattle, 
WA. 
E. Brill. 1992. A simple rule-based part of speech 
tagger. In Proceeding of the Third Conference on 
Applied Natural Language Processing (ANLP-92), 
pages 152-155, Trento, Italy. 
R~ 88
L. Burnard, 1995. Users Reference Guide for the 
British National Corpus. Oxford University Com- 
puting Services. 
J. Carletta. 1996. Assessing agreement on classific- 
ation tasks: the kappa statistic. Computational 
Linguistics, 22(2):249-254. 
N. Chinchor, P. Robinson, and E. Brown. 
1998. HUB-4 Named Entity Task Defini- 
tion (version 4.8). Technical report, SAIC. 
http ://www. nist. gov/speech/hub4-98. 
R. Cole, editor. 1996. Survey of the State of the 
Art in Human Language Technology. Available at: 
http://cslu.cse.ogi.edu/HLTsurvey/HLTsurvey.html. 
Site visited 17/11/99. 
W. Daelemans, J. Zavrel, K. van der Sloot, 
and A. van den Bosch. 1999. TiMBL: Tilburg 
memory based learner version 2.0, reference guide. 
Technical report, ILK Technical Report 98-03. 
ILK Reference Report 99-01, Available from 
http ://ilk. kub. nl/" ilk/papers/ilk9901, ps. gz. 
C. Fellbaum, J. Grabowski, S. Landes, and A. Ban- 
mann. 1998. Matching words to senses in Word- 
Net: Naive vs. expert differentiation of senses. In 
C. Fellbaum, editor, WordNet: An electronic lex- 
ieal database and some applications. MIT Press, 
Cambridge, MA. 
W. Gale and G. Sampson. 1996. Good-Turing 
frequency estimation without tears. Journal of 
Quantitave Linguistics, 2(3):217-37. 
Y. Gotoh and S. Renals. 2000. Information extrac- 
tion from broadcast news. Philosophical Trans- 
actions of the Royal Society of London, series A: 
Mathematical, Physical and Engineering Sciences. 
(to appear). 
A. Mikheev. 1998. Feature lattices for maximum en- 
tropy modelling. In Proceedings of the 36th Meet- 
ing of the Association for Computational Linguist- 
ics (COLING-ACL-98), pages 848-854, Montreal, 
Canada. 
R. Moore, J. Dowding, H. Bratt, J. Gawron, 
Y. Gorfu, and A. Cheyer. 1997. CommandTalk: 
A Spokcaa-Language Interface to Battlefield Simu- 
lations. In Proceedings of the Fifth Conference on 
Applied Natural Language Processing, pages 1-7, 
Washington, DC. 
D. Palmer and M. Hearst. 1994. Adaptive sen- 
tence boundary disambiguation. In Proceedings of 
the 1994 Conference on Applied Natural Language 
Processing, pages 78-83, Stutgart, Germany. 
J. Reynar and A. Ratnaparkhi. 1997. A max- 
imum entropy approach to identifying sentence 
boundries. In Proceedings of the Fifth Conference 
on Applied Natural Language Processing, pages 
16-19, Washington, D.C. 
S. Siegel and N. Castellan. 1988. Nonparametrie 
Statistics for the Behavioural Sciences. McGraw- 
Hill, second edition. 
M. Stevenson and Y. Wilks. 1999. Combining weak 
knowledge sources for sense disambiguation. In 
Proceedings of the Sixteenth International Joint 
Conference on Artificial Intelligence, pages 884- 
889. Stockholm, Sweden. 
C. van Rijsbergen. 1979. Information Retrieval. 
Butterworths, London. 
89 
Using Corpus-derived Name Lists for Named Entity Recognition 
Mark  Stevenson  and Rober t  Ga izauskas  
Depar tment  of  Computer  Science, 
Un ivers i ty  of  Sheff ield 
Regent  Cour t ,  211 Por tobe l lo  S t reet ,  
Sheff ield 
S1 4DP Un i ted  K ingdom 
{marks, robertg}~dcs, shef. ac.uk 
Abst ract  
This paper describes experiments to establish the 
performance of a named entity recognition system 
which builds categorized lists of names from manu- 
ally annotated training data. Names in text are then 
identified using only these lists. This approach does 
not perform as well as state-of-the-art named en- 
tity recognition systems. However, we then show 
that by using simple filtering techniques for improv- 
ing the automatically acquired lists, substantial per- 
formance benefits can be achieved, with resulting F- 
measure scores of 87% on a standard test set. These 
results provide a baseline against which the con- 
tribution of more sophisticated supervised learning 
techniques for NE recognition should be measured. 
1 In t roduct ion  
Named entity (NE) recognition is the process of 
identifying and categorising names in text. Systems 
which have attempted the NE task have, in general, 
made use of lists of common ames to provide clues. 
Name lists provide an extremely efficient way of re- 
cognising names, as the only processing required is 
to match the name pattern in the list against the 
text and no expensive advanced processing such as 
full text parsing is required. However, name lists are 
a naive method for recognising names. McDonald 
(1996) defines internal and external evidence in the 
NE task. The first is found within the name string 
itself, while the second is gathered from its context. 
For example, in the sentence "President Washington 
chopped the tree" the word "President" is clear ex- 
ternal evidence that "Washington" denotes a person. 
In this case internal evidence from the name cannot 
conclusively tell us whether "Washington" is a per- 
son or a location ("Washington, DC"). A NE sys- 
tem based solely on lists of names makes use of only 
internal evidence and examples uch as this demon- 
strate the limitations of this knowledge source. 
Despite these limitations, many NE systems use 
extensive lists of names. Krupke and Hausman 
(1998) made extensive use of name lists in their sys- 
tem. They found that reducing their size by more 
than 90% had little effect on performance, conversely 
adding just 42 entries led to improved results. This 
implies that the quality of list entries is a more im- 
portant factor in their effectiveness than the total 
number of entries. Mikheev et al (1999) experi- 
mented with different ypes of lists in an NE system 
entered for MUC7 (MUC, 1998). They concluded 
that small lists of carefully selected names are as 
effective as more complete lists, a result consistent 
with Krupke and Hausman. However, both studies 
altered name lists within a larger NE system and it 
is difficult to tell whether the consistency of perform- 
ance is due to the changes in lists or extra, external, 
evidence being used to balance against the loss of 
internal evidence. 
In this paper a NE system which uses only the in- 
ternal evidence contained in lists of names is presen- 
ted. Section 3 explains how such lists can be auto- 
matically generated from annotated text. Sections 
4 and 5 describe xperiments in which these corpus- 
generated lists are applied and their performance 
compared against hand-crafted lists. In the next sec- 
tion the NE task is described in further detail. 
2 NE background 
2.1 NE Recognition of Broadcast News 
The NE task itself was first introduced as part of 
the MUC6 (MUC, 1995) evaluation exercise and was 
continued in MUC7 (MUC, 1998). This formulation 
of the NE task defines seven types of NE: PERSON, 
ORGANIZATION, LOCATION, DATE, TIME, MONEY and 
PERCENT. Figure 1 shows a short text marked up 
in SGML with NEs in the MUC style. 
The task was duplicated for the DARPA/N IST  
HUB4 evaluation exercise (Chinchor et al, 1998) 
but this time the corpus to be processed consisted 
of single case transcribed speech, rather than mixed 
case newswire text. Participants were asked to carry 
out NE recognition on North American broadcast 
news stories recorded from radio and television and 
processed by automatic speech recognition (ASR) 
software. The participants were provided with a 
training corpus consisting of around 32,000 words 
of transcribed broadcast news stories from 1997 an- 
notated with NEs. Participants used these text to 
290 
"It's a chance to think about first-level questions," said Ms. <enamex 
type="PERS0N">Cohn<enamex>, a partner in the <enamex type="0RGANIZATION">McGlashan 
Sarrail<enamex> firm in <enamex type="L0CATION">San Mateo<enamex>, <enamex 
type="L0CATION">Calif.<enamex> 
Figure 1: Text with MUC-style NE's marked 
develop their systems and were then provided with 
new, unannotated texts, consisting of transcribed 
broadcast news from 1998 which they were given a 
short time to annotate using their systems and re- 
turn. Participants are not given access to the eval- 
uation data while developing their systems. 
After the evaluation, BBN, one of the parti- 
cipants, released a corpus of 1 million words which 
they had manually annotated to provide their sys- 
tem with more training data. Through the re- 
mainder of this paper we refer to the HUB4 training 
data provided by DARPA/NIST as the SNORT_TRAIN 
corpus and the union of this with the BBN data as 
the LONG_TRAIN corpus. The data used for the 1998 
HUB4 evaluation was kept blind, we did not exam- 
ine the text themselves, and shall be referred to as 
the TEST corpus. 
The systems were evaluated in terms of the com- 
plementary precision (P) and recall (R) metrics. 
Briefly, precision is the proportion of names pro- 
posed by a system which are true names while recall 
is the proportion of the true names which are actu- 
ally identified. These metrics are often combined 
using a weighted harmonic called the F-measure 
(F) calculated according to formula 1 where fl is a 
weighting constant often set to 1. A full explana- 
tion of these metrics is provided by van Rijsbergen 
(1979). 
F= ( f~+l)  xPxR 
(fl ? P) + R (1) 
The best performing system in the MUC7 exercise 
was produced by the Language Technology Group of 
Edinburgh University (Mikheev et al, 1999). This 
achieved an F-measure of 93.39% (broken down as 
a precision of 95% and 92% recall). In HUB4 BBN 
(Miller et al, 1999) produced the best scoring sys- 
tem which achieved an F-measure of 90.56% (preci- 
sion 91%, recall 90%) on the manually transcribed 
test data. 
2.2 A Full NE  sys tem 
The NE system used in this paper is based on Shef- 
field's LaSIE system (Wakao et al, 1996), versions 
of which have participated in MUC and HUB4 eval- 
uation exercises (Renals et al, 1999). The system 
identifies names using a process consisting of four 
main modules: 
List Lookup This module consults several ists of 
likely names and name cues, marking each oc- 
currence in the input text. The name lists in- 
clude lists of organisations, locations and per- 
son first names and the name cue lists of titles 
(eg. "Mister", "Lord"), which are likely to pre- 
cede person names, and company designators 
(eg. "Limited" or "Incorporated"), which are 
likely to follow company names. 
Par t  of speech tagger  The text is the part of 
speech tagged using the Brill tagger (Brill, 
1992). This tags some tokens as "proper name" 
but does not attempt o assign them to a NE 
class (eg. PERSON, LOCATION). 
Name pars ing  Next the text is parsed using a col- 
lection of specialised NE grammars. The gram- 
mar rules identify sequences of part of speech 
tags as added by the List Lookup and Par t  
of  speech tagger  modules. For example, there 
is a rule which says that a phrase consisting 
of a person first name followed by a word part 
of speech tagged as a proper noun is a person 
name. 
Namematch ing  The names identified so far in the 
text are compared against all unidentified se- 
quences of proper nouns produced by the part of 
speech tagger. Such sequences form candidate 
NEs and a set of heuristics is used to determ- 
ine whether any such candidate names match 
any of those already identified. For example one 
such heuristics ays that if a person is identified 
with a title (eg. "President Clinton") then any 
occurrences without the title are also likely to 
be person names '(so "Clinton" on it own would 
also be tagged as a person name). 
For the experiments described in this paper a re- 
stricted version of the system which used only the 
List Lookup module was constructed. The list 
lookup mechanism arks all words contained in any 
of the name lists and each is proposed as a NE. Any 
string occurring in more than one list is assigned the 
category form the first list in which it was found, al- 
though this did not occur in any of the sets of lists 
used in the experiments described here. 
3 L i s t  Generat ion  
The List Lookup module uses a set of hand- 
crafted lists originally created for the MUC6 eval- 
uation. They consisted of lists of names from the 
gazetteers provided for that competition, supple- 
mented by manually added entries. These lists 
291 
evolved for the MUC7 competition with new entries 
and lists being added. For HUB4 we used a se- 
lection of these lists, again manually supplementing 
them where necessary. These lists included lists of 
companies, organisations (such as government de- 
partments), countries and continents, cities, regions 
(such as US states) and person first names as well as 
company designators and person titles. We specu- 
late that this ad hoc, evolutionary, approach to cre- 
ating name lists is quite common amongst systems 
which perform the NE task. 
In order to compare this approach against a simple 
system which gathers together all the names occur- 
ring in NE annotated training text, a program was 
implemented to analyse text annotated in the MUC 
SGML style (see Figure 1) and create lists for each 
NE type found. For example, given the NE <enamex 
type="LOCATION">SAN MATE0<enamex> an entry 
SAN MATE0 would be added a list of locations. 
This simple approach is certainly acceptable for 
the LOCATION, ORGANIZATION and, to a more lim- 
ited extent, PERSON classes. It is less applicable to 
the remaining classes of names (DATE, TIME, MONEY 
and PERCENT) because these are most easily recog- 
nised by their grammatical structure. For example, 
there is a rule in the NE grammar which says a num- 
ber followed by a currency unit is as instance of the 
MONEY name class- eg. FIFTY THREE DOLLARS, FIVE 
MILLION ECU. According to Przbocki et al (1999) 
88% of names occurring in broadcast news text fall 
into one of the LOCATION, ORGANIZATION and PERSON 
categories. 
Two sets of lists were derived, one from 
the SHORT_TRAIN corpus and a second from the 
LONG_TRAIN texts. The lengths of the lists produced 
are shown in Table 1. 
Corpus 
Category SHORT_TRAIN LONG_TRAIN 
ORGANIZATION 245 2,157 
PERSON 252 3,947 
LOCATION 230 1,489 
Table 1: Lengths of lists derived from SHORT_TRAIN 
and LONG_TRAIN corpora 
4 L i s t  App l i ca t ion  
The SHORT_TRAIN and LONG_TRAIN lists were each 
applied in two ways, alone and appended to the ori- 
ginal, manually-created, lists. In addition, we com- 
puted the performance obtained using only the ori- 
ginal lists for comparison. Although both sets of lists 
were derived using the SHORT_TRAIN data (since the 
LONG_TRAIN corpus includes SHORT_TRAIN), we still 
compute the performance of the SHORT_TRAIN lists 
on that corpus since this provides some insight into 
the best possible performance which can be expected 
from NE recognition using a simple list lookup mech- 
anism. No scores were computed for the LONG_TRAIN 
lists against he SHORT_TRAIN corpus since this is un- 
likely to provide more information. 
Table 2 shows the results obtained when the 
SHORT_TRAIN lists were applied to that corpus. This 
first experiment was designed to determine how 
well the list lookup approach would perform given 
lists compiled directly from the corpus to which 
they are being applied. Only PERSON, LOCATION 
and ORGANIZATION name classes are considered since 
they form the majority of names occurring in the 
HUB4 text. As was mentioned previously, the re- 
maining categories of name are more easily recog- 
nised using the NE parser. For each configuration of 
lists the precision, recall and F-measure are calcu- 
lated for the each name class both individually and 
together. 
We can see that the original ists performed reas- 
onably well, scoring an F-measure of 79% overall. 
However, the corpus-based lists performed far bet- 
ter achieving high precision and perfect recall. We 
would expect he system to recognise very name in 
the text, since they are all in the lists, but perfect 
precision is unlikely as this would require that no 
word appeared as both a name and non-name or in 
more than one name class. Even bearing this in mind 
the calculated precision for the ORGANIZATION class 
of names is quite low. Analysis of the output showed 
that several words occurred as names a few times in 
the text but also as non-names more frequently. For 
example, "police" appeared 35 times but only once 
as an organisation; similarly "finance" and "repub- 
lican" occur frequently but only as a name a few 
times. In fact, these three list entries account for 61 
spuriously generated names, from a total of 86 for 
the ORGANIZATION class. The original lists do not 
include words which are likely to generate spurious 
entries and names like "police" would only be recog- 
nised when there was further evidence. 
The SHORT_TRAIN lists contain all the names oc- 
curring in that text. When these lists are combined 
with the original system lists the observed recall re- 
mains 100% while the precision drops. The original 
system lists introduce more spurious entries, leading 
to a drop of 3% F-measure. 
The results of applying the corpus-derived lists to 
the texts from which they were obtained show that, 
even under these circumstances, perfect results can- 
not be obtained. Table 3 shows a more meaningful 
evaluation; the SHORT_TRAIN lists are applied to the 
TEST corpus, an unseen text. The original system 
lists achieve an F-measure of 83% on this text and 
the corpus-derived lists perform 8% worse. However, 
the configuration of lists which performs best is the 
union of the original ists with those derived from the 
292 
Lists Original SHORT_TRAIN Combination 
Name Type P R F P R F P R F 
ALL 86 73 79 94 100 97 88 100 94 
ORGANIZATION 84 49 62 83 100 90 79 100 88 
PERSON 78 71 74 99 100 99 88 100 94 
LOCATION 92 88 90 98 100 99 95 100 97 
Table 2: SHORT_TRAIN lists applied to SHORT_TRAIN corpus 
corpus. This out-performs each set of lists taken in 
isolation both overall and for each name category in- 
dividually. This is clear evidence that the lists used 
by the system described could be improved with the 
addition of lists derived from annotated text. 
It is worth commenting on some of the results for 
individual classes of names in this experiment. We 
can see that the performance for the ORGANIZATION 
class actually increases when the corpus-based lists 
are used. This is partially because names which are 
made up from initials (eg. "C. N. N." and "B. B. C. ") 
are not generally recognised by the list lookup mech- 
anism in our system, but are captured by the 
parser and so were not included in the original lists. 
However, it is also likely that the organisation list is 
lacking, at least to some level. More interestingly, 
there is a very noticeable drop in the performance 
for the PERSON class. The SHORT_TRAIN lists achieved 
an F-measure of 99% on that text but only 48% on 
the TEST text. In Section 2.1 we mentioned that the 
HUB4 training data consists of news stories from 
1997, while the test data contains tories from 1998. 
We therefore suggest hat the decrease in perform- 
ance for the PERSON category demonstrates a general 
property of broadcast news: many person names 
mentioned are specific to a particular time period 
(eg. "Monica Lewinksi" and "Rodney King"). In 
contrast, the locations and organisations mentioned 
are more stable over time. 
Table 4 shows the performance obtained when the 
lists derived from LONG_TRAIN were applied to the 
TEST corpus. The corpus-derived lists perform sig- 
nificantly worse than the original system lists, show- 
ing a large drop in precision. This is to be expec- 
ted since the lists derived from LONG_TRAIN contain 
all the names occurring in a large body of text and 
therefore contain many words and phrases which are 
not names in this text, but spuriously match non- 
names. Although the F-measure result is worse than 
when the SHORT_TRAIN lists were used, the recall 
is higher showing that a higher proportion of the 
true names can be found by analysing a larger body 
of text. Combining the original and corpus-derived 
lists leads to a 1% improvement. Recall is noticeably 
improved compared with the original lists, however 
precision is lowered and this shows that the corpus- 
derived lists introduce a large number of spurious 
names. 
From this first set of experiments it can be seen 
that perfect results will not be obtained even using 
lists contain all and only the names in a particular 
text, thus demonstrating the limitations of this na- 
ive approach to named entity recognition. We have 
also demonstrated that it is possible for the addi- 
tion of corpus-derived lists to improve the perform- 
ance of a NE recognition system based on gazetteers. 
However, this is not guaranteed and it appears that 
adding too many names without any restriction may 
actually lead to poorer results, as happened when 
the LONG_TRAIN lists were applied. 
5 F i l te r ing  L is ts  
The results from our first set of experiments led us to 
question whether it is possible to restrict the entries 
being added to the lists in order to avoid those likely 
to generate spurious names. We now go on to de- 
scribe some methods which can be used to identify 
and remove list entries which may generate spurious 
names. 
Method 1: Dic t ionary  F i l te r ing  The derived 
lists can be improved by removing items in the 
list which also occur as entries in a dictionary. 
We began by taking the Longman Dictionary of 
Contemporary Englisb (LDOCE) (Procter, 1978) and 
extracting a list of words it contained including all 
derived forms, for example pluralisation of nouns 
and different verb forms. This produced a list of 
52,576 tokens which could be used to filter name 
lists. 
Method 2: Probability F i l te r ing  The lists can 
be improved by removing names which occur 
more frequently in the corpus as non-names 
than names. 
Another method for filtering lists was imple- 
mented, this time using the relative frequencies of 
phrases occurring as names and non-names. We can 
extract the probability that a phrase occurs as a 
name in the training corpus by dividing the num- 
ber of times it occurs as a name by the total number 
of corpus occurrences. If this probability estimate is 
an accurate reflection of the name's behaviour in a 
293 
Lists Original SHORT_TI~IN Combination 
Name Type P R F P R F P R F 
ALL 86 79 83 90 65 75 83 86 84 
ORGANIZATION 82 57 67 76 66 71 79 81 80 
PERSON 77 80 78 93 32 48 79 83 81 
LOCATION 93 89 91 97 81 88 92 94 93 
Table 3: SHORT_TRAIN \]ists applied to TEST corpus 
Lists Original LONG_TRAIN Combination 
Name Type P R F P R F P R F 
ALL 86 79 83 64 86 73 62 91 74 
ORGANIZATION 82 57 67 44 85 58 43 88 58 
PERSON 77 80 78 55 75 63 53 86 66 
LOCATION 93 89 91 87 92 89 84 94 89 
Table 4: LONG_TRAIN lists applied to TEST corpus 
new text we can use it to estimate the accuracy of 
adding that name to the list. Adding a name to a 
list will lead to a recall score of 1 for that name and 
a precision of Pr  (where Pr  is the probability value 
estimated from the training corpus) which implies an 
F-measure of ~.2Pr 1 Therefore the probabilities can 
be used to filter out candidate list items which imply 
low F-measure scores. We chose names whose cor- 
pus probabilities produced an F-measure lower than 
the overall score for the list. The LONG_TRAIN lists 
scored an F-measure of 73% on the unseen, TEST, 
data (see Table 4). Hence a filtering probability of 
73% was used for these lists, with the corpus stat- 
istics gathered from LONG_TRAIN. 
Method  3: Combin ing  F i l ters  These filtering 
strategies can be improved by combining them. 
We also combined these two filtering strategies in 
two ways. Firstly, all names which appeared in the 
lexicon or whose corpus probability is below the fil- 
tering probability are removed from the lists. This is 
dubbed the "or combination". The second combin- 
ation strategy removes any names which appear in 
the lexicon and occur with a corpus frequency below 
the filtering probability are removed. This second 
strategy is called the "and combination". 
These filtering strategies were applied to the 
LONG_TRAIN lists. The lengths of the lists produced 
are shown in Table 5. 
The strategies were evaluated by applying the 
filtered LONG_TRAIN lists to the TEST corpus, the res- 
ults of which are shown in Table 6. There is an 
1Analysis of the behaviour of the function f (P r )  -- 2P~ l+Pr  
shows that it does not deviate too far from the value of Pr  (ie. 
. f (P r )  ~ Pr )  and so there is an argument for simply filtering 
the lists using the raw probabilities. 
improvement in performance of 4% F-measure when 
lists filtered using the "and" combination are used 
compared to the original, hand-crafted, lists. Al- 
though this approach removes only 108 items from 
all the lists there is a 14% F-measure improvement 
over the un-filtered lists. Each filtering strategy used 
individually demonstrates a lower level of improve- 
ment: the dictionary filtered lists 12% and the prob- 
ability filtered 10%. 
The "and" combination is more successful be- 
cause filtering lists using the dictionary alone re- 
moves many names we would like to keep (eg. coun- 
try names are listed in LDOCE) but many of these 
are retained since both filters must agree. These 
experiments demonstrate hat appropriately filtered 
corpus-derived lists can be more effective for NE re- 
cognition than hand-crafted lists. The difference 
between the observed performance of our simple 
method and those reported for the best-performing 
HUB4 system is perhaps lower that one may ex- 
pect. The BBN system achieved 90.56% overall, 
and about 92% when only the PERSON, LOCATION 
and ORGANIZATION name classes are considered, 5% 
more than the method reported here. This difference 
is perhaps lower than we might expect given that 
name lists use only internal evidence (in the sense 
of Section 1). This indicates that simple application 
of the information contained in manually annotated 
NE training data can contribute massively to the 
overall performance of a system. They also provide 
a baseline against which the contribution of more 
sophisticated supervised learning techniques for NE 
recognition should be measured. 
294 
NE 
Category 
ORGANIZATION 
PERSON 
LOCATION 
Un-Filtered Dictionary Probability 
List Filtered Filtered 
2,157 1,978 2,000 
3,947 3,769 3,235 
1,489 1,412 1,364 
Or 
Combined 
1,964 
3,522 
1,382 
And 
Combined 
2,049 
3,809 
1,449 
Table 5: Lengths of corpus-derived lists 
Original t Un-Filtered Dictionary I Probability Or And 
Lists Lists Filtered Filtered Combination Combination 
Name Type P R F P R F P R F P R F P R F P R F 
ALL 
ORGANIZATION 
PERSON 
LOCATION 
86 79 83 
82 57 67 
77 80 78 
93 89 91 
64 86 73 
44 85 58 
55 75 63 
87 92 89 
95 79 85 
86 72 78 
96 66 78 
98 89 93 
96 73 83 
85 74 79 
96 40 56 
97 90 93 
95 73 83 
84 60 70 
100 49 66 
98 90 94 
93 81 87 
84 76 80 
94 66 78 
97 92 94 
Table 6: Filtered and un-filtered LONG_TRAIN lists applied to TEST corpus 
6 Conclusion 
This paper explored the role of lists of names in 
NE recognition, comparing hand-crafted and corpus- 
derived lists. It was shown that, under certain condi- 
tions, corpus-derived lists outperform hand-crafted 
ones. Also, supplementing hand-crafted lists with 
corpus-based ones often improves their performance. 
The reported method was more effective for the 
ORGANIZATION and LOCATION classes of names than 
for PERSON, which was attributed to the fact that 
reportage of these names does not change as much 
over time in broadcast news. 
The method reported here achieves 87% F- 
measure, 5% less than the best performing system 
in the HUB4 evaluation. However, it should be re- 
membered that this technique uses only a simple ap- 
plication of internal evidence. 
References 
E. Brill. 1992. A simple rule-based part of speech 
tagger. In Proceeding of the Third Conference on 
Applied Natural Language Processing (ANLP-92), 
pages 152-155, Trento, Italy. 
N. Chinchor, P. Robinson, and E. Brown. 
1998. Hub-4 named entity task defini- 
tion (version 4.8). Technical report, SAIC. 
http ://www. nist. gov/speech/hub4_98. 
G. Krupke and K. Hausman. 1998. Isoquest Inc: 
description of the NetOwl(TM) extractor system 
as used for MUC-7. In Message Understanding 
Conference Proceedings: MUC 7. Available from 
http ://www.muc. saic. com. 
D. McDonald. 1996. Internal and external evid- 
ence in the identification and semantic ategor- 
ization of proper names. In B. Boguraev and 
J. Pustejovsky, editors, Corpus Processing for 
Lexical Aquisition, chapter 2, pages 21-39. MIT 
Press, Cambridge, MA. 
A. Mikheev, M. Moens, and C. Grovel 1999. 
Named entity recognition without gazeteers. In 
Proceedings of the Ninth Conference of the 
European Chapter of the Association for Compu- 
tational Linguistics, pages 1-8, Bergen, Norway. 
D. Miller, R. Schwartz, R. Weischedel, and R. Stone. 
1999. Named entity extraction from broadcast 
news. In Proceedings of the DARPA Broadcast 
News Workshop, ages 37-40, I-Ierndon, Virginia. 
MUC. 1995. Proceedings of the Sixth Message Un- 
derstanding Conference (MUC-6}, San Mateo, 
CA. Morgan Kaufmann. 
1998. Message Understanding Conference Proceed- 
ings: MUC7.  http ://www.muc. sale com. 
P. Procter, editor. 1978. Longman Dictionary of 
Contemporary English. Longman Group, Essex, 
UK. 
M. Przbocki, J. Fiscus, J. Garofolo, and D. Pallett. 
1999. 1998 HUB4 Information Extraction Eval- 
uation. In Proceedings of the DARPA Broadcast 
News Workshop, ages 13-18, Herndon, Virginia. 
S. Renals, Y. Gotoh, R. Gaizausaks, and M. Steven- 
son. 1999. Baseline IE-NE Experimants Using the 
SPRACH/LASIE System. In Proceedings of the 
DAPRA Broadcast News Workshop, ages 47-50, 
Herndon, Virginia. 
C. van Rijsbergen. 1979. Information Retrieval. 
Butterworths, London. 
T. Wakao, R. Gaizauskas, and K. Humphreys. 1996. 
Evaluation of an algorithm for the recognition and 
classification of proper names. In Proceedings of 
the 16th International Conference on Computa- 
tional Linguistics (COLING-96), pages 418-423, 
Copenhagen, Denmark. 
295 
Information Extraction from Single and Multiple Sentences
Mark Stevenson
Department of Computer Science
Regent Court, 211 Portobello Street,
University of Sheffield
Sheffield
S1 4DP, UK
marks@dcs.shef.ac.uk
Abstract
Some Information Extraction (IE) systems
are limited to extracting events expressed
in a single sentence. It is not clear what ef-
fect this has on the difficulty of the extrac-
tion task. This paper addresses the prob-
lem by comparing a corpus which has been
annotated using two separate schemes: one
which lists all events described in the text
and another listing only those expressed
within a single sentence. It was found that
only 40.6% of the events in the first anno-
tation scheme were fully contained in the
second.
1 Introduction
Information Extraction (IE) is the process of
identifying specific pieces of information in text,
for example, the movements of company execu-
tives or the victims of terrorist attacks. IE is a
complex task and a the description of an event
may be spread across several sentences or para-
graphs of a text. For example, Figure 1 shows
two sentences from a text describing manage-
ment succession events (i.e. changes in corpo-
rate executive management personnel). It can
be seen that the fact that the executives are
leaving and the name of the organisation are
listed in the first sentence. However, the names
of the executives and their posts are listed in the
second sentence although it does not mention
the fact that the executives are leaving these
posts. The succession events can only be fully
understood from a combination of the informa-
tion contained in both sentences.
Combining the required information across
sentences is not a simple task since it is neces-
sary to identify phrases which refer to the same
entities, ?two top executives? and ?the execu-
tives? in the above example. Additional diffi-
culties occur because the same entity may be
referred to by a different linguistic unit. For ex-
ample, ?International Business Machines Ltd.?
may be referred to by an abbreviation (?IBM?),
Pace American Group Inc. said it notified
two top executives it intends to dismiss them
because an internal investigation found ev-
idence of ?self-dealing? and ?undisclosed fi-
nancial relationships.? The executives are
Don H. Pace, cofounder, president and chief
executive officer; and Greg S. Kaplan, senior
vice president and chief financial officer.
Figure 1: Event descriptions spread across two
sentences
nickname (?Big Blue?) or anaphoric expression
such as ?it? or ?the company?. These complica-
tions make it difficult to identify the correspon-
dences between different portions of the text de-
scribing an event.
Traditionally IE systems have consisted of
several components with some being responsi-
ble for carrying out the analysis of individual
sentences and other modules which combine the
events they discover. These systems were of-
ten designed for a specific extraction task and
could only be modified by experts. In an ef-
fort to overcome this brittleness machine learn-
ing methods have been applied to port sys-
tems to new domains and extraction tasks with
minimal manual intervention. However, some
IE systems using machine learning techniques
only extract events which are described within
a single sentence, examples include (Soderland,
1999; Chieu and Ng, 2002; Zelenko et al, 2003).
Presumably an assumption behind these ap-
proaches is that many of the events described
in the text are expressed within a single sen-
tence and there is little to be gained from the
extra processing required to combine event de-
scriptions.
Systems which only attempt to extract events
described within a single sentence only report
results across those events. But the proportion
of events described within a single sentence is
not known and this has made it difficult to com-
pare the performance of those systems against
ones which extract all events from text. This
question is addressed here by comparing two
versions of the same IE data set, the evaluation
corpus used in the Sixth Message Understand-
ing Conference (MUC-6) (MUC, 1995). The
corpus produced for this exercise was annotated
with all events in the corpus, including those
described across multiple sentences. An inde-
pendent annotation of the same texts was car-
ried out by Soderland (1999), although he only
identified events which were expressed within a
single sentence. Directly comparing these data
sets allows us to determine what proportion of
all the events in the corpus are described within
a single sentence.
The remainder of this paper is organised as
follows. Section 2 describes the formats for rep-
resenting events used in the MUC and Soder-
land data sets. Section 3 introduces a common
representation scheme which allows events to
be compared, a method for classifying types of
event matches and a procedure for comparing
the two data sets. The results and implications
of this experiment are presented in Section 4.
Some related work is discussed in Section 5.
2 Event Scope and Representation
The topic of the sixth MUC (MUC-6) was
management succession events (Grishman and
Sundheim, 1996). The MUC-6 data has been
commonly used to evaluate IE systems. The
test corpus consists of 100 Wall Street Jour-
nal documents from the period January 1993
to June 1994, 54 of which contained manage-
ment succession events (Sundheim, 1995). The
format used to represent events in the MUC-6
corpus is now described.
2.1 MUC Representation
Events in the MUC-6 evaluation data are
recorded in a nested template structure. This
format is useful for representing complex events
which have more than one participant, for ex-
ample, when one executive leaves a post to be
replaced by another. Figure 2 is a simplified
event from the the MUC-6 evaluation similar
to one described by Grishman and Sundheim
(1996).
This template describes an event in which
?John J. Dooner Jr.? becomes chairman of the
company ?McCann-Erickson?. The MUC tem-
plates are too complex to be described fully here
but some relevant features can be discussed.
Each SUCCESSION EVENT contains the name of
<SUCCESSION_EVENT-9402240133-2> :=
SUCCESSION_ORG:
<ORGANIZATION-9402240133-1>
POST: "chairman"
IN_AND_OUT: <IN_AND_OUT-9402240133-4>
VACANCY_REASON: DEPART_WORKFORCE
<IN_AND_OUT-9402240133-4> :=
IO_PERSON: <PERSON-9402240133-1>
NEW_STATUS: IN
ON_THE_JOB: NO
OTHER_ORG: <ORGANIZATION-9402240133-1>
REL_OTHER_ORG: SAME_ORG
<ORGANIZATION-9402240133-1> :=
ORG_NAME: "McCann-Erickson"
ORG_ALIAS: "McCann"
ORG_TYPE: COMPANY
<PERSON-9402240133-1> :=
PER_NAME: "John J. Dooner Jr."
PER_ALIAS: "John Dooner"
"Dooner"
Figure 2: Example Succession event in MUC
format
the POST, organisation (SUCCESSION ORG) and
references to at least one IN AND OUT sub-
template, each of which records an event in
which a person starts or leaves a job. The
IN AND OUT sub-template contains details of the
PERSON and the NEW STATUS field which records
whether the person is starting a new job or leav-
ing an old one.
Several of the fields, including POST, PERSON
and ORGANIZATION, may contain aliases which
are alternative descriptions of the field filler
and are listed when the relevant entity was de-
scribed in different was in the text. For ex-
ample, the organisation in the above template
has two descriptions: ?McCann-Erickson? and
?McCann?. It should be noted that the MUC
template structure does not link the field fillers
onto particular instances in the texts. Conse-
quently if the same entity description is used
more than once then there is no simple way of
identifying which instance corresponds to the
event description.
The MUC templates were manually filled by
annotators who read the texts and identified the
management succession events they contained.
The MUC organisers provided strict guidelines
about what constituted a succession event and
how the templates should be filled which the an-
notators sometimes found difficult to interpret
(Sundheim, 1995). Interannotator agreement
was measured on 30 texts which were examined
by two annotators. It was found to be 83% when
one annotator?s templates were assumed to be
correct and compared with the other.
2.2 Soderland?s Representation
Soderland (1999) describes a supervised learn-
ing system called WHISK which learned IE
rules from text with associated templates.
WHISK was evaluated on the same texts from
the MUC-6 data but the nested template struc-
ture proved too complex for the system to learn.
Consequently Soderland produced his own sim-
pler structure to represent events which he de-
scribed as ?case frames?. This representation
could only be used to annotate events described
within a single sentence and this reduced the
complexity of the IE rules which had to be
learned.
The succession event from the sentence
?Daniel Glass was named president and
chief executive officer of EMI Records
Group, a unit of London?s Thorn EMI
PLC.? would be represented as follows:1
@@TAGS Succession
{PersonIn DANIEL GLASS}
{Post PRESIDENT AND CHIEF EXECUTIVE OFFICER}
{Org EMI RECORDS GROUP}
Events in this format consist of up to four
components: PersonIn, PersonOut, Post and
Org. An event may contain all four components
although none are compulsory. The minimum
possible set of components which can form an
event are (1) PersonIn, (2) PersonOut or (3)
both Post and Org. Therefore a sentence must
contain a certain amount of information to be
listed as an event in this data set: the name
of an organisation and post participating in a
management succession event or the name of a
person changing position and the direction of
that change.
Soderland created this data from the MUC-
6 evaluation texts without using any of the
existing annotations. The texts were first
pre-processing using the University of Mas-
sachusetts BADGER syntactic analyser (Fisher
et al, 1995) to identify syntactic clauses and the
named entities relevant to the management suc-
cession task: people, posts and organisations.
Each sentence containing relevant entities was
examined and succession events manually iden-
tified.
1The representation has been simplified slightly for
clarity.
This format is more practical for machine
learning research since the entities which par-
ticipate in the event are marked directly in the
text. The learning task is simplified by the fact
that the information which describes the event
is contained within a single sentence and so the
feature space used by a learning algorithm can
be safely limited to items within that context.
3 Event Comparison
3.1 Common Representation and
Transformation
There are advantages and disadvantages to the
event representation schemes used by MUC and
Soderland. The MUC templates encode more
information about the events than Soderland?s
representation but the nested template struc-
ture can make them difficult to interpret man-
ually.
In order to allow comparison between events
each data set was transformed into a com-
mon format which contains the information
stored in both representations. In this format
each event is represented as a single database
record with four fields: type, person, post and
organisation. The type field can take the
values person in, person out or, when the di-
rection of the succession event is not known,
person move. The remaining fields take the
person, position and organisation names from
the text. These fields may contain alternative
values which are separated by a vertical bar
(?|?).
MUC events can be translated into this
format in a straightforward way since each
IN AND OUT sub-template corresponds to a sin-
gle event in the common representation. The
MUC representation is more detailed than
the one used by Soderland and so some in-
formation is discarded from the MUC tem-
plates. For example, the VACANCY REASON
filed which lists the reason for the manage-
ment succession event is not transfered to
the common format. The event listed in
Figure 2 would be represented as follows:
type(person in)
person(?John J. Dooner Jr.?|
?John Dooner?|?Dooner?)
org(?McCann-Erickson?|?McCann?)
post(chairman)
Alternative fillers for the person and org
fields are listed here and these correspond to the
PER NAME, PER ALIAS, ORG NAME and ORG ALIAS
fields in the MUC template.
The Soderland succession event shown
in Section 2.2 would be represented
as follows in the common format.
type(person in)
person(?Daniel Glass?)
post(?president?)
org(?EMI Records Group?)
type(person in)
person(?Daniel Glass?)
post(?chief executive officer?)
org(?EMI Records Group?)
In order to carry out this transformation an
event has to be generated for each PersonIn and
PersonOut mentioned in the Soderland event.
Soderland?s format also lists conjunctions of
post names as a single slot filler (?president and
chief executive officer? in this example). These
are treated as separate events in the MUC for-
mat. Consequently they are split into the sepa-
rate post names and an event generated for each
in the common representation.
It is possible for a Soderland event to consist
of only a Post and Org slot (i.e. there is nei-
ther a PersonIn or PersonOut slot). In these
cases an underspecified type, person move, is
used and no person field listed. Unlike MUC
templates Soderland?s format does not contain
alternative names for field fillers and so these
never occur when an event in Soderland?s for-
mat is translated into the common format.
3.2 Matching
The MUC and Soderland data sets can be com-
pared to determine how many of the events
in the former are also contained in the latter.
This provides an indication of the proportion of
events in the MUC-6 domain which are express-
ible within a single sentence. Matches between
Soderland and MUC events can be classified as
full, partial or nomatch. Each of these possi-
bilities may be described as follows:
Full A pair of events can only be fully match-
ing if they contain the same set of fields. In
addition there must be a common filler for
each field. The following pair of events are
an example of two which fully match.
type(person in)
person(?R. Wayne Diesel?|?Diesel?)
org(?Mechanical Technology Inc.?|
?Mechanical Technology?)
post(?chief executive officer?)
type(person in)
person(?R. Wayne Diesel?)
org(?Mechanical Technology?)
post(?chief executive officer?)
PartialA partial match occurs when one event
contains a proper subset of the fields of an-
other event. Each field shared by the two
events must also share at least one filler.
The following event would partially match
either of the above events; the org field is
absent therefore the matches would not be
full.
type(person in)
person(?R. Wayne Diesel?)
post(?chief executive officer?)
Nomatch A pair of events do not match if the
conditions for a full or partial match are not
met. This can occur if corresponding fields
do not share a filler or if the set of fields
in the two events are not equivalent or one
the subset of the other.
Matching between the two sets of events is
carried out by going through each MUC event
and comparing it with each Soderland event for
the same document. The MUC event is first
compared with each of the Soderland events to
check whether there are any equal matches. If
one is found a note is made and the matching
process moves onto the next event in the MUC
set. If an equal match is not found the MUC
event is again compared with the same set of
Soderland events to see whether there are any
partial matches. We allow more than one Soder-
land event to partially match a MUC event so
when one is found the matching process con-
tinues through the remainder of the Soderland
events to check for further partial matches.
4 Results
4.1 Event level analysis
After transforming each data set into the com-
mon format it was found that there were 276
events listed in the MUC data and 248 in the
Soderland set. Table 1 shows the number of
matches for each data set following the match-
ing process described in Section 3.2. The counts
under the ?MUC data? and ?Soderland data?
headings list the number of events which fall
into each category for the MUC and Soderland
data sets respectively along with corresponding
percentages of that data set. It can be seen that
112 (40.6%) of the MUC events are fully cov-
ered by the second data set, and 108 (39.1%)
partially covered.
Match MUC data Soderland data
Type Count % Count %
Full 112 40.6% 112 45.2%
Partial 108 39.1% 118 47.6%
Nomatch 56 20.3% 18 7.3%
Total 276 248
Table 1: Counts of matches between MUC and
Soderland data.
Table 1 shows that there are 108 events in
the MUC data set which partially match with
the Soderland data but that 118 events in the
Soderland data set record partial matches with
the MUC data. This occurs because the match-
ing process allows more than one Soderland
event to be partially matched onto a single
MUC event. Further analysis showed that the
difference was caused by MUC events which
were partially matched by two events in the
Soderland data set. In each case one event
contained details of the move type, person in-
volved and post title and another contained the
same information without the post title. This is
caused by the style in which the newswire sto-
ries which make up the MUC corpus are writ-
ten where the same event may be mentioned in
more than one sentence but without the same
level of detail. For example, one text contains
the sentence ?Mr. Diller, 50 years old, succeeds
Joseph M. Segel, who has been named to the
post of chairman emeritus.? which is later fol-
lowed by ?At that time, it was announced that
Diller was in talks with the company on becom-
ing its chairman and chief executive upon Mr.
Segel?s scheduled retirement this month.?
Table 1 also shows that there are 56 events in
the MUC data which fall into the nomatch cat-
egory. Each of these corresponds to an event in
one data set with no corresponding event in the
other. The majority of the unmatched MUC
events were expressed in such a way that there
was no corresponding event listed in the Soder-
land data. The events shown in Figure 1 are
examples of this. As mentioned in Section 2.2,
a sentence must contain a minimum amount of
information to be marked as an event in Soder-
land?s data set, either name of an organisation
and post or the name of a person changing po-
sition and whether they are entering or leaving.
In Figure 1 the first sentence lists the organisa-
tion and the fact that executives were leaving.
The second sentence lists the names of the exec-
utives and their positions. Neither of these sen-
tences contains enough information to be listed
as an event under Soderland?s representation,
consequently the MUC events generated from
these sentences fall into the nomatch category.
It was found that there were eighteen events
in the Soderland data set which were not in-
cluded in the MUC version. This is unexpected
since the events in the Soderland corpus should
be a subset of those in the MUC corpus. Anal-
ysis showed that half of these corresponded to
spurious events in the Soderland set which could
not be matched onto events in the text. Many of
these were caused by problems with the BAD-
GER syntactic analyser (Fisher et al, 1995)
used to pre-process the texts before manual
analysis stage in which the events were identi-
fied. Mistakes in this pre-processing sometimes
caused the texts to read as though the sentence
contained an event when it did not. We exam-
ined the MUC texts themselves to determine
whether there was an event rather than relying
on the pre-processed output.
Of the remaining nine events it was found
that the majority (eight) of these corresponded
to events in the text which were not listed in
the MUC data set. These were not identi-
fied as events in the MUC data because of the
the strict guidelines, for example that historical
events and non-permanent management moves
should not be annotated. Examples of these
event types include ?... Jan Carlzon, who left
last year after his plan for a merger with three
other European airlines failed.? and ?Charles
T. Young, chief financial officer, stepped down
voluntarily on a ?temporary basis pending con-
clusion? of the investigation.? The analysis also
identified one event in the Soderland data which
appeared to correspond to an event in the text
but was not listed in the MUC scenario tem-
plate for that document. It could be argued
that there nine events should be added to the
set of MUC events and treated as fully matches.
However, the MUC corpus is commonly used as
a gold standard in IE evaluation and it was de-
cided not to alter it. Analysis indicated that
one of these nine events would have been a full
match and eight partial matches.
It is worth commenting that the analysis car-
ried out here found errors in both data sets.
There appeared to be more of these in the
Soderland data but this may be because the
event structures are much easier to interpret
and so errors can be more readily identified. It is
also difficult to interpret the MUC guidelines in
some cases and it sometimes necessary to make
a judgement over how they apply to a particular
event.
4.2 Event Field Analysis
A more detailed analysis can be carried out
examining the matches between each of the
four fields in the event representation individu-
ally. There are 1,094 fields in the MUC data.
Although there are 276 events in that data
set seven of them do not mention a post and
three omit the organisation name. (Organisa-
tion names are omitted from the template when
the text mentions an organisation description
rather than its name.)
Table 4.2 lists the number of matches for each
of the four event fields across the two data sets.
Each of the pairs of numbers in the main body
of the table refers to the number of matching in-
stances of the relevant field and the total num-
ber of instances in the MUC data.
The column headed ?Full match? lists the
MUC events which were fully matched against
the Soderland data and, as would be expected,
all fields are matched. The column marked
?Partial match? lists the MUC events which
are matched onto Soderland fields via partially
matching events. The column headed ?No-
match? lists the event fields for the 56 MUC
events which are not represented at all in the
Soderland data.
Of the total 1,094 event fields in the MUC
data 727, 66.5%, can be found in the Soderland
data. The rightmost column lists the percent-
ages of each field for which there was a match.
The counts for the type and person fields are the
same since the type and person fields are com-
bined in Soderland?s event representation and
hence can only occur together. These figures
also show that there is a wide variation between
the proportion of matches for the different fields
with 76.8% of the person and type fields be-
ing matched but only 43.2% of the organisation
field.
This difference between fields can be ex-
plained by looking at the style in which the texts
forming the MUC evaluation corpus are writ-
ten. It is very common for a text to introduce
a management succession event near the start
of the newswire story and this event almost in-
variably contains all four event fields. For ex-
ample, one story starts with the following sen-
tence: ?Washington Post Co. said Katharine
Graham stepped down after 20 years as chair-
man, and will be succeeded by her son, Don-
ald E. Graham, the company?s chief executive
officer.? Later in the story further succession
events may be mentioned but many of these use
an anaphoric expression (e.g. ?the company?)
rather than explicitly mention the name of the
organisation in the event. For example, this sen-
tence appears later in the same story: ?Alan G.
Spoon, 42, will succeed Mr. Graham as presi-
dent of the company.? Other stories again may
only mention the name of the person in the suc-
cession event. For example, ?Mr. Jones is suc-
ceeded by Mr. Green? and this explains why
some of the organisation fields are also absent
from the partially matched events.
4.3 Discussion
From some perspectives it is difficult to see why
there is such a difference between the amount
of events which are listed when the entire text
is viewed compared with considering single sen-
tences. After all a text comprises of an ordered
list of sentences and all of the information the
text contains must be in these. Although, as we
have seen, it is possible for individual sentences
to contain information which is difficult to con-
nect with the rest of the event description when
a sentence is considered in isolation.
The results presented here are, to some ex-
tent, dependent on the choices made when rep-
resenting events in the two data sets. The
events listed in Soderland?s data require a min-
imal amount of information to be contained
within a sentence for it to be marked as con-
taining information about a management suc-
cession event. Although it is difficult to see how
any less information could be viewed as repre-
senting even part of a management succession
event.
5 Related Work
Huttunen et al (2002) found that there is varia-
tion between the complexity of IE tasks depend-
ing upon how the event descriptions are spread
through the text and the ways in which they are
encoded linguistically. The analysis presented
here is consistent with their finding as it has
Full match Partial match Nomatch TOTAL %
Type 112 / 112 100 / 108 0 / 56 212 / 276 76.8%
Person 112 / 112 100 / 108 0 / 56 212 / 276 76.8%
Org 112 / 112 6 / 108 0 / 53 118 / 273 43.2%
Post 111 / 111 74 / 108 0 / 50 185 / 269 68.8%
Total 447 / 447 280 / 432 0 / 215 727 / 1094 66.5%
Table 2: Matches between MUC and Soderland data at field level
been observed that the MUC texts are often
written in such as way that the name of the
organisation in the event is in a different part
of the text to the rest of the organisation de-
scription and the entire event can only be con-
structed by resolving anaphoric expressions in
the text. The choice over which information
about events should be extracted could have an
effect on the difficulty of the IE task.
6 Conclusions
It seems that the majority of events are not fully
described within a single sentence, at least for
one of the most commonly used IE evaluation
sets. Only around 40% of events in the original
MUC data set were fully expressed within the
Soderland data set. It was also found that there
is a wide variation between different event fields
and some information may be more difficult to
extract from text when the possibility of events
being described across multiple sentences is not
considered. This observation should be borne
in mind when deciding which approach to use
for a particular IE task and should be used to
put the results reported for IE systems which
extract from a single sentence into context.
Acknowledgements
I am grateful to Stephen Soderland for allowing
access to his version of the MUC-6 corpus and
advice on its construction. Robert Gaizauskas
and Beth Sundheim also provided advice on the
data used in the MUC evaluation. Mark Hep-
ple provided valuable comments on early drafts
of this paper. I am also grateful to an anony-
mous reviewer who provided several useful sug-
gestions.
References
H. Chieu and H. Ng. 2002. A Maximum
Entroy Approach to Information Extraction
from Semi-structured and Free Text. In Pro-
ceedings of the Eighteenth International Con-
ference on Artificial Intelligence (AAAI-02),
pages 768?791, Edmonton, Canada.
D. Fisher, S. Soderland, J. McCarthy, F. Feng,
and W. Lehnert. 1995. Description of the
UMass system as used for MUC-6. In Pro-
ceedings of the Sixth Message Understand-
ing Conference (MUC-6), pages 221?236, San
Francisco, CA.
R. Grishman and B. Sundheim. 1996. Mes-
sage understanding conference - 6 : A brief
history. In Proceedings of the 16th Interna-
tional Conference on Computational Linguis-
tics (COLING-96), pages 466?470, Copen-
hagen, Denmark.
S. Huttunen, R. Yangarber, and R. Grishman.
2002. Complexity of Event Structures in IE
Scenarios. In Proceedings of the 19th Interna-
tional Conference on Computational Linguis-
tics (COLING-2002), pages 376?382, Taipei,
Taiwan.
MUC. 1995. Proceedings of the Sixth Mes-
sage Understanding Conference (MUC-6),
San Mateo, CA. Morgan Kaufmann.
S. Soderland. 1999. Learning Information Ex-
traction Rules for Semi-structured and free
text. Machine Learning, 31(1-3):233?272.
B. Sundheim. 1995. Overview of results of
the MUC-6 evaluation. In Proceedings of
the Sixth Message Understanding Conference
(MUC-6), pages 13?31, Columbia, MA.
D. Zelenko, C. Aone, and A. Richardella. 2003.
Kernel methods for relation extraction. Jour-
nal of Machine Learning Research, 3:1083?
1106.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 809?816
Manchester, August 2008
Acquiring Sense Tagged Examples using Relevance Feedback
Mark Stevenson, Yikun Guo and Robert Gaizauskas
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield, S1 4DP
United Kingdom
inital.surname@dcs.shef.ac.uk
Abstract
Supervised approaches to Word Sense Dis-
ambiguation (WSD) have been shown to
outperform other approaches but are ham-
pered by reliance on labeled training ex-
amples (the data acquisition bottleneck).
This paper presents a novel approach to the
automatic acquisition of labeled examples
for WSD which makes use of the Informa-
tion Retrieval technique of relevance feed-
back. This semi-supervised method gener-
ates additional labeled examples based on
existing annotated data. Our approach is
applied to a set of ambiguous terms from
biomedical journal articles and found to
significantly improve the performance of a
state-of-the-art WSD system.
1 Introduction
The resolution of lexical ambiguities has long been
considered an important part of the process of
understanding natural language. Supervised ap-
proaches to Word Sense Disambiguation (WSD)
have been shown to perform better than unsuper-
vised ones (Agirre and Edmonds, 2007) but require
examples of ambiguous words used in context an-
notated with the appropriate sense (labeled exam-
ples). However these often prove difficult to obtain
since manual sense annotation of text is a complex
and time consuming process. In fact, Ng (1997)
estimated that 16 person years of manual effort
would be required to create enough labeled exam-
ples to train a wide-coverage WSD system. This
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
limitation is commonly referred to as the data ac-
quisition bottleneck. It is particularly acute in spe-
cific domains, such as biomedicine, where terms
may have technical usages which only domain ex-
perts are likely to be aware of. For example, pos-
sible meanings of the term ?ganglion? in UMLS
(Humphreys et al, 1998) include ?neural structure?
or ?benign mucinous tumour?, although only the
first meaning is listed in WordNet. These domain-
specific semantic distinctions make manual sense
annotation all the more difficult.
One approach to the data acquisition bottleneck
is to generate labeled training examples automat-
ically. Others, such as Leacock et al (1998) and
Agirre and Mart??nez (2004b), used information
from WordNet to construct queries which were
used to retrieve training examples. This paper
presents a novel approach to this problem. Rele-
vance feedback, a technique used in Information
Retrieval (IR) to improve search results, is adapted
to identify further examples for each sense of am-
biguous terms. These examples are then used
to train a semi-supervised WSD system either by
combining them with existing annotated data or
using them alone. The approach is applied to a set
of ambiguous terms in biomedical texts, a domain
for which existing resources containing labeled ex-
amples, such as the NLM-WSD data set (Weeber
et al, 2001), are limited.
The next section outlines previous techniques
which have been used to avoid the data acquisi-
tion bottleneck. Section 3 describes our approach
based on relevance feedback. The WSD system we
use is described in Section 4. Section 5 describes
experiments carried out to determine the useful-
ness of the automatically retrieved examples. The
final section summarises conclusions which can be
drawn from this work and outlines future work.
809
2 Previous Approaches
A variety of approaches to the data acquisition bot-
tleneck have been proposed. One is to use un-
supervised algorithms, which do not require la-
beled training data. Examples include Lesk (1986)
who disambiguated ambiguous words by examin-
ing their dictionary definitions and selecting the
sense whose definition overlapped most with def-
initions of words in the ambiguous word?s con-
text. Leroy and Rindflesch (2005) presented an
unsupervised approach to WSD in the biomedi-
cal domain using information derived from UMLS
(Humphreys et al, 1998).
However, results from SemEval (Agirre et al,
2007) and its predecessors have shown that su-
pervised approaches to WSD generally outperform
unsupervised ones. It has also been shown that re-
sults obtained from supervised methods improve
with access to additional labeled data for training
(Ng, 1997). Consequently various techniques for
automatically generating training data have been
developed.
One approach makes use of the fact that differ-
ent senses of ambiguous words often have different
translations (e.g. Ng et al (2003)). Parallel text is
used as training data with the alternative transla-
tions serving as sense labels. However, disadvan-
tages of this approach are that the alternative trans-
lations do not always correspond to the sense dis-
tinctions in the original language and parallel text
is not always available.
Another approach, developed by Leacock et
al. (1998) and extended by Agirre and Mart??nez
(2004b), is to examine a lexical resource, Word-
Net in both cases, to identify unambiguous terms
which are closely related to each of the senses of an
ambiguous term. These ?monosemous relatives?
are used to as query terms for a search engine and
the examples returned used as additional training
data.
In the biomedical domain, Humphrey et al
(2006) use journal descriptors to train models
based on the terms which are likely to co-occur
with each sense. Liu et al (2002) used informa-
tion in UMLS to disambiguate automatically re-
trieved examples which were then used as labeled
training data. The meanings of 35 ambiguous ab-
breviations were identified by examining the close-
ness of concepts in the same abstract in UMLS.
Widdows et al (2003) employ a similar approach,
although their method also makes use of parallel
corpora when available.
All of these approaches rely on the existence of
an external resource (e.g. parallel text or a domain
ontology). In this paper we present a novel ap-
proach, inspired by the relevance feedback tech-
nique used in IR, which automatically identifes ad-
ditional training examples using existing labeled
data.
3 Generating Examples using Relevance
Feedback
The aim of relevance feedback is to generate im-
proved search queries based on manual analysis of
a set of retrieved documents which has been shown
to improve search precision (Salton, 1971; Robert-
son and Spark Jones, 1976). Variations of rele-
vance feedback have been developed for a range of
IR models including Vector Space and probabilis-
tic models. The formulation of relevance feedback
for the Vector Space Model is most pertinent to our
approach.
Given a collection of documents, C, containing
a set of terms, C
terms
, a basic premise of the Vec-
tor Space Model is that documents and queries can
be represented by vectors whose dimensions repre-
sent the C
terms
. Relevance feedback assumes that
a retrieval system returns a set of documents, D,
for some query, q. It is also assumed that a user
has examined D and identified some of the docu-
ments as relevant to q and others as not relevant.
Relevant documents are denoted by D
+q
and the
irrelevant as D
?q
, where D
+q
? D, D
?q
? D
and D
+q
?D
?q
= ?. This information is used to
create a modified query, q
m
, which should be more
accurate than q. A standard approach to construct-
ing q
m
was described by Rocchio (1971):
q
m
= ?q+
?
|D
+q
|
?
?d?D
+q
d ?
?
|D
?q
|
?
?d?D
?q
d (1)
where the parameters ?, ? and ? are set for partic-
ular applications. Rocchio (1971) set ? to 1.
Our scenario is similar to the relevance feedback
problem since the sense tagged examples provide
information about the documents in which a par-
ticular meaning of an ambiguous term is likely to
be found. By identifying the features which dis-
tinguish the documents containing one sense from
the others we can create queries which can then be
used to retrieve further examples of the ambiguous
words used in the same sense. However, unlike
810
score(t, s) = idf(t)?
?
?
?
|D
+s
|
?
?d?D
+s
count(t, d)?
?
|D
?s
|
?
?d?D
?s
count(t, d)
?
?
(2)
the relevance feedback scenario there is no origi-
nal query to modify. Consequently we start with
a query containing just the ambiguous term and
use relevance feedback to generate queries which
aim to retrieve documents where that term is being
used in a particular sense.
The remainder of this section describes how this
approach is applied in more detail.
3.1 Corpus Analysis
The first stage of our process is to analyse the la-
beled examples and identify good search terms.
For each sense of an ambiguous term, s, the la-
beled examples are divided into two sets: those
annotated with the sense in question and the re-
mainder (annotated with another sense). In rele-
vance feedback terminology the documents anno-
tated with the sense in question are considered to
be relevant and the remainder irrelevant. These ex-
amples are denoted by D
+s
and D
?s
respectively.
At its core relevance feedback, as outlined
above, aims to discover how accurately each term
in the collection discriminates between relevant
and irrelevant documents. This approach was used
to inspire a technique for identifying terms which
are likely to indicate the sense in which an am-
biguous word is being used. We compute a single
score for each term, reflecting its indicativeness of
that sense, using the formula in equation 2, where
count(t, d) is the number of times term t occurs in
document d and idf(t) is the inverse document fre-
quency term weighting function commonly used in
IR. We compute idf as follows:
idf(t) = log
|C|
df(t)
(3)
where D is the set of all annotated examples (i.e.
D = D
+s
? D
?s
) and df(t) the number of docu-
ments in C which contain t.
1
In our experiments the ? and ? parameters in
equation 2 are set to 1. Documents are lemma-
tised and stopwords removed before computing
relevance scores.
1
Our computation of idf(t) is based on only information
from the labeled examples, i.e. we assume C = D
+s
?D
?s
.
Alternatively idf could be computed over a larger corpus of
labeled and unlabeled examples.
Table 1 shows the ten terms with the highest
relevance score for two senses of the term ?cul-
ture? in UMLS: ?laboratory culture? (?In periph-
eral blood mononuclear cell culture streptococcal
erythrogenic toxins are able to stimulate trypto-
phan degradation in humans?) and ?anthropolog-
ical culture? (?The aim of this paper is to de-
scribe the origins, initial steps and strategy, cur-
rent progress and main accomplishments of intro-
ducing a quality management culture within the
healthcare system in Poland.?).
?anthropological culture? ?laboratory culture?
cultural 26.17 suggest 6.32
recommendation 14.82 protein 6.13
force 14.80 presence 5.86
ethnic 14.79 demonstrate 5.86
practice 14.76 analysis 5.78
man 14.76 gene 5.58
problem 13.04 compare 5.47
assessment 12.94 level 5.36
experience 11.60 response 5.35
consider 11.58 data 5.35
Table 1: Relevant terms for two senses of ?culture?
3.2 Query Generation
Unlike the traditional formulation of relevance
feedback there is no initial query. To create a
query designed to retrieve examples of each sense
we simply combine the ambiguous term and the
n terms with the highest relevance scores. We
found that using the three highest ranked terms
provided good results. So, for example, the queries
generated for the two senses of culture shown
in Table 1 would be ?culture cultural
recommendation force? and ?culture
suggest protein presence?.
3.3 Example Collection
The next stage is to collect a set of examples using
the generated queries. We use the Entrez retrieval
system (http://www.ncbi.nlm.nih.gov/
sites/gquery) which provides an online in-
terface for carrying out boolean queries over the
PubMed database of biomedical journal abstracts.
Agirre and Mart??nez (2004b) showed that it is
important to preserve the bias of the original cor-
pus when automatically retrieving examples and
811
consequently the number retrieved for each sense
is kept in proportion to the original corpus. For
example, if our existing labeled examples contain
75 usages of ?culture? in the ?laboratoy culture?
sense and 25 meaning ?anthropological culture? we
would ensure that 75% of the examples returned
would refer to the first sense and 25% to the sec-
ond.
Unsurprisingly, we found that the most useful
abstracts for a particular sense are the ones which
contain more of the relevant terms identified using
the process in Section 3.1. However, if too many
terms are included Entrez may not return any ab-
stracts. To ensure that a sufficient number of ab-
stracts are returned we implemented a process of
query relaxation which begins by querying Entrez
with the most specific query for set of terms. If that
query matches enough abstracts these are retrieved
and the search for labeled examples for the rele-
vant sense considered complete. However, if that
query does not match enough abstracts it is relaxed
and Entrez queried again. This process is repeated
until enough examples can be retrieved for a par-
ticular sense.
The process of relaxing queries is carried out as
follows. Assume we have an ambiguous term, a,
and a set of terms T identified using the process
in Section 3.1. The first, most specific query,
is formed from the conjunction of all terms in
a ? T , i.e. ?a and t
1
AND t
2
AND ... t
|T |
?.
This is referred to as the level |T | query. If
this query does not return enough abstracts the
more relaxed level |T | ? 1 query is formed.
This query returns documents which include the
ambiguous word and all but one of the terms in T :
?a AND ((t
1
AND t
2
AND ... AND t
n?1
) OR
(t
1
AND t
2
AND ... t
n?2
AND t
n
) OR ... OR
(t
2
AND t
3
... AND t
n
))?. Similarly, level
|T | ? 2 queries return documents containing the
ambiguous term and all but two of the terms
in T . Level 1 queries, the most relaxed, return
documents containing the ambiguous term and
one of the terms in T . We do not use just the
ambiguous term as the query since this does not
contain any information which could discriminate
between the possible meanings. Figure 1 shows
the queries which are formed for the ambigu-
ous term ?culture? and the three most salient
terms identified for the ?anthropological culture?
sense. The ?matches? column lists the number
of PubMed abstracts the query matches. It can
be seen that there are no matches for the level 3
query and 83 for the more relaxed level 2 query.
For this sense, abstracts returned by the level 2
query would be used if 83 or fewer examples were
required, otherwise abstracts returned by the level
1 query would be used.
Note that the queries submitted to Entrez are re-
stricted so the terms only match against the title
and abstract of the PubMed articles. This avoids
spurious matches against other parts of the records
including metadata and authors? names.
4 WSD System
The basis of our WSD system was developed by
Agirre and Mart??nez (2004a) and participated in
the Senseval-3 challenge (Mihalcea et al, 2004)
with a performance which was close to the best
system for the English and Basque lexical sample
tasks. The system has been adapted to the biomed-
ical domain (Stevenson et al, 2008) and has the
best reported results over the NLM-WSD corpus
(Weeber et al, 2001), a standard data set for eval-
uation of WSD algorithms in this domain.
The system uses a wide range of features which
are commonly employed for WSD:
Local collocations: A total of 41 features which
extensively describe the context of the ambiguous
word and fall into two main types: (1) bigrams
and trigrams containing the ambiguous word con-
structed from lemmas, word forms or PoS tags,
and (2) preceding/following lemma/word-form of
the content words (adjective, adverb, noun and
verb) in the same sentence with the target word.
Syntactic Dependencies: This feature mod-
els longer-distance dependencies of the ambiguous
words than can be represented by the local colloca-
tions. Five relations are extracted: object, subject,
noun-modifier, preposition and sibling. These are
identified using heuristic patterns and regular ex-
pressions applied to PoS tag sequences around the
ambiguous word (Agirre and Mart??nez, 2004a).
Salient bigrams: Salient bigrams within the ab-
stract with high log-likelihood scores, as described
by Pedersen (2001).
Unigrams: Lemmas of all content words
(nouns, verbs, adjectives, adverbs) in the target
word?s sentence and, as a separate feature, lem-
mas of all content words within a 4-word window
around the target word, excluding those in a list
of corpus-specific stopwords (e.g. ?ABSTRACT?,
?CONCLUSION?). In addition, the lemmas of any
812
Level Matches Query
3 0 culture AND (cultural AND recommendation AND force)
2 83 culture AND ((cultural AND recommendation) OR (cultural AND force) OR
(recommendation AND force))
1 6,358 culture AND (cultural OR recommendation OR force)
Figure 1: Examples of various query levels
unigrams which appear at least twice in the en-
tire corpus which are found in the abstract are also
included as features. This feature was not used
by Agirre and Mart??nez (2004a), but Joshi et al
(2005) found them to be useful for this task.
Features are combined using the Vector Space
Model, a memory-based learning algorithm (see
Agirre and Mart??nez (2004a)). Each occurrence
of an ambiguous word is represented as a binary
vector in which each position indicates the oc-
currence/absence of a feature. A single centroid
vector is generated for each sense during training.
These centroids are compared with the vectors that
represent new examples using the cosine metric to
compute similarity. The sense assigned to a new
example is that of the closest centroid.
5 Experiments
5.1 Setup
The NLM-WSD corpus Weeber et al (2001) was
used for evaluation. It contains 100 examples of 50
ambiguous terms which occur frequently in MED-
LINE. Each example consists of the abstract from
a biomedical journal article which contains an in-
stance of the ambiguous terms which has been
manually annotated with a UMLS concept.
The 50 ambiguous terms which form the NLM-
WSD data set represent a range of challenges for
WSD systems. Various researchers (Liu et al,
2004; Leroy and Rindflesch, 2005; Joshi et al,
2005; McInnes et al, 2007) chose to exclude some
of the terms (generally those with highly skewed
sense distributions or low inter-annotator agree-
ment) and evaluated their systems against a subset
of the terms. The number of terms in these subsets
range between 9 and 28. The Most Frequent Sense
(MFS) heuristic has become a standard baseline in
WSD (McCarthy et al, 2004) and is simply the ac-
curacy which would be obtained by assigning the
most common meaning of a term to all of its in-
stances in a corpus. The MFS for the whole NLM-
WSD corpus is 78% and ranges between 69.9%
and 54.9% for the various subsets. We report re-
sults across the NLM-WSD corpus and four sub-
sets from the literature for completeness.
The approach described in Section 3 was ap-
plied to the NLM-WSD data set. 10-fold cross
validation is used for all experiments. Conse-
quently 10 instances of each ambiguous term were
held back for testing during each fold and addi-
tional examples generated by examining the 90 re-
maining instances. Three sets of labeled examples
were generated for each fold, containing 90, 180
and 270 examples for each ambiguous term. The
NLM-WSD corpus represents the only reliably la-
beled data to which we have access and is used to
evaluate all approaches (that is, systems trained on
combinations of the NLM-WSD corpus and/or the
automatically generated examples).
5.2 Results
Various WSD systems were created. The ?basic?
system was trained using only the NLM-WSD data
set and was used as a benchmark. Three systems,
?+90?, ?+180? and ?+270? were trained using the
combination of the NLM-WSD data set and, re-
spectively, the 90, 180 and 270 automatically re-
trieved examples for each term. A further three
systems, ?90?, ?180? and ?270? were trained us-
ing only the automatically retrieved examples.
The performance of our system is shown in Ta-
ble 2. The part of the table labeled ?Subsets prop-
erties? lists the number of terms in each subset of
the NLM-WSD corpus and the relevant MFS base-
line.
Adding the first 90 automatically retrieved ex-
amples (?+90? column) significantly improves per-
formance of our system from 87.2%, over all
words, to 88.5% (Wilcoxon Signed Ranks Test,
p < 0.01). Improvements are observed over
all subsets of the NLM-WSD corpus. Although
the improvements may seem modest they should
be understood in the context of the WSD system
we are using which has exceeded previously re-
ported performance figures and therefore repre-
sents a high baseline.
Table 2 also shows that adding more auto-
matically retrieved examples (?+180? and ?+270?
columns) causes a drop in performance and re-
813
Subset Properties Combined New only
Subset Terms MFS
basic
+90 +180 +270 90 180 270
All words 50 78.0 87.2 88.5 87.0 86.1 85.6 84.5 82.7
Joshi et. al. 28 66.9 82.3 83.8 81.6 80.9 79.8 78.0 76.3
Liu et. al. 22 69.9 77.8 79.6 76.9 76.1 74.9 72.0 70.9
Leroy 15 55.3 84.3 85.9 84.4 83.6 81.2 80.0 78.0
McInnes et. al. 9 54.9 79.6 81.8 80.4 79.4 75.2 73.0 71.4
Table 2: Performance of system using a variety of combinations of training examples
sults using these examples are worse than using the
NLM-WSD corpus alone. The query relaxation
process, outlined in Section 3.3, uses less discrim-
inating queries when more examples are required
and it is likely that this is leading to noise in the
training examples.
The rightmost portion of Table 2 shows perfor-
mance when the system is trained using only the
automatically generated examples which is con-
sistently worse than using the NLM-WSD corpus
alone. Performance also decreases as more exam-
ples are added. However, results obtained using
only the automatically generated training exam-
ples are consistently better than the relevant base-
line.
Table 3 shows the performance of the sys-
tem trained on the NLM-WSD data set compared
against training using only the 90 automatically
generated examples for each ambiguous term in
the NLM-WSD corpus. It can be seen that there
is a wide variation between the performance of
the additional examples compared with the origi-
nal corpus. For 11 terms training using the addi-
tional examples alone is more effective than using
the NLM-WSD corpus. However, there are several
words for which the performance using the auto-
matically acquired examples is considerably worse
than using the NLM-WSD corpus.
Information about the performance of a system
trained using only the 90 automatically acquired
examples can be used to boost WSD performance
further. In this scenario, which we refer to as ex-
ample filtering, the system has a choice whether
to make use of the additional training data or not.
For each word, performance of the WSD system
trained using only the 90 automatically acquired
examples is compared against the one trained on
the NLM-WSD data set (i.e. results shown in Ta-
ble 3). If the performance is as good, or better,
then the additional examples are used, otherwise
only examples in the NLM-WSD corpus are used
as training data. Since the annotated examples in
the NLM-WSD corpus have already been exam-
ined to generate the additional examples, example
filtering does not require any more labeled data.
Results obtained when example filtering is used
are shown in Table 4. The columns ?+90(f)?,
?+180(f)? and ?+270(f)? show performance when
the relevant set of examples is filtered. (Note
that all three sets of examples are filtered against
the performance of the first 90 examples, i.e. re-
sults shown in Table 3.) This table shows that
example filtering improves performance when the
WSD system is trained using the automatically re-
trieved examples. Performance using the first 90
filtered examples (?+90(f)? column) is 89%, over
all words, compared with 88.5% when filtering is
not used. While performance decreases as larger
sets of examples are used, results using each of the
three sets of filtered examples is signifcantly bet-
ter than the basic system (Wilcoxon Signed Ranks
Test, p < 0.01 for ?+90(f)? and ?+180(f)?, p <
0.05 for ?+270(f)?).
6 Conclusion and Future Work
This paper has presented a novel approach to the
data acquisition bottleneck for WSD. Our tech-
nique is inspired by the relevance feedback tech-
nique from IR. This is a semi-supervised approach
which generates labeled examples using available
sense annotated data and, unlike previously pub-
lished approaches, does not rely on external re-
sources such as parallel text or an ontology. Eval-
uation was carried out on a WSD task from the
biomedical domain for which the number of la-
beled examples available for each ambiguous term
is limited. The automatically acquired examples
improve the performance of a WSD system which
has already been shown to exceed previously pub-
lished results.
The approach presented in this paper could be
extended in several ways. Our experiments focus
814
basic +90(f) +180(f) +270(f)
All words 87.2 89.0 88.2 87.9
Joshi et. al. 82.3 84.6 83.5 83.3
Liu et. al. 84.3 86.6 85.7 85.5
Leroy 77.8 80.3 79.1 78.5
McInnes et. al. 79.6 82.4 81.6 80.8
Table 4: Performance using example filtering
on the biomedical domain. The relevance feedback
approach could be applied to other lexical ambi-
guities found in biomedical texts, such as abbre-
viations with multiple expansions (e.g. Liu et al
(2002)), or to WSD of general text, possibly using
the SemEval data for evaluation.
Future work will explore alternative methods for
generating query terms including other types of
relevance feedback and lexical association mea-
sures (e.g. Chi-squared and mutual information).
Experiments described here rely on a boolean IR
engine (Entrez). It is possible that an IR sys-
tem which takes term weights into account could
lead to the retrieval of more useful MEDLINE ab-
stracts. Finally, it would be interesting to explore
the relation between query relaxation and the use-
fulness of the retrieved abstracts.
Acknowledgments
The authors are grateful to David Martinez for the
use of his WSD system for these experiments and
to feedback provided by three anonymous review-
ers. This work was funded by the UK Engineer-
ing and Physical Sciences Research Council, grant
number EP/E004350/1.
References
E. Agirre and P. Edmonds, editors. 2007. Word
Sense Disambiguation: Algorithms and Applica-
tions. Text, Speech and Language Technology.
Springer.
E. Agirre and D. Mart??nez. 2004a. The Basque Coun-
try University system: English and Basque tasks. In
Rada Mihalcea and Phil Edmonds, editors, Senseval-
3: Third International Workshop on the Evaluation
of Systems for the Semantic Analysis of Text, pages
44?48, Barcelona, Spain, July.
E. Agirre and D. Mart??nez. 2004b. Unsupervised WSD
based on automatically retrieved examples: The im-
portance of bias. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-04), Barcelona, Spain.
E. Agirre, L. Marquez, and R. Wicentowski, editors.
2007. SemEval 2007: Proceedings of the 4th
International Workshop on Semantic Evaluations,
Prague, Czech Republic.
S. Humphrey, W. Rogers, H. Kilicoglu, D. Demner-
Fushman, and T. Rindflesch. 2006. Word Sense
Disambiguation by selecting the best semantic type
based on Journal Descriptor Indexing: Preliminary
experiment. Journal of the American Society for In-
formation Science and Technology, 57(5):96?113.
L. Humphreys, D. Lindberg, H. Schoolman, and
G. Barnett. 1998. The Unified Medical Language
System: An Informatics Research Collaboration.
Journal of the American Medical Informatics Asso-
ciation, 1(5):1?11.
M. Joshi, T. Pedersen, and R. Maclin. 2005. A Com-
parative Study of Support Vector Machines Applied
to the Word Sense Disambiguation Problem for the
Medical Domain. In Proceedings of the Second In-
dian Conference on Artificial Intelligence (IICAI-
05), pages 3449?3468, Pune, India.
C. Leacock, M. Chodorow, and G. Miller. 1998.
Using corpus statistics and WordNet relations for
sense identification. Computational Linguistics,
24(1):147?165.
G. Leroy and T. Rindflesch. 2005. Effects of Infor-
mation and Machine Learning algorithms on Word
Sense Disambiguation with small datasets. Interna-
tional Journal of Medical Informatics, 74(7-8):573?
585.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proceedings of
ACM SIGDOC Conference, pages 24?26, Toronto,
Canada.
H. Liu, S. Johnson, and C. Friedman. 2002. Automatic
Resolution of Ambiguous Terms Based on Machine
Learning and Conceptual Relations in the UMLS.
Journal of the American Medical Informatics Asso-
ciation, 9(6):621?636.
H. Liu, V. Teller, and C. Friedman. 2004. A Multi-
aspect Comparison Study of Supervised Word Sense
Disambiguation. Journal of the American Medical
Informatics Association, 11(4):320?331.
815
word basic 90 ?
adjustment 71 70 -1
association 100 100 0
blood pressure 48 50 2
cold 88 86 -2
condition 89 90 1
culture 96 91 -5
degree 96 86 -10
depression 88 85 -3
determination 87 82 -5
discharge 95 92 -3
energy 98 99 1
evaluation 76 75 -1
extraction 85 82 -3
failure 66 71 5
fat 85 83 -2
fit 87 85 -2
fluid 100 100 0
frequency 95 94 -1
ganglion 97 95 -2
glucose 91 92 1
growth 70 67 -3
immunosuppression 79 79 0
implantation 90 88 -2
inhibition 98 98 0
japanese 73 75 2
lead 91 90 -1
man 87 82 -5
mole 95 84 -11
mosaic 87 83 -4
nutrition 53 43 -10
pathology 85 85 0
pressure 94 96 2
radiation 84 82 -2
reduction 89 90 1
repair 87 86 -1
resistance 98 97 -1
scale 86 79 -7
secretion 99 99 0
sensitivity 93 91 -2
sex 87 84 -3
single 99 99 0
strains 92 92 0
support 86 89 3
surgery 97 98 1
transient 99 99 0
transport 93 93 0
ultrasound 87 85 -2
variation 94 89 -5
weight 77 77 0
white 73 74 1
Average 87.2 85.6 -1.58
Table 3: Comparison of performance using orig-
inal training data and 90 automatically generated
examples
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Finding Predominant Senses in Untagged
Text. In Proceedings of the 42nd Annual Meeting of
the Association for Computational Lingusitics (ACL-
2004), pages 280?287, Barcelona, Spain.
B. McInnes, T. Pedersen, and J. Carlis. 2007. Us-
ing UMLS Concept Unique Identifiers (CUIs) for
Word Sense Disambiguation in the Biomedical Do-
main. In Proceedings of the Annual Symposium
of the American Medical Informatics Association,
pages 533?537, Chicago, IL.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004.
The Senseval-3 English lexical sample task. In
Proceedings of Senseval-3: The Third International
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text, Barcelona, Spain.
H. Ng, B. Wang, and S. Chan. 2003. Exploiting Paral-
lel Texts for Word Sense Disambiguation: an Empir-
ical Study. In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics
(ACL-03), pages 455?462, Sapporo, Japan.
H. Ng. 1997. Getting serious about Word Sense Dis-
ambiguation. In Proceedings of the SIGLEX Work-
shop ?Tagging Text with Lexical Semantics: What,
why and how??, pages 1?7, Washington, DC.
T. Pedersen. 2001. A Decision Tree of Bigrams is an
Accurate Predictor of Word Sense. In Proceedings
of the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-01), pages 79?86, Pittsburgh, PA., June.
S. Robertson and K. Spark Jones. 1976. Relevance
weighting of search terms. Journal of the Ameri-
can Society for Information Science and Technology,
27(3):129?146.
J. Rocchio. 1971. Relevance feedback in Information
Retrieval. In G. Salton, editor, The SMART Retrieval
System ? Experiments in Automatic Document Pro-
cessing. Prentice Hall, Englewood Cliffs, NJ.
G. Salton. 1971. The SMART Retrieval System ? Ex-
periments in Automatic Document Processing. Pren-
tice Hall Inc., Englewood Cliffs, NJ.
M. Stevenson, Y. Guo, R. Gaizauskas, and D. Martinez.
2008. Knowledge Sources for Word Sense Disam-
biguation of Biomedical Text. In Proceedings of the
Workshop on Current Trends in Biomedical Natural
Language Processing at ACL 2008, pages 80?87.
M. Weeber, J. Mork, and A. Aronson. 2001. Devel-
oping a Test Collection for Biomedical Word Sense
Disambiguation. In Proceedings of AMAI Sympo-
sium, pages 746?50, Washington, DC.
D. Widdows, S. Peters, S. Cedernerg, C. Chan, D. Stef-
fen, and P. Buitelaar. 2003. Unsupervised Mono-
lingual and Bilingual Word-sense Disambiguation of
Medical Documents using UMLS. In Workshop on
?Natural Langauge Processing in Biomedicine? at
ACL 2003, pages 9?16, Sapporo, Japan.
816
The Interaction of Knowledge Sources 
in Word Sense Disambiguation 
Mark Stevenson 
University of Sheffield 
Yorick Wilks* 
University of Sheffield 
Word sense disambiguation (WSD) is a computational linguistics task likely to benefit from the 
tradition of combining different knowledge sources in artificial in telligence research. An important 
step in the exploration of this hypothesis i to determine which linguistic knowledge sources are 
most useful and whether their combination leads to improved results. We present a sense tagger 
which uses several knowledge sources. Tested accuracy exceeds 94% on our evaluation corpus. 
Our system attempts to disambiguate all content words in running text rather than limiting 
itself to treating a restricted vocabulary of words. It is argued that this approach is more likely to 
assist the creation of practical systems. 
1. Introduction 
Word sense disambiguation (WSD) is a problem long recognised in computational 
linguistics (Yngve 1955) and there has been a recent resurgence of interest, including 
a special issue of this journal devoted to the topic (Ide and V4ronis 1998). Despite this 
there is still a considerable diversity of methods employed by researchers, as well as 
differences in the definition of the problems to be tackled. The SENSEVAL evaluation 
framework (Kilgarriff 1998) was a DARPA-style competition designed to bring some 
conformity to the field of WSD, although it has yet to achieve that aim completely. The 
main sources of divergence are the choice of computational paradigm, the proportion 
of text words disambiguated, the granularity of the meanings assigned to them, and 
the knowledge sources used. We will discuss each in turn. 
Resnik and Yarowsky (1997) noted that, for the most part, part-of-speech tagging is 
tackled using the noisy channel model, although transformation rules and grammatico- 
statistical methods have also had some success. There has been far less consensus 
as to the best approach to WSD. Currently, machine learning methods (Yarowsky 
1995; Rigau, Atserias, and Agirre 1997) and combinations of classifiers (McRoy 1992) 
have been popular. This paper reports a WSD system employing elements of both 
approaches. 
Another source of difference in approach is the proportion of the vocabulary dis- 
ambiguated. Some researchers have concentrated on producing WSD systems that 
base results on a limited number of words, for example Yarowsky (1995) and Schtitze 
(1992) who quoted results for 12 words, and a second group, including Leacock, Tow- 
ell, and Voorhees (1993) and Bruce and Wiebe (1994), who gave results for just one, 
namely interest. But limiting the vocabulary on which a system is evaluated can have 
two serious drawbacks. First, the words used were not chosen by frequency-based 
sampling techniques and so we have no way of knowing whether or not they are 
special cases, a point emphasised by Kilgarriff (1997). Secondly, there is no guarantee 
? Department of Computer Science, 211 Regent Court, Portobello Street, Sheffield $1 4DP, UK 
(~) 2001 Association for Computational Linguistics 
Computational Linguistics Volume 27, Number 3 
that the techniques employed will be applicable when a larger vocabulary is tackled. 
However it is likely that mark-up for a restricted vocabulary can be carried out more 
rapidly since the subject has to learn the possible senses of fewer words. 
Among the researchers mentioned above, one must distinguish between, on the 
one hand, supervised approaches that are inherently limited in performance to the 
words over which they evaluate because of limited training data and, on the other 
hand, approaches whose unsupervised learning methodology is applied to only small 
numbers of words for evaluation, but which could in principle have been used to tag 
all content words in a text. Others, such as Harley and Glennon (1997) and ourselves 
Wilks and Stevenson (1998a, 1998b; Stevenson and Wilks 1999), have concentrated on 
approaches that disambiguate allcontent words. 1In addition to avoiding the problems 
inherent in restricted vocabulary systems, wide coverage systems are more likely to 
be useful for NLP applications, as discussed by Wilks et al (1990). 
A third difference concerns the granularity of WSD attempted, which one can 
illustrate in terms of the two levels of semantic distinctions found in many dictionaries: 
homograph and sense (see Section 3.1). Like Cowie, Guthrie, and Guthrie (1992), we 
shall give results at both levels, but it is worth pointing out that the targets of, say, work 
using translation equivalents (e.g., Brown et al 1991; Gale, Church, and Yarowsky 
1992c; and see Section 2.3) and Roget categories (Yarowsky 1992; Masterman 1957) 
correspond broadly to the wider, homograph, distinctions. 
In this paper we attempt o show that the high level of results more typical of 
systems trained on many instances of a restricted vocabulary can also be obtained 
by large vocabulary systems, and that the best results are to be obtained from an 
optimization of a combination of types of lexical knowledge (see Section 2). 
1.1 Lexical Knowledge and WSD 
Syntactic, semantic, and pragmatic information are all potentially useful for WSD, as 
can be demonstrated by considering the following sentences: 
(1) 
(2) 
(3) 
(4) 
John did not feel well. 
John tripped near the well. 
The bat slept. 
He bought a bat from the sports shop. 
The first two sentences contain the ambiguous word well; as an adjective in (1) 
where it is used in its "state of health" sense, and as a noun in (2), meaning "water 
supply". Since the two usages are different parts of speech they can be disambiguated 
by this syntactic property. 
Sentence (3) contains the word bat, whose nominal readings are ambiguous be- 
tween the "creature" and "sports equipment" meanings. Part-of-speech information 
cannot disambiguate he senses ince both are nominal usages. However, this sentence 
can be disambiguated using semantic information, such as preference r strictions. The 
verb sleep prefers an animate subject and only the "creature" sense of bat is animate. 
So Sentence (3) can be effectively disambiguated by its semantic behaviour but not by 
its syntax. 
1 In this paper we define content words as nouns, verbs, adjectives, and adverbs, although others have 
included other part-of-speech categories (Hirst 1995). 
322 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
A preference restriction will not disambiguate S ntence (4) since the direct object 
preference will be at least as general as physical object, and any restriction on the direct 
object slot of the verb sell would cover both senses. The sentence can be disambiguated 
on pragmatic grounds because it is far more likely that sports equipment will be bought 
in a sports shop. Thus pragmatic information can be used to disambiguate bat to its 
"sports equipment" sense. 
Each of these knowledge sources has been used for WSD and in Section 3 we de- 
scribe a method which performs rough-grained disambiguation using part-of-speech 
information. Wilks (1975) describes a system which performs WSD using semantic 
information in the form of preference restrictions. Lesk (1986) also used semantic in- 
formation for WSD in the form of textual definitions from dictionaries. Pragmatic in- 
formation was used by Yarowsky (1992) whose approach relied upon statistical models 
of categories from Roget's Thesaurus (Chapman, 1977), a resource that had been used 
in much earlier approaches to WSD such as Masterman (1957). 
The remainder of this paper is organised as follows: Section 2 reviews some sys- 
tems which have combined knowledge sources for WSD. In Section 3 we discuss the 
relationship between semantic disambiguation a d part-of-speech tagging, reporting 
an experiment which quantifies the connection. A general WSD system is presented 
in Section 4. In Section 5 we explain the strategy used to evaluate this system, and we 
report the results in Section 6. 
2. Background 
A comprehensive r view of WSD is beyond the scope of this paper but may be 
found in Ide and V4ronis (1998). Combining knowledge sources for WSD is not a 
new idea; in this section we will review some of the systems which have tried to do 
that. 
2.1 McRoy's System 
Early work on coarse-grained WSD based on combining knowledge sources was un- 
dertaken by McRoy (1992). Her work was carried out without the use of machine- 
readable dictionaries (MRD), necessitating the manual creation of the complex set of 
lexicons this system requires. There was a lexicon of 8,775 unique roots, a hierarchy 
of 1,000 concepts, and a set of 1,400 collocational patterns. The collocational patterns 
are automatically extracted from a corpus of text in the same domain as the text being 
disambiguated and senses are manually assigned to each. If the collocation occurs in 
the text being disambiguated, then it is assumed that the words it contains are being 
used in the same senses as were assigned manually. 
Disambiguation makes use of several knowledge sources: frequency information, 
syntactic tags, morphological information, semantic ontext (clusters), collocations and 
word associations, role-related expectations, and selectional restrictions. The knowl- 
edge sources are combined by adding their results. Each knowledge source assigns a 
(possibly negative) numeric value to each of the possible senses. The numerical value 
depends upon the type of knowledge source. Some knowledge sources have only two 
possible values, for example the frequency information has one value for frequent 
senses and another for infrequent ones. The numerical values assigned for each were 
determined manually. The selectional restrictions knowledge source assigns cores in 
the range -10 to +10, with higher scores being assigned to senses that are more specific 
(according to the concept hierarchy). Disambiguation is carried out by summing the 
scores from each knowledge source for all candidate senses and choosing the one with 
the highest overall score. 
323 
Computational Linguistics Volume 27, Number 3 
In a sample of 25,000 words from the Wall Street Journal, the system covered 98% of 
word-occurrences that were not proper nouns and were not abbreviated, emonstrat- 
ing the impressive coverage of the hand-crafted lexicons. No quantitative evaluation 
of the disambiguation quality was carried out due to the difficulty in obtaining an- 
notated test data, a problem made more acute by the use of a custom-built exicon. 
In addition, comparison of system output against manually annotated text had yet to 
become a standard evaluation strategy in WSD research. 
2.2 The Cambridge Language Survey System 
The Cambridge International Dictionary of English (CIDE) (Procter 1995) is a learners' dic- 
tionary which consists of definitions written using a 2,000 word controlled vocabulary. 
(This lexicon is similar to LDOCE, which we use for experiments presented later in this 
paper; it is described in Section 3.1.) The senses in CIDE are grouped by guidewords, 
similar to homographs in LDOCE. It was produced using a large corpus of English 
created by the Cambridge Language Survey (CLS). 
The CLS also produced a semantic tagger (Harley and Glennon 1997), a commer- 
cial product hat tags words in text with senses from their MRD. The tagger consists 
of four sub-taggers running in parallel, with their results being combined after all 
have run. The first tagger uses collocations derived from the CIDE example sentences. 
The second examines the subject codes for all words in a particular sentence and the 
number of matches with other words is calculated. A part-of-speech tagger produced 
in-house by CUP is run over the text and high scores are assigned to senses that 
agree with the syntactic tag assigned. Finally, the selectional restrictions of verbs and 
adjectives are examined. The results of these processes are combined using a simple 
weighting scheme (similar to McRoy's; see Section 2.1). This weighting scheme, in- 
spired by those used in computer chess programs, assigns each sub-process a weight 
in the range -100 to +100 before summing. Unlike McRoy, this approach does not con- 
sider the specificity of a knowledge source in a particular instance but always assigns 
the same overall weight to each. 
Harley and Glennon report 78% correct agging of all content words at the CIDE 
guideword level (which they equate to the LDOCE sense level) and 73% at the sub- 
sense level, as compared to a hand-tagged corpus of 4,000 words. 
2.3 Machine Learning applied to WSD 
An early application of machine learning to the WSD problem was carried out by 
Brown et al (1991). Several different disambiguation cues, such as first noun to the 
left/right and second word to the left/right, were extracted from parallel text. Trans- 
lation differences were used to define the senses, as this approach was used in an 
English-French machine translation system. The parallel text effectively provided su- 
pervised training examples for this algorithm. Nadas et al (1991) used the flip-flop 
algorithm to decide which of the cues was most important for each word by maxi- 
mizing mutual information scores between words. Yarowsky (1996) used an extremely 
rich features et by expanding this set with syntactic relations uch as subject-verb, 
verb-object and adjective-noun relations, part-of-speech n-grams and others. The ap- 
proach was based on the hypothesis that words exhibited "one sense per collocation" 
(Yarowsky 1993). A large corpus was examined to compute the probability of a partic- 
ular collocate occurring with a certain sense and the discriminatory power of each was 
calculated using the log-likelihood ratio. These ratios were used to create a decision 
list, with the most discriminating collocations being preferred. This approach as the 
benefit hat it does not combine the probabilities of the collocates, which are highly 
non-independent knowledge sources. 
324 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Yarowsky (1993) also examined the discriminatory power of the individual knowl- 
edge sources. It was found that each collocation indicated a particular sense with a 
very high degree of reliability, with the most successful--the first word to the left of 
a noun--achieving 99% precision. Yet collocates have limited applicability; although 
precise, they can only be applied to a limited number of tokens. Yarowsky (1995) 
dealt with this problem largely by producing an unsupervised learning algorithm that 
generates probabilistic decision list models of word senses from seed collocates. This 
algorithm achieves 97% correct disambiguation. I  these experiments Yarowsky deals 
exclusively with binary sense distinctions and evaluates his highly effective algorithms 
on small samples of word tokens. 
Ng and Lee (1996) explored an approach to WSD in which a word is assigned 
the sense of the most similar example already seen. They describe this approach as 
"exemplar-based learning" although it is also known as k-nearest neighbor learning. 
Their system is known as LEXAS (LEXical Ambiguity-resolving System), a supervised 
learning approach which requires disambiguated training text. LEXAS was based on 
PEBLS, a publically available xemplar-based learning algorithm. 
A set of features is extracted from disambiguated xample sentences, including 
part-of-speech information, morphological form, surrounding words, local collocates, 
and words in verb-object syntactic relations. When a new, untagged, usage is encoun- 
tered, it is compared with each of the training examples and the distance from each is 
calculated using a metric adopted from Cost and Salzberg (1993). This is calculated as 
the sum of the differences between each pair of features in the two vectors. The differ- 
ences between two values vl and v2 is calculated according to (5), where C1,i represents 
the number of training examples with value Vl that are classified with sense i in the 
training corpus, and C1 the number with value vl in any sense .  C2, i and C2 denote 
similar values and n denotes the total number of senses for the word under consider- 
ation. The sense of the example with the minimum distance from the untagged usage 
is chosen: if there is more than one with the same distance, one is chosen at random 
to break the tie. 
Cl,i C2,i I (5) 
6(Vl, V2) ~- C1 C2 
i=1 
Ng and Lee tested LEXAS on two separate data sets: one used previously in WSD 
research, the other a new, manually tagged, corpus. The common data set was the 
interest corpus constructed by Bruce and Wiebe (1994) consisting of 2,639 sentences 
from the Wall Street Journal, each containing an occurrence of the noun interest. Each 
occurrence is tagged with one of its six possible senses from LDOCE. Evaluation is 
carried out through 100 random trials, each trained on 1,769 sentences and tested on 
the 600 remaining sentences. The average accuracy was 87.4%, significantly higher 
than the figure of 78% reported by Bruce and Wiebe. 
Further evaluation was carried out on a larger data set constructed by Ng and 
Lee. This consisted of 192,800 occurrences of the 121 nouns and 70 verbs that are "the 
most frequently occurring and ambiguous words in English" (Ng and Lee 1996, 44). 
The corpus was made up from the Brown Corpus (Ku~era nd Francis 1967) and the 
Wall Street Journal Corpus and was tagged with the correct senses from WordNet 
by university undergraduates specializing in linguistics. Before training, two subsets 
of the corpus were put aside as test sets: the first (B?50) contains 7,119 occurrences 
of the ambiguous words from the Brown Corpus, while the second (WSd6) contained 
14,139 from the Wall Street Journal Corpus. LEXAS correctly disambiguated 54% of 
words in BCS0 and 68.6% in WSJ6. Ng and Lee point out that both results are higher 
than choosing the first, or most frequent, sense in each of the corpora. The authors 
325 
Computational Linguistics Volume 27, Number 3 
Table 1 
Relative contribution of knowledge sources in LEXAS. 
Knowledge Source Accuracy 
Collocations 80.2% 
PoS and Morphology 77.2% 
Surrounding words 62.0% 
Verb-object 43.5% 
attribute the lower performance on the Brown Corpus to the wider variety of text 
types it contains. 
Ng and Lee attempted to determine the relative contribution of each knowledge 
source. This was carried out by re-running the data from the "interest" corpus through 
the learning algorithm, this time removing all but one set of features. The results are 
shown in Table 1. They found that the local collocations were the most useful knowl- 
edge source in their system. However, it must be remembered that this experiment 
was carried out on a data set consisting of a single word and may, therefore, not be 
generalizable. 
2.4 Discussion 
This review has been extremely brief and has not covered large areas of research into 
WSD. For example, we have not discussed connectionist approaches, as used by Waltz 
and Pollack (1985), V6ronis and Ide (1990), Hirst (1987), and Cottrell (1984), However, 
we have attempted to discuss some of the approaches to combining diverse types of 
linguistic knowledge for WSD and have concentrated on those which are related to 
the techniques used in our own disambiguation system. 
Of central interest to our research is the relative contribution of the various knowl- 
edge sources which have been applied to the WSD problem. Both Ng and Lee (1996) 
and Yarowsky (1993) reported some results in the area. However, Ng and Lee reported 
results for only a single word and Yarowsky considers only words with two possible 
senses. This paper is an attempt to increase the scope of this research by discussing 
a disambiguation algorithm which operates over all content words and combines a
varied set of linguistic knowledge sources. In addition, we examine the relative ffect 
of each knowledge source to gauge which are the most important, and under what 
circumstances. 
We first report an in-depth study of a particular knowledge source, namely part- 
of-speech tags. 
3. Part of Speech and Word Senses 
3.1 LDOCE 
The experiments described in this section use the Longman Dictionary of Contemporary 
English (LDOCE) (Procter 1978). LDOCE is a learners' dictionary, designed for students 
of English, containing roughly 36,000 word types. LDOCE was innovative in its use 
of a defining vocabulary of 2,000 words with which the definitions were written. If 
a learner of English could master this small core then, it was assumed, they could 
understand every entry in the dictionary. 
In LDOCE, the senses for each word type are grouped into homographs: ets of 
senses with related meanings. For example, one of the homographs of bank means 
326 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
bank  1 n I land along the side of a river, lake, etc. 2 earth which is heaped up in a 
field or a garden, often making a border or division 3 a mass of snow, mud, clouds, 
etc.: The banks of dark cloud promised a heavy storln 4 a slope made at bends in a road or 
race-track, so that they are safer for cars to go round 5 SANDBANK: The Dogger Bank 
in the North Sea can be dangerous for ships 
bank  2 v \[If~\] (of a car or aircraft) to move with one side higher than the other, esp. 
when making a turn - see also BANK UP 
bank  3 n 1 a row, esp. of OARs in an ancient boat or KEYs on a TYPEWRITER 
bank  4 n I a place where money is kept and paid out on demand, and where related 
activities go on - see picture at STREET 2 (usu. in comb.) a place where something is
held ready for use, esp. ORGANIC product of human origin for medical use: Hospital 
bloodbanks have saved many lives 3 (a person who keeps) a supply of money or pieces 
for payment or use in a game of chance 4 break the bank to win all the money that 
the BANK4(3) has in a game of chance 
bank  5 v 1\[T1\] to put or keep (money) in a bank 2\[L9, esp. with\] to keep one's money 
(esp. in the stated bank): Where do you bank? 
Figure 1 
The entry for bank in LDOCE (slightly simplified for clarity). 
roughly "things piled up", with different senses distinguishing exactly what is piled 
(see Figure 1). If the senses are sufficiently close together in meaning there will be 
only one homograph for that word, which we then call monohomographic. However, if 
the senses are far enough apart, as in the bank case, they will be grouped into separate 
homographs,  which we call polyhomographic. 
As can be seen from the example ntry, each LDOCE homograph includes informa- 
tion about the part of speech with which the homograph is marked and that applies 
to each of the senses within that homograph.  The vast majority of homographs in 
LDOCE are marked with a single part of speech; however, about 2% of word types in 
the dictionary contain a homograph that is marked with more than one part of speech 
(e.g., noun or verb), meaning that either part of speech may apply. 
Although the granularity of the distinction between homographs in LDOCE is 
rather coarse-grained, they are, as we noted at the beginning of this paper, an appro- 
priate level for many practical computational linguistic applications. For example, bank 
in the sense of "financial institution" translates to banque in French, but when used 
in the "edge of r iver" sense it translates as bord. This level of semantic disambigua- 
tion is frequently sufficient for choosing the correct arget word in an English-to-French 
Machine Translation system and is at a similar level of granularity to the sense distinc- 
tions explored by other researchers in WSD, for example Brown et al (1991), Yarowsky 
(1996), and McRoy (1992) (see Section 2). 
327 
Computational Linguistics Volume 27, Number 3 
3.2 Using Part-of-Speech Information to Resolve Senses 
We began by examining the potential usefulness of part-of-speech information for 
sense resolution. It was found that 34% of the content-word types in LDOCE were 
polysemous, and 12% polyhomographic. (Polyhomographic words are necessarily pol- 
ysemous ince each homograph is a non-empty set of senses.) If we assume that the 
part of speech of each polyhomographic word in context is known, then 88% of word 
types would be disambiguated to the homograph level. (In other words, 88% do not 
have two homographs with the same part of speech.) Some words will be disam- 
biguated to the homograph level if they are used in a certain part of speech but not 
others. For example, beam has 3 homographs in LDOCE; the first two are marked as 
nouns while the third is marked as verb. This word would be disambiguated if used 
as a verb but not if used as a noun. If we assume that every word of this type is 
assigned a part of speech which disambiguates it (i.e., verb in the case of beam), then 
an additional 7% of words in LDOCE could, potentially, be disambiguated. Therefore, 
up to 95% of word types in LDOCE can be disambiguated to the homograph level 
by part-of-speech information alone. However, these figures do not take into account 
either errors in part-of-speech tagging or the corpus distribution of tokens, since each 
word type is counted exactly once. 
The next stage in our analysis was to attempt o disambiguate some texts us- 
ing the information obtained from part-of-speech tags. We took five articles from the 
Wall Street Journal, containing 391 polyhomographic content words. These articles were 
manually tagged with the most appropriate LDOCE homograph by one of the authors. 
The texts were then part-of-speech tagged using Brill's transformation-based l arning 
tagger (Brill, 1995). The tags assigned by the Brill tagger were manually mapped onto 
the simpler part-of-speech tag set used in LDOCE. 2 If a word has more than one ho- 
mograph with the same part of speech, then part-of-speech tags alone cannot always 
identify a single homograph; in such cases we chose the first sense listed in LDOCE 
since this is the one which occurs most frequently. 3 
It was found that 87.4% of the polyhomographic content words were assigned 
the correct homograph. A baseline for this task can be calculated by computing the 
number of tokens that would be correctly disambiguated if the first homograph for 
each was chosen regardless of part of speech. 78% of polyhomographic tokens were 
correctly disambiguated this way using this approach. 
These results show there is a clear advantage to be gained (over 42% reduction in 
error rate) by using the very simple part-of-speech-based method described compared 
with simply choosing the first homograph. However, we felt that it would be useful to 
carry out some further analysis of the data. To do this, it is useful to divide the polyho- 
mographic words into four classes, all based on the assumption that a part-of-speech 
tagger has been run over the text and that homographs which do not correspond to 
the grammatical category assigned have been removed. 
Full disambiguation (by part of speech): If only a single homograph with the 
correct part of speech remains, that word has been fully disambiguated 
by the tagger. 
2 The Brill tagger uses the 48-tag set from the Penn Tree Bank (Marcus, Santorini, and Marcinkiewicz 
1993), while LDOCE uses a set of 17 more general tags. Brill's tagger has a reported error rate of 
around 3%, although we found that mapp ing  the Penn TreeBank tags used by Brill onto the simpler 
LDOCE tag set led to a lower error rate. 
3 In the 3rd Edition of LDOCE the publ ishers claim that the senses are indeed ordered by frequency, 
a l though they make no such claim in the 1st Edition used here. However, Guo (1989) found evidence 
that there is a correspondence b tween the order in which senses are listed and the frequency of 
occurrence in the 1st Edition. 
328 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Partial disambiguation (by part of speech): If there is more than one possible ho- 
mograph with the correct part of speech but some have been removed 
from consideration, that word has been partially disambiguated by part 
of speech. 
No disambiguation (by part of speech): If all the homographs of a word have 
the same part of speech, which is then assigned by the tagger, then none 
can be removed and no disambiguation has been carried out. 
Part-of-speech error: It is possible for the part-of-speech tagger to assign an incor- 
rect part of speech, leading to the correct homograph being removed from 
consideration. It is worth mentioning that this situation has two possible 
outcomes: first, some homographs, with incorrect parts of speech, may 
remain; or second, all homographs may have been removed from consid- 
eration. 
In Table 3 we show in the column labelled Count the number of words in our 
five articles which fall into each of the four categories. The relative performance of
the baseline method (choosing the first sense) compared to the reported algorithm 
(removing homographs using part-of-speech tags) are shown in the rightmost two 
columns. The figures in brackets indicate the percentage of polyhomographic words 
correctly disambiguated by each method on a per-class basis. It can be seen that the 
majority of the polyhomographic words (297 of 342) fall into the "Full disambiguation" 
category, all of which are correctly disambiguated by the method reported here. When 
no disambiguation is carried out, the algorithm described simply chooses the first 
sense and so the results are the same for both methods. The only condition under 
which choosing the first sense is more effective than using part-of-speech information 
is when the part-of-speech tagger makes an error and all the homographs with the 
correct part of speech are removed from consideration. In most cases this means that 
the correct homograph cannot be chosen; however, in a small number of cases, this is 
equivalent to choosing the most frequent sense, since if all possible homographs have 
been removed from consideration, the algorithm reverts to using the simpler heuristic 
of choosing the word's first homograph. 4 
Although this result may seem intuitively obvious, there have, we believe, been no 
other attempts to quantify the benefit o be gained from the application of a part-of- 
speech tagger in WSD (see Wilks and Stevenson 1998a). The method escribed here is 
effective in removing incorrect senses from consideration, thereby reducing the search 
space if combined with other WSD methods. 
In the experiments reported in this section we made use of the particular struc- 
ture of LDOCE, which assigns each sense to a homograph from which its part of 
speech information is inherited. However, there is no reason to believe that the method 
reported here is limited to lexicons with this structure. In fact this approach can 
be applied to any lexicon which assigns part-of-speech information to senses, al- 
though it would not always be possible to evaluate at the homograph level as we 
do here. 
In the remainder of this paper we go on to describe a sense tagger that assigns 
senses from LDOCE using a combination of classifiers. The set of senses considered 
by the classifiers is first filtered using part-of-speech tags. 
4 An example ofthis situation is shown in the bottom row of Table 2. 
329 
Computational Linguistics Volume 27, Number 3 
Table 2 
Examples of the four word types introduced in Section 3.2. The leftmost column indicates the 
full set of homographs for the example words, with upper case indicating the correct 
homograph. The remaining columns how (respectively) the part-of-speech assigned by the 
tagger, the resulting set of senses after filtering, and the type of the word. 
All PoS After Word type 
Homographs Tag tagging 
N, v, v n N Full disambiguation 
n, adj, V v V Full disambiguation 
n, V, v v V, v Partial disambiguation 
n, N, v n n, N Partial disambiguation 
N, n n N, n No disambiguation 
v, V v v, V No disambiguation 
N, v, v v v v PoS error 
N, v, v adj N, v, v PoS error 
Table 3 
Error analysis for the experiment on WSD by part of speech alone. 
Correctly disambiguated by: 
Word Type Count Baseline method PoS method 
Full disambiguation 297 268 (90%) 297 (100%) 
Partial disambiguation 58 22 (38%) 32 (55%) 
No disambiguation 23 10 (43%) 10 (43%) 
Part-of-speech error 13 5 (38%) 3 (23%) 
All polyhomographic 391 305 (78%) 342 (87%) 
4. A Sense Tagger which Combines Knowledge Sources 
We adopt a f ramework in which different knowledge sources are appl ied as separate 
modules. One type of module,  a filter, can be used to remove senses from consideration 
when a knowledge source identifies them as unlikely in context. Another type can be 
used when a knowledge source provides evidence for a sense but cannot identify 
it confidently; we call these partial taggers (in the spirit of McCarthy's notion of 
"partial information" \[McCarthy and Hayes, 1969\]). The choice of whether to apply a 
knowledge source as either a filter or a partial tagger depends on whether it is likely to 
rule out correct senses. If a knowledge source is unlikely to reject the correct sense, then 
it can be safely implemented as a filter; otherwise implementat ion as a partial tagger 
would be more appropriate. In addition, it is necessary to represent he context of 
ambiguous words so that this information can be used in the disambiguation process. 
In the system described here these modules are referred to as feature extractors. 
Our sense tagger is implemented within this modular  architecture, one where 
each module is a filter, partial tagger, or feature extractor. The architecture of the 
system is represented in Figure 2. This system currently incorporates a single fil- 
ter (par t -o f - speech  f i l te r ) ,  three partial taggers (s imulated anneal ing,  sub jec t  
codes, se lec t iona l  res t r i c t ions )  and a single feature extractor (co l locat ion  ex- 
t rac tor ) .  
330 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  p 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
Figure 2 
Sense tagger architecture. 
331 
Computational Linguistics Volume 27, Number 3 
4.1 Preprocessing 
Before the filters or partial taggers are applied, the text is tokenized, lemmatized, 
split into sentences, and part-of-speech tagged, again using Brill's tagger. A named 
entity identifier is then run over the text to mark and categorize proper names, which 
will provide information for the selectional restrictions partial tagger (see Section 4.4). 
These preprocessing stages are carried out by modules from Sheffield University's 
Information Extraction system, LaSIE, and are described in more detail by Gaizauskas 
et al (1996). 
Our system disambiguates only the content words in the text, and the part-of- 
speech tags are used to decide which are content words. There is no attempt to dis- 
ambiguate any of the words identified as part of a named entity. These are excluded 
because they have already been analyzed semantically by means of the classification 
added by the named entity identifier (see Section 4.4). Another eason for not attempt- 
ing WSD on named entities is that when words are used as names they are not being 
used in any of the senses listed in a dictionary. For example, Rose and May are names 
but there are no senses in LDOCE for this usage. It may be possible to create a dummy 
entry in the set of LDOCE senses indicating that the word is being used as a name, 
but then the sense tagger would simply repeat work carried out by the named entity 
identifier. 
4.2 Part-of-Speech filtering 
We take the part-of-speech tags assigned by the Brill tagger and use a manually created 
mapping to translate these to the corresponding LDOCE grammatical category (see 
Section 3.2). Any senses which do not correspond to the category returned are removed 
from consideration. I  practice, the filtering is carried out at the same time as the lexical 
lookup phase and the senses whose grammatical categories do not correspond to the 
tag assigned are never attached to the ambiguous word. There is also an option of 
turning off filtering so that all senses are attached regardless of the part-of-speech tag. 
If none of the dictionary senses for a given word agree with the part-of-speech tag 
then all are kept. 
It could be reasonably argued that removing senses is a dangerous strategy since, 
if the part-of-speech tagger made an error, the correct sense could be removed from 
consideration. However, the experiments described in Section 3.2 indicate that part-of- 
speech information isunlikely to reject he correct sense and can be safely implemented 
as a filter. 
4.3 Optimizing Dictionary Definition Overlap 
Lesk (1986) proposed that WSD could be carried out using an overlap count of content 
words in dictionary definitions as a measure of semantic loseness. This method would 
tag all content words in a sentence with their senses from a dictionary that contains 
textual definitions. However, it was found that the computations which would be 
necessary to test every combination of senses, even for a sentence of modest length, 
was prohibitive. 
The approach was made practical by Cowie, Guthrie, and Guthrie (1992) (see 
also (Wilks, Slator, and Guthrie 1996)). Rather than computing the overlap for all 
possible combinations ofsenses, an approximate solution is identified by the simulated 
annealing optimization algorithm (Metropolis et al 1953). Although this algorithm is 
not guaranteed to find the global solution to an optimization problem, it has been 
shown to find solutions that are not significantly different from the optimal one (Press 
et al 1988). Cowie et al used LDOCE for their implementation a d found it correctly 
disambiguated 47% of words to the sense level and 72% to the homograph level 
332 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Z 
(no semantic restriction) 
T, W, X, Y, 2, 4 ~  
(abstract) 
I,W ? ~ ~ Q ' Y ' 5  
(animate) 
S,E, 1,2,5 L,E, 6,7 G, 7 PV '~A,  O/,V H~OO, 
(solid) (liquid) (gas) (plant) (ani~{al) (~umaXn~ 
\] N B,R D~K M,K F,R 
(movable (nonmovable (animal (ammal (human (human 
solid) solid) male) female) male) female) 
Figure 3 
Bruce and Guthrie's hierarchy of LDOCE semantic odes. 
when compared with manually assigned senses. The optimization must be carried out 
relative to a function that evaluates the suitability of a particular choice of senses. In 
the Cowie et al implementation this was done using a simple count of the number 
of words (tokens) in common between all the definitions for a given choice of senses. 
However, this method prefers longer definitions, since they have more words that 
can contribute to the overlap, and short definitions or definitions by synonym are 
correspondingly penalized. We addressed this problem by computing the overlap in a 
different way: instead of each word contributing one, we normalized its contribution 
by the number of words in the definition it came from. In their implementation Cowie 
et al also added pragmatic odes to the overlap computation; however, we prefer to 
keep different knowledge sources eparate and use this information in another partial 
tagger (see Section 4.5). The Cowie et al implementation returned one sense for each 
ambiguous word in the sentence without any indication of the system's confidence 
in its choice, but we adapted the system to return a set of suggested senses for each 
ambiguous word in the sentence. 
4.4 Selectional Preferences 
Our next partial tagger eturns the set of senses for each word that is licensed by 
selectional preferences (in the sense of Wilks 1975). LDOCE senses are marked with 
selectional restrictions expressed by 36 semantic odes not ordered in a hierarchy. 
However, the codes are clearly not of equal evels of generality; for example, the code H 
is used to represent all humans, while M represents human males. Thus for a restriction 
with type H, we would want to allow words with the more specific semantic lass M to 
meet it. This can be computed if the semantic ategories are organized into a hierarchy. 
Then all categories subsumed by another category will be regarded as satisfying the 
restriction. Bruce and Guthrie (1992) manually identified relations between the LDOCE 
semantic lasses, grouping the codes into small sets with roughly the same meaning 
and attached escriptions; for example M, K are grouped as a pair described as "human 
male". The hierarchy produced is shown in Figure 3. 
333 
Computational Linguistics Volume 27, Number 3 
Table 4 
Mapping of named entities onto LDOCE semantic odes. The named entities can be mapped 
to any semantic ode within a particular node of the hierarchy since the disambiguation 
algorithm treats all codes in the same node as equivalent. 
Named Entity Type LDOCE code 
PERSON H (= Human) 
ORGANIZATION T (= Abstract) 
LOCATION N (= Non-movable solid) 
DATE T (---- Abstract) 
TIME T (= Abstract) 
MONEY T (= Abstract) 
PERCENT T (---- Abstract) 
UNKNOWN Z (---- No  semantic restriction) 
The named entities identified as part of the preprocessing phase (Section 4.1) are 
used by this module, which requires first a mapping between the name types and 
LDOCE semantic odes, shown in Table 4. 
Any use of preferences for sense selection requires prior identification of the site 
in the sentence where such a relationship holds. Although prior identification was not 
done by syntactic methods in Wilks (1975), it is often easiest o think of the relation- 
ships as specified in grammatical terms, e.g., as subject-verb, verb-object, adjective- 
noun etc. We perform this step by means of a shallow syntactic analyzer (Stevenson 
1998) which finds the following grammatical relations: the subject, direct and indirect 
object of each verb (if any), and the noun modified by an adjective. Stevenson (1998) 
describes an evaluation of this system in which the relations identified were compared 
with those derived from Penn TreeBank parses (Marcus, Santorini, and Marcinkiewicz 
1993). It was found that the parser achieved 51% precision and 69% recall. 
The preference resolution algorithm begins by examining a verb and the nouns 
it dominates. Each sense of the verb applies a preference to those nouns such that 
some of their senses may be disallowed. Some verb senses will disallow all senses for 
a particular noun it dominates and these senses of the verb are immediately rejected. 
This process leaves us with a set of verb senses that do not conflict with the nouns 
that verb governs, and a set of noun senses licensed by at least one of those verb 
senses. For each noun, we then check whether it is modified by an adjective. If it is, 
we reject any senses of the adjectives which do not agree with any of the remaining 
noun senses. This approach is rather conservative in that it does not reject a sense 
unless it is impossible for it to fit into the preference pattern of the sentence. 
In order to explain this process more fully we provide a walk-through explanation 
of the procedure applied to a toy example shown in Table 5. It is assumed that the 
named-entity identifier has correctly identified John as a person and that the shallow 
parser has found the correct syntactic relations. In order to make this example as 
straightforward aspossible, we consider only the case in which the ambiguous words 
have few senses. The disambiguation process operates by considering the relations 
between the words in known grammatical relations, and before it begins we have 
essentially a set of possible senses for each word related via their syntax. This situation 
is represented by the topmost ree in Figure 4. 
Disambiguation is carried out by considering each verb sense in turn, beginning 
with run(l). As run is being used transitively, it places two restrictions on the sentence: 
first, the subject must satisfy the restriction human and the object abstract. In this 
334 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Table 5 
Sentence and lexicon for toy example of selectional preference resolution algorithm. 
Example sentence: 
John ran the hilly course. 
Sense Definition and Example Restriction 
John 
ran (1) 
ran (2) 
hilly (1) 
course (1) 
course (2) 
proper name 
to control an organisation run IBM 
to move quickly by foot run a marathon 
undulating terrain hilly road 
route race course 
programme of study physics course 
type:human 
subject:human object:abstract 
subject:human object:inanimate 
modifies:nonmovable sol id 
type:noumovable solid 
type:abstract 
run(l) 
restriction:human restriction:abstract 
John course(2) 
{ run(1 ),run(2) }
~ b j e c t - ~ b  I 
John { course(1),course(2) } 
f 
I adjective-noun~ 
I 
{hilly(l)} 
run(2) 
restriction:human restriction:inanimate 
John course(I) 
type:nonmovable solid 
hilly(l) 
Figure 4 
Restriction resolution in toy example. 
example, John has been identified as a named entity and marked as human, so the 
subject restriction is not broken. Note that, if the restriction were broken, then the 
verb sense run(l) would be marked as incorrect by this partial tagger and no further 
attempt would be made to resolve its restrictions. As this was not the case, we consider 
the direct-object slot, which places the restriction abst rac t  on the noun which fills it. 
course(2) fulfils this criterion, course is modif ied by hilly which expects a noun of type 
noumovable so l id .  However,  course(2) is marked abst rac t ,  which does not comply 
with this restriction. Therefore, assuming that run is being used in its second sense 
leads to a situation in which there is no set of senses which comply with all the 
restrictions placed on them; therefore run(l) is not the correct sense of run and the 
partial tagger marks this sense as wrong. This situation is represented by the tree at 
the bottom left of Figure 4. The sense course(2) is not rejected at this point since it may  
be found to be acceptable in the configuration of senses of another sense of run. 
The algorithm now assumes that run(2) is the correct sense. This implies that 
course(I) is the correct sense as it complies with the inanimate restriction that that verb 
sense places on the direct object. As well as complying with the restriction imposed 
by run(2), course(I) also complies with the one imposed by hilly(i), since nonmovable 
so l id  is subsumed by inanimate.  Therefore, assuming that the senses run(2) and 
335 
Computational Linguistics Volume 27, Number 3 
course(I) are being used does not lead to any restrictions being broken and the algo- 
rithm marks these as correct. 
Before leaving this example it is worth discussing a few additional points. The 
sense course(2) is marked as incorrect because there is no sense of run with which an 
interpretation of the sentence can be constructed using course(2). If there were further 
senses of run in our example, and course(2) was found to be suitable for those extra 
senses, then the algorithm would mark the second sense of course as correct. There is, 
however, no condition under which run(l) could be considered as correct hrough the 
consideration of further verb senses. Also, although John and hilly are not ambiguous in 
this example, they still participate in the disambiguation process. In fact they are vital 
to its success, as the correct senses could not have been identified without considering 
the restrictions placed by the adjective hilly. 
This partial tagger eturns, for all ambiguous noun, verb, and adjective occurrences 
in the text, the set of senses which satisfy the preferences imposed on those words. 
Adverbs do not have any selectional preferences in LDOCE and so are ignored by this 
partial tagger. 
4.5 Subject Codes 
Our final partial tagger is a re-implementation f the algorithm developed by Yarowsky 
(1992). This algorithm is dependent upon a categorization of words in the lexicon 
into subject areas--Yarowsky used the Roget large categories. In LDOCE, primary 
pragmatic odes indicate the general topic of a text in which a sense is likely to be 
used. For example, LN means "Linguistics and Grammar" and this code is assigned 
to some senses of words such as "ellipsis", "ablative", "bilingual" and "intransitive". 
Roget is a thesaurus, o each entry in the lexicon belongs to one of the large categories; 
but over half (56%) of the senses in LDOCE are not assigned a primary code. We 
therefore created a dummy category, denoted by --,  used to indicate a sense which 
is not associated with any specific subject area and this category is assigned to all 
senses without a primary pragmatic ode. These differences between the structures 
of LDOCE and Roget meant that we had to adapt the original algorithm reported in 
Yarowsky (1992). 
In Yarowsky's implementation, the correct subject category is estimated by apply- 
ing (6), which maximizes the sum of a Bayesian term (the fraction on the right) over 
all possible subject categories (SCat) for the ambiguous word over the words in its 
context (w). A context of 50 words on either side of the ambiguous word is used. 
ARGMAX Pr( w\[ S Cat) Pr( SCat) 
scat ~ log Pr(w) (6) 
w e context  
Yarowsky assumed the prior probability of each subject category to be constant, 
so the value Pr(SCat) has no effect on the maximization in (6), and (7) was in effect 
being maximized. 
ARCMAX Pr (w\]SCat) 
SCat ~ log Pr(w) (7) 
w e context  
By including a general pragmatic ode to deal with the lack of coverage, we created 
an extremely skewed distribution of codes across senses and Yarowsky's assumption 
that subject codes occur with equal probability is unlikely to be useful in this ap- 
plication. We gained a rough estimate of the probability of each subject category by 
determining the proportion of senses in LDOCE to which it was assigned and apply- 
ing the maximum likelihood estimate. It was found that results improved when the 
336 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
rough estimate of the likelihood of pragmatic odes was used. This procedure gener- 
ates estimates based on counts of types and it is possible that this estimate could be 
improved by counting tokens, although the problem of polysemy in the training data 
would have to be overcome in some way. 
The algorithm relies upon the calculation of probabilities gained from corpus tatis- 
tics: Yarowsky used the Grolier's Encyclopaedia, which comprised a 10 million word 
corpus. Our implementation used nearly 14 million words from the non-dialogue 
portion of the British National Corpus (Burnard 1995). Yarowsky used smoothing pro- 
cedures to compensate for data sparseness in the training corpus (detailed in Gale, 
Church, and Yarowsky \[1992b\]), which we did not implement. Instead, we attempted 
to avoid this problem by considering only words which appeared at least 10 times 
in the training contexts of a particular word. A context model is created for each 
pragmatic ode by examining 50 words on either side of any word in the corpus con- 
taining a sense marked with that code. Disambiguation is carried out by examining the 
same 100 word context window for an ambiguous word and comparing it against he 
models for each of its possible categories. Further details may be found in Yarowsky 
(1992). 
Yarowsky reports 92% correct disambiguation ver 12 test words, with an average 
of three possible Roget large categories. However, LDOCE has a higher level of aver- 
age ambiguity and does not contain as complete a thesaural hierarchy as Roget, so we 
would not expect such good results when the algorithm is adapted to LDOCE. Con- 
sequently, we implemented the approach as a partial tagger. The algorithm identifies 
the most likely pragmatic ode and returns the set of senses which are marked with 
that code. In LDOCE, several senses of a word may be marked with the same prag- 
matic code, so this partial tagger may return more than one sense for an ambiguous 
word. 
4.6 Collocation Extractor 
The final disambiguation module is the only feature-extractor in our system and is 
based on collocations. A set of 10 collocates are extracted for each ambiguous word 
in the text: first word to the left, first word to the right, second word to the left, 
second word to the right, first noun to the left, first noun to the right, first verb to 
the left, first verb to the right, first adjective to the left, and first adjective to the 
right. Some of these types of collocation were also used by Brown et al (1991) and 
Yarowsky (1993) (see Section 2.3). All collocates are searched for within the sentence 
which contains the ambiguous word. If some particular collocation does not exist for 
an ambiguous word, for example if it is the first or last word in a sentence, then a 
null value (NoColl) is stored instead. Rather than storing the surface form of the co- 
occurrence, morphological roots are stored instead, as this allows for a smaller set of 
collocations, helping to cope with data sparseness. The surface form of the ambiguous 
word is also extracted from the text and stored. The extracted collocations and surface 
form combine to represent the context of each ambiguous word. 
4.7 Combining Disambiguation Modules 
The results from the disambiguation modules (filter, partial taggers, and feature x- 
tractor) are then presented to a machine learning algorithm to combine their results. 
The algorithm we chose was the TIMBL memory-based learning algorithm (Daelemans 
et al 1999). Memory-based learning is another name for exemplar-based learning, as 
employed by Ng and Lee (Section 2.3). The TiMBL algorithm has already been used for 
various NLP tasks including part-of-speech tagging and PP-attachment (Daelemans et 
al. 1996; Zavrel, Daelemans, and Veenstra 1997). 
337 
Computational Linguistics Volume 27, Number 3 
Like PEBLS, which formed the core of Ng and Lee's LEXAS system, TiMBL classifies 
new examples by comparing them against previously seen cases. The class of the most 
similar example is assigned. At the heart of this approach is the distance metric A(X, Y) 
which computes the similarity between instances X and Y. This measure is calculated 
using the weighted overlap metric shown in (8), which calculates the total distance by 
computing the sum of the distance between each position in the feature vector. 
n 
A(X, Y) =- ~_, wi6(xi, yi) (8) 
i=1 
where: 
xl-yi if numeric, else ~ axi-  min~ ?5(xi, y i )  = i f  Xi = y i  (9) 
if xi # yi 
From (9) we can see that TiMBL treats numeric and symbolic features differently. 
For numeric features, the unweighted distance is computed as the difference between 
the values for that feature in each instance, divided by the maximum possible dis- 
tance computed over all pairs of instances in the database. 5 For symbolic features, the 
unweighted istance is 0 if they are identical, and 1 otherwise. For both numeric and 
symbolic features, this distance is multiplied by the weight for the particular feature, 
based on the Gain Ratio measure introduced by Quinlan (1993). This is a measure of 
the difference in uncertainty between the situations with and without knowledge of 
the value of that feature, as in (10). 
H(C) - ~-,v Pr(v) x H(CIv) (10) 
wi = H(v) 
Where C is the set of classifications, v ranges over all values of the feature i and 
H(C) is the entropy of the class labels. Probabilities are estimated from frequency 
of occurrence in the training data. The numerator of this formula determines the 
knowledge about the distribution of classes that is added by knowing the value of 
feature i. However, this measure can overestimate he value of features with large 
numbers of possible values. To compensate, it is divided by H(v), the entropy of the 
feature values. 
Word senses are presented to TiMBL in a feature-vector representation, with each 
sense which was not removed by the part of speech filter being represented by a 
separate vector. The vectors are formed from the following pieces of information in 
order: headword, homograph number, sense number, rank of sense (the order of the 
sense in the lexicon), part of speech from lexicon, output from the three partial tag- 
gers (simulated annealing, subject codes, and selectional restrictions), sur- 
face form of headword from the text, the ten collocates, and an indicator of whether 
the sense is appropriate or not in the context (correct or incorrect). 
Figure 5 shows the feature vectors generated for the word influence in the context 
shown. The final value in the feature vector shows whether the sense is correct or 
not in the particular context. We can see that, in this case, there is one correct sense, 
influence_l_la, the definition of which is "power to gain an effect on the mind of 
5 An earlier version of this system (Stevenson and Wilks 1999) used TiMBL version 1.0 (Daelemans et al 
1998), which supports only symbolic features. 
338 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Context 
Regard ing At lanta's new mil l ion dollar airport, the jury recommended "that when the new management  take 
charge Jan. 1 the airport be operated in a manner  that will  el iminate political influences". 
Feature Vectors 
Learning features Truth 
influence 1 la 1 n influences 1 12.03 y NoColl manner NoColl eliminate NoColl in NoColl political NoColl eliminate correct 
influence 1 lb 2 n influences 0 12.03 y NoColl manner NoColl eliminate NoColl in NoColl political NoColl eliminate incorrect 
influence 1 2 3 n influences 0 12.03 y NoColl manner NoColl eliminate NoColl in NoColl political NoColl eliminate incorrect 
influence 1 3 4 n influences 0 12.03 y NoColl manner NoColl eliminate NoColl in NoColl political NoColl eliminate incorrect 
influence 1 4 5 n influences 0 12.03 n NoColl manner NoColl eliminate NoCofl in NoColl political NoColl eliminate incorrect 
influence 1 5 6 n influences 0 12.03 n NoColl manner NoColl eliminate NoColl in NoCon political NoColl eliminate incorrect 
influence 1 6 7 n influences 0 12.03 n NoColl manner NoColl eliminate NoColl in NoColl political NoColl eliminate incorrect 
Figure 5 
Example feature-vector representation. 
or get results from, without asking or doing anything". Features 10-19 are produced 
by the collocation extractor, and these are identical since each vector is taken from 
the same content. Features 7-9 show the results of the partial taggers. The first is the 
output from simulated annealing, the second the subject  code, and the third the 
se lect iona l  res t r i c t ions .  All noun senses of influence share the same pragmatic 
code (--), and consequently this partial tagger eturns the same score for each sense. 
A final point worth noting is that in LDOCE, influence has a verb sense which the 
part-of-speech filter removed from consideration, and consequently this sense is not 
included in the feature-vector representation. 
The TiMBL algorithm is trained on tokens presented in this format. When disam- 
biguating unannotated text, the algorithm is applied to data presented in the same 
format without the classification. The unclassified vectors are then compared with all 
the training examples, and it is assigned the class of the closest one. 
5. Evaluation Strategy 
5.1 Evaluation Corpus 
The evaluation of WSD algorithms has recently become a much-studied area. Gale, 
Church, and Yarowsky (1992a), Resnik and Yarowsky (1997), and Melamed and Resnik 
(2000) each presented arguments for adopting various evaluation strategies, with 
Resnik and Yarowsky's proposal directly influencing the set-up of SENSEVAL (Kil- 
garriff 1998). At the heart of their proposals is the ability of human subjects to mark 
up text with the phenomenon i question (WSD in this case) and evaluate the results 
of computation. This linguistic phenomenon has proved to be far more elusive and 
complex than many others. We have discussed this at length elsewhere (Wilks 1997) 
and will assume here that humans can mark up text for senses to a sufficient degree. 
Kilgarriff (1993) questioned the possibility of creating sense-tagged texts, claiming the 
task to be impossible. However, it should be borne in mind that no alternative has 
yet been widely accepted and that Kilgarriff himself used the markup-and-test model 
for SENSEVAL. In the following discussion we compare the evaluation methodology 
adopted here with those proposed by others. 
339 
Computational Linguistics Volume 27, Number 3 
The standard evaluation procedure for WSD is to compare the output of the sys- 
tem against gold standard texts, but these are very labor-intensive to obtain; lexical 
semantic markup is generally considered to be a more difficult and time-consuming 
task than part-of-speech markup (Fellbaum et al 1998). Rather than expend a vast 
amount of effort on manual tagging we decided to combine two existing resources: 
SEMCOR (Landes, Leacock, and Tengi 1998), and SENSUS (Knight and Luk 1994). 
SEMCOR is a 200,000 word corpus with the content words manually tagged as part 
of the WordNet project. The semantic tagging was carried out by trained lexicogra- 
phers under disciplined conditions that attempted to keep tagging inconsistencies to
a minimum. SENSUS is a large-scale ontology designed for machine-translation a d 
was itself produced by merging the ontological hierarchies of WordNet, LDOCE (as 
derived by Bruce and Guthrie, see Section 4.4), and the Penman Upper Model (Bate- 
man et al, 1990) from ISI. To facilitate the merging of these three resources to produce 
SENSUS, Knight and Luk were required to derive a mapping between the senses in the 
two lexical resources. We used this mapping to translate the WordNet-tagged content 
words in SEMCOR to LDOCE tags. 
The mapping of senses is not one-to-one, and some WordNet synsets are mapped 
onto two or three LDOCE senses when WordNet does not distinguish between them. 
The mapping also contained significant gaps, chiefly words and senses not in the 
translation scheme. SEMCOR contains 91,808 words tagged with WordNet synsets, 
6,071 of which are proper names, which we ignored, leaving 85,737 words which 
could potentially be translated. The translation contains only 36,869 words tagged 
with LDOCE senses; however, this is a reasonable size for an evaluation corpus for the 
task, and it is several orders of magnitude larger than those used by other researchers 
working in large vocabulary WSD, for example Cowie, Guthrie, and Guthrie (1992), 
Harley and Glennon (1997), and Mahesh et al (1997). This corpus was also constructed 
without the excessive cost of additional hand-tagging and does not introduce any of 
the inconsistencies that can occur with a poorly controlled tagging strategy. 
Resnik and Yarowsky (1997) proposed to evaluate large vocabulary WSD systems 
by choosing a set of test words and providing annotated test and training examples 
for just these words, allowing supervised and unsupervised algorithms to be tested 
on the same vocabulary. This model was implemented in SENSEVAL (Kilgarriff 1998). 
However, for the evaluation of the system presented here, there would have been 
no benefit from using this strategy since it still involves the manual tagging of large 
amounts of data and this effort could be used to create a gold standard corpus in 
which all content words are disambiguated. It is possible that some computational 
techniques may evaluate well over a small vocabulary but may not work for a large 
set of words, and the evaluation strategy proposed by Resnik and Yarowsky will not 
discriminate between these cases. 
In our evaluation corpus, the most frequent ambiguous type is have, which appears 
604 times. A large number of words (2407) occur only once, and nearly 95% have 25 
occurrences or less. Table 6 shows the distribution of ambiguous types by number of 
corpus tokens. It is worth noting that, as would be expected, the observed istribution 
is highly Zipfian (Zipf 1935). 
Differences in evaluation corpora makes comparison difficult. However, some idea 
of the difficulty of WSD can be gained by calculating properties of the evaluation cor- 
pus. Gale, Church, and Yarowsky (1992a) suggest hat the lowest level of performance 
which can be reasonably expected from a WSD system is that achieved by assigning 
the most likely sense in all cases. Since the first sense in LDOCE is usually the most 
frequent, we calculate this baseline figure using a heuristic which assumes the first 
sense is always correct. This is the same baseline heuristic we used for the experiments 
340 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Table 6 
Occurrence of ambiguous words in the evaluation corpus. 
Occurrence Range Count 
1-25 5488 (94.6%) 
26-50 202 (3.5%) 
51-75 67 (1.2%) 
76-100 21 (0.04%) 
100-604 26 (0.4%) 
reported in Section 3, although those were for the homograph level. We applied the 
naive heuristic of always choosing the first sense in our corpus and found that 30.9% 
of senses were correctly disambiguated. 
Another measure that gives insight into an evaluation corpus is to count the av- 
erage polysemy, i.e., the number of possible senses we can expect for each ambiguous 
word in the corpus. The average polysemy is calculated by counting the sum of pos- 
sible senses for each ambiguous token and dividing by the number of tokens. This is 
represented by (11), where w ranges over all ambiguous tokens in the corpus, S(w) is 
the number of possible senses for word w, and N is the number of ambiguous tokens. 
The average polysemy for our evaluation corpus is 14.62. 
Average polysemy = ~w in text S( w) (11) 
N 
Our annotated corpus has the unusual property that more than one sense may 
be marked as correct for a particular token. This is an unavoidable side-effect of a 
mapping between lexicon senses which is not one-to-one. However, it does not imply 
that WSD is easier in this corpus than one in which only a single sense is marked 
for each token, as can be shown from an imaginary example. The worst case for a 
WSD algorithm is when each of the possible semantic tags for a given word occurs 
with equal frequency in a corpus, and so the prior probabilities exhibit a uniform, 
uninformative distribution. Then a corpus with an average polysemy of 5, and 2 senses 
marked correct on each ambiguous token, will have a baseline not less than 40%. 
However, one with an average polysemy of 2, and only a single sense on each, will 
have a baseline of at least 50%. Test corpora in which each ambiguous token has 
exactly two senses were used by Brown et al (1991), Yarowsky (1995) and others. 
Our system was tested using a technique known as 10-fold cross validation. This 
process is carried out by splitting the available data into ten roughly equal subsets. 
One of the subsets is chosen as the test data and the TiMBL algorithm is trained on the 
remainder. This is repeated ten times, so that each subset is used as test data exactly 
once, and results are averaged across all of the test runs. This technique provides two 
advantages: first, the best use can be made of the available data, and secondly, the 
computed results are more statistically reliable than those obtained by simply setting 
aside a single portion of the data for testing. 
5.2 Evaluation Metrics 
The choice of scoring metric is an important one in the evaluation of WSD algorithms. 
The most commonly used metric is the ratio of words for which the system has as- 
signed the correct sense compared to those which it attempted todisambiguate. Resnik 
and Yarowsky (1997) dubbed this the exact match metric, which is usually expressed 
341 
Computational Linguistics Volume 27, Number 3 
as a percentage calculated according to the formula in (12). 
Exact match = Number of correctly assigned senses x 100% (12) 
Number of senses assigned 
Resnik and Yarowsky criticize this metric because it assumes a WSD system com- 
mits to a particular sense. They propose an alternative metric based on cross-entropy 
that compares the probabilities for each sense as assigned by a WSD system against 
those in the gold standard text. The formula in (13) shows the method for computing 
this metric, where the WSD system has processed N words and Pr(csi) is the proba- 
bility assigned to the correct sense of word i. 
N 1 
N ~ l?g2 Pr(csi) (13) 
i=1 
This evaluation metric may be useful for disambiguation systems that assign probabil- 
ities to each sense, such as those developed by Resnik and Yarowsky, since it provides 
more information than the exact match metric. However, for systems which simply 
choose a single sense and do not measure confidence, it provides far less information. 
When a WSD assigns only one sense to a word and that sense is incorrect, hat word is 
scored as ~.  Consequently, the formula in (13) returns c~ if there is at least one word 
in the test set for which the tagger assigns a zero probability to the correct sense. For 
WSD systems which assign exactly one sense to each word, this metric returns 0 if 
all words are tagged correctly, and cx~ otherwise. This metric is potentially very useful 
for the evaluation of WSD systems that return non-zero probabilities for each possible 
sense; however, it is not useful for the metric presented in this paper and others that 
are not based on probabilistic models. 
Melamed and Resnik (2000) propose a metric for scoring WSD output when there 
may be more than one correct sense in the gold standard text, as with the evaluation 
corpus we use. They mention that when a WSD system returns more than one sense 
it is difficult to tell if they are intended to be disjunctive or conjunctive. The score 
for a token is computed by dividing the number of correct senses identified by the 
algorithm by the total it returns, making the metric equivalent to precision in infor- 
mation retrieval (van Rijsbergen 1979). 6For systems which return exactly one sense 
for each word, this equates to scoring a token as 1 if the sense returned is correct, and 
0 otherwise. For the evaluation of the system presented here, the metric proposed by 
Melamed and Resnik is then equivalent to the exact match metric. 
The exact match metric has the advantage of being widely used in the WSD lit- 
erature. In our experiments he exact match figure is computed at the LDOCE sense 
level, where the number of tokens correctly disambiguated to the sense level is di- 
vided by the number ambiguous at that level. At the homograph level, the number 
correctly disambiguated to the homograph is divided by the number which are poly- 
homographic. 
6. Performance 
Using the evaluation procedure described in the previous ection, it was found that the 
system correctly disambiguated 90% of the ambiguous instances to the fine-grained 
sense level, and in excess of 94% to the homograph level. 
6 The metric operates lightly differently for systems that assign probabilities to senses, 
342 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Table 7 
System results, baselines, and corpus characteristics. Sense level results are calculated over all 
polysemous words in the evaluation corpus while those reported for the homograph level are 
calculated only over polyhomographic ones. 
Entire Subcorpora 
Corpus Noun Verb Adjective Adverb 
Sense level Accuracy 90.37% 91.24% 88.38% 91.09% 70.61% 
Baseline 30.90% 34.56% 18.46% 25.76% 36.73% 
Tokens 36,774 26,091 6,465 3,310 908 
Types 5,804 4.041 1,021 1,006 125 
Average Polysemy 14.62 13.65 24.35 6.07 4.43 
Homograph level Accuracy 94.65% 94.63% 95.26% 96.89% 90.67% 
Baseline 71.24% 73.47% 60.72% 87.10% 86.87% 
Tokens 18,219 11,380 5,194 1,326 319 
Types 1,683 1,264 709 201 34 
Average Polysemy 2.52 2.32 2.81 2.95 3.13 
In order to analyze the effectiveness of our tagger in more detail, we split the 
main corpus into sub-corpora by grammatical category. In other words, we created 
four individual sub-corpora containing the ambiguous words which had been part- 
of-speech tagged as nouns, verbs, adjectives, and adverbs. The figures characterizing 
each of these corpora are shown in Table 7. The majority of the ambiguous words 
were nouns, with far fewer verbs and adjectives, and less than one thousand adverbs. 
The average polysemy for nouns, at both sense and homograph levels, is roughly 
the same as the overall corpus average although it is noticably higher for verbs at 
the sense level. At the sense level the average polysemy figures are much lower for 
adjectives and adverbs. This is because it is common for English words to act as either 
a noun or a verb and, since these are the most polysemous grammatical categories, 
the average polysemy count becomes large due to the cumulative ffect of polysemy 
across grammatical categories. However, words that can act as adjectives or adverbs 
are unlikely to be nouns or verbs. This, plus the fact that adjectives and adverbs are 
generally less polysemous in LDOCE, means that their average polysemy in text is far 
lower than it is for nouns or verbs. 
Table 7 shows the accuracy of our system over the four subcorpora. We can see 
that the tagger achieves higher results at the homograph level than the sense level 
on each of the four subcorpora, which is consistent with the result over the whole 
corpus. 
There is quite a difference in the tagger's results across the different subcorpora--  
91% for nouns and 70% for adverbs. Perhaps the learning algorithm does not perform 
as well on adverbs because that corpus is significantly smaller than the other three. 
This hypothesis was checked by testing our system on portions of each of the three 
subcorpora that were roughly equal in size to the adverb subcorpus. We found that the 
reduced data caused a slight loss of accuracy on each of the three subcorpora; how- 
ever, there was still a marked difference between the results for the adverb subcorpus 
and the other three. Further analysis showed that the differences in performance over 
different subcorpora seem linked to the behavior of different partial taggers when 
used in combination. In the following section we describe this behavior in more de- 
tail. 
343 
Computational Linguistics Volume 27, Number 3 
6.1 Interaction of Knowledge Sources 
In order to gauge the contribution of each knowledge source separately, we imple- 
mented a set of simple disambiguation algorithms, each of which uses the output 
from a single partial tagger. Each algorithm takes the result of its partial tagger and 
checks it against the disambiguated text to see if it is correct. If the partial tagger eturns 
more than one sense, as do the simulated annealing, subject code and se lect iona l  
preference taggers, the first sense is taken to break the tie. For the partial tagger based 
on Yarowsky's ubject-code algorithm, we choose the sense with the highest saliency 
value. If more than one sense has been assigned the maximum value, the tie is again 
broken by choosing the first sense. Therefore, each partial tagger eturns a single sense 
and the exact match metric is used to determine the proportion of tokens for which 
that tagger eturns the correct sense. The part-of-speech filter is run before the partial 
taggers make their decision and so they only consider the set of senses it did not re- 
move. The results of each tagger, computed at both sense and homograph levels over 
the evaluation corpus and four subcorpora, re shown in Table 7. 
We can see that the partial taggers that are most effective are those based on the 
simulated annealing algorithm and Yarowsky's ubject code approach. The success of 
these modules upports our decision to use existing disambiguation algorithms that 
have already been developed rather than creating new ones. 
The most successful of the partial taggers is the one based on Yarowsky's algorithm 
for modelling thesaural categories by wide contexts. This consistently achieves over 
70% correct disambiguation a d seems particularly successful when disambiguating 
adverbs (over 85% correct). It is quite surprising that this algorithm is so successful for 
adverbs, since it would seem quite reasonable to expect an algorithm based on subject 
codes to be more successful on nouns and less so on modifiers uch as adjectives and 
adverbs. 
Yarowsky (1992) reports that his algorithm achieves 92% correct disambiguation, 
which is nearly 13% higher than achieved in our implementation. However, Yarowsky 
tested his implementation  a restricted vocabulary of 12 words, the majority of which 
were nouns, and used Roget large categories as senses. The baseline performance for 
this corpus is 66.5%, considerably higher than the 30.9% computed for the corpus 
used in our experiments. Another possible reason for the difference in results is the 
fact that Yarowsky used smoothing algorithms to avoid problems with the probability 
estimates caused by data sparseness. We did not employ these procedures and used 
simple corpus frequency counts when calculating the probabilities ( ee Section 4.5). It 
is not possible to say for sure that the differences between implementations did not 
lead to the differences in results, but it seems likely that the difference in the semantic 
granularity of LDOCE subject codes and Roget categories was an important factor. 
The second partial tagger based on an existing approach is the one which uses 
simulated annealing to optimize the overlap of words shared by the dictionary defini- 
tions for a set of senses. In Section 4.3 we noted that Cowie et al (1992) reported 47% 
correct disambiguation to the sense level using this technique, while in our adaptation 
over 17% more words are correctly disambiguated. Our application filtered out senses 
with the incorrect part of speech in addition to using a different method to calculate 
overlap that takes account of short definitions. It seems likely that these changes are 
the source of the improved results. 
Our least successful partial tagger is the one based on selectional preferences. 
Although its overall result is slightly below the overall corpus baseline, it is very suc- 
cessful at disambiguating verbs. This is consistent with the work of Resnik (1997), who 
reported that many words do not have strong enough selectional restrictions to carry 
out WSD. We expected preferences to be successful for adjectives as well, although 
344 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Table 8 
Performance of individual partial taggers (at sense level). 
All Nouns Verbs Adjectives Adverbs 
simulated annealing (I) 65.24% 66.50% 67.51% 49.02% 50.61% 
selectional preferences (2) 44.85% 40.73% 75.80% 27.56% 0% 
subject codes (3) 79.41% 79.18% 72.75% 73.73% 85.50% 
this is not the case in our evaluation. This is because the sense discrimination of ad- 
jectives is carried out after that for nouns in our algorithm (see Section 4.4), and the 
former is hindered by the low results of the latter. Adverbs cannot be disambiguated 
by preference methods against LDOCE because it does not contain the appropriate 
information. 
Our analysis of the behavior of the individual partial taggers provides ome clues 
to the behavior of the overall system, consisting of all taggers, on the different sub- 
corpora, as shown in Table 7. The system performs to roughly the same level over 
the noun, verb, and adjective sub-corpora with only a 3% difference between the best 
and worst performance. The system's worst performance is on the abverb sub-corpus, 
where it disambiguates only slightly more than 70% of tokens successfully. This may 
be due to the fact that only two partial taggers provide evidence for this grammatical 
category. However, the system still manages to disambiguate most of the adverbs to the 
homograph level successfully, and this is probably because the part-of-speech filter has 
ruled out the incorrect homographs, not because the partial taggers performed well. 
One can legitimately wonder whether in fact the different knowledge sources for 
WSD are all ways of encoding the same semantic information, in a similar way that 
one might suspect ransformation rules and statistics encode the same information 
about part-of-speech tag sequences in different formats. However, the fact that an op- 
timized combination ofour partial taggers yields a significantly higher figure than any 
one tagger operating independently, shows that they must be orthogonal information 
sources. 
6.2 The overall value of the part-of-speech filter 
We have already examined the usefulness of part-of-speech tags for semantic disam- 
biguation in Section 3. However, we now want to know the effect it has within a 
system consisting of several disambiguation modules. It was found that accuracy at 
the sense level reduced to 87.87% and to 93.36% at the homograph level when the 
filter was removed. Although the system's performance did not decrease by a large 
amount, the part-of-speech filter brings the additional benefit of reducing the search 
space for the three partial taggers. In addition, the fact that these results are not af- 
fected much by the removal of the part-of-speech filter, shows that the WSD modules 
alone do a reasonable job of resolving part-of-speech ambiguity as a side-effect of 
semantic disambiguation. 
7. Conclusion 
Previously reported WSD systems that enjoyed a high level of accuracy have often 
operated on restricted vocabularies and employed a single WSD methodology. These 
methods have often been pursued for sound reasons to do with evaluation, but have 
been limited in their applicability and also in their persuasiveness regarding the scal- 
345 
Computational Linguistics Volume 27, Number 3 
ability and interaction of the various WSD partial methods. This paper reported a 
system which disambiguated all content words in a text, as defined by a standard 
machine readable dictionary, with a high degree of accuracy. 
Our evaluation shows that disambiguation can be carried out with more accurate 
results when several knowledge sources are combined. It remains unclear exactly what 
it means to optimize the combination of modules within a learning system like T?MBL: 
we could, in further work, treat the part-of-speech tagger as a partial tagger and not 
a filter, and we could allow the system to learn some "optimal" weighting of all 
the partial taggers. It also remains an interesting question whether, because of the 
undoubted existence of novel senses in text, a sense tagger can ever reach the level 
that part-of-speech tagging has. However, we believe we have shown that interesting 
combinations of WSD methods on a substantial training corpus are possible, and that 
this can show, among other things, the relative independence of the types of semantic 
information expressed by the various forms of lexical input. 
Acknowledgments 
The work described here was supported by 
the European Union Language Engineering 
project ECRAN - Extraction of Content: 
Research at Near-market (LE-2110). One of 
the authors was also supported by the 
EPSRC grant MALT (GR/M73521) while 
writing this paper. We are grateful for the 
feedback from many colleagues in Sheffield, 
especially Mark Hepple, and for the 
detailed comments from the anonymous 
reviewers of an earlier version of this paper. 
Gillian Callaghan was extremely helpful in 
the preparation of the final version of this 
paper. Any errors are our own. 
References 
Bateman, John, Robert Kasper, Joharu~a 
Moore, and Richard Whimey. 1990. A 
general organization of knowledge for 
natural language processing: the 
PENMAN upper model, Technical report, 
USC/Information Sciences Institute, 
Marina del Rey, CA. 
Brill, Eric. 1995. Transformation-based 
error-driven learning and natural 
language processing: A case study in part 
of speech tagging. Computational 
Linguistics, 21(4):543-566. 
Brown, Peter, Stephen Della Pietra, Vincent 
Della Pietra, and Robert Mercer. 1991. 
Word sense disambiguation using 
statistical methods. In Proceedings ofthe 
29th Meeting of the Association for 
Computational Linguistics (ACL-91), 
pages 264-270, Berkeley, CA. 
Bruce, Rebecca nd Louise Guthrie. 1992. 
Genus disambiguation: A study in 
weighted performance. In Proceedings of
the 14th International Conference on 
Computational Linguistics (COLING-92), 
pages 1187-1191, Nantes, France. 
Bruce, Rebecca nd Janyce Wiebe. 1994. 
Word-sense disambiguation using 
decomposable models. In Proceedings ofthe 
32nd Annual Meeting of the Association for 
Computational Linguistics (ACL-94), 
pages 139-145, Las Cruces, New Mexico. 
Burnard, Lou. 1995. Users Reference Guide for 
the British National Corpus. Oxford 
University Computing Services. 
Chapman, R. L. 1977. Roget's International 
Thesaurus Fourth Edition, Thomas Y. 
Crowell Company, New York, NY. 
Cost, Scott and Steven Salzberg. 1993. A 
weighted nearest neighbour algorithm for 
learning with symbolic features. Machine 
Learning, 10(1):57-78. 
Cottrell, Garrison. 1984. A model of lexical 
access of ambiguous words. In Proceedings 
of the National Conference on Artificial 
Intelligence (AAAI-84), pages 61-67, 
Austin, TX. 
Cowie, Jim, Louise Guthrie, and Joe 
Guthrie. 1992. Lexical disambiguation 
using simulated annealing. In Proceedings 
of the 14th International Conference on 
Computational Linguistics (COLING-92), 
pages 359-365, Nantes, France. 
Daelemans, Walter, Jakub Zavrel, Peter 
Berck, and Steven Gillis. 1996. MBT: A 
memory-based part of speech tagger 
generator. In Proceedings ofthe Fourth 
Workshop on Very Large Corpora, 
pages 14-27, Copenhagen. 
Daelemans, Walter, Jakub Zavrel, Ko van 
der Sloot, and Antal van den Bosch. 1998. 
TiMBL: Tilburg memory based learner 
version 1.0. Technical report, University 
of Tilburg Technical Report 98-03. 
Daelemans, Walter, Jakub Zavrel, Ko van 
der Sloot, and Antal van den Bosch. 1999. 
TiMBL: Tilburg memory based learner, 
version 2.0, reference guide. Technical 
346 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
report, University of Tilburg Technical 
Report 99-01. Available from ht tp : / / i l k .  
kub. nl/~ ilk/papers/ilk990 I. ps. 
Fellbaum, Christiane, Joachim Grabowski, 
Shari Landes, and A. Baumann. 1998. 
Matching words to senses in WordNet: 
Naive vs. expert differentiation of senses. 
In Christiane Fellbaum, editor, WordNet: 
An Electronic Lexical Database and Some 
Applications. MIT Press, Cambridge, MA. 
Gaizauskas, Robert, Takahiro Wakao, Kevin 
Humphreys, Hamish Cunningham, and 
Yorick Wilks. 1996. Description of the 
LaSIE system as used for MUC-6. In 
Proceedings ofthe Sixth Message 
Understanding Conference (MUC-6), 
pages 207-220, San Francisco, CA. 
Gale, William, Kenneth Church, and David 
Yarowsky. 1992a. Estimating upper and 
lower bounds on the performance of
word sense disambiguation programs. In 
Proceedings ofthe 30th Annual Meeting of the 
Association for Computational Linguistics 
(ACL-92), pages 249-256, Newark, DE. 
Gale, William, Kenneth Church, and David 
Yarowsky. 1992b. A method for 
disambiguating word senses in a large 
corpus. Computers and the Humanities, 
26:415-439. 
Gale, William, Kenneth Church, and David 
Yarowsky. 1992c. One sense per discourse. 
In Proceedings ofthe DARPA Speech and 
Natural Language Workshop, pages 233-237, 
Harriman, NY. 
Guo, Cheng-Ming. 1989. Constructing a 
Machine Tractable Dictionary from 
Longman Dictionary of Contemporary 
English. Technical Report MCCS-89-156, 
Computing Research Laboratory, New 
Mexico State University. 
Harley, Andrew and Dominic Glennon. 
1997. Sense tagging in action: Combining 
different ests with additive weights. In 
Proceedings ofthe SIGLEX Workshop 
"Tagging Text with Lexical Semantics", 
pages 74-78, Washington, DC. 
Hirst, Graeme. 1987. Semantic Interpretation 
and the Resolution of Ambiguity. Cambridge 
University Press, Cambridge, UK. 
Hirst, Graeme. 1995. Near-synonymy and 
the structure of lexical knowledge. In 
American Association for Artificial Intelligence 
Spring Symposium on Lexicons, pages 51-56. 
Ide, Nancy and Jean V4ronis. 1998. 
Introduction to the special issue on word 
sense disambiguation: The state of the art. 
Computational Linguistics, 24(1):1-40. 
Kilgarriff, Adam. 1993. Dictionary word 
sense distinctions: An enquiry into their 
nature. Computers and the Humanities, 
26:356-387. 
Kilgarriff, Adam. 1997. Sample the lexicon. 
Technical Report ITRI-97-01, ITRI, 
University of Brighton. 
Kilgarriff, Adam. 1998. SENSEVAL: An 
Exercise in Evaluating Word Sense 
Disambiguation Programs. In Proceedings 
of the First International Conference on 
Language Resources and Evaluation, 
pages 581-585, Granada, Spain. 
Knight, Kevin and Steve K. Luk. 1994. 
Building a large knowledge base for 
machine translation. In Proceedings ofthe 
American Association for Arti~cial 
Intelligence Conference (AAAI-94), 
pages 185-109, Seattle, WA. 
Ku~era, Henri and Winthrop Francis. 1967. 
A Computational Analysis of Present-day 
American English. Brown University Press, 
Providence, RI. 
Landes, Shari, Claudia Leacock, and Randee 
Tengi. 1998. Building a semantic 
concordance of English. In C. Fellbaum, 
editor, WordNet: An Electronic Lexical 
Database and Some Applications. MIT Press, 
Cambridge, MA. 
Leacock, Claudia, Geoffrey Towell, and 
Ellen Voorhees. 1993. Corpus-based 
statistical sense resolution. In Proceedings 
of the ARPA Human Language Technology 
Workshop, pages 260-265, Plainsboro, NJ. 
Lesk, Michael. 1986. Automatic sense 
disambiguation using machine readable 
dictionaries: how to tell a pine cone from 
an ice cream cone. In Proceedings ofACM 
SIGDOC Conference, pages 24-26, Toronto. 
Mahesh, Kavi, Sergei Nirenburg, Stephen 
Beale, Evelyne Viegas, Victor Raskin, and 
Boyan Onyshkevych. 1997. Word sense 
disambiguation: Why have statistics when 
we have these numbers? In Proceedings of
the Seventh International Conference on The- 
oretical and Methodological Issues in Machine 
Translation, pages 151-159, Sante Fe, NM. 
Marcus, Mitchell, Beatrice Santorini, and 
Mary Marcinkiewicz. 1993. Building a 
large annotated corpus of English: The 
Penn Tree Bank. Computational Linguistics, 
19(2):313-330. 
Masterman, Margaret. 1957. The thesaurus 
in syntax and semantics. Mechanical 
Translation, 4:1-2. 
McCarthy, J. and P. Hayes. 1969. Some 
philosophical problems from the 
standpoint of artificial intelligence. In B. 
Meltzer and D. Michie, editors, Machine 
Intelligence 4. Edinburgh, Edinburgh 
University Press. pages 463-502. 
McRoy, Susan. 1992. Using multiple 
knowledge sources for word sense 
disambiguation. Computational Linguistics, 
18(1):1-30. 
347 
Computational Linguistics Volume 27, Number 3 
Melamed, Daniel and Philip Resnik. 2000. 
Evaluation of sense disambiguation given 
hierarchical tag sets. Computers and the 
Humanities, 34:1-2. 
Metropolis, Norbert, Anne Rosenbluth, 
Maya Rosenbluth, Andrew Teller, and 
Edward Teller. 1953. Equation state 
calculations by fast computing machines. 
Journal of Chemical Physics, 21:1087-1092. 
Nadas, Andrew, David Nahamoo, Michael 
Picheny, and Jonathan Powell. 1991. An 
iterative "flip-flop" approximation of the 
most informative split in the construction 
of decision trees. In Proceedings ofthe IEEE 
International Conference on Acoustics, Speech 
and Signal Processing, pages 565-568, 
Toronto. 
Ng, Hwee and Hian Lee. 1996. Integrating 
multiple knowledge sources to 
disambiguate word sense: An 
exemplar-based approach. In Proceedings 
of the 34th Meeting of the Association for 
Computational Linguistics (ACL-96), 
pages 40-47, Santa Cruz, CA. 
Press, William, Saul Teukolsky, William 
Vetterling, and Brian Flannery. 1988. 
Numerical Recipes in C: The Art of Scientific 
Computing. Cambridge University Press, 
Cambridge. 
Procter, Paul, editor. 1978. Longman 
Dictionary of Contemporary English. 
Longman Group, Essex, UK. 
Procter, Paul, editor. 1995. Cambridge 
International Dictionary of English. 
Cambridge University Press, Cambridge. 
Quinlan, J. 1993. C4.5: Programs for Machine 
Learning. Morgan Kaufmann, San Mateo, 
CA. 
Resnik, Philip. 1997. Selectional preferences 
and word sense disambiguation. I  
Proceedings ofthe SIGLEX Workshop 
"Tagging Text with Lexical Semantics: What, 
why and how?", pages 52-57, Washington, 
D.C. 
Resnik, Philip and David Yarowsky. 1997. A 
perspective on word sense 
disambiguation techniques and their 
evaluation. In Proceedings ofthe SIGLEX 
Workshop "Tagging Text with Lexical 
Semantics: What, why and how?", 
pages 79-86, Washington, D.C. 
Rigau, German, Jordi Atserias, and Eneko 
Agirre. 1997. Combining unsupervised 
lexical knowledge methods for word 
sense disambiguation. I  35th Meeting of 
the Association for Computational Linguistics 
and the Eighth Meeting of the European 
Chapter of the Association for Computational 
Linguistics (ACL/EACL-97), pages 48-55, 
Madrid, Spain. 
Sch~tze, Hinrich. 1992. Dimensions of 
meaning. In Proceedings ofSupercomputing 
'92, pages 787-796, Minneapolis, MN. 
Stevenson, Mark. 1998. Extracting syntactic 
relations using heuristics. In Proceedings of
the European Summer School on Logic, 
Language and Information '98 Student 
Workshop, ages 248-256, Saarbri~cken, 
Germany. 
Stevenson, Mark and Yorick Wilks. 1999. 
Combining weak knowledge sources for 
sense disambiguation. I  Proceedings ofthe 
Sixteenth International Joint Conference on 
Artificial Intelligence (IJCAI-99), 
pages 884-889, Stockholm, Sweden. 
van Rijsbergen, Keith. 1979. Information 
Retrieval. Butterworths, London. 
V~ronis, Jean and Nancy Ide. 1990. Word 
sense disambiguation with very large 
neural networks extracted from machine 
readable dictionaries. In Proceedings ofthe 
13th International Conference on 
Computational Linguistics (COLING-90), 
pages 389-394, Helsinki. 
Waltz, David and Jordan Pollack. 1985. 
Massively parallel parsing: A strongly 
interactive model of natural anguage 
interpretation. Cognitive Science, 9:51-74. 
Wilks, Yorick. 1975. A preferential 
pattern-seeking semantics for natural 
language inference. Artificial Intelligence, 
6:53-74. 
Wilks, Yorick. 1997. Senses and Texts. 
Computers and the Humanities, 31:77-90. 
Wilks, Yorick, Dan Fass, Cheng-Ming Guo, 
James McDonald, Tony Plate, and Brian 
Slator. 1990. Providing machine tractable 
dictionary tools. Machine Translation, 
5:99-154. 
Wilks, Yorick, Brian Slator, and Louise 
Guthrie. 1996. Electric Words: Dictionaries, 
Computers and Meanings. MIT Press, 
Cambridge, MA. 
Wilks, Yorick and Mark Stevenson. 1998a. 
The grammar of sense: Using 
part-of-speech tags as a first step in 
semantic disambiguation. Journal of 
Natural Language Engineering, 4(2):135-144. 
Wilks, Yorick and Mark Stevenson. 1998b. 
Optimizing combinations of knowledge 
sources for word sense disambiguation. 
In Proceedings ofthe 36th Meeting of the 
Association for Computational Linguistics 
(COLING-ACL-98), pages 1398-1402, 
Montreal. 
Yarowsky, David. 1992. Word-sense 
disambiguation using statistical models of 
Roget's categories trained on large 
corpora. In Proceedings ofthe 14th 
International Conference on Computational 
Linguistics (COLING-92), pages 454-460, 
Nantes, France. 
348 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Yarowsky, David. 1993. One sense per 
collocation. In Proceedings ofthe ARPA 
Human Language Technology Workshop, 
pages 266-271, Princeton, NJ. 
Yarowsky, David. 1995. Unsupervised word 
sense disambiguation rivaling supervised 
methods. In Proceedings ofthe 33rd Annual 
Meeting of the Association for Computational 
Linguistics (ACL-95), pages 189-196, 
Cambridge, MA. 
Yarowsky, David. 1996. Homograph 
disambiguation i  text-to-speech 
synthesis. In J. Hirschberg, R. Sproat, and 
J. van Santen, editors, Progress in Speech 
Synthesis. Springer Verlag, New York, NY, 
pages 159-175. 
Yngve, Victor. 1995. Syntax and the problem 
of multiple meaning. In W. Locke and D. 
Booth, editors, Machine Translation of 
Languages. Wiley, New York. 
Zavrel, Jakub, Walter Daelemans, and Jorn 
Veenstra. 1997. Resolving PP-attachment 
with memory-based learning. In 
Proceedings ofthe Workshop on 
Computational Natural Language Learning 
(CoNLL '97), pages 136-144, Madrid. 
Zipf, Georg. 1935. The Psycho-Biology of 
Language. Houghton Mifflin, Boston, MA. 
349 

Proceedings of the 43rd Annual Meeting of the ACL, pages 379?386,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Semantic Approach to IE Pattern Induction
Mark Stevenson and Mark A. Greenwood
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, UK
marks,m.greenwood@dcs.shef.ac.uk
Abstract
This paper presents a novel algorithm for
the acquisition of Information Extraction
patterns. The approach makes the assump-
tion that useful patterns will have simi-
lar meanings to those already identified
as relevant. Patterns are compared using
a variation of the standard vector space
model in which information from an on-
tology is used to capture semantic sim-
ilarity. Evaluation shows this algorithm
performs well when compared with a
previously reported document-centric ap-
proach.
1 Introduction
Developing systems which can be easily adapted to
new domains with the minimum of human interven-
tion is a major challenge in Information Extraction
(IE). Early IE systems were based on knowledge en-
gineering approaches but suffered from a knowledge
acquisition bottleneck. For example, Lehnert et al
(1992) reported that their system required around
1,500 person-hours of expert labour to modify for
a new extraction task. One approach to this problem
is to use machine learning to automatically learn the
domain-specific information required to port a sys-
tem (Riloff, 1996). Yangarber et al (2000) proposed
an algorithm for learning extraction patterns for a
small number of examples which greatly reduced the
burden on the application developer and reduced the
knowledge acquisition bottleneck.
Weakly supervised algorithms, which bootstrap
from a small number of examples, have the advan-
tage of requiring only small amounts of annotated
data, which is often difficult and time-consuming
to produce. However, this also means that there
are fewer examples of the patterns to be learned,
making the learning task more challenging. Pro-
viding the learning algorithm with access to addi-
tional knowledge can compensate for the limited
number of annotated examples. This paper presents
a novel weakly supervised algorithm for IE pattern
induction which makes use of the WordNet ontology
(Fellbaum, 1998).
Extraction patterns are potentially useful for many
language processing tasks, including question an-
swering and the identification of lexical relations
(such as meronomy and hyponymy). In addition, IE
patterns encode the different ways in which a piece
of information can be expressed in text. For exam-
ple, ?Acme Inc. fired Jones?, ?Acme Inc. let Jones
go?, and ?Jones was given notice by his employers,
Acme Inc.? are all ways of expressing the same fact.
Consequently the generation of extraction patterns is
pertinent to paraphrase identification which is cen-
tral to many language processing problems.
We begin by describing the general process of pat-
tern induction and an existing approach, based on
the distribution of patterns in a corpus (Section 2).
We then introduce a new algorithm which makes use
of WordNet to generalise extraction patterns (Sec-
tion 3) and describe an implementation (Section 4).
Two evaluation regimes are described; one based on
the identification of relevant documents and another
which aims to identify sentences in a corpus which
379
are relevant for a particular IE task (Section 5). Re-
sults on each of these evaluation regimes are then
presented (Sections 6 and 7).
2 Extraction Pattern Learning
We begin by outlining the general process of learn-
ing extraction patterns, similar to one presented by
(Yangarber, 2003).
1. For a given IE scenario we assume the exis-
tence of a set of documents against which the
system can be trained. The documents are
unannotated and may be either relevant (con-
tain the description of an event relevant to the
scenario) or irrelevant although the algorithm
has no access to this information.
2. This corpus is pre-processed to generate the set
of all patterns which could be used to represent
sentences contained in the corpus, call this set
S. The aim of the learning process is to identify
the subset of S representing patterns which are
relevant to the IE scenario.
3. The user provides a small set of seed patterns,
Sseed, which are relevant to the scenario. These
patterns are used to form the set of currently
accepted patterns, Sacc, so Sacc ? Sseed. The
remaining patterns are treated as candidates for
inclusion in the accepted set, these form the set
Scand(= S ? Sacc).
4. A function, f , is used to assign a score to
each pattern in Scand based on those which
are currently in Sacc. This function as-
signs a real number to candidate patterns so
? c  Scand, f(c, Sacc) 7? <. A set of high
scoring patterns (based on absolute scores or
ranks after the set of patterns has been ordered
by scores) are chosen as being suitable for in-
clusion in the set of accepted patterns. These
form the set Slearn.
5. The patterns in Slearn are added to Sacc and
removed from Scand, so Sacc ? Sacc ? Slearn
and Scand ? Sacc ? Slearn
6. If a suitable set of patterns has been learned
then stop, otherwise go to step 4
2.1 Document-centric approach
A key choice in the development of such an algo-
rithm is step 4, the process of ranking the candidate
patterns, which effectively determines which of the
candidate patterns will be learned. Yangarber et al
(2000) chose an approach motivated by the assump-
tion that documents containing a large number of
patterns already identified as relevant to a particu-
lar IE scenario are likely to contain further relevant
patterns. This approach, which can be viewed as be-
ing document-centric, operates by associating confi-
dence scores with patterns and relevance scores with
documents. Initially seed patterns are given a maxi-
mum confidence score of 1 and all others a 0 score.
Each document is given a relevance score based on
the patterns which occur within it. Candidate pat-
terns are ranked according to the proportion of rele-
vant and irrelevant documents in which they occur,
those found in relevant documents far more than in
irrelevant ones are ranked highly. After new patterns
have been accepted all patterns? confidence scores
are updated, based on the documents in which they
occur, and documents? relevance according to the
accepted patterns they contain.
This approach has been shown to successfully ac-
quire useful extraction patterns which, when added
to an IE system, improved its performance (Yangar-
ber et al, 2000). However, it relies on an assump-
tion about the way in which relevant patterns are dis-
tributed in a document collection and may learn pat-
terns which tend to occur in the same documents as
relevant ones whether or not they are actually rele-
vant. For example, we could imagine an IE scenario
in which relevant documents contain a piece of in-
formation which is related to, but distinct from, the
information we aim to extract. If patterns expressing
this information were more likely to occur in rele-
vant documents than irrelevant ones the document-
centric approach would also learn the irrelevant pat-
terns.
Rather than focusing on the documents matched
by a pattern, an alternative approach is to rank pat-
terns according to how similar their meanings are
to those which are known to be relevant. This
semantic-similarity approach avoids the problem
which may be present in the document-centric ap-
proach since patterns which happen to co-occur in
the same documents as relevant ones but have dif-
ferent meanings will not be ranked highly. We now
go on to describe a new algorithm which implements
this approach.
380
3 Semantic IE Pattern Learning
For these experiments extraction patterns consist of
predicate-argument structures, as proposed by Yan-
garber (2003). Under this scheme patterns consist
of triples representing the subject, verb and object
(SVO) of a clause. The first element is the ?se-
mantic? subject (or agent), for example ?John? is a
clausal subject in each of these sentences ?John hit
Bill?, ?Bill was hit by John?, ?Mary saw John hit
Bill?, and ?John is a bully?. The second element is
the verb in the clause and the third the object (pa-
tient) or predicate. ?Bill? is a clausal object in the
first three example sentences and ?bully? in the final
one. When a verb is being used intransitively, the
pattern for that clause is restricted to only the first
pair of elements.
The filler of each pattern element can be either
a lexical item or semantic category such as per-
son name, country, currency values, numerical ex-
pressions etc. In this paper lexical items are rep-
resented in lower case and semantic categories are
capitalised. For example, in the pattern COM-
PANY+fired+ceo, fired and ceo are lexical
items and COMPANY a semantic category which
could match any lexical item belonging to that type.
The algorithm described here relies on identify-
ing patterns with similar meanings. The approach
we have developed to do this is inspired by the
vector space model which is commonly used in
Information Retrieval (Salton and McGill, 1983)
and language processing in general (Pado and La-
pata, 2003). Each pattern can be represented as
a set of pattern element-filler pairs. For exam-
ple, the pattern COMPANY+fired+ceo consists
of three pairs: subject COMPANY, verb fired
and object ceo. Each pair consists of either a
lexical item or semantic category, and pattern ele-
ment. Once an appropriate set of pairs has been es-
tablished a pattern can be represented as a binary
vector in which an element with value 1 denotes that
the pattern contains a particular pair and 0 that it
does not.
3.1 Pattern Similarity
The similarity of two pattern vectors can be com-
pared using the measure shown in Equation 1. Here
~a and~b are pattern vectors, ~bT the transpose of~b and
Patterns Matrix labels
a. chairman+resign 1. subject chairman
b. ceo+quit 2. subject ceo
c. chairman+comment 3. verb resign
4. verb quit
5. verb comment
Similarity matrix Similarity values
1 0.95 0 0 0
0.95 1 0 0 0
0 0 1 0.9 0.1
0 0 0.9 1 0.1
0 0 0.1 0.1 1
sim(~a,~b) = 0.925
sim(~a, ~c) = 0.55
sim(~b, ~c) = 0.525
Figure 1: Similarity scores and matrix for an exam-
ple vector space formed from three patterns
W a matrix that lists the similarity between each of
the possible pattern element-filler pairs.
sim(~a,~b) = ~aW
~bT
|~a||~b|
(1)
The semantic similarity matrix W contains infor-
mation about the similarity of each pattern element-
filler pair stored as non-negative real numbers and is
crucial for this measure. Assume that the set of pat-
terns, P , consists of n element-filler pairs denoted
by p1, p2, ...pn. Each row and column of W rep-
resents one of these pairs and they are consistently
labelled. So, for any i such that 1 ? i ? n, row i and
column i are both labelled with pair pi. If wij is the
element of W in row i and column j then the value
of wij represents the similarity between the pairs pi
and pj . Note that we assume the similarity of two
element-filler pairs is symmetric, so wij = wji and,
consequently, W is a symmetric matrix. Pairs with
different pattern elements (i.e. grammatical roles)
are automatically given a similarity score of 0. Di-
agonal elements of W represent the self-similarity
between pairs and have the greatest values.
Figure 1 shows an example using three patterns,
chairman+resign, ceo+quit and chair-
man+comment. This shows how these patterns are
represented as vectors and gives a sample semantic
similarity matrix. It can be seen that the first pair
of patterns are the most similar using the proposed
measure.
The measure in Equation 1 is similar to the cosine
metric, commonly used to determine the similarity
of documents in the vector space model approach
381
to Information Retrieval. However, the cosine met-
ric will not perform well for our application since it
does not take into account the similarity between el-
ements of a vector and would assign equal similarity
to each pair of patterns in the example shown in Fig-
ure 1.1 The semantic similarity matrix in Equation 1
provides a mechanism to capture semantic similar-
ity between lexical items which allows us to identify
chairman+resign and ceo+quit as the most
similar pair of patterns.
3.2 Populating the Matrix
It is important to choose appropriate values for the
elements of W . We chose to make use of the re-
search that has concentrated on computing similar-
ity between pairs of lexical items using the WordNet
hierarchy (Resnik, 1995; Jiang and Conrath, 1997;
Patwardhan et al, 2003). We experimented with
several of the measures which have been reported
in the literature and found that the one proposed by
Jiang and Conrath (1997) to be the most effective.
The similarity measure proposed by Jiang and
Conrath (1997) relies on a technique developed by
Resnik (1995) which assigns numerical values to
each sense in the WordNet hierarchy based upon
the amount of information it represents. These val-
ues are derived from corpus counts of the words in
the synset, either directly or via the hyponym rela-
tion and are used to derive the Information Content
(IC) of a synset c thus IC(c) = ? log(Pr(c)). For
two senses, s1 and s2, the lowest common subsumer,
lcs(s1, s2), is defined as the sense with the highest
information content (most specific) which subsumes
both senses in the WordNet hierarchy. Jiang and
Conrath used these elements to calculate the seman-
tic distance between a pair or words, w1 and w2, ac-
cording to this formula (where senses(w) is the set
1The cosine metric for a pair of vectors is given by the cal-
culation a.b|a||b| . Substituting the matrix multiplication in the nu-
merator of Equation 1 for the dot product of vectors ~a and ~b
would give the cosine metric. Note that taking the dot product
of a pair of vectors is equivalent to multiplying by the identity
matrix, i.e. ~a.~b = ~aI ~bT . Under our interpretation of the simi-
larity matrix, W , this equates to each pattern element-filler pair
being identical to itself but not similar to anything else.
of all possible WordNet senses for word w):
ARGMAX
s1  senses(w1),
s2  senses(w2)
IC(s1)+IC(s2)?2?IC(lcs(s1, s2))
(2)
Patwardhan et al (2003) convert this distance
metric into a similarity measure by taking its mul-
tiplicative inverse. Their implementation was used
in the experiments described later.
As mentioned above, the second part of a pattern
element-filler pair can be either a lexical item or a
semantic category, such as company. The identifiers
used to denote these categories, i.e. COMPANY, do
not appear in WordNet and so it is not possible to
directly compare their similarity with other lexical
items. To avoid this problem these tokens are man-
ually mapped onto the most appropriate node in the
WordNet hierarchy which is then used for similar-
ity calculations. This mapping process is not partic-
ularly time-consuming since the number of named
entity types with which a corpus is annotated is usu-
ally quite small. For example, in the experiments
described in this paper just seven semantic classes
were sufficient to annotate the corpus.
3.3 Learning Algorithm
This pattern similarity measure can be used to create
a weakly supervised approach to pattern acquisition
following the general outline provided in Section 2.
Each candidate pattern is compared against the set
of currently accepted patterns using the measure de-
scribed in Section 3.1. We experimented with sev-
eral techniques for ranking candidate patterns based
on these scores, including using the best and aver-
age score, and found that the best results were ob-
tained when each candidate pattern was ranked ac-
cording to its score when compared against the cen-
troid vector of the set of currently accepted patterns.
We also experimented with several schemes for de-
ciding which of the scored patterns to accept (a full
description would be too long for this paper) result-
ing in a scheme where the four highest scoring pat-
terns whose score is within 0.95 of the best pattern
are accepted.
Our algorithm disregards any patterns whose cor-
pus occurrences are below a set threshold, ?, since
these may be due to noise. In addition, a second
382
threshold, ?, is used to determine the maximum
number of documents in which a pattern can occur
since these very frequent patterns are often too gen-
eral to be useful for IE. Patterns which occur in more
than ? ?C, where C is the number of documents in
the collection, are not learned. For the experiments
in this paper we set ? to 2 and ? to 0.3.
4 Implementation
A number of pre-processing stages have to be ap-
plied to documents in order for the set of patterns to
be extracted before learning can take place. Firstly,
items belonging to semantic categories are identi-
fied by running the text through the named entity
identifier in the GATE system (Cunningham et al,
2002). The corpus is then parsed, using a ver-
sion of MINIPAR (Lin, 1999) adapted to process
text marked with named entities, to produce depen-
dency trees from which SVO-patterns are extracted.
Active and passive voice is taken into account in
MINIPAR?s output so the sentences ?COMPANY
fired their C.E.O.? and ?The C.E.O. was fired by
COMPANY? would yield the same triple, COM-
PANY+fire+ceo. The indirect object of ditran-
sitive verbs is not extracted; these verbs are treated
like transitive verbs for the purposes of this analysis.
An implementation of the algorithm described
in Section 3 was completed in addition to an im-
plementation of the document-centric algorithm de-
scribed in Section 2.1. It is important to mention
that this implementation is not identical to the one
described by Yangarber et al (2000). Their system
makes some generalisations across pattern elements
by grouping certain elements together. However,
there is no difference between the expressiveness of
the patterns learned by either approach and we do
not believe this difference has any effect on the re-
sults of our experiments.
5 Evaluation
Various approaches have been suggested for the
evaluation of automatic IE pattern acquisition.
Riloff (1996) judged the precision of patterns
learned by reviewing them manually. Yangarber et
al. (2000) developed an indirect method which al-
lowed automatic evaluation. In addition to learning
a set of patterns, their system also notes the rele-
vance of documents based on the current set of ac-
cepted patterns. Assuming the subset of documents
relevant to a particular IE scenario is known, it is
possible to use these relevance judgements to de-
termine how accurately a given set of patterns can
discriminate the relevant documents from the irrele-
vant. This evaluation is similar to the ?text-filtering?
sub-task used in the sixth Message Understanding
Conference (MUC-6) (1995) in which systems were
evaluated according to their ability to identify the
documents relevant to the extraction task. The doc-
ument filtering evaluation technique was used to al-
low comparison with previous studies.
Identifying the document containing relevant in-
formation can be considered as a preliminary stage
of an IE task. A further step is to identify the sen-
tences within those documents which are relevant.
This ?sentence filtering? task is a more fine-grained
evaluation and is likely to provide more information
about how well a given set of patterns is likely to
perform as part of an IE system. Soderland (1999)
developed a version of the MUC-6 corpus in which
events are marked at the sentence level. The set of
patterns learned by the algorithm after each iteration
can be compared against this corpus to determine
how accurately they identify the relevant sentences
for this extraction task.
5.1 Evaluation Corpus
The evaluation corpus used for the experiments was
compiled from the training and testing corpus used
in MUC-6, where the task was to extract information
about the movements of executives from newswire
texts. A document is relevant if it has a filled tem-
plate associated with it. 590 documents from a ver-
sion of the MUC-6 evaluation corpus described by
Soderland (1999) were used.
After the pre-processing stages described in Sec-
tion 4, the MUC-6 corpus produced 15,407 pattern
tokens from 11,294 different types. 10,512 patterns
appeared just once and these were effectively dis-
carded since our learning algorithm only considers
patterns which occur at least twice (see Section 3.3).
The document-centric approach benefits from a
large corpus containing a mixture of relevant and ir-
relevant documents. We provided this using a subset
of the Reuters Corpus Volume I (Rose et al, 2002)
which, like the MUC-6 corpus, consists of newswire
383
COMPANY+appoint+PERSON
COMPANY+elect+PERSON
COMPANY+promote+PERSON
COMPANY+name+PERSON
PERSON+resign
PERSON+depart
PERSON+quit
Table 1: Seed patterns for extraction task
texts. 3000 documents relevant to the management
succession task (identified using document meta-
data) and 3000 irrelevant documents were used to
produce the supplementary corpus. This supple-
mentary corpus yielded 126,942 pattern tokens and
79,473 types with 14,576 of these appearing more
than once. Adding the supplementary corpus to the
data set used by the document-centric approach led
to an improvement of around 15% on the document
filtering task and over 70% for sentence filtering. It
was not used for the semantic similarity algorithm
since there was no benefit.
The set of seed patterns listed in Table 1 are in-
dicative of the management succession extraction
task and were used for these experiments.
6 Results
6.1 Document Filtering
Results for both the document and sentence filter-
ing experiments are reported in Table 2 which lists
precision, recall and F-measure for each approach
on both evaluations. Results from the document fil-
tering experiment are shown on the left hand side
of the table and continuous F-measure scores for
the same experiment are also presented in graphi-
cal format in Figure 2. While the document-centric
approach achieves the highest F-measure of either
system (0.83 on the 33rd iteration compared against
0.81 after 48 iterations of the semantic similarity ap-
proach) it only outperforms the proposed approach
for a few iterations. In addition the semantic sim-
ilarity approach learns more quickly and does not
exhibit as much of a drop in performance after it has
reached its best value. Overall the semantic sim-
ilarity approach was found to be significantly bet-
ter than the document-centric approach (p < 0.001,
Wilcoxon Signed Ranks Test).
Although it is an informative evaluation, the doc-
ument filtering task is limited for evaluating IE pat-
0 20 40 60 80 100 120
Iteration
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
F-
m
ea
su
re
Semantic Similarity
Document-centric
Figure 2: Evaluating document filtering.
tern learning. This evaluation indicates whether the
set of patterns being learned can identify documents
containing descriptions of events but does not pro-
vide any information about whether it can find those
events within the documents. In addition, the set of
seed patterns used for these experiments have a high
precision and low recall (Table 2). We have found
that the distribution of patterns and documents in
the corpus means that learning virtually any pattern
will help improve the F-measure. Consequently, we
believe the sentence filtering evaluation to be more
useful for this problem.
6.2 Sentence Filtering
Results from the sentence filtering experiment are
shown in tabular format in the right hand side of
Table 22 and graphically in Figure 3. The seman-
tic similarity algorithm can be seen to outperform
the document-centric approach. This difference is
also significant (p < 0.001, Wilcoxon Signed Ranks
Text).
The clear difference between these results shows
that the semantic similarity approach can indeed
identify relevant sentences while the document-
centric method identifies patterns which match rel-
evant documents, although not necessarily relevant
sentences.
2The set of seed patterns returns a precision of 0.81 for this
task. The precision is not 1 since the pattern PERSON+resign
matches sentences describing historical events (?Jones resigned
last year.?) which were not marked as relevant in this corpus
following MUC guidelines.
384
Document Filtering Sentence Filtering
Number of Document-centric Semantic similarity Document-centric Semantic similarity
Iterations P R F P R F P R F P R F
0 1.00 0.26 0.42 1.00 0.26 0.42 0.81 0.10 0.18 0.81 0.10 0.18
20 0.75 0.68 0.71 0.77 0.78 0.77 0.30 0.29 0.29 0.61 0.49 0.54
40 0.72 0.96 0.82 0.70 0.93 0.80 0.40 0.67 0.51 0.47 0.64 0.55
60 0.65 0.96 0.78 0.68 0.96 0.80 0.32 0.70 0.44 0.42 0.73 0.54
80 0.56 0.96 0.71 0.61 0.98 0.76 0.18 0.71 0.29 0.37 0.89 0.52
100 0.56 0.96 0.71 0.58 0.98 0.73 0.18 0.73 0.28 0.28 0.92 0.42
120 0.56 0.96 0.71 0.58 0.98 0.73 0.17 0.75 0.28 0.26 0.95 0.41
Table 2: Comparison of the different approaches over 120 iterations
0 20 40 60 80 100 120
Iteration
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
F-
m
ea
su
re
Semantic Similarity
Document-centric
Figure 3: Evaluating sentence filtering.
The precision scores for the sentence filtering task
in Table 2 show that the semantic similarity al-
gorithm consistently learns more accurate patterns
than the existing approach. At the same time it
learns patterns with high recall much faster than the
document-centric approach, by the 120th iteration
the pattern set covers almost 95% of relevant sen-
tences while the document-centric approach covers
only 75%.
7 Discussion
The approach to IE pattern acquisition presented
here is related to other techniques but uses differ-
ent assumptions regarding which patterns are likely
to be relevant to a particular extraction task. Eval-
uation has showed that the semantic generalisa-
tion approach presented here performs well when
compared to a previously reported document-centric
method. Differences between the two approaches
are most obvious when the results of the sentence
filtering task are considered and it seems that this is
a more informative evaluation for this problem. The
semantic similarity approach has the additional ad-
vantage of not requiring a large corpus containing a
mixture of documents relevant and irrelevant to the
extraction task. This corpus is unannotated, and so
may not be difficult to obtain, but is nevertheless an
additional requirement.
The best score recorded by the proposed algo-
rithm on the sentence filtering task is an F-measure
of 0.58 (22nd iteration). While this result is lower
than those reported for IE systems based on knowl-
edge engineering approaches these results should be
placed in the context of a weakly supervised learning
algorithm which could be used to complement man-
ual approaches. These results could be improved by
manual filtering the patterns identified by the algo-
rithm.
The learning algorithm presented in Section 3 in-
cludes a mechanism for comparing two extraction
patterns using information about lexical similarity
derived from WordNet. This approach is not re-
stricted to this application and could be applied to
other language processing tasks such as question an-
swering, paraphrase identification and generation or
as a variant of the vector space model commonly
used in Information Retrieval. In addition, Sudo
et al (2003) proposed representations for IE pat-
terns which extends the SVO representation used
here and, while they did not appear to significantly
improve IE, it is expected that it will be straightfor-
ward to extend the vector space model to those pat-
385
tern representations.
One of the reasons for the success of the approach
described here is the appropriateness of WordNet
which is constructed on paradigmatic principles,
listing the words which may be substituted for one
another, and is consequently an excellent resource
for this application. WordNet is also a generic
resource not associated with a particular domain
which means the learning algorithm can make use
of that knowledge to acquire patterns for a diverse
range of IE tasks. This work represents a step to-
wards truly domain-independent IE systems. Em-
ploying a weakly supervised learning algorithm re-
moves much of the requirement for a human anno-
tator to provide example patterns. Such approaches
are often hampered by a lack of information but the
additional knowledge in WordNet helps to compen-
sate.
Acknowledgements
This work was carried out as part of the RE-
SuLT project funded by the EPSRC (GR/T06391).
Roman Yangarber provided advice on the re-
implementation of the document-centric algorithm.
We are also grateful for the detailed comments pro-
vided by the anonymous reviewers of this paper.
References
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: an Architecture for Devel-
opment of Robust HLT. In Proceedings of the 40th
Anniversary Meeting of the Association for Computa-
tional Linguistics (ACL-02), pages 168?175, Philadel-
phia, PA.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database and some of its Applications. MIT Press,
Cambridge, MA.
J. Jiang and D. Conrath. 1997. Semantic similarity based
on corpus statistics and lexical taxonomy. In Proceed-
ings of International Conference on Research in Com-
putational Linguistics, Taiwan.
W. Lehnert, C. Cardie, D. Fisher, J. McCarthy, E. Riloff,
and S. Soderland. 1992. University of Massachusetts:
Description of the CIRCUS System used for MUC-4.
In Proceedings of the Fourth Message Understanding
Conference (MUC-4), pages 282?288, San Francisco,
CA.
D. Lin. 1999. MINIPAR: a minimalist parser. In Mary-
land Linguistics Colloquium, University of Maryland,
College Park.
MUC. 1995. Proceedings of the Sixth Message Under-
standing Conference (MUC-6), San Mateo, CA. Mor-
gan Kaufmann.
S. Pado and M. Lapata. 2003. Constructing semantic
space models from parsed corpora. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL-03), pages 128?135, Sap-
poro, Japan.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003. Us-
ing measures of semantic relatedness for word sense
disambiguation. In Proceedings of the Fourth Inter-
national Conferences on Intelligent Text Processing
and Computational Linguistics, pages 241?257, Mex-
ico City.
P. Resnik. 1995. Using Information Content to evalu-
ate Semantic Similarity in a Taxonomy. In Proceed-
ings of the 14th International Joint Conference on Ar-
tificial Intelligence (IJCAI-95), pages 448?453, Mon-
treal, Canada.
E. Riloff. 1996. Automatically generating extraction
patterns from untagged text. In Thirteenth National
Conference on Artificial Intelligence (AAAI-96), pages
1044?1049, Portland, OR.
T. Rose, M. Stevenson, and M. Whitehead. 2002. The
Reuters Corpus Volume 1 - from Yesterday?s news to
tomorrow?s language resources. In LREC-02, pages
827?832, La Palmas, Spain.
G. Salton and M. McGill. 1983. Introduction to Modern
Information Retrieval. McGraw-Hill, New York.
S. Soderland. 1999. Learning Information Extraction
Rules for Semi-structured and free text. Machine
Learning, 31(1-3):233?272.
K. Sudo, S. Sekine, and R. Grishman. 2003. An Im-
proved Extraction Pattern Representation Model for
Automatic IE Pattern Acquisition. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL-03), pages 224?231.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic acquisition of domain
knowledge for information extraction. In Proceed-
ings of the 18th International Conference on Compu-
tational Linguistics (COLING 2000), pages 940?946,
Saarbru?cken, Germany.
R. Yangarber. 2003. Counter-training in the discovery of
semantic patterns. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics (ACL-03), pages 343?350, Sapporo, Japan.
386
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 41?48,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Learning Expressive Models for Word Sense Disambiguation 
Lucia Specia 
NILC/ICMC 
University of S?o Paulo 
Caixa Postal 668, 13560-970 
S?o Carlos, SP, Brazil 
lspecia@icmc.usp.br 
Mark Stevenson 
Department of Computer Science 
University of Sheffield 
Regent Court, 211 Portobello St. 
Sheffield, S1 4DP, UK  
marks@dcs.shef.ac.uk 
Maria das Gra?as V. Nunes 
NILC/ICMC 
University of S?o Paulo 
Caixa Postal 668, 13560-970  
S?o Carlos, SP, Brazil 
gracan@icmc.usp.br 
 
 
Abstract 
We present a novel approach to the word 
sense disambiguation problem which 
makes use of corpus-based evidence com-
bined with background knowledge. Em-
ploying an inductive logic programming 
algorithm, the approach generates expres-
sive disambiguation rules which exploit 
several knowledge sources and can also 
model relations between them. The ap-
proach is evaluated in two tasks: identifica-
tion of the correct translation for a set of 
highly ambiguous verbs in English-
Portuguese translation and disambiguation 
of verbs from the Senseval-3 lexical sam-
ple task. The average accuracy obtained for 
the multilingual task outperforms the other 
machine learning techniques investigated. 
In the monolingual task, the approach per-
forms as well as the state-of-the-art sys-
tems which reported results for the same 
set of verbs. 
1 Introduction 
Word Sense Disambiguation (WSD) is concerned 
with the identification of the meaning of ambi-
guous words in context. For example, among the 
possible senses of the verb ?run? are ?to move fast 
by using one's feet? and ?to direct or control?. 
WSD can be useful for many applications, includ-
ing information retrieval, information extraction 
and machine translation. Sense ambiguity has been 
recognized as one of the most important obstacles 
to successful language understanding since the ear-
ly 1960?s and many techniques have been pro-
posed to solve the problem. Recent approaches 
focus on the use of various lexical resources and 
corpus-based techniques in order to avoid the sub-
stantial effort required to codify linguistic know-
ledge. These approaches have shown good results; 
particularly those using supervised learning (see 
Mihalcea et al, 2004 for an overview of state-of-
the-art systems). However, current approaches rely 
on limited knowledge representation and modeling 
techniques: traditional machine learning algorithms 
and attribute-value vectors to represent disambigu-
ation instances. This has made it difficult to exploit 
deep knowledge sources in the generation of the 
disambiguation models, that is, knowledge that 
goes beyond simple features extracted directly 
from the corpus, like bags-of-words and colloca-
tions, or provided by shallow natural language 
tools like part-of-speech taggers.  
In this paper we present a novel approach for 
WSD that follows a hybrid strategy, i.e. combines 
knowledge and corpus-based evidence, and em-
ploys a first-order formalism to allow the represen-
tation of deep knowledge about disambiguation 
examples together with a powerful modeling tech-
nique to induce theories based on the examples and 
background knowledge. This is achieved using 
Inductive Logic Programming (ILP) (Muggleton, 
1991), which has not yet been applied to WSD.  
Our hypothesis is that by using a very expres-
sive representation formalism, a range of (shallow 
and deep) knowledge sources and ILP as learning 
technique, it is possible to generate models that, 
when compared to models produced by machine 
learning algorithms conventionally applied to 
41
WSD, are both more accurate for fine-grained dis-
tinctions, and ?interesting?, from a knowledge ac-
quisition point of view (i.e., convey potentially 
new knowledge that can be easily interpreted by 
humans).  
WSD systems have generally been more suc-
cessful in the disambiguation of nouns than other 
grammatical categories (Mihalcea et al, 2004). A 
common approach to the disambiguation of nouns 
has been to consider a wide context around the 
ambiguous word and treat it as a bag of words or 
limited set of collocates. However, disambiguation 
of verbs generally benefits from more specific 
knowledge sources, such as the verb?s relation to 
other items in the sentence (for example, by ana-
lysing the semantic type of its subject and object). 
Consequently, we believe that the disambiguation 
of verbs is task to which ILP is particularly well-
suited. Therefore, this paper focuses on the disam-
biguation of verbs, which is an interesting task 
since much of the previous work on WSD has con-
centrated on the disambiguation of nouns.  
WSD is usually approached as an independent 
task, however, it has been argued that different 
applications may have specific requirements (Res-
nik and Yarowsky, 1997). For example, in machine 
translation, WSD, or translation disambiguation, is 
responsible for identifying the correct translation 
for an ambiguous source word. There is not always 
a direct relation between the possible senses for a 
word in a (monolingual) lexicon and its transla-
tions to a particular language, so this represents a 
different task to WSD against a (monolingual) 
lexicon (Hutchins and Somers, 1992). Although it 
has been argued that WSD does not yield better 
translation quality than a machine translation 
system alone, it has been recently shown that a 
WSD module that is developed following specific 
multilingual requirements can significantly im-
prove the performance of a machine translation 
system (Carpuat et al, 2006). 
This paper focuses on the application of our ap-
proach to the translation of verbs in English to Por-
tuguese translation, specifically for a set of 10 
mainly light and highly ambiguous verbs. We also 
experiment with a monolingual task by using the 
verbs from Senseval-3 lexical sample task. We 
explore knowledge from 12 syntactic, semantic 
and pragmatic sources. In principle, the proposed 
approach could also be applied to any lexical dis-
ambiguation task by customizing the sense reposi-
tory and knowledge sources. 
In the remainder of this paper we first present 
related approaches to WSD and discuss their limi-
tations (Section 2). We then describe some basic 
concepts on ILP and our application of this tech-
nique to WSD (Section 3). Finally, we described 
our experiments and their results (Section 4).  
2 Related Work 
WSD approaches can be classified as (a) know-
ledge-based approaches, which make use of lin-
guistic knowledge, manually coded or extracted 
from lexical resources (Agirre and Rigau, 1996; 
Lesk 1986); (b) corpus-based approaches, which 
make use of shallow knowledge automatically ac-
quired from corpus and statistical or machine 
learning algorithms to induce disambiguation 
models (Yarowsky, 1995; Sch?tze 1998); and (c) 
hybrid approaches, which mix characteristics from 
the two other approaches to automatically acquire 
disambiguation models from corpus supported by 
linguistic knowledge (Ng and Lee 1996; Stevenson 
and Wilks, 2001). 
Hybrid approaches can combine advantages 
from both strategies, potentially yielding accurate 
and comprehensive systems, particularly when 
deep knowledge is explored. Linguistic knowledge 
is available in electronic resources suitable for 
practical use, such as WordNet (Fellbaum, 1998), 
dictionaries and parsers. However, the use of this 
information has been hampered by the limitations 
of the modeling techniques that have been ex-
plored so far: using deep sources of domain know-
ledge is beyond the capabilities of such techniques, 
which are in general based on attribute-value vec-
tor representations. 
Attribute-value vectors consist of a set of 
attributes intended to represent properties of the 
examples. Each attribute has a type (its name) and 
a single value for a given example. Therefore, 
attribute-value vectors have the same expressive-
ness as propositional formalisms, that is, they only 
allow the representation of atomic propositions and 
constants. These are the representations used by 
most of the machine learning algorithms conven-
tionally employed to WSD, for example Na?ve 
Bayes and decision-trees. First-order logic, a more 
expressive formalism which is employed by ILP, 
allows the representation of variables and n-ary 
predicates, i.e., relational knowledge.  
42
In the hybrid approaches that have been ex-
plored so far, deep knowledge, like selectional pre-
ferences, is either pre-processed into a vector 
representation to accommodate machine learning 
algorithms, or used in previous steps to filter out 
possible senses e.g. (Stevenson and Wilks, 2001). 
This may cause information to be lost and, in addi-
tion, deep knowledge sources cannot interact in the 
learning process. As a consequence, the models 
produced reflect only the shallow knowledge that 
is provided to the learning algorithm.  
Another limitation of attribute-value vectors is 
the need for a unique representation for all the ex-
amples: one attribute is created for every knowl-
edge feature and the same structure is used to 
characterize all the examples. This usually results 
in a very sparse representation of the data, given 
that values for certain features will not be available 
for many examples. The problem of data sparse-
ness increases as more knowledge is exploited and 
this can cause problems for the machine learning 
algorithms. 
A final disadvantage of attribute-value vectors 
is that equivalent features may have to be bounded 
to distinct identifiers. An example of this occurs 
when the syntactic relations between words in a 
sentence are represented by attributes for each pos-
sible relation, sentences in which there is more 
than one instantiation for a particular grammatical 
role cannot be easily represented.  For example, the 
sentence ?John and Anna gave Mary a present.? 
contains a coordinate subject and, since each fea-
ture requires a unique identifier, two are required 
(subj1-verb1, subj2-verb1). These will be treated as 
two independent pieces of knowledge by the learn-
ing algorithm.  
First-order formalisms allow a generic predicate 
to be created for every possible syntactic role, re-
lating two or more elements. For example 
has_subject(verb, subject), which could then have 
two instantiations: has_subject(give, john) and 
has_subject(give, anna). Since each example is 
represented independently from the others, the data 
sparseness problem is minimized. Therefore, ILP 
seems to provide the most general-purpose frame-
work for dealing with such data: it does not suffer 
from the limitations mentioned above since there 
are explicit provisions made for the inclusion of 
background knowledge of any form, and the repre-
sentation language is powerful enough to capture 
contextual relationships. 
3 A hybrid relational approach to WSD 
In what follows we provide an introduction to ILP 
and then outline how it is applied to WSD by pre-
senting the sample corpus and knowledge sources 
used in our experiments. 
3.1 Inductive Logic Programming 
Inductive Logic Programming (Muggleton, 1991) 
employs techniques from Machine Learning and 
Logic Programming to build first-order theories 
from examples and background knowledge, which 
are also represented by first-order clauses. It allows 
the efficient representation of substantial know-
ledge about the problem, which is used during the 
learning process, and produces disambiguation 
models that can make use of this knowledge. The 
general approach underlying ILP can be outlined 
as follows:  
Given: 
-  a set of positive and negative examples E = 
E+ ? E- 
- a predicate p specifying the target relation to 
be learned 
- knowledge ? of the domain, described ac-
cording to a language Lk, which specifies which 
predicates qi can be part of the definition of p. 
The goal is: to induce a hypothesis (or theory) 
h for p, with relation to E and ?, which covers 
most of the E+, without covering the E-, i.e., K ? h 
 E+ and K ? h  E-.  
 
We use the Aleph ILP system (Srinivasan, 2000), 
which provides a complete inference engine and 
can be customized in various ways. The default 
inference engine induces a theory iteratively using 
the following steps: 
1. One instance is randomly selected to be gen-
eralized.  
2. A more specific clause (the bottom clause) is 
built using inverse entailment (Muggleton, 1995), 
generally consisting of the representation of all the 
knowledge about that example. 
3. A clause that is more generic than the bottom 
clause is searched for using a given search (e.g., 
best-first) and evaluation strategy (e.g., number of 
positive examples covered). 
4. The best clause is added to the theory and the 
examples covered by that clause are removed from 
the sample set. Stop if there are more no examples 
in the training set, otherwise return to step 1. 
43
3.2 Sample data 
This approach was evaluated using two scenarios: 
(1) an English-Portuguese multilingual setting ad-
dressing 10 very frequent and problematic verbs 
selected in a previous study (Specia et. al., 2005); 
and (2) an English setting consisting of 32 verbs 
from Senseval-3 lexical sample task (Mihalcea et. 
al. 2004). 
For the first scenario a corpus containing 500 
sentences for each of the 10 verbs was constructed. 
The text was randomly selected from corpora of 
different domains and genres, including literary 
fiction, Bible, computer science dissertation ab-
stracts, operational system user manuals, newspa-
pers and European Parliament proceedings. This 
corpus was automatically annotated with the trans-
lation of the verb using a tagging system based on 
parallel corpus, statistical information and transla-
tion dictionaries (Specia et al, 2005), followed by 
a manual revision. For each verb, the sense reposi-
tory was defined as the set of all the possible trans-
lations of that verb in the corpus. 80% of the 
corpus was randomly selected and used for train-
ing, with the remainder retained for testing. The 10 
verbs, number of possible translations and the per-
centage of sentences for each verb which use the 
most frequent translation are shown in Table 1. 
For the monolingual scenario, we use the sense 
tagged corpus and sense repositories provided for 
verbs in Senseval-3. There are 32 verbs with be-
tween 40 and 398 examples each. The number of 
senses varies between 3 and 10 and the average 
percentage of examples with the majority (most 
frequent) sense is 55%.  
 
 Verb # Translations Most frequent 
translation - % 
ask 7 53 
come 29 36 
get 41 13 
give 22 72 
go 30 53 
live 8 66 
look 12 41 
make 21 70 
take 32 25 
tell 8 66 
Table 1. Verbs and possible senses in our corpus 
 
Both corpora were lemmatized and part-of-speech 
(POS) tagged using Minipar (Lin, 1993) and 
Mxpost (Ratnaparkhi, 1996), respectivelly. Addi-
tionally, proper nouns identified by the tagger were 
replaced by a single identifier (proper_noun) and 
pronouns replaced by identifiers representing 
classes of pronouns (relative_pronoun, etc.).  
3.3 Knowledge sources 
We now describe the background knowledge 
sources used by the learning algorithm, having as 
an example sentence (1), in which the word ?com-
ing? is the target verb being disambiguated. 
 
(1) "If there is such a thing as reincarnation, I 
would not mind coming back as a squirrel". 
 
KS1. Bag-of-words consisting of 5 words to the 
right and left of the verb (excluding stop words), 
represented using definitions of the form 
has_bag(snt, word): 
has_bag(snt1, mind). 
has_bag(snt1, not). ? 
 
KS2. Frequent bigrams consisting of pairs of adja-
cent words in a sentence (other than the target 
verb) which occur more than 10 times in the cor-
pus, represented by has_bigram(snt, word1, 
word2): 
has_bigram(snt1, back, as). 
has_bigram(snt1, such, a). ? 
 
KS3. Narrow context containing 5 content words to 
the right and left of the verb, identified using POS 
tags, represented by has_narrow(snt, 
word_position, word): 
has_narrow(snt1, 1st_word_left, mind). 
has_narrow(snt1, 1st_word_right, back). ? 
 
KS4. POS tags of 5 words to the right and left of 
the verb, represented by has_pos(snt, 
word_position, pos): 
has pos(snt1, 1st_word_left, nn). 
has pos(snt1, 1st_word_right, rb). ? 
 
KS5. 11 collocations of the verb: 1st preposition to 
the right, 1st and 2nd words to the left and right, 
1st noun, 1st adjective, and 1st verb to the left and 
right. These are represented using definitions of the 
form has_collocation(snt, type, collocation): 
has_collocation(snt1, 1st_prep_right, back). 
has_collocation(snt1, 1st_noun_left, mind).? 
44
KS6. Subject and object of the verb obtained using 
Minipar and represented by has_rel(snt, type, 
word): 
has_rel(snt1, subject, i). 
has_rel(snt1, object, nil). ? 
 
KS7. Grammatical relations not including the tar-
get verb also identified using Minipar. The rela-
tions (verb-subject, verb-object, verb-modifier, 
subject-modifier, and object-modifier) occurring 
more than 10 times in the corpus are represented 
by has_related_pair(snt, word1, word2): 
has_related_pair(snt1, there, be). ? 
 
KS8. The sense with the highest count of overlap-
ping words in its dictionary definition and in the 
sentence containing the target verb (excluding stop 
words) (Lesk, 1986), represented by 
has_overlapping(sentence, translation): 
has_overlapping(snt1, voltar). 
 
KS9. Selectional restrictions of the verbs defined 
using LDOCE (Procter, 1978). WordNet is used 
when the restrictions imposed by the verb are not 
part of the description of its arguments, but can be 
satisfied by synonyms or hyperonyms of those ar-
guments. A hierarchy of feature types is used to 
account for restrictions established by the verb that 
are more generic than the features describing its 
arguments in the sentence. This information is 
represented by definitions of the form satis-
fy_restriction(snt, rest_subject, rest_object): 
satisfy_restriction(snt1, [human], nil). 
satisfy_restriction(snt1, [animal, human], nil). 
 
KS1-KS9 can be applied to both multilingual and 
monolingual disambiguation tasks. The following 
knowledge sources were specifically designed for 
multilingual applications: 
 
KS10. Phrasal verbs in the sentence identified using 
a list extracted from various dictionaries. (This 
information was not used in the monolingual task 
because phrasal constructions are not considered 
verb senses in Senseval data.) These are 
represented by definitions of the form 
has_expression(snt, verbal_expression):  
has_expression(snt1, ?come back?). 
 
KS11. Five words to the right and left of the target 
verb in the Portuguese translation. This could be 
obtained using a machine translation system that 
would first translate the non-ambiguous words in 
the sentence. In our experiments it was extracted 
using a parallel corpus and represented using defi-
nitions of the form has_bag_trns(snt, portu-
guese_word): 
has_bag_trns(snt1, coelho). 
has_bag_trns(snt1, reincarna??o). ? 
 
KS12. Narrow context consisting of 5 collocations 
of the verb in the Portuguese translation, which 
take into account the positions of the words, 
represented by has_narrow_trns(snt, 
word_position, portuguese_word): 
has_narrow_trns(snt1, 1st_word_right, como). 
has_narrow_trns(snt1, 2nd_word_right, um). ? 
 
In addition to background knowledge, the system 
learns from a set of examples. Since all knowledge 
about them is expressed as background knowledge, 
their representation is very simple, containing only 
the sentence identifier and the sense of the verb in 
that sentence, i.e. sense(snt, sense): 
sense(snt1,voltar).  
sense(snt2,ir). ? 
 
Based on the examples, background knowledge 
and a series of settings specifying the predicate to 
be learned (i.e., the heads of the rules), the predi-
cates that can be in the conditional part of the 
rules, how the arguments can be shared among dif-
ferent  predicates and several other parameters, the 
inference engine produces a set of symbolic rules. 
Figure 1 shows examples of the rules induced for 
the verb ?to come? in the multilingual task.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Examples of rules produced for the verb 
?come? in the multilingual task 
 
Rule_1. sense(A, voltar) :- 
    has_collocation(A, 1st_prep_right, back). 
Rule_2. sense(A, chegar) :- 
   has_rel(A, subj, B), has_bigram(A, today, B), 
   has_bag_trans(A, hoje). 
Rule_3. sense(A, chegar) :- 
    satisfy_restriction(A, [animal, human], [concrete]); 
    has_expression(A, 'come at'). 
Rule_4. sense(A, vir) :- 
    satisfy_restriction(A, [animate], nil);  
    (has_rel(A, subj, B), 
    (has_pos(A, B, nnp); has_pos(A, B, prp))). 
 
45
Models learned with ILP are symbolic and can be 
easily interpreted. Additionally, innovative knowl-
edge about the problem can emerge from the rules 
learned by the system. Although some rules simply 
test shallow features such as collocates, others pose 
conditions on sets of knowledge sources, including 
relational sources, and allow non-instantiated ar-
guments to be shared amongst them by means of 
variables. For example, in Figure 1, Rule_1 states 
that the translation of the verb in a sentence A will 
be ?voltar? (return) if the first preposition to the 
right of the verb in that sentence is ?back?. Rule_2 
states that the translation of the verb will be 
?chegar? (arrive) if it has a certain subject B, 
which occurs frequently with the word ?today? as a 
bigram, and if the partially translated sentence con-
tains the word ?hoje? (the translation of ?today?). 
Rule_3 says that the translation of the verb will be 
?chegar? (reach) if the subject of the verb has the 
features ?animal? or ?human? and the object has 
the feature ?concrete?, or if the verb occurs in the 
expression ?come at?. Rule_4 states that the trans-
lation of the verb will be ?vir? (move toward) if the 
subject of the verb has the feature ?animate? and 
there is no object, or if the verb has a subject B that 
is a proper noun (nnp) or a personal pronoun (prp). 
4 Experiments and results 
To assess the performance of the approach the 
model produced for each verb was tested on the 
corresponding set of test cases by applying the 
rules in a decision-list like approach, i.e., retaining 
the order in which they were produced and backing 
off to the most frequent sense in the training set to 
classify cases that were not covered by any of the 
rules. All the knowledge sources were made avail-
able to be used by the inference engine, since pre-
vious experiments showed that they are all relevant 
(Specia, 2006). In what follows we present the re-
sults and discuss each task.  
4.1 Multilingual task 
Table 2 shows the accuracies (in terms of percen-
tage of corpus instances which were correctly dis-
ambiguated) obtained by the Aleph models. 
Results are compared against the accuracy that 
would be obtained by using the most frequent 
translation in the training set to classify all the ex-
amples of the test set (in the column labeled ?Ma-
jority sense?). For comparison, we ran experiments 
with three learning algorithms frequently used for 
WSD, which rely on knowledge represented as 
attribute-value vectors: C4.5 (decision-trees), 
Naive Bayes and Support Vector Machine (SVM)1. 
In order to represent all knowledge sources in 
attribute-value vectors, KS2, KS7, KS9 and KS10 
had to be pre-processed to be transformed into bi-
nary attributes. For example, in the case of selec-
tional restrictions (KS9), one attribute was created 
for each possible sense of the verb and a true/false 
value was assigned to it depending on whether the 
arguments of the verb satisfied any restrictions re-
ferring to that sense. Results for each of these algo-
rithms are also shown in Table 2. 
As we can see in Table 2, the accuracy of the 
ILP approach is considerably better than the most 
frequent sense baseline and also outperforms the 
other learning algorithms. This improvement is 
statistically significant (paired t-test; p < 0.05). As 
expected, accuracy is generally higher for verbs 
with fewer possible translations.  
The models produced by Aleph for all the verbs 
are reasonably compact, containing 50 to 96 rules. 
In those models the various knowledge sources 
appear in different rules and all are used. This 
demonstrates that they are all useful for the disam-
biguation of verbs. 
 
Verb Majori- 
ty sense 
C4.5 Na?ve  
Bayes 
SVM Aleph 
ask 0.68 0.68 0.82 0.88 0.92 
come 0.46 0.57 0.61 0.68 0.73 
get 0.03 0.25 0.46 0.47 0.49 
give 0.72 0.71 0.74 0.74 0.74 
go 0.49 0.61 0.66 0.66 0.66 
live 0.71 0.72 0.64 0.73 0.87 
look 0.48 0.69 0.81 0.83 0.93 
make 0.64 0.62 0.60 0.64 0.68 
take 0.14 0.41 0.50 0.51 0.59 
tell 0.65 0.67 0.66 0.68 0.82 
Average 0.50 0.59 0.65 0.68 0.74 
Table 2. Accuracies obtained by Aleph and other 
learning algorithms in the multilingual task 
 
These results are very positive, particularly if we 
consider the characteristics of the multilingual sce-
nario: (1) the verbs addressed are highly ambi-
guous; (2) the corpus was automatically tagged and 
thus distinct synonym translations were sometimes 
                                                          
1
 The implementations provided by Weka were used. Weka is 
available from http://www.cs.waikato.ac.nz/ml/weka/ 
46
used to annotate different examples (these count as 
different senses for the inference engine); and (3) 
certain translations occur very infrequently (just 1 
or 2 examples in the whole corpus). It is likely that 
a less strict evaluation regime, such as one which 
takes account of synonym translations, would re-
sult in higher accuracies. 
It is worth noticing that we experimented with a 
few relevant parameters for both Aleph and the 
other learning algorithms. Values that yielded the 
best average predictive accuracy in the training 
sets were assumed to be optimal and used to eva-
luate the test sets.  
4.2 Monolingual task 
Table 3 shows the average accuracy obtained by 
Aleph in the monolingual task (Senseval-3 verbs 
with fine-grained sense distinctions and using the 
evaluation system provided by Senseval). It also 
shows the average accuracy of the most frequent 
sense and accuracies reported on the same set of 
verbs by the best systems submitted by the sites 
which participated in this task. Syntalex-3 (Mo-
hammad and Pedersen, 2004) is based on an en-
semble of bagged decision trees with narrow 
context part-of-speech features and bigrams. 
CLaC1 (Lamjiri et al, 2004) uses a Naive Bayes 
algorithm with a dynamically adjusted context 
window around the target word. Finally, MC-WSD 
(Ciaramita and Johnson, 2004) is a multi-class av-
eraged perceptron classifier using syntactic and 
narrow context features, with one component 
trained on the data provided by Senseval and other 
trained on WordNet glosses.  
 
System % Average accuracy 
Majority sense 0.56 
Syntalex-3 0.67 
CLaC1 0.67 
MC-WSD 0.72 
Aleph 0.72 
Table 3. Accuracies obtained by Aleph and other 
approaches in the monolingual task 
 
As we can see in Table 3, results are very encour-
aging: even without being particularly customized 
for this monolingual task, the ILP approach signif-
icantly outperforms the majority sense baseline and 
performs as well as the state-of-the-art system re-
porting results for the same set of verbs. As with 
the multilingual task, the models produced contain 
a small number of rules (from 6, for verbs with a 
few examples, to 88) and all knowledge sources 
are used across different rules and verbs. 
In general, results from both multilingual and 
monolingual tasks demonstrate that the hypothesis 
put forward in Section 1, that ILP?s ability to gen-
erate expressive rules which combine and integrate 
a wide range of knowledge sources is beneficial for 
WSD systems, is correct.  
5 Conclusion 
We have introduced a new hybrid approach to 
WSD which uses ILP to combine deep and shallow 
knowledge sources. ILP induces expressive disam-
biguation models which include relations between 
knowledge sources. It is an interesting approach to 
learning which has been considered promising for 
several applications in natural language processing 
and has been explored for a few of them, namely 
POS-tagging, grammar acquisition and semantic 
parsing (Cussens et al, 1997; Mooney, 1997). This 
paper has demonstrated that ILP also yields good 
results for WSD, in particular for the disambigua-
tion of verbs.  
We plan to further evaluate our approach for 
other sets of words, including other parts-of-speech 
to allow further comparisons with other approach-
es. For example, Dang and Palmer (2005) also use 
a rich set of features with a traditional learning al-
gorithm (maximum entropy). Currently, we are 
evaluating the role of the WSD models for the 10 
verbs of the multilingual task in an English-
Portuguese statistical machine translation system. 
References 
Eneko Agirre and German Rigau. 1996. Word Sense 
Disambiguation using Conceptual Density. Proceed-
ings of the 15th Conference on Computational Lin-
guistics (COLING-96). Copenhagen, pages 16-22. 
Marine Carpuat, Yihai Shen, Xiaofeng Yu, and Dekai 
WU. 2006. Toward Integrating Word Sense and Enti-
ty Disambiguation into Statistical Machine Transla-
tion. Proceedings of the Third International 
Workshop on Spoken Language Translation,. Kyoto, 
pages 37-44. 
Massimiliano Ciaramita and Mark Johnson. 2004. Mul-
ti-component Word Sense Disambiguation. Proceed-
ings of Senseval-3: 3rd International Workshop on 
the Evaluation of Systems for the Semantic Analysis 
of Text, Barcelona, pages 97-100. 
47
James Cussens, David Page, Stephen Muggleton, and 
Ashwin Srinivasan. 1997. Using Inductive Logic 
Programming for Natural Language Processing. 
Workshop Notes on Empirical Learning of Natural 
Language Tasks, Prague, pages 25-34. 
Hoa T. Dang and Martha Palmer. 2005. The Role of 
Semantic Roles in Disambiguating Verb Senses. 
Proceedings of the 43rd Meeting of the Association 
for Computational Linguistics (ACL-05), Ann Arbor, 
pages 42?49. 
Christiane Fellbaum. 1998. WordNet: An Electronic 
Lexical Database. MIT Press, Massachusetts.  
W. John Hutchins and Harold L. Somers. 1992. An In-
troduction to Machine Translation. Academic Press, 
Great Britain. 
Abolfazl K. Lamjiri, Osama El Demerdash, Leila Kos-
seim. 2004. Simple features for statistical Word 
Sense Disambiguation. Proceedings of Senseval-3: 
3rd International Workshop on the Evaluation of Sys-
tems for the Semantic Analysis of Text, Barcelona, 
pages 133-136. 
Michael Lesk. 1986. Automatic sense disambiguation 
using machine readable dictionaries: how to tell a 
pine cone from an ice cream cone. ACM SIGDOC 
Conference, Toronto, pages 24-26. 
Dekang Lin. 1993. Principle based parsing without 
overgeneration. Proceedings of the 31st Meeting of 
the Association for Computational Linguistics (ACL-
93), Columbus, pages 112-120. 
Rada Mihalcea, Timothy Chklovski and Adam Kilga-
riff. 2004. The Senseval-3 English Lexical Sample 
Task. Proceedings of Senseval-3: 3rd International 
Workshop on the Evaluation of Systems for Semantic 
Analysis of Text, Barcelona, pages 25-28. 
Saif Mohammad and Ted Pedersen. 2004. Complemen-
tarity of Lexical and Simple Syntactic Features: The 
SyntaLex Approach to Senseval-3. Proceedings of 
Senseval-3: 3rd International Workshop on the Eval-
uation of Systems for the Semantic Analysis of Text, 
Barcelona, pages 159-162. 
Raymond J. Mooney. 1997. Inductive Logic Program-
ming for Natural Language Processing. Proceedings 
of the 6th International Workshop on ILP, LNAI 
1314, Stockolm, pages 3-24. 
Stephen Muggleton. 1991. Inductive Logic Program-
ming. New Generation Computing, 8(4):295-318. 
Stephen Muggleton. 1995. Inverse Entailment and Pro-
gol. New Generation Computing, 13:245-286. 
 
Hwee T. Ng and Hian B. Lee. 1996. Integrating mul-
tiple knowledge sources to disambiguate word sense: 
an exemplar-based approach. Proceedings of the 34th 
Meeting of the Association for Computational 
Linguistics (ACL-96), Santa Cruz, CA, pages 40-47. 
Paul Procter (editor). 1978. Longman Dictionary of 
Contemporary English. Longman Group, Essex. 
Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-
Of-Speech Tagger. Proceedings of the Conference on 
Empirical Methods in Natural Language Processing, 
New Jersey, pages 133-142. 
Phillip Resnik and David Yarowsky. 1997. A Perspec-
tive on Word Sense Disambiguation Methods and 
their Evaluating. Proceedings of the ACL-SIGLEX 
Workshop Tagging Texts with Lexical Semantics: 
Why, What and How?, Washington. 
Hinrich Sch?tze. 1998. Automatic Word Sense Discrim-
ination. Computational Linguistics, 24(1): 97-123. 
Lucia Specia, Maria G.V. Nunes, and Mark Stevenson. 
2005. Exploiting Parallel Texts to Produce a 
Multilingual Sense Tagged Corpus for Word Sense 
Disambiguation. Proceedings of the Conference on 
Recent Advances on Natural Language Processing 
(RANLP-2005), Borovets, pages 525-531. 
Lucia Specia. 2006. A Hybrid Relational Approach for 
WSD - First Results. Proceedings of the 
COLING/ACL 06 Student Research Workshop, Syd-
ney, pages 55-60.  
Ashwin Srinivasan. 2000. The Aleph Manual. Technical 
Report. Computing Laboratory, Oxford University. 
Mark Stevenson and Yorick Wilks. 2001. The Interaction 
of Knowledge Sources for Word Sense Disambiguation. 
Computational Linguistics, 27(3):321-349. 
Yorick Wilks and Mark Stevenson. 1998. The Grammar 
of Sense: Using Part-of-speech Tags as a First Step in 
Semantic Disambiguation. Journal of Natural Lan-
guage Engineering, 4(1):1-9 
David Yarowsky. 1995. Unsupervised Word-Sense Dis-
ambiguation Rivaling Supervised Methods. 
Proceedings of the 33rd Meeting of the Association 
for Computational Linguistics (ACL-05), Cambridge, 
MA, pages 189-196.  
 
48
Proceedings of the Workshop on Information Extraction Beyond The Document, pages 12?19,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Comparing Information Extraction Pattern Models
Mark Stevenson and Mark A. Greenwood
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, UK
{marks,m.greenwood}@dcs.shef.ac.uk
Abstract
Several recently reported techniques for
the automatic acquisition of Information
Extraction (IE) systems have used depen-
dency trees as the basis of their extrac-
tion pattern representation. These ap-
proaches have used a variety of pattern
models (schemes for representing IE pat-
terns based on particular parts of the de-
pendency analysis). An appropriate model
should be expressive enough to represent
the information which is to be extracted
from text without being overly compli-
cated. Four previously reported pattern
models are evaluated using existing IE
evaluation corpora and three dependency
parsers. It was found that one model,
linked chains, could represent around 95%
of the information of interest without gen-
erating an unwieldy number of possible
patterns.
1 Introduction
A common approach to Information Extraction
(IE) is to use patterns which match against text
and identify items of interest. Patterns are applied
to text which has undergone various levels of lin-
guistic analysis, such as phrase chunking (Soder-
land, 1999) and full syntactic parsing (Gaizauskas
et al, 1996). The approaches use different defini-
tions of what constitutes a valid pattern. For exam-
ple, the AutoSlog system (Riloff, 1993) uses pat-
terns which match certain grammatical categories,
mainly nouns and verbs, in phrase chunked text
while Yangarber et al (2000) use subject-verb-
object tuples derived from a dependency parse. An
appropriate pattern language must encode enough
information about the text to be able to accurately
identify the items of interest. However, it should
not contain so much information as to be complex
and impractical to apply.
Several recent approaches to IE have used pat-
terns based on a dependency analysis of the input
text (Yangarber, 2003; Sudo et al, 2001; Sudo et
al., 2003; Bunescu and Mooney, 2005; Stevenson
and Greenwood, 2005). These approaches have
used a variety of pattern models (schemes for rep-
resenting IE patterns based on particular parts of
the dependency tree). For example, Yangarber
(2003) uses just subject-verb-object tuples while
Sudo et al (2003) allow any subpart of the tree to
act as an extraction pattern. The set of patterns al-
lowed by the first model is a proper subset of the
second and therefore captures less of the informa-
tion contained in the dependency tree. Little anal-
ysis has been carried out into the appropriateness
of each model. Sudo et al (2003) compared three
models in terms of their ability to identify event
participants.
The choice of pattern model has an effect on
the number of potential patterns. This has impli-
cations on the practical application for each ap-
proach, particularly when used for automatic ac-
quisition of IE systems using learning methods
(Yangarber et al, 2000; Sudo et al, 2003; Bunescu
and Mooney, 2005). This paper evaluates the ap-
propriateness of four pattern models in terms of
the competing aims of expressive completeness
(ability to represent information in text) and com-
plexity (number of possible patterns). Each model
is examined by comparing it against a corpus an-
notated with events and determining the propor-
tion of those which it is capable of representing.
The remainder of this paper is organised as fol-
lows: a variety of dependency-tree-based IE pat-
12
Figure 1: An example dependency tree.
tern models are introduced (Sections 2 and 3).
Section 4 describes experiments comparing each
model and the results are discussed in Section 5.
2 Pattern Models
In dependency analysis (Mel?c?uk, 1987) the syn-
tax of a sentence is represented by a set of directed
binary links between a word (the head) and one of
its modifiers. These links may be labelled to in-
dicate the grammatical relation between the head
and modifier (e.g. subject, object). In general
cyclical paths are disallowed so that the analysis
forms a tree structure. An example dependency
analysis for the sentence ?Acme Inc. hired Mr
Smith as their new CEO, replacing Mr Bloggs.?
is shown Figure 1.
The remainder of this section outlines four mod-
els for representing extraction patterns which can
be derived from dependency trees.
Predicate-Argument Model (SVO): A simple
approach, used by Yangarber (2003) and Steven-
son and Greenwood (2005), is to use subject-verb-
object tuples from the dependency parse as extrac-
tion patterns. These consist of a verb and its sub-
ject and/or direct object1. An SVO pattern is ex-
tracted for each verb in a sentence. Figure 2 shows
the two SVO patterns2 which are produced for the
dependency tree shown in Figure 1.
This model may be motivated by the assump-
tion that many IE scenarios involve the extraction
1Yangarber et al (2000) and Sudo et al (2003) used a
slightly extended version of this model in which the pattern
also included certain phrases which referred to either the sub-
ject or object.
2The formalism used for representing dependency pat-
terns is similar to the one introduced by Sudo et al (2003).
Each node in the tree is represented in the format a[b/c]
(e.g. subj[N/bomber]) where c is the lexical item
(bomber), b its grammatical tag (N) and a the dependency
relation between this node and its parent (subj). The rela-
tionship between nodes is represented as X(A+B+C) which
indicates that nodes A, B and C are direct descendents of node
X.
of participants in specific events. For example,
the MUC-6 (MUC, 1995) management succession
scenario concerns the identification of individuals
who are changing job. These events are often de-
scribed using a simple predicate argument struc-
ture, e.g. ?Acme Inc. fired Smith?. However,
the SVO model cannot represent information de-
scribed using other linguistic constructions such as
nominalisations or prepositional phrases. For ex-
ample, in the MUC6 texts it is common for job ti-
tles to be mentioned within prepositional phrases,
e.g. ?Smith joined Acme Inc. as CEO?.
Chains: A pattern is defined as a path between
a verb node and any other node in the dependency
tree passing through zero or more intermediate
nodes (Sudo et al, 2001). Figure 2 shows the eight
chains which can be extracted from the tree in Fig-
ure 1.
Chains provide a mechanism for encoding in-
formation beyond the direct arguments of predi-
cates and includes areas of the dependency tree ig-
nored by the SVO model. For example, they can
represent information expressed as a nominalisa-
tion or within a prepositional phrase, e.g. ?The
resignation of Smith from the board of Acme ...?
However, a potential shortcoming of this model is
that it cannot represent the link between arguments
of a verb. Patterns in the chain model format are
unable to represent even the simplest of sentences
containing a transitive verb, e.g. ?Smith left Acme
Inc.?.
Linked Chains: The linked chains model
(Greenwood et al, 2005) represents extraction
patterns as a pair of chains which share the same
verb but no direct descendants. This model gen-
erates 14 patterns for the verb hire in Figure 1,
examples of which are shown in Figure 2. This
pattern representation encodes most of the infor-
mation in the sentence with the advantage of being
able to link together event participants which nei-
ther of the SVO or chain model can, for example
the relation between ?Smith? and ?Bloggs?.
Subtrees: The final model to be considered is
the subtree model (Sudo et al, 2003). In this
model any subtree of a dependency tree can be
used as an extraction pattern, where a subtree is
any set of nodes in the tree which are connected to
one another. Single nodes are not considered to be
subtrees. The subtree model is a richer representa-
tion than those discussed so far and can represent
any part of a dependency tree. Each of the previ-
13
SVO Chains
[V/hire](subj[N/Acme Inc.]+obj[N/Mr Smith]) [V/hire](subj[N/Acme Inc.])
[V/replace](obj[N/Mr Bloggs]) [V/hire](obj[N/Mr Smith])
[V/hire](obj[N/Mr Smith](as[N/CEO]))
[V/hire](obj[N/Mr Smith](as[N/CEO](gen[N/their])))
[V/hire](obj[N/Mr Smith](as[N/CEO](mod[A/new])))
[V/hire](vpsc mod[V/replace])
[V/hire](vpsc mod[V/replace](obj[N/Mr Bloggs]))
[V/replace](obj[N/Mr Bloggs])
Linked Chains
[V/hire](subj[N/Acme Inc.]+obj[N/Mr Smith])
[V/hire](subj[N/Acme Inc.]+obj[N/Mr Smith](as[N/CEO]))
[V/hire](obj[N/Mr Smith]+vpsc mod[V/replace](obj[N/Mr Bloggs]))
Figure 2: Example patterns for three models
ous models form a proper subset of the subtrees.
By choosing an appropriate subtree it is possible
to link together any pair of nodes in a tree and
consequently this model can represent the relation
between any set of items in the sentence.
3 Pattern Enumeration and Complexity
In addition to encoding different parts of the de-
pendency analysis, each pattern model will also
generate a different number of potential patterns.
A dependency tree, T , can be viewed as a set
of N connected nodes. Assume that V , such that
V ? N , is the set of nodes in the dependency tree
labelled as a verb.
Predicate-Argument Model (SVO): The num-
ber of SVO patterns extracted from T is:
Nsvo (T ) = |V | (1)
Chain Model: A chain can be created between
any verb and a node it dominates (directly or indi-
rectly). Now assume that d(v) denotes the count
of a node v and all its descendents then the number
of chains is given by:
Nchains (T ) =
?
v?V
( d (v) ? 1 ) (2)
Linked Chains: Let C(v) denote the set of di-
rect child nodes of node v and vi denote the i-th
child, so C(v) =
{
v1, v2, ...v|C(v)|
}
. The number
of possible linked chains in T is given by:
Nlinked chains (T ) =
?
v?V
|C(v)|
?
i=1
|C(v)|
?
j=i+1
d (vi) d (vj)
(3)
Subtrees: Now assume that sub(n) is a func-
tion denoting the number of subtrees, including
single nodes, rooted at node n. This can be de-
fined recursively as follows:
sub(n) =
?
?
?
1 if n is a leaf node
|C(n)|
?
i=1
(sub (ni) + 1) otherwise
(4)
The total number of subtrees in a tree is given
by:
Nsubtree (T ) =
(
?
n?N
sub(n)
)
? |N | (5)
The dependency tree shown in Figure 1 gener-
ates 2, 8, 14 and 42 possible SVO, chain, linked
chain and subtree patterns respectively. The num-
ber of SVO patterns is constant on the number of
verbs in the tree. The number of chains is gener-
ally a linear function on the size of the tree but,
in the worst case, can be polynomial. The linked
chain model generates a polynomial number of
patterns while the subtree model is exponential.
There is a clear tradeoff between the complex-
ity of pattern representations and the practicality
of computation using them. Some pattern rep-
resentations are more expressive, in terms of the
amount of information from the dependency tree
they make use of, than others (Section 2) and are
therefore more likely to produce accurate extrac-
tion patterns. However, the more expressive mod-
els will add extra complexities during computation
since a greater number of patterns will be gen-
erated. This complexity, both in the number of
patterns produced and the computational effort re-
quired to produce them, limits the algorithms that
can reasonably be applied to learn useful extrac-
tion patterns.
For a pattern model to be suitable for an ex-
traction task it needs to be expressive enough to
encode enough information from the dependency
parse to accurately identify the items which need
to be extracted. However, we also aim for the
14
model to be as computationally tractable as pos-
sible. The ideal model will then be one with suffi-
cient expressive power while at the same time not
including extra information which would make its
use less practical.
4 Experiments
We carried out experiments to determine how suit-
able the pattern representations detailed in Section
2 are for encoding the information of interest to
IE systems. We chose a set of IE corpora anno-
tated with the information to be extracted (detailed
in Section 4.1), generated sets of patterns using a
variety of dependency parsers (Section 4.2) which
were then examined to discover how much of the
target information they contain (Section 4.3).
4.1 Corpora
Corpora representing different genres of text were
chosen for these experiments; one containing
newspaper text and another composed of biomed-
ical abstracts. The first corpus consisted of Wall
Street Journal texts from the Sixth Message Un-
derstanding Conference (MUC, 1995) IE evalu-
ation. These are reliably annotated with details
about the movement of executives between jobs.
We make use of a version of the corpus pro-
duced by Soderland (1999) in which events de-
scribed within a single sentence were annotated.
Events in this corpus identify relations between
up to four entities: PersonIn (the person start-
ing a new job), PersonOut (person leaving a
job), Post (the job title) and Organisation
(the employer). These events were broken down
into a set of binary relationships. For exam-
ple, the sentence ?Smith was recently made chair-
man of Acme.? contains information about the
new employee (Smith), post (chairman) and or-
ganisation (Acme). Events are represented as a
set of binary relationships, Smith-chairman,
chairman-Acme and Smith-Acme for this
example.
The second corpus uses documents taken from
the biomedical domain, specifically the train-
ing corpus used in the LLL-05 challenge task
(Ne?dellec, 2005), and a pair of corpora (Craven
and Kumlien, 1999) which were derived from the
Yeast Proteome Database (YPD) (Hodges et al,
1999) and the Online Mendelian Inheritance in
Man database (OMIM) (Hamosh et al, 2002).
Each of these corpora are annotated with binary
relations between pairs of entities. The LLL-05
corpora contains interactions between genes and
proteins. For example the sentence ?Expression
of the sigma(K)-dependent cwlH gene depended
on gerE? contains relations between sigma(K) and
cwlH and between gerE and cwlH. The YPD cor-
pus is concerned with the subcellular compart-
ments in which particular yeast proteins localize.
An example sentence ?Uba2p is located largely in
the nucleus? relates Uba2p and the nucleus. The
relations in the OMIM corpora are between genes
and diseases, for example ?Most sporadic colorec-
tal cancers also have two APC mutations? con-
tains a relation between APC and colorectal can-
cer.
The MUC6 corpus contains a total of six pos-
sible binary relations. Each of the three biomedi-
cal corpora contain a single relation type, giving a
total of nine binary relations for the experiments.
There are 3911 instances of binary relations in all
corpora.
4.2 Generating Dependency Patterns
Three dependency parsers were used for these ex-
periments: MINIPAR3 (Lin, 1999), the Machinese
Syntax4 parser from Connexor Oy (Tapanainen
and Ja?rvinen, 1997) and the Stanford5 parser
(Klein and Manning, 2003). These three parsers
represent a cross-section of approaches to produc-
ing dependency analyses: MINIPAR uses a con-
stituency grammar internally before converting
the result to a dependency tree, Machinese Syn-
tax uses a functional dependency grammar, and
the Stanford Parser is a lexicalized probabilistic
parser.
Before these parsers were applied to the various
corpora the named entities participating in rela-
tions are replaced by a token indicating their class.
For example, in the MUC6 corpus ?Acme hired
Smith? would become ?Organisation hired
PersonIn?. Each parser was adapted to deal
with these tokens correctly. The parsers were ap-
plied to each corpus and patterns extracted from
the dependency trees generated.
The analyses produced by the parsers were post-
processed to make the most of the information
they contain and ensure consistent structures from
which patterns could be extracted. It was found
3http://www.cs.ualberta.ca/
?
lindek/
4http://www.connexor.com/software/syntax/
5http://www-nlp.stanford.edu/software/
15
Parser SVO Chains Linked chains Subtrees
MINIPAR 2,980 52,659 149,504 353,778,240,702,149,000
Machinese Syntax 2,382 67,690 265,631 4,641,825,924
Stanford 2,950 76,620 478,643 1,696,259,251,073
Table 1: Number of patterns produced for each pattern model by different parsers
that the parsers were often unable to generate a de-
pendency tree which included the whole sentence
and instead generate an analysis consisting of sen-
tence fragments represented as separate tree struc-
tures. Some fragments did not include a verb so
no patterns could be extracted. To take account of
this we allowed the root node of any tree fragment
to take the place of a verb in a pattern (see Sec-
tion 2). This leads to the generation of more chain
and linked chain patterns but has no effect on the
number of SVO patterns or subtrees.
Table 1 shows the number of patterns generated
from the dependency trees produced by each of the
parsers. The number of subtrees generated from
the MINIPAR parses is several orders of magnitude
higher than the others because MINIPAR allows
certain nodes to be the modifier of two separate
nodes to deal with phenomena such as conjunc-
tion, anaphora and VP-coordination. For exam-
ple, in the sentence ?The bomb caused widespread
damage and killed three people? the bomb is the
subject of both the verbs cause and kill. We made
use of this information by duplicating any nodes
(and their descendants) with more than one head.6
Overall the figures in Table 1 are consistent with
the analysis in Section 3 but there is great variation
in the number of patterns produced by the differ-
ent parsers. For example, the Stanford parser pro-
duces more chains and linked chains than the other
parsers. (If we did not duplicate portions of the
MINIPAR parses then the Stanford parser would
also generate the most subtrees.) We found that
the Stanford parser was the most likely to gen-
erate a single dependency tree for each sentence
while the other two produced a set of tree frag-
ments. A single dependency analysis contains a
greater number of patterns, and possible subtrees,
than a fragmented analysis. One reason for this
may be that the Stanford parser is unique in allow-
ing the use of an underspecified dependency rela-
tion, dep, which can be applied when the role of
the dependency is unclear. This allows the Stan-
6One dependency tree produced by MINIPAR, expanded in
this way, contained approximately 1 ? 1064 subtrees. These
are not included in the total number of subtrees for the MINI-
PAR parses shown in the table.
ford parser to generate analyses which span more
of the sentence than the other two.
4.3 Evaluating Pattern Models
Patterns from each of the four models are exam-
ined to check whether they cover the information
which should be extracted. In this context ?cover?
means that the pattern contains both elements
of the relation. For example, an SVO pattern
extracted from the dependency parse of ?Smith
was recently made chairman of Acme.? would be
[V/make](subj[N/Smith]+obj[N/chairman])
which covers the relation between Smith and
chairman but not the relations between Smith
and Acme or chairman and Acme. The coverage
of each model is computed as the percentage of
relations in the corpus for which at least one of
the patterns contains both of the participating
entities. Coverage is related to the more familiar
IE evaluation metric of recall since the coverage
of a pattern model places an upper bound on the
recall of any system using that model. The aim
of this work is to determine the proportion of
the relations in a corpus that can be represented
using the various pattern models rather than their
performance in an IE system and, consequently,
we choose to evaluate models in terms of their
coverage rather than precision and recall.7
For practical applications parsers are required
to generate the dependency analysis but these may
not always provide a complete analysis for every
sentence. The coverage of each model is influ-
enced by the ability of the parser to produce a tree
which connects the elements of the event to be ex-
tracted. To account for this we compute the cov-
erage of each model relative to a particular parser.
The subtree model covers all events whose enti-
ties are included in the dependency tree and, con-
sequently, the coverage of this model represents
the maximum number of events that the model can
7The subtree model can be used to cover any set of items
in a dependency tree. So, given accurate dependency anal-
yses, this model will cover all events. The coverage of the
subtree model can be determined by checking if the elements
of the event are connected in the dependency analysis of the
sentence and, for simplicity, we chose to do this rather than
enumerating all subtrees.
16
represent for a given dependency tree. The cover-
age of other models relative to a dependency anal-
ysis can be computed by dividing the number of
events it covers by the number covered by the sub-
tree model (i.e. the maximum which can be cov-
ered). This measure is refered to as the bounded
coverage of the model. Bounded coverage for the
subtree model is always 100%.
5 Results
Coverage and bounded-coverage results for each
pattern representation and parser combination are
given in Table 2. The table lists the corpus, the
total number of instances within that corpus and
the results for each of the four pattern models. Re-
sults for the subtree model lists the coverage and
raw count, the bounded-coverage for this model
will always be 100% and is not listed. Results
for the other three models show the coverage and
raw count along with the bounded coverage. The
coverage of each parser and pattern representa-
tion (combined across both corpora) are also sum-
marised in Figure 3.
The simplest representation, SVO, does not per-
form well in this evaluation. The highest bounded-
coverage score is 15.1% (MUC6 corpus, Stanford
parser) but the combined average over all corpora
is less than 6% for any parser. This suggests
that the SVO representation is simply not expres-
sive enough for IE. Previous work which has used
this representation have used indirect evaluation:
document and sentence filtering (Yangarber, 2003;
Stevenson and Greenwood, 2005). While the SVO
representation may be expressive enough to allow
a classifier to distinguish documents or sentences
which are relevant to a particular extraction task it
seems too limited to be used for relation extrac-
tion. The SVO representation performs notice-
ably worse on the biomedical text. Our analysis
suggests that this is because the items of interest
are commonly described in ways which the SVO
model is unable to represent.
The more complex chain model covers a greater
percentage of the relations. However its bounded-
coverage is still less than half of the relations in ei-
ther the MUC6 corpus or the biomedical texts. Us-
ing the chain model the best coverage which can
be achieved over any corpus is 41.07% (MUC6
corpus, MINIPAR and Stanford parser) which is
unlikely to be sufficient to create an IE system.
Results for the linked chain representation are
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
MINIPAR Machinese Syntax Stanford
Co
ve
ra
ge
SVO Chains Linked Chains Subtrees
Figure 3: Coverage of various pattern representa-
tion models for each of the three parsers.
much more promising covering around 70% of all
relations using the MINIPAR and Machinese Syn-
tax parsers and over 90.64% using the Stanford
parser. For all three parsers this model achieves
a bounded-coverage of close to 95%, indicating
that this model can represent the majority of re-
lations which are included in a dependency tree.
The subtree representation covers slight more of
the relations than linked chains: around 75% us-
ing the MINIPAR or Machinese Syntax parsers and
96.62% using the Stanford parser.
A one-way repeated measures ANOVA was car-
ried out to analyse the differences between the re-
sults for each model shown in Table 2. It was
found that the differences between the SVO, chain,
linked chain and subtree models are significant
(p < 0.01). A Tukey test was then applied to iden-
tify which of the individual differences between
pairs of models were significant. Differences be-
tween two pairs of models were not found to be
significant (p < 0.01): SVO and chains; linked
chains and subtrees.
These results suggest that the linked chains and
subtree models can represent significantly more of
the relations which occur in IE scenarios than ei-
ther the SVO or chain models. However, there is
little to be gained from using the subtree model
since accuracy of the linked chain model is com-
parable and the number of patterns generated is
bounded by a polynomial rather than exponential
function.
5.1 Analysis and Discussion
Examination of the relations which were cov-
ered by the subtree model but not by linked
chains suggested that there are certain construc-
tions which cause difficulties. One such construc-
tion is the appositive, e.g. the relation between
17
# of SVO Chains Linked Chains Subtrees
Parser Corpus Relations %C %B-C %C %B-C %C %B-C %C
MUC6 1322 7.49 (99) 9.07 41.07 (543) 49.73 81.92 (1083) 99.18 82.60 (1092)
MINIPAR Biomed 2589 0.93 (24) 1.30 17.38 (450) 24.44 65.31 (1691) 91.85 71.11 (1841)
Combined 3911 3.14 (123) 4.19 25.39 (993) 33.86 70.93 (2774) 94.58 74.99 (2933)
Machinese MUC6 1322 2.12 (28) 2.75 35.70 (472) 46.41 76.32 (1009) 99.21 76.93 (1017)
Syntax Biomed 2589 0.19 (5) 0.27 14.56 (377) 20.47 65.47 (1695) 92.02 71.15 (1842)Combined 3911 0.84 (33) 1.15 21.71 (849) 29.70 69.14 (2704) 94.58 73.10 (2859)
MUC6 1322 15.05 (199) 15.10 41.07 (543) 41.20 94.78 (1253) 95.07 99.70 (1318)
Stanford Biomed 2589 0.46 (12) 0.49 16.53 (428) 17.39 88.52 (2292) 93.13 95.06 (2461)
Combined 3911 5.40 (211) 5.58 24.83 (971) 25.69 90.64 (3545) 93.81 96.62 (3779)
Table 2: Evaluation results for the three different parsers.
PersonOut and Organisation in the frag-
ment ?Organisation?s Post, PersonOut,
resigned yesterday morning?. Certain nominal-
isations may also cause problems for the linked
chains representation, e.g. in biomedical text
the relation between Agent and Target in the
nominalisation ?the Agent-dependent assembly
of Target? cannot be represented by a linked
chain. In both cases the problem is caused by the
fact that the dependency tree generated includes
the two named entities in part of the tree domi-
nated by a node marked as a noun. Since each
linked chain must be anchored at a verb (or the
root of a tree fragment) and the two chains can-
not share part of their path, these relations are not
covered. It would be possible to create another
representation which allowed these relations to be
captured but it would generate more patterns than
the linked chain model.
Our results also reveal that the choice of depen-
dency parser effects the coverage of each model
(see Figure 3). The subtree model coverage scores
for each parser shown in Table 3 represent the per-
centage of sentences for which an analysis was
generated that included both items from the bi-
nary relations. These figures are noticably higher
for the Stanford parser. We previously mentioned
(Section 4.2) that this parser allows the use of an
underspecified dependency relation and suggested
that this may be a reason for the higher cover-
age. The use of underspecified dependency re-
lations may not be useful for all applications but
is unlikely to cause problems for systems which
learn IE patterns provided the trees generated by
the parser are consistent. Differences between the
results produced by the three parsers suggest that
it is important to fully evaluate their suitability for
a particular purpose.
These experiments also provide insights into the
more general question of how suitable dependency
trees are as a basis for extraction patterns. De-
pendency analysis has the advantage of generat-
ing analyses which abstract away from the sur-
face realisation of text to a greater extent than
phrase structure grammars tend to. This leads to
the semantic information being more accessible in
the representation of the text which can be use-
ful for IE. For practical applications this approach
relies on the ability to accurately generate depen-
dency analyses. The results presented here sug-
gest that the Stanford parser (Klein and Manning,
2003) is capable of generating analyses for almost
all sentences within corpora from two very differ-
ent domains. Bunescu and Mooney (2005) have
also demonstrated that dependency graphs can be
produced using Combinatory Categorial Grammar
(CCG) and context-free grammar (CFG) parsers.
6 Conclusions
This paper compares four IE pattern models:
SVO, chains, linked chains and subtrees. Us-
ing texts from the management succession and
biomedical domains it was found that the linked
chains model can represent around 95% of the
possible relations contained in the text, given a de-
pendency parse. Subtrees can represent all the re-
lations contained within dependency trees but their
use is less practical because enumerating all pos-
sible subtrees is a more complex problem and the
large number of resulting patterns could limit the
learning algorithms that can be applied. This re-
sult should be borne in mind during the design of
IE systems.
Acknowledgements
The authors are grateful to Mike Stannet for pro-
viding the method for counting subtrees intro-
duced in Section 3 and to Connexor Oy for use
of the Machinese Syntax parser. The research
18
described in this paper was funded by the En-
gineering and Physical Sciences Research Coun-
cil via the RESuLT project (GR/T06391) and par-
tially funded by the IST 6th Framework project X-
Media (FP6-26978).
References
Razvan Bunescu and Raymond Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of the Human Language Tech-
nology Conference and Conference on Empirical
Methods in Natural Language Processing, pages
724?731, Vancouver, B.C.
Mark Craven and Johan Kumlien. 1999. Construct-
ing Biological Knowledge Bases by Extracting In-
formation from Text Sources. In Proceedings of the
Seventh International Conference on Intelligent Sys-
tems for Molecular Biology, pages 77?86, Heidel-
berg, Germany. AAAI Press.
Robert Gaizauskas, Takahiro Wakao, Kevin
Humphreys, Hamish Cunningham, and Yorick
Wilks. 1996. Description of the LaSIE system
as used for MUC-6. In Proceedings of the Sixth
Message Understanding Conference (MUC-6),
pages 207?220, San Francisco, CA.
Mark A. Greenwood, Mark Stevenson, Yikun Guo,
Henk Harkema, and Angus Roberts. 2005. Au-
tomatically Acquiring a Linguistically Motivated
Genic Interaction Extraction System. In Proceed-
ings of the 4th Learning Language in Logic Work-
shop (LLL05), Bonn, Germany.
Ada Hamosh, Alan F. Scott, Joanna Amberger, Carol
Bocchini, David Valle, and Victor A. McKusick.
2002. Online Mendelian Inheritance in Man
(OMIM), a knowledgebase of human genes and ge-
netic disorders. Nucleic Acids Research, 30(1):52?
55.
Peter E. Hodges, Andrew H. Z. McKee, Brian P. Davis,
William E. Payne, and James I. Garrels. 1999. The
Yeast Proteome Database (YPD): a model for the or-
ganization and presentation of genome-wide func-
tional data. Nucleic Acids Research, 27(1):69?73.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate Unlexicalized Parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL-03), pages 423?430, Sap-
poro, Japan.
Dekang Lin. 1999. MINIPAR: A Minimalist Parser.
In Maryland Linguistics Colloquium, University of
Maryland, College Park.
Igor Mel?c?uk. 1987. Dependency Syntax: Theory and
Practice. SUNY Press, New York.
MUC. 1995. Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6), San Mateo, CA.
Morgan Kaufmann.
Claire Ne?dellec. 2005. Learning Language in Logic -
Genic Interaction Extraction Challenge. In Proceed-
ings of the 4th Learning Language in Logic Work-
shop (LLL05), Bonn, Germany, August.
Ellen Riloff. 1993. Automatically constructing a dic-
tionary for information extraction tasks. pages 811?
816.
Stephen Soderland. 1999. Learning Information Ex-
traction Rules for Semi-structured and free text. Ma-
chine Learning, 31(1-3):233?272.
Mark Stevenson and Mark A. Greenwood. 2005. A
Semantic Approach to IE Pattern Induction. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 379?386,
Ann Arbor, MI.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2001. Automatic Pattern Acquisition for Japanese
Information Extraction. In Proceedings of the Hu-
man Language Technology Conference (HLT2001).
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An Improved Extraction Pattern Representa-
tion Model for Automatic IE Pattern Acquisition. In
Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics (ACL-03),
pages 224?231, Sapporo, Japan.
Pasi Tapanainen and Timo Ja?rvinen. 1997. A Non-
Projective Dependency Parser. In Proceedings of
the 5th Conference on Applied Natural Language
Processing, pages 64?74, Washington, DC.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic Acquisition of
Domain Knowledge for Information Extraction. In
Proceedings of the 18th International Conference on
Computational Linguistics (COLING 2000), pages
940?946, Saarbru?cken, Germany.
Roman Yangarber. 2003. Counter-training in the Dis-
covery of Semantic Patterns. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL-03), pages 343?350, Sap-
poro, Japan.
19
Proceedings of the Workshop on Information Extraction Beyond The Document, pages 29?35,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Improving Semi-Supervised Acquisition of Relation Extraction Patterns
Mark A. Greenwood and Mark Stevenson
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, UK
{m.greenwood,marks}@dcs.shef.ac.uk
Abstract
This paper presents a novel approach to
the semi-supervised learning of Informa-
tion Extraction patterns. The method
makes use of more complex patterns than
previous approaches and determines their
similarity using a measure inspired by re-
cent work using kernel methods (Culotta
and Sorensen, 2004). Experiments show
that the proposed similarity measure out-
performs a previously reported measure
based on cosine similarity when used to
perform binary relation extraction.
1 Introduction
A recent approach to Information Extraction (IE)
is to make use of machine learning algorithms
which allow systems to be rapidly developed or
adapted to new extraction problems. This reduces
the need for manual development which is a major
bottleneck in the development of IE technologies
and can be extremely time consuming (e.g. Riloff
(1996)).
A number of machine learning approaches have
recently been applied. One is the use of itera-
tive learning algorithms to infer extraction patterns
from a small number of seed examples (Yangar-
ber et al, 2000; Stevenson and Greenwood, 2005).
These approaches use dependency analysis as the
basis of IE patterns. Training text is parsed and a
set of candidate patterns extracted. These patterns
are then compared against the seeds with the most
similar being selected and added to the seed set.
(Optionally, a user may verify the patterns at this
point.) The process is then repeated with the re-
maining patterns being compared to the enlarged
seed set. The process continues until a suitable set
of patterns has been learned. These approaches
require only a small number of example extraction
patterns which greatly reduces the effort required
to develop IE systems.
While it has been found that these approaches
are capable of learning useful IE patterns from
a handful of examples (Yangarber et al, 2000;
Stevenson and Greenwood, 2005) they are limited
by the use of basic extraction patterns: SVO tu-
ples. The patterns used by these systems are de-
fined as a verb and its direct subject and/or object.
They could then only extract a limited set of re-
lations; those expressed using a verb and its di-
rect arguments. For example, these patterns could
identify the relation between Jones and Smith in
the sentence ?Jones replaced Smith?. However,
no pattern consisting of a verb and its arguments
could be constructed which could identify the
same relation in ?Jones was named as Smith?s suc-
cessor.?
Others have suggested alternative approaches
for generating extraction patterns from depen-
dency trees, each of which allows a particular part
of the dependency analysis to act as an extraction
pattern. For example, Sudo et al (2003) used pat-
terns consisting of a path from a verb to any of
its descendents (direct or indirect) while Bunescu
and Mooney (2005) suggest the shortest path be-
tween the items being related. However, iterative
learning algorithms, such as the ones used by Yan-
garber et al (2000) and Stevenson and Greenwood
(2005), have not made use of these more complex
extraction patterns. Part of the reason for this is
that these algorithms require a way of determining
the similarity between patterns (in order to com-
pare candidate patterns with the seeds). This pro-
cess is straightforward for simple patterns, based
on SVO tuples, but less so for more complex ex-
29
traction patterns.
In this paper we present a semi-supervised al-
gorithm for the iterative learning of relation ex-
traction patterns which makes use of a more com-
plex pattern representation than has been previ-
ously used by these approaches (Sections 2 and 3).
The algorithm makes use of a similarity function
based on those which have been proposed for use
with non-iterative learning algorithms (Zelenko et
al., 2003; Culotta and Sorensen, 2004; Bunescu
and Mooney, 2005). These are extended to include
information about lexical similarity derived from
WordNet (Section 4). We present results of using
patterns acquired through this similarity function
to perform binary relation extraction (Sections 5
and 6).
2 Semi-Supervised Learning of
Extraction Patterns
We begin by outlining the general process of learn-
ing extraction patterns using a semi-supervised al-
gorithm, similar to one presented by Yangarber
(2003).
1. For a given IE scenario we assume the ex-
istence of a set of documents against which
the system can be trained. The documents are
unannotated and may be either relevant (con-
tain the description of an event relevant to the
scenario) or irrelevant.
2. This corpus is pre-processed to generate a set
of all patterns which could be used to repre-
sent sentences contained in the corpus, call
this set P . The aim of the learning process is
to identify the subset of P representing pat-
terns which are relevant to the IE scenario.
3. The user provides a small set of seed pat-
terns, Pseed, which are relevant to the sce-
nario. These patterns are used to form the
set of currently accepted patterns, Pacc, so
Pacc ? Pseed. The remaining patterns are
treated as candidates for inclusion in the ac-
cepted set, these form the set Pcand(= P ?
Pacc).
4. A function, f , is used to assign a score to
each pattern in Pcand based on those which
are currently in Pacc. This function as-
signs a real number to candidate patterns so
? c  Pcand, f(c, Pacc) 7? R. A set of high
scoring patterns (based on absolute scores or
ranks after the set of patterns has been or-
dered by scores) are chosen as being suitable
for inclusion in the set of accepted patterns.
These form the set Plearn.
5. (Optional) The patterns in Plearn may be re-
viewed by a user who may remove any they
do not believe to be useful for the scenario.
6. The patterns in Plearn are added to Pacc and
removed from Pcand, so Pacc ? Pacc ?
Plearn and Pcand ? Pacc ? Plearn
7. Stop if an acceptable set of patterns has been
learned, otherwise goto step 4
Previous algorithms which use this approach in-
clude those described by Yangarber et al (2000)
and Stevenson and Greenwood (2005). A key
choice in the development of an algorithm using
this approach is the process of ranking candidate
patterns (step 4) since this determines the patterns
which will be learned at each iteration. Yangar-
ber et al (2000) chose an approach motivated by
the assumption that documents containing a large
number of patterns already identified as relevant to
a particular IE scenario are likely to contain further
relevant patterns. This approach operates by asso-
ciating confidence scores with patterns and rele-
vance scores with documents. Initially seed pat-
terns are given a maximum confidence score of 1
and all others a 0 score. Each document is given
a relevance score based on the patterns which oc-
cur within it. Candidate patterns are ranked ac-
cording to the proportion of relevant and irrele-
vant documents in which they occur, those found
in relevant documents far more than in irrelevant
ones are ranked highly. After new patterns have
been accepted all patterns? confidence scores are
updated, based on the documents in which they
occur, and documents? relevance according to the
accepted patterns they contain.
Stevenson and Greenwood (2005) suggested an
alternative method for ranking the candidate pat-
terns. Their approach relied on the assumption
that useful patterns will have similar meanings to
the patterns which have already been accepted.
They chose to represent each pattern as a vector
consisting of the lexical items which formed the
pattern and used a version of the cosine metric to
determine the similarity between pairs of patterns,
consequently this approach is referred to as ?co-
sine similarity?. The metric used by this approach
incorporated information from WordNet and as-
signed high similarity scores to patterns with sim-
ilar meanings expressed in different ways.
30
Figure 1: An example dependency tree.
3 Relation Extraction Patterns
Both these approaches used extraction pat-
terns which were based on dependency analysis
(Tesnie?re, 1959) of text. Under this approach the
structure of a sentence is represented by a set of di-
rected binary links between a word (the head) and
one of its modifiers. These links may be labelled
to indicate the grammatical relation between the
head and modifier (e.g. subject, object). Cycli-
cal paths are generally disallowed and the analysis
forms a tree structure. An example dependency
analysis for the sentence ?Acme Inc. hired Mr
Smith as their new CEO, replacing Mr Bloggs.?
is shown in Figure 1.
The extraction patterns used by both Yan-
garber et al (2000) and Stevenson and Green-
wood (2005) were based on SVO tuples ex-
tracted from dependency trees. The depen-
dency tree shown in Figure 1 would gener-
ate two patterns: replace obj??? Mr Bloggs
and Acme Inc. subj??? hire obj??? Mr Smith.
While these represent some of the core informa-
tion in this sentence, they cannot be used to iden-
tify a number of relations including the connection
between Mr. Smith and CEO or between Mr. Smith
and Mr. Bloggs.
A number of alternative approaches to con-
structing extraction patterns from dependency
trees have been proposed (e.g. (Sudo et al, 2003;
Bunescu and Mooney, 2005)). Previous analysis
(Stevenson and Greenwood, 2006a) suggests that
the most useful of these is one based on pairs of
linked chains from the dependency tree. A chain
can be defined as a path between a verb node and
any other node in the dependency tree passing
through zero or more intermediate nodes (Sudo
et al, 2001). The linked chains model (Green-
wood et al, 2005) represents extraction patterns
as a pair of chains which share the same verb but
no direct descendants. It can be shown that linked
Figure 2: Example linked chain patterns
chain patterns can represent the majority of rela-
tions within a dependency analysis (Stevenson and
Greenwood, 2006a). For example, the dependency
tree shown in Figure 1 contains four named enti-
ties (Acme Inc., Mr Smith, CEO and Mr. Bloggs)
and linked chains patterns can be used to repre-
sent the relation between any pair.1 Some exam-
ple patterns extracted from the analysis in Figure 1
can be seen in Figure 2. An additional advantage
of linked chain patterns is that they do not cause
an unwieldy number of candidate patterns to be
generated unlike some other approaches for rep-
resenting extraction patterns, such as the one pro-
posed by Sudo et al (2003) where any subtree of
the dependency tree can act as a potential pattern.
When used within IE systems these pat-
terns are generalised by replacing terms
which refer to specific entities with a gen-
eral semantic class. For example, the pattern
Acme Inc.
subj??? hire obj??? Mr Smithwould
become COMPANY subj??? hire obj??? PERSON.
4 Pattern Similarity
Patterns such as linked chains have not been used
by semi-supervised approaches to pattern learn-
ing. These algorithms require a method of de-
termining the similarity of patterns. Simple pat-
terns, such as SVO tuples, have a fixed structure
containing few items and tend to occur relatively
frequently in corpora. However, more complex
patterns, such as linked chains, have a less fixed
structure and occur less frequently. Consequently,
the previously proposed approaches for determin-
ing pattern similarity (see Section 2) are unlikely
to be as successful with these more complex pat-
terns. The approach proposed by Stevenson and
1Note that we allow a linked chain pattern to represent the
relation between two items when they are on the same chain,
such as Mr Smith and CEO in this example.
31
Greenwood (2005) relies on representing patterns
as vectors which is appropriate for SVO tuples
but not when patterns may include significant por-
tions of the dependency tree. Yangarber et al
(2000) suggested a method where patterns were
compared based on their distribution across doc-
uments in a corpus. However, since more complex
patterns are more specific they occur with fewer
corpus instances which is likely to hamper this
type of approach.
Another approach to relation extraction is to use
supervised learning algorithms, although they re-
quire more training data than semi-supervised ap-
proaches. In particular various approaches (Ze-
lenko et al, 2003; Culotta and Sorensen, 2004;
Bunescu and Mooney, 2005) have used kernel
methods to determine the sentences in a corpus
which contain instances of a particular relation.
Kernel methods (Vapnik, 1998) allow the repre-
sentation of large and complicated feature spaces
and are therefore suitable when the instances are
complex extraction rules, such as linked chains.
Several previous kernels used for relation extrac-
tion have been based on trees and include meth-
ods based on shallow parse trees (Zelenko et al,
2003), dependency trees (Culotta and Sorensen,
2004) and part of a dependency tree which rep-
resents the shortest path between the items be-
ing related (Bunescu and Mooney, 2005). Ker-
nels methods rely on a similarity function between
pairs of instances (the kernel) and these can be
used within semi-supervised approaches to pattern
learning such as those outlined in Section 2.
4.1 Structural Similarity Measure
The remainder of this Section describes a similar-
ity function for pairs of linked chains, based on
the tree kernel proposed by Culotta and Sorensen
(2004). The measure compares patterns by follow-
ing their structure from the root nodes through the
patterns until they diverge too far to be considered
similar.
Each node in an extraction pattern has three fea-
tures associated with it: the word, the relation to
a parent, and the part-of-speech (POS) tag. The
values of these features for node n are denoted
by nword, nreln and npos respectively. Pairs of
nodes can be compared by examining the values of
these features and also by determining the seman-
tic similarity of the words. A set of four functions,
F = {word, relation, pos, semantic}, is used to
compare nodes. The first three of these correspond
to the node features with the same name; the rel-
evant function returns 1 if the value of the feature
is equal for the two nodes and 0 otherwise. For
example, the pos function compares the values of
the part of speech feature for nodes n1 and n2:
pos (n1, n2) =
{
1 if n1, pos = n2, pos
0 otherwise
The remaining function, semantic, returns a
value between 0 and 1 to signify the semantic sim-
ilarity of lexical items contained in the word fea-
ture of each node. This similarity is computed us-
ing the WordNet (Fellbaum, 1998) similarity func-
tion introduced by Lin (1998) .
The similarity of two nodes is zero if their part
of speech tags are different and, otherwise, is sim-
ply the sum of the scores provided by the four
functions which form the set F . This is repre-
sented by the function s:
s (n1, n2) =
{ 0 if pos(n1, n2) = 0
?
f?F
f (n1, n2) otherwise
The similarity of a pair of linked chain patterns,
l1 and l2, is determined by the function sim:
sim (l1, l2) =
?
?
?
0 if s (r1, r2) = 0
s (r1, r2)+
simc (Cr1 , Cr2) otherwise
where r1 and r2 are the root nodes of patterns l1
and l2 (respectively) and Cr is the set of children
of node r.
The final part of the similarity function calcu-
lates the similarity between the child nodes of n1
and n2.2
simc (Cn1, Cn2) =
?
c1?Cn1
?
c2?Cn2
sim (c1, c2)
Using this similarity function a pair of identi-
cal nodes have a similarity score of four. Conse-
quently, the similarity score for a pair of linked
chain patterns can be normalised by dividing the
similarity score by 4 times the size (in nodes) of
the larger pattern. This results in a similarity func-
tion that is not biased towards either small or large
patterns but will select the most similar pattern to
those already accepted as representative of the do-
main.
This similarity function resembles the one in-
troduced by Culotta and Sorensen (2004) but also
2In linked chain patterns the only nodes with multiple
children are the root nodes so, in all but the first applica-
tion, this formula can be simplified to simc (Cn1 , Cn2) =
sim(c1, c2).
32
differs in a number of ways. Both functions make
use of WordNet to compare tree nodes. Culotta
and Sorensen (2004) consider whether one node is
the hypernym of the other while the approach in-
troduced here makes use of existing techniques to
measure semantic similarity. The similarity func-
tion introduced by Culotta and Sorensen (2004)
compares subsequences of child nodes which is
not required for our measure since it is concerned
only with linked chain extraction patterns.
5 Experiments
This structural similarity metric was implemented
within the general framework for semi-supervised
pattern learning presented in Section 2. At
each iteration the candidate patterns are compared
against the set of currently accepted patterns and
ranked according to the average similarity with the
set of similar accepted patterns. The four highest
scoring patterns are considered for acceptance but
a pattern is only accepted if its score is within 0.95
of the similarity of the highest scoring pattern.
We conducted experiments which compared the
proposed pattern similarity metric with the vec-
tor space approach used by Stevenson and Green-
wood (2005) (see Section 2). That approach was
originally developed for simple extraction patterns
consisting of subject-verb-object tuples but was
extended for extraction patterns in the linked chain
format by Greenwood et al (2005). We use the
measure developed by Lin (1998) to provide infor-
mation about lexical similarity. This is the same
measure which is used within the structural simi-
larity metric (Section 4).
Three different configurations of the iterative
learning algorithm were compared. (1) Cosine
(SVO) This approach uses the SVO model for ex-
traction patterns and the cosine similarity metric
to compare them (see Section 2). This version of
the algorithm acts as a baseline which represents
previously reported approaches (Stevenson and
Greenwood, 2005; Stevenson and Greenwood,
2006b). (2) Cosine (Linked chain) uses extrac-
tion patterns based on the linked chain model
along with the cosine similarity to compare them
and is intended to determine the benefit which is
gained from using the more expressive patterns.
(3) Structural (Linked chain) also uses linked
chain extraction patterns but compares them using
the similarity measure introduced in Section 4.1.
COMPANY
subj???appoint obj???PERSON
COMPANY
subj???elect obj???PERSON
COMPANY
subj???promote obj???PERSON
COMPANY
subj???name obj???PERSON
PERSON
subj???resign
PERSON
subj???depart
PERSON
subj???quit
Table 1: Seed patterns used by the learning algo-
rithm
5.1 IE Scenario
Experiments were carried out on the management
succession extraction task used for the Sixth
Message Understanding Conference (MUC-6)
(MUC, 1995). This IE scenario concerns the
movement of executives between positions and
companies. We used a version of the evaluation
data which was produced by Soderland (1999)
in which each event was converted into a set of
binary asymmetric relations. The corpus con-
tains four types of relation: Person-Person,
Person-Post, Person-Organisation,
and Post-Organisation. At each iteration
of the algorithm the related items identified by the
current set of learned patterns are extracted from
the text and compared against the set of related
items which are known to be correct. The systems
are evaluated using the widely used precision (P)
and recall (R) metrics which are combined using
the F-measure (F).
The texts used for these experiments have been
previously annotated with named entities. MINI-
PAR (Lin, 1999), after being adapted to handle the
named entity tags, was used to produce the depen-
dency analysis from which the pattersn were gen-
erated. All experiments used the seed patterns in
Table 1 which are indicative of this extraction task
and have been used in previous experiments into
semi-supervised IE pattern acquisition (Stevenson
and Greenwood, 2005; Yangarber et al, 2000).
The majority of previous semi-supervised ap-
proaches to IE have been evaluated over prelim-
inary tasks such as the identification of event par-
ticipants (Sudo et al, 2003) or sentence filtering
(Stevenson and Greenwood, 2005). These may be
a useful preliminary tasks but it is not clear to what
extent the success of such systems will be repeated
when used to perform relation extraction. Conse-
33
0 25 50 75 100 125 150 175 200 225 250
Iteration
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
F-
m
ea
su
re
Cosine (SVO)
Cosine (Linked-Chains)
Structural (Linked-Chains)
Figure 3: F-measure scores for relation extraction
over 250 iterations
quently we chose a relation extraction task to eval-
uate the work presented here.
6 Results
Results from the relation extraction evaluation can
be seen in Table 2 and Figure 3. The seven seed
patterns achieve a precision of 0.833 and recall of
0.022. The two approaches based on cosine sim-
ilarity performs poorly, irrespective of the pattern
model being used. The maximum increase in F-
measure of 0.15 (when using the cosine measure
with the linked chain model) results in a maximum
F-measure for the cosine similarity model of 0.194
(with a precision of 0.491 and recall of 0.121) after
200 iterations.
The best result is recorded when the linked
chain model is used with the similarity measure
introduced in Section 4.1, achieving a maximum
F-measure of 0.329 (with a precision of 0.434 and
recall of 0.265) after 190 iterations. This is not
a high F-measure when compared against super-
vised IE systems, however it should be remem-
bered that this represents an increase of 0.285 in
F-measure over the original seven seed patterns
and that this is achieved with a semi-supervised
algorithm.
7 Conclusions
A number of conclusions can be drawn from
the work described in this paper. Firstly, semi-
supervised approaches to IE pattern acquisition
benefit from the use of more expressive extraction
pattern models since it has been shown that the
performance of the linked chain model on the rela-
tion extraction task is superior to the simpler SVO
model. We have previously presented a theoret-
ical analysis (Stevenson and Greenwood, 2006a)
which suggested that the linked chain model was a
more suitable format for IE patterns than the SVO
model but these experiments are, to our knowl-
edge, the first to show that applying this model
improves learning performance. Secondly, these
experiments demonstrate that similarity measures
inspired by kernel functions developed for use in
supervised learning algorithms can be applied to
semi-supervised approaches. This suggests that
future work in this area should consider applying
other similarity functions, including kernel meth-
ods, developed for supervised learning algorithms
to the task of semi-supervised IE pattern acquisi-
tion. Finally, we demonstrated that this similar-
ity measure outperforms a previously proposed ap-
proach which was based on cosine similarity and a
vector space representation of patterns (Stevenson
and Greenwood, 2005).
Acknowledgements
This work was carried out as part of the RESuLT
project funded by the Engineering and Physical
Sciences Research Council (GR/T06391) and par-
tially funded by the IST 6th Framework project X-
Media (FP6-26978).
References
Razvan Bunescu and Raymond Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of the Human Language Tech-
nology Conference and Conference on Empirical
Methods in Natural Language Processing, pages
724?731, Vancouver, B.C.
Aron Culotta and Jeffery Sorensen. 2004. Dependency
Tree Kernels for Relation Extraction. In 42nd An-
nual Meeting of the Association for Computational
Linguistics, Barcelona, Spain.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database and some of its Applica-
tions. MIT Press, Cambridge, MA.
Mark A. Greenwood, Mark Stevenson, Yikun Guo,
Henk Harkema, and Angus Roberts. 2005. Au-
tomatically Acquiring a Linguistically Motivated
Genic Interaction Extraction System. In Proceed-
ings of the 4th Learning Language in Logic Work-
shop (LLL05), Bonn, Germany.
Dekang Lin. 1998. An information-theoretic def-
inition of similarity. In Proceedings of the Fif-
34
Iteration Cosine (SVO) Cosine (Linked Chains) Structural (Linked Chains)
# P R F P R F P R F
0 0.833 0.022 0.044 0.833 0.022 0.044 0.833 0.022 0.044
25 0.600 0.081 0.142 0.833 0.022 0.044 0.511 0.103 0.172
50 0.380 0.085 0.139 0.500 0.022 0.043 0.482 0.179 0.261
75 0.383 0.103 0.163 0.417 0.022 0.043 0.484 0.197 0.280
100 0.383 0.103 0.163 0.385 0.022 0.042 0.471 0.220 0.300
125 0.383 0.103 0.163 0.500 0.081 0.139 0.441 0.220 0.293
150 0.383 0.103 0.163 0.500 0.099 0.165 0.429 0.229 0.298
175 0.383 0.103 0.163 0.481 0.112 0.182 0.437 0.247 0.315
200 0.383 0.103 0.163 0.491 0.121 0.194 0.434 0.265 0.329
225 0.383 0.103 0.163 0.415 0.121 0.188 0.434 0.265 0.329
250 0.383 0.103 0.163 0.409 0.121 0.187 0.413 0.265 0.322
Table 2: Comparison of the different similarity functions when used to perform relation extraction
teenth International Conference on Machine learn-
ing (ICML-98), Madison, Wisconsin.
Dekang Lin. 1999. MINIPAR: A Minimalist Parser.
In Maryland Linguistics Colloquium, University of
Maryland, College Park.
MUC. 1995. Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6), San Mateo, CA.
Morgan Kaufmann.
Ellen Riloff. 1996. Automatically generating extrac-
tion patterns from untagged text. In Thirteenth Na-
tional Conference on Artificial Intelligence (AAAI-
96), pages 1044?1049, Portland, OR.
Stephen Soderland. 1999. Learning Information Ex-
traction Rules for Semi-structured and free text. Ma-
chine Learning, 31(1-3):233?272.
Mark Stevenson and Mark A. Greenwood. 2005. A
Semantic Approach to IE Pattern Induction. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 379?386,
Ann Arbor, MI.
Mark Stevenson and Mark A. Greenwood. 2006a.
Comparing Information Extraction Pattern Mod-
els. In Proceedings of the Information Extraction
Beyond The Document Workshop (COLING/ACL
2006), Sydney, Australia.
Mark Stevenson and Mark A. Greenwood. 2006b.
Learning Information Extraction Patterns using
WordNet. In Third International Global WordNet
Conference (GWC-2006), Jeju Island, Korea.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2001. Automatic Pattern Acquisition for Japanese
Information Extraction. In Proceedings of the Hu-
man Language Technology Conference (HLT2001).
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An Improved Extraction Pattern Representa-
tion Model for Automatic IE Pattern Acquisition. In
Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics (ACL-03),
pages 224?231, Sapporo, Japan.
Lucien Tesnie?re. 1959. Eleme?nts de Syntaxe Struc-
turale. Klincksiek, Paris.
Vladimir Vapnik. 1998. Statistical Learning Theory.
John Wiley and Sons.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic Acquisition of
Domain Knowledge for Information Extraction. In
Proceedings of the 18th International Conference on
Computational Linguistics (COLING 2000), pages
940?946, Saarbru?cken, Germany.
Roman Yangarber. 2003. Counter-training in the Dis-
covery of Semantic Patterns. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL-03), pages 343?350, Sap-
poro, Japan.
Dimitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. Journal of Machine Learning Research,
3:1083?1106.
35
Multilingual versus Monolingual WSD 
Lucia Specia 
ICMC ? University of S?o Paulo 
Av. do Trabalhador S?o-Carlense, 400 
S?o Carlos, 13560-970, Brazil 
lspecia@icmc.usp.br
Maria das Gra?asVolpe Nunes 
ICMC ? University of S?o Paulo 
Av. do Trabalhador S?o-Carlense, 400 
S?o Carlos, 13560-970, Brazil 
gracan@icmc.usp.br
Mark Stevenson 
Computer Science ? University of Sheffield 
Regent Court, 211 Portobello Street 
Sheffield, S1 4DP, UK  
M.Stevenson@dcs.shef.ac.uk  
Gabriela Castelo Branco Ribeiro 
DL - Pontificial Catholic University - Rio 
R. Marqu?s de S?o Vicente, 225 - G?vea 
Rio de Janeiro, RJ, Brazil. CEP: 22.453-900 
gabrielacastelo@globo.com
  
Abstract 
Although it is generally agreed that Word 
Sense Disambiguation (WSD) is an ap-
plication dependent task, the great major-
ity of the efforts has aimed at the devel-
opment of WSD systems without consid-
ering their application. We argue that this 
strategy is not appropriate, since some 
aspects, such as the sense repository and 
the disambiguation process itself, vary 
according to the application. Taking Ma-
chine Translation (MT) as application 
and focusing on the sense repository, we 
present evidence for this argument by ex-
amining WSD in English-Portuguese MT 
of eight sample verbs. By showing that 
the traditional monolingual WSD strate-
gies are not suitable for multilingual ap-
plications, we intend to motivate the de-
velopment of WSD methods for particu-
lar applications.  
1 Introduction 
Word Sense Disambiguation (WSD) is con-
cerned with the choice of the most appropriate 
sense of an ambiguous word given its context. 
The applications for which WSD has been 
thought to be helpful include Information Re-
trieval, Information Extraction, and Machine 
Translation (MT) (Ide and Ver?nis, 1998). The 
usefulness of WSD for MT, particularly, has 
been recently subject of debate, with conflicting 
results. Vickrey et al (2005), e.g., show that the 
inclusion of a WSD module significantly im-
proves the performance of their statistical MT 
system. Conversely, Carpuat and Wu (2005) 
found that WSD does not yield significantly bet-
ter translation quality than a statistical MT sys-
tem alone. In this latter work, however, the WSD 
module was not specifically designed for MT: it 
is based on the use of monolingual methods to 
identify the source language senses, which are 
then mapped into the target language transla-
tions. 
In fact, although it has been agreed that WSD 
is more useful when it is meant for a specific ap-
plication (Wilks and Stevenson, 1998; Kilgarriff, 
1997; Resnik and Yarowsky, 1997), little has 
been done on the development of WSD modules 
specifically for particular applications. WSD 
models in general are application independent, 
and focus on monolingual contexts, particularly 
English. 
Approaches to WSD as an application-
independent task usually apply standardised 
sense repositories, such as WordNet (Miller, 
1990). For multilingual applications, a popular 
approach is to carry out monolingual WSD and 
then map the source language senses into the cor-
responding target word translations (Carpuat and 
Wu, 2005; Montoyo et al, 2002). Although this 
strategy can yield reasonable results for certain 
pairs of languages, especially those which have a 
common sense repository, such as EuroWordNet 
(Vossen, 1998), mapping senses between lan-
guages is a very complex issue (cf. Section 2).  
33
We believe that WSD is an intermediate, applica-
tion dependent task, and thus WSD modules for 
particular applications must be developed fol-
lowing the requirements of such applications. 
Many key factors of the process are application-
dependent. The main factor is the sense inven-
tory. As emphasized by Kilgarriff (1997), no 
sense inventory is suitable for all applications. 
Even for the same application there is often little 
consensus about the most appropriate sense in-
ventory. For example, the use of WordNet, al-
though very frequent, has been criticized due to 
characteristics such as the level sense granularity 
and the abstract criteria used for the sense dis-
tinctions in that resource (e.g., Palmer 1998). In 
particular, it is generally agreed that the granular-
ity in WordNet is too refined for MT.  
In addition to requiring different sense inven-
tories (Hutchins and Somers, 1992), the disam-
biguation process itself often can be varied ac-
cording to the application. For instance, in mono-
lingual WSD, the main information source is the 
context of the ambiguous word, that is, the sur-
rounding words in a sentence or paragraph. For 
MT purposes, the context can be also that of the 
translation in the target language, i.e., words 
which have been already translated.  
In this paper we focus on the differences in the 
sense inventory, contrasting the WordNet inven-
tory for English disambiguation, which was cre-
ated according to psycholinguistics principles, 
with the Portuguese translations assigned to a set 
of eight verbs in a corpus, simulating MT as a 
Computational Linguistics application.  
We show that the relation between the number 
of senses and translations is not a one-to-one, 
and that it is not only a matter of the level of re-
finement of WordNet. The number of transla-
tions can be either smaller or larger, i.e., either 
two or more senses can be translated as the same 
word, or the same sense can be translated using 
different words. With that, we present evidence 
that employing a monolingual WSD method for 
the task of MT is not appropriate, since monolin-
gual information offers little help to multilingual 
disambiguation. In other words, we argue that 
multilingual WSD is different from monolingual 
WSD, and thus requires specific strategies. We 
start by presenting approaches that show cognate 
results for different pairs of languages, and also 
approaches developed with the reverse goal of 
using multilingual information to help monolin-
gual WSD (Section 2). We then present our ex-
periments (Sections 3 and 4) and their results 
(Section 5).  
2 Related work  
Recently, others have also investigated the dif-
ferences between sense repositories for monolin-
gual and multilingual WSD. Chatterjee et al 
(2005), e.g., investigated the ambiguity in the 
translation of the English verb ?to have? into 
Hindi. 11 translation patterns were identified for 
the 19 senses of the verb, according to the vari-
ous target syntactic structures and/or target 
words for the verb. They argued that differences 
in both these aspects do not depend only on the 
sense of the verb. Out of the 14 senses analyzed, 
six had 2-5 different translations each.  
Bentivogli et al (2004) proposed an approach 
to create an Italian sense tagged corpus (Mul-
tiSemCor) based on the transference of the anno-
tations from the English sense tagged corpus 
SemCor (Miller et al, 1994), by means of word-
alignment methods. A gold standard corpus was 
created by manually transferring senses in Sem-
Cor to the Italian words in a translated version of 
that corpus. From a total of 1,054 English words, 
155 annotations were considered non-
transferable to their corresponding Italian words, 
mainly due to the lack of synonymy at the lexical 
level.  
Mih?ltz (2005) manually mapped senses from 
the English in a sense tagged corpus to Hungar-
ian translations, in order to carry out WSD be-
tween these languages. Out of 43 ambiguous 
nouns, 38 had all or most of their English senses 
mapped into the same Hungarian translation. 
Some senses of the remaining nouns had to be 
split into different Hungarian translations. On 
average, the sense mapping decreased the ambi-
guity from 3.97 English senses to 2.49 Hungar-
ian translations. 
As we intend to show with this work, differ-
ences like those mentioned above in the sense 
inventories make it inappropriate to use mono-
lingual WSD strategies for multilingual disam-
biguation. Nevertheless, some approaches have 
successfully employed multilingual information, 
especially parallel corpora, to support monolin-
gual WSD. They are motivated by the argument 
that the senses of a word should be determined 
based on the distinctions that are lexicalized in a 
second language (Resnik and Yarowsky, 1997). 
In general, the assumptions behind these ap-
proaches are the following:  
(1) If a source language word is translated dif-
ferently into a second language, it might be am-
biguous and the different translations can indi-
cate the senses in the source language.  
34
(2) If two distinct source language words are 
translated as the same word into a second lan-
guage, it often indicates that the two are being 
used with similar senses.  
Ide (1999), for example, analyzes translations 
of English words into four different languages, in 
order to check if the different senses of an Eng-
lish word are lexicalized by different words in all 
the other languages. A parallel aligned corpus is 
used and the translated senses are mapped into 
WordNet senses. She uses this information to 
determine a set of monolingual sense distinctions 
that is potentially useful for NLP applications. In 
subsequent work (Ide et al, 2002), seven lan-
guages and clustering techniques are employed 
to create sense groups based on the translations.  
Diab and Resnik (2002) use multilingual in-
formation to create an English sense tagged cor-
pus to train a monolingual WSD approach. An 
English sense inventory and a parallel corpus 
automatically produced by an MT system are 
employed. Sentence and word alignment systems 
are used to assign the word correspondences be-
tween the two languages. After grouping all the 
words that correspond to translations of a single 
word in the target language, all their possible 
senses are considered as candidates. The sense 
that maximizes the semantic similarity of the 
word with the others in the group is chosen.  
Similarly, Ng et al (2003) employ English-
Chinese parallel word aligned corpora to identify 
a repository of senses for English. The English 
word senses are manually defined, based on the 
WordNet senses, and then revised in the light of 
the Chinese translations. For example, if two oc-
currences of a word with two different senses in 
WordNet are translated into the same Chinese 
word, they will be considered to have the same 
English sense.  
In general, these approaches rely on the two 
previously mentioned assumptions about the in-
teraction between translations and word senses. 
Although these assumptions can be useful when 
using cross-language information as an approxi-
mation to monolingual disambiguation, they are 
not very helpful in the opposite direction, i.e., 
using monolingual information for cross-
language disambiguation, as we will show in 
Section 4.  
3 Experimental setting  
We focused our experiments on verbs, which 
represent difficult cases for WSD. In particular, 
we experimented with five frequent and highly 
ambiguous verbs identified as problematic for 
MT systems in a previous study (Specia, 2005): 
?to come?, ?to get?, ?to give?, ?to look?, and ?to 
make?; and other three frequent verbs that are 
not so ambiguous: ?to ask?, ?to live?, and ?to 
tell?. The inclusion of the additional verbs allows 
us to analyze the effect of the ambiguity level in 
the experiment. These verbs will then be trans-
lated into Portuguese so that the resulting transla-
tions can be contrasted to the English senses. 
3.1 Corpus selection 
We collected all the sentences containing one of 
the eight verbs and their corresponding phrasal 
verbs from SemCor, Senseval-2 and Senseval-3 
corpora1. These corpora were chosen because 
they are both widely used and easily available. In 
each of these corpora, ambiguous words are an-
notated with WordNet 2.0 senses. Occurrences 
which did not identify a unique sense were not 
used. The numbers of sentences selected for each 
verb and its phrasal verbs are shown in Table 1. 
Verb # Verb  
Occurrences
# Phrasal Verb 
Occurrences 
ask 414 8
come 674 330
get 683 267
give 740 79
live 242 5 
look 370 213 
make 1463 105 
tell 509 3 
Table 1. Number of verbs and phrasal verbs ex-
tracted from SemCor and Senseval corpora 
It is worth mentioning that the phrasal verbs in-
clude simple verb-particle constructions, such as 
?give up?, and more complex multi-word expres-
sions, e.g., ?get in touch with?, ?make up for?, 
?come to mind?, etc. 
In order to avoid biasing the experiment due to 
possible misunderstandings of the verb uses, and 
to make the experiment feasible, with a reason-
able number of occurrences to be analyzed, we 
selected a subset of the total number of sentences 
in Table 1, which were distributed among five 
professional English-Portuguese translators (T1, 
T2, T3, T4, T5), according to the following crite-
ria:  
- The meaning of the verb/phrasal verb in the 
context of the sentence should be understandable 
and non-ambiguous (for human translators). 
                                                
1
 Available at http://www.cs.unt.edu/~rada/downloads.html. 
35
- The experiment should be the most compre-
hensive possible, with the largest possible num-
ber of senses for each verb/phrasal. 
- Each translator should be given two occur-
rences (when available) of all the distinct senses 
of each verb/phrasal verb, in order to make it 
possible to contrast different uses of the verb.  
- The translators should not be given any in-
formation other than the sentence to select the 
translation.  
To meet these criteria, a professional translator, 
who was not involved in the translation task, 
post-processed the selected sentences, filtering 
them according to the criteria specified above. 
Due to both the scarce number of occurrences of 
each phrasal verb sense and the large number of 
different phrasal verbs for certain verbs, the post-
selection of phrasal verbs was different from the 
post-selection of verbs. In the case of verbs, the 
translator scanned the sentences in order to get 
10 distinct occurrences of each sense (two for 
each translator), eliminating those sentences 
which were too complex to understand or used 
the verb in an ambiguous way. This process did 
not eliminate any senses, and thus did not reduce 
the coverage of the experiment. When there were 
fewer than 10 occurrences of a given sense, sen-
tences were repeated among translators to guar-
antee that each translator would be given exam-
ples of all the senses of the verb. For instance, if 
a sense had only four occurrences, the first two 
occurrences were given to T1, T3 and T5, while 
the other two occurrences were given to T2 and 
T4. If a sense occurred only once for a verb, it 
was repeated for all five translators. 
For phrasal verbs, the same process was used 
to eliminate the complex and ambiguous sen-
tences. Two occurrences (when available) of 
each sense of a phrasal verb were then selected. 
Due to the large number of different phrasal 
verbs for certain verbs, they were divided among 
translators, so that each translator was given two 
occurrences of only some phrasal verbs of each 
verb. Sentences were distributed so that all trans-
lators had a similar number of cases, as shown in 
Table 2. 
In order to avoid biasing the translations ac-
cording to the English senses, the original sense 
annotations were not shown to the translators and 
the sentences for each of the verbs, together with 
their phrasal verbs, were randomly ordered. 
Additionally, we gave the same set of selected 
sentences to another group of five translators, so 
that we could analyze the reliability of the ex-
periment by investigating the agreement between 
the groups of translators on the same data. 
Translator
Verb 
# T1 # T2 # T3 # T4 # T5
ask 13 13 13 10 10
come 53 52 52 51 47
get 59 59 56 59 57
give 46 50 48 47 48
live 11 11 11 16 16
look 15 19 17 19 14
make 47 45 44 46 41
tell 14 12 12 15 10
Total  258 261 253 263 243
Table 2. Number of selected sentences and its 
distribution among the five translators 
3.2 English senses and Portuguese transla-
tions 
As mentioned above, the corpora used are tagged 
with WordNet senses. Although this may not be 
the optimal sense inventory for many purposes, it 
is the best option in terms of availability and 
comprehensiveness. Moreover, it is the most fre-
quently used repository for monolingual WSD 
systems, making it possible to generalize, to a 
certain level, our results to most of the monolin-
gual work. The number of senses for the eight 
selected verbs (and their phrasal verbs) in 
WordNet 2.0, along with the number of their 
possible translations in bilingual dictionaries2, is 
shown in Table 3. 
Verb # Senses # Translations 
ask  12 16
come 108 226
get  147 242
give  92 128
live  15 15
look  34 63
make 96 239
tell  12 28
Table 3. Verbs, possible senses and translations 
As we can see, the number of possible transla-
tions is different from the number of possible 
senses, which already shows that there is not a 
one-to-one correspondence between senses and 
translations (although there is a high correlation 
between the number of senses and translations: 
Pearson?s Correlation = 0.955). In general, the 
number of possible translations is greater than 
                                                
2
 For example, DIC Pratico Michaelis?, version 5.1. 
36
the number of possible senses, in part because 
synonyms are considered as different transla-
tions. As we will show in Section 5 (Table 4), we 
eliminate the use of synonyms as possible trans-
lations. Moreover, we are dealing with a limited 
set of possible senses, provided by the SemCor 
and Senseval data. As a consequence, the num-
ber of translations pointed out by the human 
translators for our corpus will be considerably 
smaller than the total number of possible transla-
tions. 
4 Contrasting senses and translations 
In order to contrast the English senses with the 
Portuguese translations, we submitted the se-
lected sentences (cf. Section 3.1) to two groups 
of five translators (T1, T2, T3, T4, and T5), all 
native speakers of Portuguese. We asked the 
translators to assign the appropriate translation to 
each of the verb occurrences, which we would 
then compare to the original English senses. 
They were not told what their translations were 
going to be used for. 
The translators were provided with entire sen-
tences, but for practical reasons they were asked 
to translate only the verb and were allowed to 
use any bilingual resource to search for possible 
translations, if needed. They were asked to avoid 
considering synonyms as different translations.  
The following procedure was defined to ana-
lyze the results returned by the translators, for 
each verb and its phrasal verbs separately: 
1) We grouped all the occurrences of an Eng-
lish sense and looked at all the translations used 
by the translators in order to identify synonyms 
(in those specific uses), using a dictionary of 
Portuguese synonyms. Synonyms were consid-
ered as unique translations.  
2) We then analyzed the sentences which had 
been given to multiple translators of the same 
group (when there were not enough occurrences 
of certain senses, as mentioned in Section 3.1), in 
order to identify a single translation for the oc-
currence and eliminate redundancies. The trans-
lation chosen was the one pointed out by the ma-
jority of the translators. When it was not possible 
to elect only one translation, the n equally most 
used were kept, and thus the sentence was re-
peated n times. 
3) Finally, we examined the relation between 
senses and translations, focusing on two cases: 
(1) if a sense had only one or many translations; 
and (2) if a translation referred to only one or 
many senses, i.e., whether the sense was shared 
by many translations. We placed each sense into 
two of the following categories, explained be-
low: (a) or (b), mutually exclusive, representing 
the first case; and (c), (d) or (e), also mutually 
exclusive, representing the second case. 
(a) 1 sense  1 translation: all the occur-
rences of the same sense being translated as 
the same Portuguese word. For example, ?to 
ask?, in the sense of ?inquire, enquire?, is al-
ways translated as ?perguntar?. 
(b) 1 sense  n translations: different oc-
currences of the same sense being translated as 
different, non-synonyms, Portuguese words. 
For example, ?to look?, in the sense of ?per-
ceive with attention; direct one's gaze to-
wards? can be translated as ?olhar?, ?assistir?, 
and ?voltar-se?. 
(c) n senses  1 translation (ambiguous): 
Different senses of a word being translated as 
the same Portuguese word, which encom-
passes all the English senses. For example, 
?make?, in the sense of ?engage in?, ?create?, 
and ?give certain properties to something?, is 
translated as ?fazer?, which carries the three 
senses. 
(d) n senses  1 translation (non-
ambiguous): different senses of a word being 
translated using the same Portuguese word, 
which has only one sense. For example, ?take 
advantage? in both the senses of ?draw advan-
tages from? and ?make excessive use of?, be-
ing translated as ?aproveitar-se?.  
(e) n senses  n translations: different 
senses of a word being translated as different 
Portuguese words. For example, the ?move 
fast? and ?carry out a process or program? 
senses of the verb ?run? being translated re-
spectively as ?correr? and ?executar?. 
Items (a) and (e) represent cases where multilin-
gual ambiguity only reflects the monolingual 
one, that is, to all the occurrences of every sense 
of an English word corresponds a specific Portu-
guese translation. On the other hand, items (b), 
(c) and (d) provide evidence that multilingual 
ambiguity is different from monolingual ambigu-
ity. Item (b) means that different criteria are 
needed for the disambiguation, as ambiguity 
arises only during the translation, due to specific 
principles used to distinguish senses in Portu-
guese. Items (c) and (d) mean that disambigua-
tion is not necessary, as either the Portuguese 
37
translation is also ambiguous, embracing the 
same senses of the English word, or Portuguese 
has a less refined sense distinction. 
5 Results and discussion 
Table 4 presents the number of different sen-
tences analyzed for each of the verbs (after 
grouping and eliminating the repeated sen-
tences), the English (E) senses and (non-
synonyms) Portuguese (P) translations in our 
corpus, followed by the percentage of occur-
rences of each of the categories outlined in Sec-
tion 4 (a ? e) with respect to the number of 
senses (# Senses) for that verb. Items (c) and (d) 
were grouped, since for practical purposes it is 
not important to tell if the P word translating the 
various E senses encompasses one or many 
senses. For items (b) and (c&d) we also present 
the average of P translations per E sense ((b) av-
erage), and the average of E senses per P transla-
tion, respectively ((c&d) average).  
We divided the analysis of these results ac-
cording to our two cases (cf. Section 4): the first 
covers items (c&d) and (e) (light grey in Table 
4), while the second covers items (a) and (b) 
(dark grey in Table 4). 
1) Items (c), (d) and (e): n senses ? ? transla-
tion(s) 
The number of senses in the corpus is almost 
always greater than the number of translations, 
suggesting that the level of sense distinctions in 
WordNet can be too fine-grained for translation 
applications The numbers of senses and transla-
tions are in an opposite relation comparing to the 
one shown in Table 3, where the number of pos-
sible translations was larger than the number of 
possible senses. This shows that indeed many of 
the possible translations are synonyms.  
On average, the level of ambiguity decreased 
from 40.3 (possible senses) to 24.4 (possible 
translations), if the monolingual and multilingual 
ambiguity are compared in the corpus. If we con-
sider the five most ambiguous verbs, the level of 
ambiguity decreased from 58.8 to 35. For the 
other three less ambiguous verbs, the level of 
ambiguity decreased from 9.3 to 6.7.  
Column % (c&d) shows the percentage of 
senses, with respect to the total shown in the 
third column (# Senses), which share translations 
with other senses. A shared translation means 
that several senses of the verb have the same 
translation. (c&d) average indicates the average 
number of E senses per P translation, for those 
cases where translations are shared. For all verbs, 
on average translations cover more than two 
senses. The level of variation in the number of 
shared translations among senses is high, e.g., 
from 2 (translation = ?organizar?) to 27 (transla-
tion = ?dar?) for the verb ?to give?. Contrasting 
the percentage of senses that share translations, 
in % (c), with the percentages in % (d), which 
refers to the senses for which translations are not 
shared, we can see that the great majority of 
senses have translations in common with other 
senses, and thus the disambiguation among these 
senses would not be necessary in most of the 
cases. In fact, it could result in errors, since an 
incorrect sense could be chosen. 
2) Items (a) and (b): 1 sense ? ? translation(s) 
As previously mentioned, the differences in the 
sense inventory for monolingual and multilingual 
WSD are not only due to the fact that sense dis-
tinctions in WordNet are too refined. That would 
only indicate that using monolingual WSD for 
multilingual purposes implies unnecessary work. 
However, we consider that the most important 
problem is the one evidenced by item (b) in the 
sixth column in Table 4. For all the verbs except 
?to ask? (the least ambiguous), there were cases 
in which different occurrences of the same sense 
were translated into different, non-synonyms 
words. Although the proportion of senses with 
only one translation is greater, as shown by item 
(a) in the fifth column, the percentage of senses 
with more than one translation is impressive, 
especially for the five most ambiguous verbs. In 
face of this, the lack of disambiguation of a word 
during translation based on the fact that the word 
is not ambiguous in the source language can re-
sult in very serious translation errors when 
monolingual methods are employed for multilin-
gual WSD. Therefore, this also shows that, for 
these verbs, sense inventories that are specific to 
the translation between the pair of languages un-
der consideration would be more appropriate to 
achieve effective WSD. 
5.1 Agreement between translators 
In an attempt to quantify the agreement between 
the two groups of translators, we computed the 
Kappa coefficient for annotation tasks, as de-
fined by Carletta (1996). Kappa was calculated 
separately for our two areas of inquiry, i.e., cases 
(1) and (2) discussed in Section 5.  
In the experiment referring to case (1), groups 
were considered to agree about a sense of a verb 
if they both judged that the translation of such 
38
Verb # Sen-
tences
# Senses # Transla-
tions 
% (a)  % (b) (b) av-
erage 
%  
(c&d) 
(c&d) av-
erage 
% (e) 
ask 83 8 3 100 0 0 87.5 3.5 12.5 
come 202 68 42 62 38 3.1 73.2 6.3 26.8 
get 226 90 61 70 30 2.6 61.1 3.4 38.9 
give 241 57 12 48.7 51.3 3.3 84.2 6.3 15.8 
live 55 10 7 83.3 16.7 3.0 70 2.7 30 
look 82 26 18 63.2 36.8 2.4 84.6 2.7 15.4 
make 225 53 42 51.4 48.6 2.9 77.4 4.1 22.6 
tell 73 10 10 37.5 62.5 2.8 60 4.0 40 
Table 4. Results of the procedure contrasting senses and translations 
verb was or was not shared by other senses. For 
example, both groups agreed that the word 
?fazer? should be used to translate occurrences 
of many senses of the verb ?to make?, including 
?engage in?, ?give certain properties to some-
thing?, and ?make or cause to be or to become?. 
On the other hand, the groups disagreed about 
the sense ?go off or discharge? of the phrasal 
verb ?to go off?: the first group found that the 
translation of that sense, ?disparar?, did not refer 
to any other sense, while the second group used 
that word to translate also the sense ?be dis-
charged or activated? of the same phrasal verb.  
In the experiment with case (2), groups were 
considered to agree about a sense if they both 
judged that the sense had or had not more than 
one translation. For example, both groups agreed 
that the sense ?reach a state, relation, or condi-
tion? of the verb ?to come? should be translated 
by more than one Portuguese word, including 
?terminar?, ?vir?, and ?chegar?. They also 
agreed that the sense ?move toward, travel to-
ward something or somebody or approach some-
thing or somebody? of the same verb had only 
one translation, namely ?vir?.  
The average Kappa coefficient obtained was 
0.66 for item (1), and 0.65 for item (2). There is 
not a reference value for this particular annota-
tion task (translation annotation), but the levels 
of agreement pointed by Kappa here can be con-
sidered satisfactory. The agreement levels are 
close to the coefficient suggested by Carletta as 
indicative of a good agreement level for dis-
course annotation (0.67), and which has been 
adopted as a cutoff in Computational Linguistics. 
6 Conclusions and future work 
We presented experiments contrasting monolin-
gual and multilingual WSD. It was found that, in 
fact, monolingual and multilingual disambigua-
tion differ in many respects, particularly the 
sense repository, and therefore specific strategies 
could be more appropriate to achieve effective 
multilingual WSD. We investigated the differ-
ences in sense repositories considering English-
Portuguese translation, using a set of eight am-
biguous verbs collected from sentences in Sem-
Cor and Senseval corpora. The English sense 
tags given by WordNet were compared to the 
Portuguese translations assigned by two groups 
of five human translators.  
Results corroborate previous cognate work, 
showing that there is not a one-to-one mapping 
between the English senses and their translations 
(to Portuguese, in this study). In most of the 
cases, many different senses were translated into 
the same Portuguese word. In many other cases, 
different, non-synonymous, words were neces-
sary to translate occurrences of the same sense of 
the source language, showing that differences 
between monolingual and multilingual WSD are 
not only a matter of the highly refined sense dis-
tinction criterion adopted in WordNet. Therefore, 
these results reinforce our argument that apply-
ing monolingual methods for multilingual WSD 
can either imply unnecessary work, or result in 
disambiguation errors.  
As future work we plan to carry out further in-
vestigation of the differences between monolin-
gual and multilingual WSD contrasting the Eng-
lish senses and translations into other languages, 
and analyzing other grammatical categories, par-
ticularly nouns.  
References  
Bentivogli, L., Forner, P., and Pianta, E. (2004). 
Evaluating Cross-Language Annotation Trans-
fer in the MultiSemCor Corpus. COLING-
2004, Geneva, pp. 364-370. 
Carletta, J. (1996). Assessing agreement on clas-
sification tasks: the kappa statistic. Computa-
tional Linguistics, 22(2), pp. 249-254. 
Carpuat, M. and Wu, D. (2005). Word sense dis-
ambiguation vs. statistical machine translation. 
43rd ACL Meeting, Ann Arbor, pp. 387?394. 
39
 Chatterjee, N., Goyal, S., and Naithani, A. 
(2005). Pattern Ambiguity and its Resolution 
in English to Hindi Translation. RANLP-2005, 
Borovets, pp. 152-156. 
Diab, M. and Resnik, P. (2002). An Unsuper-
vised Method for Word Sense Tagging using 
Parallel Corpora. 40th ACL Meeting, Philadel-
phia. 
Hutchins, W.J. and Somers H.L. (1992) An In-
troduction to Machine Translation. Academic 
Press, Great Britain. 
Ide, N. and V?ronis, J. (1998). Word Sense Dis-
ambiguation: The State of the Art. Computa-
tional Linguistics, 24 (1).  
Ide, N. (1999). Parallel Translations as Sense 
Discriminators. SIGLEX99 Workshop: Stan-
dardizing Lexical Resources, Maryland, pp. 
52-61. 
Ide, N., Erjavec, T., and Tufi, D. (2002). Sense 
Discrimination with Parallel Corpora. ACL'02 
Workshop on Word Sense Disambiguation: 
Recent Successes and Future Directions, 
Philadelphia, pp. 54-60.  
Kilgarriff, A. (1997). I Don't Believe in Word 
Senses. Computers and the Humanities, 31 
(2):91-113. 
Mih?ltz, M. (2005). Towards A Hybrid Ap-
proach to Word-Sense Disambiguation in Ma-
chine Translation. RANLP-2005 Workshop: 
Modern Approaches in Translation Technolo-
gies, Borovets.  
Miller, G.A., Beckwith, R.T., Fellbaum, C.D., 
Gross, D., and Miller, K. (1990). WordNet: 
An On-line Lexical Database. International 
Journal of Lexicography, 3(4):235-244. 
Miller, G.A., Chorodow, M., Landes, S., Lea-
cock, C., and Thomas, R.G. (1994). Using a 
Semantic Concordancer for Sense Identifica-
tion. ARPA Human Language Technology
Workshop - ACL, Washington, pp. 240-243. 
Montoyo, A., Romero, R., Vazquez, S., Calle, 
M., and Soler, S. (2002). The Role of WSD 
for Multilingual Natural Language Applica-
tions.  TSD?2002, Czech Republic, pp. 41-48. 
Ng, H.T., Wang, B., and Chan, Y.S. (2003). Ex-
ploiting Parallel Texts for Word Sense Disam-
biguation: An Empirical Study. 41st ACL 
Meeting, Sapporo, pp. 455-462.  
Palmer, M. (1998). Are WordNet sense distinc-
tions appropriate for computational lexicons? 
Senseval, Siglex98, Brighton.  
Resnik, P. and Yarowsky, D. (1997). A Perspec-
tive on Word Sense Disambiguation Methods 
and their Evaluating. ACL-SIGLEX Workshop 
Tagging Texts with Lexical Semantics: Why, 
What and How?, Washington. 
Specia, L. (2005). A Hybrid Model for Word 
Sense Disambiguation in English-Portuguese 
Machine Translation. 8th CLUK, Manchester, 
pp. 71-78. 
Vickrey, D., Biewald, L., Teyssier, M., and 
Koller, D. (2005). Word-Sense Disambigua-
tion for Machine Translation. HLT/EMNLP, 
Vancouver.
Vossen, P. (1998). EuroWordNet: Building a 
Multilingual Database with WordNets for 
European Languages. The ELRA Newsletter, 
3(1). 
Wilks, Y. and Stevenson, M. (1998). The Gram-
mar of Sense: Using Part-of-speech Tags as a 
First Step in Semantic Disambiguation. Natu-
ral Language Engineering, 4(1):1-9.  
40
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 81?88,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Task-based Comparison of Information Extraction Pattern Models
Mark A. Greenwood and Mark Stevenson
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, UK
{m.greenwood, marks}@dcs.shef.ac.uk
Abstract
Several recent approaches to Information
Extraction (IE) have used dependency trees
as the basis for an extraction pattern repre-
sentation. These approaches have used a va-
riety of pattern models (schemes which de-
fine the parts of the dependency tree which
can be used to form extraction patterns).
Previous comparisons of these pattern mod-
els are limited by the fact that they have used
indirect tasks to evaluate each model. This
limitation is addressed here in an experiment
which compares four pattern models using
an unsupervised learning algorithm and a
standard IE scenario. It is found that there
is a wide variation between the models? per-
formance and suggests that one model is the
most useful for IE.
1 Introduction
A common approach to Information Extraction (IE)
is to (manually or automatically) create a set of pat-
terns which match against text to identify informa-
tion of interest. Muslea (1999) reviewed the ap-
proaches which were used at the time and found
that the most common techniques relied on lexico-
syntactic patterns being applied to text which has
undergone relatively shallow linguistic processing.
For example, the extraction rules used by Soderland
(1999) and Riloff (1996) match text in which syn-
tactic chunks have been identified. More recently
researchers have begun to employ deeper syntactic
analysis, such as dependency parsing (Yangarber et
al., 2000; Stevenson and Greenwood, 2005; Sudo et
al., 2001; Sudo et al, 2003; Yangarber, 2003). In
these approaches extraction patterns are essentially
parts of the dependency tree. To perform extraction
they are compared against the dependency analysis
of a sentence to determine whether it contains the
pattern.
Each of these approaches relies on a pattern
model to define which parts of the dependency tree
can be used to form the extraction patterns. A vari-
ety of pattern models have been proposed. For ex-
ample the patterns used by Yangarber et al (2000)
are the subject-verb-object tuples from the depen-
dency tree (the remainder of the dependency parse is
discarded) while Sudo et al (2003) allow any sub-
tree within the dependency parse to act as an ex-
traction pattern. Stevenson and Greenwood (2006)
showed that the choice of pattern model has impor-
tant implications for IE algorithms including signifi-
cant differences between the various models in terms
of their ability to identify information of interest in
text.
However, there has been little comparison be-
tween the various pattern models. Those which have
been carried out have been limited by the fact that
they used indirect tasks to evaluate the various mod-
els and did not compare them in an IE scenario.
We address this limitation here by presenting a di-
rect comparison of four previously described pattern
models using an unsupervised learning method ap-
plied to a commonly used IE scenario.
The remainder of the paper is organised as fol-
lows. The next section presents four pattern models
which have been previously introduced in the litera-
81
ture. Section 3 describes two previous studies which
compared these models and their limitations. Sec-
tion 4 describes an experiment which compares the
four models on an IE task, the results of which are
described in Section 5. Finally, Section 6 discusses
the conclusions which may be drawn from this work.
2 IE Pattern Models
In dependency analysis (Mel?c?uk, 1987) the syntax
of a sentence is represented by a set of directed bi-
nary links between a word (the head) and one of its
modifiers. These links may be labelled to indicate
the relation between the head and modifier (e.g. sub-
ject, object). An example dependency analysis for
the sentence ?Acme hired Smith as their new CEO,
replacing Bloggs.? is shown Figure 1.
Figure 1: An example dependency tree.
The remainder of this section outlines four mod-
els for representing extraction patterns which can be
derived from dependency trees.
Predicate-Argument Model (SVO): A simple
approach, used by Yangarber et al (2000), Yangar-
ber (2003) and Stevenson and Greenwood (2005),
is to use subject-verb-object tuples from the depen-
dency parse as extraction patterns. These consist of
a verb and its subject and/or direct object. Figure
2 shows the two SVO patterns1 which are produced
for the dependency tree shown in Figure 1.
This model can identify information which is ex-
pressed using simple predicate-argument construc-
tions such as the relation between Acme and Smith
1The formalism used for representing dependency patterns
is similar to the one introduced by Sudo et al (2003). Each
node in the tree is represented in the format a[b/c] (e.g.
subj[N/Acme]) where c is the lexical item (Acme), b its
grammatical tag (N) and a the dependency relation between this
node and its parent (subj). The relationship between nodes is
represented as X(A+B+C) which indicates that nodes A, B and
C are direct descendents of node X.
in the dependency tree shown in Figure 1. How-
ever, the SVO model cannot represent information
described using other linguistic constructions such
as nominalisations or prepositional phrases. For ex-
ample the SVO model would not be able to recog-
nise that Smith?s new job title is CEO since these
patterns ignore the part of the dependency tree con-
taining that information.
Chains: A pattern is defined as a path between a
verb node and any other node in the dependency tree
passing through zero or more intermediate nodes
(Sudo et al, 2001). Figure 2 shows examples of the
chains which can be extracted from the tree in Figure
1.
Chains provide a mechanism for encoding infor-
mation beyond the direct arguments of predicates
and includes areas of the dependency tree ignored by
the SVO model. For example, they can represent in-
formation expressed as a nominalisation or within a
prepositional phrase, e.g. ?The resignation of Smith
from the board of Acme ...? However, a potential
shortcoming of this model is that it cannot represent
the link between arguments of a verb. Patterns in the
chain model format are unable to represent even the
simplest of sentences containing a transitive verb,
e.g. ?Smith left Acme?.
Linked Chains: The linked chains model
(Greenwood et al, 2005) represents extraction pat-
terns as a pair of chains which share the same verb
but no direct descendants. Example linked chains
are shown in Figure 2. This pattern representa-
tion encodes most of the information in the sen-
tence with the advantage of being able to link to-
gether event participants which neither of the SVO
or chain model can, for example the relation be-
tween ?Smith? and ?Bloggs? in Figure 1.
Subtrees: The final model to be considered is the
subtree model (Sudo et al, 2003). In this model any
subtree of a dependency tree can be used as an ex-
traction pattern, where a subtree is any set of nodes
in the tree which are connected to one another. Sin-
gle nodes are not considered to be subtrees. The
subtree model is a richer representation than those
discussed so far and can represent any part of a de-
pendency tree. Each of the previous models form a
proper subset of the subtrees. By choosing an appro-
priate subtree it is possible to link together any pair
of nodes in a tree and consequently this model can
82
SVO
[V/hire](subj[N/Acme]+obj[N/Smith])
[V/replace](obj[N/Bloggs])
Chains
[V/hire](subj[N/Acme])
[V/hire](obj[N/Smith])
[V/hire](obj[N/Smith](as[N/CEO]))
[V/hire](obj[N/Smith](as[N/CEO](gen[N/their])))
Linked Chains
[V/hire](subj[N/Acme]+obj[N/Smith])
[V/hire](subj[N/Acme]+obj[N/Smith](as[N/CEO]))
[V/hire](obj[N/Smith]+vpsc mod[V/replace](obj[N/Bloggs]))
Subtrees
[V/hire](subj[N/Acme]+obj[N/Smith]+vpsc mod[V/replace])
[V/hire](subj[N/Acme]+vpsc mod[V/replace](obj[N/Bloggs]))
[N/Smith](as[N/CEO](gen[N/their]+mod[A/new]))
Figure 2: Example patterns for four models
represent the relation between any set of items in the
sentence.
3 Previous Comparisons
There have been few direct comparisons of the var-
ious pattern models. Sudo et al (2003) compared
three models (SVO, chains and subtrees) on two
IE scenarios using a entity extraction task. Mod-
els were evaluated in terms of their ability to iden-
tify entities taking part in events and distinguish
them from those which did not. They found the
SVO model performed poorly in comparison with
the other two models and that the performance of
the subtree model was generally the same as, or
better than, the chain model. However, they did
not attempt to determine whether the models could
identify the relations between these entities, simply
whether they could identify the entities participating
in relevant events.
Stevenson and Greenwood (2006) compared the
four pattern models described in Section 2 in terms
of their complexity and ability to represent rela-
tions found in text. The complexity of each model
was analysed in terms of the number of patterns
which would be generated from a given depen-
dency parse. This is important since several of
the algorithms which have been proposed to make
use of dependency-based IE patterns use iterative
learning (e.g. (Yangarber et al, 2000; Yangarber,
2003; Stevenson and Greenwood, 2005)) and are un-
likely to cope with very large sets of candidate pat-
terns. The number of patterns generated therefore
has an effect on how practical computations using
that model may be. It was found that the number
of patterns generated for the SVO model is a lin-
ear function of the size of the dependency tree. The
number of chains and linked chains is a polynomial
function while the number of subtrees is exponen-
tial.
Stevenson and Greenwood (2006) also analysed
the representational power of each model by measur-
ing how many of the relations found in a standard IE
corpus they are expressive enough to represent. (The
documents used were taken from newswire texts and
biomedical journal articles.) They found that the
SVO and chain model could only represent a small
proportion of the relations in the corpora. The sub-
tree model could represent more of the relations than
any other model but that there was no statistical dif-
ference between those relations and the ones cov-
ered by the linked chain model. They concluded
that the linked chain model was optional since it is
expressive enough to represent the information of
interest without introducing a potentially unwieldy
number of patterns.
There is some agreement between these two stud-
ies, for example that the SVO model performs
poorly in comparison with other models. However,
Stevenson and Greenwood (2006) also found that
the coverage of the chain model was significantly
worse than the subtree model, although Sudo et al
83
(2003) found that in some cases their performance
could not be distinguished. In addition to these dis-
agreements, these studies are also limited by the fact
that they are indirect; they do not evaluate the vari-
ous pattern models on an IE task.
4 Experiments
We compared each of the patterns models described
in Section 2 using an unsupervised IE experiment
similar to one described by Sudo et al (2003).
Let D be a corpus of documents and R a set of
documents which are relevant to a particular extrac-
tion task. In this context ?relevant? means that the
document contains the information we are interested
in identifying. D and R are such that D = R ? R?
and R?R? = ?. As assumption behind this approach
is that useful patterns will be far more likely to occur
in R than D overall.
4.1 Ranking Patterns
Patterns for each model are ranked using a technique
inspired by the tf-idf scoring commonly used in In-
formation Retrieval (Manning and Schu?tze, 1999).
The score for each pattern, p, is given by:
score(p) = tfp ?
(
N
dfp
)?
(1)
where tfp is the number of times pattern p ap-
pears in relevant documents, N is the total number
of documents in the corpus and dfp the number of
documents in the collection containing the pattern
p.
Equation 1 combines two factors: the term fre-
quency (in relevant documents) and inverse docu-
ment frequency (across the corpus). Patterns which
occur frequently in relevant documents without be-
ing too prevalent in the corpus are preferred. Sudo
et al (2003) found that it was important to find the
appropriate balance between these two factors. They
introduced the ? parameter as a way of controlling
the relative contribution of the inverse document fre-
quency. ? is tuned for each extraction task and pat-
tern model combination.
Although simple, this approach has the advantage
that it can be applied to each of the four pattern mod-
els to provide a direct comparison.
4.2 Extraction Scenario
The ranking process was applied to the IE scenario
used for the sixth Message Understanding confer-
ence (MUC-6). The aim of this task was to iden-
tify management succession events from a corpus
of newswire texts. Relevant information describes
an executive entering or leaving a position within a
company, for example ?Last month Smith resigned
as CEO of Rooter Ltd.?. This sentence described as
event involving three items: a person (Smith), po-
sition (CEO) and company (Rooter Ltd). We made
use of a version of the MUC-6 corpus described by
Soderland (1999) which consists of 598 documents.
For these experiments relevant documents were
identified using annotations in the corpus. However,
this is not necessary since Sudo et al (2003) showed
that adequate knowledge about document relevance
could be obtained automatically using an IR system.
4.3 Pattern Generation
The texts used for these experiments were parsed
using the Stanford dependency parser (Klein and
Manning, 2002). The dependency trees were pro-
cessed to replace the names of entities belonging
to specific semantic classes with a general token.
Three of these classes were used for the manage-
ment succession domain (PERSON, ORGANISA-
TION and POST). For example, in the dependency
analysis of ?Smith will became CEO next year?,
?Smith? is replaced by PERSON and ?CEO? by
POST. This process allows more general patterns to
be extracted from the dependency trees. For exam-
ple, [V/become](subj[N/PERSON]+obj[N/POST]).
In the MUC-6 corpus items belonging to the relevant
semantic classes are already identified.
Patterns for each of the four models were ex-
tracted from the processed dependency trees. For
the SVO, chain and linked chain models this was
achieved using depth-first search. However, the
enumeration of all subtrees is less straightforward
and has been shown to be a #P -complete prob-
lem (Goldberg and Jerrum, 2000). We made use of
the rightmost extension algorithm (Abe et al, 2002;
Zaki, 2002) which is an efficient way of enumerating
all subtrees. This approach constructs subtrees iter-
atively by combining together subtrees which have
already been observed. The algorithm starts with a
84
set of trees, each of which consists of a single node.
At each stage the known trees are extended by the
addition of a single node. In order to avoid dupli-
cation the extension is restricted to allowing nodes
only to be added to the nodes on the rightmost path
of the tree. Applying the process recursively creates
a search space in which all subtrees are enumerated
with minimal duplication.
The rightmost extension algorithm is most suited
to finding subtrees which occur multiple times and,
even using this efficient approach, we were unable
to generate subtrees which occurred fewer than four
times in the MUC-6 texts in a reasonable time. Sim-
ilar restrictions have been encountered within other
approaches which have relied on the generation of
a comprehensive set of subtrees from a parse for-
est. For example, Kudo et al (2005) used subtrees
for parse ranking but could only generate subtrees
which appear at least ten times in a 40,000 sentence
corpus. They comment that the size of their data set
meant that it would have been difficult to complete
the experiments with less restrictive parameters. In
addition, Sudo et al (2003) only generated subtrees
which appeared in at least three documents. Kudo
et al (2005) and Sudo et al (2003) both used the
rightmost extension algorithm to generate subtrees.
To provide a direct comparison of the pattern
models we also produced versions of the sets of pat-
terns extracted for the SVO, chain and linked chain
models in which patterns which occurred fewer than
four times were removed. Table 1 shows the num-
ber of patterns generated for each of the four mod-
els when the patterns are both filtered and unfil-
tered. (Although the set of unfiltered subtree pat-
terns were not generated it is possible to determine
the number of patterns which would be generated
using a process described by Stevenson and Green-
wood (2006).)
Model Filtered Unfiltered
SVO 9,189 23,128
Chains 16,563 142,019
Linked chains 23,452 493,463
Subtrees 369,453 1.69 ?1012
Table 1: Number of patterns generated by each
model
It can be seen that the various pattern models gen-
erate vastly different numbers of patterns and that
the number of subtrees is significantly greater than
the other three models. Previous analysis (see Sec-
tion 3) suggested that the number of subtrees which
would be generated from a corpus could be difficult
to process computationally and this is supported by
our findings here.
4.4 Parameter Tuning
The value of ? in equation 1 was set using a sep-
arate corpus from which the patterns were gener-
ated, a methodology suggested by Sudo et al (2003).
To generate this additional text we used the Reuters
Corpus (Rose et al, 2002) which consists of a year?s
worth of newswire output. Each document in the
Reuters corpus has been manually annotated with
topic codes indicating its general subject area(s).
One of these topic codes (C411) refers to man-
agement succession events and was used to identify
documents which are relevant to the MUC6 IE sce-
nario. A corpus consisting of 348 documents anno-
tated with code C411 and 250 documents without
that code, representing irrelevant documents, were
taken from the Reuters corpus to create a corpus
with the same distribution of relevant and irrelevant
documents as found in the MUC-6 corpus. Unlike
the MUC-6 corpus, items belonging to the required
semantic classes are not annotated in the Reuters
Corpus. They were identified automatically using
a named entity identifier.
The patterns generated from the MUC-6 texts
were ranked using formula 1 with a variety of val-
ues of ?. These sets of ranked patterns were then
used to carry out a document filtering task on the
Reuters corpus - the aim of which is to differentiate
documents based on whether or not they contain a
relation of interest. The various values for ? were
compared by computing the area under the curve. It
was found that the optimal value for ? was 2 for all
pattern models and this setting was used for the ex-
periments.
4.5 Evaluation
Evaluation was carried out by comparing the ranked
lists of patterns against the dependency trees for the
MUC-6 texts. When a pattern is found to match
against a tree the items which match any seman-
85
tic classes in the pattern are extracted. These items
are considered to be related and compared against
the gold standard data in the corpus to determine
whether they are in fact related.
The precision of a set of patterns is computed as
the proportion of the relations which were identified
that are listed in the gold standard data. The recall is
the proportion of relations in the gold standard data
which are identified by the set of patterns.
The ranked set of patterns are evaluated incremen-
tally with the precision and recall of the first (highest
ranked) pattern computed. The next pattern is then
added to the relations extracted by both are evalu-
ated. This process continues until all patterns are
exhausted.
5 Results
Figure 3 shows the results when the four filtered pat-
tern models, ranked using equation 1, are compared.
A first observation is that the chain model
performs poorly in comparison to the other
three models. The highest precision achieved by
this model is 19.9% and recall never increases
beyond 9%. In comparison the SVO model in-
cludes patterns with extremely high precision but
the maximum recall achieved by this model is
low. Analysis showed that the first three SVO
patterns had very high precision. These were
[V/succeed](subj[N/PERSON]+obj[N/PERSON]),
[V/be](subj[N/PERSON]+obj[N/POST]) and
[V/become](subj[N/PERSON]+obj[N/POST]),
which have precision of 90.1%, 80.8% and 78.9%
respectively. If these high precision patterns are
removed the maximum precision of the SVO model
is around 32%, which is comparable with the linked
chain and subtree models. This suggests that, while
the SVO model includes very useful extraction
patterns, the format is restrictive and is unable to
represent much of the information in this corpus.
The remaining two pattern models, linked chains
and subtrees, have very similar performance and
each achieves higher recall than the SVO model, al-
beit with lower precision. The maximum recall ob-
tained by the linked chain model is slightly lower
than the subtree model but it does maintain higher
precision at higher recall levels.
The maximum recall achieved by all four models
is very low in this evaluation and part of the reason
for this is the fact that the patterns have been filtered
to allow direct comparison with the subtree model.
Figure 4 shows the results when the unfiltered SVO,
chain and linked chain patterns are used. (Perfor-
mance of the filtered subtrees are also included in
this graph for comparison.)
This result shows that the addition of extra pat-
terns for each model improves recall without effect-
ing the maximum precision achieved. The chain
model also performs badly in this experiment. Pre-
cision of the SVO model is still high (again this is
due to the same three highly accurate patterns) how-
ever the maximum recall achieved by this model is
not particularly increased by the addition of the un-
filtered patterns. The linked chain model benefits
most from the unfiltered patterns. The extra patterns
lead to a maximum recall which is more than dou-
ble any of the other models without overly degrad-
ing precision. The fact that the linked chain model
is able to achieve such a high recall shows that it is
able to represent the relations found in the MUC-6
text, unlike the SVO and chain models. It is likely
that the subtrees model would also produce a set of
patterns with high recall but the number of poten-
tial patterns which are allowable within this model
makes this impractical.
6 Discussion and Conclusions
Some of the results reported for each model in these
experiments are low. Precision levels are generally
below 40% (with the exception of the SVO model
which achieves high precision using a small number
of patterns). One reason for this that the the patterns
were ranked using a simple unsupervised learning
algorithm which allowed direct comparison of four
different pattern models. This approach only made
use of information about the distribution of patterns
in the corpus and it is likely that results could be im-
proved for a particular pattern model by employing
more sophisticated approaches which make use of
additional information, for example the structure of
the patterns.
The results presented here provide insight into the
usefulness of the various pattern models by evaluat-
ing them on an actual IE task. It is found that SVO
patterns are capable of high precision but that the
86
0.0 0.1 0.2 0.3 0.4
Recall
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Pr
ec
isi
on
Subject-Verb-Object
Chains
Linked Chains
Subtrees
Figure 3: Comparisons of filtered pattern models.
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Recall
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Pr
ec
isi
on
Subject-Verb-Object
Chains
Linked Chains
Subtrees
Figure 4: Comparison of unfiltered models.
87
restricted set of possible patterns leads to low re-
call. The chain model was found to perform badly
with low recall and precision regardless of whether
the patterns were filtered. Performance of the linked
chain and subtree models were similar when the pat-
terns were filtered but unfiltered linked chains were
capable of achieving far higher recall than the fil-
tered subtrees.
These experiments suggest that the linked chain
model is a useful one for IE since it is simple enough
for an unfiltered set of patterns to be extracted and
able to represent a wider range of information than
the SVO and chain models.
References
Kenji Abe, Shinji Kawasoe, Tatsuya Asai, Hiroki
Arimura, and Setsuo Arikawa. 2002. Optimised Sub-
structure Discovery for Semi-Structured Data. In Pro-
ceedings of the 6th European Conference on Princi-
ples and Practice of Knowledge in Databases (PKDD-
2002), pages 1?14.
Leslie Ann Goldberg and Mark Jerrum. 2000. Counting
Unlabelled Subtrees of a Tree is #P -Complete. Lon-
don Mathmatical Society Journal of Computation and
Mathematics, 3:117?124.
Mark A. Greenwood, Mark Stevenson, Yikun Guo, Henk
Harkema, and Angus Roberts. 2005. Automati-
cally Acquiring a Linguistically Motivated Genic In-
teraction Extraction System. In Proceedings of the
4th Learning Language in Logic Workshop (LLL05),
Bonn, Germany.
Dan Klein and Christopher D. Manning. 2002. Fast
Exact Inference with a Factored Model for Natural
Language Parsing. In Advances in Neural Informa-
tion Processing Systems 15 (NIPS 2002), Vancouver,
Canada.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based Parse Reranking with Subtree Fea-
tures. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
189?196, Ann Arbour, MI.
Chritopher Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing.
MIT Press, Cambridge, MA.
Igor Mel?c?uk. 1987. Dependency Syntax: Theory and
Practice. SUNY Press, New York.
Ion Muslea. 1999. Extraction Patterns for Information
Extraction: A Survey. In Proceedings of the AAAI-99
workshop on Machine Learning for Information Ex-
traction, Orlando, FL.
Ellen Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Thirteenth National
Conference on Artificial Intelligence (AAAI-96), pages
1044?1049, Portland, OR.
Tony Rose, Mark Stevenson, and Miles Whitehead.
2002. The Reuters Corpus Volume 1 - from Yes-
terday?s News to Tomorrow?s Language Resources.
In Proceedings of the Third International Conference
on Language Resources and Evaluation (LREC-02),
pages 827?832, La Palmas de Gran Canaria.
Stephen Soderland. 1999. Learning Information Extrac-
tion Rules for Semi-structured and Free Text. Machine
Learning, 31(1-3):233?272.
Mark Stevenson and Mark A. Greenwood. 2005. A Se-
mantic Approach to IE Pattern Induction. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 379?386, Ann Ar-
bor, MI.
Mark Stevenson and Mark A. Greenwood. 2006. Com-
paring Information Extraction Pattern Models. In Pro-
ceedings of the Information Extraction Beyond The
Document Workshop (COLING/ACL 2006), pages 12?
19, Sydney, Australia.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2001. Automatic Pattern Acquisition for Japanese
Information Extraction. In Proceedings of the Hu-
man Language Technology Conference (HLT2001),
San Diego, CA.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An Improved Extraction Pattern Representa-
tion Model for Automatic IE Pattern Acquisition. In
Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-03), pages
224?231, Sapporo, Japan.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Unsupervised Discov-
ery of Scenario-level Patterns for Information Extrac-
tion. In Proceedings of the Applied Natural Language
Processing Conference (ANLP 2000), pages 282?289,
Seattle, WA.
Roman Yangarber. 2003. Counter-training in the Dis-
covery of Semantic Patterns. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics (ACL-03), pages 343?350, Sapporo,
Japan.
Mohammed Zaki. 2002. Effectively Mining Frequent
Trees in a Forest. In 8th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, pages 71?80, Edmonton, Canada.
88
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 80?87,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Knowledge Sources for Word Sense
Disambiguation of Biomedical Text
Mark Stevenson, Yikun Guo
and Robert Gaizauskas
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello Street
Sheffield, S1 4DP
United Kingdom
{inital.surname}@dcs.shef.ac.uk
David Martinez
Department of Computer Science
& Software Engineering
University of Melbourne
Victoria 3010
Australia
davidm@csse.unimelb.edu.au
Abstract
Like text in other domains, biomedical doc-
uments contain a range of terms with more
than one possible meaning. These ambigu-
ities form a significant obstacle to the auto-
matic processing of biomedical texts. Previ-
ous approaches to resolving this problem have
made use of a variety of knowledge sources in-
cluding linguistic information (from the con-
text in which the ambiguous term is used) and
domain-specific resources (such as UMLS). In
this paper we compare a range of knowledge
sources which have been previously used and
introduce a novel one: MeSH terms. The best
performance is obtained using linguistic fea-
tures in combination with MeSH terms. Re-
sults from our system outperform published
results for previously reported systems on a
standard test set (the NLM-WSD corpus).
1 Introduction
The number of documents discussing biomedical
science is growing at an ever increasing rate, making
it difficult to keep track of recent developments. Au-
tomated methods for cataloging, searching and nav-
igating these documents would be of great benefit
to researchers working in this area, as well as hav-
ing potential benefits to medicine and other branches
of science. Lexical ambiguity, the linguistic phe-
nomena where a word or phrase has more than
one potential meaning, makes the automatic pro-
cessing of text difficult. For example, ?cold? has
six possible meanings in the Unified Medical Lan-
guage System (UMLS) Metathesaurus (Humphreys
et al, 1998) including ?common cold?, ?cold sen-
sation? and ?Chronic Obstructive Airway Disease
(COLD)?. The NLM Indexing Initiative (Aronson et
al., 2000) attempted to automatically index biomedi-
cal journals with concepts from the UMLS Metathe-
saurus and concluded that lexical ambiguity was the
biggest challenge in the automation of the indexing
process. Weeber et al (2001) analysed MEDLINE
abstracts and found that 11.7% of phrases were am-
biguous relative to the UMLS Metathesaurus.
Word Sense Disambiguation (WSD) is the pro-
cess of resolving lexical ambiguities. Previous re-
searchers have used a variety of approaches for
WSD of biomedical text. Some of them have taken
techniques proven to be effective for WSD of gen-
eral text and applied them to ambiguities in the
biomedical domain, while others have created sys-
tems using domain-specific biomedical resources.
However, there has been no direct comparison of
which knowledge sources are the most useful or
whether combining a variety of knowledge sources,
a strategy which has been shown to be successful for
WSD in the general domain (Stevenson and Wilks,
2001), improves results.
This paper compares the effectiveness of a vari-
ety of knowledge sources for WSD in the biomed-
ical domain. These include features which have
been commonly used for WSD of general text as
well as information derived from domain-specific
resources. One of these features is MeSH terms,
which we find to be particularly effective when com-
bined with generic features.
The next section provides an overview of various
approaches to WSD in the biomedical domain. Sec-
80
tion 3 outlines our approach, paying particular atten-
tion to the range of knowledge sources used by our
system. An evaluation of this system is presented
in Section 4. Section 5 summarises this paper and
provides suggestions for future work.
2 Previous Work
WSD has been actively researched since the 1950s
and is regarded as an important part of the process
of understanding natural language texts.
2.1 The NLM-WSD data set
Research on WSD for general text in the last decade
has been driven by the SemEval evaluation frame-
works1 which provide a set of standard evaluation
materials for a variety of semantic evaluation tasks.
At this point there is no specific collection for the
biomedical domain in SemEval, but a test collection
for WSD in biomedicine was developed by Wee-
ber et al (2001), and has been used as a benchmark
by many independent groups. The UMLS Metathe-
saurus was used to provide a set of possible mean-
ings for terms in biomedical text. 50 ambiguous
terms which occur frequently in MEDLINE were
chosen for inclusion in the test set. 100 instances
of each term were selected from citations added to
the MEDLINE database in 1998 and manually dis-
ambiguated by 11 annotators. Twelve terms were
flagged as ?problematic? due to substantial disagree-
ment between the annotators. There are an average
of 2.64 possible meanings per ambiguous term and
the most ambiguous term, ?cold? has five possible
meanings. In addition to the meanings defined in
UMLS, annotators had the option of assigning a spe-
cial tag (?none?) when none of the UMLS meanings
seemed appropriate.
Various researchers have chosen to evaluate their
systems against subsets of this data set. Liu et al
(2004) excluded the 12 terms identified as problem-
atic by Weeber et al (2001) in addition to 16 for
which the majority (most frequent) sense accounted
for more than 90% of the instances, leaving 22 terms
against which their system was evaluated. Leroy and
Rindflesch (2005) used a set of 15 terms for which
the majority sense accounted for less than 65% of
the instances. Joshi et al (2005) evaluated against
1http://www.senseval.org
the set union of those two sets, providing 28 am-
biguous terms. McInnes et al (2007) used the set
intersection of the two sets (dubbed the ?common
subset?) which contained 9 terms. The terms which
form these various subsets are shown in Figure 1.
The 50 terms which form the NLM-WSD data set
represent a range of challenges for WSD systems.
The Most Frequent Sense (MFS) heuristic has be-
come a standard baseline in WSD (McCarthy et al,
2004) and is simply the accuracy which would be
obtained by assigning the most common meaning of
a term to all of its instances in a corpus. Despite its
simplicity, the MFS heuristic is a hard baseline to
beat, particularly for unsupervised systems, because
it uses hand-tagged data to determine which sense
is the most frequent. Analysis of the NLM-WSD
data set showed that the MFS over all 50 ambigu-
ous terms is 78%. The different subsets have lower
MFS, indicating that the terms they contain are more
difficult to disambiguate. The 22 terms used by (Liu
et al, 2004) have a MFS of 69.9% while the set
used by (Leroy and Rindflesch, 2005) has an MFS
of 55.3%. The union and intersection of these sets
have MFS of 66.9% and 54.9% respectively.
adjustment
blood pressure
evaluation
immunosuppression
radiation
sensitivity
degree
growth
man
mosaic
nutrition
cold
depression
discharge
extraction
fat
implantation
association
condition
culture
determination
energy
failure
fit
fluid
frequency
ganglion
glucose
inhibition 
pressure 
resistance
secretion
single
strains
support
surgery
transient
transport
variation
repair
scale
weight
white
japanese
lead
mole
pathology
reduction
sex
ultrasound
NLM-WSD data set
Liu et. al. (2004)
Leroy and Rindflesch (2005)
Figure 1: The NLM-WSD test set and some of its sub-
sets. Note that the test set used by (Joshi et al, 2005)
comprises the set union of the terms used by (Liu et al,
2004) and (Leroy and Rindflesch, 2005) while the ?com-
mon subset? is formed from their intersection.
2.2 WSD of Biomedical Text
A standard approach to WSD is to make use of
supervised machine learning systems which are
trained on examples of ambiguous words in con-
text along with the correct sense for that usage. The
81
models created are then applied to new examples of
that word to determine the sense being used.
Approaches which are adapted from WSD of gen-
eral text include Liu et al (2004). Their technique
uses a supervised learning algorithm with a vari-
ety of features consisting of a range of collocations
of the ambiguous word and all words in the ab-
stract. They compared a variety of supervised ma-
chine learning algorithms and found that a decision
list worked best. Their best system correctly dis-
ambiguated 78% the occurrences of 22 ambiguous
terms in the NLM-WSD data set (see Section 2.1).
Joshi et al (2005) also use collocations as features
and experimented with five supervised learning al-
gorithms: Support Vector Machines, Naive Bayes,
decision trees, decision lists and boosting. The Sup-
port Vector Machine performed scoring 82.5% on
a set of 28 words (see Section 2.1) and 84.9% on
the 22 terms used by Liu et al (2004). Performance
of the Naive Bayes classifier was comparable to the
Support Vector Machine, while the other algorithms
were hampered by the large number of features.
Examples of approaches which have made use of
knowledge sources specific to the biomedical do-
main include Leroy and Rindflesch (2005), who re-
lied on information from the UMLS Metathesaurus
assigned by MetaMap (Aronson, 2001). Their sys-
tem used information about whether the ambigu-
ous word is the head word of a phrase identified by
MetaMap, the ambiguous word?s part of speech, se-
mantic relations between the ambiguous words and
surrounding words from UMLS as well as semantic
types of the ambiguous word and surrounding word.
Naive Bayes was used as a learning algorithm. This
approach correctly disambiguated 65.6% of word in-
stances from a set of 15 terms (see Section 2.1).
Humphrey et al (2006) presented an unsupervised
system that also used semantic types. They con-
structed semantic type vectors for each word from
a large collection of MEDLINE abstracts. This al-
lowed their method to perform disambiguation at a
coarser level, without the need for labeled training
examples. In most cases the semantic types can be
mapped to the UMLS concepts but not for five of the
terms in the NLM-WSD data set. Humphrey et al
(2006) reported 78.6% accuracy over the remaining
45. However, their approach could not be applied
to all instances of ambiguous terms and, in particu-
lar, is unable to model the ?none? tag. Their system
could only assign senses to an average of 54% of the
instances of each ambiguous term.
McInnes et al (2007) made use of Concept
Unique Identifiers (CUIs) from UMLS which are
also assigned by MetaMap. The information con-
tained in CUIs is more specific than in the semantic
types applied by Leroy and Rindflesch (2005). For
example, there are two CUIs for the term ?culture?
in UMLS: ?C0010453: Anthropological Culture?
and ?C0430400: Laboratory Culture?. The seman-
tic type for the first of these is ?Idea or Concept? and
?Laboratory Procedure? for the second. McInnes et
al. (2007) were interested in exploring whether the
more specific information contained in CUIs was
more effective than UMLS semantic types. Their
best result was reported for a system which repre-
sented each sense by all CUIs which occurred at
least twice in the abstract surrounding the ambigu-
ous word. They used a Naive Bayes classifier as the
learning algorithm. McInnes et al (2007) reported
an accuracy of 74.5% on the set of ambiguous terms
tested by Leroy and Rindflesch (2005) and 80.0% on
the set used by Joshi et al (2005). They concluded
that CUIs are more useful for WSD than UMLS se-
mantic types but that they are not as robust as fea-
tures which are known to work in general English,
such as unigrams and bigrams.
3 Approach
Our approach is to adapt a state-of-the-art WSD sys-
tem to the biomedical domain by augmenting it with
additional domain-specific and domain-independent
knowledge sources. Our basic system (Agirre and
Mart??nez, 2004) participated in the Senseval-3 chal-
lenge (Mihalcea et al, 2004) with a performance
close to the best system for the English and Basque
lexical sample tasks. The system is based on a su-
pervised learning approach. The features used by
Agirre and Mart??nez (2004) are derived from text
around the ambiguous word and are domain inde-
pendent. We refer to these as linguistic features.
This feature set has been adapted for the disam-
biguation of biomedical text by adding further lin-
guistic features and two different types of domain-
specific features: CUIs (as used by (McInnes et al,
2007)) and Medical Subject Heading (MeSH) terms.
82
3.1 Features
Our feature set contains a number of parameters
which were set empirically (e.g. threshold for un-
igram frequency in the linguistic features). In addi-
tion, we use the entire abstract as the context of the
ambiguous term for relevant features rather than just
the sentence containing the term. Effects of varying
these parameters are consistent with previous results
(Liu et al, 2004; Joshi et al, 2005; McInnes et al,
2007) and are not reported in this paper.
Linguistic features: The system uses a wide
range of domain-independent features which are
commonly used for WSD.
? Local collocations: A total of 41 features which
extensively describe the context of the am-
biguous word and fall into two main types:
(1) bigrams and trigrams containing the am-
biguous word constructed from lemmas, word
forms or PoS tags2 and (2) preceding/following
lemma/word-form of the content words (adjec-
tive, adverb, noun and verb) in the same sen-
tence with the target word. For example, con-
sider the sentence below with the target word
adjustment.
?Body surface area adjustments of
initial heparin dosing...?
The features would include the following: left-
content-word-lemma ?area adjustment?, right-
function-word-lemma ?adjustment of ?, left-
POS ?NN NNS?, right-POS ?NNS IN?, left-
content-word-form ?area adjustments?, right-
function-word-form ?adjustment of ?, etc.
? Syntactic Dependencies: These features model
longer-distance dependencies of the ambigu-
ous words than can be represented by the lo-
cal collocations. Five relations are extracted:
object, subject, noun-modifier, preposition and
sibling. These are identified using heuristic pat-
terns and regular expressions applied to PoS tag
sequences around the ambiguous word. In the
above example, ?heparin? is noun-modifier fea-
ture of ?adjustment?.
2A maximum-entropy-based part of speech tagger was used
(Ratnaparkhi, 1996) without the adaptation to the biomedical
domain.
? Salient bigrams: Salient bigrams within the ab-
stract with high log-likelihood scores, as de-
scribed by Pedersen (2001).
? Unigrams: Lemmas of unigrams which appear
more frequently than a predefined threshold in
the entire corpus, excluding those in a list of
stopwords. We empirically set the threshold
to 1. This feature was not used by Agirre and
Mart??nez (2004), but Joshi et al (2005) found
them to be useful for this task.
Concept Unique Identifiers (CUIs): We follow
the approach presented by McInnes et al (2007) to
generate features based on UMLS Concept Unique
Identifiers (CUIs). The MetaMap program (Aron-
son, 2001) identifies all words and terms in a
text which could be mapped onto a UMLS CUI.
MetaMap does not disambiguate the senses of the
concepts, instead it enumerates all the possible com-
binations of the concept names found. For exam-
ple, MetaMap will segment the phrase ?Body sur-
face area adjustments of initial heparin dosing ...?
into two chunks: ?Body surface area adjustments?
and ?of initial heparin dosing?. The first chunk
will be mapped onto four CUIs with the concept
name ?Body Surface Area?: ?C0005902: Diag-
nostic Procedure? and ?C1261466: Organism At-
tribute? and a further pair with the name ?Adjust-
ments?: ?C0456081: Health Care Activity? and
?C0871291: Individual Adjustment?. The final re-
sults from MetaMap for the first chunk will be eight
combinations of those concept names, e.g. first four
by second two concept names. CUIs which occur
more than three times in the abstract containing the
ambiguous word are included as features.
Medical Subject Headings (MeSH): The fi-
nal feature is also specific to the biomedical do-
main. Medical Subject Headings (MeSH) (Nelson
et al, 2002) is a controlled vocabulary for index-
ing biomedical and health-related information and
documents. MeSH terms are manually assigned to
abstracts by human indexers. The latest version of
MeSH contains over 24,000 terms organised into an
11 level hierarchy.
The terms assigned to the abstract in which
each ambiguous word occurs are used as fea-
tures. For example, the abstract containing our
example phrase has been assigned 16 MeSH
83
terms including ?M01.060.116.100: Aged?,
?M01.060.116.100.080: Aged, 80 and over?,
?D27.505.954.502.119: Anticoagulants? and
?G09.188.261.560.150: Blood Coagulation?. To
our knowledge MeSH terms have not been pre-
viously used as a feature for WSD of biomedical
documents.
3.2 Learning Algorithms
We compared three machine leaning algorithms
which have previously been shown to be effective
for WSD tasks.
The Vector Space Model is a memory-based
learning algorithm which was used by (Agirre and
Mart??nez, 2004). Each occurrence of an ambiguous
word is represented as a binary vector in which each
position indicates the occurrence/absence of a fea-
ture. A single centroid vector is generated for each
sense during training. These centroids are compared
with the vectors that represent new examples using
the cosine metric to compute similarity. The sense
assigned to a new example is that of the closest cen-
troid.
The Naive Bayes classifier is based on a proba-
bilistic model which assumes conditional indepen-
dence of features given the target classification. It
calculates the posterior probability that an instance
belongs to a particular class given the prior proba-
bilities of the class and the conditional probability
of each feature given the target class.
Support Vector Machines have been widely
used in classification tasks. SVMs map feature vec-
tors onto a high dimensional space and construct a
classifier by searching for the hyperplane that gives
the greatest separation between the classes.
We used our own implementation of the Vector
Space Model and Weka implementations (Witten
and Frank, 2005) of the other two algorithms.
4 Results
This system was applied to the NLM-WSD data set.
Experiments were carried out using each of the three
types of features (linguistic, CUI and MeSH) both
alone and in combination. Ten-fold cross valida-
tion was used, and the figures we report are averaged
across all ten runs.
Results from this experiment are shown in Table
1 which lists the performance using combinations of
learning algorithm and features. The figure shown
for each configuration represents the percentage of
instances of ambiguous terms which are correctly
disambiguated.
These results show that each of the three types
of knowledge (linguistic, CUIs and MeSH) can be
used to create a classifier which achieves a reason-
able level of disambiguation since performance ex-
ceeds the relevant baseline score. This suggests that
each of the knowledge sources can contribute to the
disambiguation of ambiguous terms in biomedical
text.
The best performance is obtained using a combi-
nation of the linguistic and MeSH features, a pattern
observed across all test sets and machine learning
algorithms. Although the increase in performance
gained from using both the linguistic and MeSH
features compared to only the linguistic features is
modest it is statistically significant, as is the differ-
ence between using both linguistic and MeSH fea-
tures compared with using the MeSH features alone
(Wilcoxon Signed Ranks Test, p < 0.01).
Combining MeSH terms with other features gen-
erally improves performance, suggesting that the
information contained in MeSH terms is distinct
from the other knowledge sources. However, the
inclusion of CUIs as features does not always im-
prove performance and, in several cases, causes it to
fall. This is consistent with McInnes et al (2007)
who concluded that CUIs were a useful informa-
tion source for disambiguation of biomedical text
but that they were not as robust as a linguistic knowl-
edge source (unigrams) which they had used for a
previous system. The most likely reason for this is
that our approach relies on automatically assigned
CUIs, provided by MetaMap, while the MeSH terms
are assigned manually. We do not have access to a
reliable assignment of CUIs to text; if we had WSD
would not be necessary. On the other hand, reli-
ably assigned MeSH terms are readily available in
Medline. The CUIs assigned by MetaMap are noisy
while the MeSH terms are more reliable and prove
to be a more useful knowledge source for WSD.
The Vector Space Model learning algorithm per-
forms significantly better than both Support Vector
Machine and Naive Bayes (Wilcoxon Signed Ranks
Test, p < 0.01). This pattern is observed regardless
84
Features
CUI+ Linguistic Linguistic Linguistic+Data sets Linguistic CUI MeSH
MeSH +MeSH +CUI MeSH+CUI
Vector space model
All words 87.2 85.8 81.9 86.9 87.8 87.3 87.6
Joshi subset 82.3 79.6 76.6 81.4 83.3 82.4 82.6
Leroy subset 77.8 74.4 70.4 75.8 79.0 78.0 77.8
Liu subset 84.3 81.3 78.3 83.4 85.1 84.3 84.5
Common subset 79.6 75.1 70.4 76.9 80.8 79.6 79.2
Naive Bayes
All words 86.2 81.2 85.7 81.1 86.4 81.4 81.5
Joshi subset 80.6 73.4 80.1 73.3 80.9 73.7 73.8
Leroy subset 76.4 66.1 74.6 65.9 76.8 66.3 66.3
Liu subset 81.9 75.4 81.7 75.3 82.2 75.5 75.6
Common subset 76.7 66.1 74.7 65.8 77.2 65.9 65.9
Support Vector Machine
All words 85.6 83.5 85.3 84.5 86.1 85.3 85.6
Joshi subset 79.8 76.4 79.5 78.0 80.6 79.1 79.8
Leroy subset 75.1 69.7 72.6 72.0 76.3 74.2 74.9
Liu subset 81.3 78.2 81.0 80.0 82.0 80.6 81.2
Common subset 75.7 69.8 71.6 73.0 76.8 74.7 75.2
Previous Approaches
MFS Liu et. al. Leroy and Joshi et. McInnes et.
baseline (2004) Rindflesch (2005) al. (2005) al. (2007)
All words 78.0 ? ? ? 85.3
Joshi subset 66.9 ? ? 82.5 80.0
Leroy subset 55.3 ? 65.5 77.4 74.5
Liu subset 69.9 78.0 ? 84.9 82.0
Common subset 54.9 ? 68.8 79.8 75.7
Table 1: Results from WSD system applied to various sections of the NLM-WSD data set using a variety of fea-
tures and machine learning algorithms. Results from baseline and previously published approaches are included for
comparison.
of which set of features are used, and it is consis-
tent of the results in Senseval data from (Agirre and
Mart??nez, 2004).
4.1 Per-Word Analysis
Table 2 shows the results of our best performing sys-
tem (combination of linguistic and MeSH features
using the Vector Space Model learning algorithm).
Comparable results for previous supervised systems
are also reported where available.3 The MFS base-
line for each term is shown in the leftmost column.
The performance of Leroy and Rindflesch?s sys-
3It is not possible to directly compare our results with Liu
et al (2004) or Humphrey et al (2006). The first report only
optimal configuration for each term (combination of feature sets
and learning algorithm) while the second do not assign senses
to all of the instances of each ambiguous term (see Section 2).
tem is always lower than the best result for each
word. The systems reported by Joshi et al (2005)
and McInnes et al (2007) are better than, or the
same as, all other systems for 14 and 12 words re-
spectively. The system reported here achieves re-
sults equal to or better than previously reported sys-
tems for 33 terms.
There are seven terms for which the performance
of our approach is actually lower than the MFS base-
line (shown in italics) in Table 2. (In fact, the base-
line outperforms all systems for four of these terms.)
The performance of our system is within 1% of the
baseline for five of these terms. The remaining pair,
?blood pressure? and ?failure?, are included in the
set of problematic words identified by (Weeber et
al., 2001). Examination of the possible senses show
that they include pairs with similar meanings. For
85
MFS Leroy and Joshi et. McInnes et. Reported
baseline Rindflesch (2005) al. (2005) al. (2007) system
adjustment 62 57 71 70 74
association 100 - - 97 100
blood pressure 54 46 53 46 46
cold 86 - 90 89 88
condition 90 - - 89 89
culture 89 - - 94 95
degree 63 68 89 79 95
depression 85 - 86 81 88
determination 79 - - 81 87
discharge 74 - 95 96 95
energy 99 - - 99 98
evaluation 50 57 69 73 81
extraction 82 - 84 86 85
failure 71 - - 73 67
fat 71 - 84 77 84
fit 82 - - 87 88
fluid 100 - - 99 100
frequency 94 - - 94 94
ganglion 93 - - 94 96
glucose 91 - - 90 91
growth 63 62 71 69 68
immunosuppression 59 61 80 75 80
implantation 81 - 94 92 93
inhibition 98 - - 98 98
japanese 73 - 77 76 75
lead 71 - 89 90 94
man 58 80 89 80 90
mole 83 - 95 87 93
mosaic 52 66 87 75 87
nutrition 45 48 52 49 54
pathology 85 - 85 84 85
pressure 96 - - 93 95
radiation 61 72 82 81 84
reduction 89 - 91 92 89
repair 52 81 87 93 88
resistance 97 - - 96 98
scale 65 84 81 83 88
secretion 99 - - 99 99
sensitivity 49 70 88 92 93
sex 80 - 88 87 87
single 99 - - 98 99
strains 92 - - 92 93
support 90 - - 91 89
surgery 98 - - 94 97
transient 99 - - 98 99
transport 93 - - 93 93
ultrasound 84 - 92 85 90
variation 80 - - 91 95
weight 47 68 83 79 81
white 49 62 79 74 76
Table 2: Per-word performance of best reported systems.
example, the two senses which account for 98% of
the instances of ?blood pressure?, which refer to the
blood pressure within an organism and the result ob-
tained from measuring this quantity, are very closely
related semantically.
5 Conclusion
This paper has compared a variety of knowledge
sources for WSD of ambiguous biomedical terms
and reported results which exceed the performance
of previously published approaches. We found that
accurate results can be achieved using a combina-
tion of linguistic features commonly used for WSD
86
of general text and manually assigned MeSH terms.
While CUIs are a useful source of information for
disambiguation, they do not improve the perfor-
mance of other features when used in combination
with them. Our approach uses manually assigned
MeSH terms while the CUIs are obtained automati-
cally using MetaMap.
The linguistic knowledge sources used in this pa-
per comprise a wide variety of features including
n-grams and syntactic dependencies. We have not
explored the effectiveness of these individually and
this is a topic for further work.
In addition, our approach does not make use of
the fact that MeSH terms are organised into a hierar-
chy. It would be interesting to discover whether this
information could be used to improve WSD perfor-
mance. Others have developed techniques to make
use of hierarchical information in WordNet for WSD
(see Budanitsky and Hirst (2006)) which could be
adapted to MeSH.
References
E. Agirre and D. Mart??nez. 2004. The Basque Coun-
try University system: English and Basque tasks. In
Rada Mihalcea and Phil Edmonds, editors, Senseval-
3: Third International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text, pages 44?
48, Barcelona, Spain, July.
A. Aronson, O. Bodenreider, H. Chang, S. Humphrey,
J. Mork, S. Nelson, T. Rindflesch, and W. Wilbur.
2000. The NLM Indexing Initiative. In Proceedings
of the AMIA Symposium.
A. Aronson. 2001. Effective mapping of biomedical text
to the UMLS Metathesaurus: the MetaMap program.
In Proceedings of the American Medical Informatics
Association (AMIA), pages 17?21.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of semantic distance. Computational
Linguistics, 32(1):13?47.
S. Humphrey, W. Rogers, H. Kilicoglu, D. Demner-
Fushman, and T. Rindflesch. 2006. Word Sense Dis-
ambiguation by selecting the best semantic type based
on Journal Descriptor Indexing: Preliminary experi-
ment. Journal of the American Society for Information
Science and Technology, 57(5):96?113.
L. Humphreys, D. Lindberg, H. Schoolman, and G. Bar-
nett. 1998. The Unified Medical Language System:
An Informatics Research Collaboration. Journal of the
American Medical Informatics Association, 1(5):1?11.
M. Joshi, T. Pedersen, and R. Maclin. 2005. A Compara-
tive Study of Support Vector Machines Applied to the
Word Sense Disambiguation Problem for the Medical
Domain. In Proceedings of the Second Indian Confer-
ence on Artificial Intelligence (IICAI-05), pages 3449?
3468, Pune, India.
G. Leroy and T. Rindflesch. 2005. Effects of Information
and Machine Learning algorithms on Word Sense Dis-
ambiguation with small datasets. International Jour-
nal of Medical Informatics, 74(7-8):573?585.
H. Liu, V. Teller, and C. Friedman. 2004. A Multi-aspect
Comparison Study of Supervised Word Sense Disam-
biguation. Journal of the American Medical Informat-
ics Association, 11(4):320?331.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004.
Finding predominant senses in untagged text. In Pro-
ceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Lingusitics (ACL-2004), pages
280?287, Barcelona, Spain.
B. McInnes, T. Pedersen, and J. Carlis. 2007. Using
UMLS Concept Unique Identifiers (CUIs) for Word
Sense Disambiguation in the Biomedical Domain. In
Proceedings of the Annual Symposium of the Ameri-
can Medical Informatics Association, pages 533?537,
Chicago, IL.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004. The
Senseval-3 English lexical sample task. In Proceed-
ings of Senseval-3: The Third International Workshop
on the Evaluation of Systems for the Semantic Analysis
of Text, Barcelona, Spain.
S. Nelson, T. Powell, and B. Humphreys. 2002. The
Unified Medical Language System (UMLS) Project.
In Allen Kent and Carolyn M. Hall, editors, Ency-
clopedia of Library and Information Science. Marcel
Dekker, Inc.
T. Pedersen. 2001. A Decision Tree of Bigrams is an
Accurate Predictor of Word Sense. In Proceedings
of the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-01), pages 79?86, Pittsburgh, PA., June.
A. Ratnaparkhi. 1996. A Maximum Entropy Model for
Part-of-Speech Tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 133?142.
M. Stevenson and Y. Wilks. 2001. The Interaction of
Knowledge Sources in Word Sense Disambiguation.
Computational Linguistics, 27(3):321?350.
M. Weeber, J. Mork, and A. Aronson. 2001. Developing
a Test Collection for Biomedical Word Sense Disam-
biguation. In Proceedings of AMAI Symposium, pages
746?50, Washington, DC.
I. Witten and E. Frank. 2005. Data Mining: Practical
machine learning tools and techniques. Morgan Kauf-
mann, San Francisco.
87
Proceedings of the Workshop on BioNLP, pages 71?79,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Disambiguation of Biomedical Abbreviations
Mark Stevenson1, Yikun Guo2, Abdulaziz Al Amri3 and Robert Gaizauskas4
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield, S1 4DP
United Kingdom
1,2,4{initial.surname}@dcs.shef.ac.uk, 3abdulazizmail@gmail.com
Abstract
Abbreviations are common in biomedical doc-
uments and many are ambiguous in the sense
that they have several potential expansions.
Identifying the correct expansion is necessary
for language understanding and important for
applications such as document retrieval. Iden-
tifying the correct expansion can be viewed as
a Word Sense Disambiguation (WSD) prob-
lem. A WSD system that uses a variety of
knowledge sources, including two types of in-
formation specific to the biomedical domain,
is also described. This system was tested on a
corpus of ambiguous abbreviations, created by
automatically identifying the correct expan-
sion in Medline abstracts, and found to iden-
tify the correct expansion with up to 99% ac-
curacy.
1 Introduction
Many abbreviations are ambiguous in the sense that
they have more than one possible expansion. For
example, expansions for ?NLP? include ?Neuro-
linguistic Programming? as well as ?Natural Lan-
guage Processing?. Ambiguous abbreviations form
a challenge to language understanding since iden-
tification of the correct expansion is often impor-
tant. The query ?NLP?, for example, returns pages
which refer to ?Neuro-linguistic programming? for
most web search engines, pages which are of lim-
ited value to those interested in Natural Language
Processing. In some cases this problem could be
obviated by altering the query terms, for example
including ?Natural?, ?Language? and ?Processing?.
However, this will not help when the abbreviation?s
expansion does not occur within the document. Fred
and Cheng (1999) point out that this is often the case
in biomedical documents, in this domain ubiquitous
abbreviations (such as DNA and mRNA) often ap-
pear without an expansion.
It has been reported that misinterpretation of ab-
breviations in biomedical documents has lead to
medical practitioners making fatal errors (Fred and
Cheng, 1999). However, identifying the correct ex-
pansion is not a straightforward task since an ab-
breviation may have several possible expansions.
Chang et al (2002) reported that abbreviations in
biomedical journal articles consisting of six charac-
ters or less have an average of 4.61 possible mean-
ings and Pustejovsky et al (2002) mention that the
simple abbreviation ?AC? is associated with at least
10 strings in different biomedical documents includ-
ing ?atrioventricular connection?, ?anterior colpor-
rhaphy procedure?, ?auditory cortex? and ?atypical
carcinoid?.
The problem of identifying the correct expansion
of an ambiguous abbreviation can be viewed as a
Word Sense Disambiguation (WSD) task where the
various expansions are the ?senses? of the abbrevia-
tion. In this paper we approach the problem in this
way by applying a WSD system which has previ-
ously been applied to biomedical text (Stevenson et
al., 2008). The WSD system uses a variety of infor-
mation sources, including those traditionally applied
to the WSD problem in addition to two knowledge
sources that are specific to the biomedical domain.
Evaluation of systems for disambiguating am-
biguous abbreviations has been hindered by the fact
71
that there is no freely available benchmark corpus
against which approaches can be compared. We de-
scribe a process whereby such a corpus can be cre-
ated by automatically mining abstracts from Med-
line. This corpus is being made publicly available
to encourage comparative research in this area. Our
abbreviation disambiguation system was evaluated
against this corpus and found to identify the correct
abbreviation with up to 99% accuracy.
The remainder of this paper is organised as fol-
lows. The next section describes relevant previous
work on disambiguation of abbreviations. Section
3 describes a supervised learning WSD system tai-
lored specifically to the biomedical domain. Section
4 describes the automatic creation of a corpus of am-
biguous abbreviations designed specifically for the
training and evaluation of abbreviation disambigua-
tion systems. Section 5 describes the evaluation of
our system on this corpus. Our conclusions are pre-
sented in Section 6.
2 Previous Work
Gaudan et al (2005) distinguish two types of abbre-
viations: global and local. Global abbreviations are
those found in documents without the expansion ex-
plicitly stated, while local abbreviations are defined
in the same document in which the abbreviation oc-
curs. Our work is concerned with the problem of
disambiguating global abbreviations. Gaudan et al
(2005) point out that global abbreviations are often
ambiguous.
Various researchers have explored the problem
of disambiguating global abbreviations in biomed-
ical documents. Liu et al (2001)(2002) used sev-
eral domain-specific knowledge sources to identify
terms which are semantically related to each possi-
ble expansion but which have only one sense them-
selves. Instances of these terms were identified in
a corpus of biomedical journal abstracts and used
as training data. Their learning algorithm uses a
variety of features including all words in the ab-
stract and collocations of the ambiguous abbrevia-
tion. They report an accuracy of 97% on a small set
of abbreviations. Liu et al (2004) present a fully
supervised approach. They compared a variety of
supervised machine learning algorithms and found
that the best performance over a set of 15 ambigu-
ous abbreviations, 98.6%, was obtained using Naive
Bayes. Gaudan et al (2005) use a Support Vector
Machine trained on a bag-of-words model and re-
port an accuracy of 98.5%. Yu et al (2006) exper-
imented with two supervised learning algorithms:
Naive Bayes and Support Vector Machines. They
extracted a corpus containing examples of 60 ab-
breviations from a set of biomedical journal articles
which was split so that abstracts in which the abbre-
viations were defined were used as training data and
those in which no definition is found as test data.
Abbreviations in the test portion were manually dis-
ambiguated. They report 79% coverage and 80%
precision using a Naive Bayes classifier. Pakho-
mov (2002) applied a maximum entropy model to
identify the meanings of ambiguous abbreviations in
10,000 rheumatology notes with around 89% accu-
racy. Joshi et al (2006) disambiguated abbreviations
in clinical notes using three supervised learning al-
gorithms (Naive Bayes, decision trees and Support
Vector Machines). They used a range of features and
found that the best performance was obtained when
these were combined. Unfortunately direct compari-
son of these methods is made difficult by the fact that
various researchers have evaluated their approaches
on different data sets.
A variety of approaches have also been proposed
for the problem of disambiguating local abbrevia-
tions in biomedical documents. This task is equiv-
alent to identifying the abbreviation?s expansion in
the document. The problem is relatively straight-
forward for abbreviations which are created by se-
lecting the first character from each word in the ex-
pansion, such as ?angiotensin converting enzyme
(ACE)?, but is more difficult when this convention
is not followed, for example ?acetylchlinesterase
(ACE)?, ?antisocial personality (ASP)? and ?cata-
lase (CAT)?. Okazaki et al (2008) recently pro-
posed an approach to this problem based on dis-
criminative alignment that has been shown to per-
form well. However, the most common solutions
are based on heuristic approaches, for example
Adar (2004) and Zhou et al (2006). Pustejovsky
et al (2002) used hand-built regular expressions.
Schwartz and Hearst (2003) describe an approach
which starts by identifying the set of candidate ex-
pansions in the same sentence as an abbreviation.
The most likely one is identified by searching for the
72
shortest candidate which contains all the characters
in the abbreviation in the correct order.
3 Abbreviation Disambiguation System
Our abbreviation disambiguation system is based on
a state-of-the-art WSD system that has been adapted
to the biomedical domain by augmenting it with ad-
ditional knowledge sources. The system on which
our approach is based (Agirre and Mart??nez, 2004)
participated in the Senseval-3 challenge (Mihalcea
et al, 2004) with a performance close to the best
system for the lexical sample tasks in two languages
while the version adapted to the biomedical domain
has achieved the best recorded results (Stevenson et
al., 2008) on a standard test set consisting of am-
biguous terms (Weeber et al, 2001).
This system is based on a supervised learning ap-
proach with features derived from text around the
ambiguous word that are domain independent. We
refer to these as general features. This feature set
has been adapted for the disambiguation of biomed-
ical text by adding further linguistic features and two
different types of domain-specific features: CUIs (as
used by McInnes et al (2007)) and Medical Sub-
ject Heading (MeSH) terms. This set of features is
more diverse than have been explored by previous
approaches to abbreviation disambiguation.
3.1 Features
Our feature set contains a number of parameters
(e.g. thresholds for unigram and CUI frequencies).
These parameters were set to the same values that
were used when the system was applied to gen-
eral biomedical terms (Stevenson et al, 2008) since
these were found to perform well. We also use the
entire abstract as the context of the ambiguous term
for relevant features rather than just the sentence
containing the term. Effects of altering these vari-
ables are consistent with previous results (Liu et al,
2004; Joshi et al, 2005; McInnes et al, 2007) and
are not reported here.
General features: The system uses a wide range
of domain-independent features that are commonly
employed for WSD.
? Local collocations: A total of 41 features which
extensively describe the context of the am-
biguous word and fall into two main types:
(1) bigrams and trigrams containing the am-
biguous word constructed from lemmas, word
forms or PoS tags and (2) preceding/following
lemma/word-form of the content words (adjec-
tive, adverb, noun and verb) in the same sen-
tence as the ambiguous abbreviation. For ex-
ample, consider the sentence below with the
target abreviation BSA.
?Lean BSA was obtained from height
and lean body weight ...?
The features would include the following:
left-content-word-lemma ?lean BSA?, right-
function-word-lemma ?BSA be?, left-POS ?JJ
NNP?, right-POS ?NNP VBD?, left-content-
word-form ?Lean BSA?, right-function-word-
form ?BSA was?, etc.
? Salient bigrams: Salient bigrams within the ab-
stract with high log-likelihood scores, as de-
scribed by Pedersen (2001).
? Unigrams: Lemmas of all content words in the
abstract and words within a ?4-word window
around the target word, excluding those in a list
of stopwords. In addition, the lemmas of any
unigrams appearing at least twice in the entire
corpus and which are found in the abstract are
also included as features.
Concept Unique Identifiers (CUIs): We follow
the approach presented by McInnes et al (2007) to
generate features based on UMLS Concept Unique
Identifiers (CUIs). The MetaMap program (Aron-
son, 2001) identifies all words and terms in a
text which could be mapped onto a UMLS CUI.
MetaMap does not disambiguate the senses of the
concepts, instead it enumerates likely candidate con-
cepts. For example, MetaMap will segment the
phrase ?Lean BSA was obtained from height and
lean body weight ...? into four chunks: ?Lean
BSA?, ?obtained?, ?from height? and ?lean body
weight?. The first chunk will be mapped onto
three CUIs: ?C1261466: BSA (Body surface area)?,
?C1511233: BSA (NCI Board of Scientific Ad-
visors)? and ?C0036774: BSA (Serum Albumin,
Bovine)?. The chunk ?lean body weight? is mapped
onto two concepts: ?C0005910: Body Weight?
73
and ?C1305866: Body Weight (Weighing patient)?1.
CUIs occurring more than twice in an abstract are in-
cluded as features. CUIs have been used for various
disambiguation tasks in the biomedical domain, in-
cluding disambiguation of ambiguous general terms
(McInnes et al, 2007) and gene symbol disambigua-
tion (Xu et al, 2007), but not, to our knowledge, for
abbreviation disambiguation.
Medical Subject Headings (MeSH): The fi-
nal feature is also specific to the biomedical do-
main. Medical Subject Headings (MeSH) (Nelson
et al, 2002) is a controlled vocabulary for index-
ing biomedical and health-related information and
documents. MeSH terms are manually assigned to
abstracts by human indexers. The latest version of
MeSH (2009) contains over 25,000 terms organised
into an 11 level hierarchy.
The MeSH terms assigned to the abstract in which
each ambiguous word occurs are used as features.
For example, the abstract containing our example
phrase has been assigned 16 terms including ?Body
Surface Area?, ?Body Weight?, ?Humans? and ?Or-
gan Size? . MeSH terms have previously been used
for abbreviation disambiguation by Yu et al (2006).
3.2 Learning Algorithms
We compared three machine leaning algorithms
which have previously been shown to be effective
for WSD tasks.
The Vector Space Model (VSM) is a memory-
based learning algorithm which was used by Agirre
and Mart??nez (2004). Each occurrence of an
ambiguous word is represented as a binary vec-
tor in which each position indicates the occur-
rence/absence of a feature. A single centroid vector
is generated for each sense during training. These
centroids are compared with the vectors that repre-
sent new examples using the cosine metric to com-
pute similarity. The sense assigned to a new example
is that of the closest centroid.
The Naive Bayes (NB) classifier is based on a
probabilistic model which assumes conditional in-
dependence of features given the target classifica-
tion. It calculates the posterior probability that an
1The first of these, C0005910, refers to the weight of
a patient as a property of that individual while the second,
C1305866, refers to the process of weighing a patient as part
of a diagnostic procedure.
instance belongs to a particular class given the prior
probabilities of the class and the conditional proba-
bility of each feature given the target class.
Support Vector Machines (SVM) have been
widely used in classification tasks. SVMs map
feature vectors onto a high dimensional space and
construct a classifier by searching for the hyper-
plane that gives the greatest separation between the
classes.
We used our own implementation of the Vector
Space Model and Weka implementations (Witten
and Frank, 2005) of the other two algorithms.
4 Evaluation Corpus
The most common method for generating corpora
to train and test WSD systems is to manually an-
notate instances of ambiguous terms found in text
with the appropriate meaning. However, this process
is both time-consuming and difficult (Artstein and
Poesio, 2008). An alternative to manual tagging is
to find a way of automatically creating sense tagged
corpora. For the translation of ambiguous English
words Ng et al (2003) made use of the fact that the
various senses are often translated differently. For
example when ?bank? is used in the ?financial insti-
tution? sense it is translated to French as ?banque?
and ?bord? when it is used to mean ?edge of river?.
However, a disadvantage of this approach is that it
relies on the existence of parallel text which may
not be available. In the biomedical domain Liu et al
(2001)(2002) created a corpus using unambiguous
related terms (see Section 2) although they found
that it was not always possible to identify suitable
related terms.
4.1 Corpus Creation
Liu et al (2001) also made use of the fact that
when abbreviations are introduced they are often ac-
companied by their expansion, for example ?BSA
(bovine serum albumin)?. This phenomenon was
exploited to automatically generate a corpus of ab-
breviations and associated definitions by replacing
the abbreviation and expansion with the abbrevia-
tion alone. For example, the sentence ?The adsorp-
tion behavior of bovine serum albumin (BSA) on
a Sepharose based hydrophobic interaction support
has been studied.? becomes ?The adsorption behav-
74
?BSA? AND ?body surface area? NOT ?bovine serum albumin?
?BSA? AND ?bovine serum albumin? NOT ?body surface area?
Figure 1: Example queries for abbreviation ?BSA?
ior of BSA on a Sepharose based hydrophobic inter-
action support has been studied.?
We used this approach to create a corpus of sense
tagged abbreviations in biomedical documents using
a set of 21 three letter abbreviations used in previ-
ous research on abbreviation disambiguation (Liu et
al., 2001; Liu et al, 2002; Liu et al, 2004). Pos-
sible expansions for the majority of these abbrevi-
ations were listed in these papers. For the few re-
maining ones possible expansions were taken from
the Medstract database (Pustejovsky et al, 2002).
We searched for instances of these abbreviations in
Medline, a database containing more than 18 mil-
lion abstracts from publications in biomedicine and
the life sciences. For each abbreviation we queried
Medline, using the Entrez interface, to identify doc-
uments containing one of its meanings. For exam-
ple the abbreviation ?BSA? has two possible expan-
sions: ?body surface area? and ?bovine serum alu-
min?. Medline is searched to identify documents
that contain each possible expansion of the abbre-
viation using the queries shown in Figure 1. Each
query matches documents containing the abbrevia-
tion and relevant expansion and no mentions of the
other possible expansion(s).
The retrieved documents are then processed to
remove the expansions of each abbreviation. The
Schwartz and Hearst (2003) algorithm for identi-
fying abbreviations and the relevant expansion (see
Section 2) is then run over each of the retrieved ab-
stracts to identify the correct expansion. The expan-
sion is removed from the document and stored sep-
arately, effectively creating a sense tagged corpus.
For convenience the abstracts are converted into a
format similar to the one used for the NLM-WSD
corpus (Weeber et al, 2001).
The resulting corpus consists of 55,655 docu-
ments. For each abbreviation Table 1 shows the
number of abstracts retrieved from Medline (in the
column labeled ?Abstracts?) and the number of ex-
pansions (?Count? column). The column labelled
?Rare? lists the number of expansions that account
for fewer than 1% of the occurrences of an abbre-
viation and ?Frequent? lists the percentage of occu-
rances represented by the most frequent expansion.
It can be seen that there is a wide variation between
the number of abstracts retrieved for each abbrevi-
ation. CSF occurs in 14,871 abstracts and ASP in
just 71. There is also a wide variation between the
frequency of the most common expansion with over
99% of the occurrences of ?CSF? representing one
expansion (?cerebrospinal fluid?) while for ?ASP?
two of the five possible expansions (?antisocial per-
sonality? and ?aspartate?) each account for almost
34% of the documents. In addition, several abbrevi-
ations have expansions which occur only rarely. For
example, two of the expansions of ?APC? (?atrial
pressure complexes? and ?aphidicholin?) each have
only a single document and account for just 0.03%
of the instances of that abbreviation.
4.2 Corpus Reduction
Given the diversity of the abbreviations which were
downloaded from Medline, both in terms of num-
ber of documents and distribution of senses, sub-
sets of this corpus that are more suitable for WSD
experiments were created. Corpora containing 100,
200 and 300 randomly selected examples of each ab-
breviation were generated and these are referred to
as Corpus.100, Corpus.200 and Corpus.300 respec-
tively.
Some of the 21 abbreviations were not suitable
for inclusion in these corpora. Abbreviations were
not included in the relevant corpus if an insufficient
number of examples were retrieved from Medline.
For example, only 71 abstracts containing ?ASP?
were retrieved and it is is not included in any of the
three corpora. Similarly, ?ANA? and ?FDP? are not
included in Corpus.200 or Corpus.300 and ?DIP?
not included in Corpus.300. In addition, rare senses,
those which represent fewer than 1% of the occur-
rences of an abbreviation in all retrieved abstracts,
were discarded. Finally, two abbreviations (?ACE?
and ?CSF?) have only one sense that is not ?Rare?
75
Expansions
Abstracts Count Rare Frequent
ACE 3105 3 2 98.7
ANA 100 3 0 58.0
APC 3146 5 2 39.4
ASP 71 5 0 33.8
BPD 1841 3 0 46.7
BSA 5373 2 0 86.4
CAT 4636 3 1 55.2
CML 2234 4 2 91.7
CMV 7665 2 0 96.7
CSF 14871 3 2 99.1
DIP 209 2 0 75.1
EMG 2052 2 0 88.4
FDP 130 4 0 78.5
LAM 325 4 1 48.3
MAC 955 5 1 64.3
MCP 815 5 1 50.2
PCA 2442 5 1 68.9
PCP 1642 2 0 57.8
PEG 607 2 0 94.1
PVC 234 2 2 78.2
RSV 3202 2 0 76.7
Average 2650 3.2 0.6 70.8
Table 1: Properties of abbreviations corpus retrieved
from Medline
(see Table 1) and these were also excluded from the
reduced corpora.
Consequently, Corpus.100 contains 18 abbrevia-
tions (?ACE?, ?ASP? and ?CSF? are excluded), Cor-
pus.200 contains 16 (?ANA? and ?FDP? are also
excluded) and Corpus.300 contains 14 (?DIP? and
?PVC? also excluded). Where an abbreviation is in-
cluded in more than one corpus, all the examples in
the smaller corpus are included in the larger one(s).
For example, the 100 examples of ?APC? in Cor-
pus.100 are also included in Corpus.200 and Cor-
pus.300.
5 Experiments
Various combinations of learning algorithms and
features were applied to the three reduced corpora
described in Section 4.2. Performance of the WSD
system is measured in terms of the proportion of ab-
breviation instances for which the correct expansion
is identified. 10-fold cross validation was used for
all experiments and all quoted results refer to the av-
erage performance across the 10 folds. Results are
shown in Table 2. The baseline figures, based on
selecting the most frequent expansion for each ab-
breviation, are shown for each corpus. Note that
these figures vary slightly across the three corpora
because of the different abbreviations each contains
(see Section 4.2).
A first observation is that performance of the
WSD system is consistently better than the base-
line for the relevant corpus and, with a few excep-
tions, above 90%. As might be expected, perfor-
mance improves as additional training examples are
added. However, even when the number of exam-
ples is relatively low, just 100, performance of the
best configuration (VSM learning algorithm with all
three types of feature) is 97.4%.
The best result, 99% (300 training examples,
VSM learning algorithm with all feature types), ex-
ceeds reported performance of previous abbreviation
disambiguation systems (see Section 2). Although
these results are not directly comparable, since these
studies used different evaluation corpora, the set
of ambiguous abbreviations used in this study and
methodology for corpus creation are similar to those
used by Liu et al (2001)(2002)(2004).
The best performance for each learning algorithm
is obtained when all three types of features are com-
bined. The difference between performance ob-
tained using all three feature types and using only
the MeSH or CUI features is statistically significant
(Wilcoxon Signed Ranks test, p < 0.01) although
the difference between this and performance using
just the linguistic features is not.
The VSM learning algorithm generally performs
better than either the SVM or Naive Bayes learning
algorithms. The difference between performance of
VSM and the other algorithms is statistically signif-
icant for Corpus.100 but not for the other two, sug-
gesting that this learning algorithm is better able to
cope with small number of training examples than
Naive Bayes and Support Vector Machines. Strong
performance of the VSM algorithm is consistent
with previous work which has shown that this algo-
rithm performs well on the disambiguation of am-
biguous terms in both biomedical and general text
(Agirre and Mart??nez, 2004; Stevenson et al, 2008).
76
Features
Algorithm Linguistic Linguistic CUI+ Linguistic+Linguistic CUI MeSH +CUI +MeSH MeSH MeSH+CUI
Corpus.100 (Baseline = 69.0%)
SVM 0.934 0.900 0.949 0.947 0.946 0.938 0.954
NB 0.940 0.917 0.949 0.951 0.947 0.944 0.958
VSM 0.968 0.937 0.888 0.970 0.971 0.939 0.974
Corpus.200 (Baseline = 69.1%)
SVM 0.957 0.911 0.964 0.964 0.964 0.947 0.965
NB 0.966 0.926 0.962 0.969 0.971 0.955 0.972
VSM 0.979 0.930 0.894 0.982 0.981 0.947 0.984
Corpus.300 (Baseline = 68.7%)
SVM 0.966 0.914 0.970 0.968 0.974 0.954 0.975
NB 0.971 0.933 0.960 0.971 0.976 0.960 0.978
VSM 0.981 0.938 0.894 0.987 0.985 0.957 0.990
Table 2: Performance of WSD system using various combinations of learning algorithms and features.
Performance of our system on this task is higher
than would be expected for most WSD tasks sug-
gesting that the problem of abbreviation disam-
biguation is simpler than the disambiguation of gen-
eral terms. The most probable reason for this is that
the various expansions of abbreviations in our cor-
pus are more distinct and better defined than senses
for general terms. For example, the three possi-
ble expansions for ?ANA? in our corpus are a pro-
fessional body (?American Nurses Association?), a
type of medical test (?antinuclear?) and a neuro-
transmitter (?Anandamide?). It is likely that these
diverse meanings will tend to occur in very differ-
ent contexts and in documents with different topics.
On the other hand it is widely accepted that distinc-
tions between possible meanings of words in natu-
ral language are often vague (Kilgarriff, 1993). It
is likely that clearer distinctions between possible
expansions of abbreviations make the task of iden-
tifying the correct one more straightforward than
identifying meanings of ambiguous words. In ad-
dition, the creation of annotated data for WSD is of-
ten hampered by the difficulty in obtaining sufficient
agreement between annotators (Artstein and Poesio,
2008; Weeber et al, 2001) and this problem does not
apply to our automatically-generated corpus.
Results in Table 2 indicate that CUIs are use-
ful features in the disambiguation of abbreviations.
This is in contrast with previous experiments on am-
biguous terms in biomedical documents (Stevenson
et al, 2008) in which it was found that the best
performance as obtained using only linguistic and
MeSH features. It is likely that the clear distinction
between expansions of abbreviations is the reason
behind this difference. CUIs are assigned automat-
ically by the MetaMap program (Aronson, 2001).
However, this assignment is very noisy. It is likely
that the various expansions of abbreviations are dis-
tinct enough for this noise to be tolerated by the
learning algorithms while it causes problems when
the meanings are closer together, such as in the case
of ambiguous terms.
5.1 Performance of Individual Abbreviations
Table 3 shows the performance of the best WSD sys-
tem (VSM learning algorithm with all features) for
each abbreviation in the three subsets of our corpus.
Our system performs well for all abbreviations. Ac-
curacy is no lower than 92% for any abbreviation
using Corpus.100 and no lower than 97% for Cor-
pus.300, demonstrating that the approach is robust.
In fact, the approach still performs well for abbre-
viations with low baseline scores, such as ?APC?,
?BPD? and ?LAM?.
It is interesting to note that the abbreviations with
the lowest performance tend to have expansions that
are closely related. For example, the two expansions
of ?EMG? are ?electromyography? and ?electromyo-
77
Corpus
100 200 300
ANA 0.980 - -
APC 0.980 1.000 1.000
BPD 1.000 1.000 1.000
BSA 0.970 0.970 0.982
CAT 0.990 0.990 1.000
CML 0.960 0.963 0.978
CMV 0.970 0.970 0.970
DIP 1.000 1.000 -
EMG 0.920 0.960 0.980
FDP 0.970 - -
LAM 0.960 0.980 0.980
MAC 0.970 0.990 0.989
MCP 0.980 0.978 1.000
PCA 0.960 0.987 0.992
PCP 0.990 1.000 1.000
PEG 0.980 0.982 1.000
PVC 0.990 1.000 -
RSV 0.960 0.972 0.978
Overall 0.974 0.984 0.990
Table 3: Performance of WSD system over individual ab-
breviations in three reduced corpora
gram? while for ?LAM? one expansion (?Lymphan-
gioleiomyomatosis?) is a rare lung disease and the
other (?Lipoarabinomannan?) a molecule associated
with another lung disease (tuberculosis). On the
other hand, abbreviations that are more accurately
disambiguated tend to have expansions with more
distinct meanings. For example, ?BPD? can be an
acronym for ?borderline personality disorder? (a psy-
chiatric diagnosis), ?bronchopulmonary dysplasia?
(a lung disease) or ?biparietal diameter? (diameter of
a foetus? head in an ultrasound) and the expansions
of ?DIP? are ?desquamative interstitial pneumonia?
(a lung disease) and ?distal interphalangeal joints?
(types of joints in the human hand and foot).
6 Conclusions
This paper has presented an approach to the disam-
biguation of ambiguous abbreviations in biomedi-
cal documents. We treat this problem as a form
of WSD and apply a system that combines a wider
range of features than have been previously applied,
including those which are commonly used within
WSD systems in addition to information from two
domain-specific knowledge sources. The approach
is evaluated using a corpus of abbreviations auto-
matically mined from Medline and found to iden-
tify the correct expansion with accuracy of up to
99%. This figure is higher than previously reported
results for abbreviation disambiguation systems, al-
though direct comparison is difficult due to the use
of different data sets. It was also found that best per-
formance could be obtained using a simple machine
learning algorithm and a diverse range of knowledge
sources. Performance of our system is higher than is
normally achieved by WSD systems when applied
to general terms and we suggest that the reason for
this is that the various expansions of abbreviations
are better defined and more distinct than the senses
of ambiguous words.
This study has been limited to the disambiguation
of abbreviations consisting of exactly three letters.
Possibilities for future work include experimenting
with abbreviations of various lengths.
Data
The corpus described in Section 4 has been
made freely available for research and may
be obtained from http://nlp.shef.ac.uk/
BioWSD/downloads/abbreviationdata/.
Acknowledgments
We are grateful to the anonymous reviewers of this
paper for their valuable feedback.
References
E. Adar. 2004. SaRAD: A simple and robust abbrevia-
tion dictionary. Bioinformatics, 20(4):527?533.
E. Agirre and D. Mart??nez. 2004. The Basque Coun-
try University system: English and Basque tasks. In
Rada Mihalcea and Phil Edmonds, editors, Senseval-
3: Third International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text, pages 44?
48, Barcelona, Spain, July.
A. Aronson. 2001. Effective mapping of biomedical text
to the UMLS Metathesaurus: the MetaMap program.
In Proceedings of the American Medical Informatics
Association (AMIA), pages 17?21.
R. Artstein and M. Poesio. 2008. Inter-coder agreement
for computational linguistics. Computational Linguis-
tics, 34(4):555?596.
78
J. Chang, H. Schu?tze, and R. Altman. 2002. Creating an
Online Dictionary of Abbreviations from MEDLINE.
The Journal of the American Medical Informatics As-
sociation, 9(6):612?620.
H. Fred and T. Cheng. 1999. Acronymesis: the explod-
ing misuse of acronyms. Texas Heart Institute Jour-
nal, 30:255?257.
S. Gaudan, H. Kirsch, and D. Rebholz-Schuhmann.
2005. Resolving abbreviations to their senses in Med-
line. Bioinformatics, 21(18):3658?3664.
M. Joshi, T. Pedersen, and R. Maclin. 2005. A Compara-
tive Study of Support Vector Machines Applied to the
Word Sense Disambiguation Problem for the Medical
Domain. In Proceedings of the Second Indian Confer-
ence on Artificial Intelligence (IICAI-05), pages 3449?
3468, Pune, India.
M. Joshi, S. Pakhomov, T. Pedersen, and C. Chute. 2006.
A comparative study of supervised learning as applied
to acronym expansion in clinical reports. In Proceed-
ings of the Annual Symposium of the American Medi-
cal Informatics Association, pages 399?403, Washing-
ton, DC.
A. Kilgarriff. 1993. Dictionary word sense distinctions:
An enquiry into their nature. Computers and the Hu-
manities, 26:356?387.
H. Liu, Y. Lussier, and C. Friedman. 2001. Disam-
biguating ambiguous biomedical terms in biomedical
narrative text: An unsupervised method. Journal of
Biomedical Informatics, 34:249?261.
H. Liu, S. Johnson, and C. Friedman. 2002. Au-
tomatic Resolution of Ambiguous Terms Based on
Machine Learning and Conceptual Relations in the
UMLS. Journal of the American Medical Informatics
Association, 9(6):621?636.
H. Liu, V. Teller, and C. Friedman. 2004. A Multi-aspect
Comparison Study of Supervised Word Sense Disam-
biguation. Journal of the American Medical Informat-
ics Association, 11(4):320?331.
B. McInnes, T. Pedersen, and J. Carlis. 2007. Using
UMLS Concept Unique Identifiers (CUIs) for Word
Sense Disambiguation in the Biomedical Domain. In
Proceedings of the Annual Symposium of the Ameri-
can Medical Informatics Association, pages 533?537,
Chicago, IL.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004. The
Senseval-3 English lexical sample task. In Proceed-
ings of Senseval-3: The Third International Workshop
on the Evaluation of Systems for the Semantic Analysis
of Text, Barcelona, Spain.
S. Nelson, T. Powell, and B. Humphreys. 2002. The
Unified Medical Language System (UMLS) Project.
In Allen Kent and Carolyn M. Hall, editors, Ency-
clopedia of Library and Information Science. Marcel
Dekker, Inc.
H. Ng, B. Wang, and S. Chan. 2003. Exploiting Parallel
Texts for Word Sense Disambiguation: an Empirical
Study. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics (ACL-
03), pages 455?462, Sapporo, Japan.
N. Okazaki, S. Ananiadou, and J. Tsujii. 2008. A dis-
criminative alignment model for abbreviation recogni-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 657?664, Manchester, UK.
S. Pakhomov. 2002. Semi-supervised maximum entropy
based approach to acronym and abbreviation normal-
ization in medical texts. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 160?167, Philadelphia, PA.
T. Pedersen. 2001. A Decision Tree of Bigrams is an
Accurate Predictor of Word Sense. In Proceedings
of the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-01), pages 79?86, Pittsburgh, PA.
J. Pustejovsky, J. Castano, R. Saur, A. Rumshisky,
J. Zhang, and W. Luo. 2002. Medstract: Creating
Large-scale Information Servers for Biomedical Li-
braries. In ACL 2002 Workshop on Natural Language
Processing in the Biomedical Domain.
A. Schwartz and M. Hearst. 2003. A simple algorithm
for identifying abbreviation definitions in biomedical
text. In Proceedings of the Pacific Symposium on Bio-
computing, Kauai.
M. Stevenson, Y. Guo, R. Gaizauskas, and D. Martinez.
2008. Disambiguation of biomedical text using di-
verse sources of information. BMC Bioinformatics,
9(Suppl 11):S7.
M. Weeber, J. Mork, and A. Aronson. 2001. Developing
a Test Collection for Biomedical Word Sense Disam-
biguation. In Proceedings of AMAI Symposium, pages
746?50, Washington, DC.
I. Witten and E. Frank. 2005. Data Mining: Practical
machine learning tools and techniques. Morgan Kauf-
mann, San Francisco.
H. Xu, J. Fan, G. Hripcsak, E. Mendonc?a, Markatou M.,
and Friedman C. 2007. Gene symbol disambigua-
tion using knowledge-based profiles. Bioinformatics,
23(8):1015?22.
H. Yu, W. Kim, V. Hatzivassiloglou, and J. Wilbur. 2006.
A large scale, corpus-based approach for automati-
cally disambigutaing biomedical abbreviations. ACM
Transactions on Information Systems, 24(3):380?404.
W. Zhou, I. Vetle, and N. Smalheiser. 2006. ADAM: an-
other database of abbreviations in MEDLINE. Bioin-
formatics, 22(22):2813?2818.
79
 
			ffProceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 22?27,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Measuring the Similarity between Automatically Generated Topics
Nikolaos Aletras and Mark Stevenson
Department of Computer Science,
University of Sheffield,
Regent Court, 211 Portobello,
Sheffield,
United Kingdom S1 4DP
{n.aletras, m.stevenson}@dcs.shef.ac.uk
Abstract
Previous approaches to the problem of
measuring similarity between automati-
cally generated topics have been based on
comparison of the topics? word probability
distributions. This paper presents alterna-
tive approaches, including ones based on
distributional semantics and knowledge-
based measures, evaluated by compari-
son with human judgements. The best
performing methods provide reliable esti-
mates of topic similarity comparable with
human performance and should be used in
preference to the word probability distri-
bution measures used previously.
1 Introduction
Topic models (Blei et al., 2010) have proved to be
useful for interpreting and organising the contents
of large document collections. It seems intuitively
plausible that some automatically generated topics
will be similar while others are dis-similar. For ex-
ample, a topic about basketball (team game james
season player nba play knicks coach league) is
more similar to a topic about football (world cup
team soccer africa player south game match goal)
than one about the global finance (fed financial
banks federal reserve bank bernanke rule crisis
credit). Methods for automatically determining
the similarity between topics have several poten-
tial applications, such as analysis of corpora to de-
termine topics being discussed (Hall et al., 2008)
or within topic browsers to decide which topics
should be shown together (Chaney and Blei, 2012;
Gretarsson et al., 2012; Hinneburg et al., 2012).
Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) is a popular type of topic model but can-
not capture such correlations unless the seman-
tic similarity between topics is measured. Other
topic models, such as the Correlated Topic Model
(CTM) (Blei and Lafferty, 2006), overcome this
limitation and identify correlations between top-
ics.
Approaches to identifying similar topics for a
range of tasks have been described in the litera-
ture but they have been restricted to using informa-
tion from the word probability distribution to com-
pare topics and have not been directly evaluated.
Word distributions have been compared using a
variety of measures such as KL-divergence (Li and
McCallum, 2006; Wang et al., 2009; Newman et
al., 2009), cosine measure (He et al., 2009; Ram-
age et al., 2009) and the average Log Odds Ratio
(Chaney and Blei, 2012). Kim and Oh (2011) also
applied the cosine measure and KL-Divergence
which were compared with four other measures:
Jaccard?s Coefficient, Kendall?s ? coefficient, Dis-
count Cumulative Gain and Jensen Shannon Di-
vergence (JSD).
This paper compares a wider range of ap-
proaches to measuring topic similarity than pre-
vious work. In addition these measures are eval-
uated directly by comparing them against human
judgements.
2 Measuring Topic Similarity
We compare measures based on word probability
distributions (Section 2.1), distributional semantic
methods (Sections 2.2-2.4), knowledge-based ap-
proaches (Section 2.5) and their combination (Sec-
tion 2.6).
2.1 Topic Word Probability Distribution
We first experimented with measures based on
comparison of the topics? word distributions (see
Section 1), by applying the JSD, KL-divergence
and Cosine approaches and the Log Odds Ratio
(Chaney and Blei, 2012).
22
2.2 Topic Model Semantic Space
The semantic space generated by the topic model
can be used to represent the topics and the topic
words. By definition each topic is a probability
distribution over the words in the training corpus.
For a corpus with D documents and V words, a
topic model learns a relation between words and
topics, T , as a T ?V matrix,W, that indicates the
probability of each word in each topic. W is the
topic model semantic space and each topic word
can be represented as a vector, V
i
, with topics as
features weighted by the probability of the word
in each topic. The similarity between two topics
is computed as the average pairwise cosine sim-
ilarity between their top-10 most probable words
(TS-Cos).
2.3 Reference Corpus Semantic Space
Topic words can also be represented as vectors
in a semantic space constructed from an external
source. We adapt the method proposed by Aletras
and Stevenson (2013) for measuring topic coher-
ence using distributional semantics
1
.
Top-N Features A semantic space is con-
structed considering only the top n most frequent
words in Wikipedia (excluding stop words) as con-
text features. Each topic word is represented as a
vector of n features weighted by computing the
Pointwise Mutual Information (PMI) (Church and
Hanks, 1989) between the topic word and each
context feature, PMI(w
i
, w
j
)
?
. ? is a variable for
assigning more importance to higher PMI values.
In our experiments, we set ? = 3 and found that
the best performance is obtained for n = 5000.
Similarity between two topics is defined as the av-
erage cosine similarity of the topic word vectors
(RCS-Cos-N).
Topic Word Space Alternatively, we consider
only the top-10 topic words from the two topics
as context features to generate topic word vectors.
Then, topic similarity is computed as the pairwise
cosine similarity of the topic word vectors (RCS-
Cos-TWS).
Word Association Topic similarity can also be
computed by applying word association measures
directly. Newman et al. (2010) measure topic
coherence as the average PMI between the topic
words. This approach can be adapted to measure
1
Wikipedia is used as a reference corpus to count word
co-occurrences and frequencies using a context window of
?10 words centred on a topic word.
topic similarity by computing the average pairwise
PMI between the topic words in two topics (PMI).
2.4 Training Corpus Semantic Space
Term-Document Space A matrixX can be cre-
ated using the training corpus. Each term (row)
represents a topic word vector. Element x
ij
in X
is the tf.idf of the term i in document j. Topic
similarity is computed as the pairwise cosine sim-
ilarity of the topic word vectors (TCS-Cos-TD).
Word Co-occurrence in Training Documents
Alternatively, we generate a matrix Z of co-
document frequencies. The matrix Z consists of
V rows and columns representing the V vocab-
ulary words. Element z
ij
is the log of the num-
ber of documents that contains the words i and
j normalised by the document frequency, DF, of
the word j. Mimno et al. (2011) introduced that
metric to measure topic coherence. We adapted
it to estimate topic similarity by aggregating the
co-document frequency of the words between two
topics (Doc-Co-occ).
2.5 Knowledge-based Methods
UKB (Agirre et al., 2009) is used to generate a
probability distribution over WordNet synsets for
each word in the vocabulary V of the topic model
using the Personalized PageRank algorithm. The
similarity between two topic words is calculated
by transforming these distributions into vectors
and computing the cosine metric. The similar-
ity between two topics is computed by measur-
ing pairwise similarity between their top-10 topic
words and selecting the highest score.
Explicit Semantic Analysis (ESA) proposed by
Gabrilovich and Markovitch (2007) transforms
the topic keywords into vectors that consist of
Wikipedia article titles weighted by their relevance
to the keyword. For each topic, the centroid is
computed from the keyword vectors. Similarity
between topics is computed as the cosine similar-
ity of the ESA centroid vectors.
2.6 Feature Combination Using SVR
We also evaluate the performance of a support
vector regression system (SVR) (Vapnik, 1998)
with a linear kernel using a combination of ap-
proaches described above as features
2
. The system
is trained and tested using 10-fold cross validation.
2
With the exception of JSD, features based on the topics?
word probability distributions were not used by SVR since it
was found that including them reduced performance.
23
3 Evaluation
Data We created a data set consisting of pairs of
topics generated by two topic models (LDA and
CTM) over two document collections using differ-
ent numbers of topics. The first consists of 47,229
news articles from New York Times (NYT) in the
GigaWord corpus and the second contains 50,000
articles from ukWAC (Baroni et al., 2009). Each
article is tokenised then stop words and words ap-
pearing fewer than five times in the corpora re-
moved. This results in a total of 57,651 unique to-
kens for the NYT corpus and 72,672 for ukWAC.
LDA Topics are learned by training LDA mod-
els over the two corpora using gensim
3
. The num-
ber of topics is set to T = 50, 100, 200 and hy-
perparameters, ? and ?, are set to
1
T
. Randomly
selecting pairs of topics will result to a data set
in which the majority of pairs would not be simi-
lar. We overcome that problem by assuming that
the JSD between likely relevant pairs will be low
while it will be higher for less relevant pairs of
topics. We selected 800 pairs of topics. 600 pairs
represent topics with similar word distributions (in
the top 6 most relevant topics ranked by JSD). The
remaining 200 pairs were selected randomly.
CTM is trained using the EM algorithm
4
. The
number of topics to learn is set to T =
50, 100, 200 and the rest of the settings are set to
their default values. The topic graph generated by
CTM was used to create all the possible pairs be-
tween topics that are connected. This results in a
total of 70, 468 and 695 pairs in NYT, and a total
of 80, 246 and 258 pairs in ukWAC for the 50, 100
and 200 topics respectively.
Incoherent topics are removed using an ap-
proach based on distributional semantics (Aletras
and Stevenson, 2013). Each topic is represented
using the top 10 words with the highest marginal
probability.
Human Judgements of Topic Similarity were
obtained using an online crowdsourcing platform,
Crowdflower. Annotators were provided with
pairs of topics and were asked to judge how simi-
lar the topics are by providing a rating on a scale of
0 (completely unrelated) to 5 (identical). The av-
erage response for each pair was calculated in or-
der to create the final similarity judgement for use
as a gold-standard. The average Inter-Annotator
3
http://radimrehurek.com/gensim
4
http://www.cs.princeton.edu/
?
blei/
ctm-c/index.html
agreement (IAA) across all pairs for all of the col-
lections is in the range of 0.53-0.68. The data set
together with gold-standard annotations is freely
available
5
.
4 Results
Table 1 shows the correlation (Spearman) between
the topic similarity metrics described in Section 2
and average human judgements for the LDA and
CTM topic pairs. It also shows the performance
of a Word Overlap baseline which measures the
number of terms that two topics have in common
normalised by the total number of topic terms.
The correlations obtained using the topics?
word probability distributions (Section 2.1), i.e.
JSD, KL-divergence and Cos, are comparable with
the baseline for all of the topic collections and
topic models. The metric proposed by Chaney
and Blei (2012) also compares probability distri-
butions and fails to perform well on either data
set. These results suggest that these metrics may
be sensitive to the high dimensionality of the vo-
cabulary. They also assign high similarity to top-
ics that contain ambiguous words, resulting in low
correlations with human judgements.
Performance of the cosine of the word vec-
tor (TS-Cos) in the Topic Model Semantic Space
(Section 2.2) varies implying that the quality of the
latent space generated by LDA and CTM is sensi-
tive to the number of topics.
The similarity metrics that use the reference
corpus (Section 2.3) consistently produce good
correlations for topic pairs generated using both
LDA and CTM. The best overall correlation for a
single feature in most cases is obtained using av-
erage PMI (in a range of 0.43-0.74). The perfor-
mance of the distributional semantic metric using
the Topic Word Space (RCS-Cos-TWS) is com-
parable and slightly lower for the top-N features
(RCS-Cos-N). This indicates that the reference
corpus covers a broader range of semantic subjects
than the latent space produced by the topic model.
When the term-document matrix from the train-
ing corpus is used as a vector space (Section 2.4)
performance is worse than when the reference
corpus is used. In addition, using co-document
frequency derived from the training corpus does
not correlate particularly well with human judge-
ments. These methods are sensitive to the size
of the corpus, which may be too small to gener-
5
http://staffwww.dcs.shef.ac.uk/
people/N.Aletras/resources/topicSim.
tar.gz
24
Spearman?s r
LDA CTM
NYT ukWAC NYT ukWAC
Method 50 100 200 50 100 200 50 100 200 50 100 200
Baseline
Word Overlap 0.32 0.40 0.51 0.22 0.32 0.41 0.56 0.45 0.49 0.35 0.33 0.53
Topic Word Probability Distribution
JSD 0.37 0.44 0.53 0.29 0.30 0.34 0.59 0.43 0.49 0.38 0.34 0.60
KL-Divergence 0.29 0.29 0.41 0.20 0.24 0.33 0.54 0.39 0.56 0.31 0.29 0.47
Cos 0.31 0.37 0.59 0.30 0.30 0.36 0.58 0.45 0.52 0.50 0.40 0.58
Chaney and Blei (2012) 0.16 0.26 0.18 0.29 0.21 0.25 0.29 0.40 0.31 -0.23 0.12 0.61
Topic Model Semantic Space
TS-Cos 0.35 0.41 0.67 0.29 0.35 0.42 0.67 0.51 0.49 0.51 0.42 0.42
Reference Corpus Semantic Space
RCS-Cos-N 0.37 0.46 0.61 0.35 0.32 0.39 0.60 0.47 0.61 0.57 0.42 0.41
RCS-Cos-TWS 0.40 0.54 0.70 0.38 0.43 0.51 0.63 0.59 0.62 0.60 0.55 0.54
PMI 0.43 0.63 0.74 0.43 0.53 0.64 0.68 0.70 0.64 0.58 0.62 0.64
Training Corpus Semantic Space
TCS-Cos-TD 0.36 0.42 0.67 0.29 0.31 0.40 0.64 0.54 0.58 0.49 0.43 0.43
Doc-Co-occ 0.28 0.29 0.45 0.28 0.22 0.30 0.65 0.36 0.57 0.31 0.26 0.34
Knowledge-based
UKB 0.25 0.38 0.56 0.22 0.35 0.41 0.52 0.41 0.40 0.41 0.43 0.42
ESA 0.43 0.58 0.71 0.46 0.55 0.61 0.69 0.67 0.64 0.70 0.62 0.61
Feature Combination
SVR 0.46 0.64 0.75 0.46 0.58 0.66 0.72 0.71 0.62 0.60 0.65 0.66
IAA 0.54 0.58 0.61 0.53 0.56 0.60 0.68 0.68 0.64 0.67 0.63 0.64
Table 1: Results for various approaches to topic similarity. All correlations are significant p < 0.001.
Underlined scores denote best performance of a single feature. Bold denotes best overall performance.
ate reliable estimates of tf.idf or co-document fre-
quency.
ESA, one of the knowledge-based methods
(Section 2.5), performs well and is comparable to
(or in some cases better than) PMI. UKB does
not perform particularly well because the topics
often contain named entities that do not exist in
WordNet. ESA is based on Wikipedia and does
not suffer from this problem. Overall, metrics for
computing topic similarity based on rich semantic
resources (e.g. Wikipedia) are more appropriate
than metrics based on the topic model itself be-
cause of the limited size of the training corpus.
Combining the features using SVR gives the
best overall result for LDA (in the range 0.46-
0.75) and CTM (0.60-0.72). However, the fea-
ture combination performs slightly lower than the
best single feature in two cases when CTM is
used (T=200, NYT and T=50, ukWAC). Analy-
sis of the coefficients produced by the SVR in
each fold demonstrated that including JSD and
the Word Overlap reduce SVR performance. We
repeated the experiments by removing these fea-
tures
6
which resulted in higher correlations (0.64
and 0.65 respectively).
Another interesting observation is that using
LDA the correlations of the various similarity met-
6
These features are useful for the other experiments since
performance drops when they are removed.
rics with human judgements increase with the
number of topics for both corpora. This result
is consistent with the findings of Stevens et al.
(2012) that topic model coherence increases with
the number of topics. Fewer topics makes the task
of identifying similar topics more difficult because
it is likely that they will contain some terms that do
not relate to the topic?s main subject. Correlations
in CTM are more stable for different number of
topics because of the nature of the model, the pairs
have been generated using the topic graph which
by definition contains correlated topics.
5 Conclusions
We explored the task of determining the similar-
ity between pairs of automatically generated top-
ics and described a range of approaches to the
problem. We constructed a data set of pairs of
topics generated by two topic models, LDA and
CTM, together with human judgements of simi-
larity. The data set was used to evaluate a wide
range of approaches. The most interesting finding
is the poor performance of the metrics based on
word probability distributions previously used for
this task. Our results demonstrate that word asso-
ciation measures, such as PMI, and state-of-the-art
textual similarity metrics, such as ESA, are more
appropriate.
25
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics (NAACL-HLT ?09), pages 19?27, Boulder, Col-
orado.
Nikolaos Aletras and Mark Stevenson. 2013. Evaluat-
ing topic coherence using distributional semantics.
In Proceedings of the 10th International Conference
on Computational Semantics (IWCS 2013) ? Long
Papers, pages 13?22, Potsdam, Germany.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide
web: a collection of very large linguistically pro-
cessed web-crawled corpora. Language resources
and evaluation, 43(3):209?226.
David Blei and John Lafferty. 2006. Correlated topic
models. In Y. Weiss, B. Sch?olkopf, and J. Platt,
editors, Advances in Neural Information Processing
Systems 18, pages 147?154. MIT Press, Cambridge,
MA.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
David Blei, Lawrence Carin, and David Dunson. 2010.
Probabilistic topic models. Signal Processing Mag-
azine, IEEE, 27(6):55?65.
Allison June-Barlow Chaney and David M. Blei. 2012.
Visualizing topic models. In Proceedings of the
Sixth International AAAI Conference on Weblogs
and Social Media, Dublin, Ireland.
Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information, and lexicog-
raphy. In Proceedings of the 27th Annual Meeting
of the Association for Computational Linguistics,
pages 76?83, Vancouver, British Columbia, Canada.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of
the International Joint Conference on Artificial In-
telligence (IJCAI ?07), pages 1606?1611.
Brynjar Gretarsson, John O?Donovan, Svetlin Bostand-
jiev, Tobias H?ollerer, Arthur Asuncion, David New-
man, and Padhraic Smyth. 2012. TopicNets: Visual
analysis of large text corpora with topic modeling.
ACM Trans. Intell. Syst. Technol., 3(2):23:1?23:26.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using
topic models. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 363?371, Honolulu, Hawaii.
Qi He, Bi Chen, Jian Pei, Baojun Qiu, Prasenjit Mi-
tra, and Lee Giles. 2009. Detecting topic evolution
in scientific literature: how can citations help? In
Proceedings of the 18th ACM Conference on Infor-
mation and Knowledge Management (CIKM ?09),
pages 957?966, Hong Kong, China.
Alexander Hinneburg, Rico Preiss, and Ren?e Schr?oder.
2012. TopicExplorer: Exploring document collec-
tions with topic models. In Peter A. Flach, Tijl
Bie, and Nello Cristianini, editors, Machine Learn-
ing and Knowledge Discovery in Databases, volume
7524 of Lecture Notes in Computer Science, pages
838?841. Springer Berlin Heidelberg.
Dongwoo Kim and Alice Oh. 2011. Topic chains
for understanding a news corpus. In Computational
Linguistics and Intelligent Text Processing, pages
163?176. Springer.
Wei Li and Andrew McCallum. 2006. Pachinko allo-
cation: Dag-structured mixture models of topic cor-
relations. In Proceedings of the 23rd International
Conference on Machine Learning (ICML ?06), pages
577?584.
David Mimno, Hanna Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
262?272, Edinburgh, Scotland, UK.
David Newman, Arthur Asuncion, Padhraic Smyth,
and Max Welling. 2009. Distributed algorithms for
topic models. J. Mach. Learn. Res., 10:1801?1828.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of
topic coherence. In Human Language Technologies:
The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics (NAACL-HLT ?10), pages 100?108, Los
Angeles, California.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA:
A supervised topic model for credit attribution in
multi-labeled corpora. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP ?09), pages 248?256,
Singapore.
Keith Stevens, Philip Kegelmeyer, David Andrzejew-
ski, and David Buttler. 2012. Exploring topic co-
herence over many models and many topics. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP
?12), pages 952?961, Jeju Island, Korea.
Vladimir N Vapnik. 1998. Statistical learning theory.
Wiley, New York.
26
Xiang Wang, Kai Zhang, Xiaoming Jin, and Dou Shen.
2009. Mining common topics from multiple asyn-
chronous text streams. In Proceedings of the Sec-
ond ACM International Conference on Web Search
and Data Mining (WSDM ?09), pages 192?201,
Barcelona, Spain.
27
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 353?356,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
The Effect of Ambiguity on the Automated Acquisition of WSD Examples
Mark Stevenson and Yikun Guo
Department of Computer Science,
University of Sheffield,
Regent Court, 211 Portobello,
Sheffield, S1 4DP
United Kingdom
m.stevenson@dcs.shef.ac.uk and g.yikun@dcs.shef.ac.uk
Abstract
Several methods for automatically gen-
erating labeled examples that can be
used as training data for WSD systems
have been proposed, including a semi-
supervised approach based on relevance
feedback (Stevenson et al, 2008a). This
approach was shown to generate examples
that improved the performance of a WSD
system for a set of ambiguous terms from
the biomedical domain. However, we find
that this approach does not perform as well
on other data sets. The levels of ambigu-
ity in these data sets are analysed and we
suggest this is the reason for this negative
result.
1 Introduction
Several studies, for example (Mihalcea et al,
2004; Pradhan et al, 2007), have shown that su-
pervised approaches to Word Sense Disambigua-
tion (WSD) outperform unsupervised ones. But
these rely on labeled training data which is diffi-
cult to create and not always available (e.g. (Wee-
ber et al, 2001)). Various techniques for creating
labeled training data automatically have been sug-
gested in the literature. Stevenson et al (2008a)
describe a semi-supervised approach that used rel-
evance feedback (Rocchio, 1971) to analyse ex-
isting labeled examples and use the information
produced to generate further ones. The approach
was tested on the biomedical domain and the addi-
tional examples found to improve performance of
a WSD system. However, biomedical documents
represent a restricted domain. In this paper the
same approach is tested against two data sets that
are not limited to a single domain.
2 Application to a Range of Data Sets
In this paper the relevance feedback approach de-
scribed by Stevenson et al (2008a) is evaluated us-
ing three data sets: the NLM-WSD corpus (Wee-
ber et al, 2001) which Stevenson et al (2008a)
used for their experiments, the Senseval-3 lexical
sample task (Mihalcea et al, 2004) and the coarse-
grained version of the SemEval English lexical
sample task (Pradhan et al, 2007).
2.1 Generating Examples
To generate examples for a particular sense of an
ambiguous term all of the examples where the
term is used in that sense are considered to be
?relevant documents? while the examples in which
any other sense of the term is used are considered
to be ?irrelevant documents?. Relevance feed-
back (Rocchio, 1971) is used to generate a set of
query terms designed to identify relevant docu-
ments, and therefore instances of the sense. The
top five query terms are used to retrieve docu-
ments and these are used as labeled examples of
the sense. Further details of this process are de-
scribed by Stevenson et al (2008a).
This process requires a collection of documents
that can be queried to generate the additional
examples. For the NLM-WSD data set we
used PubMed, a database of biomedical journal
abstracts queried using the Entrez retrieval sys-
tem (http://www.ncbi.nlm.nih.gov/
sites/gquery). The British National Corpus
(BNC) was used for Senseval-3 and SemEval.1
Lucene (http://lucene.apache.org) was
used to index the BNC and retrieve examples.
1We also experimented with the English WaCky corpus
(Baroni et al, 2009) which contains nearly 2 billion words
automatically retrieved from the web. However, results were
not as good as when the BNC was used.
353
2.2 WSD System
We use a WSD system that has been shown to
perform well when evaluated against ambiguities
found in both general text and the biomedical do-
main (Stevenson et al, 2008b). Medical Subject
Headings (MeSH), a controlled vocabulary used
for document indexing, are obtained from PubMed
and used as additional features for the NLM-WSD
data set since they have been shown to improve
performance. The features are combined using
the Vector Space Model, a simple memory-based
learning algorithm.
2.3 Experiment
Experiments were carried out comparing perfor-
mance when the WSD system was trained using
either the examples in the original data set (orig-
inal), the examples generated from these using
the relevance feedback approach (additional) or a
combination of these (combined). The Senseval-
3 and SemEval corpora are split into training and
test portions so the training portion is used as the
original data set and the WSD system evaluated
against the held-back data. As there is no such
recognised standard split for the NLM-WSD cor-
pus, 10-fold cross-validation was used. For each
fold the training portion is used as the original data
set and automatically generated examples created
by examining just that part of the data. Evaluation
is carried out against the fold?s test data and the
average result across the 10 folds reported.
Table 1 shows the results of this experiment.2
Examples generated using the relevance feedback
approach only improve results for one data set, the
NLM-WSD corpus. In this case there is a sig-
nificant improvement (Mann-Whitney, p < 0.01)
when the original and automatically generated ex-
amples are combined. There is no such improve-
ment for the other two data sets: WSD results us-
ing the additional data are noticeably worse than
when the original data is used alone and, although
performance improves when these examples are
combined with the original data, results are still
lower than using the original data. When exam-
ples are combined there is a drop in performance
of 1.2% and 2.9% for SemEval and Senseval-3 re-
2Results reported here for the NLM-WSD corpus are
slightly different from those reported by (Stevenson et al,
2008a). We used an additional feature (MeSH headings),
which improved the baseline performance, and more query
terms which improved the quality of the additional examples
for all three data sets.
spectively.
Corpus Original Additional Combined
NLM-WSD 87.9 87.6 89.2
SemEval 83.7 74.6 82.5
Senseval-3 68.8 56.3 65.9
Table 1: Results of relevance feedback approach
applied to three data sets
These results indicate that the relevance feed-
back approach described by Stevenson et al
(2008a) is not able to generate useful examples for
the Senseval-3 and SemEval data sets, although it
can for the NLM-WSD data set. We hypothesise
that these corpora contain different levels of ambi-
guity which effect suitability of the approach.
3 Analysis of Ambiguities
The three data sets are compared using measures
designed to determine the level of ambiguity they
contain. Section 3.1 reports results using various
widely used measures based on the distribution of
senses. Section 3.2 introduces a measure based
on the semantic similarity between the possible
senses of ambiguous terms.
3.1 Sense Distributions
Three measures for characterising the difficulty of
WSD data sets based on their sense distribution
were used. The first is the widely applied most
frequent sense (MFS) baseline (McCarthy et al,
2004), i.e. the proportion of examples for an am-
biguous term that are labeled with the commonest
sense. The second is number of senses per am-
biguous term. The final measure, the entropy of
the sense distribution, has been shown to be a good
indication of disambiguation difficulty (Kilgarriff
and Rosenzweig, 2000). For two of these mea-
sures (number of senses and entropy) a higher fig-
ure indicates greater ambiguity while for the MFS
measure a lower figure indicates a more difficult
data set.
Table 2 shows the results of computing these
measures averaged across all terms in the cor-
pus. For two measures (number of senses and en-
tropy) the NLM-WSD corpus is least ambiguous,
Senseval-3 the most ambiguous with SemEval be-
tween them. The MFS scores are very similar for
two data sets (NLM-WSD and SemEval), both of
which are much higher than for Senseval-3.
354
These measures suggest that the NLM-WSD
corpus is less ambiguous than the other two and
also that the Senseval-3 corpus is the most am-
biguous of the three.
Corpus MFS Senses Entropy
NLM-WSD 78.0 2.63 0.73
SemEval 78.4 3.60 0.91
Senseval-3 53.8 6.43 1.75
Table 2: Properties of Data Sets using sense distri-
bution measures
3.2 Semantic Similarity
We also developed a measure that takes into ac-
count the similarity in meaning between the possi-
ble senses for an ambiguous term. This measure is
similar to the one used by Passoneau et al (2009)
to analyse levels of inter-annotator agreement in
word sense annotation. Our measure is shown in
equation 1 where Senses is the set of possible
senses for an ambiguous term, |Senses| = n and
(Senses
2
)
is the set of all subsets of Senses contain-
ing two of its members (i.e the set of unordered
pairs). The similarity between a pair of senses,
sim(x, y), can be computed using any lexical sim-
ilarity measure, see Pedersen et al (2004). Essen-
tially this measure computes the mean of the sim-
ilarities between each pair of senses for the term.
sim measure =
?
{x,y}(Senses2 )
sim(x, y)
(n
2
) (1)
One problem with comparing the data sets used
here is that they use a range of sense invento-
ries. Although lexical similarity measures have
been applied to WordNet (Pedersen et al, 2004)
and UMLS (Pedersen et al, 2007), it is not clear
that the scores they produce can be meaningfully
compared. To avoid this problem we mapped the
sense inventories onto a single resource: WordNet
version 3.0.
The mapping was most straightforward for
Senseval-3 which uses WordNet 1.7.1 and could
be automatically mapped onto WordNet 3.0 senses
using publicly available mappings (Daude? et al,
2000). The SemEval data contains a mapping
from the OntoNotes senses to groups of WordNet
2.1 senses. The first sense from this group was
mapped to WordNet 3.0 using the same mappings.
Mapping the NLM-WSD corpus was more
problematic and had to be carried out manually by
comparing sense definitions in UMLS and Word-
Net 3.0. We had expected this process to be diffi-
cult but found clear mappings for the majority of
senses. There were even found cases in which the
sense definitions were identical in both resources.
(The most likely reason for this is that some of
the resources that are included in the UMLS were
used to compile WordNet.) Another, more serious,
problem is related to the annotation scheme used
in the NLM-WSD corpus. If none of the possi-
ble senses in UMLS were judged to be appropri-
ate the annotators could label the sense as ?None?.
We did not map these senses since it would require
examining each instance to determine the most ap-
propriate sense or senses in WordNet and we ex-
pected this to be error prone. In addition, there is
no guarantee that all of the instances of a particular
term labeled with ?None? refer to the same mean-
ing. All of the ?None? senses were removed from
the NLM-WSD data set and any terms where there
were more than ten instances marked as ?None?
were also rejected from the similarity analysis.
This allowed us to compute the similarity score
for just 20 examples (40% of the total) although
we felt that this was a large enough sample to pro-
vide insight into the data set.
The WordNet::Similarity package (Ped-
ersen et al, 2004) was used to compute similar-
ity scores. Results are reported for three of the
measures in this package. (Other measures pro-
duced similar results.) The simple path measure
computes the similarity between a pair of nodes in
WordNet as the reciprocal of the number of edges
in the shortest path between them, the LCh mea-
sure (Leacock et al, 1998) also uses information
about the length of the shortest path between a pair
of nodes and combines this with information about
the maximum depth in WordNet and the JCn mea-
sure (Jaing and Conrath, 1997) makes use of in-
formation theory to assign probabilities to each of
the nodes in the WordNet hierarchy and computes
similarity based on these scores.
Table 3 shows the values of equation 1 for
the three similarity measures with scores averaged
across terms. These results indicate that for all
measures the Senseval-3 data set contains the most
ambiguity and NLM-WSD the least. This analysis
is consistent with the one carried out using mea-
sures based on sense distributions (Section 3.1)
355
MeasureCorpus
Path JCn LCh
NLM-WSD 0.074 0.032 1.027
SemEval 0.136 0.061 1.292
Senseval-3 0.159 0.063 1.500
Table 3: Semantic similarity for each data set us-
ing a variety of measures
and suggest that the senses in the NLM-WSD data
set are more clearly distinguished than the other
two.
4 Conclusion
This paper has explored a semi-supervised ap-
proach to the generation of labeled training data
for WSD that is based on relevance feedback
(Stevenson et al, 2008a). It was tested on three
data sets but was only found to generate examples
that were accurate enough to improve WSD per-
formance for one of these. The data set in which
a performance improvement was observed repre-
sented a limited domain (biomedicine) while the
other two were not restricted in this way. Measures
designed to quantify the level of ambiguity were
applied to these data sets including ones based on
the distribution of senses and another designed to
quantify similarities between senses. These mea-
sures provided evidence that the corpus for which
the relevance feedback approach was successful
contained less ambiguity than the other two and
this suggests that the relevance feedback approach
is most appropriate when the level of ambiguity is
low.
The experiments described in this paper high-
light the importance of the level of ambiguity on
the relevance feedback approach?s ability to gen-
erate useful labeled examples. Since it is semi-
supervised the ambiguity level can be checked us-
ing the measures used in this paper (Section 3)
and the performance of any automatically gener-
ated examples can be compared with the manu-
ally labeled ones (see Section 2.3) before deciding
whether or not they should be applied.
References
M. Baroni, S. Bernardini, A. Ferraresi, and
E. Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed
web-crawled corpora. Language Resources and
Evaluation, 43(3):209?226.
J. Daude?, L. Padro?, and G. Rigau. 2000. Mapping
wordnets using structural information. In Proceed-
ings of ACL ?00, pages 504?511, Hong Kong.
J. Jaing and D. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proceedings of International Conference on Re-
search in Computational Linguistics, Taiwan.
A. Kilgarriff and J. Rosenzweig. 2000. Framework
and results for English SENSEVAL. Computers and
the Humanities, 34(1-2):15?48.
C. Leacock, M. Chodorow, and G. Miller. 1998.
Using corpus statistics and WordNet relations for
sense identification. Computational Linguistics,
24(1):147?165.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Finding predominant word senses in untagged
text. In Proceedings of ACL?04, pages 279?286,
Barcelona, Spain.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004.
The Senseval-3 English lexical sample task. In
Proceedings of Senseval-3, pages 25?28, Barcelona,
Spain.
R. Passoneau, A. Salleb-Aouissi, and N. Ide. 2009.
Making sense of word sense variation. In Proceed-
ings of SEW-2009, pages 2?9, Boulder, Colorado.
T. Pedersen, S. Patwardhan, and Michelizzi. 2004.
Wordnet::similarity - measuring the relatedness of
concepts. In Proceedings of AAAI-04, pages 1024?
1025, San Jose, CA.
T. Pedersen, S. Pakhomov, S. Patwardhan, and
C. Chute. 2007. Measures of semantic similarity
and relateness in the biomedical domain. Journal of
Biomedical Informatics, 40(3):288?299.
S. Pradhan, E. Loper, D. Dligach, and M. Palmer.
2007. SemEval-2007 Task-17: English Lexical
Sample, SRL and All Words. In Proceedings of
SemEval-2007, pages 87?92, Prague, Czech Repub-
lic.
J. Rocchio. 1971. Relevance feedback in Informa-
tion Retrieval. In G. Salton, editor, The SMART
Retrieval System ? Experiments in Automatic Doc-
ument Processing. Prentice Hall, Englewood Cliffs,
NJ.
M. Stevenson, Y. Guo, and R. Gaizauskas. 2008a.
Acquiring Sense Tagged Examples using Relevance
Feedback. In Proceedings of the Coling 2008, pages
809?816, Manchester, UK, August.
M. Stevenson, Y. Guo, R. Gaizauskas, and D. Martinez.
2008b. Disambiguation of biomedical text using di-
verse sources of information. BMC Bioinformatics,
9(Suppl 11):S7.
M. Weeber, J. Mork, and A. Aronson. 2001. Devel-
oping a Test Collection for Biomedical Word Sense
Disambiguation. In Proceedings of AMIA Sympo-
sium, pages 746?50, Washington, DC.
356
Proceedings of NAACL-HLT 2013, pages 158?167,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Representing Topics Using Images
Nikolaos Aletras and Mark Stevenson
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield, S1 4DP, UK
{n.aletras, m.stevenson}@dcs.shef.ac.uk
Abstract
Topics generated automatically, e.g. using
LDA, are now widely used in Computational
Linguistics. Topics are normally represented
as a set of keywords, often the n terms in a
topic with the highest marginal probabilities.
We introduce an alternative approach in which
topics are represented using images. Candi-
date images for each topic are retrieved from
the web by querying a search engine using the
top n terms. The most suitable image is se-
lected from this set using a graph-based al-
gorithm which makes use of textual informa-
tion from the metadata associated with each
image and features extracted from the images
themselves. We show that the proposed ap-
proach significantly outperforms several base-
lines and can provide images that are useful to
represent a topic.
1 Introduction
Topic models are statistical methods for summaris-
ing the content of a document collection using latent
variables known as topics (Hofmann, 1999; Blei et
al., 2003). Within a model, each topic is a multino-
mial distribution over words in the collection while
documents are represented as distributions over top-
ics. Topic modelling is now widely used in Natural
Language Processing (NLP) and has been applied to
a range of tasks including word sense disambigua-
tion (Boyd-Graber et al, 2007), multi-document
summarisation (Haghighi and Vanderwende, 2009),
information retrieval (Wei and Croft, 2006), image
labelling (Feng and Lapata, 2010a) and visualisation
of document collections (Chaney and Blei, 2012).
Topics are often represented by using the n terms
with the highest marginal probabilities in the topic to
generate a set of keywords. For example, wine, bot-
tle, grape, flavour, dry. Interpreting such lists may
not be straightforward, particularly since there may
be no access to the source collection used to train the
model. Therefore, researchers have recently begun
developing automatic methods to generate meaning-
ful and representative labels for topics. These tech-
niques have focussed on the creation of textual la-
bels (Mei et al, 2007; Lau et al, 2010; Lau et al,
2011).
An alternative approach is to represent a topic us-
ing an illustrative image (or set of images). Im-
ages have the advantage that they can be under-
stood quickly and are language independent. This is
particularly important for applications in which the
topics are used to provide an overview of a collec-
tion with many topics being shown simultaneously
(Chaney and Blei, 2012; Gretarsson et al, 2012;
Hinneburg et al, 2012).
This paper explores the problem of selecting im-
ages to illustrate automatically generated topics.
Our approach generates a set of candidate images for
each topic by querying an image search engine with
the top n topic terms. The most suitable image is
selected using a graph-based method that makes use
of both textual and visual information. Textual in-
formation is obtained from the metadata associated
with each image while visual features are extracted
from the images themselves. Our approach is evalu-
ated using a data set created for this study that was
annotated by crowdsourcing. Results of the evalu-
ation show that the proposed method significantly
158
outperforms three baselines.
The contributions of this paper are as follows: (1)
introduces the problem of labelling topics using im-
ages; (2) describes an approach to this problem that
makes use of multimodal information to select im-
ages from a set of candidates; (3) introduces a data
set to evaluate image labelling; and (4) evaluates the
proposed approach using this data set.
2 Related work
In early research on topic modelling, labels were
manually assigned to topics for convenient presen-
tation of research results (Mei and Zhai, 2005; Teh
et al, 2006).
The first attempt at automatically assigning la-
bels to topics is described by Mei et al (2007).
In their approach, a set of candidate labels are ex-
tracted from a reference collection using chunking
and statistically important bigrams. Then, a rele-
vance scoring function is defined which minimises
the Kullback-Leibler divergence between word dis-
tribution in a topic and word distribution in candi-
date labels. Candidate labels are ranked according
to their relevance and the top ranked label chosen to
represent the topic.
Magatti et al (2009) introduced an approach
for labelling topics that relied on two hierarchi-
cal knowledge resources labelled by humans, the
Google Directory and the OpenOffice English The-
saurus. A topics tree is a pre-existing hierarchi-
cal structure of labelled topics. The Automatic La-
belling Of Topics algorithm computes the similarity
between LDA inferred topics and topics in a topics
tree by computing scores using six standard similar-
ity measures. The label for the most similar topic in
the topic tree is assigned to the LDA topic.
Lau et al (2010) proposed selecting the most rep-
resentative word from a topic as its label. A la-
bel is selected by computing the similarity between
each word and all the others in the topic. Sev-
eral sources of information are used to identify the
best label including Pointwise Mutual Information
scores, WordNet hypernymy relations and distribu-
tional similarity. These features are combined in a
reranking model to achieve results above a baseline
(the most probable word in the topic).
In more recent work, Lau et al (2011) proposed
a method for automatically labelling topics by mak-
ing use of Wikipedia article titles as candidate la-
bels. The candidate labels are ranked using infor-
mation from word association measures, lexical fea-
tures and an Information Retrieval technique. Re-
sults showed that this ranking method achieves bet-
ter performance than a previous approach (Mei et al,
2007).
Mao et al (2012) introduced a method for la-
belling hierarchical topics which makes use of sib-
ling and parent-child relations of topics. Candidate
labels are generated using a similar approach to the
one used by Mei et al (2007). Each candidate la-
bel is then assigned a score by creating a distribu-
tion based on the words it contains and measuring
the Jensen-Shannon divergence between this and a
reference corpus.
Hulpus et al (2013) make use of the structured
data in DBpedia1 to label topics. Their approach
maps topic words to DBpedia concepts. The best
concepts are identified by applying graph central-
ity measures which assume that words that co-
occurring in text are likely to refer to concepts that
are close in the DBpedia graph.
Our own work differs from the approaches de-
scribed above since, to our knowledge, it is the first
to propose labelling topics with images rather than
text.
Recent advances in computer vision has lead to
the development of reliable techniques for exploit-
ing information available in images (Datta et al,
2008; Szeliski, 2010) and these have been combined
with NLP (Feng and Lapata, 2010a; Feng and Lap-
ata, 2010b; Agrawal et al, 2011; Bruni et al, 2011).
The closest work to our own is the text illustration
techniques which have been proposed for story pic-
turing (Joshi et al, 2006) and news articles illustra-
tion (Feng and Lapata, 2010b). The input to text il-
lustration models is a textual document and a set of
image candidates. The goal of the models is to as-
sociate the document with the correct image. More-
over, the problem of ranking images returned from
a text query is related to, but different from, the
one explored in our paper. Those approaches used
queries that were much smaller (e.g. between one
and three words) and more focussed than the ones
1http://dbpedia.org
159
we use (Jing and Baluja, 2008). In our work, the in-
put is a topic and the aim is to associate it with an
image, or images, denoting the main thematic sub-
ject.
3 Labelling Topics
In this section we propose an approach to identify-
ing images to illustrate automatically generated top-
ics. It is assumed that there are no candidate images
available so the first step (Section 3.1) is to generate
a set of candidate images. However, when a candi-
date set is available the first step can be skipped.
3.1 Selecting Candidate Images
For the experiments presented here we restrict our-
selves to using images from Wikipedia available un-
der the Creative Commons licence, since this allows
us to make the data available. The top-5 terms from
a topic are used to query Google using its Custom
Search API2. The search is restricted to the English
Wikipedia3 with image search enabled. The top-20
images retrieved for each search are used as candi-
dates for the topic.
3.2 Feature Extraction
Candidate images are represented by two modalities
(textual and visual) and features extracted for each.
3.2.1 Textual Information
Each image?s textual information consists of the
metadata retrieved by the search. The assumption
here is that image?s metadata is indicative of the im-
age?s content and (at least to some extent) related to
the topic. The textual information is formed by con-
catenating the title and the link fields of the search
result. These represent, respectively, the web page
title containing the image and the image file name.
The textual information is preprocessed by tokeniz-
ing and removing stop words.
3.2.2 Visual Information
Visual information is extracted using low-level
image keypoint descriptors, i.e. SIFT features
2https://developers.google.com/
apis-explorer/#s/customsearch/v1
3http://en.wikipedia.org
(Lowe, 1999; Lowe, 2004) sensitive to colour in-
formation. SIFT features denote ?interesting? ar-
eas in an image. Image features are extracted us-
ing dense sampling and described using Opponent
colour SIFT descriptors provided by the colorde-
scriptor4 software. Opponent colour SIFT descrip-
tors have been found to give the best performance
in object scene and face recognition (Sande et al,
2008). The SIFT features are clustered to form a vi-
sual codebook of 1,000 visual words using K-Means
such that each feature is mapped to a visual word.
Each image is represented as a bag-of-visual words
(BOVW).
3.3 Ranking Candidate Images
We rank images in the candidates set using graph-
based algorithms. The graph is created by treating
images as nodes and using similarity scores (textual
or visual) between images to weight the edges.
3.3.1 PageRank
PageRank (Page et al, 1999) is a graph-based al-
gorithm for identifying important nodes in a graph
that was originally developed for assigning impor-
tance to web pages. It has been used for a range
of NLP tasks including word sense disambiguation
(Agirre and Soroa, 2009) and keyword extraction
(Mihalcea and Tarau, 2004).
Let G = (V,E) be a graph with a set of ver-
tices, V , denoting image candidates and a set of
edges, E, denoting similarity scores between two
images. For example, sim(Vi, Vj) indicates the sim-
ilarity between images Vi and Vj . The PageRank
score (Pr) over G for an image (Vi) can be com-
puted by the following equation:
Pr(Vi) = d ?
?
Vj?C(Vi)
sim(Vi, Vj)
?
Vk?C(Vj)
sim(Vj , Vk)
Pr(Vj) + (1 ? d)v
(1)
where C(Vi) denotes the set of vertices which are
connected to the vertex Vi. d is the damping factor
which is set to the default value of d = 0.85 (Page et
al., 1999). In standard PageRank all elements of the
vector v are the same, 1N where N is the number of
nodes in the graph.
4http://koen.me/research/
colordescriptors
160
3.3.2 Personalised PageRank
Personalised PageRank (PPR) (Haveliwala et al,
2003) is a variant of the PageRank algorithm in
which extra importance is assigned to certain ver-
tices in the graph. This is achieved by adjusting the
values of the vector v in equation 1 to prefer certain
nodes. Nodes that are assigned high values in v are
more likely to also be assigned a high PPR score.
We make use of PPR to prefer images with textual
information that is similar to the terms in the topic.
3.3.3 Weighting Graph Edges
Three approaches were compared for computing
the values of sim(Vi, Vj) in equation 1 used to
weight the edges of the graph. Two of these make
use of the textual information associated with each
image while the final one relies on visual features.
The first approach is Pointwise Mutual Infor-
mation (PMI). The similarity between a pair of
images (vertices in the graph) is computed as the
average PMI between the terms in their metadata.
PMI is computed using word co-occurrence counts
over Wikipedia identified using a sliding window of
length 20. We also experimented with other word
association measures but these did not perform as
well. The PageRank over the graph weighted using
PMI is denoted as PRPMI.
The second approach, Explicit Semantic Anal-
ysis (ESA) (Gabrilovich and Markovitch, 2007), is
a knowledge-based similarity measure. ESA trans-
forms the text from the image metadata into vectors
that consist of Wikipedia article titles weighted by
their relevance. The similarity score between these
vectors is computed as the cosine of the angle be-
tween them. This similarity measure is used to cre-
ate the graph and its PageRank is denoted as PRESA.
The final approach uses the visual features ex-
tracted from the images themselves. The visual
words extracted from the images are used to form
feature vectors and the similarity between a pair of
images computed as the cosine of the angle between
them. The PageRank of the graph created using this
approach is PRvis and it is similar to the approach
proposed by Jing and Baluja (2008) for associating
images to text queries.
3.3.4 Initialising the Personalisation Vector
The personalisation vector (see above) is
weighted using the similarity scores computed be-
tween the topic and its image candidates. Similarity
is computed using PMI and ESA (see above). When
PMI and ESA are used to weight the personalisation
vector they compute the similarity between the
top 10 terms for a topic and the textual metadata
associated with each image in the set of candidates.
We refer to the personalisation vectors created
using PMI and ESA as Per(PMI) and Per(ESA)
respectively.
Using PPR allows information about the simi-
larity between the images? metadata and the topics
themselves to be considered when identifying a suit-
able image label. The situation is different when
PageRank is used since this only considers the sim-
ilarity between the images in the candidate set.
The personalisation vector used by PPR is em-
ployed in combination with a graph created us-
ing one of the approaches described above. For
example, the graph may be weighted using vi-
sual features and the personalisation vector created
using PMI scores. This approach is denoted as
PRvis+Per(PMI).
4 Evaluation
This section discusses the experimental design for
evaluating the proposed approaches to labelling top-
ics with images. To our knowledge no data set for
evaluating these approaches is currently available
and consequently we developed one for this study5.
Human judgements about the suitability of images
are obtained through crowdsourcing.
4.1 Data
We created a data set of topics from two collections
which cover a broad thematic range:
? NYT 47,229 New York Times news articles
(included in the GigaWord corpus) that were
published between May and December 2010.
? WIKI A set of Wikipedia categories randomly
selected by browsing its hierarchy in a breadth-
first-search manner starting from a few seed
5Data set can be downloaded from http://staffwww.
dcs.shef.ac.uk/people/N.Aletras/resources.
html.
161
police, officer, crime, street, man, city, gang, suspect, arrested, violence
game, season, team, patriot, bowl, nfl, quarterback, week, play, jet
military, afghanistan, force, official, afghan, defense, pentagon, american, war, gates
Figure 1: A sample of topics and their top-3 image candidates (i.e. with the highest average human annota-
tions).
categories (e.g. SPORTS, POLITICS, COMPUT-
ING). Categories that have more that 80 articles
associated with them are considered. These
articles are collected to produce a corpus of
approximately 60,000 articles generated from
1,461 categories.
Documents in the two collections are tokenised
and stop words removed. LDA was applied to learn
200 topics from NYT and 400 topics from WIKI.
The gensim package6 was used to implement and
compute LDA. The hyperparameters (?, ?) were set
to 1num of topics . Incoherent topics are filtered out
by applying the method proposed by Aletras and
Stevenson (2013).
We randomly selected 100 topics from NYT and
200 topics from WIKI resulting in a data set of 300
topics. Candidate images for these topics were gen-
erated using the approach described in Section 3.1,
producing a total of 6,000 candidate images (20 for
6http://pypi.python.org/pypi/gensim
each topic).
4.2 Human Judgements of Image Relevance
Human judgements of the suitability of each im-
age were obtained using an online crowdsourcing
platform, Crowdflower7. Annotators were provided
with a topic (represented as a set of 10 keywords)
and a candidate image. They were asked to judge
how appropriate the image was as a representation
of the main subject of the topic and provide a rating
on a scale of 0 (completely unsuitable) to 3 (very
suitable).
Quality control is important in crowdscourcing
experiments to ensure reliability (Kazai, 2011). To
avoid random answers, control questions with obvi-
ous answer were included in the survey. Annotations
by participants that failed to answer these questions
correctly or participants that gave the same rating for
all pairs were removed.
7http://crowdflower.com
162
The total number of filtered responses obtained
was 62, 221 from 273 participants. Each topic-
image pair was rated at least by 10 subjects. The
average response for each pair was calculated in or-
der to create the final similarity judgement for use as
a gold-standard. The average variance across judges
(excluding control questions) is 0.88.
Inter-Annotator agreement (IAA) is computed as
the average Spearman?s ? between the ratings given
by an annotator and the average ratings given by all
other annotators. The average IAA across all topics
was 0.50 which indicates the difficulty of the task,
even for humans.
Figure 1 shows three example topics from the data
set together with the images that received the highest
average score from the annotators.
4.3 Evaluation Metrics
Evaluation of the topic labelling methods is carried
out using a similar approach to the framework pro-
posed by Lau et al (2011) for labelling topics using
textual labels.
Top-1 average rating is the average human rating
assigned to the top-ranked label proposed by the sys-
tem. This provides an indication of the overall qual-
ity of the image the system judges as the best one.
The highest possible score averaged across all top-
ics is 2.68, since for many topics the average score
obtained from the human judgements is lower than
3.
The second evaluation measure is the normalized
discounted cumulative gain (nDCG) (Ja?rvelin and
Keka?la?inen, 2002; Croft et al, 2009) which com-
pares the label ranking proposed by the system to
the optimal ranking provided by humans. The dis-
counted cumulative gain at position p (DCGp) is
computed using the following equation:
DCGp = rel1 +
p?
i=2
reli
log2(i)
(2)
where reli is the relevance of the label to the topic
in position i. Then nDCG is computed as:
nDCGp =
DCGp
IDCGp
(3)
where IDCGp is the optimal ranking of the image
labels, in our experiments this is the ranking pro-
vided by the scores in the human annotated data set.
We follow Lau et al (2011) in computing nDCG-1,
nDCG-3 and nDCG-5 for the top 1, 3 and 5 ranked
system image labels respectively.
4.4 Baselines
Since there are no previous methods for labelling
topics using images, we compare our proposed mod-
els against three baselines.
The Random baseline randomly selects a label
for the topic from the 20 image candidates. The pro-
cess is repeated 10,000 times and the average score
of the selected labels is computed for each topic.
The more informed Word Overlap baseline se-
lects the image that is most similar to the topic terms
by applying a Lesk-style algorithm (Lesk, 1986) to
compare metadata for each image against the topic
terms. It is defined as the number of common terms
between a topic and image candidate normalised by
the total number of terms in the topic and image?s
metadata.
We also compared our approach with the ranking
returned by the Google Image Search for the top-20
images for a specific topic.
4.5 User Study
A user study was conducted to estimate human per-
formance on the image selection task. Three annota-
tors were recruited and asked to select the best image
for each of the 300 topics in the data set. The anno-
tators were provided with the topic (in the form of a
set of keywords) and shown all candidate images for
that topic before being asked to select exactly one.
The Average Top-1 Rating was computed for each
annotator and the mean of these values was 2.24.
5 Results
Table 1 presents the results obtained for each of the
methods on the collection of 300 topics. Results are
shown for both Top-1 Average rating and nDCG.
We begin by discussing the results obtained us-
ing the standard PageRank algorithm applied to
graphs weighted using PMI, ESA and visual features
(PRPMI, PRESA and PRvis respectively). Results us-
ing PMI consistently outperform all baselines and
those obtained using ESA. This suggests that distri-
butional word association measures are more suit-
able for identifying useful images than knowledge-
based similarity measures. The best results using
163
Model Top-1 Av. Rating nDCG-1 nDCG-3 nDCG-5
Baselines
Random 1.79 - - -
Word Overlap 1.85 0.69 0.72 0.74
Google Image Search 1.89 0.73 0.75 0.77
PageRank
PRPMI 1.87 0.70 0.73 0.75
PRESA 1.81 0.67 0.68 0.70
PRvis 1.96 0.73 0.75 0.76
Personalised PageRank
PRPMI+Per(PMI) 1.98 0.74 0.76 0.77
PRPMI+Per(ESA) 1.92 0.70 0.72 0.74
PRESA+Per(PMI) 1.91 0.70 0.72 0.73
PRESA+Per(ESA) 1.88 0.69 0.72 0.74
PRvis+Per(PMI) 2.00 0.74 0.75 0.76
PRvis+Per(ESA) 1.94 0.72 0.75 0.76
User Study 2.24 ? ? ?
Table 1: Results for various approaches to topic labelling.
standard PageRank are obtained when the visual
similarity measures are used to weight the graph,
with performance that significantly outperforms the
word overlap baseline (paired t-test, p < 0.05). This
demonstrates that visual features are a useful source
of information for deciding which images are suit-
able topic labels.
The Personalised version of PageRank produces
consistently higher results compared to standard
PageRank, demonstrating that the additional infor-
mation provided by comparing the image metadata
with the topics is useful for this task. The best
results are obtained when the personalisation vec-
tor is weighted using PMI (i.e. Per(PMI)). The
best overall result for the top-1 average rating (2.00)
is obtained when the graph is weighted using vi-
sual features and the personalisation vector using the
PMI scores (PRvis+Per(PMI)) while the best results
for the various DCG metrics are produced when
both the graph and the personalisation vector are
weighted using PMI scores (PRPMI+Per(PMI)). In
addition, these two methods, PRvis+Per(PMI) and
PRPMI+Per(PMI), perform significantly better than
the word overlap and the Google Image Search base-
lines (p < 0.01 and p < 0.05 respectively). Weight-
ing the personalisation vector using ESA consis-
tently produces lower performance compared to
PMI. These results indicate that graph-based meth-
ods for ranking images are useful for illustrating top-
ics.
6 Discussion
Figure 2 shows a sample of three topics together
with the top-3 candidates (left-to-right) selected by
applying the PRvis+Per(PMI) approach. Reasonable
labels have been selected for the first two topics. On
the other hand, the images selected for the third topic
do not seem to be as appropriate.
We observed that inappropriate labels can be gen-
erated for two reasons. Firstly, the topic may be ab-
stract and difficult to illustrate. For example, one of
the topics in our data set refers to the subject AL-
GEBRAIC NUMBER THEORY and contains the terms
number, ideal, group, field, theory, algebraic, class,
ring, prime, theorem. It is difficult to find a represen-
tative image for topics such as this one. Secondly,
there are topics for which none of the candidate im-
ages returned by the search engine is relevant. An
example of a topic like this in our data set is one
that refers to PLANTS and contains the terms family,
sources, plants, familia, order, plant, species, taxon-
omy, classification, genera. The images returned by
the search engine include pictures of the Sagrada Fa-
milia cathedral in Barcelona, a car called ?Familia?
164
dance, ballet, dancer, swan, company, dancing, nutcracker, balanchine, ballerina, choreographer
2.3 2.7 2.5 2.8 2.8 2.73
wine, bottle, grape, flavor, dry, vineyard, curtis, winery, sweet, champagne
2.1 2.6 2.7 2.83 2.8 2.8
haiti, haitian, earthquake, paterson, jean, prince, governor, au, cholera, country
1.0 1.2 0.2 1.91 1.7 1.6
Figure 2: A sample of topics and their top-3 images selected by applying the the PRvis+Per(PMI) approach
(left side) and the ones with the highest average human annotations (right side). The number under each
image represents its average human annotations score.
and pictures of families but no pictures of plants.
7 Conclusions
This paper explores the use of images to represent
automatically generated topics. An approach to se-
lecting appropriate images was described. This be-
gins by identifying a set of candidate images us-
ing a search engine and then attempts to select the
most suitable. Images are ranked using a graph-
based method that makes use of both textual and
visual information. Evaluation is carried out on a
data set created for this study. The results show that
the visual features are a useful source of information
for this task while the proposed graph-based method
significantly outperforms several baselines.
This paper demonstrates that it is possible to iden-
tify images to illustrate topics. A possible applica-
tion for this technique is to represent the contents
of large document collections in a way that supports
rapid interpretation and can be used to enable nav-
igation (Chaney and Blei, 2012; Gretarsson et al,
2012; Hinneburg et al, 2012). We plan to explore
this possibility in future work. Other possible exten-
sions to this work include exploring alternative ap-
proaches to generating candidate images and devel-
oping techniques to automatically identify abstract
topics for which suitable images are unlikely to be
found, thereby avoiding the problem cases described
in Section 6.
Acknowledgments
The research leading to these results was
carried out as part of the PATHS project
(http://paths-project.eu) funded by
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 270082.
165
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for word sense disambiguation. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL ?09), pages 33?41, Athens, Greece.
Rakesh Agrawal, Sreenivas Gollapudi, Anitha Kannan,
and Krishnaram Kenthapadi. 2011. Enriching text-
books with images. In Proceedings of the 20th ACM
International Conference on Information and Knowl-
edge Management (CIKM ?11), pages 1847?1856,
Glasgow, Scotland, UK.
Nikolaos Aletras and Mark Stevenson. 2013. Evaluat-
ing topic coherence using distributional semantics. In
Proceedings of the 10th International Conference on
Computational Semantics (IWCS ?13) ? Long Papers,
pages 13?22, Potsdam, Germany.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jordan Boyd-Graber, David Blei, and Xiaojin Zhu. 2007.
A topic model for word sense disambiguation. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-CoNLL
?07), pages 1024?1033, Prague, Czech Republic.
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. In Pro-
ceedings of the Workshop on GEometrical Models of
Natural Language Semantics (GEMS ?11), pages 22?
32, Edinburgh, UK.
Allison June-Barlow Chaney and David M. Blei. 2012.
Visualizing topic models. In Proceedings of the Sixth
International AAAI Conference on Weblogs and Social
Media, Dublin, Ireland.
Bruce W. Croft, Donald Metzler, and Trevor Strohman.
2009. Search engines: Information retrieval in prac-
tice. Addison-Wesley.
Ritendra Datta, Dhiraj Joshi, Jia Li, and James Z. Wang.
2008. Image Retrieval: Ideas, Influences, and Trends
of the New Age. ACM Computing Surveys, 40(2):1?
60.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? Automatic caption gener-
ation for news images. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1239?1249, Uppsala, Sweden.
Yansong Feng and Mirella Lapata. 2010b. Topic Models
for Image Annotation and Text Illustration. In Pro-
ceedings of Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
831?839, Los Angeles, California.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using Wikipedia-based
explicit semantic analysis. In Proceedings of the In-
ternational Joint Conference on Artificial Intelligence
(IJCAI ?07), pages 1606?1611, Hyberabad, India.
Brynjar Gretarsson, John O?Donovan, Svetlin Bostand-
jiev, Tobias Ho?llerer, Arthur Asuncion, David New-
man, and Padhraic Smyth. 2012. TopicNets: Vi-
sual analysis of large text corpora with topic modeling.
ACM Trans. Intell. Syst. Technol., 3(2):23:1?23:26.
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 362?370, Boulder, Colorado.
Taher Haveliwala, Sepandar Kamvar, and Glen Jeh.
2003. An analytical comparison of approaches to
personalizing PageRank. Technical Report 2003-35,
Stanford InfoLab.
Alexander Hinneburg, Rico Preiss, and Rene? Schro?der.
2012. TopicExplorer: Exploring document collec-
tions with topic models. In Peter A. Flach, Tijl Bie,
and Nello Cristianini, editors, Machine Learning and
Knowledge Discovery in Databases, volume 7524 of
Lecture Notes in Computer Science, pages 838?841.
Springer Berlin Heidelberg.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR ?99), pages
50?57, Berkeley, California, United States.
Ioana Hulpus, Conor Hayes, Marcel Karnstedt, and
Derek Greene. 2013. Unsupervised graph-based topic
labelling using DBpedia. In Proceedings of the 6th
ACM International Conference on Web Search and
Data Mining (WSDM ?13), pages 465?474, Rome,
Italy.
Kalervo Ja?rvelin and Jaana Keka?la?inen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Trans. Inf. Syst., 20(4):422?446.
Yushi Jing and Shumeet Baluja. 2008. PageRank for
product image search. In Proceedings of the 17th In-
ternational Conference on World Wide Web (WWW
?08), pages 307?316, Beijing, China.
Dhiraj Joshi, James Z. Wang, and Jia Li. 2006. The Story
Picturing Engine?A system for automatic text illus-
tration. ACM Trans. Multimedia Comput. Commun.
Appl., 2(1):68?89.
Gabriella Kazai. 2011. In search of quality in crowd-
sourcing for search engine evaluation. Advances in In-
formation Retrieval, pages 165?176.
Jey Han Lau, David Newman, Sarvnaz Karimi, and Tim-
othy Baldwin. 2010. Best topic word selection for
166
topic labelling. In The 23rd International Conference
on Computational Linguistics (COLING ?10), pages
605?613, Beijing, China.
Jey Han Lau, Karl Grieser, David Newman, and Timothy
Baldwin. 2011. Automatic labelling of topic models.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 1536?1545, Portland, Ore-
gon, USA.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
5th Annual International Conference on Systems Doc-
umentation (SIGDOC ?86), pages 24?26, Toronto, On-
tario, Canada.
David G. Lowe. 1999. Object Recognition from Local
Scale-invariant Features. In Proceedings of the Sev-
enth IEEE International Conference on Computer Vi-
sion, pages 1150?1157, Kerkyra, Greece.
David G. Lowe. 2004. Distinctive Image Features from
Scale-Invariant Keypoints. International Journal of
Computer Vision, 60(2):91?110.
Davide Magatti, Silvia Calegari, Davide Ciucci, and
Fabio Stella. 2009. Automatic Labeling of Topics.
In Proceedings of the 9th International Conference on
Intelligent Systems Design and Applications (ICSDA
?09), pages 1227?1232, Pisa, Italy.
Xian-Li Mao, Zhao-Yan Ming, Zheng-Jun Zha, Tat-Seng
Chua, Hongfei Yan, and Xiaoming Li. 2012. Auto-
matic labeling hierarchical topics. In Proceedings of
the 21st ACM International Conference on Informa-
tion and Knowledge Management (CIKM ?12), Shera-
ton, Maui Hawai.
Qiaozhu Mei and ChengXiang Zhai. 2005. Discovering
evolutionary theme patterns from text: an exploration
of temporal text mining. In Proceedings of the 11th
ACM International Conference on Knowledge Discov-
ery in Data Mining (SIGKDD ?05), pages 198?207,
Chicago, Illinois, USA.
Qiaozhu Mei, Xuehua Shen, and Cheng Xiang Zhai.
2007. Automatic Labeling of Multinomial Topic Mod-
els. In Proceedings of the 13th ACM International
Conference on Knowledge Discovery and Data Mining
(SIGKDD ?07), pages 490?499, San Jose, California.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing order into texts. In Proceedings of Interna-
tional Conference on Empirical Methods in Natural
Language Processing (EMNLP ?04), pages 404?411,
Barcelona, Spain.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The PageRank citation ranking:
Bringing order to the web. Technical Report 1999-66,
Stanford InfoLab.
Koen E.A. Sande, Theo Gevers, and Cees G. M. Snoek.
2008. Evaluation of Color Descriptors for Object and
Scene Recognition. In Proceedings of the IEEE Com-
puter Society Conference on Computer Vision and Pat-
tern Recognition (CVPR ?08), pages 1?8, Anchorage,
Alaska, USA.
Richard Szeliski. 2010. Computer Vision: Algorithms
and Applications. Springer-Verlag Inc.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566?1581.
Xing Wei and W. Bruce Croft. 2006. LDA-based Doc-
ument Models for Ad-hoc Retrieval. In Proceedings
of the 29th annual international ACM SIGIR confer-
ence on Research and Development in Information Re-
trieval (SIGIR ?06), pages 178?185, Seattle, Washing-
ton, USA.
167
Proceedings of NAACL-HLT 2013, pages 680?684,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Unsupervised Domain Tuning to Improve Word Sense Disambiguation
Judita Preiss and Mark Stevenson
j.preiss@sheffield.ac.uk and m.stevenson@dcs.shef.ac.uk
Department of Computer Science, University of Sheffield
211 Portobello, Sheffield, S1 4DP, UK
Abstract
The topic of a document can prove to be use-
ful information for Word Sense Disambigua-
tion (WSD) since certain meanings tend to be
associated with particular topics. This paper
presents an LDA-based approach for WSD,
which is trained using any available WSD sys-
tem to establish a sense per (Latent Dirich-
let alocation based) topic. The technique is
tested using three unsupervised and one su-
pervised WSD algorithms within the SPORT
and FINANCE domains giving a performance
increase each time, suggesting that the tech-
nique may be useful to improve the perfor-
mance of any available WSD system.
1 Introduction
Assigning each word its most frequent sense (MFS)
is commonly used as a baseline in Word Sense Dis-
ambiguation (WSD). This baseline can be difficult to
beat, particularly for unsupervised systems which do
not have access to the annotated training data used to
determine the MFS. However, it has also been shown
that unsupervised methods can be used to identify
the most likely sense for each ambiguous word type
and this approach can be effective for disambigua-
tion (McCarthy et al, 2004).
Knowledge of the domain of a document has been
shown to be useful information for WSD. For ex-
ample, Khapra et al (2010) improve the perfor-
mance of a graph-based WSD system using a small
number of hand-tagged examples, but further ex-
amples would be required for each new domain.
Agirre et al (2009) automatically construct a the-
saurus from texts in a domain which they use for
WSD. Unfortunately, performance drops when the
thesaurus is combined with information from local
context. Stevenson et al (2011) showed that per-
formance of an unsupervised WSD algorithm can
be improved by supplementing the context with do-
main information. Cai et al (2007) use LDA to
create an additional feature for a supervised WSD
algorithm, by inferring topics for labeled training
data. Boyd-Graber et al (2007) integrate a topic
model with WordNet and use it to carry out dis-
ambiguation and learn topics simultaneously. Li et
al. (2010) use sense paraphrases to estimate prob-
abilities of senses and carry out WSD. Koeling et
al. (2005) showed that automatically acquiring the
predominant sense of a word from a corpus from
the same domain increases performance (over using
a predominant sense acquired from a balanced cor-
pus), but their work requires a separate thesaurus to
be built for each domain under investigation. Nav-
igli et al (2011) extracted relevant terms from texts
in a domain and used them to initialize a random
walk over the WordNet graph.
Our approaches rely on a one sense per topic
hypothesis (Gale et al, 1992), making use of top-
ics induced using LDA ? we present three novel
techniques for exploiting domain information that
are employable with any WSD algorithm (unsuper-
vised or supervised). Using any WSD algorithm, we
create a sense per topic distribution for each LDA
topic, and the classification of a new document into a
topic determines the sense distribution of the words
within. Once a sense per topic distribution is ob-
tained, no further WSD annotation of new texts is
required. Instead of fixing domains, our technique
680
allows these to be dynamically created (using LDA)
and we using four existing publicly available WSD
algorithms (three unsupervised and one supervised)
to show that our technique increases their perfor-
mance with no changes to the original algorithm.
Section 2 briefly introduces LDA, while Section 3
describes our three techniques for adding domain
information to a WSD algorithm. The WSD algo-
rithms employed in the evaluation of our techniques
are described in Section 4 with experiments and re-
sults in Section 5. Section 6 draws our conclusions
and presents avenues for future work.
2 Latent Dirichlet alocation
LDA (Blei et al, 2003) is a widely used topic model,
which views the underlying document distribution
as having a Dirichlet prior. We employ a pub-
licly available implementation of LDA1 which has
two main execution methods: parameter estimation
(model building) and inference for new data (classi-
fication of a new document). Both invocation meth-
ods produce ? distributions (the topic-document dis-
tributions, i.e., p(ti|d) for ti topics and d document),
and ? distributions (word-topic distributions, i.e.,
p(wj |ti) for words wj). The parameter estimation
phase also creates a list of n words most likely to be
associated with each topic.
3 Using LDA for WSD
The underlying idea of our approach lies in deriv-
ing a document invariant sense distribution for each
topic, p(w, s|t). Once this word sense distribution
is obtained, the underlying WSD algorithm is never
needed again. We make the assumption that while
the WSD algorithm may not be able to select the
correct sense within an individual text due to insuf-
ficient domain information, the topic specific sense
will be selected with a greater frequency over all
documents pertaining to a topic, and thus the prob-
ability distributions over senses generated in this
fashion should be more accurate.
Only the distribution p(w, s|t) is dependent on an
underlying WSD algorithm ? once this distribution
is obtained, it can be combined with the LDA de-
rived ? distribution, p(t|dnew), to compute the de-
1http://jgibblda.sourceforge.net/.
sired word sense distribution within the new docu-
ment dnew:
p(w, s|dnew) =
?
t
p(w, s|t)p(t|dnew)
Sections 3.1, 3.2 and 3.3 describe three different
methods for deriving p(w, s|t), and we investigate
the performance changes with different WSD algo-
rithms: two versions of Personalized PageRank, de-
scribed in Section 4.1, a similarity based WSD sys-
tem outlined in Section 4.2, and a supervised graph
based algorithm (Section 4.3).
3.1 Sense-based topic model (SBTM)
In its usual form, the ? distribution generated
by LDA merely provides a word-topic distribution
(p(w|t)). However, we modify the approach to di-
rectly output p(w, s|t), but we remain able to clas-
sify (non WSD annotated) new text. The topic
model is built from documents annotated with word
senses using the chosen WSD algorithm.2 The topic
model created from this data is based on word-sense
combinations and thus ? represents p(w, s|t).
To classify new (non sense disambiguated) doc-
uments, the model is transformed to a word (rather
than word-sense) based for: i.e., the p(w, s|t) prob-
abilities are summed over all senses of w to give re-
sulting probabilities for the wordform. A new docu-
ment, dnew, classified using this system gives rise to
a number of distributions, including the probability
of a topic given a document distribution (p(t|dnew)).
3.2 Linear equations (LinEq)
If the topic model is created directly from word-
forms, we can use the known probabilities p(s|w, d)
(obtained from the WSD algorithm), and p(t|d)
(from the LDA classifier) to yield an overdetermined
system of linear equations of the form
p(s|w, d) =
?
t
p(s|w, t)p(t|d)
We use an existing implementation of linear least
squares to find a solution (i.e. p(s|w, t) for each t)
2It is not crucial to word sense disambiguate all words in the
text ? a word can be passed to LDA in either its word-sense, dis-
ambiguated, form or in its raw form. While we do not attempt
this in our work, it would be possible to build a model specifi-
cally for noun senses of a word, by including noun senses of the
word and leaving the raw form for any non-noun occurrences.
681
by minimizing the sum of squared differences be-
tween the data values and their corresponding mod-
eled values, i.e., minimizing:
?
d
(
p(s|w, d)?
?
t
p(s|w, t)p(t|d)
)2
3.3 Topic words (TopicWord)
The techniques presented in Sections 3.1 and 3.2
both require the WSD algorithm to annotate a rea-
sonably high proportion of the data used to build the
topic model. For systems which do not rely on word
order, an alternative based on the most likely words
per topic is possible: the LDA algorithm generates
?, a word-topic distribution. It is therefore possible
to extract the most likely words per topic.
To acquire a sense-topic distribution for a topic t,
each target word w is included in a bag of words
which includes the most likely words for t and
the unsupervised WSD algorithm is executed (w is
added to the list if t does not already contain it).
This technique is not applicable to non bag-of-words
WSD algorithms, as structure is absent.
4 Word Sense Disambiguation
Only the topic model documents need to be auto-
matically annotated with the chosen WSD system,
after this, the WSD system is never applied again
(an LDA classification determines the sense distri-
bution) ? this is particularly useful for supervised
system which frequently have a long execution time.
We explore three different types of WSD system:
two versions of a knowledge base based system
(Section 4.1), an unsupervised system (Section 4.2)
and a supervised system (Section 4.3).
4.1 Personalized PageRank (ppr and w2w)
We use the freely available3 Personalized PageRank
algorithm (Agirre and Soroa, 2009) with WordNet
3.0. In Section 5 we present results from two options
of the Personalized PageRank algorithm: ppr, which
performs one PageRank calculation for a whole con-
tent, and w2w, which performs one PageRank cal-
culation for every word in the context to be disam-
biguated.
3Available from http://ixa2.si.ehu.es/ukb/
4.2 WordNet similarity (sim)
We also evaluated another unsupervised approach,
the Perl package WordNet::SenseRelate::AllWords
(Pedersen and Kolhatkar, 2009), which finds senses
of each word in a text based on senses of the sur-
rounding words. The algorithm is invoked with Lesk
similarity (Banerjee and Pedersen, 2002).
4.3 Vector space model (vsm)
An existing vector space model (VSM) based state-
of-the-art supervised WSD system with features de-
rived from the text surrounding the ambiguous word
(Stevenson et al, 2008) is trained on Semcor (Miller
et al, 1993).4
5 Experiments
5.1 Data
The approach is evaluated using a domain-specific
WSD corpus (Koeling et al, 2005) which includes
articles from the FINANCE and SPORTS domains
taken from the Reuters corpus (Rose et al, 2002).
This corpus contains 100 manually annotated in-
stances (from each domain) for 41 words.5
The word-sense LDA topic models are created
from 80,128 documents randomly selected from the
Reuters corpus (this corresponds to a tenth of the en-
tire Reuters corpus). LDA can abstract a model from
a relatively small corpus and a tenth of the Reuters
corpus is much more manageable in terms of mem-
ory and time requirements, particularly given the
need to word sense disambiguate (some part of) each
document in this dataset.6
4A version of Semcor automatically transformed to
WordNet 3.0 available from http://www.cse.unt.edu/
?rada/downloads.html#semcor was used in this work.
5Unfortunately, the entire domain-specific sense disam-
biguated corpus could not be used in the evaluation of
our system, as the released corpus does not link each
annotated sentence to its source document, and it is
not always possible to recover these; approximately 87%
of the data could be used. This dataset is available
at http://staffwww.dcs.shef.ac.uk/people/J.
Preiss/downloads/source_texts.tgz
6In this work, all 80,128 documents were word sense disam-
biguated. However, it would be possible to restrict this set to a
smaller number, as long as a reliable distribution of word senses
per topic could be obtained.
682
ppr w2w sim vsm
Baseline 36 41 23 27
SBTM model 39 43 30 31
LinEq 41 44 ? 33
TopicWord 38 41 ? ?
Table 1: Summary of results based on 150 topics
5.2 Results
Table 1 presents the performance results for the four
WSD algorithms based on 150 topics. A range of
topic values was explored, and 150 topics yielded
highest performance, though the variance between
the performance based on different topics (ranging
from 50 to 250) was very small (0.4% difference to
the average performance with 250 topics, and 3%
with 50). The performance shown indicates the pre-
cision (number correct / number attempted). Recall
is 100% in all cases.
The similarity algorithm (sim) fails on certain
documents and therefore the linear equations tech-
nique could not be applied. The topic word tech-
nique (TopicWord) could not be evaluated using the
similarity algorithm, due to the high sensitivity to
word order within the test paragraph. In addition,
the topic words technique is not applicable to su-
pervised systems, due to its reliance on structured
sentences. The best results with this technique were
obtained with including all likely words with proba-
bilities exceeding 0.001 and smoothing of 0.1 of the
topic document distribution.
Using a Wilcoxon signed-rank test, the results
were found to be significantly better over the orig-
inal algorithms in every case (apart from Topic-
Words). Both the WordNet similarity (sim) and
the VSM approach (vsm) have a lower performance
than the two PPR based WSD algorithms (ppt and
w2w). For example, sim assigns the same (usually
incorrect) sense to all occurrences of the word tie,
while both PPR based algorithms detect an obvious
domain change. The vsm approach suffers from a
lack of training data (only a small number of exam-
ples of each word appear in Semcor), while sim does
not get enough information from the context.
As an interesting aside, the topic models based on
word-sense combinations, as opposed to wordforms
only, are more informative with less overlap. Exam-
ining the word stake annotated with the w2w WSD
algorithm: only topic 1 contains stake among the top
12 terms associated with a topic in the word-sense
model, while 10 topics are found in the wordform
topic model. Table 2 shows the top 12 terms associ-
ated with topics containing the word stake.
Topic Word-based model
39 say, will, company, share, deal, euro-
pean, buy, agreement, stake, new, hun-
gary, oil
63 say, share, united, market, offer, stock,
union, percent, stake, will, point, new
90 say, will, fund, price, london, sell,
stake, indonesia, court, investment,
share, buy
91 say, market, bond, russia, press, party,
stake, russian, country, indonesia, new,
election
97 say, million, bank, uk, percent, share,
stake, world, will, year, central, british
113 say, will, percent, week, billion, last,
italy, plan, stake, year, budget, czech
134 say, china, percent, hong, kong, offi-
cial, stake, billion, report, buy, group,
year
142 say, percent, market, first, bank, rate,
year, dealer, million, money, close,
stake
145 say, will, new, brazil, dollar, group,
percent, stake, year, one, make, do
147 say, yen, forecast, million, parent, mar-
ket, share, will, profit, percent, stake,
group
Sense-based model
1 stake*13286801-n, share*13285176-
n, sell*02242464-v, buy*02207206-v,
have*02204692-v, group*00031264-
n, company*08058098-n,
percent*13817526-n, hold*02203362-
v, deal*01110274-n, shareholder,
interest*13286801-n
Table 2: The presence of stake within the word- and
sense-based topic models
683
6 Conclusion
We present three unsupervised techniques based on
acquiring LDA topics which can be used to improve
the performance of a number of WSD algorithms.
All approaches make use of topic information ob-
tained using LDA and do not require any modifi-
cation of the underlying WSD system. While the
technique is dependent on the accuracy of the WSD
algorithm, it consistently outperforms the baselines
for all four different algorithms.
Acknowledgments
This research was supported by a Google Research
Award. Our thanks also go to the two anonymous
reviewers whose comments have made this paper
much clearer.
References
Agirre, E., de Lacalle, O. L., and Soroa, A. (2009).
Knowledge-based WSD on specific domains: per-
forming better than generic supervised WSD. In Pro-
ceedings of the 21st International Joint Conference on
Artificial Intelligence, pages 1501?1506.
Agirre, E. and Soroa, A. (2009). Personalizing pager-
ank for word sense disambiguation. In Proceedings of
EACL.
Banerjee, S. and Pedersen, T. (2002). An adapted lesk al-
gorithm for word sense disambiguation using wordnet.
In Proceedings of the Third International Conference
on Intelligent Text Processing and Computational Lin-
guistics, pages 135?145.
Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent
Dirichlet alocation. Journal of Machine Learning Re-
search, 3:993?1022.
Boyd-Graber, J., Blei, D., and Zhu, X. (2007). A topic
model for word sense disambiguation. In Proceedings
of the EMNLP-CoNLL, pages 1024?1033.
Cai, J. F., Lee, W. S., and Teh, Y. W. (2007). Nus-ml:
Improving word sense disambiguation using topic fea-
tures. In Proceedings of SEMEVAL.
Gale, W. A., Church, K. W., and Yarowsky, D. (1992).
One sense per discourse. In Proceedings of the
4th DARPA Speech and Natural Language Workshop,
pages 233?237.
Khapra, M., Kulkarni, A., Sohoney, S., and Bhat-
tacharyya, P. (2010). All words domain adapted WSD:
Finding a middle ground between supervision and
unsupervision. In Proceedings of ACL 2010, pages
1532?1541, Uppsala, Sweden.
Koeling, R., Mccarthy, D., and Carroll, J. (2005). Do-
main specific sense distributions and predominant
sense acquisition. In Proceedings of Joint HLT-
EMNLP05, pages 419?426.
Li, L., Roth, B., and Sporleder, C. (2010). Topic models
for word sense disambiguation and token-based idiom
detection. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 1138?1147.
McCarthy, D., Koeling, R., Weeds, J., and Carroll, J.
(2004). Finding predominant senses in untagged text.
In Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 280?
287.
Miller, G. A., Leacock, C., Tengi, R., and Bunker, R. T.
(1993). A semantic concordance. In Proceedings of
the ARPA Workshop on Human Language Technology,
pages 303?308.
Navigli, R., Faralli, S., Soroa, A., de Lacalle, O. L., and
Agirre, E. (2011). Two birds with one stone: learn-
ing semantic models for text categorization and word
sense disambiguation. In CIKM, pages 2317?2320.
Pedersen, T. and Kolhatkar, V. (2009). Word-
net::senserelate::allwords - a broad coverageword
sense tagger that maximizes semantic relatedness
(demonstration system). In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies
Conference, pages 17?20.
Rose, T. G., Stevenson, M., and Whitehead, M. (2002).
The Reuters corpus volume 1 - from yesterday?s news
to tomorrow?s language resources. In Proceedings of
the Third International Conference on Language Re-
sources and Evaluation, pages 827?832.
Stevenson, M., Agirre, E., and Soroa, A. (2012). Exploit-
ing domain information for word sense disambigua-
tion of medical documents. Journal of the American
Medical Informatics Association, 19(2):235?240.
Stevenson, M., Guo, Y., Gaizauskas, R., and Martinez,
D. (2008). Knowledge sources for word sense disam-
biguation of biomedical text. In Proceedings of the
Workshop on Current Trends in Biomedical Natural
Language Processing at ACL, pages 80?87.
684
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 1?4,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
DALE: A Word Sense Disambiguation System for Biomedical Documents
Trained using Automatically Labeled Examples
Judita Preiss and Mark Stevenson
Department of Computer Science, University of Sheffield
Regent Court, 211 Portobello
Sheffield S1 4DP, United Kingdom
j.preiss,m.stevenson@dcs.shef.ac.uk
Abstract
Automatic interpretation of documents is
hampered by the fact that language contains
terms which have multiple meanings. These
ambiguities can still be found when language
is restricted to a particular domain, such as
biomedicine. Word Sense Disambiguation
(WSD) systems attempt to resolve these am-
biguities but are often only able to identify the
meanings for a small set of ambiguous terms.
DALE (Disambiguation using Automatically
Labeled Examples) is a supervised WSD sys-
tem that can disambiguate a wide range of
ambiguities found in biomedical documents.
DALE uses the UMLS Metathesaurus as both
a sense inventory and as a source of infor-
mation for automatically generating labeled
training examples. DALE is able to disam-
biguate biomedical documents with the cover-
age of unsupervised approaches and accuracy
of supervised methods.
1 Introduction
Word Sense Disambiguation (WSD) is an impor-
tant challenge for any automatic text processing sys-
tem since language contains ambiguous terms which
can be difficult to interpret. Ambiguous terms that
are found in biomedical documents include words,
phrases and abbreviations (Schuemie et al, 2005).
Identifying the correct interpretation of ambiguous
terms is important to ensure that the text can be pro-
cessed appropriately.
Many WSD systems developed for biomedical
documents are based on supervised learning, for ex-
ample (McInnes et al, 2007; Martinez and Baldwin,
2011); these have the advantage of being more accu-
rate than unsupervised approaches. However, WSD
systems based on supervised learning rely on man-
ually labeled examples consisting of instances of an
ambiguous term marked with their correct interpre-
tations. Manually labeled examples are very expen-
sive to create and are consequently only available for
a few hundred terms, with each new domain (with
its specialist vocabulary) needing new examples la-
beled. The majority of supervised WSD systems are
limited to resolving a small number of ambiguous
terms and, despite their accuracy, are not suitable for
use within applications.
An alternative approach is to use automatically
labeled examples which can be generated without
manual annotation (Leacock et al, 1998). These
have been used to generate an all-words WSD sys-
tem that assigns senses from WordNet (Zhong and
Ng, 2010). For biomedical documents the UMLS
Metathesaurus (Humphreys et al, 1998b) is a more
suitable lexical resource than WordNet and tech-
niques have been developed to create automatically
labeled examples for this resource (Stevenson and
Guo, 2010). However, to date, automatically labeled
examples have only been used as substitutes for am-
biguous terms for which manually labeled examples
are not available, rather than using them to create a
WSD system that can resolve a wider range of am-
biguities in biomedical documents.
DALE (Disambiguation using Automatically La-
beled Examples) is an online WSD system for
biomedical documents that was developed by creat-
ing automatically labeled examples for all ambigu-
ous terms in the UMLS Metathesaurus. DALE is
1
able to identify a meaning for any term that is am-
biguous in the Metathesaurus and therefore has far
greater coverage of ambiguous terms than other su-
pervised WSD systems. Other all-words WSD sys-
tems for biomedical documents are unsupervised
and do not have as high accuracy as supervised ap-
proaches, e.g. (McInnes, 2008; Agirre et al, 2010).
An unsupervised WSD algorithm (Humphreys et al,
1998a) is included in MetaMap (Aronson and Lang,
2010) but is unable to resolve all types of sense dis-
tinction.
2 The DALE System
2.1 Automatically Labeling Examples
DALE assigns Concept Unique Identifiers (CUIs)
from the UMLS Metathesaurus. The WSD algo-
rithm in DALE is based around a supervised algo-
rithm (Stevenson et al, 2008) trained using automat-
ically labeled examples. The examples are gener-
ated using two methods: Monosemous relatives and
Co-occurring concepts (Stevenson and Guo, 2010).
Both approaches take a single CUI, c, as input and
use information from the UMLS Metathesaurus to
search Medline and identify instances of c that can
be used as labeled examples. The difference be-
tween the two approaches is that they make use of
different information from the Metathesaurus.
Both approaches are provided with a set of
ambiguous CUIs from the UMLS Metathesaurus,
which represent the possible meanings of an am-
biguous term, and a target number of training ex-
amples to be generated for each CUI. The UMLS
Metathesaurus contains a number of data files which
are exploited within these techniques, including: 1.
AMBIGLUI: a list of cases where a LUI, a particular
lexical variant of a term, is linked to multiple CUIs;
2. MRCON: list of all strings and concept names in
the Metathesaurus; 3. MRCOC: co-occurring con-
cepts.
For the monosemous relatives approach, the
strings of monosemous LUIs of the target CUI and
its relatives are used to search Medline to retrieve
training examples. The monosemous LUIs related
to a CUI are defined as any LUIs associated with the
CUI in MRCON table and not listed in AMBIGLUI
table. For example, one of the LUIs associated with
CUI ?C0028707? is L0875433 ?Nutrition Science?
in MRCON table. It is not listed in AMBIGLUI ta-
ble and therefore considered to be a monosemous
LUI of CUI ?C0028707?. The string ?Nutrition
Science? can be used to identify examples of CUI
?C0028707?.
The co-occurring concept approach works differ-
ently: instead of using strings of monosemous LUIs
of the target CUI and its relatives, the strings associ-
ated with LUIs of a number of co-occurring CUIs
of the target CUI and its relatives found in MR-
COC table are used. For instance, ?C0025520?,
?C1524024? and ?C0079107? are the top three co-
occurring CUIs of CUI ?C0015677? in MRCOC ta-
ble. The strings associated with LUIs of these
three CUIs can be used to retrieve examples of CUI
?C0015677? by searching for abstracts containing
all the LUIs of the co-occurring CUIs.
These approaches were used to create labeled
examples for ambiguous CUIs in the 2010AB,
2011AA, 2011AB and 2012AA versions of the
UMLS Metathesaurus. Examples could be gener-
ated for 95.2%, 96.2%, 96.2% and 98% of the CUIs
in each version of the Metathesaurus respectively.
Neither technique was able to generate examples for
the remaining CUIs, however none of these CUIs ap-
pear in the corresponding MetaMapped version of
the Medline Baseline Repository (http://mbr.
nlm.nih.gov), suggesting these CUIs do not tend
to be mentioned within documents. 100 examples
were generated for each CUI since using an equal
number of examples for each CUI produces the best
WSD performance in the absence of other informa-
tion about the likelihood of each CUI (Cheng et al,
2012).
The labeled examples are converted into feature
vectors consisting of lemmas of all content words in
the same sentence as the ambiguous word and, in
addition, the lemmas of all content words in a ?4-
word window around it. A single feature vector is
created for each CUI by taking the centroid of the
feature vectors created from the labeled examples of
that CUI. These vectors are stored in the Centroid
Database for later use.
2.2 Word Sense Disambiguation
WSD of an ambiguous term is carried out by com-
piling a list of all its possible CUIs and comparing
their centroids against a feature vector created from
2
Figure 1: DALE system diagram showing the stages in
the WSD process
the sentence containing the ambiguous term. Pro-
cessing is carried out in multiple stages (see Fig.
1). MetaMap (Aronson and Lang, 2010) is applied
to the text to identify ambiguous terms (identify-
ing terms includes some level of multiword detec-
tion) and their possible CUIs (UMLS lookup of the
identified terms). The input text is also fed into a
pipeline to carry out sentence splitting, tokenization,
part-of-speech tagging and morphological analysis.
Information added by this pipeline is used to cre-
ate a feature vector for each ambiguous term identi-
fied by MetaMap. Finally, the Word Sense Disam-
biguation module uses cosine similarity to compare
the centroid of each possible CUI of the ambiguous
term (retrieved from the Centroid Database) with the
ambiguous term?s feature vector (Stevenson et al,
2008). The most similar CUI is selected for each
ambiguous term.
2.3 Online System
DALE is available as a web service with multiple
interfaces:
The Interactive interface enables a user to submit
a piece of text to the system and view the result in an
intuitive way. Terms in the result are marked accord-
ing to their polysemy: blue denotes that it has only
one meaning in Metathesaurus (i.e. is not ambigu-
ous) while green means that it has multiple mean-
ings. Rolling the mouse over the highlighted items
provides access to additional information in a tooltip
style window, including the set of possible CUIs
and their preferred names. Clicking on one of these
CUIs links to the appropriate page from the UMLS
Terminology Services (http://uts.nlm.nih.
gov/). The CUI chosen by the WSD process is
shown underlined at the bottom of the window. The
result is also available in XML format which can be
downloaded by clicking a link in the result page.
The Batch interface is more suitable for disam-
biguating large amounts of texts. A user can upload
plain text files to be processed by DALE using the
batch interface. The results will be sent to user?s
email address in XML format as soon as the system
finishes processing the file. This interface is sup-
ported by a Job management interface. A job is cre-
ated every time a user uploads a file and each job as-
signed the status of being either ?Waiting? or ?Run-
ning?. The user is also emailed a pin code allowing
them to access this interface to check the status of
their jobs and cancel any waiting jobs.
3 Conclusion
This paper describes DALE, a WSD system for
the biomedical domain based on automatically la-
beled examples. The system is able to disambiguate
all ambiguous terms found in the UMLS Metathe-
saurus. A freely accessible web service is available
and offers a set of easy to use interfaces. We intend
to update DALE with new versions of the UMLS
Metathesaurus as they become available.
The DALE system is available at http://kta.
rcweb.dcs.shef.ac.uk/dale/
Acknowledgments
The authors are grateful to Weiwei Cheng for his
work on the development of the original version of
the DALE system. The development of DALE was
funded by the UK Engineering and Physical Sci-
ences Research Council (grants EP/H500170/1 and
EP/J008427/1) and by a Google Research Award.
We would also like to thank the three reviewers
whose feedback has improved the clarity of this pa-
per.
References
E. Agirre, A. Sora, and M. Stevenson. 2010. Graph-
based word sense disambiguation of biomedical docu-
ments. Bioinformatics, 26(22):2889?2896.
A. Aronson and F. Lang. 2010. An overview
of MetaMap: historical perspective and recent ad-
3
Figure 2: Disambiguation results shown in DALE?s Interactive Interface with the ambiguous term ?cold? selected.
DALE shows the three possible CUIs for ?cold? identified by MetaMap with the selected CUI (C0009443) highlighted
vances. Journal of the American Medical Association,
17(3):229?236.
W. Cheng, J. Preiss, and M. Stevenson. 2012. Scal-
ing up WSD with Automatically Generated Examples.
In BioNLP: Proceedings of the 2012 Workshop on
Biomedical Natural Language Processing, pages 231?
239, Montre?al, Canada.
K. Humphreys, R. Gaizauskas, S. Azzam, C. Huyck,
B. Mitchell, H. Cunningham, and Y. Wilks. 1998a.
Description of the LaSIE-II System used in MUC-7.
In Proceedings of the Seventh Message Understanding
Conference (MUC-7).
L. Humphreys, D. Lindberg, H. Schoolman, and G. Bar-
nett. 1998b. The Unified Medical Language System:
An Informatics Research Collaboration. Journal of the
AmericanMedical Informatics Association, 1(5):1?11.
C. Leacock, M. Chodorow, and G. Miller. 1998. Us-
ing Corpus Statistics and WordNet Relations for Sense
Identification. Computational Linguistics, 24(1):147?
165.
D. Martinez and T. Baldwin. 2011. Word sense
disambiguation for event trigger word detection in
biomedicine. BMC Bioinformatics, 12(Suppl 2):S4.
B. McInnes, T. Pedersen, and J. Carlis. 2007. Using
UMLS Concept Unique Identifiers (CUIs) for Word
Sense Disambiguation in the Biomedical Domain. In
Proceedings of the Annual Symposium of the Ameri-
can Medical Informatics Association, pages 533?537,
Chicago, IL.
Bridget McInnes. 2008. An unsupervised vector ap-
proach to biomedical term disambiguation: Integrat-
ing UMLS and Medline. In Proceedings of the ACL-
08: HLT Student Research Workshop, pages 49?54,
Columbus, Ohio, June. Association for Computational
Linguistics.
M. Schuemie, J. Kors, and B. Mons. 2005. Word Sense
Disambiguation in the Biomedical Domain. Journal
of Computational Biology, 12, 5:554?565.
M. Stevenson and Y. Guo. 2010. Disambiguation of Am-
biguous Biomedical Terms using Examples Generated
from the UMLS Metathesaurus. Journal of Biomedi-
cal Informatics, 43(5):762?773.
M. Stevenson, Y. Guo, R. Gaizauskas, and D. Martinez.
2008. Disambiguation of biomedical text using di-
verse sources of information. BMC Bioinformatics,
9(Suppl 11):S7.
Z. Zhong and H. Ng. 2010. It makes sense: A wide-
coverage word sense disambiguation system for free
text. In Proceedings of the ACL 2010 System Demon-
strations, pages 78?83, Uppsala, Sweden.
4
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 151?156,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
PATHS: A System for Accessing Cultural Heritage Collections
Eneko Agirre?, Nikolaos Aletras?, Paul Clough?, Samuel Fernando?,
Paula Goodale?, Mark Hall?, Aitor Soroa? and Mark Stevenson?
(?) IXA NLP Group, University of the Basque Country
Manuel Lardizabal, 1, 20.018 Donostia, Basque Country
(?) Department of Computer Science, Sheffield University
211 Portobello, Sheffield S1 4DP, United Kingdom
Abstract
This paper describes a system for navigat-
ing large collections of information about
cultural heritage which is applied to Eu-
ropeana, the European Library. Euro-
peana contains over 20 million artefacts
with meta-data in a wide range of Euro-
pean languages. The system currently pro-
vides access to Europeana content with
meta-data in English and Spanish. The pa-
per describes how Natural Language Pro-
cessing is used to enrich and organise this
meta-data to assist navigation through Eu-
ropeana and shows how this information is
used within the system.
1 Introduction
Significant amounts of information about cultural
heritage has been digitised in recent years and is
now easily available through online portals. How-
ever, this vast amount of material can also be over-
whelming for many users since they are provided
with little or no guidance on how to find and inter-
pret this information. Potentially useful and rel-
evant content is hidden from the users who are
typically offered simple keyword-based searching
functionality as the entry point into a cultural her-
itage collection. The situation is very different
within traditional mechanisms for viewing cultural
heritage (e.g. museums) where artefacts are or-
ganised thematically and users guided through the
collection.
This paper describes a system that allows users
to explore large cultural heritage collections. Nav-
igation is based around the metaphor of pathways
(or trails) through the collection, an approach that
has been widely explored as an alternative to stan-
dard keyword-based search (Furuta et al, 1997;
Reich et al, 1999; Shipman et al, 2000; White and
Huang, 2010). Pathways are sets of artefacts or-
ganised around a theme which form access points
to the collection.
Pathways are a useful way to access informa-
tion about cultural heritage. Users accessing these
collections are often unfamiliar with their content,
making keyword-based search unsuitable since
they are unable to formulate appropriate queries
(Wilson et al, 2010). Non-keyword-based search
interfaces have been shown to be suitable for ex-
ploratory search (Marchionini, 2006). Pathways
support this exploration by echoing the organised
galleries and guided tours found in museums.
2 Related Work
Heitzman et al (1997) describe the ILEX system
which acts as a guide through the jewellery col-
lection of the National Museum of Scotland. The
user explores the collection through a set of web
pages which provide descriptions of each artefact
that are personalised for each user. The system
makes use of information about the artefacts the
user has viewed to build up a model of their in-
terests and uses this to customise the descriptions
of each artefact and provide recommendations for
further artefacts in which they may be interested.
Grieser et al (2007) also explore providing rec-
ommendations based on the artefacts a user has
viewed so far. They make use of a range of tech-
niques including language modelling, geospatial
modelling and analysis of previous visitors? be-
haviour to provide recommendations to visitors to
the Melbourne Museum.
Grieser et al (2011) explore methods for de-
termining the similarity between museum arte-
facts, commenting that this is useful for navigation
through these collections and important for per-
sonalisation (Bowen and Filippini-Fantoni, 2004;
O?Donnell et al, 2001), recommendation (Bohn-
ert et al, 2009; Trant, 2009) and automatic tour
generation (Finkelstein et al, 2002; Roes et al,
2009). They also use exhibits from Melbourne
151
Museum and apply a range of approaches to deter-
mine the similarity between them, including com-
paring descriptions and measuring physical dis-
tance between them in the museum.
These approaches, like many of the systems
that have been developed for online access to cul-
tural heritage (e.g. (Hage et al, 2010)), are based
around virtual access to a concrete physical space
(i.e. a museum). They often provide tours which
are constrained by the physical layout of the mu-
seum, such as virtual museum visits. However,
these approaches are less suitable for unstructured
collections such as databases of cultural heritage
artefacts collected from multiple institutions or
artefacts not connected with existing physical pre-
sentation (e.g. in a museum). The PATHS sys-
tem is designed for these types of collections and
makes use of natural language analysis to sup-
port navigation. In particular, similarity between
artefacts is computed automatically (see Section
4.1), background information automatically added
to artefact descriptions (see Section 4.2) and a hi-
erarchy of artefacts generated (see Section 4.3).
3 Cultural Heritage Data
The PATHS system has been applied to data from
Europeana1. This is a web-portal to collections
of cultural heritage artefacts provided by a wide
range of European institutions. Europeana cur-
rently provides access to over 20 million artefacts
including paintings, films, books, archival records
and museum objects. The artefacts are provided
by around 1,500 institutions which range from
major institutions, including the Rijksmuseum in
Amsterdam, the British Library and the Louvre,
to smaller organisations such as local museums.
It therefore contains an aggregation of digital con-
tent from several sources and is not connected with
any one physical museum.
The PATHS system makes use of three collec-
tions from Europeana. The first of these con-
tains artefacts from content providers in the United
Kingdom which has meta-data in English. The
artefacts in the remaining two collections come
from institutions in Spain and have meta-data in
Spanish.
CultureGrid Culture Grid2 is a digital content
provider service from the Collection Trust3.
1http://www.europeana.eu
2http://www.culturegrid.org.uk
3http://www.collectionstrust.org.uk
It contains information about over one mil-
lion artefacts from 40 different UK content
providers such as national and regional mu-
seums and libraries.
Cervantes Biblioteca Virtual Miguel De Cer-
vantes4 contains digitalised Spanish text in
various formats. In total, the online library
contains about 75,000 works from a range of
periods in Spanish history.
Hispana The Biblioteca Nacional de Espan?a5
contains information about a diverse set of
content including text and drawings. The ma-
terial is collected from different providers in
Spain including museums and libraries.
Europeana stores metadata for each artefact in
an XML-based format which includes information
such as its title, the digital format, the collection,
the year of creation and also a short description of
each artefact. However, this meta-data is created
by the content providers and varies significantly
across artefacts. Many of the artefacts have only
limited information associated with them, for ex-
ample a single word title. In addition, the content
providers that contribute to Europeana use differ-
ent hierarchical structures to organise their collec-
tions (e.g. Library of Congress Subject Headings6
and the Art and Architecture Thesaurus7), or do
not organise their content into any structure. Con-
sequently the various hierarchies that are used in
Europeana only cover some of the artefacts and
are not compatible with each other.
3.1 Filtering Data
Analysis of the artefacts in these three collections
revealed that many have short and uninformative
titles or lack a description. This forms a challenge
to language processing techniques since the arte-
fact?s meta-data does not contain enough informa-
tion to model it accurately.
The collections were filtered by removing any
artefacts that have no description and have either
fewer than four words in their title or have a title
that is repeated more than 100 times in the col-
lection. Table 1 shows the number of artefacts
in each of the Europeana collections before and
4http://www.cervantesvirtual.com
5http://www.bne.es
6http://authorities.loc.gov/
7http://www.getty.edu/research/tools/
vocabularies/aat/
152
after this filter has been applied. Applying the
heuristic leads to the removal of around 31% of the
artefacts, although the number varies significantly
across the collections with 61% of the artefacts in
CultureGrid being removed and only 1% of those
in Hispana.
Collection Lang. Total Filtered
CultureGrid Eng. 1,207,781 466,958
Hispana Sp. 1,235,133 1,219,731
Cervantes Sp. 19,278 14,983
2,462,192 1,701,672
Table 1: Number of artefacts in Europeana collec-
tions before and after filtering
4 Data Processing
A range of pre-preprocessing steps were carried
out on these collections to provide additional in-
formation to support navigation in the PATHS sys-
tem.
4.1 Artefact Similarity
We begin by computing the similarity between
the various artefacts in the Europeana collections.
This information is useful for navigation and rec-
ommendation but is not available in the Europeana
collections since they are drawn from a diverse
range of sources.
Similarity is computed using an approach de-
scribed by Aletras et al (2012). in which the top-
ics generated from each artefact?s metadata using
a topic model are compared. Latent Dirichlet Al-
location (LDA) (Blei et al, 2003) is a widely used
type of topic model in which documents can be
viewed as probability distributions over topics, ?.
The similarity between a pair of documents can be
estimated by comparing their topic distributions.
This is achieved by viewing each distribution as
a vector of probabilities and then computing the
cosine of the angle between them:
sim(a, b) =
~?a.~?b
|~?a| ? | ~?b|
(1)
where ~?a is the vector created from the probability
distribution generated by LDA for document a.
This approach is evaluated using a set of 295
pairs of artefacts for which human judgements
of similarity were obtained using crowdsourcing
(Aletras et al, 2012). Pearson correlation between
the similarity scores and human judgements was
0.53.
The similarity between all the artefacts in the
collection is computed in a pairwise fashion. The
25 artefacts with the highest score are retained for
each artefact.
4.2 Background Links
The metadata associated with Europeana artefacts
is often very limited. Consequently links to rele-
vant articles in Wikipedia were added to each the
meta-data of each artefact using Wikipedia Miner
(Milne and Witten, 2008) to provide background
information. In addition to the link, Wikipedia
Miner returns a confidence value between 0 and
1 for each link based on the context of the item.
The accuracy of the links added by Wikipedia
Miner were evaluated using the meta-data associ-
ated with 21 randomly selected artefacts. Three
annotators analysed the links added and found that
a confidence value of 0.5 represented a good bal-
ance between accuracy and coverage. See Fer-
nando and Stevenson (2012) for further details.
4.3 Hierarchies
The range of hierarchies used by the various col-
lections that comprise the Europeana collection
make navigation difficult (see Section 3). Con-
sequently, the Wikipedia links added to the arte-
fact meta-data were used to automatically gener-
ate hierarchies that the cover the entire collection.
These hierarchies are used by the PATHS system
to assist browsing and exploration.
Two approaches are used to generate hierarchies
of Europeana artefacts (WikiFreq and WikiTax).
These are combined to generate the WikiMerge hi-
erarchy which is used in the PATHS system.
WikiFreq uses link frequencies across the en-
tire collection to organise the artefacts. The first
stage in the hierarchy generation process is to
compute the frequency with which each linked
Wikipedia article appears in the collection. The
links in each artefact are these analysed to con-
struct a hierarchy consisting of Wikipedia articles.
The links in the meta-data associated with each
artefact are ordered based on their frequency in the
entire collection and that set of links then inserted
into the hierarchy. For example, if the set of or-
dered links for an artefact is a1, a2, a3 ? ? ? an then
the artefact is then inserted into the hierarchy un-
der the branch a1 ? a2 ? a3 ? ? ? ? an, with
a1 at the top level in the tree and the artefact ap-
pearing under the node an. If this branch does not
already exist in the tree then it is created.
153
The hierarchy is pruned to removing nodes with
fewer than 20 artefacts in them. In addition, if a
node has more than 20 child nodes, only the 20
most frequent are used.
WikiTax uses the Wikipedia Taxonomy
(Ponzetto and Strube, 2011), a taxonomy derived
from Wikipedia categories. Europeana artefacts
are inserted into this taxonomy using the links
added by Wikipedia Miner with each artefact
being added to the taxonomy for all categories
listed in the links. This leads to a taxonomy in
which artefacts can occur in multiple locations.
Each approach was used to generate hierarchies
from the Europeana collections. The resulting hi-
erarchies were evaluated via online surveys, see
Fernando et al (2012) for further details. It was
found that WikiFreq performed well at placing
items into the correct location in the taxonomy and
grouping together similar items under the same
node. However, the overall structure of WikiTax
was judged to be more coherent and comprehensi-
ble.
WikiMerge combines combines WikiFreq and
WikiTax. WikiFreq is used to link each artefact
to Wikipedia articles a1 . . . an, but only the link
to the most specific article, an, is retained. The
an articles are linked to their parent WikiTax top-
ics based on the Wikipedia categories the articles
belong to. The resulting hierarchy is pruned re-
moving all WikiTax topics that do not have a Wik-
iFreq child or have only one child topic. Finally
top-level topics in the combined hierarchy are then
linked to their respective Wikipedia root node.
The resulting WikiMerge hierarchy has Wik-
iFreq topics as its leaves and WikiTax topics as
its interior and root nodes. Experiments showed
that this approach was successful in combining
the strengths of the two methods (Fernando et al,
2012).
5 The PATHS System
The PATHS system provides access to the Euro-
peana collections described in Section 3 by mak-
ing use of the additional information generated us-
ing the approaches described in Section 4. The in-
terface of the PATHS system has three main areas:
Paths enables users to navigate via pathways (see
Section 5.1).
Search supports discovery of both collection arte-
facts and pathways through keyword search
(see Section 5.2).
Explore enables users to explore the collections
using a variety of types of overview (see Sec-
tion 5.3).
5.1 Paths Area
This area provides users with access to Europeana
through pathways or trails. These are manually
generated sets of artefacts organised into a tree
structure which are designed to showcase the con-
tent available to the user in an organised way.
These can be created by users and can be pub-
lished for others to follow. An example path-
way on the topic ?railways? is shown in Figure
1. A short description of the pathway?s content is
shown towards the top of the figure and a graphical
overview of its contents at the bottom.
Figure 1: Example pathway on the topic ?rail-
ways?
Figure 2 shows as example artefact as displayed
in the system. The example artefact is a portrait
of Catherine the Great. The left side of the figure
shows information extracted directly from the Eu-
ropeana meta-data for this artefact. The title and
textual description are shown towards the top left
together with a thumbnail image of the artefact.
Other information from the meta-data is shown be-
neath the ?About this item? heading. The right
side of the figure shows additional information
Figure 2: Example artefact displayed in system in-
terface. Related artefacts and background links are
displayed on right hand side
154
Figure 3: Example visualisations of hierarchy: thesaurus view (top left), tag cloud (top right), map views
(bottom)
about the artefact generated using the approaches
described in Sections 4.1 and 4.2. Related arte-
facts are shown to the user one at a time, click-
ing on the thumbnail image leads to the equivalent
page for the related artefact. Below this are links
to the Wikipedia articles that are identified in the
text of the article?s title and description.
5.2 Search Area
This area allows users to search for artefacts and
pathways using standard keyword search imple-
mented using Lucene (McCandless et al, 2010).
5.3 Explore Area
The system provides a variety of ways to view
the hierarchies generated using the approach de-
scribed in Section 4.3. Figure 3 shows how these
are displayed for a section of the hierarchy with
the label ?Society?. The simplest view (shown in
the top left of Figure 3) is a thesaurus type view
in which levels of the hierarchy are represented by
indentation. The system also allows levels of the
hierarchy to be viewed as a tag cloud (top right of
Figure 3). The final representation of the hierar-
chy is as a map, shown in the bottom of Figure 3.
In this visualisation categories in the hierarchy are
represented as ?islands? on the map. Zooming in
on the map provides more detail about that area of
the hierarchy.
6 Summary and Future Developments
This paper describes a system for navigating Eu-
ropeana, an aggregation of collections of cultural
heritage artefacts. NLP analysis is used to organ-
ise the collection and provide additional informa-
tion. The results of this analysis are provided to
the user through an online interface which pro-
vides access to English and Spanish content in Eu-
ropeana.
Planned future development of this system in-
cludes providing recommendations and more per-
sonalised access. Similarity information (Sec-
tion 4.1) can be used to provide information from
which the recommendations can be made. Person-
alised access will make use of information about
individual users (e.g. from their browsing be-
haviour or information they provide about their
preferences) to generate individual views of Eu-
ropeana.
155
Online Demo
The PATHS system is available at
http://explorer.paths-project.eu/
Acknowledgments
The research leading to these results was
carried out as part of the PATHS project
(http://paths-project.eu) funded by
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 270082
References
N. Aletras, M. Stevenson, and P. Clough. 2012. Com-
puting similarity between items in a digital library
of cultural heritage. Journal of Computing and Cul-
tural Heritage, 5(4):no. 16.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research,
3:993?1022.
F. Bohnert, D. Schmidt, and I. Zuckerman. 2009. Spa-
tial Process for Recommender Systems. In Proc. of
IJCAI 2009, pages 2022?2027, Pasadena, CA.
J. Bowen and S. Filippini-Fantoni. 2004. Personaliza-
tion and the Web from a Museum Perspective. In
Proc. of Museums and the Web 2004, pages 63?78.
Samuel Fernando and Mark Stevenson. 2012. Adapt-
ing Wikification to Cultural Heritage. In Proceed-
ings of the 6th Workshop on Language Technology
for Cultural Heritage, Social Sciences, and Human-
ities, pages 101?106, Avignon, France.
Samuel Fernando, Mark Hall, Eneko Agirre, Aitor
Soroa, Paul Clough, and Mark Stevenson. 2012.
Comparing taxonomies for organising collections of
documents. In Proc. of COLING 2012, pages 879?
894, Mumbai, India.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing Search in Context: The Concept Revisited. ACM
Trans. on Information Systems, 20(1):116?131.
R. Furuta, F.. Shipman, C. Marshall, D. Brenner, and
H. Hsieh. 1997. Hypertext paths and the World-
Wide Web: experiences with Walden?s Paths. In
Proc. of the Eighth ACM conference on Hypertext,
pages 167?176, New York, NY.
K. Grieser, T. Baldwin, and S. Bird. 2007. Dynamic
Path Prediction and Recommendation in a Museum
Environment. In Proc. of the Workshop on Lan-
guage Technology for Cultural Heritage Data (LaT-
eCH 2007), pages 49?56, Prague, Czech Republic.
K. Grieser, T. Baldwin, F. Bohnert, and L. Sonenberg.
2011. Using Ontological and Document Similarity
to Estimate Museum Exhibit Relatedness. Journal
of Computing and Cultural Heritage, 3(3):1?20.
W.R. van Hage, N. Stash, Y. Wang, and L.M. Aroyo.
2010. Finding your way through the Rijksmuseum
with an adaptive mobile museum guide. In Proc. of
ESWC 2010, pages 46?59.
J. Heitzman, C. Mellish, and J. Oberlander. 1997. Dy-
namic Generation of Museum Web Pages: The In-
telligent Labelling Explorer. Archives and Museum
Informatics, 11(2):117?125.
G. Marchionini. 2006. Exploratory Search: from Find-
ing to Understanding. Comm. ACM, 49(1):41?46.
M. McCandless, E. Hatcher, and O. Gospodnetic.
2010. Lucene in Action. Manning Publications.
D. Milne and I. Witten. 2008. Learning to Link with
Wikipedia. In Proc. of CIKM 2008, Napa Valley,
California.
M. O?Donnell, C. Mellish, J. Oberlander, and A. Knott.
2001. ILEX: An architecture for a dynamic hy-
pertext generation system. Natural Language En-
gineering, 7:225?250.
S.P. Ponzetto and M. Strube. 2011. Taxonomy in-
duction based on a collaboratively built knowledge
repository. Artificial Intelligence, 175(9-10):1737?
1756.
S. Reich, L. Carr, D. De Roure, and W. Hall. 1999.
Where have you been from here? Trails in hypertext
systems. ACM Computing Surveys, 31.
I. Roes, N. Stash, Y. Wang, and L. Aroyo. 2009. A
personalized walk through the museum: the CHIP
interactive tour guide. In Proc. of the 27th Interna-
tional Conference on Human Factors in Computing
Systems, pages 3317?3322, Boston, MA.
F. Shipman, R. Furuta, D. Brenner, C. Chung, and
H. Hsieh. 2000. Guided paths through web-based
collections: Design, experiences, and adaptations.
Journal of the American Society for Information Sci-
ence, 51(3):260?272.
J. Trant. 2009. Tagging, folksonomies and art mu-
seums: Early experiments and ongoing research.
Journal of Digital Information, 10(1).
R. White and J. Huang. 2010. Assessing the scenic
route: measuring the value of search trails in web
logs. In Proc. of SIGIR 2010, pages 587?594.
M. Wilson, Kulesm B., M. Schraefel, and B. Schnei-
derman. 2010. From keyword search to explo-
ration: Designing future search interfaces for the
web. Foundations and Trends in Web Science,
2(1):1?97.
156
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 631?636,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Labelling Topics using Unsupervised Graph-based Methods
Nikolaos Aletras and Mark Stevenson
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield, S1 4DP
United Kingdom
{n.aletras, m.stevenson}@dcs.shef.ac.uk
Abstract
This paper introduces an unsupervised
graph-based method that selects textual
labels for automatically generated topics.
Our approach uses the topic keywords to
query a search engine and generate a graph
from the words contained in the results.
PageRank is then used to weigh the words
in the graph and score the candidate labels.
The state-of-the-art method for this task is
supervised (Lau et al, 2011). Evaluation
on a standard data set shows that the per-
formance of our approach is consistently
superior to previously reported methods.
1 Introduction
Topic models (Hofmann, 1999; Blei et al, 2003)
have proved to be a useful way to represent the
content of document collections, e.g. (Chaney and
Blei, 2012; Ganguly et al, 2013; Gretarsson et
al., 2012; Hinneburg et al, 2012; Snyder et al,
2013). In these interfaces, topics need to be pre-
sented to users in an easily interpretable way. A
common way to represent topics is as set of key-
words generated from the n terms with the highest
marginal probabilities. For example, a topic about
the global financial crisis could be represented
by its top 10 most probable terms: FINANCIAL,
BANK, MARKET, GOVERNMENT, MORTGAGE,
BAILOUT, BILLION, STREET, WALL, CRISIS. But
interpreting such lists is not always straightfor-
ward, particularly since background knowledge
may be required (Chang et al, 2009).
Textual labels could assist with the interpre-
tations of topics and researchers have developed
methods to generate these automatically (Mei et
al., 2007; Lau et al, 2010; Lau et al, 2011). For
example, a topic which has keywords SCHOOL,
STUDENT, UNIVERSITY, COLLEGE, TEACHER,
CLASS, EDUCATION, LEARN, HIGH, PROGRAM,
could be labelled as EDUCATION and a suitable la-
bel for the topic shown above would be GLOBAL
FINANCIAL CRISIS. Approaches that make use of
alternative modalities, such as images (Aletras and
Stevenson, 2013), have also been proposed.
Mei et al (2007) label topics using statistically
significant bigrams identified in a reference collec-
tion. Magatti et al (2009) introduced an approach
for labelling topics that relied on two hierarchical
knowledge resources labelled by humans, while
Lau et al (2010) proposed selecting the most rep-
resentative word from a topic as its label. Hulpus
et al (2013) make use of structured data from DB-
pedia to label topics.
Lau et al (2011) proposed a method for auto-
matically labelling topics using information from
Wikipedia. A set of candidate labels is gener-
ated from Wikipedia article titles by querying us-
ing topic terms. Additional labels are then gen-
erated by chunk parsing the article titles to iden-
tify n-grams that represent Wikipedia articles as
well. Outlier labels (less relevant to the topic) are
identified and removed. Finally, the top-5 topic
terms are added to the candidate set. The la-
bels are ranked using Support Vector Regression
(SVR) (Vapnik, 1998) and features extracted us-
ing word association measures (i.e. PMI, t-test, ?
2
and Dice coefficient), lexical features and search
engine ranking. Lau et al (2011) report two ver-
sions of their approach, one unsupervised (which
is used as a baseline) and another which is super-
vised. They reported that the supervised version
achieves better performance than a previously re-
ported approach (Mei et al, 2007).
This paper introduces an alternative graph-
based approach which is unsupervised and less
computationally intensive than Lau et al (2011).
Our method uses topic keywords to form a query.
A graph is generated from the words contained in
the search results and these are then ranked using
the PageRank algorithm (Page et al, 1999; Mihal-
631
{?Description?: ?Microsoft will accelerate your journey to cloud computing with an
agile and responsive datacenter built from your existing technology investments.?,
?DisplayUrl?: ?www.microsoft.com/en-us/server-cloud/datacenter/virtualization.aspx?,
?ID?: ?a42b0908-174e-4f25-b59c-70bdf394a9da?,
?Title?: ?Microsoft | Server & Cloud | Datacenter | Virtualization ...?,
?Url?: ?http://www.microsoft.com/en-us/server-cloud/datacenter/virtualization.aspx?,
... }
Figure 1: Sample of the metadata associated with a search result.
cea and Tarau, 2004). Evaluation on a standard
data set shows that our method consistently out-
performs the best performing previously reported
method, which is supervised (Lau et al, 2011).
2 Methodology
We use the topic keywords to query a search en-
gine. We assume that the search results returned
are relevant to the topic and can be used to identify
and weigh relevant keywords. The most impor-
tant keywords can be used to generate keyphrases
for labelling the topic or weight pre-existing can-
didate labels.
2.1 Retrieving and Processing Text
Information
We use the approach described by Lau et al (2011)
to generate candidate labels from Wikipedia arti-
cles. The 10 terms with the highest marginal prob-
abilities in the topic are used to query Wikipedia
and the titles of the articles retrieved used as candi-
date labels. Further candidate labels are generated
by processing the titles of these articles to identify
noun chunks and n-grams within the noun chunks
that are themselves the titles of Wikipedia arti-
cles. Outlier labels, identified using a similarity
measure (Grieser et al, 2011), are removed. This
method has been proved to produce labels which
effectively summarise a topic?s main subject.
However, it should be noted that our method is
flexible and could be applied to any set of can-
didate labels. We have experimented with various
approaches to candidate label generation but chose
to report results using the approach described by
Lau et al (2011) to allow direct comparison of ap-
proaches.
Information obtained from web searches is used
to identify the best labels from the set of candi-
dates. The top n keywords, i.e. those with highest
marginal probability within the topic, are used to
form a query which was submitted to the Bing
1
search engine. Textual information included in the
Title field
2
of the search results metadata was ex-
tracted. Each title was tokenised using openNLP
3
and stop words removed.
Figure 1 shows a sample of the metadata asso-
ciated with a search result for the topic: VMWARE,
SERVER, VIRTUAL, ORACLE, UPDATE, VIRTU-
ALIZATION, APPLICATION, INFRASTRUCTURE,
MANAGEMENT, MICROSOFT.
2.2 Creating a Text Graph
We consider any remaining words in the search
result metadata as nodes, v ? V , in a graph
G = (V,E). Each node is connected to its neigh-
bouring words in a context window of ?n words.
In the previous example, the words added to the
graph from the Title of the search result are mi-
crosoft, server, cloud, datacenter and virtualiza-
tion.
We consider both unweighted and weighted
graphs. When the graph is unweighted we assume
that all the edges have a weight e = 1. In addi-
tion, we weight the edges of the graph by comput-
ing the relatedness between two nodes, v
i
and v
j
,
as their normalised Pointwise Mutual Information
(NPMI) (Bouma, 2009). Word co-occurrences are
computed using Wikipedia as a a reference cor-
pus. Pairs of words are connected with edges only
if NPMI(w
i
, w
j
) > 0.2 avoiding connections be-
tween words co-occurring by chance and hence in-
troducing noise.
2.3 Identifying Important Terms
Important terms are identified by applying the
PageRank algorithm (Page et al, 1999) in a sim-
ilar way to the approach used by Mihalcea and
1
http://www.bing.com/
2
We also experimented with using the Description field
but found that this reduced performance.
3
http://opennlp.apache.org/
632
Tarau (2004) for document keyphrase extraction.
The PageRank score (Pr) over G for a word (v
i
)
can be computed by the following equation:
Pr(v
i
) = d ?
?
v
j
?C(v
i
)
sim(v
i
, v
j
)
?
v
k
?C(v
j
)
sim(v
j
, v
k
)
Pr(v
j
)
+ (1? d)v (1)
where C(v
i
) denotes the set of vertices which are
connected to the vertex v
i
. d is the damping factor
which is set to the default value of d = 0.85 (Page
et al, 1999). In standard PageRank all elements
of the vector v are the same,
1
N
where N is the
number of nodes in the graph.
2.4 Ranking Labels
Given a candidate label L = {w
1
, ..., w
m
} con-
taining m keywords, we compute the score of L
by simply adding the PageRank scores of its con-
stituent keywords:
Score(L) =
m
?
i=1
Pr(w
i
) (2)
The label with the highest score amongst the set
of candidates is selected to represent the topic. We
also experimented with normalised versions of the
score, e.g. mean of the PageRank scores. How-
ever, this has a negative effect on performance
since it favoured short labels of one or two words
which were not sufficiently descriptive of the top-
ics. In addition, we expect that candidate labels
containing words that do not appear in the graph
(with the exception of stop words) are unlikely to
be good labels for the topic. In these cases the
score of the candidate label is set to 0. We also
experimented with removing this restriction but
found that it lowered performance.
3 Experimental Evaluation
3.1 Data
We evaluate our method on the publicly avail-
able data set published by Lau et al (2011). The
data set consists of 228 topics generated using
text documents from four domains, i.e. blog
posts (BLOGS), books (BOOKS), news articles
(NEWS) and scientific articles from the biomedi-
cal domain (PUBMED). Each topic is represented
by its ten most probable keywords. It is also as-
sociated with candidate labels and human ratings
denoting the appropriateness of a label given the
topic. The full data set consists of approximately
6,000 candidate labels (27 labels per topic).
3.2 Evaluation Metrics
Our evaluation follows the framework proposed
by Lau et al (2011) using two metrics, i.e. Top-
1 average rating and nDCG, to compare various
labelling methods.
Top-1 average rating is the average human rat-
ing (between 0 and 3) assigned to the top-ranked
label proposed by the system. This provides an in-
dication of the overall quality of the label the sys-
tem judges as the best one.
Normalised discounted cumulative gain
(nDCG) (J?arvelin and Kek?al?ainen, 2002; Croft et
al., 2009) compares the label ranking proposed
by the system to the ranking provided by human
annotators. The discounted cumulative gain
at position p, DCG
p
, is computed using the
following equation:
DCG
p
= rel
1
+
p
?
i=2
rel
i
log
2
(i)
(3)
where rel
i
is the relevance of the label to the topic
in position i. Then nDCG is computed as:
nDCG
p
=
DCG
p
IDCG
p
(4)
where IDCG
p
is the superviseed ranking of the
image labels, in our experiments this is the rank-
ing provided by the scores in the human annotated
data set.
3.3 Model Parameters
Our proposed model requires two parameters to
be set: the context window size when connecting
neighbouring words in the graph and the number
of the search results considered when constructing
the graph.
We experimented with different sizes of context
window, n, between?1 words to the left and right
and all words in the title. The best results were ob-
tained when n = 2 for all of the domains. In addi-
tion, we experimented with varying the number of
search results between 10 and 300. We observed
no noticeable difference in the performance when
the number of search results is equal or greater
than 30 (see below). We choose to report results
obtained using 30 search results for each topic. In-
cluding more results did not improve performance
but required additional processing.
633
Domain Model Top-1 Av. Rating nDCG-1 nDCG-3 nDCG-5
BLOGS
Lau et al (2011)-U 1.84 0.75 0.77 0.79
Lau et al (2011)-S 1.98 0.81 0.82 0.83
PR 2.05? 0.83 0.84 0.83
PR-NPMI 2.08? 0.84 0.84 0.83
Upper bound 2.45 1.00 1.00 1.00
BOOKS
Lau et al (2011)-U 1.75 0.77 0.77 0.79
Lau et al (2011)-S 1.91 0.84 0.81 0.83
PR 1.98? 0.86 0.88 0.87
PR-NPMI 2.01? 0.87 0.88 0.87
Upper bound 2.29 1.00 1.00 1.00
NEWS
Lau et al (2011)-U 1.96 0.80 0.79 0.78
Lau et al (2011)-S 2.02 0.82 0.82 0.84
PR 2.04? 0.83 0.81 0.81
PR-NPMI 2.05? 0.83 0.81 0.81
Upper bound 2.45 1.00 1.00 1.00
PUBMED
Lau et al (2011)-U 1.73 0.75 0.77 0.79
Lau et al (2011)-S 1.79 0.77 0.82 0.84
PR 1.88?? 0.80 0.80 0.80
PR-NPMI 1.90?? 0.81 0.80 0.80
Upper bound 2.31 1.00 1.00 1.00
Table 1: Results for Various Approaches to Topic Labelling (?: significant difference (t-test, p < 0.05)
to Lau et al (2011)-U; ?: significant difference (p < 0.05) to Lau et al (2011)-S).
4 Results and Discussion
Results are shown in Table 1. Performance when
PageRank is applied to the unweighted (PR) and
NPMI-weighted graphs (PR-NPMI) (see Section
2.2) is shown. Performance of the best unsuper-
vised (Lau et al (2011)-U) and supervised (Lau
et al (2011)-S) methods reported by Lau et al
(2011) are shown. Lau et al (2011)-U uses the av-
erage ?
2
scores between the topic keywords and
the label keywords while Lau et al (2011)-S uses
SVR to combine evidence from all features. In
addition, upper bound figures, the maximum pos-
sible value given the scores assigned by the anno-
tators, are also shown.
The results obtained by applying PageRank
over the unweighted graph (2.05, 1.98, 2.04 and
1.88) are consistently better than the supervised
and unsupervised methods reported by Lau et al
(2011) for the Top-1 Average scores and this im-
provement is observed in all domains. The differ-
ence is significant (t-test, p < 0.05) for the un-
supervised method. A slight improvement in per-
formance is observed when the weighted graph is
used (2.08, 2.01, 2.05 and 1.90). This is expected
since the weighted graph contains additional in-
formation about word relatedness. For example,
the word hardware is more related and, therefore,
closer in the graph to the word virtualization than
to the word investments.
Results from the nDCG metric imply that our
methods provide better rankings of the candidate
labels in the majority of the cases. It is outper-
formed by the best supervised approach in two do-
mains, NEWS and PUBMED, using the nDCG-
3 and nDCG-5 metrics. However, the best label
proposed by our methods is judged to be better
(as shown by the nDCG-1 and Top-1 Av. Rat-
ing scores), demonstrating that it is only the lower
ranked labels in our approach that are not as good
as the supervised approach.
An interesting finding is that, although limited
in length, the textual information in the search re-
sult?s metadata contain enough salient terms rel-
evant to the topic to provide reliable estimates of
634
50 100 150 200 250 300
1.7
1.8
1.9
2
2.1
2.2
Number of Search Results
T
o
p
-
1
A
v
.
R
a
t
i
n
g
(a) BLOGS
50 100 150 200 250 300
1.7
1.8
1.9
2
2.1
2.2
Number of Search Results
(b) BOOKS
50 100 150 200 250 300
1.7
1.8
1.9
2
2.1
2.2
Number of Search Results
T
o
p
-
1
A
v
.
R
a
t
i
n
g
(c) NEWS
50 100 150 200 250 300
1.7
1.8
1.9
2
2.1
2.2
Number of Search Results
Lau et al (2011)-U
Lau et al (2011)-S
PR
PR-NPMI
(d) PUBMED
Figure 2: Top-1 Average Rating obtained for different number of search results.
term importance. Consequently, it is not necessary
to measure semantic similarity between topic key-
words and candidate labels as previous approaches
have done. In addition, performance improvement
gained from using the weighted graph is mod-
est, suggesting that the computation of association
scores over a large reference corpus could be omit-
ted if resources are limited.
In Figure 2, we show the scores of Top-1 av-
erage rating obtained in the different domains by
experimenting with the number of search results
used to generate the text graph. The most inter-
esting finding is that performance is stable when
30 or more search results are considered. In addi-
tion, we observe that quality of the topic labels in
the four domains remains stable, and higher than
the supervised method, when the number of search
results used is between 150 and 200. The only
domain in which performance of the supervised
method is sometimes better than the approach pro-
posed here is NEWS. The main reason is that news
topics are more fine grained and the candidate
labels of better quality (Lau et al, 2011) which
has direct impact in good performance of ranking
methods.
5 Conclusion
We described an unsupervised graph-based
method to associate textual labels with automati-
cally generated topics. Our approach uses results
retrieved from a search engine using the topic
keywords as a query. A graph is generated from
the words contained in the search results metadata
and candidate labels ranked using the PageRank
algorithm. Evaluation on a standard data set
shows that our method consistently outperforms
the supervised state-of-the-art method for the task.
Acknowledgments
We would like to thank Jey Han Lau for providing
us with the labels selected by Lau et al (2011)-
U and Lau et al (2011)-S. We also thank Daniel
Preot?iuc-Pietro for his useful comments on early
drafts of this paper.
635
References
Nikolaos Aletras and Mark Stevenson. 2013. Rep-
resenting topics using images. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 158?167, At-
lanta, Georgia.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Gerlof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. In Proceed-
ings of GSCL.
Allison June-Barlow Chaney and David M. Blei. 2012.
Visualizing topic models. In Proceedings of the
Sixth International AAAI Conference on Weblogs
and Social Media, Dublin, Ireland.
Jonathan Chang, Jordan Boyd-Graber, and Sean Ger-
rish. 2009. Reading Tea Leaves: How Humans In-
terpret Topic Models. Neural Information, pages 1?
9.
Bruce W. Croft, Donald Metzler, and Trevor Strohman.
2009. Search engines: Information retrieval in
practice. Addison-Wesley.
Debasis Ganguly, Manisha Ganguly, Johannes Level-
ing, and Gareth J.F. Jones. 2013. TopicVis: A GUI
for Topic-based feedback and navigation. In Pro-
ceedings of the Thirty-Sixth Annual International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval (SIGIR 13), Dublin,
Ireland.
Brynjar Gretarsson, John O?Donovan, Svetlin Bostand-
jiev, Tobias H?ollerer, Arthur Asuncion, David New-
man, and Padhraic Smyth. 2012. TopicNets: Visual
analysis of large text corpora with topic modeling.
ACM Trans. Intell. Syst. Technol., 3(2):23:1?23:26.
Karl Grieser, Timothy Baldwin, Fabian Bohnert, and
Liz Sonenberg. 2011. Using Ontological and Doc-
ument Similarity to Estimate Museum Exhibit Re-
latedness. Journal on Computing and Cultural Her-
itage (JOCCH), 3(3):10:1?10:20.
Alexander Hinneburg, Rico Preiss, and Ren?e Schr?oder.
2012. TopicExplorer: Exploring document collec-
tions with topic models. In Peter A. Flach, Tijl
Bie, and Nello Cristianini, editors, Machine Learn-
ing and Knowledge Discovery in Databases, volume
7524 of Lecture Notes in Computer Science, pages
838?841. Springer Berlin Heidelberg.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR ?99),
pages 50?57, Berkeley, California, United States.
Ioana Hulpus, Conor Hayes, Marcel Karnstedt, and
Derek Greene. 2013. Unsupervised graph-based
topic labelling using DBpedia. In Proceedings of the
6th ACM International Conference on Web Search
and Data Mining (WSDM ?13), pages 465?474,
Rome, Italy.
Kalervo J?arvelin and Jaana Kek?al?ainen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Trans. Inf. Syst., 20(4):422?446.
Jey Han Lau, David Newman, Sarvnaz Karimi, and
Timothy Baldwin. 2010. Best topic word selec-
tion for topic labelling. In The 23rd International
Conference on Computational Linguistics (COLING
?10), pages 605?613, Beijing, China.
Jey Han Lau, Karl Grieser, David Newman, and Tim-
othy Baldwin. 2011. Automatic labelling of topic
models. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1536?
1545, Portland, Oregon, USA.
Davide Magatti, Silvia Calegari, Davide Ciucci, and
Fabio Stella. 2009. Automatic Labeling of Top-
ics. In Proceedings of the 9th International Confer-
ence on Intelligent Systems Design and Applications
(ICSDA ?09), pages 1227?1232, Pisa, Italy.
Qiaozhu Mei, Xuehua Shen, and Cheng Xiang Zhai.
2007. Automatic Labeling of Multinomial Topic
Models. In Proceedings of the 13th ACM Inter-
national Conference on Knowledge Discovery and
Data Mining (SIGKDD ?07), pages 490?499, San
Jose, California.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing order into texts. In Proceedings of Inter-
national Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP ?04), pages 404?
411, Barcelona, Spain.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The PageRank citation
ranking: Bringing order to the web. Technical Re-
port 1999-66, Stanford InfoLab.
Justin Snyder, Rebecca Knowles, Mark Dredze,
Matthew Gormley, and Travis Wolfe. 2013. Topic
models and metadata for visualizing text corpora. In
Proceedings of the 2013 NAACL-HLT Demonstra-
tion Session, pages 5?9, Atlanta, Georgia. Associa-
tion for Computational Linguistics.
Vladimir N Vapnik. 1998. Statistical learning theory.
Wiley, New York.
636
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 387?391,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
IIITH: Domain Specific Word Sense Disambiguation
Siva Reddy
IIIT Hyderabad
India
gvsreddy@students.iiit.ac.in
Diana McCarthy
Lexical Computing Ltd.
United Kingdom
diana@dianamccarthy.co.uk
Abhilash Inumella
IIIT Hyderabad
India
abhilashi@students.iiit.ac.in
Mark Stevenson
University of Sheffield
United Kingdom
m.stevenson@dcs.shef.ac.uk
Abstract
We describe two systems that participated
in SemEval-2010 task 17 (All-words Word
Sense Disambiguation on a Specific Do-
main) and were ranked in the third and
fourth positions in the formal evaluation.
Domain adaptation techniques using the
background documents released in the
task were used to assign ranking scores to
the words and their senses. The test data
was disambiguated using the Personalized
PageRank algorithm which was applied
to a graph constructed from the whole of
WordNet in which nodes are initialized
with ranking scores of words and their
senses. In the competition, our systems
achieved comparable accuracy of 53.4 and
52.2, which outperforms the most frequent
sense baseline (50.5).
1 Introduction
The senses in WordNet are ordered according to
their frequency in a manually tagged corpus, Sem-
Cor (Miller et al, 1993). Senses that do not oc-
cur in SemCor are ordered arbitrarily after those
senses of the word that have occurred. It is known
from the results of SENSEVAL2 (Cotton et al,
2001) and SENSEVAL3 (Mihalcea and Edmonds,
2004) that first sense heuristic outperforms many
WSD systems (see McCarthy et al (2007)). The
first sense baseline?s strong performance is due to
the skewed frequency distribution of word senses.
WordNet sense distributions based on SemCor are
clearly useful, however in a given domain these
distributions may not hold true. For example, the
first sense for ?bank? in WordNet refers to ?slop-
ing land beside a body of river? and the second
to ?financial institution?, but in the domain of ?fi-
nance? the ?financial institution? sense would be
expected to be more likely than the ?sloping land
beside a body of river? sense. Unfortunately, it
is not feasible to produce large manually sense-
annotated corpora for every domain of interest.
McCarthy et al (2004) propose a method to pre-
dict sense distributions from raw corpora and use
this as a first sense heuristic for tagging text with
the predominant sense. Rather than assigning pre-
dominant sense in every case, our approach aims
to use these sense distributions collected from do-
main specific corpora as a knowledge source and
combine this with information from the context.
Our approach focuses on the strong influence of
domain for WSD (Buitelaar et al, 2006) and the
benefits of focusing on words salient to the do-
main (Koeling et al, 2005). Words are assigned
a ranking score based on its keyness (salience) in
the given domain. We use these word scores as
another knowledge source.
Graph based methods have been shown to
produce state-of-the-art performance for unsu-
pervised word sense disambiguation (Agirre and
Soroa, 2009; Sinha and Mihalcea, 2007). These
approaches use well-known graph-based tech-
niques to find and exploit the structural properties
of the graph underlying a particular lexical knowl-
edge base (LKB), such as WordNet. These graph-
based algorithms are appealing because they take
into account information drawn from the entire
graph as well as from the given context, making
them superior to other approaches that rely only
on local information individually derived for each
word.
Our approach uses the Personalized PageRank
algorithm (Agirre and Soroa, 2009) over a graph
387
representing WordNet to disambiguate ambigu-
ous words by taking their context into consider-
ation. We also combine domain-specific informa-
tion from the knowledge sources, like sense distri-
bution scores and keyword ranking scores, into the
graph thus personalizing the graph for the given
domain.
In section 2, we describe domain sense ranking.
Domain keyword ranking is described in Section
3. Graph construction and personalized page rank
are described in Section 4. Evaluation results over
the SemEval data are provided in Section 5.
2 Domain Sense Ranking
McCarthy et al (2004) propose a method for
finding predominant senses from raw text. The
method uses a thesaurus acquired from automat-
ically parsed text based on the method described
by Lin (1998). This provides the top k nearest
neighbours for each target word w, along with the
distributional similarity score between the target
word and each neighbour. The senses of a word
w are each assigned a score by summing over the
distributional similarity scores of its neighbours.
These are weighted by a semantic similarity score
(using WordNet Similarity score (Pedersen et al,
2004) between the sense of w and the sense of the
neighbour that maximizes the semantic similarity
score.
More formally, let N
w
= {n
1
, n
2
, . . . n
k
}
be the ordered set of the top k scoring
neighbours of w from the thesaurus with
associated distributional similarity scores
{dss(w, n
1
), dss(w, n
2
), . . . dss(w, n
k
)}. Let
senses(w) be the set of senses of w. For each
sense of w (ws
i
? senses(w)) a ranking score is
obtained by summing over the dss(w, n
j
) of each
neighbour (n
j
? N
w
) multiplied by a weight.
This weight is the WordNet similarity score
(wnss) between the target sense (ws
i
) and the
sense of n
j
(ns
x
? senses(n
j
)) that maximizes
this score, divided by the sum of all such WordNet
similarity scores for senses(w) and n
j
. Each
sense ws
i
? senses(w) is given a sense ranking
score srs(ws
i
) using
srs(ws
i
) =
?
n
j
N
w
dss(w, n
j
)?
wnss(ws
i
, n
j
)
?
ws
i
senses(w)
wnss(ws
i
, n
j
)
where wnss(ws
i
, n
j
) =
max
ns
x
?senses(n
j
)
(wnss(ws
i
, ns
x
))
Since this approach requires only raw text,
sense rankings for a particular domain can be gen-
erated by simply training the algorithm using a
corpus representing that domain. We used the
background documents provided to the partici-
pants in this task as a domain specific corpus. In
general, a domain specific corpus can be obtained
using domain-specific keywords (Kilgarriff et al,
2010). A thesaurus is acquired from automatically
parsed background documents using the Stanford
Parser (Klein and Manning, 2003). We used k = 5
to built the thesaurus. As we increased k we found
the number of non-domain specific words occur-
ring in the thesaurus increased and negatively af-
fected the sense distributions. To counter this, one
of our systems IIITH2 used a slightly modified
ranking score by multiplying the effect of each
neighbour with its domain keyword ranking score.
The modified sense ranking msrs(ws
j
) score of
sense ws
i
is
msrs(ws
i
) =
?
n
j
N
w
dss(w, n
j
)?
wnss(ws
i
, n
j
)
?
ws
i
senses(w)
wnss(ws
i
, n
j
)
?krs(n
j
)
where krs(n
j
) is the keyword ranking score of
the neighbour n
j
in the domain specific corpus. In
the next section we describe the way in which we
compute krs(n
j
).
WordNet::Similarity::lesk (Pedersen et al,
2004) was used to compute word similarity wnss.
IIITH1 and IIITH2 systems differ in the way
senses are ranked. IIITH1 uses srs(ws
j
) whereas
IIITH2 system uses msrs(ws
j
) for computing
sense ranking scores in the given domain.
3 Domain Keyword Ranking
We extracted keywords in the domain by compar-
ing the frequency lists of domain corpora (back-
ground documents) and a very large general cor-
pus, ukWaC (Ferraresi et al, 2008), using the
method described by Rayson and Garside (2000).
For each word in the frequency list of the domain
corpora, words(domain), we calculated the log-
likelihood (LL) statistic as described in Rayson
and Garside (2000). We then normalized LL to
compute keyword ranking score krs(w) of word
w words(domain) using
388
krs(w) =
LL(w)
?
w
i
?words(domain)
LL(w
i
)
The above score represents the keyness of the
word in the given domain. Top ten keywords (in
descending order of krs) in the corpora provided
for this task are species, biodiversity, life, habitat,
natura
1
, EU, forest, conservation, years, amp
2
.
4 Personalized PageRank
Our approach uses the Personalized PageRank al-
gorithm (Agirre and Soroa, 2009) with WordNet
as the lexical knowledge base (LKB) to perform
WSD. WordNet is converted to a graph by repre-
senting each synset as a node (synset node) and the
relationships in WordNet (hypernymy, hyponymy
etc.) as edges between synset nodes. The graph is
initialized by adding a node (word node) for each
context word of the target word (including itself)
thus creating a context dependent graph (person-
alized graph). The popular PageRank (Page et al,
1999) algorithm is employed to analyze this per-
sonalized graph (thus the algorithm is referred as
personalized PageRank algorithm) and the sense
for each disambiguous word is chosen by choos-
ing the synset node which gets the highest weight
after a certain number of iterations of PageRank
algorithm.
We capture domain information in the personal-
ized graph by using sense ranking scores and key-
word ranking scores of the domain to assign initial
weights to the word nodes and their edges (word-
synset edge). This way we personalize the graph
for the given domain.
4.1 Graph Initialization Methods
We experimented with different ways of initial-
izing the graph, described below, which are de-
signed to capture domain specific information.
Personalized Page rank (PPR): In this method,
the graph is initialized by allocating equal prob-
ability mass to all the word nodes in the context
including the target word itself, thus making the
graph context sensitive. This does not include do-
main specific information.
1
In background documents this word occurs in reports de-
scribing Natura 2000 networking programme.
2
This new word ?amp? is created by our programs while
extracting body text from background documents. The
HTML code ?&amp;? which represents the symbol?&? is
converted into this word.
Keyword Ranking scores with PPR (KRS +
PPR): This is same as PPR except that context
words are initialized with krs.
Sense Ranking scores with PPR (SRS + PPR):
Edges connecting words and their synsets are as-
signed weights equal to srs. The initialization of
word nodes is same as in PPR.
KRS + SRS + PPR: Word nodes are initialized
with krs and edges are assigned weights equal to
srs.
In addition to the above methods of unsuper-
vised graph initialization, we also initialized the
graph in a semi-supervised manner. WordNet (ver-
sion 1.7 and above) have a field tag cnt for each
synset (in the file index.sense) which represents
the number of times the synset is tagged in vari-
ous semantic concordance texts. We used this in-
formation, concordance score (cs) of each synset,
with the above methods of graph initialization as
described below.
Concordance scores with PPR (CS + PPR): The
graph initialization is similar to PPR initialization
additionally with concordance score of synsets on
the edges joining words and their synsets.
CS + KRS + PPR: The initialization graph of
KRS + PPR is further initialized by assigning con-
cordance scores to the edges connecting words and
their synsets.
CS + SRS + PPR: Edges connecting words and
their synsets are assigned weights equal to sum of
the concordance scores and sense ranking scores
i.e. cs + srs. The initialization of word nodes is
same as in PPR.
CS + KRS + SRS + PPR: Word nodes are ini-
tialized with krs and edges are assigned weights
equal to cs + srs.
PageRank was applied to all the above graphs to
disambiguate a target word.
4.2 Experimental details of PageRank
Tool: We used UKB tool
3
(Agirre and Soroa,
2009) which provides an implementation of per-
sonalized PageRank. We modified it to incorpo-
rate our methods of graph initialization. The LKB
used in our experiments is WordNet3.0 + Gloss
which is provided in the tool. More details of the
tools used can be found in the Appendix.
Normalizations: Sense ranking scores (srs) and
keyword ranking scores (krs) have diverse ranges.
We found srs generally in the range between 0 to
3
http://ixa2.si.ehu.es/ukb/
389
Precision Recall
Unsupervised Graph Initialization
PPR 37.3 36.8
KRS + PPR 38.1 37.6
SRS + PPR 48.4 47.8
KRS + SRS + PPR 48.0 47.4
Semi-supervised Graph Initialization
CS + PPR 50.2 49.6
CS + KRS + PPR 50.1 49.5
* CS + SRS + PPR 53.4 52.8
CS + KRS + SRS + PPR 53.6 52.9
Others
1
st
sense 50.5 50.5
PSH 49.8 43.2
Table 1: Evaluation results on English test data of SemEval-2010 Task-17. * represents the system which
we submitted to SemEval and is ranked 3rd in public evaluation.
1 and krs in the range 0 to 0.02. Since these scores
are used to assign initial weights in the graph,
these ranges are scaled to fall in a common range
of [0, 100]. Using any other scaling method should
not effect the performance much since PageRank
(and UKB tool) has its own internal mechanisms
to normalize the weights.
5 Evaluation Results
Test data released for this task is disambiguated
using IIITH1 and IIITH2 systems. As described
in Section 2, IIITH1 and IIITH2 systems differ in
the way the sense ranking scores are computed.
Here we project only the results of IIITH1 since
IIITH1 performed slightly better than IIITH2 in all
the above settings. Results of 1
st
sense system pro-
vided by the organizers which assigns first sense
computed from the annotations in hand-labeled
corpora is also presented. Additionally, we also
present the results of Predominant Sense Heuristic
(PSH) which assigns every word w with the sense
ws
j
(ws
j
? senses(w)) which has the highest
value of srs(ws
j
) computed in Section 2 similar
to (McCarthy et al, 2004).
Table 1 presents the evaluation results. We used
TreeTagger
4
to Part of Speech tag the test data.
POS information was used to discard irrelevant
senses. Due to POS tagging errors, our precision
values were not equal to recall values. In the com-
petition, we submitted IIITH1 and IIITH2 systems
with CS + SRS + PPR graph initialization. IIITH1
4
http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger/
and IIIH2 gave performances of 53.4 % and 52.2
% precision respectively. In our later experiments,
we found CS + KRS + SRS + PPR has given the
best performance of 53.6 % precision.
From the results, it can be seen when srs in-
formation is incorporated in the graph, precision
improved by 11.1% compared to PPR in unsuper-
vised graph initialization and by 3.19% compared
to CS + PPR in semi-supervised graph initializa-
tion. Also little improvements are seen when krs
information is added. This shows that domain
specific information like sense ranking scores and
keyword ranking scores play a major role in do-
main specific WSD.
The difference between the results in unsu-
pervised and semi-supervised graph initializations
may be attributed to the additional information the
semi-supervised graph is having i.e. the sense dis-
tribution knowledge of non-domain specific words
(common words).
6 Conclusion
This paper proposes a method for domain specific
WSD. Our method is based on a graph-based al-
gorithm (Personalized Page Rank) which is mod-
ified to include information representing the do-
main (sense ranking and key word ranking scores).
Experiments show that exploiting this domain spe-
cific information within the graph based methods
produces better results than when this information
is used individually.
390
Acknowledgements
The authors are grateful to Ted Pedersen for his
helpful advice on the WordNet Similarity Pack-
age. We also thank Rajeev Sangal for supporting
the authors Siva Reddy and Abhilash Inumella.
References
Eneko Agirre and Aitor Soroa. 2009. Personaliz-
ing pagerank for word sense disambiguation. In
EACL ?09: Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 33?41, Morristown, NJ,
USA. Association for Computational Linguistics.
Paul Buitelaar, Bernardo Magnini, Carlo Strapparava,
and Piek Vossen. 2006. Domain-specific wsd. In
Word Sense Disambiguation. Algorithms and Appli-
cations, Editors: Eneko Agirre and Philip Edmonds.
Springer.
Scott Cotton, Phil Edmonds, Adam Kilgarriff, and
Martha Palmer. 2001. Senseval-2. http://www.
sle.sharp.co.uk/senseval2.
A. Ferraresi, E. Zanchetta, M. Baroni, and S. Bernar-
dini. 2008. Introducing and evaluating ukwac,
a very large web-derived corpus of english. In
Proceed-ings of the WAC4 Workshop at LREC 2008,
Marrakesh, Morocco.
Adam Kilgarriff, Siva Reddy, Jan Pomik?alek, and Avi-
nesh PVS. 2010. A corpus factory for many lan-
guages. In LREC 2010, Malta.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In ACL ?03: Proceedings
of the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 423?430, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In HLT ?05: Proceed-
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 419?426, Morristown, NJ, USA.
Association for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of the 17th
international conference on Computational linguis-
tics, pages 768?774, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In ACL ?04: Proceedings of the 42nd
Annual Meeting on Association for Computational
Linguistics, page 279, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
33(4):553?590.
Rada Mihalcea and Phil Edmonds, editors. 2004.
Proceedings Senseval-3 3rd International Workshop
on Evaluating Word Sense Disambiguation Systems.
ACL, Barcelona, Spain.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proceedings of the ARPA Workshop on Human Lan-
guage Technology, pages 303?308. Morgan Kauf-
man.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web. Technical Report
1999-66, Stanford InfoLab, November.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the re-
latedness of concepts. In HLT-NAACL ?04: Demon-
stration Papers at HLT-NAACL 2004 on XX, pages
38?41, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Paul Rayson and Roger Garside. 2000. Comparing
corpora using frequency profiling. In Proceedings
of the workshop on Comparing corpora, pages 1?
6, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Ravi Sinha and Rada Mihalcea. 2007. Unsupervised
graph-basedword sense disambiguation using mea-
sures of word semantic similarity. In ICSC ?07: Pro-
ceedings of the International Conference on Seman-
tic Computing, pages 363?369, Washington, DC,
USA. IEEE Computer Society.
Appendix
Domain Specific Thesaurus, Sense Ranking
Scores and Keyword Ranking Scores are accessi-
ble at
http://web.iiit.ac.in/
?
gvsreddy/
SemEval2010/
Tools Used:
? UKB is used with options ?ppr ?dict weight. Dictio-
nary files which UKB uses are automatically generated
using sense ranking scores srs.
? Background document words are canonicalized using
KSTEM, a morphological analyzer
? The Stanford Parser is used to parse background docu-
ments to build thesaurus
? Test data is part of speech tagged using TreeTagger.
391
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 54?58,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Detecting Text Reuse with Modified and Weighted N-grams
Rao Muhammad Adeel Nawab?, Mark Stevenson? and Paul Clough?
?Department of Computer Science and ?iSchool
University of Sheffield, UK.
{r.nawab@dcs, m.stevenson@dcs, p.d.clough@} .shef.ac.uk
Abstract
Text reuse is common in many scenarios and
documents are often based, at least in part, on
existing documents. This paper reports an ap-
proach to detecting text reuse which identifies
not only documents which have been reused
verbatim but is also designed to identify cases
of reuse when the original has been rewrit-
ten. The approach identifies reuse by compar-
ing word n-grams in documents and modifies
these (by substituting words with synonyms
and deleting words) to identify when text has
been altered. The approach is applied to a cor-
pus of newspaper stories and found to outper-
form a previously reported method.
1 Introduction
Text reuse is the process of creating new docu-
ment(s) using text from existing document(s). Text
reuse is standard practice in some situations, such as
journalism. Applications of automatic detection of
text reuse include the removal of (near-)duplicates
from search results (Hoad and Zobel, 2003; Seo and
Croft, 2008), identification of text reuse in journal-
ism (Clough et al, 2002) and identification of pla-
giarism (Potthast et al, 2011).
Text reuse is more difficult to detect when the
original text has been altered. We propose an ap-
proach to the identification of text reuse which is
intended to identify reuse in such cases. The ap-
proach is based on comparison of word n-grams, a
popular approach to detecting text reuse. However,
we also account for synonym replacement and word
deletion, two common text editing operations (Bell,
1991). The relative importance of n-grams is ac-
counted for using probabilities obtained from a lan-
guage model. We show that making use of modified
n-grams and their probabilities improves identifica-
tion of text reuse in an existing journalism corpus
and outperforms a previously reported approach.
2 Related Work
Approaches for identifying text reuse based on
word-level comparison (such as the SCAM copy de-
tection system (Shivakumar and Molina, 1995)) tend
to identify topical similarity between a pair of doc-
uments, whereas methods based on sentence-level
comparison (e.g. the COPS copy detection sys-
tem (Brin et al, 1995)) are unable to identify when
text has been reused if only a single word has been
changed in a sentence.
Comparison of word and character n-grams has
proven to be an effective method for detecting text
reuse (Clough et al, 2002; Ceden?o et al, 2009; Chiu
et al, 2010). For example, Ceden?o et al (2009)
showed that comparison of word bigrams and tri-
grams are an effective method for detecting reuse in
journalistic text. Clough et al (2002) also applied
n-gram overlap to identify reuse of journalistic text,
combining it with other approaches such as sentence
alignment and string matching algorithms. Chiu et
al. (2010) compared n-grams to identify duplicate
and reused documents on the web. Analysis of word
n-grams has also proved to be an effective method
for detecting plagiarism, another form of text reuse
(Lane et al, 2006).
However, a limitation of n-gram overlap approach
is that it fails to identify reuse when the original
54
text has been altered. To overcome this problem we
propose using modified n-grams, which have been
altered by deleting or substituting words in the n-
gram. The modified n-grams are intended to im-
prove matching with the original document.
3 Determining Text Reuse with N-gram
Overlap
3.1 N-grams Overlap (NG)
Following Clough et al (2002), the asymmetric con-
tainment measure (eqn 1) was used to quantify the
degree of text within a document (A) that is likely to
have been reused in another document (B).
scoren(A,B) =
?
ngram?B
count(ngram,A)
?
ngram?B
count(ngram,B)
(1)
where count(ngram,A) is the number of times
ngram appears in document A. A score of 1 means
that document B is contained in document A and a
score of 0 that none of the n-grams in B occur in A.
3.2 Modified N-grams
N-gram overlap has been shown to be useful for
measuring text reuse as derived texts typically share
longer n-grams (? 3 words). However, the approach
breaks down when an original document has been
altered. To counter this problem we applied vari-
ous techniques for modifying n-grams that allow for
word deletions (Deletions) and word substitutions
(WordNet and Paraphrases), two common text edit-
ing operations.
Deletions (Del) Assume that w1, w2, ...wn is an
n-gram. Then a set of modified n-grams can be cre-
ated by removing one of the w2 ... wn?1. The first
and last words in the n-gram are not removed since
they will also be generated as shorter n-grams. An
n-gram will generate n ? 2 deleted n-grams and no
deleted n-grams will be generated for unigrams and
bigrams.
Substitutions Further n-grams can be created by
substituting one of the words in an n-gram with one
of its synonyms from WordNet (WN). For words
with multiple senses we use synonyms from all
senses. Modified n-grams are created by substitut-
ing one of the words in the n-gram with one of its
synonyms from WordNet.
Similarly to the WordNet approach, n-grams can
be created by substituting one of the words with an
equivalent term from a paraphrase lexicon, which
we refer to as Paraphrases (Para). A paraphrase
lexicon was generated automatically (Burch, 2008)
and ten lexical equivalents (the default setting) pro-
duced for each word. Modified n-grams were cre-
ated by substituting one of the words in the n-gram
with one of the lexical equivalents.
3.3 Comparing Modified N-grams
The modified n-grams are applied in the text reuse
score by generating modified n-grams for the docu-
ment that is suspected to contain reused text. These
n-grams are then compared with the original docu-
ment to determine the overlap. However, the tech-
niques in Section 3.2 generate a large number of
modified n-grams which means that the number
of n-grams that overlap with document A can be
greater than the total number of n-grams in B, lead-
ing to similarity scores greater than 1. To avoid this
the n-gram overlap counts are constrained in a simi-
lar way that they are clipped in BLEU and ROUGE
(Papineni et al, 2002; Lin, 2004).
For each n-gram in B, a set of modified n-grams,
mod(ngram), is created.1 The count for an in-
dividual n-gram in B, exp count(ngram,B), can
be computed as the number of times any n-gram in
mod(ngram) occurs in A, see equation 2.
?
ngram? ?mod(ngram)
count(ngram?, A) (2)
However, the contribution of this count to the text
reuse score has to be bounded to ensure that the com-
bined count of the modified n-grams appearing in
A does not exceed the number of times the origi-
nal n-gram occurs in B. Consequently the text reuse
score, scoren(A,B), is computed using equation 3.
?
ngram
?B
min(exp count(ngram,A), count(ngram,B))
?
ngram?B
count(ngram,B)
(3)
3.4 Weighting N-grams
Probabilities of each n-gram, obtained using a lan-
guage model, are used to increase the importance of
1This is the set of n-grams that could have been created by
modifing an n-gram in B and includes the original n-gram itself.
55
rare n-grams and decrease the contribution of com-
mon ones. N-gram probabilities are computed us-
ing the SRILM language modelling toolkit (Stolcke,
2002). The score for each n-gram is computed as
its Information Content (Cover and Thomas, 1991),
ie. ?log(P ). When the language model (LM) is
applied the scores associated with each n-gram are
used instead of counts in equations 2 and 3.
4 Experiments
4.1 METER Corpus
The METER corpus (Gaizauskas et al, 2001) con-
tains 771 Press Association (PA) articles, some of
which were used as source(s) for 945 news stories
published by nine British newspapers.
These 945 documents are classified as Wholly De-
rived (WD), Partially Derived (PD) and Non De-
rived (ND). WD means that the newspaper article
is likely derived entirely from the PA source text;
PD reflects the situation where some of the newspa-
per article is derived from the PA source text; news
stories likely to be written independently of the PA
source fall into the category of ND. In our experi-
ments, the 768 stories from court and law reporting
were used (WD=285, PD=300, ND=183) to allow
comparison with Clough et al (2002). To provide a
collection to investigate binary classification we ag-
gregated the WD and PD cases to form a Derived set.
Each document was pre-processed by converting to
lower case and removing all punctuation marks.
4.2 Determining Reuse
The text reuse task aims to distinguish between lev-
els of text reuse, i.e. WD, PD and ND. Two versions
of a classification task were used: binary classifica-
tion distinguishes between Derived (i.e. WD ? PD)
and ND documents, and ternary classification distin-
guishes all three levels of reuse.
A Naive Bayes classifier (Weka version 3.6.1) and
10-fold cross validation were used for the experi-
ments. Containment similarity scores between all
PA source texts and news articles on the same story
were computed for word uni-grams, bi-grams, tri-
grams, four-grams and five-grams. These five simi-
larity scores were used as features. Performance was
measured using precision, recall and F1 measures
with the macro-average reported across all classes.
The language model (Section 3.4) was trained us-
ing 806,791 news articles from the Reuters Corpus
(Rose et al, 2002). A high proportion of the news
stories selected were related to the topics of enter-
tainment and legal reports to reflect the subjects of
the new articles in the METER corpus.
5 Results and Analysis
Tables 1 and 2 show the results of the binary
and ternary classification experiments respectively.
?NG? refers to the comparison of n-grams in each
document (Section 3.1), while ?Del?, ?WN? and
?Para? refer to the modified n-grams created us-
ing deletions, WordNet and paraphrases respectively
(Section 3.2). The prefix ?LM? (e.g. ?LM-NG?) in-
dicates that the n-grams are weighted using the lan-
guage model probability scores (Section 3.4).
For the binary classification task (Table 1) it can
be observed that including modified n-grams im-
proves performance. This improvement is observed
when each of the three types of modified n-grams
is applied individually, with a greater increase being
observed for the n-grams created using the WordNet
and paraphrase approaches. Further improvement is
observed when different types of modified n-grams
are combined with the best performance obtained
when all three types are used. All improvements
over the baseline approach (NG) are statistically
significant (Wilcoxon signed-rank test, p < 0.05).
These results demonstrate that the various types of
modified n-grams all contribute to identifying when
text is being reused since they capture different types
of rewrite operations.
In addition, performance consistently improves
when n-grams are weighted using language model
scores. The improvement is significant for all types
of n-grams. This demonstrates that the information
provided by the language model is useful in deter-
mining the relative importance of n-grams.
Several of the results are higher than those re-
ported by Clough et al (2002) (F1=0.763), despite
the fact their approach supplements n-gram overlap
with additional techniques such as sentence align-
ment and string search algorithms.
Results of the ternary classification task are
shown in Table 2. Results show a similar pattern
to those observed for the binary classification task
56
Approach P R F1
NG 0.836 0.706 0.732
LM-NG 0.846 0.722 0.746
Del 0.851 0.745 0.767
LM-Del 0.858 0.765 0.785
WN 0.876 0.801 0.817
LM-WN 0.879 0.810 0.825
Para 0.884 0.821 0.834
LM-Para 0.888 0.831 0.843
Del+WN 0.889 0.835 0.847
LM-Del+WN 0.884 0.848 0.855
Del+Para 0.892 0.841 0.853
LM-Del+Para 0.896 0.849 0.860
WN+Para 0.894 0.848 0.858
LM-WN+Para 0.896 0.865 0.871
Del+WN+Para 0.897 0.856 0.865
LM-Del+WN+Para 0.903 0.876 0.882
(Clough et al, 2002) ? ? 0.763
Table 1: Results for binary classification
and the best result is also obtained when all three
types of modified n-grams are included and n-grams
are weighted with probability scores. Once again
weighting n-grams with language model scores im-
proves results for all types of n-gram and this im-
provement is significant. Results for several types of
n-gram are also better than those reported by Clough
et al (2002) (F1=0.664).
Results for all approaches are lower for the
ternary classification. This is because the binary
classification task involves distinguishing between
two classes of documents which are relatively dis-
tinct (derived and non-derived) while the ternary
task divides the derived class into two (WD and PD)
which are more difficult to separate (see Table 3
showing confusion matrix for the approach which
gave best results for ternary classification).
6 Conclusion
This paper describes an approach to the analysis of
text reuse which is based on comparison of n-grams.
This approach is augmented by modifying the n-
grams in various ways and weighting them with
probabilities derived from a language model. Evalu-
ation is carried out on a standard data set containing
examples of reused journalistic texts. Making use of
Approach P R F1
NG 0.596 0.557 0.551
LM-NG 0.615 0.579 0.574
Del 0.612 0.584 0.579
LM-Del 0.633 0.611 0.606
WN 0.644 0.636 0.631
LM-WN 0.649 0.640 0.635
Para 0.662 0.653 0.647
LM-Para 0.669 0.659 0.654
Del+WN 0.655 0.649 0.643
LM-Del+WN 0.668 0.656 0.650
Del+Para 0.665 0.658 0.652
LM-Del+Para 0.661 0.662 0.655
WN+Para 0.668 0.661 0.655
LM-WN+Para 0.680 0.675 0.668
Del+WN+Para 0.669 0.666 0.660
LM-Del+WN+Para 0.688 0.689 0.683
(Clough et al, 2002) ? ? 0.664
Table 2: Results for ternary classification
Classified as WD PD ND
WD 139 94 14
PD 57 206 54
ND 1 13 191
Table 3: Confusion matrix when ?LM-Del+WN+Para?
approach used for ternary classification
modified n-grams with appropriate weights is found
to improve performance when detecting text reuse
and the approach described here outperforms an ex-
isting approach. In future we plan to experiment
with other methods for modifying n-grams and also
to apply this approach to other types of text reuse.
Acknowledgments
This work was funded by the COMSATS Institute
of Information Technology, Islamabad, Pakistan un-
der the Faculty Development Program (FDP) and a
Google Research Award.
References
Alberto B. Ceden?o, Paolo Rosso, and Jose M. Bened
2009. Reducing the Plagiarism Detection Search
Space on the basis of the Kullback-Leibler Distance
Proceedings of CICLing-09, 523?534.
57
Allan Bell 1991. The Language of News Media. Black-
well.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
901?904.
Chin-Yew Lin. 2004. Rouge: A Package for Automatic
Evaluation of Summaries. In Proceedings of the ACL-
04 Workshop, 74?81.
Chris Callison-Burch. 2008. Syntactic Constraints on
Paraphrases Extracted from Parallel Corpora. In Pro-
ceedings of EMNLP?08, 196?205.
Jangwon Seo and W. Bruce Croft. 2008. Local Text
Reuse Detection. In Proceedings of SIGIR?08, 571?
578. In Proceedings of the 31st Annual International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, 571?578.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
J. Zhu. 2002. Bleu: A Method for Automatic Eval-
uation of Machine Translation. In Proceedings of
ACL?02, 311?318.
Martin Potthast, Andreas Eiselt, Alberto Barro?n-Ceden?o,
Benno Stein and Paolo Rosso. 2011. Overview of the
3rd International Competition on Plagiarism Detec-
tion. Notebook Papers of CLEF 11 Labs and Work-
shops.
Narayanan Shivakumar and Hector G. Molina. 1995.
SCAM: A Copy Detection Mechanism for Digital Doc-
uments. Proceedings of the 2nd Annual Conference
on the Theory and Practice of Digital Libraries, Texas,
USA.
Paul Clough, Robert Gaizauskas, Scott S.L. Piao, and
Yorick Wilks. 2002. Measuring Text Reuse. In Pro-
ceedings of ACL?02, Philadelphia, USA, 152?159.
Peter C. R. Lane, Caroline M. Lyon, and James A. Mal-
colm. 2006. Demonstration of the Ferret plagiarism
detector. Proceedings of the 2nd International Plagia-
rism Conference, Newcastle, UK.
Robert Gaizauskas, Jonathan Foster, Yorick Wilks, John
Arundel, Paul Clough, and Scott S.L. Piao. 2001. The
METER Corpus: A Corpus for Analysing Journalistic
Text Reuse. In Proceedings of the Corpus Linguistics
Conference, 214?223.
Sergey Brin, James Davis and Hector G. Molina. 1995.
Copy Detection Mechanisms for Digital Documents.
Proceedings ACM SIGMOD?95, 398?409.
Stanford Chiu, Ibrahim Uysal, Bruce W. Croft. 2010.
Evaluating text reuse discovery on the web. In Pro-
ceedings of the third symposium on Information inter-
action in context, 299?304.
Thomas M. Cover, Joy A. Thomas. 1991. Elements of
Information Theory. Wiley, New York, USA.
Timothy C. Hoad and Justin Zobel. 2003. Methods
for Identifying Versioned and Plagiarized Documents.
Journal of the American Society for Information Sci-
ence and Technology, 54(3):203?215.
Tony Rose, Mark Stevenson, Miles Whitehead. 2002.
The Reuters Corpus Volume 1 - from Yesterday?s news
to tomorr ow?s language resources. In Proceedings of
the Third International Conference on Language Re-
sources and Evaluation (LREC-02), 827?832.
58
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 655?661,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
University Of Sheffield: Two Approaches to Semantic Text Similarity
Sam Biggins, Shaabi Mohammed, Sam Oakley,
Luke Stringer, Mark Stevenson and Judita Priess
Department of Computer Science
University of Sheffield
Sheffield
S1 4DP, UK
{aca08sb, aca08sm, coa07so, aca08ls,
r.m.stevenson, j.preiss}@shef.ac.uk
Abstract
This paper describes the University of
Sheffield?s submission to SemEval-2012 Task
6: Semantic Text Similarity. Two approaches
were developed. The first is an unsupervised
technique based on the widely used vector
space model and information from WordNet.
The second method relies on supervised ma-
chine learning and represents each sentence as
a set of n-grams. This approach also makes
use of information from WordNet. Results
from the formal evaluation show that both ap-
proaches are useful for determining the simi-
larity in meaning between pairs of sentences
with the best performance being obtained by
the supervised approach. Incorporating infor-
mation from WordNet alo improves perfor-
mance for both approaches.
1 Introduction
This paper describes the University of Sheffield?s
submission to SemEval-2012 Task 6: Semantic Text
Similarity (Agirre et al, 2012). The task is con-
cerned with determining the degree of semantic
equivalence between a pair of sentences.
Measuring the similarity between sentences is an
important problem that is relevant to many areas
of language processing, including the identification
of text reuse (Seo and Croft, 2008; Bendersky and
Croft, 2009), textual entailment (Szpektor et al,
2004; Zanzotto et al, 2009), paraphrase detection
(Barzilay and Lee, 2003; Dolan et al, 2004), In-
formation Extraction/Question Answering (Lin and
Pantel, 2001; Stevenson and Greenwood, 2005), In-
formation Retrieval (Baeza-Yates and Ribeiro-Neto,
1999), short answer grading (Pulman and Sukkarieh,
2005; Mohler and Mihalcea, 2009), recommenda-
tion (Tintarev and Masthoff, 2006) and evaluation
(Papineni et al, 2002; Lin, 2004).
Many of the previous approaches to measuring the
similarity between texts have relied purely on lexi-
cal matching techniques, for example (Baeza-Yates
and Ribeiro-Neto, 1999; Papineni et al, 2002; Lin,
2004). In these approaches the similarity of texts is
computed as a function of the number of matching
tokens, or sequences of tokens, they contain. How-
ever, this approach fails to identify similarities when
the same meaning is conveyed using synonymous
terms or phrases (for example, ?The dog sat on the
mat? and ?The hound sat on the mat?) or when the
meanings of the texts are similar but not identical
(for example, ?The cat sat on the mat? and ?A dog
sat on the chair?).
Significant amounts of previous work on text
similarity have focussed on comparing the mean-
ings of texts longer than a single sentence, such as
paragraphs or documents (Baeza-Yates and Ribeiro-
Neto, 1999; Seo and Croft, 2008; Bendersky and
Croft, 2009). The size of these texts means that
there is a reasonable amount of lexical items in each
document that can be used to determine similarity
and failing to identify connections between related
terms may not be problematic. The situation is dif-
ferent for the problem of semantic text similarity
where the texts are short (single sentences). There
are fewer lexical items to match in this case, making
it more important that connections between related
terms are identified. One way in which this infor-
mation has been incorporated in NLP systems has
655
been to make use of WordNet to provide informa-
tion about similarity between word meanings, and
this approach has been shown to be useful for com-
puting text similarity (Mihalcea and Corley, 2006;
Mohler and Mihalcea, 2009).
This paper describes two approaches to the se-
mantic text similarity problem that use WordNet
(Miller et al, 1990) to provide information about
relations between word meanings. The two ap-
proaches are based on commonly used techniques
for computing semantic similarity based on lexical
matching. The first is unsupervised while the other
requires annotated data to train a learning algorithm.
Results of the SemEval evaluation show that the su-
pervised approach produces the best overall results
and that using the information provided by WordNet
leads to an improvement in performance.
The remainder of this paper is organised as fol-
lows. The next section describes the two approaches
for computing semantic similarity between pairs of
sentences that were developed. The system submit-
ted for the task is described in Section 3 and its per-
formance in the official evaluation in Section 4. Sec-
tion 5 contains the conclusions and suggestions for
future work.
2 Computing Semantic Text Similarity
Two approaches for computing semantic similar-
ity between sentences were developed. The first
method, described in Section 2.1, is unsupervised. It
uses an enhanced version of the vector space model
by calculating the similarity between word senses,
and then finding the distances between vectors con-
structed using these distances. The second method,
described in Section 2.2, is based on supervised ma-
chine learning and compares sentences based on the
overlap of the n-grams they contain.
2.1 Vector Space Model
The first approach is inspired by the vector space
model (Salton et al, 1975) commonly used to com-
pare texts in Information Retrieval and Natural Lan-
guage Processing (Baeza-Yates and Ribeiro-Neto,
1999; Manning and Schu?tze, 1999; Jurafsky and
Martin, 2009).
2.1.1 Creating vectors
Each sentence is tokenised, stop words removed
and the remaining words lemmatised using NLTK
(Bird et al, 2009). (The WordPunctTokenizer
and WordNetLemmatizer are applied.) Binary
vectors are then created for each sentence.
The similarity between sentences can be com-
puted by comparing these vectors using the cosine
metric. However, this does not take account of
words with similar meanings, such as ?dog? and
?hound? in the sentences ?The dog sat on the mat?
and ?The hound sat on the mat?. To take account
of these similarities WordNet-based similarity mea-
sures are used (Patwardhan and Pedersen, 2006).
Any terms that occur in only one of the sentences
do not contribute to the similarity score since they
will have a 0 value in the binary vector. Any words
with a 0 value in one of the binary vectors are com-
pared with all of the words in the other sentence and
the similarity values computed. The highest similar-
ity value is selected and use to replace the 0 value
in that vector, see Figure 1. (If the similarity score
is below the set threshold of 0.5 then the similarity
value is not used and in these cases the 0 value re-
mains unaltered.) This substitution of 0 values in the
vectors ensures that similarity between words can be
taken account of when computing sentence similar-
ity.
Figure 1: Determining word similarity values for
vectors
Various techniques were explored for determining
the similarity values between words. These are de-
scribed and evaluated in Section 2.1.3.
2.1.2 Computing Sentence Similarity
The similarity between two sentences is com-
puted using the cosine metric. Since the cosine met-
ric is a distance measure, which returns a score of 0
for identical vectors, its complement is used to pro-
656
duce the similarity score. This score is multiplied by
5 in order to generate a score in the range required
for the task.
2.1.3 Computing Word Similarity
The similarity values for the vectors are computed
by first disambiguating each sentence and then ap-
plying a similarity measure. Various approaches for
carrying out these tasks were explored.
Word Sense Disambiguation Two simple and
commonly used techniques for Word Sense
Disambiguation were applied.
Most Frequent Sense (MFS) simply selects
the first sense in WordNet, i.e., the most
common occurring sense for the word.
This approach is commonly used as a
baseline for word sense disambiguation
(McCarthy et al, 2004).
Lesk (1986) chooses a synset by comparing its
definition against the sentence and select-
ing the one with the highest number of
words in common.
Similarity measures WordNet-based similarity
measures have been found to perform well
when used in combination with text similarity
measures (Mihalcea and Corley, 2006) and
several of these were compared. Implementa-
tions of these measures from the NLTK (Bird
et al, 2009) were used.
Path Distance uses the length of the shortest
path between two senses to determine the
similarity between them.
Leacock and Chodorow (1998) expand upon
the path distance similarity measure by
scaling the path length by the maximum
depth of the WordNet taxonomy.
Resnik (1995) makes use of techniques from
Information Theory. The measure of re-
latedness between two concepts is based
on the Information Content of the Least
Common Subsumer.
Jiang and Conrath (1997) also uses the In-
formation Content of the two input
synsets.
Lin (1998) uses the same values as Jiang and
Conrath (1997) but takes the ratio of the
shared information content to that of the
individual concepts.
Results produced by the various combinations of
word sense disambiguation strategy and similarity
measures are shown in Table 1. This table shows
the Pearson correlation of the system output with the
gold standard over all of the SemEval training data.
The row labelled ?Binary? shows the results using
binary vectors which are not augmented with any
similarity values. The remainder of the table shows
the performance of each of the similarity measures
when the senses are selected using the two word
sense disambiguation algorithms.
Metric MFS Lesk
Binary 0.657
Path Distance 0.675 0.669
Leacock and Chodorow (1998) 0.087 0.138
Resnik (1995) 0.158 0.153
Jiang and Conrath (1997) 0.435 0.474
Lin (1998) 0.521 0.631
Table 1: Performance of Vector Space Model us-
ing various disambiguation strategies and similarity
measures
The results in this table show that the only simi-
larity measure that leads to improvement above the
baseline is the path measure. When this is applied
there is a modest improvement over the baseline for
each of the word sense disambiguation algorithms.
However, all other similarity measures lead to a drop
in performance. Overall there seems to be little dif-
ference between the performance of the two word
sense disambiguation algorithms. The best perfor-
mance is obtained using the paths distance and MFS
disambiguation.
Table 2 shows the results of the highest scoring
method broken down by the individual corpora used
for the evaluation. There is a wide range between the
highest (0.726) and lowest (0.485) correlation scores
with the best performance being obtained for the
MSRvid corpus which contains short, simple sen-
tences.
657
Metric Correlation
MSRpar 0.591
MSRvid 0.726
SMTeuroparl 0.485
Table 2: Correlation scores across individual cor-
pora using Path Distance and Most Frequent Sense.
2.2 Supervised Machine Learning
For the second approach the sentences are repre-
sented as sets of n-grams of varying length, a com-
mon approach in text comparison applications which
preserves some information about the structure of
the document. However, like the standard vector
space model (Section 2.1) this technique also fails to
identify similarity between texts when an alternative
choice of lexical item is used to express the same,
or similar, meaning. To avoid this problem Word-
Net is used to generate sets of alternative n-grams.
After the n-grams have been generated for each sen-
tence they are augmented with semantic alternatives
created using WordNet (Section 2.2.1). The overlap
scores between the n-grams from the two sentences
are used as features for a supervised learning algo-
rithm (Section 2.2.2).
2.2.1 Generating n-grams
Preprocessing is carried out using NLTK. Each
sentence is tokenised, lemmatised and stop words
removed. A set of n-grams are then extracted from
each sentence. The set of n-grams for the sentence
S is referred to as So.
For every n-gram in So a list of alternative n-
grams is generated using WordNet. Each item in
the n-gram is considered in turn and checked to de-
termine whether it occurs in WordNet. If it does
then a set of alternative lexical items is constructed
by combining all terms that are found in all synsets
containing that item as well as their immediate hy-
pernyms and hyponyms of the terms. An additional
n-gram is created for each item in this set of alterna-
tive lexical items by substituting each for the origi-
nal term. This set of expanded n-grams is referred to
as Sa.
2.2.2 Sentence Comparison
Overlap metrics to determine the similarity be-
tween the sets of n-grams are used to create features
for the learning algorithm. For two sentences, S1
and S2, four sets of n-grams are compared: S1o,
S2o, S1a and S2a (i.e., the n-grams extracted di-
rectly from sentences S1 and S2 as well as the mod-
ified versions created using WordNet).
The n-grams that are generated using WordNet
(Sa) are not as important as the original n-grams
(So) for determining the similarity between sen-
tences and this is accounted for by generating three
different scores reflecting the overlap between the
two sets of n-grams for each sentence. These scores
can be expressed using the following equations:
|S1o ? S2o|
?
|S1o| ? |S2o|
(1)
|(S1o ? S2a)?(S2o ? S1a)|
?
|(S1o ? S2a)| ? |(S2o ? S1a)|
(2)
|S1a ? S2a|
?
|S1a| ? |S2a|
(3)
Equation 1 is the cosine measure applied to the
two sets of original n-grams, equation 2 compares
the original n-grams in each sentence with the alter-
native n-grams in the other while equation 3 com-
pares the alternative n-grams with each other.
Other features are used in addition to these sim-
ilarity scores: the mean length of S1 and S2, the
difference between the lengths of S1 and S2 and the
corpus label (indicating which part of the SemEval
training data the sentence pair was drawn from). We
found that these additional features substantially in-
crease the performance of our system, particularly
the corpus label.
3 University of Sheffield?s entry for Task 6
Our entry for this task consisted of three runs using
the two approaches described in Section 2.
Run 1: Vector Space Model (VS) The first run
used the unsupervised vector space approach (Sec-
tion 2.1). Comparison of word sense disambiguation
strategies and semantic similarity measures on the
training data showed that the best results were ob-
tained using the Path Distance Measure combined
658
with the Most Frequent Sense approach (see Ta-
bles 1 and 2) and these were used for the official
run. Post evaluation analysis also showed that this
strategy produced the best performance on the test
data.
Run 2: Machine Learning (NG) The second
run used the supervised machine learning approach
(Section 2.2.2). The various parameters used by
this approach were explored using 10-fold cross-
validation applied to the SemEval training data. We
varied the lengths of the n-grams generated, exper-
imented with various pre-processing strategies and
machine learning algorithms. The best performance
was obtained using short n-grams, unigrams and bi-
grams, and these were used for the official run. In-
cluding longer n-grams did not lead to any improve-
ment in performance but created significant com-
putational cost due to the number of alternative n-
grams that were created using WordNet. When
the pre-processing strategies were compared it was
found that the best performance was obtained by ap-
plying both stemming and stop word removal before
creating n-grams and this approach was used in the
official run. The Weka1 LinearRegression al-
gorithm was used for the official run and a single
model was created by training on all of the data pro-
vided for the task.
Run 3: Hybrid (VS + NG) The third run is a
hybrid combination of the two methods. The su-
pervised approach (NG) was used for the three data
sets that had been made available in the training data
(MSRpar, MSRvid and SMT-eur) while the vector
space model (VS) was used for the other two data
sets. This strategy was based on analysis of perfor-
mance of the two approaches on the training data.
The NG approach was found to provide the best
performance. However it was sensitive to the data
set from which the training data was obtained from
while VS, which does not require training data, is
more robust.
A diagram depicting the various components of
the submitted entry is shown in Figure 2.
4 Evaluation
The overall performance (ALLnrm) of NG, VG and
the hybrid systems is significantly higher than the
1http://www.cs.waikato.ac.nz/ml/weka/
Figure 2: System Digram for entry
official baseline (see Table 3). The table also in-
cludes separate results for each of the evaluation
corpora (rows three to seven): the unsupervised VS
model performance is significantly higher than the
baseline (p-value of 0.06) over all corpus types, as is
that of the hybrid model.
However, the performance of the supervised NG
model is below the baseline for the (unseen in train-
ing data) SMT-news corpus. Given a pair of sen-
tences from an unknown source, the algorithm em-
ploys a model trained on all data combined (i.e.,
omits the corpus information), which may resemble
the input (On-WN) or it may not (SMT-news).
After stoplist removal, the average sentence
length within MSRvid is 4.5, whereas it is 6.0 and
6.9 in MSRpar and SMT-eur respectively, and thus
the last two corpora are expected to form better train-
ing data for each other. The overall performance on
the MSRvid data is higher than for the other cor-
pora, which may be due to the small number of ad-
jectives and the simpler structure of the shorter sen-
tences within the corpus.
The hybrid system, which selects the supervised
system (NG)?s output when the test sentence pair
is drawn from a corpus within the training data
659
Corpus Baseline Vector Space (VS) Machine Learning (NG) Hybrid (NG+VS)
ALL .3110 .6054 .7241 .6485
ALLnrm .6732 .7946 .8169 .8238
MSRpar .4334 .5460 .5166 .5166
MSRvid .2996 .7241 .8187 .8187
SMT-eur .4542 .4858 .4859 .4859
On-WN .5864 .6676 .6390 .6676
SMT-news .3908 .4280 .2089 .4280
Table 3: Correlation scores from official SemEval results
Rank (/89) Rank Ranknrm RankMean
Baseline 87 85 70
Vector Space (VS) 48 44 29
Machine Learning (NG) 17 18 37
Hybrid 34 15 20
Table 4: Ranks from official SemEval results
and selects the unsupervised system (VS)?s answer
otherwise, outperforms both systems in combina-
tion. Contrary to expectations, the supervised sys-
tem did not always outperform VS on phrases based
on training data ? the performance of VS on MSR-
par, with its long and complex sentences, proved
to be slightly higher than that of NG. However, the
unsupervised system was clearly the correct choice
when the source was unknown.
5 Conclusion and Future Work
Two approaches for computing semantic similarity
between sentences were explored. The first, unsu-
pervised approach, uses a vector space model and
computes similarity between sentences by compar-
ing vectors while the second is supervised and rep-
resents the sentences as sets of n-grams. Both
approaches used WordNet to provide information
about similarity between lexical items. Results from
evaluation show that the supervised approach pro-
vides the best results on average but also that per-
formance of the unsupervised approach is better for
some data sets. The best overall results for the Se-
mEval evaluation were obtained using a hybrid sys-
tem that attempts to choose the most suitable ap-
proach for each data set.
The results reported here show that the semantic
text similarity task can be successfully approached
using lexical overlap techniques augmented with
limited semantic information derived from Word-
Net. In future, we would like to explore whether
performance can be improved by applying deeper
analysis to provide information about the structure
and semantics of the sentences being compared. For
example, parsing the input sentences would provide
more information about their structure than can be
obtained by representing them as a bag of words or
set of n-grams. We would also like to explore meth-
ods for improving performance of the n-gram over-
lap approach and making it more robust to different
data sets.
Acknowledgements
This research has been supported by a Google Re-
search Award.
References
E. Agirre, D. Cer, M Diab, and A. Gonzalez-Agirre.
2012. Semeval-2012 task 6: A pilot on semantic tex-
tual similarity. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval 2012), in
conjunction with the First Joint Conference on Lexical
and Computational Semantics (*SEM 2012).
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Modern In-
formation Retrieval. Addison Wesley Longman Lim-
ited, Essex.
660
R. Barzilay and L. Lee. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence
alignment. In Proceedings of the 2003 Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics.
M. Bendersky and W.B. Croft. 2009. Finding text reuse
on the web. In Proceedings of the Second ACM Inter-
national Conference on Web Search and Data Mining,
pages 262?271. ACM.
S. Bird, E. Klein, and E. Loper. 2009. Natural Language
Processing with Python. O?Reilly.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Ex-
ploiting massively parallel news sources. In Proceed-
ings of Coling 2004, pages 350?356, Geneva, Switzer-
land.
J.J. Jiang and D.W. Conrath. 1997. Semantic Similarity
Based on Corpus Statistics and Lexical Taxonomy. In
International Conference Research on Computational
Linguistics (ROCLING X).
D. Jurafsky and J. Martin. 2009. Speech and Language
Processing. Pearson, second edition.
C. Leacock and M. Chodorow, 1998. Combining local
context and WordNet similarity for word sense identi-
fication, pages 305?332. In C. Fellbaum (Ed.), MIT
Press.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: how to tell a pine cone
from an ice cream cone. In Proceedings of ACM SIG-
DOC Conference, pages 24?26, Toronto, Canada.
D. Lin and P. Pantel. 2001. Discovery of interence rules
for question answering. Natural Language Engineer-
ing, 7(4):343?360.
D. Lin. 1998. An information-theoretic definition of
similarity. In In Proceedings of the 15th International
Conference on Machine Learning, pages 296?304.
C. Lin. 2004. Rouge: A package for automatic evalu-
ation of summaries. In Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74?
81, Barcelona, Spain, July.
C. Manning and H. Schu?tze. 1999. Foundations of
Statistical Natural Language Processing. MIT Press,
Cambridge, MA.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004.
Finding predominant senses in untagged text. In Pro-
ceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Lingusitics (ACL-2004), pages
280?287, Barcelona, Spain.
R. Mihalcea and C. Corley. 2006. Corpus-based and
knowledge-based measures of text semantic similarity.
In In AAAI06, pages 775?780.
G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K.J. Miller. 1990. WordNet: An On-line Lexi-
cal Database. International Journal of Lexicography,
3(4):235?312.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 567?575,
Athens, Greece.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of 40th Annual Meeting of
the Association for Computational Linguistics, pages
311?318, Philadelphia, Pennsylvania, USA.
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based context vectors to estimate the semantic related-
ness of concept. In Proceedings of the workshop on
?Making Sense of Sense: Bringing Psycholinguistics
and Computational Linguistics Together? held in con-
junction with the EACL 2006, pages 1?8.
S.G. Pulman and J.Z. Sukkarieh. 2005. Automatic
short answer marking. In Proceedings of the Second
Workshop on Building Educational Applications Us-
ing NLP, pages 9?16, Ann Arbor, Michigan.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence, pages 448?453.
G. Salton, A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Commun. ACM,
18(11):613?620.
J. Seo and W.B. Croft. 2008. Local text reuse detection.
In Proceedings of the 31st Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 571?578.
M. Stevenson and M. Greenwood. 2005. A Semantic
Approach to IE Pattern Induction. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL-05), pages 379?386, Ann
Arbour, MI.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004.
Scaling web-based acquisition of entailment relations.
In Proceedings of the 2004 Conference on Empirical
Methods in Natural Language Processing, pages 41?
48, Barcelona, Spain.
N. Tintarev and J. Masthoff. 2006. Similarity for
news recommender systems. In In Proceedings of the
AH?06 Workshop on Recommender Systems and Intel-
ligent User Interfaces.
F.M. Zanzotto, M. Pennacchiotti, and A. Moschitti.
2009. A machine learning approach to textual entail-
ment recognition. Natural Language Engineering, 15-
04:551?582.
661
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 80?84, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Distinguishing Common and Proper Nouns
Judita Preiss and Mark Stevenson
{j.preiss, r.m.stevenson}@sheffield.ac.uk
Department of Computer Science,
University of Sheffield
211 Portobello, Sheffield S1 4DP
United Kingdom
Abstract
We describe a number of techniques for auto-
matically deriving lists of common and proper
nouns, and show that the distinction between
the two can be made automatically using a
vector space model learning algorithm. We
present a direct evaluation on the British Na-
tional Corpus, and application based evalua-
tions on Twitter messages and on automatic
speech recognition (where the system could be
employed to restore case).
1 Introduction
Some nouns are homographs (they have the same
written form, but different meaning) which can be
used to denote either a common or proper noun, for
example the word apple in the following examples:
(1) Apple designs and creates iPod (2) The Apple II
series is a set of 8-bit home computers (3) The apple
is the pomaceous fruit of the apple tree (4) For apple
enthusiasts ? tasting notes and apple identification.
The common and proper uses are not always as
clearly distinct as in this example; for example, a
specific instance of a common noun, e.g., District
Court turns court into a proper noun.
While heuristically, proper nouns often start with
a capital letter in English, capitalization can be in-
consistent, incorrect or omitted, and the presence or
absence of an article cannot be relied on.
The problem of distinguishing between common
and proper usages of nouns has not received much
attention within language processing, despite being
an important component for many tasks including
machine translation (Lopez, 2008; Hermjakob et al,
2008), sentiment analysis (Pang and Lee, 2008; Wil-
son et al, 2009) and topic tracking (Petrovic? et al,
2010). Approaches to the problem also have appli-
cations to tasks such as web search (Chen et al,
1998; Baeza-Yates and Ribeiro-Neto, 2011), and
case restoration (e.g., in automatic speech recogni-
tion output) (Baldwin et al, 2009), but frequently
involve the manual creation of a gazeteer (a list of
proper nouns), which suffer not only from omissions
but also often do not allow the listed words to as-
sume their common role in text.
This paper presents methods for generating lists
of nouns that have both common and proper usages
(Section 2) and methods for identifying the type of
usage (Section 3) which are evaluated using data de-
rived automatically from the BNC (Section 4) and
on two applications (Section 5). It shows that it is
difficult to automatically construct lists of ambigu-
ous nouns but also that they can be distinguished ef-
fectively using standard features from Word Sense
Disambiguation.
2 Generating Lists of Nouns
To our knowledge, no comprehensive list of com-
mon nouns with proper noun usage is available. We
develop a number of heuristics to generate such lists
automatically.
Part of speech tags A number of part of speech
(PoS) taggers assign different tags to common and
proper nouns. Ambiguous nouns are identified by
tagging a corpus and extracting those that have
had both tags assigned, together with the frequency
of occurrence of the common/proper usage. The
CLAWS (Garside, 1987) and the RASP taggers
80
(Briscoe et al, 2006) were applied to the British Na-
tional Corpus (BNC) (Leech, 1992) to generate the
lists BNCclaws and BNCrasp respectively. In addi-
tion the RASP tagger was also run over the 1.75 bil-
lion word Gigaword corpus (Graff, 2003) to extract
the list Gigaword.
Capitalization Nouns appearing intra-
sententially with both lower and upper case
first letters are assumed to be ambiguous. This
technique is applied to the 5-grams from the Google
corpus (Brants and Franz, 2006) and the BNC
(creating the lists 5-grams and BNCcaps).
Wikipedia includes disambiguation pages for
ambiguous words which provide information about
their potential usage. Wikipedia pages for nouns
with senses (according to the disambiguation page)
in a set of predefined categories were identified to
form the list Wikipedia.
Named entity recognition The Stanford Named
Entity Recogniser (Finkel et al, 2005) was run over
the BNC and any nouns that occur in the corpus with
both named entity and non-named entity tags are ex-
tracted to form the list Stanford.
WordNet The final heuristic makes use of Word-
Net (Fellbaum, 1998) which lists nouns that are of-
ten used as proper nouns with capitalisation. Nouns
which appeared in both a capitalized and lowercased
form were extracted to create the list WordNet.
Table 1 shows the number of nouns identified by
each technique in the column labeled words which
demonstrates that the number of nouns identified
varies significantly depending upon which heuris-
tic is used. A pairwise score is also shown to in-
dicate the consistency between each list and two ex-
ample lists, BNCclaws and Gigaword. It can be seen
that the level of overlap is quite low and the various
heuristics generate quite different lists of nouns. In
particular the recall is low, in almost all cases less
than a third of nouns in one list appear in the other.
One possible reason for the low overlap between
the noun lists is mistakes by the heuristics used to
extract them. For example, if a PoS tagger mistak-
enly tags just one instance of a common noun as
proper then that noun will be added to the list ex-
tracted by the part of speech heuristic. Two filter-
ing schemes were applied to improve the accuracy of
the lists: (1) minimum frequency of occurrence, the
noun must appear more than a set number of times
words BNCclaws Gigaword
P R P R
BNCclaws 41,110 100 100 31 2
BNCrasp 20,901 52 27 45 17
BNCcaps 18,524 56 26 66 21
5-grams 27,170 45 29 59 28
Gigaword 57,196 22 31 100 100
Wikipedia 7,351 49 9 59 8
WordNet 798 75 1 68 1
Stanford 64,875 43 67 26 29
Table 1: Pairwise comparison of lists. The nouns in each
list are compared against the BNCclaws and Gigaword
lists. Results are computed for P(recision) and R(ecall).
in the corpus and (2) bias, the least common type of
noun usage (i.e., common or proper) must account
for more than a set percentage of all usages.
We experimented with various values for these fil-
ters and a selection of results is shown in Table 2,
where freq is the minimum frequency of occurrence
filter and bias indicates the percentage of the less
frequent noun type.
bias freq words BNCclaws Gigaword
P R P R
BNCclaws 40 100 274 100 1 53 1
BNCrasp 30 100 253 94 1 85 0
5-grams 40 150 305 80 1 67 0
Stanford 40 200 260 87 1 47 0
Table 2: Pairwise comparison of lists with filtering
Precision (against BNCclaws) increased as the fil-
ters become more aggressive. However comparison
with Gigaword does not show such high precision
and recall is extremely low in all cases.
These experiments demonstrate that it is difficult
to automatically generate a list of nouns that exhibit
both common and proper usages. Manual analy-
sis of the lists generated suggest that the heuristics
can identify ambiguous nouns but intersecting the
lists results in the loss of some obviously ambigu-
ous nouns (however, their union introduces a large
amount of noise). We select nouns from the lists
created by these heuristics (such that the distribu-
tion of either the common or proper noun sense in
the data was not less than 45%) for experiments in
the following sections.1
1The 100 words selected for our evaluation are available at
http://pastehtml.com/view/cjsbs4xvl.txt
81
3 Identifying Noun Types
We cast the problem of distinguishing between com-
mon and proper usages of nouns as a classification
task and develop the following approaches.
3.1 Most frequent usage
A naive baseline is supplied by assigning each word
its most frequent usage form (common or proper
noun). The most frequent usage is derived from the
training portion of labeled data.
3.2 n-gram system
A system based on n-grams was implemented using
NLTK (Bird et al, 2009). Five-grams, four-grams,
trigrams and bigrams from the training corpus are
matched against a test corpus sentence, and results
of each match are summed to yield a preferred use in
the given context with a higher weight (experimen-
tally determined) being assigned to longer n-grams.
The system backs off to the most frequent usage (as
derived from the training data).
3.3 Vector Space Model (VSM)
Distinguishing between common and proper nouns
can be viewed as a classification problem. Treating
the problem in this manner is reminiscent of tech-
niques commonly employed in Word Sense Disam-
biguation (WSD). Our supervised approach is based
on an existing WSD system (Agirre and Martinez,
2004) that uses a wide range of features:
? Word form, lemma or PoS bigrams and tri-
grams containing the target word.
? Preceding or following lemma (or word form)
content word appearing in the same sentence as
the target word.
? High-likelihood, salient, bigrams.
? Lemmas of all content words in the same sen-
tence as the target word.
? Lemmas of all content words within a?4 word
window of the target word.
? Non stopword lemmas which appear more than
twice throughout the corpus.
Each occurrence of a common / proper noun is
represented as a binary vector in which each position
indicates the presence or absence of a feature. A
centroid vector is created during the training phase
for the common noun and the proper noun instances
of a word. During the test phase, the centroids are
compared to the vector of each test instance using
the cosine metric, and the word is assigned the type
of the closest centroid.
4 Evaluation
The approaches described in the previous section are
evaluated on two data sets extracted automatically
from the BNC. The BNC-PoS data set is created
using the output from the CLAWS tagger. Nouns
assigned the tag NP0 are treated as proper nouns
and those assigned any other nominal tag as com-
mon nouns. (According to the BNC manual the
NP0 tag has a precision 83.99% and recall 97.76%.2)
This data set consists of all sentences in the BNC in
which the target word appears. The second data set,
BNC-Capital, is created using capitalisation infor-
mation and consists of instances of the target noun
that do not appear sentence-initially. Any instances
that are capitalised are treated as proper nouns and
those which are non-capitalised as common nouns.
Experiments were carried out using capitalised
and decapitalized versions of the two test corpora.
The decapitalised versions by lowercasing each cor-
pus and using it for training and testing. Results are
presented in Table 3. Ten fold cross validation is
used for all experiments: i.e. 9/10th of the corpus
were used to acquire the training data centroids and
1/10th was used for evaluation. The average perfor-
mance over the 10 experiments is reported.
The vector space model (VSM) outperforms other
approaches on both corpora. Performance is partic-
ularly high when capitalisation is included (VSM w
caps). However, this approach still outperforms the
baseline without case information (VSM w/o caps),
demonstrating that using this simple approach is less
effective than making use of local context.
2No manual annotation of common and proper nouns in this
corpus exists and thus an exact accuracy figure for this corpus
cannot be obtained.
82
Gold standard
BNC-PoS BNC-Capital
Most frequent 79% 67%
n-gram w caps 80% 77%
n-gram w/o caps 68% 56%
VSM w caps 90% 100%
VSM w/o caps 86% 80%
Table 3: BNC evaluation results
5 Applications
We also carried out experiments on two types of
text in which capitalization information may not be
available: social media and ASR output.
5.1 Twitter
As demonstrated in the BNC based evaluations, the
system can be applied to text which does not contain
capitalization information to identify proper nouns
(and, as a side effect, enable the correction of capi-
talization). An example of such a dataset are the (up
to) 140 character messages posted on Twitter.
There are some interesting observations to be
made on messages downloaded from Twitter. Al-
though some users choose to always tweet in lower
case, the overall distribution of capitalization in
tweets is high for the 100 words selected in Section 2
and only 3.7% of the downloaded tweets are entirely
lower case. It also appeared that users who capital-
ize, do so fairly consistently.
This allows the creation of a dataset based on
downloaded Twitter data3:
1. Identify purely lower case tweets containing
the target word. These will form the test data
(and are manually assigned usage).
2. Any non-sentence initial occurrences of the tar-
get word are used as training instances: lower
case indicating a common instance, upper case
indicating a proper instance.
14 words4 were randomly selected from the list
used in Section 4 and their lowercase tweet instances
were manually annotated by a single annotator. The
3http://search.twitter.com/api
4abbot, bull, cathedral, dawn, herald, justice, knight, lily,
lodge, manor, park, president, raven and windows
Training corpus MF n-grams VSM
Twitter 59% 40% 60%
BNCclaw decap 59% 44% 79%
Table 4: Results on the Twitter data
average proportion of proper nouns in the test data
was 59%.
The results for the three systems are presented in
Table 4. As the length of the average sentence in the
Twitter data is only 15 words (compared to 27 words
in the BNCclaws data for the same target words),
the Twitter data is likely to be suffering sparseness
issues. This hypothesis is partly supported by the in-
crease in performance when the BNCclaws decapi-
talized data is added to the training data, however,
the performance of the n-gram system remains be-
low the most frequent use. On closer examination,
this is likely due to the skew in the data ? there are
many more examples for the common use of each
noun, and thus each context is much more likely to
have been seen in this setting.
5.2 Automatic speech recognition
Most automatic speech recognition (ASR) systems
do not provide capitalization. However, our sys-
tem does not rely on capitalization information, and
therefore can identify proper / common nouns even
if capitalization is absent. Also, once proper nouns
are identified, the system can be used to restore case
? a feature which allows an evaluation to take place
on this dataset. We use the TDT2 Test and Speech
corpus (Cieri et al, 1999), which contains ASR and
a manually transcribed version of news texts from
six different sources, to demonstrate the usefulness
of this system for this task.
The ASR corpus is restricted to those segments
which contain an equal number of target word oc-
currences in the ASR text and the manually tran-
scribed version, and all such segments are extracted.
The gold standard, and the most frequent usage, are
drawn from the manually transcribed data.
Again, results are based on an average perfor-
mance obtained using a ten fold cross validation.
Three versions of training data are used: the 9/10 of
ASR data (with labels provided by the manual tran-
scription), the equivalent 9/10 of lowercased manu-
83
Training corpus MF n-grams VSM
Manual 66% 42% 73%
ASR 63% 41% 79%
Table 5: Results on the ASR data
ally transcribed data, and a combination of the two.
The results can be seen in Table 5. The perfor-
mance rise obtained with the VSM model when the
ASR data is used is likely due to the repeated errors
within this, which will not be appearing in the man-
ually transcribed texts. The n-gram performance is
greatly affected by the low volume of training data
available, and again, a large skew within this.
6 Conclusion
We automatically generate lists of common and
proper nouns using a number of different techniques.
A vector space model technique for distinguish-
ing common and proper nouns is found to achieve
high performance when evaluated on the BNC. This
greatly outperforms a simple n-gram based system,
due to its better adaptability to sparse training data.
Two application based evaluations also demonstrate
the system?s performance and as a side effect the
system could serve as a technique for automatic case
restoration.
Acknowledgments
The authors are grateful to the funding for this
research received from Google (Google Research
Award) and the UK Engineering and Physical Sci-
ences Research Council (EP/J008427/1).
References
Agirre, E. and Martinez, D. (2004). The Basque Coun-
try University system: English and Basque tasks.
In Senseval-3: Third International Workshop on the
Evaluation of Systems for the Semantic Analysis of
Text, pages 44?48.
Baeza-Yates, R. and Ribeiro-Neto, B. (2011). Modern
Information Retrieval: The Concepts and Technology
Behind Search. Addison Wesley Longman Limited,
Essex.
Baldwin, T., Paul, M., and Joseph, A. (2009). Restoring
punctuation and casing in English text. In Proceedings
of the 22nd Australian Joint Conference on Artificial
Intelligence (AI09), pages 547?556.
Bird, S., Klein, E., and Loper, E. (2009). Natural Lan-
guage Processing with Python ? Analyzing Text with
the Natural Language Toolkit. O?Reilly.
Brants, T. and Franz, A. (2006). Web 1T 5-gram v1.
Briscoe, T., Carroll, J., and Watson, R. (2006). The sec-
ond release of the RASP system. In Proceedings of the
COLING/ACL 2006 Interactive Presentation Sessions.
Chen, H., Huang, S., Ding, Y., and Tsai, S. (1998).
Proper name translation in cross-language information
retrieval. In Proceedings of the 36th Annual Meeting
of the Association for Computational Linguistics and
17th International Conference on Computational Lin-
guistics, Volume 1, pages 232?236, Montreal, Canada.
Cieri, C., Graff, D., Liberman, M., Martey, N., and
Strassel, S. (1999). The TDT-2 text and speech cor-
pus. In Proceedings of DARPA Broadcast News Work-
shop, pages 57?60.
Fellbaum, C., editor (1998). WordNet: An Electronic
Lexical Database and some of its Applications. MIT
Press, Cambridge, MA.
Finkel, J. R., Grenager, T., and Manning, C. (2005). In-
corporating non-local information into information ex-
traction systems by Gibbs sampling. In Proceedings of
the 43nd Annual Meeting of the Association for Com-
putational Linguistics, pages 363?370.
Garside, R. (1987). The CLAWS word-tagging system.
In Garside, R., Leech, G., and Sampson, G., editors,
The Computational Analysis of English: A Corpus-
based Approach. London: Longman.
Graff, D. (2003). English Gigaword. Technical report,
Linguistic Data Consortium.
Hermjakob, U., Knight, K., and Daume? III, H. (2008).
Name translation in statistical machine translation -
learning when to transliterate. In Proceedings of ACL-
08: HLT, pages 389?397, Columbus, Ohio.
Leech, G. (1992). 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
Lopez, A. (2008). Statistical machine translation. ACM
Computing Surveys, 40(3):1?49.
Pang, B. and Lee, L. (2008). Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, Vol. 2(1-2):pp. 1?135.
Petrovic?, S., Osborne, M., and Lavrenko, V. (2010).
Streaming first story detection with application to twit-
ter. In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
181?189, Los Angeles, California.
Wilson, T., Wiebe, J., and Hoffman, P. (2009). Recogniz-
ing contextual polarity: an exploration of features for
phrase-level sentiment analysis. Computational Lin-
guistics, 35(5).
84
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 132?137, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UBC UOS-TYPED: Regression for Typed-similarity
Eneko Agirre
University of the Basque Country
Donostia, 20018, Basque Country
e.agirre@ehu.es
Nikolaos Aletras
University of Sheffield
Sheffield, S1 4DP, UK
n.aletras@dcs.shef.ac.uk
Aitor Gonzalez-Agirre
University of the Basque Country
Donostia, 20018, Basque Country
agonzalez278@ikasle.ehu.es
German Rigau
University of the Basque Country
Donostia, 20018, Basque Country
german.rigau@ehu.es
Mark Stevenson
University of Sheffield
Sheffield, S1 4DP, UK
m.stevenson@dcs.shef.ac.uk
Abstract
We approach the typed-similarity task using
a range of heuristics that rely on information
from the appropriate metadata fields for each
type of similarity. In addition we train a linear
regressor for each type of similarity. The re-
sults indicate that the linear regression is key
for good performance. Our best system was
ranked third in the task.
1 Introduction
The typed-similarity dataset comprises pairs of Cul-
tural Heritage items from Europeana1, a single ac-
cess point to digitised versions of books, paintings,
films, museum objects and archival records from in-
stitutions throughout Europe. Typically, the items
comprise meta-data describing a cultural heritage
item and, sometimes, a thumbnail of the item itself.
Participating systems need to compute the similarity
between items using the textual meta-data. In addi-
tion to general similarity, the dataset includes spe-
cific kinds of similarity, like similar author, similar
time period, etc.
We approach the problem using a range of sim-
ilarity techniques for each similarity types, these
make use of information contained in the relevant
meta-data fields.In addition, we train a linear regres-
sor for each type of similarity, using the training data
provided by the organisers with the previously de-
fined similarity measures as features.
We begin by describing our basic system in Sec-
tion 2, followed by the machine learning system in
1http://www.europeana.eu/
Section 3. The submissions are explained in Section
4. Section 5 presents our results. Finally, we draw
our conclusions in Section 6.
2 Basic system
The items in this task are taken from Europeana.
They cannot be redistributed, so we used the urls
and scripts provided by the organizers to extract the
corresponding metadata. We analysed the text in the
metadata, performing lemmatization, PoS tagging,
named entity recognition and classification (NERC)
and date detection using Stanford CoreNLP (Finkel
et al, 2005; Toutanova et al, 2003). A preliminary
score for each similarity type was then calculated as
follows:
? General: cosine similarity of TF.IDF vectors of
tokens, taken from all fields.
? Author: cosine similarity of TF.IDF vectors of
dc:Creator field.
? People involved, time period and location:
cosine similarity of TF.IDF vectors of loca-
tion/date/people entities recognized by NERC
in all fields.
? Events: cosine similarity of TF.IDF vectors of
event verbs and nouns. A list of verbs and
nouns possibly denoting events was derived us-
ing the WordNet Morphosemantic Database2.
? Subject and description: cosine similarity of
TF.IDF vectors of respective fields.
IDF values were calculated using a subset of Eu-
ropeana items (the Culture Grid collection), avail-
able internally. These preliminary scores were im-
2urlhttp://wordnetcode.princeton.edu/standoff-
files/morphosemantic-links.xls
132
proved using TF.IDF based on Wikipedia, UKB
(Agirre and Soroa, 2009) and a more informed time
similarity measure. We describe each of these pro-
cesses in turn.
2.1 TF.IDF
A common approach to computing document sim-
ilarity is to represent documents as Bag-Of-Words
(BOW). Each BOW is a vector consisting of the
words contained in the document, where each di-
mension corresponds to a word, and the weight is
the frequency in the corresponding document. The
similarity between two documents can be computed
as the cosine of the angle between their vectors. This
is the approached use above.
This approach can be improved giving more
weight to words which occur in only a few docu-
ments, and less weight to words occurring in many
documents (Baeza-Yates and Ribeiro-Neto, 1999).
In our system, we count document frequencies of
words using Wikipedia as a reference corpus since
the training data consists of only 750 items associ-
ated with short textual information and might not be
sufficient for reliable estimations. The TF.IDF sim-
ilarity between items a and b is defined as:
simtf.idf(a, b) =
?
w?a,b tfw,a ? tfw,b ? idf
2
w
??
w?a(tfw,a ? idfw)
2 ?
??
w?b(tfw,b ? idfw)
2
where tfw,x is the frequency of the term w in x ?
{a, b} and idfw is the inverted document frequency
of the word w measured in Wikipedia. We substi-
tuted the preliminary general similarity score by the
obtained using the TF.IDF presented in this section.
2.2 UKB
The semantic disambiguation UKB3 algorithm
(Agirre and Soroa, 2009) applies personalized
PageRank on a graph generated from the English
WordNet (Fellbaum, 1998), or alternatively, from
Wikipedia. This algorithm has proven to be very
competitive in word similarity tasks (Agirre et al,
2010).
To compute similarity using UKB we represent
WordNet as a graph G = (V,E) as follows: graph
nodes represent WordNet concepts (synsets) and
3http://ixa2.si.ehu.es/ukb/
dictionary words; relations among synsets are rep-
resented by undirected edges; and dictionary words
are linked to the synsets associated to them by di-
rected edges.
Our method is provided with a pair of vectors of
words and a graph-based representation of WordNet.
We first compute the personalized PageRank over
WordNet separately for each of the vector of words,
producing a probability distribution over WordNet
synsets. We then compute the similarity between
these two probability distributions by encoding them
as vectors and computing the cosine between the
vectors. We present each step in turn.
Once personalized PageRank is computed, it
returns a probability distribution over WordNet
synsets. The similarity between two vectors of
words can thus be implemented as the similarity be-
tween the probability distributions, as given by the
cosine between the vectors.
We used random walks to compute improved sim-
ilarity values for author, people involved, location
and event similarity:
? Author: UKB over Wikipedia using person en-
tities recognized by NERC in the dc:Creator
field.
? People involved and location: UKB over
Wikipedia using people/location entities recog-
nized by NERC in all fields.
? Events: UKB over WordNet using event nouns
and verbs recognized in all fields.
Results on the training data showed that perfor-
mance using this approach was quite low (with the
exception of events). This was caused by the large
number of cases where the Stanford parser did not
find entities which were in Wikipedia. With those
cases on mind, we combined the scores returned by
UKB with the similarity scores presented in Section
2 as follows: if UKB similarity returns a score, we
multiply both, otherwise we return the square of the
other similarity score. Using the multiplication of
the two scores, the results on the training data im-
proved.
2.3 Time similarity measure
In order to measure the time similarity between a
pair of items, we need to recognize time expres-
sions in both items. We assume that the year of
133
creation or the year denoting when the event took
place in an artefact are good indicators for time sim-
ilarity. Therefore, information about years is ex-
tracted from each item using the following pattern:
[1|2][0 ? 9]{3}. Using this approach, each item is
represented as a set of numbers denoting the years
mentioned in the meta-data.
Time similarity between two items is computed
based on the similarity between their associated
years. Similarity between two years is defined as:
simyear(y1, y2) = max{0, 1? |y1? y2| ? k}
k is a parameter to weight the difference between
two years, e.g. for k = 0.1 all items that have differ-
ence of 10 years or more assigned a score of 0. We
obtained best results for k = 0.1.
Finally, time similarity between items a and b is
computed as the maximum of the pairwise similarity
between their associated years:
simtime(a, b) = max?i?a
?j?b
{0, simyear(ai, bj)}
We substituted the preliminary time similarity
score by the measure obtained using the method pre-
sented in this section.
3 Applying Machine Learning
The above heuristics can be good indicators for the
respective kind of similarity, and can be thus applied
directly to the task. In this section, we take those
indicators as features, and use linear regression (as
made available by Weka (Hall et al, 2009)) to learn
models that fit the features to the training data.
We generated further similarity scores for gen-
eral similarity, including Latent Dirichlet Allocation
(LDA) (Blei et al, 2003), UKB and Wikipedia Link
Vector Model (WLVM)(Milne, 2007) using infor-
mation taken from all fields, as explained below.
3.1 LDA
LDA (Blei et al, 2003) is a statistical method that
learns a set of latent variables called topics from a
training corpus. Given a topic model, documents
can be inferred as probability distributions over top-
ics, ?. The distribution for a document i is denoted
as ?i. An LDA model is trained using the train-
ing set consisting of 100 topics using the gensim
package4. The hyperparameters (?, ?) were set to
1
num of topics . Therefore, each item in the test set is
represented as a topic distribution.
The similarity between a pair of items is estimated
by comparing their topic distributions following the
method proposed in Aletras et al (2012; Aletras and
Stevenson (2012). This is achieved by considering
each distribution as a vector (consisting of the topics
corresponding to an item and its probability) then
computing the cosine of the angle between them, i.e.
simLDA(a, b) =
~?a ? ~?b
|~?a| ? | ~?b|
where ~?a is the vector created from the probability
distribution generated by LDA for item a.
3.2 Pairwise UKB
We run UKB (Section 2.2) to generate a probabil-
ity distribution over WordNet synsets for all of the
words of all items. Similarity between two words
is computed by creating vectors from these distri-
butions and comparing them using the cosine of the
angle between the two vectors. If a words does not
appear in WordNet its similarity value to every other
word is set to 0. We refer to that similarity metric as
UKB here.
Similarity between two items is computed by per-
forming pairwise comparison between their words,
for each, selecting the highest similarity score:
sim(a, b) =
1
2
(?
w1?a
argmaxw2?b UKB(w1, w2)
|a|
+
?
w2?b
argmaxw1?a UKB(w2, w1)
|b|
)
where a and b are two items, |a| the number of
tokens in a and UKB(w1, w2) is the similarity be-
tween words w1 and w2.
3.3 WLVM
An algorithm described by Milne and Witten (2008)
associates Wikipedia articles which are likely to be
relevant to a given text snippet using machine learn-
ing techniques. We make use of that method to rep-
resent each item as a set of likely relevant Wikipedia
4http://pypi.python.org/pypi/gensim
134
articles. Then, similarity between Wikipedia arti-
cles is measured using the Wikipedia Link Vector
Model (WLVM) (Milne, 2007). WLVM uses both
the link structure and the article titles of Wikipedia
to measure similarity between two Wikipedia arti-
cles. Each link is weighted by the probability of it
occurring. Thus, the value of the weight w for a link
x? y between articles x and y is:
w(x? y) = |x? y| ? log
(
t?
z=1
t
z ? y
)
where t is the total number of articles in Wikipedia.
The similarity of articles is compared by forming
vectors of the articles which are linked from them
and computing the cosine of their angle. For exam-
ple the vectors of two articles x and y are:
x = (w(x? l1), w(x? l2), ..., w(x? ln))
y = (w(y ? l1), w(y ? l2), ..., w(y ? ln))
where x and y are two Wikipedia articles and x? li
is a link from article x to article li.
Since the items have been mapped to Wikipedia
articles, similarity between two items is computed
by performing pairwise comparison between articles
using WLVM, for each, selecting the highest simi-
larity score:
sim(a, b) =
1
2
(?
w1?a
argmaxw2?bWLVM(w1, w2)
|a|
+
?
w2?b
argmaxw1?aWLVM(w2, w1)
|b|
)
where a and b are two items, |a| the number of
Wikipedia articles in a and WLVM(w1, w2) is the
similarity between concepts w1 and w2.
4 Submissions
We selected three systems for submission. The first
run uses the similarity scores of the basic system
(Section 2) for each similarity types as follows:
? General: cosine similarity of TF.IDF vectors,
IDF based on Wikipedia (as shown in Section
2.1).
? Author: product of the scores obtained ob-
tained using TF.IDF vectors and UKB (as
shown in Section 2.2) using only the data ex-
tracted from dc:Creator field.
? People involved and location: product of co-
sine similarity of TF.IDF vectors and UKB (as
shown in Section 2.2) using the data extracted
from all fields.
? Time period: time similarity measure (as
shown in Section 2.3).
? Events: product of cosine similarity of TF.IDF
vectors and UKB (as shown in Section 2.2) of
event nouns and verbs recognized in all fields.
? Subject and description: cosine similarity of
TF.IDF vectors of respective fields (as shown
in Section 2).
For the second run we trained a ML model for
each of the similarity types, using the following fea-
tures:
? Cosine similarity of TF.IDF vectors as shown
in Section 2 for the eight similarity types.
? Four new values for general similarity: TF.IDF
(Section 2.1), LDA (Section 3.1), UKB and
WLVM (Section 3.3).
? Time similarity as shown in Section 2.3.
? Events similarity computed using UKB initial-
ized with the event nouns and verbs in all fields.
We decided not to use the product of TF.IDF
and UKB presented in Section 2.2 in this system
because our intention was to measure the power of
the linear regression ML algorithm to learn on the
given raw data.
The third run is similar, but includes all available
features (21). In addition to the above, we included:
? Author, people involved and location similar-
ity computed using UKB initialized with peo-
ple/location recognized by NERC in dc:Creator
field for author, and in all fields for people in-
volved and location.
? Author, people involved, location and event
similarity scores computed by the product of
TF.IDF vectors and UKB values as shown in
Section 2.2.
5 Results
Evaluation was carried out using the official scorer
provided by the organizers, which computes the
Pearson Correlation score for each of the eight sim-
ilarity types plus an additional mean correlation.
135
Team and run General Author People involved Time Location Event Subject Description Mean
UBC UOS-RUN1 0.7269 0.4474 0.4648 0.5884 0.4801 0.2522 0.4976 0.5389 0.5033
UBC UOS-RUN2 0.7777 0.6680 0.6767 0.7609 0.7329 0.6412 0.7516 0.8024 0.7264
UBC UOS-RUN3 0.7866 0.6941 0.6965 0.7654 0.7492 0.6551 0.7586 0.8067 0.7390
Table 1: Results of our systems on the training data, using cross-validation when necessary.
Team and run General Author People involved Time Location Event Subject Description Mean Rank
UBC UOS-RUN1 0.7256 0.4568 0.4467 0.5762 0.4858 0.3090 0.5015 0.5810 0.5103 6
UBC UOS-RUN2 0.7457 0.6618 0.6518 0.7466 0.7244 0.6533 0.7404 0.7751 0.7124 4
UBC UOS-RUN3 0.7461 0.6656 0.6544 0.7411 0.7257 0.6545 0.7417 0.7763 0.7132 3
Table 2: Results of our submitted systems.
5.1 Development
The three runs mentioned above were developed us-
ing the training data made available by the organiz-
ers. In order to avoid overfitting we did not change
the default parameters of the linear regressor, and
10-fold cross-validation was used for evaluating the
models on the training data. The results of our sys-
tems on the training data are shown on Table 1. The
table shows that the heuristics (RUN1) obtain low
results, and that linear regression improves results
considerably in all types. Using the full set of fea-
tures, RUN3 improves slightly over RUN2, but the
improvement is consistent across all types.
5.2 Test
The test dataset was composed of 750 pairs of items.
Table 2 illustrates the results of our systems in the
test dataset. The results of the runs are very similar
to those obtained on the training data, but the dif-
ference between RUN2 and RUN3 is even smaller.
Our systems were ranked #3 (RUN 3), #4 (RUN
2) and #6 (RUN 1) among 14 systems submitted
by 6 teams. Our systems achieved good correlation
scores for almost all similarity types, with the excep-
tion of author similarity, which is the worst ranked
in comparison with the rest of the systems.
6 Conclusions and Future Work
In this paper, we presented the systems submitted
to the *SEM 2013 shared task on Semantic Tex-
tual Similarity. We combined some simple heuris-
tics for each type of similarity, based on the appro-
priate metadata fields. The use of lineal regression
improved the results considerably across all types.
Our system fared well in the competition. We sub-
mitted three systems and the highest-ranked of these
achieved the third best results overall.
Acknowledgements
This work is partially funded by the PATHS
project (http://paths-project.eu) funded by the Eu-
ropean Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
no. 270082. Aitor Gonzalez-Agirre is supported by
a PhD grant from the Spanish Ministry of Education,
Culture and Sport (grant FPU12/06243).
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Proceed-
ings of the 12th conference of the European chapter of
the Association for Computational Linguistics (EACL-
2009), Athens, Greece.
Eneko Agirre, Montse Cuadros, German Rigau, and Aitor
Soroa. 2010. Exploring knowledge bases for sim-
ilarity. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC10). European Language Resources Associa-
tion (ELRA). ISBN: 2-9517408-6-7. Pages 373?377.?.
Nikolaos Aletras and Mark Stevenson. 2012. Computing
similarity between cultural heritage items using multi-
modal features. In Proceedings of the 6th Workshop
on Language Technology for Cultural Heritage, So-
cial Sciences, and Humanities, pages 85?93, Avignon,
France.
Nikolaos Aletras, Mark Stevenson, and Paul Clough.
2012. Computing similarity between items in a digi-
tal library of cultural heritage. J. Comput. Cult. Herit.,
5(4):16:1?16:19, December.
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Modern In-
formation Retrieval. Addison Wesley Longman Lim-
ited, Essex.
136
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022, March.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 363?370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
D. Milne and I. Witten. 2008. Learning to Link
with Wikipedia. In Proceedings of the ACM Con-
ference on Information and Knowledge Management
(CIKM?2008), Napa Valley, California.
D. Milne. 2007. Computing semantic relatedness using
Wikipedia?s link structure. In Proceedings of the New
Zealand Computer Science Research Student Confer-
ence.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 173?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
137
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 55?63,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Improving Summarization of Biomedical Documents
using Word Sense Disambiguation
Laura Plaza?
lplazam@fdi.ucm.es
Mark Stevenson?
m.stevenson@dcs.shef.ac.uk
? Universidad Complutense de Madrid, C/Prof. Jose? Garc??a Santesmases, 28040 Madrid, Spain
? University of Sheffield, Regent Court, 211 Portobello St., Sheffield, S1 4DP, UK
Alberto D??az?
albertodiaz@fdi.ucm.es
Abstract
We describe a concept-based summariza-
tion system for biomedical documents and
show that its performance can be improved
using Word Sense Disambiguation. The
system represents the documents as graphs
formed from concepts and relations from
the UMLS. A degree-based clustering al-
gorithm is applied to these graphs to dis-
cover different themes or topics within
the document. To create the graphs, the
MetaMap program is used to map the
text onto concepts in the UMLS Metathe-
saurus. This paper shows that applying a
graph-based Word Sense Disambiguation
algorithm to the output of MetaMap im-
proves the quality of the summaries that
are generated.
1 Introduction
Extractive text summarization can be defined as
the process of determining salient sentences in a
text. These sentences are expected to condense
the relevant information regarding the main topic
covered in the text. Automatic summarization of
biomedical texts may benefit both health-care ser-
vices and biomedical research (Reeve et al, 2007;
Hunter and Cohen, 2006). Providing physicians
with summaries of their patient records can help
to reduce the diagnosis time. Researchers can use
summaries to quickly determine whether a docu-
ment is of interest without having to read it all.
Summarization systems usually work with a
representation of the document consisting of in-
formation that can be directly extracted from the
document itself (Erkan and Radev, 2004; Mihalcea
and Tarau, 2004). However, recent studies have
demonstrated the benefit of summarization based
on richer representations that make use of external
knowledge sources (Plaza et al, 2008; Fiszman et
al., 2004). These approaches can represent seman-
tic associations between the words and terms in the
document (i.e. synonymy, hypernymy, homonymy
or co-occurrence) and use this information to im-
prove the quality of the summaries. In the biomed-
ical domain the Unified Medical Language Sys-
tem (UMLS) (Nelson et al, 2002) has proved to
be a useful knowledge source for summarization
(Fiszman et al, 2004; Reeve et al, 2007; Plaza et
al., 2008). In order to access the information con-
tained in the UMLS, the vocabulary of the doc-
ument being summarized has to be mapped onto
it. However, ambiguity is common in biomedi-
cal documents (Weeber et al, 2001). For exam-
ple, the string ?cold? is associated with seven pos-
sible meanings in the UMLS Metathesuarus in-
cluding ?common cold?, ?cold sensation? , ?cold
temperature? and ?Chronic Obstructive Airway
Disease?. The majority of summarization sys-
tems in the biomedical domain rely on MetaMap
(Aronson, 2001) to map the text onto concepts
from the UMLS Metathesaurus (Fiszman et al,
2004; Reeve et al, 2007). However, MetaMap fre-
quently fails to identify a unique mapping and, as
a result, various concepts with the same score are
returned. For instance, for the phrase ?tissues are
often cold? MetaMap returns three equally scored
concepts for the word ??cold?: ?common cold?,
?cold sensation? and ?cold temperature?.
The purpose of this paper is to study the ef-
fect of lexical ambiguity in the knowledge source
on semantic approaches to biomedical summariza-
tion. To this end, the paper describes a concept-
based summarization system for biomedical doc-
uments that uses the UMLS as an external knowl-
edge source. To address the word ambiguity prob-
lem, we have adapted an existing WSD system
(Agirre and Soroa, 2009) to assign concepts from
the UMLS. The system is applied to the summa-
rization of 150 biomedical scientific articles from
the BioMed Central corpus and it is found that
55
WSD improves the quality of the summaries. This
paper is, to our knowledge, the first to apply WSD
to the summarization of biomedical documents
and also demonstrates that this leads to an im-
provement in performance.
The next section describes related work on sum-
marization and WSD. Section 3 introduces the
UMLS resources used in the WSD and sum-
marization systems. Section 4 describes our
concept-based summarization algorithm. Section
5 presents a graph-based WSD algorithm which
has been adapted to assign concepts from the
UMLS. Section 6 describes the experiments car-
ried out to evaluate the impact of WSD and dis-
cusses the results. The final section provides
concluding remarks and suggests future lines of
work.
2 Related work
Summarization has been an active area within
NLP research since the 1950s and a variety of ap-
proaches have been proposed (Mani, 2001; Afan-
tenos et al, 2005). Our focus is on graph-based
summarization methods. Graph-based approaches
typically represent the document as a graph, where
the nodes represent text units (i.e. words, sen-
tences or paragraphs), and the links represent co-
hesion relations or similarity measures between
these units. The best-known work in the area is
LexRank (Erkan and Radev, 2004). It assumes a
fully connected and undirected graph, where each
node corresponds to a sentence, represented by
its TF-IDF vector, and the edges are labeled with
the cosine similarity between the sentences. Mi-
halcea and Tarau (2004) present a similar method
where the similarity among sentences is measured
in terms of word overlaps.
However, methods based on term frequencies
and syntactic representations do not exploit the se-
mantic relations among the words in the text (i.e.
synonymy, homonymy or co-occurrence). They
cannot realize, for instance, that the phrases my-
ocardial infarction and heart attack refer to the
same concepts, or that pneumococcal pneumonia
and mycoplasma pneumonia are two similar dis-
eases that differ in the type of bacteria that causes
them. This problem can be partially solved by
dealing with concepts and semantic relations from
domain-specific resources, rather than terms and
lexical or syntactic relations. Consequently, some
recent approaches have adapted existing methods
to represent the document at a conceptual level. In
particular, in the biomedical domain Reeve et al
(2007) adapt the lexical chaining approach (Barzi-
lay and Elhadad, 1997) to work with UMLS con-
cepts, using the MetaMap Transfer Tool to anno-
tate these concepts. Yoo et al (2007) represent a
corpus of documents as a graph, where the nodes
are the MeSH descriptors found in the corpus, and
the edges represent hypernymy and co-occurrence
relations between them. They cluster the MeSH
concepts in the corpus to identify sets of docu-
ments dealing with the same topic and then gen-
erate a summary from each document cluster.
Word sense disambiguation attempts to solve
lexical ambiguities by identifying the correct
meaning of a word based on its context. Super-
vised approaches have been shown to perform bet-
ter than unsupervised ones (Agirre and Edmonds,
2006) but need large amounts of manually-tagged
data, which are often unavailable or impractical to
create. Knowledge-based approaches are a good
alternative that do not require manually-tagged
data.
Graph-based methods have recently been shown
to be an effective approach for knowledge-based
WSD. They typically build a graph for the text in
which the nodes represent all possible senses of
the words and the edges represent different kinds
of relations between them (e.g. lexico-semantic,
co-occurrence). Some algorithm for analyzing
these graphs is then applied from which a rank-
ing of the senses of each word in the context is
obtained and the highest-ranking one is chosen
(Mihalcea and Tarau, 2004; Navigli and Velardi,
2005; Agirre and Soroa, 2009). These methods
find globally optimal solutions and are suitable for
disambiguating all words in a text.
One such method is Personalized PageRank
(Agirre and Soroa, 2009) which makes use of
the PageRank algorithm used by internet search
engines (Brin and Page, 1998). PageRank as-
signs weight to each node in a graph by analyz-
ing its structure and prefers ones that are linked to
by other nodes that are highly weighted. Agirre
and Soroa (2009) used WordNet as the lexical
knowledge base and creates graphs using the en-
tire WordNet hierarchy. The ambiguous words in
the document are added as nodes to this graph and
directed links are created from them to each of
their possible meanings. These nodes are assigned
weight in the graph and the PageRank algorithm is
56
applied to distribute this information through the
graph. The meaning of each word with the high-
est weight is chosen. We refer to this approach
as ppr. It is efficient since it allows all ambigu-
ous words in a document to be disambiguated si-
multaneously using the whole lexical knowledge
base, but can be misled when two of the possible
senses for an ambiguous word are related to each
other in WordNet since the PageRank algorithm
assigns weight to these senses rather than transfer-
ring it to related words. Agirre and Soroa (2009)
also describe a variant of the approach, referred
to as ?word to word? (ppr w2w), in which a sep-
arate graph is created for each ambiguous word.
In these graphs no weight is assigned to the word
being disambiguated so that all of the information
used to assign weights to the possible senses of the
word is obtained from the other words in the doc-
ument. The ppr w2w is more accurate but less
efficient due to the number of graphs that have to
be created and analyzed. Agirre and Soroa (2009)
show that the Personalized PageRank approach
performs well in comparison to other knowledge-
based approaches to WSD and report an accuracy
of around 58% on standard evaluation data sets.
3 UMLS
The Unified Medical Language System (UMLS)
(Humphreys et al, 1998) is a collection of con-
trolled vocabularies related to biomedicine and
contains a wide range of information that can
be used for Natural Language Processing. The
UMLS comprises of three parts: the Specialist
Lexicon, the Semantic Network and the Metathe-
saurus.
The Metathesaurus forms the backbone of the
UMLS and is created by unifying over 100 con-
trolled vocabularies and classification systems. It
is organized around concepts, each of which repre-
sents a meaning and is assigned a Concept Unique
Identifier (CUI). For example, the following CUIs
are all associated with the term ?cold?: C0009443
?Common Cold?, C0009264 ?Cold Temperature?
and C0234192 ?Cold Sensation?.
The MRREL table in the Metathesaurus lists re-
lations between CUIs found in the various sources
that are used to form the Metathesaurus. This ta-
ble lists a range of different types of relations, in-
cluding CHD (?child?), PAR (?parent?), QB (?can
be qualified by?), RQ (?related and possibly syn-
onymous?) and RO (?other related?). For exam-
ple, the MRREL table states that C0009443 ?Com-
mon Cold? and C0027442 ?Nasopharynx? are con-
nected via the RO relation.
The MRHIER table in the Metathesaurus lists
the hierarchies in which each CUI appears, and
presents the whole path to the top or root of
each hierarchy for the CUI. For example, the
MRHIER table states that C0035243 ?Respiratory
Tract Infections? is a parent of C0009443 ?Com-
mon Cold?.
The Semantic Network consists of a set of cat-
egories (or semantic types) that provides a consis-
tent categorization of the concepts in the Metathe-
saurus, along with a set of relationships (or seman-
tic relations) that exist between the semantic types.
For example, the CUI C0009443 ?Common Cold?
is classified in the semantic type ?Disease or Syn-
drome?.
The SRSTR table in the Semantic Network de-
scribes the structure of the network. This table
lists a range of different relations between seman-
tic types, including hierarchical relations (is a)
and non hierarchical relations (e.g. result of,
associated with and co-occurs with).
For example, the semantic types ?Disease or Syn-
drome? and ?Pathologic Function? are connected
via the is a relation in this table.
4 Summarization system
The method presented in this paper consists of 4
main steps: (1) concept identification, (2) doc-
ument representation, (3) concept clustering and
topic recognition, and (4) sentence selection. Each
step is discussed in detail in the following subsec-
tions.
4.1 Concept identification
The first stage of our process is to map the doc-
ument to concepts from the UMLS Metathesaurus
and semantic types from the UMLS Semantic Net-
work.
We first run the MetaMap program over the text
in the body section of the document1 MetaMap
(Aronson, 2001) identifies all the phrases that
could be mapped onto a UMLS CUI, retrieves
and scores all possible CUI mappings for each
phrase, and returns all the candidates along with
1We do not make use of the disambiguation algorithm
provided by MetaMap, which is invoked using the -y flag
(Aronson, 2006), since our aim is to compare the effect of
WSD on the performance of our summarization system rather
than comparing WSD algorithms.
57
their score. The semantic type for each concept
mapping is also returned. Table 1 shows this map-
ping for the phrase tissues are often cold. This ex-
ample shows that MetaMap returns a single CUI
for two words (tissues and often) but also returns
three equally scored CUIs for cold (C0234192,
C0009443 and C0009264). Section 5 describes
how concepts are selected when MetaMap is un-
able to return a single CUI for a word.
Phrase: ?Tissues?
Meta Mapping (1000)
1000 C0040300:Tissues (Body tissue)
Phrase: ?are?
Phrase: ?often cold?
MetaMapping (888)
694 C0332183:Often (Frequent)
861 C0234192:Cold (Cold Sensation)
MetaMapping (888)
694 C0332183:Often (Frequent)
861 C0009443:Cold (Common Cold)
MetaMapping (888)
694 C0332183:Often (Frequent)
861 C0009264:Cold (cold temperature)
Table 1: An example of MetaMap mapping for the
phrase Tissues are often cold
UMLS concepts belonging to very general se-
mantic types are discarded, since they have been
found to be excessively broad or unrelated to the
main topic of the document. These types are
Quantitative Concept, Qualitative Concept, Tem-
poral Concept, Functional Concept, Idea or Con-
cept, Intellectual Product, Mental Process, Spatial
Concept and Language. Therefore, the concept
C0332183 ?Often? in the previous example, which
belongs to the semantic type Temporal Concept, is
discarded.
4.2 Document representation
The next step is to construct a graph-based repre-
sentation of the document. To this end, we first ex-
tend the disambiguated UMLS concepts with their
complete hierarchy of hypernyms and merge the
hierarchies of all the concepts in the same sentence
to construct a graph representing it. The two upper
levels of these hierarchies are removed, since they
represent concepts with excessively broad mean-
ings and may introduce noise to later processing.
Next, all the sentence graphs are merged into
a single document graph. This graph is extended
with more semantic relations to obtain a more
complete representation of the document. Vari-
ous types of information from the UMLS can be
used to extend the graph. We experimented us-
ing different sets of relations and finally used the
hypernymy and other related relations between
concepts from the Metathesaurus, and the asso-
ciated with relation between semantic types from
the Semantic Network. Hypernyms are extracted
from the MRHIER table, RO (?other related?) re-
lations are extracted from the MRREL table, and
associated with relations are extracted from
the SRSTR table (see Section 3). Finally, each
edge is assigned a weight in [0, 1]. This weight
is calculated as the ratio between the relative posi-
tions in their corresponding hierarchies of the con-
cepts linked by the edge.
Figure 1 shows an example graph for a sim-
plified document consisting of the two sentences
below. Continuous lines represent hypernymy re-
lations, dashed lines represent other related rela-
tions and dotted lines represent associated with re-
lations.
1. The goal of the trial was to assess cardiovascular
mortality and morbidity for stroke, coronary heart
disease and congestive heart failure, as an evidence-
based guide for clinicians who treat hypertension.
2. The trial was carried out in two groups: the first
group taking doxazosin, and the second group tak-
ing chlorthalidone.
4.3 Concept clustering and topic recognition
Our next step consists of clustering the UMLS
concepts in the document graph using a degree-
based clustering method (Erkan and Radev, 2004).
The aim is to construct sets of concepts strongly
related in meaning, based on the assumption that
each of these sets represents a different topic in the
document.
We assume that the document graph is an in-
stance of a scale-free network (Barabasi and Al-
bert, 1999). A scale-free network is a complex net-
work that (among other characteristics) presents a
particular type of node which are highly connected
to other nodes in the network, while the remain-
ing nodes are quite unconnected. These highest-
degree nodes are often called hubs. This scale-
free power-law distribution has been empirically
observed in many large networks, including lin-
guistic and semantic ones.
To discover these prominent or hub nodes, we
compute the salience or prestige of each vertex
58
Figure 1: Example of a simplified document graph
in the graph (Yoo et al, 2007), as shown in (1).
Whenever an edge from vi to vj exists, a vote from
node i to node j is added with the strength of this
vote depending on the weight of the edge. This
ranks the nodes according to their structural im-
portance in the graph.
salience(vi) =
?
?ej |?vk?ejconnect(vi,vk)
weight(ej) (1)
The n vertices with a highest salience are
named Hub Vertices. The clustering algorithm
first groups the hub vertices into Hub Vertices
Sets (HVS). These can be seen as set of concepts
strongly related in meaning, and will represent the
centroids of the clusters. To construct these HVS,
the clustering algorithm first searches, iteratively
and for each hub vertex, the hub vertex most con-
nected to it, and merges them into a single HVS.
Second, the algorithm checks, for every pair of
HVS, if their internal connectivity is lower than
the connectivity between them. If so, both HVS
are merged. The remaining vertices (i.e. those
not included in the HVS) are iteratively assigned
to the cluster to which they are more connected.
This connectivity is computed as the sum of the
weights of the edges that connect the target vertex
to the other vertices in the cluster.
4.4 Sentence selection
The last step of the summarization process con-
sists of computing the similarity between all sen-
tences in the document and each of the clusters,
and selecting the sentences for the summary based
on these similarities. To compute the similarity be-
tween a sentence graph and a cluster, we use a non-
democratic vote mechanism (Yoo et al, 2007), so
that each vertex of a sentence assigns a vote to
a cluster if the vertex belongs to its HVS, half a
vote if the vertex belongs to it but not to its HVS,
and no votes otherwise. Finally, the similarity be-
tween the sentence and the cluster is computed as
the sum of the votes assigned by all the vertices in
the sentence to the cluster, as expressed in (2).
similarity(Ci, Sj) =
?
vk|vk?Sj
wk,j (2)
where
{
wk,j=0 if vk 6?Ci
wk,j=1 if vk?HV S(Ci)
wk,j=0.5 if vk 6?HV S(Ci)
Finally, we select the sentences for the sum-
mary based on the similarity between them and
the clusters as defined above. In previous work
(blind reference), we experimented with different
heuristics for sentence selection. In this paper, we
just present the one that reported the best results.
For each sentence, we compute a single score, as
59
the sum of its similarity to each cluster adjusted
to the cluster?s size (expression 3). Then, the N
sentences with higher scores are selected for the
summary.
Score(Sj) =
?
Ci
similarity(Ci, Sj)
|Ci|
(3)
In addition to semantic-graph similarity
(SemGr) we have also tested two further features
for computing the salience of sentences: sentence
location (Location) and similarity with the title
section (Title). The sentence location feature
assigns higher scores to the sentences close to the
beginning and the end of the document, while
the similarity with the title feature assigns higher
scores as the proportion of common concepts be-
tween the title and the target sentence is increased.
Despite their simplicity, these are well accepted
summarization heuristics that are commonly used
(Bawakid and Oussalah, 2008; Bossard et al,
2008).
The final selection of the sentences for the sum-
mary is based on the weighted sum of these feature
values, as stated in (4). The values for the param-
eters ?, ? and ? have been empirically set to 0.8,
0.1, and 0.1 respectively.
Score(Sj) = ?? SemGr(Sj) +
? ? Location(Sj) + ?? Title(Sj) (4)
5 WSD for concept identification
Since our summarization system is based on the
UMLS it is important to be able to accurately map
the documents onto CUIs. The example in Section
4.1 shows that MetaMap does not always select a
single CUI and it is therefore necessary to have
some method for choosing between the ones that
are returned. Summarization systems typically
take the first mapping as returned by MetaMap,
and no attempt is made to solve this ambiguity
(Plaza et al, 2008). This paper reports an alter-
native approach that uses a WSD algorithm that
makes use of the entire UMLS Metathesaurus.
The Personalized PageRank algorithm (see Sec-
tion 2) was adapted to use the UMLS Metathe-
saurus and used to select a CUI from the MetaMap
output2. The UMLS is converted into a graph
in which the CUIs are the nodes and the edges
2We use a publicly available implementation of the Per-
sonalized Page Rank algorithm (http://ixa2.si.ehu.
es/ukb/) for the experiments described here.
are derived from the MRREL table. All possible
relations in this table are included. The output
from MetaMap is used to provide the list of pos-
sible CUIs for each term in a document and these
are passed to the disambiguation algorithm. We
use both the standard (ppr) and ?word to word?
(ppr w2w) variants of the Personalized PageRank
approach.
It is difficult to evaluate how well the Person-
alized PageRank approach performs when used
in this way due to a lack of suitable data. The
NLM-WSD corpus (Weeber et al, 2001) con-
tains manually labeled examples of ambiguous
terms in biomedical text but only provides exam-
ples for 50 terms that were specifically chosen be-
cause of their ambiguity. To evaluate an approach
such as Personalized PageRank we require doc-
uments in which the sense of every ambiguous
word has been identified. Unfortunately no such
resource is available and creating one would be
prohibitively expensive. However, our main in-
terest is in whether WSD can be used to improve
the summaries generated by our system rather than
its own performance and, consequently, decided to
evaluate the WSD by comparing the output of the
summarization system with and without WSD.
6 Experiments
6.1 Setup
The ROUGE metrics (Lin, 2004) are used to eval-
uate the system. ROUGE compares automati-
cally generated summaries (called peers) against
human-created summaries (called models), and
calculates a set of measures to estimate the con-
tent quality of the summaries. Results are re-
ported for the ROUGE-1 (R-1), ROUGE-2 (R-
2), ROUGE-SU4 (R-SU) and ROUGE-W (R-W)
metrics. ROUGE-N (e.g. ROUGE-1 and ROUGE-
2) evaluates n-gram co-occurrences among the
peer and models summaries, where N stands for
the length of the n-grams. ROUGE-SU4 allows
bi-gram to have intervening word gaps no longer
than four words. Finally, ROUGE-W computes
the union of the longest common subsequences be-
tween the candidate and the reference summaries
taking into account the presence of consecutive
matches.
To the authors? knowledge, no specific corpus
for biomedical summarization exists. To evalu-
ate our approach we use a collection of 150 doc-
uments randomly selected from the BioMed Cen-
60
tral corpus3 for text mining research. This collec-
tion is large enough to ensure significant results in
the ROUGE evaluation (Lin, 2004) and allows us
to work with the ppr w2w disambiguation soft-
ware, which is quite time consuming. We generate
automatic summaries by selecting sentences until
the summary reaches a length of the 30% over the
original document size. The abstract of the papers
(i.e. the authors? summaries) are removed from
the documents and used as model summaries.
A separate development set was used to deter-
mine the optimal values for the parameters in-
volved in the algorithm. This set consists of 10
documents from the BioMed Central corpus. The
model summaries for these documents were man-
ually created by medical students by selecting be-
tween 20-30% of the sentences within the paper.
The parameters to be estimated include the per-
centage of vertices considered as hub vertices by
the clustering method (see Section 4.3) and the
combination of summarization features used to
sentence selection (see Section 4.4). As a result,
the percentage of hub vertices was set to 15%, and
no additional summarization features (apart from
the semantic-graph similarity) were used.
Two baselines were also implemented. The
first, lead baseline, generate summaries by select-
ing the first n sentences from each document. The
second, random baseline, randomly selects n sen-
tences from the document. The n parameter is
based on the desired compression rate (i.e. 30%
of the document size).
6.2 Results
Various summarizers were created and evaluated.
First, we generated summaries using our method
without performing word sense disambiguation
(SemGr), but selecting the first CUI returned by
MetaMap. Second, we repeated these experiments
using the Personalized Page Rank disambigua-
tion algorithm (ppr) to disambiguate the CUIs re-
turned by MetaMap (SemGr + ppr). Finally, we
use the ?word to word? variant of the Personalized
Page Rank algorithm (ppr w2w) to perform the
disambiguation (SemGr + ppr w2w).
Table 2 shows ROUGE scores for the different
configurations of our system together with the two
baselines. All configurations significantly outper-
form both baselines (Wilcoxon Signed Ranks Test,
p < 0.01).
3http://www.biomedcentral.com/info/about/datamining/
Summarizer R-1 R-2 R-W R-SU
random .5089 .1879 .1473 .2349
lead .6483 .2566 .1621 .2646
SemGr .7504 .3283 .1915 .3117
SemGr+ppr .7737 .3419 .1937 .3178
SemGr+ppr w2w .7804 .3530 .1966 .3262
Table 2: ROUGE scores for two baselines and
SemGr (with and without WSD). Significant dif-
ferences among the three versions of SemGr are
indicated in bold font.
The use of WSD improves the average ROUGE
score for the summarizer. The ?standard? (i.e.
ppr) version of the WSD algorithm signifi-
cantly improves ROUGE-1 and ROUGE-2 metrics
(Wilcoxon Signed Ranks Test, p < 0.01), com-
pared with no WSD (i.e. SemGr). The ?word to
word? variant (ppr w2w) significantly improves
all ROUGE metrics. Performance using the ?word
to word? variant is also higher than standard ppr
in all ROUGE scores.
These results demonstrate that employing a
state of the art WSD algorithm that has been
adapted to use the UMLS Metathesaurus improves
the quality of the summaries generated by a sum-
marization system. To our knowledge this is
the first result to demonstrate that WSD can im-
prove summarization systems. However, this im-
provement is less than expected and this is prob-
ably due to errors made by the WSD system.
The Personalized PageRank algorithms (ppr and
ppr w2w) have been reported to correctly dis-
ambiguate around 58% of words in general text
(see Section 2) and, although we were unable to
quantify their performance when adapted for the
biomedical domain (see Section 5), it is highly
likely that they will still make errors. However, the
WSD performance they do achieve is good enough
to improve the summarization process.
6.3 Analysis
The results presented above demonstrate that us-
ing WSD improves the performance of our sum-
marizer. The reason seems to be that, since the ac-
curacy in the concept identification step increases,
the document graph built in the following steps is
a better approximation of the structure of the doc-
ument, both in terms of concepts and relations. As
a result, the clustering method succeeds in finding
the topics covered in the document, and the infor-
mation in the sentences selected for the summary
61
is closer to that presented in the model summaries.
We have observed that the clustering method
usually produces one big cluster along with a vari-
able number of small clusters. As a consequence,
though the heuristic for sentence selection was de-
signed to select sentences from all the clusters in
the document, the fact is that most of the sentences
are extracted from this single large cluster. This
allows our system to identify sentences that cover
the main topic of the document, while it occasion-
ally fails to extract other ?satellite? information.
We have also observed that the ROUGE scores
differ considerably from one document to others.
To understand the reasons of these differences we
examined the two documents with the highest and
lowest ROUGE scores respectively. The best case
is one of the largest document in the corpus, while
the worst case is one of the shortest (6 versus 3
pages). This was expected, since according to our
hypothesis that the document graph is an instance
of a scale-free network (see Section 4.3), the sum-
marization algorithm works better with larger doc-
uments. Both documents also differ in their under-
lying subject matter. The best case concerns the
reactions of some kind of proteins over the brain
synaptic membranes; while the worst case regards
the use of pattern matching for database searching.
We have verified that UMLS covers the vocabu-
lary contained in the first document better than in
the second one. We have also observed that the use
in the abstract of synonyms of terms presented in
the document body is quite frequent. In particular
the worst case document uses different terms in the
abstract and the body, for example ?pattern match-
ing? and ?string searching?. Since the ROUGE
metrics rely on evaluating summaries based on the
number of strings they have in common with the
model summaries the system?s output is unreason-
ably penalised.
Another problem is related to the use of
acronyms and abbreviations. Most papers in the
corpus do not include an Abbreviations section but
define them ad hoc in the document body. These
contracted forms are usually non-standard and do
not exist in the UMLS Metathesaurus. This seri-
ously affects the performance of both the disam-
biguation and the summarization algorithms, es-
pecially considering that it has been observed that
the terms (or phrases) represented in an abbrevi-
ated form frequently correspond to central con-
cepts in the document. For example, in a pa-
per from the corpus that presents an analysis tool
for simple sequence repeat tracts in DNA, only
the first occurrence of ?simple sequence repeat?
is presented in its expanded form. In the re-
maining of the document, this phrase is named
by its acronym ?SSR?. The same occurs in a pa-
per that investigates the developmental expression
of survivin during embryonic submandibular sali-
vary gland development, where ?embryonic sub-
mandibular gland? is always referred as ?SMG?.
7 Conclusion and future work
In this paper we propose a graph-based approach
to biomedical summarization. Our algorithm rep-
resents the document as a semantic graph, where
the nodes are concepts from the UMLS Metathe-
saurus and the links are different kinds of seman-
tic relations between them. This produces a richer
representation than the one provided by traditional
models based on terms.
This approach relies on accurate mapping of
the document being summarized into the concepts
in the UMLS Metathesaurus. Three methods for
doing this were compared and evaluated. The
first was to select the first mapping generated by
MetaMap while the other two used a state of the
art WSD algorithm. This WSD algorithm was
adapted for the biomedical domain by using the
UMLS Metathesaurus as a knowledge based and
MetaMap as a pre-processor to identify the pos-
sible CUIs for each term. Results show that the
system performs better when WSD is used.
In future work we plan to make use of the dif-
ferent types of information within the UMLS to
create different configurations of the Personalized
PageRank WSD algorithm and explore their ef-
fect on the summarization system (i.e. consider-
ing different UMLS relations and assigning differ-
ent weights to different relations). It would also
be interesting to test the system with other disam-
biguation algorithms and use a state of the art al-
gorithm for identifying and expanding acronyms
and abbreviations.
Acknowledgments
This research is funded by the Spanish Govern-
ment through the FPU program and the projects
TIN2009-14659-C03-01 and TSI 020312-2009-
44. Mark Stevenson acknowledges the support of
the Engineering and Physical Sciences Research
Council (grant EP/D069548/1).
62
References
S.D. Afantenos, V. Karkaletsis, and P. Stamatopou-
los. 2005. Summarization from medical docu-
ments: a survey. Artificial Intelligence in Medicine,
33(2):157?177.
E. Agirre and P. Edmonds, editors, 2006. Word
Sense Disambiguation: Algorithms and Applica-
tions. Springer.
E. Agirre and A. Soroa. 2009. Personalizing PageRank
for Word Sense Disambiguation. In Proceedings of
EACL-09, pages 33?41, Athens, Greece.
A. Aronson. 2001. Effective mapping of biomedi-
cal text to the UMLS Metathesaurus: the MetaMap
program. In Proceedings of the AMIA Symposium,
pages 17?21.
A. Aronson. 2006. MetaMap: Mapping text to the
UMLS Metathesaurus. Technical report, U.S. Na-
tional Library of Medicine.
A.L. Barabasi and R. Albert. 1999. Emergence of scal-
ing in random networks. Science, 268:509?512.
R. Barzilay and M. Elhadad. 1997. Using lexical
chains for text summarization. In Proceedings of the
ACL Workshop on Intelligent Scalable Text Summa-
rization, pages 10?17.
A. Bawakid and M. Oussalah. 2008. A semantic
summarization system: University of Birmingham
at TAC 2008. In Proceedings of the First Text Anal-
ysis Conference (TAC 2008).
A. Bossard, M. Gnreux, and T. Poibeau. 2008. De-
scription of the LIPN systems at TAC 2008: sum-
marizing information and opinions. In Proceedings
of the First Text Analysis Conference (TAC 2008).
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual web search engine. Computer
Networks and ISDN Systems, 30:1?7.
G. Erkan and D. R. Radev. 2004. LexRank: Graph-
based lexical centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR), 22:457?479.
M. Fiszman, T. C. Rindflesch, and H. Kilicoglu.
2004. Abstraction summarization for managing the
biomedical research literature. In Proceedings of
the HLT-NAACL Workshop on Computational Lex-
ical Semantics, pages 76?83.
L. Humphreys, D. Lindberg, H. Schoolman, and
G. Barnett. 1998. The Unified Medical Lan-
guage System: An informatics research collabora-
tion. Journal of the American Medical Informatics
Association, 1(5):1?11.
L. Hunter and K. B. Cohen. 2006. Biomedical
Language Processing: Perspective Whats Beyond
PubMed? Mol Cell., 21(5):589?594.
C.-Y. Lin. 2004. Rouge: A package for automatic eval-
uation of summaries. In Proceedings of the ACL-
04 Workshop: Text Summarization Branches Out.,
pages 74?81, Barcelona, Spain.
I. Mani. 2001. Automatic summarization. Jonh Ben-
jamins Publishing Company.
R. Mihalcea and P. Tarau. 2004. TextRank - Bringing
order into text. In Proceedings of the Conference
EMNLP 2004, pages 404?411.
R. Navigli and P. Velardi. 2005. Structural seman-
tic interconnections: A knowledge-based approach
to word sense disambiguation. IEEE Trans. Pattern
Anal. Mach. Intell., 27(7):1075?1086.
S. Nelson, T. Powell, and B. Humphreys. 2002. The
Unified Medical Language System (UMLS) Project.
In Allen Kent and Carolyn M. Hall, editors, Ency-
clopedia of Library and Information Science. Mar-
cel Dekker, Inc.
L. Plaza, A. D??az, and P. Gerva?s. 2008. Concept-
graph based biomedical automatic summarization
using ontologies. In TextGraphs ?08: Proceedings
of the 3rd Textgraphs Workshop on Graph-Based Al-
gorithms for Natural Language Processing, pages
53?56.
L.H. Reeve, H. Han, and A.D. Brooks. 2007. The
use of domain-specific concepts in biomedical text
summarization. Information Processing and Man-
agement, 43:1765?1776.
M. Weeber, J. Mork, and A. Aronson. 2001. Devel-
oping a Test Collection for Biomedical Word Sense
Disambiguation. In Proceedings of AMIA Sympo-
sium, pages 746?50, Washington, DC.
I. Yoo, X. Hu, and I-Y. Song. 2007. A coherent
graph-based semantic clustering and summarization
approach for biomedical literature and a new sum-
marization evaluation method. BMC Bioinformat-
ics, 8(9).
63
Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 85?93,
Avignon, France, 24 April 2012. c?2012 Association for Computational Linguistics
Computing Similarity between Cultural Heritage
Items using Multimodal Features
Nikolaos Aletras
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield, S1 4DP, UK
n.aletras@dcs.shef.ac.uk
Mark Stevenson
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield, S1 4DP, UK
m.stevenson@dcs.shef.ac.uk
Abstract
A significant amount of information about
Cultural Heritage artefacts is now available
in digital format and has been made avail-
able in digital libraries. Being able to iden-
tify items that are similar would be use-
ful for search and navigation through these
data sets. Information about items in these
repositories is often multimodal, such as
pictures of the artefact and an accompa-
nying textual description. This paper ex-
plores the use of information from these
various media for computing similarity be-
tween Cultural Heritage artefacts. Results
show that combining information from im-
ages and text produces better estimates of
similarity than when only a single medium
is considered.
1 Introduction and Motivation
In recent years a vast amount of Cultural Heritage
(CH) artefacts have been digitised and made avail-
able on-line. For example, the Louvre and the
British Museum provide information about ex-
hibits on their web pages1. In addition, informa-
tion is also available via sites that aggregate CH
information from multiple resources. A typical
example is Europeana2, a web-portal to collec-
tions from several European institutions that pro-
vides access to over 20 million items including
paintings, films, books, archives and museum ex-
hibits.
However, online information about CH arte-
facts is often unstructured and varies by collec-
1http://www.louvre.fr/,
http://www.britishmuseum.org/
2http://www.europeana.eu
tion. This makes it difficult to identify informa-
tion of interest in sites that aggregate informa-
tion from multiple sources, such as Europeana,
or to compare information across multiple collec-
tions (such as the Louvre and British Museum).
These problems form a significant barrier to ac-
cessing the information available in these online
collections. A first step towards improving access
would be to identify similar items in collections.
This could assist with several applications that are
of interest to those working in CH including rec-
ommendation of interesting items (Pechenizkzy
and Calders, 2007; Wang et al, 2008), generation
of virtual tours (Joachims et al, 1997; Wang et al,
2009), visualisation of collections (Kauppinen et
al., 2009; Hornbaek and Hertzum, 2011) and ex-
ploratory search (Marchionini, 2006; Amin et al,
2008).
Information in digital CH collections often in-
cludes multiple types of media such as text, im-
ages and audio. It seems likely that informa-
tion from all of these types would help humans
to identify similar items and that it could help to
identify them automatically. However, previous
work on computing similarity in the CH domain
has been limited and, in particular, has not made
use of information from multiple types of media.
For example, Grieser et al (2011) computed sim-
ilarity between exhibits in Melbourne Museum
by applying a range of text similarity measures
but did not make use of other media. Tech-
niques for exploiting information from multi-
media collections have been developed and are
commonly applied to a wide range of problems
such as Content-based Image Retrieval (Datta et
al., 2008) and image annotation (Feng and Lap-
ata, 2010).
85
This paper makes use of information from two
media (text and images) to compute the similar-
ity between items in a large collection of CH
items (Europeana). A range of similarity mea-
sures for text and images are compared and com-
bined. Evaluation is carried out using a set of
items from Europeana with similarity judgements
that were obtained in a crowdsourcing experi-
ment. We find that combining information from
both media produces better results than when ei-
ther is used alone.
The main contribution of this paper is to
demonstrate the usefulness of applying informa-
tion from more than one medium when compar-
ing CH items. In addition, it explores the effec-
tiveness of different similarity measures when ap-
plied to this domain and introduces a data set of
similarity judgements that can be used as a bench-
mark.
The remainder of this paper is structured as fol-
lows. Section 2 describes some relevant previous
work. Sections 3, 4 and 5 describe the text and im-
age similarity measures applied in this paper and
how they are combined. Sections 6 describes the
experiments used in this paper and the results are
reported in Section 7. Finally, Section 8 draws
the conclusions and provides suggestions for fu-
ture work.
2 Background
2.1 Text Similarity
Two main approaches for determining the similar-
ity between two texts have been explored: corpus-
based and knowledge-based methods. Corpus-
based methods rely on statistics that they learn
from corpora while knowledge-based methods
make use of some external knowledge source,
such as a thesaurus, dictionary or semantic net-
work (Agirre et al, 2009; Gabrilovich and
Markovitch, 2007).
A previous study (Aletras et al, 2012) com-
pared the effectiveness of various methods for
computing the similarity between items in a CH
collection based on text extracted from their
descriptions, including both corpus-based and
knowledge-based approaches. The corpus-based
approaches varied from simple word counting ap-
proaches (Manning and Schutze, 1999) to more
complex ones based on techniques from Infor-
mation Retrieval (Baeza-Yates and Ribeiro-Neto,
1999) and topic models (Blei et al, 2003). The
knowledge-based approaches relied on Wikipedia
(Milne, 2007). Aletras et al (2012) concluded
that corpus-based measures were more effective
than knowledge-based ones for computing simi-
larity between these items.
2.2 Image Similarity
Determining the similarity between images has
been explored in the fields such as Computer Vi-
sion (Szeliski, 2010) and Content-based Image
Retrieval (CBIR) (Datta et al, 2008). A first step
in computing the similarity between images is to
transform them into an appropriate set of features.
Some major feature types which have been used
are colour, shape, texture or salient points. Fea-
tures are also commonly categorised into global
and local features.
Global features characterise an entire image.
For example, the average of the intensities of red,
green and blue colours gives an estimation of the
overall colour distribution in the image. The main
advantages of global features are that they can be
computed efficiently. However, they are unable
to represent information about elements in an im-
age (Datta et al, 2008). On the other hand, lo-
cal features aim to identify interesting areas in
the image, such as where significant differences
in colour intensity between adjacent pixels is de-
tected.
Colour is one of the most commonly used
global features and has been applied in sev-
eral fields including image retrieval (Jacobs et
al., 1995; Sebe and Michael S. Lew, 2001;
Yu et al, 2002), image clustering (Cai et al,
2004; Strong and Gong, 2009), database index-
ing (Swain and Ballard, 1991) and, object/scene
recognition (Schiele and Crowley, 1996; Ndjiki-
Nya et al, 2004; Sande et al, 2008). A common
method for measuring similarity between images
is to compare the colour distributions of their his-
tograms. A histogram is a graphical representa-
tion of collected counts for predefined categories
of data. To create a histogram we have to specify
the range of the data values, the number of dimen-
sions and the bins (intervals into which ranges
of values are combined). A colour histogram
records the number of the pixels that fall in the
interval of each bin. Schiele and Crowley (1996)
describe several common metrics for comparing
colour histograms including ?2, correlation and
86
intersection.
2.3 Combining Text and Image Features
The integration of information from text and im-
age features has been explored in several fields.
In Content-based Image Retrieval image features
are combined together with words from captions
to retrieve images relevant to a query (La Cas-
cia et al, 1998; Srihari et al, 2000; Barnard
and Forsyth, 2001; Westerveld, 2002; Zhou and
Huang, 2002; Wang et al, 2004). Image cluster-
ing methods have been developed to combine in-
formation from images and text to create clusters
of similar images (Loeff et al, 2006; Bekkerman
and Jeon, 2007). Techniques for automatic image
annotation that generate models as a mixture of
word and image features have also been described
(Jeon et al, 2003; Blei and Jordan, 2003; Feng
and Lapata, 2010).
2.4 Similarity in Cultural Heritage
Despite the potential usefulness of similarity in
CH, there has been little previous work on the
area. An exception is the work of Grieser et al
(2011). They computed the similarity between a
set of 40 exhibits from Melbourne Museum by
analysing the museum?s web pages and physi-
cal layout. They applied a range of text similar-
ity techniques (see Section 2.1) to the web pages
as well as similarity measures that made use of
Wikipedia. However, the Wikipedia-based tech-
niques relied on a manual mapping between the
items and an appropriate Wikipedia article. Al-
though the web pages often contained images of
the exhibits, Grieser et al (2011) did not make use
of them.
3 Text Similarity
We make use of various corpus-based approaches
for computing similarity between CH items since
previous experiments (see Section 2.1) have
shown that these outperformed knowledge-based
methods in a comparison of text-based similarity
methods for the CH domain.
We assume that we wish to compute the simi-
larity between a pair of items, A and B, and that
each item has both text and an image associated
with it. The text is denoted as At and Bt while
the images are denoted by Ai and Bi.
3.1 Word Overlap
A common approach to computing similarity is to
count the number of common words (Lesk, 1986).
The text associated with each item is compared
and the similarity is computed as the number of
words (tokens) they have in common normalised
by the combined total:
simWO(A,B) =
|At ?Bt|
|At ?Bt|
3.2 N-gram Overlap
The Word Overlap approach is a bag of words
method that does not take account of the order
in which words appear, despite the fact that this
is potentially useful information for determining
similarity. One way in which this information can
be used is to compare n-grams derived from a text.
Patwardhan et al (2003) used this approach to ex-
tend the Word Overlap measure. This approach
identifies n-grams in common between the two
text and increases the score by n2 for each one
that is found, where n is the length of the n-gram.
More formally,
simngram(A,B) =
?
n ? n?gram(At,Bt)
n2
|At ?Bt|
where n?gram(At, Bt) is the set of n-grams that
occur in both At and Bt.
3.3 TF.IDF
The word and n-gram overlap measures assign
the same importance to each word but some are
more important for determining similarity be-
tween texts than others. A widely used approach
to computing similarity between documents is to
represent them as vectors in which each term is
assigned a weighting based on its estimated im-
portance (Manning and Schutze, 1999). The vec-
tors can then be compared using the cosine met-
ric. A widely used scheme for weighting terms
is tf.idf, which takes account of the frequency of
each term in individual documents and the num-
ber of documents in a corpus in which it occurs.
3.4 Latent Dirichlet Allocation
Topic models (Blei et al, 2003) are a useful tech-
nique for representing the underlying content of
documents. LDA is a widely used topic model
87
that assumes each document is composed of a
number of topics. For each document LDA re-
turns a probability distribution over a set of topics
that have been derived from an unlabeled corpus.
Similarity between documents can be computed
by converting these distributions into vectors and
using the cosine metric.
4 Image Similarity
Two approaches are compared for computing the
similarity between images. These are largely
based on colour features and are more suitable for
the images in the data set we use for evaluation
(see Section 6).
4.1 Colour Similarity (RGB)
The first approach is based on comparison of
colour histograms derived from images.
In the RGB (Red Green Blue) colour model,
each pixel is represented as an integer in range of
0-255 in three dimensions (Red, Green and Blue).
One histogram is created for each dimension. For
grey-scale images it is assumed that the value of
each dimension is the same in each pixel and a
single histogram, called the luminosity histogram,
is created. Similarity between the histograms in
each colour channel is computed using the inter-
section metric. The intersection metric (Swain
and Ballard, 1991) measures the number of cor-
responding pixels that have same colour in two
images. It is defined as follows:
Inter(h1, h2) =
?
I
min(h1(I), h2(I))
where hi is the histogram of image i, I is the set
of histogram bins and min(a, b) is the minimum
between corresponding pixel colour values.
The final similarity score is computed as the av-
erage of the red, green and blue histogram simi-
larity scores:
simRGB(Ai, Bi) =
?
i?{R,G,B}
Inter(hAi , hBi)
3
4.2 Image Querying Metric (imgSeek)
Jacobs et al (1995) described an image similar-
ity metric developed for Content-based Image Re-
trieval. It makes use of Haar wavelet decompo-
sition (Beylkin et al, 1991) to create signatures
of images that contain colour and basic shape in-
formation. Images are compared by determining
the number of significant coefficients they have in
common using the following function:
distimgSeek(Ai, Bi) = w0|CAi(0, 0)? CBi(0, 0)|
+
?
i,j:C?Ai (i,j) 6=0
wbin(i,j)(C?Ai(i, j) 6= C?Bi(i, j))
where wb are weights, CI represents a single
colour channel for an image I , CI(0, 0) are scal-
ing function coefficients of the overall average in-
tensity of the colour channel and C?I(i, j) is the
(i, j)-th truncated, quantised wavelet coefficient
of image I . For more details please refer to Ja-
cobs et al (1995).
Note that this function measures the distance
between two images with low scores indicating
similar images and high scores dis-similar ones.
We assign the negative sign to this metric to assign
high scores to similar images. It is converted into
a similarity metric as follows:
simimgSeek(Ai, Bi) = ?distimgSeek(Ai, Bi)
5 Combining Text and Image Similarity
A simple weighted linear combination is used to
combine the results of the text and image similar-
ities, simimg and simt. The similarity between a
pair of items is computed as follows
simT+I(A,B) = w1 ? simt(At, Bt)
+ w2 ? simimg(Ai, Bi)
where wi are weights learned using linear regres-
sion (see Section 6.4).
6 Evaluation
This section describes experiments used to evalu-
ate the similarity measures described in the previ-
ous sections.
6.1 Europeana
The similarity measures are evaluated using infor-
mation from Europeana3, a web-portal that pro-
vides access to information CH artefacts. Over
2,000 institutions through out Europe have con-
tributed to Europeana and the portal provides ac-
cess to information about over 20 million CH arte-
facts, making it one of the largest repositories
3http://www.europeana.eu
88
of digital information about CH currently avail-
able. It contains information about a wide vari-
ety of types of artefacts including paintings, pho-
tographs and newspaper archives. The informa-
tion is in a range of European languages, with
over 1 million items in English. The diverse na-
ture of Europeana makes it an interesting resource
for exploring similarity measures.
The Europeana portal provides various types of
information about each artefact, including textual
information, thumbnail images of the items and
links to additional information available for the
providing institution?s web site. The textual in-
formation is derived from metadata obtained from
the providing institution and includes title, de-
scription as well as details of the subject, medium
and creator.
An example artefact from the Europeana por-
tal is shown in Figure 1. This particular artefact
is an image showing detail of an architect?s office
in Nottingham, United Kingdom. The informa-
tion provided for this item is relatively rich com-
pared to other items in Europeana since the title is
informative and the textual description is of rea-
sonable length. However, the amount of informa-
tion associated with items in Europeana is quite
varied and it is common for items to have short
titles, which may be uninformative, or have very
limited textual descriptions. In addition, the meta-
data associated with items in Europeana is poten-
tially a valuable source of information that could
be used for, among other things, computing simi-
larity between items. However, the various pro-
viding institutions do not use consistent coding
schemes to populate these fields which makes it
difficult to compare items provided by different
institutions. These differences in the information
provided by the various institutions form a signif-
icant challenge in processing the Europeana data
automatically.
6.2 Evaluation Data
A data set was created by selecting 300 pairs of
items added to Europeana by two providers: Cul-
ture Grid4 and Scran5. The items added to Eu-
ropeana by these providers represent the major-
ity that are in English and they contain different
types of items such as objects, archives, videos
and audio files. We removed five pairs that did
4http://www.culturegrid.org.uk/
5http://www.scran.ac.uk/
not have any images associated with one of the
items. (These items were audiofiles.) The result-
ing dataset consists of 295 pairs of items and is
referred to as Europeana295.
Each item corresponds to a metadata record
consisting of textual information together with a
URI and a link to its thumbnail. Figure 1 shows an
item taken from the Europeana website. The title,
description and subject fields have been shown
to be useful information for computing similar-
ity (Aletras et al, 2012). These are extracted and
concatenated to form the textual information as-
sociated with each item. In addition, the accom-
panying thumbnail image (or ?preview?) was also
extracted to be used as the visual information. The
size of these images varies from 7,000 to 10,000
pixels.
We have pre-processed the data by removing
stop words and applying stemming. For the
tf.idf and LDA the training corpus was a total of
759,896 Europeana items. We have filtered out
all items that have no description and have a ti-
tle shorter than 4 words, or have a title which has
been repeated more than 100 times.
6.3 Human Judgements of Similarity
Crowdflower6, a crowdsourcing platform, was
used to obtain human judgements of the simi-
larity between each pair of items. Participants
were asked to rate each item pair using a 5 point
scale where 4 indicated that the pair of items were
highly similar or near-identical while 0 indicated
that they were completely different. Participants
were presented with a page containing 10 pairs of
items and asked to rate all of them. Participants
were free to rate as many pages as they wanted up
to a maximum of 30 pages (i.e. the complete Eu-
ropeana295 data set). To ensure that the annota-
tors were not returning random answers each page
contained a pair for which the similarity had been
pre-identified as being at one end of the similarity
scale (i.e. either near-identical or completely dif-
ferent). Annotations from participants that failed
to answer correctly these questions or participants
that have given same rating to all of their answers
were removed. A total of 3,261 useful annotations
were collected from 99 participants and each pair
was rated by at least 10 participants.
The final similarity score for each pair was gen-
6http://crowdflower.com/
89
Figure 1: Example item from Europeana portal showing how both textual and image information are displayed.
(Taken from http://www.europeana.eu/portal/)
erated by averaging the ratings. Inter-annotator
agreement was computed as the average of the
Pearson correlation between the ratings of each
participant and the average ratings of the other
participants, a methodology used by Grieser et
al. (2011). The inter-annotator agreement for the
data set was ? = +0.553, which is comparable
with the agreement score of ? = +0.507 previ-
ously reported by Grieser et al (2011).
6.4 Experiments
Experiments were carried out comparing the re-
sults of the various techniques for computing text
and image similarity (Sections 3 and 4) and their
combination (Section 5). Performance is mea-
sured as the Pearson?s correlation coefficient with
the gold-standard data.
The combination of text and image similarity
(Section 5) relies on a linear combination of text
and image similarities. The weights for this com-
bination are obtained using a linear regression
model. The input values were the results obtained
for the individual text and similarity methods and
the target value was the gold-standard score for
each pair in the dataset. 10-fold cross-validation
was used for evaluation.
7 Results
An overview of the results obtained is shown in
Table 1. Results for the text and image similarity
methods used alone are shown in the left and top
part of the table while the results for their combi-
Image Similarity
RGB imgSeek
Text Similarity 0.254 0.370
Word Overlap 0.487 0.450 0.554
tf.idf 0.437 0.426 0.520
N-gram overlap 0.399 0.384 0.504
LDA 0.442 0.419 0.517
Table 1: Performance of similarity measures applied
to Europeana295 data set (Pearson?s correlation coef-
ficient).
nation are in the main body.
The best performance for text similarity (0.487)
is achieved by Word Overlap and the lowest by
N-gram Overlap (0.399). The results are surpris-
ing since the simplest approach produces the best
results. It is likely that the reason for these re-
sults is the nature of the textual data in Europeana.
The documents are often short, in some cases the
description missing or the subject information is
identical to the title.
For image similarity, results using imgSeek are
higher than RGB (0.370 and 0.254 respectively).
There is also a clear difference between the per-
formance of the text and image similarity meth-
ods and results obtained from both image similar-
ity measures is lower than all four that are based
on text. The reason for these results is the nature
of the Europeana images. There are a large num-
ber of black-and-white image pairs which means
that colour information cannot be obtained from
90
many of them. In addition, the images are low
resolution, since they are thumbnails, which lim-
its the amount of shape information that can be
derived from them, restricting the effectiveness of
imgSeek. However, the fact that performance is
better for imgSeek and RGB suggests that it is still
possible to obtain useful information about shape
from these images.
When the image and text similarity measures
are combined the highest performance is achieved
by the combination of the Word Overlap and
imgSeek (0.554), the best performing text and im-
age similarity measures when applied individu-
ally. The performance of all text similarity mea-
sures improves when combined with imgSeek.
All results are above 0.5 with the highest gain
observed for N-gram Overlap (from 0.399 to
0.504), the worst performing text similarity mea-
sure when applied individually. On the other
hand, combining text similarity measures with
RGB consistently leads to performance that is
lower than when the text similarity measure is
used alone.
These results demonstrate that improvements
in similarity scores can be obtained by making
use of information from both text and images. In
addition, better results are obtained for the text
similarity methods and this is likely to be caused
by the nature of the images which are associated
with the items in our data set. It is also impor-
tant to make use of an appropriate image similar-
ity method since combing text similarity methods
with RGB reduces performance.
8 Conclusion
This paper demonstrates how information from
text and images describing CH artefacts can be
combined to improve estimates of the similarity
between them. Four corpus-based and two image-
based similarity measures are explored and eval-
uated on a data set consisting of 295 manually-
annotated pairs of items from Europeana. Results
showed that combing information from text and
image similarity improves performance and that
imgSeek similarity method consistently improves
performance of text similarity methods.
In future work we intend to make use of other
types of image features including the low-level
ones used by approaches such as Scale Invari-
ant Feature Transformation (SIFT) (Lowe, 1999;
Lowe, 2004) and the bag-of-visual words model
(Szeliski, 2010). In addition we plan to apply
these approaches to higher resolution images to
determine how the quality and size of an image
affects similarity algorithms.
Acknowledgments
The research leading to these results was
carried out as part of the PATHS project
(http://paths-project.eu) funded by
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 270082.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL ?09), pages 19?27, Boulder, Colorado.
Nikolaos Aletras, Mark Stevenson, and Paul Clough.
2012. Computing similarity between items in a dig-
ital library of cultural heritage. Submitted.
Alia Amin, Jacco van Ossenbruggen, Lynda Hard-
man, and Annelies van Nispen. 2008. Understand-
ing Cultural Heritage Experts? Information Seeking
Needs. In Proceedings of the 8th ACM/IEEE-CS
Joint Conference on Digital Libraries, pages 39?47,
Pittsburgh, PA.
Ricardo Baeza-Yates and Berthier Ribeiro-Neto.
1999. Modern Information Retrieval. Addison
Wesley Longman Limited, Essex.
Kobus Barnard and David Forsyth. 2001. Learn-
ing the Semantics of Words and Pictures. Pro-
ceedings Eighth IEEE International Conference on
Computer Vision (ICCV ?01), 2:408?415.
Ron Bekkerman and Jiwoon Jeon. 2007. Multi-modal
clustering for multimedia collections. In IEEE Con-
ference on Computer Vision and Pattern Recogni-
tion (CVPR ?07), pages 1?8.
Gregory Beylkin, Ronald Coifman, and Vladimir
Rokhlin. 1991. Fast Wavelet Transforms and Nu-
merical Algorithms I. Communications on Pure
and Applied Mathematics, 44:141?183.
David M. Blei and Michael I. Jordan. 2003. Modeling
Annotated Data. Proceedings of the 26th annual
international ACM SIGIR conference on Research
and Development in Information Retrieval (SIGIR
?03), pages 127?134.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
91
Deng Cai, Xiaofei He, Zhiwei Li, Wei-Ying Ma, and
Ji-Rong Wen. 2004. Hierarchical Clustering of
WWW Image Search Results Using Visual, Textual
and Link Information. Proceedings of the 12th an-
nual ACM international conference on Multimedia
(MULTIMEDIA ?04), pages 952?959.
Ritendra Datta, Dhiraj Joshi, Jia Li, and James Z.
Wang. 2008. Image Retrieval: Ideas, Influences,
and Trends of the New Age. ACM Computing Sur-
veys, 40(2):1?60.
Yansong Feng and Mirella Lapata. 2010. Topic
Models for Image Annotation and Text Illustration.
In Proceedings of Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 831?839, Los Angeles, California,
June.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of
the International Joint Conference on Artificial In-
telligence (IJCAI ?07), pages 1606?1611.
Karl Grieser, Timothy Baldwin, Fabian Bohnert, and
Liz Sonenberg. 2011. Using Ontological and Doc-
ument Similarity to Estimate Museum Exhibit Re-
latedness. Journal on Computing and Cultural Her-
itage (JOCCH), 3(3):10:1?10:20.
Kasper Hornbaek and Morten Hertzum. 2011. The
notion of overview in information visualization. In-
ternational Journal of Human-Computer Studies,
69:509?525.
Charles E. Jacobs, Adam Finkelstein, and David H.
Salesin. 1995. Fast multiresolution image query-
ing. In Proceedings of the 22nd annual conference
on Computer Graphics and Interactive Techniques
(SIGGRAPH ?95), pages 277?286, New York, NY,
USA.
Jiwoon Jeon, Victor Lavrenko, and Raghavan Man-
matha. 2003. Automatic image annotation and re-
trieval using cross-media relevance models. In Pro-
ceedings of the 26th annual international ACM SI-
GIR Conference on Research and Development in
Information Retrieval (SIGIR ?03), pages 119?126,
New York, NY, USA.
Thorsten Joachims, Dayne Freitag, and Tom Mitchell.
1997. Webwatcher: A tour guide for the world wide
web. In Proceedings of the International Joint Con-
ference on Artificial Intelligence (IJCAI ?97), pages
770?777.
Tomi Kauppinen, Kimmo Puputti, Panu Paakkarinen,
Heini Kuittinen, Jari Va?a?ta?inen, and Eero Hyvo?nen.
2009. Learning and visualizing cultural heritage
connections between places on the semantic web.
In Proceedings of the Workshop on Inductive Rea-
soning and Machine Learning on the Semantic Web
(IRMLeS2009) and the 6th Annual European Se-
mantic Web Conference (ESWC2009), Heraklion,
Crete, Greece.
Marco La Cascia, Sarathendu Sethi, and Stan Sclaroff.
1998. Combining textual and visual cues for
content-based image retrieval on the world wide
web. In IEEE Workshop on Content-Based Access
of Image and Video Libraries, pages 24?28.
Michael Lesk. 1986. Automatic Sense Disambigua-
tion using Machine Readable Dictionaries: how to
tell a pine cone from an ice cream cone. In Proceed-
ings of the ACM Special Interest Group on the De-
sign of Communication Conference (SIGDOC ?86),
pages 24?26, Toronto, Canada.
Nicolas Loeff, Cecilia Ovesdotter Alm, and David A.
Forsyth. 2006. Discriminating image senses by
clustering with multimodal features. In Proceed-
ings of the COLING/ACL on Main Conference
Poster Sessions (COLING-ACL ?06), pages 547?
554, Stroudsburg, PA, USA.
David G. Lowe. 1999. Object Recognition from Local
Scale-invariant Features. Proceedings of the Sev-
enth IEEE International Conference on Computer
Vision, pages 1150?1157.
David G. Lowe. 2004. Distinctive Image Fea-
tures from Scale-Invariant Keypoints. International
Journal of Computer Vision, 60(2):91?110.
Christopher D. Manning and Hinrich Schutze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. The MIT Press.
Gary Marchionini. 2006. Exploratory Search: from
Finding to Understanding. Communications of the
ACM, 49(1):41?46.
David Milne. 2007. Computing Semantic Relatedness
using Wikipedia Link Structure. In Proceedings of
the New Zealand Computer Science Research Stu-
dent Conference.
Patrick Ndjiki-Nya, Oleg Novychny, and Thomas Wie-
gand. 2004. Merging MPEG 7 Descriptors for Im-
age Content Analysis. In Proceedings of IEEE In-
ternational Conference on Acoustics, Speech, and
Signal Processing (ICASSP ?04), pages 5?8.
Siddhard Patwardhan, Satanjeev Banerjee, and Ted
Pedersen. 2003. Using Measures of Semantic Re-
latedness for Word Sense Disambiguation. In Pro-
ceedings of the 4th International Conference on In-
telligent Text Processing and Computational Lin-
guistics, pages 241?257.
Mykola Pechenizkzy and Toon Calders. 2007. A
framework for guiding the museum tours personal-
ization. In Proceedings of the Workshop on Person-
alised Access to Cultural Heritage (PATCH ?07),
pages 11?28.
Koen E.A. Sande, Theo Gevers, and Cees G. M.
Snoek. 2008. Evaluation of Color Descriptors for
Object and Scene Recognition. In Proceedings of
the IEEE Computer Society Conference on Com-
puter Vision and Pattern Recognition (CVPR ?08),
pages 1?8.
92
Bernt Schiele and James L. Crowley. 1996. Object
recognition using multidimensional receptive field
histograms. In Proceedings of the 4th European
Conference on Computer Vision (ECCV ?96), pages
610?619, London, UK.
Nicu Sebe and Michael S. Lew. 2001. Color-based
Retrieval. Pattern Recognition Letters, 22:223?
230, February.
Rohini K. Srihari, Aibing Rao, Benjamin Han,
Srikanth Munirathnam, and Xiaoyun Wu. 2000. A
model for multimodal information retrieval. In Pro-
ceedings of the IEEE International Conference on
Multimedia and Expo (ICME ?00), pages 701?704.
Grant Strong and Minglun Gong. 2009. Organizing
and Browsing Photos using Different Feature Vec-
tors and their Evaluations. Proceedings of the ACM
International Conference on Image and Video Re-
trieval (CIVR ?09), pages 3:1?3:8.
Michael J. Swain and Dana H. Ballard. 1991. Color
indexing. International Journal of Computer Vi-
sion, 7:11?32.
Richard Szeliski. 2010. Computer Vision: Algorithms
and Applications. Springer-Verlag Inc. New York.
Xin-Jing Wang, Wei-Ying Ma, Gui-Rong Xue, and
Xing Li. 2004. Multi-model similarity propaga-
tion and its application for web image retrieval.
In Proceedings of the 12th annual ACM Interna-
tional Conference on Multimedia (MULTIMEDIA
?04), pages 944?951, New York, NY, USA.
Yiwen Wang, Natalia Stash, Lora Aroyo, Peter
Gorgels, Lloyd Rutledge, and Guus Schreiber.
2008. Recommendations based on semantically-
enriched museum collections. Journal of Web Se-
mantics: Science, Services and Agents on the World
Wide Web, 6(4):43?50.
Yiwen Wang, Lora Aroyo, Natalia Stash, Rody Sam-
beek, Schuurmans Yuri, Guus Schreiber, and Pe-
ter Gorgels. 2009. Cultivating personalized mu-
seum tours online and on-site. Journal of Interdis-
ciplinary Science Reviews, 34(2):141?156.
Thijs Westerveld. 2002. Probabilistic multimedia re-
trieval. In Proceedings of the 25th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR ?02),
pages 437?438, New York, NY, USA.
Hui Yu, Mingjing Li, Hong-Jiang Zhang, and Jufu
Feng. 2002. Color Texture Moments for Content-
based Image Retrieval. In Proceedings of the
IEEE International Conference on Image Process-
ing (ICIP ?02), pages 929?932.
Xiang Sean Zhou and Thomas S. Huang. 2002. Uni-
fying keywords and visual contents in image re-
trieval. IEEE Multimedia, 9(2):23 ?33.
93
Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 101?106,
Avignon, France, 24 April 2012. c?2012 Association for Computational Linguistics
Adapting Wikification to Cultural Heritage
Samuel Fernando and Mark Stevenson
Department of Computer Science
Regent Court
211 Portobello
Sheffield, S1 4DP
s.fernando@shef.ac.uk
m.stevenson@dcs.shef.ac.uk
Abstract
Large numbers of cultural heritage items
are now archived digitally along with ac-
companying metadata and are available to
anyone with internet access. This informa-
tion could be enriched by adding links to
resources that provide background informa-
tion about the items. Techniques have been
developed for automatically adding links
to Wikipedia to text but the methods are
general and not designed for use with cul-
tural heritage data. This paper explores a
range of methods for adapting a system for
adding links to Wikipedia to cultural her-
itage items. The approaches make use of
the structure of Wikipedia, including the
category hierarchy. It is found that an ap-
proach that makes use of Wikipedia?s link
structure can be used to improve the quality
of the Wikipedia links that are added.
1 Introduction
Cultural heritage (CH) items are now increasingly
being digitised and stored online where they can
be viewed by anyone with a web browser. These
items are usually annotated with metadata which
gives the title of the item, subject keywords, de-
scriptions and so on. However such metadata
can often be very limited, with some items hav-
ing very little metadata at all. This paper exam-
ines methods to enrich such metadata with inline
links to Wikipedia. These links allow users to find
interesting background information on the items
and related topics, and provides a richer expe-
rience especially where the metadata is limited.
Additionally the links may also help to categorise
and organise the collections using the Wikipedia
category hierarchy.
CH items from Europeana1 are used for the
evaluation. Europeana is a large online aggrega-
tion of cultural heritage collections from across
Europe. The WikiMiner software (Milne and
Witten, 2008) is used to automatically enrich
the Europeana items collections with Wikipedia
links. Two methods are used to improve the
quality of the links. The first makes use of the
Wikipedia category hierarchy. Top-level cate-
gories of interest are selected and articles close
to these categories are used as training data for
WikiMiner. The second method uses existing
links from Wikipedia as evidence to find useful
links for the CH items.
2 Background
Mihalcea and Csomai (2007) first addressed the
task of automatically adding inline Wikipedia
links into text and coined the term Wikification
for the process. Their procedure for wikification
used two stages. The first stage was detection,
which involved identifying the terms and phrases
from which links should be made. The most ac-
curate method for this was found to be using link
probability, defined as the number of Wikipedia
articles that use the term as an anchor, divided by
the number of Wikipedia articles that mention it
at all. The next stage, disambiguation ensure that
the detected phrases link to the appropriate arti-
cle. For example the term plane usually links to
an article about fixed wing aircraft. However it
sometimes points to a page describing the mathe-
matical concept of a theoretical surface, or of the
tool for flattening wooden surfaces. To find the
correct destination a classifier is trained using fea-
tures from the context. Although the quality of re-
1http://www.europeana.eu
101
sults obtained is very good, a large amount of pre-
processing is required, since the entire Wikipedia
encyclopedia must be parsed.
Milne and Witten (2008) build upon this previ-
ous work with the WikiMiner program. The soft-
ware is trained on Wikipedia articles, and thus
learns to disambiguate and detect links in the
same way as Wikipedia editors. Disambiguation
of terms within the text is performed first. A
machine-learning classifier is used with several
features. The main features used are commonness
and relatedness, as in Medelyan et al (2008). The
commonness of a target sense is defined by the
number of times it is used a destination from some
anchor text e.g. the anchor text ?Tree? links to the
article about the plant more often than the math-
ematical concept and is thus more common. Re-
latedness gives a measure of the similarity of two
articles by comparing their incoming and outgo-
ing links. The performance achieved using their
approach is currently state of the art for this task.
The WikiMiner software is freely available2, and
has been used as the basis for the approaches pre-
sented here.
Recent work on named entity linking and wik-
ification makes use of categories and link infor-
mation (Bunescu and Pasca, 2006; Dakka and
Cucerzan, 2008; Kulkarni et al, 2009). Wikifi-
cation has also been applied to the medical do-
main (He et al, 2011). Wikipedia categories and
links have been used previously to find the sim-
ilarity between CH items (Grieser et al, 2011).
The category retraining approach presented here
differs in that it only makes use of the top-level
categories.
3 Methods
Three approaches to improving the quality of
Wikipedia links added by WikiMiner were devel-
oped. The first two make use of Wikipedia?s cat-
egory structure while the third uses the links be-
tween Wikipedia articles.
3.1 Wikipedia Categories
Almost all articles in Wikipedia are manually as-
signed to one or more categories. For example the
page ALBERT EINSTEIN belongs to the categories
Swiss physicists, German-language philoso-
phers and several others. The category pages thus
2http://wikipedia-miner.cms.waikato.
ac.nz/
group together articles of interest. Furthermore,
each category may itself be a sub-category of one
or more categories. So for example Swiss physi-
cists is a sub-category of the categories Swiss
scientists, Physicists by nationality etc.
The categories give a general indication of the
topic of the article and we assume that articles rel-
evant to Cultural Heritage items are likely to be
closely associated with certain categories.
3.2 Retraining using Categories
The first approach is to retrain WikiMiner us-
ing articles associated with particular categories.
Three top-level categories manually judged to in-
dicate articles that are relevant to cultural her-
itage were selected: Culture, Arts and Human-
ities. All articles within 2 links of these selected
categories were found and used as training data
for WikiMiner. (We also explored using differ-
ent numbers of links but found that fewer than
2 links produced a very small number of arti-
cles while more than 2 generated very large num-
bers which would be prohibitively expensive for
retraining.) The same approach was also tested
with categories which are unlikely to be related to
cultural heritage (Computers, Mathematics and
Science) in order to test the effect of using dif-
ferent categories.
3.3 Filtering using Categories
This approach uses the category information to
filter articles after WikiMiner has been run. Each
article added by WikiMiner is examined and any
which are more than a certain distance from a top-
level category which has been identified as being
relevant to cultural heritage is removed. The as-
sumption behind this approach is that relevant ar-
ticles are much more likely to be closely associ-
ated with these categories than ones which are not
relevant.
3.4 Exploiting Wikipedia?s Link Structure
The final method makes use of Wikipedia?s link
structure rather than the category hierarchy and is
similar to the previous method since it filters the
links added by WikiMiner to identify those which
are relevant to a particular article.
The first stage is to run the item through
WikiMiner to detect suitable links. This is done
with 2 parameter settings, each returning a set of
links. The aim of the first run is to find as many
102
!"#$%&'(%#)*+
,$-./0$-1+/(-#
23$++$)&456&+#-(#+7
'*%$89&4:;(3"(%<7
!"#$%&#'"
!"#$%&'()%*+',"+%-./'0$*11*-'2#3%%#/'4*35/'6*3#7'4*3517"3%
289:%;#&',"+%-.
<%1;3"=#"*+&'>?#%3"*3'@"%A'*B'#7%';.+*=CD0$-1
=.#8/#%&>$+#8/&5/#*.-#&
?*;3&@-#<<
A(""3#+:-$;</
B(%"3#9C&,$-./&0$-1+/(-#&
!"!#
$#
Figure 1: Example illustrating the method, where articles (on the left) which link to the high precision articles
(SP ) are used to find good links in the high recall set (SR).
potential links in the text as possible, for example
by using a low confidence threshold. This give a
set of links SR which is high recall (because most
links are included), but low precision, since many
incorrect or irrelevant links are also present. The
aim of the second run is to find a smaller set of
links which are likely to be of good quality, for
example by setting a high confidence threshold.
The resulting set SP is high precision but lower
recall since good links may be discarded.
The result set of links is initialised with the high
precision articles R = SP . The aim is then to try
to find additional good links within SR. This is
done by finding a list of articles AP which con-
tain links to 1 or more of the articles in SP . Let
O(a) be the set of outlinks from an article a. Each
article inAP is then scored on how many links are
shared with SP :
?a ? AP : score(a) = |O(a) ? SP | (1)
The N top scoring articles in AP are then used
to find further good links with within SR. For
each of these articles a:
R ??= R ? (O(a) ? SR) (2)
Figure 1 gives an example illustrating how the
method works on an Europeana item about an
old Odeon Cinema in York. The article on Paul
Gregg links to the articles in the SP set {Odeon
Cinemas, North Yorkshire}. Since it also links
to the York article in the SR set, the method takes
this as evidence that Yorkmight also be a good ar-
ticle to link to, and so this would be added to the
result set R.
4 Annotation
To evaluate the quality of the Wikipedia links, a
sample of CH items was manually annotated. The
sample of 21 items was randomly selected from
Europeana. When run through WikiMiner with
no probability threshold (i.e. including all possi-
ble links), a total of 366 potential links were iden-
tified. A further 16 links were manually added
which the WikiMiner software had missed, giving
a total of 381 links.
Web surveys were created to allow the annota-
tors to judge the links. For each item in the survey
users were presented with a picture of the item,
the metadata text, and the set of possible links
(with the anchor text identified). The annotators
were then given a binary choice for each link to
decide if it should be included or not.
Two separate surveys were taken by three flu-
ent English speakers. The first was to determine if
each link was correctly disambiguated within the
context of the item (regardless of whether the link
was useful or appropriate for that item). For each
link the majority decision was used to judge if the
link was indeed correct or not. Out of the 381
links, 70% were judged to be correct and 30% as
incorrect. For 80% of the links the judgement was
unanimous with all 3 annotators agreeing on the
correctness of the links. The remaining 20% were
2-to-1 judgements. This gives an overall inter-
annotator agreement of 93.4%.
The second survey was to determine which of
the correct links were useful and appropriate for
103
the corresponding items. As before each of the
21 items was presented to the annotators, but this
time only with the 267 links that had been judged
as correct within the previous survey. Again,
three annotators completed the survey. Out of the
267 correct links, 49.8% were judged to be use-
ful/appropriate and 50.2% as not. For 67.7% of
the links the judgement was unanimous. The re-
maining 32.2% were 2-1 judgements. This gives
an inter-annotator agreement of 89.3%. The 133
links judged to be correct, useful and appropriate
were then used as the gold standard to evaluate the
automatic methods.
As an example, the links and judgements for
the following text are shown in Table 1:
Title: Odeon Cinema, Blossom Street, York,
North Yorkshire
Subject: Cinema
Description: Exterior view of the canopy.
Link Correct Useful
Odeon Cinemas Yes Yes
Blossom (TV series) No N/A
York Yes Yes
North Yorkshire Yes No
Cinema Yes No
Canopy Yes Yes
Table 1: Examples of links and judgements
5 Experiments
The methods from Section 3 were used to identify
links in the items from Europeana. The results
were evaluated against the gold standard manu-
ally annotated data that was described in Section
4. For all experiments the standard metrics of pre-
cision, recall and F-measure are used to measure
the performance of the methods.
Milne and Witten (2008) noted that training us-
ing articles with a similar length and link density
to the target documents can improve WikiMiner?s
performance. The descriptions associated with
Europeana items are relatively short so further ex-
periments were carried out in which WikiMiner
was retrained with different sets of articles. The
best results were obtained using a set of arti-
cles between 100 and 500 words that contained
a minimum of five links to other articles. (Re-
sults for experiments comparing other configura-
tions are not reported here for brevity.) Table 2
shows results obtained using the default model,
when WikiMiner is run ?off the shelf?, and when
it has been retrained. These results demonstrate
that retraining WikiMiner improves performance.
Precision improves to over 50% and, although
there is a drop in recall, F-measure is also higher.
Results using the retrained model are used as a
baseline against which alternative approaches are
compared.
Model P R F
Default 34.0 91.7 49.6
Retrained 56.6 77.4 65.4
Table 2: Results obtained using WikiMiner using de-
fault model and after retraining
5.1 Category retraining
The category retraining approach (Section 3.2)
was applied using all articles within two links of
selected categories as training data forWikiMiner.
The results are shown in Table 3 and show that
precision is improved over the baseline for all cat-
egories. However the results do not fit the hy-
pothesis, with Science giving the best F-measure
overall, a statistically significant improvement
over the baseline (p < 0.05, t-test). This may be
for various reasons. Firstly the category hierarchy
in Wikipedia is often messy with articles assigned
to many different categories, and each category
can contain a diverse sets of articles which may
not be very useful. Secondly it may be that the
topics of the articles are not so important for the
training, but rather factors like the length of the
articles and the link densities. However it is in-
teresting that using articles close to the top level
categories does appear to improve performance.
Method P R F
Baseline 56.6 77.4 65.4
Culture 65.5 71.4 68.3
Arts 69.6 65.4 67.4
Humanities 71.9 65.4 68.5
Mathematics 72.9 58.6 65.0
Science 72.4 69.1 70.8
Computers 76.7 59.4 66.9
Table 3: Retraining using top level categories.
104
5.2 Category filtering
The category filtering approach (Section 3.3) was
applied. Articles within a distance of 1 to 4 links
from selected top level categories are kept and
all others are discarded. The following combina-
tions of categories were used: C (Culture), CHA
(Culture, Humanities and Arts), and CHAGSE
(Culture, Humanities, Arts, Geography, Soci-
ety and Education).
Results are shown in Table 4 and are surpris-
ingly low. Both precision and recall drop signif-
icantly when category filtering is applied. This
may be because the articles within categories are
often very diverse and do not capture many of
the possible topics found within cultural heritage
items.
Method Precision Recall F
Baseline 56.6 77.4 65.4
C 35.1 19.5 25.1
CHA 27.4 27.8 27.6
CHAGSE 24.5 34.6 28.7
Table 4: Filtering using top level categories.
5.3 Using Wikipedia links
The final experiment explores the link filtering ap-
proach described in Section 3.4. The high preci-
sion SP set is chosen to be those returned by the
retrained WikiMiner model (?Retrained? in Table
2) while the high recall SR set is the default model
(?Default? in Table 2). Experiments were per-
formed varyingN , the number of top scoring arti-
cles used (using the score metric defined in Equa-
tion 1).
No. of similar articles P R F
Baseline 56.6 77.4 65.4
1 74.0 53.4 62.0
2 70.7 61.7 65.9
3 68.5 63.9 66.1
4 67.4 68.4 67.9
5 66.9 69.9 68.4
6 66.2 70.6 68.4
7 66.2 70.7 68.4
8 65.5 71.4 68.3
9 65.1 71.4 68.1
10 63.9 72.9 68.1
Table 5: Filtering using Wikipedia?s link structure
The results are shown in Table 5 and show a
clear improvement in precision for N from 1 to 10.
The F-measure peaks when 5-7 related articles are
used. The improvement in the F-measure over
the baseline is statistically significant (p < 0.05
t-test).
6 Conclusions and future work
This paper explores a variety of methods for im-
proving the quality of Wikipedia links added by
the WikiMiner software when applied to the cul-
tural heritage domain. Approaches that make
use of the Wikipedia category hierarchy and link
structure were compared and evaluated using a
data set of manual judgements created for this
study.
The approaches based on the category hier-
archy appeared to be less promising than those
which used the link structure. Improvements were
obtained by retraining WikiMiner using articles
associated with particular categories. However
the results were unexpected, with categories such
as Science giving better performance as train-
ing data than categories such as Culture or Arts.
Although a higher score was obtained using this
method than the link approach, this may be due to
factors such as document length and link density
rather than the topic of the articles.
Results obtained using a novel method based
on existing links within Wikipedia suggest this
approach is promising. The method is fully un-
supervised so it can be easily applied to domains
other than cultural heritage.
Information from both categories and links
could be combined in a similar way to that sug-
gested by Grieser et al (2011). Enriching cultural
heritage data with Wikipedia links should im-
prove the experience for users while they browse
the data. In addition the links themselves may be
useful to categorise, cluster and find similar items.
Further work will investigate these possibilities.
Acknowledgments
The research leading to these results was
carried out as part of the PATHS project
(http://paths-project.eu) funded by
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 270082.
105
References
Razvan Bunescu and Marius Pasca. 2006. Using
Encyclopedic Knowledge for Named Entity Dis-
ambiguation. In Proceedings of European Chap-
ter of the Association of Computational Linguistics
(EACL), volume 6, pages 9?16.
Wisam Dakka and Silviu Cucerzan. 2008. Augment-
ing Wikipedia with Named Entity Tags. In Pro-
ceedings of The Third International Joint Confer-
ence on Natural Language Processing (IJCNLP).
Karl Grieser, Timothy Baldwin, Fabian Bohnert, and
Liz Sonenberg. 2011. Using Ontological and Doc-
ument Similarity to Estimate Museum Exhibit Re-
latedness. Journal on Computing and Cultural Her-
itage (JOCCH), 3(3):10.
Jiyin He, Maarten de Rijke, Maarten de Rijke, Rob
van Ommering, and Yuechen Qian. 2011. Generat-
ing Links to Background Knowledge: A Case Study
Using Narrative Radiology Reports. In 20th ACM
Conference on Information and Knowledge Man-
agement (CIKM), pages 1867?1876, Glasgow.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective An-
notation of Wikipedia Entities in Web Text. In
Proceedings of the 15th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and
Data Mining, pages 457?466.
Olena Medelyan, Ian H. Witten, and David Milne.
2008. Topic Indexing with Wikipedia. In Proceed-
ings of the Association for the Advancement of Ar-
tificial Intelligence (AAAI) WikiAI workshop.
Rada Mihalcea and Andras Csomai. 2007. Wikify!:
Linking Documents to Encyclopedic Knowledge.
In ACM Sixteenth Conference on Information and
Knowledge Management (CIKM), volume 7, pages
233?242.
David Milne and Ian H. Witten. 2008. Learning to
Link with Wikipedia. In Proceeding of the 17th
ACM conference on Information and Knowledge
Management, pages 509?518.
106
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 231?239,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Scaling up WSD with Automatically Generated Examples
Weiwei Cheng, Judita Preiss and Mark Stevenson
Department of Computer Science,
Sheffield University,
Regent Court, 211 Portobello,
Sheffield, S1 4DP
United Kingdom
{W.Cheng, J.Preiss, M.Stevenson}@dcs.shef.ac.uk
Abstract
The most accurate approaches to Word Sense
Disambiguation (WSD) for biomedical docu-
ments are based on supervised learning. How-
ever, these require manually labeled training
examples which are expensive to create and
consequently supervised WSD systems are
normally limited to disambiguating a small set
of ambiguous terms. An alternative approach
is to create labeled training examples automat-
ically and use them as a substitute for manu-
ally labeled ones. This paper describes a large
scale WSD system based on automatically la-
beled examples generated using information
from the UMLS Metathesaurus. The labeled
examples are generated without any use of la-
beled training data whatsoever and is therefore
completely unsupervised (unlike some previ-
ous approaches). The system is evaluated on
two widely used data sets and found to outper-
form a state-of-the-art unsupervised approach
which also uses information from the UMLS
Metathesaurus.
1 Introduction
The information contained in the biomedical liter-
ature that is available in electronic formats is use-
ful for health professionals and researchers (West-
brook et al, 2005). The amount is so vast that
it is difficult for researchers to identify informa-
tion of interest without the assistance of automated
tools (Krallinger and Valencia, 2005). However,
processing these documents automatically is made
difficult by the fact that they contain terms that
are ambiguous. For example, ?culture? can mean
?laboratory procedure? (e.g. ?In peripheral blood
mononuclear cell culture?) or ?anthropological cul-
ture? (e.g. ?main accomplishments of introducing a
quality management culture?). These lexical ambi-
guities are problematic for language understanding
systems.
Word sense disambiguation (WSD) is the process
of automatically identifying the meanings of am-
biguous terms. Some WSD systems for the biomed-
ical domain are only able to disambiguate a small
number of ambiguous terms (see Section 2). How-
ever, for WSD systems to be useful in applications
they should be able to disambiguate all ambiguous
terms. One way to create such a WSD system is to
automatically create the labeled data that is used to
train supervised WSD systems. Several approaches
(Liu et al, 2002; Stevenson and Guo, 2010; Jimeno-
Yepes and Aronson, 2010) have used information
from the UMLS Metathesaurus1 to create labeled
training data that have successfully been used to cre-
ate WSD systems.
A key decision for any system that automatically
generates labeled examples is the number of exam-
ples of each sense to create, known as the bias of the
data set. It has been shown that the bias of a set of la-
beled examples affects the performance of the WSD
system it is used to train (Mooney, 1996; Agirre and
Mart??nez, 2004b). Some of the previous approaches
to generating labeled data relied on manually anno-
tated examples to determine the bias of the data sets
and were therefore not completely unsupervised.
This paper describes the development of a large
scale WSD system that is able to disambiguate all
1http://www.nlm.nih.gov/research/umls/
231
terms that are ambiguous in the UMLS Metathe-
saurus. The system relies on labeled examples that
are created using information from UMLS. Various
bias options are explored, including ones that do not
make use of information from manually labeled ex-
amples, and thus we can create a completely unsu-
pervised system. Evaluation is carried out on two
standard datasets (the NLM-WSD and MSH-WSD
corpora). We find that WSD systems can be cre-
ated without using any information from manually
labeled examples and that their performance is bet-
ter than a state-of-the-art unsupervised approach.
The remainder of this paper is organized as fol-
lows. Previous approaches to WSD in biomedical
documents are described in the next Section. Section
3 presents the methods used to identify bias in the
labeled examples and WSD system. Experiments in
which these approaches are compared are described
in Section 4 and their results in Section 5.
2 Background
Many WSD systems for the biomedical domain are
based on supervised learning (McInnes et al, 2007;
Xu et al, 2007; Stevenson et al, 2008; Yepes and
Aronson, 2011). These systems require labeled
training data, examples of an ambiguous term la-
beled with the correct meaning. Some sets of labeled
data have been developed for the biomedical domain
(Weeber et al, 2001; Savova et al, 2008; Jimeno-
Yepes et al, 2011). However, these data sets only
contain examples for a few hundred terms and can
only be used to develop WSD systems to identify
the meanings of those terms. The process of creat-
ing labeled examples is extremely time-consuming
and difficult (Artstein and Poesio, 2008), making it
impractical to create labeled examples of all possible
ambiguous terms found in biomedical documents.
Two alternative approaches have been explored to
develop systems which are able to disambiguate all
ambiguous terms in biomedical documents. The first
makes use of unsupervised WSD algorithms (see
Section 2.1) and the second creates labeled data au-
tomatically and uses it to train a supervised WSD
system (see Section 2.2).
2.1 Unsupervised WSD
Unsupervised WSD algorithms make use of infor-
mation from some knowledge source, rather than re-
lying on training data.
Humphrey et al (2006) describe an unsupervised
system which uses semantic types in UMLS to dis-
tinguish between the possible meanings of ambigu-
ous words. However, it cannot disambiguate be-
tween senses with the same semantic type, i.e., it
is not possible for the system to recognise all sense
distinctions.
The Personalised Page Rank (PPR) system
(Agirre et al, 2010; Jimeno-Yepes and Aronson,
2010) relies on a a graph-based algorithm similar
to the Page Rank algorithm originally developed for
use in search engines (Brin, 1998). It performs
WSD by converting the UMLS Metathesaurus into
a graph in which the possible meanings of ambigu-
ous words are nodes and relations between them are
edges. Disambiguation is carried out by providing
the algorithm with a list of senses that appear in the
text that is being disambiguated. This information is
then combined with the graph and a ranked list of the
possible senses for each ambiguous word generated.
Unsupervised systems have the advantage of be-
ing able to disambiguate all ambiguous terms. How-
ever, the performance of unsupervised systems that
have been developed for biomedical documents is
lower than that of supervised ones.
2.2 Automatic Generation of Labeled Data
Automatic generation of labeled data for WSD com-
bines the accuracy of supervised approaches with
the ability of unsupervised approaches to disam-
biguate all ambiguous terms. It was first suggested
by Leacock et al (1998). Their approach is based
on the observation that some terms in a lexicon oc-
cur only once and, consequently, there is no doubt
about their meaning. These are referred to as being
monosemous. Examples for each possible meaning
of an ambiguous term are generated by identifying
the closest monosemous term (the monosemous rel-
ative) in the lexicon and using examples of that term.
Variants of the approach have been applied to the
biomedical domain using the UMLS Metathesaurus
as the sense inventory.
232
Liu et al (2002) were the first to apply the
monosemous relatives approach to biomedical WSD
and use it to disambiguate a set of 35 abbreviations.
They reported high precision but low recall, indicat-
ing that labeled examples could not be created for
many of the abbreviations. Jimeno-Yepes and Aron-
son (2010) applied a similar approach and found
that it performed better than a number of alternative
approaches on a standard evaluation resource (the
NLM-WSD corpus) but did not perform as well as
supervised WSD. Stevenson and Guo (2010) com-
pared two techniques for automatically creating la-
beled data, including the monosemous relatives ap-
proach. They found that the examples which were
generated were as good as manually labeled exam-
ples when used to train a supervised WSD system.
However, Stevenson and Guo (2010) relied on la-
beled data to determine the number of examples of
each sense to create, and therefore the bias of the
data set. Consequently their approach is not com-
pletely unsupervised since it could not be applied to
ambiguous terms that do not have labeled training
data available.
3 Approach
3.1 WSD System
The WSD system is based on a supervised approach
that has been adapted for the biomedical domain
(Stevenson et al, 2008). The system was tested on
the NLM-WSD corpus (see Section 4.1) and found
to outperform alternative approaches.
The system can exploit a wide range of fea-
tures, including several types of linguistic informa-
tion from the context of an ambiguous term, MeSH
codes and Concept Unique Identifiers (CUIs) from
the UMLS Metathesaurus. However, computing
these features for every example is a time consum-
ing process and to make the system suitable for large
scale WSD it was restricted to using a smaller set
of features. Previous experiments (Stevenson et al,
2008) showed that this only leads to a small drop in
disambiguation accuracy while significantly reduc-
ing the computational cost of generating features.
3.1.1 Features
Two types of context words are used as features:
the lemmas of all content words in the same sen-
tence as the ambiguous word and the lemmas of all
content words in a?4-word window around the am-
biguous term. A list of corpus-specific stopwords
was created containing terms that appear frequently
in Medline abstracts but which are not useful for dis-
ambiguation (e.g. ?abstract?, ?conclusion?). Any
lemmas found in this list were not used as features.
3.1.2 Learning algorithm
Disambiguation is carried out using the Vector
Space Model, a memory-based learning algorithm
in which each occurrence of an ambiguous word is
represented as a vector created using the features ex-
tracted to represent it (Agirre and Mart??nez, 2004a).
The Vector Space Model was found to outperform
other learning algorithms when evaluated using the
NLM-WSD corpus (Stevenson et al, 2008).
During the algorithm?s training phase a single
centroid vector, ~Csj , is generated for each possible
sense, sj . This is shown in equation 1 where T is
the set of training examples for a particular term and
sense(~t) is the sense associated with the vector ~t.
~Csj =
?
~ti  T :sense(~ti)=sj
~ti
|~ti  T : sense(~ti) = sj |
(1)
Disambiguation is carried out by comparing the
vector representing the ambiguous word, ~a, against
the centroid of each sense using the cosine metric,
shown in equation 2, and choosing the one with the
highest score.
score(sj ,~a) = cos( ~Csj ,~a) =
~Csj .~a
| ~Csj ||~a|
(2)
Note that the learning algorithm does not ex-
plicitly model the prior probability of each possi-
ble sense, unlike alternative approaches (e.g. Naive
Bayes), since it was found that including this infor-
mation did not improve performance.
3.2 Automatically generating training
examples
The approaches used for generating training exam-
ples used here are based on the work of Stevenson
and Guo (2010), who describe two approaches:
1. Monosemous relatives
2. Co-occurring concepts
233
Both approaches are provided with a set of ambigu-
ous CUIs from the UMLS Metathesaurus, which
represent the possible meanings of an ambiguous
term, and a target number of training examples to be
generated for each CUI. Each CUI is associated with
at least one term and each term is labeled with a lex-
ical unique identifier (LUI) which represents a range
of lexical variants for a particular term. The UMLS
Metathesaurus contains a number of data files which
are exploited within these techniques, including:
AMBIGLUI: a list of cases where a LUI is linked
to multiple CUIs.
MRCON: every string or concept name in the
Metathesaurus appears in this file.
MRCOC: co-occuring concepts.
For the monosemous relatives approach, the
strings of monosemous LUIs of the target CUI
and its relatives are used to search Medline to re-
trieve training examples. The monosemous LUIs re-
lated to a CUI are defined as any LUIs associated
with the CUI in the MRCON table and not listed in
AMBIGLUI table.
The co-occurring concept approach works differ-
ently. Instead of using strings of monosemous LUIs
of the target CUI and its relatives, the strings associ-
ated with LUIs of a number of co-occurring CUIs of
the target CUI and its relatives found in MRCOC ta-
ble are used. The process starts by finding the LUIs
of the top n co-occurring CUIs of the target CUI.
These LUIs are then used to form search queries.
The query is quite restrictive in the beginning and re-
quires all terms appear in the Medline citations files.
Subsequently queries are made less restrictive by re-
ducing the number of required terms in the query.
These techniques were used to generate labeled
examples for all terms that are ambiguous in the
2010 AB version of the UMLS Metathesaurus.2 The
set of all ambiguous terms was created by analysing
the AMBIGLUI table, to identify CUIs that are asso-
ciated with multiple LUIs. The Medline Baseline
Repository (MBR)3 was also analysed and it was
found that some terms were ambiguous in this re-
source, in the sense that more than one CUI had been
2Stevenson and Guo (2010) applied them to a small set of
examples from the NLM-WSD data set (see Section 4.1).
3http://mbr.nlm.nih.gov
assigned to an instance of a term, but could not be
identified from the AMBIGLUI table. The final list
of ambiguous CUIs was created by combining those
identified from the AMBIGLUI table and those find
in the MBR. This list contained a total of 103,929
CUIs.
Both techniques require large number of searches
over the Medline database and to carry this out ef-
ficiently the MBR was indexed using the Lucene
Information Retrieval system4 and all searches ex-
ecuted locally.
Examples were generated using both approaches.
The monosemous relatives approach generated ex-
amples for 98,462 CUIs and the co-occurring con-
cepts for 98,540. (Examples generated using the
monosemous relatives approach were preferred for
the experiments reported later.) However, neither
technique was able to generate examples for 5,497
CUIs, around 5% of the total. This happened when
none of the terms associated with a CUI returned
any documents when queried against the MBR and
that CUI does not have any monosemous relatives.
An example is C1281723 ?Entire nucleus pulpo-
sus of intervertebral disc of third lumbar vertebra?.
The lengthy terms associated with this CUI do not
return any documents when used as search terms
and, in addition, it is only related to one other CUI
(C0223534 ?Structure of nucleus pulposus of inter-
vertebral disc of third lumbar vertebra?) which is it-
self only connected to C1281723. Fortunately there
are relatively few CUIs for which no examples could
be generated and none of them appear in the MBR,
suggesting they refer to UMLS concepts that do not
tend to be mentioned in documents.
3.3 Generating Bias
Three different techniques for deciding the number
of training examples to be generated for each CUI
(i.e. the bias) were explored.
Uniform Bias (UB) uses an equal number of
training examples to generate centroid vectors for
each of the possible senses of the ambiguous term.
Gold standard bias (GSB) is similar to the uni-
form bias but instead of being the same for all pos-
sible CUIs the number of training examples for each
CUI is determined by the number of times it appears
4http://lucene.apache.org/
234
in a manually labeled gold standard corpus. Assume
t is an ambiguous term and Ct is the set of possible
meanings (CUIs). The number of training examples
used to generate the centroid for that CUI, Ec, is
computed according to equation 3 where Gc is the
number of instances in the gold standard corpus an-
notated with CUI c and n is a constant which is set
to 100 for these experiments.5
Ec =
Gc
?
ci  Ct
Gci,t
.n (3)
The final technique, Metamap Baseline Repos-
itory Bias (MBB), is based on the distribution of
CUIs in the MBR. The number of training examples
are generated in a similar way to the gold standard
bias with MBR being used instead of a manually la-
beled corpus and is shown in equation 4 whereMc is
the number of times the CUI c appears in the MBR.
Ec =
Mc
?
ci  Ct
Mci
.n (4)
For example, consider the three possible CUIs as-
sociated with term ?adjustment? in the NLM-WSD
corpus: C0376209, C0456081 and C06832696.
The corpus contains 18 examples of C0376209,
62 examples of C0456081 and 13 of C0683269.
Using equation 3, the number of training exam-
ples when GSB is applied for C0376209 is 20,
67 for C0456081 and 14 for C0683269. In the
Metamap Baseline Repository files, C0376209 has
a frequency count of 98046, C0456081 a count of
292809 and C0683269 a count of 83530. Therefore
the number of training examples used for the three
senses when applying MBB is: 21 for C0376209, 62
for C0456081 and 18 for C0683269.
4 Evaluation
4.1 Data sets
We evaluate our system on two datasets: the NLM-
WSD and MSH-WSD corpora.
5Small values for Ec are rounded up to ensure that any rare
CUIs have at least one training example.
6These CUIs are obtained using the mappings from NLM-
WSD senses to CUIs available on the NLM website: http:
//wsd.nlm.nih.gov/collaboration.shtml
The NLM-WSD corpus7 (Weeber et al, 2001) has
been widely used for experiments on WSD in the
biomedical domain, for example (Joshi et al, 2005;
Leroy and Rindflesch, 2005; McInnes et al, 2007;
Savova et al, 2008). It contains 50 ambiguous terms
found in Medline with 100 examples of each. These
examples were manually disambiguated by 11 an-
notators. The guidelines provided to the annotators
allowed them to label a senses as ?None? if none
of the concepts in the UMLS Metathesaurus seemed
appropriate. These instances could not be mapped
onto UMLS Metathesaurus and were ignored for our
experiments.
The larger MSH-WSD corpus (Jimeno-Yepes et
al., 2011) contains 203 strings that are associated
with more than one possible MeSH code in the
UMLS Metathesaurus. 106 of these are ambiguous
abbreviations, 88 ambiguous terms and 9 a combi-
nation of both. The corpus contains up to 100 ex-
amples for each possible sense and a total of 37,888
examples of ambiguous strings taken from Medline.
Unlike the NLM-WSD corpus, all of the instances
can be mapped to the UMLS Metathesaurus and
none was removed from the dataset for our exper-
iments.
The two data sets differ in the way the number
of instances of each sense was determined. For
the NLM-WSD corpus manual annotation is used to
decide the number of instances that are annotated
with each sense of an ambiguous term. However,
the NLM-MSH corpus was constructed automati-
cally and each ambiguous term has roughly the same
number of examples of each possible sense.
4.2 Experiments
The WSD system described in Section 3 was tested
using each of the three techniques for determining
the bias, i.e. number of examples generated for each
CUI. Performance is compared against various alter-
native approaches.
Two supervised approaches are included. The
first, most frequent sense (MFS) (McCarthy et al,
2004), is widely used baseline for supervised WSD
systems. It consists of assigning each ambiguous
term the meaning that is more frequently observed
in the training data. The second supervised approach
7http://wsd.nlm.nih.gov
235
is to train the WSD system using manually labeled
examples from the NLM-WSD and MSH-WSD cor-
pora. 10-fold cross validation is applied to evaluate
this approach.
Performance of the Personalised Page Rank ap-
proach described in Section 2.1 is also provided to
allow comparison with an unsupervised algorithm.
Both Personalised Page Rank and the techniques
we employ to generate labeled data, base disam-
biguation decisions on information from the UMLS
Metathesaurus.
The performance of all approaches is measured
in terms of the percentage of instances which are
correctly disambiguated for each term with the av-
erage across all terms reported. Confidence inter-
vals (95%) computed using bootstrap resampling
(Noreen, 1989) are also shown.
5 Results
Results of the experiments are shown in Table 1
where the first three rows show performance of the
approach described in Section 3 using the three
methods for computing the bias (UB, MMB and
GSB). MFS and Sup refer to the Most Frequent
Sense supervised baseline and using manually la-
beled examples, respectively, and PPR to the Per-
sonalised PageRank approach.
When the performance of the approaches us-
ing automatically labeled examples (UB, MMB and
GSB) is compared it is not surprising that the best re-
sults are obtained using the gold standard bias since
this is obtained from manually labeled data. Results
using this technique for computing bias always out-
perform the other two, which are completely unsu-
pervised and do not make use of any information
from manually labeled data. However, the improve-
ment in performance varies according to the corpus,
for the NLM-WSD corpus there is an improvement
of over 10% in comparison to UB while the corre-
sponding improvement for the MSH-WSD corpus is
less than 0.5%.
A surprising result is that performance obtained
using the uniform bias (UB) is consistently better
than using the bias obtained by analysis of the MBR
(MMB). It would be reasonable to expect that in-
formation about the distribution of CUIs in this cor-
pus would be helpful for WSD but it turns out that
making no assumptions whatsoever about their rel-
ative frequency, i.e., assigning a uniform baseline,
produces better results.
The relative performance of the supervised (MFS,
Sup and GSB) and unsupervised approaches (UB,
MMB and PPR) varies according to the corpus. Un-
surprisingly using manually labeled data (Sup) out-
performs all other approaches on both corpora. The
supervised approaches also outperform the unsuper-
vised ones on the NLM-WSD corpus. However, for
the MSH-WSD corpus all of the unsupervised ap-
proaches outperform the MFS baseline.
A key reason for the differences in these results is
the different distributions of senses in the two cor-
pora, as shown by the very different performance of
the MFS approach on the two corpora. This is dis-
cussed in more detail later (Section 5.2).
Comparison of the relative performance of the un-
supervised approaches (UB, MMB and PPR) shows
that training a supervised system with the automat-
ically labeled examples using a uniform bias (UB)
always outperforms PPR. This demonstrates that
this approach outperforms a state-of-the-art unsu-
pervised algorithm that relies on the same infor-
mation used to generate the examples (the UMLS
Metathesaurus).
5.1 Performance by Ambiguity Type
The MSH-WSD corpus contains both ambiguous
terms and abbreviations (see Section 4.1). Perfor-
mance of the approaches on both types of ambiguity
are shown in Table 2.
MSH-WSD Ambiguity Type
Approach Abbreviation Term
UB 91.40 [91.00, 91.75] 72.68 [72.06, 73.32]
MMB 84.43 [83.97, 84.89] 69.45 [68.86, 70.10]
GSB 90.82 [90.45, 91.22] 73.96 [73.40, 74.62]
MFS 52.43 [51.73, 53.05] 51.76 [51.11, 52.36]
Sup. 97.41 [97.19, 97.62] 91.54 [91.18, 91.94]
PPR 86.40 [86.00, 86.85] 68.40 [67.80, 69.14]
Table 2: WSD evaluation results for abbreviations and
terms in the MSH-WSD data set.
The relative performance of the different ap-
proaches on the terms and abbreviations is similar to
the entire MSH-WSD data set (see Table 1). In par-
236
Corpus
Approach Type NLM-WSD MSH-WSD
UB Unsup. 74.00 [72.80, 75.29] 83.19 [82.87, 83.54]
MMB Unsup. 71.18 [69.94, 72.38] 78.09 [77.70, 78.46]
GSB Sup. 84.28 [83.12, 85.36] 83.39 [83.08, 83.67]
MFS Sup. 84.70 [83.67, 85.81] 52.01 [51.50, 52.45]
Sup Sup. 90.69 [89.87, 91.52] 94.83 [94.63, 95.02]
PPR Unsup. 68.10 [66.80, 69.23] 78.60 [78.23, 78.90]
Table 1: WSD evaluation results on NLM-WSD and MSH-WSD data sets.
ticular using automatically generated examples with
a uniform bias (UB) outperforms using the bias de-
rived from the Medline Baseline Repository (MBR)
while using the gold standard baseline (GSB) im-
proves results slightly for terms and actually reduces
them for abbreviations.
Results for all approaches are higher when disam-
biguating abbreviations than terms which is consis-
tent with previous studies that have suggested that
in biomedical text abbreviations are easier to disam-
biguate than terms.
5.2 Analysis
An explanation of the reason for some of the re-
sults can be gained by looking at the distributions
of senses in the various data sets used for the ex-
periments. Kullback-Leibler divergence (or KL di-
vergence) (Kullback and Leibler, 1951) is a com-
monly used measure for determining the difference
between two probability distributions. For each term
t, we define S as the set of possible senses of t,
the sense probability distributions of t as D and D?.
Then the KL divergence between the sense probabil-
ity distributions D and D? can be calculated accord-
ing to equation 5.
KL(D||D?) =
?
s  S
D(s). log
D(s)
D?(s)
(5)
The three techniques for determining the bias de-
scribed in Section 3.3 each generate a probability
distribution over senses. Table 2 shows the average
KL divergence when the gold standard distribution
obtained from the manually labeled data (GSB) is
compared with the uniform bias (UB) and bias ob-
tained by analysing the Medline Baseline Reposi-
tory (MMB).
Corpus
Avg. KL Divergence NLM-WSD MSH-WSD
KL(GSB||MMB) 0.5649 0.4822
KL(GSB||UB) 0.4600 0.0406
Table 3: Average KL divergence of sense probability dis-
tributions in the NLM-WSD and MSH-WSD data sets.
The average KL divergence scores in the table
are roughly similar with the exception of the much
lower score obtained for the gold-standard and uni-
form bias for the MSH-WSD corpus (0.0406). This
is due to the fact that the MSH-WSD corpus was
designed to have roughly the same number of ex-
amples for each sense, making the sense distribu-
tion close to uniform (Jimeno-Yepes et al, 2011).
This is evident from the MFS scores for the MSH-
WSD corpus which are always close to 50%. This
also provides as explanation of why performance us-
ing automatically generated examples on the MSH-
WSD corpus only improves by a small amount when
the gold standard bias is used (see Table 1). The gold
standard bias simply does not provide much addi-
tional information to the WSD system. The situa-
tion is different in the NLM-WSD corpus, where the
MFS score is much higher. In this case the additional
information available in the gold standard sense dis-
tribution is useful for the WSD system and leads to
a large improvement in performance.
In addition, this analysis demonstrates why per-
formance does not improve when the bias gener-
ated from the MBR is used. The distributions which
are obtained are different from the gold standard
and are therefore mislead the WSD system rather
than providing useful information. The difference
between these distributions would be expected for
237
the MSH-WSD corpus, since it contains roughly the
same number of examples for each possible sense
and does not attempt to represent the relative fre-
quency of the different senses. However, it is sur-
prising to observe a similar difference for the NLM-
WSD corpus, which does not have this constraint.
The difference suggests the information about CUIs
in the MBR, which is generated automatically, has
some limitations.
Table 4 shows a similar analysis for the MSH-
WSD corpus when abbreviations and terms are con-
sidered separately and supports this analysis. The
figures in this table show that the gold standard and
uniform distributions are very similar for both ab-
breviations and terms, which explains the similar re-
sults for UB and GSB in Table 2. However, the gold
standard distribution is different from the one ob-
tained from the MBR. The drop in performance of
MMB compared with GBS in Table 2 is a conse-
quence of this.
Ambiguity Type
Avg. KL Divergence Abbreviation Term
KL(GSB||MMB) 0.4554 0.4603
KL(GSB||UB) 0.0544 0.0241
Table 4: Average KL divergence for abbreviations and
terms in the MSH-WSD data set.
6 Conclusion
This paper describes the development of a large
scale WSD system based on automatically labeled
examples. We find that these examples can be gener-
ated for the majority of CUIs in the UMLS Metathe-
saurus. Evaluation on the NLM-WSD and MSH-
WSD data sets demonstrates that the WSD system
outperforms the PPR approach without making any
use of labeled data.
Three techniques for determining the number of
examples to use for training are explored. It is
found that a supervised approach (which makes use
of manually labeled data) provides the best results.
Surprisingly it was also found that using information
from the MBR did not improve performance. Anal-
ysis showed that the sense distributions extracted
from the MBR were different from those observed
in the evaluation data, providing an explanation for
this result.
Evaluation showed that accurate information
about the bias of training examples is useful for
WSD systems and future work will explore other un-
supervised ways of obtaining this information. Al-
ternative techniques for generating labeled examples
will also be explored. In addition, further evaluation
of the WSD system will be carried out, such as ap-
plying it to an all words task and within applications.
Acknowledgements
This research has been supported by the Engineer-
ing and Physical Sciences Research Council and a
Google Research Award.
References
E. Agirre and D. Mart??nez. 2004a. The Basque Country
University system: English and Basque tasks. In Rada
Mihalcea and Phil Edmonds, editors, Proceedings of
Senseval-3, pages 44?48, Barcelona, Spain.
E. Agirre and D. Mart??nez. 2004b. Unsupervised WSD
Based on Automatically Retrieved Examples: The
Importance of Bias. In Proceedings of EMNLP-04,
Barcelona, Spain.
E. Agirre, A. Sora, and M. Stevenson. 2010. Graph-
based word sense disambiguation of biomedical docu-
ments. Bioinformatics, 26(22):2889?2896.
R. Artstein and M. Poesio. 2008. Inter-Coder Agree-
ment for Computational Linguistics. Computational
Linguistics, 34(4):555?596.
S. Brin. 1998. Extracting Patterns and relations from the
Word-Wide Web. In Proceedings of WebDB?98.
S. Humphrey, W. Rogers, H. Kilicoglu, D. Demner-
Fushman, and T. Rindflesch. 2006. Word Sense
Disambiguation by Selecting the Best Semantic Type
Based on Journal Descriptor Indexing: Preliminary ex-
periment. Journal of the American Society for Infor-
mation Science and Technology, 57(5):96?113.
A. Jimeno-Yepes and A. Aronson. 2010. Knowledge-
based biomedical word sense disambiguation: com-
parison of approaches. BMC Bioinformatics,
11(1):569.
A. Jimeno-Yepes, B. McInnes, and A. Aronson. 2011.
Exploiting MeSH indexing in MEDLINE to generate
a data set for word sense disambiguation. BMC Bioin-
formatics, 12(1):223.
M. Joshi, T. Pedersen, and R. Maclin. 2005. A Com-
parative Study of Support Vector Machines Applied to
the Word Sense Disambiguation Problem for the Med-
ical Domain. In Proceedings of IICAI-05, pages 3449?
3468, Pune, India.
238
M. Krallinger and A. Valencia. 2005. Text mining and
information retrieval services for molecular biology.
Genome Biology, 6(7):224.
S. Kullback and R. A. Leibler. 1951. On Information and
Sufficiency. The Annals of Mathematical Statistics,
22(1):79?86.
C. Leacock, M. Chodorow, and G. Miller. 1998. Us-
ing Corpus Statistics and WordNet Relations for Sense
Identification. Computational Linguistics, 24(1):147?
165.
G. Leroy and T. Rindflesch. 2005. Effects of Information
and Machine Learning algorithms on Word Sense Dis-
ambiguation with Small Datasets. International Jour-
nal of Medical Informatics, 74(7-8):573?585.
H. Liu, S. Johnson, and C. Friedman. 2002. Au-
tomatic Resolution of Ambiguous Terms Based on
Machine Learning and Conceptual Relations in the
UMLS. Journal of the American Medical Informatics
Association, 9(6):621?636.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Finding Predominant Word Senses in Untagged
Text. In Proceedings of ACL-2004, pages 280?287,
Barcelona, Spain.
B. McInnes, T. Pedersen, and J. Carlis. 2007. Using
UMLS Concept Unique Identifiers (CUIs) for Word
Sense Disambiguation in the Biomedical Domain. In
Proceedings of the AMIA Symposium, pages 533?537,
Chicago, IL.
R. Mooney. 1996. Comparative Experiments on Disam-
biguating Word Senses: An Illustration of the Role of
Bias in Machine Learning. In Proceedings of EMNLP-
96, pages 82?91, Philadelphia, PA.
E. W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley & Sons.
G. Savova, A. Coden, I. Sominsky, R. Johnson, P. Ogren,
C. de Groen, and C. Chute. 2008. Word Sense Disam-
biguation across Two Domains: Biomedical Literature
and Clinical Notes. Journal of Biomedical Informat-
ics, 41(6):1088?1100.
M. Stevenson and Y. Guo. 2010. Disambiguation of Am-
biguous Biomedical Terms using Examples Generated
from the UMLS Metathesaurus. Journal of Biomedi-
cal Informatics, 43(5):762?773.
M. Stevenson, Y. Guo, R. Gaizauskas, and D. Martinez.
2008. Disambiguation of biomedical text using di-
verse sources of information. BMC Bioinformatics,
9(Suppl 11):S7.
M. Weeber, J. Mork, and A. Aronson. 2001. Developing
a Test Collection for Biomedical Word Sense Disam-
biguation. In Proceedings of AMIA Symposium, pages
746?50, Washington, DC.
J. Westbrook, E. Coiera, and A. Gosling. 2005. Do On-
line Information Retrieval Systems Help Experienced
Clinicians Answer Clinical Questions? Journal of the
American Medical Informatics Association, 12:315?
321.
H. Xu, J. Fan, G. Hripcsak, E. Mendonc?a, Markatou M.,
and Friedman C. 2007. Gene symbol disambigua-
tion using knowledge-based profiles. Bioinformatics,
23(8):1015?22.
A. Jimeno Yepes and A. Aronson. 2011. Self-training
and co-training in biomedical word sense disambigua-
tion. In Proceedings of BioNLP 2011 Workshop, pages
182?183, Portland, Oregon, USA, June.
239
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 125?129,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Identification of Genia Events using Multiple Classifiers
Roland Roller and Mark Stevenson
Department of Computer Science,
University of Sheffield
Regent Court, 211 Portobello
Sheffield, S1 4DP
United Kingdom
{R.Roller, M.Stevenson}@dcs.shef.ac.uk
Abstract
We describe our system to extract genia
events that was developed for the BioNLP
2013 Shared Task. Our system uses a su-
pervised information extraction platform
based on Support Vector Machines (SVM)
and separates the process of event clas-
sification into multiple stages. For each
event type the SVM parameters are ad-
justed and feature selection carried out.
We find that this optimisation improves
the performance of our approach. Overall
our system achieved the highest precision
score of all systems and was ranked 6th
of 10 participating systems on F-measure
(strict matching).
1 Introduction
The BioNLP 2013 Shared Task focuses on infor-
mation extraction in the biomedical domain and
comprises of a range of extraction tasks. Our sys-
tem was developed to participate within the Genia
Event Extraction task (GE), which focuses on the
detection of gene events and their regulation. The
task considers 13 different types of events which
can be divided into four groups: simple events,
bindings, protein modifications and regulations.
All events consist of a core event, which contains
a trigger word and a theme. With the exception of
regulation events, the theme always refer to a pro-
tein. A regulation event theme can either refer to
a protein or to another event. Binding events can
include up to two proteins as themes. In addition
to the core event, events may include additional
arguments such as ?cause? or ?to location?.
Figure 1 shows examples of events from the
BioNLP 2013 corpus. More details about the Ge-
nia Event task can be found in Kim et al (2011).
Previous editions of the BioNLP Shared Task
took place in 2009 (Kim et al, 2009) and 2011
Figure 1: Two events from the BioNLP 2013 GE
task: a phosphorylation event consisting of a trig-
ger and a protein and a positive-regulation event
consisting of a trigger, a theme referring to an
event and a cause argument.
(Kim et al, 2011). Promising approaches in the
most recent competition were event parsing (Mc-
Closky et al, 2011) and dual decomposition mod-
els (Riedel and McCallum, 2011). The winner of
the GE task 2011, FAUST (Riedel et al, 2011),
combined these two approaches by using result
from the event parser as an additional input fea-
ture for the dual decomposition.
The UTurku system of Bjo?rne et al (2009) was
the winner of the GE task in 2009. The system
was based on a pipeline containing three main
stages: trigger detection, argument detection and
post-processing. Bjo?rne and Salakoski (2011) im-
proved the performance of this system for BioNLP
2011, but was outperformed by FAUST.
Our approach to the BioNLP Shared Task re-
lies on separating the process of event classifica-
tion into multiple stages and creates separate clas-
sifiers for each event type. Our system begins by
pre-processing the input text, followed by multiple
classification stages and a post-processing stage.
The pre-processing applies tokenization, sentence
splitting and dictionary-based trigger detection,
similar to Bui and Sloot (2011). Classification is
based on a Support Vector Machine (SVM) and
uses three main stages: trigger-protein detection,
trigger-event detection and event-cause detection.
Post-processing is a combination of classification
and rule-based approaches. We train a separate
classifier for each event type, rather that relying on
a single classifier to recognise trigger-theme rela-
125
tionships for all event types. In addition, we also
optimise the SVM?s parameters and apply feature
selection for each event type.
Our system participated in subtask 1 of the
GE task, which involves the recognition of core
events, including identification of their ?cause?.
The remainder of this paper describes our sys-
tem in detail (Section 2), presents results from the
Genia Event Extraction task (Section 3) and draws
the conclusions of this work (Section 4).
2 System Description
2.1 Preprocessing
Our system begins by preprocessing the input text,
by applying the sentence splitter and biomedical
named entity tagger from LingPipe1. The sentence
splitter is trained on the MEDLINE data set. The
text is then tokenised. Tokens containing punc-
tuation marks are split, as are tokens containing
a protein or suffixes which could be utilised as
a trigger word. For instance the term ?Foxp3-
expression? will be split into ?Foxp3 - expression?,
since ?Foxp3? is as a protein and ?expression? a
suffix often used as trigger word. The tokens are
then stemmed using the Porter Stemmer from the
NLTK2 toolkit. The Stanford Parser3 is used to ex-
tract part-of-speech tags, syntax trees and depen-
dency trees.
2.1.1 Trigger Detection
The names of proteins in the text are provided in
the GE task, however the trigger words that form
part of the relation have to be identified. Our sys-
tem uses a dictionary-based approach to trigger
detection. The advantage of this approach is that it
is easy to implement and allows us to easily iden-
tify as many potential trigger words as possible.
However, it will also match many words which
are not true triggers. We rely on the classification
stage later in our approach to identify the true trig-
ger words.
A training corpus was created by combining the
training data from the 2013 Shared Task with all
of the data from the 2011 task. All words that are
used as a trigger in this corpus are extracted and
stored in a set of dictionaries. Separate dictionar-
ies are created for different event types (e.g. local-
ization, binding). Each type has its own dictionary,
1http://alias-i.com/lingpipe/index.html
2http://nltk.org/
3http://nlp.stanford.edu/software/lex-parser.shtml
with the exception of protein modification events
(protein modification, phosphorylation, ubiquiti-
nation, acetylation, deacetylation). The corpus did
not contain enough examples of trigger terms for
these events and consequently they are combined
into a single dictionary. The words in the dictio-
naries are stemmed and sorted by their frequency.
Irrelevant words (such as punctuations) are filtered
out.
Trigger detection is carried out by matching the
text against each of the trigger dictionaries, start-
ing with the trigger words with the highest fre-
quency. A word may be annotated as a trigger
word by different dictionaries. If a word is anno-
tated as a trigger word for a specific event then it
may not be annotated as being part of another trig-
ger word from the same dictionary. This restric-
tion prevents the generation of overlapping trigger
words for the same event as well as preventing too
many words being identified as potential triggers.
2.2 Classification
Classification of relations is based on SVM with
a polynomial kernel, using LibSVM (Chang and
Lin, 2011), and is carried out in three stages. The
first covers the core event, which consists of a trig-
ger and a theme referring to a protein. The second
takes all classified events and tries to detect regu-
lation events consisting of a trigger and a theme
that refers to one of these events (see positive-
regulation event in figure 1). In addition to a trig-
ger and theme, regulation and protein modification
events may also include a cause argument. The
third stage is responsible for identifying this addi-
tional argument for events detected in the previous
two stages.
Classification in each stage is always between
pairs of object: trigger-protein (stage 1), trigger-
event (stage 2), event-protein (stage 3) or event-
event (stage 3). At each stage the role of the clas-
sifier is to determine whether there is in fact a re-
lation between a given pair of objects. This ap-
proach is unable to identify binding events involv-
ing two themes. These are identified in a post-
processing step (see Section 2.3) which consid-
ers binding events involving the same trigger word
and decides whether they should be merged or not.
2.2.1 Feature Set
The classification process uses a wide range of
features constructed from words, stemmed words,
part of speech tags, NE tags and syntactic analysis.
126
Object Features: The classification process
always considers a pair of objects (e.g. trigger-
protein, trigger-event, event-protein). Object fea-
tures are derived from the tokens (words, stemmed
words etc.) which form the objects. We consider
the head of this object, extracted from the depen-
dency tree, as a feature and all other tokens within
that object as bag of word features. We also con-
sider the local context of each object and include
the three words preceding and following the ob-
jects as features.
Sentence Features: The tokens between the
two objects are also used to form features. A
bag of word is formed from the tokens between
the features and, in addition, the complete se-
quence of tokens is also used as a feature. Differ-
ent sentence features are formed from the words,
stemmed words, part of speech tags and NE tags .
Syntactic Features: A range of features are ex-
tracted from the dependency and phrase-structure
trees generated for each sentence. These fea-
tures are formed from the paths between the the
objects within dependency tree, collapsed depen-
dency tree and phrase-structure tree. The paths are
formed from tokens, stemmed tokens etc.
The features are organised into 57 groups for
use in the feature selection process described later.
For example all of the features relating to the bag
of words between the two objects in the depen-
dency tree are treated as a single group, as are all
of the features related to the POS tags in the three
word range around one of the objects.
2.2.2 Generation of Training and Test Data
Using the training data, a set of positive and neg-
ative examples were generated to train our classi-
fiers. Pairs of entities which occur in a specific
relation in the training data are used to generate
positive examples and all other pairs used to gen-
erate negative ones. Since we do not attempt to
resolve coreference, we only consider pairs of en-
tities that occur within the same sentence.
Due to the fact that we run a dictionary-based
trigger detection on a stemmed corpus we might
cover many trigger words, but unfortunately also
many false ones. To handle this situation our clas-
sifier should learn whether a word serves as a trig-
ger of an event or not. To generate sufficient nega-
tive examples we also run the trigger detection on
the training data set, which already contains the
right trigger words.
2.2.3 Classifier optimisation
Two optimisation steps were applied to the rela-
tion classifiers and found to improve their perfor-
mance.
SVM bias adjustment: The ratio of positive
and negative examples differs in the training data
generated for each relation. For instance the data
for the protein catabolism event contains 156 pos-
itive examples and 643 negatives ones while the
gene expression event has 3617 positive but 34544
negative examples. To identify the best configura-
tion for two SVM parameters (cost and gamma),
we ran a grid search for each classification step
using 5-fold cross validation on the training set.
Feature Selection: We also perform feature se-
lection for each event type. We remove each fea-
ture in turn and carry out 5-fold cross validation on
the training data to identify whether the F-measure
improves. If improvement is found then the fea-
ture that leads to the largest increase in F-measure
is removed from the feature set for that event type
and the process repeated. The process is continued
until no improvement in F-measure is observed
when any of the features are removed. The set of
features which remain are used as the final set for
the classifier.
The feature selection shows the more positive
training examples we have for an event type the
fewer features are removed. For example, gene
expression events have the highest amount of pos-
itive examples (3617) and achieve the best F-
measure score without removing any feature. On
the other hand, there are just 156 training exam-
ples for protein catabolism events and the best re-
sults are obtained when 39 features are removed.
On average we remove around 14 features for each
event classifier. We observed that sentence fea-
tures and those derived from the local context of
the object are those which are removed most of-
ten.
2.3 Post-Processing
The output from the classification stage is post-
processed in order to reduce errors. Two stages of
post-processing are applied: one of which is based
on a classifier and another which is rule based.
Binding Re-Ordering: As already mentioned
in Section 2.2, our classification is only capable
of detecting single trigger-protein bindings. How-
ever if two binding events share the same trig-
ger, they could be merged into a single binding
127
containing two themes. A classifier is trained to
decide whether to merge pairs of binding events.
The classifier is provided with the two themes that
share a trigger word and is constructed in the same
way as the classifiers that were used for relations.
We utilise the same feature set as in the other clas-
sification steps and run a grid search to adjust the
SVM parameter to decide whether to merge two
bindings or not.
Rule-Based Post-Processing: The second
stage of post-processing considers all the events
detected within a sentence and applies a set of
manually created rules designed to select the most
likely. Some of the most important rules include:
? Assume that the classifier has identified both
a simple event (e1) and regulation event (e2)
using the same trigger word and theme. If an-
other event uses a different trigger word with
e1 as its theme then e2 is removed.
? If transcription and gene expression events
are identified which use the same trigger and
theme then the gene expression event is re-
moved. This situation occurs since transcrip-
tion is a type of a gene expression and the
classifiers applied in Section 2.2 may identify
both types.
? Assume there are two events (e1 and e2) of
the same type (e.g. binding) that use the same
trigger word but refer to different proteins. If
the theme of a regulation event refers to e1
then a new regulation event referring to e2 is
introduced.
3 Results
Our approach achieved the highest precision score
(63.00) in the formal evaluation in terms of strict
matching in the GE task 1. The next highest preci-
sion scores were achieved by BioSEM (60.67) and
NCBI (56.72). We believe that the classifier opti-
misation (Section 2.2.3) for each event and the use
of manually created post-processing rules (Section
2.3) contributed to the high precision score. Our
system was ranked 6th place of 10 in terms of F-
measure with a score of 42.06.
Table 1 presents detailed results of our system
for the GE task. Our approach leads to high preci-
sion scores for many of the event types with a pre-
cision of 79.23 for all simple events and 92.68 for
protein modifications. Our system?s performance
is lower for regulation events than other types with
a precision of 52.69. Unlike other types of events,
the theme of a regulation event may refer to an-
other event. The detection of regulation events can
therefore be affected by errors in the detection of
simple events.
Results of our system are closer to the best re-
ported results when strict matching is used as the
evaluation metric. In this case the F-measure is
6.86 lower than the winning system (BioSEM).
However, when the approximate span & recursive
matching metric is used the results of our sys-
tem are 8.74 lower than the best result, which is
achieved by the EVEX system.
Event Class Recall Prec. Fscore
Gene expression 62.20 85.37 71.96
Transcription 33.66 45.33 38.64
Protein catabolism 57.14 53.33 55.17
Localization 23.23 85.19 36.51
SIMPLE ALL 54.02 79.23 64.24
Binding 31.53 46.88 37.70
Phosphorylation 47.50 92.68 62.81
PROT-MOD ALL 39.79 92.68 55.68
Regulation 11.46 42.86 18.08
Positive regulation 23.72 53.60 32.88
Negative regulation 20.91 54.19 30.18
REG. ALL 21.14 52.69 30.18
EVENT TOTAL 31.57 63.00 42.06
Table 1: Evaluation Results (strict matching)
4 Conclusion
Our approach to the BioNLP GE task 1 was to cre-
ate a separate SVM-based classifier for each event
type. We adjusted the SVM parameters and ap-
plied feature selection for each classifier. Our sys-
tem post-processed the outputs from these classi-
fiers using a further classifier (to decide whether
events should be merged) and manually created
rules (to select between conflicting events). Re-
sults show that our approach achieves the high-
est precision of all systems and was ranked 6th in
terms of F-measure when strict matching is used.
In the future we would like to improve the recall
of our approach and also aim to explore the use of
a wider range of features. We would also like to
experiment with post-processing based on a clas-
sifier and compare performance with the manually
created rules currently used.
128
References
Jari Bjo?rne and Tapio Salakoski. 2011. Generaliz-
ing biomedical event extraction. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 183?
191, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting complex biological events with rich graph-
based feature sets. In Proceedings of the BioNLP
2009 Workshop Companion Volume for Shared Task,
pages 10?18, Boulder, Colorado, June. Association
for Computational Linguistics.
Quoc-Chinh Bui and Peter. M.A. Sloot. 2011. Extract-
ing biological events from text using simple syntac-
tic patterns. In Proceedings of BioNLP Shared Task
2011 Workshop, pages 143?146, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/?cjlin/libsvm.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
bionlp?09 shared task on event extraction. In Pro-
ceedings of the BioNLP 2009 Workshop Companion
Volume for Shared Task, pages 1?9, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011. Overview of genia event
task in bionlp shared task 2011. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 7?15,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
David McClosky, Mihai Surdeanu, and Christopher
Manning. 2011. Event extraction as dependency
parsing for bionlp 2011. In Proceedings of BioNLP
Shared Task 2011 Workshop, pages 41?45, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Sebastian Riedel and Andrew McCallum. 2011. Ro-
bust biomedical event extraction with dual decom-
position and minimal domain adaptation. In Pro-
ceedings of BioNLP Shared Task 2011 Workshop,
pages 46?50, Portland, Oregon, USA, June. Asso-
ciation for Computational Linguistics.
Sebastian Riedel, David McClosky, Mihai Surdeanu,
Andrew McCallum, and Christopher D. Manning.
2011. Model combination for event extraction in
bionlp 2011. In Proceedings of BioNLP Shared
Task 2011 Workshop, pages 51?55, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
129
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 1?10,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Generating Paths through Cultural Heritage Collections
Samuel Fernando1, Paula Goodale2, Paul Clough2,
Mark Stevenson1, Mark Hall2, Eneko Agirre3
1Department of Computer Science, University of Sheffield
2Information School, University of Sheffield
3Computer Science Department, University of the Basque Country
{s.fernando, p.goodale, p.d.clough,
r.m.stevenson, m.mhall}@sheffield.ac.uk
e.agirre@ehu.es
Abstract
Cultural heritage collections usually or-
ganise sets of items into exhibitions or
guided tours. These items are often
accompanied by text that describes the
theme and topic of the exhibition and pro-
vides background context and details of
connections with other items. The PATHS
project brings the idea of guided tours
to digital library collections where a tool
to create virtual paths are used to assist
with navigation and provide guides on par-
ticular subjects and topics. In this pa-
per we characterise and analyse paths of
items created by users of our online sys-
tem. The analysis highlights that most
users spend time selecting items relevant
to their chosen topic, but few users took
time to add background information to the
paths. In order to address this, we con-
ducted preliminary investigations to test
whether Wikipedia can be used to au-
tomatically add background text for se-
quences of items. In the future we would
like to explore the automatic creation of
full paths.
1 Introduction
Paths (or trails) have been studied as a means of
assisting users with the navigation of digital col-
lections as an alternative to standard keyword-
based search (Furuta et al, 1997; Reich et al,
1999; Shipman et al, 2000; White and Huang,
2010). Paths can be particularly useful to users
who are unfamilar with the content of digital col-
lections (e.g. historical documents) and may find
it difficult to formulate appropriate queries (Wil-
son et al, 2010). Paths can be used to assist users
with the navigation of collections through the pro-
vision of narratives and subject guides. From an
educational perspective paths can provide tangible
learning objects, created by teachers and followed
by students. Alternatively from a cultural her-
itage perspective paths can be used to create activ-
ity trails and guided tours support exploration by
visitors through collections of cultural artefacts.
This echoes the organised galleries and guided
tours found in physical museums. The existance
of tools, such as Walden?s paths1, Trailmeme2 and
Storify3, provide functionalities for users to record
and share paths through web resources and digital
libraries. From this perspective everyone can take
on role of curator and provide access to their own
personal collections.
We have developed an online system called
PATHS that allows curators and end-users to cre-
ate and view paths to navigate through the Eu-
ropeana4 cultural heriage collection. As part of
evaluations of the prototype PATHS system par-
ticipants have created paths on various topics. In
this paper we describe a number of these paths and
their characteristics. Analysing paths that are cre-
ated manually and characterising them can be seen
as a first step towards developing methods to sup-
port the creation of paths automatically and semi-
automatically. Within the context of the PATHS
project this is being considered to deal with the
following limitations of manual creation of paths.
Firstly, the effort required in generating them often
means that a sufficient number of paths on a vari-
ety of topics are not available. Secondly, the man-
ual creation of paths is a very time-consuming pro-
cess that would benefit from computational sup-
port in whatever form this might take. This pa-
per presents initial work in automatically creat-
ing paths and provides the following novel con-
1http://www.csdl.tamu.edu/walden/
2http://open.xerox.com/Services/
xerox-trails
3http://storify.com/
4http://www.europeana.eu/
1
tributions: (1) we present results of user stud-
ies describing what people want from paths and
how they use them to navigate digital collections;
(2) we analyse a set of manually-created paths
to identify their properties and be able to charac-
terise them; and (3) we present work on automati-
cally generating background text for sequences of
items, thus providing an efficient way to enrich
paths with additional information with little man-
ual input required.
The paper is structured as follows: Section 2 de-
scribes related work on the use of narratives in cul-
tural heritage and previous approaches to automat-
ically generate paths; Section 3 defines the prob-
lem of generating paths and describes the datasets
used in the experiments; Section 4 presents analy-
sis of manually-created paths; Section 5 shows re-
sults of using automatic methods to generate back-
ground text; and finally Section 6 concludes the
paper and provides avenues for further work.
2 Related Work
2.1 Narratives and Cultural Heritage
The potential of narrative in digital CH to sup-
port learning, creativity and exploration is clear,
providing opportunities for supporting a more ac-
tive user interaction, including deeper engagement
with context, representation of the collecting pro-
cess, and facilitation of a more entertaining expe-
rience of learning (Mulholland and Collins, 2002).
Walker et al (2013) also propose narrative as a
major element of interaction and informal learn-
ing, suggesting that meaning is made when the
links between people and artefacts, and interpreta-
tion and ideas are surfaced, especially within so-
cial groups. Their experiments involve the use
of mobile and handheld technologies in a physi-
cal museum environment, capturing audio annota-
tions, but have much in common with experimen-
tal systems designed for path creation online. In a
similar vein the StoryBank project utilises collec-
tions of photographs and audio narratives to create
and share stories as information in the developing
world (Frohlich and Rachovides, 2008).
Whilst technologies have aided the creation and
sharing of narratives in physical cultural encoun-
ters, Manovich (1999) critiques the lack of narra-
tive in digital cultural environments, offering that
online collections and many CH web sites are
databases with constantly changing content that
inevitably lack a cohesive and persistent story.
However, since ?narrative is constructed by link-
ing elements of this database in a particular or-
der? (Manovich, 1999), it is possible to offer users
any number of explicit ?trajectories? (narratives)
through a digital information space, and by merg-
ing database and narrative in this way, creating
a more dynamic, discovery-led experience. This
view might be interpreted at its simplest level as
a virtual representation of the guided tours rou-
tinely offered in physical CH spaces, and indeed
there is a small strand of research into the creation
of systems for generating and exploring online ex-
hibitions and tours from items held within digital
collections. A scenario of users creating and edit-
ing trails in a CH context is described by Walker
(2006), including functionality for collecting, or-
dering and annotating museum objects.
2.2 Automatically Creating Paths
Generation of implicit trails through physical and
virtual museum spaces has been related to the
learning process (Peterson and Levene, 2003). In
this example, trails are automatically created by
users as they navigate their way through an infor-
mation space, and may be used for individual or
collaborative purposes. Research on the applica-
tion of curated pathways in web environments has
often focused on providing trails pre-prepared by
experts (e.g. curators, educationalists) as a means
of assisting novice users to navigate information
online (Shipman et al, 2000). Indeed, it has been
found that domain knowledge or expertise can
considerably enhance the quality of trails created
(Yuan and White, 2012). Automatic extraction
and generation of trails in information spaces has
been explored as a means of harnessing the wis-
dom of crowds, using the mass actions of earlier
user behaviour to establish relevance, and recom-
mend content or navigation routes to later users.
Such trails can be readily mined from search en-
gine transaction logs and have been shown to pro-
vide added value (White and Huang, 2010; Has-
san and White, 2012; Liao et al, 2012). West and
Leskovec (2012) take this notion a stage further
and attempt to identify wayfinding strategies em-
ployed by browsers in Wikipedia, with the goal of
assisting future users in their navigation by surfac-
ing potentially useful hyperlinks.
Guided tours or pathways are essentially more
structured, purposeful forms of trails, taking the
user through a specific sequence of information
2
nodes and may also be automatically generated,
rather than manually curated as in the examples
above. Wheeldon and Levene (2003) offer an al-
gorithm for generating trails from site-search, en-
abling elements of structure and context to be in-
corporated into the trails created in this way, but
noting potential scalability issues for web scale
search tasks. In the CH domain, a small num-
ber of projects have attempted to automatically
generate digital content in the form of exhibi-
tions, tours and trails. Ma?kela? et al (2007) de-
scribe a system which utilises semantically an-
notated content to generate personalised ?exhi-
bitions? from a structured narrative-based search
query. Similarly, Zdrahal et al (2008) demonstrate
how pathways can be generated through a collec-
tion of semantically related documents to provide
a means of exploration, using non-NLP cluster-
ing and path creation techniques. Sophisticated
approaches such as linear programming and evo-
lutionary algorithms have also been proposed for
generating summaries and stories (McIntyre and
Lapata, 2010; Woodsend and Lapata, 2010). In
contrast, Wang et al (2007) use a recommender
system approach to generate museum tours on
the basis of ratings stored within a dynamic user
model, and Pechenizkiy and Calders (2007) pro-
pose the additional use of data mining techniques
on log data to improve this type of tour personali-
sation.
In summary, online tours and trails are made
possible either through manually curated content
generated through the efforts of experts or other
end users, or have been automatically generated
from the mining of large scale search logs, or from
collections benefitting from semantically-linked
content and/or detailed user models.
3 Methodology
This study brings together work from several ar-
eas of the PATHS project. An analysis of what
paths might be used for and what form they are ex-
pected to take, has had implications for the system
design and functionality and evaluation measures.
A user study focused upon evaluation of the first
prototype has provided manually-created paths as
a basis for analysing path content and attributes,
which in turn informs the desired characteristics
of automated paths and the algorithm designed for
generating paths automatically.
3.1 Utilisation of Paths
Initial user requirements interviews with 22 ex-
pert users in the heritage, education and profes-
sional domains found a strong affinity with the
path metaphor, revealing a range of different in-
terpretations of what it means in the CH context
and how they could be employed in an online en-
vironment to engage with key audiences. Eight
interpretations of the path metaphor emerged:
1. Path as search history
2. Path as information seeking journey
3. Path as linked metadata
4. Path as a starting point or way in
5. Path as a route through
6. Path as augmented reality
7. Path as information literacy journey / learn-
ing process
8. Path as transaction process
The first three of these are closest to the idea
of hypertext trails, with trails defined by user in-
teraction in 1 and 2, and trails defined automati-
cally, by the system in 3. Variations 4-6 are more
creative interpretations, all suggesting opportuni-
ties for guiding the user into and through collec-
tions, encouraging exploration and/or offering an
immersive experience, conducive with our initial
vision for the PATHS system.
In addition to expert-defined routes, 5 also in-
corporates the idea of users being able to see and
follow ?well-trodden path? defined by the cumula-
tive interactions of other users, thus extending the
opportunities for utilizing search histories. Con-
versely, 7 and 8 are both process oriented, al-
though 7 is experiential, user-defined, learning-
oriented, typified by trial and error and unique to
the individual, whilst 8 is a rigid process designed
to escort all users consistently through a standard
process of pre-defined steps.
A strong emphasis was placed on path content
being carefully selected or ?curated? by the path-
creator, with the addition of context and interpre-
tation so that the objects within the path convey
a narrative or meaning. Content may be derived
from one collection, but there were seen to be sig-
nificant benefits from including objects from di-
verse collections, along with other materials from
external web sites.
Paths facilitate topic-based information re-
trieval typified by the berry-picking mode of in-
teraction (Bates, 1989), rather than known item
searching. Furthermore, paths may be a useful tool
3
for personal information management in both for-
mal and informal research scenarios, enabling the
user to record, reuse and share their research activ-
ity, or helping them to organize their ideas. Cre-
ativity is also encouraged, as user-generated paths
provide the means to repurpose CH objects into
users? own narratives for private or public con-
sumption.
A summary of specific user scenarios high-
lighted by participants is given below:
? Teachers/lecturers presentations and class-
room activities
? Museum personnel curating collections, giv-
ing an overview, or covering a topic in depth
? Leisure users browsing, collecting interest-
ing and/or visually appealing content
? Researchers to aid image-based research,
sharing and discussing findings with fellow
researchers and supervisors
? Non-academic specialists (e.g. local histori-
ans) collecting and sharing items of interest
with other enthusiasts
3.2 Defining the Problem
To create a path or narrative that guides a user
through a set of items from a collection, whether
as a manual process or automatically, there are
three main activities: (1) the selection of items to
include in the path; (2) the arrangement of items
to form a path or narrative and (3) the annota-
tion of the path to with descriptive text and back-
ground information. We envision techniques to
automate the entire process; however, a first step is
to analyse existing manually-created paths to iden-
tify their characteristics and inform the automatic
creation of similar structures.
3.3 User Study
The manually generated paths used for this study
were created as part of a more detailed user study
to evaluate the first prototype, conducted using
a protocol informed by the Interactive IR eval-
uation framework (Borlund, 2003). Twenty-two
users, including subject experts, students and gen-
eral users (subject novices), each completed a 2-
hour session, during which they participated in the
following activities:
? Profile questionnaire and cognitive style test
? Familiarisation with the system
? 4x short information seeking tasks (5 minutes
each)
? 1x long simulated work task - path creation
(30 minutes)
? Task feedback questionnaire
? Session/system feedback questionnaire
? Think-after interview based upon the com-
plex task
Of most interest here is the simulated work task,
with associated observations, feedback and reflec-
tions. This task focused on the creation of a path,
using a scenario adapted to the type of user. Free-
dom was given in choosing a subject for the path,
and limited instructions were provided in what
might be needed to complete the task, for exam-
ple:
?Imagine you are a student who has been asked
to create a path as part of a university assignment.
You have been asked to use primary source ma-
terials to create a mini online exhibition suitable
for a target group within the general public and/or
school visitor categories. Your goal is to introduce
a historical or art-focussed topic in a popular, ac-
cessible way, and to encourage further use and ex-
ploration of cultural heritage resources.?
Data on the tasks was captured via log files, as
well as screen recording and observations using
the Morae usability software. Detailed analysis
was undertaken of user behaviour in the process of
completing the task, and of the paths created, from
both quantitative and qualitative perspectives.
4 Analysing Manually-created Paths
In this section we describe the results of analysing
the 22 paths created manually in the PATHS pro-
totype system.
4.1 User behaviour
On average users spend 25.3 mins on creating a
path (min=11.7; max=33.6) with an average of
201 mouse clicks (min=53; max=380). From the
observations, it was noted that some participants
spent quite a lot of time thinking about the task
and pondering their next move, whilst others en-
gaged in more rapid fire activity in the face of
uncertainty. Analysis of the screen recordings
showed a variety of primary interaction styles for
this task, with a fairly even split between serial
searching (33%) and serial browsing (39%), as the
two most popular strategies. Serial searching in-
volves repetitive search and reformulation, with
only a page or two of search results viewed before
searching again, and serial browsing involves very
4
few searches, with large numbers of search re-
sults pages viewed (over 50 pages in some cases).
These are then in effect, polar opposites of interac-
tion. Only 6% engaged primarily in exploring be-
haviour (using the explore and similar items con-
tent), and 22% of participants occupied the middle
ground, utilising a mix of search, browse and ex-
plore, with no strong preference for any one style.
4.2 Properties of paths
The mean number of items in a path was 10.7 (std
dev=6.7 items) with a minimum of 5 items and
maximum of 29 items. Most popular bin is 6-
10 items in a path (59%). We found 85% of the
items included in the paths included an image with
the metadata. The paths created were manually
categorised by theme to ascertain whether there
are any distinct preferences for the subject mat-
ter of content included. The most popular cate-
gories were paths about places (23%), art subjects
(23%) and history subjects (32%). These themes
are likely to have been influenced at least partly
by what content is currently available in our col-
lection, although the amount of art-related content
is much less than for history, and also appear to
have been influenced by the topics covered in ex-
isting paths in the system (e.g. places, topics re-
lated to the world wars). There were, however a
significant number of expert users who attempted
to build paths related to their own research inter-
ests, with varying degrees of success.
4.3 Descriptions and ordering
Once items have been selected and they have been
transferred in the path creation workspace, users
have the opportunity to modify and enhance their
path with a number of tools for adding content and
metadata, and for re-ordering the content. On cre-
ating the path, most users immediately went to the
metadata fields and added information for the path
description and duration fields, as well as a num-
ber of tags (or keywords). A short 1-2 line de-
scription of the path appears to be the norm and
was added in 91% of cases. Tags were added by
82% of users and a duration by only 46% of users.
It is clear from further investigation that the tags
were added incorrectly (without commas between
them) by a significant number of users and a tip
for successful use is required.
The items within a path can be annotated with
the user?s own contextual information, and can be
re-ordered into a more meaningful sequence, such
as a chronological or narrative sequence. These
more advanced features were used by significantly
fewer users, which could indicate a learning issue,
a lack of need, or a time constraint. On reviewing
the paths created by our evaluation participants it
is found that in 41% of cases, contextual informa-
tion was not added to any items in the path. There
are however 32% in which annotations were added
to all items (generally these were shorter paths
with fewer items), and a further 27% where anno-
tations were added to some or most of the items.
In 72% of cases the items in the paths created
were re-ordered to some degree, with 17% spend-
ing a considerable amount of time on this activity.
This finding is encouraging, as the default is for
items to be included in the path in the order they
were saved to the workspace, and re-ordering in-
dicates that users are thinking about their path as a
whole and trying to make sense of the information
it is intended to convey. Typical types of ordering
included chronology (32%), narrative (23%), ge-
ography (for example, a walking tour - 9%), theme
(9%) and ?interestingness? (5%).
5 Enriching paths with background
information
This section describes preliminary work on the
task of semi-automated path creation. In par-
ticular we describe efforts to enrich paths with
background contextual information using relevant
Wikipedia articles. The related work described
in Section 2.2 shows that there have been previ-
ous efforts to automatically select cultural heritage
items to form paths, trails and exhibitions. How-
ever to our knowledge no significant effort has
been made to automatically annotate such paths
with descriptive or contextual information. The
interviews described in Section 3.1 highlighted
the importance CH experts placed on having ad-
ditional information to give context for the items
in the path. It was also noted during the manual
path-creation exercise (Section 4.3) that a signif-
icant number of the users did not add any such
information to the path. The reasons for this are
unclear, but nevertheless there seems to be suffi-
cient motivation to devise automatic methods for
this task. Although the methods have previously
been well established in other tasks5 , we believe
5INEX Tweet Contextualization Track (https:
//inex.mmci.uni-saarland.de/tracks/qa/)
and Link-the-wiki Track (http://www.inex.otago.
ac.nz/tracks/wiki-link/wiki-link.asp)
5
this is the first time they have been applied for the
task of annotating sequences of items in this way.
5.1 Method
Manually generated paths contain sequences of
items selected from Europeana on some topic or
theme. Creators provide their own title, subject
keywords and description for the path. To aid
creation of paths we explore whether background
information could be generated automatically for
such paths. An approach is presented here which
shows promise as a potential way to achieve this
task. The input for this approach is a sequence of
items and a key Wikipedia article which describes
the overall topic of the path. The output comprises
sentences taken from a relevant Wikipedia article.
The aim is for this output to provide useful and
interesting additional background information re-
lated to the items and theme of the path. In this
paper experiments are focussed on how to select
good quality text to present as additional informa-
tion for the path. For this reason the key Wikipedia
article is manually chosen, and the task is to find a
good approach for selecting the most relevant sen-
tences from this key article for the text.
Two methods are tested in this paper. The first
method simply takes the first n sentences of the
article and outputs this. Since Wikipedia articles
are always structured to have a summary of the
article in the first paragraph we can expect this text
to perform well as a summary of the path topic.
The second method is more advanced and at-
tempts to find text in the article that is relevant to
the actual items that have been chosen for the path.
This approach uses the Wikipedia Miner software
(Milne and Witten, 2008) to add inline links to
the text in the items for this approach. This soft-
ware disambiguates terms in the text and then de-
tects links using various features such as the com-
monness of the term, the overall relatedness of the
terms in the text and so on. The result is text en-
riched with inline links to relevant Wikipedia arti-
cles. Each link also has an associated confidence
value which indicates how sure the software is that
the link is correctly disambiguated and relevant to
the text.
The approach works as follows for a sequence
of items S and a key article K. First Wikipedia
Miner is run over the items in S. The text input to
Wikipedia Miner comprises the title, subject and
description fields of each item. The output is a set
of article titles W comprising the titles of all the
linked articles which were found in the text fields
of S. For each title in W we also have the associ-
ated confidence value for the link as calculated by
Wikipedia Miner. The next step is to select from
K the most relevant sentences to output as the gen-
erated text. For each sentence in K a score is as-
signed if any of the words in the sentence match
one of the titles in W . The score is then simply the
sum of the confidence values associated with these
titles. The top scoring sentences are then output as
the background text. This method can be consid-
ered to be a kind of query based summarisation
(Jurafsky and Martin, 2008).
5.2 Results
The automatic approaches for generating back-
ground text were run over the items in the 22 man-
ually created paths described in the previous sec-
tion. To ensure a fair test the user-added text was
was discarded and only the original Europeana
metadata for the items was used as source for the
methods.
For each path a single key Wikipedia article was
manually found which best captured the overall
theme or topic of the path. For all paths at least
one reasonably good article was found, although
for some paths the articles were better matches for
the path topic than for others. Some examples are
shown in Table 1.
With the input ready, the approaches described
were run over the items and key articles to gener-
ate the additional text. This was limited to 3 sen-
tences to provide a short but useful piece of text
for viewers of the paths. For example one path
was entitled ?Bronze Age metallurgy?. The user
provided description was as follows:
Bronze Age metalsmiths produce a variety of
objects, however, we learn as much from the tools,
leftover debris and the way in which objects were
deposited as we do from the finished objects them-
selves.
The key article chosen for the path was ?Bronze
Age?. The items in the path included various ar-
tifacts and working debris from the Bronze Age.
For the first approach the output is the first 3 sen-
tences from this article:
The Bronze Age is a period characterized by
the use of copper and its alloy bronze as the chief
hard materials in the manufacture of some imple-
ments and weapons. Chronologically, it stands be-
6
Path title Key Wikipedia article
Canals in England Canals of the United Kingdom
A brief history of Sheffield History of Sheffield
Art and Criticism of John Ruskin John Ruskin
Early medieval features in English
Parish Churches
Architecture of the medieval cathedrals
of England
Brooches through the ages Brooch
Table 1: Key articles for paths
tween the Stone Age and Iron Age. The term Stone
Age implies the inability to smelt any ore, the term
Bronze Age implies the inability to smelt iron ore
and the term Iron Age implies the ability to man-
ufacture artifacts in any of the three types of hard
material.
For the second approach the top 3 highest scor-
ing sentences were output:
A region could be in the Bronze Age either by
smelting its own copper and alloying with tin or
by trading for bronze from production areas else-
where. The Bronze Age in Ireland commenced
around 2000 BC, when copper was alloyed with
tin and used to manufacture Ballybeg type flat axes
and associated metalwork. The Bronze Age is a
period characterized by the use of copper and its
alloy bronze as the chief hard materials in the
manufacture of some implements and weapons.
These sentences scored most highly since they
contained the most high-confidence terms from
the items, for example terms such as ?copper?, ?al-
loy? and ?Bronze Age?.
5.3 Evaluation
To evaluate the two approaches, 5 annotators were
presented with the paths and the text and asked to
rate each path on 3 dimensions:
? The relevance of the text to the theme and
items of the path. Text which relates strongly
to the path is scored highly while off-topic or
irrelevant text is given a low score.
? The coherence or quality of the text itself.
Text which appears well-written and well-
structured is scored highly, while poorly writ-
ten or incoherent text is given a low score.
? The contextualisation of the text in relation
to the path. To achieve a high score the
text should offer useful or interesting addi-
tional information which is not found else-
where within the content, i.e. the text helps
to provide a context for items in the path.
Annotators were asked to grade from A (very
good) to E (very poor) on each dimension. The
results are shown in Figure 1. The results for
the first 3 sentences are shown as First3 and for
the weighted approach as Weighted. For each di-
mension, the distribution of judgements across the
paths is shown. The First3 approach was found
to be superior in every dimension. For relevance
scores 90% of the scores were either A or B com-
pared to 63% for the Weighted approach. Sim-
ilarly for the coherence judgements 97% were A
or B compared to 62% for the weighted approach.
The reason for this superior performance seems to
be that the first few sentences of Wikipedia arti-
cles are deliberately created to give a short sum-
mary introduction of the topic of the article. This
explains the high scores for relevance and coher-
ence.
Both approaches scored lower on the contex-
tualisation dimension, with First3 getting 67%
A or B grades and the Weighted approach get-
ting 43%. There may be several reasons for this.
Firstly one problem is that the auto-generated text
sometimes repeats information that is already in
the path and item descriptions; thus the text fails
to meet the requirement of ?useful additional in-
formation?. Secondly the text is sometimes quite
general and vague, rather than focussing on spe-
cific details which might be most relevant to the
items chosen for the path.
To measure the agreement among the annotators
the following approach was used. First the scores
were converted to numeric values; A to 1, B to 2
and so on. Then the scores for each annotator were
compared to the average of the scores of all the
other annotators. The correlation was computing
using Spearman?s correlation coefficient. These
scores were then averaged amongst all annotators
to give a final agreement value. The results are
shown in Table 2.
7
Figure 1: Comparing the results of the two methods.
First3 Weighted
Relevance 0.57 0.57
Coherence 0.28 0.56
Contextualisation 0.56 0.78
Table 2: Agreement amongst annotators.
For both approaches there was good agreement
on the Relevance dimension. For the Coherence
dimension the First3 approach got quite a low
score. This may be because one annotator gave
lower scores for all paths, while the others all gave
consistently high scores, which seems to have
skewed the correlation co-efficient. For the con-
textualisation dimension the correlation scores for
high for both approaches, and the Weighted ap-
proach in particular achieved a very high agree-
ment value.
6 Conclusions
This paper presented results of interviews about
creating paths through cultural heritage collec-
tions. These results inform us on how people
want to navigate through cultural heritage collec-
tions using the path metaphor, how they wish to
make use of paths for their work and education,
and what information and qualities they consider
it important for a path to contain. The paper also
presents results from studies using the PATHS pro-
totype software where users were able to search
and explore a large digital library collection and
create their own paths of items from the collection
on topics of their interest.
From the interviews it was clear that the experts
considered it important that the paths contain ad-
ditional information to convey contextual informa-
tion to understand the meaning of the items in the
path. The results from the user studies showed that
this need was not being met in a significant num-
ber of cases; users were putting items together on
a topic but adding little or no descriptive text about
the topic and the items in the path. Therefore we
identified this as a key task which might benefit
from automatic methods. The simpler approach
which output the first n sentences from the key
Wikipedia article was found to generate the best
results. The resulting generated text was found to
be relevant and coherent. In most cases the text
was also found to add useful context about the
topic.
Future work will further refine the text genera-
tion approach. The approach depends on success-
fully identifying a good key article for each path.
In these experiments the key article was manually
chosen, however we are devising methods to se-
lect this article automatically. To correct the prob-
lem with repeated information a filtering approach
could eliminate information that is already con-
tained within the paths.
Acknowledgments
The research leading to these results was car-
ried out as part of the PATHS project (http:
//paths-project.eu) funded by the Eu-
ropean Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
no. 270082.
References
Marcia J Bates. 1989. The design of browsing and
berrypicking techniques for the online search inter-
8
face. Online Information Review.
Pia Borlund. 2003. The IIR evaluation model: a
framework for evaluation of interactive information
retrieval systems. Information research, 8(3).
David M Frohlich and Dorothy Rachovides. 2008. Us-
ing digital stories for local and global information
sharing. In Community and International Develop-
ment, CHI 2008 Workshop.
R. Furuta, F.. Shipman, C. Marshall, D. Brenner, and
H. Hsieh. 1997. Hypertext paths and the World-
Wide Web: experiences with Walden?s Paths. In
Proceedings of the eighth ACM conference on Hy-
pertext, pages 167?176, New York, NY.
Ahmed Hassan and Ryen W White. 2012. Task tours:
helping users tackle complex search tasks. In Pro-
ceedings of the 21st ACM international conference
on Information and knowledge management, pages
1885?1889. ACM.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing (2nd Edition) (Prentice
Hall Series in Artificial Intelligence). Prentice Hall.
Zhen Liao, Yang Song, Li-wei He, and Yalou Huang.
2012. Evaluating the effectiveness of search task
trails. In Proceedings of the 21st international con-
ference on World Wide Web, pages 489?498. ACM.
Eetu Ma?kela?, Osma Suominen, and Eero Hyvo?nen.
2007. Automatic exhibition generation based on
semantic cultural content. In Proc. of the Cultural
Heritage on the Semantic Web Workshop at ISWC+
ASWC, volume 2007.
Lev Manovich. 1999. Database as symbolic form.
Convergence: The International Journal of Re-
search into New Media Technologies, 5(2):80?99.
Neil McIntyre and Mirella Lapata. 2010. Plot induc-
tion and evolutionary search for story generation. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1562?
1572. Association for Computational Linguistics.
D. Milne and I.H. Witten. 2008. Learning to link with
wikipedia. In Proceeding of the 17th ACM confer-
ence on Information and knowledge management,
pages 509?518. ACM.
Paul Mulholland and Trevor Collins. 2002. Using dig-
ital narratives to support the collaborative learning
and exploration of cultural heritage. In Database
and Expert Systems Applications, 2002. Proceed-
ings. 13th International Workshop on, pages 527?
531. IEEE.
Mykola Pechenizkiy and Toon Calders. 2007. A
framework for guiding the museum tours person-
alization. In Proceedings of the Workshop on Per-
sonalised Access to Cultural Heritage (PATCH07),
pages 11?28.
Don Peterson and Mark Levene. 2003. Trail records
and navigational learning. London review of Educa-
tion, 1(3):207?216.
S. Reich, L. Carr, D. De Roure, and W. Hall. 1999.
Where have you been from here? Trails in hypertext
systems. ACM Computing Surveys, 31.
Frank M Shipman, Richard Furuta, Donald Brenner,
Chung-Chi Chung, and Hao-wei Hsieh. 2000.
Guided paths through web-based collections: De-
sign, experiences, and adaptations. Journal of
the American Society for Information Science,
51(3):260?272.
K. Walker, A. Main, and Fass. J. 2013. User-
Generated Trails in Third Places. In HCI-3P Work-
shop on Human Computer Interaction for Third
Places at Computer Human Interaction 2013.
Kevin Walker. 2006. Story structures. building nar-
rative trails in museums. In Technology-Mediated
Narrative Environments for Learning, pages 103?
114. Sense Publishers.
Yiwen Wang, Lora M Aroyo, Natalia Stash, and Lloyd
Rutledge. 2007. Interactive user modeling for per-
sonalized access to museum collections: The ri-
jksmuseum case study. In User Modeling 2007,
pages 385?389. Springer.
Robert West and Jure Leskovec. 2012. Human
wayfinding in information networks. In Proceed-
ings of the 21st international conference on World
Wide Web, pages 619?628. ACM.
Richard Wheeldon and Mark Levene. 2003. The best
trail algorithm for assisted navigation of web sites.
In Web Congress, 2003. Proceedings. First Latin
American, pages 166?178. IEEE.
Ryen W White and Jeff Huang. 2010. Assessing the
scenic route: measuring the value of search trails in
web logs. In Proceedings of the 33rd international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 587?594. ACM.
M. Wilson, Kulesm B., M. Schraefel, and B. Schnei-
derman. 2010. From keyword search to explo-
ration: Designing future search interfaces for the
web. Foundations and Trends in Web Science,
2(1):1?97.
Kristian Woodsend and Mirella Lapata. 2010. Auto-
matic generation of story highlights. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 565?574. Associ-
ation for Computational Linguistics.
Xiaojun Yuan and Ryen White. 2012. Building the
trail best traveled: effects of domain knowledge on
web search trailblazing. In Proceedings of the 2012
ACM annual conference on Human Factors in Com-
puting Systems, pages 1795?1804. ACM.
9
Zdenek Zdrahal, Paul Mulholland, and Trevor Collins.
2008. Exploring pathways across stories. In Proc.
of International Conference on Distributed Human-
Machine Systems.
10
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 80?84,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Applying UMLS for Distantly Supervised Relation Detection
Roland Roller and Mark Stevenson
University of Sheffield
Regent Court, 211 Portobello
S1 4DP Sheffield, UK
{R.Roller,M.Stevenson}@dcs.shef.ac.uk
Abstract
This paper describes first results using
the Unified Medical Language System
(UMLS) for distantly supervised relation
extraction. UMLS is a large knowledge
base which contains information about
millions of medical concepts and relations
between them. Our approach is evaluated
using existing relation extraction data sets
that contain relations that are similar to
some of those in UMLS.
1 Introduction
Distant supervision has proved to be a popular ap-
proach to relation extraction (Craven and Kum-
lien, 1999; Mintz et al., 2009; Hoffmann et al.,
2010; Nguyen and Moschitti, 2011). It has the
advantage that it does not require manually anno-
tated training data. Distant supervision avoids this
by using information from a knowledge base to
automatically identify instances of a relation from
text and use them in order to generate training data
for a relation extraction system.
Distant supervision has already been applied
to the biomedical domain (Craven and Kumlien,
1999; Thomas et al., 2011). Craven and Kum-
lien (1999) were the first to apply distant supervi-
sion and used the Yeast Protein Database (YPD) to
detect sentences containing subcellar-localization
relations. Thomas et al. (2011) trained a clas-
sifier for protein-protein interactions (PPI) using
the knowledge base IntAct and evaluated their ap-
proach on different PPI corpora.
There have also been recent applications of dis-
tant supervision outside the biomedical domain.
The use of Freebase to train a classifier, e.g.
(Mintz et al., 2009; Riedel et al., 2010), has proved
popular. Other, such as Hoffmann et al. (2010),
use Wikipedia info-boxes as the knowledge base.
Applications of distant supervision face several
challenges. The main problem is ensuring the
quality of the automatically identified training in-
stances identified by the self-annotation. The use
of instances that have been incorrectly labelled as
positive can lower performance (Takamatsu et al.,
2012). Another problem arises when positive ex-
amples are included in the set of negative train-
ing instances, which can occur when information
is missing from the knowledge base (Min et al.,
2013; Ritter et al., 2013; Xu et al., 2013).
Evaluation of relation extraction systems that
use distant supervision represents a further chal-
lenge. In the ideal case an annotated evaluation set
is available. Others, such as Ritter et al. (2013) and
Hoffmann et al. (2011), use Freebase as knowl-
edge base and evaluate their classifier on an an-
notated New York Times corpus. However, if no
evaluation set is available leave-out can be used
where the data identified using distant supervision
used for both training and testing (Hoffmann et al.,
2010).
This paper makes use of the Unified Medical
Language System (UMLS) as a knowledge source
for distant supervision. It is widely used for
biomedical language processing and readily avail-
able. The advantage of UMLS is that it contains
information about a wide range of different types
of relations and therefore has the potential to gen-
erate a large number of relation classifiers. To our
knowledge, it has not been used as a knowledge
source to train relation extraction systems.
Evaluating such as wide range of relation clas-
sifiers is not straightforward due to the lack of
gold-standard data. As an alternative approach we
make use of existing annotated data sets and iden-
tify ones which contain relations that are similar to
those included in UMLS.
The next section provides a short description of
UMLS. We then describe how we acquire existing
data sets to evaluate certain relations. In section 4
we present our first results using UMLS for distant
supervision.
80
2 Unified Medical Language System
The Unified Medical Language System
1
is a set
of files and software which combines different
biomedical vocabularies, knowledge bases and
standards. The Metathesaurus is a database within
UMLS which contains several million biomedical
and health related names and concepts and rela-
tionships among them. All different names of a
concept are unified by the Concept Unique Identi-
fiers (CUI). MRREL is a subset of the Metathe-
saurus and involves different relationships be-
tween different medical concepts defined by a pair
of CUIs. Many of them are child-parent rela-
tionships, express a synonymy or are vaguely de-
fined as broader or narrower relation. Other re-
lations are more specific, such as has location or
drug contraindicated for. This work focuses on
more specific types of relations.
3 Acquiring Evaluation Data Sets
We examined a number of relation extraction data
sets in order to identify ones that could be used to
evaluate our system. The aim is to find a data set
that is annotated with relations that are similar to
some of those found in the UMLS. If an appropri-
ate relation can be identified then a relation extrac-
tion system can be trained using information from
the UMLS and evaluated using the data set.
To determine whether a data set is suitable we
used MetaMap (Aronson and Lang, 2010) to iden-
tify the CUIs for each related item. We then com-
pared each pair against the MRREL table to deter-
mine whether it is included as a relation. To in-
crease coverage we also included parent and child
nodes in the mapping process.
Table 1 shows the mappings obtained for two
of the data sets: the DDI 2011 data set (Segura-
Bedmar et al., 2011) and the data set described by
Rosario and Hearst (2004).
The DDI data set contains information about
drug-drug interactions and includes a single re-
lation (DDI). The relations it contained were
mapped onto 701 CUI pairs. 266 (37.9%) of these
mappings could be matched to the MRREL rela-
tion has contraindicated drug. Many of the CUI
pairs could also be mapped to the isa relationship
in MRREL, but this is a very general relationship
and the matches are caused by the large number of
these in UMLS rather than it being a reasonable
1
https://www.nlm.nih.gov/research/umls/
match for the DDI relation.
The data set described by Rosario and Hearst
(2004) focuses on different relationships between
treatments and diseases. The two most com-
mon relations TREAT FOR DIS (TREAT), denot-
ing the treatment for a particular disease, and PRE-
VENT (PREV), which indicates that a treatment
can be used to prevent a disease. The MRREL
isa relationship also matches many of these re-
lations, again due to its prevalence in MRREL.
Other MRREL relations (may be prevented by
and may be treated by) match fewer CUI pairs but
seem to be better matches for the TREAT and
PREV relations.
Relation MRREL
DDI (701) has contraindicated drug (266),
isa (185), may treat (57),
has contraindication (51)
PREV (41) isa (11), may be prevented by (5)
TREAT (741) isa (172), may be treated by (118)
Table 1: Relation mapping to MRREL
It is important to note that it is not always possi-
ble to find a CUI mapping for each entity and the
mapping process means that the mapping cannot
be guaranteed to be correct in all cases. High cov-
erage does not necessarily mean that a corpus is
very similar to a certain MRREL relation, just that
many of the CUI pairs which have been mapped
to the related entities in the corpus occur often to-
gether in a certain MRREL relation. However, in
the absence of any other suitable evaluation data
we assume that high coverage is an indicator that
the relations are strongly similar and use these two
data sets for evaluation.
4 Distant Supervision using UMLS
In this section we carry out two different dis-
tant supervised experiments using UMLS. The
first experiment will be evaluated on a subset
of the DDI 2011 training data set using the
MRREL relation has contraindicated drug and
has contraindication. The second experiment
uses the MRREL relations may be treated by and
may be prevented by and are evaluated on the
Rosario & Hearst data set.
We use 7,500,000 Medline abstracts annotated
with CUIs using MetaMap (choosing the best
mapping as annotation) as a corpus for distant su-
pervision. Our information extraction platform
based on a system developed for the BioNLP
81
Shared Task 2013 (Roller and Stevenson, 2013).
In contrast to our previous work, our classification
process relies on the Shallow Linguistic Kernel
(Giuliano et al., 2006) in combination with Lib-
SVM (Chang and Lin, 2011) taking the kernel as
input.
4.1 Experiment 1: DDI 2011
The DDI 2011 data set was split into training and
test sets for the experiments. Table 2 presents
results that place the distant supervision perfor-
mance in context. The naive classification ap-
proach predicts all candidate pairs as positive. The
supervised approach is trained on the training set,
using the same kernel method as our distant su-
pervised experiments and evaluated on the test set.
This represents the performance that can be ob-
tained using manually labelled training data and
can be considered as an upper bound for distant
supervision.
Method Prec. / Recall / F1
naive 0.098 / 1.000 / 0.178
supervised 0.428 / 0.702 / 0.532
Table 2: DDI 2011 baseline results
The distant supervision approach requires pairs
of positive and negative CUI to be identified.
These pairs are used to identify positive and nega-
tive examples of the target relation from a corpus.
Pairs which occur in our target MRREL relation
are used as positive CUI pairs. Negative pairs are
generated by selecting pairs of CUIs that are occur
in any other MRREL relation.
Sentences containing these CUI pairs are iden-
tified in the subset of the MetaMapped Medline.
In the basic setup (basic), sentences containing a
positive pair will be considered as a positive train-
ing example. There are many cases where just the
occurrence of a positive MRREL pair does not ex-
press the target relation. In an effort to remove
this noisy data we apply some simple heuristics.
The first discards all training instances with more
than five words (5w) between the two entities, an
approach similar to one applied by Takamatsu et
al. (2012). The second discards positive sentences
containing a comma between the related entities
(com). We found that commas often indicate a sen-
tence containing a list of items (e.g. genes or dis-
eases) and that these sentences do not form good
training examples due to the multiple relations that
are possible when there are several items. Finally
we also apply a combination of both techniques
(5w+com).
1000 positive examples were generated using
each approach and used for training. Although it
would be possible to generate more examples for
some approaches, for example basic, applying the
combination of techniques (5w+com) significantly
reduces the number of instances available.
Method has contraindication has contraindicated
(P./R./F1) drug (P./R./F1)
basic 0.146 / 0.371 / 0.210 0.158 / 0.598 / 0.250
5w 0.109 / 0.641 / 0.187 0.207 / 0.487 / 0.290
com 0.212 / 0.560 / 0.308 0.177 / 0.498 / 0.261
5w+com 0.207 / 0.487 / 0.291 0.214 / 0.471 / 0.294
Table 3: Evaluation with DDI 2011
Table 3 presents results of the experiments.
The results show that all applied techniques for
both MRREL relations outperform the naive ap-
proach. The best results in terms of F1 score
for the has contraindication MRREL relation
are obtained using the com selection technique.
Applying just 5w leads to worse results than
using the basic approach. The situation for
has contraindicated drug is different. The classi-
fier provides for all techniques a better F1 score
than the basic approach. The best results are
achieved by using 5w+com. It is interesting to see,
that both MRREL relations provide similar aver-
age classification results, even if both relations are
different from the target relation and cover com-
pletely different CUI pairs. It is also interest-
ing that the MRREL relation has contraindication
has a lower coverage to the DDI relation than
has contraindicated drug, but provides slightly
better results overall. A problem with the distant
supervised classification of these two MRREL re-
lations is their low occurrence in our Medline sub-
set. Using more training data will often lead to
better results. In our case, if we apply the com-
bined selection technique, there are fewer positive
training instances than are available to the super-
vised approach, making it difficult to outperform
the supervised approach.
4.2 Experiment 2: Rosario & Hearst
The second experiment addresses the prob-
lem of detecting the MRREL relations
may be prevented by and may be treated by.
Parts of the Rosario & Hearst data set are used
to evaluate this relation. This data set differs
in structure from the DDI data set. Instead of
82
annotating the entities in the sentence according
to its relation, the annotations in the data set
indicate whether a certain relation occurs in the
sentence. This data set does not contain any
negative examples. If a sentence contains two
entities, it will always describe a certain relation.
A supervised classifier is created by dividing the
data set into training and test sets. The test set
contains 253 different sentences (221 describe
a TREAT relation, 15 a PREV relation and 17
involve other relationships). Positive and negative
CUI pairs are selected in a different way to the
previous experiment. The two most frequent
relations in the data set are TREAT and PREV.
A classifier for a particular relation is trained
using sentences annotated with the corresponding
MRREL relation as positive instances. Negative
instances are identified using the other relation.
For example, the classifier for the TREAT relation
is trained using positive examples identified
using may be treated by with negative examples
generated using may be treated by.
Table 4 shows the baseline results on the data
set using a naive and a supervised approach on the
two original relations TREAT and PREV. Perfor-
mance of the naive approach for TREAT is very
high since the majority of sentences in the data set
are annotated with that relation.
Data Set Method Prec. / Recall / F1
TREAT
naive 0.874 / 1.000 / 0.933
supervised 0.944 / 0.923 / 0.934
PREV
naive 0.059 / 1.000 / 0.112
supervised 0.909 / 0.667 / 0.769
Table 4: Rosario & Hearst baseline results
Table 5 shows the results for the various dis-
tant supervision approaches. Again, 1000 positive
training examples were used to train the classifier.
Since the F-Score of the naive and the supervised
approaches of TREAT are very high, it is difficult
to compete with the may be treated by distant su-
pervised classifier. However, considering that just
15.9% of the TREAT instance pairs of the train-
ing set match the MRREL may be treated by re-
lation, the results are promising. Furthermore, the
precision of all may be treated by distant super-
vised experiments outperform the naive approach.
The best results are achieved using com as selec-
tion technique.
The experiments using the PREV relation for
evaluation are more interesting. Due to its low
occurrence in the test set it is more difficult to
detect this relation. The distant supervised clas-
sifier trained with the may be prevented by rela-
tion easily outperforms the naive approach. The
best overall F1 scoer results are achieved using
the 5w technique. As expected the distant super-
vised results are outperformed by the supervised
approach. However, the recall for all distantly su-
pervised approaches are at least as high as those
obtained using the supervised approach.
may be treated by may be prevented by
evaluated on TREAT evaluated on PREV
Method (P./R./F1) (P./R./F1)
basic 0.926 / 0.733 / 0.818 0.286 / 0.667 / 0.400
5w 0.925 / 0.783 / 0.848 0.407 / 0.733 / 0.524
com 0.928 / 0.819 / 0.870 0.222 / 0.800 / 0.348
5w+com 0.924 / 0.769 / 0.840 0.361 / 0.867 / 0.510
Table 5: Evaluation with Rosario & Hearst data
set
5 Conclusion and Discussion
In this paper we presented first results using
UMLS to train a distant supervised relational clas-
sifier. Evaluation was carried out using existing
evaluation data sets since no resources directly an-
notated with UMLS relations were available. We
showed that using a distantly supervised classifier
trained on MRREL relations similar to those found
in the evaluation data set provides promising re-
sults.
Overall, our system works with some compo-
nents which should be improved to achieve better
results. First, we rely on a cheap and fast anno-
tation using MetaMap, which might produce an-
notation errors. In addition, the use of noisy dis-
tant supervised training data decreases the classi-
fication quality. An improvement of the selection
process and an improvement of the classification
method, such as Chowdhury and Lavelli (2013),
could lead to better classification results. In future
we would also like to make further use of existing
data sets with similar relations to those of interest
to evaluate distant supervision approaches.
Acknowledgements
The authors are grateful to the Engineering
and Physical Sciences Research Council for
supporting the work described in this paper
(EP/J008427/1).
83
References
A. Aronson and F. Lang. 2010. An overview of
MetaMap: historical perspective and recent ad-
vances. Journal of the American Medical Associ-
ation, 17(3):229?236.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1?27:27.
Md. Faisal Mahbub Chowdhury and Alberto Lavelli.
2013. Fbk-irst : A multi-phase kernel based ap-
proach for drug-drug interaction detection and clas-
sification that exploits linguistic information. In
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM), Volume 2: Proceedings
of the Seventh International Workshop on Seman-
tic Evaluation (SemEval 2013), pages 351?355, At-
lanta, Georgia, USA, June. Association for Compu-
tational Linguistics.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In In Proceedings of the Sev-
enth International Conference on Intelligent Systems
for Molecular Biology (ISMB), pages 77?86. AAAI
Press.
Claudio Giuliano, Alberto Lavelli, and Lorenza Ro-
mano. 2006. Exploiting shallow linguistic infor-
mation for relation extraction from biomedical liter-
ature. In In Proc. EACL 2006.
Raphael Hoffmann, Congle Zhang, and Daniel S.
Weld. 2010. Learning 5000 relational extractors. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ?10,
pages 286?295, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics, ACL ?11, pages 541?
550.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 777?782, Atlanta, Georgia, June.
Association for Computational Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2 - Volume 2, ACL ?09, pages 1003?1011,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Truc-Vien T. Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant super-
vision from external semantic repositories. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
?11, pages 277?282, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of the European
Conference on Machine Learning and Knowledge
Discovery in Databases (ECML PKDD ?10).
Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Et-
zioni. 2013. Modeling missing data in distant su-
pervision for information extraction. In Association
for Computational Linguistics Vol. 1 (TACL).
Roland Roller and Mark Stevenson. 2013. Identi-
fication of genia events using multiple classifiers.
In Proceedings of BioNLP Shared Task 2013 Work-
shop, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Barbara Rosario and Marti A. Hearst. 2004. Classi-
fying semantic relations in bioscience texts. In Pro-
ceedings of the 42nd Annual Meeting on Association
for Computational Linguistics, ACL ?04, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Isabel Segura-Bedmar, Paloma Martnez, and Daniel
Snchez-Cisneros. 2011. The 1st ddi extraction-
2011 challenge task: Extraction of drug-drug inter-
actions from biomedical texts. In Proceedings of
DDI Extraction-2011 challenge task., pages 1?9.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers - Volume 1, ACL
?12, pages 721?729, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Philippe Thomas, Ill?es Solt, Roman Klinger, and Ulf
Leser. 2011. Learning protein protein interaction
extraction using distant supervision. In Proceedings
of Robust Unsupervised and Semi-Supervised Meth-
ods in Natural Language Processing, pages 34?41.
Wei Xu, Raphael Hoffmann, Le Zhao, and Ralph Gr-
ishman. 2013. Filling knowledge base gaps for dis-
tant supervision of relation extraction. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 665?670, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
84
