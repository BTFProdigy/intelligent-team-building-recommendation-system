Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 227?235,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Efficient Parsing for Transducer Grammars
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein
Computer Science Division
University of California, Berkeley
{denero, mbansal, adpauls, klein}@cs.berkeley.edu
Abstract
The tree-transducer grammars that arise in
current syntactic machine translation systems
are large, flat, and highly lexicalized. We ad-
dress the problem of parsing efficiently with
such grammars in three ways. First, we
present a pair of grammar transformations
that admit an efficient cubic-time CKY-style
parsing algorithm despite leaving most of the
grammar in n-ary form. Second, we show
how the number of intermediate symbols gen-
erated by this transformation can be substan-
tially reduced through binarization choices.
Finally, we describe a two-pass coarse-to-fine
parsing approach that prunes the search space
using predictions from a subset of the origi-
nal grammar. In all, parsing time reduces by
81%. We also describe a coarse-to-fine prun-
ing scheme for forest-based language model
reranking that allows a 100-fold increase in
beam size while reducing decoding time. The
resulting translations improve by 1.3 BLEU.
1 Introduction
Current approaches to syntactic machine translation
typically include two statistical models: a syntac-
tic transfer model and an n-gram language model.
Recent innovations have greatly improved the effi-
ciency of language model integration through multi-
pass techniques, such as forest reranking (Huang
and Chiang, 2007), local search (Venugopal et al,
2007), and coarse-to-fine pruning (Petrov et al,
2008; Zhang and Gildea, 2008). Meanwhile, trans-
lation grammars have grown in complexity from
simple inversion transduction grammars (Wu, 1997)
to general tree-to-string transducers (Galley et al,
2004) and have increased in size by including more
synchronous tree fragments (Galley et al, 2006;
Marcu et al, 2006; DeNeefe et al, 2007). As a result
of these trends, the syntactic component of machine
translation decoding can now account for a substan-
tial portion of total decoding time. In this paper,
we focus on efficient methods for parsing with very
large tree-to-string grammars, which have flat n-ary
rules with many adjacent non-terminals, as in Fig-
ure 1. These grammars are sufficiently complex that
the purely syntactic pass of our multi-pass decoder is
the compute-time bottleneck under some conditions.
Given that parsing is well-studied in the mono-
lingual case, it is worth asking why MT grammars
are not simply like those used for syntactic analy-
sis. There are several good reasons. The most im-
portant is that MT grammars must do both analysis
and generation. To generate, it is natural to mem-
orize larger lexical chunks, and so rules are highly
lexicalized. Second, syntax diverges between lan-
guages, and each divergence expands the minimal
domain of translation rules, so rules are large and
flat. Finally, we see most rules very few times, so
it is challenging to subcategorize non-terminals to
the degree done in analytic parsing. This paper de-
velops encodings, algorithms, and pruning strategies
for such grammars.
We first investigate the qualitative properties of
MT grammars, then present a sequence of parsing
methods adapted to their broad characteristics. We
give normal forms which are more appropriate than
Chomsky normal form, leaving the rules mostly flat.
We then describe a CKY-like algorithm which ap-
plies such rules efficiently, working directly over the
n-ary forms in cubic time. We show how thoughtful
227
NNP
1
 no daba una bofetada a DT
2
 NN
3
 verde
S ? NNP no daba una bofetada a DT NN verde
DT+NN ? DT NN
Maria no daba una bofetada a la bruja verde
Mary did not slap the green witch
S
NNP NNDT
NNP
1
 did not slap DT
2
 green NN
3
S ?
Lexical normal form (LNF) transformation
S ? NNP no daba una bofetada a DT NN verde
S\NNP ? no daba una bofetada a DT+NN verde
S ? NNP S\NNP
Anchored LNF transformation
DT+NN ? DT NN
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
0
17,500
35,000
52,500
70,000
1 2 3 4 5 6+
Original grammar rules
NP ? DT NN NNS
S ? NNP no daba una bofetada a DT+NN verde
NP ? DT+NN NNS NP ? DT NN+NNSor
Type-minimizing binarization
Required symbols Sequences to build
DT+NN DT NN
NNS NNP NP S
DT,NN
DT,NN,NNS 
Minimal binary rules for LNF
NP ? DT+NN NNS
NP ? DT+NN NNS
X
no
la
X
daba
bruja
daba
VP ? no daba
NP ? la bruja
... Maria daba ...
S ? NP   daba
S ? NP daba
(a)
(b)
(c)
Figure 1: (a) A synchronous transducer rule has co-
indexed non-terminals on the source and target side. In-
ternal grammatical structure of the target side has been
omitted. (b) The source-side projection of the rule is a
monolingual source-language rule with target-side gram-
mar symbols. (c) A training sentence pair is annotated
with a target-side parse tree and a word alignment, which
license this rule to be extracted.
binarization can further increase parsing speed, and
we present a new coarse-to-fine scheme that uses
rule subsets rather than symbol clustering to build
a coarse grammar projection. These techniques re-
duce parsing time by 81% in aggregate. Finally,
we demonstrate that we can accelerate forest-based
reranking with a language model by pruning with
information from the parsing pass. This approach
enables a 100-fold increase in maximum beam size,
improving translation quality by 1.3 BLEU while
decreasing total decoding time.
2 Tree Transducer Grammars
Tree-to-string transducer grammars consist of
weighted rules like the one depicted in Figure 1.
Each n-ary rule consists of a root symbol, a se-
quence of lexical items and non-terminals on the
source-side, and a fragment of a syntax tree on
the target side. Each non-terminal on the source
side corresponds to a unique one on the target side.
Aligned non-terminals share a grammar symbol de-
rived from a target-side monolingual grammar.
These grammars are learned from word-aligned
sentence pairs annotated with target-side phrase
structure trees. Extraction proceeds by using word
alignments to find correspondences between target-
side constituents and source-side word spans, then
discovering transducer rules that match these con-
NNP
1
 no daba una bofetada a DT
2
 NN
3
 verde
S ? NNP no daba una bofetada a DT NN verde
DT+NN ? DT NN
Maria no daba una bofetada a la bruja verde
Mary did not slap the green witch
S
NNP NNDT
NNP
1
 did not slap DT
2
 green NN
3
S ?
Lexical rules cannot contain adjacent non-terminals
S ? NNP no daba una bofetada a DT NN verde
S\NNP ? no daba una bofetada a DT+NN verde
S ? NNP S\NNP
Anchored LNF rules are bounded by lexical items
DT+NN ? DT NN
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
0
17,500
35,000
52,500
70,000
1 2 3 4 5 6+
Original grammar rules are flat and lexical
NP ? DT NN NNS
S ? NNP no daba una bofetada a DT+NN verde
Non-lexical rules are binarized using few symbols
Required symbols Sequences to build
DT+NN DT NN
NNS NNP NP S
DT  NN
DT  NN  NNS 
Binary rules for LNF that minimize symbol count
NP ? DT+NN NNS
NP ? DT+NN NNS
X
no
la
X
daba
bruja
daba
VP ? no daba
NP ? la bruja
... Maria daba ...
S ? NP   daba
S ? NP daba
(a)
(b)
(c)
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7+
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
Right-branching
Left-branching
Greedy
Optimal (ILP)
0 3,000 6,000 9,000
443
1,101
5,871
8,095
0
30,000
60,000
90,000
1 2 3 4 5 6 7+
Figure 2: Transducer grammars are composed of very flat
rules. Above, the histogram shows rule counts for each
rule size among the 332,000 rules that apply to an indi-
vidual 30-word sentence. The size of a rule is the total
number of non-terminals and lexical items in its source-
side yield.
stituent alignments (Galley et al, 2004). Given this
correspondence, an array of extraction procedures
yields rules that are well-suited to machine trans-
lation (Galley et al, 2006; DeNeefe et al, 2007;
Marcu et al, 2006). Rule weights are estimated
by discriminatively combining relative frequency
counts and other rule features.
A transducer grammarG can be projected onto its
source language, inducing a monolingual grammar.
If we weight each rule by the maximumweight of its
projecting synchronous rules, then parsing with this
projected grammar maximizes the translation model
score for a source sentence. We need not even con-
sider the target side of transducer rules until integrat-
ing an n-gram language model or other non-local
features of the target language.
We conduct experiments with a grammar ex-
tracted from 220 million words of Arabic-English
bitext, extracting rules with up to 6 non-terminals. A
histogram of the size of rules applicable to a typical
30-word sentence appears in Figure 2. The grammar
includes 149 grammatical symbols, an augmentation
of the Penn Treebank symbol set. To evaluate, we
decoded 300 sentences of up to 40 words in length
from the NIST05 Arabic-English test set.
3 Efficient Grammar Encodings
Monolingual parsing with a source-projected trans-
ducer grammar is a natural first pass in multi-pass
decoding. These grammars are qualitatively dif-
ferent from syntactic analysis grammars, such as
the lexicalized grammars of Charniak (1997) or the
heavily state-split grammars of Petrov et al (2006).
228
In this section, we develop an appropriate grammar
encoding that enables efficient parsing.
It is problematic to convert these grammars into
Chomsky normal form, which CKY requires. Be-
cause transducer rules are very flat and contain spe-
cific lexical items, binarization introduces a large
number of intermediate grammar symbols. Rule size
and lexicalization affect parsing complexity whether
the grammar is binarized explicitly (Zhang et al,
2006) or implicitly binarized using Early-style inter-
mediate symbols (Zollmann et al, 2006). Moreover,
the resulting binary rules cannot be Markovized to
merge symbols, as in Klein andManning (2003), be-
cause each rule is associated with a target-side tree
that cannot be abstracted.
We also do not restrict the form of rules in the
grammar, a common technique in syntactic machine
translation. For instance, Zollmann et al (2006)
follow Chiang (2005) in disallowing adjacent non-
terminals. Watanabe et al (2006) limit grammars
to Griebach-Normal form. However, general tree
transducer grammars provide excellent translation
performance (Galley et al, 2006), and so we focus
on parsing with all available rules.
3.1 Lexical Normal Form
Sequences of consecutive non-terminals complicate
parsing because they require a search over non-
terminal boundaries when applied to a sentence
span. We transform the grammar to ensure that all
rules containing lexical items (lexical rules) do not
contain sequences of non-terminals. We allow both
unary and binary non-lexical rules.
Let L be the set of lexical items and V the set
of non-terminal symbols in the original grammar.
Then, lexical normal form (LNF) limits productions
to two forms:
Non-lexical: X ? X1(X2)
Lexical: X ? (X1)?(X2)
? = w+(Xiw+)?
Above, all Xi ? V and w+ ? L+. Symbols in
parentheses are optional. The nucleus ? of lexical
rules is a mixed sequence that has lexical items on
each end and no adjacent non-terminals.
Converting a grammar into LNF requires two
steps. In the sequence elimination step, for every
NNP
1
 no d ba una bofetada  DT
2
 NN
3
 verde
S ? NNP no daba una bofetada a DT NN verd
Maria no daba una bofetada a la bruja verde
Mary did not slap the green witch
S
NNP NNDT
NNP
1
 did not slap DT
2
 green NN
3
S ?
LNF replaces non-terminal sequences in lexical rules
S ? NNP no daba una bofetada a DT NN verde
S\NNP ? no daba una bofetada a DT+NN verde
S ? NNP S\NNP
Anchored LNF rules are bounded by lexical items
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
0
17,500
35,000
52,500
70,000
1 2 3 4 5 6+
Original grammar rules are flat and lexical
NP ? DT NN NNS
S ? NNP no daba una bofetada a DT+NN verde
Non-lexical rules are binarized using few symbols
Non-lexical rules before binarization:
Equivalent binary rules, minimizing symbol count:
X
no
la
X
daba
bruja
daba
VP ? no daba
NP ? la bruja
... Maria daba ...
S ? NP   daba
S ? NP daba
(a)
(b)
(c)
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7+
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
Right-branching
Left-branching
Greedy
Optimal (ILP)
0 3,000 6,000 9,000
443
1,101
5,871
8,095
0
30,000
60,000
90,000
1 2 3 4 5 6 7+
DT+NN ? DT NN
NP ? DT NN NNS DT+NN ? DT NN
NP ? DT+NN NNS DT+NN ? DT NN
NP ? DT+NN NNS DT+NN ? DT NN
Figure 3: We transform the original grammar by first
eliminating non-terminal sequences in lexical rules.
Next, we binarize, adding a minimal number of inter-
mediate grammar symbols and binary non-lexical rules.
Finally, anchored LNF further transforms lexical rules
to begin and end with lexical items by introducing ad-
ditional symbols.
lexical rule we replace each sequence of consecutive
non-terminalsX1 . . . Xn with the intermediate sym-
bol X1+. . .+Xn (abbreviated X1:n) and introduce a
non-lexical rule X1+. . .+Xn ? X1 . . . Xn. In the
binarization step, we introduce further intermediate
symbols and rules to binarize all non-lexical rules
in the grammar, including those added by sequence
elimination.
3.2 Non-terminal Binarization
Exactly howwe binarize non-lexical rules affects the
total number of intermediate symbols introduced by
the LNF transformation.
Binarization involves selecting a set of symbols
that will allow us to assemble the right-hand side
X1 . . . Xn of every non-lexical rule using binary
productions. This symbol set must at least include
the left-hand side of every rule in the grammar
(lexical and non-lexical), including the intermediate
229
NNP
1
 no daba una bofetada a DT
2
 NN
3
 verde
S ? NNP no daba una bofetada a DT NN verde
DT+NN ? DT NN
Maria no daba una bofetada a la bruja verde
Mary did not slap the green witch
S
NNP NNDT
NNP
1
 did not slap DT
2
 green NN
3
S ?
Lexical rules cannot contain adjacent non-terminals
S ? NNP no daba una bofetada a DT NN verde
S\NNP ? no daba una bofetada a DT+NN verde
S ? NNP S\NNP
Anchored LNF rules are bounded by lexical items
DT+NN ? DT NN
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
0
17,500
35,000
52,500
70,000
1 2 3 4 5 6+
Original grammar rules are flat and lexical
NP ? DT NN NNS
S ? NNP no daba una bofetada a DT+NN verde
Non-lexical rules are binarized using few symbols
Required symbols Sequences to build
DT+NN DT NN
NNS NNP NP S
DT  NN
DT  NN  NNS 
Binary rules for LNF that minimize symbol count
NP ? DT+NN NNS
NP ? DT+NN NNS
X
no
la
X
daba
bruja
daba
VP ? no daba
NP ? la bruja
... Maria daba ...
S ? NP   daba
S ? NP daba
(a)
(b)
(c)
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7+
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
Right-branching
Left-branching
Greedy
Optimal (ILP)
0 3,000 6,000 9,000
443
1,101
5,871
8,095
0
30,000
60,000
90,000
1 2 3 4 5 6 7+ Figure 4: The number of non-terminal symbols intro-
duced to the grammar through LNF binarization depends
upon the policy for binarizing type sequences. This ex-
periment shows results from transforming a grammar that
has already been filtered for a particular short sentence.
Both the greedy and optimal binarizations use far fewer
symbols than naive binarizations.
symbols X1:n introduced by sequence elimination.
To ensure that a symbol sequence X1 . . . Xn can
be constructed, we select a split point k and add in-
termediate types X1:k and Xk+1:n to the grammar.
We must also ensure that the sequences X1 . . . Xk
and Xk+1 . . . Xn can be constructed. As baselines,
we used left-branching (where k = 1 always) and
right-branching (where k = n? 1) binarizations.
We also tested a greedy binarization approach,
choosing k to minimize the number of grammar
symbols introduced. We first try to select k such that
both X1:k and Xk+1:n are already in the grammar.
If no such k exists, we select k such that one of the
intermediate types generated is already used. If no
such k exists again, we choose k = ?12n
?. This pol-
icy only creates new intermediate types when nec-
essary. Song et al (2008) propose a similar greedy
approach to binarization that uses corpus statistics to
select common types rather than explicitly reusing
types that have already been introduced.
Finally, we computed an optimal binarization that
explicitly minimizes the number of symbols in the
resulting grammar. We cast the minimization as an
integer linear program (ILP). Let V be the set of
all base non-terminal symbols in the grammar. We
introduce an indicator variable TY for each symbol
Y ? V + to indicate that Y is used in the grammar.
Y can be either a base non-terminal symbol Xi or
an intermediate symbol X1:n. We also introduce in-
dicators AY,Z for each pairs of symbols, indicating
that both Y and Z are used in the grammar. Let
L ? V + be the set of left-hand side symbols for
all lexical and non-lexical rules already in the gram-
mar. Let R be the set of symbol sequences on the
right-hand side of all non-lexical rules. Then, the
ILP takes the form:
min ?
Y ?V +
TY (1)
s.t. TY = 1 ? Y ? L (2)
1 ??
k
AX1:k,Xk+1:n ? X1 . . . Xn ? R (3)
TX1:n ?
?
k
AX1:k,Xk+1:n ? X1:n (4)
AY,Z ? TY , AY,Z ? TZ ? Y, Z (5)
The solution to this ILP indicates which symbols
appear in a minimal binarization. Equation 1 explic-
itly minimizes the number of symbols. Equation 2
ensures that all symbols already in the grammar re-
main in the grammar.
Equation 3 does not require that a symbol repre-
sent the entire right-hand side of each non-lexical
rule, but does ensure that each right-hand side se-
quence can be built from two subsequence symbols.
Equation 4 ensures that any included intermediate
type can also be built from two subsequence types.
Finally, Equation 5 ensures that if a pair is used, each
member of the pair is included. This program can be
optimized with an off-the-shelf ILP solver.1
Figure 4 shows the number of intermediate gram-
mar symbols needed for the four binarization poli-
cies described above for a short sentence. Our ILP
solver could only find optimal solutions for very
short sentences (which have small grammars after
relativization). Because greedy requires very little
time to compute and generates symbol counts that
are close to optimal when both can be computed, we
use it for our remaining experiments.
3.3 Anchored Lexical Normal Form
We also consider a further grammar transformation,
anchored lexical normal form (ALNF), in which the
yield of lexical rules must begin and end with a lex-
ical item. As shown in the following section, ALNF
improves parsing performance over LNF by shifting
work from lexical rule applications to non-lexical
1We used lp solve: http://sourceforge.net/projects/lpsolve.
230
rule applications. ALNF consists of rules with the
following two forms:
Non-lexical: X ? X1(X2)
Lexical: X ? w+(Xiw+)?
To convert a grammar into ALNF, we first transform
it into LNF, then introduce additional binary rules
that split off non-terminal symbols from the ends of
lexical rules, as shown in Figure 3.
4 Efficient CKY Parsing
We now describe a CKY-style parsing algorithm for
grammars in LNF. The dynamic program is orga-
nized into spans Sij and computes the Viterbi score
w(i, j,X) for each edge Sij [X], the weight of the
maximum parse over words i+1 to j, rooted at sym-
bol X . For each Sij , computation proceeds in three
phases: binary, lexical, and unary.
4.1 Applying Non-lexical Binary Rules
For a span Sij , we first apply the binary non-lexical
rules just as in standard CKY, computing an interme-
diate Viterbi score wb(i, j,X). Let ?r be the weight
of rule r. Then, wb(i, j,X) =
max
r=X?X1X2
?r
j?1max
k=i+1
w(i, k,X1) ? w(k, j,X2).
The quantitiesw(i, k,X1) andw(k, j,X2) will have
already been computed by the dynamic program.
The work in this phase is cubic in sentence length.
4.2 Applying Lexical Rules
On the other hand, lexical rules in LNF can be ap-
plied without binarization, because they only apply
to particular spans that contain the appropriate lexi-
cal items. For a given Sij , we first compute all the le-
gal mappings of each rule onto the span. A mapping
consists of a correspondence between non-terminals
in the rule and subspans of Sij . In practice, there
is typically only one way that a lexical rule in LNF
can map onto a span, because most lexical items will
appear only once in the span.
Let m be a legal mapping and r its corresponding
rule. Let S(i)k` [X] be the edge mapped to the ith non-terminal of r underm, and ?r the weight of r. Then,
wl(i, j,X) = maxm ?r
?
S(i)k` [X]
w(k, `,X).
Again, w(k, `,X) will have been computed by the
dynamic program. Assuming only a constant num-
ber of mappings per rule per span, the work in this
phase is quadratic. We can then merge wl and wb:
w(i, j,X) = max(wl(i, j,X), wb(i, j,X)).
To efficiently compute mappings, we store lexi-
cal rules in a trie (or suffix array) ? a searchable
graph that indexes rules according to their sequence
of lexical items and non-terminals. This data struc-
ture has been used similarly to index whole training
sentences for efficient retrieval (Lopez, 2007). To
find all rules that map onto a span, we traverse the
trie using depth-first search.
4.3 Applying Unary Rules
Unary non-lexical rules are applied after lexical
rules and non-lexical binary rules.
w(i, j,X) = max
r:r=X?X1
?rw(i, j,X1).
While this definition is recursive, we allow only one
unary rule application per symbol X at each span
to prevent infinite derivations. This choice does not
limit the generality of our algorithm: chains of unar-
ies can always be collapsed via a unary closure.
4.4 Bounding Split Points for Binary Rules
Non-lexical binary rules can in principle apply to
any span Sij where j ? i ? 2, using any split point
k such that i < k < j. In practice, however, many
rules cannot apply to many (i, k, j) triples because
the symbols for their children have not been con-
structed successfully over the subspans Sik and Skj .
Therefore, the precise looping order over rules and
split points can influence computation time.
We found the following nested looping order for
the binary phase of processing an edge Sij [X] gave
the fastest parsing times for these grammars:
1. Loop over symbols X1 for the left child
2. Loop over all rules X ? X1X2 containing X1
3. Loop over split points k : i < k < j
4. Update wb(i, j,X) as necessary
This looping order allows for early stopping via
additional bookkeeping in the algorithm. We track
the following statistics as we parse:
231
Grammar Bound checks Parsing time
LNF no 264
LNF yes 181
ALNF yes 104
Table 1: Adding bound checks to CKY and transforming
the grammar from LNF to anchored LNF reduce parsing
time by 61% for 300 sentences of length 40 or less. No
approximations have been applied, so all three scenarios
produce no search errors. Parsing time is in minutes.
minEND(i,X), maxEND(i,X): The minimum and
maximum position k for which symbol X was
successfully built over Sik.
minSTART(j,X), maxSTART(j,X): The minimum
and maximum position k for which symbol X
was successfully built over Skj .
We then bound k by mink and maxk in the inner
loop using these statistics. If ever mink > maxk,
then the loop is terminated early.
1. set mink = i+ 1,maxk = j ? 1
2. loop over symbols X1 for the left child
mink = max(mink,minEND(i,X1))
maxk = min(maxk,maxEND(i,X1))
3. loop over rules X ? X1X2
mink = max(mink,minSTART(j,X2))
maxk = min(maxk,maxSTART(j,X2))
4. loop over split points k : mink ? k ? maxk
5. update wb(i, j,X) as necessary
In this way, we eliminate unnecessary work by
avoiding split points that we know beforehand can-
not contribute to wb(i, j,X).
4.5 Parsing Time Results
Table 1 shows the decrease in parsing time from in-
cluding these bound checks, as well as switching
from lexical normal form to anchored LNF.
Using ALNF rather than LNF increases the num-
ber of grammar symbols and non-lexical binary
rules, but makes parsing more efficient in three
ways. First, it decreases the number of spans for
which a lexical rule has a legal mapping. In this way,
ALNF effectively shifts work from the lexical phase
to the binary phase. Second, ALNF reduces the time
spent searching the trie for mappings, because the
first transition into the trie must use an edge with a
lexical item. Finally, ALNF improves the frequency
that, when a lexical rule matches a span, we have
successfully built every edge Sk`[X] in the mapping
for that rule. This frequency increases from 45% to
96% with ALNF.
5 Coarse-to-Fine Search
We now consider two coarse-to-fine approximate
search procedures for parsing with these grammars.
Our first approach clusters grammar symbols to-
gether during the coarse parsing pass, following
work in analytic parsing (Charniak and Caraballo,
1998; Petrov and Klein, 2007). We collapse all
intermediate non-terminal grammar symbols (e.g.,
NP) to a single coarse symbol X, while pre-terminal
symbols (e.g., NN) are hand-clustered into 7 classes
(nouns, verbals, adjectives, punctuation, etc.). We
then project the rules of the original grammar into
this simplified symbol set, weighting each rule of
the coarse grammar by the maximum weight of any
rule that mapped onto it.
In our second and more successful approach, we
select a subset of grammar symbols. We then in-
clude only and all rules that can be built using those
symbols. Because the grammar includes many rules
that are compositions of smaller rules, parsing with
a subset of the grammar still provides meaningful
scores that can be used to prune base grammar sym-
bols while parsing under the full grammar.
5.1 Symbol Selection
To compress the grammar, we select a small sub-
set of symbols that allow us to retain as much of
the original grammar as possible. We use a voting
scheme to select the symbol subset. After conver-
sion to LNF (or ALNF), each lexical rule in the orig-
inal grammar votes for the symbols that are required
to build it. A rule votes as many times as it was ob-
served in the training data to promote frequent rules.
We then select the top nl symbols by vote count and
include them in the coarse grammar C.
We would also like to retain as many non-lexical
rules from the original grammar as possible, but the
right-hand side of each rule can be binarized in many
ways. We again use voting, but this time each non-
232
Pruning Minutes Model score BLEU
No pruning 104 60,179 44.84
Clustering 79 60,179 44.84
Subsets 50 60,163 44.82
Table 2: Coarse-to-fine pruning speeds up parsing time
with minimal effect on either model score or translation
quality. The coarse grammar built using symbol subsets
outperforms clustering grammar symbols, reducing pars-
ing time by 52%. These experiments do not include a
language model.
lexical rule votes for its yield, a sequence of sym-
bols. We select the top nu symbol sequences as the
set R of right-hand sides.
Finally, we augment the symbol set of C with in-
termediate symbols that can construct all sequences
in R, using only binary rules. This step again re-
quires choosing a binarization for each sequence,
such that a minimal number of additional symbols is
introduced. We use the greedy approach from Sec-
tion 3.2. We then include in C all rules from the
original grammar that can be built from the symbols
we have chosen. Surprisingly, we are able to re-
tain 76% of the grammar rules while excluding 92%
of the grammar symbols2, which speeds up parsing
substantially.
5.2 Max Marginal Thresholding
We parse first with the coarse grammar to find the
Viterbi derivation score for each edge Sij [X]. We
then perform a Viterbi outside pass over the chart,
like a standard outside pass but replacing ? with
max (Goodman, 1999). The product of an edge?s
Viterbi score and its Viterbi outside score gives a
max marginal, the score of the maximal parse that
uses the edge.
We then prune away regions of the chart that de-
viate in their coarse max marginal from the global
Viterbi score by a fixed margin tuned on a develop-
ment set. Table 2 shows that both methods of con-
structing a coarse grammar are effective in pruning,
but selecting symbol subsets outperformed the more
typical clustering approach, reducing parsing time
by an additional factor of 2.
2We used nl of 500 and nu of 4000 for experiments. These
parameters were tuned on a development set.
6 Language Model Integration
Large n-gram language models (LMs) are critical
to the performance of machine translation systems.
Recent innovations have managed the complexity
of LM integration using multi-pass architectures.
Zhang and Gildea (2008) describes a coarse-to-fine
approach that iteratively increases the order of the
LM. Petrov et al (2008) describes an additional
coarse-to-fine hierarchy over language projections.
Both of these approaches integrate LMs via bottom-
up dynamic programs that employ beam search. As
an alternative, Huang and Chiang (2007) describes a
forest-based reranking algorithm called cube grow-
ing, which also employs beam search, but focuses
computation only where necessary in a top-down
pass through a parse forest.
In this section, we show that the coarse-to-fine
idea of constraining each pass using marginal pre-
dictions of the previous pass also applies effectively
to cube growing. Max marginal predictions from the
parse can substantially reduce LM integration time.
6.1 Language Model Forest Reranking
Parsing produces a forest of derivations, where each
edge in the forest holds its Viterbi (or one-best)
derivation under the transducer grammar. In forest
reranking via cube growing, edges in the forest pro-
duce k-best lists of derivations that are scored by
both the grammar and an n-gram language model.
Using ALNF, each edge must first generate a k-best
list of derivations that are not scored by the language
model. These derivations are then flattened to re-
move the binarization introduced by ALNF, so that
the resulting derivations are each rooted by an n-
ary rule r from the original grammar. The leaves of
r correspond to sub-edges in the chart, which are
recursively queried for their best language-model-
scored derivations. These sub-derivations are com-
bined by r, and new n-grams at the edges of these
derivations are scored by the language model.
The language-model-scored derivations for the
edge are placed on a priority queue. The top of
the priority queue is repeatedly removed, and its
successors added back on to the queue, until k
language-model-scored derivations have been dis-
covered. These k derivations are then sorted and
233
Pruning Max TM LM Total Inside Outside LM Total
strategy beam BLEU score score score time time time time
No pruning 20 57.67 58,570 -17,202 41,368 99 0 247 346
CTF parsing 200 58.43 58,495 -16,929 41,556 53 0 186 239
CTF reranking 200 58.63 58,582 -16,998 41,584 98 64 79 241
CTF parse + rerank 2000 58.90 58,602 -16,980 41,622 53 52 148 253
Table 3: Time in minutes and performance for 300 sentences. We used a trigram language model trained on 220
million words of English text. The no pruning baseline used a fix beam size for forest-based language model reranking.
Coarse-to-fine parsing included a coarse pruning pass using a symbol subset grammar. Coarse-to-fine reranking used
max marginals to constrain the reranking pass. Coarse-to-fine parse + rerank employed both of these approximations.
supplied to parent edges upon request.3
6.2 Coarse-to-Fine Parsing
Even with this efficient reranking algorithm, inte-
grating a language model substantially increased de-
coding time and memory use. As a baseline, we
reranked using a small fixed-size beam of 20 deriva-
tions at each edge. Larger beams exceeded the mem-
ory of our hardware. Results appear in Table 3.
Coarse-to-fine parsing before LM integration sub-
stantially improved language model reranking time.
By pruning the chart with max marginals from the
coarse symbol subset grammar from Section 5, we
were able to rerank with beams of length 200, lead-
ing to a 0.8 BLEU increase and a 31% reduction in
total decoding time.
6.3 Coarse-to-Fine Forest Reranking
We realized similar performance and speed bene-
fits by instead pruning with max marginals from the
full grammar. We found that LM reranking explored
many edges with low max marginals, but used few
of them in the final decoder output. Following the
coarse-to-fine paradigm, we restricted the reranker
to edges with a max marginal above a fixed thresh-
old. Furthermore, we varied the beam size of each
edge based on the parse. Let ?m be the ratio of
the max marginal for edge m to the global Viterbi
derivation for the sentence. We used a beam of size?
k ? 2ln?m? for each edge.
Computing max marginals under the full gram-
mar required an additional outside pass over the full
parse forest, adding substantially to parsing time.
3Huang and Chiang (2007) describes the cube growing al-
gorithm in further detail, including the precise form of the suc-
cessor function for derivations.
However, soft coarse-to-fine pruning based on these
max marginals also allowed for beams up to length
200, yielding a 1.0 BLEU increase over the baseline
and a 30% reduction in total decoding time.
We also combined the coarse-to-fine parsing ap-
proach with this soft coarse-to-fine reranker. Tiling
these approximate search methods allowed another
10-fold increase in beam size, further improving
BLEU while only slightly increasing decoding time.
7 Conclusion
As translation grammars increase in complexity
while innovations drive down the computational cost
of language model integration, the efficiency of the
parsing phase of machine translation decoding is be-
coming increasingly important. Our grammar nor-
mal form, CKY improvements, and symbol subset
coarse-to-fine procedure reduced parsing time for
large transducer grammars by 81%.
These techniques also improved forest-based lan-
guage model reranking. A full decoding pass with-
out any of our innovations required 511 minutes us-
ing only small beams. Coarse-to-fine pruning in
both the parsing and language model passes allowed
a 100-fold increase in beam size, giving a perfor-
mance improvement of 1.3 BLEU while decreasing
total decoding time by 50%.
Acknowledgements
This work was enabled by the Information Sci-
ences Institute Natural Language Group, primarily
through the invaluable assistance of Jens Voeckler,
and was supported by the National Science Founda-
tion (NSF) under grant IIS-0643742.
234
References
Eugene Charniak and Sharon Caraballo. 1998. New fig-
ures of merit for best-first probabilistic chart parsing.
In Computational Linguistics.
Eugene Charniak. 1997. Statistical techniques for natu-
ral language parsing. In National Conference on Arti-
ficial Intelligence.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In The Annual Con-
ference of the Association for Computational Linguis-
tics.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Hu-
man Language Technologies: The Annual Conference
of the North American Chapter of the Association for
Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In The An-
nual Conference of the Association for Computational
Linguistics.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
The Annual Conference of the Association for Compu-
tational Linguistics.
Dan Klein and Chris Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of the Association for
Computational Linguistics.
Adam Lopez. 2007. Hierarchical phrase-based transla-
tion with suffix arrays. In The Conference on Empiri-
cal Methods in Natural Language Processing.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In The Annual Conference
of the North American Chapter of the Association for
Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In The Annual Conference of
the Association for Computational Linguistics.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using lan-
guage projections. In The Conference on Empirical
Methods in Natural Language Processing.
Xinying Song, Shilin Ding, and Chin-Yew Lin. 2008.
Better binarization for the CKY parsing. In The Con-
ference on Empirical Methods in Natural Language
Processing.
Ashish Venugopal, Andreas Zollmann, and Stephan Vo-
gel. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In In Pro-
ceedings of the Human Language Technology and
North American Association for Computational Lin-
guistics Conference.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In The Annual Conference
of the Association for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?404.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free grammars.
In The Annual Conference of the Association for Com-
putational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In North American Chapter of the Associ-
ation for Computational Linguistics.
Andreas Zollmann, Ashish Venugopal, and Stephan Vo-
gel. 2006. Syntax augmented machine translation via
chart parsing. In The Statistical Machine Translation
Workshop at the North American Association for Com-
putational Linguistics Conference.
235
Coling 2008: Companion volume ? Posters and Demonstrations, pages 15?18
Manchester, August 2008
The power of negative thinking:
Exploiting label disagreement in the min-cut classification framework
Mohit Bansal
Dept. of Computer Science & Engineering
Indian Institute of Technology Kanpur
mbansal47@gmail.com
Claire Cardie and Lillian Lee
Dept. of Computer Science
Cornell University
{cardie,llee}@cs.cornell.edu
Abstract
Treating classification as seeking minimum
cuts in the appropriate graph has proven ef-
fective in a number of applications. The
power of this approach lies in its abil-
ity to incorporate label-agreement prefer-
ences among pairs of instances in a prov-
ably tractable way. Label disagreement
preferences are another potentially rich
source of information, but prior NLP work
within the minimum-cut paradigm has not
explicitly incorporated it. Here, we re-
port on work in progress that examines
several novel heuristics for incorporating
such information. Our results, produced
within the context of a politically-oriented
sentiment-classification task, demonstrate
that these heuristics allow for the addition
of label-disagreement information in a way
that improves classification accuracy while
preserving the efficiency guarantees of the
minimum-cut framework.
1 Introduction
Classification algorithms based on formulating the
classification task as one of finding minimum s-t
cuts in edge-weighted graphs ? henceforth min-
imum cuts or min cuts ? have been successfully
employed in vision, computational biology, and
natural language processing. Within NLP, appli-
cations include sentiment-analysis problems (Pang
and Lee, 2004; Agarwal and Bhattacharyya, 2005;
Thomas et al, 2006) and content selection for text
generation (Barzilay and Lapata, 2005).
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
As a classification framework, the minimum-
cut approach is quite attractive. First, it provides
a principled, yet flexible mechanism for allowing
problem-specific relational information ? includ-
ing several types of both hard and soft constraints
? to influence a collection of classification deci-
sions. Second, in many important cases, such as
when all the edge weights are non-negative, find-
ing a minimum cut can be done in a provably effi-
cient manner.
To date, however, researchers have restricted the
semantics of the constraints mentioned above to
encode pair-wise ?agreement? information only.
There is a computational reason for this restriction:
?agreement? and ?disagreement? information are
arguably most naturally expressed via positive and
negative edge weights, respectively; but in general,
the inclusion of even a relatively small number of
negative edge weights makes finding a minimum
cut NP-hard (McCormick et al, 2003).
To avoid this computational issue, we propose
several heuristics that encode disagreement infor-
mation with non-negative edge weights. We in-
stantiate our approach on a sentiment-polarity clas-
sification task ? determining whether individual
conversational turns in U.S. Congressional floor
debates support or oppose some given legislation.
Our preliminary results demonstrate promising im-
provements over the prior work of Thomas et al
(2006), who considered only the use of agreement
information in this domain.
2 Method
2.1 Min-cut classification framework
Binary classification problems are usually ap-
proached by considering each classification deci-
sion in isolation. More formally, let X
test
=
15
{x
1
, x
2
, . . . , x
n
} be the test instances, drawn from
some universe X , and let C = {c
1
, c
2
} be
the two possible classes. Then, the usual ap-
proach can often be framed as labeling each x
i
according to some individual-preference function
Ind:X ? C?<, such as the signed distance to
the dividing hyperplane according to an SVM or
the posterior class probability assigned by a Naive
Bayes classifier.
But when it is difficult to accurately classify a
particular x
i
in isolation, there is a key insight
that can help: knowing that x
i
has the same la-
bel as an easily-categorized x
j
makes labeling x
i
easy. Thus, suppose we also have an association-
preference function Assoc:X ?X?< express-
ing a reward for placing two items in the same
class; an example might be the output of an agree-
ment classifier or a similarity function. Then, we
can search for a classification function c(x
i
|X
test
)
? note that all of X
test
can affect an instance?s la-
bel ? that minimizes the total ?pining? of the test
items for the class they were not assigned to due to
either their individual or associational preferences:
?
i
Ind(x
i
, c(x
i
|X
test
)) +
??
?
i,j:c(x
i
|X
test
)=c(x
j
|X
test
)
Assoc(x
i
, x
j
),
where c(x
i
|X
test
) is the class ?opposite? to
c(x
i
|X
test
), and the free parameter ? regulates
the emphasis on agreement information. Solutions
to the above minimization problem correspond to
minimum s-t cuts in a certain graph, and if both
Ind and Assoc are non-negative functions, then,
surprisingly, minimum cuts can be found in poly-
nomial time; see Kleinberg and Tardos (2006, Sec-
tion 7.10) for details. But, as mentioned above,
allowing negative values makes finding a solution
intractable in the general case.
2.2 Prior work discards some negative values
The starting point for our work is Thomas et
al. (2006) (henceforth TPL). The reason for this
choice is that TPL used minimum-cut-based classi-
fication wherein signed distances to dividing SVM
hyperplanes were employed to define Ind(x, c)
and Assoc(x, x?). It was natural to use SVMs,
since association was determined by classification
rather than similarity ? specifically, categorizing
references by one congressperson to another as re-
flecting agreement or not ? but as a result, neg-
ative association-preferences (e.g., negative dis-
tance to a hyperplane) had to be accounted for.
We formalize TPL?s approach at a high
level as follows. Let Ind?:X ? C?< and
Assoc
?
:X ?X?< be initial individual- and
association-preference functions, such as the
signed distances mentioned above. TPL create two
non-negative conversion functions f :<? [0, 1]
and g:<? [0, 1], and then define
Ind(x
i
, c) := f(Ind
?
(x
i
, c))
Assoc(x
i
, x
j
) := g(Assoc
?
(x
i
, x
j
))
so that an optimal classification can be found in
polynomial time, as discussed above. We omit the
exact definitions of f and g in order to focus on
what is important here: roughly speaking, f and
g normalize values and handle outliers1, with the
following crucial exception. While negative initial
individual preferences for one class can be trans-
lated into positive individual preferences for the
other, there is no such mechanism for negative val-
ues of Assoc?; so TPL resort to defining g to be
0 for negative arguments. They thus discard po-
tentially key information regarding the strength of
label disagreement preferences.
2.3 Encoding negative associations
Instead of discarding the potentially crucial label-
disagreement information represented by negative
Assoc
? values, we propose heuristics that seek to
incorporate this valuable information, but that keep
Ind and Assoc non-negative (by piggy-backing off
of TPL?s pre-existing conversion-function strat-
egy2) to preserve the min-cut-classification effi-
ciency guarantees.
We illustrate our heuristics with a running
example. Consider a simplified setting with only
two instances x
1
and x
2
; f(z) = z; g(z) = 0 if
z < 0, 1 otherwise; and Ind? values (numbers
labeling left or right arrows in the diagrams below,
e.g., Ind?(x
1
, c
1
) = .7) and Assoc? value (the -2
labeling the up-and-down arrow) as depicted here:
? [.7]? x
1
?[.3]?
c
1
m [?2] c
2
? [.6]? x
2
?[.4]?
Then, the resulting TPL Ind and Assoc values are
1Thus, strictly speaking, f and g also depend on Ind?,
Assoc
?
, and X
test
, but we suppress this dependence for nota-
tional compactness.
2Our approach also applies to definitions of f and g dif-
ferent from TPL?s.
16
? [.7]? x
1
?[.3]?
c
1
m [0] c
2
? [.6]? x
2
?[.4]?
Note that since the initial -2 association value is
ignored, c(x
1
|X
test
) = c(x
2
|X
test
) = c
1
appears
to be a good classification according to TPL.
The Scale all up heuristic Rather than discard
disagreement information, a simple strategy is to
just scale up all initial association preferences by a
sufficiently large positive constant N :
Ind(x
i
, c) := f(Ind
?
(x
i
, c))
Assoc(x
i
, x
j
) := g(Assoc
?
(x
i
, x
j
) + N)
For N = 3 in our example, we get
? [.7]? x
1
?[.3]?
c
1
m [1] c
2
? [.6]? x
2
?[.4]?
This heuristic ensures that the more negative the
Assoc
? value, the lower the cost of separating the
relevant item pair (whereas TPL don?t distinguish
between negative Assoc? values). The heuristic
below tries to be more proactive, forcing such
pairs to receive different labels.
The SetTo heuristic We proceed through
x
1
, x
2
, . . . in order. Each time we encounter
an x
i
where Assoc?(x
i
, x
j
) < 0 for some
j > i, we try to force x
i
and x
j
into dif-
ferent classes by altering the four relevant
individual-preferences affecting this pair of in-
stances, namely, f(Ind?(x
i
, c
1
)), f(Ind
?
(x
i
, c
2
)),
f(Ind
?
(x
j
, c
1
)), and f(Ind?(x
j
, c
2
)). Assume
without loss of generality that the largest of
these values is the first one. If we respect
that preference to put x
i
in c
1
, then according
to the association-preference information, it
follows that we should put x
j
in c
2
. We can
instantiate this chain of reasoning by setting
Ind(x
i
, c
1
) := max(?, f(Ind
?
(x
i
, c
1
)))
Ind(x
i
, c
2
) := min(1? ?, f(Ind
?
(x
i
, c
2
)))
Ind(x
j
, c
1
) := min(1? ?, f(Ind
?
(x
j
, c
1
)))
Ind(x
j
, c
2
) := max(?, f(Ind
?
(x
j
, c
2
)))
for some constant ? ? (.5, 1], and making no
change to TPL?s definition of Assoc. For ? = .8
in our example, we get
? [.8]? x
1
?[.2]?
c
1
m [0] c
2
? [.2]? x
2
?[.8]?
Note that as we proceed through x
1
, x
2
, . . . in
order, some earlier changes may be undone.
The IncBy heuristic A more conservative ver-
sion of the above heuristic is to increment and
decrement the individual-preference values so that
they are somewhat preserved, rather than com-
pletely replace them with fixed constants:
Ind(x
i
, c
1
) := min(1, f(Ind
?
(x
i
, c
1
)) + ?)
Ind(x
i
, c
2
) := max(0, f(Ind
?
(x
i
, c
2
))? ?)
Ind(x
j
, c
1
) := max(0, f(Ind
?
(x
j
, c
1
))? ?)
Ind(x
j
, c
2
) := min(1, f(Ind
?
(x
j
, c
2
)) + ?)
For ? = .1, our example becomes
? [.8]? x
1
?[.2]?
c
1
m [0] c
2
? [.5]? x
2
?[.5]?
3 Evaluation
For evaluation, we adopt the sentiment-
classification problem tackled by TPL: clas-
sifying speech segments (individual conversational
turns) in a U.S. Congressional floor debate as
to whether they support or oppose the legis-
lation under discussion. TPL describe many
reasons why this is an important problem. For
our purposes, this task is also very convenient
because all of TPL?s computed raw and converted
Ind
? and Assoc? data are publicly available at
www.cs.cornell.edu/home/llee/data/convote.html.
Thus, we used their calculated values to imple-
ment our algorithms as well as to reproduce their
original results.3
One issue of note is that TPL actually in-
ferred association preferences between speakers,
not speech segments. We do the same when ap-
plying SetTo or IncBy to a pair {x
i
, x
j
} by con-
sidering the average of f(Ind?(x
k
, c
1
)) over all
x
k
uttered by the speaker of x
i
, instead of just
f(Ind
?
(x
i
, c
1
)). The other three relevant individ-
ual values are treated similarly. We also make
appropriate modifications (according to SetTo and
IncBy) to the individual preferences of all such x
k
simultaneously, not just x
i
, and similarly for x
j
.
A related issue is that TPL assume that all
speech segments by the same speaker should have
the same label. To make experimental compar-
isons meaningful, we follow TPL in considering
two different instantiations of this assumption. In
segment-based classification, Assoc(x
i
, x
j
) is set
to an arbitrarily high positive constant if the same
speaker uttered both x
i
and x
j
. In speaker-based
classification, Ind?(x
i
, c) is produced by running
3For brevity, we omit TPL?s ?high-threshold? variants.
17
 60
 62
 64
 66
 68
 70
 72
 74
 76
 78
SetTo(.6)SVM SetTo(1) IncBy(.25)IncBy(.15)TPL IncBy(.05)Scale all up SetTo(.8)
pe
rc
en
t c
or
re
ct
ALGORITHMS
Test-set classification accuracies, using held-out parameter estimation
segment-based, test
speaker-based, test
best TPL, test
Figure 1: Experimental results. ?SVM?: classification using only individual-preference information.
Values of ? are indicated in parentheses next to the relevant algorithm names.
an SVM on the concatenation of all speeches ut-
tered by x
i
?s speaker.
Space limits preclude inclusion of further de-
tails; please see TPL for more information.
3.1 Results and future plans
The association-emphasis parameter ? was trained
on held-out data, with ties broken in favor of the
largest ? in order to emphasize association in-
formation. We used Andrew Goldberg?s HIPR
code (http://avglab.com/andrew/soft.html) to com-
pute minimum cuts. The resultant test-set classifi-
cation accuracies are presented in Figure 1.
We see that Scale all up performs worse
than TPL, but the more proactive heuristics
(SetTo, IncBy) almost always outperform TPL on
segment-based classification, sometimes substan-
tially so, and outperform TPL on speaker-based
classification for half of the variations. We there-
fore conclude that label disagreement informa-
tion is indeed valuable; and that incorporating la-
bel disagreement information on top of the (posi-
tive) label agreement information that TPL lever-
aged can be achieved using simple heuristics; and
that good performance enhancements result with-
out any concomitant significant loss of efficiency.
These results are preliminary, and the diver-
gence in behaviors between different heuristics
in different settings requires investigation. Ad-
ditional future work includes investigating more
sophisticated (but often therefore less tractable)
formalisms for joint classification; and looking
at whether approximation algorithms for finding
minimum cuts in graphs with negative edge capac-
ities can be effective.
Acknowledgments We thank Jon Kleinberg and the
reviewers for helpful comments. Portions of this work
were done while the first author was visiting Cornell Uni-
versity. This paper is based upon work supported in part
by the National Science Foundation under grant nos. IIS-
0329064, BCS-0624277, and IIS-0535099, a Cornell Univer-
sity Provost?s Award for Distinguished Scholarship, a Yahoo!
Research Alliance gift, an Alfred P. Sloan Research Fellow-
ship, and by DHS grant N0014-07-1-0152. Any opinions,
findings, and conclusions or recommendations expressed are
those of the authors and do not necessarily reflect the views
or official policies, either expressed or implied, of any spon-
soring institutions, the U.S. government, or any other entity.
References
A. Agarwal, P. Bhattacharyya. 2005. Sentiment analysis: A
new approach for effective use of linguistic knowledge and
exploiting similarities in a set of documents to be classi-
fied. ICON.
R. Barzilay, M. Lapata. 2005. Collective content selection for
concept-to-text generation. HLT/EMNLP, pp. 331?338.
J. Kleinberg, ?E. Tardos. 2006. Algorithm Design. Addison
Wesley.
S. T. McCormick, M. R. Rao, G. Rinaldi. 2003. Easy and dif-
ficult objective functions for max cut. Mathematical Pro-
gramming, Series B(94):459?466.
B. Pang, L. Lee. 2004. A sentimental education: Sentiment
analysis using subjectivity summarization based on mini-
mum cuts. ACL, pp. 271?278.
M. Thomas, B. Pang, L. Lee. 2006. Get out the vote: De-
termining support or opposition from Congressional floor-
debate transcripts. EMNLP, pp. 327?335.
18
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1329?1341,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Weakly-Supervised Learning with
Cost-Augmented Contrastive Estimation
Kevin Gimpel Mohit Bansal
Toyota Technological Institute at Chicago, IL 60637, USA
{kgimpel,mbansal}@ttic.edu
Abstract
We generalize contrastive estimation in
two ways that permit adding more knowl-
edge to unsupervised learning. The first
allows the modeler to specify not only the
set of corrupted inputs for each observa-
tion, but also how bad each one is. The
second allows specifying structural prefer-
ences on the latent variable used to explain
the observations. They require setting ad-
ditional hyperparameters, which can be
problematic in unsupervised learning, so
we investigate new methods for unsuper-
vised model selection and system com-
bination. We instantiate these ideas for
part-of-speech induction without tag dic-
tionaries, improving over contrastive esti-
mation as well as strong benchmarks from
the PASCAL 2012 shared task.
1 Introduction
Unsupervised NLP aims to discover useful struc-
ture in unannotated text. This structure might
be part-of-speech (POS) tag sequences (Merialdo,
1994), morphological segmentation (Creutz and
Lagus, 2005), or syntactic structure (Klein and
Manning, 2004), among others. Unsupervised
systems typically improve when researchers incor-
porate knowledge to bias learning to capture char-
acteristics of the desired structure.
1
There are many successful examples of adding
knowledge to improve learning without labeled
examples, including: sparsity in POS tag distri-
butions (Johnson, 2007; Ravi and Knight, 2009;
Ganchev et al., 2010), short attachments for
dependency parsing (Smith and Eisner, 2006),
1
We note that doing so strains the definition of the term
unsupervised. Hence we will use the term weakly-supervised
to refer to methods that do not explicitly train on labeled ex-
amples for the task of interest, but do use some form of task-
specific knowledge.
agreement of word alignment models (Liang et
al., 2006), power law effects in lexical distribu-
tions (Blunsom and Cohn, 2010; Blunsom and
Cohn, 2011), multilingual constraints (Smith and
Eisner, 2009; Ganchev et al., 2009; Snyder et al.,
2009; Das and Petrov, 2011), and orthographic
cues (Spitkovsky et al., 2010c; Spitkovsky et al.,
2011b), inter alia.
Contrastive estimation (CE; Smith and Eisner,
2005) is a general approach to weakly-supervised
learning with a particular way of incorporating
knowledge. CE increases the likelihood of the ob-
servations at the expense of those in a particular
neighborhood of each observation. The neighbor-
hood typically contains corrupted versions of the
observations. The latent structure is marginalized
out for both the observations and their corruptions;
the intent is to learn latent structure that helps to
explain why the observation was generated rather
than any of the corrupted alternatives.
In this paper, we present a new objective func-
tion for weakly-supervised learning that general-
izes CE by including two types of cost functions,
one on observations and one on output structures.
The first (?4) allows us to specify not only the set
of corrupted observations, but also how bad each
corruption was. We use n-gram language models
to measure the severity of each corruption.
The second (?5) allows us to specify prefer-
ences on desired output structures, regardless of
the input sentence. For POS tagging, we attempt
to learn language-independent tag frequencies by
computing counts from treebanks for 11 languages
not used in our POS induction experiments. For
example, we encourage tag sequences that contain
adjacent nouns and penalize those that contain ad-
jacent adpositions.
We consider several unsupervised ways to set
hyperparameters for these cost functions (?7), in-
cluding the recently-proposed log-likelihood esti-
mator of Bengio et al. (2013). We also circumvent
1329
hyperparameter selection via system combination,
developing a novel voting scheme for POS induc-
tion that aligns tag identifiers across runs.
We evaluate our approach, which we call cost-
augmented contrastive estimation (CCE), on
POS induction without tag dictionaries for five
languages from the PASCAL shared task (Gelling
et al., 2012). We find that CCE improves over both
standard CE as well as strong baselines from the
shared task. In particular, our final average accu-
racies are better than all entries in the shared task
that use the same number of tags.
2 Related Work
Weakly-supervised techniques can be roughly cat-
egorized in terms of whether they influence the
model, the learning procedure, or explicitly target
the output structure. Examples abound in NLP;
we focus on those that have been applied to POS
tagging.
There have been many efforts at biasing
models, including features (Smith and Eisner,
2005a; Berg-Kirkpatrick et al., 2010), sparse
priors (Johnson, 2007; Goldwater and Griffiths,
2007; Toutanova and Johnson, 2007), sparsity
in tag transition distributions (Ravi and Knight,
2009), small models via minimum description
length criteria (Vaswani et al., 2010; Poon et al.,
2009), a one-tag-per-type constraint (Blunsom and
Cohn, 2011), and power law effects via Bayesian
nonparametrics (Van Gael et al., 2009; Blunsom
and Cohn, 2010; Blunsom and Cohn, 2011).
We focus below on efforts that induce bias into
the learning (?2.1) or more directly in the output
structure (?2.2), as they are more closely related
to our contributions in this paper.
2.1 Biasing Learning
Some unsupervised methods do not change the
model or attempt to impose structural bias; rather,
they change the learning. This may involve op-
timizing a different objective function for the
same model, e.g., by switching from soft to hard
EM (Spitkovsky et al., 2010b). Or it may in-
volve changing the objective during learning via
annealing (Smith and Eisner, 2004) or more gen-
eral multi-objective techniques (Spitkovsky et al.,
2011a; Spitkovsky et al., 2013).
Other learning modifications relate to automatic
data selection, e.g., choosing examples for genera-
tive learning (Spitkovsky et al., 2010a) or automat-
ically generating negative examples for discrimi-
native unsupervised learning (Li et al., 2010; Xiao
et al., 2011).
CE does both, automatically generating nega-
tive examples and changing the objective function
to include them. Our observation cost function al-
ters CE?s objective function, sharpening the effec-
tive distribution of the negative examples.
2.2 Structural Bias
Our output cost function is used to directly spec-
ify preferences on desired output structures. Sev-
eral others have had similar aims. For dependency
grammar induction, Smith and Eisner (2006) fa-
vored short attachments using a fixed-weight fea-
ture whose weight was optionally annealed during
learning. Their bias could be implemented as an
output cost function in our framework.
Posterior regularization (PR; Ganchev et al.,
2010) is a general framework for declaratively
specifying preferences on model outputs. Naseem
et al. (2010) proposed universal syntactic rules for
unsupervised dependency parsing and used them
in a PR regime; we use analogous universal tag
sequences in our cost function.
Our output cost is similar to posterior regular-
ization. The difference is that we specify pref-
erences via an arbitrary cost function on output
structures, while PR uses expectation constraints
on posteriors of the model. We compare to the PR
tag induction system of Grac?a et al. (2011) in our
experiments, improving over it in several settings.
2.3 Exploiting Resources
Much of the work mentioned above also benefits
from leveraging existing resources. These may be
curated or crowdsourced resources like the Wik-
tionary (Li et al., 2012), or traditional annotated
treebanks for languages other than those under in-
vestigation (Cohen et al., 2011). In this paper, we
use tag statistics from treebanks for 11 languages
to impose our structural bias for a different set of
languages used in our POS induction experiments.
Substantial recent work has improved many
NLP tasks by leveraging multilingual or paral-
lel text (Cohen and Smith, 2009; Snyder et al.,
2009; Wang and Manning, 2014), including un-
supervised POS tagging (Naseem et al., 2009; Das
and Petrov, 2011; T?ackstr?om et al., 2013; Ganchev
and Das, 2013). This sort of multilingual guidance
could also be captured by particular output cost
functions, though we leave this to future work.
1330
3 Unsupervised Structure Learning
We consider a structured unsupervised learning
setting. We use X to denote our set of possible
structured inputs, and for a particular x ? X,
we use Y(x) to denote the set of valid structured
outputs for x. We are given a dataset of inputs
{x
(i)
}
N
i=1
. To map inputs to outputs, we start by
building a model of the joint probability distribu-
tion p?(x,y). We use a log-linear parameteriza-
tion with feature vector f and weight vector ?:
p?(x,y) =
exp
{
?
>
f(x,y)
}
?
x??X,y??Y(x?) exp
{
?
>
f(x
?
,y
?
)
}
where the sum in the denominator ranges over all
possible inputs and all valid outputs for them.
In this paper, we consider ways of learning the
parameters ?. Given ?, at test time we output a y
for a new x using, e.g., Viterbi or minimum Bayes
risk decoding; we use the latter in this paper.
3.1 EM and Contrastive Estimation
We start by reviewing two ways of choosing
?. The expectation-maximization algorithm (EM;
Dempster et al., 1977) finds a local optimum of
the marginal (log-)likelihood of the observations
{x
(i)
}
N
i=1
. The marginal log-likelihood is a sum
over all x
(i)
of the gain function ?
EM
(x
(i)
):
?
EM
(x
(i)
) = log
?
y?Y(x(i))
p?(x
(i)
,y)
= log
?
y?Y(x(i))
exp
{
?
>
f(x
(i)
,y)
}
? log
?
x??X,y??Y(x?)
exp
{
?
>
f(x
?
,y
?
)
}
? ?? ?
Z(?)
The difficulty is the final term, logZ(?), which
requires summing over all possible inputs and
all valid outputs for them. This summation is
typically intractable for structured problems, and
may even diverge. For this reason, EM is typi-
cally only used to train log-linear model weights
when Z(?) = 1, e.g., for hidden Markov models,
probabilistic context-free grammars, and models
composed of locally-normalized log-linear mod-
els (Berg-Kirkpatrick et al., 2010), among others.
There have been efforts at approximating the
summation over elements of X, whether by limit-
ing sequence length (Haghighi and Klein, 2006),
only summing over observations in the training
data (Riezler, 1999), restricting the observation
space based on the task (Dyer et al., 2011), or us-
ing Gibbs sampling to obtain an unbiased sample
of the full space (Della Pietra et al., 1997; Rosen-
feld, 1997).
Contrastive estimation (CE) addresses this chal-
lenge by using a neighborhood function N : X?
2
X
that generates a set of inputs that are ?corrup-
tions? of an input x; N(x) always includes x. Us-
ing shorthand N
i
for N(x
(i)
), CE corresponds to
maximizing the sum over inputs x
(i)
of the gain
?
CE
(x
(i)
)= log Pr(x
(i)
| N
i
)
= log
?
y?Y(x(i)) p?(x
(i)
,y)
?
x??N
i
?
y??Y(x?) p?(x
?
,y
?
)
= log
?
y?Y(x(i))
exp
{
?
>
f(x
(i)
,y)
}
?
log
?
x??N
i
?
y??Y(x?)
exp
{
?
>
f(x
?
,y
?
)
}
Two logZ(?) terms cancel out, leaving the sum-
mation over input/output pairs in the neighbor-
hood instead of the full summation over pairs.
Two desiderata govern the choice of N. One is
to make the summation over its elements computa-
tionally tractable. If N(x) = X for all x ? X, we
obtain EM, so a smaller neighborhood typically
must be used in practice. The second considera-
tion is to target learning for the task of interest. For
POS tagging and dependency parsing, Smith and
Eisner (2005a, 2005b) used neighborhood func-
tions that corrupted the observations in systematic
ways, e.g., their TRANS1 neighborhood contains
the original sentence along with those that result
from transposing a single pair of adjacent words.
The intent was to force the learner to explain why
the given sentences were observed at the expense
of the corrupted sentences.
Next we present our modifications to con-
trastive estimation. Both can be viewed as adding
specialized cost functions that penalize some part
of the structured input/output pair.
4 Modeling Corruption Costs
While CE allows us to specify a set of corrupted
x for each x
(i)
via the neighborhood function N,
it says nothing about how bad each corruption is.
The same type of corruption might be harmful in
one context and not harmful in another.
This fact was suggested as the reason why cer-
tain neighborhoods did not work as well for POS
1331
tagging as others (Smith and Eisner, 2005a). One
poorly-performing neighborhood consisted of sen-
tences in which a single word of the original
was deleted. Deleting a single word in a sen-
tence might not harm grammaticality. By contrast,
neighborhoods that transpose adjacent words led
to better results. These kinds of corruptions are ex-
pected to be more frequently harmful, at least for
languages with relatively rigid word order. How-
ever, there may still be certain transpositions that
are benign, at least for grammaticality.
To address this, we introduce an observation
cost function ? : X ? X ? R
?0
that indicates
how much two observations differ. Using ?, we
define the following gain function ?
CCE
1
(x
(i)
) =
log
?
y?Y(x(i))
exp
{
?
>
f(x
(i)
,y)
}
?
log
?
x??N
i
?
y??Y(x?)
exp
{
?
>
f(x
?
,y
?
) + ?(x
(i)
,x
?
)
}
The function ? inflates the score of neighbor-
hood entries with larger differences from the ob-
served x
(i)
. This gain function is inspired by ideas
from structured large-margin learning (Taskar et
al., 2003; Tsochantaridis et al., 2005), specifi-
cally softmax-margin (Povey et al., 2008; Gimpel
and Smith, 2010). Softmax-margin extends con-
ditional likelihood by allowing the user to specify
a cost function to give partial credit for structures
that are partially correct. Conditional likelihood,
by contrast, treats all incorrect structures equally.
While softmax-margin uses a cost function to
specify how two output structures differ, our gain
function ?
CCE
1
uses a cost function ? to specify
how two inputs differ. But the motivations are sim-
ilar: since poor structures have their scores artifi-
cially inflated by ?, learning pays more attention
to them, choosing weights that penalize them more
than the lower-cost structures.
4.1 Observation Cost Functions
What types of cost functions should we consider?
For efficient inference, we want to ensure that
? decomposes additively across parts of the cor-
rupted input x
?
in the same way as the features; we
assume unigram and bigram features in this paper.
In addition, the choice of the observation cost
function ? is tied to the choice of neighborhood
function. In our experiments, we use neighbor-
hoods that change the order of words in the obser-
vation but not the set of words. Our first cost func-
tion simply counts the number of novel bigrams
introduced when corrupting the original:
?
I
(x
(i)
,x) = ?
|x|+1
?
j=1
I
[
x
j?1
x
j
/? 2grams(x
(i)
)
]
where x
j
is the jth word of sentence x, x
0
is
the start-of-sentence marker, x
|x|+1 is the end-of-
sentence marker, 2grams(x) returns the set of bi-
grams in x, I[] returns 1 if its argument is true and
0 otherwise, and ? is a constant to be tuned. We
call this cost function MATCH. Only x
(i)
(which
is always contained in N
i
) is guaranteed to have
cost 0. In the TRANS1 neighborhood, corrupted
sequences will be penalized more if their transpo-
sitions occur in the middle of the sentence rather
than at the beginning or end.
We also consider a version that weights the in-
dicator by the negative log probability of the novel
bigram: ?
LM
(x
(i)
,x) =
?
|x|+1
?
j=1
?log P(x
j
|x
j?1
)I
[
x
j?1
x
j
/? 2grams(x
(i)
)
]
where P(x
j
|x
j?1
) is obtained from a bigram lan-
guage model. Among novel bigrams in the cor-
ruption x, if the second word is highly surprising
conditioned on the first, the bigram will incur high
cost. We refer to ?
LM
(x
(i)
,x) as MATLM.
5 Expressing Structural Preferences
Our second modification to CE allows us to spec-
ify structural preferences for outputs y. We first
note that there exist objective functions for su-
pervised structure prediction that never require
computing the feature vector for the true output
y
(i)
. Examples include Bayes risk (Kaiser et al.,
2000; Povey and Woodland, 2002) and structured
ramp loss (Do et al., 2008). These two objec-
tives do, however, need to compute a cost func-
tion cost(y
(i)
,y), which requires the true output
y
(i)
. We start with the following form of struc-
tured ramp loss from Gimpel and Smith (2012),
transformed here to a gain function:
max
y?Y(x(i))
(
?
>
f(x
(i)
,y)? cost(y
(i)
,y)
)
?
max
y??Y(x(i))
(
?
>
f(x
(i)
,y
?
) + cost(y
(i)
,y
?
)
)
(1)
Maximizing this gain function for supervised
learning corresponds to increasing the model score
1332
of outputs that have both high model score (?
>
f )
and low cost, while decreasing the model score of
outputs with high model score and high cost.
For unsupervised learning, we do not have y
(i)
,
so we simply drop y
(i)
from the cost function. The
result is an output cost function pi : Y ? R
?0
which captures our a priori knowledge about de-
sired output structures. The value of pi(y) should
be large for outputs y that are far from the ideal.
In this paper, we consider POS induction and use
intrinsic evaluation; however, in a real-world sce-
nario, the output cost function could use signals
derived from the downstream task in which the
tags are being used.
Given pi, we convert each max to a log
?
exp in
Eq. 1 and introduce the contrastive neighborhood
into the second term, defining our new gain func-
tion ?
CCE
2
(x
(i)
) =
log
?
y?Y(x(i))
exp
{
?
>
f(x
(i)
,y)? pi(y)
}
?
log
?
x??N
i
?
y??Y(x?)
exp
{
?
>
f(x
?
,y
?
) + pi(y
?
)
}
Gimpel (2012) found that using such ?softened?
versions of the ramp losses worked better than the
original versions (e.g., Eq. 1) when training ma-
chine translation systems.
5.1 Output Cost Functions
The output cost pi should capture our desider-
ata about y for the task of interest. We con-
sider universal POS tag subsequences analogous
to the universal syntactic rules of Naseem et al.
(2010). In doing so, we use the universal tags of
Petrov et al. (2012): NOUN, VERB, ADJ (ad-
jective), ADV (adverb), PRON (pronoun), DET
(determiner), ADP (pre/postposition), NUM (nu-
meral), CONJ (conjunction), PRT (particle), ?.?
(punctuation), and X (other).
We aimed for a set of rules that would be ro-
bust across languages. So, we used treebanks for
11 languages from the CoNLL 2006/2007 shared
tasks (Buchholz and Marsi, 2006; Nivre et al.,
2007) other than those used in our POS induc-
tion experiments. In particular, we used Arabic,
Bulgarian, Catalan, Czech, English, Spanish, Ger-
man, Hungarian, Italian, Japanese, and Turkish.
We replicated shorter treebanks a sufficient num-
ber of times until they were a similar size as the
largest treebank. Then we counted gold POS tag
unigrams and bigrams from the concatenation.
tag unigram count cost
X 50783 3.83
NUM 174613 2.59
PRT 179131 2.57
ADV 330210 1.96
CONJ 436649 1.68
PRON 461880 1.62
DET 615284 1.33
ADJ 694685 1.21
ADP 906922 0.95
VERB 1018989 0.83
. 1042662 0.81
NOUN 2337234 0
tag bigram count cost
DET PRT 109 84.41
DET CONJ 518 68.82
NUM ADV 1587 57.63
NOUN NOUN 409828 2.09
DET NOUN 454980 1.04
NOUN . 504897 0
Table 1: Counts and costs for universal tags based
on treebanks for 11 languages not used in POS in-
duction experiments.
Where #(y) is the count of tag y in the treebank
concatenation, the cost of y is
u(y) = log
(
max
y
?
#(y
?
)
#(y)
)
and, where #(?y
1
, y
2
?) is the count of tag bigram
?y
1
, y
2
?, the cost of ?y
1
, y
2
? is
u(?y
1
, y
2
?) = 10?log
(
max
?y
?
1
,y
?
2
?
#(?y
?
1
, y
?
2
?)
#(?y
1
, y
2
?)
)
We use a multiplier of 10 in order to exaggerate
count differences among bigrams, which gener-
ally are closer together than unigram counts. In
Table 1, we show counts and costs for all tag uni-
grams and selected tag bigrams.
2
Given these costs for individual tag unigrams
and bigrams, we use the following pi function,
which we call UNIV:
pi(y) = ?
|y|+1
?
j=1
u(y
j
) + u(?y
j?1
, y
j
?)
where ? is a constant to be tuned and y
j
is the
jth tag of y. We define y
0
to be the beginning-
of-sentence marker and y
|y|+1 to be the end-of-
sentence marker (which has unigram cost 0).
Many POS induction systems use one-tag-
per-type constraints (Blunsom and Cohn, 2011;
Gelling et al., 2012), which often lead to higher
2
The complete tag bigram list is provided in the supple-
mentary material.
1333
max
?
N?
i=1
log
?
y?Y(x(i))
exp
{
?>f(x(i),y)
}
? log
?
x??N
i
?
y??Y(x?)
exp
{
?>f(x?,y?)
}
(2)
max
?
N?
i=1
log
?
y?Y(x(i))
exp
{
?>f(x(i),y)? pi(y)
}
? log
?
x??N
i
?
y??Y(x?)
exp
{
?>f(x?,y?) + ?(x(i),x?) + pi(y?)
}
(3)
Figure 1: Contrastive estimation (Eq. 2) and cost-augmented contrastive estimation (Eq. 3). L2 regular-
ization terms (
C
2
?
|?|
j=1
?
2
j
) are not shown here but were used in our experiments.
accuracies even though the gold standard is not
constrained in this way. This constraint can be en-
coded as an output cost function, though it would
require approximate inference (Poon et al., 2009).
6 Cost-Augmented CE
We extended the objective function underlying
CE by defining two new types of cost functions,
one on observations (?4) and one on outputs (?5).
We combine them into a single objective, which
we call cost-augmented contrastive estimation
(CCE), shown as Eq. 3 in Figure 1.
If the cost functions ? and pi factor in the same
way as the features f , then it is straightforward
to implement CCE atop an existing CE implemen-
tation. The additional terms in the cost functions
can be implemented as features with fixed weights
(albeit where the weight differs depending on the
context).
7 Model Selection
Our modifications give increased flexibility, but
require setting new hyperparameters. In addition
to the choice of the cost functions, each has a
weight: ? for ? and ? for pi. We need ways to
set these weights that do not require labeled data.
Smith and Eisner (2005a) chose the hyperpa-
rameter values that yielded the best CE objec-
tive on held-out development data. We use their
strategy, though we experiment with two others as
well.
3
In particular, we estimate held-out data log-
likelihood via the method of Bengio et al. (2013)
and also consider ways of combining outputs from
multiple models.
7.1 Estimating Held-Out Log-Likelihood
Bengio et al. (2013) recently proposed ways to
efficiently estimate held-out data log-likelihood
3
When using their strategy for CCE, we compute the CE
criterion only, omitting the costs. We do so because the
weights of the cost terms can have a large impact on the mag-
nitude of the objective, making it difficult to do a fair com-
parison of models with different cost weights.
for generative models. They showed empirically
that a simple, biased version of their conserva-
tive sampling-based log-likelihood (CSL) estima-
tor can be useful for model selection.
The biased CSL requires a Markov chain on the
variables in the model (i.e., x and y) as well as
the ability to compute p?(x|y). It generates con-
secutive samples of y from a Markov chain ini-
tialized at each x in a development set D, with
S Markov chains run for each x. We compute
and sum p?(x|y
j
) for each sampled y
j
, then sum
over all x in D. The result is a biased estimate for
the log-likelihood of D. Bengio et al. showed that
these biased estimates could give the same model
ranking as unbiased estimates, though more effi-
ciently. They also showed that taking the single,
initial sample from the S Markov chains resulted
in the same model ranking as using many samples
from each chain. We follow suit here.
Our Markov chain is a blocked Gibbs sam-
pler in which we alternate between sampling from
p?(y|x) and p?(x|y). Since we only use a sin-
gle sample from each Markov chain and initialize
each chain to x, this simply amounts to drawing S
samples from p?(y|x). To sample from p?(y|x),
we use the exact algorithm obtained by running
the backward algorithm and then performing left-
to-right sampling of tags using the local features
and requisite backward terms to define the local
tag distributions.
We then compute p?(x|y) for each sampled y.
If there are no features in f that look at more than
one word (which is the case with the features used
in our experiments), then this probability factors:
p?(x|y) =
?
|y|
k=1
p?(xk|yk)
This is easily computable assuming that we have
normalization constants Z(y) cached for each tag
y. To compute each Z(y), we sum over all words
observed in the training data (replacing some with
a special UNK token; see below). We can then
compute likelihoods for individual words and mul-
1334
tiply them across the words in the sentence to com-
pute p?(x|y).
To summarize, we get a log-likelihood estimate
for development setD = {x
(i)
}
|D|
i=1
by sampling S
times from p?(y|x
(i)
) for each x
(i)
, getting sam-
ples {{y
(i),j
}
S
j=1
}
|D|
i=1
, then we compute
?
|D|
i=1
?
S
j=1
log p?(x
(i)
|y
(i),j
)
We used values of S ? {1, 10, 100}, finding that
the ranking of models was consistent across S val-
ues. We used S = 10 in all results reported below.
We note that this estimator was originally pre-
sented for generative models, and that (C)CE is
not a generative training criterion. It seeks to max-
imize the conditional probability of an observation
given its neighborhood. Nonetheless, when imple-
menting our log-likelihood estimator, we treat the
model as a generative model, computing the Z(y)
constants by summing over all words in the vocab-
ulary.
7.2 System Combination
We can avoid choosing a single model by com-
bining the outputs of multiple models via system
combination. We decode test data by using poste-
rior decoding. To combine the outputs of multiple
models, we find the max-posterior tag under each
model, then choose the highest vote-getter, break-
ing ties arbitrarily.
However, when doing POS induction without a
tag dictionary, the tags are simply unique identi-
fiers and may not have consistent meaning across
runs. To address this, we propose a novel voting
scheme that is inspired by the widely-used 1-to-1
accuracy metric for POS induction (Haghighi and
Klein, 2006). This metric maps system tags to
gold tags to maximize accuracy with the constraint
that each gold tag is mapped to at most once. The
optimal mapping can be found by solving a maxi-
mum weighted bipartite matching problem.
We adapt this idea to map tags between two sys-
tems, rather than between system tags and gold
tags. Given k systems that we want to combine,
we choose one to be the backbone and map the re-
maining k ? 1 systems? outputs to the backbone.
4
After mapping each system?s output to the back-
bone system, we perform simple majority voting
among all k systems. To choose the backbone, we
4
We use the LEMON C++ toolkit (Dezs et al., 2011) to
solve the maximum weighted bipartite matching problems.
consider each of the k systems in turn as back-
bone and maximize the sum of the weights of the
weighted bipartite matching solutions found. This
is a heuristic that attempts to choose a backbone
that is similar to all other systems. We found
that highly-weighted matchings often led to high
POS tagging accuracy metrics. We call this vot-
ing scheme ALIGN. To see the benefit of ALIGN,
we also compare to a simple scheme (NA
?
IVE) that
performs majority voting without any tag map-
ping.
8 Experiments
Task and Datasets We consider POS induction
without tag dictionaries using five freely-available
datasets from the PASCAL shared task (Gelling
et al., 2012).
5
These include Danish (DA), using
the Copenhagen Dependency Treebank v2 (Buch-
Kromann et al., 2007); Dutch (NL), using the
Alpino treebank (Bouma et al., 2001); Por-
tuguese (PT), using the Floresta Sint?a(c)tica tree-
bank (Afonso et al., 2002); Slovene (SL), us-
ing the jos500k treebank (Erjavec et al., 2010);
and Swedish (SV), using the Talbanken tree-
bank (Nivre et al., 2006). We use their provided
training, development, and test sets.
Evaluation We fix the number of tags in our
models to 12, which matches the number of uni-
versal tags from Petrov et al. (2012). We use
both many-to-1 (M-1) and 1-to-1 (1-1) accuracy
as our evaluation metrics, using the universal tags
for the gold standard (which was done for the of-
ficial evaluation for the shared task).
6
We note
that our pi function assigns identities to tags (e.g.,
tag 1 is assumed to be NOUN), so we could use
actual tagging accuracy when training with the pi
cost function. But we use M-1 and 1-1 accuracy
to enable easier comparison both among different
settings and to prior work.
Baselines From the shared task, we compare
to all entries that used 12 tags. These include
5
http://wiki.cs.ox.ac.uk/
InducingLinguisticStructure/SharedTask
6
It is common to use a greedy algorithm to com-
pute 1-to-1 accuracy, e.g., as in the shared task scor-
ing script (http://www.dcs.shef.ac.uk/
?
tcohn/
wils/eval.tar.gz), though the optimal mapping can
be computed efficiently via the maximum weighted bipartite
matching algorithm, as stated above. We use the shared task
scorer for all results here for ease of comparison. When we
instead evaluate using the optimal mapping, we find that ac-
curacies are usually only slightly higher than those found by
the greedy algorithm.
1335
BROWN clusters (Brown et al., 1992), clusters ob-
tained using the mkcls tool (Och, 1995), and the
featurized HMM with sparsity constraints trained
using posterior regularization (PR), described by
Grac?a et al. (2011). The PR system achieved the
highest average 1-1 accuracy in the shared task.
We restrict our attention to systems that use 12
tags because the M-1 and 1-1 metrics are highly
dependent upon the number of hypothesized tags.
In general, using more tags leads to higher M-1
and lower 1-1 (Gelling et al., 2012). By keep-
ing the number of tags fixed, we hope to provide a
cleaner comparison among approaches.
We compare to two other baselines: an HMM
trained with 500 iterations of EM and an HMM
trained with 100 iterations of stepwise EM (Liang
and Klein, 2009). We used random initialization
as done by Liang and Klein: we set each param-
eter in each multinomial to exp{1 + c}, where
c ? U [0, 1], then normalized to get probability
distributions. For stepwise EM, we used mini-
batch size 3 and stepsize reduction power 0.7.
For all models we trained, including both base-
lines and CCE, we used only the training data
during training and used the unannotated devel-
opment data for certain model selection criteria.
No labels were used except for final evaluation on
the test data. Therefore, we need a way to handle
unknown words in test data. When running EM
and stepwise EM, while reading in the final 10%
of sentences in the training set, we replace novel
words with the special token UNK. We then re-
place unknown words in test data with UNK.
8.1 CCE Setup
Features We use standard indicator features on
tag-tag transitions and tag-word emissions, the
spelling features from Smith and Eisner (2005a),
and additional emission features based on Brown
clusters. The latter features are simply indicators
for tag-cluster pairs?analogous to tag-word emis-
sions in which the word is replaced by its Brown
cluster identifier. We run Brown clustering (Liang,
2005) on the POS training data for each language,
once with 12 clusters and once with 40, then add
tag-cluster emission features for each clustering
and one more for their conjunction.
7
7
To handle unknown words: for words that only appear
in the final 10% of training sentences, we replace them with
UNK when firing their tag-word emission features. We use
special Brown cluster identifiers reserved for UNK. But we
still use all spelling features derived from the actual word
Learning We solve Eq. 2 and Eq. 3 by running
LBFGS until convergence on the training data, up
to 100 iterations. We tag the test data with mini-
mum Bayes risk decoding and evaluate.
We use two neighborhood functions:
? TRANS1: the original sentence along with all
sentences that result from doing a single trans-
position of adjacent words.
? SHUFF10: the original sentence along with 10
random permutations of it.
We use L2 regularization, adding
C
2
?
|?|
j=1
?
2
j
to
the objectives shown in Figure 1. We use a fixed
(untuned) C = 0.0001 for all experiments re-
ported below.
8
We initialize each CE model by
sampling weights from N(0, 1).
Cost Functions The cost functions ? and pi
have constants ? and ? which balance their con-
tributions relative to the model score and must be
tuned. We consider the ways proposed in Sec-
tion 7, namely tuning based on the contrastive es-
timation criterion computed on development data
(CE), the log-likelihood estimate on development
data with S = 10 (LL), and our two system com-
bination algorithms: na??ve voting (NA
?
IVE) and
aligned voting (ALIGN), both of which use as in-
put the 4 system outputs whose hyperparameters
led to the highest values for the CE criterion on
development data.
We used ? ? {3 ? 10
?4
, 10
?3
, 3 ?
10
?3
, 0.01, 0.03, 0.1, 0.3} and ? ? {3 ?
10
?6
, 10
?5
, 3 ? 10
?5
, 10
?4
, 3 ? 10
?4
}. Setting
? = ? = 0 gives us CE, which we also compare
to. When using both MATLM and UNIV simul-
taneously, we first choose the best two ? values
by the LL criterion and the best two ? values by
the CE criterion when using only those individual
costs. This gives us 4 pairs of values; we run ex-
periments with these pairs and choose the pair to
report using each of the model selection criteria.
For system combination, we use the 4 system out-
puts resulting from these 4 pairs.
For training bigram language models for the
MATLM cost, we use the language?s POS train-
ing data concatenated with its portion of the Eu-
roparl v7 corpus (Koehn, 2005) and the text of its
type. For unknown words at test time, we use the UNK emis-
sion feature, the Brown cluster features with the special UNK
cluster identifiers, and the word?s actual spelling features.
8
In subsequent experiments we tried C ? {0.01, 0.001}
for the baseline CE setting and found minimal differences.
1336
neigh-
cost
mod. DA NL PT SL SV avg
borhood sel. M-1 1-1 M-1 1-1 M-1 1-1 M-1 1-1 M-1 1-1 M-1 1-1
SHUFF10
none N/A 45.0 38.0 55.1 45.7 54.2 38.0 54.7 45.7 47.4 31.3 51.3 39.7
MATCH
CE 48.9 31.5 56.5 46.4 54.2 37.7 55.9 46.8 48.9 33.8 52.9 39.2
LL 49.9 34.4 56.5 46.4 54.1 38.9 57.2 48.9 48.9 33.8 53.3 40.5
MATLM
CE 49.1 34.3 59.6 50.4 53.6 37.1 55.0 46.2 48.8 33.1 53.2 40.2
LL 50.2 40.0 59.6 50.4 53.1 36.0 58.0 48.4 48.8 33.1 53.9 41.6
TRANS1
none N/A 58.5 42.7 62.5 49.5 70.7 43.8 58.6 46.1 58.7 53.8 61.8 47.2
MATCH
CE 58.5 42.5 66.3 53.3 70.6 43.3 59.1 45.6 59.3 54.2 62.7 47.8
LL 58.8 42.8 66.3 53.3 70.6 43.3 60.3 43.7 59.8 54.9 63.1 47.6
MATLM
CE 59.4 43.5 63.8 50.1 70.2 43.0 58.5 46.1 59.2 54.8 62.2 47.5
LL 58.7 42.8 66.5 60.4 70.5 43.6 59.1 47.7 59.2 54.8 62.8 49.9
Table 2: Results for observation cost functions. The CE baseline corresponds to rows where cost=?none?.
Other rows are CCE. Best score for each column and each neighborhood is bold.
Wikipedia. The word counts for the Wikipedias
used range from 18M for Slovene to 1.9B for
Dutch. We used modified Kneser-Ney smoothing
as implemented by SRILM (Stolcke, 2002).
8.2 Results
We present two sets of results. First we compare
our MATCH and MATLM observation cost func-
tions for our two neighborhoods and two ways of
doing model selection. Then we do a broader com-
parison, comparing both types of costs and their
combination to our full set of baselines.
Observation Cost Functions In Table 2, we
show results for observation cost functions. We
note that the TRANS1 neighborhood works much
better than the SHUFF10 neighborhood, but we
find that using cost functions can close the gap in
certain cases, particularly for Dutch and Slovene
for which the SHUFF10 MATLM scores approach
or exceed the TRANS1 scores without a cost.
Since the SHUFF10 neighborhood exhibits
more diversity than TRANS1, we expect to see
larger gains from using observation cost functions.
We do in fact see larger gains in M-1, e.g., average
improvements are 1.6-2.6 for SHUFF10 and 0.4-
1.3 for TRANS1, though 1-1 gains are closer.
For TRANS1, while MATCH does reach a
slightly higher average M-1 than MATLM, the lat-
ter does much better in 1-1 (49.9 vs. 47.6 when
using LL for model selection). For SHUFF10,
MATLM consistently does better than MATCH.
Nonetheless, we suspect MATCH works as well as
it does because it at least differentiates the obser-
vation (which is always part of the neighborhood)
from the corruptions.
We find that the LL model selection criterion
consistently works better than the CE criterion for
model selection. When using LL model selection
and fixing the neighborhood, all average scores are
better than their CE baselines. For M-1, the aver-
age improvement is 1.0 to 2.6 points, and for 1-1
the average improvement ranges from 0.4 to 2.7.
We find the best overall performance when us-
ing MATLM with LL model selection with the
TRANS1 neighborhood, and we report this setting
in our subsequent experiments.
Output Cost Function Table 3 shows results
when using our UNIV output cost function, as well
as our full set of baselines. All (C)CE experiments
used the TRANS1 neighborhood.
We find that our contrastive estimation baseline
(cost=?none?) has a higher average M-1 (61.8)
than all results from the shared task, but its average
1-1 accuracy is lower than that reached by poste-
rior regularization, the best system in the shared
task according to 1-1. Using an observation cost
function increases both M-1 and 1-1: MATLM
yields an average 1-1 of 49.9, nearing the 50.1 of
PR while exceeding it in M-1 by nearly 2 points.
When using the UNIV cost function, we see
some variation in performance across model selec-
tion criteria, but we find improvements in both M-
1 and 1-1 accuracy under most settings. When do-
ing model selection via ALIGN voting, we roughly
match the average 1-1 of PR, and when using the
CE criterion, we beat it by 1 point on average (51.3
vs. 50.1).
Combined Costs When using the UNIV cost,
we find that model selection via CE works bet-
ter than LL. So for the combined costs, we took
the two best MATLM weights (? values) accord-
ing to LL and the two best UNIV weights (? val-
ues) according to CE and ran combined cost ex-
periments (MATCHLM+UNIV) with the four pairs
of hyperparameters. Then from among these four,
1337
system
DA NL PT SL SV avg
M-1 1-1 M-1 1-1 M-1 1-1 M-1 1-1 M-1 1-1 M-1 1-1
HMM, EM 42.5 28.1 53.0 40.6 59.4 33.7 50.3 34.7 49.3 33.9 50.9 34.2
HMM, stepwise EM 51.7 38.2 61.6 45.2 66.5 46.7 53.6 35.7 55.3 39.6 57.7 41.1
BROWN 47.1 39.2 57.3 43.1 67.6 51.6 58.3 42.3 57.6 51.3 57.6 45.5
mkcls 53.1 44.2 63.0 54.1 68.1 46.3 50.4 40.6 57.3 43.6 58.4 45.8
posterior regularization 53.8 45.6 57.6 45.4 74.4 56.1 60.0 48.5 58.8 54.9 60.9 50.1
contrastive estimation
cost model sel.
none N/A 58.5 42.7 62.5 49.5 70.7 43.8 58.6 46.1 58.7 53.8 61.8 47.2
MATCH LL 58.8 42.8 66.3 53.3 70.6 43.3 60.3 43.7 59.8 54.9 63.1 47.6
MATLM LL 58.7 42.8 66.5 60.4 70.5 43.6 59.1 47.7 59.2 54.8 62.8 49.9
UNIV
CE 59.7 45.6 60.6 51.1 70.0 62.7 60.9 44.1 57.1 52.8 61.7 51.3
LL 59.5 42.2 62.1 56.3 70.7 43.1 60.9 44.1 57.1 52.8 62.1 47.7
NA
?
IVE 59.2 45.6 62.2 52.8 72.7 52.7 60.0 43.8 56.2 53.0 62.2 49.6
ALIGN 61.6 47.3 63.7 54.5 74.4 53.1 59.7 42.1 56.6 53.2 63.2 50.0
MATLM CE 59.8 45.7 60.4 48.4 70.0 62.8 52.9 45.0 59.4 54.9 60.5 51.4
+
LL 59.3 42.5 61.9 56.2 70.8 43.1 59.3 41.9 60.0 55.1 62.3 47.8
NA
?
IVE 58.5 44.4 64.9 60.3 65.4 52.1 55.5 45.9 59.0 54.4 60.6 51.4
UNIV ALIGN 61.1 45.4 66.2 60.9 75.8 49.8 59.5 48.2 59.0 54.4 64.3 51.7
Table 3: Unsupervised POS tagging accuracies for five languages, showing results for three systems from
the PASCAL shared task as well as three other baselines (EM, stepwise EM, and contrastive estimation).
All (C)CE results use the TRANS1 neighborhood. The best score in each column is bold.
we again chose results by CE, LL, and both voting
schemes.
The results are shown in the lower part of Ta-
ble 3. We find different trends in M-1 and 1-
1 depending on whether we use CE or LL for
model selection, which may be due to our lim-
ited hyperparameter search stemming from com-
putational constraints. However, by comparing
NA
?
IVE to ALIGN, we see a consistent benefit
from aligning tags before voting, leading to our
highest average accuracies. In particular, using
MATCHLM+UNIV and ALIGN, we improve over
CE by 2.5 in M-1 and 4.5 in 1-1, also improving
over the best results from the shared task.
9 Conclusion
We have shown how to modify contrastive estima-
tion to use additional sources of knowledge, both
in terms of observation and output cost functions.
We adapted a recently-proposed technique for es-
timating the log-likelihood of held-out data, find-
ing it to be effective as a model selection criterion
when using observation cost functions. We im-
proved tagging accuracy by using weak supervi-
sion in the form of universal tag frequencies. We
proposed a system combination method for POS
induction systems that consistently performs bet-
ter than na??ve voting and circumvents hyperpa-
rameter selection. We reported results on par with
or exceeding the best systems from the PASCAL
2012 shared task.
Contrastive estimation has been shown effective
for numerous NLP tasks, including dependency
grammar induction (Smith and Eisner, 2005b),
bilingual part-of-speech induction (Chen et al.,
2011), morphological segmentation (Poon et al.,
2009), and machine translation (Xiao et al., 2011).
The hope is that our contributions can benefit these
and other applications of weakly-supervised learn-
ing.
Acknowledgments
We thank the anonymous reviewers for their in-
sightful comments and Waleed Ammar, Chris
Dyer, David McAllester, Sasha Rush, Nathan
Schneider, Noah Smith, and John Wieting for
helpful discussions.
References
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002.
Floresta sint?a(c)tica: a treebank for Portuguese. In
Proc. of LREC.
Y. Bengio, L. Yao, and K. Cho. 2013. Bounding
the test log-likelihood of generative models. arXiv
preprint arXiv:1311.6184.
T. Berg-Kirkpatrick, A. Bouchard-C?ot?e, J. DeNero,
and D. Klein. 2010. Painless unsupervised learn-
ing with features. In Proc. of NAACL.
P. Blunsom and T. Cohn. 2010. Unsupervised induc-
tion of tree substitution grammars for dependency
parsing. In Proc. of EMNLP.
1338
P. Blunsom and T. Cohn. 2011. A hierarchical Pitman-
Yor process HMM for unsupervised part of speech
induction. In Proc. of ACL.
G. Bouma, G. Van Noord, and R. Malouf. 2001.
Alpino: Wide-coverage computational analysis of
Dutch. Language and Computers, 37(1).
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della
Pietra, and J. C. Lai. 1992. Class-based N-gram
models of natural language. Computational Lin-
guistics, 18(4).
M. Buch-Kromann, J. Wedekind, and J. Elming.
2007. The Copenhagen Danish-English dependency
treebank v. 2.0. code.google.com/p/copenhagen-
dependency-treebank.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc.
of CoNLL.
D. Chen, C. Dyer, S. B. Cohen, and N. A. Smith. 2011.
Unsupervised bilingual POS tagging with Markov
random fields. In Proc. of the First Workshop on
Unsupervised Learning in NLP.
S. Cohen and N. A. Smith. 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsu-
pervised grammar induction. In Proc. of NAACL.
S. B. Cohen, D. Das, and N. A. Smith. 2011. Unsu-
pervised structure prediction with non-parallel mul-
tilingual guidance. In Proc. of EMNLP.
M. Creutz and K. Lagus. 2005. Unsupervised mor-
pheme segmentation and morphology induction from
text corpora using Morfessor 1.0. Helsinki Univer-
sity of Technology.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projec-
tions. In Proc. of ACL.
S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997.
Inducing features of random fields. IEEE Trans.
Pattern Anal. Mach. Intell., 19(4).
A. Dempster, N. Laird, and D. Rubin. 1977. Maxi-
mum likelihood estimation from incomplete data via
the EM algorithm. Journal of the Royal Statistical
Society B, 39:1?38.
B. Dezs, A. J?uttner, and P. Kov?acs. 2011. LEMON - an
open source C++ graph template library. Electron.
Notes Theor. Comput. Sci., 264(5).
C. B. Do, Q. Le, C. H. Teo, O. Chapelle, and A. Smola.
2008. Tighter bounds for structured estimation. In
Advances in NIPS.
C. Dyer, J. H. Clark, A. Lavie, and N. A. Smith. 2011.
Unsupervised word alignment with arbitrary fea-
tures. In Proc. of ACL.
T. Erjavec, D. Fiser, S. Krek, and N. Ledinek. 2010.
The JOS linguistically tagged corpus of Slovene. In
Proc. of LREC.
K. Ganchev and D. Das. 2013. Cross-lingual discrim-
inative learning of sequence models with posterior
regularization. In Proc. of EMNLP.
K. Ganchev, J. Gillenwater, and B. Taskar. 2009. De-
pendency grammar induction via bitext projection
constraints. In Proc. of ACL.
K. Ganchev, J. V. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search, 11.
D. Gelling, T. Cohn, P. Blunsom, and J. V. Grac?a.
2012. The PASCAL challenge on grammar induc-
tion. In Proc. of NAACL-HLT Workshop on the In-
duction of Linguistic Structure.
K. Gimpel and N. A. Smith. 2010. Softmax-margin
CRFs: Training log-linear models with cost func-
tions. In Proc. of NAACL.
K. Gimpel and N. A. Smith. 2012. Structured ramp
loss minimization for machine translation. In Proc.
of NAACL.
K. Gimpel. 2012. Discriminative Feature-Rich Mod-
eling for Syntax-Based Machine Translation. Ph.D.
thesis, Carnegie Mellon University.
S. Goldwater and T. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Proc. of ACL.
J. V. Grac?a, K. Ganchev, L. Coheur, F. Pereira, and
B. Taskar. 2011. Controlling complexity in part-
of-speech induction. J. Artif. Int. Res., 41(2).
A. Haghighi and D. Klein. 2006. Prototype-driven
learning for sequence models. In Proc. of HLT-
NAACL.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proc. of EMNLP-CoNLL.
J. Kaiser, B. Horvat, and Z. Kacic. 2000. A novel loss
function for the overall risk criterion based discrimi-
native training of HMM models. In Proc. of ICSLP.
D. Klein and C. D. Manning. 2004. Corpus-based
induction of syntactic structure: Models of depen-
dency and constituency. In Proc. of ACL.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In Proc. of MT Summit.
Z. Li, Z. Wang, S. Khudanpur, and J. Eisner. 2010.
Unsupervised discriminative language model train-
ing for machine translation using simulated confu-
sion sets. In Proc. of COLING.
S. Li, J. V. Grac?a, and B. Taskar. 2012. Wiki-ly super-
vised part-of-speech tagging. In Proc. of EMNLP.
1339
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In Proc. of NAACL.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. of HLT-NAACL.
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute
of Technology.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational Linguistics, 20(2).
T. Naseem, B. Snyder, J. Eisenstein, and R. Barzilay.
2009. Multilingual part-of-speech tagging: Two un-
supervised approaches. JAIR, 36.
T. Naseem, H. Chen, R. Barzilay, and M. Johnson.
2010. Using universal linguistic knowledge to guide
grammar induction. In Proc. of EMNLP.
J. Nivre, J. Nilsson, and J. Hall. 2006. Talbanken05: A
Swedish treebank with phrase structure and depen-
dency annotation. In Proc. of LREC.
J. Nivre, J. Hall, S. K?ubler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc.
of CoNLL.
F. J. Och. 1995. Maximum-likelihood-sch?atzung
von wortkategorien mit verfahren der kombina-
torischen optimierung. Bachelor?s thesis (Studien-
arbeit), Friedrich-Alexander-Universit?at Erlangen-
N?urnburg, Germany.
S. Petrov, D. Das, and R. McDonald. 2012. A univer-
sal part-of-speech tagset. In Proc. of LREC.
H. Poon, C. Cherry, and K. Toutanova. 2009. Unsuper-
vised morphological segmentation with log-linear
models. In Proc. of HLT: NAACL.
D. Povey and P. C. Woodland. 2002. Minimum
phone error and I-smoothing for improved discrima-
tive training. In Proc. of ICASSP.
D. Povey, D. Kanevsky, B. Kingsbury, B. Ramabhad-
ran, G. Saon, and K. Visweswariah. 2008. Boosted
MMI for model and feature space discriminative
training. In Proc. of ICASSP.
S. Ravi and K. Knight. 2009. Minimized models for
unsupervised part-of-speech tagging. In Proc. of
ACL.
S. Riezler. 1999. Probabilistic Constraint Logic Pro-
gramming. Ph.D. thesis, Universit?at T?ubingen.
R. Rosenfeld. 1997. A whole sentence maximum en-
tropy language model. In Proc. of ASRU.
N. A. Smith and J. Eisner. 2004. Annealing techniques
for unsupervised statistical language learning. In
Proc. of ACL.
N. A. Smith and J. Eisner. 2005a. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proc. of ACL.
N. A. Smith and J. Eisner. 2005b. Guiding unsuper-
vised grammar induction using contrastive estima-
tion. In Proc. of IJCAI Workshop on Grammatical
Inference Applications.
N. A. Smith and J. Eisner. 2006. Annealing structural
bias in multilingual weighted grammar induction. In
Proc. of COLING-ACL.
D. A. Smith and J. Eisner. 2009. Parser adaptation
and projection with quasi-synchronous features. In
Proc. of EMNLP.
B. Snyder, T. Naseem, and R. Barzilay. 2009. Unsu-
pervised multilingual grammar induction. In Proc.
of ACL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010a.
From Baby Steps to Leapfrog: How ?Less is More?
in unsupervised dependency parsing. In Proc. of
NAACL-HLT.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D.
Manning. 2010b. Viterbi training improves unsu-
pervised dependency parsing. In Proc. of CoNLL.
V. I. Spitkovsky, D. Jurafsky, and H. Alshawi. 2010c.
Profiting from mark-up: Hyper-text annotations for
guided parsing. In Proc. of ACL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011a.
Lateen EM: Unsupervised training with multiple ob-
jectives, applied to dependency grammar induction.
In Proc. of EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b.
Punctuation: Making a point in unsupervised depen-
dency parsing. In Proc. of CoNLL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2013.
Breaking out of local optima with count transforms
and model recombination: A study in grammar in-
duction. In Proc. of EMNLP.
A. Stolcke. 2002. SRILM?an extensible language
modeling toolkit. In Proc. of ICSLP.
O. T?ackstr?om, D. Das, S. Petrov, R. McDonald, and
J. Nivre. 2013. Token and type constraints for cross-
lingual part-of-speech tagging. Transactions of the
Association for Computational Linguistics, 1.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-
margin Markov networks. In Advances in NIPS 16.
K. Toutanova and M. Johnson. 2007. A Bayesian
LDA-based model for semi-supervised part-of-
speech tagging. In Advances in NIPS.
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2005. Large margin methods for structured and
interdependent output variables. Journal of Machine
Learning Research, 6.
1340
J. Van Gael, A. Vlachos, and Z. Ghahramani. 2009.
The infinite HMM for unsupervised POS tagging.
In Proc. of EMNLP.
A. Vaswani, A. Pauls, and D. Chiang. 2010. Efficient
optimization of an MDL-inspired objective function
for unsupervised part-of-speech tagging. In Proc. of
ACL.
M. Wang and C. D. Manning. 2014. Cross-lingual
projected expectation regularization for weakly su-
pervised learning. Transactions of the Association
for Computational Linguistics, 2.
X. Xiao, Y. Liu, Q. Liu, and S. Lin. 2011. Fast gen-
eration of translation forest for large-scale SMT dis-
criminative training. In Proc. of EMNLP.
1341
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 773?782,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Unsupervised Translation Sense Clustering
Mohit Bansal?
UC Berkeley
mbansal@cs.berkeley.edu
John DeNero
Google
denero@google.com
Dekang Lin
Google
lindek@google.com
Abstract
We propose an unsupervised method for clus-
tering the translations of a word, such that
the translations in each cluster share a com-
mon semantic sense. Words are assigned to
clusters based on their usage distribution in
large monolingual and parallel corpora using
the softK-Means algorithm. In addition to de-
scribing our approach, we formalize the task
of translation sense clustering and describe a
procedure that leverages WordNet for evalu-
ation. By comparing our induced clusters to
reference clusters generated from WordNet,
we demonstrate that our method effectively
identifies sense-based translation clusters and
benefits from both monolingual and parallel
corpora. Finally, we describe a method for an-
notating clusters with usage examples.
1 Introduction
The ability to learn a bilingual lexicon from a
parallel corpus was an early and influential area
of success for statistical modeling techniques in
natural language processing. Probabilistic word
alignment models can induce bilexical distributions
over target-language translations of source-language
words (Brown et al, 1993). However, word-to-word
correspondences do not capture the full structure of
a bilingual lexicon. Consider the example bilingual
dictionary entry in Figure 1; in addition to enumerat-
ing the translations of a word, the dictionary author
has grouped those translations into three sense clus-
ters. Inducing such a clustering would prove use-
ful in generating bilingual dictionaries automatically
or building tools to assist bilingual lexicographers.
?Author was a summer intern with Google Research while
conducting this research project.
Colocar [co?lo?car?], va. 1. To arrange, to put in
due place or order. 2. To place, to put in any place,
rank condition or office, to provide a place or em-
ployment. 3. To collocate, to locate, to lay.
Figure 1: This excerpt from a bilingual dictionary groups
English translations of the polysemous Spanish word colocar
into three clusters that correspond to different word senses
(Vela?zquez de la Cadena et al, 1965).
This paper formalizes the task of clustering a set
of translations by sense, as might appear in a pub-
lished bilingual dictionary, and proposes an unsu-
pervised method for inducing such clusters. We also
show how to add usage examples for the translation
sense clusters, hence providing complete structure
to a bilingual dictionary.
The input to this task is a set of source words and
a set of target translations for each source word. Our
proposed method clusters these translations in two
steps. First, we induce a global clustering of the en-
tire target vocabulary using the soft K-Means algo-
rithm, which identifies groups of words that appear
in similar contexts (in a monolingual corpus) and are
translated in similar ways (in a parallel corpus). Sec-
ond, we derive clusters over the translations of each
source word by projecting the global clusters.
We evaluate these clusters by comparing them to
reference clusters with the overlapping BCubed met-
ric (Amigo et al, 2009). We propose a clustering cri-
terion that allows us to derive reference clusters from
the synonym groups of WordNet R? (Miller, 1995).1
Our experiments using Spanish-English and
Japanese-English datasets demonstrate that the au-
tomatically generated clusters produced by our
method are substantially more similar to the
1WordNet is used only for evaluation; our sense clustering
method is fully unsupervised and language-independent.
773
Sense cluster WordNet sense description Usage example
collocate group or chunk together in a certain
order or place side by side
colocar juntas todas los libros
collocate all the books
invest, place, put make an investment capitales para colocar
capital to invest
locate, place assign a location to colocar el nu?mero de serie
locate the serial number
place, position, put put into a certain place or abstract
location
colocar en un lugar
put in a place
Figure 2: Correct sense clusters for the translations of Spanish verb s = colocar, assuming that it has translation set Ts =
{collocate, invest, locate, place, position, put}. Only the sense clusters are outputs of the translation sense clustering task; the
additional columns are presented for clarity.
WordNet-based reference clusters than naive base-
lines. Moreover, we show that bilingual features
collected from parallel corpora improve clustering
accuracy over monolingual distributional similarity
features alone.
Finally, we present a method for annotating clus-
ters with usage examples, which enrich our automat-
ically generated bilingual dictionary entries.
2 Task Description
We consider a three-step pipeline for generating
structured bilingual dictionary entries automatically.
(1) The first step is to identify a set of high-quality
target-side translations for source lexical items. In
our experiments, we ask bilingual human annota-
tors to create these translation sets.2 We restrict our
present study to word-level translations, disallowing
multi-word phrases, in order to leverage existing lex-
ical resources for evaluation.
(2) The second step is to cluster translations of each
word according to common word senses. This clus-
tering task is the primary focus of the paper, and we
formalize it in this section.
(3) The final step annotates clusters with usage ex-
amples to enrich the structure of the output. Sec-
tion 7 describes a method of identifying cluster-
specific usage examples.
In the task of translation sense clustering, the
second step, we assume a fixed set of source lexi-
cal items of interest S, each with a single part of
2We do not use automatically extracted translation sets in
our experiments, in order to isolate the clustering task on clean
input.
speech3, and for each s ? S a set Ts of target trans-
lations. Moreover, we assume that each target word
t ? Ts has a set of senses in common with s. These
senses may also be shared among different target
words. That is, each target word may have multiple
senses and each sense may be expressed by multiple
words.
Given a translation set Ts, we define a clusterG ?
Ts to be a correct sense cluster if it is both coherent
and complete.
? A sense cluster G is coherent if and only if
there exists some sense B shared by all of the
target words in G.
? A sense clusterG is complete if and only if, for
every sense B shared by all words in G, there
is no other word in Ts but not in G that also
shares that sense.
The full set of correct clusters for a set of translations
consists of all sense clusters that are both coherent
and complete.
The example translation set for the Spanish word
colocar in Figure 2 is shown with four correct sense
clusters. For descriptive purposes, these clusters are
annotated by WordNet senses and bilingual usage
examples. However, the task we have defined does
not require the WordNet sense or usage example
to be identified: we must only produce the correct
sense clusters within a set of translations. In fact, a
cluster may correspond to more than one sense.
Our definition of correct sense clusters has sev-
eral appealing properties. First, we do not attempt
to enumerate all senses of the source word. Sense
3A noun and verb that share the same word form would con-
stitute two different source lexical items.
774
Notation
Ts : The set of target-language translations (given)
Dt : The set of synsets in which t appears (given)
C : A synset; a set of target-language words
B : A source-specific synset; a subset of Ts
B : A set of source-specific synsets
G : A set of correct sense clusters for Ts
The Cluster Projection Algorithm:
B ?
{
C ? Ts : C ?
?
t?Ts
Dt
}
G ? ?
for B ? B do
if @B? ? B such that B ? B? then
add B to G
return G
Figure 3: The Cluster Projection (CP) algorithm projects
language-level synsets (C) to source-specific synsets (B) and
then filters the set of synsets for redundant subsets to produce
the complete set of source-specific synsets that are both coher-
ent and complete (G).
distinctions are only made when they affect cross-
lingual lexical choice. If a source word has many
fine-grained senses but translates in the same way
regardless of the sense intended, then there is only
one correct sense cluster for that translation.
Second, no correct sense cluster can be a super-
set of another, because the subset would violate the
completeness condition. This criterion encourages
larger clusters that are easier to interpret, as their
unifying senses can be identified as the intersection
of senses of the translations in the cluster.
Third, the correct clusters need not form a parti-
tion of the input translations. It is common in pub-
lished bilingual dictionaries for a translation to ap-
pear in multiple sense clusters. In our example, the
polysemous English verbs place and put appear in
multiple clusters.
3 Generating Reference Clusters
To construct a reference set for the translation
sense clustering task, we first collected English
translations of Spanish and Japanese nouns, verbs,
and adverbs. Translation sets were curated by hu-
man annotators to keep only high-quality single-
word translations.
Rather than gathering reference clusters via an ad-
ditional annotation effort, we leverage WordNet, a
large database of English lexical semantics (Miller,
1995). WordNet groups words into sets of cogni-
Synsets 
 collocate collocate, lump, chunk 
 invest, put, commit, place invest, clothe, adorn invest, vest, enthrone ? 
 locate, turn up situate, locate locate, place, site ? 
 put, set, place, pose, position, lay rate, rank, range, order, grade, place locate, place, site invest, put, commit, place 
? 
 position put, set, place, pose, position, lay 
 put, set, place, pose, position, lay put frame, redact, cast, put, couch invest, put, commit, place ? 
Words 
 collocate 
  
invest 
  
 locate 
 
  
 place 
  
 
 position   
put 
Sense Clusters 
 collocate 
 invest, place, put 
 
locate, place  
place, position, put 
Figure 4: An example of cluster projection on WordNet, for the
Spanish source word colocar. We show the target translation
words to be clustered, their WordNet synsets (with words not in
the translation set grayed out), and the final set of correct sense
clusters.
tive synonyms called synsets, each expressing a dis-
tinct concept. We use WordNet version 2.1, which
has wide coverage of nouns, verbs, and adverbs, but
sparser coverage of adjectives and prepositions.4
Reference clusters for the set of translations Ts
of some source word s are generated algorithmi-
cally from WordNet synsets via the Cluster Projec-
tion (CP) algorithm defined in Figure 3. An input
to the CP algorithm is the translation set Ts of some
source word s. Also, each translation t ? Ts be-
longs to some set of synsets Dt, where each synset
C ? Dt contains target-language words that may
or may not be translations of s. First, the CP algo-
rithm constructs a source-specific synset B for each
C, which contains only translations of s. Second,
it identifies all correct sense clusters G that are both
coherent and complete with respect to the source-
specific senses B. A sense cluster must correspond
to some synset B ? B to be coherent, and it must
4WordNet version 2.1 is almost identical to ver-
sion 3.0, for Unix-like systems, as described in
http://wordnetcode.princeton.edu/3.0/CHANGES. The lat-
est version 3.1 is not yet available for download.
775
not have a proper superset in B to be complete.5
Figure 4 illustrates the CP algorithm for the trans-
lations of the Spanish source word colocar that ap-
pear in our input dataset.
4 Clustering with K-Means
In this section, we describe an unsupervised method
for inducing translation sense clusters from the us-
age statistics of words in large monolingual and par-
allel corpora. Our method is language independent.
4.1 Distributed SoftK-Means Clustering
As a first step, we cluster all words in the target-
language vocabulary in a way that relates words that
have similar distributional features. Several methods
exist for this task, such as the K-Means algorithm
(MacQueen, 1967), the Brown algorithm (Brown
et al, 1992) and the exchange algorithm (Kneser
and Ney, 1993; Martin et al, 1998; Uszkoreit and
Brants, 2008). We use a distributed implementa-
tion of the ?soft? K-Means clustering algorithm de-
scribed in Lin and Wu (2009). Given a feature vec-
tor for each element (a word type) and the number
of desired clusters K, the K-Means algorithm pro-
ceeds as follows:
1. Select K elements as the initial centroids for
K clusters.
repeat
2. Assign each element to the top M clusters
with the nearest centroid, according to a simi-
larity function in feature space.
3. Recompute each cluster?s centroid by aver-
aging the feature vectors of the elements in that
cluster.
until convergence
4.2 Monolingual Features
Following Lin and Wu (2009), each word to be clus-
tered is represented as a feature vector describing the
distributional context of that word. In our setup, the
5One possible shortcoming of our approach to constructing
reference sets for translation sense clustering is that a cluster
may correspond to a sense that is not shared by the original
source word used to generate the translation set. All translations
must share some sense with the source word, but they may not
share all senses with the source word. It is possible that two
translations are synonymous in a sense that is not shared by the
source. However, we did not observe this problem in practice.
context of a word w consists of the words immedi-
ately to the left and right of w. The context feature
vector of w is constructed by first aggregating the
frequency counts of each word f in the context of
each w. We then compute point-wise mutual infor-
mation (PMI) features from the frequency counts:
PMI(w, f) = log
c(w, f)
c(w)c(f)
where w is a word, f is a neighboring word, and
c(?) is the count of a word or word pair in the cor-
pus.6 A feature vector for w contains a PMI feature
for each word type f (with relative position left or
right) for all words that appears a sufficient number
of times as a neighbor of w. The similarity of two
feature vectors is the cosine of the angle between the
vectors. We follow Lin and Wu (2009) in applying
various thresholds during K-Means, such as a fre-
quency threshold for the initial vocabulary, a total-
count threshold for the feature vectors, and a thresh-
old for PMI scores.
4.3 Bilingual Features
In addition to the features described in Lin and Wu
(2009), we introduce features from a bilingual par-
allel corpus that encode reverse-translation informa-
tion from the source-language (Spanish or Japanese
in our experiments). We have two types of bilin-
gual features: unigram features capture source-side
reverse-translations ofw, while bigram features cap-
ture both the reverse-translations and source-side
neighboring context words to the left and right. Fea-
tures are expressed again as PMI computed from
frequency counts of aligned phrase pairs in a par-
allel corpus. For example, one unigram feature for
place would be the PMI computed from the number
of times that place was in the target side of a phrase
pair whose source side was the unigram lugar. Sim-
ilarly, a bigram feature for place would be the PMI
computed from the number of times that place was
in the target side of a phrase pair whose source side
was the bigram lugar de. These features characterize
the way in which a word is translated, an indication
of its meaning.
6PMI is typically defined in terms of probabilities, but has
proven effective previously when defined in terms of counts.
776
4.4 Predicting Translation Clusters
As a result of softK-Means clustering, each word in
the target-language vocabulary is assigned to a list of
up to M clusters. To predict the sense clusters for a
set of translations of a source word, we apply the CP
algorithm (Figure 3), treating the K-Means clusters
as synsets (Dt).
5 Related Work
To our knowledge, the translation sense clustering
task has not been explored previously. However,
much prior work has explored the related task of
monolingual word and phrase clustering. Uszkor-
eit and Brants (2008) uses an exchange algorithm
to cluster words in a language model, Lin and Wu
(2009) uses distributed K-Means to cluster phrases
for various discriminative classification tasks, Vla-
chos et al (2009) uses Dirichlet Process Mixture
Models for verb clustering, and Sun and Korhonen
(2011) uses a hierarchical Levin-style clustering to
cluster verbs.
Previous word sense induction work (Diab and
Resnik, 2002; Kaji, 2003; Ng et al, 2003; Tufis
et al, 2004; Apidianaki, 2009) relates to our work
in that these approaches discover word senses au-
tomatically through clustering, even using multilin-
gual parallel corpora. However, our task of clus-
tering multiple words produces a different type of
output from the standard word sense induction task
of clustering in-context uses of a single word. The
underlying notion of ?sense? is shared across these
tasks, but the way in which we use and evaluate in-
duced senses is novel.
6 Experiments
The purpose of our experiments is to assess whether
our unsupervised soft K-Means clustering method
can effectively recover the reference sense clusters
derived from WordNet.
6.1 Datasets
We conduct experiments using two bilingual
datasets: Spanish-to-English (S?E) and Japanese-
to-English (J?E). Table 1 shows, for each dataset,
the number of source words and the total number
of target words in their translation sets. The datasets
Dataset No. of src-words Total no. of tgt-words
S?E 52 230
J?E 369 1639
Table 1: Sizes of the Spanish-to-English (S?E) and Japanese-
to-English (J?E) datasets.
are limited in size because we solicited human anno-
tators to filter the set of translations for each source
word. The S?E dataset has 52 source-words with a
part-of-speech-tag distribution of 38 nouns, 10 verbs
and 4 adverbs. The J?E dataset has 369 source-
words with 319 nouns, 38 verbs and 12 adverbs. We
included only these parts of speech because Word-
Net version 2.1 has adequate coverage for them.
Most source words have 3 to 5 translations each.
Monolingual features for K-Means clustering
were computed from an English corpus of Web
documents with 700 billion tokens of text. Bilin-
gual features were computed from 0.78 (S?E) and
1.04 (J?E) billion tokens of parallel text, primar-
ily extracted from the Web using automated paral-
lel document identification (Uszkoreit et al, 2010).
Word alignments were induced from the HMM-
based alignment model (Vogel et al, 1996), initial-
ized with the bilexical parameters of IBM Model 1
(Brown et al, 1993). Both models were trained us-
ing 2 iterations of the expectation maximization al-
gorithm. Phrase pairs were extracted from aligned
sentence pairs in the same manner used in phrase-
based machine translation (Koehn et al, 2003).
6.2 Clustering Evaluation Metrics
The quality of text clustering algorithms can be eval-
uated using a wide set of metrics. For evaluation
by set matching, the popular measures are Purity
(Zhao and Karypis, 2001) and Inverse Purity and
their harmonic mean (F measure, see Van Rijsber-
gen (1974)). For evaluation by counting pairs, the
popular metrics are the Rand Statistic and Jaccard
Coefficient (Halkidi et al, 2001; Meila, 2003).
Metrics based on entropy include Cluster Entropy
(Steinbach et al, 2000), Class Entropy (Bakus et al,
2002), VI-measure (Meila, 2003), Q0 (Dom, 2001),
V-measure (Rosenberg and Hirschberg, 2007) and
Mutual Information (Xu et al, 2003). Lastly, there
exist the BCubed metrics (Bagga and Baldwin,
1998), a family of metrics that decompose the clus-
777
tering evaluation by estimating precision and recall
for each item in the distribution.
Amigo et al (2009) compares the various clus-
tering metrics mentioned above and their properties.
They define four formal but intuitive constraints on
such metrics that explain which aspects of clustering
quality are captured by the different metric families.
Their analysis shows that of the wide range of met-
rics, only BCubed satisfies those constraints. After
defining each constraint below, we briefly describe
its relevance to the translation sense clustering task.
Homogeneity: In a cluster, we should not mix items
belonging to different categories.
Relevance: All words in a proposed cluster should
share some common WordNet sense.
Completeness: Items belonging to the same cate-
gory should be grouped in the same cluster.
Relevance: All words that share some common
WordNet sense should appear in the same cluster.
Rag Bag: Introducing disorder into a disordered
cluster is less harmful than introducing disorder into
a clean cluster.
Relevance: We prefer to maximize the number of
error-free clusters, because these are most easily in-
terpreted and therefore most useful.
Cluster Size vs. Quantity: A small error in a big
cluster is preferable to a large number of small er-
rors in small clusters.
Relevance: We prefer to minimize the total number
of erroneous clusters in a dictionary.
Amigo et al (2009) also show that BCubed ex-
tends cleanly to settings with overlapping clusters,
where an element can simultaneously belong to
more than one cluster. For these reasons, we focus
on BCubed for cluster similarity evaluation.7
The BCubed metric for scoring overlapping clus-
ters is computed from the pair-wise precision and
recall between pairs of items:
P(e, e?) =
min(|C(e) ? C(e?)|, |L(e) ? L(e?)|)
|C(e) ? C(e?)|
R(e, e?) =
min(|C(e) ? C(e?)|, |L(e) ? L(e?)|)
|L(e) ? L(e?)|
where e and e? are two items, L(e) is the set of ref-
erence clusters for e and C(e) is the set of predicted
7An evaluation using purity and inverse purity (extended to
overlapping clusters) has been omitted for space, but leads to
the same conclusions as the evaluation using BCubed.
clusters for e (i.e., clusters to which e belongs). Note
that P(e, e?) is defined only when e and e? share
some predicted cluster, and R(e, e?) when e and e?
share some reference cluster.
The BCubed precision associated to one item is its
averaged pair-wise precision over other items shar-
ing some of its predicted clusters, and likewise for
recall8; and the overall BCubed precision (or recall)
is the averaged precision (or recall) of all items:
PB3 = Avge[Avge?s.t.C(e)?C(e?)6=?[P(e, e
?)]]
RB3 = Avge[Avge?s.t.L(e)?L(e?)6=?[R(e, e
?)]]
6.3 Results
Figure 5 shows the F?-score for various ? values:
F? =
(1 + ?2) ? PB3 ? RB3
?2 ? PB3 + RB3
This graph gives us a trade-off between precision
and recall (? = 0 is exact precision and ? ? ?
tends to exact recall).9
Each curve in Figure 5 represents a particular
clustering method. We include three naive baselines:
ewnc: Each word in its own cluster
aw1c: All words in one cluster
Random: Each target word is assigned M random
cluster id?s in the range 1 to K, then translation
sets are clustered with the CP algorithm.
The curves for K-Means clustering include one
condition with monolingual features alone and two
curves that include bilingual features as well.10 The
bilingual curves correspond to two different feature
sets: the first includes only unigram features (t1),
while the second includes both unigram and bigram
features (t1t2).
Each point on an F? curve in Figure 5 (including
the baseline curves) represents a maximum over two
8The metric does include in this computation the relation of
each item with itself.
9Note that we use the micro-averaged version of F-score
where we first compute PB3 and RB3 for each source-word,
then compute the average PB3 and RB3 over all source-words,
and finally compute the F-score using these averaged PB3 and
RB3.
10All bilingual K-Means experiments include monolingual
features also. K-Means with only bilingual features does not
produce accurate clusters.
778
0.65 
0.7 
0.75 
0.8 
0.85 
0.9 
0.95 
0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 
F ! s
core
 
!  
Spanish-English BCubed Results 
ewnc aw1c Random Kmeans-monolingual Kmeans-bilingual-t1 Kmeans-bilingual-t1t2 
0.7 
0.75 
0.8 
0.85 
0.9 
0.95 
0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 
F ! s
core
 
!  
Japanese-English BCubed Results 
ewnc aw1c Random Kmeans-monolingual Kmeans-bilingual-t1 Kmeans-bilingual-t1t2 
Figure 5: BCubed F? plot for the Spanish-English dataset (top) and Japanese-English dataset (bottom).
Source word: ayudar
Monolingual [[aid], [assist, help]] P=1.0, R=0.56
Bilingual [[aid, assist, help]] P=1.0, R=1.0
Source word: concurso
Monolingual [[competition, contest, match], [concourse], [contest, meeting]] P=0.58, R=1.0
Bilingual [[competition, contest], [concourse], [match], [meeting]] P=1.0, R=1.0
Table 2: Examples showing improvements in clustering when we move from K-Means clustering with only monolingual features
to clustering with additional bilingual features.
779
0.79 
0.84 
0.89 
0.94 
0.99 
0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 
Reca
ll 
Precision 
Japanese-English BCubed Results 
Random Kmeans-monolingual Kmeans-bilingual-t1 Kmeans-bilingual-t1t2 ewnc aw1c 
Figure 6: BCubed Precision-Recall scatter plot for the Japanese-English dataset. Each point represents a particular choice of cluster
count K and clusters per word M .
parameters: K, the number of clusters created in the
whole corpus andM , the number of clusters allowed
per word (in M -best soft K-Means). As both the
random baseline and proposed clustering methods
can be tuned to favor precision or recall, we show
the best result from each technique across this spec-
trum of F? metrics. We vary ? to highlight different
potential objectives of translation sense clustering.
An application that focuses on synonym discovery
would favor recall, while an application portraying
highly granular sense distinctions would favor pre-
cision.
Clustering accuracy improves over the baselines
with monolingual features alone, and it improves
further with the addition of bilingual features, for a
wide range of ? values. Our unsupervised approach
with bilingual features achieves up to 6-8% absolute
improvement over the random baseline, and is par-
ticularly effective for recall-weighted metrics.11 As
an example, in a S?E experiment with a K-Means
setting ofK = 4096 : M = 3, the overall F1.5 score
11It is not surprising that a naive baseline like random clus-
tering can achieve a high precision: BCubed counts each word
itself as correctly clustered, and so even trivial techniques that
create many singleton clusters will have high precision. High
recall (without very low precision) is harder to achieve, because
it requires positing larger clusters, and it is for recall-focused
objectives that our technique substantially outperforms the ran-
dom baseline.
increases from 80.58% to 86.12% upon adding bilin-
gual features. Table 2 shows two examples from that
experiment for which bilingual features improve the
output clusters.
The parameter values we use in our experiments
are K ? {23, 24, . . . , 212} and M ? {1, 2, 3, 4, 5}.
To provide additional detail, Figure 6 shows the
BCubed precision and recall for each induced clus-
tering, as the values of K and M vary, for Japanese-
English.12 Each point in this scatter plot represents a
clustering methodology and a particular value for K
and M . Soft K-Means with bilingual features pro-
vides the strongest performance across a broad range
of cluster parameters.
6.4 Evaluation Details
Certain special cases needed to be addressed in order
to complete this evaluation.
Target words not in WordNet: Words that did not
have any synset in WordNet were each assigned to a
singleton reference cluster.13 The S?E dataset has
only 2 out of 225 target types missing in WordNet
and the J?E dataset has only 55 out of 1351 target
12Spanish-English precision-recall results are omitted due to
space constraints, but depict similar trends.
13Note that certain words with WordNet synsets also end up
in their own singleton cluster because all other words in their
cluster are not in the translation set.
780
types missing.
Target words not clustered by K-Means: The K-
Means algorithm applies various thresholds during
different parts of the process. As a result, there
are some target word types that are not assigned
any cluster at the end of the algorithm. For ex-
ample, in the J?E experiment with K = 4096
and with bilingual (t1 only) features, only 49 out
of 1351 target-types are not assigned any cluster by
K-Means. These unclustered words were each as-
signed to a singleton cluster in post-processing.
7 Identifying Usage Examples
We now briefly consider the task of automatically
extracting usage examples for each predicted clus-
ter. We identify these examples among the extracted
phrase pairs of a parallel corpus.
Let Ps be the set of source phrases containing
source word s, and letAt be the set of source phrases
that align to target phrases containing target word
t. For a source word s and target sense cluster G,
we identify source phrases that contain s and trans-
late to all words in G. That is, we collect the set
of phrases Ps ?
?
t?GAt. We use the same parallel
corpus as we used to compute bilingual features.
For example, if we consider the cluster [place, po-
sition, put] for the Spanish word colocar, then we
find Spanish phrases that contain colocar and also
align to English phrases containing place, position,
and put somewhere in the parallel corpus. Sample
usage examples extracted by this approach appear in
Figure 7. We have not performed a quantitative eval-
uation of these extracted examples, although quali-
tatively we have found that the technique surfaces
useful phrases. We look forward to future research
that further explores this important sub-task of auto-
matically generating bilingual dictionaries.
8 Conclusion
We presented the task of translation sense clustering,
a critical second step to follow translation extraction
in a pipeline for generating well-structured bilingual
dictionaries automatically. We introduced a method
of projecting language-level clusters into clusters for
specific translation sets using the CP algorithm. We
used this technique both for constructing reference
clusters, via WordNet synsets, and constructing pre-
debajo
["below","beneath"]    ? debajo de la superficie (below the surface)
["below","under"]     ? debajo de la l?nea (below the line)
["underneath"]     ? debajo de la piel (under the skin)
??
["break"]     ? ???? ?? ? ?? ?? ?? ? ? ?? ?? . 
(I worked hard and I deserve a good break.)
["recreation"]     ? ?? ? ?? ? ?? ?? 
(Traditional healing and recreation activities)
["rest"]     ? ??? ? ?? ?? ?? ? ?? ?? . 
(Bed rest is the only treatment required.)
??
["application"]     ? ??????? ?? ?? 
(Computer-aided technique)
["use","utilization"]     ? ?? ? ?? ?? ? ?? ?? 
(Promote effective use of land)
??
["draw","pull"]     ? ???? ? ?? 
(Draw the curtain)
["subtract"]     ? A ?? B ? ?? 
(Subtract B from A)
["tug"]     ? ? ? ??? ?? 
(Tug at someone's sleeve)
Figure 7: Usage examples for Spanish and Japanese words and
their English sense clusters. Our approach extracts multiple
examples per cluster, but we show only one. We also show
the translation of the examples back into English produced by
Google Translate.
dicted clusters from the output of a vocabulary-level
clustering algorithm.
Our experiments demonstrated that the soft K-
Means clustering algorithm, trained using distribu-
tional features from very large monolingual and
bilingual corpora, recovered a substantial portion of
the structure of reference clusters, as measured by
the BCubed clustering metric. The addition of bilin-
gual features improved clustering results over mono-
lingual features alone; these features could prove
useful for other clustering tasks as well. Finally, we
annotated our clusters with usage examples.
In future work, we hope to combine our cluster-
ing method with a system for automatically gen-
erating translation sets. In doing so, we will de-
velop a system that can automatically induce high-
quality, human-readable bilingual dictionaries from
large corpora using unsupervised learning methods.
Acknowledgments
We would like to thank Jakob Uszkoreit, Adam
Pauls, and the anonymous reviewers for their helpful
suggestions.
781
References
Enrique Amigo, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Infor-
mation Retrieval, 12(4):461486.
Marianna Apidianaki. 2009. Data-driven semantic anal-
ysis for multilingual WSD and lexical selection in
translation. In Proceedings of EACL.
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space model.
In Proceedings of COLING-ACL.
J. Bakus, M. F. Hussin, and M. Kamel. 2002. A SOM-
based document clustering using phrases. In Proceed-
ings of ICONIP.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jenifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467479.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of ACL.
B.E. Dom. 2001. An information-theoretic external
cluster-validity measure. In IBM Technical Report RJ-
10219.
M. Halkidi, Y. Batistakis, and M. Vazirgiannis. 2001. On
clustering validation techniques. Journal of Intelligent
Information Systems, 17(2-3):107?145.
Hiroyuki Kaji. 2003. Word sense acquisition from bilin-
gual comparable corpora. In Proceedings of NAACL.
Reinherd Kneser and Hermann Ney. 1993. Improved
clustering techniques for class-based statistical lan-
guage modelling. In Proceedings of the 3rd European
Conference on Speech Communication and Technol-
ogy.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of ACL.
J. B. MacQueen. 1967. Some methods for classifica-
tion and analysis of multivariate observations. In Pro-
ceedings of 5th Berkeley Symposium on Mathematical
Statistics and Probability.
Sven Martin, Jorg Liermann, and Hermann Ney. 1998.
Algorithms for bigram and trigram word clustering.
Speech Communication, 24:19?37.
M. Meila. 2003. Comparing clusterings by the variation
of information. In Proceedings of COLT.
George A. Miller. 1995. Wordnet: A lexical database for
English. In Communications of the ACM.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Ex-
ploiting parallel texts for word sense disambiguation:
An empirical study. In Proceedings of ACL.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of EMNLP.
Michael Steinbach, George Karypis, and Vipin Kumar.
2000. A comparison of document clustering tech-
niques. In Proceedings of KDD Workshop on Text
Mining.
Lin Sun and Anna Korhonen. 2011. Hierarchical verb
clustering using graph factorization. In Proceedings
of EMNLP.
Dan Tufis, Radu Ion, and Nancy Ide. 2004. Fine-grained
word sense disambiguation based on parallel corpora,
word alignment, word clustering and aligned word-
nets. In Proceedings of COLING.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed
word clustering for large scale class-based language
modeling in machine translation. In Proceedings of
ACL.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Du-
biner. 2010. Large scale parallel document mining for
machine translation. In Proceedings of COLING.
C. Van Rijsbergen. 1974. Foundation of evaluation.
Journal of Documentation, 30(4):365?373.
Mariano Vela?zquez de la Cadena, Edward Gray, and
Juan L. Iribas. 1965. New Revised Vela?zques Spanish
and English Dictionary. Follet Publishing Company.
Andreas Vlachos, Anna Korhonen, and Zoubin Ghahra-
mani. 2009. Unsupervised and constrained Dirichlet
process mixture models for verb clustering. In Pro-
ceedings of the Workshop on Geometrical Models of
Natural Language Semantics.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the Conference on Computa-
tional linguistics.
W. Xu, X. Liu, and Y. Gong. 2003. Document-clustering
based on non-negative matrix factorization. In Pro-
ceedings of SIGIR.
Y. Zhao and G. Karypis. 2001. Criterion functions for
document clustering: Experiments and analysis. In
Technical Report TR 01-40, Department of Computer
Science, University of Minnesota, Minneapolis, MN.
782
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1098?1107,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Simple, Accurate Parsing with an All-Fragments Grammar
Mohit Bansal and Dan Klein
Computer Science Division
University of California, Berkeley
{mbansal,klein}@cs.berkeley.edu
Abstract
We present a simple but accurate parser
which exploits both large tree fragments
and symbol refinement. We parse with
all fragments of the training set, in con-
trast to much recent work on tree se-
lection in data-oriented parsing and tree-
substitution grammar learning. We re-
quire only simple, deterministic grammar
symbol refinement, in contrast to recent
work on latent symbol refinement. More-
over, our parser requires no explicit lexi-
con machinery, instead parsing input sen-
tences as character streams. Despite its
simplicity, our parser achieves accuracies
of over 88% F1 on the standard English
WSJ task, which is competitive with sub-
stantially more complicated state-of-the-
art lexicalized and latent-variable parsers.
Additional specific contributions center on
making implicit all-fragments parsing effi-
cient, including a coarse-to-fine inference
scheme and a new graph encoding.
1 Introduction
Modern NLP systems have increasingly used data-
intensive models that capture many or even all
substructures from the training data. In the do-
main of syntactic parsing, the idea that all train-
ing fragments1 might be relevant to parsing has a
long history, including tree-substitution grammar
(data-oriented parsing) approaches (Scha, 1990;
Bod, 1993; Goodman, 1996a; Chiang, 2003) and
tree kernel approaches (Collins and Duffy, 2002).
For machine translation, the key modern advance-
ment has been the ability to represent and memo-
rize large training substructures, be it in contigu-
ous phrases (Koehn et al, 2003) or syntactic trees
1In this paper, a fragment means an elementary tree in a
tree-substitution grammar, while a subtree means a fragment
that bottoms out in terminals.
(Galley et al, 2004; Chiang, 2005; Deneefe and
Knight, 2009). In all such systems, a central chal-
lenge is efficiency: there are generally a combina-
torial number of substructures in the training data,
and it is impractical to explicitly extract them all.
On both efficiency and statistical grounds, much
recent TSG work has focused on fragment selec-
tion (Zuidema, 2007; Cohn et al, 2009; Post and
Gildea, 2009).
At the same time, many high-performance
parsers have focused on symbol refinement ap-
proaches, wherein PCFG independence assump-
tions are weakened not by increasing rule sizes
but by subdividing coarse treebank symbols into
many subcategories either using structural anno-
tation (Johnson, 1998; Klein and Manning, 2003)
or lexicalization (Collins, 1999; Charniak, 2000).
Indeed, a recent trend has shown high accura-
cies from models which are dedicated to inducing
such subcategories (Henderson, 2004; Matsuzaki
et al, 2005; Petrov et al, 2006). In this paper,
we present a simplified parser which combines the
two basic ideas, using both large fragments and
symbol refinement, to provide non-local and lo-
cal context respectively. The two approaches turn
out to be highly complementary; even the simplest
(deterministic) symbol refinement and a basic use
of an all-fragments grammar combine to give ac-
curacies substantially above recent work on tree-
substitution grammar based parsers and approach-
ing top refinement-based parsers. For example,
our best result on the English WSJ task is an F1
of over 88%, where recent TSG parsers2 achieve
82-84% and top refinement-based parsers3 achieve
88-90% (e.g., Table 5).
Rather than select fragments, we use a simplifi-
cation of the PCFG-reduction of DOP (Goodman,
2Zuidema (2007), Cohn et al (2009), Post and Gildea
(2009). Zuidema (2007) incorporates deterministic refine-
ments inspired by Klein and Manning (2003).
3Including Collins (1999), Charniak and Johnson (2005),
Petrov and Klein (2007).
1098
1996a) to work with all fragments. This reduction
is a flexible, implicit representation of the frag-
ments that, rather than extracting an intractably
large grammar over fragment types, indexes all
nodes in the training treebank and uses a com-
pact grammar over indexed node tokens. This in-
dexed grammar, when appropriately marginalized,
is equivalent to one in which all fragments are ex-
plicitly extracted. Our work is the first to apply
this reduction to full-scale parsing. In this direc-
tion, we present a coarse-to-fine inference scheme
and a compact graph encoding of the training set,
which, together, make parsing manageable. This
tractability allows us to avoid selection of frag-
ments, and work with all fragments.
Of course, having a grammar that includes all
training substructures is only desirable to the ex-
tent that those structures can be appropriately
weighted. Implicit representations like those
used here do not allow arbitrary weightings of
fragments. However, we use a simple weight-
ing scheme which does decompose appropriately
over the implicit encoding, and which is flexible
enough to allow weights to depend not only on fre-
quency but also on fragment size, node patterns,
and certain lexical properties. Similar ideas have
been explored in Bod (2001), Collins and Duffy
(2002), and Goodman (2003). Our model empir-
ically affirms the effectiveness of such a flexible
weighting scheme in full-scale experiments.
We also investigate parsing without an explicit
lexicon. The all-fragments approach has the ad-
vantage that parsing down to the character level
requires no special treatment; we show that an ex-
plicit lexicon is not needed when sentences are
considered as strings of characters rather than
words. This avoids the need for complex un-
known word models and other specialized lexical
resources.
The main contribution of this work is to show
practical, tractable methods for working with an
all-fragments model, without an explicit lexicon.
In the parsing case, the central result is that ac-
curacies in the range of state-of-the-art parsers
(i.e., over 88% F1 on English WSJ) can be ob-
tained with no sampling, no latent-variable mod-
eling, no smoothing, and even no explicit lexicon
(hence negligible training overall). These tech-
niques, however, are not limited to the case of
monolingual parsing, offering extensions to mod-
els of machine translation, semantic interpretation,
and other areas in which a similar tension exists
between the desire to extract many large structures
and the computational cost of doing so.
2 Representation of Implicit Grammars
2.1 All-Fragments Grammars
We consider an all-fragments grammar G (see
Figure 1(a)) derived from a binarized treebank
B. G is formally a tree-substitution grammar
(Resnik, 1992; Bod, 1993) wherein each subgraph
of each training tree in B is an elementary tree,
or fragment f , in G. In G, each derivation d is
a tree (multiset) of fragments (Figure 1(c)), and
the weight of the derivation is the product of the
weights of the fragments: ?(d) =
?
f?d ?(f). In
the following, the derivation weights, when nor-
malized over a given sentence s, are interpretable
as conditional probabilities, soG induces distribu-
tions of the form P (d|s).
In models like G, many derivations will gen-
erally correspond to the same unsegmented tree,
and the parsing task is to find the tree whose
sum of derivation weights is highest: tmax =
arg maxt
?
d?t ?(d). This final optimization is in-
tractable in a way that is orthogonal to this pa-
per (Sima?an, 1996); we describe minimum Bayes
risk approximations in Section 4.
2.2 Implicit Representation of G
Explicitly extracting all fragment-rules of a gram-
marG is memory and space intensive, and imprac-
tical for full-size treebanks. As a tractable alter-
native, we consider an implicit grammar GI (see
Figure 1(b)) that has the same posterior probabil-
ities as G. To construct GI , we use a simplifi-
cation of the PCFG-reduction of DOP by Good-
man (1996a).4 GI has base symbols, which are
the symbol types from the original treebank, as
well as indexed symbols, which are obtained by
assigning a unique index to each node token in
the training treebank. The vast majority of sym-
bols in GI are therefore indexed symbols. While
it may seem that such grammars will be overly
large, they are in fact reasonably compact, being
linear in the treebank size B, while G is exponen-
tial in the length of a sentence. In particular, we
found that GI was smaller than explicit extraction
of all depth 1 and 2 unbinarized fragments for our
4The difference is that Goodman (1996a) collapses our
BEGIN and END rules into the binary productions, giving a
larger grammar which is less convenient for weighting.
1099
!
SYMBOLS: X, for all types in treebank "
RULES: X?#, for all fragments in "
! $
SYMBOLS:
?Base: X   for all types in treebank "
?Indexed: Xi for all tokens of X in "
RULES:
?Begin: ;?;i for all Xi in "
?Continue: Xi?<j Zk for all rule-tokens in "
?End: Xi?;IRUDOO;i in " %$
FRAGMENTSDERIVATIONS
(a)
(b)
GRAMMAR
%
#$
A
X
Al
CONTINUE
END
Xi
ZkYj
BEGIN
B
Bm
C
Cn
#
A
X
ZY
B C
CBA
X
words 
X
CBA
words 
EX
PL
IC
IT
IM
PL
IC
IT
MAP  ?
Figure 1: Grammar definition and sample derivations and fragments in the grammar for (a) the explicitly extracted all-fragments
grammar G, and (b) its implicit representation GI .
treebanks ? in practice, even just the raw treebank
grammar grows almost linearly in the size of B.5
There are 3 kinds of rules inGI , which are illus-
trated in Figure 1(d). The BEGIN rules transition
from a base symbol to an indexed symbol and rep-
resent the beginning of a fragment from G. The
CONTINUE rules use only indexed symbols and
correspond to specific depth-1 binary fragment to-
kens from training trees, representing the internal
continuation of a fragment in G. Finally, END
rules transition from an indexed symbol to a base
symbol, representing the frontier of a fragment.
By construction, all derivations in GI will seg-
ment, as shown in Figure 1(d), into regions corre-
sponding to tokens of fragments from the training
treebank B. Let pi be the map which takes appro-
priate fragments in GI (those that begin and end
with base symbols and otherwise contain only in-
dexed symbols), and maps them to the correspond-
ing f in G. We can consider any derivation dI in
GI to be a tree of fragments f I , each fragment a
token of a fragment type f = pi(f I) in the orig-
inal grammar G. By extension, we can therefore
map any derivation dI in GI to the corresponding
derivation d = pi(dI) in G.
The mapping pi is an onto mapping from GI to
5Just half the training set (19916 trees) itself had 1.7 mil-
lion depth 1 and 2 unbinarized rules compared to the 0.9 mil-
lion indexed symbols in GI (after graph packing). Even ex-
tracting binarized fragments (depth 1 and 2, with one order
of parent annotation) gives us 0.75 million rules, and, practi-
cally, we would need fragments of greater depth.
G. In particular, each derivation d in G has a non-
empty set of corresponding derivations {dI} =
pi?1(d) in GI , because fragments f in d corre-
spond to multiple fragments f I in GI that differ
only in their indexed symbols (one f I per occur-
rence of f in B). Therefore, the set of derivations
in G is preserved in GI . We now discuss how
weights can be preserved under pi.
2.3 Equivalence for Weighted Grammars
In general, arbitrary weight functions ? on frag-
ments in G do not decompose along the increased
locality of GI . However, we now consider a use-
fully broad class of weighting schemes for which
the posterior probabilities under G of derivations
d are preserved in GI . In particular, assume that
we have a weighting ? on rules in GI which does
not depend on the specific indices used. There-
fore, any fragment f I will have a weight in GI of
the form:
?I(f
I) = ?BEGIN(b)
?
r?C
?CONT(r)
?
e?E
?END(e)
where b is the BEGIN rule, r are CONTINUE rules,
and e are END rules in the fragment f I (see Fig-
ure 1(d)). Because ? is assumed to not depend on
the specific indices, all f I which correspond to the
same f under pi will have the same weight ?I(f)
in GI .
In this case, we can define an induced weight
1100
Xi
BEGIN
A
X
Al
CONTINUE
END
ZkYj
Bm
word
DOP1MIN-FRAGMENTS OUR MODEL
!!
" #$ !"%#$%!
!!CONTINUE
RULE TYPES WEIGHTS
Figure 2: Rules defined for grammar GI and weight schema
for the DOP1 model, the Min-Fragments model (Goodman
(2003)) and our model. Here s(X) denotes the total number
of fragments rooted at base symbol X .
for fragments f in G by
?G(f) =
?
fI?pi?1(f)
?I(f
I) = n(f)?I(f)
= n(f)?BEGIN(b
?)
?
r??C
?CONT(r
?)
?
e??E
?END(e
?)
where now b?, r? and e? are non-indexed type ab-
stractions of f ?s member productions in GI and
n(f) = |pi?1(f)| is the number of tokens of f in
B.
Under the weight function ?G(f), any deriva-
tion d in G will have weight which obeys
?G(d) =
?
f?d
?G(f) =
?
f?d
n(f)?I(f)
=
?
dI?d
?I(d
I)
and so the posterior P (d|s) of a derivation d for
a sentence s will be the same whether computed
in G or GI . Therefore, provided our weighting
function on fragments f in G decomposes over
the derivational representation of f in GI , we can
equivalently compute the quantities we need for
inference (see Section 4) using GI instead.
3 Parameterization of Implicit
Grammars
3.1 Classical DOP1
The original data-oriented parsing model ?DOP1?
(Bod, 1993) is a particular instance of the general
weighting scheme which decomposes appropri-
ately over the implicit encoding, described in Sec-
tion 2.3. Figure 2 shows rule weights for DOP1
in the parameter schema we have defined. The
END rule weight is 0 or 1 depending on whether
A is an intermediate symbol or not.6 The local
fragments in DOP1 were flat (non-binary) so this
weight choice simulates that property by not al-
lowing switching between fragments at intermedi-
ate symbols.
The original DOP1 model weights a fragment f
in G as ?G(f) = n(f)/s(X), i.e., the frequency
of fragment f divided by the number of fragments
rooted at base symbol X . This is simulated by our
weight choices (Figure 2) where each fragment f I
inGI has weight ?I(f I) = 1/s(X) and therefore,
?G(f) =
?
fI?pi?1(f) ?I(f
I) = n(f)/s(X).
Given the weights used for DOP1, the recursive
formula for the number of fragments s(Xi) rooted
at indexed symbol Xi (and for the CONTINUE rule
Xi ? Yj Zk) is
s(Xi) = (1 + s(Yj))(1 + s(Zk)), (1)
where s(Yj) and s(Zk) are the number of frag-
ments rooted at indexed symbols Yj and Zk (non-
intermediate) respectively. The number of frag-
ments s(X) rooted at base symbol X is then
s(X) =
?
Xi
s(Xi).
Implicitly parsing with the full DOP1 model (no
sampling of fragments) using the weights in Fig-
ure 2 gives a 68% parsing accuracy on the WSJ
dev-set.7 This result indicates that the weight of a
fragment should depend on more than just its fre-
quency.
3.2 Better Parameterization
As has been pointed out in the literature, large-
fragment grammars can benefit from weights of
fragments depending not only on their frequency
but also on other properties. For example, Bod
(2001) restricts the size and number of words
in the frontier of the fragments, and Collins and
Duffy (2002) and Goodman (2003) both give
larger fragments smaller weights. Our model can
incorporate both size and lexical properties. In
particular, we set ?CONT(r) for each binary CON-
TINUE rule r to a learned constant ?BODY, and we
set the weight for each rule with a POS parent to a
6Intermediate symbols are those created during binariza-
tion.
7For DOP1 experiments, we use no symbol refinement.
We annotate with full left binarization history to imitate the
flat nature of fragments in DOP1. We use mild coarse-pass
pruning (Section 4.1) without which the basic all-fragments
chart does not fit in memory. Standard WSJ treebank splits
used: sec 2-21 training, 22 dev, 23 test.
1101
Rule score: r(A? B C, i, k, j) =
?
x
?
y
?
z
O(Ax, i, j)?(Ax ? By Cz)I(By, i, k)I(Cz, k, j)
Max-Constituent: q(A, i, j) =
?
x O(Ax,i,j)I(Ax,i,j)?
r I(rootr,0,n)
tmax = argmax
t
?
c?t
q(c)
Max-Rule-Sum: q(A? B C, i, k, j) = r(A?B C,i,k,j)?
r I(rootr,0,n)
tmax = argmax
t
?
e?t
q(e)
Max-Variational: q(A? B C, i, k, j) = r(A?B C,i,k,j)?
x O(Ax,i,j)I(Ax,i,j)
tmax = argmax
t
?
e?t
q(e)
Figure 3: Inference: Different objectives for parsing with posteriors. A, B, C are base symbols, Ax, By , Cz are indexed
symbols and i,j,k are between-word indices. Hence, (Ax, i, j) represents a constituent labeled with Ax spanning words i
to j. I(Ax, i, j) and O(Ax, i, j) denote the inside and outside scores of this constituent, respectively. For brevity, we write
c ? (A, i, j) and e ? (A? B C, i, k, j). Also, tmax is the highest scoring parse. Adapted from Petrov and Klein (2007).
constant ?LEX (see Figure 2). Fractional values of
these parameters allow the weight of a fragment to
depend on its size and lexical properties.
Another parameter we introduce is a
?switching-penalty? csp for the END rules
(Figure 2). The DOP1 model uses binary values
(0 if symbol is intermediate, 1 otherwise) as
the END rule weight, which is equivalent to
prohibiting fragment switching at intermediate
symbols. We learn a fractional constant asp
that allows (but penalizes) switching between
fragments at annotated symbols through the
formulation csp(Xintermediate) = 1 ? asp and
csp(Xnon?intermediate) = 1 + asp. This feature
allows fragments to be assigned weights based on
the binarization status of their nodes.
With the above weights, the recursive formula
for s(Xi), the total weighted number of fragments
rooted at indexed symbol Xi, is different from
DOP1 (Equation 1). For rule Xi ? Yj Zk, it is
s(Xi) = ?BODY.(csp(Yj)+s(Yj))(csp(Zk)+s(Zk)).
The formula uses ?LEX in place of ?BODY if r is a
lexical rule (Figure 2).
The resulting grammar is primarily parameter-
ized by the training treebank B. However, each
setting of the hyperparameters (?BODY, ?LEX, asp)
defines a different conditional distribution on
trees. We choose amongst these distributions by
directly optimizing parsing F1 on our develop-
ment set. Because this objective is not easily dif-
ferentiated, we simply perform a grid search on
the three hyperparameters. The tuned values are
?BODY = 0.35, ?LEX = 0.25 and asp = 0.018.
For generalization to a larger parameter space, we
would of course need to switch to a learning ap-
proach that scales more gracefully in the number
of tunable hyperparameters.8
8Note that there has been a long history of DOP estima-
tors. The generative DOP1 model was shown to be inconsis-
dev (? 40) test (? 40) test (all)
Model F1 EX F1 EX F1 EX
Constituent 88.4 33.7 88.5 33.0 87.6 30.8
Rule-Sum 88.2 34.6 88.3 33.8 87.4 31.6
Variational 87.7 34.4 87.7 33.9 86.9 31.6
Table 1: All-fragments WSJ results (accuracy F1 and exact
match EX) for the constituent, rule-sum and variational ob-
jectives, using parent annotation and one level of markoviza-
tion.
4 Efficient Inference
The previously described implicit grammarGI de-
fines a posterior distribution P (dI |s) over a sen-
tence s via a large, indexed PCFG. This distri-
bution has the property that, when marginalized,
it is equivalent to a posterior distribution P (d|s)
over derivations in the correspondingly-weighted
all-fragments grammar G. However, even with
an explicit representation of G, we would not be
able to tractably compute the parse that maxi-
mizes P (t|s) =
?
d?t P (d|s) =
?
dI?t P (d
I |s)
(Sima?an, 1996). We therefore approximately
maximize over trees by computing various exist-
ing approximations to P (t|s) (Figure 3). Good-
man (1996b), Petrov and Klein (2007), and Mat-
suzaki et al (2005) describe the details of con-
stituent, rule-sum and variational objectives re-
spectively. Note that all inference methods depend
on the posterior P (t|s) only through marginal ex-
pectations of labeled constituent counts and an-
chored local binary tree counts, which are easily
computed from P (dI |s) and equivalent to those
from P (d|s). Therefore, no additional approxima-
tions are made in GI over G.
As shown in Table 1, our model (an all-
fragments grammar with the weighting scheme
tent by Johnson (2002). Later, Zollmann and Sima?an (2005)
presented a statistically consistent estimator, with the basic
insight of optimizing on a held-out set. Our estimator is not
intended to be viewed as a generative model of trees at all,
but simply a loss-minimizing conditional distribution within
our parametric family.
1102
shown in Figure 2) achieves an accuracy of
88.5% (using simple parent annotation) which is
4-5% (absolute) better than the recent TSG work
(Zuidema, 2007; Cohn et al, 2009; Post and
Gildea, 2009) and also approaches state-of-the-
art refinement-based parsers (e.g., Charniak and
Johnson (2005), Petrov and Klein (2007)).9
4.1 Coarse-to-Fine Inference
Coarse-to-fine inference is a well-established way
to accelerate parsing. Charniak et al (2006) in-
troduced multi-level coarse-to-fine parsing, which
extends the basic pre-parsing idea by adding more
rounds of pruning. Their pruning grammars
were coarse versions of the raw treebank gram-
mar. Petrov and Klein (2007) propose a multi-
stage coarse-to-fine method in which they con-
struct a sequence of increasingly refined gram-
mars, reparsing with each refinement. In par-
ticular, in their approach, which we adopt here,
coarse-to-fine pruning is used to quickly com-
pute approximate marginals, which are then used
to prune subsequent search. The key challenge
in coarse-to-fine inference is the construction of
coarse models which are much smaller than the
target model, yet whose posterior marginals are
close enough to prune with safely.
Our grammar GI has a very large number of in-
dexed symbols, so we use a coarse pass to prune
away their unindexed abstractions. The simple,
intuitive, and effective choice for such a coarse
grammar GC is a minimal PCFG grammar com-
posed of the base treebank symbols X and the
minimal depth-1 binary rules X ? Y Z (and
with the same level of annotation as in the full
grammar). If a particular base symbolX is pruned
by the coarse pass for a particular span (i, j) (i.e.,
the posterior marginal P (X, i, j|s) is less than a
certain threshold), then in the full grammar GI ,
we do not allow building any indexed symbol
Xl of type X for that span. Hence, the pro-
jection map for the coarse-to-fine model is piC :
Xl (indexed symbol)? X (base symbol).
We achieve a substantial improvement in speed
and memory-usage from the coarse-pass pruning.
Speed increases by a factor of 40 and memory-
usage decreases by a factor of 10 when we go
9All our experiments use the constituent objective ex-
cept when we report results for max-rule-sum and max-
variational parsing (where we use the parameters tuned for
max-constituent, therefore they unsurprisingly do not per-
form as well as max-constituent). Evaluations use EVALB,
see http://nlp.cs.nyu.edu/evalb/.
87.8
88.0
88.2
88.4
-4.0 -4.5 -5.0 -5.5 -6.0 -6.5 -7.0 -7.5Coarse-pass Log Posterior Threshold (PT)
F1
-6.2
Figure 4: Effect of coarse-pass pruning on parsing accuracy
(for WSJ dev-set, ? 40 words). Pruning increases to the left
as log posterior threshold (PT) increases.
86.086.5
87.087.5
88.088.5
89.089.5
90.0
-1 -3 -5 -7 -9 -11 -13Coarse-pass Log Posterior Threshold (PT)
-6
F1
89.6 No Pruning (PT = -inf)
89.8
Figure 5: Effect of coarse-pass pruning on parsing accuracy
(WSJ, training ? 20 words, tested on dev-set ? 20 words).
This graph shows that the fortuitous improvement due to
pruning is very small and that the peak accuracy is almost
equal to the accuracy without pruning (the dotted line).
from no pruning to pruning with a ?6.2 log pos-
terior threshold.10 Figure 4 depicts the variation
in parsing accuracies in response to the amount
of pruning done by the coarse-pass. Higher pos-
terior pruning thresholds induce more aggressive
pruning. Here, we observe an effect seen in previ-
ous work (Charniak et al (1998), Petrov and Klein
(2007), Petrov et al (2008)), that a certain amount
of pruning helps accuracy, perhaps by promoting
agreement between the coarse and full grammars
(model intersection). However, these ?fortuitous?
search errors give only a small improvement and
the peak accuracy is almost equal to the pars-
ing accuracy without any pruning (as seen in Fig-
ure 5).11 This outcome suggests that the coarse-
pass pruning is critical for tractability but not for
performance.
10Unpruned experiments could not be run for 40-word test
sentences even with 50GB of memory, therefore we calcu-
lated the improvement factors using a smaller experiment
with full training and sixty 30-word test sentences.
11To run experiments without pruning, we used training
and dev sentences of length ? 20 for the graph in Figure 5.
1103
tree-to-graph encoding
Figure 6: Collapsing the duplicate training subtrees converts
them to a graph and reduces the number of indexed symbols
significantly.
4.2 Packed Graph Encoding
The implicit all-fragments approach (Section 2.2)
avoids explicit extraction of all rule fragments.
However, the number of indexed symbols in our
implicit grammar GI is still large, because ev-
ery node in each training tree (i.e., every symbol
token) has a unique indexed symbol. We have
around 1.9 million indexed symbol tokens in the
word-level parsing model (this number increases
further to almost 12.3 million when we parse char-
acter strings in Section 5.1). This large symbol
space makes parsing slow and memory-intensive.
We reduce the number of symbols in our im-
plicit grammar GI by applying a compact, packed
graph encoding to the treebank training trees. We
collapse the duplicate subtrees (fragments that
bottom out in terminals) over all training trees.
This keeps the grammar unchanged because in an
tree-substitution grammar, a node is defined (iden-
tified) by the subtree below it. We maintain a
hashmap on the subtrees which allows us to eas-
ily discover the duplicates and bin them together.
The collapsing converts all the training trees in the
treebank to a graph with multiple parents for some
nodes as shown in Figure 6. This technique re-
duces the number of indexed symbols significantly
as shown in Table 2 (1.9 million goes down to 0.9
million, reduction by a factor of 2.1). This reduc-
tion increases parsing speed by a factor of 1.4 (and
by a factor of 20 for character-level parsing, see
Section 5.1) and reduces memory usage to under
4GB.
We store the duplicate-subtree counts for each
indexed symbol of the collapsed graph (using a
hashmap). When calculating the number of frag-
Parsing Model No. of Indexed Symbols
Word-level Trees 1,900,056
Word-level Graph 903,056
Character-level Trees 12,280,848
Character-level Graph 1,109,399
Table 2: Number of indexed symbols for word-level and
character-level parsing and their graph versions (for all-
fragments grammar with parent annotation and one level of
markovization).
Figure 7: Character-level parsing: treating the sentence as a
string of characters instead of words.
ments s(Xi) parented by an indexed symbol Xi
(see Section 3.2), and when calculating the inside
and outside scores during inference, we account
for the collapsed subtree tokens by expanding the
counts and scores using the corresponding multi-
plicities. Therefore, we achieve the compaction
with negligible overhead in computation.
5 Improved Treebank Representations
5.1 Character-Level Parsing
The all-fragments approach to parsing has the
added advantage that parsing below the word level
requires no special treatment, i.e., we do not need
an explicit lexicon when sentences are considered
as strings of characters rather than words.
Unknown words in test sentences (unseen in
training) are a major issue in parsing systems for
which we need to train a complex lexicon, with
various unknown classes or suffix tries. Smooth-
ing factors need to be accounted for and tuned.
With our implicit approach, we can avoid training
a lexicon by building up the parse tree from char-
acters instead of words. As depicted in Figure 7,
each word in the training trees is split into its cor-
responding characters with start and stop bound-
ary tags (and then binarized in a standard right-
branching style). A test sentence?s words are split
up similarly and the test-parse is built from train-
ing fragments using the same model and inference
procedure as defined for word-level parsing (see
Sections 2, 3 and 4). The lexical items (alphabets,
digits etc.) are now all known, so unlike word-level
parsing, no sophisticated lexicon is needed.
We choose a slightly richer weighting scheme
1104
dev (? 40) test (? 40) test (all)
Model F1 EX F1 EX F1 EX
Constituent 88.2 33.6 88.0 31.9 87.1 29.8
Rule-Sum 88.0 33.9 87.8 33.1 87.0 30.9
Variational 87.6 34.4 87.2 32.3 86.4 30.2
Table 3: All-fragments WSJ results for the character-level
parsing model, using parent annotation and one level of
markovization.
for this representation by extending the two-
weight schema for CONTINUE rules (?LEX and
?BODY) to a three-weight one: ?LEX, ?WORD, and
?SENT for CONTINUE rules in the lexical layer, in
the portion of the parse that builds words from
characters, and in the portion of the parse that
builds the sentence from words, respectively. The
tuned values are ?SENT = 0.35, ?WORD = 0.15,
?LEX = 0.95 and asp = 0. The character-level
model achieves a parsing accuracy of 88.0% (see
Table 3), despite lacking an explicit lexicon.12
Character-level parsing expands the training
trees (see Figure 7) and the already large indexed
symbol space size explodes (1.9 million increases
to 12.3 million, see Table 2). Fortunately, this
is where the packed graph encoding (Section 4.2)
is most effective because duplication of character
strings is high (e.g., suffixes). The packing shrinks
the symbol space size from 12.3 million to 1.1 mil-
lion, a reduction by a factor of 11. This reduction
increases parsing speed by almost a factor of 20
and brings down memory-usage to under 8GB.13
5.2 Basic Refinement: Parent Annotation
and Horizontal Markovization
In a pure all-fragments approach, compositions
of units which would have been independent in
a basic PCFG are given joint scores, allowing
the representation of certain non-local phenom-
ena, such as lexical selection or agreement, which
in fully local models require rich state-splitting
or lexicalization. However, at substitution sites,
the coarseness of raw unrefined treebank sym-
bols still creates unrealistic factorization assump-
tions. A standard solution is symbol refinement;
Johnson (1998) presents the particularly simple
case of parent annotation, in which each node is
12Note that the word-level model yields a higher accuracy
of 88.5%, but uses 50 complex unknown word categories
based on lexical, morphological and position features (Petrov
et al, 2006). Cohn et al (2009) also uses this lexicon.
13Full char-level experiments (w/o packed graph encoding)
could not be run even with 50GB of memory. We calcu-
late the improvement factors using a smaller experiment with
70% training and fifty 20-word test sentences.
Parsing Model F1
No Refinement (P=0, H=0)? 71.3
Basic Refinement (P=1, H=1)? 80.0
All-Fragments + No Refinement (P=0, H=0) 85.7
All-Fragments + Basic Refinement (P=1, H=1) 88.4
Table 4: F1 for a basic PCFG, and incorporation of basic
refinement, all-fragments and both, for WSJ dev-set (? 40
words). P = 1 means parent annotation of all non-terminals,
including the preterminal tags. H = 1 means one level of
markovization. ?Results from Klein and Manning (2003).
marked with its parent in the underlying treebank.
It is reasonable to hope that the gains from us-
ing large fragments and the gains from symbol re-
finement will be complementary. Indeed, previous
work has shown or suggested this complementar-
ity. Sima?an (2000) showed modest gains from en-
riching structural relations with semi-lexical (pre-
head) information. Charniak and Johnson (2005)
showed accuracy improvements from composed
local tree features on top of a lexicalized base
parser. Zuidema (2007) showed a slight improve-
ment in parsing accuracy when enough fragments
were added to learn enrichments beyond manual
refinements. Our work reinforces this intuition by
demonstrating how complementary they are in our
model (?20% error reduction on adding refine-
ment to an all-fragments grammar, as shown in the
last two rows of Table 4).
Table 4 shows results for a basic PCFG, and its
augmentation with either basic refinement (parent
annotation and one level of markovization), with
all-fragments rules (as in previous sections), or
both. The basic incorporation of large fragments
alone does not yield particularly strong perfor-
mance, nor does basic symbol refinement. How-
ever, the two approaches are quite additive in our
model and combine to give nearly state-of-the-art
parsing accuracies.
5.3 Additional Deterministic Refinement
Basic symbol refinement (parent annotation), in
combination with all-fragments, gives test-set ac-
curacies of 88.5% (? 40 words) and 87.6% (all),
shown as the Basic Refinement model in Table 5.
Klein and Manning (2003) describe a broad set
of simple, deterministic symbol refinements be-
yond parent annotation. We included ten of their
simplest annotation features, namely: UNARY-DT,
UNARY-RB, SPLIT-IN, SPLIT-AUX, SPLIT-CC, SPLIT-%,
GAPPED-S, POSS-NP, BASE-NP and DOMINATES-V.
None of these annotation schemes use any head
information. This additional annotation (see Ad-
1105
83
84
85
86
87
88
89
0 20 40 60 80 100
F1
Percentage of WSJ sections 2-21 used for training
Figure 8: Parsing accuracy F1 on the WSJ dev-set (? 40
words) increases with increasing percentage of training data.
ditional Refinement, Table 5) improves the test-
set accuracies to 88.7% (? 40 words) and 88.1%
(all), which is equal to a strong lexicalized parser
(Collins, 1999), even though our model does not
use lexicalization or latent symbol-split induc-
tion.
6 Other Results
6.1 Parsing Speed and Memory Usage
The word-level parsing model using the whole
training set (39832 trees, all-fragments) takes ap-
proximately 3 hours on the WSJ test set (2245
trees of ?40 words), which is equivalent to
roughly 5 seconds of parsing time per sen-
tence; and runs in under 4GB of memory. The
character-level version takes about twice the time
and memory. This novel tractability of an all-
fragments grammar is achieved using both coarse-
pass pruning and packed graph encoding. Micro-
optimization may further improve speed and mem-
ory usage.
6.2 Training Size Variation
Figure 8 shows how WSJ parsing accuracy in-
creases with increasing amount of training data
(i.e., percentage of WSJ sections 2-21). Even if we
train on only 10% of the WSJ training data (3983
sentences), we still achieve a reasonable parsing
accuracy of nearly 84% (on the development set,
? 40 words), which is comparable to the full-
system results obtained by Zuidema (2007), Cohn
et al (2009) and Post and Gildea (2009).
6.3 Other Language Treebanks
On the French and German treebanks (using the
standard dataset splits mentioned in Petrov and
test (? 40) test (all)
Parsing Model F1 EX F1 EX
FRAGMENT-BASED PARSERS
Zuidema (2007) ? ? 83.8? 26.9?
Cohn et al (2009) ? ? 84.0 ?
Post and Gildea (2009) 82.6 ? ? ?
THIS PAPER
All-Fragments
+ Basic Refinement 88.5 33.0 87.6 30.8
+ Additional Refinement 88.7 33.8 88.1 31.7
REFINEMENT-BASED PARSERS
Collins (1999) 88.6 ? 88.2 ?
Petrov and Klein (2007) 90.6 39.1 90.1 37.1
Table 5: Our WSJ test set parsing accuracies, compared
to recent fragment-based parsers and top refinement-based
parsers. Basic Refinement is our all-fragments grammar with
parent annotation. Additional Refinement adds determinis-
tic refinement of Klein and Manning (2003) (Section 5.3).
?Results on the dev-set (? 100).
Klein (2008)), our simple all-fragments parser
achieves accuracies in the range of top refinement-
based parsers, even though the model parameters
were tuned out of domain on WSJ. For German,
our parser achieves an F1 of 79.8% compared
to 81.5% by the state-of-the-art and substantially
more complex Petrov and Klein (2008) work. For
French, our approach yields an F1 of 78.0% vs.
80.1% by Petrov and Klein (2008).14
7 Conclusion
Our approach of using all fragments, in combi-
nation with basic symbol refinement, and even
without an explicit lexicon, achieves results in the
range of state-of-the-art parsers on full scale tree-
banks, across multiple languages. The main take-
away is that we can achieve such results in a very
knowledge-light way with (1) no latent-variable
training, (2) no sampling, (3) no smoothing be-
yond the existence of small fragments, and (4) no
explicit unknown word model at all. While these
methods offer a simple new way to construct an
accurate parser, we believe that this general ap-
proach can also extend to other large-fragment
tasks, such as machine translation.
Acknowledgments
This project is funded in part by BBN under
DARPA contract HR0011-06-C-0022 and the NSF
under grant 0643742.
14All results on the test set (? 40 words).
1106
References
Rens Bod. 1993. Using an Annotated Corpus as a
Stochastic Grammar. In Proceedings of EACL.
Rens Bod. 2001. What is the Minimal Set of Frag-
ments that Achieves Maximum Parse Accuracy? In
Proceedings of ACL.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of ACL.
Eugene Charniak, Sharon Goldwater, and Mark John-
son. 1998. Edge-Based Best-First Chart Parsing.
In Proceedings of the 6th Workshop on Very Large
Corpora.
Eugene Charniak, Mark Johnson, et al 2006. Multi-
level Coarse-to-fine PCFG Parsing. In Proceedings
of HLT-NAACL.
Eugene Charniak. 2000. A Maximum-Entropy-
Inspired Parser. In Proceedings of NAACL.
David Chiang. 2003. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Data-Oriented Parsing.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Pro-
ceedings of ACL.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing Compact but Accurate Tree-
Substitution Grammars. In Proceedings of NAACL.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In
Proceedings of ACL.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis, Uni-
versity of Pennsylvania, Philadelphia.
Steve Deneefe and Kevin Knight. 2009. Synchronous
Tree Adjoining Machine Translation. In Proceed-
ings of EMNLP.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT-NAACL.
Joshua Goodman. 1996a. Efficient Algorithms for
Parsing the DOP Model. In Proceedings of EMNLP.
Joshua Goodman. 1996b. Parsing Algorithms and
Metrics. In Proceedings of ACL.
Joshua Goodman. 2003. Efficient parsing of DOP with
PCFG-reductions. In Bod R, Scha R, Sima?an K
(eds.) Data-Oriented Parsing. University of Chicago
Press, Chicago, IL.
James Henderson. 2004. Discriminative Training of
a Neural Network Statistical Parser. In Proceedings
of ACL.
Mark Johnson. 1998. PCFG Models of Linguistic
Tree Representations. Computational Linguistics,
24:613?632.
Mark Johnson. 2002. The DOP Estimation Method Is
Biased and Inconsistent. In Computational Linguis-
tics 28(1).
Dan Klein and Christopher Manning. 2003. Accurate
Unlexicalized Parsing. In Proceedings of ACL.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceed-
ings of HLT-NAACL.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of ACL.
Slav Petrov and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. In Proceedings of
NAACL-HLT.
Slav Petrov and Dan Klein. 2008. Sparse Multi-Scale
Grammars for Discriminative Latent Variable Pars-
ing. In Proceedings of EMNLP.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and
Interpretable Tree Annotation. In Proceedings of
COLING-ACL.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-Fine Syntactic Machine Translation using
Language Projections. In Proceedings of EMNLP.
Matt Post and Daniel Gildea. 2009. Bayesian Learning
of a Tree Substitution Grammar. In Proceedings of
ACL-IJCNLP.
Philip Resnik. 1992. Probabilistic Tree-Adjoining
Grammar as a Framework for Statistical Natural
Language Processing. In Proceedings of COLING.
Remko Scha. 1990. Taaltheorie en taaltechnologie;
competence en performance. In R. de Kort and
G.L.J. Leerdam (eds.): Computertoepassingen in de
Neerlandistiek.
Khalil Sima?an. 1996. Computational Complexity
of Probabilistic Disambiguation by means of Tree-
Grammars. In Proceedings of COLING.
Khalil Sima?an. 2000. Tree-gram Parsing: Lexical De-
pendencies and Structural Relations. In Proceedings
of ACL.
Andreas Zollmann and Khalil Sima?an. 2005. A
Consistent and Efficient Estimator for Data-Oriented
Parsing. Journal of Automata, Languages and Com-
binatorics (JALC), 10(2/3):367?388.
Willem Zuidema. 2007. Parsimonious Data-Oriented
Parsing. In Proceedings of EMNLP-CoNLL.
1107
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 693?702,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Web-Scale Features for Full-Scale Parsing
Mohit Bansal and Dan Klein
Computer Science Division
University of California, Berkeley
{mbansal,klein}@cs.berkeley.edu
Abstract
Counts from large corpora (like the web) can
be powerful syntactic cues. Past work has
used web counts to help resolve isolated am-
biguities, such as binary noun-verb PP attach-
ments and noun compound bracketings. In
this work, we first present a method for gener-
ating web count features that address the full
range of syntactic attachments. These fea-
tures encode both surface evidence of lexi-
cal affinities as well as paraphrase-based cues
to syntactic structure. We then integrate our
features into full-scale dependency and con-
stituent parsers. We show relative error re-
ductions of 7.0% over the second-order depen-
dency parser of McDonald and Pereira (2006),
9.2% over the constituent parser of Petrov et
al. (2006), and 3.4% over a non-local con-
stituent reranker.
1 Introduction
Current state-of-the art syntactic parsers have
achieved accuracies in the range of 90% F1 on the
Penn Treebank, but a range of errors remain. From
a dependency viewpoint, structural errors can be
cast as incorrect attachments, even for constituent
(phrase-structure) parsers. For example, in the
Berkeley parser (Petrov et al, 2006), about 20%
of the errors are prepositional phrase attachment er-
rors as in Figure 1, where a preposition-headed (IN)
phrase was assigned an incorrect parent in the im-
plied dependency tree. Here, the Berkeley parser
(solid blue edges) incorrectly attaches from debt to
the noun phrase $ 30 billion whereas the correct at-
tachment (dashed gold edges) is to the verb rais-
ing. However, there are a range of error types, as
shown in Figure 2. Here, (a) is a non-canonical PP
VBG 
VP 
NP 
NP ? raising 
$ 30 billion 
PP 
from debt ? 
Figure 1: A PP attachment error in the parse output of the
Berkeley parser (on Penn Treebank). Guess edges are in solid
blue, gold edges are in dashed gold and edges common in guess
and gold parses are in black.
attachment ambiguity where by yesterday afternoon
should attach to had already, (b) is an NP-internal
ambiguity where half a should attach to dozen and
not to newspapers, and (c) is an adverb attachment
ambiguity, where just should modify fine and not the
verb ?s.
Resolving many of these errors requires informa-
tion that is simply not present in the approximately
1M words on which the parser was trained. One
way to access more information is to exploit sur-
face counts from large corpora like the web (Volk,
2001; Lapata and Keller, 2004). For example, the
phrase raising from is much more frequent on the
Web than $ x billion from. While this ?affinity? is
only a surface correlation, Volk (2001) showed that
comparing such counts can often correctly resolve
tricky PP attachments. This basic idea has led to a
good deal of successful work on disambiguating iso-
lated, binary PP attachments. For example, Nakov
and Hearst (2005b) showed that looking for para-
phrase counts can further improve PP resolution.
In this case, the existence of reworded phrases like
raising it from on the Web also imply a verbal at-
693
S 
NP 
NP PP 
?/eKman +Xtton ,nFby yesterday afternoon 
VP 
Kad aOread\ ? PDT 
NP 
? KaOI 
DT 
a 
PDT 
dozen 
PDT 
newspapers 
QP VBZ 
VP 
? ?s 
ADVP 
RB 
just 
ADJP 
JJ 
fine 
ADJP 
(a) (b) (c) 
Figure 2: Different kinds of attachment errors in the parse output of the Berkeley parser (on Penn Treebank). Guess edges are in
solid blue, gold edges are in dashed gold and edges common in guess and gold parses are in black.
tachment. Still other work has exploited Web counts
for other isolated ambiguities, such as NP coordina-
tion (Nakov and Hearst, 2005b) and noun-sequence
bracketing (Nakov and Hearst, 2005a; Pitler et al,
2010). For example, in (b), half dozen is more fre-
quent than half newspapers.
In this paper, we show how to apply these ideas
to all attachments in full-scale parsing. Doing so
requires three main issues to be addressed. First,
we show how features can be generated for arbitrary
head-argument configurations. Affinity features are
relatively straightforward, but paraphrase features,
which have been hand-developed in the past, are
more complex. Second, we integrate our features
into full-scale parsing systems. For dependency
parsing, we augment the features in the second-order
parser of McDonald and Pereira (2006). For con-
stituent parsing, we rerank the output of the Berke-
ley parser (Petrov et al, 2006). Third, past systems
have usually gotten their counts from web search
APIs, which does not scale to quadratically-many
attachments in each sentence. Instead, we consider
how to efficiently mine the Google n-grams corpus.
Given the success of Web counts for isolated am-
biguities, there is relatively little previous research
in this direction. The most similar work is Pitler
et al (2010), which use Web-scale n-gram counts
for multi-way noun bracketing decisions, though
that work considers only sequences of nouns and
uses only affinity-based web features. Yates et al
(2006) use Web counts to filter out certain ?seman-
tically bad? parses from extraction candidate sets
but are not concerned with distinguishing amongst
top parses. In an important contrast, Koo et al
(2008) smooth the sparseness of lexical features in a
discriminative dependency parser by using cluster-
based word-senses as intermediate abstractions in
addition to POS tags (also see Finkel et al (2008)).
Their work also gives a way to tap into corpora be-
yond the training data, through cluster membership
rather than explicit corpus counts and paraphrases.
This work uses a large web-scale corpus (Google
n-grams) to compute features for the full parsing
task. To show end-to-end effectiveness, we incor-
porate our features into state-of-the-art dependency
and constituent parsers. For the dependency case,
we can integrate them into the dynamic program-
ming of a base parser; we use the discriminatively-
trained MST dependency parser (McDonald et al,
2005; McDonald and Pereira, 2006). Our first-order
web-features give 7.0% relative error reduction over
the second-order dependency baseline of McDon-
ald and Pereira (2006). For constituent parsing, we
use a reranking framework (Charniak and Johnson,
2005; Collins and Koo, 2005; Collins, 2000) and
show 9.2% relative error reduction over the Berke-
ley parser baseline. In the same framework, we
also achieve 3.4% error reduction over the non-local
syntactic features used in Huang (2008). Our web-
scale features reduce errors for a range of attachment
types. Finally, we present an analysis of influential
features. We not only reproduce features suggested
in previous work but also discover a range of new
ones.
2 Web-count Features
Structural errors in the output of state-of-the-art
parsers, constituent or dependency, can be viewed
as attachment errors, examples of which are Figure 1
and Figure 2.1 One way to address attachment errors
is through features which factor over head-argument
1For constituent parsers, there can be minor tree variations
which can result in the same set of induced dependencies, but
these are rare in comparison.
694
raising          $           from    debt 
?(raising     from) ?($     from) 
?(head     arg) 
Figure 3: Features factored over head-argument pairs.
pairs, as is standard in the dependency parsing liter-
ature (see Figure 3). Here, we discuss which web-
count based features ?(h, a) should fire over a given
head-argument pair (we consider the words h and
a to be indexed, and so features can be sensitive to
their order and distance, as is also standard).
2.1 Affinity Features
Affinity statistics, such as lexical co-occurrence
counts from large corpora, have been used previ-
ously for resolving individual attachments at least as
far back as Lauer (1995) for noun-compound brack-
eting, and later for PP attachment (Volk, 2001; La-
pata and Keller, 2004) and coordination ambigu-
ity (Nakov and Hearst, 2005b). The approach of
Lauer (1995), for example, would be to take an am-
biguous noun sequence like hydrogen ion exchange
and compare the various counts (or associated con-
ditional probabilities) of n-grams like hydrogen ion
and hydrogen exchange. The attachment with the
greater score is chosen. More recently, Pitler et al
(2010) use web-scale n-grams to compute similar
association statistics for longer sequences of nouns.
Our affinity features closely follow this basic idea
of association statistics. However, because a real
parser will not have access to gold-standard knowl-
edge of the competing attachment sites (see Atterer
and Schutze (2007)?s criticism of previous work),
we must instead compute features for all possible
head-argument pairs from our web corpus. More-
over, when there are only two competing attachment
options, one can do things like directly compare two
count-based heuristics and choose the larger. Inte-
gration into a parser requires features to be functions
of single attachments, not pairwise comparisons be-
tween alternatives. A learning algorithm can then
weight features so that they compare appropriately
across parses.
We employ a collection of affinity features of
varying specificity. The basic feature is the core ad-
jacency count feature ADJ, which fires for all (h, a)
pairs. What is specific to a particular (h, a) is the
value of the feature, not its identity. For example, in
a naive approach, the value of the ADJ feature might
be the count of the query issued to the web-corpus ?
the 2-gram q = ha or q = ah depending on the or-
der of h and a in the sentence. However, it turns out
that there are several problems with this approach.
First, rather than a single all-purpose feature like
ADJ, the utility of such query counts will vary ac-
cording to aspects like the parts-of-speech of h and
a (because a high adjacency count is not equally in-
formative for all kinds of attachments). Hence, we
add more refined affinity features that are specific
to each pair of POS tags, i.e. ADJ ? POS(h) ?
POS(a). The values of these POS-specific features,
however, are still derived from the same queries as
before. Second, using real-valued features did not
work as well as binning the query-counts (we used
b = floor(logr(count)/5) ? 5) and then firing in-
dicator features ADJ ? POS(h) ? POS(a) ? b for
values of b defined by the query count. Adding still
more complex features, we conjoin to the preceding
features the order of the words h and a as they occur
in the sentence, and the (binned) distance between
them. For features which mark distances, wildcards
(?) are used in the query q = h ? a, where the num-
ber of wildcards allowed in the query is proportional
to the binned distance between h and a in the sen-
tence. Finally, we also include unigram variants of
the above features, which are sensitive to only one of
the head or argument. For all features used, we add
cumulative variants where indicators are fired for all
count bins b? up to query count bin b.
2.2 Paraphrase Features
In addition to measuring counts of the words present
in the sentence, there exist clever ways in which
paraphrases and other accidental indicators can help
resolve specific ambiguities, some of which are dis-
cussed in Nakov and Hearst (2005a), Nakov and
Hearst (2005b). For example, finding attestations of
eat : spaghetti with sauce suggests a nominal attach-
ment in Jean ate spaghetti with sauce. As another
example, one clue that the example in Figure 1 is
695
a verbal attachment is that the proform paraphrase
raising it from is commonly attested. Similarly, the
attestation of be noun prep suggests nominal attach-
ment.
These paraphrase features hint at the correct at-
tachment decision by looking for web n-grams
with special contexts that reveal syntax superficially.
Again, while effective in their isolated disambigua-
tion tasks, past work has been limited by both the
range of attachments considered and the need to in-
tuit these special contexts. For instance, frequency
of the pattern The noun prep suggests noun attach-
ment and of the pattern verb adverb prep suggests
verb attachment for the preposition in the phrase
verb noun prep, but these features were not in the
manually brainstormed list.
In this work, we automatically generate a large
number of paraphrase-style features for arbitrary at-
tachment ambiguities. To induce our list of fea-
tures, we first mine useful context words. We take
each (correct) training dependency relation (h, a)
and consider web n-grams of the form cha, hca,
and hac. Aggregating over all h and a (of a given
POS pair), we determine which context words c are
most frequent in each position. For example, for h =
raising and a = from (see Figure 1), we look at web
n-grams of the form raising c from and see that one
of the most frequent values of c on the web turns out
to be the word it.
Once we have collected context words (for each
position p in {BEFORE, MIDDLE, AFTER}), we
turn each context word c into a collection of features
of the form PARA ? POS(h) ? POS(a) ? c ? p ?
dir, where dir is the linear order of the attachment
in the sentence. Note that h and a are head and ar-
gument words and so actually occur in the sentence,
but c is a context word that generally does not. For
such features, the queries that determine their val-
ues are then of the form cha, hca, and so on. Con-
tinuing the previous example, if the test set has a
possible attachment of two words like h = lower-
ing and a = with, we will fire a feature PARA ?
VBG ? IN ? it ? MIDDLE ? ? with value (indi-
cator bins) set according to the results of the query
lowering it with. The idea is that if frequent oc-
currences of raising it from indicated a correct at-
tachment between raising and from, frequent occur-
rences of lowering it with will indicate the correct-
ness of an attachment between lowering and with.
Finally, to handle the cases where no induced con-
text word is helpful, we also construct abstracted
versions of these paraphrase features where the con-
text words c are collapsed to their parts-of-speech
POS(c), obtained using a unigram-tagger trained on
the parser training set. As discussed in Section 5, the
top features learned by our learning algorithm dupli-
cate the hand-crafted configurations used in previous
work (Nakov and Hearst, 2005b) but also add nu-
merous others, and, of course, apply to many more
attachment types.
3 Working with Web n-Grams
Previous approaches have generally used search en-
gines to collect count statistics (Lapata and Keller,
2004; Nakov and Hearst, 2005b; Nakov and Hearst,
2008). Lapata and Keller (2004) uses the number
of page hits as the web-count of the queried n-
gram (which is problematic according to Kilgarriff
(2007)). Nakov and Hearst (2008) post-processes
the first 1000 result snippets. One challenge with
this approach is that an external search API is now
embedded into the parser, raising issues of both
speed and daily query limits, especially if all pos-
sible attachments trigger queries. Such methods
also create a dependence on the quality and post-
processing of the search results, limitations of the
query process (for instance, search engines can ig-
nore punctuation (Nakov and Hearst, 2005b)).
Rather than working through a search API (or
scraper), we use an offline web corpus ? the Google
n-gram corpus (Brants and Franz, 2006) ? which
contains English n-grams (n = 1 to 5) and their ob-
served frequency counts, generated from nearly 1
trillion word tokens and 95 billion sentences. This
corpus allows us to efficiently access huge amounts
of web-derived information in a compressed way,
though in the process it limits us to local queries.
In particular, we only use counts of n-grams of the
form x ? y where the gap length is ? 3.
Our system requires the counts from a large col-
lection of these n-gram queries (around 4.5 million).
The most basic queries are counts of head-argument
pairs in contiguous h a and gapped h ? a configura-
tions.2 Here, we describe how we process queries
2Paraphrase features give situations where we query ? h a
696
of the form (q1, q2) with some number of wildcards
in between. We first collect all such queries over
all trees in preprocessing (so a new test set requires
a new query-extraction phase). Next, we exploit a
simple but efficient trie-based hashing algorithm to
efficiently answer all of them in one pass over the
n-grams corpus.
Consider Figure 4, which illustrates the data
structure which holds our queries. We first create
a trie of the queries in the form of a nested hashmap.
The key of the outer hashmap is the first word q1
of the query. The entry for q1 points to an inner
hashmap whose key is the final word q2 of the query
bigram. The values of the inner map is an array of
4 counts, to accumulate each of (q1q2), (q1 ? q2),
(q1 ? ?q2), and (q1 ? ? ? q2), respectively. We use k-
grams to collect counts of (q1...q2) with gap length
= k? 2, i.e. 2-grams to get count(q1q2), 3-grams to
get count(q1 ? q2) and so on.
With this representation of our collection of
queries, we go through the web n-grams (n = 2 to
5) one by one. For an n-gram w1...wn, if the first n-
gram word w1 doesn?t occur in the outer hashmap,
we move on. If it does match (say q?1 = w1), then
we look into the inner map for q?1 and check for the
final word wn. If we have a match, we increment the
appropriate query?s result value.
In similar ways, we also mine the most frequent
words that occur before, in between and after the
head and argument query pairs. For example, to col-
lect mid words, we go through the 3-gramsw1w2w3;
if w1 matches q?1 in the outer hashmap and w3 oc-
curs in the inner hashmap for q?1, then we store w2
and the count of the 3-gram. After the sweep, we
sort the context words in decreasing order of count.
We also collect unigram counts of the head and ar-
gument words by sweeping over the unigrams once.
In this way, our work is linear in the size of the
n-gram corpus, but essentially constant in the num-
ber of queries. Of course, if the number of queries is
expected to be small, such as for a one-off parse of
a single sentence, other solutions might be more ap-
propriate; in our case, a large-batch setting, the num-
ber of queries was such that this formulation was
chosen. Our main experiments (with no paralleliza-
tion) took 115 minutes to sweep over the 3.8 billion
and h a ?; these are handled similarly.
????? 
????? 
Web N-grams Query Count-Trie 
counts 
???? 
????? 
?????? 
??????? 
??????? 
SCA
N 
???? hash 
???? hash 
Figure 4: Trie-based nested hashmap for collecting ngram web-
counts of queries.
n-grams (n = 1 to 5) to compute the answers to 4.5
million queries, much less than the time required to
train the baseline parsers.
4 Parsing Experiments
Our features are designed to be used in full-sentence
parsing rather than for limited decisions about iso-
lated ambiguities. We first integrate our features into
a dependency parser, where the integration is more
natural and pushes all the way into the underlying
dynamic program. We then add them to a constituent
parser in a reranking approach. We also verify that
our features contribute on top of standard reranking
features.3
4.1 Dependency Parsing
For dependency parsing, we use the
discriminatively-trained MSTParser4, an im-
plementation of first and second order MST parsing
models of McDonald et al (2005) and McDonald
and Pereira (2006). We use the standard splits of
Penn Treebank into training (sections 2-21), devel-
opment (section 22) and test (section 23). We used
the ?pennconverter?5 tool to convert Penn trees from
constituent format to dependency format. Following
Koo et al (2008), we used the MXPOST tagger
(Ratnaparkhi, 1996) trained on the full training data
to provide part-of-speech tags for the development
3All reported experiments are run on all sentences, i.e. with-
out any length limit.
4http://sourceforge.net/projects/mstparser
5This supersedes ?Penn2Malt? and is available at
http://nlp.cs.lth.se/software/treebank converter. We follow
its recommendation to patch WSJ data with NP bracketing by
Vadas and Curran (2007).
697
Order 2 + Web features % Error Redn.
Dev (sec 22) 92.1 92.7 7.6%
Test (sec 23) 91.4 92.0 7.0%
Table 1: UAS results for English WSJ dependency parsing. Dev
is WSJ section 22 (all sentences) and Test is WSJ section 23
(all sentences). The order 2 baseline represents McDonald and
Pereira (2006).
and the test set, and we used 10-way jackknifing to
generate tags for the training set.
We added our first-order Web-scale features to
the MSTParser system to evaluate improvement over
the results of McDonald and Pereira (2006).6 Ta-
ble 1 shows unlabeled attachments scores (UAS)
for their second-order projective parser and the im-
proved numbers resulting from the addition of our
Web-scale features. Our first-order web-scale fea-
tures show significant improvement even over their
non-local second-order features.7 Additionally, our
web-scale features are at least an order of magnitude
fewer in number than even their first-order base fea-
tures.
4.2 Constituent Parsing
We also evaluate the utility of web-scale features
on top of a state-of-the-art constituent parser ? the
Berkeley parser (Petrov et al, 2006), an unlexical-
ized phrase-structure parser. Because the underly-
ing parser does not factor along lexical attachments,
we instead adopt the discriminative reranking frame-
work, where we generate the top-k candidates from
the baseline system and then rerank this k-best list
using (generally non-local) features.
Our baseline system is the Berkeley parser, from
which we obtain k-best lists for the development set
(WSJ section 22) and test set (WSJ section 23) using
a grammar trained on all the training data (WSJ sec-
tions 2-21).8 To get k-best lists for the training set,
we use 3-fold jackknifing where we train a grammar
6Their README specifies ?training-k:5 iters:10 loss-
type:nopunc decode-type:proj?, which we used for all final ex-
periments; we used the faster ?training-k:1 iters:5? setting for
most development experiments.
7Work such as Smith and Eisner (2008), Martins et al
(2009), Koo and Collins (2010) has been exploring more non-
local features for dependency parsing. It will be interesting to
see how these features interact with our web features.
8Settings: 6 iterations of split and merge with smoothing.
k = 1 k = 2 k = 10 k = 25 k = 50 k = 100
Dev 90.6 92.3 95.1 95.8 96.2 96.5
Test 90.2 91.8 94.7 95.6 96.1 96.4
Table 2: Oracle F1-scores for k-best lists output by Berkeley
parser for English WSJ parsing (Dev is section 22 and Test is
section 23, all lengths).
on 2 folds to get parses for the third fold.9 The ora-
cle scores of the k-best lists (for different values of
k) for the development and test sets are shown in Ta-
ble 2. Based on these results, we used 50-best lists
in our experiments. For discriminative learning, we
used the averaged perceptron (Collins, 2002; Huang,
2008).
Our core feature is the log conditional likelihood
of the underlying parser.10 All other features are in-
dicator features. First, we add all the Web-scale fea-
tures as defined above. These features alone achieve
a 9.2% relative error reduction. The affinity and
paraphrase features contribute about two-fifths and
three-fifths of this improvement, respectively. Next,
we rerank with only the features (both local and
non-local) from Huang (2008), a simplified merge
of Charniak and Johnson (2005) and Collins (2000)
(here configurational). These features alone achieve
around the same improvements over the baseline as
our web-scale features, even though they are highly
non-local and extensive. Finally, we rerank with
both our Web-scale features and the configurational
features. When combined, our web-scale features
give a further error reduction of 3.4% over the con-
figurational reranker (and a combined error reduc-
tion of 12.2%). All results are shown in Table 3.11
5 Analysis
Table 4 shows error counts and relative reductions
that our web features provide over the 2nd-order
dependency baseline. While we do see substantial
gains for classic PP (IN) attachment cases, we see
equal or greater error reductions for a range of at-
tachment types. Further, Table 5 shows how the to-
9Default: we ran the Berkeley parser in its default ?fast?
mode; the output k-best lists are ordered by max-rule-score.
10This is output by the flag -confidence. Note that baseline
results with just this feature are slightly worse than 1-best re-
sults because the k-best lists are generated by max-rule-score.
We report both numbers in Table 3.
11We follow Collins (1999) for head rules.
698
Dev (sec 22) Test (sec 23)
Parsing Model F1 EX F1 EX
Baseline (1-best) 90.6 39.4 90.2 37.3
log p(t|w) 90.4 38.9 89.9 37.3
+ Web features 91.6 42.5 91.1 40.6
+ Configurational features 91.8 43.8 91.1 40.6
+ Web + Configurational 92.1 44.0 91.4 41.4
Table 3: Parsing results for reranking 50-best lists of Berkeley
parser (Dev is WSJ section 22 and Test is WSJ section 23, all
lengths).
Arg Tag # Attach Baseline This Work % ER
NN 5725 5387 5429 12.4
NNP 4043 3780 3804 9.1
IN 4026 3416 3490 12.1
DT 3511 3424 3429 5.8
NNS 2504 2319 2348 15.7
JJ 2472 2310 2329 11.7
CD 1845 1739 1738 -0.9
VBD 1705 1571 1580 6.7
RB 1308 1097 1100 1.4
CC 1000 855 854 -0.7
VB 983 940 945 11.6
TO 868 761 776 14.0
VBN 850 776 786 13.5
VBZ 705 633 629 -5.6
PRP 612 603 606 33.3
Table 4: Error reduction for attachments of various child (argu-
ment) categories. The columns depict the tag, its total attach-
ments as argument, number of correct ones in baseline (Mc-
Donald and Pereira, 2006) and this work, and the relative error
reduction. Results are for dependency parsing on the dev set for
iters:5,training-k:1.
tal errors break down by gold head. For example,
the 12.1% total error reduction for attachments of an
IN argument (which includes PPs as well as comple-
mentized SBARs) includes many errors where the
gold attachments are to both noun and verb heads.
Similarly, for an NN-headed argument, the major
corrections are for attachments to noun and verb
heads, which includes both object-attachment am-
biguities and coordination ambiguities.
We next investigate the features that were given
high weight by our learning algorithm (in the con-
stituent parsing case). We first threshold features
by a minimum training count of 400 to focus on
frequently-firing ones (recall that our features are
not bilexical indicators and so are quite a bit more
Arg Tag % Error Redn for Various Parent Tags
NN IN: 18, NN: 23, VB: 30, NNP:20, VBN: 33
IN NN: 11, VBD: 11, NNS: 20, VB:18, VBG: 23
NNS IN: 9, VBD: 29, VBP: 21, VB:15, CC: 33
Table 5: Error reduction for each type of parent attachment for
a given child in Table 4.
POShead POSarg Example (head, arg)
RB IN back? into
NN IN review? of
NN DT The? rate
NNP IN Regulation? of
VB NN limit? access
VBD NN government? cleared
NNP NNP Dean? Inc
NN TO ability? to
JJ IN active? for
NNS TO reasons? to
IN NN under? pressure
NNS IN reports? on
NN NNP Warner? studio
NNS JJ few? plants
Table 6: The highest-weight features (thresholded at a count of
400) of the affinity schema. We list only the head and argu-
ment POS and the direction (arrow from head to arg). We omit
features involving punctuation.
frequent). We then sort them by descending (signed)
weight.
Table 6 shows which affinity features received the
highest weights, as well as examples of training set
attachments for which the feature fired (for concrete-
ness), suppressing both features involving punctua-
tion and the features? count and distance bins. With
the standard caveats that interpreting feature weights
in isolation is always to be taken for what it is,
the first feature (RB?IN) indicates that high counts
for an adverb occurring adjacent to a preposition
(like back into the spotlight) is a useful indicator
that the adverb actually modifies that preposition.
The second row (NN?IN) indicates that whether a
preposition is appropriate to attach to a noun is well
captured by how often that preposition follows that
noun. The fifth row (VB?NN) indicates that when
considering an NP as the object of a verb, it is a good
sign if that NP?s head frequently occurs immediately
following that verb. All of these features essentially
state cases where local surface counts are good indi-
699
POShead mid-word POSarg Example (head, arg)
VBN this IN leaned, from
VB this IN publish, in
VBG him IN using, as
VBG them IN joining, in
VBD directly IN converted, into
VBD held IN was, in
VBN jointly IN offered, by
VBZ it IN passes, in
VBG only IN consisting, of
VBN primarily IN developed, for
VB us IN exempt, from
VBG this IN using, as
VBD more IN looked, like
VB here IN stay, for
VBN themselves IN launched, into
VBG down IN lying, on
Table 7: The highest-weight features (thresholded at a count of
400) of the mid-word schema for a verb head and preposition
argument (with head on left of argument).
cators of (possibly non-adjacent) attachments.
A subset of paraphrase features, which in the
automatically-extracted case don?t really correspond
to paraphrases at all, are shown in Table 7. Here
we show features for verbal heads and IN argu-
ments. The mid-words m which rank highly are
those where the occurrence of hma as an n-gram
is a good indicator that a attaches to h (m of course
does not have to actually occur in the sentence). In-
terestingly, the top such features capture exactly the
intuition from Nakov and Hearst (2005b), namely
that if the verb h and the preposition a occur with
a pronoun in between, we have evidence that a at-
taches to h (it certainly can?t attach to the pronoun).
However, we also see other indicators that the prepo-
sition is selected for by the verb, such as adverbs like
directly.
As another example of known useful features
being learned automatically, Table 8 shows the
previous-context-word paraphrase features for a
noun head and preposition argument (N ? IN).
Nakov and Hearst (2005b) suggested that the attes-
tation of be N IN is a good indicator of attachment to
the noun (the IN cannot generally attach to forms of
auxiliaries). One such feature occurs on this top list
? for the context word have ? and others occur far-
ther down. We also find their surface marker / punc-
bfr-word POShead POSarg Example (head, arg)
second NN IN season, in
The NN IN role, of
strong NN IN background, in
our NNS IN representatives, in
any NNS IN rights, against
A NN IN review, of
: NNS IN Results, in
three NNS IN years, in
In NN IN return, for
no NN IN argument, about
current NN IN head, of
no NNS IN plans, for
public NN IN appearance, at
from NNS IN sales, of
net NN IN revenue, of
, NNS IN names, of
you NN IN leave, in
have NN IN time, for
some NN IN money, for
annual NNS IN reports, on
Table 8: The highest-weight features (thresholded at a count of
400) of the before-word schema for a noun head and preposition
argument (with head on left of argument).
tuation cues of : and , preceding the noun. However,
we additionally find other cues, most notably that if
the N IN sequence occurs following a capitalized de-
terminer, it tends to indicate a nominal attachment
(in the n-gram, the preposition cannot attach left-
ward to anything else because of the beginning of
the sentence).
In Table 9, we see the top-weight paraphrase fea-
tures that had a conjunction as a middle-word cue.
These features essentially say that if two heads w1
and w2 occur in the direct coordination n-gram w1
and w2, then they are good heads to coordinate (co-
ordination unfortunately looks the same as comple-
mentation or modification to a basic dependency
model). These features are relevant to a range of
coordination ambiguities.
Finally, Table 10 depicts the high-weight, high-
count general paraphrase-cue features for arbitrary
head and argument categories, with those shown
in previous tables suppressed. Again, many inter-
pretable features appear. For example, the top entry
(the JJ NNS) shows that when considering attaching
an adjective a to a noun h, it is a good sign if the
700
POShead mid-CC POSarg Example (head, arg)
NNS and NNS purchases, sales
VB and VB buy, sell
NN and NN president, officer
NN and NNS public, media
VBD and VBD said, added
VBZ and VBZ makes, distributes
JJ and JJ deep, lasting
IN and IN before, during
VBD and RB named, now
VBP and VBP offer, need
Table 9: The highest-weight features (thresholded at a count
of 400) of the mid-word schema where the mid-word was a
conjunction. For variety, for a given head-argument POS pair,
we only list features corresponding to the and conjunction and
h? a direction.
trigram the a h is frequent ? in that trigram, the ad-
jective attaches to the noun. The second entry (NN
- NN) shows that one noun is a good modifier of
another if they frequently appear together hyphen-
ated (another punctuation-based cue mentioned in
previous work on noun bracketing, see Nakov and
Hearst (2005a)). While they were motivated on sep-
arate grounds, these features can also compensate
for inapplicability of the affinity features. For exam-
ple, the third entry (VBD this NN) is a case where
even if the head (a VBD like adopted) actually se-
lects strongly for the argument (a NN like plan), the
bigram adopted plan may not be as frequent as ex-
pected, because it requires a determiner in its mini-
mal analogous form adopted the plan.
6 Conclusion
Web features are a way to bring evidence from a
large unlabeled corpus to bear on hard disambigua-
tion decisions that are not easily resolvable based on
limited parser training data. Our approach allows re-
vealing features to be mined for the entire range of
attachment types and then aggregated and balanced
in a full parsing setting. Our results show that these
web features resolve ambiguities not correctly han-
dled by current state-of-the-art systems.
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful suggestions. This research is sup-
POSh POSa mid/bfr-word Example (h, a)
NNS JJ b = the other? things
NN NN m = - auto? maker
VBD NN m = this adopted? plan
NNS NN b = of computer? products
NN DT m = current the? proposal
VBG IN b = of going? into
NNS IN m = ? clusters? of
IN NN m = your In? review
TO VB b = used to? ease
VBZ NN m = that issue? has
IN NNS m = two than? minutes
IN NN b = used as? tool
IN VBD m = they since? were
VB TO b = will fail? to
Table 10: The high-weight high-count (thresholded at a count of
2000) general features of the mid and before paraphrase schema
(examples show head and arg in linear order with arrow from
head to arg).
ported by BBN under DARPA contract HR0011-06-
C-0022.
References
M. Atterer and H. Schutze. 2007. Prepositional phrase
attachment without oracles. Computational Linguis-
tics, 33(4):469476.
Thorsten Brants and Alex Franz. 2006. The Google Web
1T 5-gram corpus version 1.1. LDC2006T13.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of ACL.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25?70.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of ICML.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL.
701
Adam Kilgarriff. 2007. Googleology is bad science.
Computational Linguistics, 33(1).
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL.
Mirella Lapata and Frank Keller. 2004. The Web as a
baseline: Evaluating the performance of unsupervised
Web-based models for a range of NLP tasks. In Pro-
ceedings of HLT-NAACL.
M. Lauer. 1995. Corpus statistics meet the noun com-
pound: some empirical results. In Proceedings of
ACL.
Andre? F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formula-
tions for dependency parsing. In Proceedings of ACL-
IJCNLP.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL.
Preslav Nakov and Marti Hearst. 2005a. Search en-
gine statistics beyond the n-gram: Application to noun
compound bracketing. In Proceedings of CoNLL.
Preslav Nakov and Marti Hearst. 2005b. Using the web
as an implicit training set: Application to structural
ambiguity resolution. In Proceedings of EMNLP.
Preslav Nakov and Marti Hearst. 2008. Solving rela-
tional similarity problems using the web as a corpus.
In Proceedings of ACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and
Interpretable Tree Annotation. In Proceedings of
COLING-ACL.
Emily Pitler, Shane Bergsma, Dekang Lin, , and Kenneth
Church. 2010. Using web-scale n-grams to improve
base NP parsing performance. In Proceedings of COL-
ING.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of
EMNLP.
David Vadas and James R. Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proceedings
of ACL.
Martin Volk. 2001. Exploiting the WWW as a corpus to
resolve PP attachment ambiguities. In Proceedings of
Corpus Linguistics.
Alexander Yates, Stefan Schoenmackers, and Oren Et-
zioni. 2006. Detecting parser errors using web-based
semantic filters. In Proceedings of EMNLP.
702
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1308?1317,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Gappy Phrasal Alignment by Agreement
Mohit Bansal?
UC Berkeley, CS Division
mbansal@cs.berkeley.edu
Chris Quirk
Microsoft Research
chrisq@microsoft.com
Robert C. Moore
Google Research
robert.carter.moore@gmail.com
Abstract
We propose a principled and efficient phrase-
to-phrase alignment model, useful in machine
translation as well as other related natural lan-
guage processing problems. In a hidden semi-
Markov model, word-to-phrase and phrase-
to-word translations are modeled directly by
the system. Agreement between two direc-
tional models encourages the selection of par-
simonious phrasal alignments, avoiding the
overfitting commonly encountered in unsu-
pervised training with multi-word units. Ex-
panding the state space to include ?gappy
phrases? (such as French ne ? pas) makes the
alignment space more symmetric; thus, it al-
lows agreement between discontinuous align-
ments. The resulting system shows substantial
improvements in both alignment quality and
translation quality over word-based Hidden
Markov Models, while maintaining asymptot-
ically equivalent runtime.
1 Introduction
Word alignment is an important part of statisti-
cal machine translation (MT) pipelines. Phrase
tables containing pairs of source and target lan-
guage phrases are extracted from word alignments,
forming the core of phrase-based statistical ma-
chine translation systems (Koehn et al, 2003).
Most syntactic machine translation systems extract
synchronous context-free grammars (SCFGs) from
aligned syntactic fragments (Galley et al, 2004;
Zollmann et al, 2006), which in turn are de-
rived from bilingual word alignments and syntactic
?Author was a summer intern at Microsoft Research during
this project.
French
English
voudrais voyager par chemin de fer
would like traveling by railroad
ne pas
not
Figure 1: French-English pair with complex word alignment.
parses. Alignment is also used in various other NLP
problems such as entailment, paraphrasing, question
answering, summarization and spelling correction.
A limitation to word-based alignment is undesir-
able. As seen in the French-English example in Fig-
ure 1, many sentence pairs are naturally aligned with
multi-word units in both languages (chemin de fer;
would ? like, where ? indicates a gap). Much work
has addressed this problem: generative models for
direct phrasal alignment (Marcu and Wong, 2002),
heuristic word-alignment combinations (Koehn et
al., 2003; Och and Ney, 2003), models with pseudo-
word collocations (Lambert and Banchs, 2006; Ma
et al, 2007; Duan et al, 2010), synchronous gram-
mar based approaches (Wu, 1997), etc. Most have a
large state-space, using constraints and approxima-
tions for efficient inference.
We present a new phrasal alignment model based
on the hidden Markov framework (Vogel et al,
1996). Our approach is semi-Markov: each state can
generate multiple observations, representing word-
to-phrase alignments. We also augment the state
space to include contiguous sequences. This cor-
responds to phrase-to-word and phrase-to-phrase
alignments. We generalize alignment by agreement
(Liang et al, 2006) to this space, and find that agree-
ment discourages EM from overfitting. Finally, we
make the alignment space more symmetric by in-
cluding gappy (or non-contiguous) phrases. This al-
lows agreement to reinforce non-contiguous align-
1308
f1
f2
f3
e1 e2 e3 f1 f2 f3
e1
e2
e3
Observations? 
?
?
S
ta
tes?
 
HMM(E|F) HMM(F|E)
Figure 2: The model of E given F can represent the phrasal
alignment {e1, e2} ? {f1}. However, the model of F given
E cannot: the probability mass is distributed between {e1} ?
{f1} and {e2} ? {f1}. Agreement of the forward and back-
ward HMM alignments tends to place less mass on phrasal links
and greater mass on word-to-word links.
ments, such English not to French ne ? pas. Prun-
ing the set of allowed phrases preserves the time
complexity of the word-to-word HMM alignment
model.
1.1 Related Work
Our first major influence is that of conditional
phrase-based models. An early approach by Deng
and Byrne (2005) changed the parameterization of
the traditional word-based HMM model, modeling
subsequent words from the same state using a bi-
gram model. However, this model changes only the
parameterization and not the set of possible align-
ments. More closely related are the approaches
of Daume? III and Marcu (2004) and DeNero et
al. (2006), which allow phrase-to-phrase alignments
between the source and target domain. As DeN-
ero warns, though, an unconstrained model may
overfit using unusual segmentations. Interestingly,
the phrase-based hidden semi-Markov model of
Andre?s-Ferrer and Juan (2009) does not seem to
encounter these problems. We suspect two main
causes: first, the model interpolates with Model 1
(Brown et al, 1994), which may help prevent over-
fitting, and second, the model is monotonic, which
screens out many possible alignments. Monotonic-
ity is generally undesirable, though: almost all par-
allel sentences exhibit some reordering phenomena,
even when languages are syntactically very similar.
The second major inspiration is alignment by
agreement by Liang et al (2006). Here, soft inter-
section between the forward (F?E) and backward
(E?F) alignments during parameter estimation pro-
duces better word-to-word correspondences. This
unsupervised approach produced alignments with
incredibly low error rates on French-English, though
only moderate gains in end-to-end machine transla-
tion results. Likely this is because the symmetric
portion of the HMM space contains only single word
to single word links. As shown in Figure 2, in order
to retain the phrasal link f1 ? e1, e2 after agree-
ment, we need the reverse phrasal link e1, e2 v f1
in the backward direction. However, this is not pos-
sible in a word-based HMM where each observa-
tion must be generated by a single state. Agreement
tends to encourage 1-to-1 alignments with very high
precision and but lower recall. As each word align-
ment acts as a constraint on phrase extraction, the
phrase-pairs obtained from those alignments have
high recall and low precision.
2 Gappy Phrasal Alignment
Our goal is to unify phrasal alignment and align-
ment by agreement. We use a phrasal hidden semi-
Markov alignment model, but without the mono-
tonicity requirement of Andre?s-Ferrer and Juan
(2009). Since phrases may be used in both the state
and observation space of both sentences, agreement
during EM training no longer penalizes phrasal links
such as those in Figure 2. Moreover, the benefits of
agreement are preserved: meaningful phrasal links
that are likely in both directions of alignment will be
reinforced, while phrasal links likely in only one di-
rection will be discouraged. This avoids segmenta-
tion problems encountered by DeNero et al (2006).
Non-contiguous sequences of words present an
additional challenge. Even a semi-Markov model
with phrases can represent the alignment between
English not and French ne ? pas in one direction
only. To make the model more symmetric, we ex-
tend the state space to include gappy phrases as
well.1 The set of alignments in each model becomes
symmetric, though the two directions model gappy
phrases differently. Consider not and ne ? pas:
when predicting French given English, the align-
ment corresponds to generating multiple distinct ob-
1We only allow a single gap with one word on each end.
This is sufficient for the vast majority of the gapped phenomena
that we have seen in our training data.
1309
voudrais
voyager
par
chemin
de
fer
wo
ul
d
lik
e
tra
ve
lin
g
by ra
ilr
oa
d
C
would
like
traveling
by
railroad
vo
ud
ra
is
vo
ya
ge
r
pa
r
ch
em
in
de fe
r
no
t
pas
ne
not
ne pa
s
Observations? 
S
ta
tes?
 
Observations? 
S
ta
tes?
 
Figure 3: Example English-given-French and French-given-English alignments of the same sentence pair using the Hidden Semi-
Markov Model (HSMM) for gapped-phrase-to-phrase alignment. It allows the state side phrases (denoted by vertical blocks),
observation side phrases (denoted by horizontal blocks), and state-side gaps (denoted by discontinuous blocks in the same column
connected by a hollow vertical ?bridge?). Note both directions can capture the desired alignment for this sentence pair.
servations from the same state; in the other direction,
the word not is generated by a single gappy phrase
ne ? pas. Computing posteriors for agreement is
somewhat complicated, so we resort to an approx-
imation described later. Exact inference retains a
low-order polynomial runtime; we use pruning to in-
crease speed.
2.1 Hidden Markov Alignment Models
Our model can be seen as an extension of the stan-
dard word-based Hidden Markov Model (HMM)
used in alignment (Vogel et al, 1996). To
ground the discussion, we first review the struc-
ture of that model. This generative model has
the form p(O|S) =
?
A p(A,O|S), where S =
(s1, . . . , sI) ? ?? is a sequence of words from a
vocabulary ?; O = (o1, . . . , oJ) ? ?? is a sequence
from vocabulary ?; and A = (a1, . . . , aJ) is the
alignment between the two sequences. Since some
words are systematically inserted during translation,
the target (state) word sequence is augmented with
a special NULL word. To retain the position of the
last aligned word, the state space contains I copies
of the NULL word, one for each position (Och and
Ney, 2003). The alignment uses positive positions
for words and negative positions for NULL states, so
aj ? {1..I} ? {?1..? I}, and si = NULL if i < 0.
It uses the following generative procedure. First
the length of the observation sequence is selected
based on pl(J |I). Then for each observation posi-
tion, the state is selected based on the prior state: a
null state with probability p0, or a non-null state at
position aj with probability (1 ? p0) ? pj(aj |aj?1)
where pj is a jump distribution. Finally the observa-
tion word oj at that position is generated with prob-
ability pt(oj |saj ), where pt is an emission distribu-
tion:
p(A,O|S) = pl(J |I)
J?
j=1
pj(aj |aj?1)pt(oj |saj )
pj(a|a
?) =
{
(1? p0) ? pd(a? |a?|) a > 0
p0 ? ?(|a|, |a?|) a < 0
We pick p0 using grid search on the development
set, pl is uniform, and the pj and pt are optimized by
EM.2
2.2 Gappy Semi-Markov Models
The HMM alignment model identifies a word-
to-word correspondence between the observation
2Note that jump distances beyond -10 or 10 share a single
parameter to prevent sparsity.
1310
words and the state words. We make two changes
to expand this model. First, we allow contiguous
phrases on the observation side, which makes the
model semi-Markov: at each time stamp, the model
may emit more than one observation word. Next, we
also allow contiguous and gappy phrases on the state
side, leading to an alignment model that can retain
phrasal links after agreement (see Section 4).
The S and O random variables are unchanged.
Since a single state may generate multiple observa-
tion words, we add a new variable K representing
the number of states. K should be less than J , the
number of observations. The alignment variable is
augmented to allow contiguous and non-contiguous
ranges of words. We allow only a single gap, but of
unlimited length. The null state is still present, and
is again represented by negative numbers.
A =(a1, . . . , aK) ? A(I)
A(I) ={(i1, i2, g)|0 < i1 ? i2 ? I,
g ? {GAP, CONTIG}}?
{(?i,?i, CONTIG) | 0 < i ? I}
We add one more random variable to capture the to-
tal number of observations generated by each state.
L ? {(l0, l1, . . . , lK) | 0 = l0 < ? ? ? < lK = J}
The generative model takes the following form:
p(A,L,O|S) =pl(J |I)pf (K|J)
K?
k=1
pj(ak|ak?1)?
pt(lk, o
lk
lk?1+1
|S[ak], lk?1)
First, the length of the observation sequence (J)
is selected, based on the number of words in the
state-side sentence (I). Since it does not affect the
alignment, pl is modeled as a uniform distribution.
Next, we pick the total number of states to use (K),
which must be less than the number of observations
(J). Short state sequences receive an exponential
penalty: pf (K|J) ? ?(J?K) if 0 ? K ? J , or 0
otherwise. A harsh penalty (small positive value of
?) may prevent the systematic overuse of phrases.3
3We found that this penalty was crucial to prevent overfitting
in independent training. Joint training with agreement made it
basically unnecessary.
Next we decide the assignment of each state.
We retain the first-order Markov assumption: the
selection of each state is conditioned only on the
prior state. The transition distribution is identical
to the word-based HMM for single word states. For
phrasal and gappy states, we jump into the first word
of that state, and out of the last word of that state,
and then pay a cost according to how many words
are covered within that state. If a = (i1, i2, g), then
the beginning word of a is F (a) = i1, the end-
ing word is L(a) = i2, and the length N(a) is 2
for gapped states, 0 for null states, and last(a) ?
first(a) + 1 for all others. The transition probabil-
ity is:
pj(a|a
?) =
?
??
??
p0 ? ?(|F (a)|, |L(a?)|) if F (a) < 0
(1? p0)pd(F (a)? |L(a?)|)?
pn(N(a)) otherwise
where pn(c) ? ?c is an exponential distribution. As
in the word HMM case, we use a mixture parameter
p0 to determine the likelihood of landing in a NULL
state. The position of that NULL state remembers the
last position of the prior state. For non-null words,
we pick the first word of the state according to the
distance from the last word of the prior state. Finally,
we pick a length for that final state according to an
exponential distribution: values of ? less than one
will penalize the use of phrasal states.
For each set of state words, we maintain an emis-
sion distribution over observation word sequences.
Let S[a] be the set of state words referred to by
the alignment variable a. For example, the English
given French alignment of Figure 3 includes the fol-
lowing state word sets:
S[(2, 2, CONTIG)] = voudrais
S[(1, 3, GAP)] = ne ? pas
S[(6, 8, CONTIG)] = chemin de fer
For the emission distribution we keep a multinomial
over observation phrases for each set of state words:
p(l, oll? |S[a], l
?) ? c(oll? |S[a])
In contrast to the approach of Deng and Byrne
(2005), this encourages greater consistency across
instances, and more closely resembles the com-
monly used phrasal translation models.
1311
We note in passing that pf (K|J) may be moved
inside the product: pf (K|J) ? ?(J?K) =
?K
k=1 ?
(lk?lk?1?1). The following form derived us-
ing the above rearrangement is helpful during EM.
p(A,L,O|S) ?
K?
k=1
pj(ak|ak?1)?
pt(lk, o
lk
lk?1+1
|S[ak], lk?1)?
?(lk?lk?1?1)
where lk ? lk?1 ? 1 is the length of the observation
phrase emitted by state S[ak].
2.3 Minimality
At alignment time we focus on finding the minimal
phrase pairs, under the assumption that composed
phrase pairs can be extracted in terms of these min-
imal pairs. We are rather strict about this, allowing
only 1 ? k and k ? 1 phrasal alignment edges
(or links). This should not cause undue stress, since
edges of the form 2 ? 3 (say e1e2 ? f1f2f3) can
generally be decomposed into 1 ? 1 ? 1 ? 2 (i.e.,
e1 ? f1 ? e2 ? f2f3), etc. However, the model
does not require this to be true: we will describe re-
estimation for unconstrained general models, but use
the limited form for word alignment.
3 Parameter Estimation
We use Expectation-Maximization (EM) to estimate
parameters. The forward-backward algorithm effi-
ciently computes posteriors of transitions and emis-
sions in the word-based HMM. In a standard HMM,
emission always advances the observation position
by one, and the next transition is unaffected by
the emission. Neither of these assumptions hold
in our model: multiple observations may be emit-
ted at a time, and a state may cover multiple state-
side words, which affects the outgoing transition. A
modified dynamic program computes posteriors for
this generalized model.
The following formulation of the forward-
backward algorithm for word-to-word alignment is
a good starting point. ?[x, 0, y] indicates the total
mass of paths that have just transitioned into state y
at observation x but have not yet emitted; ?[x, 1, y]
represents the mass after emission but before subse-
quent transition. ? is defined similarly. (We omit
NULL states for brevity; the extension is straightfor-
ward.)
?[0, 0, y] = pj(y|INIT)
?[x, 1, y] = ?[x, 0, y] ? pt(ox|sy)
?[x, 0, y] =
?
y?
?[x? 1, 1, y?] ? pj(y|y
?)
?[n, 1, y] = 1
?[x, 0, y] = pt(ox|sy) ? ?[x, 1, y]
?[x, 1, y] =
?
y?
pj(y
?|y) ? ?[x+ 1, 0, y?]
Not only is it easy to compute posteriors of both
emissions (?[x, 0, y]pt(ox|sy)?[x, 1, y]) and transi-
tions (?[x, 1, y]pj(y?|y)?[x+ 1, 0, y?]) with this for-
mulation, it also simplifies the generalization to
complex emissions. We update the emission forward
probabilities to include a search over the possible
starting points in the state and observation space:
?[0, 0, y] =pj(y|INIT)
?[x, 1, y] =
?
x?<x,y??y
?[x?, 0, y?] ? EMIT(x? : x, y? : y)
?[x, 0, y] =
?
y?
?[x? 1, 1, y?] ? pj(y|y
?)
?[n, 1, y] =1
?[x?, 0, y?] =
?
x?<x,y??y
EMIT(x? : x, y? : y) ? ?[x, 1, y]
?[x, 1, y] =
?
y?
pj(y
?|y) ? ?[x+ 1, 0, y?]
Phrasal and gapped emissions are pooled into EMIT:
EMIT(w : x, y : z) =pt(o
x
w|s
z
y) ? ?
z?y+1 ? ?x?w+1+
pt(o
x
w|sy ? sz) ? ?
2 ? ?x?w+1
The transition posterior is the same as above. The
emission is very similar: the posterior probability
that oxw is aligned to s
z
y is proportional to ?[w, 0, y] ?
pt(oxw|s
z
y) ??
z?y+1 ??x?w+1 ??[x, 1, z]. For a gapped
phrase, the posterior is proportional to ?[w, 0, y] ?
pt(oxw|sy ? sz) ? ?
2 ? ?x?w+1 ? ?[x, 1, z].
Given an inference procedure for computing pos-
teriors, unsupervised training with EM follows im-
mediately. We use a simple maximum-likelihood
update of the parameters using expected counts
based on the posterior distribution.
1312
4 Alignment by Agreement
Following Liang et al (2006), we quantify agree-
ment between two models as the probability that the
alignments produced by the two models agree on the
alignment z of a sentence pair x = (S,O):
?
z
p1(z|x; ?1)p2(z|x; ?2)
To couple the two models, the (log) probability of
agreement is added to the standard log-likelihood
objective:
max
?1,?2
?
x
[
log p1(x; ?1) + log p2(x; ?2)+
log
?
z
p1(z|x; ?1)p2(z|x; ?2)
]
We use the heuristic estimator from Liang et al
(2006), letting q be a product of marginals:
E : q(z; x) :=
?
z?z
p1(z|x; ?1)p2(z|x; ?2)
where each pk(z|x; ?k) is the posterior marginal of
some edge z according to each model. Such a
heuristic E step computes the marginals for each
model separately, then multiplies the marginals cor-
responding to the same edge. This product of
marginals acts as the approximation to the posterior
used in the M step for each model. The intuition is
that if the two models disagree on a certain edge z,
then the marginal product is small, hence that edge
is dis-preferred in each model.
Contiguous phrase agreement. It is simple to
extend agreement to alignments in the absence of
gaps. Multi-word (phrasal) links are assigned some
posterior probability in both models, as shown in the
example in Figure 3, and we multiply the posteriors
of these phrasal links just as in the single word case.4
?F?E(fi, ej) := ?E?F (ej , fi)
:= [?F?E(fi, ej)? ?E?F (ej , fi)]
4Phrasal correspondences can be represented in multiple
ways: multiple adjacent words could be generated from the
same state either using one semi-Markov emission, or using
multiple single word emissions followed by self-jumps. Only
the first case is reinforced through agreement, so the latter is
implicitly discouraged. We explored an option to forbid same-
state transitions, but found it made little difference in practice.
Gappy phrase agreement. When we introduce
gappy phrasal states, agreement becomes more chal-
lenging. In the forward direction F?E, if we have a
gappy state aligned to an observation, say fi ? fj ?
ek, then its corresponding edge in the backward di-
rection E?F would be ek v fi ? fj . How-
ever, this is represented by two distinct and unre-
lated emissions. Although it is possible the compute
the posterior probability of two non-adjacent emis-
sions, this requires running a separate dynamic pro-
gram for each such combination to sum the mass be-
tween these emissions. For the sake of efficiency
we resort to an approximate computation of pos-
terior marginals using the two word-to-word edges
ek v fi and ek v fj .
The forward posterior ?F?E for edge fi ? fj ?
ek is multiplied with the min of the backward pos-
teriors of the edges ek v fi and ek v fj .
?F?E(fi ? fj , ek) := ?F?E(fi ? fj , ek)?
min
{
?E?F (ek, fi), ?E?F (ek, fj)
}
Note that this min is an upper bound on the desired
posterior of edge ek v fi ? fj , since every path
that passes through ek v fi and ek v fj must pass
through ek v fi, therefore the posterior of ek v
fi ? fj is less than that of ek v fi, and likewise less
than that of ek v fj .
The backward posteriors of the edges ek v fi and
ek v fj are also mixed with the forward posteriors
of the edges to which they correspond.
?E?F (ek, fi) := ?E?F (ek, fi)?
[
?F?E(fi, ek)+
?
h<i<j
{
?F?E(fh ? fi, ek) + ?F?E(fi ? fj , ek)
}
]
5 Pruned Lists of ?Allowed? Phrases
To identify contiguous and gapped phrases that are
more likely to lead to good alignments, we use word-
to-word HMM alignments from the full training data
in both directions (F?E and E?F). We collect ob-
servation phrases of length 2 toK aligned to a single
state, i.e. oji ? s, to add to a list of allowed phrases.
For gappy phrases, we find all non-consecutive ob-
servation pairs oi and oj such that: (a) both are
1313
aligned to the same state sk, (b) state sk is aligned to
only these two observations, and (c) at least one ob-
servation between oi and oj is aligned to a non-null
state other than sk. These observation phrases are
collected from F?E and E?F models to build con-
tiguous and gappy phrase lists for both languages.
Next, we order the phrases in each contiguous list
using the discounted probability:
p?(o
j
i ? s|o
j
i ) =
max(0, count(oji ? s)? ?)
count(oji )
where count(oji ? s) is the count of occurrence of
the observation-phrase oji , all aligned to some sin-
gle state s, and count(oji ) is the count of occur-
rence of the observation phrase oji , not all necessar-
ily aligned to a single state. Similarly, we rank the
gappy phrases using the discounted probability:
p?(oi ? oj ? s|oi ? oj) =
max(0, count(oi ? oj ? s)? ?)
count(oi ? oj)
where count(oi ? oj ? s) is the count of occur-
rence of the observations oi and oj aligned to a sin-
gle state s with the conditions mentioned above, and
count(oi ? oj) is the count of general occurrence of
the observations oi and oj in order. We find that 200
gappy phrases and 1000 contiguous phrases works
well, based on tuning with a development set.
6 Complexity Analysis
Let m be the length of the state sentence S and n
be the length of the observation sentence O. In IBM
Model 1 (Brown et al, 1994), with only a translation
model, we can infer posteriors or max alignments
in O(mn). HMM-based word-to-word alignment
model (Vogel et al, 1996) adds a distortion model,
increasing the complexity to O(m2n).
Introducing phrases (contiguous) on the observa-
tion side, we get a HSMM (Hidden Semi-Markov
Model). If we allow phrases of length no greater
than K, then the number of observation types
rises from n to Kn for an overall complexity of
O(m2Kn). Introducing state phrases (contiguous)
with length ? K grows the number of state types
from m to Km. Complexity further increases to
O((Km)2Kn) = O(K3m2n).
Finally, when we introduce gappy state phrases of
the type si ? sj , the number of such phrases is
O(m2), since we may choose a start and end point
independently. Thus, the total complexity rises to
O((Km + m2)2Kn) = O(Km4n). Although this
is less than the O(n6) complexity of exact ITG (In-
version Transduction Grammar) model (Wu, 1997),
a quintic algorithm is often quite slow.
The pruned lists of allowed phrases limit this
complexity. The model is allowed to use observa-
tion (contiguous) and state (contiguous and gappy)
phrases only from these lists. The number of
phrases that match any given sentence pair from
these pruned lists is very small (? 2 to 5). If the
number of phrases in the lists that match the obser-
vation and state side of a given sentence pair are
small constants, the complexity remains O(m2n),
equal to that of word-based models.
7 Results
We evaluate our models based on both word align-
ment and end-to-end translation with two language
pairs: English-French and English-German. For
French-English, we use the Hansards NAACL 2003
shared-task dataset, which contains nearly 1.1 mil-
lion training sentence pairs. We also evaluated
on German-English Europarl data from WMT2010,
with nearly 1.6 million training sentence pairs. The
model from Liang et al (2006) is our word-based
baseline.
7.1 Training Regimen
Our training regimen begins with both the forward
(F?E) and backward (E?F) iterations of Model 1
run independently (i.e. without agreement). Next,
we train several iterations of the forward and back-
ward word-to-word HMMs, again with independent
training. We do not use agreement during word
alignment since it tends to produce sparse 1-1 align-
ments, which in turn leads to low phrase emission
probabilities in the gappy model.
Initializing the emission probabilities of the semi-
Markov model is somewhat complicated, since the
word-based models do not assign any mass to
the phrasal or gapped configurations. Therefore
we use a heuristic method. We first retrieve the
Viterbi alignments of the forward and backward
1314
word-to-word HMM aligners. For phrasal corre-
spondences, we combine these forward and back-
ward Viterbi alignments using a common heuris-
tic (Union, Intersection, Refined, or Grow-Diag-
Final), and extract tight phrase-pairs (no unaligned
words on the boundary) from this alignment set.
We found that Grow-Diag-Final was most effective
in our experiments. The counts gathered from this
phrase extraction are used to initialize phrasal trans-
lation probabilities. For gappy states in a forward
(F?E) model, we use alignments from the back-
ward (E?F) model. If a state sk is aligned to two
non-consecutive observations oi and oj such that sk
is not aligned to any other observation, and at least
one observation between oi and oj is aligned to a
non-null state other than sk, then we reverse this
link to get oi ? oj ? sk and use it as a gapped-
state-phrase instance for adding fractional counts.
Given these approximate fractional counts, we per-
form a standard MLE M-step to initialize the emis-
sion probability distributions. The distortion proba-
bilities from the word-based model are used without
changes.
7.2 Alignment Results (F1)
The validation and test sentences have been hand-
aligned (see Och and Ney (2003)) and are marked
with both sure and possible alignments. For French-
English, following Liang et al (2006), we lowercase
all words, and use the validation set plus the first
100 test sentences as our development set and the
remaining 347 test-sentences as our test-set for fi-
nal F1 evaluation.5 In German-English, we have a
development set of 102 sentences, and a test set of
258 sentences, also annotated with a set of sure and
possible alignments. Given a predicted alignmentA,
precision and recall are computed using sure align-
ments S and possible alignments P (where S ? P )
as in Och and Ney (2003):
Precision =
|A ? P |
|A|
? 100%
Recall =
|A ? S|
|S|
? 100%
5We report F1 rather than AER because AER appears not to
correlate well with translation quality.(Fraser and Marcu, 2007)
Language pair Word-to-word Gappy
French-English 34.0 34.5
German-English 19.3 19.8
Table 2: BLEU results on German-English and French-English.
AER =
(
1?
|A ? S|+ |A ? P |
|A|+ |S|
)
? 100%
F1 =
2? Precision?Recall
Precision+Recall
? 100%
Many free parameters were tuned to optimize
alignment F1 on the development set, including the
number of iterations of each Model 1, HMM, and
Gappy; the NULL weight p0, the number of con-
tiguous and gappy phrases to include, and the max-
imum phrase length. Five iterations of all models,
p0 = 0.3, using the top 1000 contiguous phrases
and the top 200 gappy phrases, maximum phrase
length of 5, and penalties ? = ? = 1 produced
competitive results. Note that by setting ? and ? to
one, we have effectively removed the penalty alto-
gether without affecting our results. In Table 1 we
see a consistent improvement with the addition of
contiguous phrases, and some additional gains with
gappy phrases.
7.3 Translation Results (BLEU)
We assembled a phrase-based system from the align-
ments (using only contiguous phrases consistent
with the potentially gappy alignment), with 4 chan-
nel models, word and phrase count features, dis-
tortion penalty, lexicalized reordering model, and a
5-gram language model, weighted by MERT. The
same free parameters from above were tuned to opti-
mize development set BLEU using grid search. The
improvements in Table 2 are encouraging, especially
as a syntax-based or non-contiguous phrasal system
(Galley and Manning, 2010) may benefit more from
gappy phrases.
8 Conclusions and Future Work
We have described an algorithm for efficient unsu-
pervised alignment of phrases. Relatively straight-
forward extensions to the base HMM allow for ef-
ficient inference, and agreement between the two
1315
Data Decoding method Word-to-word +Contig phrases +Gappy phrases
FE 10K Viterbi 89.7 90.6 90.3
FE 10K Posterior ? 0.1 90.1 90.4 90.7
FE 100K Viterbi 93.0 93.6 93.8
FE 100K Posterior ? 0.1 93.1 93.7 93.8
FE All Viterbi 94.1 94.3 94.3
FE All Posterior ? 0.1 94.2 94.4 94.5
GE 10K Viterbi 76.2 79.6 79.7
GE 10K Posterior ? 0.1 76.7 79.3 79.3
GE 100K Viterbi 81.0 83.0 83.2
GE 100K Posterior ? 0.1 80.7 83.1 83.4
GE All Viterbi 83.0 85.2 85.6
GE All Posterior ? 0.1 83.7 85.3 85.7
Table 1: F1 scores of automatic word alignments, evaluated on the test set of the hand-aligned sentence pairs.
models prevents EM from overfitting, even in the ab-
sence of harsh penalties. We also allow gappy (non-
contiguous) phrases on the state side, which makes
agreement more successful but agreement needs ap-
proximation of posterior marginals. Using pruned
lists of good phrases, we maintain complexity equal
to the baseline word-to-word model.
There are several steps forward from this point.
Limiting the gap length also prevents combinato-
rial explosion; we hope to explore this in future
work. Clearly a translation system that uses discon-
tinuous mappings at runtime (Chiang, 2007; Gal-
ley and Manning, 2010) may make better use of
discontinuous alignments. This model can also be
applied at the morpheme or character level, allow-
ing joint inference of segmentation and alignment.
Furthermore the state space could be expanded and
enhanced to include more possibilities: states with
multiple gaps might be useful for alignment in lan-
guages with template morphology, such as Arabic or
Hebrew. More exploration in the model space could
be useful ? a better distortion model might place a
stronger distribution on the likely starting and end-
ing points of phrases.
Acknowledgments
We would like to thank the anonymous reviewers for
their helpful suggestions. This project is funded by
Microsoft Research.
References
Jesu?s Andre?s-Ferrer and Alfons Juan. 2009. A phrase-
based hidden semi-Markov approach to machine trans-
lation. In Proceedings of EAMT.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1994. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263?311.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics.
Hal Daume? III and Daniel Marcu. 2004. A phrase-based
HMM approach to document/abstract alignment. In
Proceedings of EMNLP.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In Proceedings of ACL.
Yonggang Deng and William Byrne. 2005. HMM word
and phrase alignment for statistical machine transla-
tion. In Proceedings of HLT-EMNLP.
Xiangyu Duan, Min Zhang, and Haizhou Li. 2010.
Pseudo-word for phrase-based machine translation. In
Proceedings of ACL.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. Computational Linguistics, 33(3):293?303.
Michel Galley and Christopher D. Manning. 2010. Ac-
curate non-hierarchical phrase-based translation. In
HLT/NAACL.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT-NAACL.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Sta-
tistical Phrase-Based Translation. In Proceedings of
HLT-NAACL.
Patrik Lambert and Rafael Banchs. 2006. Grouping
multi-word expressions according to part-of-speech in
1316
statistical machine translation. In Proc. of the EACL
Workshop on Multi-Word-Expressions in a Multilin-
gual Context.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL.
Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007.
Boostrapping word alignment via word packing. In
Proceedings of ACL.
Daniel Marcu and Daniel Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29:19?51.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of COLING.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?404.
Andreas Zollmann, Ashish Venugopal, and Stephan Vo-
gel. 2006. Syntax augmented machine translation via
chart parsing. In Processings of the Statistical Ma-
chine Translation Workshop at NAACL.
1317
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 720?725,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
The Surprising Variance in Shortest-Derivation Parsing
Mohit Bansal and Dan Klein
Computer Science Division
University of California, Berkeley
{mbansal,klein}@cs.berkeley.edu
Abstract
We investigate full-scale shortest-derivation
parsing (SDP), wherein the parser selects an
analysis built from the fewest number of train-
ing fragments. Shortest derivation parsing
exhibits an unusual range of behaviors. At
one extreme, in the fully unpruned case, it
is neither fast nor accurate. At the other ex-
treme, when pruned with a coarse unlexical-
ized PCFG, the shortest derivation criterion
becomes both fast and surprisingly effective,
rivaling more complex weighted-fragment ap-
proaches. Our analysis includes an investi-
gation of tie-breaking and associated dynamic
programs. At its best, our parser achieves an
accuracy of 87% F1 on the English WSJ task
with minimal annotation, and 90% F1 with
richer annotation.
1 Introduction
One guiding intuition in parsing, and data-driven
NLP more generally, is that, all else equal, it is ad-
vantageous to memorize large fragments of training
examples. Taken to the extreme, this intuition sug-
gests shortest derivation parsing (SDP), wherein a
test sentence is analyzed in a way which uses as few
training fragments as possible (Bod, 2000; Good-
man, 2003). SDP certainly has appealing properties:
it is simple and parameter free ? there need not even
be an explicit lexicon. However, SDP may be too
simple to be competitive.
In this paper, we consider SDP in both its pure
form and with several direct modifications, finding a
range of behaviors. In its pure form, with no prun-
ing or approximation, SDP is neither fast nor accu-
rate, achieving less than 70% F1 on the English WSJ
task. Moreover, basic tie-breaking variants and lexi-
cal augmentation are insufficient to achieve compet-
itive accuracies.1 On the other hand, SDP is dramat-
ically improved in both speed and accuracy when
a simple, unlexicalized PCFG is used for coarse-
to-fine pruning (and tie-breaking). On the English
WSJ, the coarse PCFG and the fine SDP together
achieve 87% F1 with basic treebank annotation (see
Table 2) and up to 90% F1 with richer treebank an-
notation (see Table 4).
The main contribution of this work is to analyze
the behavior of shortest derivation parsing, showing
both when it fails and when it succeeds. Our final
parser, which combines a simple PCFG coarse pass
with an otherwise pure SPD fine pass, can be quite
accurate while being straightforward to implement.
2 Implicit Grammar for SDP
The all-fragments grammar (AFG) for a (binarized)
treebank is formally the tree-substitution grammar
(TSG) (Resnik, 1992; Bod, 1993) that consists of
all fragments (elementary trees) of all training trees
in the treebank, with some weighting on each frag-
ment. AFGs are too large to fully extract explicitly;
researchers therefore either work with a tractable
subset of the fragments (Sima?an, 2000; Bod, 2001;
Post and Gildea, 2009; Cohn and Blunsom, 2010) or
use a PCFG reduction like that of Goodman (1996a),
in which each treebank node token Xi is given its
own unique grammar symbol.
We follow Bansal and Klein (2010) in choosing
the latter, both to permit comparison to their results
and because SDP is easily phrased as a PCFG re-
duction. Bansal and Klein (2010) use a carefully pa-
1Bod (2000) presented another SDP parser, but with a sam-
pled subset of the training fragments.
720
rameterized weighting of the substructures in their
grammar in an effort to extend the original DOP1
model (Bod, 1993; Goodman, 1996a). However, for
SDP, the grammar is even simpler (Goodman, 2003).
In principle, the implicit SDP grammar needs just
two rule schemas: CONTINUE (Xp ? Yq Zr) and
SWITCH (Xp ? Xq), with additive costs 0 and 1,
respectively. CONTINUE rules walk along training
trees, while SWITCH rules change between trees for
a unit cost.2 Assuming that the SWITCH rules are in
practice broken down into BEGIN and END sub-rules
as in Bansal and Klein (2010), the grammar is linear
in the size of the treebank.3 Note that no lexicon
is needed in this grammar: lexical switches are like
any other.
A derivation in our grammar has weight (cost) w
where w is the number of switches (or the num-
ber of training fragments minus one) used to build
the derivation (see Figure 1). The Viterbi dy-
namic program for finding the shortest derivation is
quite simple: it requires CKY to store only byte-
valued switch-counts s(Xp, i, j) (i.e., the number
of switches) for each chart item and compute the
derivation with the least switch-count. Specifically,
in the dynamic program, if we use a SWITCH rule
Xp ? Xq, then we update
s(Xp, i, j) := s(Xq, i, j) + 1.
If we use a continue rule Xp ? Yq Zr, then the up-
date is
s(Xp, i, j) := s(Yq, i, k) + s(Zr, k, j),
where k is a split point in the chart. Using this
dynamic program, we compute the exact shortest
derivation parse in the full all-fragments grammar
(which is reduced to a PCFG with 2 rules schemas
as described above).
3 Basic SDP: Inaccurate and Slow
SDP in its most basic form is appealingly simple,
but has two serious issues: it is both slow and in-
accurate. Because there are millions of grammar
2This grammar is a very minor variant of the reduction of
SDP suggested by Goodman (2003).
3For a compact WSJ training set with graph packing (see
Bansal and Klein (2010)) and one level of parent annotation
and markovization, our grammar has 0.9 million indexed sym-
bols compared to 7.5 million unbinarized (and 0.75 million bi-
narized) explicitly-extracted fragments of just depth 1 and 2.
Test Sentence 
Test Parse 
The  girl 
Training Data 
DT-2 
The girl 
NP-4 
DT-5 NN-6 
girl The 
NP-1 
DT-2 NN-3 
Derivation 2 Derivation 1 
NP 
DT NN 
The girl 
NP-1 
DT-2 NN-3 
The girl 
NP-4 
DT-5 
A girl 
NN-6 
SWITCH 
Figure 1: SDP - the best parse corresponds to the shortest
derivation (fewest switches).
symbols, exact SDP parsing takes more than 45 sec-
onds per sentence in our implementation (in addition
to being highly memory-intensive). Many methods
exist for speeding up parsing through approxima-
tion, but basic SDP is too inaccurate to merit them.
When implemented as described in Section 2, SDP
achieves only 66% F1 on the WSJ task (dev set, ?
40 words).
Why does SDP perform so poorly? One reason
for low accuracy may be that there are many short-
est derivations, i.e. derivations that are all built with
the fewest number of fragments, and that tie break-
ing could be at fault. To investigate this, we tried
various methods for tie-breaking: FIRST/LAST (pro-
cedurally break ties), UNIFORM (sample derivations
equally), FREQ (use the frequency of local rules).
However, none of these methods help much, giv-
ing results within a percentage of F1. In fact, even
oracle tie-breaking, where ties are broken to favor
the number of gold constituents in the derivation
achieves only 80% F1, indicating that correct deriva-
tions are often not the shortest ones. Another rea-
son for the poor performance of SDP may be that
the parameter-free treatment of the lexical layer is
particularly pathological. Indeed, this hypothesis is
partially verified by the result that using a lexicon
(similar to that in Petrov et al (2006)) at the termi-
nal layer brings the uniform tie-breaking result up to
80% F1. However, combining a lexicon with oracle
tie-breaking yields only 81.8% F1.
These results at first seem quite discouraging, but
we will show that they can be easily improved with
information from even a simple PCFG.
721
4 Improvements from a Coarse PCFG
The additional information that makes shortest
derivation parsing work comes from a coarse un-
lexicalized PCFG. In the standard way, our PCFG
consists of the local (depth-1) rules X ? Y Z with
probability P (Y Z|X) computed using the count
of the rule and the count of the nonterminal X in
the given treebank (no smoothing was used). Our
coarse grammar uses a lexicon with unknown word
classes, similar to that in Petrov et al (2006). When
taken from a binarized treebank with one level of
parent annotation (Johnson, 1998) and horizontal
markovization, the PCFG is quite small, with around
3500 symbols and 25000 rules; it achieves an accu-
racy of 84% on its own (see Table 2), so the PCFG
on its own is better than the basic SDP, but still rela-
tively weak.
When filtered by a coarse PCFG pass, how-
ever, SDP becomes both fast and accurate, even for
the basic, lexicon-free SDP formulation. Summed
marginals (posteriors) are computed in the coarse
PCFG and used for pruning and tie-breaking in the
SDP chart, as described next. Pruning works in the
standard coarse-to-fine (CTF) way (see Charniak et
al. (2006)). If a particular base symbol X is pruned
by the PCFG coarse pass for a particular span (i, j)
(i.e., the posterior marginal P (X, i, j|s) is less than
a certain threshold), then in the full SDP pass we do
not allow building any indexed symbol Xl of type X
for span (i, j). In all our pruning-based experiments,
we use a log posterior threshold of ?3.8, tuned on
the WSJ development set.
We also use the PCFG coarse pass for tie-
breaking. During Viterbi shortest-derivation pars-
ing (after coarse-pruning), if two derivations have
the same cost (i.e., the number of switches), then we
break the tie between them by choosing the deriva-
tion which has a higher sum of coarse posteriors
(i.e., the sum of the coarse PCFG chart-cell pos-
teriors P (X, i, j|s) used to build the derivation).4
The coarse PCFG has an extremely beneficial in-
teraction with the fine all-fragments SDP grammar,
wherein the accuracy of the combined grammars
is significantly higher than either individually (see
4This is similar to the maximum recall objective for approx-
imate inference (Goodman, 1996b). The product of posteriors
also works equally well.
dev (? 40) test (? 40)
Model F1 EX F1 EX
B&K2010 pruned 88.4 33.7 88.5 33.0
B&K2010 unpruned 87.9 32.4 88.1 31.9
Table 1: Accuracy (F1) and exact match (EX) for Bansal and
Klein (2010). The pruned row shows their original results with
coarse-to-fine pruning. The unpruned row shows new results
for an unpruned version of their parser; these accuracies are
very similar to their pruned counterparts.
Table 2). In addition, the speed of parsing and
memory-requirements improve by more than an or-
der of magnitude over the exact SDP pass alone.
It is perhaps surprising that coarse-pass pruning
improves accuracy by such a large amount for SDP.
Indeed, given that past all-fragments work has used
a coarse pass for speed, and that we are the first (to
our knowledge) to actually parse at scale with an
implicit grammar without such a coarse pass, it is
a worry that previous results could be crucially de-
pendent on fortuitous coarse-pass pruning. To check
one such result, we ran the full, weighted AFG con-
struction of Bansal and Klein (2010) without any
pruning (using the maximum recall objective as they
did). Their results hold up without pruning: the re-
sults of the unpruned version are only around 0.5%
less (in parsing F1) than the results achieved with
pruning (see Table 1). However, in the case of our
shortest-derivation parser, the coarse-pass is essen-
tial for high accuracies (and for speed and memory,
as always).
5 Results
We have seen that basic, unpruned SDP is both slow
and inaccurate, but improves greatly when comple-
mented by a coarse PCFG pass; these results are
shown in Table 2. Shortest derivation parsing with a
PCFG coarse-pass (PCFG+SDP) achieves an accu-
racy of nearly 87% F1 (on the WSJ test set, ? 40
word sentences), which is significantly higher than
the accuracy of the PCFG or SDP alone.5 When
the coarse PCFG is combined with basic SDP, the
majority of the improvement comes from pruning
with the coarse-posteriors; tie-breaking with coarse-
posteriors contributes around 0.5% F1 over pruning.
5PCFG+SDP accuracies are around 3% higher in F1 and
10% higher in EX than the PCFG-only accuracies.
722
dev (? 40) test (? 40) test (all)
Model F1 EX F1 EX F1 EX
SDP 66.2 18.0 66.9 18.4 64.9 17.3
PCFG 83.8 20.0 84.0 21.6 83.2 20.1
PCFG+SDP 86.4 30.6 86.9 31.5 86.0 29.4
Table 2: Our primary results on the WSJ task. SDP is the
basic unpruned shortest derivation parser. PCFG results are
with one level of parent annotation and horizontal markoviza-
tion. PCFG+SDP incorporates the coarse PCFG posteriors into
SDP. See end of Section 5 for a comparison to other parsing
approaches.
Figure 2 shows the number of fragments for short-
est derivation parsing (averaged for each sentence
length). Note that the number of fragments is of
course greater for the combined PCFG+SDP model
than the exact basic SDP model (which is guaranteed
to be minimal). This result provides some analysis
of how coarse-pruning helps SDP: it illustrates that
the coarse-pass filters out certain short but inaccu-
rate derivations (that the minimal SDP on its own is
forced to choose) to improve performance.
Figure 3 shows the parsing accuracy of the
PCFG+SDP model for various pruning thresholds
in coarse-to-fine pruning. Note how this is differ-
ent from the standard coarse-pass pruning graphs
(see Charniak et al (1998), Petrov and Klein (2007),
Bansal and Klein (2010)) where only a small im-
provement is achieved from pruning. In contrast,
coarse-pass pruning provides large accuracy benefits
here, perhaps because of the unusual complementar-
ity of the two grammars (typical coarse passes are
designed to be as similar as possible to their fine
counterparts, even explicitly so in Petrov and Klein
(2007)).
Our PCFG+SDP parser is more accurate than re-
cent sampling-based TSG?s (Post and Gildea, 2009;
Cohn and Blunsom, 2010), who achieve 83-85% F1,
and it is competitive with more complex weighted-
fragment approaches.6 See Bansal and Klein (2010)
for a more thorough comparison to other parsing
work. In addition to being accurate, the PCFG+SDP
parser is simple and fast, requiring negligible train-
ing and tuning. It takes 2 sec/sentence, less than 2
GB of memory and is written in less than 2000 lines
6Bansal and Klein (2010) achieve around 1.0% higher F1
than our results without a lexicon (character-level parsing) and
1.5% higher F1 with a lexicon.
0
5
10
15
20
25
0 4 8 12 16 20 24 28 32 36 40
# o
f fra
gmen
ts
 
sentence length  
PCFG + SDP
SDP
Figure 2: The average number of fragments in shortest deriva-
tion parses, computed using the basic version (SDP) and the
pruned version (PCFG+SDP), for WSJ dev-set (? 40 words).
65.0
7 0.0
75.0
8 0.0
85.0
9 0.0
-3 -5 -7 -9 -11 -13 -15 -17Coarse-pass Log Posterior Threshold (PT)  
F1 
No Pruning (PT = -inf)  
Figure 3: Parsing accuracy for various coarse-pass pruning
thresholds (on WSJ dev-set ? 40 words). A larger threshold
means more pruning. These are results without the coarse-
posterior tie-breaking to illustrate the sole effect of pruning.
of Java code, including I/O.7
5.1 Other Treebanks
One nice property of the parameter-free, all-
fragments SDP approach is that we can easily trans-
fer it to any new domain with a treebank, or any
new annotation of an existing treebank. Table 3
shows domain adaptation performance by the re-
sults for training and testing on the Brown and
German datasets.8 On Brown, we perform better
than the relatively complex lexicalized Model 1 of
Collins (1999). For German, our parser outperforms
Dubey (2005) and we are not far behind latent-
variable parsers, for which parsing is substantially
7These statistics can be further improved with standard pars-
ing micro-optimization.
8See Gildea (2001) and Petrov and Klein (2007) for the ex-
act experimental setup that we followed here.
723
test (? 40) test (all)
Model F1 EX F1 EX
BROWN
Gildea (2001) 84.1 ? ? ?
This Paper (PCFG+SDP) 84.7 34.6 83.1 32.6
GERMAN
Dubey (2005) 76.3 ? ? ?
Petrov and Klein (2007) 80.8 40.8 80.1 39.1
This Paper (PCFG+SDP) 78.1 39.3 77.1 38.2
Table 3: Results for training and testing on the Brown and
German treebanks. Gildea (2001) uses the lexicalized Collins?
Model 1 (Collins, 1999).
test (? 40) test (all)
Annotation F1 EX F1 EX
STAN-ANNOTATION 88.1 34.3 87.4 32.2
BERK-ANNOTATION 90.0 38.9 89.5 36.8
Table 4: Results with richer WSJ-annotations from Stanford
and Berkeley parsers.
more complex.
5.2 Treebank Annotations
PCFG+SDP achieves 87% F1 on the English WSJ
task using basic annotation only (i.e., one level
of parent annotation and horizontal markoviza-
tion). Table 4 shows that by pre-transforming the
WSJ treebank with richer annotation from previ-
ous work, we can obtain state-of-the-art accuracies
of up to 90% F1 with no change to our simple
parser. In STAN-ANNOTATION, we annotate the
treebank symbols with annotations from the Stan-
ford parser (Klein and Manning, 2003). In BERK-
ANNOTATION, we annotate with the splits learned
via hard-EM and 5 split-merge rounds of the Berke-
ley parser (Petrov et al, 2006).
6 Conclusion
Our investigation of shortest-derivation parsing
showed that, in the exact case, SDP performs poorly.
When pruned (and, to a much lesser extent, tie-
broken) by a coarse PCFG, however, it is competi-
tive with a range of other, more complex techniques.
An advantage of this approach is that the fine SDP
pass is actually quite simple compared to typical fine
passes, while still retaining enough complementarity
to the coarse PCFG to increase final accuracies. One
aspect of our findings that may apply more broadly
is the caution that coarse-to-fine methods may some-
times be more critical to end system quality than
generally thought.
Acknowledgments
We would like to thank Adam Pauls, Slav Petrov
and the anonymous reviewers for their helpful sug-
gestions. This research is supported by BBN un-
der DARPA contract HR0011-06-C-0022 and by the
Office of Naval Research under MURI Grant No.
N000140911081.
References
Mohit Bansal and Dan Klein. 2010. Simple, Accurate
Parsing with an All-Fragments Grammar. In Proceed-
ings of ACL.
Rens Bod. 1993. Using an Annotated Corpus as a
Stochastic Grammar. In Proceedings of EACL.
Rens Bod. 2000. Parsing with the Shortest Derivation.
In Proceedings of COLING.
Rens Bod. 2001. What is the Minimal Set of Fragments
that Achieves Maximum Parse Accuracy? In Proceed-
ings of ACL.
Eugene Charniak, Sharon Goldwater, and Mark Johnson.
1998. Edge-Based Best-First Chart Parsing. In Pro-
ceedings of the 6th Workshop on Very Large Corpora.
Eugene Charniak, Mark Johnson, et al 2006. Multi-
level Coarse-to-fine PCFG Parsing. In Proceedings of
HLT-NAACL.
Trevor Cohn and Phil Blunsom. 2010. Blocked Inference
in Bayesian Tree Substitution Grammars. In Proceed-
ings of NAACL.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia.
A. Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In ACL ?05.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Proceedings of EMNLP.
Joshua Goodman. 1996a. Efficient Algorithms for Pars-
ing the DOP Model. In Proceedings of EMNLP.
Joshua Goodman. 1996b. Parsing Algorithms and Met-
rics. In Proceedings of ACL.
Joshua Goodman. 2003. Efficient parsing of DOP with
PCFG-reductions. In Bod R, Scha R, Sima?an K (eds.)
Data-Oriented Parsing. University of Chicago Press,
Chicago, IL.
724
Mark Johnson. 1998. PCFG Models of Linguistic Tree
Representations. Computational Linguistics, 24:613?
632.
Dan Klein and Christopher Manning. 2003. Accurate
Unlexicalized Parsing. In Proceedings of ACL.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Proceedings of NAACL-
HLT.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and
Interpretable Tree Annotation. In Proceedings of
COLING-ACL.
Matt Post and Daniel Gildea. 2009. Bayesian Learning
of a Tree Substitution Grammar. In Proceedings of
ACL-IJCNLP.
Philip Resnik. 1992. Probabilistic Tree-Adjoining
Grammar as a Framework for Statistical Natural Lan-
guage Processing. In Proceedings of COLING.
Khalil Sima?an. 2000. Tree-gram Parsing: Lexical De-
pendencies and Structural Relations. In Proceedings
of ACL.
725
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 389?398,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Coreference Semantics from Web Features
Mohit Bansal and Dan Klein
Computer Science Division
University of California, Berkeley
{mbansal,klein}@cs.berkeley.edu
Abstract
To address semantic ambiguities in corefer-
ence resolution, we use Web n-gram features
that capture a range of world knowledge in a
diffuse but robust way. Specifically, we ex-
ploit short-distance cues to hypernymy, se-
mantic compatibility, and semantic context, as
well as general lexical co-occurrence. When
added to a state-of-the-art coreference base-
line, our Web features give significant gains on
multiple datasets (ACE 2004 and ACE 2005)
and metrics (MUC and B3), resulting in the
best results reported to date for the end-to-end
task of coreference resolution.
1 Introduction
Many of the most difficult ambiguities in corefer-
ence resolution are semantic in nature. For instance,
consider the following example:
When Obama met Jobs, the president dis-
cussed the economy, technology, and educa-
tion. His election campaign is expected to [...]
For resolving coreference in this example, a sys-
tem would benefit from the world knowledge that
Obama is the president. Also, to resolve the pro-
noun his to the correct antecedent Obama, we can
use the knowledge that Obama has an election cam-
paign while Jobs does not. Such ambiguities are
difficult to resolve on purely syntactic or configu-
rational grounds.
There have been multiple previous systems that
incorporate some form of world knowledge in coref-
erence resolution tasks. Most work (Poesio et
al., 2004; Markert and Nissim, 2005; Yang et
al., 2005; Bergsma and Lin, 2006) addresses spe-
cial cases and subtasks such as bridging anaphora,
other anaphora, definite NP reference, and pronoun
resolution, computing semantic compatibility via
Web-hits and counts from large corpora. There
is also work on end-to-end coreference resolution
that uses large noun-similarity lists (Daume? III and
Marcu, 2005) or structured knowledge bases such as
Wikipedia (Yang and Su, 2007; Haghighi and Klein,
2009; Kobdani et al, 2011) and YAGO (Rahman
and Ng, 2011). However, such structured knowledge
bases are of limited scope, and, while Haghighi and
Klein (2010) self-acquires knowledge about corefer-
ence, it does so only via reference constructions and
on a limited scale.
In this paper, we look to the Web for broader if
shallower sources of semantics. In order to harness
the information on the Web without presupposing
a deep understanding of all Web text, we instead
turn to a diverse collection of Web n-gram counts
(Brants and Franz, 2006) which, in aggregate, con-
tain diffuse and indirect, but often robust, cues to
reference. For example, we can collect the co-
occurrence statistics of an anaphor with various can-
didate antecedents to judge relative surface affinities
(i.e., (Obama, president) versus (Jobs, president)).
We can also count co-occurrence statistics of com-
peting antecedents when placed in the context of an
anaphoric pronoun (i.e., Obama?s election campaign
versus Jobs? election campaign).
All of our features begin with a pair of head-
words from candidate mention pairs and compute
statistics derived from various potentially informa-
tive queries? counts. We explore five major cat-
egories of semantically informative Web features,
based on (1) general lexical affinities (via generic
co-occurrence statistics), (2) lexical relations (via
Hearst-style hypernymy patterns), (3) similarity of
entity-based context (e.g., common values of y for
389
which h is a y is attested), (4) matches of distribu-
tional soft cluster ids, and (5) attested substitutions
of candidate antecedents in the context of a pronom-
inal anaphor.
We first describe a strong baseline consisting of
the mention-pair model of the Reconcile system
(Stoyanov et al, 2009; Stoyanov et al, 2010) us-
ing a decision tree (DT) as its pairwise classifier. To
this baseline system, we add our suite of features
in turn, each class of features providing substantial
gains. Altogether, our final system produces the best
numbers reported to date on end-to-end coreference
resolution (with automatically detected system men-
tions) on multiple data sets (ACE 2004 and ACE
2005) and metrics (MUC and B3), achieving signif-
icant improvements over the Reconcile DT baseline
and over the state-of-the-art results of Haghighi and
Klein (2010).
2 Baseline System
Before describing our semantic Web features, we
first describe our baseline. The core inference and
features come from the Reconcile package (Stoy-
anov et al, 2009; Stoyanov et al, 2010), with modi-
fications described below. Our baseline differs most
substantially from Stoyanov et al (2009) in using a
decision tree classifier rather than an averaged linear
perceptron.
2.1 Reconcile
Reconcile is one of the best implementations of the
mention-pair model (Soon et al, 2001) of coref-
erence resolution. The mention-pair model relies
on a pairwise function to determine whether or not
two mentions are coreferent. Pairwise predictions
are then consolidated by transitive closure (or some
other clustering method) to form the final set of
coreference clusters (chains). While our Web fea-
tures could be adapted to entity-mention systems,
their current form was most directly applicable to
the mention-pair approach, making Reconcile a par-
ticularly well-suited platform for this investigation.
The Reconcile system provides baseline features,
learning mechanisms, and resolution procedures that
already achieve near state-of-the-art results on mul-
tiple popular datasets using multiple standard met-
rics. It includes over 80 core features that exploit
various automatically generated annotations such as
named entity tags, syntactic parses, and WordNet
classes, inspired by Soon et al (2001), Ng and
Cardie (2002), and Bengtson and Roth (2008). The
Reconcile system also facilitates standardized em-
pirical evaluation to past work.1
In this paper, we develop a suite of simple seman-
tic Web features based on pairs of mention head-
words which stack with the default Reconcile fea-
tures to surpass past state-of-the-art results.
2.2 Decision Tree Classifier
Among the various learning algorithms that Recon-
cile supports, we chose the decision tree classifier,
available in Weka (Hall et al, 2009) as J48, an open
source Java implementation of the C4.5 algorithm of
Quinlan (1993).
The C4.5 algorithm builds decision trees by incre-
mentally maximizing information gain. The train-
ing data is a set of already classified samples, where
each sample is a vector of attributes or features. At
each node of the tree, C4.5 splits the data on an
attribute that most effectively splits its set of sam-
ples into more ordered subsets, and then recurses on
these smaller subsets. The decision tree can then be
used to classify a new sample by following a path
from the root downward based on the attribute val-
ues of the sample.
We find the decision tree classifier to work better
than the default averaged perceptron (used by Stoy-
anov et al (2009)), on multiple datasets using multi-
ple metrics (see Section 4.3). Many advantages have
been claimed for decision tree classifiers, including
interpretability and robustness. However, we sus-
pect that the aspect most relevant to our case is that
decision trees can capture non-linear interactions be-
tween features. For example, recency is very im-
portant for pronoun reference but much less so for
nominal reference.
3 Semantics via Web Features
Our Web features for coreference resolution are sim-
ple and capture a range of diffuse world knowledge.
Given a mention pair, we use the head finder in Rec-
oncile to find the lexical heads of both mentions (for
1We use the default configuration settings of Reconcile
(Stoyanov et al, 2010) unless mentioned otherwise.
390
example, the head of the Palestinian territories is the
word territories). Next, we take each headword pair
(h1, h2) and compute various Web-count functions
on it that can signal whether or not this mention pair
is coreferent.
As the source of Web information, we use the
Google n-grams corpus (Brants and Franz, 2006)
which contains English n-grams (n = 1 to 5) and
their Web frequency counts, derived from nearly 1
trillion word tokens and 95 billion sentences. Be-
cause we have many queries that must be run against
this corpus, we apply the trie-based hashing algo-
rithm of Bansal and Klein (2011) to efficiently an-
swer all of them in one pass over it. The features
that require word clusters (Section 3.4) use the out-
put of Lin et al (2010).2
We describe our five types of features in turn. The
first four types are most intuitive for mention pairs
where both members are non-pronominal, but, aside
from the general co-occurrence group, helped for all
mention pair types. The fifth feature group applies
only to pairs in which the anaphor is a pronoun but
the antecedent is a non-pronoun. Related work for
each feature category is discussed inline.
3.1 General co-occurrence
These features capture co-occurrence statistics of
the two headwords, i.e., how often h1 and h2 are
seen adjacent or nearly adjacent on the Web. This
count can be a useful coreference signal because,
in general, mentions referring to the same entity
will co-occur more frequently (in large corpora) than
those that do not. Using the n-grams corpus (for n
= 1 to 5), we collect co-occurrence Web-counts by
allowing a varying number of wildcards between h1
and h2 in the query. The co-occurrence value is:
bin
(
log10
(
c12
c1 ? c2
))
2These clusters are derived form the V2 Google n-grams
corpus. The V2 corpus itself is not publicly available, but
the clusters are available at http://www.clsp.jhu.edu/
?sbergsma/PhrasalClusters
where
c12 = count(?h1 ? h2?)
+ count(?h1 ? ? h2?)
+ count(?h1 ? ? ? h2?),
c1 = count(?h1?), and
c2 = count(?h2?).
We normalize the overall co-occurrence count of the
headword pair c12 by the unigram counts of the indi-
vidual headwords c1 and c2, so that high-frequency
headwords do not unfairly get a high feature value
(this is similar to computing scaled mutual infor-
mation MI (Church and Hanks, 1989)).3 This nor-
malized value is quantized by taking its log10 and
binning. The actual feature that fires is an indica-
tor of which quantized bin the query produced. As
a real example from our development set, the co-
occurrence count c12 for the headword pair (leader,
president) is 11383, while it is only 95 for the head-
word pair (voter, president); after normalization and
log10, the values are -10.9 and -12.0, respectively.
These kinds of general Web co-occurrence statis-
tics have been used previously for other supervised
NLP tasks such as spelling correction and syntac-
tic parsing (Bergsma et al, 2010; Bansal and Klein,
2011). In coreference, similar word-association
scores were used by Kobdani et al (2011), but from
Wikipedia and for self-training.
3.2 Hearst co-occurrence
These features capture templated co-occurrence of
the two headwords h1 and h2 in the Web-corpus.
Here, we only collect statistics of the headwords co-
occurring with a generalized Hearst pattern (Hearst,
1992) in between. Hearst patterns capture various
lexical semantic relations between items. For ex-
ample, seeing X is a Y or X and other Y indicates
hypernymy and also tends to cue coreference. The
specific patterns we use are:
? h1 {is | are | was | were} {a | an | the}? h2
? h1 {and | or} {other | the other | another} h2
? h1 other than {a | an | the}? h2
3We also tried adding count(?h1 h2?) to c12 but this
decreases performance, perhaps because truly adjacent occur-
rences are often not grammatical.
391
? h1 such as {a | an | the}? h2
? h1 , including {a | an | the}? h2
? h1 , especially {a | an | the}? h2
? h1 of {the| all}? h2
For this feature, we again use a quantized nor-
malized count as in Section 3.1, but c12 here is re-
stricted to n-grams where one of the above patterns
occurs in between the headwords. We did not al-
low wildcards in between the headwords and the
Hearst-patterns because this introduced a significant
amount of noise. Also, we do not constrain the or-
der of h1 and h2 because these patterns can hold
for either direction of coreference.4 As a real ex-
ample from our development set, the c12 count for
the headword pair (leader, president) is 752, while
for (voter, president), it is 0.
Hypernymic semantic compatibility for corefer-
ence is intuitive and has been explored in varying
forms by previous work. Poesio et al (2004) and
Markert and Nissim (2005) employ a subset of our
Hearst patterns and Web-hits for the subtasks of
bridging anaphora, other-anaphora, and definite NP
resolution. Others (Haghighi and Klein, 2009; Rah-
man and Ng, 2011; Daume? III and Marcu, 2005)
use similar relations to extract compatibility statis-
tics from Wikipedia, YAGO, and noun-similarity
lists. Yang and Su (2007) use Wikipedia to auto-
matically extract semantic patterns, which are then
used as features in a learning setup. Instead of ex-
tracting patterns from the training data, we use all
the above patterns, which helps us generalize to new
datasets for end-to-end coreference resolution (see
Section 4.3).
3.3 Entity-based context
For each headword h, we first collect context seeds
y using the pattern
h {is | are | was | were} {a | an | the}? y
taking seeds y in order of decreasing Web count.
The corresponding ordered seed list Y = {y} gives
us useful information about the headword?s entity
type. For example, for h = president, the top
4Two minor variants not listed above are h1 including h2
and h1 especially h2.
30 seeds (and their parts of speech) include impor-
tant cues such as president is elected (verb), pres-
ident is authorized (verb), president is responsible
(adjective), president is the chief (adjective), presi-
dent is above (preposition), and president is the head
(noun).
Matches in the seed lists of two headwords can
be a strong signal that they are coreferent. For ex-
ample, in the top 30 seed lists for the headword
pair (leader, president), we get matches including
elected, responsible, and expected. To capture this
effect, we create a feature that indicates whether
there is a match in the top k seeds of the two head-
words (where k is a hyperparameter to tune).
We create another feature that indicates whether
the dominant parts of speech in the seed lists
matches for the headword pair. We first collect the
POS tags (using length 2 character prefixes to indi-
cate coarse parts of speech) of the seeds matched in
the top k? seed lists of the two headwords, where
k? is another hyperparameter to tune. If the domi-
nant tags match and are in a small list of important
tags ({JJ, NN, RB, VB}), we fire an indicator feature
specifying the matched tag, otherwise we fire a no-
match indicator. To obtain POS tags for the seeds,
we use a unigram-based POS tagger trained on the
WSJ treebank training set.
3.4 Cluster information
The distributional hypothesis of Harris (1954) says
that words that occur in similar contexts tend to have
a similar linguistic behavior. Here, we design fea-
tures with the idea that this hypothesis extends to
reference: mentions occurring in similar contexts
in large document sets such as the Web tend to be
compatible for coreference. Instead of collecting the
contexts of each mention and creating sparse fea-
tures from them, we use Web-scale distributional
clustering to summarize compatibility.
Specifically, we begin with the phrase-based clus-
ters from Lin et al (2010), which were created us-
ing the Google n-grams V2 corpus. These clusters
come from distributional K-Means clustering (with
K = 1000) on phrases, using the n-gram context as
features. The cluster data contains almost 10 mil-
lion phrases and their soft cluster memberships. Up
to twenty cluster ids with the highest centroid sim-
ilarities are included for each phrase in this dataset
392
(Lin et al, 2010).
Our cluster-based features assume that if the
headwords of the two mentions have matches in
their cluster id lists, then they are more compatible
for coreference. We check the match of not just the
top 1 cluster ids, but also farther down in the 20 sized
lists because, as discussed in Lin and Wu (2009),
the soft cluster assignments often reveal different
senses of a word. However, we also assume that
higher-ranked matches tend to imply closer mean-
ings. To this end, we fire a feature indicating the
value bin(i+j), where i and j are the earliest match
positions in the cluster id lists of h1 and h2. Binning
here means that match positions in a close range
generally trigger the same feature.
Recent previous work has used clustering infor-
mation to improve the performance of supervised
NLP tasks such as NER and dependency parsing
(Koo et al, 2008; Lin and Wu, 2009). However, in
coreference, the only related work to our knowledge
is from Daume? III and Marcu (2005), who use word
class features derived from a Web-scale corpus via a
process described in Ravichandran et al (2005).
3.5 Pronoun context
Our last feature category specifically addresses pro-
noun reference, for cases when the anaphoric men-
tion NP2 (and hence its headword h2) is a pronoun,
while the candidate antecedent mention NP1 (and
hence its headword h1) is not. For such a head-
word pair (h1, h2), the idea is to substitute the non-
pronoun h1 into h2?s position and see whether the
result is attested on the Web.
If the anaphoric pronominal mention is h2 and its
sentential context is l? l h2 r r?, then the substituted
phrase will be l? l h1 r r?.5 High Web counts of sub-
stituted phrases tend to indicate semantic compati-
bility. Perhaps unsurprisingly for English, only the
right context was useful in this capacity. We chose
the following three context types, based on perfor-
mance on a development set:
5Possessive pronouns are replaced with an additional apos-
trophe, i.e., h1 ?s. We also use features (see R1Gap) that allow
wildcards (?) in between the headword and the context when
collecting Web-counts, in order to allow for determiners and
other filler words.
? h1 r (R1)
? h1 r r? (R2)
? h1 ? r (R1Gap)
As an example of the R1Gap feature, if the
anaphor h2 + context is his victory and one candidate
antecedent h1 is Bush, then we compute the normal-
ized value
count(?Bush ?s ? victory?)
count(? ? ?s ? victory?)count(?Bush?)
In general, we compute
count(?h1 ?s ? r?)
count(? ? ?s ? r?)count(?h1?)
The final feature value is again a normalized count
converted to log10 and then binned.
6 We have three
separate features for the R1, R2, and R1Gap context
types. We tune a separate bin-size hyperparameter
for each of these three features.
These pronoun resolution features are similar to
selectional preference work by Yang et al (2005)
and Bergsma and Lin (2006), who compute seman-
tic compatibility for pronouns in specific syntactic
relationships such as possessive-noun, subject-verb,
etc. In our case, we directly use the general context
of any pronominal anaphor to find its most compat-
ible antecedent.
Note that all our above features are designed to be
non-sparse by firing indicators of the quantized Web
statistics and not the lexical- or class-based identities
of the mention pair. This keeps the total number of
features small, which is important for the relatively
small datasets used for coreference resolution. We
go from around 100 features in the Reconcile base-
line to around 165 features after adding all our Web
features.
6Normalization helps us with two kinds of balancing. First,
we divide by the count of the antecedent so that when choos-
ing the best antecedent for a fixed anaphor, we are not biased
towards more frequently occurring antecedents. Second, we di-
vide by the count of the context so that across anaphora, an
anaphor with rarer context does not get smaller values (for all its
candidate antecedents) than another anaphor with a more com-
mon context.
393
Dataset docs dev test ment chn
ACE04 128 63/27 90/38 3037 1332
ACE05 81 40/17 57/24 1991 775
ACE05-ALL 599 337/145 482/117 9217 3050
Table 1: Dataset characteristics ? docs: the total number of doc-
uments; dev: the train/test split during development; test: the
train/test split during testing; ment: the number of gold men-
tions in the test split; chn: the number of coreference chains in
the test split.
4 Experiments
4.1 Data
We show results on three popular and comparatively
larger coreference resolution data sets ? the ACE04,
ACE05, and ACE05-ALL datasets from the ACE
Program (NIST, 2004). In ACE04 and ACE05, we
have only the newswire portion (of the original ACE
2004 and 2005 training sets) and use the standard
train/test splits reported in Stoyanov et al (2009)
and Haghighi and Klein (2010). In ACE05-ALL,
we have the full ACE 2005 training set and use the
standard train/test splits reported in Rahman and Ng
(2009) and Haghighi and Klein (2010). Note that
most previous work does not report (or need) a stan-
dard development set; hence, for tuning our fea-
tures and its hyper-parameters, we randomly split
the original training data into a training and devel-
opment set with a 70/30 ratio (and then use the full
original training set during testing). Details of the
corpora are shown in Table 1.7
Details of the Web-scale corpora used for extract-
ing features are discussed in Section 3.
4.2 Evaluation Metrics
We evaluated our work on both MUC (Vilain et al,
1995) and B3 (Bagga and Baldwin, 1998). Both
scorers are available in the Reconcile infrastruc-
ture.8 MUC measures how many predicted clusters
need to be merged to cover the true gold clusters.
B3 computes precision and recall for each mention
by computing the intersection of its predicted and
gold cluster and dividing by the size of the predicted
7Note that the development set is used only for ACE04, be-
cause for ACE05, and ACE05-ALL, we directly test using the
features tuned on ACE04.
8Note that B3 has two versions which handle twinless (spu-
rious) mentions in different ways (see Stoyanov et al (2009) for
details). We use the B3All version, unless mentioned otherwise.
MUC B3
Feature P R F1 P R F1
AvgPerc 69.0 63.1 65.9 82.2 69.9 75.5
DecTree 80.9 61.0 69.5 89.5 69.0 77.9
+ Co-occ 79.8 62.1 69.8 88.7 69.8 78.1
+ Hearst 80.0 62.3 70.0 89.1 70.1 78.5
+ Entity 79.4 63.2 70.4 88.1 70.9 78.6
+ Cluster 79.5 63.6 70.7 87.9 71.2 78.6
+ Pronoun 79.9 64.3 71.3 88.0 71.6 79.0
Table 2: Incremental results for the Web features on the ACE04
development set. AvgPerc is the averaged perceptron baseline,
DecTree is the decision tree baseline, and the +Feature rows
show the effect of adding a particular feature incrementally (not
in isolation) to the DecTree baseline. The feature categories
correspond to those described in Section 3.
and gold cluster, respectively. It is well known
(Recasens and Hovy, 2010; Ng, 2010; Kobdani et
al., 2011) that MUC is biased towards large clus-
ters (chains) whereas B3 is biased towards singleton
clusters. Therefore, for a more balanced evaluation,
we show improvements on both metrics simultane-
ously.
4.3 Results
We start with the Reconcile baseline but employ the
decision tree (DT) classifier, because it has signifi-
cantly better performance than the default averaged
perceptron classifier used in Stoyanov et al (2009).9
Table 2 compares the baseline perceptron results to
the DT results and then shows the incremental addi-
tion of the Web features to the DT baseline (on the
ACE04 development set).
The DT classifier, in general, is precision-biased.
The Web features somewhat balance this by increas-
ing the recall and decreasing precision to a lesser ex-
tent, improving overall F1. Each feature type incre-
mentally increases both MUC and B3 F1-measures,
showing that they are not taking advantage of any
bias of either metric. The incremental improve-
ments also show that each Web feature type brings
in some additional benefit over the information al-
ready present in the Reconcile baseline, which in-
cludes alias, animacy, named entity, and WordNet
9Moreover, a DT classifier takes roughly the same amount of
time and memory as a perceptron on our ACE04 development
experiments. It is, however, slower and more memory-intensive
(?3x) on the bigger ACE05-ALL dataset.
394
MUC B3
System P R F1 P R F1
ACE04-TEST-RESULTS
Stoyanov et al (2009) - - 62.0 - - 76.5
Haghighi and Klein (2009) 67.5 61.6 64.4 77.4 69.4 73.2
Haghighi and Klein (2010) 67.4 66.6 67.0 81.2 73.3 77.0
This Work: Perceptron Baseline 65.5 61.9 63.7 84.1 70.9 77.0
This Work: DT Baseline 76.0 60.7 67.5 89.6 70.3 78.8
This Work: DT + Web Features 74.8 64.2 69.1 87.5 73.7 80.0
This Work: ? of DT+Web over DT (p < 0.05) 1.7 (p < 0.005) 1.3
ACE05-TEST-RESULTS
Stoyanov et al (2009) - - 67.4 - - 73.7
Haghighi and Klein (2009) 73.1 58.8 65.2 82.1 63.9 71.8
Haghighi and Klein (2010) 74.6 62.7 68.1 83.2 68.4 75.1
This Work: Perceptron Baseline 72.2 61.6 66.5 85.0 65.5 73.9
This Work: DT Baseline 79.6 59.7 68.2 89.4 64.2 74.7
This Work: DT + Web Features 75.0 64.7 69.5 81.1 70.8 75.6
This Work: ? of DT+Web over DT (p < 0.12) 1.3 (p < 0.1) 0.9
ACE05-ALL-TEST-RESULTS
Rahman and Ng (2009) 75.4 64.1 69.3 54.4 70.5 61.4
Haghighi and Klein (2009) 72.9 60.2 67.0 53.2 73.1 61.6
Haghighi and Klein (2010) 77.0 66.9 71.6 55.4 74.8 63.8
This Work: Perceptron Baseline 68.9 60.4 64.4 80.6 60.5 69.1
This Work: DT Baseline 78.0 60.4 68.1 85.1 60.4 70.6
This Work: DT + Web Features 77.6 64.0 70.2 80.7 65.9 72.5
This Work: ? of DT+Web over DT (p < 0.001) 2.1 (p < 0.001) 1.9
Table 3: Primary test results on the ACE04, ACE05, and ACE05-ALL datasets. All systems reported here use automatically
extracted system mentions. B3 here is the B3All version of Stoyanov et al (2009). We also report statistical significance of the
improvements from the Web features on the DT baseline, using the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1993). The
perceptron baseline in this work (Reconcile settings: 15 iterations, threshold = 0.45, SIG for ACE04 and AP for ACE05, ACE05-
ALL) has different results from Stoyanov et al (2009) because their current publicly available code is different from that used in
their paper (p.c.). Also, the B3 variant used by Rahman and Ng (2009) is slightly different from other systems (they remove all and
only the singleton twinless system mentions, so it is neither B3All nor B3None). For completeness, our (untuned) B3None results
(DT + Web) on the ACE05-ALL dataset are P=69.9|R=65.9|F1=67.8.
class / sense information.10
Table 3 shows our primary test results on the
ACE04, ACE05, and ACE05-ALL datasets, for the
MUC and B3 metrics. All systems reported use au-
tomatically detected mentions. We report our re-
sults (the 3 rows marked ?This Work?) on the percep-
tron baseline, the DT baseline, and the Web features
added to the DT baseline. We also report statistical
significance of the improvements from the Web fea-
10We also initially experimented with smaller datasets
(MUC6 and MUC7) and an averaged perceptron baseline, and
we did see similar improvements, arguing that these features are
useful independently of the learning algorithm and dataset.
tures on the DT baseline.11 For significance testing,
we use the bootstrap test (Noreen, 1989; Efron and
Tibshirani, 1993).
Our main comparison is against Haghighi and
Klein (2010), a mostly-unsupervised generative ap-
proach that models latent entity types, which gen-
erate specific entities that in turn render individual
mentions. They learn on large datasets including
11All improvements are significant, except on the small
ACE05 dataset with the MUC metric (where it is weak, at
p < 0.12). However, on the larger version of this dataset,
ACE05-ALL, we get improvements which are both larger and
more significant (at p < 0.001).
395
Wikipedia, and their results are state-of-the-art in
coreference resolution. We outperform their system
on most datasets and metrics (except on ACE05-
ALL for the MUC metric). The other systems we
compare to and outperform are the perceptron-based
Reconcile system of Stoyanov et al (2009), the
strong deterministic system of Haghighi and Klein
(2009), and the cluster-ranking model of Rahman
and Ng (2009).
We develop our features and tune their hyper-
parameter values on the ACE04 development set and
then use these on the ACE04 test set.12 On the
ACE05 and ACE05-ALL datasets, we directly trans-
fer our Web features and their hyper-parameter val-
ues from the ACE04 dev-set, without any retuning.
The test improvements we get on all the datasets (see
Table 3) suggest that our features are generally use-
ful across datasets and metrics.13
5 Analysis
In this section, we briefly discuss errors (in the DT
baseline) corrected by our Web features, and ana-
lyze the decision tree classifier built during training
(based on the ACE04 development experiments).
To study error correction, we begin with the men-
tion pairs that are coreferent according to the gold-
standard annotation (after matching the system men-
tions to the gold ones). We consider the pairs that are
wrongly predicted to be non-coreferent by the base-
line DT system but correctly predicted to be corefer-
ent when we add our Web features. Some examples
of such pairs include:
Iran ; the country
the EPA ; the agency
athletic director ; Mulcahy
Democrat Al Gore ; the vice president
12Note that for the ACE04 dataset only, we use the ?SmartIn-
stanceGenerator? (SIG) filter of Reconcile that uses only a fil-
tered set of mention-pairs (based on distance and other proper-
ties of the pair) instead of the ?AllPairs? (AP) setting that uses
all pairs of mentions, and makes training and tuning very slow.
13For the ACE05 and ACE05-ALL datasets, we revert to the
?AllPairs? (AP) setting of Reconcile because this gives us base-
lines competitive with Haghighi and Klein (2010). Since we did
not need to retune on these datasets, training and tuning speed
were not a bottleneck. Moreover, the improvements from our
Web features are similar even when tried over the SIG baseline;
hence, the filter choice doesn?t affect the performance gain from
the Web features.
Barry Bonds ; the best baseball player
Vojislav Kostunica ; the pro-democracy leader
its closest rival ; the German magazine Das Motorrad
One of those difficult-to-dislodge judges ; John Marshall
These pairs are cases where our features
on Hearst-style co-occurrence and entity-based
context-match are informative and help discriminate
in favor of the correct antecedents. One advan-
tage of using Web-based features is that the Web
has a surprising amount of information on even rare
entities such as proper names. Our features also
correct coreference for various cases of pronominal
anaphora, but these corrections are harder to convey
out of context.
Next, we analyze the decision tree built after
training the classifier (with all our Web features in-
cluded). Around 30% of the decision nodes (both
non-terminals and leaves) correspond to Web fea-
tures, and the average error in classification at the
Web-feature leaves is only around 2.5%, suggest-
ing that our features are strongly discriminative for
pairwise coreference decisions. Some of the most
discriminative nodes correspond to the general co-
occurrence feature for most (binned) log-count val-
ues, the Hearst-style co-occurrence feature for its
zero-count value, the cluster-match feature for its
zero-match value, and the R2 pronoun context fea-
ture for certain (binned) log-count values.
6 Conclusion
We have presented a collection of simple Web-count
features for coreference resolution that capture a
range of world knowledge via statistics of general
lexical co-occurrence, hypernymy, semantic com-
patibility, and semantic context. When added to a
strong decision tree baseline, these features give sig-
nificant improvements and achieve the best results
reported to date, across multiple datasets and met-
rics.
Acknowledgments
We would like to thank Nathan Gilbert, Adam Pauls,
and the anonymous reviewers for their helpful sug-
gestions. This research is supported by Qualcomm
via an Innovation Fellowship to the first author
and by BBN under DARPA contract HR0011-12-C-
0014.
396
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of MUC-7
and LREC Workshop.
Mohit Bansal and Dan Klein. 2011. Web-scale features
for full-scale parsing. In Proceedings of ACL.
Eric Bengtson and Dan Roth. 2008. Understanding the
value of features for coreference resolution. In Pro-
ceedings of EMNLP.
Shane Bergsma and Dekang Lin. 2006. Bootstrap-
ping path-based pronoun resolution. In Proceedings
of COLING-ACL.
Shane Bergsma, Emily Pitler, and Dekang Lin. 2010.
Creating robust supervised classifiers via web-scale n-
gram data. In Proceedings of ACL.
Thorsten Brants and Alex Franz. 2006. The Google Web
1T 5-gram corpus version 1.1. LDC2006T13.
Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information, and lexicogra-
phy. In Proceedings of ACL.
Hal Daume? III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In Proceedings of
EMNLP.
B. Efron and R. Tibshirani. 1993. An introduction to the
bootstrap. Chapman & Hall CRC.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Proceedings of EMNLP.
Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Proceed-
ings of NAACL-HLT.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
Zellig Harris. 1954. Distributional structure. Word,
10(23):146162.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING.
Hamidreza Kobdani, Hinrich Schutze, Michael
Schiehlen, and Hans Kamp. 2011. Bootstrap-
ping coreference resolution using word associations.
In Proceedings of ACL.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of ACL.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
n-grams. In Proceedings of LREC.
Katja Markert and Malvina Nissim. 2005. Comparing
knowledge sources for nominal anaphora resolution.
Computational Linguistics, 31(3):367?402.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of ACL.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of
ACL.
NIST. 2004. The ACE evaluation plan. In NIST.
E.W. Noreen. 1989. Computer intensive methods for
hypothesis testing: An introduction. Wiley, New York.
Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004. Learning to resolve bridging
references. In Proceedings of ACL.
J. R. Quinlan. 1993. C4.5: Programs for machine learn-
ing. Morgan Kaufmann Publishers Inc., San Fran-
cisco, CA, USA.
Altaf Rahman and Vincent Ng. 2009. Supervised models
for coreference resolution. In Proceedings of EMNLP.
Altaf Rahman and Vincent Ng. 2011. Coreference reso-
lution with world knowledge. In Proceedings of ACL.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized algorithms and NLP: Using local-
ity sensitive hash functions for high speed noun clus-
tering. In Proceedings of ACL.
Marta Recasens and Eduard Hovy. 2010. Corefer-
ence resolution across corpora: Languages, coding
schemes, and preprocessing information. In Proceed-
ings of ACL.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-art.
In Proceedings of ACL/IJCNLP.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010. Rec-
oncile: A coreference resolution research platform. In
Technical report, Cornell University.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of MUC-6.
Xiaofeng Yang and Jian Su. 2007. Coreference resolu-
tion using semantic relatedness information from auto-
matically discovered patterns. In Proceedings of ACL.
397
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Im-
proving pronoun resolution using statistics-based se-
mantic compatibility information. In Proceedings of
ACL.
398
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1041?1051,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Structured Learning for Taxonomy Induction with Belief Propagation
Mohit Bansal
TTI Chicago
mbansal@ttic.edu
David Burkett
Twitter Inc.
dburkett@twitter.com
Gerard de Melo
Tsinghua University
gdm@demelo.org
Dan Klein
UC Berkeley
klein@cs.berkeley.edu
Abstract
We present a structured learning approach
to inducing hypernym taxonomies using a
probabilistic graphical model formulation.
Our model incorporates heterogeneous re-
lational evidence about both hypernymy
and siblinghood, captured by semantic
features based on patterns and statistics
from Web n-grams and Wikipedia ab-
stracts. For efficient inference over tax-
onomy structures, we use loopy belief
propagation along with a directed span-
ning tree algorithm for the core hyper-
nymy factor. To train the system, we ex-
tract sub-structures of WordNet and dis-
criminatively learn to reproduce them, us-
ing adaptive subgradient stochastic opti-
mization. On the task of reproducing
sub-hierarchies of WordNet, our approach
achieves a 51% error reduction over a
chance baseline, including a 15% error re-
duction due to the non-hypernym-factored
sibling features. On a comparison setup,
we find up to 29% relative error reduction
over previous work on ancestor F1.
1 Introduction
Many tasks in natural language understanding,
such as question answering, information extrac-
tion, and textual entailment, benefit from lexical
semantic information in the form of types and hy-
pernyms. A recent example is IBM?s Jeopardy!
system Watson (Ferrucci et al, 2010), which used
type information to restrict the set of answer can-
didates. Information of this sort is present in term
taxonomies (e.g., Figure 1), ontologies, and the-
sauri. However, currently available taxonomies
such as WordNet are incomplete in coverage (Pen-
nacchiotti and Pantel, 2006; Hovy et al, 2009),
unavailable in many domains and languages, and
vertebrate
mammal
placental
cow rodent
squirrel rat
metatherian
marsupial
kangaroo
reptile
diapsid
snake crocodilian
anapsid
chelonian
turtle
1
Figure 1: An excerpt of WordNet?s vertebrates taxonomy.
time-intensive to create or extend manually. There
has thus been considerable interest in building lex-
ical taxonomies automatically.
In this work, we focus on the task of taking col-
lections of terms as input and predicting a com-
plete taxonomy structure over them as output. Our
model takes a loglinear form and is represented
using a factor graph that includes both 1st-order
scoring factors on directed hypernymy edges (a
parent and child in the taxonomy) and 2nd-order
scoring factors on sibling edge pairs (pairs of hy-
pernym edges with a shared parent), as well as in-
corporating a global (directed spanning tree) struc-
tural constraint. Inference for both learning and
decoding uses structured loopy belief propagation
(BP), incorporating standard spanning tree algo-
rithms (Chu and Liu, 1965; Edmonds, 1967; Tutte,
1984). The belief propagation approach allows us
to efficiently and effectively incorporate hetero-
geneous relational evidence via hypernymy and
siblinghood (e.g., coordination) cues, which we
capture by semantic features based on simple sur-
face patterns and statistics from Web n-grams and
Wikipedia abstracts. We train our model to max-
imize the likelihood of existing example ontolo-
gies using stochastic optimization, automatically
learning the most useful relational patterns for full
taxonomy induction.
As an example of the relational patterns that our
1041
system learns, suppose we are interested in build-
ing a taxonomy for types of mammals (see Fig-
ure 1). Frequent attestation of hypernymy patterns
like rat is a rodent in large corpora is a strong sig-
nal of the link rodent ? rat. Moreover, sibling
or coordination cues like either rats or squirrels
suggest that rat is a sibling of squirrel and adds
evidence for the links rodent ? rat and rodent
? squirrel. Our supervised model captures ex-
actly these types of intuitions by automatically dis-
covering such heterogeneous relational patterns as
features (and learning their weights) on edges and
on sibling edge pairs, respectively.
There have been several previous studies on
taxonomy induction. e.g., the incremental tax-
onomy induction system of Snow et al (2006),
the longest path approach of Kozareva and Hovy
(2010), and the maximum spanning tree (MST)
approach of Navigli et al (2011) (see Section 4 for
a more detailed overview). The main contribution
of this work is that we present the first discrimina-
tively trained, structured probabilistic model over
the full space of taxonomy trees, using a struc-
tured inference procedure through both the learn-
ing and decoding phases. Our model is also the
first to directly learn relational patterns as part of
the process of training an end-to-end taxonomic
induction system, rather than using patterns that
were hand-selected or learned via pairwise clas-
sifiers on manually annotated co-occurrence pat-
terns. Finally, it is the first end-to-end (i.e., non-
incremental) system to include sibling (e.g., coor-
dination) patterns at all.
We test our approach in two ways. First, on
the task of recreating fragments of WordNet, we
achieve a 51% error reduction on ancestor-based
F1 over a chance baseline, including a 15% error
reduction due to the non-hypernym-factored sib-
ling features. Second, we also compare to the re-
sults of Kozareva and Hovy (2010) by predicting
the large animal subtree of WordNet. Here, we
get up to 29% relative error reduction on ancestor-
based F1. We note that our approach falls at a
different point in the space of performance trade-
offs from past work ? by producing complete,
highly articulated trees, we naturally see a more
even balance between precision and recall, while
past work generally focused on precision.
1
To
1
While different applications will value precision and
recall differently, and past work was often intentionally
precision-focused, it is certainly the case that an ideal solu-
tion would maximize both.
avoid presumption of a single optimal tradeoff, we
also present results for precision-based decoding,
where we trade off recall for precision.
2 Structured Taxonomy Induction
Given an input term set x = {x
1
, x
2
, . . . , x
n
},
we wish to compute the conditional distribution
over taxonomy trees y. This distribution P (y|x)
is represented using the graphical model formu-
lation shown in Figure 2. A taxonomy tree y is
composed of a set of indicator random variables
y
ij
(circles in Figure 2), where y
ij
= ON means
that x
i
is the parent of x
j
in the taxonomy tree
(i.e. there exists a directed edge from x
i
to x
j
).
One such variable exists for each pair (i, j) with
0 ? i ? n, 1 ? j ? n, and i 6= j.
2
In a factor graph formulation, a set of factors
(squares and rectangles in Figure 2) determines the
probability of each possible variable assignment.
Each factor F has an associated scoring function
?
F
, with the probability of a total assignment de-
termined by the product of all these scores:
P (y|x) ?
?
F
?
F
(y) (1)
2.1 Factor Types
In the models we present here, there are three
types of factors: EDGE factors that score individ-
ual edges in the taxonomy tree, SIBLING factors
that score pairs of edges with a shared parent, and
a global TREE factor that imposes the structural
constraint that y form a legal taxonomy tree.
EDGE Factors. For each edge variable y
ij
in
the model, there is a corresponding factor E
ij
(small blue squares in Figure 2) that depends only
on y
ij
. We score each edge by extracting a set
of features f(x
i
, x
j
) and weighting them by the
(learned) weight vector w. So, the factor scoring
function is:
?
E
ij
(y
ij
) =
{
exp(w ? f(x
i
, x
j
)) y
ij
= ON
exp(0) = 1 y
ij
= OFF
SIBLING Factors. Our second model also in-
cludes factors that permit 2nd-order features look-
ing at terms that are siblings in the taxonomy tree.
For each triple (i, j, k) with i 6= j, i 6= k, and
j < k,
3
we have a factor S
ijk
(green rectangles in
2
We assume a special dummy root symbol x
0
.
3
The ordering of the siblings x
j
and x
k
doesn?t mat-
ter here, so having separate factors for (i, j, k) and (i, k, j)
would be redundant.
1042
y01 y02 y0n
y1ny12
y21 y2n
yn1 yn2
E02E01 E0n
E1nE12
E21 E2n
En1 En2
T
(a) Edge Features Only
y01 y02 y0n
y1ny12
y21 y2n
yn1 yn2
E02E01 E0n
E1nE12
E21 E2n
En1 En2
S12n
S21n
Sn12
T
(b) Full Model
Figure 2: Factor graph representation of our model, both without (a) and with (b) SIBLING factors.
Figure 2b) that depends on y
ij
and y
ik
, and thus
can be used to encode features that should be ac-
tive whenever x
j
and x
k
share the same parent, x
i
.
The scoring function is similar to the one above:
?
S
ijk
(y
ij
, y
ik
) =
{
exp(w ? f(x
i
, x
j
, x
k
)) y
ij
= y
ik
= ON
1 otherwise
TREE Factor. Of course, not all variable as-
signments y form legal taxonomy trees (i.e., di-
rected spanning trees). For example, the assign-
ment ?i, j, y
ij
= ON might get a high score, but
would not be a valid output of the model. Thus,
we need to impose a structural constraint to ensure
that such illegal variable assignments are assigned
0 probability by the model. We encode this in our
factor graph setting using a single global factor T
(shown as a large red square in Figure 2) with the
following scoring function:
?
T
(y) =
{
1 y forms a legal taxonomy tree
0 otherwise
Model. For a given global assignment y, let
f(y) =
?
i,j
y
ij
=ON
f(x
i
, x
j
) +
?
i,j,k
y
ij
=y
ik
=ON
f(x
i
, x
j
, x
k
)
Note that by substituting our model?s factor scor-
ing functions into Equation 1, we get:
P (y|x) ?
{
exp(w ? f(y)) y is a tree
0 otherwise
Thus, our model has the form of a standard loglin-
ear model with feature function f .
2.2 Inference via Belief Propagation
With the model defined, there are two main in-
ference tasks we wish to accomplish: computing
expected feature counts and selecting a particular
taxonomy tree for a given set of input terms (de-
coding). As an initial step to each of these pro-
cedures, we wish to compute the marginal prob-
abilities of particular edges (and pairs of edges)
being on. In a factor graph, the natural infer-
ence procedure for computing marginals is belief
propagation. Note that finding taxonomy trees is
a structurally identical problem to directed span-
ning trees (and thereby non-projective dependency
parsing), for which belief propagation has previ-
ously been worked out in depth (Smith and Eisner,
2008). Therefore, we will only briefly sketch the
procedure here.
Belief propagation is a general-purpose infer-
ence method that computes marginals via directed
messages passed from variables to adjacent fac-
tors (and vice versa) in the factor graph. These
messages take the form of (possibly unnormal-
ized) distributions over values of the variable. The
two types of messages (variable to factor or fac-
tor to variable) have mutually recursive defini-
tions. The message from a factor F to an adjacent
variable V involves a sum over all possible val-
ues of every other variable that F touches. While
the EDGE and SIBLING factors are simple enough
to compute this sum by brute force, performing
the sum na??vely for computing messages from the
TREE factor would take exponential time. How-
1043
ever, due to the structure of that particular factor,
all of its outgoing messages can be computed si-
multaneously in O(n
3
) time via an efficient adap-
tation of Kirchhoff?s Matrix Tree Theorem (MTT)
(Tutte, 1984) which computes partition functions
and marginals for directed spanning trees.
Once message passing is completed, marginal
beliefs are computed by merely multiplying to-
gether all the messages received by a particular
variable or factor.
2.2.1 Loopy Belief Propagation
Looking closely at Figure 2a, one can observe
that the factor graph for the first version of our
model, containing only EDGE and TREE factors,
is acyclic. In this special case, belief propagation
is exact: after one round of message passing, the
beliefs computed (as discussed in Section 2.2) will
be the true marginal probabilities under the cur-
rent model. However, in the full model, shown
in Figure 2b, the SIBLING factors introduce cy-
cles into the factor graph, and now the messages
being passed around often depend on each other
and so they will change as they are recomputed.
The process of iteratively recomputing messages
based on earlier messages is known as loopy belief
propagation. This procedure only finds approx-
imate marginal beliefs, and is not actually guar-
anteed to converge, but in practice can be quite
effective for finding workable marginals in mod-
els for which exact inference is intractable, as is
the case here. All else equal, the more rounds
of message passing that are performed, the closer
the computed marginal beliefs will be to the true
marginals, though in practice, there are usually di-
minishing returns after the first few iterations. In
our experiments, we used a fairly conservative up-
per bound of 20 iterations, but in most cases, the
messages converged much earlier than that.
2.3 Training
We used gradient-based maximum likelihood
training to learn the model parameters w. Since
our model has a loglinear form, the derivative
of w with respect to the likelihood objective is
computed by just taking the gold feature vec-
tor and subtracting the vector of expected feature
counts. For computing expected counts, we run
belief propagation until completion and then, for
each factor in the model, we simply read off the
marginal probability of that factor being active (as
computed in Section 2.2), and accumulate a par-
tial count for each feature that is fired by that fac-
tor. This method of computing the gradient can be
incorporated into any gradient-based optimizer in
order to learn the weights w. In our experiments
we used AdaGrad (Duchi et al, 2011), an adaptive
subgradient variant of standard stochastic gradient
ascent for online learning.
2.4 Decoding
Finally, once the model parameters have been
learned, we want to use the model to find taxon-
omy trees for particular sets of input terms. Note
that if we limit our scores to be edge-factored,
then finding the highest scoring taxonomy tree
becomes an instance of the MST problem (also
known as the maximum arborescence problem
for the directed case), which can be solved effi-
ciently in O(n
2
) quadratic time (Tarjan, 1977) us-
ing the greedy, recursive Chu-Liu-Edmonds algo-
rithm (Chu and Liu, 1965; Edmonds, 1967).
4
Since the MST problem can be solved effi-
ciently, the main challenge becomes finding a way
to ensure that our scores are edge-factored. In the
first version of our model, we could simply set the
score of each edge to be w?f(x
i
, x
j
), and the MST
recovered in this way would indeed be the high-
est scoring tree: arg maxyP (y|x). However, this
straightforward approach doesn?t apply to the full
model which also uses sibling features. Hence, at
decoding time, we instead start out by once more
using belief propagation to find marginal beliefs,
and then set the score of each edge to be its belief
odds ratio:
b
Y
ij
(ON)
b
Y
ij
(OFF)
.
5
3 Features
While spanning trees are familiar from non-
projective dependency parsing, features based on
the linear order of the words or on lexical identi-
4
See Georgiadis (2003) for a detailed algorithmic proof,
and McDonald et al (2005) for an illustrative example. Also,
we constrain the Chu-Liu-Edmonds MST algorithm to out-
put only single-root MSTs, where the (dummy) root has ex-
actly one child (Koo et al, 2007), because multi-root span-
ning ?forests? are not applicable to our task.
Also, note that we currently assume one node per term. We
are following the task description from previous work where
the goal is to create a taxonomy for a specific domain (e.g.,
animals). Within a specific domain, terms typically just have
a single sense. However, our algorithms could certainly be
adapted to the case of multiple term senses (by treating the
different senses as unique nodes in the tree) in future work.
5
The MST that is found using these edge scores is actually
the minimum Bayes risk tree (Goodman, 1996) for an edge
accuracy loss function (Smith and Eisner, 2008).
1044
ties or syntactic word classes, which are primary
drivers for dependency parsing, are mostly unin-
formative for taxonomy induction. Instead, induc-
ing taxonomies requires world knowledge to cap-
ture the semantic relations between various unseen
terms. For this, we use semantic cues to hyper-
nymy and siblinghood via features on simple sur-
face patterns and statistics in large text corpora.
We fire features on both the edge and the sibling
factors. We first describe all the edge features
in detail (Section 3.1 and Section 3.2), and then
briefly describe the sibling features (Section 3.3),
which are quite similar to the edge ones.
For each edge factor E
ij
, which represents the
potential parent-child term pair (x
i
, x
j
), we add
the surface and semantic features discussed below.
Note that since edges are directed, we have sepa-
rate features for the factors E
ij
versus E
ji
.
3.1 Surface Features
Capitalization: Checks which of x
i
and x
j
are
capitalized, with one feature for each value of the
tuple (isCap(x
i
), isCap(x
j
)). The intuition is that
leaves of a taxonomy are often proper names and
hence capitalized, e.g., (bison, American bison).
Therefore, the feature for (true, false) (i.e., parent
capitalized but not the child) gets a substantially
negative weight.
Ends with: Checks if x
j
ends with x
i
, or not. This
captures pairs such as (fish, bony fish) in our data.
Contains: Checks if x
j
contains x
i
, or not. This
captures pairs such as (bird, bird of prey).
Suffix match: Checks whether the k-length suf-
fixes of x
i
and x
j
match, or not, for k =
1, 2, . . . , 7.
LCS: We compute the longest common substring
of x
i
and x
j
, and create indicator features for
rounded-off and binned values of |LCS|/((|x
i
|+
|x
j
|)/2).
Length difference: We compute the signed length
difference between x
j
and x
i
, and create indica-
tor features for rounded-off and binned values of
(|x
j
| ? |x
i
|)/((|x
i
| + |x
j
|)/2). Yang and Callan
(2009) use a similar feature.
3.2 Semantic Features
3.2.1 Web n-gram Features
Patterns and counts: Hypernymy for a term pair
(P=x
i
, C=x
j
) is often signaled by the presence
of surface patterns like C is a P, P such as C
in large text corpora, an observation going back
to Hearst (1992). For each potential parent-child
edge (P=x
i
, C=x
j
), we mine the top k strings
(based on count) in which both x
i
and x
j
occur
(we use k=200). We collect patterns in both direc-
tions, which allows us to judge the correct direc-
tion of an edge (e.g., C is a P is a positive signal
for hypernymy whereas P is a C is a negative sig-
nal).
6
Next, for each pattern in this top-k list, we
compute its normalized pattern count c, and fire
an indicator feature on the tuple (pattern, t), for
all thresholds t (in a fixed set) s.t. c ? t. Our
supervised model then automatically learns which
patterns are good indicators of hypernymy.
Pattern order: We add features on the order (di-
rection) in which the pair (x
i
, x
j
) found a pattern
(in its top-k list) ? indicator features for boolean
values of the four cases: P . . . C, C . . . P , neither
direction, and both directions. Ritter et al (2009)
used the ?both? case of this feature.
Individual counts: We also compute the indi-
vidual Web-scale term counts c
x
i
and c
x
j
, and
add a comparison feature (c
x
i
>c
x
j
), plus features
on values of the signed count difference (|c
x
i
| ?
|c
x
j
|)/((|c
x
i
| + |c
x
j
|)/2), after rounding off, and
binning at multiple granularities. The intuition is
that this feature could learn whether the relative
popularity of the terms signals their hypernymy di-
rection.
3.2.2 Wikipedia Abstract Features
The Web n-grams corpus has broad coverage but
is limited to up to 5-grams, so it may not contain
pattern-based evidence for various longer multi-
word terms and pairs. Therefore, we supplement
it with a full-sentence resource, namely Wikipedia
abstracts, which are concise descriptions (hence
useful to signal hypernymy) of a large variety of
world entities.
Presence and distance: For each potential edge
(x
i
, x
j
), we mine patterns from all abstracts in
which the two terms co-occur in either order, al-
lowing a maximum term distance of 20 (because
beyond that, co-occurrence may not imply a rela-
tion). We add a presence feature based on whether
the process above found at least one pattern for
that term pair, or not. We also fire features on
the value of the minimum distance d
min
at which
6
We also allow patterns with surrounding words, e.g., the
C is a P and C , P of.
1045
the two terms were found in some abstract (plus
thresholded versions).
Patterns: For each term pair, we take the top-k
?
patterns (based on count) of length up to l from
its full list of patterns, and add an indicator feature
on each pattern string (without the counts). We use
k
?
=5, l=10. Similar to the Web n-grams case, we
also fire Wikipedia-based pattern order features.
3.3 Sibling Features
We also incorporate similar features on sibling
factors. For each sibling factor S
ijk
which rep-
resents the potential parent-children term triple
(x
i
, x
j
, x
k
), we consider the potential sibling term
pair (x
j
, x
k
). Siblinghood for this pair would be
indicated by the presence of surface patterns such
as either C
1
or C
2
, C
1
is similar to C
2
in large cor-
pora. Hence, we fire Web n-gram pattern features
and Wikipedia presence, distance, and pattern fea-
tures, similar to those described above, on each
potential sibling term pair.
7
The main difference
here from the edge factors is that the sibling fac-
tors are symmetric (in the sense that S
ijk
is redun-
dant to S
ikj
) and hence the patterns are undirected.
Therefore, for each term pair, we first symmetrize
the collected Web n-grams and Wikipedia patterns
by accumulating the counts of symmetric patterns
like rats or squirrels and squirrels or rats.
8
4 Related Work
In our work, we assume a known term set and
do not address the problem of extracting related
terms from text. However, a great deal of past
work has considered automating this process, typ-
ically taking one of two major approaches. The
clustering-based approach (Lin, 1998; Lin and
Pantel, 2002; Davidov and Rappoport, 2006; Ya-
mada et al, 2009) discovers relations based on the
assumption that similar concepts appear in sim-
7
One can also add features on the full triple (x
i
, x
j
, x
k
)
but most such features will be sparse.
8
All the patterns and counts for our Web and Wikipedia
edge and sibling features described above are extracted after
stemming the words in the terms, the n-grams, and the ab-
stracts (using the Porter stemmer). Also, we threshold the
features (to prune away the sparse ones) by considering only
those that fire for at least t trees in the training data (t = 4 in
our experiments).
Note that one could also add various complementary types of
useful features presented by previous work, e.g., bootstrap-
ping using syntactic heuristics (Phillips and Riloff, 2002),
dependency patterns (Snow et al, 2006), doubly anchored
patterns (Kozareva et al, 2008; Hovy et al, 2009), and Web
definition classifiers (Navigli et al, 2011).
ilar contexts (Harris, 1954). The pattern-based
approach uses special lexico-syntactic patterns to
extract pairwise relation lists (Phillips and Riloff,
2002; Girju et al, 2003; Pantel and Pennacchiotti,
2006; Suchanek et al, 2007; Ritter et al, 2009;
Hovy et al, 2009; Baroni et al, 2010; Ponzetto
and Strube, 2011) and semantic classes or class-
instance pairs (Riloff and Shepherd, 1997; Katz
and Lin, 2003; Pas?ca, 2004; Etzioni et al, 2005;
Talukdar et al, 2008).
We focus on the second step of taxonomy induc-
tion, namely the structured organization of terms
into a complete and coherent tree-like hierarchy.
9
Early work on this task assumes a starting par-
tial taxonomy and inserts missing terms into it.
Widdows (2003) place unknown words into a re-
gion with the most semantically-similar neigh-
bors. Snow et al (2006) add novel terms by greed-
ily maximizing the conditional probability of a set
of relational evidence given a taxonomy. Yang and
Callan (2009) incrementally cluster terms based
on a pairwise semantic distance. Lao et al (2012)
extend a knowledge base using a random walk
model to learn binary relational inference rules.
However, the task of inducing full taxonomies
without assuming a substantial initial partial tax-
onomy is relatively less well studied. There is
some prior work on the related task of hierarchical
clustering, or grouping together of semantically
related words (Cimiano et al, 2005; Cimiano and
Staab, 2005; Poon and Domingos, 2010; Fountain
and Lapata, 2012). The task we focus on, though,
is the discovery of direct taxonomic relationships
(e.g., hypernymy) between words.
We know of two closely-related previous sys-
tems, Kozareva and Hovy (2010) and Navigli et
al. (2011), that build full taxonomies from scratch.
Both of these systems use a process that starts
by finding basic level terms (leaves of the fi-
nal taxonomy tree, typically) and then using re-
lational patterns (hand-selected ones in the case of
Kozareva and Hovy (2010), and ones learned sep-
arately by a pairwise classifier on manually anno-
tated co-occurrence patterns for Navigli and Ve-
lardi (2010), Navigli et al (2011)) to find interme-
diate terms and all the attested hypernymy links
between them.
10
To prune down the resulting tax-
9
Determining the set of input terms is orthogonal to our
work, and our method can be used in conjunction with vari-
ous term extraction approaches described above.
10
Unlike our system, which assumes a complete set of
terms and only attempts to induce the taxonomic structure,
1046
onomy graph, Kozareva and Hovy (2010) use a
procedure that iteratively retains the longest paths
between root and leaf terms, removing conflicting
graph edges as they go. The end result is acyclic,
though not necessarily a tree; Navigli et al (2011)
instead use the longest path intuition to weight
edges in the graph and then find the highest weight
taxonomic tree using a standard MST algorithm.
Our work differs from the two systems above
in that ours is the first discriminatively trained,
structured probabilistic model over the full space
of taxonomy trees that uses structured inference
via spanning tree algorithms (MST and MTT)
through both the learning and decoding phases.
Our model also automatically learns relational pat-
terns as a part of the taxonomic training phase, in-
stead of relying on hand-picked rules or pairwise
classifiers on manually annotated co-occurrence
patterns, and it is the first end-to-end (i.e., non-
incremental) system to include heterogeneous re-
lational information via sibling (e.g., coordina-
tion) patterns.
5 Experiments
5.1 Data and Experimental Regime
We considered two distinct experimental setups,
one that illustrates the general performance of
our model by reproducing various medium-sized
WordNet domains, and another that facilitates
comparison to previous work by reproducing the
much larger animal subtree provided by Kozareva
and Hovy (2010).
General setup: In order to test the accuracy
of structured prediction on medium-sized full-
domain taxonomies, we extracted from WordNet
3.0 all bottomed-out full subtrees which had a
tree-height of 3 (i.e., 4 nodes from root to leaf),
and contained (10, 50] terms.
11
This gives us
761 non-overlapping trees, which we partition into
both these systems include term discovery in the taxonomy
building process.
11
Subtrees that had a smaller or larger tree height were dis-
carded in order to avoid overlap between the training and test
divisions. This makes it a much stricter setting than other
tasks such as parsing, which usually has repeated sentences,
clauses and phrases between training and test sets.
To project WordNet synsets to terms, we used the first (most
frequent) term in each synset. A few WordNet synsets have
multiple parents so we only keep the first of each such pair of
overlapping trees. We also discard a few trees with duplicate
terms because this is mostly due to the projection of different
synsets to the same term, and theoretically makes the tree a
graph.
70/15/15% (533/114/114 trees) train/dev/test sets.
Comparison setup: We also compare our method
(as closely as possible) with related previous work
by testing on the much larger animal subtree made
available by Kozareva and Hovy (2010), who cre-
ated this dataset by selecting a set of ?harvested?
terms and retrieving all the WordNet hypernyms
between each input term and the root (i.e., an-
imal), resulting in ?700 terms and ?4,300 is-a
ancestor-child links.
12
Our training set for this an-
imal test case was generated from WordNet us-
ing the following process: First, we strictly re-
move the full animal subtree from WordNet in or-
der to avoid any possible overlap with the test data.
Next, we create random 25-sized trees by picking
random nodes as singleton trees, and repeatedly
adding child edges from WordNet to the tree. This
process gives us a total of ?1600 training trees.
13
Feature sources: The n-gram semantic features
are extracted from the Google n-grams corpus
(Brants and Franz, 2006), a large collection of
English n-grams (for n = 1 to 5) and their fre-
quencies computed from almost 1 trillion tokens
(95 billion sentences) of Web text. The Wikipedia
abstracts are obtained via the publicly available
dump, which contains almost ?4.1 million ar-
ticles.
14
Preprocessing includes standard XML
parsing and tokenization. Efficient collection of
feature statistics is important because these must
be extracted for millions of query pairs (for each
potential edge and sibling pair in each term set).
For this, we use a hash-trie on term pairs (sim-
ilar to that of Bansal and Klein (2011)), and scan
once through the n-gram (or abstract) set, skipping
many n-grams (or abstracts) based on fast checks
of missing unigrams, exceeding length, suffix mis-
matches, etc.
5.2 Evaluation Metric
Ancestor F1: Measures the precision, recall, and
F
1
= 2PR/(P +R) of correctly predicted ances-
12
This is somewhat different from our general setup where
we work with any given set of terms; they start with a large
set of leaves which have substantial Web-based relational
information based on their selected, hand-picked patterns.
Their data is available at http://www.isi.edu/
?
kozareva/
downloads.html.
13
We tried this training regimen as different from that of
the general setup (which contains only bottomed-out sub-
trees), so as to match the animal test tree, which is of depth
12 and has intermediate nodes from higher up in WordNet.
14
We used the 20130102 dump.
1047
System P R F1
Edges-Only Model
Baseline 5.9 8.3 6.9
Surface Features 17.5 41.3 24.6
Semantic Features 37.0 49.1 42.2
Surface+Semantic 41.1 54.4 46.8
Edges + Siblings Model
Surface+Semantic 53.1 56.6 54.8
Surface+Semantic (Test) 48.0 55.2 51.4
Table 1: Main results on our general setup. On the devel-
opment set, we present incremental results on the edges-only
model where we start with the chance baseline, then use sur-
face features only, semantic features only, and both. Finally,
we add sibling factors and features to get results for the full,
edges+siblings model with all features, and also report the
final test result for this setting.
tors, i.e., pairwise is-a relations:
P =
|isa
gold
? isa
predicted
|
|isa
predicted
|
, R =
|isa
gold
? isa
predicted
|
|isa
gold
|
5.3 Results
Table 1 shows our main results for ancestor-based
evaluation on the general setup. We present a de-
velopment set ablation study where we start with
the edges-only model (Figure 2a) and its random
tree baseline (which chooses any arbitrary span-
ning tree for the term set). Next, we show results
on the edges-only model with surface features
(Section 3.1), semantic features (Section 3.2), and
both. We see that both surface and semantic fea-
tures make substantial contributions, and they also
stack. Finally, we add the sibling factors and fea-
tures (Figure 2b, Section 3.3), which further im-
proves the results significantly (8% absolute and
15% relative error reduction over the edges-only
results on the ancestor F1 metric). The last row
shows the final test set results for the full model
with all features.
Table 2 shows our results for comparison to
the larger animal dataset of Kozareva and Hovy
(2010).
15
In the table, ?Kozareva2010? refers
to Kozareva and Hovy (2010) and ?Navigli2011?
refers to Navigli et al (2011).
16
For appropri-
15
These results are for the 1st order model due to the scale
of the animal taxonomy (?700 terms). For scaling the 2nd
order sibling model, one can use approximations, e.g., prun-
ing the set of sibling factors based on 1st order link marginals,
or a hierarchical coarse-to-fine approach based on taxonomy
induction on subtrees, or a greedy approach of adding a few
sibling factors at a time. This is future work.
16
The Kozareva and Hovy (2010) ancestor results are ob-
tained by using the output files provided on their webpage.
System P R F1
Previous Work
Kozareva2010 98.6 36.2 52.9
Navigli2011
??
97.0
??
43.7
??
60.3
??
This Paper
Fixed Prediction 84.2 55.1 66.6
Free Prediction 79.3 49.0 60.6
Table 2: Comparison results on the animal dataset of
Kozareva and Hovy (2010). Here, ?Kozareva2010? refers to
Kozareva and Hovy (2010) and ?Navigli2011? refers to Nav-
igli et al (2011). For appropriate comparison to each previ-
ous work, we show our results both for the ?Fixed Prediction?
setup, which assumes the true root and leaves, and for the
?Free Prediction? setup, which doesn?t assume any prior in-
formation. The ?? results of Navigli et al (2011) represent a
different ground-truth data condition, making them incompa-
rable to our results; see Section 5.3 for details.
ate comparison to each previous work, we show
results for two different setups. The first setup
?Fixed Prediction? assumes that the model knows
the true root and leaves of the taxonomy to provide
for a somewhat fairer comparison to Kozareva and
Hovy (2010). We get substantial improvements
on ancestor-based recall and F1 (a 29% relative
error reduction). The second setup ?Free Predic-
tion? assumes no prior knowledge and predicts the
full tree (similar to the general setup case). On
this setup, we do compare as closely as possible
to Navigli et al (2011) and see a small gain in F1,
but regardless, we should note that their results are
incomparable (denoted by ?? in Table 2) because
they have a different ground-truth data condition:
their definition and hypernym extraction phase in-
volves using the Google define keyword, which
often returns WordNet glosses itself.
We note that previous work achieves higher an-
cestor precision, while our approach achieves a
more even balance between precision and recall.
Of course, precision and recall should both ide-
ally be high, even if some applications weigh one
over the other. This is why our tuning optimized
for F1, which represents a neutral combination
for comparison, but other F
?
metrics could also
be optimized. In this direction, we also tried an
experiment on precision-based decoding (for the
?Free Prediction? scenario), where we discard any
edges with score (i.e., the belief odds ratio de-
scribed in Section 2.4) less than a certain thresh-
old. This allowed us to achieve high values of pre-
cision (e.g., 90.8%) at still high enough F1 values
(e.g., 61.7%).
1048
Hypernymy features
C and other P > P > C
C , P of C is a P
C , a P P , including C
C or other P P ( C
C : a P C , american P
C - like P C , the P
Siblinghood features
C
1
and C
2
C
1
, C
2
(
C
1
or C
2
of C
1
and / or C
2
, C
1
, C
2
and either C
1
or C
2
the C
1
/ C
2
<s> C
1
and C
2
</s>
Table 3: Examples of high-weighted hypernymy and sibling-
hood features learned during development.
butterfly
copper
American copper
hairstreak
Strymon melinus
admiral
white admiral
1
Figure 3: Excerpt from the predicted butterfly tree. The terms
attached erroneously according to WordNet are marked in red
and italicized.
6 Analysis
Table 3 shows some of the hypernymy and sibling-
hood features given highest weight by our model
(in general-setup development experiments). The
training process not only rediscovers most of the
standard Hearst-style hypernymy patterns (e.g., C
and other P, C is a P), but also finds various
novel, intuitive patterns. For example, the pattern
C, american P is prominent because it captures
pairs like Lemmon, american actor and Bryon,
american politician, etc. Another pattern > P >
C captures webpage navigation breadcrumb trails
(representing category hierarchies). Similarly, the
algorithm also discovers useful siblinghood fea-
tures, e.g., either C
1
or C
2
, C
1
and / or C
2
, etc.
Finally, we look at some specific output errors
to give as concrete a sense as possible of some sys-
tem confusions, though of course any hand-chosen
examples must be taken as illustrative. In Figure
3, we attach white admiral to admiral, whereas
the gold standard makes these two terms siblings.
In reality, however, white admirals are indeed a
species of admirals, so WordNet?s ground truth
turns out to be incomplete. Another such example
is that we place logistic assessment in the evalu-
bottle
flask
vacuum flask thermos Erlenmeyer flask
wine bottle jeroboam
1
Figure 4: Excerpt from the predicted bottle tree. The terms
attached erroneously according to WordNet are marked in red
and italicized.
ation subtree of judgment, but WordNet makes it
a direct child of judgment. However, other dictio-
naries do consider logistic assessments to be eval-
uations. Hence, this illustrates that there may be
more than one right answer, and that the low re-
sults on this task should only be interpreted as
such. In Figure 4, our algorithm did not recog-
nize that thermos is a hyponym of vacuum flask,
and that jeroboam is a kind of wine bottle. Here,
our Web n-grams dataset (which only contains fre-
quent n-grams) and Wikipedia abstracts do not
suffice and we would need to add richer Web data
for such world knowledge to be reflected in the
features.
7 Conclusion
Our approach to taxonomy induction allows het-
erogeneous information sources to be combined
and balanced in an error-driven way. Direct indi-
cators of hypernymy, such as Hearst-style context
patterns, are the core feature for the model and are
discovered automatically via discriminative train-
ing. However, other indicators, such as coordina-
tion cues, can indicate that two words might be
siblings, independently of what their shared par-
ent might be. Adding second-order factors to our
model allows these two kinds of evidence to be
weighed and balanced in a discriminative, struc-
tured probabilistic framework. Empirically, we
see substantial gains (in ancestor F1) from sibling
features, and also over comparable previous work.
We also present results on the precision and recall
trade-offs inherent in this task.
Acknowledgments
We would like to thank the anonymous review-
ers for their insightful comments. This work
was supported by BBN under DARPA contract
HR0011-12-C-0014, 973 Program China Grants
2011CBA00300, 2011CBA00301, and NSFC
Grants 61033001, 61361136003.
1049
References
Mohit Bansal and Dan Klein. 2011. Web-scale fea-
tures for full-scale parsing. In Proceedings of ACL.
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2010. Strudel: A corpus-based seman-
tic model based on properties and types. Cognitive
Science, 34(2):222?254.
Thorsten Brants and Alex Franz. 2006. The Google
Web 1T 5-gram corpus version 1.1. LDC2006T13.
Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On the
shortest arborescence of a directed graph. Science
Sinica, 14(1396-1400):270.
Philipp Cimiano and Steffen Staab. 2005. Learning
concept hierarchies from text with a guided agglom-
erative clustering algorithm. In Proceedings of the
ICML 2005 Workshop on Learning and Extending
Lexical Ontologies with Machine Learning Meth-
ods.
Philipp Cimiano, Andreas Hotho, and Steffen Staab.
2005. Learning concept hierarchies from text cor-
pora using formal concept analysis. Journal of Arti-
ficial Intelligence Research, 24(1):305?339.
Dmitry Davidov and Ari Rappoport. 2006. Effi-
cient unsupervised discovery of word categories us-
ing symmetric patterns and high frequency words.
In Proceedings of COLING-ACL.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121?2159.
Jack Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards B,
71:233?240.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the Web:
An experimental study. Artificial Intelligence,
165(1):91?134.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya A Kalyanpur,
Adam Lally, J William Murdock, Eric Nyberg, John
Prager, Nico Schlaefer, and Chris Welty. 2010.
Building watson: An overview of the DeepQA
project. AI magazine, 31(3):59?79.
Trevor Fountain and Mirella Lapata. 2012. Taxonomy
induction using hierarchical random graphs. In Pro-
ceedings of NAACL.
Leonidas Georgiadis. 2003. Arborescence optimiza-
tion problems solvable by edmonds algorithm. The-
oretical Computer Science, 301(1):427?437.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the auto-
matic discovery of part-whole relations. In Proceed-
ings of NAACL.
Joshua Goodman. 1996. Parsing algorithms and met-
rics. In Proceedings of ACL.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING.
Eduard Hovy, Zornitsa Kozareva, and Ellen Riloff.
2009. Toward completeness in concept extraction
and classification. In Proceedings of EMNLP.
Boris Katz and Jimmy Lin. 2003. Selectively using re-
lations to improve precision in question answering.
In Proceedings of the Workshop on NLP for Ques-
tion Answering (EACL 2003).
Terry Koo, Amir Globerson, Xavier Carreras, and
Michael Collins. 2007. Structured prediction mod-
els via the matrix-tree theorem. In Proceedings of
EMNLP-CoNLL.
Zornitsa Kozareva and Eduard Hovy. 2010. A
semi-supervised method to learn and construct tax-
onomies using the Web. In Proceedings of EMNLP.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In Proceedings of
ACL.
Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W. Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Pro-
ceedings of EMNLP.
Dekang Lin and Patrick Pantel. 2002. Concept discov-
ery from text. In Proceedings of COLING.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji?c. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT-EMNLP.
Roberto Navigli and Paola Velardi. 2010. Learning
word-class lattices for definition and hypernym ex-
traction. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics.
Roberto Navigli, Paola Velardi, and Stefano Faralli.
2011. A graph-based algorithm for inducing lexical
taxonomies from scratch. In Proceedings of IJCAI.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proceedings
of COLING-ACL.
1050
Marius Pas?ca. 2004. Acquisition of categorized named
entities for web search. In Proceedings of CIKM.
Marco Pennacchiotti and Patrick Pantel. 2006. On-
tologizing semantic relations. In Proceedings of
COLING-ACL.
William Phillips and Ellen Riloff. 2002. Exploiting
strong syntactic heuristics and co-training to learn
semantic lexicons. In Proceedings of EMNLP.
Simone Paolo Ponzetto and Michael Strube. 2011.
Taxonomy induction based on a collaboratively
built knowledge repository. Artificial Intelligence,
175(9):1737?1756.
Hoifung Poon and Pedro Domingos. 2010. Unsuper-
vised ontology induction from text. In Proceedings
of ACL.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-
based approach for building semantic lexicons. In
Proceedings of EMNLP.
Alan Ritter, Stephen Soderland, and Oren Etzioni.
2009. What is this, anyway: Automatic hypernym
discovery. In Proceedings of AAAI Spring Sympo-
sium on Learning by Reading and Learning to Read.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of
EMNLP.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of COLING-ACL.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of WWW.
Partha Pratim Talukdar, Joseph Reisinger, Marius
Pas?ca, Deepak Ravichandran, Rahul Bhagat, and
Fernando Pereira. 2008. Weakly-supervised acqui-
sition of labeled class instances using graph random
walks. In Proceedings of EMNLP.
Robert E. Tarjan. 1977. Finding optimum branchings.
Networks, 7:25?35.
William T. Tutte. 1984. Graph theory. Addison-
Wesley.
Dominic Widdows. 2003. Unsupervised methods
for developing taxonomies by combining syntactic
and statistical information. In Proceedings of HLT-
NAACL.
Ichiro Yamada, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, Masaki Murata, Stijn De Saeger, Fran-
cis Bond, and Asuka Sumida. 2009. Hypernym dis-
covery based on distributional similarity and hierar-
chical structures. In Proceedings of EMNLP.
Hui Yang and Jamie Callan. 2009. A metric-based
framework for automatic taxonomy induction. In
Proceedings of ACL-IJCNLP.
1051
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 809?815,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Tailoring Continuous Word Representations for Dependency Parsing
Mohit Bansal Kevin Gimpel Karen Livescu
Toyota Technological Institute at Chicago, IL 60637, USA
{mbansal,kgimpel,klivescu}@ttic.edu
Abstract
Word representations have proven useful
for many NLP tasks, e.g., Brown clusters
as features in dependency parsing (Koo et
al., 2008). In this paper, we investigate the
use of continuous word representations as
features for dependency parsing. We com-
pare several popular embeddings to Brown
clusters, via multiple types of features, in
both news and web domains. We find that
all embeddings yield significant parsing
gains, including some recent ones that can
be trained in a fraction of the time of oth-
ers. Explicitly tailoring the representations
for the task leads to further improvements.
Moreover, an ensemble of all representa-
tions achieves the best results, suggesting
their complementarity.
1 Introduction
Word representations derived from unlabeled text
have proven useful for many NLP tasks, e.g., part-
of-speech (POS) tagging (Huang et al, 2014),
named entity recognition (Miller et al, 2004),
chunking (Turian et al, 2010), and syntactic
parsing (Koo et al, 2008; Finkel et al, 2008;
T?ackstr?om et al, 2012). Most word representa-
tions fall into one of two categories. Discrete rep-
resentations consist of memberships in a (possibly
hierarchical) hard clustering of words, e.g., via k-
means or the Brown et al (1992) algorithm. Con-
tinuous representations (or distributed representa-
tions or embeddings) consist of low-dimensional,
real-valued vectors for each word, typically in-
duced via neural language models (Bengio et al,
2003; Mnih and Hinton, 2007) or spectral meth-
ods (Deerwester et al, 1990; Dhillon et al, 2011).
Koo et al (2008) found improvement on in-
domain dependency parsing using features based
on discrete Brown clusters. In this paper, we ex-
periment with parsing features derived from con-
tinuous representations. We find that simple at-
tempts based on discretization of individual word
vector dimensions do not improve parsing. We
see gains only after first performing a hierarchi-
cal clustering of the continuous word vectors and
then using features based on the hierarchy.
We compare several types of continuous rep-
resentations, including those made available by
other researchers (Turian et al, 2010; Collobert et
al., 2011; Huang et al, 2012), and embeddings we
have trained using the approach of Mikolov et al
(2013a), which is orders of magnitude faster than
the others. The representations exhibit different
characteristics, which we demonstrate using both
intrinsic metrics and extrinsic parsing evaluation.
We report significant improvements over our base-
line on both the Penn Treebank (PTB; Marcus et
al., 1993) and the English Web treebank (Petrov
and McDonald, 2012).
While all embeddings yield some parsing im-
provements, we find larger gains by tailoring them
to capture similarity in terms of context within
syntactic parses. To this end, we use two sim-
ple modifications to the models of Mikolov et al
(2013a): a smaller context window, and condition-
ing on syntactic context (dependency links and la-
bels). Interestingly, the Brown clusters of Koo et
al. (2008) prove to be difficult to beat, but we find
that our syntactic tailoring can lead to embeddings
that match the parsing performance of Brown (on
all test sets) in a fraction of the training time. Fi-
nally, a simple parser ensemble on all the represen-
tations achieves the best results, suggesting their
complementarity for dependency parsing.
2 Continuous Word Representations
There are many ways to train continuous represen-
tations; in this paper, we are primarily interested
in neural language models (Bengio et al, 2003),
which use neural networks and local context to
learn word vectors. Several researchers have
made their trained representations publicly avail-
809
Representation Source Corpus Types, Tokens V D Time
BROWN Koo et al (2008) BLLIP 317K, 43M 316,710 ? 2.5 days
?
SENNA Collobert et al (2011) Wikipedia 8.3M, 1.8B 130,000 50 2 months
?
TURIAN Turian et al (2010) RCV1 269K, 37M 268,810 50 few weeks
?
HUANG Huang et al (2012) Wikipedia 8.3M, 1.8B 100,232 50 ?
CBOW, SKIP, SKIP
DEP
Mikolov et al (2013a) BLLIP 317K, 43M 316,697 100 2-4 mins.
?
Table 1: Details of word representations used, including datasets, vocabulary size V , and dimensionality D. Continuous
representations require an additional 4 hours to run hierarchical clustering to generate features (?3.2). RCV1 = Reuters Corpus,
Volume 1. ? = time reported by authors. ? = run by us on a 3.50 GHz desktop, using a single thread.
able, which we use directly in our experiments.
In particular, we use the SENNA embeddings of
Collobert et al (2011); the scaled TURIAN em-
beddings (C&W) of Turian et al (2010); and the
HUANG global-context, single-prototype embed-
dings of Huang et al (2012). We also use the
BROWN clusters trained by Koo et al (2008). De-
tails are given in Table 1.
Below, we describe embeddings that we train
ourselves (?2.1), aiming to make them more useful
for parsing via smaller context windows (?2.1.1)
and conditioning on syntactic context (?2.1.2). We
then compare the representations using two intrin-
sic metrics (?2.2).
2.1 Syntactically-tailored Representations
We train word embeddings using the continu-
ous bag-of-words (CBOW) and skip-gram (SKIP)
models described in Mikolov et al (2013a;
2013b) as implemented in the open-source toolkit
word2vec. These models avoid hidden layers
in the neural network and hence can be trained
in only minutes, compared to days or even weeks
for the others, as shown in Table 1.
1
We adapt
these embeddings to be more useful for depen-
dency parsing in two ways, described next.
2.1.1 Smaller Context Windows
The CBOW model learns vectors to predict a
word given its set of surrounding context words
in a window of size w. The SKIP model learns
embeddings to predict each individual surround-
ing word given one particular word, using an anal-
ogous window size w. We find that w affects
the embeddings substantially: with large w, words
group with others that are topically-related; with
small w, grouped words tend to share the same
POS tag. We discuss this further in the intrinsic
evaluation presented in ?2.2.
1
We train both models on BLLIP (LDC2000T43) with
PTB removed, the same corpus used by Koo et al (2008) to
train their BROWN clusters. We created a special vector for
unknown words by averaging the vectors for the 50K least
frequent words; we did not use this vector for the SKIP
DEP
(?2.1.2) setting because it performs slightly better without it.
2.1.2 Syntactic Context
We expect embeddings to help dependency pars-
ing the most when words that have similar parents
and children are close in the embedding space. To
target this type of similarity, we train the SKIP
model on dependency context instead of the linear
context in raw text. When ordinarily training SKIP
embeddings, words v
?
are drawn from the neigh-
borhood of a target word v, and the sum of log-
probabilities of each v
?
given v is maximized. We
propose to instead choose v
?
from the set contain-
ing the grandparent, parent, and children words of
v in an automatic dependency parse.
A simple way to implement this idea is to train
the original SKIP model on a corpus of depen-
dency links and labels. For this, we parse the
BLLIP corpus (minus PTB) using our baseline de-
pendency parser, then build a corpus in which each
line contains a single child word c, its parent word
p, its grandparent g, and the dependency label ` of
the ?c, p? link:
?`
<L>
g
<G>
p c `
<L>
?,
that is, both the dependency label and grandparent
word are subscripted with a special token to avoid
collision with words.
2
We train the SKIP model on
this corpus of tuples with window size w = 1, de-
noting the result SKIP
DEP
. Note that this approach
needs a parsed corpus, but there also already ex-
ist such resources (Napoles et al, 2012; Goldberg
and Orwant, 2013).
2.2 Intrinsic Evaluation of Representations
Short of running end-to-end parsing experiments,
how can we choose which representations to use
for parsing tasks? Several methods have been pro-
posed for intrinsic evaluation of word representa-
2
We use a subscript on g so that it will be treated dif-
ferently from c when considering the context of p. We re-
moved all g
<G>
from the vocabulary after training. We also
tried adding information about POS tags. This increases M-1
(?2.2), but harms parsing performance, likely because the em-
beddings become too tag-like. Similar ideas have been used
for clustering (Sagae and Gordon, 2009; Haffari et al, 2011;
Grave et al, 2013), semantic space models (Pad?o and Lapata,
2007), and topic modeling (Boyd-Graber and Blei, 2008).
810
Representation SIM M-1
BROWN ? 89.3
SENNA 49.8 85.2
TURIAN 29.5 87.2
HUANG 62.6 78.1
CBOW, w = 2 34.7 84.8
SKIP, w = 1 37.8 86.6
SKIP, w = 2 43.1 85.8
SKIP, w = 5 44.4 81.1
SKIP, w = 10 44.6 71.5
SKIP
DEP
34.6 88.3
Table 2: Intrinsic evaluation of representations. SIM column
has Spearman?s ?? 100 for 353-pair word similarity dataset.
M-1 is our unsupervised POS tagging metric. For BROWN,
M-1 is simply many-to-one accuracy of the clusters. Best
score in each column is bold.
tions; we discuss two here:
Word similarity (SIM): One widely-used evalu-
ation compares distances in the continuous space
to human judgments of word similarity using the
353-pair dataset of Finkelstein et al (2002). We
compute cosine similarity between the two vectors
in each word pair, then order the word pairs by
similarity and compute Spearman?s rank correla-
tion coefficient (?) with the gold similarities. Em-
beddings with high ? capture similarity in terms of
paraphrase and topical relationships.
Clustering-based tagging accuracy (M-1): In-
tuitively, we expect embeddings to help parsing
the most if they can tell us when two words are
similar syntactically. To this end, we use a met-
ric based on unsupervised evaluation of POS tag-
gers. We perform clustering and map each cluster
to one POS tag so as to maximize tagging accu-
racy, where multiple clusters can map to the same
tag. We cluster vectors corresponding to the to-
kens in PTB WSJ sections 00-21.
3
Table 2 shows these metrics for representations
used in this paper. The BROWN clusters have
the highest M-1, indicating high cluster purity in
terms of POS tags. The HUANG embeddings have
the highest SIM score but low M-1, presumably
because they were trained with global context,
making them more tuned to capture topical sim-
ilarity. We compare several values for the win-
dow size (w) used when training the SKIP embed-
dings, finding that smallw leads to higher M-1 and
lower SIM. Table 3 shows examples of clusters
obtained by clustering SKIP embeddings of w = 1
versus w = 10, and we see that the former cor-
respond closely to POS tags, while the latter are
3
For clustering, we use k-means with k = 1000 and ini-
tialize by placing centroids on the 1000 most-frequent words.
w Example clusters
1 [Mr., Mrs., Ms., Prof., ...], [Jeffrey, Dan, Robert,
Peter, ...], [Johnson, Collins, Schmidt, Freedman,
...], [Portugal, Iran, Cuba, Ecuador, ...], [CST, 4:30,
9-10:30, CDT, ...], [his, your, her, its, ...], [truly,
wildly, politically, financially, ...]
10 [takeoff, altitude, airport, carry-on, airplane, flown,
landings, ...], [health-insurance, clinic, physician,
doctor, medical, health-care, ...], [financing, equity,
investors, firms, stock, fund, market, ...]
Table 3: Example clusters for SKIP embeddings with win-
dow size w = 1 (syntactic) and w = 10 (topical).
much more topically-coherent and contain mixed
POS tags.
4
For parsing experiments, we choose
w = 2 for CBOW and w = 1 for SKIP. Finally,
our SKIP
DEP
embeddings, trained with syntactic
context and w = 1 (?2.1.2), achieve the highest
M-1 of all continuous representations. In ?4, we
will relate these intrinsic metrics to extrinsic pars-
ing performance.
3 Dependency Parsing Features
We now discuss the features that we add to our
baseline dependency parser (second-order MST-
Parser; McDonald and Pereira, 2006) based on
discrete and continuous representations.
3.1 Brown Cluster Features
We start by replicating the features of Koo et al
(2008) using their BROWN clusters; each word is
represented by a 0-1 bit string indicating the path
from the root to the leaf in the binary merge tree.
We follow Koo et al in adding cluster versions of
the first- and second-order features in MSTParser,
using bit string prefixes of the head, argument,
sibling, intermediate words, etc., to augment or
replace the POS and lexical identity information.
We tried various sets of prefix lengths on the devel-
opment set and found the best setting to use pre-
fixes of length 4, 6, 8, and 12.
5
3.2 Continuous Representation Features
We tried two kinds of indicator features:
Bucket features: For both parent and child vec-
tors in a potential dependency, we fire one indi-
cator feature per dimension of each embedding
4
A similar effect, when changing distributional context
window sizes, was found by Lin and Wu (2009).
5
See Koo et al (2008) for the exact feature templates.
They used the full string in place of the length-12 prefixes,
but that setting worked slightly worse for us. Note that the
baseline parser used by Koo et al (2008) is different from the
second-order MSTParser that we use here; their parser allows
grandparent interactions in addition to the sibling interactions
in ours. We use their clusters, available at http://people.
csail.mit.edu/maestro/papers/bllip-clusters.gz.
811
vector, where the feature consists of the dimen-
sion index d and a bucketed version of the embed-
ding value in that dimension, i.e., bucket
k
(E
vd
)
for word index v and dimension d, where E is the
V ?D embedding matrix.
6
We also tried standard
conjunction variants of this feature consisting of
the bucket values of both the head and argument
along with their POS-tag or word information, and
the attachment distance and direction.
7
Cluster bit string features: To take into account
all dimensions simultaneously, we perform ag-
glomerative hierarchical clustering of the embed-
ding vectors. We use Ward?s minimum variance
algorithm (Ward, 1963) for cluster distance and
the Euclidean metric for vector distance (via MAT-
LAB?s linkage function with {method=ward,
metric=euclidean}). Next, we fire features on the
hierarchical clustering bit strings using templates
identical to those for BROWN, except that we use
longer prefixes as our clustering hierarchies tend
to be deeper.
8
4 Parsing Experiments
Setup: We use the publicly-available MST-
Parser for all experiments, specifically its second-
order projective model.
9
We remove all fea-
tures that occur only once in the training data.
For WSJ parsing, we use the standard train(02-
21)/dev(22)/test(23) split and apply the NP brack-
eting patch by Vadas and Curran (2007). For
Web parsing, we still train on WSJ 02-21, but
test on the five Web domains (answers, email,
newsgroup, reviews, and weblog) of the ?English
Web Treebank? (LDC2012T13), splitting each do-
main in half (in original order) for the develop-
ment and test sets.
10
For both treebanks, we con-
vert from constituent to dependency format us-
ing pennconverter (Johansson and Nugues,
2007), and generate POS tags using the MXPOST
tagger (Ratnaparkhi, 1996). To evaluate, we use
6
Our bucketing function bucket
k
(x) converts the real
value x to its closest multiple of k. We choose a k value
of around 1/5th of the embedding?s absolute range.
7
We initially experimented directly with real-valued fea-
tures (instead of bucketed indicator features) and similar con-
junction variants, but these did not perform well.
8
We use prefixes of length 4, 6, 8, 12, 16, 20, and full-
length, again tuned on the development set.
9
We use the recommended MSTParser settings: training-
k:5 iters:10 loss-type:nopunc decode-type:proj
10
Our setup is different from SANCL 2012 (Petrov and
McDonald, 2012) because the exact splits and test data were
only available to participants.
System Dev Test
Baseline 92.38 91.95
BROWN 93.18 92.69
SENNA (Buckets) 92.64 92.04
SENNA (Bit strings) 92.88 92.30
HUANG (Buckets) 92.44 91.86
HUANG (Bit strings) 92.55 92.36
CBOW (Buckets) 92.57 91.93
CBOW (Bit strings) 93.06 92.53
Table 4: Bucket vs. bit string features (UAS on WSJ).
System Dev Test
Baseline 92.38 91.95
BROWN 93.18 92.69
SENNA 92.88 92.30
TURIAN 92.84 92.26
HUANG 92.55 92.36
CBOW 93.06 92.53
SKIP 92.94 92.29
SKIP
DEP
93.33 92.69
Ensemble Results
ALL ? BROWN 93.46 92.90
ALL 93.54 92.98
Table 5: Full results with bit string features (UAS on WSJ).
unlabeled attachment score (UAS).
11
We report
statistical significance (p < 0.01, 100K sam-
ples) using the bootstrap test (Efron and Tibshi-
rani, 1994).
Comparing bucket and bit string features: In
Table 4, we find that bucket features based on in-
dividual embedding dimensions do not lead to im-
provements in test accuracy, while bit string fea-
tures generally do. This is likely because indi-
vidual embedding dimensions rarely correspond to
interpretable or useful distinctions among words,
whereas the hierarchical bit strings take into ac-
count all dimensions of the representations simul-
taneously. Their prefixes also naturally define fea-
tures at multiple levels of granularity.
WSJ results: Table 5 shows our main WSJ
results. Although BROWN yields one of the
highest individual gains, we also achieve statis-
tically significant gains over the baseline from
all embeddings. The CBOW embeddings per-
form as well as BROWN (i.e., no statistically
significant difference) but are orders of magni-
tude faster to train. Finally, the syntactically-
trained SKIP
DEP
embeddings are statistically indis-
tinguishable from BROWN and CBOW, and sig-
nificantly better than all other embeddings. This
suggests that targeting the similarity captured by
syntactic context is useful for dependency parsing.
11
We find similar improvements under labeled attachment
score (LAS). We ignore punctuation : , ? ? . in our evalua-
tion (Yamada and Matsumoto, 2003; McDonald et al, 2005).
812
System ans eml nwg rev blog Avg
Baseline 82.6 81.2 84.3 83.8 85.5 83.5
BROWN 83.4 81.7 85.2 84.5 86.1 84.2
SENNA 83.7 81.9 85.0 85.0 86.0 84.3
TURIAN 83.0 81.5 85.0 84.1 85.7 83.9
HUANG 83.1 81.8 85.1 84.7 85.9 84.1
CBOW 82.9 81.3 85.2 83.9 85.8 83.8
SKIP 83.1 81.1 84.7 84.1 85.4 83.7
SKIP
DEP
83.3 81.5 85.2 84.3 86.0 84.1
Ensemble Results
ALL?BR 83.9 82.2 85.9 85.0 86.6 84.7
ALL 84.2 82.3 85.9 85.1 86.8 84.9
Table 6: Main UAS test results on Web treebanks. Here,
ans=answers, eml=email, nwg=newsgroup, rev=reviews,
blog=weblog, BR=BROWN, Avg=Macro-average.
Web results: Table 6 shows our main Web re-
sults.
12
Here, we see that the SENNA, BROWN,
and SKIP
DEP
embeddings perform the best on av-
erage (and are statistically indistinguishable, ex-
cept SENNA vs. SKIP
DEP
on the reviews domain).
They yield statistically significant UAS improve-
ments over the baseline across all domains, except
weblog for SENNA (narrowly misses significance,
p=0.014) and email for SKIP
DEP
.
13
Ensemble results: When analyzing errors, we
see differences among the representations, e.g.,
BROWN does better at attaching proper nouns,
prepositions, and conjunctions, while CBOW
does better on plural common nouns and adverbs.
This suggests that the representations might be
complementary and could benefit from combina-
tion. To test this, we use a simple ensemble parser
that chooses the highest voted parent for each ar-
gument.
14
As shown in the last two rows of Ta-
bles 5 and 6, this leads to substantial gains. The
?ALL ? BROWN? ensemble combines votes from
all non-BROWN continuous representations, and
the ?ALL? ensemble also includes BROWN.
Characteristics of representations: We now re-
late the intrinsic metrics from ?2.2 to parsing
performance. The clearest correlation appears
when comparing variations of a single model,
e.g., for SKIP, the WSJ dev accuracies are 93.33
(SKIP
DEP
), 92.94 (w = 1), 92.86 (w = 5), and
92.70 (w = 10), which matches the M-1 score or-
der and is the reverse of the SIM score order.
12
We report individual domain results and macro-average
over domains. We do not tune any features/parameters on
Web dev sets; we only show the test results for brevity.
13
Note that SENNA and HUANG are trained on Wikipedia
which may explain why they work better on Web parsing as
compared to WSJ parsing.
14
This does not guarantee a valid tree. Combining features
from representations will allow training to weigh them appro-
priately and also guarantee a tree.
5 Related Work
In addition to work mentioned above, relevant
work that uses discrete representations exists for
POS tagging (Ritter et al, 2011; Owoputi et
al., 2013), named entity recognition (Ratinov
and Roth, 2009), supersense tagging (Grave et
al., 2013), grammar induction (Spitkovsky et al,
2011), constituency parsing (Finkel et al, 2008),
and dependency parsing (Tratz and Hovy, 2011).
Continuous representations in NLP have been
evaluated for their ability to capture syntactic and
semantic word similarity (Huang et al, 2012;
Mikolov et al, 2013a; Mikolov et al, 2013b) and
used for tasks like semantic role labeling, part-
of-speech tagging, NER, chunking, and sentiment
classification (Turian et al, 2010; Collobert et al,
2011; Dhillon et al, 2012; Al-Rfou? et al, 2013).
For dependency parsing, Hisamoto et al (2013)
also used embedding features, but there are several
differences between their work and ours. First,
they use only one set of pre-trained embeddings
(TURIAN) while we compare several and also train
our own, tailored to the task. Second, their em-
bedding features are simpler than ours, only us-
ing flat (non-hierarchical) cluster IDs and binary
strings obtained via sign quantization (1[x > 0])
of the vectors. They also compare to a first-order
baseline and only evaluate on the Web treebanks.
Concurrently, Andreas and Klein (2014) inves-
tigate the use of embeddings in constituent pars-
ing. There are several differences: we work on de-
pendency parsing, use clustering-based features,
and tailor our embeddings to dependency-style
syntax; their work additionally studies vocabulary
expansion and relating in-vocabulary words via
embeddings.
6 Conclusion
We showed that parsing features based on hierar-
chical bit strings work better than those based on
discretized individual embedding values. While
the Brown clusters prove to be well-suited to pars-
ing, we are able to match their performance with
our SKIP
DEP
embeddings that train much faster.
Finally, we found the various representations to
be complementary, enabling a simple ensemble
to perform best. Our SKIP
DEP
embeddings and
bit strings are available at ttic.edu/bansal/
data/syntacticEmbeddings.zip.
813
References
Rami Al-Rfou?, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual NLP. In Proceedings of CoNLL.
Jacob Andreas and Dan Klein. 2014. How much do
word embeddings encode about syntax? In Pro-
ceedings of ACL.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155, March.
Jordan L. Boyd-Graber and David M. Blei. 2008. Syn-
tactic topic models. In Proceedings of NIPS.
Peter F. Brown, Peter V. Desouza, Robert L. Mercer,
Vincent J. Della Pietra, and Jenifer C. Lai. 1992.
Class-based N-gram models of natural language.
Computational Linguistics, 18(4):467?479.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391?407.
Paramveer Dhillon, Dean P. Foster, and Lyle H. Ungar.
2011. Multi-view learning of word embeddings via
CCA. In Proceedings of NIPS.
Paramveer Dhillon, Jordan Rodu, Dean P. Foster, and
Lyle H. Ungar. 2012. Two Step CCA: A new spec-
tral method for estimating vector models of words.
In Proceedings of ICML.
Bradley Efron and Robert J. Tibshirani. 1994. An in-
troduction to the bootstrap, volume 57. CRC press.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. In Proceedings of ACL.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116?131.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of English books. In Second Joint Conference
on Lexical and Computational Semantics (* SEM),
volume 1, pages 241?247.
Edouard Grave, Guillaume Obozinski, and Francis
Bach. 2013. Hidden markov tree models for se-
mantic class induction. In Proceedings of CoNLL.
Gholamreza Haffari, Marzieh Razavi, and Anoop
Sarkar. 2011. An ensemble model that combines
syntactic and semantic clustering for discriminative
dependency parsing. In Proceedings of ACL.
Sorami Hisamoto, Kevin Duh, and Yuji Matsumoto.
2013. An empirical investigation of word repre-
sentations for parsing the web. In Proceedings of
ANLP.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. In Proceedings of ACL.
Fei Huang, Arun Ahuja, Doug Downey, Yi Yang,
Yuhong Guo, and Alexander Yates. 2014. Learning
representations for weakly supervised natural lan-
guage processing tasks. Computational Linguistics,
40(1).
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In 16th Nordic Conference of Computa-
tional Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of ACL-
IJCNLP.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational Linguistics, 19:313?330.
Ryan T. McDonald and Fernando C. Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In Proceedings of EACL.
Ryan T. McDonald, Koby Crammer, and Fernando C.
Pereira. 2005. Spanning tree methods for discrim-
inative training of dependency parsers. Technical
Report MS-CIS-05-11, University of Pennsylvania.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of ICLR.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013b. Distributed repre-
sentations of words and phrases and their composi-
tionality. In Proceedings of NIPS.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In Proceedings of HLT-NAACL.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of ICML.
814
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge
Extraction, AKBC-WEKEX ?12, pages 95?100,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL.
Sebastian Pad?o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of CoNLL.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of EMNLP.
Kenji Sagae and Andrew S. Gordon. 2009. Cluster-
ing words by syntactic similarity improves depen-
dency parsing of predicate-argument structures. In
Proceedings of the 11th International Conference on
Parsing Technologies.
Valentin I. Spitkovsky, Hiyan Alshawi, Angel X.
Chang, and Daniel Jurafsky. 2011. Unsupervised
dependency parsing without gold part-of-speech
tags. In Proceedings of EMNLP.
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
NAACL.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of EMNLP.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of ACL.
David Vadas and James R. Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proceed-
ings of ACL.
Joe H. Ward. 1963. Hierarchical grouping to optimize
an objective function. Journal of the American sta-
tistical association, 58(301):236?244.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of International Conference
on Parsing Technologies.
815
Transactions of the Association for Computational Linguistics, 1 (2013) 279?290. Action Editor: Lillian Lee.
Submitted 11/2012; Revised 1/2013; Published 7/2013. c?2013 Association for Computational Linguistics.
Good, Great, Excellent:
Global Inference of Semantic Intensities
Gerard de Melo
ICSI, Berkeley
demelo@icsi.berkeley.edu
Mohit Bansal
CS Division, UC Berkeley
mbansal@cs.berkeley.edu
Abstract
Adjectives like good, great, and excellent are
similar in meaning, but differ in intensity. In-
tensity order information is very useful for
language learners as well as in several NLP
tasks, but is missing in most lexical resources
(dictionaries, WordNet, and thesauri). In this
paper, we present a primarily unsupervised
approach that uses semantics from Web-scale
data (e.g., phrases like good but not excel-
lent) to rank words by assigning them posi-
tions on a continuous scale. We rely on Mixed
Integer Linear Programming to jointly deter-
mine the ranks, such that individual decisions
benefit from global information. When rank-
ing English adjectives, our global algorithm
achieves substantial improvements over pre-
vious work on both pairwise and rank corre-
lation metrics (specifically, 70% pairwise ac-
curacy as compared to only 56% by previous
work). Moreover, our approach can incorpo-
rate external synonymy information (increas-
ing its pairwise accuracy to 78%) and extends
easily to new languages. We also make our
code and data freely available.1
1 Introduction
Current lexical resources such as dictionaries and
thesauri do not provide information about the in-
tensity order of words. For example, both WordNet
(Miller, 1995) and Roget?s 21st Century Thesaurus
(thesaurus.com) present acceptable, great, and su-
perb as synonyms of the adjective good. However,
a native speaker knows that these words represent
varying intensity and can in fact generally be ranked
by intensity as acceptable< good< great< superb.
Similarly, warm < hot < scorching are identified as
synonyms in these resources. Ranking information,
1http://demelo.org/gdm/intensity/
however, is crucial because it allows us to differen-
tiate e.g. between various intensities of an emotion,
and is hence very useful for humans when learning a
language or judging product reviews, as well as for
automatic text understanding and generation tasks
such as sentiment and subjectivity analysis, recog-
nizing textual entailment, question answering, sum-
marization, and coreference and discourse analysis.
In this work, we attempt to automatically rank
sets of related words by intensity, focusing in par-
ticular on adjectives. This is made possible by the
vast amounts of world knowledge that are now avail-
able. We use lexico-semantic information extracted
from a Web-scale corpus in conjunction with an al-
gorithm based on a Mixed Integer Linear Program
(MILP). Linguistic analyses have identified phrases
such as good but not great or hot and almost scorch-
ing in a text corpus as sources of evidence about the
relative intensities of words. However, pure infor-
mation extraction approaches often fail to provide
enough coverage for real-world downstream appli-
cations (Tandon and de Melo, 2010), unless some
form of advanced inference is used (Snow et al,
2006; Suchanek et al, 2009).
In our work, we address this sparsity problem by
relying on Web-scale data and using an MILP model
that extends the pairwise scores to a more com-
plete joint ranking of words on a continuous scale,
while maintaining global constraints such as transi-
tivity and giving more weight to the order of word
pairs with higher corpus evidence scores. Instead
of considering intensity ranking as a pairwise deci-
sion process, we thus exploit the fact that individual
decisions may benefit from global information, e.g.
about how two words relate to some third word.
Previous work (Sheinman and Tokunaga, 2009;
Schulam and Fellbaum, 2010; Sheinman et al,
2012) has also used lexico-semantic patterns to or-
279
der adjectives. They mainly evaluate their algorithm
on a set of pairwise decisions, but also present a par-
titioning approach that attempts to form scales by
placing each adjective to the left or right of pivot
words. Unfortunately, this approach often fails be-
cause many pairs lack order-based evidence even on
the Web, as explained in more detail in Section 3.
In contrast, our MILP jointly uses information
from all relevant word pairs and captures com-
plex interactions and inferences to produce inten-
sity scales. We can thus obtain an order between
two adjectives even when there is no explicit evi-
dence in the corpus (using evidence for related pairs
and transitive inference). Our global MILP is flex-
ible and can also incorporate additional synonymy
information if available (which helps the MILP find
an even better ranking solution). Our approach also
extends easily to new languages. We describe two
approaches for this multilingual extension: pattern
projection and cross-lingual MILPs.
We evaluate our predicted intensity rankings us-
ing both pairwise classification accuracy and rank-
ing correlation coefficients, achieving strong results,
significantly better than the previous approach by
Sheinman & Tokunaga (32% relative error reduc-
tion) and quite close to human-level performance.
2 Method
In this section, we describe each step of our ap-
proach to ordering adjectives on a single, relative
scale. Our method can also be applied to other word
classes and to languages other than English.
2.1 Web-based Scoring Model
2.1.1 Intensity Scales
Near-synonyms may differ in intensity, e.g. joy
vs. euphoria, or drizzle vs. rain. This is particu-
larly true of adjectives, which can represent different
degrees of a given quality or attribute such as size
or age. Many adjectives are gradable and thus al-
low for grading adverbial modifiers to express such
intensity degrees, e.g., a house can be very big or
extremely big. Often, however, completely differ-
ent adjectives refer to varying degrees on the same
scale, e.g., huge, gigantic, gargantuan. Even adjec-
tives like enormous (or superb, impossible) that are
considered non-gradable from a syntactic perspec-
tive can be placed on a such a scale.
Weak-Strong Patterns Strong-Weak Patterns
? (,) but not ? not ? (,) just ?
? (,) if not ? not ? (,) but just ?
? (,) although not ? not ? (,) still ?
? (,) though not ? not ? (,) but still ?
? (,) (and/or) even ? not ? (,) although still ?
? (,) (and/or) almost ? not ? (,) though still ?
not only ? but ? ? (,) or very ?
not just ? but ?
Table 1: Ranking patterns used in this work. Among the
patterns represented by the regular expressions above, we
use only those that capture less than or equal to five words
(to fit in the Google n-grams, see Section 2.1.2). Articles
(a, an, the) are allowed to appear before the wildcards
wherever possible.
2.1.2 Intensity Patterns
Linguistic studies have found lexical patterns like
?? but not ?? (e.g. good but not great) to reveal order
information between a pair of adjectives (Sheinman
and Tokunaga, 2009). We assume that we have two
sets of lexical patterns that allow us to infer the most
likely ordering between two words when encoun-
tered in a corpus. A first pattern set, Pws, contains
patterns that reflect a weak-strong order between a
pair of word (the first word is weaker than the sec-
ond), and a second pattern set, Psw, captures the
strong-weak order. See Table 1 for the adjective pat-
terns that we used in this work (and see Section 4.1
for implementation details regarding our pattern col-
lection). Many of these patterns also apply to other
parts of speech (e.g. ?drizzle but not rain?, ?running
or even sprinting?), with significant discrimination
on the Web in the right direction.
2.1.3 Pairwise Scores
Given an input set of words to be placed on a
scale, we first collect evidence of their intensity or-
der by using the above-mentioned intensity patterns
and a large, Web-scale text corpus.
Previous work on information extraction from
limited-sized raw text corpora revealed that cover-
age is often limited (Hearst, 1992; Hatzivassiloglou
and McKeown, 1993). Some studies (Chklovski
and Pantel, 2004; Sheinman and Tokunaga, 2009)
used hit counts from an online search engine, but
this is unstable and irreproducible (Kilgarriff, 2007).
To avoid these issues, we use the largest available
280
(good, great) (great, good) (small, minute)
good , but not great? 24492.0 not great , just good? 248.0 small , almost minute? 97.0
good , if not great? 1912.0 great or very good? 89.0 small , even minute? 41.0
good , though not great? 504.0 not great but still good? 47.0
good , or even great? 338.0
not just good but great? 181.0
good , almost great? 156.0
Table 2: Some examples from the Web-scale corpus of useful intensity-based phrases on adjective pairs.
static corpus of counts, the Google n-grams corpus
(Brants and Franz, 2006), which contains English
n-grams (n = 1 to 5) and their observed frequency
counts, generated from nearly 1 trillion word tokens
and 95 billion sentences.
We consider each pair of words (a1, a2) in the in-
put set in turn. For each pattern p in the two pattern
sets (weak-strong Pws and strong-weak Psw), we in-
sert the word pair into the pattern as p(a1, a2) to get
a phrasal query like ?big but not huge?. This is done
by replacing the two wildcards in the pattern by the
two words in order. Finally, we scan the Web n-
grams corpus in a batch approach similar to Bansal
and Klein (2011) and collect frequencies of all our
phrase queries. Table 2 depicts some examples of
useful intensity-based phrase queries and their fre-
quencies in the Web-scale corpus. We also collect
frequencies for the input word unigrams and the pat-
terns for normalization purposes. Given a word pair
(a1, a2) and a corpus count function cnt, we define
W1 =
1
P1
?
p1?Pws
cnt(p1(a1, a2))
S1 =
1
P2
?
p2?Psw
cnt(p2(a1, a2))
W2 =
1
P1
?
p1?Pws
cnt(p1(a2, a1))
S2 =
1
P2
?
p2?Psw
cnt(p2(a2, a1)) (1)
with
P1 =
?
p1?Pws
cnt(p1)
P2 =
?
p2?Psw
cnt(p2), (2)
such that the final overall weak-strong score is
score(a1, a2) =
(W1 ? S1)? (W2 ? S2)
cnt(a1) ? cnt(a2)
. (3)
Here W1 and S1 represent Web evidence of a1
and a2 being in the weak-strong and strong-weak
relation, respectively. W2 and S2 fit the reverse
pair (a2, a1) in the patterns and hence represent
the strong-weak and weak-strong relations, respec-
tively, in the opposite direction. Hence, overall,
(W1 ? S1) ? (W2 ? S2) represents the total weak-
strong score of the pair (a1, a2), i.e. the score of a1
being on the left of a2 on a relative intensity scale,
such that score(a1, a2) = ?score(a2, a1). The raw
frequencies in the score are divided by counts of the
patterns and by individual word unigram counts to
obtain a pointwise mutual information (PMI) style
normalization and hence avoid any bias in the score
due to high-frequency patterns or word unigrams.2
2.2 Global Ordering with an MILP
2.2.1 Objective and Constraints
Given pairwise scores, we now aim at producing a
global ranking of the input words that is much more
informative than the original pairwise scores. Joint
inference from multiple word pairs allows us to ben-
efit from global information: Due to the sparsity of
the pattern evidence, determining how two adjec-
tives relate to each other can sometimes e.g. only
be inferred by observing how each of them relate to
some third adjective.
We assume that we are given N input words A =
a1, . . . , aN that we wish to place on a linear scale,
say [0, 1]. Thus each word ai is to be assigned a
position xi ? [0, 1] based on the pairwise weak-
strong weights score(ai, aj). A positive value for
2In preliminary experiments on a development set, we also
evaluated other intuitive forms of normalization.
281
Figure 1: The input weak-strong data may contain one
or more cycles, e.g. due to noisy patterns, so the final
ranking will have to choose which input scores to honor
and which to remove.
score(ai, aj) means that ai is supposedly weaker
than aj and hence we would like to obtain xi < xj .
A negative value for score(ai, aj) means that ai is
assumed to be stronger than aj , so we would want
to obtain xi > xj . Therefore, intuitively, our goal
corresponds to maximizing the objective
?
i,j
sgn(xj ? xi) ? score(ai, aj) (4)
Note that it is important to use the signum func-
tion sgn() here, because we only care about the rel-
ative order of xi and xj . Maximizing?ij(xj?xi) ?
score(ai, aj) would lead to all words being placed
at the edges of the scale, because the highest scores
would dominate over all other ones. We do include
the score magnitudes in the objective, because they
help resolve contradictions in the pairwise scores
(e.g., see Figure 1). This is discussed in more de-
tail in Section 2.2.2.
In order to maximize this non-differentiable ob-
jective, we use Mixed Integer Linear Programming
(MILP), a variant of linear programming in which
some but not all of the variables are constrained to
be integers. Using an MILP formalization, we can
find a globally optimal solution in the joint deci-
sion space, and unlike previous work, we jointly ex-
ploit global information rather than just individual
local (pairwise) scores. To encode the objective in a
MILP, we need to introduce additional variables dij ,
wij , sij to capture the effect of the signum function,
as explained below.
We additionally also enable our MILP to make
use of any external equivalence (synonymy) infor-
mation E ? {1, . . . , N} ? {1, . . . , N} that may be
available. In this context, two words are considered
synonymous if they are close enough in meaning to
be placed on (almost) the same position in the inten-
sity scale. If (i, j) ? E, we can safely assume that
ai, aj have near-equivalent intensity, so we should
encourage xi, xj to remain close to each other. The
MILP is defined as follows:
maximize?
(i,j) 6?E
(wij ? sij) ? score(ai, aj)
?
?
(i,j)?E
(wij + sij) C
subject to
dij = xj ? xi ?i, j ? {1, . . . , N}
dij ? wijC ? 0 ?i, j ? {1, . . . , N}
dij + (1? wij)C > 0 ?i, j ? {1, . . . , N}
dij + sijC ? 0 ?i, j ? {1, . . . , N}
dij ? (1? sij)C < 0 ?i, j ? {1, . . . , N}
xi ? [0, 1] ?i ? {1, . . . , N}
wij ? {0, 1} ?i, j ? {1, . . . , N}
sij ? {0, 1} ?i, j ? {1, . . . , N}
The difference variables dij simply capture differ-
ences between xi, xj . C is any very large constant
greater than ?i,j |score(ai, aj)|; the exact value is
irrelevant. The indicator variables wij and sij are
jointly used to determine the value of the signum
function sgn(dij) = sgn(xj ? xi). Variables wij
become 1 if and only if dij > 0 and hence serve
as indicator variables for weak-strong relationships
in the output. Variables sij become 1 if and only if
dij < 0 and hence serve as indicator variables for
a strong-weak relationship in the output. The ob-
jective encourages wij = 1 for score(ai, aj) > 0
and sij = 1 for score(ai, aj) < 0.3 When equiva-
lence (synonymy) information is available, then for
(i, j) ? E both sij = 0 andwij = 0 are encouraged.
2.2.2 Discussion
Our MILP uses intensity evidence of all input
pairs together and assimilates all the scores via
global transitivity constraints to determine the posi-
tions of the input words on a continuous real-valued
scale. Hence, our approach addresses drawbacks
3In order to avoid numeric instability issues due to very
small score(ai, aj) values after frequency normalization, in
practice we have found it necessary to rescale them by a fac-
tor of 1 over the smallest |score(ai, aj)| > 0.
282
Figure 2: Equivalence Information: Knowing that am, a2
are synonyms gives the MILP an indication of where to
place an on the scale with respect to a1, a2, a3
of local or divide-and-conquer approaches, where
adjectives are scored with respect to selected pivot
words, and hence many adjectives that lack pairwise
evidence with the pivots are not properly classified,
although they may have order evidence with some
third adjective that could help establish the ranking.
Optional synonymy information can further help, as
shown in Figure 2.
Moreover, our MILP also gives higher weight
to pairs with higher scores, which is useful when
breaking global constraint cycles as in the simple
example in Figure 1. If we need to break a con-
straint violating triangle or cycle, we would have to
make arbitrary choices if we were ranking based on
sgn(score(a, b)) alone. Instead, we can choose a
better ranking based on the magnitude of the pair-
wise scores. A stronger score between an adjective
pair doesn?t necessarily mean that they should be
further apart in the ranking. It means that these two
words are attested together on the Web with respect
to the intensity patterns more than with other candi-
date words. Therefore, we try to respect the order of
such word pairs more in the final ranking when we
are breaking constraint-violating cycles.
3 Related Work
Hatzivassiloglou and McKeown (1993) presented
the first step towards automatic identification of ad-
jective scales, thoroughly discussing the background
of adjective semantics and a means of discovering
clusters of adjectives that belong on the same scale,
thus providing one way of creating the input for our
ranking algorithm.
Inkpen and Hirst (2006) study near-synonyms and
nuances of meaning differentiation (such as stylistic,
attitudinal, etc.). They attempt to automatically ac-
quire a knowledge base of near-synonym differences
via an unsupervised decision-list algorithm. How-
ever, their method depends on a special dictionary
of synonym differences to learn the extraction pat-
terns, while we use only a raw Web-scale corpus.
Mohammad et al (2013) proposed a method of
identifying whether two adjectives are antonymous.
This problem is related but distinct, because the de-
gree of antonymy does not necessarily determine
their position on an intensity scale. Antonyms (e.g.,
little, big) are not necessarily on the extreme ends of
scales.
Sheinman and Tokunaga (2009) and Sheinman et
al. (2012) present the most closely related previous
work on adjective intensities. They collect lexico-
semantic patterns via bootstrapping from seed adjec-
tive pairs to obtain pairwise intensities, albeit using
search engine ?hits?, which are unstable and prob-
lematic (Kilgarriff, 2007). While their approach
is primarily evaluated in terms of a local pairwise
classification task, they also suggest the possibil-
ity of ordering adjectives on a scale using a pivot-
based partitioning approach. Although intuitive in
theory, the extracted pairwise scores are frequently
too sparse for this to work. Thus, many adjec-
tives have no score with a particular headword. In
our experiments, we reimplemented this approach
and show that our MILP method improves over it
by allowing individual pairwise decisions to benefit
more from global information. Schulam and Fell-
baum (2010) apply the approach of Sheinman and
Tokunaga (2009) to German adjectives. Our method
extends easily to various foreign languages as de-
scribed in Section 5.
Another related task is the extraction of lexico-
syntactic and lexico-semantic intensity-order pat-
terns from large text corpora (Hearst, 1992;
Chklovski and Pantel, 2004; Tandon and de Melo,
2010). Sheinman and Tokunaga (2009) follows
Davidov and Rappoport (2008) to automatically
bootstrap adjective scaling patterns using seed ad-
jectives and Web hits. These methods thus can be
used to provide the input patterns for our algorithm.
VerbOcean by Chklovski and Pantel (2004) ex-
tracts various fine-grained semantic relations (in-
cluding the stronger-than relation) between pairs of
verbs, using lexico-syntactic patterns over the Web.
283
Our approach of jointly ranking a set of words using
pairwise evidence is also applicable to the VerbO-
cean pairs, and should help address similar sparsity
issues of local pairwise decisions. Such scales will
again be quite useful for language learners and lan-
guage understanding tools.
de Marneffe et al (2010) infer yes-or-no answers
to questions with responses involving scalar adjec-
tives in a dialogue corpus. They correlate adjectives
with ratings in a movie review corpus to find that
good appears in lower-rated reviews than excellent.
Finally, there has been a lot of work on measuring
the general sentiment polarity of words (Hatzivas-
siloglou and McKeown, 1997; Hatzivassiloglou and
Wiebe, 2000; Turney and Littman, 2003; Liu and
Seneff, 2009; Taboada et al, 2011; Yessenalina and
Cardie, 2011; Pang and Lee, 2008). Our work in-
stead aims at producing a large, unrestricted number
of individual intensity scales for different qualities
and hence can help in fine-grained sentiment analy-
sis with respect to very particular content aspects.
4 Experiments
4.1 Data
Input Clusters In order to obtain input clusters for
evaluation, we started out with the satellite cluster or
?dumbbell? structure of adjectives in WordNet 3.0,
which consists of two direct antonyms as the poles
and a number of other satellite adjectives that are se-
mantically similar to each of the poles (Gross and
Miller, 1990). For each antonymy pair, we deter-
mined an extended dumbbell set by looking up syn-
onyms and words in related (satellite adjective and
?see-also?) synonym sets. We cut such an extended
dumbbell into two antonymous halves and treated
each of these halves as a potential input adjective
cluster.
Most of these WordNet clusters are noisy for the
purpose of our task, i.e. they contain adjectives that
appear unrelatable on a single scale due to polysemy
and semantic drift, e.g. violent with respect to super-
natural and affected. Motivated by Sheinman and
Tokunaga (2009), we split such hard-to-relate ad-
jectives into smaller scale-specific subgroups using
the corpus evidence4. For this, we consider an undi-
4Note that we do not use the WordNet dataset of Sheinman
and Tokunaga (2009) for evaluation, as it does not provide full
438 
115 60 35 19 12 14 5 4 3 0 100 
200 300 
400 500 
2 3 4 5 6 7 8 9 10-14 15-17 
# of c
hains
 
Length of chain 
Figure 3: The histogram of cluster sizes after partitioning.
41 
27 
12 3 3 2 0 
10 
20 
30 
40 
50 
3 4 5 6 7 8 
# of 
chai
ns 
Length of chain 
Figure 4: The histogram of cluster sizes in the test set.
rected edge between each pair of adjectives that has
a non-zero intensity score (based on the Web-scale
scoring procedure described in Section 2.1.3). The
resulting graph is then partitioned into connected
components such that any adjectives in a subgraph
are at least indirectly connected via some path and
thus much more likely to belong to the same inten-
sity scale. While this does break up partitions when-
ever there is no corpus evidence connecting them,
ordering the adjectives within each such partition re-
mains a challenging task. This is because the Web
evidence will still not necessarily directly relate all
adjectives (in a partition) to each other. Addition-
ally, the Web evidence may still indicate the wrong
direction. Figure 3 shows the size distribution of the
resulting partitions.
Patterns To construct our intensity pattern set, we
started with a couple of common rankable adjective
seed pairs such as (good, great) and (hot, boiling)
and used the Web-scale n-grams corpus (Brants and
Franz, 2006) to collect the few most frequent pat-
terns between and around these seed-pairs (in both
directions). Among these, we manually chose a
scales. Instead, their annotators only made pairwise compar-
isons with select words, using a 5-way classification scheme
(neutral, mild, very mild, intense, very intense).
284
small set of intuitive patterns that are linguistically
useful for ordering adjectives, several of which had
not been discovered in previous work. These are
shown in Table 1. Note that we only collected pat-
terns that were not ambiguous in the two orders, for
example the pattern ?? , not ?? is ambiguous be-
cause it can be used as both ?good, not great? and
?great, not good?. Alternatively, one can easily also
use fully-automatic bootstrapping techniques based
on seed word pairs (Hearst, 1992; Chklovski and
Pantel, 2004; Yang and Su, 2007; Turney, 2008;
Davidov and Rappoport, 2008). However, our semi-
automatic approach is a simple and fast process that
extracts a small set of high-quality and very gen-
eral adjective-scaling patterns. This process can
quickly be repeated from scratch in any other lan-
guage. Moreover, as described in Section 5.1, the
English patterns can also be projected automatically
to patterns in other languages.
Development and Test Sets Section 2.1 describes
the method for collecting the intensity scores for ad-
jective pairs, using Web-scale n-grams (Brants and
Franz, 2006). We relied on a small development
set to test the MILP structure and the pairwise score
setup. For this, we manually chose 5 representative
adjective clusters from the full set of clusters.
The final test set, distinct from this development
set, consists of 569 word pairs in 88 clusters, each
annotated by two native speakers of English. Both
the gold test data (and our code) are freely avail-
able.5 To arrive at this data, we randomly drew 30
clusters each for cluster sizes 3, 4, and 5+ from the
histogram of partitioned adjective clusters in Fig-
ure 3. While labeling a cluster, annotators could ex-
clude words that they deemed unsuitable to fit on
a single shared intensity scale with the rest of the
cluster. Fortunately, the partitioning described ear-
lier had already separated most such cases into dis-
tinct clusters. The annotators ordered the remaining
words on a scale. Words that seemed indistinguish-
able in strength could share positions in their anno-
tation.
As our goal is to compare scale formation algo-
rithms, we did not include trivial clusters of size 2.
On such trivial clusters, the Web evidence alone de-
termines the output and hence all algorithms, includ-
5http://demelo.org/gdm/intensity/
ing the baseline, obtain the same pairwise accuracy
(defined below) of 93.3% on a separate set of 30 ran-
dom clusters of size 2.
Figure 4 shows the distribution of cluster sizes in
our main gold set. The inter-annotator agreement in
terms of Cohen?s ? (Cohen, 1960) on the pairwise
classification task with 3 labels (weaker, stronger,
or equal/unknown) was 0.64. In terms of pairwise
accuracy, the agreement was 78.0%.
4.2 Metrics
In order to thoroughly evaluate the performance of
our adjective ordering procedure, we rely on both
pairwise and ranking-correlation evaluation metrics.
Consider a set of input words A = {a1, a2, . . . , an}
and two rankings for this set ? a gold-standard rank-
ing rG(A) and a predicted ranking rP (A).
4.2.1 Pairwise Accuracy
For a pair of words ai, aj , we may consider the
classification task of choosing one of three labels (<,
>, =?) for the case of ai being weaker, stronger, and
equal (or unknown) in intensity, respectively, com-
pared to a2:
L(a1, a2) =
?
?
?
< if r(ai) < r(aj)
> if r(ai) > r(aj)
=? if r(ai) = r(aj)
For each pair (a1, a2), we compute gold-standard
labelsLG(a1, a2) and predicted labelsLP (a1, a2) as
above, and then the pairwise accuracy PW (A) for
a particular ordering on A is simply the fraction of
pairs that are correctly classified, i.e. for which the
predicted label is same as the gold-standard label:
PW (A) =
?
i<j
1{LG(ai, aj) = LP (ai, aj)}
?
i<j
1
4.2.2 Ranking Correlation Coefficients
Our second type of evaluation assesses the
rank correlation between two ranking permutations
(gold-standard and predicted). Many studies use
Kendall?s tau (Kendall, 1938), which measures the
total number of pairwise inversions, while others
prefer Spearman?s rho (Spearman, 1904), which
measures the L1 distance between ranks.
285
Kendall?s tau correlation coefficient We use the
?b version of Kendall?s correlation metric, as it in-
corporates a correction for ties (Kruskal, 1958; Dou
et al, 2008):
?b =
P ?Q?
(P +Q+X0) ? (P +Q+ Y0)
where P is the number of concordant pairs, Q is
the number of discordant pairs, X0 is the number
of pairs tied in the first ranking, Y0 is the number of
pairs tied in the second ranking. Given the two rank-
ings of an adjective set A, the gold-standard ranking
rG(A) and the predicted ranking rP (A), two words
ai, aj are:
? concordant iff both rankings have the same strict
order of the two elements, i.e., rG(ai) > rG(aj)
and rP (ai) > rP (aj), or rG(ai) < rG(aj) and
rP (ai) < rP (aj).
? discordant iff the two rankings have an inverted
strict order of the two elements, i.e., rG(ai) >
rG(aj) and rP (ai) < rP (aj), or rG(ai) <
rG(aj) and rP (ai) > rP (aj).
? tied iff rG(ai) = rG(aj) or rP (ai) = rP (aj).
Spearman?s rho correlation coefficient For two
n-sized ranked lists {xi} and {yi}, the Spearman
correlation coefficient is defined as the Pearson cor-
relation coefficient between the ranks of variables:
? =
?
i
(xi ? x?) ? (yi ? y?)
??
i
(xi ? x?)2 ?
?
i
(yi ? y?)2
Here, x? and y? denote the means of the values in the
respective lists. We use the standard procedure for
handling ties correctly. Tied values are assigned the
average of all ranks of items sharing the same value
in the ranked list sorted in ascending order of the
values.
Handling Inversions While annotating, we some-
times observed that the ordering itself was very clear
but the annotators disagreed about which end of a
particular scale was to count as the strong one, e.g.
when transitioning from soft to hard or from alpha
to beta. We thus also report average absolute values
of both correlation coefficients, as these properly ac-
count for anticorrelations. Our test set only contains
clusters of size 3 or larger, so there is no need to
account for inversions in clusters of size 2.
4.3 Results
In Table 3, we use the evaluation metrics mentioned
above to compare several different approaches.
Web Baseline The first baseline simply reflects
the original pairwise Web-based intensity scores.
We classify (with one of 3 labels) a given pair of
adjectives using the Web-based intensity scores (as
described in Section 2.1.3) as follows:
Lbaseline(a1, a2) =
?
?
?
< if score(ai, aj) > 0
> if score(ai, aj) < 0
=? if score(ai, aj) = 0
Since score(ai, aj) represents the weak-strong
score of the two adjectives, a more positive value
means a higher likelihood of ai being weaker (<, on
the left) in intensity than aj .
In Table 3, we observe that the (micro-averaged)
pairwise accuracy, as defined earlier, for the origi-
nal Web baseline is 48.2%, while the ranking mea-
sures are undefined because the individual pairs do
not lead to a coherent scale.
Divide-and-Conquer The divide-and-conquer
baseline recursively splits a set of words into three
subgroups, placed to the left (weaker), on the same
position (no evidence), or to the right (stronger) of a
given randomly chosen pivot word.
While this approach shows only a minor improve-
ment in terms of the pairwise accuracy (50.6%), its
main benefit is that one obtains well-defined inten-
sity scales rather than just a collection of pairwise
scores.
Sheinman and Tokunaga The approach by
Sheinman and Tokunaga (2009) involves a simi-
lar divide-and-conquer based partitioning in the first
phase, except that their method makes use of syn-
onymy information from WordNet and uses all syn-
onyms in WordNet?s synset for the headword as
neutral pivot elements (if the headword is not in
WordNet, then the word with the maximal unigram
frequency is chosen). In the second phase, their
method performs pairwise comparisons within the
more intense and less intense subgroups. We reim-
plement their approach here, using the Google N-
Grams dataset instead of online Web search engine
hits. We observe a small improvement over the Web
baseline in terms of pairwise accuracy. Note that the
286
Method Pairwise Accuracy Avg. ? Avg. |? | Avg. ? Avg. |?|
Web Baseline 48.2% N/A N/A N/A N/A
Divide-and-Conquer 50.6% 0.45 0.53 0.52 0.62
Sheinman and Tokunaga (2009) 55.5% N/A N/A N/A N/A
MILP 69.6% 0.57 0.65 0.64 0.73
MILP with synonymy 78.2% 0.57 0.66 0.67 0.80
Inter-Annotator Agreement 78.0% 0.67 0.76 0.75 0.86
Table 3: Main test results
Predicted Class
Weaker Tie Stronger
True Class
Weaker 117 127 15
Tie 5 42 15
Stronger 11 122 115
Table 4: Confusion matrix (Web baseline)
rank correlation measure scores are undefined for
their approach. This is because in some cases their
method placed all words on the same position in the
scale, which these measures cannot handle even in
their tie-corrected versions. Overall, the Sheinman
and Tokunaga approach does not aggregate informa-
tion sufficiently well at the global level and often
fails to make use of transitive inference.
MILP Our MILP exploits the same pairwise
scores to induce significantly more accurate pair-
wise labels with 69.6% accuracy, a 41% relative
error reduction over the Web baseline, 38% over
Divide-and-Conquer, and 32% over Sheinman and
Tokunaga (2009). We further see that our MILP
method is able to exploit external synonymy (equiv-
alence) information (using synonyms marked by the
annotators). The accuracy of the pairwise scores as
well as the quality of the overall ranking increase
even further to 78.2%, approaching the human inter-
annotator agreement. In terms of average correlation
coefficients, we observe similar improvement trends
from the MILP, but of different magnitudes, because
these averages give small clusters the same weight
as larger ones.
4.4 Analysis
Confusion Matrices For a given approach, we
can study the confusion matrix obtained by cross-
tabulating the gold classification with the predicted
Predicted Class
Weaker Tie Stronger
True Class
Weaker 177 29 53
Tie 9 24 29
Stronger 15 38 195
Table 5: Confusion matrix (MILP)
classification of every unique pair of adjectives in
the ground truth data. Table 4 shows the confusion
matrix for the Web baseline. We observe that due to
the sparsity of pairwise intensity order evidence, the
baseline method predicts too many ties.
Table 5 provides the confusion matrix for the
MILP (without external equivalence information)
for comparison. Although the middle column still
shows that the MILP predicts more ties than humans
annotators, we find that a clear majority of all unique
pairs are now correctly placed along the diagonal.
This confirms that our MILP successfully infers new
ordering decisions, although it uses the same input
(corpus evidence) as the baseline. The remaining
ties are mostly just the result of pairs for which there
simply is no evidence at all in the input Web counts.
Note that this problem could for instance be circum-
vented by relying on a crowdsourcing approach: A
few dispersed tie-breakers are enough to allow our
MILP to correct many other predictions.
Predicted Examples Finally, in Table 6, we pro-
vide a selection of real results obtained by our algo-
rithm. For instance, it correctly inferred that terri-
fying is more intense than creepy or scary, although
the Web pattern counts did not provide any explicit
information about these words pairs. In some cases,
however, the Web evidence did not suffice to draw
the right conclusions, or it was misleading due to is-
sues like polysemy (as for the word funny).
287
Accuracy Prediction Gold Standard
Good
hard
< painful
< hopeless
hard
< painful
< hopeless
full
< stuffed
< (overflowing,
overloaded)
full
< stuffed
< overflowing
< overloaded
unusual
< uncommon
< rare
< exceptional
< extraordinary
uncommon
< unusual
< rare
< extraordinary
< exceptional
Average creepy
< scary
< sinister
< frightening
< terrifying
creepy
< (scary, frightening)
< terrifying
< sinister
Bad (awake, conscious)< alive
< aware
alive
< awake
< (aware, conscious)
strange
< (unusual, weird)
< (funny, eerie)
(strange, funny)
< unusual
< weird
< eerie
Table 6: Some examples (of bad, average and good accu-
racy) of our MILP predictions (without synonymy infor-
mation) and the corresponding gold-standard annotation.
While we show results on gold-standard chains
here for evaluation purposes, in practice one can also
recombine two [0, 1] chains for a pair of antonymic
clusters to form a single scale from [?1, 1] that visu-
alizes the full spectrum of available adjectives along
a dimension, from adjacent all the way to removed,
or from black to glaring.
5 Extension to Multilingual Ordering
Our method for globally ordering words on a scale
can easily be applied to languages other than En-
glish. The entire process is language-independent
as long as the required resources are available and a
small number of patterns are chosen. For morpho-
logically rich languages, the information extraction
step of course may require additional morphologi-
cal analysis tools for stemming and aggregating fre-
quencies across different forms.
Alternatively, a cross-lingual projection approach
is possible at multiple levels, utilizing information
from the English data and ranking. As the first step,
the set of words in the target language that we wish
to rank can be projected from the English word set if
necessary ? e.g., as shown in de Melo and Weikum
(2009). Next, we outline two projection methods for
the ordering step. The first method is based on pro-
jection of the English intensity-ordering patterns to
the new language, and then using the same MILP
as described in Section 2.2. In the second method,
we also change the MILP and add cross-lingual con-
straints to better inform the target language?s ad-
jective ranking. A detailed empirical evaluation of
these approaches remains future work.
5.1 Cross-Lingual Pattern Projection
Instead of creating new patterns, in many cases
we obtain quite adequate intensity patterns by us-
ing cross-lingual projection. We simply take sev-
eral adjective pairs, instantiate the English patterns
with them, and obtain new patterns using a machine
translation system. Filling the wildcards in a pat-
tern, say ?? but not ??, with good/excellent results in
?good but not excellent?. This phrase is then trans-
lated into the target language using the translation
system, say into German ?gut aber nicht ausgezeich-
net?. Finally, put back the wildcards in the place of
the translations of the adjective words, here gut and
ausgezeichnet, to get the corresponding German pat-
tern ?? aber nicht ??. Table 7 shows various German
intensity patterns that we obtain by projecting from
the English patterns as described. The process is re-
peated with multiple adjective pairs in case different
variants are returned, e.g. due to morphology. Most
of these translations deliver useful results.
Now that we have the target language adjectives
and the ranking patterns, we can compute the pair-
wise intensity scores using large-scale data in that
language. We can use the Google n-grams cor-
pora for 10 European languages (Brants and Franz,
2009), and also for Chinese (LDC2010T02) and
Japanese (LDC2009T08). For other languages, one
can use available large raw-text corpora or Web
crawling tools.
5.2 Crosslingual MILP
To improve the rankings for lesser-resourced lan-
guages, we can further use a joint MILP approach
for the new language we want to transfer this pro-
cess to. Additional constraints between the English
288
Weak-Strong Patterns Strong-Weak Patterns
English German English German
? but not ? ? aber nicht ? not ? just ? nicht ? gerade ?
? if not ? ? wenn nicht ? not ? but just ? nicht ? aber nur ?
? and almost ? ? und fast ? not ? though still ? nicht ? aber immer noch ?
not just ? but ? nicht nur ? sondern ? ? or very ? ? oder sehr ?
Table 7: Examples of German intensity patterns projected (translated) directly from the English patterns.
words and their corresponding target language trans-
lations, in combination with the English ranking in-
formation, allow the algorithm to obtain better rank-
ings for the target words whenever the non-English
target language corpus does not provide sufficient
intensity order evidence.
In this case, the input set A contains words
in multiple languages. The Web intensity scores
score(ai, aj) should be set to zero when comparing
words across languages. We instead link them using
a translation table T ? {1, . . . , N} ? {1, . . . , N}
from a translation dictionary or phrase table. Here,
(i, j) ? T signifies that ai is a translation of aj . We
do not require a bijective relationship between them
(i.e., translations needn?t be unique). The objective
function is augmented by adding the new term
?
(i,j)?T
(w?ij + s?ij)CT (5)
for a constant CT > 0 that determines how much
weight we assign to translations as opposed to the
corpus count scores. The MILP is extended by
adding the following extra constraints.
dij ? w?ijCT < ?dmax ?i, j ? {1, . . . , N}
dij + (1? w?ij)CT ? ?dmax ?i, j ? {1, . . . , N}
dij + s?ijCT > dmax ?i, j ? {1, . . . , N}
dij ? (1? s?ij)CT ? dmax ?i, j ? {1, . . . , N}
w?ij ? {0, 1} ?i, j ? T
s?ij ? {0, 1} ?i, j ? T
The variables di,j , as before, encode distances be-
tween positions of words on the scale, but now also
include cross-lingual pairs of words in different lan-
guages. The new constraints encourage translational
equivalents to remain close to each other, preferably
within a desired (but not strictly enforced) maximum
distance dmax. The new variables w?ij , s?ij are sim-
ilar to wij , sij in the standard MILP. However, the
w?ij become 1 if and only if dij ? ?dmax and the s?ij
become 1 if and only if dij ? dmax. If both w?ij and
s?ij are 1, then the two words have a small distance
?dmax ? dij ? dmax. The augmented objective
function explicitly encourages this for translational
equivalents. Overall, this approach thus allows evi-
dence from a language with more Web evidence to
improve the process of adjective ordering in lesser-
resourced languages.
6 Conclusion
In this work, we have presented an approach to the
challenging and little-studied task of ranking words
in terms of their intensity on a continuous scale. We
address the issue of sparsity of the intensity order ev-
idence in two ways. First, pairwise intensity scores
are computed using linguistically intuitive patterns
in a very large, Web-scale corpus. Next, a Mixed
Integer Linear Program (MILP) expands on this fur-
ther by inferring new relative relationships. Instead
of making ordering decisions about word pairs in-
dependently, our MILP considers the joint decision
space and factors in e.g. how two adjectives relate
to some third adjective, thus enforcing global con-
straints such as transitivity.
Our approach is general enough to allow addi-
tional evidence such as synonymy in the MILP,
and can straightforwardly be applied to other word
classes (such as verbs), and to other languages
(monolingually as well as cross-lingually). The
overall results across multiple metrics are substan-
tially better than previous approaches, and fairly
close to human agreement on this challenging task.
Acknowledgments
We would like to thank the editor and the anony-
mous reviewers for their helpful feedback.
289
References
Mohit Bansal and Dan Klein. 2011. Web-scale features
for full-scale parsing. In Proceedings of ACL 2011.
Thorsten Brants and Alex Franz. 2006. The Google Web
1T 5-gram corpus version 1.1. LDC2006T13.
Thorsten Brants and Alex Franz. 2009. Web 1T 5-gram,
10 European languages, version 1. LDC2009T25.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In Proceedings of EMNLP 2004.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20(1):37?46.
Dmitry Davidov and Ari Rappoport. 2008. Unsuper-
vised discovery of generic relationships using pattern
clusters and its evaluation by automatically generated
sat analogy questions. In Proceedings of ACL 2008.
Marie-Catherine de Marneffe, Christopher D. Manning,
and Christopher Potts. 2010. Was it good? it was
provocative. learning the meaning of scalar adjectives.
In Proceedings of ACL 2010.
Gerard de Melo and Gerhard Weikum. 2009. Towards
a universal wordnet by learning from combined evi-
dence. In Proceedings of CIKM 2009.
Zhicheng Dou, Ruihua Song, Xiaojie Yuan, and Ji-Rong
Wen. 2008. Are click-through data adequate for learn-
ing web search rankings? In Proc. of CIKM 2008.
Derek Gross and Katherine J. Miller. 1990. Adjectives
in WordNet. International Journal of Lexicography,
3(4):265?277.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1993. Towards the automatic identification of adjecti-
val scales: Clustering adjectives according to meaning.
In Proceedings of ACL 1993.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of ACL 1997.
Vasileios Hatzivassiloglou and Janyce M. Wiebe. 2000.
Effects of adjective orientation and gradability on sen-
tence subjectivity. In Proceedings of COLING 2000.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING
1992.
Diana Inkpen and Graeme Hirst. 2006. Building and
using a lexical knowledge base of near-synonym dif-
ferences. Computational Linguistics, 32(2):223?262.
Maurice G. Kendall. 1938. A new measure of rank cor-
relation. Biometrika, 30(1/2):81?93.
Adam Kilgarriff. 2007. Googleology is bad science.
Computational Linguistics, 33(1).
William H. Kruskal. 1958. Ordinal measures of associa-
tion. Journal of the American Statistical Association,
53(284):814?861.
Jingjing Liu and Stephanie Seneff. 2009. Review senti-
ment scoring via a parse-and-paraphrase paradigm. In
Proceedings of EMNLP 2009.
George A. Miller. 1995. WordNet: A lexical database for
english. Communications of the ACM, 38(11):39?41.
Said M. Mohammad, Bonnie J. Dorr, Graeme Hirst, and
Peter D. Turney. 2013. Computing lexical contrast.
Computational Linguistics.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135, January.
Peter F. Schulam and Christiane Fellbaum. 2010. Au-
tomatically determining the semantic gradation of ger-
man adjectives. In Proceedings of KONVENS 2010.
Vera Sheinman and Takenobu Tokunaga. 2009. AdjS-
cales: Visualizing differences between adjectives for
language learners. IEICE Transactions on Information
and Systems, 92(8):1542?1550.
Vera Sheinman, Takenobu Tokunaga, I. Julien, P. Schu-
lam, and C. Fellbaum. 2012. Refining WordNet adjec-
tive dumbbells using intensity relations. In Proceed-
ings of Global WordNet Conference 2012.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of COLING/ACL 2006.
Charles Spearman. 1904. The proof and measurement of
association between two things. The American journal
of psychology, 15(1):72?101.
Fabian M. Suchanek, Mauro Sozio, and Gerhard
Weikum. 2009. SOFIE: a self-organizing framework
for information extraction. In Proceedings of WWW
2009.
Maite Taboada, Julian Brooke, Milan Tofiloskiy, and
Kimberly Vollz. 2011. Lexicon-based methods for
sentiment analysis. Computational Linguistics.
Niket Tandon and Gerard de Melo. 2010. Information
extraction from web-scale n-gram data. In Proceed-
ings of the SIGIR 2010 Web N-gram Workshop.
Peter D. Turney and Michael L. Littman. 2003. Mea-
suring praise and criticism: Inference of semantic
orientation from association. ACM Trans. Inf. Syst.,
21(4):315?346, October.
Peter D. Turney. 2008. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proceed-
ings of COLING 2008.
Xiaofeng Yang and Jian Su. 2007. Coreference resolu-
tion using semantic relatedness information from auto-
matically discovered patterns. In Proceedings of ACL
2007.
Ainur Yessenalina and Claire Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
Proceedings of EMNLP 2011.
290
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 102?106,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Mention Detection: Heuristics for the OntoNotes annotations
Jonathan K. Kummerfeld, Mohit Bansal, David Burkett and Dan Klein
Computer Science Division
University of California at Berkeley
{jkk,mbansal,dburkett,klein}@cs.berkeley.edu
Abstract
Our submission was a reduced version of
the system described in Haghighi and Klein
(2010), with extensions to improve mention
detection to suit the OntoNotes annotation
scheme. Including exact matching mention
detection in this shared task added a new and
challenging dimension to the problem, partic-
ularly for our system, which previously used
a very permissive detection method. We im-
proved this aspect of the system by adding
filters based on the annotation scheme for
OntoNotes and analysis of system behavior on
the development set. These changes led to im-
provements in coreference F-score of 10.06,
5.71, 6.78, 6.63 and 3.09 on the MUC, B3,
Ceaf-e, Ceaf-m and Blanc, metrics, respec-
tively, and a final task score of 47.10.
1 Introduction
Coreference resolution is concerned with identifying
mentions of entities in text and determining which
mentions are referring to the same entity. Previously
the focus in the field has been on the latter task.
Typically, mentions were considered correct if their
span was within the true span of a gold mention, and
contained the head word. This task (Pradhan et al,
2011) has set a harder challenge by only considering
exact matches to be correct.
Our system uses an unsupervised approach based
on a generative model. Unlike previous work, we
did not use the Bllip or Wikipedia data described in
Haghighi and Klein (2010). This was necessary for
the system to be eligible for the closed task.
The system detects mentions by finding the max-
imal projection of every noun and pronoun. For the
OntoNotes corpus this approach posed several prob-
lems. First, the annotation scheme explicitly rejects
noun phrases in certain constructions. And second,
it includes coreference for events as well as things.
In preliminary experiments on the development set,
we found that spurious mentions were our primary
source of error. Using an oracle to exclude all spu-
rious mentions at evaluation time yielded improve-
ments ranging from five to thirty percent across the
various metrics used in this task. Thus, we decided
to focus our efforts on methods for detecting and fil-
tering spurious mentions.
To improve mention detection, we filtered men-
tions both before and after coreference resolution.
Filters prior to coreference resolution were con-
structed based on the annotation scheme and partic-
ular cases that should never be mentions (e.g. single
word spans with the EX tag). Filters after corefer-
ence resolution were constructed based on analysis
of common errors on the development set.
These changes led to considerable improvement
in mention detection precision. The heuristics used
in post-resolution filtering had a significant negative
impact on recall, but this cost was out-weighed by
the improvements in precision. Overall, the use of
these filters led to a significant improvement in F1
across all the coreference resolution evaluation met-
rics considered in the task.
2 Core System
We use a generative approach that is mainly un-
supervised, as described in detail in Haghighi and
102
Klein (2010), and briefly below.
2.1 Model
The system uses all three of the standard abstrac-
tions in coreference resolution; mentions, entities
and types. A mention is a span in the text, the en-
tity is the actual object or event the mention refers
to, and each type is a group of entities. For example,
?the Mountain View based search giant? is a men-
tion that refers to the entity Google, which is of type
organization.
At each level we define a set of properties (e.g.
proper-head). For mentions, these properties are
linked directly to words from the span. For enti-
ties, each property corresponds to a list of words,
instances of which are seen in specific mentions of
that entity. At the type level, we assign a pair of
multinomials to each property. The first of these
multinomials is a distribution over words, reflecting
their occurrence for this property for entities of this
type. The second is a distribution over non-negative
integers, representing the length of word lists for this
property in entities of this type.
The only form of supervision used in the system
is at the type level. The set of types is defined and
lists of prototype words for each property of each
type are provided. We also include a small number
of extra types with no prototype words, for entities
that do not fit well in any of the specified types.
These abstractions are used to form a generative
model with three components; a semantic module, a
discourse module and a mention module. In addi-
tion to the properties and corresponding parameters
described above, the model is specified by a multi-
nomial prior over types (?), log-linear parameters
over discourse choices (pi), and a small number of
hyperparameters (?).
Entities are generated by the semantic module by
drawing a type t according to ?, and then using that
type?s multinomials to populate word lists for each
property.
The assignment of entities to mentions is handled
by the discourse module. Affinities between men-
tions are defined by a log-linear model with param-
eters pi for a range of standard features.
Finally, the mention module generates the ac-
tual words in the span. Words are drawn for each
property from the lists for the relevant entity, with
a hyper-parameter for interpolation between a uni-
form distribution over the words for the entity and
the underlying distribution for the type. This allows
the model to capture the fact that some properties
use words that are very specific to the entity (e.g.
proper names) while others are not at all specific
(e.g. pronouns).
2.2 Learning and Inference
The learning procedure finds parameters that are
likely under the model?s posterior distribution. This
is achieved with a variational approximation that
factors over the parameters of the model. Each set
of parameters is optimized in turn, while the rest are
held fixed. The specific update methods vary for
each set of parameters; for details see Section 4 of
Haghighi and Klein (2010).
3 Mention detection extensions
The system described in Haghighi and Klein (2010)
includes every NP span as a mention. When run on
the OntoNotes data this leads to a large number of
spurious mentions, even when ignoring singletons.
One challenge when working with the OntoNotes
data is that singleton mentions are not annotated.
This makes it difficult to untangle errors in coref-
erence resolution and errors in mention detection. A
mention produced by the system might not be in the
gold set for one of two reasons; either because it is
a spurious mention, or because it is not co-referent.
Without manually annotating the singletons in the
data, these two cases cannot be easily separated.
3.1 Baseline mention detection
The standard approach used in the system to detect
mentions is to consider each word and its maximal
projection, accepting it only if the span is an NP or
the word is a pronoun. This approach will intro-
duce spurious mentions if the parser makes a mis-
take, or if the NP is not considered a mention in the
OntoNotes corpus. In this work, we considered the
provided parses and parses produced by the Berke-
ley parser (Petrov et al, 2006) trained on the pro-
vided training data. We added a set of filters based
on the annotation scheme described by Pradhan et al
(2007). Some filters are applied before coreference
resolution and others afterward, as described below.
103
Data Set Filters P R F
Dev
None 37.59 76.93 50.50
Pre 39.49 76.83 52.17
Post 59.05 68.08 63.24
All 58.69 67.98 63.00
Test All 56.97 69.77 62.72
Table 1: Mention detection performance with various
subsets of the filters.
3.2 Before Coreference Resolution
The pre-resolution filters were based on three reli-
able features of spurious mentions:
? Appositive constructions
? Attributes signaled by copular verbs
? Single word mentions with a POS tag in the set:
EX, IN, WRB, WP
To detect appositive constructions we searched
for the following pattern:
NP
NP , NP . . .
And to detect attributes signaled by copular struc-
tures we searched for this pattern:
VP
cop verb NP
where we used the fairly conservative set of cop-
ular verbs: {is, are, was, ?m}. In both
cases, any mention whose maximal NP projection
appeared as the bold node in a subtree matching the
pattern was excluded.
In all three cases, errors from the parser (or POS
tagger) may lead to the deletion of valid mentions.
However, we found the impact of this was small and
was outweighed by the number of spurious mentions
removed.
3.3 After Coreference Resolution
To construct the post-coreference filters we analyzed
system output on the development set, and tuned
Filters MUC B3 Ceaf-e Blanc
None 25.24 45.89 50.32 59.12
Pre 27.06 47.71 50.15 60.17
Post 42.08 62.53 43.88 66.54
All 42.03 62.42 43.56 66.60
Table 2: Precision for coreference resolution on the dev
set.
Filters MUC B3 Ceaf-e Blanc
None 50.54 78.54 26.17 62.77
Pre 51.20 77.73 27.23 62.97
Post 45.93 64.72 39.84 61.20
All 46.21 64.96 39.24 61.28
Table 3: Recall for coreference resolution on the dev set.
based on MUC and B3 performance. The final set
of filters used were:
? Filter if the head word is in a gazetteer, which
we constructed based on behavior on the devel-
opment set (head words found using the Collins
(1999) rules)
? Filter if the POS tag is one of WDT, NNS, RB,
JJ, ADJP
? Filter if the mention is a specific case of you
or it that is more often generic (you know,
you can, it is)
? Filter if the mention is any cardinal other than
a year
A few other more specific filters were also in-
cluded (e.g. ?s when tagged as PRP) and one type
of exception (if all words are capitalized, the men-
tion is kept).
4 Other modifications
The parses in the OntoNotes data include the addi-
tion of structure within noun phrases. Our system
was not designed to handle the NML tag, so we
removed such nodes, reverting to the standard flat-
tened NP structures found in the Penn Treebank.
We also trained the Berkeley parser on the pro-
vided training data, and used it to label the develop-
ment and test sets.1 We found that performance was
1In a small number of cases, the Berkeley parser failed, and
we used the provided parse tree instead.
104
Filters MUC B3 Ceaf-e Ceaf-m Blanc
None 33.67 57.93 34.43 42.72 60.60
Pre 35.40 59.13 35.29 43.72 61.38
Post 43.92 63.61 41.76 49.74 63.26
All 44.02 63.66 41.29 49.46 63.34
Table 4: F1 scores for coreference resolution on the dev
set.
slightly improved by the use of these parses instead
of the provided parses.
5 Results
Since our focus when extending our system for this
task was on mention detection, we present results
with variations in the sets of mention filters used. In
particular, we have included results for our baseline
system (None), when only the filters before coref-
erence resolution are used (Pre), when only the fil-
ters after coreference resolution are used (Post), and
when all filters are used (All).
The main approach behind the pre-coreference fil-
ters was to consider the parse to catch cases that are
almost never mentions. In particular, these filters
target cases that are explicitly excluded by the an-
notation scheme. As Table 1 shows, this led to a
1.90% increase in mention detection precision and
0.13% decrease in recall, which is probably a result
of parse errors.
For the post-coreference filters, the approach was
quite different. Each filter was introduced based on
analysis of the errors in the mention sets produced
by our system on the development set. Most of the
filters constructed in this way catch some true men-
tions as well as spurious mentions, leading to signif-
icant improvements in precision at the cost of recall.
Specifically an increase of 21.46% in precision and
decrease of 8.85% in recall, but an overall increase
of 12.74% in F1-score.
As Tables 2 and 3 show, these changes in mention
detection performance generally lead to improve-
ments in precision at the expense of recall, with the
exception of Ceaf-e where the trends are reversed.
However, as shown in Table 4, there is an overall
improvement in F1 in all cases.
In general the change from only post-coreference
filters to all filters is slightly negative. The final sys-
Metric R P F1
MUC 46.39 39.56 42.70
B3 63.60 57.30 60.29
Ceaf-m 45.35 45.35 45.35
Ceaf-e 35.05 42.26 38.32
Blanc 58.74 61.58 59.91
Table 5: Complete results on the test set
tem used all of the filters because the process used to
create the post-coreference filters was more suscep-
tible to over-fitting, and the pre-coreference filters
provided such an unambiguously positive contribu-
tion to mention detection.
6 Conclusion
We modified the coreference system of Haghighi
and Klein (2010) to improve mention detection per-
formance. We focused on tuning using the MUC and
B3 metrics, but found considerable improvements
across all metrics.
One important difference between the system de-
scribed here and previous work was the data avail-
able. Unlike Haghighi and Klein (2010), no extra
data from Wikipedia or Bllip was used, a restriction
that was necessary to be eligible for the closed part
of the task.
By implementing heuristics based on the annota-
tion scheme for the OntoNotes data set and our own
analysis of system behavior on the development set
we were able to achieve the results shown in Table 5,
giving a final task score of 47.10.
7 Acknowledgments
We would like to thank the anonymous reviewers
for their helpful suggestions. This research is sup-
ported by the Office of Naval Research under MURI
Grant No. N000140911081, and a General Sir John
Monash Fellowship.
References
Michael John Collins. 1999. Head-driven statistical
models for natural language parsing. Ph.D. thesis,
Philadelphia, PA, USA. AAI9926110.
Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Proceed-
105
ings of NAACL, pages 385?393, Los Angeles, Califor-
nia, June. Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, pages 433?440, Sydney, Australia, July. Associ-
ation for Computational Linguistics.
Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted coreference: Identifying entities and events in
ontonotes. In Proceedings of the International Confer-
ence on Semantic Computing, pages 446?453, Wash-
ington, DC, USA. IEEE Computer Society.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011), Portland, Oregon,
June.
106
