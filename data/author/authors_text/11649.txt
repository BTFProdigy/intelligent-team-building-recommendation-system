Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 165?168,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Hierarchical Multi-Class Text Categorization
with Global Margin Maximization
Xipeng Qiu
School of Computer Science
Fudan University
xpqiu@fudan.edu.cn
Wenjun Gao
School of Computer Science
Fudan University
wjgao616@gmail.com
Xuanjing Huang
School of Computer Science
Fudan University
xjhuang@fudan.edu.cn
Abstract
Text categorization is a crucial and well-
proven method for organizing the collec-
tion of large scale documents. In this pa-
per, we propose a hierarchical multi-class
text categorization method with global
margin maximization. We not only max-
imize the margins among leaf categories,
but also maximize the margins among
their ancestors. Experiments show that the
performance of our algorithm is competi-
tive with the recently proposed hierarchi-
cal multi-class classification algorithms.
1 Introduction
In the past serval years, hierarchical text catego-
rization has become an active research topic in
database area (Koller and Sahami, 1997; Weigend
et al, 1999) and machine learning area (Rousu et
al., 2006; Cai and Hofmann, 2007).
Hierarchical categorization methods can be di-
vided in two types: local and global approaches
(Wang et al, 1999; Sun and Lim, 2001). A lo-
cal approach usually proceeds in a top-down fash-
ion, which firstly picks the most relevant cate-
gories of the top level and then recursively making
the choice among the low-level categories. The
global approach builds only one classifier to dis-
criminate all categories in a hierarchy. Due that the
global hierarchical categorization can avoid the
drawbacks about those high-level irrecoverable er-
ror, it is more popular in the machine learning do-
main.
The essential idea behind global approach is
that the close classes(nodes) have some common
underlying factors. Especially, the descendant
classes can share the characteristics of the ances-
tor classes, which is similar with multi-task learn-
ing(Caruana, 1997). A key problem for global hi-
erarchical categorization is how to combine these
underlying factors.
In this paper, we propose an method for hierar-
chical multi-class text categorization with global
margin maximization. We emphasize that it is im-
portant to separate all the nodes of the correct path
in the class hierarchy from their sibling node, then
we incorporate such information into the formula-
tion of hierarchical support vector machine.
The rest of the paper is organized as follows.
Section 2 describes the basic model of multi-class
hierarchical categorization with maximizing mar-
gin. Then we propose our improved versions in
section 3. Section 4 gives the experimental analy-
sis. Section 5 concludes the paper.
2 Hierarchical Multi-Class Text
Categorization
Multiclass SVM can be generalized to the problem
of hierarchical categorization (Cai and Hofmann,
2007), which has more than two categories in most
of the case. Denote Y
i
as the multilabels of x
i
and
?
Y
i
the multilabels set not in Y
i
. The separation
margin of w, with respect to x
i
, can be approxi-
mated as:
?
i
(w) = min
y?Y
i
,?y?
?
Y
i
??(x
i
,y)? ?(x
i
,
?
y),w? (1)
The loss function can be accommodated to
multi-class SVM to scale the penalties for margin
violations proportional to the loss. This is moti-
vated by the fact that margin violations involving
an incorrect class with high loss should be penal-
ized more severely. So the cost-sensitive hierar-
chical multiclass formulation takes takes the fol-
lowing form:
min
w,?
1
2
||w||
2
+ C
n
?
i=1
?
i
(2)
s.t.?w,??
i
(y,
?
y)??1?
?
i
l(y,?y)
, (?i,y?Y
i
,
?
y?
?
Y
i
)
?
i
? 0(?i)
165
where ??
i
(y,
?
y) = ?(x
i
,y) ? ?(x
i
,
?
y),
l(y,
?
y) > 0 and ?(x,y) is the joint feature of in-
put x and output y, which can be represented as:
?(x,y) = ?(y)? ?(x) (3)
where ? is the tensor product. ?(y) is the feature
representation of y.
Thus, we can classify a document x to label y
?
:
y
?
= arg max
y
F (w,?(x,y)) (4)
where F (?) is a map function.
There are different kinds of loss functions
l(y,
?
y).
One is thezero-one loss, l
0/1
(y,u) = [y 6= u].
Another is specially designed for the hierarchy
is tree loss(Dekel et al, 2004). Tree loss is defined
as the length of the path between two multilabels
with positive microlabels,
l
tr
= |path(i : y
i
= 1, j : u
j
= 1)| (5)
(Rousu et al, 2006) proposed a simplified ver-
sion of l
H
, namely l
?
H
:
l
?
H
=
?
j
c
j
[y
j
6= u
j
&y
pa
(j) = u
pa(j)
], (6)
that penalizes a mistake in a child only if the label
of the parent was correct. There are some different
choices for setting c
j
. One naive idea is to use
a uniform weighting (c
j
= 1). Another possible
choice is to divide the loss among the sibling:
c
root
= 1, c
j
= c
Parent(j)
/(|Sib(j)|+ 1) (7)
Another possible choice is to scale the loss by the
proportion of the hierarchy that is in the subtree
T (j) rooted by j:
c
j
= |T (j)|/|T (root)| (8)
Using these scaling weights, the derived losses are
referred as l
?
uni
,l
?
sib
and l
?
sub
respectively.
3 Hierarchical Multi-Class Text
Categorization with Global Margin
Maximization
In previous literature (Cai and Hofmann, 2004;
Tsochantaridis et al, 2005), they focused on sep-
arating the correct path from those incorrect path.
Inspired by the example in Figure 1, we emphasize
it is also important to separate the ancestor node in
the correct path from their sibling node.
The vector w can be decomposed in to the set
of w
i
for each node (category) in the hierarchy. In
Figure 1, the example hierarchy has 7 nodes and 4
of them are leaf nodes. The category is encode
as an integer, 1, . . . , 7. Suppose that the train-
ing pattern x belongs to category 4. Both w in
the Figure 1a and Figure 1b can successfully clas-
sify x into category 4, since F (w,?(x,y
4
)) =
?
1,2,4
?w
i
,x? is the maximal among all the possi-
ble discriminate functions. So both learned param-
eter w is acceptable in current hierarchical support
vector machine.
Here we claim the w in Figure 1b is better than the
w in Figure 1a. Since we notice in Figure 1a, the
discriminate function ?w
2
,x? is smaller than the
discriminate function ?w
3
,x?. The discriminate
function ?w
i
,x? measures the similarity of x to
category i. The larger the discriminate function is,
the more similar x is to category i. Since category
2 is in the path from the root to the correct cate-
gory and category 3 is not, intuitively, x should be
closer to category 2 than category 3. But the dis-
criminate function in Figure 1a is contradictive to
this assumption. But such information is reflected
correctly in Figure 1b. So we conclude w in Fig.
1b is superior to w in 1a.
Here we propose a novel formulation to incor-
porate such information. Denote A
i
as the mul-
tilabel in Y
i
that corresponds to the nonleaf cate-
gories and Sib(z) denotes the sibling nodes of z,
that is the set of nodes that have the same parent
with z, except z itself. Implementing the above
idea, we can get the following formulation:
min
w,?,?
1
2
?w?
2
+ C
1
?
i
?
i
+ C
2
?
i
?
i
(9)
s.t.?w, ??
i
(y,
?
y)? ? 1?
?
i
l(y,
?
y)
, (?i,
y ? Y
i
?
y ?
?
Y
i
)
?w, ??
i
(z,
?
z)? ? 1?
?
i
l(z,
?
z)
, (?i,
z ? A(i)
?
z ? Sib(z)
)
?
i
? 0(?i)
?
i
? 0(?i)
It arrives at the following Lagrangian:
L(w, ?
1
, ..., ?
n
, ?
1
, ..., ?
n
)
=
1
2
?w?
2
+ C
1
X
i
?
i
+ C
2
X
i
?
i
?
X
i
X
y?Y
i
?y?
?
Y
i
?
iy?y
(?w, ??
i
(y,
?
y)? ? 1 +
?
i
l(y,
?
y)
)
166
1
2 3
4 5 6 7
10,1  xw
3,2  xw 5,3  xw
5,4  xw 1,7  xw1,5  xw 2,6  xw
1
2 3
4 5 6 7
10,1  xw
7,2  xw 3,3  xw
5,4  xw 1,7  xw1,5  xw 2,6  xw
a) b)
Figure 1: Two different discriminant function in a hierarchy
?
X
i
X
z?A
i
?z?Sib(z)
?
iz?z
(?w, ??
i
(z,
?
z)? ? 1 +
?
i
l(z,
?
z)
)
?
X
i
c
i
?
i
?
X
i
d
i
?
i
(10)
The dual QP becomes
max
?
?(?) =
?
i
?
y?Y
i
?y?
?
Y
i
?
iy
?
y
+
?
i
?
z?A
i
?z?Sib(z)
?
iz
?
z
?
1
2
?
i,j
?
y?Y
i
?y?
?
Y
i
?
r?Y
j
?r?
?
Y
j
?
1
i,j,y,
?
y,r,
?
r
(11)
?
1
2
?
i,j
?
z?A
i
?z?Sib(z)
?
k?A
j
?
k?Sib(k)
?
2
i,j,z,
?
z,k,
?
k
,
s.t.?
iy
?
y
? 0, (12)
?
jz
?
z
? 0, (13)
?
y?Y
i
?y?
?
Y
i
?
iy
?
y
l(y,
?
y)
? C
1
, (14)
?
z?A
i
?z?Sib(z)
?
iz
?
z
l(z,
?
z)
? C
2
, (15)
where ?
1
i,j,y,
?
y,r,
?
r
=
?
iy
?
y
?
jr
?
r
???
i
(y,
?
y), ??
j
(r,
?
r)? and ?
2
i,j,z,
?
z,k,
?
k
=
?
iz
?
z
?
jk
?
k
???
i
(z,
?
z), ??
j
(k,
?
k)?.
3.1 Optimization Algorithm
The derived QP can be very large, since the num-
ber of ? and ? variables is up to O(n?2
N
), where
n is number of training pattern and N is the num-
ber of nodes in the hierarchy. But two properties
of the dual problem can be exploited to design a
much more efficient optimization.
First, the constraints in the dual problem Eq. 11
- Eq. 15 factorize over the instance index for both
?-variables and ?-variables. The constraints in
Eq. 14 do not couple ?-variables and ?-variables
together. Further, dual variables ?
iy
?
y
and ?
jy
?
?
y
?
belonging to different training instances i and j do
not join in a same constraints. This inspired an
optimization procedure which iteratively performs
subspace optimization over all dual variables ?
iy
?
y
belonging to the same training instance. This will
in general reduced to a much smaller QP, since
it freezes all ?
jy
?
y
with j 6= i and ?-variables at
their current values. This strategy can be applied
in solving ?-variables.
Secondly, the number of active constraints at the
solution is expected to be relatively small, since
only a small fraction of categories
?
y ?
?
Y
i
( or
?
y ? Sib(y) when y ? A
i
) will typically fail to
achieve the required margin. The expected sparse-
ness of the variable for the dual problem can be
exploited by employing a variable selection strat-
egy. Equivalently, this corresponds to a cutting
plane algorithm for the primal QP. Intuitively, we
will identify the most violated margin constraint
with index (i,y,
?
y) and then add the correspond-
ing variable to the optimization problem. This
means that we start with extremely sparse prob-
lems and only successively increase the number of
variables in the active set. This general approach
to deal with large linear or quadratic optimization
problems is also known as column selection. In
practice, it is often not necessary to optimize until
final convergence, which adds to the attractiveness
of this approach.
We have used the LOQO optimization package
(Vanderbei, 1999) in our experiments.
4 Experiment
We evaluate our proposed model on the section D
in the WIPO-alpha collection
1
, which consists of
the 1372 training and 358 testing document. The
1
World Intellectual Property Organization (WIPO)
167
Table 1: Prediction losses (%) obtained on WIPO.
The values per column is calculated with the dif-
ferent loss function.
X
X
X
X
X
X
X
Train
Test
l
0/1
l
?
l
tr
l
uni
l
sib
l
sub
HSVM 48.6 188.8 94.4 97.2 5.4 7.5
l
0/1
HSVM-S 48.3 186.6 93.3 96.6 5.2 7.4
HSVM 49.7 187.7 93.9 99.4 5.0 7.1
l
?
HSVM-S 47.8 165.3 89.7 90.5 4.8 6.9
HM3 70.9 167.0 - 89.1 5.0 7.0
HSVM 49.4 186.0 93.0 98.9 5.0 7.5
l
tr
HSVM-S 48.9 181.4 90.2 97.8 4.9 7.1
HSVM 47.2 181.0 90.5 94.4 5.0 7.0
l
?
uni
HSVM-S 46.9 179.3 88.7 91.9 4.9 6.9
HM3 70.1 172.1 - 88.8 5.2 7.4
HSVM 49.4 184.9 92.5 98.9 4.8 7.4
l
?
sib
HSVM-S 48.9 170.2 91.6 90.8 4.7 7.4
HM3 64.8 172.9 - 92.7 4.8 7.1
HSVM 50.6 189.9 95.0 101.1 5.2 7.5
l
?
sub
HSVM-S 47.2 169.4 85.2 89.4 4.3 6.6
HM3 65.0 170.9 - 91.9 4.8 7.2
number of nodes in the hierarchy is 188, with max-
imum depth 3.
We compared the performance of our proposed
method HSVM-S with two algorithms: HSVM(Cai
and Hofmann, 2007) and HM3(Rousu et al, 2006).
4.1 Effect of Different Loss Function
We compare the methods based on different loss
functions, l
0/1
, l
?
, l
tr
, l
u?ni
, l
s?ib
and l
s?ub
. The per-
formances for three algorithms can be seen in Ta-
ble 1. Those empty cells, denoted by ?-?, are not
available in (Rousu et al, 2006).
As expected, l
0/1
is inferior to other hierarchi-
cal losses by getting poorest performance in all the
testing losses, since it can not take into account the
hierarchical information between categories. The
results suggests that training with a hierarchical
losses function, like l
s?ib
or l
u?ni
, would lead to a
better reduced l
0/1
on the test set as well as in
terms of the hierarchical loss. In Table 1, we can
also point out that when training with the same
hierarchical loss, the performance of HSVM-S is
better than HSVM under the measure of most hier-
archical losses, since HSVM-S includes more hier-
archical information,the relationship between the
sibling categories, than HSVM which only separate
the leave categories.
5 Conclusion
In this paper we present a hierarchical multi-class
document categorization, which focus on maxi-
mize the margin of the classes at the different
levels in the class hierarchy. In future work, we
plan to extend the proposed hierarchical learning
method to the case where the hierarchy is a DAG
instead of tree and scale up the method further.
Acknowledgments
This work was (partially) funded by Chinese
NSF 60673038, Doctoral Fund of Ministry of
Education of China 200802460066, and Shang-
hai Science and Technology Development Funds
08511500302.
References
L. Cai and T Hofmann. 2004. Hierarchical docu-
ment categorization with support vector machines.
In Proceedings of the ACM Conference on Informa-
tion and Knowledge Management.
L. Cai and T. Hofmann. 2007. Exploiting known tax-
onomies in learning overlapping concepts. In Pro-
ceedings of International Joint Conferences on Arti-
ficial Intelligence.
R. Caruana. 1997. Multi-task learning. Machine
Learning, 28(1):41?75.
Ofer Dekel, Joseph Keshet, and Yoram Singer. 2004.
Large margin hierarchical classification. In Pro-
ceedings of the 21 st International Conference on
Machine Learning.
D. Koller and M Sahami. 1997. Hierarchically classi-
fying documents using very few words. In Proceed-
ings of the International Conference on Machine
Learning (ICML).
Juho Rousu, Craig Saunders, Sandor Szedmak, and
John Shawe-Taylor. 2006. Kernel-based learning
of hierarchical multilabel classification models. In
Journal of Machine Learning Research.
A. Sun and E.-P Lim. 2001. Hierarchical text classi-
fication and evaluation. In Proceedings of the IEEE
International Conference on Data Mining (ICDM).
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large mar-
gin methods for structured and interdependent out-
put variables. In Journal of Machine Learning.
R. J. Vanderbei. 1999. Loqo: An interior point code
for quadratic programming. In Optimization Meth-
ods and Software.
K. Wang, S. Zhou, and S Liew. 1999. Building hier-
archical classifiers using class proximities. In Pro-
ceedings of the International Conference on Very
Large Data Bases (VLDB).
A. Weigend, E. Wiener, and J Pedersen. 1999. Exploit-
ing hierarchy in text categorization. In Information
Retrieval.
168
Adaptive Chinese Word Segmentation with
Online Passive-Aggressive Algorithm
Wenjun Gao
School of Computer Science
Fudan University
Shanghai, China
wjgao616@gmail.com
Xipeng Qiu
School of Computer Science
Fudan University
Shanghai, China
xpqiu@fudan.edu.cn
Xuanjing Huang
School of Computer Science
Fudan University
Shanghai, China
xjhuang@fudan.edu.cn
Abstract
In this paper, we describe our system1
for CIPS-SIGHAN-2010 bake-off task of
Chinese word segmentation, which fo-
cused on the cross-domain performance
of Chinese word segmentation algorithms.
We use the online passive-aggressive al-
gorithm with domain invariant informa-
tion for cross-domain Chinese word seg-
mentation.
1 Introduction
In recent years, Chinese word segmentation
(CWS) has undergone great development (Xue,
2003; Peng et al, 2004). The popular method is
to regard word segmentation as a sequence label-
ing problems. The goal of sequence labeling is to
assign labels to all elements of a sequence.
Due to the exponential size of the output
space, sequence labeling problems tend to be
more challenging than the conventional classifi-
cation problems. Many algorithms have been
proposed and the progress has been encourag-
ing, such as SVMstruct (Tsochantaridis et al,
2004), conditional random fields (CRF) (Lafferty
et al, 2001), maximum margin Markov networks
(M3N) (Taskar et al, 2003) and so on. After years
of intensive researches, Chinese word segmenta-
tion achieves a quite high precision. However, the
performance of segmentation is not so satisfying
for out-of-domain text.
There are two domains in domain adaption
problem, a source domain and a target domain.
When we use the machine learning methods for
1Available at http://code.google.com/p/
fudannlp/
Chinese word segmentation, we assume that train-
ing and test data are drawn from the same distri-
bution. This assumption underlies both theoreti-
cal analysis and experimental evaluations of learn-
ing algorithms. However, the assumption does
not hold for domain adaptation(Ben-David et al,
2007; Blitzer et al, 2006). The challenge is the
difference of distribution between the source and
target domains.
In this paper, we use online margin max-
imization algorithm and domain invariant fea-
tures for domain adaptive CWS. The online learn-
ing algorithm is Passive-Aggressive (PA) algo-
rithm(Crammer et al, 2006), which passively ac-
cepts a solution whose loss is zero, while it ag-
gressively forces the new prototype vector to stay
as close as possible to the one previously learned.
The rest of the paper is organized as follows.
Section 2 introduces the related works. Then we
describe our algorithm in section 3 and 4. The
feature templates are described in section 5. Sec-
tion 6 gives the experimental analysis. Section 7
concludes the paper.
2 Related Works
There are several approaches to deal with the do-
main adaption problem.
The first approach is to use semi-supervised
learning (Zhu, 2005).
The second approach is to incorporate super-
vised learning with domain invariant information.
The third approach is to improve the present
model with a few labeled domain data.
Altun et al (2006) investigated structured clas-
sification in a semi-supervised setting. They pre-
sented a discriminative approach that utilizes the
intrinsic geometry of inputs revealed by unlabeled
data points and we derive a maximum-margin for-
mulation of semi-supervised learning for struc-
tured variables.
Self-training (Zhu, 2005) is also a popular tech-
nology. In self-training a classifier is first trained
with the small amount of labeled data. The clas-
sifier is then used to classify the unlabeled data.
Typically the most confident unlabeled points, to-
gether with their predicted labels, are added to the
training set. The classifier is re-trained and the
procedure repeated. Note the classifier uses its
own predictions to teach itself. Yarowsky (1995)
uses self-training for word sense disambiguation,
e.g. deciding whether the word plant means a liv-
ing organism or a factory in a given context.
Zhao and Kit (2008) integrated unsupervised
segmentation and CRF learning for Chinese word
segmentation and named entity recognition. They
found word accessory variance (Feng et al, 2004)
is useful to CWS.
3 Online Passive-Aggressive Algorithm
Sequence labeling, the task of assigning labels
y = y1, . . . , yL to an input sequence x =
x1, . . . , xL.
Give a sample (x,y), we define the feature is
?(x,y). Thus, we can label x with a score func-
tion,
y? = argmax
z
F (w,?(x, z)), (1)
where w is the parameter of function F (?).
The score function of our algorithm is linear
function.
Given an example (x,y), y? is denoted as the
incorrect label with the highest score,
y? = argmax
z 6=y
wT?(x, z). (2)
The margin ?(w; (x,y)) is defined as
?(w; (x,y)) = wT?(x,y)?wT?(x, y?). (3)
Thus, we calculate the hinge loss.
`(w; (x,y) =
{
0, ?(w; (x,y)) > 1
1? ?(w; (x,y)), otherwise
(4)
We use the online PA learning algorithm to
learn the weights of features. In round t, we find
new weight vector wt+1 by
wt+1 = arg min
w?Rn
1
2
||w ?wt||2 + C ? ?,
s.t. `(w; (xt,yt)) <= ? and ? >= 0 (5)
where C is a positive parameter which controls
the influence of the slack term on the objective
function.
The algorithms goal is to achieve a margin at
least 1 as often as possible, thus the Hamming loss
is also reduced indirectly. On rounds where the
algorithm attains a margin less than 1 it suffers an
instantaneous loss.
We abbreviate `(wt; (x, y)) to `t. If `t = 0
then wt itself satisfies the constraint in Eq. (5)
and is clearly the optimal solution. We therefore
concentrate on the case where `t > 0.
First, we define the Lagrangian of the optimiza-
tion problem in Eq. (5) to be
L(w, ?, ?, ?) = 1
2
||w ?wt||2 + C ? ?
+ ?(`t ? ?)? ??
s.t. ? >= 0, ? >= 0. (6)
where ?, ? is a Lagrange multiplier.
Setting the partial derivatives of L with respect
to the elements of ? to zero gives
? + ? = C. (7)
The gradient of w should be zero,
w ? wt ? ?(?(x,y) ? ?(x, y?)) = 0, (8)
we get
w = wt + ?(?(x,y) ? ?(x, y?)). (9)
Substitute Eq. (7) and Eq. (9) to dual objective
function Eq. (6), we get
L(?) = ?1
2
||?(?(x,y)? ?(x, y?))||2
? ?(wtT (?(x,y)? ?(x, y?)) + ? (10)
Differentiate with ?, and set it to zero, we get
?||?(x,y)? ?(x, y?)||2
+wtT (?(x,y)? ?(x, y?))? 1 = 0. (11)
So,
?? = 1?wt
T (?(x,y)? ?(x, y?))
||?(x,y)? ?(x, y?)||2
. (12)
From ? + ? = C, we know that ? < C, so
??? = min(C, ??). (13)
Finally, we get update strategy,
wt+1 = wt + ???(?(x,y)? ?(x, y?)). (14)
Our final algorithm is shown in Algorithm 1. In
order to avoiding overfitting, the averaging tech-
nology is employed.
input : training data set:
(xn,yn), n = 1, ? ? ? , N , and
parameters: C,K
output: w
Initialize: cw? 0,;
for k = 0 ? ? ?K ? 1 do
w0 ? 0 ;
for t = 0 ? ? ?T ? 1 do
receive an example (xt,yt);
predict:
y?t = argmax
z 6=yt
?wt,?(xt, z)?;
calculate `(w; (x,y));
update wt+1 with Eq.(14);
end
cw = cw +wT ;
end
w = cw/K ;
Algorithm 1: Labelwise Margin Maxi-
mization Algorithm
4 Inference
The PA algorithm is used to learn the weights of
features in training procedure. In inference pro-
cedure, we use Viterbi algorithm to calculate the
maximum score label.
Let ?(n) be the best score of the partial label
sequence ending with yn. The idea of the Viterbi
algorithm is to use dynamic programming to com-
pute ?(n):
?(n) = max
n?1
(
?(n? 1) +wT?(x, yn, yn?1)
)
(15)
+wt?(x, yn)
Using this recursive definition, we can evalu-
ate ?(N) for all yN , where N is the input length.
This results in the identification of the best label
sequence.
The computational cost of the Viterbi algorithm
is O(NL2), where L is the number of labels.
5 Feature Templates
All feature templates used in this paper are shown
in Table 1. C represents a Chinese character while
the subscript of C indicates its position in the sen-
tence relative to the current character, whose sub-
script is 0. T represents the character-based tag:
?B?, ?B2?, ?B3?, ?M?, ?E? and ?S?, which repre-
sent the beginning, second, third, middle, end or
single character of a word respectively.
The type of character includes: digital, letter,
punctuation and other.
We also use the word accessor variance for do-
main adaption. Word accessor variance (AV) was
proposed by (Feng et al, 2004) and was used to
evaluate how independently a string is used, and
thus how likely it is that the string can be a word.
The accessor variety of a string s of more than one
character is defined as
AV (s) = min{Lav(s), Rav(s)} (16)
Lav(s) is called the left accessor variety and is
defined as the number of distinct characters (pre-
decessors) except ?S? that precede s plus the num-
ber of distinct sentences of which s appears at
the beginning. Similarly, the right accessor va-
riety Rav(s) is defined as the number of distinct
characters (successors) except ?E? that succeed s
plus the number of distinct sentences in which s
appears at the end. The characters ?S? and ?E?
are defined as the begin and end of a sentence.
The word accessor variance was found effective
for CWS with unsegmented text (Zhao and Kit,
2008).
Table 1: Feature templates
Ci, T0, (i = ?1, 0, 1, 2)
Ci, Ci+1, T0, (i = ?2,?1, 0, 1)
T?1,0
Tc: Type of Character
AV : word accessor variance
6 CIPS-SIGHAN-2010 Bakeoff
CIPS-SIGHAN-2010 bake-off task of Chinese
word segmentation focused on the cross-domain
performance of Chinese word segmentation algo-
rithms. There are two subtasks for this evaluation:
(1) Word Segmentation for Simplified Chinese
Text;
(2) Word Segmentation for Traditional Chinese
Text.
The test corpus of each subtask covers four do-
mains: literature, computer science, medicine and
finance.
We participate in closed training evaluation of
both subtasks.
Firstly, we calculate the word accessor variance
AVL(s)of the continuous string s from labeled
corpus. Here, we set the largest length of string
s to be 4.
Secondly, we train our model with feature tem-
ples and AVL(s).
Thirdly, when we process the different domain
unlabeled corpus, we recalculate the word ac-
cessory variance AVU (s) from the corresponding
corpus.
Fourthly, we segment the domain corpus with
new word accessory variance AVU (s) instead of
AVL(s).
The results are shown in Table 2 and 3. The
results show our method has a poor performance
in OOV ( Out-Of-Vocabulary) word.
The running environment is shown in Table 4.
Table 4: Experimental environment
OS Win 2003
CPU Intel Xeon 2.0G
Memory 4G
We set the max iterative number is 20. Our run-
ning time is shown in Table 5. ?s? represents sec-
ond, ?chars? is the number of Chinese character,
and ?MB? is the megabyte. In practice, we found
the system can achieve the same performance af-
ter 7 loops. Therefore, we just need less half the
time in Table 5 actually.
7 Conclusion
In this paper, we describe our system in CIPS-
SIGHAN-2010 bake-off task of Chinese word
segmentation. Although our method just achieve
a consequence of being average and not outstand-
ing, it has an advantage of faster training than
other batch learning algorithm, such as CRF and
M3N.
In the future, we wish to improve our method
in the following aspects. Firstly, we will investi-
gate more effective domain invariant feature rep-
resentation. Secondly, we will integrate our algo-
rithm with self-training and other semi-supervised
learning methods.
Acknowledgments
This work was (partially) funded by 863 Pro-
gram (No. 2009AA01A346), 973 Program (No.
2010CB327906), and Shanghai Science and Tech-
nology Development Funds (No. 08511500302).
References
Altun, Y., D. McAllester, and M. Belkin. 2006. Max-
imum margin semi-supervised learning for struc-
tured variables. Advances in neural information
processing systems, 18:33.
Ben-David, S., J. Blitzer, K. Crammer, and F. Pereira.
2007. Analysis of representations for domain adap-
tation. Advances in Neural Information Processing
Systems, 19:137.
Blitzer, J., R. McDonald, and F. Pereira. 2006.
Domain adaptation with structural correspondence
learning. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, pages 120?128. Association for Computational
Linguistics.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
Table 2: Evaluation results on simplified corpus
R P F1 OOV RR IV RR
Best 0.945 0.946 0.946 0.816 0.954
Literature Our 0.915 0.925 0.92 0.577 0.94
Best 0.953 0.95 0.951 0.827 0.975
Computer Our 0.934 0.919 0.926 0.739 0.969
Best 0.942 0.936 0.939 0.75 0.965
Medicine Our 0.927 0.924 0.925 0.714 0.953
Best 0.959 0.96 0.959 0.827 0.972
Finance Our 0.94 0.942 0.941 0.719 0.961
Table 3: Evaluation results on traditional corpus
R P F1 OOV RR IV RR
Best 0.942 0.942 0.942 0.788 0.958
Literature Our 0.869 0.91 0.889 0.698 0.887
Best 0.948 0.957 0.952 0.666 0.977
Computer Our 0.933 0.949 0.941 0.791 0.948
Best 0.953 0.957 0.955 0.798 0.966
Medicine Our 0.908 0.932 0.92 0.771 0.919
Best 0.964 0.962 0.963 0.812 0.975
Finance Our 00.925 0.939 0.932 0.793 0.935
Table 5: Execution time of training and test phase.
Task A B C D
Training Simp 817.2s 795.6s 774.0s 792.0s
Trad 903.6s 889.2s 885.6s 874.8s
Test 20327 chars/s, or 17.97 s/MB
Feng, H., K. Chen, X. Deng, and W. Zheng. 2004. Ac-
cessor variety criteria for chinese word extraction.
Computational Linguistics, 30(1):75?93.
Lafferty, John D., Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In ICML ?01: Proceedings of the
Eighteenth International Conference on Machine
Learning.
Peng, F., F. Feng, and A. McCallum. 2004. Chinese
segmentation and new word detection using condi-
tional random fields. Proceedings of the 20th inter-
national conference on Computational Linguistics.
Taskar, Ben, Carlos Guestrin, and Daphne Koller.
2003. Max-margin markov networks. In Proceed-
ings of Neural Information Processing Systems.
Tsochantaridis, I., T. Hofmann, T. Joachims, and Y Al-
tun. 2004. Support vector machine learning for in-
terdependent and structured output spaces. In Pro-
ceedings of the International Conference on Ma-
chine Learning(ICML).
Xue, N. 2003. Chinese word segmentation as charac-
ter tagging. Computational Linguistics and Chinese
Language Processing, 8(1):29?48.
Yarowsky, D. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of the 33rd annual meeting on Association for
Computational Linguistics, pages 189?196. Associ-
ation for Computational Linguistics.
Zhao, H. and C. Kit. 2008. Unsupervised segmenta-
tion helps supervised learning of character tagging
for word segmentation and named entity recogni-
tion. In The Sixth SIGHAN Workshop on Chinese
Language Processing, pages 106?111. Citeseer.
Zhu, Xiaojin. 2005. Semi-supervised learning
literature survey. Technical Report 1530, Com-
puter Sciences, University of Wisconsin-Madison.
http://www.cs.wisc.edu/?jerryzhu/pub/ssl survey.pdf.
