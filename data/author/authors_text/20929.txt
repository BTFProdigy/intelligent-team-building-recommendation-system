Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 62?70,
Gothenburg, Sweden, April 26 2014.
c?2014 Association for Computational Linguistics
Mining the Twentieth Century?s History from the Time Magazine Corpus
Mike Kestemont
University of Antwerp
Prinsstraat 13, D.188
B-2000, Antwerp
Belgium
mike.kestemont
@uantwerpen.be
Folgert Karsdorp
Meertens Institute
Postbus 94264
1090 GG Amsterdam
The Netherlands
Folgert.Karsdorp
@meertens.knaw.nl
Marten D?uring
University of North-Carolina
551 Hamilton Hall
CB 3195, Chapel Hill
North Carolina 27599
United States
marten@live.unc.edu
Abstract
In this paper we report on an explorative
study of the history of the twentieth cen-
tury from a lexical point of view. As
data, we use a diachronic collection of
270,000+ English-language articles har-
vested from the electronic archive of the
well-known Time Magazine (1923?2006).
We attempt to automatically identify sig-
nificant shifts in the vocabulary used in
this corpus using efficient, yet unsuper-
vised computational methods, such as Par-
simonious Language Models. We offer a
qualitative interpretation of the outcome
of our experiments in the light of momen-
tous events in the twentieth century, such
as the Second World War or the rise of
the Internet. This paper follows up on a
recent string of frequentist approaches to
studying cultural history (?Culturomics?),
in which the evolution of human culture is
studied from a quantitative perspective, on
the basis of lexical statistics extracted from
large, textual data sets.
1 Introduction: Culturomics
Although traditionally, the Humanities have been
more strongly associated with qualitative rather
than quantitative methodologies, it is hard to miss
that ?hipster? terms like ?Computational Analy-
sis?, ?Big Data? and ?Digitisation?, are currently
trending in Humanities scholarship. In the interna-
tional initiative of Digital Humanities, researchers
from various disciplines are increasingly explor-
ing novel, computational means to interact with
their object of research. Often, this is done in col-
laboration with researchers from Computational
Linguistics, who seem to have adopted quantita-
tive approaches relatively sooner than other Hu-
manities disciplines. The subfield of Digital His-
tory (Zaagsma, 2013), in which the present paper
is to be situated, is but one of the multiple Human-
ities disciplines in which rapid progress is being
made as to the application of computational meth-
ods. Although the vibrant domain of Digital His-
tory cannot be exhaustively surveyed here due to
space limits, it is nevertheless interesting to refer
to a recent string of frequentist lexical approaches
to the study of human history, and the evolution of
human culture in particular: ?Culturomics?.
This line of computational, typically data-
intensive research seeks to study various aspects of
human history, by researching the ways in which
(predominantly cultural) phenomena are reflected
in, for instance, word frequency statistics extracted
from large textual data sets. The field has been ini-
tiated in a lively, yet controversial publication by
Michel et al. (2011), which ? while it has invited
a lot of attention in popular media ? has not gone
uncriticized in the international community of Hu-
manities.
1
In this paper, the authors show how a
number of major historical events show interest-
ing correlations with word counts in a vast corpus
of n-grams extracted from the Google Books, al-
legedly containing 4% percent of all books ever
printed.
In recent years, the term ?Culturomics? seems
to have become an umbrella term for studies en-
gaging, often at an increasing level of complex-
ity, with the seminal, publicly available Google
Books NGram Corpus (Juola, 2013; Twenge et al.,
2012; Acerbi et al., 2013b). Other studies, like
the inspiring contribution by Leetaru (2011) have
independently explored other data sets for simi-
lar purposes, such as the retroactive prediction of
the Arab Spring Revolution using news data. In
1
Consult, for instance, the critical report by A. Grafton
on the occasion of a presentation by Michel and Lieberman
Aiden at one of the annual meetings of the American His-
torical Association (https://www.historians.
org/publications-and-directories/
perspectives-on-history/march-2011/
loneliness-and-freedom).
62
the present paper, we seek to join this recent line
of Culturomics research: we will discuss a series
of quantitative explorations of the Time Magazine
Corpus (1923?2006), a balanced textual data set
covering a good deal of the twentieth (and early
twenty-first) century.
The structure of the paper is as follows: in the
following section 2, we will discuss the data set
used. In section 3, we will introduce some of
the fundamental assumptions underlying the Cul-
turomics approach for Big Data, and report on
an experiment that replicates an earlier sentiment-
related analysis of the Google Books Corpus
(Acerbi et al., 2013b) using our Time data. Subse-
quently, we will apply a Parsimonious Language
Model to our data (section 4) and assess from a
qualitative perspective how and whether this tech-
nique can be used to extract the characteristic vo-
cabulary from specific time periods. To conclude,
we will use these Parsimonious Language Mod-
els in a variability-based neighbor clustering (sec-
tion 5), in an explorative attempt to computation-
ally identify major turning points in the twentieth
century?s history.
2 Data: Time Magazine Corpus
For the present research, we have used a collection
of electronic articles harvested from the archive of
the well-known weekly publication Time Maga-
zine. The magazine?s online archive is protected
by international copyright law and it can only be
consulted via a paying subscription.
2
Therefore,
the corpus cannot be freely redistributed in any
format. To construct the corpus, we have used
metadata provided by corpus linguist Mark Davies
who has published a searchable interface to the
Time Corpus (Davies, 2013). For the present
paper, we were only dependent on the unique
identification number and publication year which
Davies provides for each article. Users who are in-
terested in downloading (a portion of) the corpus
which we used, can use this metadata to replicate
our findings.
We have used the Stanford CoreNLP Suite to
annotate this collection (with its default settings
for the English language).
3
We have tokenized
and lemmatized the corpus with this tool suite.
Additionally, we have applied part-of-speech tag-
2
http://content.time.com/time/archive.
3
http://nlp.stanford.edu/software/
corenlp.shtml
Period # Documents # Word forms # Unique forms
1920s 24,332 11,155,681 158,443
1930s 32,788 20,622,526 222,777
1940s 41,832 22,547,958 234,918
1950s 42,249 25,638,032 251,658
1960s 35,440 27,355,389 258,276
1970s 27,804 25,449,488 218,322
1980s 25,651 24,185,889 208,678
1990s 23,300 20,637,179 204,393
2000s 17,299 14,151,399 176,515
Overall 270,695 191,743,541 867,399
Table 1: General word frequency statistics on the
reconstructed version of the Time Corpus (1923-
2006).
ging (Toutanova et al., 2003) and named entity
recognition (Finkel et al., 2005). In the end, our
reconstructed version of the Time Corpus in to-
tal amounted to 270,695 individual articles. In
its entirety, the corpus counted 191,743,541 dis-
tinct word forms (including punctuation marks),
867,399 forms of which proved unique in their
lowercased format. Some general statistics about
our reconstructed version of the Time Corpus are
given in Table 1. In addition to the cumulative
word count statistics about the corpus, we have
included the frequency information per decade
(1920s, 1930s, etc.), as this periodisation will
prove important for the experiments described in
section 4.
In its entirety, the corpus covers the period
March 1923 throughout December 2006. It only
includes articles from the so-called ?U.S. edition?
of Time (i.e., it does not contain articles which
only featured in the e.g. European edition of the
Magazine). Because of Time?s remarkably contin-
uous publication history, as well as the consider-
able attention the magazine traditionally pays to
international affairs and politics, the Time Cor-
pus can be expected to offer an interesting, al-
beit exclusively American perspective on the re-
cent world history. As far as we know, the cor-
pus has only been used so far in corpus linguistic
publications and we do not know of any advanced
studies in the field of cultural history that make
extensive use of the corpus.
3 Assumption: Lexical frequency
Previous contributions to the field of Culturomics
all have in common that they attempt to establish a
correlation between word frequency statistics and
cultural phenomena. While this is rarely explicitly
voiced, the broader assumption underlying these
studies is that frequency statistics extracted from
63
the texts produced by a society at specific mo-
ment in history, will necessarily reflect that soci-
ety?s cultural specific (e.g. cultural) concerns at
that time. As such, it can for instance be expected
that the frequency of conflict-related terminology
will tend to be more elevated in texts produced by
a society at war than one at peace. (Needless to
say, this need not imply that a society e.g. sup-
ports that war, since the same conflict-related ter-
minology will be frequent in texts that oppose a
particular conflict.) Obviously, the resulting as-
sumption is that the study of developments in the
vocabulary of a large body of texts should enable
the study of the evolution of the broader histori-
cal concerns that exist(ed) in the culture in which
these texts were produced.
Frequency has been considered a key measure
in recent studies into cultural influence (Skiena
and Ward, 2013). The more frequent a word
in a corpus, the more weighty the cultural con-
cerns which that word might be related to. A
naive illustration of this frequency effect can be
gleaned from Figure 1. In the subplots of the fig-
ure, we have plotted the absolute frequency with
which the last names of U.S. presidents have been
yearly mentioned throughout the Time Corpus (in
their lowercased form, and only when tagged as
a named entity). The horizontal axis represents
time, with grey zones indicating start and end
dates of the administration periods. The absolute
frequencies have been normalised in each year, by
taking their ratio over the frequency of the defi-
nite article the. Before plotting, these relative fre-
quencies have been mean-normalised. (Readers
are kindly requested to zoom in on the digital PDF
to view more detail for all figures.) Although this
is by no means a life-changing observation, each
presidential reign is indeed clearly characterised
by a significant boost in the frequency of the cor-
responding president?s last name. Nevertheless,
the graph also displays some notable deficiencies,
such the confusion of father and son Bush, or the
increase in frequency right before an administra-
tion period, which seems related to the presidential
election campaigns.
Importantly, it has been stressed that reliable
frequency information can only be extracted from
large enough corpora, in order to escape the bias
caused by limiting oneself to e.g. too restricted a
number of topics or text varieties. This has caused
studies to stress the importance of so-called ?Big
Figure 1: Diachronic visualisation of mean-
normalised frequencies-of-mention of the last
names of U.S. presidents in the Time corpus, to-
gether with their administration periods.
Coolidge
Hoover
Roosevelt
Truman
Eisenhower
Kennedy
Johnson
Nixon
Ford
Carter
Reagan
Bush
Clinton
1920 1930 1940 1950 1960 1970 1980 1990 2000 2010
Bush
Data? when it comes to Culturomics, reviving the
old adagium from the field of Machine Learning
?There?s no data like more data?, attributed to Mer-
cer. In terms of data size, it is therefore an impor-
tant question whether the Time Corpus is a reli-
able enough resource for practicing Culturomics.
While the Time Corpus (just under 200 million to-
kens) is not a small data set, it is of course orders
of magnitude smaller than the Google Books cor-
pus with its intimidating 361 billion words. As
such, the Time Corpus might hardly qualify as
?Big Data? in the eyes of many contemporary data
scientists. One distinct advantage which the Time
Corpus might offer to counter-balance the disad-
vantage of its limited size, is the high quality, both
of the actual text, as well as the metadata (OCR-
errors are for instance extremely rare).
In order to assess whether a smaller, yet higher-
quality corpus like the Time Corpus might yield
valid results when it comes to Culturomics we
have attempted to replicate an interesting experi-
ment reported by Acerbi et al. (2013b) in the con-
text of a paper on the expression of emotions in
twentieth century books. For their research, they
used the publicly available Google Books unigram
corpus. In our Figure 2 we have reproduced their
?Figure 1: Historical periods of positive and neg-
ative moods?. For this analysis, they used the so-
called LIWC-procedure: a methodology which at-
tempts to measure the presence of particular emo-
tions in texts by calculating the relative occur-
rences of a set of key words (Tausczik and Pen-
64
nebaker, 2010).
4
In the authors? own words, the
graph ?shows that moods tracked broad histori-
cal trends, including a ?sad? peak corresponding
to Second World War, and two ?happy? peaks, one
in the 1920?s and the other in the 1960?s.?
We have exactly re-engineered their methodol-
ogy and applied it to the Time Corpus. The result
of this entirely parallel LIWC-analysis (Tausczik
and Pennebaker, 2010) of the Time Corpus is visu-
alized in Figure 3. While our data of course only
starts in 1923 instead of 1900 (cf. grey area), it
is clear that our experiment has produced a sur-
prisingly similar curve, especially when it comes
to the ?sad? and ?happy? periods in the 1940s and
1960s respectively. These pronounced similarities
are especially remarkable because, to our knowl-
edge, the Time Corpus is not only much smaller
but also completely unrelated to the Google Books
corpus. This experiment thus serves to emphasise
the remarkable stability of certain cultural trends
as reflected across various text types and unrelated
text corpora.
5
Moreover, these results suggest that
the Time Corpus, in spite of limited size, might
still yield interesting and valid results in the con-
text of Culturomics research.
4 Parsimonious Language Models
As discussed above, Michel et al. (2011) have
proposed a methodology in their seminal paper,
whereby, broadly speaking, they try to establish
a correlation between historical events and word
counts in corpora. They show, for instance, that
the term ?Great War? is only frequent in their data
until the 1940s: at that point the more distinc-
tive terms ?World War I? and ?World War II? sud-
denly become more frequent. One interesting is-
sue here is that this methodology is characterised
by a modest form a ?cherry picking?: with this
way of working, a researcher will only try out
word frequency plots of which (s)he expects be-
forehand that they will display interesting trends.
Inevitably, this fairly supervised approach might
lower one?s chance to discover new phenomena,
and thus reduces the chance for scientific serendip-
ity to occur. An equally interesting, yet much less
4
We would like to thank Ben Verhoeven for sharing
his LIWC-implementation. The methodology adopted by
Acerbi et al. (2013b) has been detailed in the follow-
ing blog post: http://acerbialberto.wordpress.
com/tag/emotion/.
5
Acerbi et al. (2013a) have studied the robustness of their
own experiments recently, using different metrics.
Figure 2: Figure reproduced from Acerbi et
al. (2013b): ?Figure 1: Historical periods of posi-
tive and negative moods?. Also see Figure 3.
supervised approach might therefore be to auto-
matically identify which terms are characteristic
for a given time span in a corpus.
In this respect, it is interesting to refer to Par-
simonious Language Models (PLMs), a fairly re-
cent addition to the field of Information Retrieval
(Hiemstra et al., 2004). PLMs can be used to cre-
ate a probabilistic model of a text collection, de-
scribing the relevance of words in individual docu-
ments in contrast to all other texts in the collection.
From the point of view of indexing in Information
Retrieval, the question which a PLM in reality tries
to answer is: ?Suppose that in the future, a user
will be looking for this document, which search
terms is (s)he most likely to use?? As such, PLMs
offers a powerful alternative to the established TF-
IDF metric, in that they are also able to estimate
which words are most characteristic of a given
document. While PLMs are completely unsuper-
vised (i.e. no manual annotation of documents is
needed), they do require setting the ? parameter
beforehand. The ? parameter will of course have
a major influence on the final results, since it will
control the rate at which the language of each doc-
ument will grow different from that of all other
documents, during the subsequent updates of the
model. (For the mathematical details on ?, con-
sult Hiemstra et al. (2004).) Thus, PLMs can be
expected to single out more characteristic vocab-
65
Figure 3: LIWC-analysis carried on the Time Cor-
pus (cf. Figure 2), attempting to replicate the
trends found by Acerbi et al. (2013b). Plotted
is the absolute difference between the z-scores
for the LIWC-categories ?Positive emotions? and
?Negative emotions?. The same smoother (?Fried-
man?s supersmoother?) has been applied (R Core
Team, 2013).
l
l
l
l
l
l
llll
ll
ll
l
l
l
l
l
l
ll
l
l
l
ll
l
l
lll
l
l
ll
llll
l
l
l
l
lll
l
l
lll
l
l
ll
lll
ll
l
l
l
l
l
l
l
l
ll
ll
l
lll
l
l
l
l
l
l
l
1900 1920 1940 1960 1980 2000
0.00
2
0.00
4
0.00
6
0.00
8
0.01
0
Year
Joy?
Sad
ness
 (z?s
cores
)
ulary than simpler frequentist approaches and, in-
terestingly, they are more lightweight to run than
e.g. temporal topic models.
In a series of explorative experiments, we have
applied PLMs to the Time Corpus. In particu-
lar, we have build PLMs for this data, by com-
bining individual articles into much larger docu-
ments: both for each year in the data, as well as all
?decades? (e.g. 1930 = 1930?1939) we have con-
structed such large, multi-article documents. For
both document types (years and decades), we have
subsequently generated PLMs. In Figure 4 to Fig-
ure 12 we have plotted the results for the PLMs
based on the decade documents (for ? = 0.1).
In the left subpanel, we show the 25 words (tech-
nically, the lowercased lemma?s) which the PLM
estimated to be most discriminative for a given
decade. In the right subpanel, we have plotted the
evolution of the relevance scores for each of these
25 words in the year-based PLM (the grey zone in-
dicates the decade). Higher scores indicate a more
pronounced relevance for a given decade. For the
sake of interpretation, we have restricted our anal-
ysis to words which were tagged as nouns (?NN?
Figure 4: PLM for the 1920s.
manplay
newspapername
sonbill
gentlemanmotor
letterdaughter
tariffp.
railroadlady
footinterest
speechstatement
governorperson
shipcondition
honorautomobile
a.
Highest PML scores (1920s)
1920 1930 1940 1950 1960 1970 1980 1990 2000 2010
Figure 5: PLM for the 1930s.
weekp.
centfortnight
a.picture
presidentson
governorcinema
nameauthor
employefriend
automobileno.
goldrailroad
m.bank
daughternewshawk
edwife
depression
Highest PML scores (1930s)
1920 1930 1940 1950 1960 1970 1980 1990 2000 2010
& ?NNS?).
It is not immediately clear how the output of
these PLMs can be evaluated using quantitative
means. A concise qualitative discussion seems
most appropriate to assess the results. For this
reason, we have combined individual articles into
larger decade documents in these experiments,
since this offers a very intuitive manner of ar-
ranging the available sources from a historical,
interpretative point of view. Often, when peo-
ple address the periodisation of the twentieth cen-
tury they will use decades, where terms like e.g.
?the seventies? or ?the twenties? refer to a fairly
well-delineated concept in people?s minds, associ-
ated with a particular set of political events, peo-
ple and cultural phenomena, etc. By sticking
to this decade-based periodisation, we can verify
fairly easily to what extent the top 25 yielded by
the PLM corresponds to commonplace historical
66
Figure 6: PLM for the 1940s.
warweek
manplane
dayair
shipsoldier
radiojap
productionbattle
enemyadmiral
laborfront
officerbomber
bombmile
tonjob
tanktroops
plant
Highest PML scores (1940s)
1920 1930 1940 1950 1960 1970 1980 1990 2000 2010
Figure 7: PLM for the 1950s.
manweek
timeday
tvstory
productionpicture
handrecord
boyred
worldarmy
endparty
ft.committee
wifepp
mileshow
girlfarm
trouble
Highest PML scores (1950s)
1920 1930 1940 1950 1960 1970 1980 1990 2000 2010
Figure 8: PLM for the 1960s.
manp.m.
studentnation
schoolcity
churchnegro
stateart
centuryspace
worldgirl
universitytoday
areacollege
jetlife
moonfact
ft.market
rights
Highest PML scores (1960s)
1920 1930 1940 1950 1960 1970 1980 1990 2000 2010
Figure 9: PLM for the 1970s.
%price
oilcountry
problemnation
inflationgovernment
energystate
blackofficial
areacourt
ratewoman
exampletax
policyincrease
leaderaide
citypeople
group
Highest PML scores (1970s)
1920 1930 1940 1950 1960 1970 1980 1990 2000 2010
Figure 10: PLM for the 1980s.
%official
countrygovernment
computerleader
missilepolicy
firmpeople
budgetcompany
rateprogram
weaponarm
deficitsystem
drugforce
problemhostage
grouptelevision
aide
Highest PML scores (1980s)
1920 1930 1940 1950 1960 1970 1980 1990 2000 2010
Figure 11: PLM for the 1990s.
peoplechild
nokid
familymovie
drugwoman
computerparent
issuetv
wayfilm
directorlot
showguy
decademedia
thinggroup
phonenetwork
sex
Highest PML scores (1990s)
1920 1930 1940 1950 1960 1970 1980 1990 2000 2010
67
Figure 12: PLM for the 2000s.
kidpeople
companyparent
lotmovie
drugguy
thingfamily
cellphone
siteinternet
risktechnology
attackstudy
officialintelligence
waysource
websiteterrorism
group
Highest PML scores (2000s)
1920 1930 1940 1950 1960 1970 1980 1990 2000 2010
stereotypes about the decades in the twentieth cen-
tury.
Let us start by inspecting the top 25 for the
1940s, a decade in which the Second War II natu-
rally played a major role. Already at first glance,
it is clear that the top 25 is dominated by war-
related terminology (war, soldier, enemy, . . . ). In-
terestingly, the list also contains words referring
to WWII, but not from the politically correct jar-
gon which we would nowadays use to address the
issue (e.g. jap). Remarkable is the pronounced
position of aviary vocabulary (bomber, air, plane,
. . . ), which is perhaps less surprising if we con-
sider the fact that WWI was one of the first inter-
national conflicts in which aircrafts played a major
military role.
Interestingly, the 1920s are hardly characterised
by an equally focused set of relevant words. Al-
though mobility does seem to play an important
role (cf. the recently invented automobile, but
also ship and railroad), a number of less mean-
ingful abbreviations (such as p. for ?page?) pop
up that seem connected to superficial changes in
Time?s editorial policies, rather than cultural de-
velopments. (Future analyses might want to re-
move such words manually.) On the other hand,
the use of the terms lady and honour might be
rooted in a cultural climate that is different from
ours (?lady? seems the equivalent of woman to-
day). A number of parallel observations can be
made for the 1930s, although here, the high rank-
ing word depression is of course striking (cf. the
economic crisis of 1929). Fascinatingly, a variety
of denominations for (popular) media play a ma-
jor role throughout the decade PLMs. Note, that
while the 1920s? top 25 mentioned the radio as the
primary communication medium, the popular cin-
ema and (moving?) picture show up in the 1930s.
Interestingly, the popular media of tv and record
make their appearance in the 1950s. (In the 1980s
and 1990s top 25, television moreover continues
to show up.)
The PLM also seems to offer an excellent cul-
tural characterisation of the 1960s and the associ-
ated baby boom, with an emphasis on the contro-
versies of the time, debate involving human rights
(rights, negro, nation), and in particular educa-
tional (college, university, school). The use of
?educational? words might well be related to the
social unrest, much of which took place in and
around universities. Does the striking presence of
the word ?today? in the list reveal an elevated hic et
nunc mentality in the contemporary States? Amer-
ica?s well-documented interest in space traveling
at the time is also appropriately reflected (space,
moon). Perhaps unexpectedly, this seemingly op-
timistic ?Zeitgeist? is more strongly associated in
the Time Corpus with the sixties, than with the
seventies: in the flower-power era, Time displays a
remarkable focus on political and especially eco-
nomic issues. Rather, the oil crisis seems to domi-
nate Time?s lexis in the seventies.
In the 1990s and 2000s, we can observe a fo-
cus on what one might unrespectfully call ?first-
world problems?, involving for instance family re-
lations (family, kid, parent, child, etc.). Apart
from the fact that Time?s vocabulary seems to grow
more colloquial in general in this period (at least
in our eyes, e.g. guy, lot, thing), a number of
controversial taboo subjects seem to have become
discussable: sex, drug. ?Terrorism? and ?intelli-
gence? seem to have become major concerns in
post-09/11 America, and perhaps the presence of
the word ?attack? and ?technology? might be (par-
tially) interpreted in the same light. Again, we
see how vocabulary related to media absolutely
dominates the final rankings in the corpus: Hol-
lywood seems to have enjoyed an increasing pop-
ularity (film, director, movie, . . . ) but it is in-
formation technology that seems to have had the
biggest cultural impact: mobile communication
devices (phone, cell) and Internet-related termi-
nology (network, computer, internet, . . . ) seem to
have caused a major turning point in Time?s lexis.
6
6
Due to lack of space, we only report results for ? = 0.1
applied to nouns, but highly similar results could be obtained
68
5 Twentieth Century Turning Points?
An interesting technique in this respect is a clus-
tering method called VNC or ?Variability-Based
Neighbor Clustering? (Gries and Hilpert, 2008).
The technique has been introduced in the field of
historical linguistics as an aid in the automated
identification of temporal stages in diachronic
data. The method will apply a fairly straightfor-
ward clustering algorithm to a data set (with e.g.
Ward linkage applied to a Cosine distance matrix)
but, importantly, it will add the connectivity con-
straint that (clusters of) data points can only merge
with each other at the next level in a dendrogram,
if they are immediately adjacent. That is to say
that e.g. in a series of yearly observations 1943
would be allowed to merge with 1942 and 1944,
but not with 1928 (even if 1943 would be much
more similar to 1928 than to 1943). We have ap-
plied VNC (with Ward linkage applied to a plain
Cosine distance table) to a series of vectors which
for each year in our data (1923-2006) contained
the PML scores of 5,000 words deemed most rel-
evant for that year by the model.
The dendrogram resulting from the VNC pro-
cedure is visualised in Figure 13. The early his-
tory of Time Magazine (1923-1927) does not re-
ally seem to fit in with the rest and takes up a fairly
deviant position. However, the most attention-
grabbing feature of this tree structure is the major
divide which the dendrogram suggests (cf. red vs.
green-blue cluster) between the years before and
after 1945, the end of the Second World War. An-
other significant rupture seem to be present before
and after 1996: the discussion leaves us to won-
der whether this turning point might related to the
recent introduction of new communication tech-
nologies, in particular the rise of the Internet.
Historically speaking, these turning points do
not come as a surprise. There is, for instance,
widespread acceptance among historians WWII
has indeed been the single most influential event
in the twentieth century. What does surprise, how-
ever, is the relative easy with which a completely
unsupervised procedure has managed to suggest
using other part-of-speech categories and settings for ?. An
interesting effect was associated with ?fiddling the knob? of
this last parameter: for lower values (0.01, 0.001 etc.), the
model would come up with perhaps increasingly characteris-
tic, but also increasingly obscure and much less frequent vo-
cabulary. For the fourties, for instance, instead of returning
the word ?bomber? the analysis would return the exact name
of a particular bomber type which was used at the time. This
parameter setting deserves further exploration.
Figure 13: Dendrogram resulting from apply-
ing Variability-Based Neighbor Analysis to vec-
tors which contain for each year the 5,000 words
deemed most relevant by the PML.
1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006
0.000
0.005
0.010
0.015
0.020
Variability-Based Neighbor Clustering Dendrogram(5000 highest PML scores per year)
this identification. Arguably, this is where we
leave the realm of the obvious when it comes to
the computational study of cultural history. The
identification of major events and turning points in
human history is normally a task which requires a
good deal of formal education and some advanced
reasoning skills. Here, we might be nearing a
modest form of Artificial Intelligence when we ap-
ply computational methods to achieve a fairly sim-
ilar goal. Hopefully, these analyses, as well as the
ones reported above, illustrate the huge potential
of computational methods in the study of cultural
history, even if only as a discovery tool.
6 Conclusion and criticism
In this paper we have discussed a series of analy-
ses that claim to mine a data-driven cultural char-
acterization of the ?Zeitgeist? of some of the main
periods in the twentieth century. Nevertheless, we
must remain vigilant not to overstate the achieve-
ment of these techniques: it remains to be deter-
mined to which extent can we truly call these ap-
plications Digital History and whether these anal-
yses have taught us anything which we did not
know before. Because the twentieth century is
so well known to most of us, the evidence often
tends to be self-referential and self-explanatory,
and merely confirms that which we already knew
intuitively. Like with most distant reading ap-
proaches, the results urge us to go back to the orig-
inal material for the close reading of individual
sources in their historical context, in order to ver-
69
ify the macro-hypotheses that might be suggested
at a higher level. Therefore, the proposed method
might in fact be more suitable for the study of time
periods and corpora of which we know less.
Nevertheless, our methodology seems promis-
ing for future applications in Digital History: our
na??ve periodisation in decades, for instance, might
be hugely fine-tuned by processing the results of
a VNC-dendrogram. Breaking up history into
meaningful units is a much more complex, and of-
ten controversial matter (e.g. ?When does moder-
nity start??). In this light, it would be helpful to
have at our disposal unbiased, computational tools
that might help us to identify cultural ruptures or
even turning points in history. Our results reported
in the final section do show that this application
yields interesting results, and again, the method
seems promising for the analysis of lesser known
corpora.
Acknowledgments
The authors would like to thank the anonymous re-
viewers and Kalliopi Zervanou for their valuable
feedback on earlier drafts of this paper, as well as
Walter Daelemans and Antal van den Bosch for
the inspiring discussions on the topic. For this
study, Mike Kestemont was funded as a postdoc-
toral research fellow for the Research Foundation
of Flanders (FWO). Folgert Karsdorp was sup-
ported as a Ph.D. candidate by the Computational
Humanities Programme of the Royal Netherlands
Academy of Arts and Sciences, as part of the
Tunes & Tales project.
References
Alberto Acerbi, Vasileios Lampos, and Alexander R.
Bentley. 2013a. Robustness of emotion extraction
from 20th century English books. In BigData ?13.
IEEE, IEEE.
Alberto Acerbi, Vasileios Lampos, Philip Garnett, and
Alexander R. Bentley. 2013b. The Expression
of Emotions in 20th Century Books. PLoS ONE,
8(3):e59030.
Mark Davies. 2013. TIME Magazine Corpus: 100
million words, 1920s-2000s.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ?05, pages 363?370, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Stefan Th. Gries and Martin Hilpert. 2008. The iden-
tification of stages in diachronic data: variability-
based neighbour clustering. Corpora, 3(1):59?81.
Djoerd Hiemstra, Stephen E. Robertson, and Hugo
Zaragoza. 2004. Parsimonious language models for
information retrieval. In Mark Sanderson, Kalervo
Jrvelin, James Allan, and Peter Bruza, editors, SI-
GIR, pages 178?185. ACM.
Patrick Juola. 2013. Using the Google N-Gram corpus
to measure cultural complexity. Literary and Lin-
guistic Computing, 28(4):668?675.
Kalev H. Leetaru. 2011. Culturomics 2.0: Forecasting
large-scale human behavior using global news media
tone in time and space. First Monday, 16(9).
Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser
Aiden, Adrian Veres, Matthew K. Gray, The
Google Books Team, Joseph P. Pickett, Dale
Hoiberg, Dan Clancy, Peter Norvig, Jon Orwant,
Steven Pinker, Martin A. Nowak, and Erez Lieber-
man Aiden. 2011. Quantitative Analysis of Cul-
ture Using Millions of Digitized Books. Science,
331(6014):176?182.
R Core Team, 2013. R: A Language and Environment
for Statistical Computing. R Foundation for Statis-
tical Computing, Vienna, Austria.
Steve Skiena and Charles Ward. 2013. Who Belongs
in Bonnie?s Textbook? Cambridge University Press.
Yla R. Tausczik and James W. Pennebaker. 2010. The
Psychological Meaning of Words: LIWC and Com-
puterized Text Analysis Methods. Journal of Lan-
guage and Social Psychology, 29(1):24?54.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich Part-of-
speech Tagging with a Cyclic Dependency Network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, NAACL ?03, pages 173?180, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Jean M. Twenge, Keith W. Campbell, and Brittany
Gentile. 2012. Increases in Individualistic Words
and Phrases in American Books, 19602008. PLoS
ONE, 7(7):e40181.
Gerben Zaagsma. 2013. On Digital History. BMGN ?
Low Countries Historical Review, 128(4):3?29.
70
Proceedings of the 3rd Workshop on Computational Linguistics for Literature (CLfL) @ EACL 2014, pages 59?66,
Gothenburg, Sweden, April 27, 2014.
c?2014 Association for Computational Linguistics
Function Words in Authorship Attribution
From Black Magic to Theory?
Mike Kestemont
University of Antwerp
CLiPS Computational Linguistics Group
Prinsstraat 13, D.188
B-2000, Antwerp
Belgium
mike.kestemont@uantwerpen.be
Abstract
This position paper focuses on the use
of function words in computational au-
thorship attribution. Although recently
there have been multiple successful appli-
cations of authorship attribution, the field
is not particularly good at the explication
of methods and theoretical issues, which
might eventually compromise the accep-
tance of new research results in the tra-
ditional humanities community. I wish to
partially help remedy this lack of explica-
tion and theory, by contributing a theoreti-
cal discussion on the use of function words
in stylometry. I will concisely survey the
attractiveness of function words in stylom-
etry and relate them to the use of charac-
ter n-grams. At the end of this paper, I
will propose to replace the term ?function
word? by the term ?functor? in stylometry,
due to multiple theoretical considerations.
1 Introduction
Computational authorship attribution is a popu-
lar application in current stylometry, the compu-
tational study of writing style. While there have
been significant advances recently, it has been no-
ticed that the field is not particularly good at the
explication of methods, let alone at developing a
generally accepted theoretical framework (Craig,
1999; Daelemans, 2013). Much of the research
in the field is dominated by an ?an engineering
perspective?: if a certain attribution technique per-
forms well, many researchers do not bother to ex-
plain or interpret this from a theoretical perspec-
tive. Thus, many methods and procedures con-
tinue to function as a black box, a situation which
might eventually compromise the acceptance of
experimental results (e.g. new attributions) by
scholars in the traditional humanities community.
In this short essay I wish to try to help partially
remedy this lack of theoretical explication, by con-
tributing a focused theoretical discussion on the
use of function words in stylometry. While these
features are extremely popular in present-day re-
search, few studies explicitly address the method-
ological implications of using this word category.
I will concisely survey the use of function words in
stylometry and render more explicit why this word
category is so attractive when it comes to author-
ship attribution. I will deliberately use a generic
language that is equally intelligible to people in
linguistic as well as literary studies. Due to mul-
tiple considerations, I will argue at the end of this
paper that it might be better to replace the term
?function word? by the term ?functor? in stylome-
try.
2 Seminal Work
Until recently, scholars agreed on the supremacy
of word-level features in computational authorship
studies. In a 1994 overview paper Holmes (1994,
p. 87) claimed that ?to date, no stylometrist has
managed to establish a methodology which is bet-
ter able to capture the style of a text than that based
on lexical items?. Important in this respect is a
line of research initiated by Mosteller and Wal-
lace (1964), whose work marks the onset of so-
called non-traditional authorship studies (Holmes,
1994; Holmes, 1998). Their work can be con-
trasted with the earlier philological practice of au-
thorship attribution (Love, 2002), often character-
ized by a lack of a clearly defined methodological
framework. Scholars adopted widely diverging at-
tribution methodologies, the quality of whose re-
sults remained difficult to assess in the absence of
a scientific consensus about a best practice (Sta-
matatos, 2009; Luyckx, 2010). Generally speak-
ing, scholars? subjective intuitions (Gelehrtenintu-
ition, connoisseurship) played far too large a role
and the low level of methodological explicitness in
59
early (e.g. nineteenth century) style-based author-
ship studies firmly contrasts with today?s prevail-
ing criteria for scientific research, such as replica-
bility or transparency.
Apart from the rigorous quantification
Mosteller and Wallace pursued, their work is
often praised because of a specific methodolog-
ical novelty they introduced: the emphasis on
so-called function words. Earlier authorship
attribution was often based on checklists of
stylistic features, which scholars extracted from
known oeuvres. Based on their previous reading
experiences, expert readers tried to collect style
markers that struck them as typical for an oeuvre.
The attribution of works of unclear provenance
would then happen through a comparison of
this text?s style to an author?s checklist (Love,
2002, p. 185?193). The checklists were of course
hand-tailored and often only covered a limited set
of style markers, in which lexical features were
for instance freely mixed with hardly compara-
ble syntactic features. Because the checklist?s
construction was rarely documented, it seemed
a matter of scholarly taste which features were
included in the list, while it remained unclear why
others were absent from it.
Moreover, exactly because these lists were
hand-selected, they were dominated by striking
stylistic features that because of their low over-
all frequency seemed whimsicalities to the human
expert. Such low-frequency features (e.g. an un-
common noun) are problematic in authorship stud-
ies, since they are often tied to a specific genre
or topic. If such a characteristic was absent in
an anonymous text, it did not necessarily argue
against a writer?s authorship in whose other texts
(perhaps in different topics or genres) the charac-
teristic did prominently feature. Apart from the
limited scalability of such style (Luyckx, 2010;
Luyckx and Daelemans, 2011), a far more trou-
blesome issue is associated with them. Because of
their whimsical nature these low-frequency phe-
nomena could have struck an author?s imitators or
followers as strongly as they could have struck a
scholar. When trying to imitate someone?s style
(e.g. within the same stylistic school), those low-
frequency features are the first to copy in the eyes
of forgers (Love, 2002, p. 185?193). The funda-
mental novelty of the work by Mosteller and Wal-
lace was that they advised to move away from a
language?s low-frequency features to a language?s
high-frequency features, which often tend to be
function words.
3 Content vs Function
Let us briefly review why function words are in-
teresting in authorship attribution. In present-day
linguistics, two main categories of words are com-
monly distinguished (Morrow, 1986, p. 423). The
open-class category includes content words, such
as nouns, adjectives or verbs (Clark and Clark,
1977). This class is typically large ? there are
many nouns ? and easy to expand ? new nouns
are introduced every day. The closed-class cat-
egory of function words refers to a set of words
(prepositions, particles, determiners) that is much
smaller and far more difficult to expand ? it is
hard to invent a new preposition. Words from the
open class can be meaningful in isolation because
of their straightforward semantics (e.g. ?cat?).
Function words, however, are heavily grammati-
calized and often do not carry a lot of meaning
in isolation (e.g. ?the?). Although the set of dis-
tinct function words is far smaller than the set
of open-class words, function words are far more
frequently used than content words (Zipf, 1949).
Consequently, less than 0.04% of our vocabulary
accounts for over half of the words we actually use
in daily speech (Chung et al., 2007, p. 347). Func-
tion words have methodological advantages in the
study of authorial style (Binongo, 2003, p. 11), for
instance:
? All authors writing in the same language and
period are bound to use the very same func-
tion words. Function words are therefore a
reliable base for textual comparison;
? Their high frequency makes them interesting
from a quantitative point of view, since we
have many observations for them;
? The use of function words is not strongly af-
fected by a text?s topic or genre: the use of
the article ?the?, for instance, is unlikely to be
influenced by a text?s topic.
? The use of function words seems less under
an author?s conscious control during the writ-
ing process.
Any (dis)similarities between texts regarding
function words are therefore relatively content-
independent and can be far more easily associated
60
with authorship than topic-specific stylistics. The
underlying idea behind the use of function words
for authorship attribution is seemingly contradic-
tory: we look for (dis)similarities between texts
that have been reduced to a number of features in
which texts should not differ at all (Juola, 2006,
p. 264?65).
Nevertheless, it is dangerous to blindly over-
estimate the degree of content-independence of
function words. A number of studies have shown
that function words, and especially (personal) pro-
nouns, do correlate with genre, narrative perspec-
tive, an author?s gender or even a text?s topic (Her-
ring and Paolillo, 2006; Biber et al., 2006; New-
man et al., 2008). A classic reference in this
respect is John Burrows?s pioneering study of,
amongst other topics, the use of function words
in Jane Austen?s novels (Burrows, 1987). This
explains why many studies into authorship will
in fact perform so-called ?pronoun culling? or the
automated deletion of (personal) pronouns which
seem too heavily connected to a text?s narrative
perspective or genre. Numerous empirical studies
have nevertheless demonstrated that various anal-
yses restricted to higher frequency strata, yield re-
liable indications about a text?s authorship (Arga-
mon and Levitan, 2005; Stamatatos, 2009; Koppel
et al., 2009).
It has been noted that the switch from content
words to function words in authorship attribution
studies has an interesting historic parallel in art-
historic research (Kestemont et al., 2012). Many
paintings have survived anonymously as well,
hence the large-scale research into the attribu-
tion of them. Giovanni Morelli (1816-1891) was
among the first to suggest that the attribution of,
for instance, a Quattrocento painting to some Ital-
ian master, could not happen based on ?content?
(Wollheim, 1972, p. 177ff). What kind of coat
Mary Magdalene was wearing or the particular de-
piction of Christ in a crucifixion scene seemed all
too much dictated by a patron?s taste, contempo-
rary trends or stylistic influences. Morelli thought
it better to restrict an authorship analysis to dis-
crete details such as ears, hands and feet: such
fairly functional elements are naturally very fre-
quent in nearly all paintings, because they are to
some extent content-independent. It is an inter-
esting illustration of the surplus value of function
words in stylometry that the study of authorial
style in art history should depart from the ears,
hands and feet in a painting ? its inconspicuous
function words, so to speak.
4 Subconsciousness
Recall the last advantage listed above: the argu-
ment is often raised that the use of these words
would not be under an author?s conscious control
during the writing process (Stamatatos, 2009; Bi-
nongo, 2003; Argamon and Levitan, 2005; Peng et
al., 2003). This would indeed help to explain why
function words might act as an author invariant
throughout an oeuvre (Koppel et al., 2009, p. 11).
Moreover, from a methodological point of view,
this would have to be true for forgers and imitators
as well, hence, rendering function words resistant
to stylistic imitation and forgery. Surprisingly, this
claim is rarely backed up by scholarly references
in the stylometric literature ? an exception seems
Koppel et al. (2009, p. 11) with a concise refer-
ence to Chung et al. (2007). Nevertheless, some
attractive references in this respect can be found in
psycholinguistic literature. Interesting is the ex-
periment in which people have to quickly count
how often the letter ?f? occurs in the following sen-
tence:
Finished files are the result
of years of scientific study
combined with the experience
of many years.
It is common for most people to spot only
four or five instances of all six occurrences of
the grapheme (Schindler, 1978). Readers com-
monly miss the f s in the preposition ?of? in the
sentence. This is consistent with other reading
research showing that readers have more difficul-
ties in spotting spelling errors in function words
than in content words (Drewnowski and Healy,
1977). A similar effect is associated with phrases
like ?Paris in the the spring? (Aronoff and Fude-
man, 2005, p. 40?41). Experiments have demon-
strated that during their initial reading, many peo-
ple will not be aware of the duplication of the ar-
ticle ?the?. Readers typically fail to spot such er-
rors because they take the use of function words
for granted ? note that this effect would be absent
for ?Paris in the spring spring?, in which a content
word is wrongly duplicated. Such a subconscious
attitude needs not imply that function words would
be unimportant in written communication. Con-
61
sider the following passage:
1
Aoccdrnig to a rscheearch at Cmabrigde
Uinervtisy, it deosn?t mttaer in waht
oredr the ltteers in a wrod are, the olny
iprmoetnt tihng is taht the frist and lsat
ltteer be at the rghit pclae. The rset can
be a toatl mses and you can sitll raed
it wouthit porbelm. Tihs is bcuseae the
huamn mnid deos not raed ervey lteter
by istlef, but the wrod as a wlohe.
Although the words? letters in this passage seem
randomly jumbled, the text is still relatively read-
able (Rawlinson, 1976). As the quote playfully
states itself, it is vital in this respect that the first
and final letter of each word are not moved ? and,
depending on the language, this is in fact not the
only rule that must be obeyed. It is crucial how-
ever that this limitation causes the shorter func-
tion words in running English text to remain fairly
intact (McCusker et al., 1981). The intact nature
alone of the function words in such jumbled text,
in fact greatly adds to the readability of such pas-
sages. Thus, while function words are vital to
structure linguistic information in our communi-
cation (Morrow, 1986), psycholinguistic research
suggests that they do not attract attention to them-
selves in the same way as content words do.
Unfortunately, it should be stressed that all ref-
erences discussed in this section are limited to
reader?s experience, and not writer?s experience.
While there will exist similarities between a lan-
guage user?s perception and production of func-
tion words, it cannot be ruled out that writers will
take on a much more conscious attitude towards
function words than readers. Nevertheless, the
apparent inattentiveness with which readers ap-
proach function words might be reminiscent of
a writer?s attitude towards them, although much
more research would be needed in order to prop-
erly substantiate this hypothesis.
5 Character N-grams
Recall Holmes?s 1994 claim that ?to date, no sty-
lometrist has managed to establish a methodol-
ogy which is better able to capture the style of
1
Matt Davis maintains an interesting website on this
topic: http://www.mrc-cbu.cam.ac.uk/people/
matt.davis/Cmabrigde/. I thank Bram Vandekerck-
hove for pointing out this website. The ?Cmabridge?-passage
as well the ?of?-example have anonymously circulated on the
Internet for quite a while.
a text than that based on lexical items? (Holmes,
1994, p. 87). In 1994 other types of style mark-
ers (e.g. syntactical) were ? in isolation ? never
able to outperform lexical style markers (Van Hal-
teren et al., 2005). Interestingly, advanced fea-
ture selection methods did not always outperform
frequency-based selection methods, that plainly
singled out function words (Argamon and Levitan,
2005; Stamatatos, 2009). The supremacy of func-
tion words was challenged, however, later in the
1990s when character n-grams came to the fore
(Kjell, 1994). This representation was originally
borrowed from the field of Information Retrieval
where the technique had been used in automatic
language identification. Instead of cutting texts up
into words, this particular text representation seg-
mented a text into a series of consecutive, partially
overlapping groups of n characters. A first order
n-gram model only considers so-called unigrams
(n = 1); a second order n-gram model consid-
ers bigrams (n = 2), and so forth. Note that word
boundaries are typically explicitly represented: for
instance, ? b?, ?bi?, ?ig?, ?gr?, ?ra?, ?am?, ?m ?.
Since Kjell (1994), character n-grams have
proven to be the best performing feature type
in state-of-the-art authorship attribution (Juola,
2006), although at first sight, they might seem
uninformative and meaningless. Follow-up re-
search learned that this outstanding performance
was not only largely language independent but
also fairly independent of the attribution algo-
rithms used (Peng et al., 2003; Stamatatos, 2009;
Koppel et al., 2009). The study of character n-
grams for authorship attribution has since then sig-
nificantly grown in popularity, however, mostly
in the more technical literature where the tech-
nique originated. In these studies, performance
issues play an important role, with researchers fo-
cusing on actual attribution accuracy in large cor-
pora (Luyckx, 2010). This focus might help ex-
plain why, so far, few convincing attempts have
been made to interpret the discriminatory qualities
of characters n-grams, which is why their use (like
function words) in stylometry can be likened to a
sort of black magic. One explanation so far has
been that these units tend to capture ?a bit of ev-
erything?, being sensitive to both the content and
form of a text (Houvardas and Stamatatos, 2006;
Koppel et al., 2009; Stamatatos, 2009). One could
wonder, however, whether such an answer does
much more than reproducing the initial question:
62
Then why does it work? Moreover, Koppel et al.
expressed words of caution regarding the caveats
of character n-grams, since many of them ?will be
closely associated to particular content words and
roots? (Koppel et al., 2009, p. 13).
The reasons for this outstanding performance
could partially be of a prosaic, information-
theoretical nature, relating to the unit of stylis-
tic measurement. Recall that function words are
quantitatively interesting, at least partially because
they are simply frequent in text. The more obser-
vations we have available per text, the more trust-
worthily one can represent it. Character n-grams
push this idea even further, simply because texts
by definition have more data points for character
n-grams than for entire words (Stamatatos, 2009;
Daelemans, 2013). Thus the mere number of ob-
servations, relatively larger for character n-grams
than for function words, might account for their
superiority from a purely quantitative perspective.
Nevertheless, more might be said on the topic.
Rybicki & Eder (2011) report on a detailed com-
parative study of a well-known attribution tech-
nique, Burrows?s Delta. John Burrows is consid-
ered one of the godfathers of modern stylometry ?
D.I. Holmes (1994) ranked him alongside the pi-
oneers Mosteller and Wallace. He introduced his
influential Delta-technique in his famous Busa lec-
ture (Burrows, 2002). Many subsequent discus-
sions agree that Delta essentially is a fairly intu-
itive algorithm which generally achieves decent
performance (Argamon, 2008), comparing texts
on the basis of the frequencies of common func-
tion words. In their introductory review of Delta?s
applications, Rybicki and Eder tackled the as-
sumption of Delta?s language independence: fol-
lowing the work of Juola (2006, p. 269), they ques-
tion the assumption ?that the use of methods rely-
ing on the most frequent words in a corpus should
work just as well in other languages as it does in
English? (Rybicki and Eder, 2011, p. 315).
Their paper proves this assumption wrong, re-
porting on various, carefully set-up experiments
with a corpus, comprising 7 languages (English,
Polish, French, Latin, German, Hungarian and
Italian). Although they consider other parameters
(such as genre), their most interesting results con-
cern language (Rybicki and Eder, 2011, p. 319?
320):
while Delta is still the most successful method
of authorship attribution based on word frequen-
cies, its success is not independent of the lan-
guage of the texts studied. This has not been
noticed so far for the simple reason that Delta
studies have been done, in a great majority, on
English-language prose. [. . . ] The relatively
poorer results for Latin and Polish, both highly
inflected in comparison with English and Ger-
man, suggests the degree of inflection as a pos-
sible factor. This would make sense in that the
top strata of word frequency lists for languages
with low inflection contain more uniform words,
especially function words; as a result, the most
frequent words in languages such as English are
relatively more frequent than the most frequent
words in agglutinative languages such as Latin.
Their point of criticism is obvious but vital: the
restriction to function words for stylometric re-
search seems sub-optimal for languages that make
less use of function words. They suggest that this
relatively recent discovery might be related to the
fact that most of the seminal and influential work
in authorship attribution has been carried out on
English-language texts.
English is a typical example of a language that
does not make extensive use of case endings or
other forms of inflection (Sapir, 1921, chapter
VI). Such weakly inflected languages express a lot
of their functional linguistic information through
the use of small function words, such as preposi-
tions (e.g. ?with a sword?). Structural information
in these languages tends to be expressed through
minimal units of meaning or grammatical mor-
phemes, which are typically realized as individ-
ual words (Morrow, 1986). At this point, it makes
sense to contrast English with another major his-
torical lingua franca but one that has received far
less stylometric attention: Latin.
Latin is a school book example of a heavily in-
flected language, like Polish, that makes far more
extensive use of affixes: endings that which are
added to words to mark their grammatical func-
tion in a sentence. An example: in the Latin word
ensi (ablative singular: ?with a sword?) the case
ending (?i) is a separate morpheme that takes on
grammatical role which is similar to that of the
English preposition ?with?. Nevertheless, it is not
realized as a separate word separated by whites-
pace from surrounding morphemes. It is rather
concatenated to another morpheme (ens-) express-
ing a more tangible meaning.
This situation renders a straightforward appli-
cation of the Delta-method ? so heavily biased to-
wards words ? problematic for more synthetic or
agglutinative languages. What has been said about
function words in previous stylometric research,
63
obviously relates to their special status as func-
tional linguistic items. The inter-related character-
istics of ?high frequency?, ?content-independence?
and ?good dispersion? (Kestemont et al., 2012)
even only apply to them, insofar as they are gram-
matical morphemes. Luckily for English, a lot of
grammatical morphemes can easily be detected by
splitting running text into units that do not con-
tain whitespace or punctuation and selecting the
most frequent items among them (Burrows, 2002;
Stamatatos, 2009). For languages that display an-
other linguistic logic, however, the situation is far
more complicated, because the functional infor-
mation contained in grammatical morphemes is
more difficult to gain access to, since these need
not be solely or even primarily realized as separate
words. If one restricts analyses to high-frequency
words in these languages, one obviously ignores
a lot of the functional information inside less fre-
quent words (e.g. inflection).
6 Functors
At the risk of being accused of quibbling about
terms, I wish to argue that the common empha-
sis on function words in stylometry should be re-
placed by an emphasis on the broader concept of
functors, a term which can be borrowed from psy-
cholinguistics, used to denote grammatical mor-
phemes (Kwon, 2005, p. 1?2) or:
forms that do not, in any simple way, make ref-
erence. They mark grammatical structures and
carry subtle modulatory meanings. The word
classes or parts of speech involved (inflections,
auxiliary verbs, articles, prepositions, and con-
junctions) all have few members and do not read-
ily admit new members (Brown, 1973, p. 75).
In my opinion, the introduction of the term ?func-
tor? would have a number of advantages ? the first
and least important of which is that it is aestheti-
cally more pleasing than the identical term ?gram-
matical morphemes?. Note, first of all, that func-
tion words ? grammatical morphemes realized as
individual words ? are included in the definition
of a functor. The concept of a functor as such does
not replace the interest in function words but rather
broadens it and extends it towards all grammatical
morphemes, whether they be realized as individ-
ual words or not. Note how all advantages, previ-
ously only associated with function words in sty-
lometry (high frequency, good dispersion, content-
independence, unconscious use) apply to every
member in the category of functors.
A second advantage has to do with language
independence. Note that stylometry?s ultimate
goal regarding authorship seems of a universal na-
ture: a majority of stylometrists in the end are
concerned with the notorious Stylome-hypothesis
(Van Halteren et al., 2005) or finding a way to
characterize an author?s individual writing style,
regardless of text variety, time and, especially, lan-
guage. Restricting the extraction of functional in-
formation from text to the word level might work
for English, but seems too language-specific a
methodology to be operable in many other lan-
guages, as suggested by Rybicki and Eder (2011)
and earlier Juola (2006, p. 269). Stylometric re-
search into high-frequency, functional linguistic
items should therefore break up words and harvest
more and better information from text. The scope
of stylistic focus should be broadened to include
all functors.
The superior performance of character n-grams
in capturing authorial style ? in English, as well as
other languages ? seems relevant in this respect.
First of all, the most frequent n-grams in a corpus
often tend to be function words: ?me?, ?or? and
?to? are very frequent function words in English,
but they are also very frequent character bigrams.
Researchers often restrict their text representation
to the most frequent n-grams in a corpus (2009,
p. 541), so that n-gram approaches include func-
tion words rather than exclude them. In addition,
high-frequency n-grams are often able to capture
more refined grammatical information. Note how
a text representation in terms of n-grams subtly
exploits the presence of whitespace. In most pa-
pers advocating the use of n-grams, whitespace
is explicitly encoded. Again, this allows more
observations-per-word but, in addition, makes a
representation sensitive to e.g. inflectional infor-
mation. A high frequency of the bigram ?ed? could
reflect any use of the character series (reduce vs.
talked). A trigram representation ?ed ? reveals a
word-final position of the character series, thus in-
dicating it being used for expressing grammatical
information through affixation. Psycholinguistic
research also stresses the important status of the
first letter(s) of words, especially with respect to
how words are cognitively accessed in the lexicon
(Rubin, 1995, p. 74). Note that this word-initial
aspect too is captured under an n-gram representa-
tion (? aspect?).
64
A widely accepted theoretical ground for the
outstanding performance of character n-grams,
will have to consider the fact that n-grams offer
a more powerful way of capturing the functional
information in text. They are sensitive to the inter-
nal morphemic structure of words, capturing many
functors which are simply ignored in word-level
approaches. Although some n-grams can indeed
be ?closely associated to particular content words
and roots? (Koppel et al., 2009, p. 13), I would
be inclined to hypothesize that high-frequency n-
grams work in spite of this, not because of this.
This might suggest that extending techniques, like
Delta, to all functors in text, instead of just func-
tion words, will increase both their performance
and language independence.
A final advantage of the introduction of the con-
cept of a functor is that it would facilitate the team-
ing up with a neighbouring field of research that
seems extremely relevant for the field of stylome-
try from a theoretical perspective, but so far has
only received limited attention in it: psycholin-
guistics. The many parallels with the reading re-
search discussed above indicate that both fields
might have a lot to learn from each other. An il-
lustrative example is the study of functor acquisi-
tion by children. It has been suggested that simi-
lar functors are not only present in all languages
of the world, but acquired by all children in an
extremely similar ?natural order? (Kwon, 2005).
This is intriguing given stylometry?s interest in the
Stylome-hypothesis. If stylometry is ultimately
looking for linguistic variables that are present in
each individual?s parole, the universal aspects of
functors further stress the benefits of the term?s
introduction. All of this justifies the question
whether the functor should not become a privi-
leged area of study in future stylometric research.
Acknowledgments
The author was funded as a postdoctoral research
fellow by the Research Foundation of Flanders
(FWO). The author would like to thank Matthew
Munson, Bram Vandekerckhove, Dominiek San-
dra, Stan Szpakowicz as well as the anonymous re-
viewers of this paper for their substantial feedback
on earlier drafts. Finally, I am especially indebted
to Walter Daelemans for the inspiring discussions
on the topic of this paper.
References
S. Argamon and S. Levitan. 2005. Measuring the use-
fulness of function words for authorship attribution.
In Proceedings of the Joint Conference of the Asso-
ciation for Computers and the Humanities and the
Association for Literary and Linguistic Computing
(2005). Association for Computing and the Human-
ities.
S. Argamon. 2008. Interpreting Burrows?s Delta: Ge-
ometric and Probabilistic Foundations. Literary and
Linguistic Computing, (23):131?147.
M. Aronoff and K. Fudeman. 2005. What is Morphol-
ogy? Blackwell.
D. Biber, S. Conrad, and R. Reppen. 2006. Corpus lin-
guistics - Investigating language structure and use.
Cambridge University Press, 5 edition.
J. Binongo. 2003. Who Wrote the 15th Book of Oz?
An application of multivariate analysis to authorship
attribution. Chance, (16):9?17.
R. Brown. 1973. A First Language. Harvard Univer-
sity Press.
J. Burrows. 1987. Computation into Criticism: A
Study of Jane Austen?s Novels and an Experiment in
Method. Clarendon Press; Oxford University Press.
J. Burrows. 2002. ?Delta?: a measure of stylistic dif-
ference and a guide to likely authorship. Literary
and Linguistic Computing, (17):267?287.
C. Chung and J. Pennebaker. 2007. The psychologi-
cal functions of function words. In K. Fiedler et al.,
editor, Social Communication, pages 343?359. Psy-
chology Press.
H. Clark and E. Clark. 1977. Psychology and lan-
guage: an introduction to psycholinguistics. Har-
court, Brace & Jovanovich.
H. Craig. 1999. Authorial attribution and computa-
tional stylistics: if you can tell authors apart, have
you learned anything about them? Literary and Lin-
guistic Computing, 14(1):103?113.
W. Daelemans. 2013. Explanation in Computa-
tional Stylometry. In Proceedings of the 14th In-
ternational Conference on Computational Linguis-
tics and Intelligent Text Processing - Volume 2,
CICLing?13, pages 451?462, Berlin, Heidelberg.
Springer-Verlag.
A. Drewnowski and A. Healy. 1977. Detection errors
on the and and: Evidence for reading units larger
than the word. Memory & Cognition, (5).
S. Herring and John C. Paolillo. 2006. Gender and
genre variation in weblogs. Journal of Sociolinguis-
tics, 10(4):439?459.
D. Holmes. 1994. Authorship Attribution. Computers
and the Humanities, 28(2):87?106.
65
D. Holmes. 1998. The Evolution of Stylometry in Hu-
manities Scholarship. Literary and Linguistic Com-
puting, 13(3):111?117.
J. Houvardas and E. Stamatatos. 2006. N-gram feature
selection for authorship identification. In J. Euzenat
and J. Domingue, editors, Proceedings of Artificial
Intelligence: Methodologies, Systems, and Applica-
tions (AIMSA 2006), pages 77?86. Springer-Verlag.
P. Juola. 2006. Authorship Attribution. Foundations
and Trends in Information Retrieval, 1(3):233?334.
M. Kestemont, W. Daelemans, and D. Sandra. 2012.
Robust Rhymes? The Stability of Authorial Style in
Medieval Narratives. Journal of Quantitative Lin-
guistics, 19(1):1?23.
B. Kjell. 1994. Discrimination of authorship using
visualization. Information Processing and Manage-
ment, 30(1):141?50.
M. Koppel, J. Schler, and S. Argamon. 2009. Compu-
tational Methods in Authorship Attribution. Journal
of the American Society for Information Science and
Technology, 60(1):9?26.
E. Kwon. 2005. The Natural Order of Morpheme
Acquisition: A Historical Survey and Discussion
of Three Putative Determinants. Teachers? College
Columbia Working Papers in TESOL and Applied
Linguistics, 5(1):1?21.
H. Love. 2002. Authorship Attribution: An Introduc-
tion. Cambridge University Press.
K. Luyckx and W. Daelemans. 2011. The effect of
author set size and data size in authorship attribution.
Literary and Linguistic Computing, (26):35?55.
K. Luyckx. 2010. Scalability Issues in Authorship At-
tribution. Ph.D. thesis, University of Antwerp.
L. McCusker, P. Gough, and R. Bias. 1981. Word
recognition inside out and outside in. Journal of
Experimental Psychology: Human Perception and
Performance, 7(3):538?551.
D. Morrow. 1986. Grammatical morphemes and con-
ceptual structure in discourse processing. Cognitive
Science, 10(4):423?455.
F. Mosteller and D. Wallace. 1964. Inference and dis-
puted authorship: The Federalist. Addison-Wesley.
M. Newman, C. Groom, L. Handelman, and J. Pen-
nebaker. 2008. Gender Differences in Language
Use: An Analysis of 14,000 Text Samples. Dis-
course Processes, 45(3):211?236, May.
F. Peng, D. Schuurmans, V. Keselj, and S. Wang. 2003.
Language independent authorship attribution using
character level language models. In Proceedings of
the 10th Conference of the European Chapter of the
Association for Computational Linguistics, pages
267?274.
D. Rubin. 1995. Memory in Oral Traditions. The Cog-
nitive Psychology of Epic, Ballads and Counting-out
Rhymes. Oxford University Press.
J. Rybicki and M. Eder. 2011. Deeper Delta across
genres and languages: do we really need the most
frequent words? Literary and Linguistic Comput-
ing, pages 315?321.
E. Sapir. 1921. Language: An Introduction to the
Study of Speech. Harcourt, Brace & Co.
R. Schindler. 1978. The effect of prose context on
visual search for letters. Memory & Cognition,
(6):124?130.
E. Stamatatos. 2009. A survey of modern author-
ship attribution methods. Journal of the American
Society For Information Science and Technology,
(60):538?556.
H. Van Halteren, H. Baayen, F. Tweedie, M. Haverkort,
and A. Neijt. 2005. New Machine Learning Meth-
ods Demonstrate the Existence of a Human Stylome.
Journal of Quantitative Linguistics, (12):65?77.
R. Wollheim. 1972. On Art and the Mind: Essays and
Lectures. Harvard University Press.
G. Zipf. 1949. Human Behavior and the Principle of
Least Effort. Addison-Wesley.
66
