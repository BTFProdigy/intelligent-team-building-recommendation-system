Bootstrapping Distributional Feature
Vector Quality
Maayan Zhitomirsky-Geffet?
Bar-Ilan University
Ido Dagan??
Bar-Ilan University
This article presents a novel bootstrapping approach for improving the quality of feature vector
weighting in distributional word similarity. The method was motivated by attempts to utilize
distributional similarity for identifying the concrete semantic relationship of lexical entailment.
Our analysis revealed that a major reason for the rather loose semantic similarity obtained by
distributional similarity methods is insufficient quality of the word feature vectors, caused by
deficient feature weighting. This observation led to the definition of a bootstrapping scheme
which yields improved feature weights, and hence higher quality feature vectors. The under-
lying idea of our approach is that features which are common to similar words are also most
characteristic for their meanings, and thus should be promoted. This idea is realized via a
bootstrapping step applied to an initial standard approximation of the similarity space. The
superior performance of the bootstrapping method was assessed in two different experiments,
one based on direct human gold-standard annotation and the other based on an automatically
created disambiguation dataset. These results are further supported by applying a novel quanti-
tative measurement of the quality of feature weighting functions. Improved feature weighting
also allows massive feature reduction, which indicates that the most characteristic features
for a word are indeed concentrated at the top ranks of its vector. Finally, experiments with
three prominent similarity measures and two feature weighting functions showed that the
bootstrapping scheme is robust and is independent of the original functions over which it is
applied.
1. Introduction
1.1 Motivation
Distributional word similarity has long been an active research area (Hindle 1990; Ruge
1992; Grefenstette 1994; Lee 1997; Lin 1998; Dagan, Lee, and Pereira 1999; Weeds and
? Department of Information Science, Bar-Ilan University, Ramat-Gan, Israel.
E-mail: zhitomim@mail.biu.ac.il.
?? Department of Computer Science, Bar-Ilan University, Ramat-Gan, Israel. E-mail: dagan@cs.biu.ac.il.
Submission received: 6 December 2006; revised submission received: 9 July 2008; accepted for publication:
21 November 2008.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 3
Weir 2005). This paradigm is inspired by Harris?s distributional hypothesis (Harris
1968), which states that semantically similar words tend to appear in similar contexts.
In a computational realization, each word is characterized by a weighted feature vector,
where features typically correspond to other words that co-occur with the characterized
word in the context. Distributional similarity measures quantify the degree of similarity
between a pair of such feature vectors. It is then assumed that two words that occur
within similar contexts, as measured by similarity of their context vectors, are indeed
semantically similar.
The distributional word similarity measures were often applied for two types of
inferences. The first type is making similarity-based generalizations for smoothing
word co-occurrence probabilities, in applications such as language modeling and dis-
ambiguation. For example, assume that we need to estimate the likelihood of the verb?
object co-occurrence pair visit?country, although it did not appear in our sample corpus.
Co-occurrences of the verb visit with words that are distributionally similar to country,
such as state, city, and region, however, do appear in the corpus. Consequently, we
may infer that visit?country is also a plausible expression, using some mathematical
scheme of similarity-based generalization (Essen and Steinbiss 1992; Dagan, Marcus,
and Markovitch 1995; Karov and Edelman 1996; Ng and Lee 1996; Ng 1997; Dagan,
Lee, and Pereira 1999; Lee 1999; Weeds and Weir 2005). The rationale behind this
inference is that if two words are distributionally similar then the occurrence of one
word in some contexts indicates that the other word is also likely to occur in such
contexts.
A second type of semantic inference, which primarily motivated our own research,
is meaning-preserving lexical substitution. Many NLP applications, such as question
answering, information retrieval, information extraction, and (multi-document) sum-
marization, need to recognize that one word can be substituted by another one in a
given context while preserving, or entailing the original meaning. Naturally, recogniz-
ing such substitutable lexical entailments is a prominent component within the textual
entailment recognition paradigm, which models semantic inference as an application-
independent task (Dagan, Glickman, and Magnini 2006). Accordingly, several textual
entailment systems did utilize the output of distributional similarity measures to model
entailing lexical substitutions (Jijkoun and de Rijke 2005; Adams 2006; Ferrandez et al
2006; Nicholson, Stokes, and Baldwin 2006; Vanderwende, Menezes, and Snow 2006).
In some of these papers the distributional information typically complements man-
ual lexical resources in textual entailment systems, most notably WordNet (Fellbaum
1998).
Lexical substitution typically requires that the meaning of one word entails
the meaning of the other. For instance, in question answering, the word company
in a question can be substituted in an answer text by firm, automaker, or subsidiary,
whose meanings entail the meaning of company. However, as it turns out, traditional
distributional similarity measures do not capture well such lexical substitution
relationships, but rather capture a somewhat broader (and looser) notion of semantic
similarity. For example, quite distant co-hyponyms such as party and company
also come out as distributionally similar to country, due to a partial overlap of
their semantic properties. Clearly, the meanings of these words do not entail each
other.
Motivated by these observations, our long-term goal is to investigate whether the
distributional similarity scheme may be improved to yield tighter semantic similarities,
and eventually better approximation of lexical entailments. This article presents one
component of this research plan, which focuses on improving the underlying semantic
436
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
quality of distributional word feature vectors. The article describes the methodology,
definitions, and analysis of our investigation and the resulting bootstrapping scheme
for feature weighting which yielded improved empirical performance.
1.2 Main Contributions and Outline
As a starting point for our investigation, an operational definition was needed for
evaluating the correctness of candidate pairs of similar words. Following the lexical
substitution motivation, in Section 3 we formulate the substitutable lexical entailment
relation (or lexical entailment, for brevity), refining earlier definitions in Geffet and
Dagan (2004, 2005). Generally speaking, this relation holds for a pair of words if a
possible meaning of one word entails a meaning of the other, and the entailing word can
substitute the entailed one in some typical contexts. Lexical entailment overlaps partly
with traditional lexical semantic relationships, while capturing more generally the
lexical substitution needs of applications. Empirically, high inter-annotator agreement
was obtained when judging the output of distributional similarity measures for lexical
entailment.
Next, we analyzed the typical behavior of existingword similaritymeasures relative
to the lexical entailment criterion. Choosing the commonly used measure of Lin (1998)
as a representative case, the analysis shows that quite noisy feature vectors are a major
cause for generating rather ?loose? semantic similarities. On the other hand, one may
expect that features which seem to be most characteristic for a word?s meaning should
receive the highest feature weights. This does not seem to be the case, however, for
common feature weighting functions, such as Point-wise Mutual Information (Church
and Patrick 1990; Hindle 1990).
Following these observations, we developed a bootstrapping formula that improves
the original feature weights (Section 4), leading to better feature vectors and better
similarity predictions. The general idea is to promote the weights of features that are
common for semantically similar words, since these features are likely to be most char-
acteristic for the word?s meaning. This idea is implemented by a bootstrapping scheme,
where the initial (and cruder) similarity measure provides an initial approximation for
semantic word similarity. The bootstrapping method yields a high concentration of
semantically characteristic features among the top-ranked features of the vector, which
also allows aggressive feature reduction.
The bootstrapping scheme was evaluated in two experimental settings, which cor-
respond to the two types of applications for distributional similarity. First, it achieved
significant improvements in predicting lexical entailment as assessed by human judg-
ments, when applied over several base similarity measures (Section 5). Additional
analysis relative to the lexical entailment dataset revealed cleaner and more charac-
teristic feature vectors for the bootstrapping method. To obtain a quantitative analysis
of this behavior, we defined a measure called average common-feature rank ratio.
This measure captures the idea that a prominent feature for a word is expected to be
prominent also for semantically similar words, while being less prominent for unrelated
words. To the best of our knowledge this is the first proposedmeasure for direct analysis
of the quality of feature weighting functions, without the need to employ them within
some vector similarity measure.
As a second evaluation, we applied the bootstrapping scheme for similarity-based
prediction of co-occurrence likelihood within a typical pseudo-word sense disambigua-
tion experiment, obtaining substantial error reductions (Section 7). Section 8 concludes
437
Computational Linguistics Volume 35, Number 3
this article, suggesting the relevance of our analysis and bootstrapping scheme for the
general use of distributional feature vectors.1
2. Background: Distributional Similarity Models
This section reviews the components of the distributional similarity approach and
specifies the measures and functions that were utilized by our work.
The Distributional Hypothesis assumes that semantically similar words appear in
similar contexts, suggesting that semantic similarity can be detected by comparing
contexts of words. This is the underlying principle of the vector-based distributional
similarity model, which comprises two phases. First, context features for each word are
constructed and assigned weights; then, the weighted feature vectors of pairs of words
are compared by a vector similarity measure. The following two subsections review
typical methods for each phase.
2.1 Features and Weighting Functions
In the typical computational setting, word contexts are represented by feature vectors.
A feature represents another word (or term) w? with which w co-occurs, and possibly
specifies also the syntactic relationship between the twowords, as in Grefenstette (1994),
Lin (1998), and Weeds and Weir (2005). Thus, a word (or term) w is represented by
a feature vector, where each entry in the vector corresponds to a feature f . Pado and
Lapata (2007) demonstrate that using syntactic dependency-based features helps to
distinguish among classes of lexical relations, which seems to be more difficult when
using ?bag of words? features that are based on co-occurrence in a text window.
A syntactic-based feature f for a word w is defined as a triple:
? fw, syn rel, f role?
where fw is a context word (or term) that co-occurs with w under the syntactic depen-
dency relation syn rel. The feature role ( f role) corresponds to the role of the feature word
fw in the syntactic dependency, being either the head (denoted h) or the modifier (de-
noted m) of the relation. For example, given the word company, the feature ?earnings, gen,
h? corresponds to the genitive relationship company?s earnings, and ?investor, pcomp of, m?
corresponds to the prepositional complement relationship the company of the investor.2
Throughout this article we use syntactic dependency relationships generated by the
Minipar dependency parser (Lin 1993). Table 1 lists common Minipar dependency
relations involving nouns. Minipar also identifies multi-word expressions, which is
1 A preliminary version of the bootstrapping method was presented in Geffet and Dagan (2004). That
paper presented initial results for the bootstrapping scheme, when applied only over Lin?s measure and
tested by the manually judged dataset of lexical entailment. The current research extends our initial
results in many respects. It refines the definition of lexical entailment; utilizes a revised test set of larger
scope and higher quality, annotated by three assessors; extends the experiments to two additional
similarity measures; provides comparative qualitative and quantitative analysis of the bootstrapped
vectors, while employing our proposed average common-feature rank ratio; and presents an additional
evaluation based on a pseudo-WSD task.
2 Following a common practice, we consider the relationship between a head noun (company in the
example) and the nominal complement of a modifying prepositional phrase (investor) as a single direct
dependency relationship. The preposition itself is encoded in the dependency relation name, with a
distinct relation for each preposition.
438
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
Table 1
Common grammatical relations of Minipar involving nouns.
Relation Description
appo apposition
comp1 first complement
det determiner
gen genitive marker
mod the relationship between a word and its adjunct modifier
pnmod post nominal modifier
pcomp nominal complement of prepositions
post post determiner
vrel passive verb modifier of nouns
obj object of verbs
obj2 second object of ditransitive verbs
subj subject of verbs
s surface subject
advantageous for detecting distributional similarity for such terms. For example,
Curran (2004) reports that multi-word expressions make up between 14?25% of the
synonyms in a gold-standard thesaurus.
Thus, in our representation the corpus is first transformed to a set S of dependency
relationship instances of the form ?w,f ?, where each pair corresponds to a single co-
occurrence of w and f in the corpus. f is termed as a feature of w. Then, a word
w is represented by a feature vector, where each entry in the vector corresponds to
one feature f . The value of the entry is determined by a feature weighting function
weight(w, f ), which quantifies the degree of statistical association between w and f in the
set S. For example, some feature weighting functions are based on the logarithm of the
word?feature co-occurrence frequency (Ruge 1992), or on the conditional probability of
the feature given the word (Pereira, Tishby, and Lee 1993; Dagan, Lee, and Pereira 1999;
Lee 1999).
Probably the most widely used feature weighting function is (point-wise) Mutual
Information (MI) (Church and Patrick 1990; Hindle 1990; Luk 1995; Lin 1998; Gauch,
Wang, and Rachakonda 1999; Dagan 2000; Baroni and Vegnaduzzo 2004; Chklovski
and Pantel 2004; Pantel and Ravichandran 2004; Pantel, Ravichandran, and Hovy 2004;
Weeds, Weir, and McCarthy 2004), defined by:
weightMI(w, f ) = log2
P(w,f )
P(w)P( f ) (1)
We calculate the MI weights by the following statistics in the space of co-occurrence
instances S:
weightMI(w, f ) = log2
count(w, f ) ? nrels
count(w) ? count( f )
(2)
where count(w, f ) is the frequency of the co-occurrence pair ?w,f ? in S, count(w) and
count( f ) are the independent frequencies of w and f in S, and nrels is the size of S. High
MI weights are assumed to correspond to strong word?feature associations.
439
Computational Linguistics Volume 35, Number 3
Curran and Moens (2002) argue that, generally, informative features are statis-
tically correlated with their corresponding headword. Thus, they suggest that any
statistical test used for collocations is a good starting point for improving feature-
weight functions. In their experiments the t-test-based metric yielded the best empirical
performance.
However, a known weakness of MI and most of the other statistical weighting
functions used for collocation extraction, including t-test and ?2, is their tendency to
inflate the weights for rare features (Dunning 1993). In addition, a major property of
lexical collocations is their ?non-substitutability?, as termed in Manning and Schutze
(1999). That is, typically neither a headword nor a modifier in the collocation can be
substituted by their synonyms or other related terms. This implies that using modifiers
within strong collocations as features for a head word would provide a rather small
amount of common features for semantically similar words. Hence, these functions
seem less suitable for learning broader substitutability relationships, such as lexical
entailment.
Similarity measures that utilize MI weights showed good performance, however.
In particular, a common practice is to filter out features by minimal frequency and
weight thresholds. Then, a word?s vector is constructed from the remaining (not filtered)
features that are strongly associated with the word. These features are denoted here as
active features.
In the current work we use MI for data analysis, and for the evaluations of vector
quality and word similarity performance.
2.2 Vector Similarity Measures
Once feature vectors have been constructed the similarity between two words is de-
fined by some vector similarity measure. Similarity measures which have been used
in the cited papers include weighted Jaccard (Grefenstette 1994), Cosine (Ruge 1992),
and various information theoretic measures, as introduced and reviewed in Lee (1997,
1999). In the current work we experiment with the following three popular similarity
measures.
1. The basic Jaccard measure compares the number of common features with
the overall number of features for a pair of words. One of the weighted
generalizations of this scheme to non-binary values replaces intersection
with minimum weight, union with maximum weight, and set cardinality
with summation. This measure is commonly referred to as weighted Jaccard
(WJ) (Grefenstette 1994; Dagan, Marcus, and Markovitch 1995; Dagan
2000; Gasperin and Vieira 2004), defined as follows:
simWJ(w, v) =
?
f?F(w)?F(v) min(weight(w,f ),weight(v,f ))
?
f?F(w)?F(v) max(weight(w,f ),weight(v,f ))
(3)
where F(w) and F(v) are the sets of active features of the two words w
and v. The appealing property of this measure is that it considers the
association weights rather than just the number of common features.
2. The standard Cosine measure (COS), which is popularly employed for
information retrieval (Salton and McGill 1983) and also utilized for
440
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
learning distributionally similar words (Ruge 1992; Caraballo 1999; Gauch,
Wang, and Rachakonda 1999; Pantel and Ravichandran 2004), is defined as
follows:
simCOS(w, v) =
?
f (weight(w,f )?weight(v,f ))?
?
f (weight(w,f ))
2?
?
?
f (weight(v,f ))
2 (4)
This measure computes the cosine of the angle between the two feature
vectors, which normalizes the vector lengths and thus avoids inflated
discrimination between vectors of significantly different lengths.
3. A popular state of the art measure has been developed by Lin (1998),
motivated by Information Theory principles. This measure behaves quite
similarly to the weighted Jaccard measure (Weeds, Weir, and McCarthy
2004), and is defined as follows:
simLIN(w, v) =
?
f?F(w)?F(v) (weightMI (w,f )+weightMI (v,f ))
?
f?F(w) weightMI (w,f )+
?
f?F(v) weightMI (v,f )
(5)
where F(w) and F(v) are the active features of the two words. The weight
function used originally by Lin is MI (Equation 1).
It is interesting to note that a relatively recent work by Weeds and Weir (2005) inves-
tigates a more generic similarity framework. Within their framework, the similarity of
two nouns is viewed as the ability to predict the distribution of one of them based on
that of the other. Their proposed formula combines the precision and recall of a potential
?retrieval? of similar words based on the features of the target word. The precision of
w?s prediction of v?s feature distribution indicates howmany of the features of the word
w co-occurred with the word v. The recall of w?s prediction of v?s features indicates
how many of the features of v co-occurred with w. Words with both high precision
and high recall can be obtained by computing their harmonic mean, mh (or F-score),
and a weighted arithmetic mean. However, after empirical tuning of weights for the
arithmetic mean, Weeds and Weir?s formula practically reduces to Lin?s measure, as
was anticipated by their own analysis (in Section 4 of their paper).
Consequently, we choose the Lin measure (Equation 5) (henceforth denoted as LIN)
as representative for the state of the art and utilize it for data analysis and as a starting
point for improvement. To further explore and evaluate our new weighting scheme,
independently of a single similarity measure, we conduct evaluations also with the
other two similarity measures of weighted Jaccard and Cosine.
3. Substitutable Lexical Entailment
As mentioned in the Introduction, the long term research goal which inspired our work
is modeling meaning?entailing lexical substitution. Motivated by this goal, we
proposed in earlier work (Geffet and Dagan 2004, 2005) a new type of lexical
relationship which aims to capture such lexical substitution needs. Here we adopt that
approach and formulate a refined definition for this relationship, termed substitutable
lexical entailment. In the context of the current article, utilizing a concrete target notion
of word similarity enabled us to apply direct human judgment for the ?correctness?
(relative to the defined notion) of candidate word pairs suggested by distributional
similarity. Utilizing these judgments we could analyze the behavior of alternative
441
Computational Linguistics Volume 35, Number 3
distributional vector representations and, in particular, conduct error analysis for word
pair candidates that were judged negatively.
The discussion in the Introduction suggested that multiple text understanding
applications need to identify term pairs whose meanings are both entailing and sub-
stitutable. Such pairs seem to be most appropriate for lexical substitution in a meaning
preserving scenario. Tomodel this goal we present an operational definition for a lexical
semantic relationship that integrates the two aspects of entailment and substitutability,3
which is termed substitutable lexical entailment (or lexical entailment, for brevity).
This relationship holds for a given directional pair of terms (w, v), saying that w entails
v, if the following two conditions are fulfilled:
1. Word meaning entailment: the meaning of a possible sense of w implies a
possible sense of v;
2. Substitutability: w can substitute for v in some naturally occurring sentence,
such that the meaning of the modified sentence would entail the meaning
of the original one.
To operationally assess the first condition (by annotators) we propose considering
the meaning of terms by existential statements of the form ?there exists an instance of
the meaning of the term w in some context? (notice that, unlike propositions, it is not
intuitive for annotators to assign truth values to terms). For example, the word company
would correspond to the existential statement ?there exists an instance of the concept
company in some context.? Thus, if in some context ?there is a company? (in the sense
of ?commercial organization?) then necessarily ?there is a firm? in that context (in the
corresponding sense). Therefore, we conclude that the meaning of company implies the
meaning of firm. On the other hand, ?there is an organization? does not necessarily imply
the existence of company, since organization might stand for some non-profit association,
as well. Therefore, we conclude that organization does not entail company.
To assess the second condition, the annotators need to identify some natural con-
text in which the lexical substitution would satisfy entailment between the modified
sentence and the original one. Practically, in our experiments presented in Section 5 the
human assessors could consult external lexical resources and the entireWeb to obtain all
the senses of the words and possible sentences for substitution. We note that the task of
identifying the common sense of two given words is quite easy since they mutually dis-
ambiguate each other, and once the common sense is known it naturally helps finding a
corresponding common context. We note that this condition is important, in particular,
in order to eliminate cases of anaphora and co-reference in contexts, where two words
quite different in their meaning can sometimes appear in the same contexts only due
to the text pragmatics in a particular situation. For example, in some situations worker
and demonstrator could be used interchangeably in text, but clearly it is a discourse co-
reference rather than common meaning that makes the substitution possible. Instead,
we are interested in identifying word pairs in which one word?s meaning provides
a reference to the entailed word?s meaning. This purpose is exactly captured by the
existential propositions of the first criterion above.
3 The WordNet definition of the lexical entailment relation is specified only for verbs and, therefore, is not
felicitous for general purposes: A verb X entails Y if X cannot be done unless Y is, or has been, done (e.g.,
snore and sleep).
442
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
As reported further in Section 5.1, we observed that assessing these two conditions
for candidate word similarity pairs was quite intuitive for annotators, and yielded good
cross-annotator agreement. Overall, substitutable lexical entailment captures directly
the typical lexical substitution scenario in text understanding applications, as well as in
generic textual entailment modeling. In fact, this relation partially overlaps with several
traditional lexical semantic relations that are known as relevant for lexical substitution,
such as synonymy, hyponymy, and some cases of meronymy. For example, we say
that the meaning of company is lexically entailed by the meaning of firm (synonym)
or automaker (hyponym), while the word government entails minister (meronym) as The
government voted for the new law entails A minister in the government voted for the new law.
On the other hand, lexical entailment is not just a superset of other known relations,
but it is rather designed to select those sub-cases of other lexical relations that are needed
for applied entailment inference. For example, lexical entailment does not cover all cases
of meronyms (e.g., division does not entail company), but only some sub-cases of part-
whole relationship mentioned herein. In addition, some other relations are also covered
by lexical entailment, like ocean and water and murder and death, which do not seem to
directly correspond to meronymy or hyponymy relations.
Notice also that whereas lexical entailment is a directional relation that specifies
which word of the pair entails the other, the relation may hold in both directions
for a pair of words, as is the case for synonyms. More detailed motivations for the
substitutable lexical entailment relation and analysis of its relationship to traditional
lexical semantic relations appear in Geffet (2006) and Geffet and Dagan (2004, 2005).
4. Bootstrapping Feature Weights
To gain a better understanding of distributional similarity behavior we first analyzed
the output of the LIN measure, as a representative case for the state of the art, and
regarding lexical entailment as a reference evaluation criterion. We judge as correct,
with respect to lexical entailment, those candidate pairs of the distributional similarity
method for which entailment holds at least in one direction.
For example, the word area is entailed by country, since the existence of country
entails the existence of area, and the sentence There is no rain in subtropical countries during
the summer period entails the sentence There is no rain in subtropical areas during the summer
period. As another example, democracy is a type of country in the political sense, thus the
existence entailment holds and also the sentence Israel is a democracy in the Middle East
entails Israel is a country in the Middle East.
On the other hand, our analysis revealed that many candidate word similarity pairs
suggested by distributional similarity measures do not correspond to ?tight? semantic
relationships. In particular, many word pairs suggested by the LIN measure do not
satisfy the lexical entailment relation, as demonstrated in Table 2.
A deeper look at the corresponding word feature vectors reveals typical reasons
for these lexical entailment prediction errors. Most relevant for the scope of the cur-
rent article, in many cases highly ranked features in a word vector (when sorting the
features by their weight) do not seem very characteristic for the word meaning. This
is demonstrated in Table 3, which shows the top 10 features in the vector for country.
As can be seen, some of the top features are either too specific (landlocked, airspace),
and are thus less reliable, or too general (destination, ambition), thus not indicative and
may co-occur with many different types of words. On the other hand, intuitively more
characteristic features of country, like population and governor, occur further down the
443
Computational Linguistics Volume 35, Number 3
Table 2
The top 20 most similar words for country (and their ranks) in the similarity list of LIN, followed
by the next four words in the similarity list that were judged as entailing at least in one direction.
nation 1 *city 7 economy 13 *company 19
region 2 territory 8 *neighbor 14 *industry 20
state 3 area 9 *sector 15 kingdom 30
*world 4 *town 10 *member 16 place 35
*island 5 republic 11 *party 17 colony 41
*province 6 *north 12 government 18 democracy 82
Twelve out of 20 top similarities (60%) were judged as mutually non-entailing and are marked
with an asterisk. The similarity data was produced as described in Section 5.
Table 3
The top 10 ranked features for country produced by MI, the weighting function employed in the
LIN method.
Feature weightMI
Commercial bank, gen, h 8.08
Destination, pcomp of, m 7.97
Airspace, pcomp of, h 7.83
Landlocked, mod, m 7.79
Trade balance, gen, h 7.78
Sovereignty, pcomp of, h 7.78
Ambition, nn, h 7.77
Bourse, gen, h 7.72
Politician, gen, h 7.54
Border, pcomp of, h 7.53
sorted feature list, at positions 461 and 832. Overall, features that seem to characterize
the word meaning well are scattered across the ranked feature list, while many non-
indicative features receive high weights. This behavior often yields high similarity
scores for word pairs whose semantic similarity is rather loose while missing some
much tighter similarities.
Furthermore, we observed that characteristic features for a word w, which should
receive higher weights, are expected to be common for w and other words that are
semantically similar to it. This observation suggests a computational scheme which
would promote the weights of features that are common for semantically similar words.
Of course, there is an inherent circularity in such a scheme: to determine which features
should receive high weights we need to know which words are semantically similar,
while computing distributional semantic similarity already requires pre-determined
feature weights.
This kind of circularity can be approached by a bootstrapping scheme. We first
compute initial distributional similarity values, based on an initial feature weighting
function. Then, to learn more accurate feature weights for a word w, we promote
features that characterize other words that are initially known to be similar to w. By
the same rationale, features that do not characterize many words that are sufficiently
similar to w are demoted. Even if such features happen to have a strong direct statis-
tical association with w they would not be considered reliable, because they are not
supported by additional words that have a similar meaning to that of w.
444
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
4.1 Bootstrapped Feature Weight Definition
The bootstrapped feature weight is defined as follows. First, some standard word
similarity measure sim is computed to obtain an initial approximation of the similarity
space. Then, we define the word set of a feature f , denoted by WS( f ), as the set of words
for which f is an active feature. Recall from Section 2.2 that an active feature is a feature
that is strongly associated with the word, that is, its (initial) weight is higher than an
empirically predefined threshold, ?weight. The semantic neighborhood of w, denoted by
N(w), is defined as the set of all words v which are considered sufficiently similar to
w, satisfying sim(w, v) > ?sim, where ?sim is a second empirically determined threshold.
The bootstrapped feature weight, denoted weightB, is then defined by:
weightB(w, f ) =
?
v?WS( f )?N(w) sim(w, v) (6)
That is, we identify all words v that are in the semantic neighborhood of w and are also
characterized by f , and then sum the values of their similarities to w.
Intuitively, summing these similarity values captures simultaneously a desired
balance between feature specificity and generality, addressing the observations in the
beginning of this section. Some features might characterize just a single word that is
very similar to w, but then the sum of similarities will include a single element, yielding
a relatively low weight. This is why the sum of similarities is used rather than an
average value, which might become too high by chance when computed over just a
single element (or very few elements). Relatively generic features, which occur with
many words and are thus less indicative, may characterize more words within N(w)
but then on average the similarity values of these words with w is likely to be lower,
contributing smaller values to the sum. To receive a high overall weight a reliable feature
has to characterize multiple words that are highly similar to w.
We note that the bootstrapped weight is a sum of word similarity values rather
than a direct function of word?feature association values, which is the more common
approach. It thus does not depend on the exact statistical co-occurrence level between
w and f . Instead, it depends on a more global assessment of the association between
f and the semantic vicinity of w. We notice that the bootstrapped weight is deter-
mined separately relative to each individual word. This differs from measures that are
global word-independent functions of the feature, such as the feature entropy used in
Grefenstette (1994) and the feature term strength relative to a predefined class as em-
ployed in Pekar, Krkoska, and Staab (2004) for supervised word classification.
4.2 Feature Reduction and Similarity Re-Computation
Once the bootstrappedweights have been computed, their accuracy is sufficient to allow
for aggressive feature reduction. As shown in the following section, in our experiments
it sufficed to use only the top 100 features for each word in order to obtain optimal
word similarity results, because the most informative features now receive the highest
weights.
Finally, similarity betweenwords is re-computed over the reduced vectors using the
sim function with weightB replacing the original feature weights. The resulting similarity
measure is further referred to as simB.
445
Computational Linguistics Volume 35, Number 3
5. Evaluation by Lexical Entailment
To test the effectiveness of the bootstrapped weighting scheme, we first evaluated
whether it contributes to better prediction of lexical entailment. This evaluation was
based on gold-standard annotations determined by human judgments of the substi-
tutable lexical entailment relation, as defined in Section 3. The new similarity scheme,
simB, based on the bootstrapped weights, was first computed using the standard LIN
method as the initial similarity measure. The resulting similarity lists of simLIN (the
original LIN method) and simBLIN (Bootstrapped LIN) schemes were evaluated for a sam-
ple of nouns (Section 5.2). Then, the evaluation was extended (Section 5.3) to apply the
bootstrapping scheme over the two additional similarity measures that were presented
in Section 2.2, simWJ (weighted Jaccard) and simCOS (Cosine). Along with these lexical
entailment evaluations we also analyzed directly the quality of the bootstrapped fea-
ture vectors, according to the average common-feature rank ratio measure, which was
defined in Section 6.
5.1 Experimental Setting
Our experiments were conducted using statistics from an 18 million token subset of the
Reuters RCV1 corpus (known as Reuters Corpus, Volume 1, English Language, 1996-
08-20 to 1997-08-19), parsed by Lin?s Minipar dependency parser (Lin 1993).
The test set of candidate word similarity pairs was constructed for a sample of
30 randomly selected nouns whose corpus frequency exceeds 500. In our primary exper-
iment we computed the top 40 most similar words for each noun by the simLIN and by
simBLIN measures, yielding 1,200 pairs for each method, and 2,400 pairs altogether. About
800 of these pairs were common for the two methods, therefore leaving approximately
1,600 distinct candidate word similarity pairs. Because the lexical entailment relation is
directional, each candidate pair was duplicated to create two directional pairs, yielding
a test set of 3,200 pairs. Thus, for each pair of words,w and v, the two ordered pairs (w, v)
and (v,w) were created to be judged separately for entailment in the specified direction
(whether the first word entails the other). Consequently, a non-directional candidate
similarity pair w, v is considered as a correct entailment if it was assessed as an entailing
pair at least in one direction.
The assessors were only provided with a list of word pairs without any contextual
information and could consult any available dictionary, WordNet, and the Web.
The judgment criterion follows the criterion presented in Section 3. In particular,
the judges were asked to apply the two operational conditions, existence and sub-
stitutability in context, to each given pair. Prior to performing the final test of the
annotation experiment, the judges were presented with an annotated set of entailing
and non-entailing pairs along with the existential statements and sample sentences for
substitution, demonstrating how the two conditions could be applied in different cases
of entailment. In addition, they had to judge a training set of several dozen pairs and
then discuss their judgment decisions with each other to gain a better understanding of
the two criteria.
The following example illustrates this process. Given a non-directional pair
{company, organization} two directional pairs are created: (company, organization) and
(organization, company). The former pair is judged as a correct entailment: the existence
of a company entails the existence of an organization, and the meaning of the sentence:
John works for a large company entails the meaning of the sentence with substitution:
John works for a large organization. Hence, company lexically entails organization, but not
446
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
vice versa (as shown in Section 3.3), therefore the second pair is judged as not entailing.
Eventually, the non-directional pair {company, organization} is considered as a correct
entailment.
Finally, the test set of 3,200 pairs was split into three disjoint subsets that were
judged by three native English speaking assessors, each of whom possessed a Bach-
elors degree in English Linguistics. For each subset a different pair of assessors was
assigned, each person judging the entire subset. The judges were grouped into three
different pairs (i.e., JudgeI+JudgeII, JudgeII+JudgeIII, and JudgeI+JudgeIII). Each pair
was assigned initially to judge all the word similarities in each subset, and the third
assessor was employed in cases of disagreement between the first two. The majority
vote was taken as the final decision. Hence, each assessor had to fully annotate two
thirds of the data and for a third subset she only had to judge the pairs for which there
was disagreement between the other two judges. This was done in order to measure the
agreement achieved for different pairs of annotators.
The output pairs from bothmethodsweremixed so the assessors could not associate
a pair with the method that proposed it. We note that this evaluation methodology,
in which human assessors judge the correctness of candidate pairs by some semantic
substitutability criterion, is similar to common evaluation methodologies used for para-
phrase acquisition (Barzilay and McKeown 2001; Lin and Pantel 2001; Szpektor et al
2004).
Measuring human agreement level for this task, the proportions of matching de-
cisions were 93.5% between Judge I and Judge II, 90% for Judge I and Judge III, and
91.2% for Judge II and Judge III. The corresponding kappa values are 0.83, 0.80, and 0.80,
which is regarded as ?very good agreement? (Landis and Koch 1997). It is interesting
to note that after some discussion most of the disagreements were settled, and the few
remaining mismatches were due to different understandings of word meanings. These
findings seem to have a similar flavor to the human agreement findings reported for the
Recognizing Textual Entailment challenges (Bar-Haim et al 2006; Dagan, Glickman, and
Magnini 2006), in which entailment was judged for pairs of sentences. In fact, the kappa
values obtained in our evaluation are substantially higher than reported for sentence-
level textual entailment, which suggests that it is easier to make entailment judgments
at the lexical level than at the full sentence level.
The parameter values of the algorithms were tuned using a development set of
similarity pairs generated for 10 additional nouns, distinct from the 30 nouns used for
the test set. The parameters were optimized by running the algorithm systematically
with various values across the parameter scales and judging a sample subset of the
results. weightMI = 4 was found as the optimal MI threshold for active feature weights
(features included in the feature vectors), yielding a 10% precision increase of simLIN and
removing over 50% of the data relative to no feature filtering. Accordingly, this value
also serves as the ?weight threshold in the bootstrapping scheme (Section 4). As for the
?sim parameter, the best results on the development set were obtained for ?sim = 0.04,
?sim = 0.02, and ?sim = 0.01 when bootstrapping over the initial similarity measures
LIN, WJ, and COS, respectively.
5.2 Evaluation Results for simBLIN
We measured the contribution of the improved feature vectors to the resulting preci-
sion of simLIN and sim
B
LIN in predicting lexical entailment. The results are presented in
Table 4, where precision and error reduction values were computed for the top 20,
30, and 40 word similarity pairs produced by each method. It can be seen that the
447
Computational Linguistics Volume 35, Number 3
Table 4
Lexical entailment precision values for top-n similar words by the Bootstrapped LIN and the
original LIN method.
Top-n Correct Error Rate
Words Entailments (%) Reduction (%)
simLIN sim
B
LIN
Top 20 52.0 57.9 12.3
Top 30 48.2 56.2 15.4
Top 40 41.0 49.7 14.7
Bootstrapped LIN method outperformed the original LIN approach by 6?9 precision
points at all top-n levels. As expected, the precision for the shorter top 20 list is higher
for both methods, thus leaving a bit less room for improvement.
Overall, the Bootstrapped LIN method extracted 104 (21%) more correct similarity
pairs than the other measure and reduced the number of errors by almost 15%. We also
computed the relative recall, which shows the percentage of correct word similarities
found by each method relative to the joint set of similarities that were extracted by
both methods. The overall relative recall of the Bootstrapped LIN was quite high (94%),
exceeding LIN?s relative recall (of 78%) by 16 percentage points. We found that the
bootstrapped method covers over 90% of the correct similarities learned by the original
method, while also identifying many additional correct pairs.
It should be noted at this point that the current limited precision levels are deter-
mined not just by the quality of the feature vectors but significantly by the nature of the
vector comparison measure itself (i.e., the LIN formula, as well weighted Jaccard and
Cosine as reported in Section 5.3). It was observed in other work (Geffet and Dagan
2005) that these common types of vector comparison schemes exhibit certain flaws
in predicting lexical entailment. Our present work thus shows that the bootstrapping
method yields a significant improvement in feature vector quality, but future research
is needed to investigate improved vector comparison schemes.
An additional indication of the improved vector quality is the massive feature
reduction allowed by having the most characteristic features concentrated at the top
ranks of the vectors. The vectors of active features of LIN, as constructed after standard
feature filtering (Section 5.1), could be further reduced by the bootstrapped weighting
to about one third of their size. As illustrated in Figure 1, changing the vector size
significantly affects the similarity results. In simBLIN the best result was obtained with
the top 100 features per word, while using less than 100 or more than 150 features
caused a 5?10% decrease in performance. On the other hand, an attempt to cut off the
lower ranked features of the MI weighting always resulted in a noticeable decrease in
precision. These results show that for MI weighting many important features appear
further down in the ranked vectors, while for the bootstrapped weighting adding too
many features adds mostly noise, since most characteristic features are concentrated
at the top ranks. Thus, in addition to better feature weighting, the bootstrapping step
provides effective feature reduction, which improves vector quality and consequently
the similarity results.
We note that the optimal vector size we obtained conforms to previous results?
for example, by Widdows (2003), Curran (2004), and Curran and Moens (2002)?who
also used reduced vectors of up to 100 features as optimal for learning hyponymy and
synonymy, respectively. In Widdows the known SVD method for dimension reduction
448
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
Figure 1
Percentage of correct entailments within the top 40 candidate pairs of each of the methods,
LIN and Bootstrapped LIN (denoted as LINB in the figure), when using varying numbers of
top-ranked features in the feature vector. The value of ?All? corresponds to the full size of
vectors and is typically in the range of 300?400 features.
of LSA-based vectors is applied, whereas in Curran, and Curran and Moens, only
the strongly associated verbs (direct and indirect objects of the noun) are selected as
?canonical features? that are expected to be shared by true synonyms.
Finally, we tried executing an additional bootstrapping iteration of weightB calcula-
tion over the similarity results of simBLIN. The resulting increase in precision was much
smaller, of about 2%, showing that most of the potential benefit is exploited in the
first bootstrapping iteration (which is not uncommon for natural language data). On
the other hand, computing the bootstrapping weight twice increases computation time
significantly, which led us to suggest a single bootstrapping iteration as a reasonable
cost-effectiveness tradeoff for our data.
5.3 Evaluation for simBWJ and sim
B
COS
To further validate the behavior of the bootstrapping schemewe experimentedwith two
additional similaritymeasures, weighted Jaccard (simWJ) and Cosine (simCOS) (described
in Section 2.2). For each of the additional measures the experiment repeats the main
three steps described in Section 4: Initially, the basic similarity lists are calculated for
each of the measures using MI weighting; then, the bootstrapped weighting, weightB, is
computed based on the initial similarities, yielding new word feature vectors; finally,
the similarity values are recomputed by the same vector similarity measure using the
new feature vectors.
To assess the effectiveness of weightB we computed the four alternative output
similarity lists, using the simWJ and simCOS similarity measures, each with the weightMI
449
Computational Linguistics Volume 35, Number 3
Table 5
Comparative precision values for the top 20 similarity lists of the three selected similarity
measures, with MI and Bootstrapped feature weighting for each.
Measure LIN?LINB WJ?WJB COS?COSB
Correct Similarities (%) 52.0?57.9 51.0?54.8 46.1?50.9
and weightB weighting functions. The four lists were judged for lexical entailment by
three assessors, according to the same procedure described in Section 5.1. To make the
additional manual evaluation affordable we judged the top 20 similar words in each list
for each of the 30 target nouns of Section 5.1.
Table 5 summarizes the precision values achieved by LIN, WJ, and COS with
both weightMI and weight
B. As shown in the table, bootstrapped weighting consistently
contributed between 4?6 points to the accuracy of each method in the top 20 similarity
list. We view the results as quite positive, considering that improving over top 20
similarities is a much more challenging task than improving over longer similarity lists,
while the improvement was achieved only by modifying the feature vectors without
changing the similarity measure itself (as hinted in Section 5.2). Our results are also
compatible with previous findings in the literature (Dagan, Lee, and Pereira 1999;
Weeds, Weir, and McCarthy 2004) that found LIN and WJ to be more accurate for
similarity acquisition than COS. Overall, the results demonstrate that the bootstrapped
weighting scheme consistently produces improved results.
An interesting behavior of the bootstrapping process is that the most prominent
features for a given target word converge across the different initial similarity measures,
as exemplified in Table 6. In particular, although the initial similarity lists overlap only
partly,4 the overlap of the top 30 features for our 30-word sample was ranging between
88% and 100%. This provides additional evidence that the quality of the bootstrapped
weighting is quite similar for various initial similarity measures.
6. Analyzing the Bootstrapped Feature Vector Quality
In this section we provide an in-depth analysis of the bootstrapping feature weighting
quality compared to the state-of-the-art MI weighting function.
6.1 Qualitative Observations
The problematic feature ranking noticed at the beginning of Section 4 can be revealed
more objectively by examining the common features which contribute most to the word
similarity scores. To that end, we examine the common features of the two given words
and sort them by the sum of their weights in both word vectors. Table 7 shows the top
10 common features by this sorting for a pair of truly similar (lexically entailing) words
(country?state), and for a pair of non-entailing words (country?party). For each common
feature the table shows its two corresponding ranks in the feature vectors of the two
words.
4 Overlap rate was about 40% between COS and WJ or LIN, and 70% between WJ and LIN. The overlap
was computed following the procedure of Weeds, Weir, and McCarthy (2004), disregarding the order of
the similar words in the lists. Interestingly, they obtained roughly similar figures, of 28% overlap for COS
and WJ, 32% overlap for COS and LIN, and 81% overlap between LIN and WJ.
450
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
Table 6
Top 30 features of town by bootstrapped weighting based on LIN, WJ, and COS as initial
similarities. The three sets of words are almost identical, with relatively minor ranking
differences.
LINB WJB COSB
southern southern northern
northern northern southern
office office remote
eastern official eastern
remote coastal official
official eastern based
troop northeastern northeastern
northeastern remote office
people troop coastal
coastal people northwestern
attack based people
based populated attack
populated attack troop
northwestern home home
base northwestern south
home south western
south western city
west west populated
western resident base
neighboring neighboring resident
resident house north
plant city west
police base neighboring
held trip trip
locate camp surrounding
trip held police
city north held
site locate locate
camp surrounding house
surrounding police camp
It can be observed in Table 7 that for both word pairs the common features are
scattered across the pair of feature vectors, making it difficult to distinguish between
the truly similar and the non-similar pairs. We suggest, on the other hand, that the
desired behavior of effective feature weighting is that the common features of truly
similar words would be concentrated at the top ranks of both word vectors. In other
words, if the two words are semantically similar then we expect them to share their
most characteristic features, which are in turn expected to appear at the higher ranks
of each feature vector. The common features for non-similar words are expected to be
scattered all across each of the vectors. In fact, these expectations correspond exactly to
the rationale behind distributional similarity measures: Such measures are designed to
assign higher similarity scores for vector pairs that share highly weighted features.
Comparatively, we illustrate the behavior of the Bootstrapped LIN method relative to
the observations regarding the original LIN method, using the same running example.
Table 8 shows the top 10 features of country. We observe that the list now contains
features that are intuitively quite indicative and reliable, while many too specific or
idiomatic features, and too general ones, were demoted (compare with Table 3). Table 9
shows that most of the top 10 common features for country?state are now ranked highly
451
Computational Linguistics Volume 35, Number 3
for both words. On the other hand, there are only two common features (among the
top 100 features) for the incorrect pair country?party, bothwith quite low ranks (compare
with Table 7), while the rest of the common features for this pair did not pass the top 100
cutoff.
Consequently, Table 10 demonstrates a much more accurate similarity list for coun-
try, wheremany incorrect (non-entailing) word similarities, like party and company, were
demoted. Instead, additional correct similarities, like kingdom and land, were promoted
(compare with Table 2). In this particular case all the remaining errors correspond to
words that are related quite closely to country, denoting geographic concepts. Many of
these errors are context dependent entailments which might be substitutable in some
cases, but they violate the word meaning entailment condition (e.g., country?neighbor,
country?port). Apparently, these words tend to occur in contexts that are typical for
country in the Reuters corpus. Some errors violating the substitutability condition of
lexical entailment were identified as well, such as industry?product. These cases are
quite hard to differentiate from correct entailments, since the two words are usually
closely related to each other and also share highly ranked features, because they often
appear in similar characteristic contexts. It may therefore be difficult to filter out such
Table 7
LIN (MI) weighting: The top 10 common features for country?state and country?party, along with
their corresponding ranks in each of the two feature vectors. The features are sorted by the sum
of their feature weights with both words.
Country?State Ranks Country?Party Ranks
Broadcast, pcomp in, h 24 50 Brass, nn, h 64 22
Goods, mod, h 140 16 Concluding, pcomp of, h 73 20
Civil servant, gen, h 64 54 Representation, pcomp of, h 82 27
Bloc, gen, h 30 77 Patriarch, pcomp of, h 128 28
Nonaligned, mod, m 55 60 Friendly, mod, m 58 83
Neighboring, mod, m 15 165 Expel, pcomp from, h 59 30
Statistic, pcomp on, h 165 43 Heartland, pcomp of, h 102 23
Border, pcomp of, h 10 247 Surprising, pcomp of, h 114 38
Northwest, mod, h 41 174 Issue, pcomp between, h 103 51
Trip, pcomp to, h 105 34 Contravention, pcomp in, m 129 43
Table 8
Top 10 features of country by the Bootstrapped feature weighting.
Feature WeightB
Industry, gen, h 1.21
Airport, gen, h 1.16
Visit, pcomp to, h 1.06
Neighboring, mod, m 1.04
Law, gen, h 1.02
Economy, gen, h 1.02
Population, gen, h 0.93
Stock market, gen, h 0.92
Governor, pcomp of, h 0.92
Parliament, gen, h 0.91
452
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
Table 9
Bootstrapped weighting: top 10 common features for country?state and country?party along with
their corresponding ranks in the two (sorted) feature vectors.
Country?State Ranks Country?Party Ranks
Neighboring, mod, m 3 1 Relation, pcomp with, h 12 26
Industry, gen, h 1 11 Minister, pcomp from, h 77 49
Impoverished, mod, m 8 8
Governor, pcomp of, h 10 9
Population, gen, h 6 16
City, gen, h 17 18
Economy, gen, h 5 15
Parliament, gen, h 10 22
Citizen, pcomp of, h 14 25
Law, gen, h 4 33
Table 10
Top 20 most similar words for country and their ranks in the similarity list by the Bootstrapped
LIN measure.
nation 1 territory 6 *province 11 zone 16
state 2 *neighbor 7 *city 12 land 17
*island 3 colony 8 *town 13 place 18
region 4 *port 9 kingdom 14 economy 19
area 5 republic 10 *district 15 *world 20
Note that four of the incorrect similarities from Table 2 were replaced with correct entailments
resulting in a 20% increase of precision (reaching 60%).
non-substitutable similarities merely by the standard distributional similarity scheme,
suggesting that additional mechanisms and data types would be required.
6.2 The Average Common-Feature Rank Ratio
It should be noted at this point that these observations regarding feature weight be-
havior are based on subjective intuition of how characteristic features are for a word
meaning, which is quite difficult to assess systematically. Therefore, we next propose a
quantitative measure for analyzing the quality of feature vector weights.
More formally, given a pair of feature vectors for words w and v we first define
their average common-feature rankwith respect to the top-n common features, denoted
acfrn, as follows:
acfrn(w, v) =
1
n
?
f?top?n(F(w)?F(v))
1
2 [rank(w, f )+ rank(v, f )]
(7)
where rank(w, f ) is the rank of feature f in the vector of the word w when features are
sorted by their weight, and F(w) is the set of features in w?s vector. top-n is the set of
top n common features to consider, where common features are sorted by the sum of
their weights in the two word vectors (the same sorting as in Table 7). In other words,
acfrn(w, v) is the average rank in the two feature vectors of their top n common features.
453
Computational Linguistics Volume 35, Number 3
Using this measure, we expect that a good feature weighting function would
typically yield lower values of acfrn for truly similar words (as low ranking values
correspond to higher positions in the vectors) than for non-similar words. Hence, given
a pre-judged test set of pairs of similar and non-similar words, we define the ratio,
acfr-ratio, between the average acfrn of the set of all the non-similar words, denoted as
Non-Sim, and the average acfrn of the set of all the known pairs of similar words, Sim, to
be an objective measure for feature weighting quality, as follows:
acfrn ? ratio =
1
|Non?Sim|
?
w,v?Non?Sim acfrn(w,v)
1
|Sim|
?
w,v?Sim acfrn(w,v)
(8)
As an illustration, the two word pairs in Table 7 yielded acfr10(country, state) = 78
and acfr10(country, party) = 64. Both values are quite high, showing no principal differ-
ence between the tighter lexically entailing similarity versus a pair of non-similar (or
rather loosely related) words. This behavior indicates the deficiency of the MI feature
weighting function in this case. On the other hand, the corresponding values for the
two pairs produced by the Bootstrapped LIN method (for the features in Table 9) are
acfr10(country, state) = 12 and acfr10(country, party) = 41. These figures clearly reflect the
desired distinction between similar and non-similar words, showing that the common
features of the similar words are indeed concentrated at much higher ranks in the
vectors than the common features of the non-similar words.
In recent work on distributional similarity (Curran 2004; Weeds and Weir 2005) a
variety of alternative weighting functions were compared. However, the quality of these
weighting functions was evaluated only through their impact on the performance of
a particular word similarity measure, as we did in Section 5. Our acfr-ratio measure
provides the first attempt to analyze the quality of weighting functions directly, relative
to a pre-judged word similarity set, without reference to a concrete similarity measure.
6.3 An Empirical Assessment of the acfr-ratio
In this subsection we report an empirical comparison of the acfr-ratio obtained for theMI
and BootstrappedLIN weighting functions. To that end, we have run the Minipar system
on the full Reuters RCV1 corpus, which contains 2.5 GB of English news stories, and
then calculated theMI-weighted feature vectors. The optimized threshold on the feature
weights, ?weight, was set to 0.2. Further, to compute the Bootstrapped LIN feature weights
a ?sim of 0.02 was applied to the LIN similarity values. In this experiment we employed
the full bootstrapped vectors (i.e., without applying feature reduction by the top 100
cutoff). This was done to avoid the effect of the feature vector size on the acfrn metric,
which tends to naturally assign higher scores to shorter vectors.
As computing the acfr-ratio requires a pre-judged sample of candidate word simi-
larity pairs, we utilized the annotated test sample of candidate pairs of word similarities
described in Section 5, which contains both entailing and non-entailing pairs.
First, we computed the average common-feature rank scores (acfrn) (with varying
values of n) forweightMI and forweight
B over all the pairs in the test sample. Interestingly,
the mean acfrn scores for weight
B range within 110?264 for n = 10. . . 100, while the
corresponding range for weightMI is by an order of magnitude higher: 780?1,254, despite
the insignificant differences in vector sizes. Therefore, we conclude that the common
features that are relevant to establishing distributional similarity in general (regardless
of entailment) are much more scattered across the vectors by MI weighting, while with
bootstrapping they tend to appear at higher positions in the vectors. These figures
454
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
reflect a desired behavior of the bootstrapping function which concentrates most of the
prominent common features for all the distributionally similar words (whether entailing
or not) at the lower ranks of their vectors. In particular, this explains the ability of our
method to perform a massive feature reduction as demonstrated in Section 5, and to
produce more informative vectors, while demoting and eliminating much of the noise
in the original vectors.
Next, we aim to measure the discriminative power of the compared methods to
distinguish between entailing and non-entailing pairs. To this end we calculated the
acfr-ratio, which captures the difference in the average common feature ranks between
entailing vs. non-entailing pairs, for both the MI-based and bootstrapped vectors.
The obtained results are presented in Figure 2. As can be seen the acfr-ratio values
are consistently higher for Bootstrapped LIN than for MI. That is, the bootstrapping
method assigns much higher acfrn scores to entailing words than to non-entailing ones,
whereas for MI the corresponding acfrn scores for entailing and non-entailing pairs are
roughly equal. In particular, we notice that the largest gaps in acfr-ratio occur for lower
numbers of top common features, whose weights are indeed the most important and
influential in distributional similarity measures. Thus, these findings suggest a direct
indication of an improved quality of the bootstrapped feature vectors.
7. A Pseudo-Word Sense Disambiguation Evaluation
The lexical entailment evaluation reported herein corresponds to the lexical substitution
application of distributional similarity. The other type of application, as reviewed in the
Introduction, is similarity-based prediction of word co-occurrence likelihood, needed
for disambiguation applications. Comparative evaluations of distributional similarity
methods for this type of application were commonly conducted using a pseudo-word
sense disambiguation scheme, which is replicated here. In the next subsections we first
describe how distributional similarity can help improve word sense disambiguation
(WSD). Then we describe how the pseudo-word sense disambiguation task, which
Figure 2
Comparison between the acfr-ratio for MI and Bootstrapped LIN methods, when using varying
numbers of common top-ranked features in the words? feature vectors.
455
Computational Linguistics Volume 35, Number 3
corresponds to the general WSD setting, was used to evaluate the co-occurrence like-
lihood predictions obtained by alternative similarity methods.
7.1 Similarity Modeling for Word Sense Disambiguation
WSD methods need to identify the correct sense of an ambiguous word in a given
context. For example, a test instance for the verb save might be presented in the con-
text saving Private Ryan. The disambiguation method must decide whether save in this
particular context means rescue, preserve, keep, lay aside, or some other alternative.
Sense recognition is typically based on context features collected from a sense-
annotated training corpus. For example, the system might learn from the annotated
training data that the word soldier is a typical object for the rescuing sense of save, as in:
They saved the soldier. In this setting, distributional similarity is used to reduce the data
sparseness problem via similarity-based generalization. The general idea is to predict
the likelihood of unobserved word co-occurrences based on observed co-occurrences
of distributionally similar words. For example, assume that the noun private did not
occur as a direct object of save in the training data. Yet, some of the words that are
distributionally similar to private, like soldier or sergeant, might have occurred with save.
Thus, a WSD system may infer that the co-occurrence save private is more likely for the
rescuing sense of save because private is distributionally similar to soldier, which did co-
occur with this sense of save in the annotated training corpus. In general terms, theWSD
method estimates the co-occurrence likelihood for the target sense and a given context
word based on training data for words that are distributionally similar to the context
word.
This idea of similarity-based estimation of co-occurrence likelihood was applied
in Dagan, Marcus, and Markovitch (1995) to enhance WSD performance in machine
translation and recently in Gliozzo, Giuliano, and Strapparava (2005), who employed a
Latent Semantic Analysis (LSA)-based kernel function as a similarity-based represen-
tation for WSD. Other works employed the same idea for pseudo-word sense dis-
ambiguation, as explained in the next subsection.
7.2 The Pseudo-Word Sense Disambiguation Setting
Sense disambiguation typically requires annotated training data, created with consid-
erable human effort. Yarowsky (1992) suggested that when using WSD as a test bed
for comparative algorithmic evaluation it is possible to set up a pseudo-word sense
disambiguation scheme. This scheme was later adopted in several experiments, and
was popular for comparative evaluations of similarity-based co-occurrence likelihood
estimation (Dagan, Lee, and Pereira 1999; Lee 1999; Weeds andWeir 2005). We followed
closely the same experimental scheme, as described subsequently.
First, a list of pseudo-words is constructed by ?merging? pairs of words into a single
pseudo word. In our experiment each pseudo-word constitutes a pair of randomly
chosen verbs, (v, v?), where each verb represents an alternative ?sense? of the pseudo-
word. The two verbs are chosen to have almost identical probability of occurrence,
which avoids a word frequency bias on the co-occurrence likelihood predictions.
Next, we consider occurrences of pairs of the form ?n, (v, v?)? , where (v, v?) is a
pseudo-word and n is a noun representing the object of the pseudo-word. Such pairs
are constructed from all co-occurrences of either v or v? with the object n in the corpus.
For example, given the pseudo-word (rescue, keep) and the verb?object co-occurrence in
the corpus rescue?private we construct the pair ?private, (rescue, keep)?. Given such a test
456
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
pair, the disambiguation task is to decide which of the two verbs is more likely to co-
occur with the given object noun, aiming to recover the original verb from which this
pair was constructed. In this example we would like to predict that rescue is more likely
to co-occur with private as an object than keep.
In our experiment 80% of the constructed pairs were used for training, providing
the co-occurrence statistics for the original known verb in each pair (i.e., either ?n, v?
or ?n, v??). From the remaining 20% of the pairs those occurring in the training corpus
were discarded, leaving as a test set only pairs which do not appear in the training part.
Thus, predicting the co-occurrence likelihood of the noun with each of the two verbs
cannot rely on direct frequency estimation for the co-occurrences, but rather only on
similarity-based information.
To make the similarity-based predictions we first compute the distributional sim-
ilarity scores for all pairs of nouns based on the training set statistics, where the co-
occurring verbs serve as the features in the distributional vectors of the nouns. Then,
given a test pair ?(v,v?), n? our task is to predict which of the two verbs is more likely
to co-occur with n. This verb is thus predicted as being the original verb from which
the pair was constructed. To this end, the noun n is substituted in turn with each of its
k distributionally most similar nouns, ni, and then both of the obtained ?similar? pairs
?ni, v? and ?ni, v?? are sought in the training set.
Next, wewould like to predict that the more likely co-occurrence between ?n, v? and
?n, v?? is the one for which more pairs of similar words were found in the training set.
Several approaches were used in the literature to quantify this decision procedure and
we have followed the most recent one from Weeds and Weir (2005). Each similar noun
ni is given a vote, which is equal to the difference between the frequencies of the two
co-occurrences (ni, v) and (ni, v
?), and which it casts to the verb with which it co-occurs
more frequently. The votes for each of the two verbs are summed over all k similar
nouns ni and the one with most votes wins. The winning verb is considered correct if it
is indeed the original verb from which the pair was constructed, and a tie is recorded
if the votes for both verbs are equal. Finally, the overall performance of the prediction
method is calculated by its error rate:
error = 1
T
(#of incorrect choices+
#of ties
2
) (9)
where T is the number of test instances.
In the experiment, we used the 1,000 most frequent nouns in our subset of the
Reuters corpus (of Section 5.1). The training and test data were created as described
herein, using the Minipar parser (Lin 1993) to produce verb?object co-occurrence pairs.
The k = 40 most similar nouns for each test noun were computed by each of the three
examined similarity measures LIN, WJ, and COS (as in Section 5), with and without
bootstrapping. The six similarity lists were utilized in turn for the pseudo-word sense
disambiguation task, calculating the corresponding error rate.
7.3 Results
Table 11 shows the error rate improvements after applying the bootstrapped weighting
for each of the three similarity measures. The largest error reduction, by over 15%, was
obtained for the LIN method, with quite similar results for WJ. This result is better than
the one reported by Weeds and Weir (2005), who achieved about 6% error reduction
compared to LIN.
457
Computational Linguistics Volume 35, Number 3
Table 11
The comparative error rates of the pseudo-disambiguation task for the three examined similarity
measures, with and without applying the bootstrapped weighting for each of them.
Measure LIN?LINB WJ?WJB COS?COSB
Error rate 0.157?0.133 0.150?0.132 0.155?0.145
This experiment shows that learning tighter semantic similarities, based on the im-
proved bootstrapped feature vectors, correlates also with better similarity-based infer-
ence for co-occurrence likelihood prediction. Furthermore, we have seen once again that
the bootstrapping scheme does not depend on a specific similarity measure, reducing
the error rates for all three measures.
8. Conclusions
The primary contribution of this article is the proposal of a bootstrapping method
that substantially improves the quality of distributional feature vectors, as needed for
statistical word similarity. The main idea is that features which are common for similar
words are also most characteristic for their meanings and thus should be promoted. In
fact, beyond its intuitive appeal, this idea corresponds to the underlying rationale of
the distributional similarity scheme: Semantically similar words are expected to share
exactly those context features which are most characteristic for their meaning.
The superior empirical performance of the resulting vectors was assessed in the
context of the two primary applications of distributional word similarity. The first is
lexical substitution, which was represented in our work by a human gold standard
for the substitutable lexical entailment relation. The second is co-occurrence likelihood
prediction, which was assessed by the automatically computed scores of the common
pseudo-word sense disambiguation evaluation. An additional outcome of the improved
feature weighting is massive feature reduction.
Experimenting with three prominent similarity measures showed that the boot-
strapping scheme is robust and performs well when applied over different measures.
Notably, our experiments show that the underlying assumption behind the boot-
strapping scheme is valid, that is, available similarity metrics do provide a reason-
able approximation of the semantic similarity space which can be then exploited via
bootstrapping.
The methodology of our investigation has yielded several additional contributions:
1. Utilizing a refined definition of substitutable lexical entailment both as an
end goal and as an analysis vehicle for distributional similarity. It was
shown that the refined definition can be judged directly by human subjects
with very good agreement. Overall, lexical entailment is suggested as a
useful model for lexical substitution needs in semantic-oriented
applications.
2. A thorough error analysis of state of the art distributional similarity
performance was conducted. The main observation was deficient quality
of the feature vectors, which reduces the eventual quality of similarity
measures.
458
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
3. Inspired by the qualitative analysis, we proposed a new analytic measure
for feature vector quality, namely average common-feature rank ratio
(acfr-ratio), which is based on the common ranks of the features for pairs of
words. This measure estimates the ability of a feature weighting method to
distinguish between pairs of similar vs. non-similar words. To the best of
our knowledge this is the first proposed measure for direct analysis of the
quality of feature weighting functions, without the need to employ them
within some vector similarity measure.
The ability to identify the most characteristic features of words can have additional ben-
efits, beyond their impact on traditional word similarity measures (as evaluated in this
article). A demonstration of such potential appears in Geffet and Dagan (2005), which
presents a novel feature inclusion scheme for vector comparison. That scheme utilizes
our bootstrapping method to identify the most characteristic features of a word and
then tests whether these particular features co-occur also with a hypothesized entailed
word. The empirical success reported in that paper provides additional evidence for the
utility of the bootstrapping method.
More generally, our motivation and methodology can be extended in several di-
rections by future work on acquiring lexical entailment or other lexical-semantic rela-
tions. One direction is to explore better vector comparison methods that will utilize
the improved feature weighting, as shown in Geffet and Dagan (2005). Another direc-
tion is to integrate distributional similarity and pattern-based acquisition approaches,
which were shown to provide largely complementary information (Mirkin, Dagan, and
Geffet 2006). An additional potential is to integrate automatically acquired relationships
with the information found in WordNet, which seems to suffer from several serious
limitations (Curran 2005), and typically overlaps to a rather limited extent with the
output of automatic acquisition methods. As a parallel direction, future research should
explore in detail the impact of different lexical-semantic acquisition methods on text
understanding applications.
Finally, our proposed bootstrapping scheme seems to have a general appeal for
improving feature vector quality in additional unsupervised settings. We thus hope that
this idea will be explored further in other NLP and machine learning contexts.
References
Adams, Rod. 2006. Textual entailment
through extended lexical overlap. In
Proceedings of the Second PASCAL Challenges
Workshop on Recognising Textual Entailment,
pages 68?73, Venice.
Bar-Haim, Roy, Ido Dagan, Bill Dolan,
Lisa Ferro, Danilo Giampiccolo, Bernardo
Magnini, and Idan Szpektor. 2006. The
second PASCAL recognising textual
entailment challenge. In Proceedings of the
Second PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 1?9,
Venice.
Baroni, Marco and S. Vegnaduzzo. 2004.
Identifying subjective adjectives through
web-based mutual information. In
Proceedings of KONVENS?04, pages 17?24,
Vienna.
Barzilay, Regina and Kathleen McKeown.
2001. Extracting paraphrases from a
parallel corpus. In Proceedings of ACL /
EACL?01, pages 50?57, Toulouse.
Caraballo, Sharon A. 1999. Automatic
construction of a hypernym-labeled noun
hierarchy from text. In Proceedings of
ACL?99, pages 120?126, College Park, MD.
Chklovski, Timothy and Patrick Pantel.
2004. VerbOcean: Mining the web for
fine-grained Semantic Verb Relations. In
Proceedings of EMNLP?04, pages 33?40,
Barcelona.
Church, Kenneth W. and Hanks Patrick.
1990. Word association norms, mutual
information, and lexicography.
Computational Linguistics, 16(1):22?29.
Curran, James R. 2004. From Distributional
to Semantic Similarity. Ph.D. Thesis,
459
Computational Linguistics Volume 35, Number 3
School of Informatics of the University
of Edinburgh, Scotland.
Curran, James R. 2005. Supersense tagging of
unknown nouns using semantic similarity.
In Proceedings of ACL?2005, pages 26?33,
Ann Arbor, MI.
Curran, James R. and Marc Moens. 2002.
Improvements in automatic thesaurus
extraction. In Proceedings of the Workshop
on Unsupervised Lexical Acquisition,
pages 59?67, Philadelphia, PA.
Dagan, Ido. 2000. Contextual Word
Similarity. In Rob Dale, Hermann Moisl,
and Harold Somers, editors, Handbook
of Natural Language Processing. Marcel
Dekker Inc, Chapter 19, pages 459?476,
New York, NY.
Dagan, Ido, Oren Glickman, and Bernardo
Magnini. 2006. The PASCAL recognising
textual entailment challenge. Lecture Notes
in Computer Science, 3944:177?190.
Dagan, Ido, Lillian Lee, and Fernando
Pereira. 1999. Similarity-based models of
co-occurrence probabilities. Machine
Learning, 34(1-3):43?69.
Dagan, Ido, Shaul Marcus, and Shaul
Markovitch. 1995. Contextual word
similarity and estimation from sparse
data. Computer, Speech and Language,
9:123?152.
Dunning, Ted E. 1993. Accurate methods for
the statistics of surprise and coincidence.
Computational Linguistics, 19(1):61?74.
Essen, U., and V. Steinbiss. 1992.
Co-occurrence smoothing for stochastic
language modeling. In ICASSP?92,
1:161?164, Piscataway, NJ.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Ferrandez, O., R. M. Terol, R. Munoz, P.
Martinez-Barco, and M. Palomar. 2006.
An approach based on logic forms and
WordNet relationships to textual
entailment performance. In Proceedings
of the Second PASCAL Challenges
Workshop on Recognising Textual Entailment,
pages 22?26, Venice.
Gasperin, Caroline and Renata Vieira. 2004.
Using word similarity lists for resolving
indirect anaphora. In Proceedings of
ACL?04 Workshop on Reference Resolution,
pages 40?46, Barcelona.
Gauch, Susan, J. Wang, and S. Mahesh
Rachakonda. 1999. A corpus analysis
approach for automatic query expansion
and its extension to multiple databases.
ACM Transactions on Information Systems
(TOIS), 17(3):250?269.
Geffet, Maayan. 2006. Refining the
Distributional Similarity Scheme for Lexical
Entailment. Ph.D. Thesis. School of
Computer Science and Engineering,
Hebrew University, Jerusalem, Israel.
Geffet, Maayan and Ido Dagan. 2004. Feature
vector quality and distributional similarity.
In Proceedings of COLING?04, Article
number: 247, Geneva.
Geffet, Maayan and Ido Dagan. 2005. The
distributional inclusion hypotheses and
lexical entailment. In Proceedings of
ACL?05, pages 107?114, Ann Arbor, MI.
Gliozzo, Alfio, Claudio Giuliano, and Carlo
Strapparava. 2005. Domain kernels for
word sense disambiguation. In Proceedings
of ACL?05, pages 403?410, Ann Arbor, MI.
Grefenstette, Gregory. 1994. Exploration in
Automatic Thesaurus Discovery. Kluwer
Academic Publishers, Norwell, MA.
Harris, Zelig S. 1968. Mathematical structures
of language. Wiley, New Jersey.
Hindle, D. 1990. Noun classification from
predicate-argument structures. In
Proceedings of ACL?90, pages 268?275,
Pittsburgh, PA.
Jijkoun, Valentin and Maarten de Rijke. 2005.
Recognizing textual entailment: Is word
similarity enough? In Joaquin Quinonero
Candela, Ido Dagan, Bernardo Magnini,
and Florence d?Alche-Buc, editors, Machine
Learning Challenges, Evaluating Predictive
Uncertainty, Visual Object Classification
and Recognizing Textual Entailment, First
PASCAL Machine Learning Challenges
Workshop, MLCW 2005, Southampton, UK,
Lecture Notes in Computer Science 3944,
pages 449?460, Springer, New York, NY.
Karov, Y. and S. Edelman. 1996.
Learning similarity-based word sense
disambiguation from sparse data.
In E. Ejerhed and I. Dagan, editors,
Fourth Workshop on Very Large Corpora.
Association for Computational Linguistics,
Somerset, NJ, pages 42?55.
Landis, J. R. and G. G. Koch. 1997. The
measurements of observer agreement for
categorical data. Biometrics, 33:159?174.
Lee, Lillian. 1997. Similarity-Based Approaches
to Natural Language Processing. Ph.D. thesis,
Harvard University, Cambridge, MA.
Lee, Lillian.1999. Measures of distributional
similarity. In Proceedings of ACL?99,
pages 25?32, College Park, MD.
Lin, Dekang. 1993. Principle-based parsing
without overgeneration. In Proceedings of
ACL?93, pages 112?120, Columbus, OH.
Lin, Dekang. 1998. Automatic retrieval and
clustering of similar words. In Proceedings
460
Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality
of COLING/ACL?98, pages 768?774,
Montreal.
Lin, Dekang and Patrick Pantel. 2001.
Discovery of inference rules for question
answering. Natural Language Engineering,
7(4):343?360.
Luk, Alpha K. 1995. Statistical sense
disambiguation with relatively small
corpora using dictionary definitions.
In Proceedings of ACL?95, pages 181?188,
Cambridge, MA.
Manning, Christopher D. and Hinrich
Schutze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press,
Cambridge, MA.
Mirkin, Shachar, Ido Dagan, and Maayan
Geffet. 2006. Integrating pattern-based
and distributional similarity methods
for lexical entailment acquisition. In
Proceedings of the COLING/ACL?06 Main
Conference Poster Sessions, pages 579?586,
Sydney.
Ng, H. T. 1997. Exemplar-based word
sense disambiguation: Some recent
improvements. In Proceedings of
EMNLP? 97, pages 208?213,
Providence, RI.
Ng, H. T. and H. B. Lee. 1996. Integrating
multiple knowledge sources to
disambiguate word sense: An
exemplar-based approach. In
Proceedings of ACL?1996, pages 40?47,
Santa Cruz, CA.
Nicholson, Jeremy, Nicola Stokes, and
Timothy Baldwin. 2006. Detecting
entailment using an extended
implementation of the basic elements
overlap metric. In Proceedings of the
Second PASCAL Challenges Workshop
on Recognising Textual Entailment,
pages 122?127, Venice.
Pado, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of
semantic space models. Computational
Linguistics, 33(2):161?199.
Pantel, Patrick and Deepak Ravichandran.
2004. Automatically labeling semantic
classes. In Proceedings of HLT/NAACL?04,
pages 321?328, Boston, MA.
Pantel, Patrick, D. Ravichandran, and
E. Hovy. 2004. Towards terascale
knowledge acquisition. In Proceedings of
COLING?04, Article number: 771, Geneva.
Pekar, Viktor, M. Krkoska, and S. Staab.
2004. Feature weighting for
co-occurrence-based classification
of Words. In Proceedings of COLING?04,
Article number: 799, Geneva.
Pereira, Fernando, Naftali Tishby, and
Lillian Lee. 1993. Distributional
clustering of English words. In
Proceedings of ACL?93, pages 183?190,
Colombus, OH.
Ruge, Gerda. 1992. Experiments on
linguistically-based term associations.
Information Processing & Management,
28(3):317?332.
Salton, G. and M. J. McGill. 1983.
Introduction to Modern Information
Retrieval. McGraw-Hill, New York, NY.
Szpektor, Idan, H. Tanev, Ido Dagan, and
B. Coppola. 2004. Scaling web-based
acquisition of entailment relations. In
Proceedings of EMNLP?04, pages 41?48,
Barcelona.
Vanderwende, Lucy, Arul Menezes, and Rion
Snow. 2006. Microsoft research at RTE-2:
Syntactic contributions in the entailment
task: An implementation. In Proceedings
of the Second PASCAL Challenges Workshop
on Recognising Textual Entailment,
pages 27?32, Venice.
Weeds, Julie and David Weir. 2005.
Co-occurrence retrieval: A flexible
framework for lexical distributional
similarity. Computational Linguistics,
31(4):439?476.
Weeds, Julie, D. Weir, and D. McCarthy.
2004. Characterizing measures of lexical
distributional similarity. In Proceedings
of COLING?04, pages 1015?1021,
Switzerland.
Widdows, D. 2003. Unsupervised methods
for developing taxonomies by combining
syntactic and statistical information.
In Proceedings of HLT/NAACL 2003,
pages 197?204, Edmonton.
Yarowsky, D. 1992. Word-sense
disambiguation using statistical models
of Roget?s categories trained on large
corpora. In Proceedings of COLING?92,
pages 454?460, Nantes.
461

Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 579?586,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Integrating Pattern-based and Distributional Similarity Methods for 
Lexical Entailment Acquisition 
 
                   Shachar Mirkin     Ido Dagan         Maayan Geffet 
School of Computer Science and Engineering 
The Hebrew University, Jerusalem, Israel, 
91904 
mirkin@cs.huji.ac.il  
 
Department of Computer Science 
Bar-Ilan University, Ramat Gan, Israel,  
52900 
{dagan,zitima}@cs.biu.ac.il 
 
Abstract 
This paper addresses the problem of acquir-
ing lexical semantic relationships, applied to 
the lexical entailment relation. Our main con-
tribution is a novel conceptual integration 
between the two distinct acquisition para-
digms for lexical relations ? the pattern-
based and the distributional similarity ap-
proaches. The integrated method exploits 
mutual complementary information of the 
two approaches to obtain candidate relations 
and informative characterizing features. 
Then, a small size training set is used to con-
struct a more accurate supervised classifier, 
showing significant increase in both recall 
and precision over the original approaches. 
1 Introduction 
Learning lexical semantic relationships is a fun-
damental task needed for most text understand-
ing applications. Several types of lexical 
semantic relations were proposed as a goal for 
automatic acquisition. These include lexical on-
tological relations such as synonymy, hyponymy 
and meronymy, aiming to automate the construc-
tion of WordNet-style relations. Another com-
mon target is learning general distributional 
similarity between words, following Harris' Dis-
tributional Hypothesis (Harris, 1968). Recently, 
an applied notion of entailment between lexical 
items was proposed as capturing major inference 
needs which cut across multiple semantic rela-
tionship types (see Section 2 for further back-
ground).  
The literature suggests two major approaches 
for learning lexical semantic relations: distribu-
tional similarity and pattern-based. The first ap-
proach recognizes that two words (or two multi-
word terms) are semantically similar based on 
distributional similarity of the different contexts 
in which the two words occur. The distributional 
method identifies a somewhat loose notion of 
semantic similarity, such as between company 
and government, which does not ensure that the 
meaning of one word can be substituted by the 
other. The second approach is based on identify-
ing joint occurrences of the two words within 
particular patterns, which typically indicate di-
rectly concrete semantic relationships. The pat-
tern-based approach tends to yield more accurate 
hyponymy and (some) meronymy relations, but 
is less suited to acquire synonyms which only 
rarely co-occur within short patterns in texts. It 
should be noted that the pattern-based approach 
is commonly applied also for information and 
knowledge extraction to acquire factual instances 
of concrete meaning relationships (e.g. born in, 
located at) rather than generic lexical semantic 
relationships in the language. 
While the two acquisition approaches are 
largely complementary, there have been just few 
attempts to combine them, usually by pipeline 
architecture. In this paper we propose a method-
ology for integrating distributional similarity 
with the pattern-based approach. In particular, 
we focus on learning the lexical entailment rela-
tionship between common nouns and noun 
phrases (to be distinguished from learning rela-
tionships for proper nouns, which usually falls 
within the knowledge acquisition paradigm).  
The underlying idea is to first identify candi-
date relationships by both the distributional ap-
proach, which is applied exhaustively to a local 
corpus, and the pattern-based approach, applied 
to the web. Next, each candidate is represented 
by a unified set of distributional and pattern-
based features. Finally, using a small training set 
we devise a supervised (SVM) model that classi-
fies new candidate relations as correct or incor-
rect. 
To implement the integrated approach we de-
veloped state of the art pattern-based acquisition 
579
methods and utilized a distributional similarity 
method that was previously shown to provide 
superior performance for lexical entailment ac-
quisition. Our empirical results show that the 
integrated method significantly outperforms each 
approach in isolation, as well as the na?ve com-
bination of their outputs. Overall, our method 
reveals complementary types of information that 
can be obtained from the two approaches. 
 
2 Background 
2.1 Distributional Similarity and 
Lexical Entailment 
The general idea behind distributional similarity 
is that words which occur within similar contexts 
are semantically similar (Harris, 1968). In a 
computational framework, words are represented 
by feature vectors, where features are context 
words weighted by a function of their statistical 
association with the target word. The degree of 
similarity between two target words is then de-
termined by a vector comparison function. 
Amongst the many proposals for distributional 
similarity measures, (Lin, 1998) is maybe the 
most widely used one, while (Weeds et al, 2004) 
provides a typical example for recent research. 
Distributional similarity measures are typically 
computed through exhaustive processing of a 
corpus, and are therefore applicable to corpora of 
bounded size. 
It was noted recently by Geffet and Dagan 
(2004, 2005) that distributional similarity cap-
tures a quite loose notion of semantic similarity, 
as exemplified by the pair country ? party (iden-
tified by Lin's similarity measure). Consequently, 
they proposed a definition for the lexical entail-
ment relation, which conforms to the general 
framework of applied textual entailment (Dagan 
et al, 2005). Generally speaking, a word w lexi-
cally entails another word v if w can substitute v 
in some contexts while implying v's original 
meaning. It was suggested that lexical entailment 
captures major application needs in modeling 
lexical variability, generalized over several types 
of known ontological relationships. For example, 
in Question Answering (QA), the word company 
in a question can be substituted in the text by 
firm (synonym), automaker (hyponym) or sub-
sidiary (meronym), all of which entail company. 
Typically, hyponyms entail their hypernyms and 
synonyms entail each other, while entailment 
holds for meronymy only in certain cases. 
In this paper we investigate automatic acquisi-
tion of the lexical entailment relation. For the 
distributional similarity component we employ 
the similarity scheme of (Geffet and Dagan, 
2004), which was shown to yield improved pre-
dictions of (non-directional) lexical entailment 
pairs. This scheme utilizes the symmetric simi-
larity measure of (Lin, 1998) to induce improved 
feature weights via bootstrapping. These weights 
identify the most characteristic features of each 
word, yielding cleaner feature vector representa-
tions and better similarity assessments. 
2.2 Pattern-based Approaches 
Hearst (1992) pioneered the use of lexical-
syntactic patterns for automatic extraction of 
lexical semantic relationships. She acquired hy-
ponymy relations based on a small predefined set 
of highly indicative patterns, such as ?X, . . . , Y 
and/or other Z?, and ?Z such as X, . . . and/or Y?, 
where X and Y are extracted as hyponyms of Z. 
Similar techniques were further applied to pre-
dict hyponymy and meronymy relationships us-
ing lexical or lexico-syntactic patterns (Berland 
and Charniak, 1999; Sundblad, 2002), and web 
page structure was exploited to extract hy-
ponymy relationships by Shinzato and Torisawa 
(2004). Chklovski and Pantel (2004) used pat-
terns to extract a set of relations between verbs, 
such as similarity, strength and antonymy. Syno-
nyms, on the other hand, are rarely found in such 
patterns. In addition to their use for learning lexi-
cal semantic relations, patterns were commonly 
used to learn instances of concrete semantic rela-
tions for Information Extraction (IE) and QA, as 
in (Riloff and Shepherd, 1997; Ravichandran and 
Hovy, 2002; Yangarber et al, 2000).  
Patterns identify rather specific and informa-
tive structures within particular co-occurrences 
of the related words. Consequently, they are rela-
tively reliable and tend to be more accurate than 
distributional evidence. On the other hand, they 
are susceptive to data sparseness in a limited size 
corpus. To obtain sufficient coverage, recent 
works such as (Chklovski and Pantel, 2004) ap-
plied pattern-based approaches to the web. These 
methods form search engine queries that match 
likely pattern instances, which may be verified 
by post-processing the retrieved texts. 
Another extension of the approach was auto-
matic enrichment of the pattern set through boot-
strapping. Initially, some instances of the sought 
580
relation are found based on a set of manually 
defined patterns.  Then, additional co-
occurrences of the related terms are retrieved, 
from which new patterns are extracted (Riloff 
and Jones, 1999; Pantel et al, 2004). Eventually, 
the list of effective patterns found for ontological 
relations has pretty much converged in the litera-
ture. Amongst these, Table 1 lists the patterns 
that were utilized in our work. 
Finally, the selection of candidate pairs for a 
target relation was usually based on some func-
tion over the statistics of matched patterns. To 
perform more systematic selection Etzioni et al 
(2004) applied a supervised Machine Learning 
algorithm (Na?ve Bayes), using pattern statistics 
as features. Their work was done within the IE 
framework, aiming to extract semantic relation 
instances for proper nouns, which occur quite 
frequently in indicative patterns. In our work we 
incorporate and extend the supervised learning 
step for the more difficult task of acquiring gen-
eral language relationships between common 
nouns. 
2.3 Combined Approaches 
It can be noticed that the pattern-based and dis-
tributional approaches have certain complemen-
tary properties. The pattern-based method tends 
to be more precise, and also indicates the direc-
tion of the relationship between the candidate 
terms. The distributional similarity approach is 
more exhaustive and suitable to detect symmetric 
synonymy relations. Few recent attempts on re-
lated (though different) tasks were made to clas-
sify (Lin et al, 2003) and label (Pantel and 
Ravichandran, 2004) distributional similarity 
output using lexical-syntactic patterns, in a pipe-
line architecture. We aim to achieve tighter inte-
gration of the two approaches, as described next. 
 
3 An Integrated Approach for Lexi-
cal Entailment Acquisition 
This section describes our integrated approach 
for acquiring lexical entailment relationships, 
applied to common nouns. The algorithm re-
ceives as input a target term and aims to acquire 
a set of terms that either entail or are entailed by 
it. We denote a pair consisting of the input target 
term and an acquired entailing/entailed term as 
entailment pair. Entailment pairs are directional, 
as in bank  company. 
Our approach applies a supervised learning 
scheme, using SVM, to classify candidate en-
tailment pairs as correct or incorrect. The SVM 
training phase is applied to a small constant 
number of training pairs, yielding a classification 
model that is then used to classify new test en-
tailment pairs. The designated training set is also 
used to tune some additional parameters of the 
method. Overall, the method consists of the fol-
lowing main components:  
1: Acquiring candidate entailment pairs for 
the input term by pattern-based and distribu-
tional similarity methods (Section 3.2); 
2: Constructing a feature set for all candidates 
based on pattern-based and distributional in-
formation (Section 3.3); 
3: Applying SVM training and classification 
to the candidate pairs (Section 3.4).  
The first two components, of acquiring candidate 
pairs and collecting features for them, utilize a 
generic module for pattern-based extraction from 
the web, which is described first in Section 3.1.    
3.1 Pattern-based Extraction Mod-
ule 
The general pattern-based extraction module re-
ceives as input a set of lexical-syntactic patterns 
(as in Table 1) and either a target term or a can-
didate pair of terms. It then searches the web for 
occurrences of the patterns with the input term(s). 
A small set of effective queries is created for 
each pattern-terms combination, aiming to re-
trieve as much relevant data with as few queries 
as possible. 
Each pattern has two variable slots to be in-
stantiated by candidate terms for the sought rela-
tion. Accordingly, the extraction module can be 
1 NP1 such as NP2 
2 Such NP1 as NP2 
3 NP1 or other NP2 
4 NP1 and other NP2 
5 NP1 ADV known as NP2 
6 NP1 especially NP2 
7 NP1 like NP2 
8 NP1 including NP2 
9 NP1-sg is (a OR an) NP2-sg 
10 NP1-sg (a OR an) NP2-sg 
11 NP1-pl are NP2-pl 
Table 1: The patterns we used for entailment ac-
quisition based on (Hearst, 1992) and (Pantel et al, 
2004). Capitalized terms indicate variables. pl and 
sg stand for plural and singular forms. 
 
581
used in two modes: (a) receiving a single target 
term as input and searching for instantiations of 
the other variable to identify candidate related 
terms (as in Section 3.2); (b) receiving a candi-
date pair of terms for the relation and searching 
pattern instances with both terms, in order to 
validate and collect information about the rela-
tionship between the terms (as in Section 3.3). 
Google proximity search1 provides a useful tool 
for these purposes, as it allows using a wildcard 
which might match either an un-instantiated term 
or optional words such as modifiers.  For exam-
ple, the query "such ** as *** (war OR wars)" is 
one of the queries created for the input pattern 
such NP1 as NP2 and the input target term war, 
allowing new terms to match the first pattern 
variable. For the candidate entailment pair war 
? struggle, the first variable is instantiated as 
well. The corresponding query would be: "such * 
(struggle OR struggles) as *** (war OR wars)?. 
This technique allows matching terms that are 
sub-parts of more complex noun phrases as well 
as multi-word terms. 
The automatically constructed queries, cover-
ing the possible combinations of multiple wild-
cards, are submitted to Google2 and a specified 
number of snippets is downloaded, while avoid-
ing duplicates. The snippets are passed through a 
word splitter and a sentence segmenter3, while 
filtering individual sentences that do not contain 
all search terms. Next, the sentences are proc-
essed with the OpenNLP4  POS tagger and NP 
chunker. Finally, pattern-specific regular expres-
sions over the chunked sentences are applied to 
verify that the instantiated pattern indeed occurs 
in the sentence, and to identify variable instantia-
tions.  
On average, this method extracted more than 
3300 relationship instances for every 1MB of 
downloaded text, almost third of them contained 
multi-word terms. 
3.2 Candidate Acquisition 
Given an input target term we first employ pat-
tern-based extraction to acquire entailment pair 
candidates and then augment the candidate set 
with pairs obtained through distributional simi-
larity. 
                                                          
1
 Previously used by (Chklovski and Pantel, 2004). 
2
 http://www.google.com/apis/ 
3 Available from the University of Illinois at Urbana-
Champaign, http://l2r.cs.uiuc.edu/~cogcomp/tools.php 
4
 www.opennlp.sourceforge.net/ 
3.2.1 Pattern-based Candidates 
At the candidate acquisition phase pattern in-
stances are searched with one input target term, 
looking for instantiations of the other pattern 
variable to become the candidate related term 
(the first querying mode described in Section 
3.1). We construct two types of queries, in which 
the target term is either the first or second vari-
able in the pattern, which corresponds to finding 
either entailing or entailed terms that instantiate 
the other variable.  
In the candidate acquisition phase we utilized 
patterns 1-8 in Table 1, which we empirically 
found as most suitable for identifying directional 
lexical entailment pairs. Patterns 9-11 are not 
used at this stage as they produce too much noise 
when searched with only one instantiated vari-
able. About 35 queries are created for each target 
term in each entailment direction for each of the 
8 patterns. For every query, the first n snippets 
are downloaded (we used n=50). The 
downloaded snippets are processed as described 
in Section 3.1, and candidate related terms are 
extracted, yielding candidate entailment pairs 
with the input target term.  
Quite often the entailment relation holds be-
tween multi-word noun-phrases rather than 
merely between their heads. For example, trade 
center lexically entails shopping complex, while 
center does not necessarily entail complex. On 
the other hand, many complex multi-word noun 
phrases are too rare to make a statistically based 
decision about their relation with other terms. 
Hence, we apply the following two criteria to 
balance these constraints:  
1. For the entailing term we extract only the 
complete noun-chunk which instantiate the 
pattern. For example: we extract housing 
project ? complex, but do not extract pro-
ject as entailing complex since the head noun 
alone is often too general to entail the other 
term. 
2. For the entailed term we extract both the 
complete noun-phrase and its head in order 
to create two separate candidate entailment 
pairs with the entailing term, which will be 
judged eventually according to their overall 
statistics. 
As it turns out, a large portion of the extracted 
pairs constitute trivial hyponymy relations, 
where one term is a modified version of the other, 
like low interest loan ? loan. These pairs were 
removed, along with numerous pairs including 
proper nouns, following the goal of learning en-
582
tailment relationships for distinct common 
nouns.  
Finally, we filter out the candidate pairs whose 
frequency in the extracted patterns is less than a 
threshold, which was set empirically to 3. Using 
a lower threshold yielded poor precision, while a 
threshold of 4 decreased recall substantially with 
just little effect on precision. 
3.2.2 Distributional Similarity 
Candidates 
As mentioned in Section 2, we employ the distri-
butional similarity measure of (Geffet and Da-
gan, 2004) (denoted here GD04 for brevity), 
which was found effective for extracting non-
directional lexical entailment pairs.  Using local 
corpus statistics, this algorithm produces for each 
target noun a scored list of up to a few hundred 
words with positive distributional similarity 
scores. 
Next we need to determine an optimal thresh-
old for the similarity score, considering words 
above it as likely entailment candidates. To tune 
such a threshold we followed the original meth-
odology used to evaluate GD04. First, the top-k 
(k=40) similarities of each training term are 
manually annotated by the lexical entailment cri-
terion (see Section 4.1). Then, the similarity 
value which yields the maximal micro-averaged 
F1 score is selected as threshold, suggesting an 
optimal recall-precision tradeoff. The selected 
threshold is then used to filter the candidate simi-
larity lists of the test words.   
Finally, we remove all entailment pairs that al-
ready appear in the candidate set of the pattern-
based approach, in either direction (recall that the 
distributional candidates are non-directional). 
Each of the remaining candidates generates two 
directional pairs which are added to the unified 
candidate set of the two approaches. 
3.3 Feature Construction 
Next, each candidate is represented by a set of 
features, suitable for supervised classification. To 
this end we developed a novel feature set based 
on both pattern-based and distributional data. 
 
To obtain pattern statistics for each pair, the 
second mode of the pattern-based extraction 
module is applied (see Section 3.1). As in this 
case, both variables in the pattern are instantiated 
by the terms of the pair, we could use all eleven 
patterns in Table 1, creating a total of about 55 
queries per pair and downloading m=20 snippets 
for each query. The downloaded snippets are 
processed as described in Section 3.1 to identify 
pattern matches and obtain relevant statistics for 
feature scores.  
Following is the list of feature types computed 
for each candidate pair. The feature set was de-
signed specifically for the task of extracting the 
complementary information of the two methods. 
Conditional Pattern Probability: This type of 
feature is created for each of the 11 individual 
patterns. The feature value is the estimated con-
ditional probability of having the pattern 
matched in a sentence given that the pair of terms 
does appear in the sentence (calculated as the 
fraction of pattern matches for the pair amongst 
all unique sentences that contain the pair). This 
feature yields normalized scores for pattern 
matches regardless of the number of snippets 
retrieved for the given pair. This normalization is 
important in order to bring to equal grounds can-
didate pairs identified through either the pattern-
based or distributional approaches, since the lat-
ter tend to occur less frequently in patterns. 
Aggregated Conditional Pattern Probability: 
This single feature is the conditional probability 
that any of the patterns match in a retrieved sen-
tence, given that the two terms appear in it. It is 
calculated like the previous feature, with counts 
aggregated over all patterns, and aims to capture 
overall appearance of the pair in patterns, regard-
less of the specific pattern. 
Conditional List-Pattern Probability: This fea-
ture was designed to eliminate the typical non-
entailing cases of co-hyponyms (words sharing 
the same hypernym), which nevertheless tend to 
co-occur in entailment patterns. We therefore 
also check for pairs' occurrences in lists, using 
appropriate list patterns, expecting that correct 
entailment pairs would not co-occur in lists. The 
probability estimate, calculated like the previous 
one, is expected to be a negative feature for the 
learning model. 
Relation Direction Ratio: The value of this fea-
ture is the ratio between the overall number of 
pattern matches for the pair and the number of 
pattern matches for the reversed pair (a pair cre-
ated with the same terms in the opposite entail-
ment direction). We found that this feature 
strongly correlates with entailment likelihood. 
Interestingly, it does not deteriorate performance 
for synonymous pairs. 
Distributional Similarity Score: The GD04 simi-
larity score of the pair was used as a feature. We 
583
also attempted adding Lin's (1998) similarity 
scores but they appeared to be redundant. 
Intersection Feature: A binary feature indicating 
candidate pairs acquired by both methods, which 
was found to indicate higher entailment likeli-
hood. 
    In summary, the above feature types utilize 
mutually complementary pattern-based and dis-
tributional information. Using cross validation 
over the training set we verified that each feature 
makes marginal contribution to performance 
when added on top of the remaining features.  
3.4 Training and Classification 
In order to systematically integrate different fea-
ture types we used the state-of-the-art supervised 
classifier SVMlight (Joachims, 1999) for entail-
ment pair classification. Using 10-fold cross-
validation over the training set we obtained the 
SVM configuration that yields an optimal micro-
averaged F1 score. Through this optimization we 
chose the RBF kernel function and obtained op-
timal values for the J, C and the RBF's Gamma 
parameters. The candidate test pairs classified as 
correct entailments constitute the output of our 
integrated method. 
 
4 Empirical Results 
4.1 Data Set and Annotation 
We utilized the experimental data set from Geffet 
and Dagan (2004). The dataset includes the simi-
larity lists calculated by GD04 for a sample of 30 
target (common) nouns, computed from an 18 
million word subset of the Reuters corpus5. We 
randomly picked a small set of 10 terms for train-
ing, leaving the remaining 20 terms for testing. 
Then, the set of entailment pair candidates for all 
nouns was created by applying the filtering 
method of Section 3.2.2 to the distributional 
similarity lists, and by extracting pattern-based 
                                                          
5
 Reuters Corpus, Volume 1, English Language, 1996-08-20 to 1997-08-19. 
candidates from the web as described in Section 
3.2.1. 
Gold standard annotations for entailment pairs 
were created by three judges. The judges were 
guided to annotate as ?Correct? the pairs con-
forming to the lexical entailment definition, 
which was reflected in two operational tests: i) 
Word meaning entailment: whether the meaning 
of the first (entailing) term implies the meaning 
of the second (entailed) term under some com-
mon sense of the two terms; and ii) Substitutabil-
ity: whether the first term can substitute the 
second term in some natural contexts, such that 
the meaning of the modified context entails the 
meaning of the original one. The obtained Kappa 
values (varying between 0.7 and 0.8) correspond 
to substantial agreement on the task. 
4.2 Results 
The numbers of candidate entailment pairs col-
lected for the test terms are shown in Table 2. 
These figures highlight the markedly comple-
mentary yield of the two acquisition approaches, 
where only about 10% of all candidates were 
identified by both methods. On average, 120 
candidate entailment pairs were acquired for 
each target term. 
The SVM classifier was trained on a quite 
small annotated sample of 700 candidate entail-
ment pairs of the 10 training terms. Table 3 pre-
sents comparative results for the classifier, for 
each of the two sets of candidates produced by 
each method alone, and for the union of these 
two sets (referred as Na?ve Combination). The 
results were computed for an annotated random 
sample of about 400 candidate entailment pairs 
of the test terms. Following common pooling 
evaluations in Information Retrieval, recall is 
calculated relatively to the total number of cor-
rect entailment pairs acquired by both methods 
together.  
METHOD P R F 
Pattern-based  0.44 0.61 0.51 
Distributional  
Similarity 0.33 0.53 0.40 
Na?ve Combina-
tion 0.36 1.00 0.53 
Integrated  0.57 0.69 0.62 
Table 3: Precision, Recall and F1 figures for the 
test words under each method. 
 
PATTERN-
BASED 
DISTRIBU-
TIONAL TOTAL 
1186 1420 2350 
Table 2: The numbers of distinct entailment pair 
candidates obtained for the test words by each of 
the methods, and when combined.  
 
584
The first two rows of the table show quite 
moderate precision and recall for the candidates 
of each separate method. The next row shows the 
great impact of method combination on recall, 
relative to the amount of correct entailment pairs 
found by each method alone, validating the com-
plementary yield of the approaches. The inte-
grated classifier, applied to the combined set of 
candidates, succeeds to increase precision sub-
stantially by 21 points (a relative increase of al-
most 60%), which is especially important for 
many precision-oriented applications like Infor-
mation Retrieval and Question Answering. The 
precision increase comes with the expense of 
some recall, yet having F1 improved by 9 points. 
The integrated method yielded on average about 
30 correct entailments per target term. Its classi-
fication accuracy (percent of correct classifica-
tions) reached 70%, which nearly doubles the 
na?ve combination's accuracy.  
It is impossible to directly compare our results 
with those of other works on lexical semantic 
relationships acquisition, since the particular task 
definition and dataset are different. As a rough 
reference point, our result figures do match those 
of related papers reviewed in Section 2, while we 
notice that our setting is relatively more difficult 
since we excluded the easier cases of proper 
nouns. (Geffet and Dagan, 2005), who exploited 
the distributional similarity approach over the 
web to address the same task as ours, obtained 
higher precision but substantially lower recall, 
considering only distributional candidates. Fur-
ther research is suggested to investigate integrat-
ing their approach with ours. 
 
 
 
4.3 Analysis and Discussion 
Analysis of the data confirmed that the two 
methods tend to discover different types of rela-
tions. As expected, the distributional similarity 
method contributed most (75%) of the synonyms 
that were correctly classified as mutually entail-
ing pairs (e.g. assault ? abuse in Table 4). On 
the other hand, about 80% of all correctly identi-
fied hyponymy relations were produced by the 
pattern-based method (e.g. abduction ? abuse). 
The integrated method provides a means to de-
termine the entailment direction for distributional 
similarity candidates which by construction are 
non-directional. Thus, amongst the (non-
synonymous) distributional similarity pairs clas-
sified as entailing, the direction of 73% was cor-
rectly identified. In addition, the integrated 
method successfully filters 65% of the non-
entailing co-hyponym candidates (hyponyms of 
the same hypernym), most of them originated in 
the distributional candidates, which is a large 
portion (23%) of all correctly discarded pairs. 
Consequently, the precision of distributional 
similarity candidates approved by the integrated 
system was nearly doubled, indicating the addi-
tional information that patterns provide about 
distributionally similar pairs. 
Yet, several error cases were detected and 
categorized. First, many non-entailing pairs are 
context-dependent, such as a gap which might 
constitute a hazard in some particular contexts, 
even though these words do not entail each other 
in their general meanings. Such cases are more 
typical for the pattern-based approach, which is 
sometimes permissive with respect to the rela-
tionship captured and may also extract candi-
dates from a relatively small number of pattern 
occurrences. Second, synonyms tend to appear 
less frequently in patterns. Consequently, some 
synonymous pairs discovered by distributional 
similarity were rejected due to insufficient pat-
tern matches. Anecdotally, some typos and spell-
ing alternatives, like privatization ? 
privatisation, are also included in this category 
as they never co-occur in patterns. 
In addition, a large portion of errors is caused 
by pattern ambiguity. For example, the pattern 
"NP1, a|an NP2", ranked among the top IS-A pat-
terns by (Pantel et al, 2004), can represent both 
apposition (entailing) and a list of co-hyponyms 
(non-entailing). Finally, some misclassifications 
can be attributed to technical web-based process-
ing errors and to corpus data sparseness.  
 
Pattern-based Distributional 
abduction ? abuse assault ? abuse 
government ?  
organization 
government ?  
administration 
drug therapy ?  
treatment budget deficit ?gap 
gap ? hazard* broker ? analyst* 
management ? issue* government ?  parliament* 
Table 4: Typical entailment pairs acquired by the 
integrated method, illustrating Section 4.3. The 
columns specify the method that produced the 
candidate pair. Asterisk indicates a non-entailing 
pair. 
 
585
5 Conclusion 
The main contribution of this paper is a novel 
integration of the pattern-based and distributional 
approaches for lexical semantic acquisition, ap-
plied to lexical entailment. Our investigation 
highlights the complementary nature of the two 
approaches and the information they provide. 
Notably, it is possible to extract pattern-based 
information that complements the weaker evi-
dence of distributional similarity. Supervised 
learning was found effective for integrating the 
different information types, yielding noticeably 
improved performance. Indeed, our analysis re-
veals that the integrated approach helps eliminat-
ing many error cases typical to each method 
alone. We suggest that this line of research may 
be investigated further to enrich and optimize the 
learning processes and to address additional lexi-
cal relationships.  
 
Acknowledgement 
We wish to thank Google for providing us with 
an extended quota for search queries, which 
made this research feasible.  
 
References  
Berland, Matthew and Charniak, Eugene. 1999. Find-
ing parts in very large corpora. In Proc. of ACL-99. 
Maryland, USA. 
Chklovski, Timothy and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic 
Verb Relations. In Proc. of EMNLP-04. Barcelona, 
Spain. 
Dagan, Ido, Oren Glickman and Bernardo Magnini. 
2005. The PASCAL Recognizing Textual Entail-
ment Challenge. In Proc. of the PASCAL Chal-
lenges Workshop for Recognizing Textual 
Entailment. Southampton, U.K.  
Etzioni, Oren, M. Cafarella, D. Downey, S. Kok, A.-
M. Popescu, T. Shaked, S. Soderland, D.S. Weld, 
and A. Yates. 2004. Web-scale information extrac-
tion in KnowItAll. In Proc. of WWW-04. NY, 
USA. 
Geffet, Maayan and Ido Dagan. 2004. Feature Vector 
Quality and Distributional Similarity. In Proc. of 
COLING-04. Geneva, Switzerland. 
Geffet, Maayan and Ido Dagan. 2005. The Distribu-
tional Inclusion Hypothesis and Lexical Entail-
ment. In Proc of ACL-05. Michigan, USA. 
Harris, Zelig S. 1968. Mathematical Structures of 
Language. Wiley. 
Hearst, Marti. 1992. Automatic Acquisition of Hypo-
nyms from Large Text Corpora. In Proc. of 
COLING-92. Nantes, France. 
Joachims, Thorsten. 1999. Making large-Scale SVM 
Learning Practical. Advances in Kernel Methods - 
Support Vector Learning, B. Sch?lkopf and C. 
Burges and A. Smola (ed.), MIT-Press. 
Lin, Dekang. 1998. Automatic Retrieval and Cluster-
ing of Similar Words.  In Proc. of COLING?
ACL98, Montreal, Canada. 
Lin, Dekang, Shaojun Zhao, Lijuan Qin, and Ming 
Zhou. 2003. Identifying synonyms among distribu-
tionally similar words. In Proc. of  IJCAI-03. Aca-
pulco, Mexico. 
Pantel, Patrick, Deepak Ravichandran, and Eduard 
Hovy. 2004. Towards Terascale Semantic Acquisi-
tion. In Proc. of COLING-04. Geneva, Switzer-
land. 
Pantel, Patrick and Deepak Ravichandran. 2004. 
Automatically Labeling Semantic Classes. In Proc. 
of HLT/NAACL-04. Boston, MA. 
Ravichandran, Deepak and Eduard Hovy. 2002. 
Learning Surface Text Patterns for a Question An-
swering System. In Proc. of ACL-02. Philadelphia, 
PA. 
Riloff, Ellen and Jessica Shepherd. 1997. A corpus-
based approach for building semantic lexicons. In 
Proc. of EMNLP-97. RI, USA. 
Riloff, Ellen and Rosie Jones. 1999. Learning Dic-
tionaries for Information Extraction by Multi-Level 
Bootstrapping. In Proc. of AAAI-99. Florida, USA. 
Shinzato, Kenji and Kentaro Torisawa. 2004. Acquir-
ing Hyponymy Relations from Web Documents. In 
Proc. of HLT/NAACL-04. Boston, MA. 
Sundblad, H. Automatic Acquisition of Hyponyms 
and Meronyms from Question Corpora. 2002. In 
Proc. of the ECAI-02 Workshop on Natural Lan-
guage Processing and Machine Learning for On-
tology Engineering. Lyon, France. 
Weeds, Julie, David Weir, and Diana McCarthy. 
2004. Characterizing Measures of Lexical Distribu-
tional Similarity. In Proc. of COLING-04. Geneva, 
Switzerland. 
Yangarber, Roman, Ralph Grishman, Pasi Tapanainen 
and Silja Huttunen. 2000. Automatic Acquisition 
of Domain Knowledge for Information Extraction. 
In Proc. of COLING-00. Saarbr?cken, Germany. 
586
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 69?72,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Directional Distributional Similarity for Lexical Expansion
Lili Kotlerman, Ido Dagan, Idan Szpektor
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
lili.dav@gmail.com
{dagan,szpekti}@cs.biu.ac.il
Maayan Zhitomirsky-Geffet
Department of Information Science
Bar-Ilan University
Ramat Gan, Israel
maayan.geffet@gmail.com
Abstract
Distributional word similarity is most
commonly perceived as a symmetric re-
lation. Yet, one of its major applications
is lexical expansion, which is generally
asymmetric. This paper investigates the
nature of directional (asymmetric) similar-
ity measures, which aim to quantify distri-
butional feature inclusion. We identify de-
sired properties of such measures, specify
a particular one based on averaged preci-
sion, and demonstrate the empirical bene-
fit of directional measures for expansion.
1 Introduction
Much work on automatic identification of seman-
tically similar terms exploits Distributional Simi-
larity, assuming that such terms appear in similar
contexts. This has been now an active research
area for a couple of decades (Hindle, 1990; Lin,
1998; Weeds and Weir, 2003).
This paper is motivated by one of the prominent
applications of distributional similarity, namely
identifying lexical expansions. Lexical expansion
looks for terms whose meaning implies that of a
given target term, such as a query. It is widely
employed to overcome lexical variability in ap-
plications like Information Retrieval (IR), Infor-
mation Extraction (IE) and Question Answering
(QA). Often, distributional similarity measures are
used to identify expanding terms (e.g. (Xu and
Croft, 1996; Mandala et al, 1999)). Here we de-
note the relation between an expanding term u and
an expanded term v as ?u ? v?.
While distributional similarity is most promi-
nently modeled by symmetric measures, lexical
expansion is in general a directional relation. In
IR, for instance, a user looking for ?baby food?
will be satisfied with documents about ?baby pap?
or ?baby juice? (?pap ? food?, ?juice ? food?);
but when looking for ?frozen juice? she will not
be satisfied by ?frozen food?. More generally, di-
rectional relations are abundant in NLP settings,
making symmetric similarity measures less suit-
able for their identification.
Despite the need for directional similarity mea-
sures, their investigation counts, to the best of
our knowledge, only few works (Weeds and Weir,
2003; Geffet and Dagan, 2005; Bhagat et al,
2007; Szpektor and Dagan, 2008; Michelbacher et
al., 2007) and is utterly lacking. From an expan-
sion perspective, the common expectation is that
the context features characterizing an expanding
word should be largely included in those of the ex-
panded word.
This paper investigates the nature of directional
similarity measures. We identify their desired
properties, design a novel measure based on these
properties, and demonstrate its empirical advan-
tage in expansion settings over state-of-the-art
measures
1
. In broader prospect, we suggest that
asymmetric measures might be more suitable than
symmetric ones for many other settings as well.
2 Background
The distributional word similarity scheme follows
two steps. First, a feature vector is constructed
for each word by collecting context words as fea-
tures. Each feature is assigned a weight indicating
its ?relevance? (or association) to the given word.
Then, word vectors are compared by some vector
similarity measure.
1
Our directional term-similarity resource will be available
at http://aclweb.org/aclwiki/index.php?
title=Textual_Entailment_Resource_Pool
69
To date, most distributional similarity research
concentrated on symmetric measures, such as the
widely cited and competitive (as shown in (Weeds
and Weir, 2003)) LIN measure (Lin, 1998):
LIN(u, v) =
?
f?FV
u
?FV
v
[w
u
(f) + w
v
(f)]
?
f?FV
u
w
u
(f) +
?
f?FV
v
w
v
(f)
where FV
x
is the feature vector of a word x and
w
x
(f) is the weight of the feature f in that word?s
vector, set to their pointwise mutual information.
Few works investigated a directional similarity
approach. Weeds and Weir (2003) and Weeds et
al. (2004) proposed a precision measure, denoted
here WeedsPrec, for identifying the hyponymy re-
lation and other generalization/specification cases.
It quantifies the weighted coverage (or inclusion)
of the candidate hyponym?s features (u) by the hy-
pernym?s (v) features:
WeedsPrec(u ? v) =
?
f?FV
u
?FV
v
w
u
(f)
?
f?FV
u
w
u
(f)
The assumption behind WeedsPrec is that if one
word is indeed a generalization of the other then
the features of the more specific word are likely to
be included in those of the more general one (but
not necessarily vice versa).
Extending this rationale to the textual entail-
ment setting, Geffet and Dagan (2005) expected
that if the meaning of a word u entails that of
v then all its prominent context features (under
a certain notion of ?prominence?) would be in-
cluded in the feature vector of v as well. Their
experiments indeed revealed a strong empirical
correlation between such complete inclusion of
prominent features and lexical entailment, based
on web data. Yet, such complete inclusion cannot
be feasibly assessed using an off-line corpus, due
to the huge amount of required data.
Recently, (Szpektor and Dagan, 2008) tried
identifying the entailment relation between
lexical-syntactic templates using WeedsPrec, but
observed that it tends to promote unreliable rela-
tions involving infrequent templates. To remedy
this, they proposed to balance the directional
WeedsPrec measure by multiplying it with the
symmetric LIN measure, denoted here balPrec:
balPrec(u?v)=
?
LIN(u, v)?WeedsPrec(u?v)
Effectively, this measure penalizes infrequent tem-
plates having short feature vectors, as those usu-
ally yield low symmetric similarity with the longer
vectors of more common templates.
3 A Statistical Inclusion Measure
Our research goal was to develop a directional
similarity measure suitable for learning asymmet-
ric relations, focusing empirically on lexical ex-
pansion. Thus, we aimed to quantify most effec-
tively the above notion of feature inclusion.
For a candidate pair ?u ? v?, we will refer to
the set of u?s features, which are those tested for
inclusion, as tested features. Amongst these fea-
tures, those found in v?s feature vector are termed
included features.
In preliminary data analysis of pairs of feature
vectors, which correspond to a known set of valid
and invalid expansions, we identified the follow-
ing desired properties for a distributional inclusion
measure. Such measure should reflect:
1. the proportion of included features amongst
the tested ones (the core inclusion idea).
2. the relevance of included features to the ex-
panding word.
3. the relevance of included features to the ex-
panded word.
4. that inclusion detection is less reliable if the
number of features of either expanding or ex-
panded word is small.
3.1 Average Precision as the Basis for an
Inclusion Measure
As our starting point we adapted the Average
Precision (AP) metric, commonly used to score
ranked lists such as query search results. This
measure combines precision, relevance ranking
and overall recall (Voorhees and Harman, 1999):
AP =
?
N
r=1
[P (r) ? rel(r)]
total number of relevant documents
where r is the rank of a retrieved document
amongst the N retrieved, rel(r) is an indicator
function for the relevance of that document, and
P (r) is precision at the given cut-off rank r.
In our case the feature vector of the expanded
word is analogous to the set of all relevant docu-
ments while tested features correspond to retrieved
documents. Included features thus correspond to
relevant retrieved documents, yielding the follow-
70
ing analogous measure in our terminology:
AP (u ? v) =
?
|FV
u
|
r=1
[P (r) ? rel(f
r
)]
|FV
v
|
rel(f) =
{
1, if f ? FV
v
0, if f /? FV
v
P (r) =
|included features in ranks 1 to r|
r
where f
r
is the feature at rank r in FV
u
.
This analogy yields a feature inclusion measure
that partly addresses the above desired properties.
Its score increases with a larger number of in-
cluded features (correlating with the 1
st
property),
while giving higher weight to highly ranked fea-
tures of the expanding word (2
nd
property).
To better meet the desired properties we in-
troduce two modifications to the above measure.
First, we use the number of tested features |FV
u
|
for normalization instead of |FV
v
|. This captures
better the notion of feature inclusion (1
st
property),
which targets the proportion of included features
relative to the tested ones.
Second, in the classical AP formula all relevant
documents are considered relevant to the same ex-
tent. However, features of the expanded word dif-
fer in their relevance within its vector (3
rd
prop-
erty). We thus reformulate rel(f) to give higher
relevance to highly ranked features in |FV
v
|:
rel
?
(f) =
{
1 ?
rank(f,FV
v
)
|FV
v
|+1
, if f ? FV
v
0 , if f /? FV
v
where rank(f, FV
v
) is the rank of f in FV
v
.
Incorporating these twomodifications yields the
APinc measure:
APinc(u?v)=
?
|FV
u
|
r=1
[P (r) ? rel
?
(f
r
)]
|FV
u
|
Finally, we adopt the balancing approach in
(Szpektor and Dagan, 2008), which, as explained
in Section 2, penalizes similarity for infrequent
words having fewer features (4
th
property) (in our
version, we truncated LIN similarity lists after top
1000 words). This yields our proposed directional
measure balAPinc:
balAPinc(u?v) =
?
LIN(u, v) ? APinc(u?v)
4 Evaluation and Results
4.1 Evaluation Setting
We tested our similarity measure by evaluating its
utility for lexical expansion, compared with base-
lines of the LIN, WeedsPrec and balPrec measures
(Section 2) and a balanced version of AP (Sec-
tion 3), denoted balAP. Feature vectors were cre-
ated by parsing the Reuters RCV1 corpus and tak-
ing the words related to each term through a de-
pendency relation as its features (coupled with the
relation name and direction, as in (Lin, 1998)). We
considered for expansion only terms that occur at
least 10 times in the corpus, and as features only
terms that occur at least twice.
As a typical lexical expansion task we used
the ACE 2005 events dataset
2
. This standard IE
dataset specifies 33 event types, such as Attack,
Divorce, and Law Suit, with all event mentions
annotated in the corpus. For our lexical expan-
sion evaluation we considered the first IE subtask
of finding sentences that mention the event.
For each event we specified a set of representa-
tive words (seeds), by selecting typical terms for
the event (4 on average) from its ACE definition.
Next, for each similarity measure, the terms found
similar to any of the event?s seeds (?u ? seed?)
were taken as expansion terms. Finally, to mea-
sure the sole contribution of expansion, we re-
moved from the corpus all sentences that contain
a seed word and then extracted all sentences that
contain expansion terms as mentioning the event.
Each of these sentences was scored by the sum of
similarity scores of its expansion terms.
To evaluate expansion quality we compared the
ranked list of sentences for each event to the gold-
standard annotation of event mentions, using the
standard Average Precision (AP) evaluation mea-
sure. We report Mean Average Precision (MAP)
for all events whose AP value is at least 0.1 for at
least one of the tested measures
3
.
4.1.1 Results
Table 1 presents the results for the different tested
measures over the ACE experiment. It shows that
the symmetric LIN measure performs significantly
worse than the directional measures, assessing that
a directional approach is more suitable for the ex-
pansion task. In addition, balanced measures con-
sistently perform better than unbalanced ones.
According to the results, balAPinc is the best-
performing measure. Its improvement over all
other measures is statistically significant accord-
ing to the two-sided Wilcoxon signed-rank test
2
http://projects.ldc.upenn.edu/ace/, training part.
3
The remaining events seemed useless for our compar-
ative evaluation, since suitable expansion lists could not be
found for them by any of the distributional methods.
71
LIN WeedsPrec balPrec AP balAP balAPinc
0.068 0.044 0.237 0.089 0.202 0.312
Table 1: MAP scores of the tested measures on the
ACE experiment.
seed LIN balAPinc
death murder, killing, inci-
dent, arrest, violence
suicide, killing, fatal-
ity, murder, mortality
marry divorce, murder, love, divorce, remarry,
dress, abduct father, kiss, care for
arrest detain, sentence,
charge, jail, convict
detain, extradite,
round up, apprehend,
imprison
birth abortion, pregnancy, wedding day,
resumption, seizure, dilation, birthdate,
passage circumcision, triplet
injure wound, kill, shoot, wound, maim, beat
detain, burn up, stab, gun down
Table 2: Top 5 expansion terms learned by LIN
and balAPinc for a sample of ACE seed words.
(Wilcoxon, 1945) at the 0.01 level. Table 2
presents a sample of the top expansion terms
learned for some ACE seeds with either LIN or
balAPinc, demonstrating the more accurate ex-
pansions generated by balAPinc. These results
support the design of our measure, based on the
desired properties that emerged from preliminary
data analysis for lexical expansion.
Finally, we note that in related experiments we
observed statistically significant advantages of the
balAPincmeasure for an unsupervised text catego-
rization task (on the 10 most frequent categories in
the Reuters-21578 collection). In this setting, cat-
egory names were taken as seeds and expanded by
distributional similarity, further measuring cosine
similarity with categorized documents similarly to
IR query expansion. These experiments fall be-
yond the scope of this paper and will be included
in a later and broader description of our work.
5 Conclusions and Future work
This paper advocates the use of directional similar-
ity measures for lexical expansion, and potentially
for other tasks, based on distributional inclusion of
feature vectors. We first identified desired proper-
ties for an inclusion measure and accordingly de-
signed a novel directional measure based on av-
eraged precision. This measure yielded the best
performance in our evaluations. More generally,
the evaluations supported the advantage of multi-
ple directional measures over the typical symmet-
ric LIN measure.
Error analysis showed that many false sentence
extractions were caused by ambiguous expanding
and expanded words. In future work we plan to
apply disambiguation techniques to address this
problem. We also plan to evaluate the performance
of directional measures in additional tasks, and
compare it with additional symmetric measures.
Acknowledgements
This work was partially supported by the NEGEV
project (www.negev-initiative.org), the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886 and by the Israel
Science Foundation grant 1112/08.
References
R. Bhagat, P. Pantel, and E. Hovy. 2007. LEDIR: An
unsupervised algorithm for learning directionality of
inference rules. In Proceedings of EMNLP-CoNLL.
M. Geffet and I. Dagan. 2005. The distributional in-
clusion hypotheses and lexical entailment. In Pro-
ceedings of ACL.
D. Hindle. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL.
R. Mandala, T. Tokunaga, and H. Tanaka. 1999. Com-
bining multiple evidence from different types of the-
saurus for query expansion. In Proceedings of SI-
GIR.
L. Michelbacher, S. Evert, and H. Schutze. 2007.
Asymmetric association measures. In Proceedings
of RANLP.
I. Szpektor and I. Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of COL-
ING.
E. M. Voorhees and D. K. Harman, editors. 1999. The
Seventh Text REtrieval Conference (TREC-7), vol-
ume 7. NIST.
J. Weeds and D. Weir. 2003. A general framework for
distributional similarity. In Proceedings of EMNLP.
J. Weeds, D. Weir, and D. McCarthy. 2004. Character-
ising measures of lexical distributional similarity. In
Proceedings of COLING.
F. Wilcoxon. 1945. Individual comparisons by ranking
methods. Biometrics Bulletin, 1:80?83.
J. Xu and W. B. Croft. 1996. Query expansion using
local and global document analysis. In Proceedings
of SIGIR.
72
Feature Vector Quality and Distributional Similarity 
Maayan Geffet 
School of Computer Science and Engineering, 
Hebrew University  
Givat Ram Campus, 
Jerusalem, Israel, 91904 
mary@cs.huji.ac.il 
Ido Dagan 
Department of Computer Science,  
Bar-Ilan University 
Ramat-Gan, Israel, 52900 
 
dagan@cs.biu.ac.il 
 
Abstract 
We suggest a new goal and evaluation criterion for 
word similarity measures. The new criterion - 
meaning-entailing substitutability - fits the needs 
of semantic-oriented NLP applications and can be 
evaluated directly (independent of an application) 
at a good level of human agreement. Motivated by 
this semantic criterion we analyze the empirical 
quality of distributional word feature vectors and 
its impact on word similarity results, proposing an 
objective measure for evaluating feature vector 
quality. Finally, a novel feature weighting and se-
lection function is presented, which yields superior 
feature vectors and better word similarity perform-
ance.  
1 Introduction 
    Distributional Similarity has been an active re-
search area for more than a decade (Hindle, 1990), 
(Ruge, 1992), (Grefenstette, 1994), (Lee, 1997), 
(Lin, 1998), (Dagan et al, 1999), (Weeds and 
Weir, 2003). Inspired by Harris distributional hy-
pothesis (Harris, 1968), similarity measures com-
pare a pair of weighted feature vectors that 
characterize two words. Features typically corre-
spond to other words that co-occur with the charac-
terized word in the same context. It is then 
assumed that different words that occur within 
similar contexts are semantically similar. 
As it turns out, distributional similarity captures 
a somewhat loose notion of semantic similarity 
(see Table 1). By construction, if two words are 
distributionally similar then the occurrence of one 
word in some contexts indicates that the other 
word is also likely to occur in such contexts. But it 
does not ensure that the meaning of the first word 
is preserved when replacing it with the other one in 
the given context. For example, words of similar 
semantic types, such as company ? government, 
tend to come up as distributionally similar, even 
though they are not substitutable in a meaning pre-
serving sense. 
On the other hand, many semantic-oriented appli-
cations, such as Question Answering, Paraphrasing 
and Information Extraction, do need to recognize 
which words may substitute each other in a mean-
ing preserving manner. For example, a question 
about company may be answered by a sentence 
about firm, but not about government. Such appli-
cations usually utilize reliable taxonomies or on-
tologies like WordNet, but cannot rely on the 
?loose? type of output of distributional similarity 
measures. 
In recent work Dagan and Glickman (2004) ob-
serve that applications usually do not require a 
strict meaning preserving criterion between text 
expressions, but rather need to recognize that the 
meaning of one expression entails the other. En-
tailment modeling is thus proposed in their work as 
a generic (application-independent) framework for 
practical semantic inference. We suggest adopting 
such (directional) entailment criterion at the lexical 
level for judging whether one word can be substi-
tuted by another one.  For example, certain ques-
tions about companies might be answered by 
sentences about automakers, since the meaning of 
automaker entails the meaning of company (though 
not vice versa). In this paper we adapt this new 
criterion, termed meaning entailing substitutability, 
as a direct evaluation criterion for the "correctness" 
of the output of word similarity measures (as op-
posed to indirect evaluations through WSD or dis-
tance in WordNet).  
Our eventual research goal is improving word 
similarity measures to predict better the more deli-
cate meaning entailment relationship between 
words. As a first step it was necessary to analyze 
the typical behavior of current similarity measures 
and categorize their errors (Section 3). Our main 
observation is that the quality of similarity scores 
 nation 
 region 
 state 
 *world 
 island 
 province 
 1 
 2 
 3 
 4 
 5 
 6 
 *city 
 territory 
 area 
 *town 
 republic 
 african_country 
 
 7 
 8 
 9 
 10 
 11 
 12 
  
 *north 
 *economy 
 *neighbor 
 *member 
 *party 
 *government 
 
 13 
 14 
 15 
 16 
 17 
 18 
 *company 
 *industry 
 kingdom 
 european_country 
 place 
 colony 
 19 
 20 
 25 
 31 
 36 
 41 
 
Table 1: The 20 top most similar words of country (and their ranks) in the similarity list by Lin98, 
followed by the next 4 words in the similarity list that are judged as correct. Incorrect similarities, 
under the substitutability criterion, are marked with ?*?.  
is often hurt by improper feature weights, which 
yield rather noisy feature vectors. We quantify this 
problem by a new measure for feature vector qual-
ity, which is independent of any particular vector 
similarity measure. 
To improve feature vector quality a novel fea-
ture weighting function is introduced, called rela-
tive feature focus (RFF) (Section 4). While having 
a simple (though non-standard) definition, this 
function yields improved performance relative to 
the two suggested evaluation criteria ? for vector 
quality and for word similarity. The underlying 
idea is that a good characteristic feature for a word 
w should characterize also multiple words that are 
highly similar to w. In other words, such feature 
should have a substantial "focus" within the close 
semantic vicinity of w.   
Applying RFF weighting achieved about 10% 
improvement in predicting meaning entailing sub-
stitutability (Section 5). Further analysis shows 
that RFF also leads to "cleaner" characteristic fea-
ture vectors, which may be useful for additional 
feature-based tasks like clustering. 
2 Background and Definitions 
   In the distributional similarity scheme each word 
w is represented by a feature vector, where an entry 
in the vector corresponds to a feature f. Each fea-
ture represents another word (or term) with which 
w co-occurs, and possibly specifies also the syntac-
tic relation between the two words. The value of 
each entry is determined by some weight function 
weight(w,f), which quantifies the degree of statisti-
cal association between the feature and the corre-
sponding word. 
Typical feature weighting functions include the 
logarithm of the frequency of word-feature co-
occurrence (Ruge, 1992), and the conditional prob-
ability of the feature given the word (within prob-
abilistic-based measures) (Pereira et al, 1993), 
(Lee, 1997), (Dagan et al, 1999). Probably the 
most widely used association weight function is 
(point-wise) Mutual Information (MI) (Church et 
al., 1990), (Hindle, 1990), (Lin, 1998), (Dagan, 
2000), defined by: 
)()(
),(log),( 2 fPwP
fwPfwMI =  
A known weakness of MI is its tendency to assign 
high weights for rare features. Yet, similarity 
measures that utilize MI showed good perform-
ance. In particular, a common practice is to filter 
out features by minimal frequency and weight 
thresholds. A word's vector is then constructed 
from the remaining features, which we call here 
active features.  
Once feature vectors have been constructed, the 
similarity between two words is defined by some 
vector similarity metric. Different metrics have 
been used in the above cited papers, such as 
Weighted Jaccard (Dagan, 2000), cosine (Ruge, 
1992), various information theoretic measures 
(Lee, 1997), and others. We picked the widely 
cited and competitive (e.g. (Weeds and Weir, 
2003)) measure of Lin (1998) as a representative 
case, and utilized it for our analysis and as a start-
ing point for improvement. 
2.1 Lin's (?98) Similarity Measure  
    Lin's similarity measure between two words, w 
and v, is defined as follows: 
,),(),(
),(),(
),(
)()(
)()(
? ?
?
??
??
+
+
=
fvweightfwweight
fvweightfwweight
vwsim
vFfwFf
vFwFf  
where F(w) and F(v) are the active features of the 
two words and the weight function is defined as 
MI. A feature is defined as a pair <term, syntac-
Country- 
State 
Ranks Country-
Economy 
Ranks 
Broadcast 
Goods 
Civil_servant 
Bloc 
Nonaligned 
Neighboring 
Statistic 
Border 
Northwest 
 24 
 140 
 64 
 30 
 55 
 15 
 165 
 10 
 41 
 50 
 16 
 54 
 77 
 60 
 165 
 43 
 247 
 174 
Devastate 
Developed 
Dependent 
Industrialized 
Shattered 
Club 
Black 
Million 
Electricity 
 81 
 36 
 101 
 49 
 16 
 155 
 122 
 31 
 130 
 8 
 78 
 26 
 85 
 141 
 38 
 109 
 245 
 154 
 
Table 3: The top-10 common features for the  
word pairs country-state and country-economy, 
along with their corresponding ranks in the 
sorted feature lists of the two words.  
 Feature  Weight 
 Commercial-bank, gen ?  
 Destination, pcomp ?  
 Airspace, pcomp ?  
 Landlocked, mod ?  
 Trade_balance, gen ?  
 Sovereignty, pcomp ?  
 Ambition , nn ?  
 Bourse, gen ?  
 Politician, gen ?  
 Border, pcomp ?  
 8.08  
 7.9 7  
 7.83 
 7.79 
 7.78 
 7.78 
 7.77 
 7.72 
 7.54 
 7.53        
 
Table 2: The top-10 ranking features for 
country.  
tic_relation>. For example, given the word ?com-
pany? the feature <earnings_report, gen?> (geni-
tive) corresponds to the phrase ?company?s 
earnings report?, and <profit, pcomp?> (preposi-
tional complement) corresponds to ?the profit of 
the company?. The syntactic relations are gener-
ated by the Minipar dependency parser (Lin, 
1993). The arrows indicate the direction of the syn-
tactic dependency: a downward arrow indicates 
that the feature is the parent of the target word, and 
the upward arrow stands for the opposite. 
In our implementation we filtered out features 
with overall frequency lower than 10 in the corpus 
and with MI weights lower than 4. (In the tuning 
experiments the filtered version showed 10% im-
provement in precision over no feature filtering.) 
From now on we refer to this implementation as 
Lin98.  
3 Empirical Analysis of Lin98 and 
Vector Quality Measure 
    To gain better understanding of distributional 
similarity we first analyzed the empirical behavior 
of Lin98, as a representative case for state of the 
art (see Section 5.1 for corpus details). 
As mentioned in the Introduction, distributional 
similarity may not correspond very tightly to 
meaning entailing substitutability. Under this 
judgment criterion two main types of errors occur: 
(1) word pairs that are of similar semantic types, 
but are not substitutable, like firm and government; 
and (2) word pairs that are of different semantic 
types, like firm and contract, which might (or 
might not) be related only at a topical level. Table 
1 shows the top most similar words for the target 
word country according to Lin98 .  The two error 
types are easily recognized, e.g. world and city for 
the first type, and economy for the second. 
A deeper look at the word feature vectors re-
veals typical reasons for such errors. In many 
cases, high ranking features in a word vector, when 
sorting the features by their weight, do not seem 
very characteristic for the word meaning. This is 
demonstrated in Table 2, which shows the top-10 
features in the vector of country. As can be seen, 
some of the top features are either too specific 
(landlocked, airspace), and so are less reliable, or 
too general (destination, ambition), and hence not 
indicative and may co-occur with many different 
types of words. On the other hand, more character-
istic features, like population and governor, occur 
further down the list, at positions 461 and 832. 
Overall, features that characterize well the word 
meaning are scattered across the ranked list, while 
many non-indicative features receive high weights. 
This may yield high similarity scores for less simi-
lar word pairs, while missing other correct similari-
ties. 
An objective indication of the problematic fea-
ture ranking is revealed by examining the common 
features that contribute mostly to the similarity 
score of a pair of similar words. We look at the 
common features of the two words and sort them 
by the sum of their weights in the two word vectors 
(which is the enumerator of Lin's sim formula in 
Section 2.1). Table 3 shows the top-10 common 
features for a pair of substitutable words (country - 
state) and non-substitutable words (country - econ-
omy). In both cases the common features are scat-
tered across each feature vector, making it difficult 
to distinguish between similar and non-similar 
word pairs.  
We suggest that the desired behavior of feature 
ranking is that the common features of truly similar 
words will be concentrated at the top ranks of their 
vectors. The common features for non-similar 
words are expected to be scattered all across each 
of the vectors. More formally, given a pair of simi-
lar words (judged as substitutable) w and v we de-
fine the top joint feature rank criterion for 
evaluating feature vector quality:  
   
],),(),([
2
11
),,(
))()((? +
=?
??? fvrankfwrank
n
nvwranktop
vFwFntopf
 
where rank(w,f) is the feature?s position in the 
sorted vector of the word w, and n is the number of 
top joint features to consider (top-n), when sorted 
by the sum of their weights in the two word vec-
tors. We thus expect that a good weighting func-
tion would yield (on average) a low top-rank score 
for truly similar words.  
4 Relative Feature Focus (RFF) 
    Motivated by the observations above we propose 
a new feature weight function, called relative fea-
ture focus (RFF). The basic idea is to promote fea-
tures which characterize many words that are 
highly similar to w. These features are considered 
as having a strong "focus" around w's meaning. 
Features which do not characterize sufficiently 
many words that are sufficiently similar to w are 
demoted. Even if such features happen to have a 
strong direct association with w they are not con-
sidered reliable, as they do not have sufficient sta-
tistical mass in w's semantic vicinity. 
4.1 RFF Definition 
 RFF is defined as follows. First, a standard 
word similarity measure sim is computed to obtain 
initial approximation of the similarity space (Lin98 
was used in this work). Then, we define the word 
set of a feature f, denoted by WS(f), as the set of 
words for which f is an active feature. The seman-
tic neighborhood of w, denoted by N(w), is defined 
as the set of all words v which are considered suf-
ficiently similar to w, satisfying  sim(w,v)>s where 
s is a threshold  (0.04 in our experiments). RFF is 
then defined by: 
? ??= ),(),( )()( vwsimfwRFF wNfWSv . 
That is, we identify all words v that are in the se-
mantic neighborhood of w and are also character-
ized by f and sum their similarities to w. 
Notice that RFF is a sum of word similarity val-
ues rather than being a direct function of word-
feature association values (which is the more com-
mon approach). It thus does not depend on the ex-
act co-occurrence level between w and f. Instead, it 
depends on a more global assessment of the asso-
ciation between f and the semantic vicinity of w. 
Unlike the entropy measure, used in (Grefenstette, 
1994), our "focused" global view is relative to 
each individual word w and is not a global inde-
pendent function of the feature. 
We notice that summing the above similarity 
values captures simultaneously a desired balance 
between feature specificity and generality, address-
ing the observations in Section 3. Some features 
might characterize just a single word that is very 
similar to w. But then the sum of similarities will 
include a single element, yielding a relatively low 
weight.1 General features may characterize more 
words within N(f), but then on average the similar-
ity with w over multiple words is likely to become 
lower, contributing smaller values to the sum. A 
reliable feature has to characterize multiple words 
(not too specific) that are highly similar to w (not 
too general).  
4.2 Re-computing Similarities  
Once RFF weights have been computed they are 
sufficiently accurate to allow for aggressive feature 
reduction. In our experiments it sufficed to use 
only the top 100 features for each word in order to 
obtain optimal results, since the most informative 
features now have the highest weights. Similarity 
between words is then recomputed over the re-
duced vectors using Lin's sim function (in Section 
2.1), with RFF replacing MI as the new weight 
function. 
 
 
 
 
 
                                                          
1
 This is why the sum of similarities is used rather than 
an average value, which might become too high by 
chance when computed over just a single element (or 
very few elements). 
#Words Judge 1 (%) Judge 2 (%) Total (%) 
Top 10 63.4 / 54.1 62.6 / 53.4 63.0 / 53.7 
Top 20 57.0 / 48.3 56.4 / 45.8 56.8 / 47.0 
Top 30 55.3 / 45.1 53.3 / 43.4 54.2 / 44.2 
Top 40 53.5 / 44.6 51.6 / 42.0 52.6 / 43.3 
 
Table 4: Precision values for Top-N similar 
words by the RFF / Lin98 methods. 
5 Evaluation 
5.1 Experimental Setting 
    The performance of the RFF-based similarity 
measure was evaluated for a sample of nouns and 
compared with that of Lin98. The experiment was 
conducted using an 18 million tokens subset of the 
Reuters RCV1 corpus,2 parsed by Lin?s Minipar 
dependency parser (Lin, 1993). We considered first 
an evaluation based on WordNet data as a gold 
standard, as in (Lin, 1998; Weeds and Weir, 2003). 
However, we found that many word pairs from the 
Reuters Corpus that are clearly substitutable are 
not linked appropriately in WordNet. 
We therefore conducted a manual evaluation 
based on the judgments of two human subjects. 
The judgment criterion follows common evalua-
tions of paraphrase acquisition (Lin and Pantel, 
2001), (Barzilay and McKeown, 2001), and corre-
sponds to the meaning-entailing substitutability 
criterion discussed in Section 1. Two words are 
judged as substitutable (correct similarity) if there 
are some contexts in which one of the words can 
be substituted by the other, such that the meaning 
of the original word can be inferred from the new 
one. 
Typically substitutability corresponds to certain 
ontological relations. Synonyms are substitutable 
in both directions. For example, worker and em-
ployee entail each other's meanings, as in the con-
text ?high salaried worker/employee?. Hyponyms 
typically entail their hypernyms. For example, dog 
entails animal, as in ?I have a dog? which entails 
?I have an animal? (but not vice versa). In some 
cases part-whole and member-set relations satisfy 
the meaning-entailing substitutability criterion. For 
example, a discussion of division entails in many 
contexts the meaning of company. Similarly, the 
plural form of employee(s) often entails the mean-
ing of staff. On the other hand, non-synonymous 
words that share a common hypernym (co-
hyponyms) like company and government, or 
country and city, are not substitutable since they 
always refer to different meanings (such as differ-
ent entities). 
Our test set included a sample of 30 randomly 
selected nouns whose corpus frequency is above 
                                                          
2
 Known as Reuters Corpus, Volume 1, English Lan-
guage, 1996-08-20 to 1997-08-19. 
500. For each noun we computed the top 40 most 
similar words by both similarity measures, yielding 
a total set of about 1600 (different) suggested word 
similarity pairs. Two independent assessors were 
assigned, each judging half of the test set (800 
pairs). The output pairs from both methods were 
mixed so the assessor could not relate a pair with 
the method that suggested it. 
5.2 Similarity Results 
    The evaluation results are displayed in Table 4. 
As can be seen RFF outperformed Lin98 by 9-10 
percentage points of precision at all Top-N levels, 
by both judges. Overall, RFF extracted 111 (21%) 
more correct similarity pairs than Lin98.  The 
overall relative recall3 of RFF is quite high (89%), 
exceeding Lin98 by 16% (73%). These figures in-
dicate that our method covers most of the correct 
similarities found by Lin98, while identifying 
many additional correct pairs. 
We note that the obtained precision values for 
both judges are very close at all table rows. To fur-
ther assess human agreement level for this task the 
first author of this paper judged two samples of 
100 word pairs each, which were selected ran-
domly from the two test sets of the original judges. 
The proportions of matching decisions between the 
author's judgments and the original ones were 
91.3% (with Judge 1) and 88.9% (with Judge 2). 
The corresponding Kappa values are 0.83 (?very 
good agreement?) and 0.75 (?good agreement?).  
As for feature reduction, vector sizes were re-
duced on average to about one third of their origi-
nal size in the Lin98 method (recall that standard 
feature reduction, tuned for the corpus, was already 
applied to the Lin98 vectors). 
 
 
                                                          
3
 Relative recall shows the percentage of correct word 
similarities found by each method relative to the joint 
set of similarities that were extracted by both methods. 
 Feature Weight 
Industry, gen ?  
Airport, gen  ?  
Neighboring, mod ?  
Law, gen ?  
Economy, gen ?  
Population, gen ?  
City, gen ?  
Impoverished, mod ?  
Governor, pcomp ?  
Parliament, gen ?  
 1.21 
 1.16 
 1.06  
 1.04 
 1.02 
 1.02 
 0.93 
 0.92 
 0.92 
 0.91 
 
     Table 5: Top-10 features of country by  
                    RFF. 
5.3 Empirical Observations for RFF  
    We now demonstrate the typical behavior of 
RFF relative to the observations and motivations 
of Section 3 (through the same example).  
Table 5 shows the top-10 features of country. 
We observe (subjectively) that the list now con-
tains quite indicative and reliable features, where 
too specific (anecdotal) and too general features 
were demoted (compare with Table 2).  
More objectively, Table 6 shows that most of 
the top-10 common features for country-state are 
now ranked highly for both words. On the other 
hand, there are only two common features for the 
incorrect pair country-economy, both with quite 
low ranks (compare with Table 3). Overall, given 
the set of all the correct (judged as substitutable) 
word similarities produced by both methods, the 
average top joint feature rank of the top-10 com-
mon features by RFF is 21, satisfying the desired 
behavior which was suggested in Section 3. The 
same figure is much larger for the Lin98 vectors, 
which have an average top joint feature rank of 
105.  
Consequently, Table 7 shows a substantial im-
provement in the similarity list for country, where 
most incorrect words, like economy and company, 
disappeared. Instead, additional correct similari-
ties, like kingdom and land, were promoted (com-
pare with Table 1). Some semantically related but 
non-substitutable words, like ?world? and ?city?, 
still remain in the list, but somewhat demoted. In 
this case all errors correspond to quite close se-
mantic relatedness, being geographic concepts. 
The remaining errors are mostly of the first type 
discussed in Section 3 ? pairs of words that are 
ontologically or thematically related but are not 
substitutable. Typical examples are co-hyponyms 
(country - city) or agent-patient and agent-action 
pairs (industry ? product, worker ? job). Usually, 
such word pairs also have highly ranked common 
features since they naturally appear with similar 
characteristic features. It may therefore be difficult 
to filter out such non-substitutable similarities 
solely by the standard distributional similarity 
scheme, suggesting that additional mechanisms are 
required. 
6 Conclusions and Future Work 
    This paper proposed the following contributions:  
 
1. Considering meaning entailing substitutability 
as a target goal and evaluation criterion for word 
similarity. This criterion is useful for many seman-
tic-oriented NLP applications, and can be evalu-
ated directly by human subjects.  
2. A thorough empirical error analysis of state of 
the art performance was conducted. The main ob-
servation was deficient quality of the feature vec-
tors which reduces the quality of similarity 
measures.    
3. Inspired by the qualitative observations we iden-
tified a new qualitative condition for feature vector 
evaluation ? top joint feature rank. Thus, feature 
vector quality can be measured independently of 
the final similarity output.  
4. Finally, we presented a novel feature weighting 
function, relative feature focus. This measure was 
designed based on error analysis insights and im-
Country-
State 
Ranks Country- 
Economy 
Ranks 
Neighboring 
Industry 
Impoverished 
Governor 
Population 
City 
Economy 
Parliament 
Citizen 
Law 
 3 
 1 
 8 
 10 
 6 
 17 
 5 
 10 
 14 
 4 
 1 
 11 
 8 
 9 
 16 
 18 
 15 
 22 
 25 
 33 
Developed  
Liberalization 
50 
100 
 100 
 79 
 
Table 6: RFF weighting: Top-10 common 
features for country-state and country-
economy along with their corresponding 
ranks in the two (sorted) feature vectors. 
proves performance over all the above criteria. 
    We intend to further investigate the contribution 
of our measure to word sense disambiguation and 
to evaluate its performance for clustering methods. 
Error analysis suggests that it might be difficult 
to improve similarity output further within the 
common distributional similarity schemes. We 
need to seek additional criteria and data types, such 
as identifying evidence for non-similarity, or ana-
lyzing more carefully disjoint features. 
Further research is suggested to extend the 
learning framework towards richer notions of on-
tology generation. We would like to distinguish 
between different ontological relationships that 
correspond to the substitutability criterion, such as 
identifying the entailment direction, which was 
ignored till now. Towards these goals we plan to 
investigate combining unsupervised distributional 
similarity with supervised methods for learning 
ontological relationships, and with paraphrase ac-
quisition methods. 
References  
Barzilay, Regina and Kathleen McKeown. 2001. 
Extracting Paraphrases from a Parallel Corpus. 
In Proc. of ACL/EACL, 2001.  
Church, Kenneth W. and Hanks Patrick. 1990. 
Word association norms, mutual information, 
and Lexicography. Computational Linguistics, 
16(1), pp. 22?29. 
Dagan, Ido. 2000. Contextual Word Similarity, in 
Rob Dale, Hermann Moisl and Harold Somers 
(Eds.), Handbook of Natural Language Process-
ing, Marcel Dekker Inc, 2000, Chapter 19, pp. 
459-476.  
Dagan, Ido and Oren Glickman. 2004. Probabilis-
tic Textual Entailment: Generic Applied Model-
ing of Language Variability. PASCAL 
Workshop on Text Understanding and Mining. 
Dagan, Ido, Lillian Lee and Fernando Pereira. 
1999. Similarity-based models of cooccurrence 
probabilities. Machine Learning, 1999, Vol. 
 34(1-3), special issue on Natural Language 
Learning, pp. 43-69.  
Grefenstette, Gregory. 1994. Exploration in Auto-
matic Thesaurus Discovery. Kluwer Academic 
Publishers. 
Harris, Zelig S. 1968. Mathematical structures of 
language. Wiley, 1968. 
Hindle, D. 1990. Noun classification from predi-
cate-argument structures. In Proc. of ACL, pp. 
268?275.  
Lee, Lillian. 1997. Similarity-Based Approaches to 
Natural Language Processing. Ph.D. thesis, Har-
vard University, Cambridge, MA.   
Lin, Dekang. 1993. Principle-Based Parsing with-
out Overgeneration. In Proc. of ACL-93, pages 
112-120, Columbus, Ohio, 1993. 
Lin, Dekang. 1998. Automatic Retrieval and Clus-
tering of Similar Words.  In Proc. of COLING?
ACL98, Montreal, Canada, August, 1998. 
Lin, Dekang and Patrick Pantel. 2001. Discovery 
of Inference Rules for Question Answering. 
Natural Language Engineering 7(4), pp. 343-
360, 2001. 
Pereira, Fernando, Tishby Naftali, and Lee Lillian. 
1993. Distributional clustering of English 
words. In Proc. of ACL-93, pp. 183?190.  
Ruge,  Gerda.  1992.  Experiments on linguisti-
cally-based term associations. Information 
Processing & Management, 28(3), pp. 317?332. 
Weeds, Julie and David Weir. 2003. A General 
Framework for Distributional Similarity. In 
Proc. of EMNLP-03. Spain. 
 
 nation 
 state 
 island 
 region 
 area 
 1 
 2 
 3 
 4 
 5 
 territory  
 *neighbor 
 colony 
 *port  
 republic 
 
 6 
 7 
 8 
 9 
 10 
  
 african_country 
 province  
 *city 
 *town 
 kingdom 
 
 11 
 12  
 13 
 14 
 15 
 *district 
 european_country 
 zone  
 land 
 place 
 16 
 17 
 18 
 19 
 20 
 
Table 7: Top-20 most similar words for country and their ranks in the similarity list by the 
RFF-based measure. Incorrect similarities (non-substitutable) are marked with ?*?.  
Proceedings of the 43rd Annual Meeting of the ACL, pages 107?114,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
The Distributional Inclusion Hypotheses and Lexical Entailment 
 
Maayan Geffet 
School of Computer Science and Engineering 
Hebrew University, Jerusalem, Israel, 91904 
mary@cs.huji.ac.il 
Ido Dagan 
Department of Computer Science 
Bar-Ilan University, Ramat-Gan, Israel, 52900 
dagan@cs.biu.ac.il 
 
Abstract 
This paper suggests refinements for the 
Distributional Similarity Hypothesis. Our 
proposed hypotheses relate the distribu-
tional behavior of pairs of words to lexical 
entailment ? a tighter notion of semantic 
similarity that is required by many NLP 
applications. To automatically explore the 
validity of the defined hypotheses we de-
veloped an inclusion testing algorithm for 
characteristic features of two words, which 
incorporates corpus and web-based feature 
sampling to overcome data sparseness. The 
degree of hypotheses validity was then em-
pirically tested and manually analyzed with 
respect to the word sense level. In addition, 
the above testing algorithm was exploited 
to improve lexical entailment acquisition. 
1 Introduction 
Distributional Similarity between words has been 
an active research area for more than a decade. It is 
based on the general idea of Harris' Distributional 
Hypothesis, suggesting that words that occur 
within similar contexts are semantically similar 
(Harris, 1968). Concrete similarity measures com-
pare a pair of weighted context feature vectors that 
characterize two words (Church and Hanks, 1990; 
Ruge, 1992; Pereira et al, 1993; Grefenstette, 
1994; Lee, 1997; Lin, 1998; Pantel and Lin, 2002; 
Weeds and Weir, 2003). 
    As it turns out, distributional similarity captures 
a somewhat loose notion of semantic similarity 
(see Table 1). It does not ensure that the meaning 
of one word is preserved when replacing it with 
the other one in some context. 
However, many semantic information-oriented 
applications like Question Answering, Information 
Extraction and Paraphrase Acquisition require a 
tighter similarity criterion, as was also demon-
strated by papers at the recent PASCAL Challenge 
on Recognizing Textual Entailment (Dagan et al, 
2005). In particular, all these applications need to 
know when the meaning of one word can be in-
ferred (entailed) from another word, so that one 
word could substitute the other in some contexts. 
This relation corresponds to several lexical seman-
tic relations, such as synonymy, hyponymy and 
some cases of meronymy. For example, in Ques-
tion Answering, the word company in a question 
can be substituted in the text by firm (synonym), 
automaker (hyponym) or division (meronym). Un-
fortunately, existing manually constructed re-
sources of lexical semantic relations, such as 
WordNet, are not exhaustive and comprehensive 
enough for a variety of domains and thus are not 
sufficient as a sole resource for application needs1. 
    Most works that attempt to learn such concrete 
lexical semantic relations employ a co-occurrence 
pattern-based approach (Hearst, 1992; Ravi-
chandran and Hovy, 2002; Moldovan et al, 2004). 
Typically, they use a set of predefined lexico-
syntactic patterns that characterize specific seman-
tic relations. If a candidate word pair (like com-
pany-automaker) co-occurs within the same 
sentence satisfying a concrete pattern (like " 
?companies, such as automakers"), then it is ex-
pected that the corresponding semantic relation 
holds between these words (hypernym-hyponym in 
this example). 
    In recent work (Geffet and Dagan, 2004) we 
explored the correspondence between the distribu-
tional characterization of two words (which may 
hardly co-occur, as is usually the case for syno-
                                                           
1We found that less than 20% of the lexical entailment relations extracted by our 
method appeared as direct or indirect WordNet relations (synonyms, hyponyms 
or meronyms). 
107
nyms) and the kind of tight semantic relationship 
that might hold between them. We formulated a 
lexical entailment relation that corresponds to the 
above mentioned substitutability criterion, and is 
termed meaning entailing substitutability (which 
we term here for brevity as lexical entailment). 
Given a pair of words, this relation holds if there 
are some contexts in which one of the words can 
be substituted by the other, such that the meaning 
of the original word can be inferred from the new 
one. We then proposed a new feature weighting 
function (RFF) that yields more accurate distribu-
tional similarity lists, which better approximate the 
lexical entailment relation. Yet, this method still 
applies a standard measure for distributional vector 
similarity (over vectors with the improved feature 
weights), and thus produces many loose similari-
ties that do not correspond to entailment. 
    This paper explores more deeply the relationship 
between distributional characterization of words 
and lexical entailment, proposing two new hy-
potheses as a refinement of the distributional simi-
larity hypothesis. The main idea is that if one word 
entails the other then we would expect that virtu-
ally all the characteristic context features of the 
entailing word will actually occur also with the 
entailed word. 
     To test this idea we developed an automatic 
method for testing feature inclusion between a pair 
of words. This algorithm combines corpus statis-
tics with a web-based feature sampling technique. 
The web is utilized to overcome the data sparse-
ness problem, so that features which are not found 
with one of the two words can be considered as 
truly distinguishing evidence.  
    Using the above algorithm we first tested the 
empirical validity of the hypotheses. Then, we 
demonstrated how the hypotheses can be leveraged 
in practice to improve the precision of automatic 
acquisition of the entailment relation. 
 
2 Background  
2.1 Implementations of Distribu-
tional  Similarity 
This subsection reviews the relevant details of ear-
lier methods that were utilized within this paper.  
In the computational setting contexts of words 
are represented by feature vectors. Each word w is 
represented by a feature vector, where an entry in 
the vector corresponds to a feature f. Each feature 
represents another word (or term) with which w co-
occurs, and possibly specifies also the syntactic 
relation between the two words as in (Grefenstette, 
1994; Lin, 1998; Weeds and Weir, 2003). Pado 
and Lapata (2003) demonstrated that using syntac-
tic dependency-based vector space models can help 
distinguish among classes of different lexical rela-
tions, which seems to be more difficult for tradi-
tional ?bag of words? co-occurrence-based models. 
A syntactic feature is defined as a triple <term, 
syntactic_relation, relation_direction> (the direc-
tion is set to 1, if the feature is the word?s modifier 
and to 0 otherwise). For example, given the word 
?company? the feature <earnings_report, gen, 0> 
(genitive) corresponds to the phrase ?company?s 
earnings report?, and <profit, pcomp, 0> (preposi-
tional complement) corresponds to ?the profit of 
the company?. Throughout this paper we used syn-
tactic features generated by the Minipar depend-
ency parser (Lin, 1993).  
    The value of each entry in the feature vector is 
determined by some weight function weight(w,f), 
which quantifies the degree of statistical associa-
tion between the feature and the corresponding 
word. The most widely used association weight 
function is (point-wise) Mutual Information (MI) 
(Church and Hanks, 1990; Lin, 1998; Dagan, 2000; 
Weeds et al, 2004). 
<=> element, component <=> gap, spread *      town, airport <=   loan, mortgage 
=>   government, body *      warplane, bomb <=> program, plan *      tank, warplane 
*      match, winner =>   bill, program <=   conflict, war =>   town, location    
Table 1: Sample of the data set of top-40 distributionally similar word pairs produced by the RFF-
based method of (Geffet and Dagan, 2004). Entailment judgments are marked by the arrow direction, 
with '*' denoting no entailment.  
108
    Once feature vectors have been constructed, the 
similarity between two words is defined by some 
vector similarity metric. Different metrics have 
been used, such as weighted Jaccard (Grefenstette, 
1994; Dagan, 2000), cosine (Ruge, 1992), various 
information theoretic measures (Lee, 1997), and 
the widely cited and competitive (see (Weeds and 
Weir, 2003)) measure of Lin (1998) for similarity 
between two words, w and v, defined as follows: 
 
  
,
),(),(
),(),(
),(
)()(
)()(
 

??
??
+
+
=
fvweightfwweight
fvweightfwweight
vwsim
vFfwFf
vFwFf
Lin
 
 
where F(w) and F(v) are the active features of the 
two words (positive feature weight) and the weight 
function is defined as MI. As typical for vector 
similarity measures, it assigns high similarity 
scores if many of the two word?s features overlap, 
even though some prominent features might be 
disjoint. This is a major reason for getting such 
semantically loose similarities, like company - 
government and country - economy. 
Investigating the output of Lin?s (1998) similar-
ity measure with respect to the above criterion in 
(Geffet and Dagan, 2004), we discovered that the 
quality of similarity scores is often hurt by inaccu-
rate feature weights, which yield rather noisy fea-
ture vectors.  Hence, we tried to improve the 
feature weighting function to promote those fea-
tures that are most indicative of the word meaning. 
A new weighting scheme was defined for boot-
strapping feature weights, termed RFF (Relative 
Feature Focus). First, basic similarities are gener-
ated by Lin?s measure. Then, feature weights are 
recalculated, boosting the weights of features that 
characterize many of the words that are most simi-
lar to the given one2. As a result the most promi-
nent features of a word are concentrated within the 
top-100 entries of the vector. Finally, word simi-
larities are recalculated by Lin's metric over the 
vectors with the new RFF weights. 
    The lexical entailment prediction task of 
(Geffet and Dagan, 2004) measures how many of 
the top ranking similarity pairs produced by the 
                                                           
2
 In concrete terms RFF is defined by: 
 ??= ),()()(),( vwsimwNfWSvfwRFF ,  
where sim(w,v) is an initial approximation of the similarity space by Lin?s 
measure, WS(f) is a set of words co-occurring with feature f, and N(w) is the set 
of the most similar words of w by Lin?s measure. 
RFF-based metric hold the entailment relation, in 
at least one direction. To this end a data set of 
1,200 pairs was created, consisting of top-N 
(N=40) similar words of 30 randomly selected 
nouns, which were manually judged by the lexical 
entailment criterion. Quite high Kappa agreement 
values of 0.75 and 0.83 were reported, indicating 
that the entailment judgment task was reasonably 
well defined. A subset of the data set is demon-
strated in Table 1.     
The RFF weighting produced 10% precision 
improvement over Lin?s original use of MI, sug-
gesting the RFF capability to promote semantically 
meaningful features. However, over 47% of the 
word pairs in the top-40 similarities are not related 
by entailment, which calls for further improve-
ment. In this paper we use the same data set 3 and 
the RFF metric as a basis for our experiments. 
2.2 Predicting  Semantic Inclusion 
Weeds et al (2004) attempted to refine the distri-
butional similarity goal to predict whether one 
term is a generalization/specification of the other. 
They present a distributional generality concept 
and expect it to correlate with semantic generality. 
Their conjecture is that the majority of the features 
of the more specific word are included in the fea-
tures of the more general one. They define the fea-
ture recall of w with respect to v as the weighted 
proportion of features of v that also appear in the 
vector of w. Then, they suggest that a hypernym 
would have a higher feature recall for its hypo-
nyms (specifications), than vice versa.  
    However, their results in predicting the hy-
ponymy-hyperonymy direction (71% precision) are 
comparable to the na?ve baseline (70% precision) 
that simply assumes that general words are more 
frequent than specific ones. Possible sources of 
noise in their experiment could be ignoring word 
polysemy and data sparseness of word-feature co-
occurrence in the corpus. 
3 The Distributional Inclusion Hy-
potheses 
In this paper we suggest refined versions of the 
distributional similarity hypothesis which relate 
distributional behavior with lexical entailment. 
                                                           
3 Since the original data set did not include the direction of entailment, we have 
enriched it by adding the judgments of entailment direction. 
109
     Extending the rationale of Weeds et al, we 
suggest that if the meaning of a word v entails an-
other word w then it is expected that all the typical 
contexts (features) of v will occur also with w. That 
is, the characteristic contexts of v are expected to 
be included within all w's contexts (but not neces-
sarily amongst the most characteristic ones for w). 
Conversely, we might expect that if v's characteris-
tic contexts are included within all w's contexts 
then it is likely that the meaning of  v does entail 
w. Taking both directions together, lexical entail-
ment is expected to highly correlate with character-
istic feature inclusion. 
     Two additional observations are needed before 
concretely formulating these hypotheses. As ex-
plained in Section 2, word contexts should be rep-
resented by syntactic features, which are more 
restrictive and thus better reflect the restrained se-
mantic meaning of the word (it is difficult to tie 
entailment to looser context representations, such 
as co-occurrence in a text window). We also notice 
that distributional similarity principles are intended 
to hold at the sense level rather than the word 
level, since different senses have different charac-
teristic contexts (even though computational com-
mon practice is to work at the word level, due to 
the lack of robust sense annotation). 
    We can now define the two distributional inclu-
sion hypotheses, which correspond to the two di-
rections of inference relating distributional feature 
inclusion and lexical entailment. Let vi and wj be 
two word senses of the words w and v, correspond-
ingly, and let vi => wj denote the (directional) en-
tailment relation between these senses. Assume 
further that we have a measure that determines the 
set of characteristic features for the meaning of 
each word sense. Then we would hypothesize: 
Hypothesis I: 
If vi => wj then all the characteristic (syntactic-
based) features of vi are expected to appear with wj.  
Hypothesis II: 
If all the characteristic (syntactic-based) features of 
vi appear with wj then we expect that vi => wj. 
4 Word Level Testing of Feature In-
clusion  
To check the validity of the hypotheses we need to 
test feature inclusion. In this section we present an 
automated word-level feature inclusion testing 
method, termed ITA (Inclusion Testing Algorithm). 
To overcome the data sparseness problem we in-
corporated web-based feature sampling. Given a 
test pair of words, three main steps are performed, 
as detailed in the following subsections:  
Step 1: Computing the set of characteristic features 
for each word. 
Step 2: Testing feature inclusion for each pair, in 
both directions, within the given corpus data.  
Step 3: Complementary testing of feature inclusion 
for each pair in the web. 
4.1 Step 1: Corpus-based generation 
of characteristic features 
To implement the first step of the algorithm, the 
RFF weighting function is exploited and its top-
100 weighted features are taken as most character-
istic for each word. As mentioned in Section 2, 
(Geffet and Dagan, 2004) shows that RFF yields 
high concentration of good features at the top of 
the vector. 
4.2 Step 2: Corpus-based feature 
inclusion test 
We first check feature inclusion in the corpus that 
was used to generate the characteristic feature sets.  
For each word pair (w, v) we first determine which 
features of w do co-occur with v in the corpus. The 
same is done to identify features of v that co-occur 
with w in the corpus. 
4.3 Step 3: Complementary Web-
based Inclusion Test 
This step is most important to avoid inclusion 
misses due to the data sparseness of the corpus. A 
few recent works (Ravichandran and Hovy, 2002; 
Keller et al, 2002; Chklovski and Pantel, 2004) 
used the web to collect statistics on word co-
occurrences. In a similar spirit, our inclusion test is 
completed by searching the web for the missing 
(non-included) features on both sides. We call this 
web-based technique mutual web-sampling. The 
web results are further parsed to verify matching of 
the feature's syntactic relationship. 
110
     We denote the subset of w's features that are 
missing for v as M(w, v) (and equivalently M(v, 
w)). Since web sampling is time consuming we 
randomly sample a subset of k features (k=20 in 
our experiments), denoted as M(v,w,k).  
Mutual Web-sampling Procedure: 
For each pair (w, v) and their k-subsets  
M(w, v, k) and M(v, w, k) execute: 
 
1.  Syntactic Filtering of ?Bag-of-Words? Search: 
 
Search the web for sentences including v and a fea-
ture f from M(w, v, k) as ?bag of words?, i. e. sen-
tences where w and f appear in any distance and in 
either order. Then filter out the sentences that do 
not match the defined syntactic relation between f 
and v (based on parsing). Features that co-occur 
with w in the correct syntactic relation are removed 
from M(w, v, k). Do the same search and filtering 
for w and features from M(v, w, k). 
2.   Syntactic Filtering of ?Exact String? Matching: 
On the missing features on both sides (which are 
left in M(w, v, k) and M(v, w, k) after stage 1), ap-
ply ?exact string? search of the web. For this, con-
vert the tuple (v, f) to a string by adding 
prepositions and articles where needed. For exam-
ple, for (element, <project, pcomp_of, 1>) gener-
ate the corresponding string ?element of the 
project? and search the web for exact matches of 
the string. Then validate the syntactic relationship 
of f and v in the extracted sentences. Remove the 
found features from M(w, v, k) and M(v, w, k), re-
spectively. 
3.   Missing Features Validation:  
Since some of the features may be too infrequent 
or corpus-biased, check whether the remaining 
missing features do co-occur on the web with their 
original target words (with which they did occur in 
the corpus data). Otherwise, they should not be 
considered as valid misses and are also removed 
from M(w, v, k) and M(v, w, k).  
Output: Inclusion in either direction holds if the 
corresponding set of missing features is now 
empty. 
We also experimented with features consisting of 
words without syntactic relations. For example, 
exact string, or bag-of-words match. However, al-
most all the words (also non-entailing) were found 
with all the features of each other, even for seman-
tically implausible combinations (e.g. a word and a 
feature appear next to each other but belong to dif-
ferent clauses of the sentence). Therefore we con-
clude that syntactic relation validation is very 
important, especially on the web, in order to avoid 
coincidental co-occurrences.  
5 Empirical Results 
To test the validity of the distributional inclusion 
hypotheses we performed an empirical analysis on 
a selected test sample using our automated testing 
procedure. 
5.1 Data and setting 
We experimented with a randomly picked test 
sample of about 200 noun pairs of 1,200 pairs pro-
duced by RFF (for details see Geffet and Dagan, 
2004) under Lin?s similarity scheme (Lin, 1998). 
The words were judged by the lexical entailment 
criterion (as described in Section 2). The original 
percentage of correct (52%) and incorrect (48%) 
entailments was preserved. 
    To estimate the degree of validity of the distri-
butional inclusion hypotheses we decomposed 
each word pair of the sample (w, v) to two direc-
tional pairs ordered by potential entailment direc-
tion: (w, v) and (v, w). The 400 resulting ordered 
pairs are used as a test set in Sections 5.2 and 5.3.   
    Features were computed from co-occurrences in 
a subset of the Reuters corpus of about 18 million 
words. For the web feature sampling the maximal 
number of web samples for each query (word - 
feature) was set to 3,000 sentences. 
5.2 Automatic Testing the Validity 
of the Hypotheses at the  Word 
Level  
The test set of 400 ordered pairs was examined in 
terms of entailment (according to the manual 
judgment) and feature inclusion (according to the 
ITA algorithm), as shown in Table 2. 
    According to Hypothesis I we expect that a pair 
(w, v) that satisfies entailment will also preserve 
feature inclusion. On the other hand, by Hypothe-
sis II if all the features of w are included by v then 
we expect that w entails v.  
111
    We observed that Hypothesis I is better attested 
by our data than the second hypothesis. Thus 86% 
(97 out of 113) of the entailing pairs fulfilled the 
inclusion condition. Hypothesis II holds for ap-
proximately 70% (97 of 139) of the pairs for which 
feature inclusion holds. In the next section we ana-
lyze the cases of violation of both hypotheses and 
find that the first hypothesis held to an almost per-
fect extent with respect to word senses.  
    It is also interesting to note that thanks to the 
web-sampling procedure over 90% of the non-
included features in the corpus were found on the 
web, while most of the missing features (in the 
web) are indeed semantically implausible. 
5.3 Manual Sense Level Testing of 
Hypotheses Validity  
Since our data was not sense tagged, the automatic 
validation procedure could only test the hypotheses 
at the word level. In this section our goal is to ana-
lyze the findings of our empirical test at the word 
sense level as our hypotheses were defined for 
senses.  Basically, two cases of hypotheses invalid-
ity were detected: 
Case 1: Entailments with non-included features 
(violation of Hypothesis I); 
Case 2: Feature Inclusion for non-entailments 
(violation of Hypothesis II).     
    At the word level we observed 14% invalid pairs 
of the first case and 30% of the second case. How-
ever, our manual analysis shows, that over 90% of 
the first case pairs were due to a different sense of 
one of the entailing word, e.g. capital - town (capi-
tal as money) and spread - gap (spread as distribu-
tion) (Table 3). Note that ambiguity of the entailed 
word does not cause errors (like town ? area, area 
as domain) (Table 3). Thus the first hypothesis 
holds at the sense level for over 98% of the cases 
(Table 4). 
    Two remaining invalid instances of the first case 
were due to the web sampling method limitations 
and syntactic parsing filtering mistakes, especially 
for some less characteristic and infrequent features 
captured by RFF. Thus, in virtually all the exam-
ples tested in our experiment Hypothesis I was 
valid. 
   We also explored the second case of invalid 
pairs: non-entailing words that pass the feature in-
clusion test. After sense based analysis their per-
centage was reduced slightly to 27.4%. Three 
possible reasons were discovered. First, there are 
words with features typical to the general meaning 
of the domain, which tend to be included by many 
other words of this domain, like valley ? town. The 
features of valley (?eastern valley?, ?central val-
ley?, ?attack in valley?, ?industry of the valley?) 
are not discriminative enough to be distinguished 
from town, as they are all characteristic to any geo-
graphic location.  
             Inclusion 
Entailment 
       +     - 
 
              +      97       16 
               -      42           245 
Table 2: Distribution of 400 entailing/non-
entailing ordered pairs that hold/do not hold 
feature inclusion at the word level.  
           Inclusion 
Entailment 
        +     - 
 
             +        111       2 
              -        42       245 
Table 4: Distribution of the entailing/non-
entailing ordered pairs that hold/do not hold 
feature inclusion at the sense level.  
spread ? gap (mutually entail each other) 
<weapon, pcomp_of> 
The Committee was discussing the Pro-
gramme of the ?Big Eight,? aimed against 
spread of weapon of mass destruction. 
town ? area (?town? entails ?area?) 
<cooperation, pcomp_for> 
This is a promising area for cooperation and 
exchange of experiences.  
capital ? town (?capital? entails ?town?) 
<flow, nn> 
Offshore financial centers affect cross-border 
capital flow in China. 
Table 3: Examples of ambiguity of entailment-
related words, where the disjoint features be-
long to a different sense of the word. 
112
    The second group consists of words that can be 
entailing, but only in a context-dependent (ana-
phoric) manner rather than ontologically. For ex-
ample, government and neighbour, while 
neighbour is used in the meaning of ?neighbouring 
(country) government?. Finally, sometimes one or 
both of the words are abstract and general enough 
and also highly ambiguous to appear with a wide 
range of features on the web, like element (vio-
lence ? element, with all the tested features of vio-
lence included by element). 
To prevent occurrences of the second case more 
characteristic and discriminative features should be 
provided. For this purpose features extracted from 
the web, which are not domain-biased (like fea-
tures from the corpus) and multi-word features 
may be helpful. Overall, though, there might be 
inherent cases that invalidate Hypothesis II. 
6 Improving Lexical Entailment Pre-
diction by ITA (Inclusion Testing 
Algorithm) 
In this section we show that ITA can be practically 
used to improve the (non-directional) lexical en-
tailment prediction task described in Section 2. 
Given the output of the distributional similarity 
method, we employ ITA at the word level to filter 
out non-entailing pairs. Word pairs that satisfy fea-
ture inclusion of all k features (at least in one direc-
tion) are claimed as entailing.  
 The same test sample of 200 word pairs men-
tioned in Section 5.1 was used in this experiment. 
The results were compared to RFF under Lin?s 
similarity scheme (RFF-top-40 in Table 5).  
 Precision was significantly improved, filtering 
out 60% of the incorrect pairs. On the other hand, 
the relative recall (considering RFF recall as 
100%) was only reduced by 13%, consequently 
leading to a better relative F1, when considering 
the RFF-top-40 output as 100% recall (Table 5). 
 Since our method removes about 35% of the 
original top-40 RFF output, it was interesting to 
compare our results to simply cutting off the 35% 
of the lowest ranked RFF words (top-26). The 
comparison to the baseline (RFF-top-26 in Table 
5) showed that ITA filters the output much better 
than just cutting off the lowest ranking similarities.  
 We also tried a couple of variations on feature 
sampling for the web-based procedure. In one of 
our preliminary experiments we used the top-k 
RFF features instead of random selection. But we 
observed that top ranked RFF features are less dis-
criminative than the random ones due to the nature 
of the RFF weighting strategy, which promotes 
features shared by many similar words. Then, we 
attempted doubling the sampling to 40 random fea-
tures. As expected the recall was slightly de-
creased, while precision was increased by over 5%. 
In summary, the behavior of ITA sampling of 
k=20 and k=40 features is closely comparable 
(ITA-20 and ITA-40 in Table 5, respectively)4.     
7 Conclusions and Future Work 
The main contributions of this paper were: 
1.  We defined two Distributional Inclusion Hy-
potheses that associate feature inclusion with lexi-
cal entailment at the word sense level. The 
Hypotheses were proposed as a refinement for 
Harris? Distributional hypothesis and as an exten-
sion to the classic distributional similarity scheme. 
2.   To estimate the empirical validity of the de-
fined hypotheses we developed an automatic inclu-
sion testing algorithm (ITA). The core of the 
algorithm is a web-based feature inclusion testing 
procedure, which helped significantly to compen-
sate for data sparseness. 
3.    Then a thorough analysis of the data behavior 
with respect to the proposed hypotheses was con-
ducted. The first hypothesis was almost fully at-
tested by the data, particularly at the sense level, 
while the second hypothesis did not fully hold.  
4.   Motivated by the empirical analysis we pro-
posed to employ ITA for the practical task of im-
proving lexical entailment acquisition. The 
algorithm was applied as a filtering technique on 
the distributional similarity (RFF) output. We ob-
                                                           
4
 The ITA-40 sampling fits the analysis from section 5.2 and 5.3 as well. 
Method Precision Recall F1 
ITA-20 0.700 0.875 0.777 
ITA-40 0.740 0.846 0.789 
RFF-top-40 0.520 1.000 0.684 
RFF-top-26 0.561 0.701 0.624 
Table 5: Comparative results of using the 
filter, with 20 and 40 feature sampling, com-
pared to RFF top-40 and RFF top-26 simi-
larities. ITA-20 and ITA-40 denote the web-
sampling method with 20 and random 40 
features, respectively. 
113
tained 17% increase of precision and succeeded to 
improve relative F1 by 15% over the baseline.      
    Although the results were encouraging our man-
ual data analysis shows that we still have to handle 
word ambiguity. In particular, this is important in 
order to be able to learn the direction of entailment.  
     To achieve better precision we need to increase 
feature discriminativeness. To this end syntactic 
features may be extended to contain more than one 
word, and ways for automatic extraction of fea-
tures from the web (rather than from a corpus) may 
be developed. Finally, further investigation of 
combining the distributional and the co-occurrence 
pattern-based approaches over the web is desired. 
Acknowledgement 
We are grateful to Shachar Mirkin for his help in 
implementing the web-based sampling procedure 
heavily employed in our experiments. We thank 
Idan Szpektor for providing the infrastructure sys-
tem for web-based data extraction.    
References  
Chklovski, Timothy and Patrick Pantel. 2004. 
VERBOCEAN: Mining the Web for Fine-Grained Se-
mantic Verb Relations. In Proc. of EMNLP-04. Bar-
celona, Spain. 
 
Church, Kenneth W. and Hanks Patrick. 1990. Word 
association norms, mutual information, and Lexicog-
raphy. Computational Linguistics, 16(1), pp. 22?29. 
 
Dagan, Ido. 2000. Contextual Word Similarity, in Rob 
Dale, Hermann Moisl and Harold Somers (Eds.), 
Handbook of Natural Language Processing, Marcel 
Dekker Inc, 2000, Chapter 19, pp. 459-476.  
 
Dagan, Ido, Oren Glickman and Bernardo Magnini. 
2005. The PASCAL Recognizing Textual Entailment 
Challenge. In Proc. of the PASCAL Challenges 
Workshop for Recognizing Textual Entailment. 
Southampton, U.K.  
 
Geffet, Maayan and Ido Dagan, 2004. Feature Vector 
Quality and Distributional Similarity. In Proc. of Col-
ing-04. Geneva. Switzerland. 
 
Grefenstette, Gregory. 1994. Exploration in Automatic 
Thesaurus Discovery. Kluwer Academic Publishers. 
 
Harris, Zelig S. Mathematical structures of language. 
Wiley, 1968. 
 
Hearst, Marti. 1992. Automatic acquisition of hypo-
nyms from large text corpora. In Proc. of COLING-
92. Nantes, France. 
 
Keller, Frank, Maria Lapata, and Olga Ourioupina. 
2002. Using the Web to Overcome Data Sparseness. 
In Jan Hajic and Yuji Matsumoto, eds., In Proc. of 
EMNLP-02. Philadelphia, PA. 
 
Lee, Lillian. 1997. Similarity-Based Approaches to 
Natural Language Processing. Ph.D. thesis, Harvard 
University, Cambridge, MA.   
 
Lin, Dekang. 1993. Principle-Based Parsing without 
Overgeneration. In Proc. of ACL-93. Columbus, 
Ohio. 
. 
Lin, Dekang. 1998. Automatic Retrieval and Clustering 
of Similar Words.  In Proc. of COLING?ACL98, 
Montreal, Canada. 
 
Moldovan, Dan, Badulescu, A., Tatu, M., Antohe, D., 
and Girju, R. 2004. Models for the semantic classifi-
cation of noun phrases. In Proc. of HLT/NAACL-
2004 Workshop on Computational Lexical Seman-
tics. Boston. 
 
Pado, Sebastian and Mirella Lapata. 2003. Constructing 
semantic space models from parsed corpora. In Proc. 
of ACL-03, Sapporo, Japan. 
 
Pantel, Patrick and Dekang Lin. 2002. Discovering 
Word Senses from Text. In Proc. of ACM SIGKDD 
Conference on Knowledge Discovery and Data Min-
ing (KDD-02). Edmonton, Canada. 
 
Pereira, Fernando, Tishby Naftali, and Lee Lillian. 
1993. Distributional clustering of English words. In 
Proc. of ACL-93. Columbus, Ohio. 
 
Ravichandran, Deepak and Eduard Hovy. 2002. Learn-
ing Surface Text Patterns for a Question Answering 
System. In Proc. of ACL-02. Philadelphia, PA. 
 
Ruge,  Gerda.  1992.  Experiments on linguistically-
based term associations. Information Processing & 
Management, 28(3), pp. 317?332. 
 
Weeds, Julie and David Weir. 2003. A General Frame-
work for Distributional Similarity. In Proc. of 
EMNLP-03. Sapporo, Japan. 
 
Weeds, Julie, D. Weir, D. McCarthy. 2004. Characteriz-
ing Measures of Lexical Distributional Similarity. In 
Proc. of Coling-04. Geneva, Switzerland. 
114
