Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 209?216, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Minimum Sample Risk Methods for Language Modeling1  
Jianfeng Gao  
Microsoft Research Asia 
jfgao@microsoft.com 
Hao Yu,  Wei Yuan 
 Shanghai Jiaotong Univ., China 
Peng Xu 
John Hopkins Univ., U.S.A. 
xp@clsp.jhu.edu 
                                                     
1
  The work was done while the second, third and fourth authors were visiting Microsoft Research Asia. Thanks to Hisami Suzuki for 
her valuable comments. 
Abstract 
This paper proposes a new discriminative 
training method, called minimum sample risk 
(MSR), of estimating parameters of language 
models for text input. While most existing 
discriminative training methods use a loss 
function that can be optimized easily but 
approaches only approximately to the objec-
tive of minimum error rate, MSR minimizes 
the training error directly using a heuristic 
training procedure. Evaluations on the task 
of Japanese text input show that MSR can 
handle a large number of features and train-
ing samples; it significantly outperforms a 
regular trigram model trained using maxi-
mum likelihood estimation, and it also out-
performs the two widely applied discrimi-
native methods, the boosting and the per-
ceptron algorithms, by a small but statisti-
cally significant margin. 
1 Introduction 
Language modeling (LM) is fundamental to a wide 
range of applications, such as speech recognition 
and Asian language text input (Jelinek 1997; Gao et 
al. 2002). The traditional approach uses a paramet-
ric model with maximum likelihood estimation (MLE), 
usually with smoothing methods to deal with data 
sparseness problems. This approach is optimal 
under the assumption that the true distribution of 
data on which the parametric model is based is 
known. Unfortunately, such an assumption rarely 
holds in realistic applications. 
An alternative approach to LM is based on the 
framework of discriminative training, which uses a 
much weaker assumption that training and test 
data are generated from the same distribution but 
the form of the distribution is unknown. Unlike the 
traditional approach that maximizes the function 
(i.e. likelihood of training data) that is loosely as-
sociated with error rate, discriminative training 
methods aim to directly minimize the error rate on 
training data even if they reduce the likelihood. So, 
they potentially lead to better solutions. However, 
the error rate of a finite set of training samples is 
usually a step function of model parameters, and 
cannot be easily minimized. To address this prob-
lem, previous research has concentrated on the 
development of a loss function that approximates 
the exact error rate and can be easily optimized. 
Though these methods (e.g. the boosting method) 
have theoretically appealing properties, such as 
convergence and bounded generalization error, we 
argue that the approximated loss function may 
prevent them from attaining the original objective 
of minimizing the error rate. 
In this paper we present a new estimation pro-
cedure for LM, called minimum sample risk (MSR). It 
differs from most existing discriminative training 
methods in that instead of searching on an ap-
proximated loss function, MSR employs a simple 
heuristic training algorithm that minimizes the 
error rate on training samples directly. MSR oper-
ates like a multidimensional function optimization 
algorithm: first, it selects a subset of features that 
are the most effective among all candidate features. 
The parameters of the model are then optimized 
iteratively: in each iteration, only the parameter of 
one feature is adjusted. Both feature selection and 
parameter optimization are based on the criterion 
of minimizing the error on training samples. Our 
evaluation on the task of Japanese text input shows 
that MSR achieves more than 20% error rate reduc-
tion over MLE on two newswire data sets, and it 
also outperforms the other two widely applied 
discriminative methods, the boosting method and 
the perceptron algorithm, by a small but statisti-
cally significant margin. 
Although it has not been proved in theory that 
MSR is always robust, our experiments of cross- 
domain LM adaptation show that it is. MSR can 
effectively adapt a model trained on one domain to 
209
different domains. It outperforms the traditional 
LM adaptation method significantly, and achieves 
at least comparable or slightly better results to the 
boosting method and the perceptron algorithm. 
2 IME Task and LM 
This paper studies LM on the task of Asian lan-
guage (e.g. Chinese or Japanese) text input. This is 
the standard method of inputting Chinese or 
Japanese text by converting the input phonetic 
symbols into the appropriate word string. In this 
paper we call the task IME, which stands for input 
method editor, based on the name of the commonly 
used Windows-based application. 
Performance on IME is measured in terms of the 
character error rate (CER), which is the number of 
characters wrongly converted from the phonetic 
string divided by the number of characters in the 
correct transcript. Current IME systems make 
about 5-15% CER in conversion of real data in a 
wide variety of domains (e.g. Gao et al 2002).  
Similar to speech recognition, IME is viewed as 
a Bayes decision problem. Let A be the input pho-
netic string. An IME system?s task is to choose the 
most likely word string W* among those candidates 
that could be converted from A: 
)|()(maxarg)|(maxarg
(A))(
* WAPWPAWPW
WAW GENGEN ??
==  (1) 
where GEN(A) denotes the candidate set given A. 
Unlike speech recognition, however, there is no 
acoustic ambiguity since the phonetic string is 
inputted by users. Moreover, if we do not take into 
account typing errors, it is reasonable to assume a  
unique mapping from W and A in IME, i.e. P(A|W) 
= 1. So the decision of Equation (1) depends solely 
upon P(W), making IME a more direct evaluation 
test bed for LM than speech recognition. Another 
advantage is that it is easy to convert W to A (for 
Chinese and Japanese), which enables us to obtain 
a large number of training data for discriminative 
learning, as described later.  
The values of P(W) in Equation (1) are tradi-
tionally calculated by MLE: the optimal model 
parameters ?* are chosen in such a way that 
P(W|?*) is maximized on training data. The argu-
ments in favor of MLE are based on the assumption 
that the form of the underlying distributions is 
known, and that only the values of the parameters 
characterizing those distributions are unknown. In 
using MLE for LM, one always assumes a multi-
nomial distribution of language. For example, a 
trigram model makes the assumption that the next 
word is predicted depending only on two preced-
ing words. However, there are many cases in 
natural language where words over an arbitrary 
distance can be related. MLE is therefore not opti-
mal because the assumed model form is incorrect. 
What are the best estimators when the model is 
known to be false then? In IME, we can tackle this 
question empirically. Best IME systems achieve the 
least CER. Therefore, the best estimators are those 
which minimize the expected error rate on unseen 
test data. Since the distribution of test data is un-
known, we can approximately minimize the error 
rate on some given training data (Vapnik 1999). 
Toward this end, we have developed a very simple 
heuristic training procedure called minimum sample 
risk, as presented in the next section. 
3 Minimum Sample Risk 
3.1 Problem Definition 
We follow the general framework of linear dis-
criminant models described in (Duda et al 2001). In 
the rest of the paper we use the following notation, 
adapted from Collins (2002). 
? Training data is a set of example input/output 
pairs. In LM for IME, training samples are repre-
sented as {Ai, WiR}, for i = 1?M, where each Ai is an 
input phonetic string and WiR is the reference tran-
script of Ai. 
? We assume some way of generating a set of 
candidate word strings given A, denoted by 
GEN(A).  In our experiments, GEN(A) consists of 
top N word strings converted from A using a base-
line IME system that uses only a word trigram 
model. 
? We assume a set of D+1 features fd(W), for d = 
0?D. The features could be arbitrary functions that 
map W to real values. Using vector notation, we 
have f(W)??D+1, where f(W) = [f0(W), f1(W), ?, 
fD(W)]T. Without loss of generality, f0(W) is called 
the base feature, and is defined in our case as the 
log probability that the word trigram model as-
signs to W. Other features (fd(W), for d = 1?D) are 
defined as the counts of word n-grams (n = 1 and 2 
in our experiments) in W. 
? Finally, the parameters of the model form a 
vector of D+1 dimensions, each for one feature 
function, ? = [?0, ?1, ?, ?D]. The score of a word 
string W can be written as  
210
)(),( WWScore ?f? = ?
=
=
D
d
dd Wf?
0
)( . (2)
The decision rule of Equation (1) is rewritten as 
),(maxarg),(
(A)
* ??
GEN
WScoreAW
W?
= . (3)
Equation (3) views IME as a ranking problem, 
where the model gives the ranking score, not 
probabilities. We therefore do not evaluate the 
model via perplexity. 
Now, assume that we can measure the number 
of conversion errors in W by comparing it with a 
reference transcript WR using an error function 
Er(WR,W) (i.e.  the string edit distance function in 
our case). We call the sum of error counts over the 
training samples sample risk. Our goal is to mini-
mize the sample risk while searching for the pa-
rameters as defined in Equation (4), hence the name 
minimum sample risk (MSR). Wi* in Equation (4) is 
determined by Equation (3), 
?
=
=
Mi
ii
R
i
def
MSR AWW
...1
* )),(,Er(minarg ??
?
. (4)
We first present the basic MSR training algorithm, 
and then the two improvements we made. 
3.2 Training Algorithm 
The MSR training algorithm is cast as a multidi-
mensional function optimization approach (Press 
et al 1992): taking the feature vector as a set of 
directions; the first direction (i.e. feature) is selected 
and the objective function (i.e. sample risk) is 
minimized along that direction using a line search; 
then from there along the second direction to its 
minimum, and so on, cycling through the whole set 
of directions as many times as necessary, until the 
objective function stops decreasing.  
This simple method can work properly under 
two assumptions. First, there exists an implemen-
tation of line search that optimizes the function 
along one direction efficiently. Second, the number 
of candidate features is not too large, and these 
features are not highly correlated. However, nei-
ther of the assumptions holds in our case. First of 
all, Er(.) in Equation (4) is a step function of ?, thus 
cannot be optimized directly by regular gradient- 
based procedures ? a grid search has to be used 
instead. However, there are problems with simple 
grid search: using a large grid could miss the op-
timal solution whereas using a fine-grained grid 
would lead to a very slow algorithm. Secondly, in 
the case of LM, there are millions of candidate 
features, some of which are highly correlated. We 
address these issues respectively in the next two 
subsections. 
3.3 Grid Line Search 
Our implementation of a grid search is a modified 
version of that proposed in (Och 2003). The modi-
fications are made to deal with the efficiency issue 
due to the fact that there is a very large number of 
features and training samples in our task, compared 
to only 8 features used in (Och 2003). Unlike a 
simple grid search where the intervals between any 
two adjacent grids are equal and fixed, we deter-
mine for each feature a sequence of grids with 
differently sized intervals, each corresponding to a 
different value of sample risk. 
As shown in Equation (4), the loss function (i.e. 
sample risk) over all training samples is the sum of 
the loss function (i.e. Er(.)) of each training sample. 
Therefore, in what follows, we begin with a discus-
sion on minimizing Er(.) of a training sample using 
the line search.  
Let ? be the current model parameter vector, 
and fd be the selected feature. The line search aims to 
find the optimal parameter ?d* so as to minimize 
Er(.). For a training sample (A, WR), the score of each 
candidate word string W?GEN(A), as in Equation 
(2), can be decomposed into two terms: 
)()()(),(
'0'
'' WfWfWWScore dd
D
ddd
dd ?? +== ?
??=
?f? , 
where the first term on the right hand side does not 
change with ?d. Note that if several candidate word 
strings have the same feature value fd(W), their 
relative rank will remain the same for any ?d. Since 
fd(W) takes integer values in our case (fd(W) is the 
count of a particular n-gram in W), we can group the 
candidates using fd(W) so that candidates in each 
group have the same value of fd(W). In each group, 
we define the candidate with the highest value of  
? ??=D ddd dd Wf'0' '' )(?  
as the active candidate of the group because no 
matter what value ?d takes, only this candidate 
could be selected according to Equation (3). 
Now, we reduce GEN(A) to a much smaller list 
of active candidates. We can find a set of intervals 
for ?d, within each of which a particular active 
candidate will be selected as W*. We can compute 
the Er(.) value of that candidate as the Er(.) value for 
the corresponding interval. As a result, for each 
211
training sample, we obtain a sequence of intervals 
and their corresponding Er(.) values. The optimal 
value ?d* can then be found by traversing the se-
quence and taking the midpoint of the interval with 
the lowest Er(.) value.  
305
306
307
308
309
310
311
312
0. 85 0. 9 0. 95 1 1.05 1.1 1. 15 1. 2lambda
SR
(.)
sample risk
smoothed sample risk
 
Figure 1. Examples of line search.  
This process can be extended to the whole 
training set as follows. By merging the sequence of 
intervals of each training sample in the training set, 
we obtain a global sequence of intervals as well as 
their corresponding sample risk. We can then find 
the optimal value ?d* as well as the minimal sample 
risk by traversing the global interval sequence. An 
example is shown in Figure 1. 
The line search can be unstable, however. In 
some cases when some of the intervals are very 
narrow (e.g. the interval A in Figure 1), moving the 
optimal value ?d* slightly can lead to much larger 
sample risk. Intuitively, we prefer a stable solution 
which is also known as a robust solution (with even 
slightly higher sample risk, e.g. the interval B in 
Figure 1). Following Quirk et al (2004), we evaluate 
each interval in the sequence by its corresponding 
smoothed sample risk. Let ? be the midpoint of an 
interval and SR(?) be the corresponding sample risk 
of the interval. The smoothed sample risk of the 
interval is defined as 
???? d
b
b
 )SR(? +?   
where b is a smoothing factor whose value is de-
termined empirically  (0.06 in our experiments). As 
shown in Figure 1, a more stable interval B is se-
lected according to the smoothed sample risk. 
In addition to reducing GEN(A) to an active 
candidate list described above, the efficiency of the 
line search can be further improved. We find that 
the line search only needs to traverse a small subset 
of training samples because the distribution of 
features among training samples are very sparse. 
Therefore, we built an inverted index that lists for 
each feature all training samples that contain it. As 
will be shown in Section 4.2, the line search is very 
efficient even for a large training set with millions of 
candidate features. 
3.4 Feature Subset Selection 
This section describes our method of selecting 
among millions of features a small subset of highly 
effective features for MSR learning. Reducing the 
number of features is essential for two reasons: to 
reduce computational complexity and to ensure the 
generalization property of the linear model. A large 
number of features lead to a large number of pa-
rameters of the resulting linear model, as described 
in Section 3.1. For a limited number of training 
samples, keeping the number of features suffi-
ciently small should lead to a simpler model that is 
less likely to overfit to the training data. 
The first step of our feature selection algorithm 
treats the features independently. The effectiveness 
of a feature is measured in terms of the reduction of 
the sample risk on top of the base feature f0. For-
mally, let SR(f0) be the sample risk of using the base 
feature only, and SR(f0 + ?dfd) be the sample risk of 
using both f0 and fd and the parameter ?d that has 
been optimized using the line search. Then the 
effectiveness of fd, denoted by E(fd), is given by 
))SR()(SR(max
)SR()SR(
)(
00
...1,
00
ii
Dif
dd
d fff
fff
fE
i
?
?
+?
+?=
=
, (5)
where the denominator is a normalization term to 
ensure that E(f) ? [0, 1]. 
The feature selection procedure can be stated as 
follows: The value of E(.) is computed according to 
Equation (5) for each of the candidate features. 
Features are then ranked in the order of descending 
values of E(.). The top l features are selected to form 
the feature vector in the linear model. 
Treating features independently has the ad-
vantage of computational simplicity, but may not 
be effective for features with high correlation. For 
instance, although two features may carry rich 
discriminative information when treated sepa-
rately, there may be very little gain if they are com-
bined in a feature vector, because of the high cor-
relation between them. Therefore, in what follows, 
we describe a technique of incorporating correla-
tion information in the feature selection criterion.  
Let xmd, m = 1?M and d = 1?D, be a Boolean 
value: xmd = 1 if the sample risk reduction of using 
the d-th feature on the m-th training sample, com-
B 
A
212
puted by Equation (5), is larger than zero, and 0 
otherwise. The cross correlation coefficient be-
tween two features fi and fj is estimated as 
??
?
==
==
M
m mj
M
m mi
M
m mjmi
xx
xx
jiC
1
2
1
2
1),( . (6)
It can be shown that C(i, j) ? [0, 1]. Now, similar to  
(Theodoridis and Koutroumbas 2003), the feature 
selection procedure consists of the following steps, 
where fi denotes any selected feature and fj denotes 
any candidate feature to be selected. 
Step 1. For each of the candidate features (fd, for d = 
1?D), compute the value of E(f) according to 
Equation (5). Rank them in a descending order and 
choose the one with the highest E(.) value. Let us 
denote this feature as f1. 
Step 2. To select the second feature, compute the 
cross correlation coefficient between the selected 
feature f1 and each of the remaining M-1 features, 
according to Equation (6). 
Step 3. Select the second feature f according to { } ),1()1()(maxarg*
...2
jCfEj j
Dj
?? ??=
=
 
where ? is the weight that determines the relative 
importance we give to the two terms. The value of 
? is optimized on held-out data (0.8 in our experi-
ments). This means that for the selection of the 
second feature, we take into account not only its 
impact of reducing the sample risk but also the 
correlation with the previously selected feature. It 
is expected that choosing features with less corre-
lation gives better sample risk minimization. 
Step 4. Select k-th features, k = 3?K, according to 
??
?
??
?
?
??= ??
=
1
1
),(
1
1
)(maxarg*
k
i
j
j
jiC
k
fEj
??  (7)
That is, we select the next feature by taking into 
account its average correlation with all previously 
selected features. The optimal number of features, l, 
is determined on held-out data. 
Similarly to the case of line search, we need to 
deal with the efficiency issue in the feature selec-
tion method. As shown in Equation (7), the esti-
mates of E(.) and C(.) need to be computed. Let D 
and K (K << D) be the number of all candidate 
features and the number of features in the resulting 
model, respectively. According to the feature se-
lection method described above, we need to esti-
mate E(.) for each of the D candidate features only 
once in Step 1. This is not very costly due to the 
efficiency of our line search algorithm. Unlike the 
case of E(.), O(K?D) estimates of C(.) are required in 
Step 4. This is computationally expensive even for a 
medium-sized K. Therefore, every time a new fea-
ture is selected (in Step 4), we only estimate the 
value of C(.) between each of the selected features 
and each of the top N remaining features with the 
highest value of E(.). This reduces the number of 
estimates of C(.) to O(K?N). In our experiments we 
set N = 1000, much smaller than D. This reduces the 
computational cost significantly without producing 
any noticeable quality loss in the resulting model. 
The MSR algorithm used in our experiments is 
summarized in Figure 2. It consists of feature se-
lection (line 2) and optimization (lines 3 - 5) steps. 
1 Set ?0 = 1 and ?d = 0 for d=1?D 
2 Rank all features and select the top K features, using 
the feature selection method described in Section 3.4
3 For t = 1?T (T= total number of iterations) 
4 For each k = 1?K  
5    Update the parameter of fk using line search.  
Figure 2: The MSR algorithm 
4 Evaluation 
4.1 Settings 
We evaluated MSR on the task of Japanese IME. 
Two newspaper corpora are used as training and 
test data: Nikkei and Yomiuri Newspapers. Both 
corpora have been pre-word-segmented using a 
lexicon containing 167,107 entries. A 5,000-sentence 
subset of the Yomiuri Newspaper corpus  was used 
as held-out data (e.g. to determine learning rate, 
number of iterations and features etc.). We tested 
our models on another  5,000-sentence subset of the 
Yomiuri Newspaper corpus.  
We used an 80,000-sentence subset of the Nikkei 
Newspaper corpus as the training set. For each A, 
we produced a word lattice using the baseline 
system described in (Gao et al 2002), which uses a 
word trigram model trained via MLE on anther 
400,000-sentence subset of the Nikkei Newspaper 
corpus. The two subsets do not overlap so as to 
simulate the case where unseen phonetic symbol 
strings are converted by the baseline system. For 
efficiency, we kept for each training sample the 
best 20 hypotheses in its candidate conversion set 
GEN(A) for discriminative training. The oracle best 
hypothesis, which gives the minimum number of 
errors, was used as the reference transcript of A. 
213
4.2 Results 
We used unigrams and bigrams that occurred more 
than once in the training set as features. We did not 
use trigram features because they did not result in a 
significant improvement in our pilot study. The 
total number of candidate features we used was 
around 860,000.  
Our main experimental results are shown in 
Table 1. Row 1 is our baseline result using the word 
trigram model. Notice that the result is much better 
than the state-of-the-art performance currently 
available in the marketplace (e.g. Gao et al 2002), 
presumably due to the large amount of training 
data we used, and to the similarity between the 
training and the test data. Row 2 is the result of the 
model trained using the MSR algorithm described 
in Section 3. We also compared the MSR algorithm 
to two of the state-of-the-art discriminative training 
methods: Boosting in Row 3 is an implementation 
of the improved algorithm for the boosting loss 
function proposed in (Collins 2000), and Percep-
tron in Row 4 is an implementation of the averaged 
perceptron algorithm described in (Collins 2002).  
We see that all discriminative training methods 
outperform MLE significantly (p-value < 0.01). In 
particular, MSR outperforms MLE by more than 
20% CER reduction. Notice that we used only uni-
gram and bigram features that have been included 
in the baseline trigram model, so the improvement 
is solely attributed to the high performance of MSR. 
We also find that MSR outperforms the perceptron 
and boosting methods by a small but statistically 
significant margin. 
The MSR algorithm is also very efficient: using a 
subset of 20,000 features, it takes less than 20 min-
utes to converge on an XEON(TM) MP 1.90GHz 
machine. It is as efficient as the perceptron algo-
rithm and slightly faster than the boosting method. 
4.3 Robustness Issues 
Most theorems that justify the robustness of dis-
criminative training algorithms concern two ques-
tions. First, is there a guarantee that a given algo-
rithm converges even if the training samples are 
not linearly separable? This is called the convergence 
problem. Second, how well is the training error 
reduction preserved when the algorithm is applied 
to unseen test samples? This is called the generali-
zation problem. Though we currently cannot give a 
theoretical justification, we present empirical evi-
dence here for the robustness of the MSR approach. 
As Vapnik (1999) pointed out, the most robust 
linear models are the ones that achieve the least 
training errors with the least number of features. 
Therefore, the robustness of the MSR algorithm are 
mainly affected by the feature selection method. To 
verify this, we created four different subsets of 
features using different settings of the feature se-
lection method described in Section 3.4. We se-
lected different numbers of features (i.e. 500 and 
2000) with and without taking into account the 
correlation between features (i.e. ? in Equation (7) 
is set to 0.8 and 1, respectively). For each of the four 
feature subsets, we used the MSR algorithm to 
generate a set of models. The CER curves of these 
models on training and test data sets are shown in 
Figures 3 and 4, respectively.  
2.08
2.10
2.12
2.14
2.16
2.18
2.20
2.22
2.24
2.26
2.28
1 250 500 750 1000 1250 1500 1750 2000
# of rounds
C
E
R
(%
)
MSR(?=1)-2000
MSR(?=1)-500
MSR(?=0.8)-2000
MSR(?=0.8)-500
 
Figure 3. Training error curves of the MSR algorithm 
2.94
2.99
3.04
3.09
3.14
3.19
3.24
1 250 500 750 1000 1250 1500 1750 2000
# of rounds
C
E
R
(%
)
MSR(?=1)-2000
MSR(?=1)-500
MSR(?=0.8)-2000
MSR(?=0.8)-500
 
Figure 4. Test error curves of the MSR algorithm 
The results reveal several facts. First, the con-
vergence properties of MSR are shown in Figure 3 
where in all cases, training errors drop consistently 
with more iterations. Secondly, as expected, using 
more features leads to overfitting, For example, 
MSR(? =1)-2000 makes fewer errors than MSR(? 
=1)-500 on training data but more errors on test 
data. Finally, taking into account the correlation 
between features (e.g. ? = 0.8 in Equation (7)) re-
 Model CER (%) % over MLE 
1. MLE  3.70 -- 
2. MSR (K=2000) 2.95 20.9 
3. Boosting  3.06 18.0 
4. Perceptron 3.07 17.8 
Table 1. Comparison of CER results. 
214
sults in a better subset of features that lead to not 
only fewer training errors, as shown in Figure 3, 
but also better generalization properties (fewer test 
errors), as shown in Figure 4. 
4.4 Domain Adaptation Results  
Though MSR achieves impressive performance in 
CER reduction over the comparison methods, as 
described in Section 4.2, the experiments are all 
performed using newspaper text for both training 
and testing, which is not a realistic scenario if we 
are to deploy the model in an application. This 
section reports the results of additional experi-
ments in which we adapt a model trained on one 
domain to a different domain, i.e., in a so-called 
cross-domain LM adaptation paradigm. See (Su-
zuki and Gao 2005) for a detailed report. 
The data sets we used stem from five distinct 
sources of text. The Nikkei newspaper corpus de-
scribed in Section 4.1 was used as the background 
domain, on which the word trigram model was 
trained. We used four adaptation domains: Yomi-
uri (newspaper corpus), TuneUp (balanced corpus 
containing newspapers and other sources of text), 
Encarta (encyclopedia) and Shincho (collection of 
novels). For each of the four domains, we used an 
72,000-sentence subset as adaptation training data, 
a 5,000-sentence subset as held-out data and an-
other 5,000-sentence subset as test data. Similarly, 
all corpora have been word-segmented, and we 
kept for each training sample, in the four adapta-
tion domains, the best 20 hypotheses in its candi-
date conversion set for discriminative training.  
We compared MSR with three other LM adap-
tation methods:  
Baseline is the background word trigram model, 
as described in Section 4.1. 
MAP (maximum a posteriori) is a traditional LM 
adaptation method where the parameters of the 
background model are adjusted in such a way that 
maximizes the likelihood of the adaptation data. 
Our implementation takes the form of linear in-
terpolation as P(wi|h) = ?Pb(wi|h) + (1-?)Pa(wi|h), 
where Pb is the probability of the background 
model, Pa is the probability trained on adaptation 
data using MLE and the history h corresponds to 
two preceding words (i.e. Pb and Pa are trigram 
probabilities). ? is the interpolation weight opti-
mized on held-out data.  
Perceptron, Boosting and MSR are the three 
discriminative methods described in the previous 
sections.  For each of them, the base feature was 
Model Yomiuri TuneUp Encarta Shincho 
Baseline 3.70 5.81 10.24 12.18 
MAP  3.69 5.47 7.98 10.76 
MSR  2.73 5.15 7.40 10.16 
Boosting  2.78 5.33 7.53 10.25 
Perceptron 2.78 5.20 7.44 10.18 
Table 2. CER(%) results on four adaptation test sets . 
derived from the word trigram model trained on 
the background data, and other n-gram features (i.e. 
fd, d = 1?D in Equation (2)) were trained on adap-
tation data. That is, the parameters of the back-
ground model are adjusted in such a way that 
minimizes the errors on adaptation data made by 
background model. 
Results are summarized in Table 2. First of all, 
in all four adaptation domains, discriminative 
methods outperform MAP significantly. Secondly, 
the improvement margins of discriminative 
methods over MAP correspond to the similarities 
between background domain and adaptation do-
mains. When the two domains are very similar to 
the background domain (such as Yomiuri), dis-
criminative methods outperform MAP by a large 
margin. However, the margin is smaller when the 
two domains are substantially different (such as 
Encarta and Shincho). The phenomenon is attrib-
uted to the underlying difference between the two 
adaptation methods: MAP aims to improve the 
likelihood of a distribution, so if the adaptation 
domain is very similar to the background domain, 
the difference between the two underlying distri-
butions is so small that MAP cannot adjust the 
model effectively. However, discriminative meth-
ods do not have this limitation for they aim to 
reduce errors directly. Finally, we find that in most 
adaptation test sets, MSR achieves slightly better 
CER results than the two competing discriminative 
methods. Specifically, the improvements of MSR 
are statistically significant over the boosting 
method in three out of four domains, and over the 
perceptron algorithm in the Yomiuri domain. The 
results demonstrate again that MSR is robust. 
5 Related Work 
Discriminative models have recently been proved 
to be more effective than generative models in 
some NLP tasks, e.g., parsing (Collins 2000), POS 
tagging (Collins 2002) and LM for speech recogni-
tion (Roark et al 2004). In particular, the linear 
models, though simple and non-probabilistic in 
nature, are preferred to their probabilistic coun-
215
terpart such as logistic regression. One of the rea-
sons, as pointed out by Ng and Jordan (2002), is 
that the parameters of a discriminative model can 
be fit either to maximize the conditional likelihood 
on training data, or to minimize the training errors. 
Since the latter optimizes the objective function that 
the system is graded on, it is viewed as being more 
truly in the spirit of discriminative learning. 
The MSR method shares the same motivation: to 
minimize the errors directly as much as possible. 
Because the error function on a finite data set is a 
step function, and cannot be optimized easily, 
previous research approximates the error function 
by loss functions that are suitable for optimization 
(e.g. Collins 2000; Freund et al 1998; Juang et al 
1997; Duda et al 2001). MSR uses an alternative 
approach. It is a simple heuristic training proce-
dure to minimize training errors directly without 
applying any approximated loss function. 
MSR shares many similarities with previous 
methods. The basic training algorithm described in 
Section 3.2 follows the general framework of multi- 
dimensional optimization (e.g., Press et al 1992). 
The line search is an extension of that described in 
(Och 2003; Quirk et al 2005. The extension lies in 
the way of handling large number of features and 
training samples. Previous algorithms were used to 
optimize linear models with less than 10 features. 
The feature selection method described in Section 
3.4 is a particular implementation of the feature 
selection methods described in (e.g., Theodoridis 
and Koutroumbas 2003). The major difference 
between the MSR and other methods is that it es-
timates the effectiveness of each feature in terms of 
its expected training error reduction while previ-
ous methods used metrics that are loosely coupled 
with reducing training errors. The way of dealing 
with feature correlations in feature selection in 
Equation (7), was suggested by Finette et al (1983). 
6 Conclusion and Future Work 
We show that MSR is a very successful discrimina-
tive training algorithm for LM. Our experiments 
suggest that it leads to significantly better conver-
sion performance on the IME task than either the 
MLE method or the two widely applied discrimi-
native methods, the boosting and perceptron 
methods. However, due to the lack of theoretical 
underpinnings, we are unable to prove that MSR 
will always succeed. This forms one area of our 
future work. 
One of the most interesting properties of MSR is 
that it can optimize any objective function (whether 
its gradient is computable or not), such as error rate 
in IME or speech, BLEU score in MT, precision and 
recall in IR (Gao et al 2005). In particular, MSR can 
be performed on large-scale training set with mil-
lions of candidate features. Thus, another area of 
our future work is to test MSR on wider varieties of 
NLP tasks such as parsing and tagging. 
References 
Collins, Michael. 2002. Discriminative training methods 
for Hidden Markov Models: theory and experiments 
with the perceptron algorithm. In EMNLP 2002. 
Collins, Michael. 2000. Discriminative reranking for 
natural language parsing. In ICML 2000. 
Duda, Richard O, Hart, Peter E. and Stork, David G. 2001. 
Pattern classification. John Wiley & Sons, Inc. 
Finette S., Blerer A., Swindel W. 1983. Breast tissue clas-
sification using diagnostic ultrasound and pattern rec-
ognition techniques: I. Methods of pattern recognition. 
Ultrasonic Imaging, Vol. 5, pp. 55-70. 
Freund, Y, R. Iyer, R. E. Schapire, and Y. Singer. 1998. An 
efficient boosting algorithm for combining preferences. 
In ICML?98.  
Gao, Jianfeng, Hisami Suzuki and Yang Wen. 2002. 
Exploiting headword dependency and predictive clus-
tering for language modeling. In EMNLP 2002. 
Gao, J, H. Qin, X. Xiao and J.-Y. Nie. 2005. Linear dis-
criminative model for information retrieval. In SIGIR.  
Jelinek, Fred. 1997. Statistical methods for speech recognition. 
MIT Press, Cambridge, Mass. 
Juang, B.-H., W.Chou and C.-H. Lee. 1997. Minimum 
classification error rate methods for speech recognition. 
IEEE Tran. Speech and Audio Processing 5-3: 257-265.  
Ng, A. N. and M. I. Jordan. 2002. On discriminative vs. 
generative classifiers: a comparison of logistic regres-
sion and na?ve Bayes. In NIPS 2002: 841-848. 
Och, Franz Josef. 2003. Minimum error rate training in 
statistical machine translation. In ACL 2003 
Press, W. H., S. A. Teukolsky, W. T. Vetterling and B. P. 
Flannery. 1992. Numerical Recipes In C: The Art of Scien-
tific Computing. New York: Cambridge Univ. Press. 
Quirk, Chris, Arul Menezes, and Colin Cherry. 2005. 
Dependency treelet translation: syntactically informed 
phrasal SMT. In ACL 2005: 271-279.  
Roark, Brian, Murat Saraclar and Michael Collins. 2004. 
Corrective language modeling for large vocabulary ASR 
with the perceptron algorithm. In ICASSP 2004. 
Suzuki, Hisami and Jianfeng Gao. 2005. A comparative 
study on language model adaptation using new 
evaluation metrics. In HLT/EMNLP 2005. 
Theodoridis, Sergios and Konstantinos Koutroumbas. 
2003. Pattern Recognition. Elsevier. 
Vapnik, V. N. 1999. The nature of statistical learning theory. 
Springer-Verlag, New York. 
216
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 957 ? 968, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
An Empirical Study on Language Model Adaptation 
Using a Metric of Domain Similarity 
Wei Yuan1, Jianfeng Gao2, and Hisami Suzuki3 
1
 Shanghai Jiao Tong University, 1954 Huashan Road, Shanghai 200030 
sunnyuanovo@sjtu.edu.cn 
2
 Microsoft Research, Asia, 49 Zhichun Road, Haidian District, Beijing 100080 
jfgao@microsoft.com 
3
 Microsoft Research, One Microsoft Way, Redmond WA 98052 
hisamis@microsoft.com 
Abstract. This paper presents an empirical study on four techniques of lan-
guage model adaptation, including a maximum a posteriori (MAP) method and 
three discriminative training models, in the application of Japanese Kana-Kanji 
conversion. We compare the performance of these methods from various angles 
by adapting the baseline model to four adaptation domains. In particular, we at-
tempt to interpret the results given in terms of the character error rate (CER) by 
correlating them with the characteristics of the adaptation domain measured us-
ing the information-theoretic notion of cross entropy. We show that such a met-
ric correlates well with the CER performance of the adaptation methods, and 
also show that the discriminative methods are not only superior to a MAP-based 
method in terms of achieving larger CER reduction, but are also more robust 
against the similarity of background and adaptation domains.  
1   Introduction 
Language model (LM) adaptation attempts to adjust the parameters of a LM so that it 
performs well on a particular domain of data. This paper presents an empirical study 
of several LM adaptation methods on the task of Japanese text input. In particular, we 
focus on the so-called cross-domain LM adaptation paradigm, i.e. to adapt a LM 
trained on one domain to a different domain, for which only a small amount of train-
ing data is available. 
The LM adaptation methods investigated in this paper can be grouped into two 
categories: maximum a posterior (MAP) and discriminative training. Linear interpola-
tion is representative of the MAP methods [1]. The other three methods, including the 
boosting [2] and perceptron [3] algorithms and minimum sample risk (MSR) method 
[4], are discriminative methods, each of which uses a different training algorithm. 
We carried out experiments over many training data sizes on four distinct adapta-
tion corpora, the characteristics of which were measured using the information-
theoretic notion of cross entropy. We found that discriminative training methods  
                                                          
1
 This research was conducted while the author was visiting Microsoft Research Asia.  
958 W. Yuan, J. Gao, and H. Suzuki 
outperformed the LI method in all cases, and were more robust across different train-
ing sets of different domains and sizes. However, none of the discriminative training 
methods was found to outperform the others in our experiments.  
The paper is organized as follow. Section 2 introduces the task of IME and the role 
of LM. In Section 3, we review related work. After a description of the LM adaptation 
methods in our experiments in Section 4, Sections 5 and 6 present experimental re-
sults and their discussions. We conclude our paper in Section 7. 
2   Language Model and the Task of IME 
Our study falls into the context of Asian language (Japanese in this study) text input. 
The standard method for doing this is that the users first input the phonetic strings, 
which are then converted into the appropriate word string by software. The task of 
automatic conversion is called IME in this paper, which stands for Input Method Edi-
tor, based on the name of the commonly used Windows-based application. 
The performance of IME is typically measured in terms of the character error rate 
(CER), which is the number of characters wrongly converted from the phonetic string 
divided by the number of characters in the correct transcript. Current Japanese IME 
systems exhibit about 5-15% CER in conversion of real-world data in a wide variety 
of domains. In the following, we argue that the IME is a similar problem to speech 
recognition but is a better choice for evaluating language modeling techniques. 
Similar to speech recognition, IME can also be viewed as a Bayesian decision 
problem. Let A be the input phonetic string (which corresponds to the acoustic signal 
in speech). The task of IME is to choose the most likely word string W* among those 
candidates that could have been converted from A: 
)|()(maxarg)(
),(
maxarg)|(maxarg*
)()()(
WAPWP
AP
AWPAWPW
AGENWAGENWAGENW ???
===
 (1) 
where GEN(A) denotes the candidate set given A. 
Unlike speech recognition, there is almost no acoustic ambiguity in IME, because 
the phonetic string is provided directly by users. Moreover, we can assume a many-to-
one mapping from W to A in IME, i.e. P(A|W) = 1. So the decision of Equation (1) 
depends solely upon P(W), making IME a more direct evaluation test-bed for LM 
than speech recognition. Another advantage is that it is relatively easy to convert W to 
A, making it possible to obtain a large amount of training data for discriminative 
learning, as described later.  
3   Related Work 
Our goal is to quantify the characteristics of different domains of text, and to correlate 
them with the performance of various techniques for LM adaptation to compare their 
effectiveness and robustness. This relates our work to the study of domain similarity 
calculation and to different techniques for LM adaptation. 
 An Empirical Study on Language Model Adaptation 959 
3.1 Measuring Domain Similarity 
Statistical language modeling (SLM) assumes that language is generated from under-
lying distributions. When we discuss different domains of text, we assume that the 
text from each of these domains is generated from a different underlying distribution. 
We therefore consider the problem of distributional similarity in this paper. 
Cross entropy is a widely used measure in evaluating LM. Given a language L 
with its true underlying probability distribution p and another distribution q (e.g. a 
SLM) which attempts to model L, the cross entropy of L with respect to q is 
?
??
?=
nww
nn
n
wwqwwp
n
qLH
...
11
1
)...(log)...(1lim),(  (2) 
where w1?wn is a word string in L. However, in reality, the underlying p is never 
known and the corpus size is never infinite. We therefore make the assumption that L 
is an ergodic and stationary process [5], and approximate the cross entropy by calcu-
lating it for a sufficiently large n instead of calculating it for the limit. 
)...(log1),( 1 nwwq
n
qLH ??  (3) 
This measures how well a model approximates the language L.  
The KL divergence, or relative entropy, is another measure of distributional simi-
larity that has been widely used in NLP and IR [6]. Given the two distributions p and 
q above, the KL divergence is defined as 
?=
nww n
n
nnn
wwq
wwp
wwpwwqwwpD
...1 1
1
111 )...(
)...(log)...())...(||)...((  (4) 
The cross entropy and the KL divergence are related notions. Given the notations 
of L, p and q above, [5] shows that   
)||()(),( qpDLHqLH +=
 (5) 
In other words, the cross entropy takes into account both the similarity between two 
distributions (given by KL divergence) and the entropy of the corpus in question, both 
of which contribute to the complexity of a LM task. In this paper we are interested in 
measuring the complexity of the LM adaptation task. We therefore define the similar-
ity between two domains using the cross entropy. We will also use the metric that 
approximates the entropy of the corpus to capture the in-domain diversity of a corpus 
in Section 5.2.2  
3.2 LM Adaptation Methods 
In this paper, two major approaches to cross-domain adaptation have been investi-
gated: maximum a posteriori (MAP) estimation and discriminative training methods. 
                                                          
2
  There are other well-known metrics of similarity within NLP literature, such as the mutual 
information or cosine similarity [7], which we do not discuss in this paper.  
960 W. Yuan, J. Gao, and H. Suzuki 
In MAP estimation methods, adaptation data is used to adjust the parameters of the 
background model so as to maximize the likelihood of the adaptation data [1]. Dis-
criminative training methods to LM adaptation, on the other hand, aim at using the 
adaptation data to directly minimize the errors on the adaptation data made by the 
background model. These techniques have been applied successfully to the task of 
language modeling in non-adaptation [8] as well as adaptation scenarios [9] for speech 
recognition. But most of them focused on the investigation of performance of certain 
methods for LM adaptation, without analyzing in detail the underlying reasons of 
different performance achieved by different methods. In this paper we attempt to 
investigate the effectiveness of different discriminative methods in an IME adaptation 
scenario, with a particular emphasis on correlating their performance with the charac-
teristics of adaptation domain. 
4   LM Adaptation Methods 
We implement four methods in our experiments. The Linear Interpolation (LI) falls 
into the framework of MAP while the boosting, the perceptron and the MSR methods 
fall into that of discriminative training.  
4.1   Linear Interpolation (MAP) 
In MAP estimation methods, adaptation data is used to adjust the parameters of the 
background model so as to maximize the likelihood of the adaptation data. 
The linear interpolation is a special case of MAP according to [10]. At first, we 
generate trigram models on background data and adaptation data respectively. The 
two models are then combined into one as: 
)|()1()|()|( hwPhwPhwP iAiBi ?? ?+=  (6) 
where PB is the probability of the background model, PA is the probability of the adap-
tation model and the history h corresponds to two preceding words. For simplicity, we 
chose a single ? for all histories and tuned it on held-out data. 
4.2   Discriminative Training 
Discriminative training follows the general framework of linear models [2][3]. We use 
the following notation in the rest of the paper. 
? Training data is a set of example input/output pairs {Ai, WiR} for i = 1?M, where 
each Ai is an input phonetic string and each WiR is the reference transcript of Ai. 
? We assume a set of D+1 features, fd(W), for d=0?D, where each feature is a func-
tion that maps W to a real value. Using vector notation, we have f(W)={ f0(W), 
f1(W)?fD(W)} and f(W) .1+? DR  Without loss of generality, f0(W) is called the base 
model feature, and is defined in this paper as the log probability that the back-
ground trigram model assigns to W. fd(W), for d=1?D, are defined as the counts of 
the word n-gram in W, where n = 1 and 2 in our case. 
 An Empirical Study on Language Model Adaptation 961 
? Finally, the parameters of the model form a vector of D + 1 dimensions, each for 
one feature function, ?= {?0, ?1? ?D}. The likelihood score of a word string W is 
?
=
==
D
d
dd WfWWScore
0
)()(),( ??f?  (7) 
Then the decision rule of Equation (1) can be re-written as  
),(maxarg),(
(A)
* ??
GEN
WScoreAW
W?
=
 (8) 
Assume that we can measure the number of conversion errors in W by comparing it 
with a reference transcript WR using an error function Er(WR, W), which is an edit 
distance in our case. We call the sum of conversion errors over the training data as 
sample risk (SR). Discriminative training methods strive to minimize the SR by opti-
mizing the model parameters, as defined in Equation (9). 
?
=
==
Mi
ii
R
i
* AWWErSR
...1
)),(,(minarg)(minarg ???
??
 (9) 
However, SR(.) cannot be optimized easily since Er(.) is a piecewise constant (or 
step) function of ? and its gradient is undefined. Therefore, discriminative methods 
apply different approaches that optimize it approximately. As we shall describe be-
low, the boosting and perceptron algorithms approximate SR(.) by loss functions that 
are suitable for optimization, while MSR uses a simple heuristic training procedure to 
minimize SR(.) directly without applying any approximated loss function. We now 
describe each of the discriminative methods in turn. 
The boosting algorithm [2] uses an exponential function to approximate SR(.). We 
define a ranking error in a case where an incorrect candidate conversion W gets a 
higher score than the correct conversion WR. The margin of the pair (WR, W) with 
respect to the model ? is estimated as 
),(),(),( ?? WScoreWScoreWWM RR ?=  (10) 
Then we define an upper bound to the number of ranking errors as the loss function, 
? ?
= ?
?=
Mi AW
i
R
i
ii
WWM
...1 )(
)),(exp()ExpLoss(
GEN
?  (11) 
Now, ExpLoss(.) is convex with respect to ?, so there are no problems with local 
minima when optimizing it. The boosting algorithm can be viewed as an iterative 
feature selection method: at each iteration, the algorithm selects from all possible 
features the one that is estimated to have the largest impact on reducing the ExpLoss 
function with respect to the current model, and then optimizes the current model by 
adjusting only the parameter of the selected feature while keeping the parameters of 
other features fixed. 
The perceptron algorithm [3] can be viewed as a form of incremental training pro-
cedure that optimizes a minimum square error (MSE) loss function, which is an ap-
proximation of SR(.). As shown in Figure 1, it starts with an initial parameter setting 
and adapts it each time a training sample is wrongly converted.  
962 W. Yuan, J. Gao, and H. Suzuki 
1 Initialize all parameters in the model, i.e. ?0 = 1 and ?d = 0 for d=1?D 
2 For t = 1?T, where T is the total number of iterations 
For each training sample (Ai, WiR), i = 1?M 
Use current model ? to choose some Wi from GEN(Ai) by Equation (8) 
    For d = 1 ? D 
        ?d  = ?d + ?(fd (WiR)- fd (Wi)), where ? is the size of the learning step  
Fig. 1. The standard perceptron algorithm with delta rule 
In our experiments, we used the average perceptron algorithm in [3], a simple refine-
ment to the algorithm in Figure 1, which has been proved to be more robust. Let ?dt,i 
be the value for the dth parameter after the ith training sample has been processed in 
pass t over the training data. Then the ?average parameters? are defined as in Equa-
tion (12). 
)/()()(
1 1
, MT
T
t
M
i
it
davgd ?= ??
= =
??  (12) 
The minimum sample risk (MSR) method [4] can be viewed as a greedy stage-
wise learning algorithm that minimizes the sample risk SR(.) directly as it appears in 
Equation (9). Similar to the boosting method, it is an iterative procedure. In each 
iteration, MSR selects a feature that is estimated to be most effective in terms of re-
ducing SR(.), and then optimizes the current model by adjusting the parameters of the 
selected feature. MSR, however, differs from the boosting method in that MSR tries 
to optimize the sample risk directly while the boosting optimizes the loss function that 
is an upper bound of the sample risk.  
As mentioned earlier, SR(.) can be optimized using regular gradient-based optimi-
zation algorithms. MSR therefore uses a particular implementation of line search, 
originally proposed in [11], to optimize the current model by adjusting the parameter 
of a selected feature while keeping other parameters fixed.  
Assuming fd is the selected feature, its parameter ?d is optimized by line search as 
follows. Recall that Er(WR,W) is the function that measures the number of conversion 
errors in W versus its reference transcript WR. The value of SR(.) is the sum of Er(.) 
over all training samples. For each A in training set, let GEN(A) be the set of n-best 
candidate word strings that could be converted from A. By adjusting ?d, we obtain for 
each training sample an ordered sequence of ?d intervals. For ?d in each interval, a 
particular candidate would be selected according to Equation (8). Then the corre-
sponding Er(.) is associated with the interval. As a result, for each training sample, we 
obtain a sequence of ?d intervals and their corresponding Er(.) values. By combining 
the sequences over all training samples, we obtain a global sequence of ?d intervals, 
each of which is associated with a SR(.) value. Therefore we can find the optimal 
interval of ?d as well as its corresponding sample risk by traversing the sequence and 
taking the center of the interval as the optimal value of ?d. 
 An Empirical Study on Language Model Adaptation 963 
5   Experimental Results 
5.1   Data 
The data used in our experiments stem from five distinct sources of text. A 36-
million-word Nikkei newspaper corpus was used as the background domain. We used 
four adaptation domains: Yomiuri (newspaper corpus), TuneUp (balanced corpus 
containing newspaper and other sources of text), Encarta (encyclopedia) and Shincho 
(collection of novels).  
For the computation of domain characteristics (Section 5.2), we extracted 1 million 
words from the training data of each domain respectively (corresponding to 13K to 
78K sentences depending on the domain). For this experiment, we also used a lexicon 
consisting of the words in our baseline lexicon (167,107 words) plus all words in the 
corpora used for this experiment (that is, 1M words times 5 domains), which included 
216,565 entries. The use of such a lexicon was motivated by the need to eliminate the 
effect of out-of-vocabulary (OOV) items.  
For the experiment of LM adaptation (Section 5.3), we created training data con-
sisting of 72K sentences (0.9M~1.7M words) and test data of 5K sentences 
(65K~120K words) from each adaptation domain. The first 800 and 8,000 sentences 
of each adaptation training data were also used to show how different sizes of adapta-
tion training data affected the performances of various adaptation methods. Another 
5K-sentence subset was used as held-out data for each domain. For domain adaptation 
experiments, we used our baseline lexicon consisting of 167,107 entries.   
5.2   Computation of Domain Characteristics 
The first domain characteristic we computed was the similarity between two domains 
for the task of LM. As discussed in Section 3, we used the cross entropy as the metric: 
we first trained a word trigram model using the system described in [12] on the 1-
million-word corpus of domain B, and used it in the computations of the cross entropy 
H(LA, qB) following equation (3). For simplicity, we denote H(LA, qB) as H(A,B).  
Table 1 displays the cross entropy between two domains of text. Note that the cross 
entropy is not symmetric, i.e., H(A,B) is not necessarily the same as H(B,A). In order 
to have a representative metric of similarity between two domains, we computed the 
average cross entropy between two domains, shown in Table 2, and used this quantity 
as the metric for domain similarity.   
Along the main diagonal in the tables below, we also have the cross entropy com-
puted for H(A,A), i.e., when two domains we compare are the same (in boldface). This 
value, which we call self entropy for convenience, is an approximation of the entropy 
of the corpus A, and measures the amount of information per word, i.e., the diversity 
of the corpus. Note that the self entropy increases in the order of Nikkei ? Yomiuri 
? Encarta ? TuneUp ? Shincho. This indeed reflects the in-domain variability of 
text: Nikkei, Yomiuri and Encarta are highly edited text, following style guidelines; 
they also tend to have repetitious content. In contrast, Shincho is a collection of nov-
els, on which no style or content restriction is imposed. We expect that the LM task to 
964 W. Yuan, J. Gao, and H. Suzuki 
be more difficult as the corpus is more diverse; we will further discuss the effect of 
diversity in Section 6.3 
Table 1. Cross entropy (rows: corpora; column: models) 
 Nikkei Yomiuri TuneUp Encarta Shincho 
Nikkei 3.94 7.46 7.65 9.81 10.10 
Yomiuri 7.93 4.09 7.62 9.26 9.97 
TuneUp 8.25 8.03 4.41 9.04 9.06 
Encarta 8.79 8.66 8.60 4.40 9.30 
Shincho 8.70 8.61 8.07 9.10 4.61 
Table 2. Average cross entropy 
 Nikkei Yomiuri TuneUp Encarta Shincho 
Nikkei 3.94 7.69 7.95 9.30 9.40 
Yomiuri  4.09 7.82 8.96 9.29 
TuneUp   4.41 8.82 8.56 
Encarta    4.40 9.20 
Shincho     4.61 
5.3   Results of LM Adaptation 
We trained our baseline trigram model on our background (Nikkei) corpus using the 
system described in [12]. The CER (%) of this model on each adaptation domain is in 
the second column of Table 3. For the LI adaptation method (the third column of 
Table 3), we trained a word trigram model on the adaptation data, and linearly com-
bined it with the background model, as described in Equation (6).  
For the discriminative methods (the last three columns in Table 3), we produced  a 
candidate word lattice for each input phonetic string in the adaptation training set 
using the background trigram model mentioned above. For efficiency purposes, we 
kept only the best 20 hypotheses from the lattice as the candidate conversion set for 
discriminative training. The lowest CER hypothesis in the lattice, rather than the ref-
erence transcript, was used as the gold standard.  
To compare the performances of different discriminative methods, we fixed the 
following parameter settings: we set the number of iterations N to be 2,000 for the 
boosting and MSR methods (i.e., at most 2,000 features in the final models); for the 
perceptron algorithm, we set T = 40 (in Figure 1). These settings might lead to an 
                                                          
3
  Another derivative notion from Table 1 is the notion of balanced corpus. In Table 1, the 
smallest cross entropy for each text domain (rows) is the self entropy (in boldface), as ex-
pected. Note, however, that the second smallest cross entropy (underlined) is always obtained 
from the TuneUp model (except for Nikkei, for which Yomiuri provides the second smallest 
cross entropy). This reflects the fact that the TuneUp corpus was created by collecting sen-
tences from various sources of text, in order to create a representative test corpus. Using the 
notion of cross entropy, such a characteristic of a test corpus can also be quantified.  
 An Empirical Study on Language Model Adaptation 965 
unfair comparison, as the perceptron algorithm will select far more features than the 
boosting and MSR algorithm. However, we used these settings as they all converged 
under these settings.  All other parameters were tuned empirically. 
In evaluating both MAP and discriminative methods, we used an N-best rescoring 
approach. That is, we created N best hypotheses using the background trigram model 
(N=100 in our experiments) for each sentence in test data, and used domain-adapted 
models to rescore the lattice. The oracle CERs (i.e., the minimal possible CER given 
the hypotheses in the lattice) ranged from 1.45% to 5.09% depending on the adapta-
tion domain. Table 3 below summarizes the results of various adaptation methods in 
terms of CER (%) and CER reduction (in parentheses) over the baseline model. In the 
first column, the numbers in parentheses next to the domain name indicates the num-
ber of training sentences used for adaptation. 
Table 3. CER (%) and CER reduction over Baseline 
Domain Baseline LI Boosting Perceptron MSR 
Yomiuri (800) 3.70 3.70 (0.00%) 3.13 (15.41%) 3.18 (14.05%) 3.17 (14.32%) 
Yomiuri (8K) 3.70 3.69 (0.27%) 2.88 (22.16%) 2.85 (22.97%) 2.88 (22.16%) 
Yomiuri (72K) 3.70 3.69 (0.27%) 2.78 (24.86%) 2.78 (24.86%) 2.73 (26.22%) 
TuneUp (800) 5.81 5.81 (0.00%) 5.69 (2.07%) 5.69 (2.07%) 5.70 (1.89%) 
TuneUp (8K) 5.81 5.70 (1.89%) 5.47 (5.85%) 5.47 (5.85%) 5.47 (5.85%) 
TuneUp (72K) 5.81 5.47 (5.85%) 5.33 (8.26%) 5.20 (10.50%) 5.15 (11.36%) 
Encarta (800) 10.24 9.60 (6.25%) 9.82 (4.10%) 9.43 (7.91%) 9.44 (7.81%) 
Encarta (8K) 10.24 8.64 (15.63%) 8.54 (16.60%) 8.34 (18.55%) 8.42 (17.77%) 
Encarta (72K) 10.24 7.98 (22.07%) 7.53 (26.46%) 7.44 (27.34%) 7.40 (27.73%) 
Shincho (800) 12.18 11.86 (2.63%) 11.91 (2.22%) 11.90 (2.30%) 11.89 (2.38%) 
Shincho (8K) 12.18 11.15 (8.46%) 11.09 (8.95%) 11.20 (8.05%) 11.04 (9.36%) 
Shincho (72K) 12.18 10.76 (11.66%) 10.25 (15.85%) 10.18 (16.42%) 10.16 (16.58%) 
6   Discussion 
6.1   Domain Similarity and CER 
The first row of Table 2 shows that the average cross entropy with respect to the 
background domain (Nikkei) increases in the following order: Yomiuri ? TuneUp ? 
Encarta ? Shincho. This indicates that among the adaptation domains, Yomiuri is the 
most similar to Nikkei, closely followed by TuneUp; Shincho and Encarta are the 
least similar to Nikkei. This is consistent with our intuition, since Nikkei and Yomiuri 
are both newspaper corpora, and TuneUp, which is a manually constructed corpus 
from various representative domains of text, contains newspaper articles.  
966 W. Yuan, J. Gao, and H. Suzuki 
This metric of similarity correlates perfectly with the CER. In Table 3, we see that 
for all sizes of training data for all adaptation methods, the following order of CER 
performance is observed, from better to worse: Yomiuri ? TuneUp ? Encarta ? 
Shincho. In other words, the more similar the adaptation domain is to the background 
domain, the better the CER results are.  
6.2   Domain Similarity and the Effectiveness of Adaptation Methods  
The effectiveness of a LM adaptation method is measured by the relative CER reduc-
tion over the baseline model. Figure 3 shows the CER reduction of various methods 
for each domain when the training data size was 8K.4 
0.00%
5.00%
10.00%
15.00%
20.00%
25.00%
Yomiuri TuneUp Encarta Shincho
LI
Boosting
Perceptron
MSR
  
Fig. 2. CER reduction by different adaptation methods 
In Figure 2 the X-axis is arranged in the order of domain similarity with the back-
ground domain, i.e., Yomiuri ? TuneUp ? Encarta ? Shincho. The first thing we 
note is that the discriminative methods outperform LI in all cases: in fact, for all rows 
in Table 3, MSR outperforms LI in a statistically significant manner (p < 0.01 using t-
test);5 the differences among the three discriminative methods, on the other hand, are 
not statistically significant in most cases. 
We also note that the performance of LI is greatly influenced by domain similarity. 
More specifically, when the adaptation domain is similar to the background domain 
(i.e., for Yomiuri and TuneUp corpora), the contribution of the LI model is extremely 
limited. This can be explained as follows: if the adaptation data is too similar to the 
background, the difference between the two underlying distributions is so slight that 
adding adaptation data leads to no or very small improvements.  
Such a limitation is not observed with the discriminative methods. For example, all 
discriminative methods are quite effective on Yomiuri, achieving more than 20% 
CER reduction. We therefore conclude that discriminative methods, unlike LI, are 
robust against the similarity between background and adaptations domains.  
                                                          
4
 Essentially the same trend is observed with other training data sizes.  
5
 The only exception to this was Shincho (800).  
 An Empirical Study on Language Model Adaptation 967 
6.3   Adaptation Data Size and CER Reduction 
We have seen in Table 3 that in all cases, discriminative methods outperform LI. 
Among the discriminative methods, an interesting characteristic regarding the CER 
reduction and the data size is observed. Figure 3 displays the self entropy of four 
adaptation corpora along the X-axis, and the improvement in CER reduction when 
72K-sentence adaptation data is used over when 800 sentences are used along the Y-
axis. In other words, for each adaptation method, each point in the figure corresponds 
to the CER reduction ratio on a domain (corresponding to Yomiuri, Encarta, TuneUp, 
Shincho from left to right) when 90 times more adaptation data was available.  
0
1
2
3
4
5
6
7
4 4.2 4.4 4.6 4.8
self entropy
C
E
R
 
r
e
d
u
c
t
i
o
n
 
r
a
t
i
o
 
(
%
)
Boosting
Perceptron
MSR
  
Fig. 3. Improvement in CER reduction for discriminative methods by increasing the adaptation 
data size from 800 to 72K sentences 
From this figure, we can see that there is a positive correlation between the diversity 
of the adaptation corpus and the benefit of having more training data available. This 
has an intuitive explanation: the less diverse the adaptation data is, the less distinct 
training examples it will include for discriminative training. This result is useful in 
guiding the process of adaptation data collection.  
7   Conclusion and Future Work 
In this paper, we have examined the performance of various LM adaptation methods 
in terms of domain similarity and diversity. We have found that (1) the notion of 
cross-domain similarity, measured by the cross entropy, correlates with the CER of all 
models (Section 6.1), and (2) the notion of in-domain diversity, measured by the self 
entropy, correlates with the utility of more adaptation training data for discriminative 
training methods (Section 6.3). In comparing discriminative methods with a MAP-
based method, we have also found that (1) the former uniformly achieve better CER 
performance than the latter, and (2) are more robust against the similarity of back-
ground and adaptation data (Section 6.2).  
968 W. Yuan, J. Gao, and H. Suzuki 
Though we believe these results are useful in designing the future experiments in 
domain adaptation, some results and correlations indicated in the paper are still incon-
clusive. We hope to run additional experiments to confirm these findings. We also did 
not fully investigate into characterizing the differences among the three discriminative 
methods; such an investigation is also left for future research.  
References 
1. Bellagarda, J. An Overview of Statistical Language Model Adaptation. In ITRW on Adap-
tation Methods for Speech Recognition (2001): 165-174. 
2. Collins, M. Ranking Algorithms for Name-Entity Extraction: Boosting and the Voted Per-
ceptron. ACL (2002).  
3. Collins, M. Discriminative Training Methods for Hidden Markov Models: Theory and Ex-
periments with Perceptron Algorithms. EMNLP (2002). 
4. Gao. J., H. Yu, P. Xu, and W. Yuan. Minimum Sample Risk Methods for Language Mod-
eling. To Appear (2005). 
5. Manning, C.D., and H. Sch?tze. Foundations of Statistical Natural Language Processing. 
The MIT Press (1999).  
6. Dagan, I., L. Lee, and F. Pereira. Similarity-based models of cooccurrence probabilities. 
Machine Learning, 34(1-3): 43-69 (1999).  
7. Lee, L. Measures of distributional similarity. ACL (1999): 25-32. 
8. Roark, B, M. Saraclar and M. Collins. Corrective Language Modeling for Large Vocabu-
lary ASR with the Perceptron Algorithm. ICASSP (2004): 749-752. 
9. Bacchiani, M., B. Roark and M. Saraclar. Language Model Adaptation with MAP Estima-
tion and the Perceptron Algorithm. HLT-NAACL (2004): 21-24. 
10. Bacchiani, M. and B. Roark. Unsupervised language model adaptation. ICASSP (2003): 
224-227 
11. Och, F.J. Minimum error rate training in statistical machine translation. ACL (2003): 160-
167. 
12. Gao, J., J. Goodman, M. Li, and K.F. Lee. Toward a unified approach to statistical lan-
guage modeling for Chinese. ACM Transactions on Asian Language Information Process-
ing l-1 (2002): 3-33.  
