Speech to Speech Translation for Medical Triage in Korean 
 
Farzad Ehsani, Jim Kimzey, Demitrios Master, Karen Sudre Hunil Park 
Engineering Department  
Sehda, Inc. Independent Consultant 
Mountain View, CA 94043 Seoul, Korea 
{farzad,jkimzey,dlm,karen}@sehda.com phunil@hotmail.com 
 
  
Abstract 
S-MINDS is a speech translation engine, 
which allows an English speaker to communi-
cate with a non-English speaker easily within 
a question-and-answer, interview-style format. 
It can handle limited dialogs such as medical 
triage or hospital admissions. We have built 
and tested an English-Korean system for do-
ing medical triage with a translation accuracy 
of 79.8% (for English) and 78.3% (for Ko-
rean) for all non-rejected utterances.  We will 
give an overview of the system building proc-
ess and the quantitative and qualitatively sys-
tem performance. 
1 Introduction 
Speech translation technology has the potential to 
give nurses and other clinicians immediate access 
to consistent, easy-to-use, and accurate medical 
interpretation for routine patient encounters. This 
could improve safety and quality of care for pa-
tients who speak a different language from that of 
the healthcare provider. 
This paper describes the building and testing of a 
speech translation system, S-MINDS (Speaking 
Multilingual Interactive Natural Dialog System), 
built in less than 4 months from specification to the 
test scenario described. Although this paper shows 
a number of areas for improvement in the S-
MINDS system, it does demonstrate that building 
and deploying a successful speech translation sys-
tem is becoming possible and perhaps even com-
mercially viable. 
 
 
2 Background 
Sehda is focused on creating speech translation 
systems to overcome language barriers in health-
care settings in the U.S. The number of people in 
the U.S. who speak a language other than English 
is large and growing, and Spanish is the most 
commonly spoken language next to English. Ac-
cording to the 2000 census, 18% of the U.S. popu-
lation aged 5 and older (47 million people) did not 
speak English at home.1 This represents a 48% in-
crease from the 1990 figure. In 2000, 8% of the 
population (21 million) was Limited English Profi-
cient (LEP). More than 65% of the LEP population 
(almost 14 million people) spoke Spanish. 
A body of research shows that language barriers 
impede access to care, compromise quality, and 
increase the risk of adverse outcomes. Although 
trained medical interpreters and bilingual health-
care providers are effective in overcoming such 
language barriers, the use of semi-fluent healthcare 
professionals and ad hoc interpreters causes more 
interpreter errors and lower quality of care (Flores 
2005). 
One study analyzed the problem of language barri-
ers for hospitalized inpatients. The study, which 
focused on pediatric patients, sought to determine 
whether patients whose families have a language 
barrier are more likely to incur serious medical 
errors than patients without a language barrier 
(Cohen et al, 2005). The study?s conclusion was 
that patients of LEP families had a twofold in-
creased risk for serious medical incident compared 
with patients whose families did not have a lan-
guage barrier. It is important to note that the LEP 
                                                          
1   US Census Bureau, 2000 
patients in this study were identified as needing 
interpreters during their inpatient stay and medical 
interpreters were available.  
Although the evidence favors using trained medi-
cal interpreters, there is a gap between best prac-
tice and reality. Many patients needing an 
interpreter do not get one, and many must use ad 
hoc interpreters. In a study of 4,161 uninsured pa-
tients who received care in 23 hospitals in 16 cit-
ies, more than 50% who needed an interpreter did 
not get one (Andrulis et al, 2002). 
Another study surveyed 59 residents in a pediatric 
residency program in an urban children?s hospital 
(O?Leary and Hampers, 2003). Forty of the 59 resi-
dents surveyed spoke little or no Spanish. Again, it 
is important to note that this hospital had in-house 
medical interpreters. Of this group of nonproficient 
residents: 
? 100% agreed that the hospital interpreters 
were effective; however, 75% ?never? or 
only ?sometimes? used the hospital inter-
preters. 
? 53% used their inadequate language skills 
in the care of patients ?often? or ?every 
day.? 
? 53% believed the families ?never? or only 
?sometimes? understood their child?s diag-
nosis. 
? 43% believed the families ?never? or only 
?sometimes? understood discharge instruc-
tions. 
? 40% believed the families ?never? or only 
?sometimes? understood the follow-up 
plan. 
? 28% believed the families ?never? or only 
?sometimes? understood the medications. 
? 53% reported calling on their Spanish-
proficient colleagues ?often? or ?every 
day? for help. 
? 80% admitted to avoiding communication 
with non-English-speaking families. 
 
The conclusion of the study was as follows: ?De-
spite a perception that they are providing subopti-
mal communication, nonproficient residents rarely 
use professional interpreters. Instead, they tend to 
rely on their own inadequate language skills, im-
pose on their proficient colleagues, or avoid com-
munication with Spanish-speaking families with 
LEP.? 
Virtually every study on language barriers suggests 
that these residents are not unique. Physicians and 
staff at several hospitals have told Sehda that they 
are less likely to use a medical interpreter or tele-
phone-based interpreter because it takes too long 
and is too inconvenient. Sehda believes that to 
bridge this gap requires 2-way speech translation 
solutions that are immediately available, easy to 
use, accurate, and consistent in interpretation. 
The need for speech translation exists in health-
care, and a lot of work has been done in speech 
translation over the past two decades.  Carnegie-
Mellon University has been experimenting with 
spoken language translation in its JANUS project 
since the late 1980s (Waibel et al, 1996). The 
University of Karlsruhe, Germany, has also been 
involved in an expansion of JANUS. In 1992, these 
groups joined ATR in the C-STAR consortium 
(Consortium for Speech Translation Advanced Re-
search) and in January 1993 gave a successful pub-
lic demonstration of telephone translation between 
English, German and Japanese, within the limited 
domain of conference registrations (Woszczyna, 
1993). A number of other large companies and 
laboratories including NEC (Isotani, et al, 2003) in 
Japan, the Verbmobil Consortium (Wahlster, 
2000), NESPOLE! Consortium (Florian et al, 
2002), AT&T (Bangalore and Riccardi, 2001), and 
ATR have been making their own research effort 
(Yasuda et al, 2003). LC-Star and TC-Star are two 
recent European efforts to gather the data and the 
industrial requirements to enable pervasive speech-
to-speech translation (Zhang, 2003). Most recently, 
the DARPA TransTac program (previously known 
as Babylon) has been focusing on developing de-
ployable systems for English to Iraqi Arabic. 
3 System Description 
Unlike other systems that try to solve the speech 
translation problem with the assumption that there 
is a moderate amount of data available, S-MINDS 
focuses on rapid building and deployment of 
speech translation systems in languages where lit-
tle or no data is available. S-MINDS allows the 
user to communicate easily in a question-and-
answer, interview-style conversation across lan-
guages in limited domains such as border control, 
hospital admissions or medical triage, or other nar-
row interview fields.  
S-MINDS uses a number of voice-independent 
speech recognition engines with the usage depend-
ent on the languages and the particular domain. 
These engines include Nuance 8.52, SRI EduSpeak 
2.03, and Entropic?s HTK-based engine.4 There is a 
dialog/translation creation tool that allows us to 
compile and run our created dialogs with any of 
these engines. This allows our developers to be 
free from the nuances of any particular engine that 
is deployed. S-MINDS uses a combination of 
grammars and language models with these engines, 
depending on the task and the availability of train-
ing data. In the case of the system described in this 
document, we were using Nuance 8.5 for both 
English and Korean speech recognition. 
We use our own semantic parser, which identifies 
keywords and phrases that are tagged by the user; 
these in turn are fed into an interpretation engine. 
Because of the limited context, we can achieve 
high translation accuracy with the interpretation 
engine. However, as the name suggests, this engine 
does not directly translate users? utterances but 
interprets what they say and paraphrases their 
statements. Finally, we use a voice generation sys-
tem (which splices human recordings) along with 
the Festival TTS engine to output the translations. 
This has been recently replaced by the Cepstral 
TTS engine. 
Additionally, S-MINDS includes a set of tools to 
modify and augment the existing system with addi-
tional words and phrases in the field in a matter of 
a few minutes. 
The initial task given to us was a medical disaster 
recovery scenario that might occur near an Ameri-
can military base in Korea. We were given about 
270 questions and an additional 90 statements that 
might occur on the interviewer side. Since our sys-
tem is an interview-driven system (sometimes re-
ferred to as ?1.5-way?), the second-language 
person is not given the option of initiating conver-
sations. The questions and statements given to us 
covered several domains related to the task above, 
including medical triage, force protection at the 
                                                          
2   http://www.nuance.com/nuancerecognition/ 
3   http://www.speechatsri.com/products/eduspeak.shtml 
4   http://htk.eng.cam.ac.uk/ 
installation gate, and some disaster recovery ques-
tions. In addition to the 270 assigned questions, we 
created 120 of our own in order to make the do-
mains more complete.  
3.1 Data Collection 
Since we assumed that we could internally gener-
ate the English language data used to ask the ques-
tion but not the language data on the Korean side, 
our entire focus for the data collection task was on 
Korean. As such, we collected about 56,000 utter-
ances from 144 people to answer the 390 questions 
described above. This data collection was con-
ducted over the course of 2 months via a tele-
phone-based computer system that the native 
Korean speakers could call. The system first intro-
duced the purpose of the data collection and then 
presented the participants with 12 different scenar-
ios. The participants were then asked a subset of 
the questions after each of the scenarios. One ad-
vantage of the phone-based system ? in addition to 
the savings in administrative costs ? was that the 
participants were free to do the data collection any 
time during the day or night, from any location. 
The system also allowed participants to hang up 
and call back at a later time. The participants were 
paid only if they completed all the scenarios.  
Of this data, roughly 7% was unusable and was 
thrown away. Another 31% consisted of one-word 
answers (like ?yes?). The rest of the data consisted 
of utterances 2 to 25 words long. Approximately 
85% of the usable data was used for training; the 
remainder was used for testing.  
The transcription of the data started one week after 
the start of the data collection, and we started 
building the grammars three weeks later.  
3.2  System Development 
We have an extensive set of tools that allow non-
specialists, with a few days of training, to build 
complete mission-oriented domains. In this project, 
we used three bilingual college graduates who had 
no knowledge of linguistics. We spent the first 10 
days training them and the next two weeks closely 
supervising their work. Their work involved taking 
the sentences that were produced from the data 
collection and building grammars for them until 
the ?coverage? of our grammars ? that is, the num-
ber of utterances from the training set that our sys-
tem would handle ? was larger than a set threshold 
(generally set between 80% and 90%). Because of 
the scarcity of Korean-language data, we built this 
system based entirely on grammar language mod-
els rather than statistical language models.  Gram-
mars are generally more rigid than statistical 
language models, and as such grammars tend to 
have higher in-domain accuracy and much lower 
out-of-domain accuracy5 than statistical language 
models.  This means that the system performance 
will depend greatly upon on how well our gram-
mars cover the domains.   
The semantic tagging and the paraphrase transla-
tions were built simultaneously with the grammars. 
This involved finding and tagging the semantic 
classes as well as the key concepts in each utter-
ance.  Frame-based translations were performed by 
doing concept and semantic transfer. Because our 
tools allowed the developers to see the resulting 
frame translations right away, they were able to 
make fixes to the system as they were building it; 
hence, the system-building time was greatly re-
duced.  
We used about 15% of the collected telephone data 
for batch testing. Before deployment, our average 
word accuracy on the batch results was 92.9%. The 
translation results were harder to measure directly, 
mostly because of time constraints.  
3.3 System Testing 
We tested our system with 11 native Korean 
speakers, gathering 968 utterances from them. The 
results of the test are shown in Table 1. Most of the 
valid rejected utterances occurred because partici-
pants spoke too softly, too loudly, before the 
prompt, or in English. Note that there was one ut-
terance with bad translation; that and a number of 
other problems were fixed before the actual field 
testing.  
 
                                                          
5   Note that there are many factors effecting both gram-
mar-based and statistical language model based speech 
recognition, including noise, word perplexity, acoustic 
confusability, etc.  The statement above has been true 
with some of the experiments that we have done, but we 
can not claim that it is universally true.   
Category Percentage
Total Recognized Correctly 82.0% 
Total Recognized Incorrectly 5.8% 
Total Valid Rejection 8.0% 
Total Invalid Rejected   4.1% 
Total unclear translations 0.1% 
Table 1: Korean-to-English system testing re-
sults for the 11 native Korean speakers.  
4 Experimental Setup 
A military medical group used S-MINDS during a 
medical training exercise in January 2005 in Carls-
bad, California. The testing of speech translation 
systems was integrated into the exercise to assess 
the viability of such systems in realistic situations. 
The scenario involved a medical aid station near 
the front lines treating badly injured civilians. The 
medical facilities were designed to quickly triage 
severely wounded patients, provide life-saving 
surgery if necessary, and transfer the patients to a 
safer area as soon as possible. 
4.1 User Training 
Often the success or failure of these interactive 
systems is determined by how well the users are 
trained on the systems? features.  
Training and testing on S-MINDS took place from 
November 2004 through January 2005. The train-
ing had three parts: a system demonstration in No-
vember, two to three hours of training per person 
in December, and another three-hour training ses-
sion in January. About 30 soldiers were exposed to 
S-MINDS during this period. Because of the tsu-
nami in Southeast Asia, many of the people who 
attended the November demo and December train-
ing were not available for the January training and 
the exercise. Nine service members used S-
MINDS during the exercise. Most of them had at-
tended only the training session in January. 
4.2 Test Scenarios 
Korean-speaking ?patients? arrived by military am-
bulance. They were received into one of three tents 
where they were (notionally) triaged, treated, and 
prepared for surgery. The tents were about 20 feet 
wide by 25 feet deep, and each had six to eight cots 
for patients. The tents had lights and electricity. 
The environment was noisy, sandy, and ?bloody.? 
The patients? makeup coated our handsets by the 
end of the day. There were many soldiers available 
to help and watch. Nine service members used S-
MINDS during a four-hour period.  
All of the ?patients? spoke both English and Ko-
rean. A few ?patients? were native Korean speak-
ers, and two were American service members who 
spoke Korean fairly fluently but with an accent. 
The ?patients? were all presented as severely in-
jured from burns, explosions, and cuts and in need 
of immediate trauma care. 
The ?patients? were instructed to act as if they were 
in great pain. Some did, and they sounded quite 
realistic. In fact, their recorded answers to ques-
tions were sometimes hard for a native Korean 
speaker to understand. The background noise in the 
tents was quite loud (because of the number of 
people involved, screaming patients and close 
quarters). Although we did not directly measure 
the noise; we estimate it ranged from 65 to 75 deci-
bels. 
4.3 Physical and Hardware Setup 
S-MINDS is a flexible system that can be config-
ured in different ways depending on the needs of 
the end user. Because of the limited time available 
for training, the users were trained on a single 
hardware setup, tailored to our understanding of 
how the exercises would be conducted. Diagrams 
available before the exercises showed that each 
tent would have a ?translation station? where Ko-
rean-speaking patients would be brought. The ex-
perimenters (two of the authors) had expected that 
the tents would be positioned at least 40 feet apart. 
In reality, the tents were positioned about 5 feet 
apart, and there was no translation station.  
Our original intent was to use S-MINDS on a Sony 
U-50 tablet computer mounted on a computer 
stand with a keyboard and mouse at the translation 
station, and for a prototype wireless device ? based 
on a Bluetooth-like technology to eliminate the 
need for wires between the patient and the system 
? that we had built previously. However, because 
of changes in the conduct of the exercise, the ex-
perimenters had to step in and quickly set up two 
of the S-MINDS systems without the wireless sys-
tem (because of the close proximity of the tents) 
and without the computer stands. The keyboards 
and mice were also removed so that the S-MINDS 
systems could be made portable. The medics 
worked in teams of two; one medic would hold the 
computer and headset for the injured patient while 
the other medic conducted the interview. 
5 Results 
The nine participants used our system to commu-
nicate with ?patients? over a four-hour period. We 
analyzed qualitative problems with using the sys-
tem and quantitative results of translation accu-
racy. 
5.1 Problems with System Usage 
We observed a number of problems in the test sce-
narios with our system. These represent some of 
the more common problems with the S-MINDS 
system. The authors suspect these may be endemic 
of all such systems.  
5.1.1 Inadequate Training on the System 
Users were trained to use the wireless units, which 
interfered with each other when used in close prox-
imity. For the exercise, we had to set up the units 
without the wireless devices because the users had 
not been trained on this type of setup. As a result, 
service members were forced to use a different 
system from the one they were trained on. 
Also, the users had difficulty navigating to the 
right domain. S-MINDS has multiple domains 
each optimized for a particular scenario (medical 
triage, pediatrics, etc.), but the user training did not 
include navigation among domains. 
5.1.2 User Interface Issues 
The user interface and the system?s user feedback 
messages caused unnecessary confusion with the 
interviewers. The biggest problem was that the 
system responded with, ?I?m sorry, I didn?t hear 
that clearly? whenever a particular utterance 
wasn?t recognized. This made the users think they 
should just repeat their utterance over and over. In 
fact, the problem was that they were saying some-
thing that were out of domain or did not fit any 
dialogs in S-MINDS, so no matter how many times 
they repeated the phrase, it would not be recog-
nized. This caused the users significant frustration. 
5.2. Quantative Analysis 
During the system testing, there were 363 recorded 
interactions for the English speakers. Unfortu-
nately, the system was not set up to record the ut-
terances that had a very low confidence score (as 
determined by the Nuance engine), and the user 
was asked to repeat those utterances again. Here is 
the rough breakdown for all of the English interac-
tions:  
? 52.5% were translated correctly into Ko-
rean 
? 34.2% were rejected by the system 
? 13.3% had misrecognition or mistranslation 
errors 
 
This means that S-MINDS tried to recognize and 
translate 65.8% of the English utterances and of 
those 79.8% were correctly translated. A more de-
tailed analysis is presented in Figure 1. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: Detailed breakdown for the English 
utterances and percentage breakdown for 
each category.  
 
The Korean speakers? responses to each of the 
questions that were recognized and translated are 
analyzed in Figure 2. Note that the accuracy for the 
non-rejected responses is 78.3%.  
 
 
 
 
 
 
 
 
Figure 2: Detailed breakdown of the recogni-
tion for the Korean utterances and percentage 
breakdown for each category. 
6 Discussion 
Although these results are less than impressive, a 
close evaluation pointed to three areas where a 
concentration of effort would significantly improve 
translation accuracy and reduce mistranslations. 
These areas were: 
1) Data collection with English speakers to in-
crease coverage on the dialogs.  
a) 34% of the things the soldiers said were 
things S-MINDS was not designed to 
translate. 
b) We had assumed that our existing English 
system would have adequate coverage 
without any additional data collection.  
2) User verification on low-confidence results.  
3) Improved feedback prompts when a phrase is 
not recognized; for example: 
a) One user said, ?Are you allergic to any al-
lergies?? three times before he caught him-
self and said, ?Are you allergic to any 
medications?? 
b) Another user said, ?How old are you?? 
seven times before realizing he needed to 
switch to a different domain, where he was 
able to have the phrase translated. 
c) Another user repeated, ?What is your 
name?? nine times before giving up on the 
phrase (this phrase wasn?t in the S-MINDS 
Korean medical mission set). 
 
Beyond improving the coverage, the system?s pri-
mary problem seemed to be in the voice user inter-
face since even the trained users had a difficult 
time in using the system. 
Statements + 
Questions 
(100%) 
Concepts in 
Dialog (90%) 
Concepts not 
in Dialog 
(10%)
Rejected  
(7.4%) 
Incorrect 
Transl. (2.5%)
In Grammar  
(64.7%) 
Not in Gram-
mar (25.3%) 
Correct  
Transl. (50%) 
Rejected 
(8.3%) 
Correct  
Transl. (2.5%) 
Rejected 
 (14.9%) 
Incorrect 
Transl. (8.0%) 
Incorrect  
Transl. (2.8%) 
Wrong topic 
Select (3.6%) 
 
Korean  
Responses  
(100%) 
Translated  
Correctly  
(63.4%) 
Mistranslated  
(4.2%) 
Could Not  
Hear 
(13.4%) 
Rejected  
(19.0%) 
The attempt at realism in playing out a high-trauma 
scenario may have detracted from the effectiveness 
of the event as a test of the systems? abilities under 
more routine (but still realistic) conditions. 
7 New Results 
Based on the results of this experiment, we had a 
secondary deployment in a medical setting for a 
very similar system.  
We applied what we had learned to that setting and 
achieved better results in a few areas. For example: 
1. Data collection in English helped tremen-
dously. S-MINDS recognized about 40% 
more concepts than it had been able to rec-
ognize using only grammars created by 
subject-matter experts. 
2. Verbal verification of the recognized utter-
ance was added to system, and that im-
proved the user confidence, although too 
much verification tended to frustrate the 
users. 
3. Feedback prompts were designed to give 
more specific feedback, which seemed to 
reduce user frustration and the number of 
mistakes. 
 
Overall, the system performance seemed to im-
prove. We continue to gather data on this task, and 
we believe that this is going to enable us to identify 
the next set of problems that need to be solved. 
8 Acknowledgement 
This research was funded in part by the LASER 
ACTD. We specially wish to thank Mr. Pete Fisher 
of ARL for his generous support and his participa-
tion in discussions related to this project. 
References  
 
Andrulis Dennis, Nanette Goodman, Carol Pryor 
(2002), ?What a Difference an Interpreter Can 
make? April 2002. Access Project, 
www.accessproject.org/downloads/c_LEPreport
ENG.pdf 
Bangalore, S. and G. Riccardi, (2001), ?A Finite 
State Approach to Machine Translation,? North 
American ACL 2001, Pittsburgh.  
Cohen, L, F. Rivara, E. K. Marcuse, H. McPhillips, 
and R. Davis, (2005), ?Are Language Barriers 
Associated With Serious Medical Events in 
Hospitalized Pediatric Patients??, Pediatrics, 
September 1, 2005; 116(3): 575 - 579  
Flores Glenn, (2005), ?The Impact of Medical In-
terpreter Services on the Quality of Health Care: 
A Systematic Review,? Medical Care Research 
and Review, Vol. 62, No. 3, pp. 255-299 
Florian M., et. al. (2002), ?Enhancing the Usability 
and Performance of NESPOLE!: a Real-World 
Speech-to-Speech Translation System?, HLT 
2002, San Diego, California U.S., March 2002. 
Isotani, R., Kiyoshi Yamabana, Shinichi Ando, 
Ken Hanazawa, Shin-ya Ishikawa and Ken-ichi 
ISO  (2003), ?Speech-to-Speech Translation 
Software on PDAs for Travel Conversation,? 
NEC Research and Development, Apr. 2003, 
Vol.44, No.2. 
O?Leary and Hampers (2003) ?The Truth About 
Language Barriers: One Residency Program's 
Experience,? Pediatrics, May 1, 2003; 111(5): 
pp. 569 - 573. 
Keiji Yasuda, Eiichiro Sumita, Seiichi Yamamoto, 
Genichiro Kikui, Masazo Yanagida, ?Real-Time 
Evaluation Architecture for MT Using Multiple 
Backward Translations,? Recent Advances in 
Natural Language Processing, pp. 518-522, 
Sep., 2003  
Wahlster, W. (2000), Verbmobil: Foundations of 
Speech-to-Speech Translation.  Springer. 
Waibel, A., (1996), ?Interactive Translation of 
Conversational Speech,? IEEE Computer, July 
1996, 29-7, pp. 41-48. 
Woszczyna, et al, (1993), ?Recent Advances in 
JANUS: A Speech Translation System,? 
DARPA Speech and Natural Language Work-
shop 1993, session 6 ? MT. 
Zhang, Ying, (2003), ?Survey of Current Speech 
Translation Research,? Found on Web: 
http://projectile.is.cs.cmu.edu/research/public/tal
ks/ speechTranslation/sst-survey-joy.pdf 
   
 
S-MINDS 2-Way Speech-to-Speech Translation System 
 
Farzad Ehsani, Jim Kimzey, Demitrios Master,  
Karen Sudre, David Domingo 
Hunil Park 
Engineering Department  
Sehda, Inc. Independent Consultant 
Mountain View, CA 94043 Seoul, Korea 
{farzad, jkimzey, dlm, karen, ddomingo}@sehda.com phunil@hotmail.com 
 
  
 
Abstract 
 
Sehda?s 2-way speech translation 
system, S-MINDS, interprets between 
provider and patient in routine medical 
interactions with very high accuracy. 
Optimizing the system for new tasks 
or languages requires very little data. 
New developments include a hybrid 
translation approach that allows 
participants to say complex or out-of-
domain utterances, the expansion of 
hands-free functionality, and the 
ability to deliver the most urgent 
expressions instantaneously. 
1 Introduction 
Speech translation technology has the potential 
to give nurses and other clinicians immediate 
access to consistent, easy-to-use, and accurate 
medical interpretation for routine patient 
encounters. This could improve safety and 
quality of care for patients who speak a different 
language from that of the healthcare provider. 
The most common hospital interactions are 
interview-style dialogs where the provider?s and 
patient?s utterances are simple and relatively 
predictable. Sehda?s speech translation system, 
S-MINDS, focuses on translating in such 
situations with extremely high accuracy. 
One key difference between S-MINDS and other 
speech translation systems is the amount of data 
required in development. Most other systems 
depend on a moderate amount of domain-
specific data being available. If the data is not 
already available, it is extremely time- and 
labor-intensive for a developer to collect enough 
realistic data to effectively model a pure SMT 
system ? even if the developer has direct access 
to a group of actual users for whom its system is 
being optimized. 
For this and other reasons, Sehda focuses on 
rapid building and deployment of speech 
translation systems for tasks or languages where 
little or no data is available.  
 2 System Description 
This section describes the speech recognition, 
translation, speech generation, interface and 
hardware components that make up S-MINDS. 
2.1 Speech Recognition 
S-MINDS uses a number of voice-independent 
automated speech recognition (ASR) engines, 
with the usage dependent on the languages and 
the particular domain. These engines include 
Nuance 8.5i, SRI EduSpeak 2.0ii, and Entropic?s 
HTK-based engine.iii  
Sehda?s (internal) dialog/translation creation 
tools allow developers to compile and run new 
dialogs with any ASR engine so they do not 
have to be encumbered by the nuances of any 
particular engine. 
2.2 Translation 
S-MINDS processes ASR output using a 
combination of grammars and language models 
that is selected based on the task and the 
availability of training data.  
S-MINDS first employs a semantic parser to 
extract the essential words and phrases from the 
ASR output. This information is then fed into 
   
 
Sehda?s proprietary interpretation engine, which 
matches the information against a finite set of 
concepts in the specified domain. The resulting 
translation is extremely accurate ? often more 
accurate than the ASR output itself. However, as 
the name suggests, this engine does not directly 
translate users? utterances but interprets what 
they say and paraphrases their statements.  
2.3 Speech Generation 
S-MINDS uses its own voice generation system, 
which splices human recordings, to output most 
translations. If recordings do not exist for a word 
or phrase, S-MINDS generates the speech using 
a text-to-speech (TTS) engine. 
S-MINDS includes a set of tools by which users 
can modify and augment the existing system 
with additional words and phrases in the field in 
a matter of a few minutes. 
2.4 Interface 
A variety of interface features make S-MINDS 
particularly easy to use in a hospital 
environment. 
Most S-MINDS functions can be performed 
hands-free and eyes-free via a voice user 
interface (VUI) so the provider can focus on the 
patient and the operation of hospital equipment.  
A picture viewer allows digital images to be 
displayed to aid communication with the patient 
and add clarity to the log. 
Verbal or on-screen verification can be 
employed (with adjustable upper and lower 
thresholds) to put an additional check on 
recognition accuracy. 
2.5 Hardware 
A complete S-MINDS system contains three 
main hardware components: a Windows XP 
computer with S-MINDS software installed; 
a headset microphone, which the healthcare 
provider uses to control S-MINDS and 
communicate with the patient; and a 
telephone handset, which the patient uses to 
communicate with the provider. 
3 Current Developments  
Under a contract with DARPA, Sehda has 
developed a more interactive system using a 
combination of SMT and interpretation 
engines.  This allows the users to speak 
more freely. If an utterance is too complex or 
too far ?out of domain? to be handled by the 
interpretation engine, S-MINDS falls back to the 
SMT engine, which returns a fairly reliable 
word-for-word translation of the ASR output.  
The VUI in S-MINDS is being enhanced to 
include nearly all system control functions, 
reducing the need to change settings 
manually. In addition, users will be able to 
deliver some urgent expressions (such as 
?Hold still? or ?You can breathe?) 
instantaneously without saying a ?hotword? 
first.  
 
                                                 
i   http://www.nuance.com/nuancerecognition/ 
ii   http://www.speechatsri.com/products/eduspeak.shtml 
iii   http://htk.eng.cam.ac.uk/ 
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 54?59
Manchester, August 2008
Speech to Speech translation for Nurse Patient Interaction 
Farzad Ehsani, Jim Kimzey, Elaine 
Zuber, Demitrios Master 
Fluential Inc./ 1153 Bordeaux Dr., 
Sunnyvale, CA 94089 
{farzad,jkimzey,elaine,dlm} 
@fluentialinc.com 
Karen Sudre 
TeleNav, Inc./1130 Kifer Road, 
Sunnyvale CA, 94086 
karens@telenav.com 
 
 
Abstract  
S-MINDS is a speech translation system, 
which allows an English speaker to commu-
nicate with a limited English proficiency 
speaker easily within a question-and-answer, 
interview-style format. It can handle dialogs 
in specific settings such as nurse-patient in-
teraction, or medical triage.  We have built 
and tested an English-Spanish system for ena-
bling nurse-patient interaction in a number of 
domains in Kaiser Permanente achieving a to-
tal translation accuracy of 92.8% (for both 
English and Spanish).  We will give an over-
view of the system as well as the quantitative 
and qualitatively system performance. 
1 Introduction 
There has been a lot of work in the area of 
speech to speech translation by CMU, IBM, SRI, 
University of Geneva and others. In a health care 
setting, this technology has the potential to give 
nurses and other clinical staff immediate access 
to consistent, easy-to-use, and accurate medical 
interpretation for routine patient encounters. This 
could greatly improve safety and quality of care 
for patients who speak a different language from 
that of the healthcare provider. 
This paper describes the building and testing of a 
speech translation system, S-MINDS (Speaking 
Multilingual Interactive Natural Dialog System), 
built in less than 3 months with Kaiser Perma-
nente Hospital in San Francisco, CA.  The sys-
tem was able to gain fairly robust results for the 
domains that it was designed for, and we believe 
                                               
 
 ? 2008. Licensed under the Creative Commons At-
tribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
 
that it does demonstrate that building and deploy-
ing a successful speech translation system is be-
coming possible and even commercially viable. 
 
2 Background 
The number of people in the U.S. who speak a 
language other than English is large and grow-
ing, and Spanish is the most commonly spoken 
language next to English. According to the 2000 
census, 18% of the U.S. population over age 5 
(47 million people) did not speak English at 
home?a 48% increase from 1990. In 2000, 8% 
of the population (21 million) was LEP (Limited 
English Proficiency), with more than 65% of that 
population (almost 14 million people) speaking 
Spanish. 
A body of research shows that language barriers 
impede access to care, compromise quality, and 
increase the risk of adverse outcomes. Although 
trained medical interpreters and bilingual health-
care providers are effective in overcoming such 
language barriers, the use of semi-fluent health-
care professionals and ad hoc interpreters (such 
as family members and friends) cause more in-
terpreter errors and lower quality of care (Flores 
2005).  
When friends and family interpret, they are prone 
to omit, add, and substitute information. Often 
they inject their own opinions and observations, 
or impose their own values and judgments, rather 
than interpreting what the patient actually said. 
Frequently these ad hoc interpreters have limited 
English capabilities themselves and are 
unfamiliar with medical terminology. 
Furthermore, many patients are reluctant to 
disclose private or sensitive information in front 
of a family member, thus giving the doctor an 
incomplete picture; this sometimes prevents a 
54
correct diagnosis. For example, a battered 
woman is unlikely to reveal the true cause of her 
injuries if her husband is being used as the 
interpreter. 
The California Academy of Family Physicians 
Foundation conducted practice visits in 2006 and 
found that, ?Although they realize the use of 
family members or friends as interpreters is 
probably not the best means of interpretation, all 
practice teams use them.? (Chen et al2007) 
3 System Description 
Fluential?s speech translation system, S-
MINDS1, has a hybrid architecture (Figure 1) 
that combines multiple ASR engines and multi-
ple translation engines. This approach only 
slightly increases the development cost of new 
translation applications, but it greatly improves 
the accuracy and the coverage of the system by 
leveraging the strengths of both statistical and 
grammar/rules-based systems. Furthermore, this 
hybrid approach enables rapid integration of new 
speech recognition and translation engines as 
they become available. 
 
 
Figure 1. The hybrid system architecture of S-
MINDS combines multiple ASR engines with an in-
terpretation engine and an SMT engine. Note that this 
figure describes the interaction in English to-second 
language direction only. The 2nd language-to-English 
direction has only the small ASR engine and the in-
terpretation engine. 
3.1 Components of Speech Translation 
System 
S-MINDS has a modular architecture with the 
components described below. All of these com-
ponents already exist, so they will not need to be 
                                               
1
  Speaking Multilingual Interactive Natural Dialog 
System 
developed to conduct the research proposed in 
Phase I. 
3.1.1 ASR Engine 
S-MINDS employs multiple acoustic engines so 
the best engine can be chosen for each language. 
Within each language, two separate language 
models are active at the same time, telling the 
ASR engines which words and phrases to recog-
nize. A smaller, more directed language model 
with higher accuracy is used to capture important 
and frequently used concepts. For less frequently 
used concepts, a larger language model that gen-
erally has broader coverage but somewhat lower 
accuracy is used. The combination of these two 
provides high accuracy for responses that can be 
anticipated and slightly lower accuracy but 
broader coverage for everything else. This 
method also allows development of new domains 
with very little data?for each domain, only a 
new domain-specific small language model 
needs to be built.   
3.1.2 Interpretation Engine 
Fluential has created an interpretation engine that 
is an alternative to an SMT engine. The S-
MINDS interpretation engine uses information 
extracted from the output of the ASR engine and 
then performs a paraphrase translation in seman-
tic space. This process is similar to what human 
interpreters do when they convey the essential 
meaning without providing a literal translation. 
The advantage of an interpretation engine is that 
new domains can be added more quickly and 
with less data than is possible with an SMT en-
gine. For high?volume, routine interactions, an 
interpretation engine can be extremely fast and 
highly accurate; however, the translation may 
lose some of the nuance. Again, this means that 
highly accurate target applications can be built 
with very little data?only a few examples of 
each concept are needed to train the interpreta-
tion engine.  
3.1.3 Statistical Machine Translation En-
gine  
For the S-MINDS SMT engine, Fluential is de-
veloping a novel approach that has generally im-
proved the accuracy of speech translation sys-
tems.2 This approach capitalizes on the intuition 
that language is broadly divided into two levels:  
                                               
2
  This effort is ongoing; it has not yet been fully 
implemented.    
55
structure and vocabulary. Traditional statistical 
approaches force the system to learn both types 
of information simultaneously. However, if the 
acquisition of structural information is kept sepa-
rate from the acquisition of vocabulary, the re-
sulting system should learn both levels more ef-
ficiently. And by modifying the existing corpus 
to separate structure and vocabulary, we have 
been able to take full advantage of all the infor-
mation in the bilingual corpus, producing higher-
quality MT without requiring large bodies of 
training data. The most recent modification to 
this approach was the use of distance-based or-
dering (Zens and Ney, 2003) and lexicalized or-
dering (Tillmann and Zhang, 2005) to allow for 
multiple language models, including non-word 
models such as part-of-speech improved search 
algorithm, in order to improve its speed and effi-
ciency.   
3.1.4 VUI+GUI System 
S-MINDS has a flexible user interface that can 
be configured to use VUI only or VUI+GUI for 
either the English speaker or the second-
language speaker. Also, the English speaker can 
experience a different user interface than the sec-
ond-language speaker. The system has the flexi-
bility to use multiple types of microphones, in-
cluding open microphones, headsets, and tele-
phone headsets. Speech recognition can be con-
firmed by VUI, GUI, or both, and it can be con-
figured to verify all utterances, no utterances, or 
just utterances that fall below a certain confi-
dence level.   
3.1.5 Synthesis Engine 
S-MINDS can use text-to-speech (TTS) synthesis 
throughout the system; alternatively, it can use 
TTS in its SMT-based system and chunk-based 
recordings that are spliced together in its inter-
pretation engine. Fluential licenses its TTS tech-
nology from Cepstral, and other vendors.   In 
general we do not expect to be doing any re-
search and development activities in this area, as 
Cepstral can easily create good synthesis models 
from the 10 hours of provided speech data 
(Schultz and Black, 2006, Peterson, 2007).   
4 System Building 
Fluential conducted fiver activities in order to 
build the system.  They included: (1) Defining 
the task, (2) Collecting speech data to model 
nurse-patient interactions, (3) Building and test-
ing a speech translation system in English and 
Spanish, (4) Using the system with patients and 
nurses and collecting data to measure system 
performance, and (5) Analyzing the results. 
To define the task, Fluential conducted a two-
hour focus group with six registered nurses from 
Med/Surg unit of Kaiser Medical Center in San 
Francisco. In this focus group, the nurses identi-
fied six nurse-patient encounter types that they 
wanted to use for the evaluation.  These were:  
(1) Greeting/Goodbye, (2) Vital Signs, (3) Pain 
Assessment, (4) Respiratory Assessment, (5) 
Blood Sugar, (6) Placement of an I.V. 
Fluential then collected speech data over a four-
week period by recording nurse-patient interac-
tions involving 11 nurses and 25 patients. Fluen-
tial also recruited 59 native Spanish speakers 
who provided speech data using an automated 
system that walked them through hypothetical 
scenarios and elicited their responses in Spanish. 
The English recognizer had a vocabulary of 
2,003 and it was trained with 9,683 utterances.   
The Spanish recognizer had a vocabulary of 822, 
and it was trained with 1,556 utterances.   We 
suspect that the vocabulary size in Spanish would 
have been much bigger if we had more data. 
5 System Evaluation  
After building and testing the speech translation 
system, Fluential conducted a two-hour training 
session for each of the nurses before using the 
system with patients. A bilingual research assis-
tant explained the study to patients, obtained 
their consent, and trained them for less than five 
minutes on the system. Nurses then used the sys-
tem with Spanish-speaking patients for the six 
nurse-patient encounters that were built into the 
system. The system was used by three nurses 
with eleven patients for a total of 95 nurse-
patient encounters creating a total of 500 conver-
sation segments.3 
To protect patients from a mistranslation, each 
encounter was remotely monitored by a bilingual 
interpreter, who immediately notified the nurse 
any time the system mistranslated. Each encoun-
ter was recorded, transcribed, and translated by a 
human.  
3.1 Scoring Accuracy 
                                               
3
  A conversation segment is a single con-
tinuous sequence of speech in a single language 
plus the translation of what was said. 
56
The human translations were compared to the 
system?s translations and given a score using the 
Laws Methodology of either Good, Fair, Poor, 
Mistranslated, or Not Translated. (Laws, 2004).  
If a translation were scored as Good or Fair, it 
was considered accurate. If the translation were 
scored as Poor, Mistranslated, or Not Translated, 
it was considered inaccurate. 
Table 2 and 3 give examples of how we have 
used Law?s method to grade actual interaction 
results from nurses and patients. 
Table 2: Nurse Scoring Examples 
 
What 
Nurse Said 
S-MINDSTM  
Translation 
Human  
Translation 
S-MINDS 
Accuracy 
I will give 
you an I.V. 
Voy a colo-
carle un 
cateter para 
liquidos 
intraveno-
sos. 
Voy a colo-
carle un 
cateter de 
liquidos 
intravenosos. 
Good 
Let me 
check if I 
can give 
you medi-
cation for 
that. 
Dejeme 
chequear si 
puedo darle 
algun medi-
camento. 
Permitame 
reviso si 
puedo darle 
algun medi-
camento para 
eso. 
Fair 
I will 
check 
your? 
Yo voy a 
revisarle los 
vendajes 
Voy a revisar 
su ? 
Poor 
Did some-
one take 
your vi-
tals? 
?Le tomare 
sus signos 
vitals? 
?Alguien 
tomo sus 
signos vi-
tals? 
Mis-
translated 
Your heart 
rate is 
normal. 
--- 
Su frecuen-
cia cardiaca 
es normal. 
Not 
Trans-
lated 
 
 Table 3: Patient Scoring Examples 
 
What 
Patient 
Said 
S-MINDS 
Translation 
Human  
Translation 
S-MINDS 
Accuracy 
No, no 
tengo 
dificultad 
en respi-
rar. 
I don't have 
difficulty 
breathing. 
No, I don't 
have diffi-
culty breath-
ing. 
Good 
En la 
parte baja 
del esto-
mago. 
The lower 
part of my 
stomach. 
In the lower 
part of my 
stomach. 
Fair 
N/A N/A N/A Poor 
N/A N/A N/A Mistranslated 
Los 
huesos. --- My bones. 
Not Trans-
lated 
6 Results  
- Our internal milestones for Phase I was to 
achieve 80% accuracy using the Laws Method-
ology. Out of 500 conversation segments, the 
speech translation system had an overall accu-
racy rate of 93% combining both nurse- and pa-
tient-conversation segments,  
 
91.80%
1.00% 0.60%
4.20% 2.40%
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
Good Fair Poor Mistranslated Not Translated
 
Figure 2: Total results for both nurses and 
patients.  
6.1 Nurse Translation Results 
Looking at just nurse conversation segments, the 
speech translation system had higher accuracy 
than for patient segments. Out of 404 nurse seg-
ments, the speech translation system had a 94% 
accuracy rate. 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
Good Fair Poor Mistranslated Not Translated
 
Figure 3: Accuracy for Nurse Conversational 
Segments 
The biggest problem with system performance 
with nurses was with mistranslations. When 
nurses tried to say things that were not in the sys-
tem, the system tried to map their utterances to 
something that was in the system. In each case of 
mistranslation, the system told the nurse what it 
was about to translate, gave the nurse a chance to 
stop the translation, and then translated the 
wrong thing when the nurse did not respond. We 
believe that system performance can be greatly 
improved in by collecting more speech data from 
patients and nurses, making changes to the user 
interface, and improving our training program. 
57
6.2  Patient Translation Results 
Looking at just patient conversation segments, 
the speech translation system had lower overall 
accuracy than for nurse segments. Out of 96 pa-
tient segments, the speech translation system had 
a 90% accuracy rate. 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
Good Fair Poor Mistranslated Not Translated
 
Figure 4: Results for Patients 
All of the problems with system performance 
with patients were with responses that the system 
was not able to translate. The system never gave 
a Poor translation or Mistranslated. So there were 
times when the nurse knew that the patient tried 
to say something that the system could not trans-
late, but there was never a time when the system 
gave the nurse false information. However, this 
percentage is quite high, and in a large context, it 
might cause additional problems. 
6.3 Nurse Survey Results 
After each time using the system, the nurses 
completed a user satisfaction survey that had five 
statements and asked them assign a 1-to-5 Likert 
score to each statement with 1 meaning 
?Strongly Disagree? and 5 meaning ?Strongly 
Agree.? Average scores for each question were: 
4.7  The speech translator was easy 
to use. 
4.5  The English voice was fluent 
and easy to understand. 
4.4  I understood the patient better 
because of the speech translator. 
4.5  I feel that I am providing better 
medical care because of the speech translator. 
4.7  I would like to use the speech 
translator with my patients in the future. 
6.4 Patient Survey Results 
The patients also completed a similar user satis-
faction survey, translated to Spanish, after using 
the system. Their average scores for each ques-
tion were: 
4.6  The speech translator was easy 
to use. 
4.8  The Spanish voice was fluent 
and easy to understand. 
4.7  I understood my nurse better be-
cause of the speech translator. 
5.0  I feel that I am receiving better 
medical care because of the speech translator. 
4.9  I would like to use the speech 
translator with my nurse in the future. 
 
6.5 ANOVA Testing 
We conducted Analysis of Variance (ANOVA) 
testing to evaluate whether there were any sig-
nificant variations in translation accuracy by pa-
tient, nurse, or encounter type. There were no 
significant differences. 
7 Discussion  
We were able to build and evaluate a system in 3 
months and show its utility by nurses and pa-
tients in clinical setting.   The system seemed to 
work and was liked by both nurses and patients.  
The next question is whether such a system can 
scale and cover a much larger domain, and how 
much data and training is required to accomplish 
this.   
References 
Chen A., et al (2007), Addressing Language and 
Culture?A Practice Assessment for Health 
Care Professionals, p3.  
Flores Glenn, (2005), ?The Impact of Medical 
Interpreter Services on the Quality of Health 
Care: A Systematic Review,? Medical Care 
Research and Review, Vol. 62, No. 3, pp. 255-
29 
Laws, MB, Rachel Heckscher, Sandra Mayo, 
Wenjun. Li, Ira  Wilson, (2004), ?A New 
Method for Evaluating the Quality of Medical 
Interpretation,? Medical Care. 42(1):71-80, 
January 2004 
Peterson Kay (2007).  Senior Linguist, Cepstral 
LLC, Personal Communication.  
Schultz, Tanja and A. W Black (2006), 
?Challenges with Rapid Adaptation of Speech 
Translation Systems to New Language Pairs? 
58
Proceedings of the IEEE International 
Conference on Acoustics, Speech, and Signal 
Processing (ICASSP-2006), Toulouse, France, 
May 15-19, 2006.  
Tillmann, Christoph and T. Zhang, (2005), ?A 
localized prediction model for statistical 
machine translation,? in Proceedings of the 
43rd Annual Meeting of the ACL, pp. 557-564, 
Ann Arbor, June 2005. 
Zens, Richard, and H. Ney, (2003), ?A compara-
tive study of reordering constraints in statisti-
cal machine translation,? in Proceedings of the 
41st Annual Meetings of the ACL, pp. 144-
151, Sapporo, Japan, July 2003Association for 
Computing Machinery. 1983. Computing Re-
views, 24(11):503-512. 
 
59
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 60?63
Manchester, August 2008
A Small-Vocabulary Shared Task for Medical Speech Translation
Manny Rayner1, Pierrette Bouillon1, Glenn Flores2, Farzad Ehsani3
Marianne Starlander1, Beth Ann Hockey4, Jane Brotanek2, Lukas Biewald5
1 University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
{Emmanuel.Rayner,Pierrette.Bouillon}@issco.unige.ch
Marianne.Starlander@eti.unige.ch
2 UT Southwestern Medical Center, Children?s Medical Center of Dallas
{Glenn.Flores,Jane.Brotanek}@utsouthwestern.edu
3 Fluential, Inc, 1153 Bordeaux Drive, Suite 211, Sunnyvale, CA 94089, USA
farzad@fluentialinc.com
4 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035?1000
bahockey@ucsc.edu
5 Dolores Labs
lukeab@gmail.com
Abstract
We outline a possible small-vocabulary
shared task for the emerging medical
speech translation community. Data would
consist of about 2000 recorded and tran-
scribed utterances collected during an eval-
uation of an English ? Spanish version
of the Open Source MedSLT system; the
vocabulary covered consisted of about 450
words in English, and 250 in Spanish. The
key problem in defining the task is to agree
on a scoring system which is acceptable
both to medical professionals and to the
speech and language community. We sug-
gest a framework for defining and admin-
istering a scoring system of this kind.
1 Introduction
In computer science research, a ?shared task? is a
competition between interested teams, where the
goal is to achieve as good performance as possible
on a well-defined problem that everyone agrees to
work on. The shared task has three main compo-
nents: training data, test data, and an evaluation
metric. Both test and training data are divided
up into sets of items, which are to be processed.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
The evaluation metric defines a score for each pro-
cessed item. Competitors are first given the train-
ing data, which they use to construct and/or train
their systems. They are then evaluated on the test
data, which they have not previously seen.
In many areas of speech and language process-
ing, agreement on a shared task has been a major
step forward. Often, it has in effect created a new
subfield, since it allows objective comparison of
results between different groups. For example, it
is very common at speech conference to have spe-
cial sessions devoted to recognition within a par-
ticular shared task database. In fact, a conference
without at least a couple of such sessions would
be an anomaly. A recent success story in language
processing is the Recognizing Textual Entailment
(RTE) task1. Since its inception in 2004, this has
become extremely popular; the yearly RTE work-
shop now attracts around 40 submissions, and error
rates on the task have more than halved.
Automatic medical speech translation would
clearly benefit from a shared task. As was made
apparent at the initial 2006 workshop in New
York2, nearly every group has both a unique ar-
chitecture and a unique set of data, essentially
making comparisons impossible. In this note, we
will suggest an initial small-vocabulary medical
1http://www.pascal-network.org/
Challenges/RTE/
2http://www.issco.unige.ch/pub/
SLT workshop proceedings book.pdf
60
shared task. The aspect of the task that is hard-
est to define is the evaluation metric, since there
unfortunately appears to be considerable tension
between the preferences of medical professionals
and speech system implementers. Medical profes-
sionals would prefer to carry out a ?deep? evalu-
ation, in terms of possible clinical consequences
following from a mistranslation. System evalua-
tors will on the other hand prefer an evaluation
method that can be carried out quickly, enabling
frequent evaluations of evolving systems. The plan
we will sketch out is intended to be a compromise
between these two opposing positions.
The rest of the note is organised as follows.
Section 2 describes the data we propose to use,
and Section 3 discusses our approach to evaluation
metrics. Section 4 concludes.
2 Data
The data we would use in the task is for the English
? Spanish language pair, and was collected us-
ing two different versions of the MedSLT system3.
In each case, the scenario imagines an English-
speaking doctor conducting a verbal examination
of a Spanish-speaking patient, who was assumed
to be have visited the doctor because they were
displaying symptoms which included a sore throat.
The doctor?s task was to use the translation sys-
tem to determine the likely reason for the patient?s
symptoms.
The two versions of the system differed in
terms of the linguistic coverage offered. The
more restricted version supported a minimal range
of English questions (vocabulary size, about 200
words), and only allowed the patient to respond
using short phrases (vocabulary size, 100 words).
Thus for example the doctor could ask ?How long
have you had a sore throat??, and the patient would
respond Hace dos d??as (?for two days?). The
less restricted version supported a broader range
of doctor questions (vocabulary size, about 450
words), and allowed the patient to respond using
both short phrases and complete sentences (vocab-
ulary size, about 225 words). Thus in response
to ?How long have you had a sore throat??, the
patient could say either Hace dos d??as (?for two
days?) or Tengo dolor en la garganta hace dos d??as
(?I have had a sore throat for two days?).
Data was collected in 64 sessions, carried out
3http://www.issco.unige.ch/projects/
medslt/
over two days in February 2008 at the University
of Texas Medical Center, Dallas. In each session,
the part of the ?doctor? was played by a real physi-
cian, and the part of the ?patient? by a Spanish-
speaking interpreter. This resulted in 1005 En-
glish utterances, and 967 Spanish utterances. All
speech data is available in SPHERE-headed form,
and totals about 90 MB. A master file, organised in
spreadsheet form, lists metadata for each recorded
file. This includes a transcription, a possible valid
translation (verified by a bilingual translator), IDs
for the ?doctor?, the ?patient?, the session and the
system version, and the preceding context. Con-
text is primarily required for short answers, and
consists of the most recent preceding doctor ques-
tion.
3 Evaluation metrics
The job of the evaluation component in the shared
task is to assign a score to each translated utter-
ance. Our basic model will be the usual one for
shared tasks in speech and language. Each pro-
cessed utterance will be assigned to a category;
each category will be associated with a specified
score; the score for a complete testset will the sum
of the scores for all of its utterances. We thus have
three sub-problems: deciding what the categories
are, deciding how to assign a category to a pro-
cessing utterance, and deciding what scores to as-
sociate with each category.
3.1 Defining categories
If the system attempts to translate an utterance,
there are a priori three things that can happen:
it can produce a correct translation, an incorrect
translation, or no translation. Medical speech
translation is a safety-critical problem; a mistrans-
lation may have serious consequences, up to and
including the death of the patient. This implies
that the negative score for an incorrect translation
should be high in comparison to the positive score
for a correct translation. So a naive scoring func-
tion might be ?1 point for a correct translation, 0
points for no translation, ?1000 points for an in-
correct translation.?
However, since the high negative score for a
mistranslation is justified by the possible serious
consequences, not all mistranslations are equal;
some are much more likely than others to result in
clinical consequences. For example, consider the
possible consequences of two different mistrans-
61
lations of the Spanish sentence La penicilina me
da alergias. Ideally, we would like the system to
translate this as ?I am allergic to penicillin?. If it
instead says ?I am allergic to the penicillin?, the
translation is slightly imperfect, but it is hard to see
any important misunderstanding arising as a result.
In contrast, the translation ?I am not allergic to
penicillin?, which might be produced as the result
of a mistake in speech recognition, could have very
serious consequences indeed. (Note in passing that
both errors are single-word insertions). Another
type of result is a nonsensical translation, perhaps
due to an internal system error. For instance, sup-
pose the translation of our sample sentence were
?The allergy penicillin does me?. In this case, it
is not clear what will happen. Most users will
probably dismiss the output as meaningless; a few
might be tempted to try and decipher it, with un-
predictable results.
Examples like these show that it is important for
the scoring metric to differentiate between differ-
ent classes of mistranslations, with the differentia-
tion based on possible clinical consequences of the
error. For similar reasons, it is important to think
about the clinical consequences when the system
produces correct translations, or fails to produce
a translation. For example, when the system cor-
rectly translates ?Hello? as Buenas d??as, there are
not likely to be any clinical consequences, so it is
reasonable to reward it with a lower score than the
one assigned to a clinically contentful utterance.
When no translation is produced, it also seems cor-
rect to distinguish the case where the user was able
recover by a suitably rephrasing the utterance from
the one where they simply gave up. For example,
if the system failed to translate ?How long has this
cough been troubling you??, but correctly handled
the simpler formulation ?How long have you had a
cough??, we would give this a small positive score,
rather than a simple zero.
Summarising, we propose to classify transla-
tions into the following seven categories:
1. Perfect translation, useful clinical conse-
quences.
2. Perfect translation, no useful clinical conse-
quences.
3. Imperfect translation, but not dangerous in
terms of clinical consequences.
4. Imperfect translation, potentially dangerous.
5. Nonsense.
6. No translation produced, but later rephrased
in a way the system handled adequately.
7. No translation produced, but not rephrased in
a way the system handled adequately.
3.2 Assigning utterances to categories
At the moment, medical professionals will only
accept the validity of category assignments made
by trained physicians. In the worst case, it is
clearly true that a layman, even one who has re-
ceived some training, will not be able to determine
whether or not a mistranslation has clinical signif-
icance.
Physician time is, however, a scarce and valu-
able resource, and, as usual, typical case and worst
case may be very different. Particularly for routine
testing during system development, it is clearly not
possible to rely on expert physician assessments.
We consequently suggest a compromise strategy.
We will first carry out an evaluation using medical
experts, in order to establish a gold standard. We
will then repeat this evaluation using non-experts,
and determine how large the differential is in prac-
tice.
We initially intend to experiment with two dif-
ferent groups of non-experts. At Geneva Uni-
versity, we will use students from the School of
Translation. These students will be selected for
competence in English and Spanish, and will re-
ceive a few hours of training on determination of
clinical significance in translation, using guide-
lines developed in collaboration with Glenn Flores
and his colleagues at the UT Southwestern Medi-
cal Center, Texas. Given that the corpus material
is simple and sterotypical, we think that this ap-
proach should yield a useful approximation to ex-
pert judgements.
Although translation students are far cheaper
than doctors, they are still quite expensive, and
evaluation turn-around will be slow. For these rea-
sons, we also propose to investigate the idea of per-
forming evaluations using Amazon?s Mechanical
Turk4. This will be done by Dolores Labs, a new
startup specialising in Turk-based crowdsourcing.
3.3 Scores for categories
We have not yet agreed on exact scores for the
different categories, and this is something that is
4http://www.mturk.com/mturk/welcome
62
probably best decided after mutual discussion at
the workshop. Some basic principles will be evi-
dent from the preceding discussion. The scale will
be normalised so that failure to produce a trans-
lation is counted as zero; potentially dangerous
mistranslations will be associated with a negative
score large in comparison to the positive score for
a useful correct translation. Inability to communi-
cate can certainly be dangerous (this is the point of
having a translation system in the first place), but
mistakenly believing that one has communicated
is usually much worse. As Mark Twain put it: ?It
ain?t what you don?t know that gets you into trou-
ble. It?s what you know for sure that just ain?t so?.
3.4 Discarding uncertain responses
Given that both speech recognition and machine
translation are uncertain technologies, a high
penalty for mistranslations means that systems
which attempt to translate everything may eas-
ily end up with an average negative score - in
other words, they would score worse than a system
which did nothing! For the shared task to be in-
teresting, we must address this problem, and in the
doctor to patient direction there is a natural way
to do so. Since the doctor can reasonably be as-
sumed to be a trained professional who has had
time to learn to operate the system, we can say that
he has the option of aborting any translation where
the machine does not appear to have understood
correctly.
We thus relativise the task with respect to a ?fil-
ter?: for each utterance, we produce both a transla-
tion in the target language, and a ?reference trans-
lation? in the source language, which in some way
gives information about what the machine has un-
derstood. The simplest way to produce this ?ref-
erence translation? is to show the words produced
by speech recognition. When scoring, we evaluate
both translations, and ignore all examples where
the reference translation is evaluated as incorrect.
To go back to the ?penicillin? example, suppose
that Spanish source-language speech recognition
has incorrectly recognised La penicilina me da
alergias as La penicilina no me da alergias. Even
if this produces the seriously incorrect translation
?I am not allergic to penicillin?, we can score it
as a zero rather than a negative, on the grounds
that the speech recognition result already shows
the Spanish-speaking doctor that something has
gone wrong before any translation has happened.
The reference translation may also be produced in
a more elaborate way; a common approach is to
translate back from the target language result into
the source language.
Although the ?filtered? version of the medical
speech translation task makes good sense in the
doctor to patient direction, it is less clear how
meaningful it is in the patient to doctor direction.
Most patients will not have used the system before,
and may be distressed or in pain. It is consequently
less reasonable to expect them to be able to pay at-
tention to the reference translation when using the
system.
4 Summary and conclusions
The preceding notes are intended to form a frame-
work which will serve as a basis for discussion at
the workshop. As already indicated, the key chal-
lenge here is to arrive at metrics which are ac-
ceptable to both the medical and the speech and
language community. This will certainly require
more negotiation. We are however encouraged by
the fact that the proposal, as presented here, has
been developed jointly by representatives of both
communities, and that we appear to be fairly near
agreement. Another important parameter which
we have intentionally left blank is the duration of
the task; we think it will be more productive to de-
termine this based on the schedules of interested
parties.
Realistically, the initial definition of the metric
can hardly be more than a rough guess. Experi-
mentation during the course of the shared task will
probably show that some adjustment will be desir-
able, in order to make it conform more closely to
the requirements of the medical community. If we
do this, we will, in the interests of fairness, score
competing systems using all versions of the metric.
63
