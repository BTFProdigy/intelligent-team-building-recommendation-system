Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 92?100,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Extracting Transfer Rules for Multiword Expressions from Parallel Corpora
Petter Haugereid and Francis Bond
Division of Linguistics and Multilingual Studies,
Nanyang Technological University, Singapore
petterha@ntu.edu.sg,bond@ieee.org
Abstract
This paper presents a procedure for extract-
ing transfer rules for multiword expressions
from parallel corpora for use in a rule based
Japanese-English MT system. We show that
adding the multi-word rules improves transla-
tion quality and sketch ideas for learning more
such rules.
1 Introduction
Because of the great ambiguity of natural language,
it is hard to translate from one language to another.
To deal with this ambiguity it is common to try to
add more context to a word, either in the form of
multi-word translation patterns (Ikehara et al, 1991)
or by adding more context to the translations in sta-
tistical MT systems (Callison-Burch et al, 2005).
In this paper, we present a way to learn large
numbers of multi-word translation rules from either
dictionaries or parallel text, and show their effec-
tiveness in a semantic?transfer-based Japanese-to-
English machine translation system. This research
is similar to work such as Nichols et al (2007). The
novelty lies in (i) the fact that we are learning rules
from parallel text and (ii) that we are learning much
more complex rules.
In Section 2, we outline the semantic transfer ma-
chinery and we introduce the DELPH-IN machine
translation initiative that provided the resources used
in its construction. We describe in more detail how
we learn new rules in Section 3, and show their ef-
fect in Section 4. We briefly discuss the results and
outline future work in Section 5 and, finally, we con-
clude this paper in Section 6.
2 Semantic transfer
All experiments are carried out using Jaen, a se-
mantic transfer based machine translation system
(Bond et al, 2011). The system uses Minimal Re-
cursion Semantics (MRS) as its semantic representa-
tion (Copestake et al, 2005). The transfer process
takes place in three steps. First, a Japanese string is
parsed with the Japanese HPSG grammar, JACY. The
grammar produces an MRS with Japanese predicates.
Second, the Japanese MRS is transferred into an En-
glish MRS. And finally, the English HPSG grammar
ERG generates an English string from the English
MRS.
At each step of the translation process, stochastic
models are used to rank the output. There is a cutoff
at 5, so the maximal amount of generated sentences
is 125 (5x5x5). The final results are reranked using
a combined model (Oepen et al, 2007).
While JACY and the ERG have been developed
over many years, less effort has been put into the
transfer grammar, and this component is currently
the bottleneck of the system. In general, transfer
rules are the bottleneck for any system, and there
is a long history of trying to expand the number of
transfer rules types (Matsuo et al, 1997) and tokens
(Yamada et al, 2002).
In order to increase the coverage of the system
(the number of words that we can translate) we build
rules automatically. We look at strings that have
a high probability of being a translation (identified
from parallel corpora), and see if they fit a pattern
defined in the transfer grammar. A very simple pat-
tern would be that of a noun predicate being trans-
ferred as another noun predicate. The transfer rule
type for this pattern is given in (1). The type makes
92
sure that the LBL and the ARG0 values are kept when
the relation is transferred, while the PRED value is
left underspecified.1
(1)
?
?
?
?
?
?
?
noun-mtr
IN|RELS
?[
LBL h1 , ARG0 x1
]?
OUT|RELS
?[
LBL h1 , ARG0 x1
]?
?
?
?
?
?
?
?
The rule for? (hon)? book, which is a subtype
of noun-mtr, is given in (2).
(2)
?
?
?
?
?
?
?
hon_book
IN|RELS
?[
PRED _hon_n_rel
]?
OUT|RELS
?[
PRED _book_n_of_rel
]?
?
?
?
?
?
?
?
A linguistically more interesting transfer rule is
that for PP ? Adjective transfer (see (3)), which
takes as input 3 relations (the first for the noun, the
second for the postposition, and the third for the
quantifier of the noun, all properly linked), and out-
puts one relation (for the adjective), for example of
an angle ? angular, to give an English-to-English
example. The output adjective relation is given the
same handle, index and external argument as the in-
put postposition, so that the semantic linking with
the rest of the MRS is preserved. In this way, modi-
fiers of the PP will modify the Adjective, and so on.
The use of this transfer rule is demonstrated in Sec-
tion 3.1.2
1the LBL (label) of the relation is a tag, which can be used to
refer to the relation (conventionally written with an h for han-
dle). The ARG0 is the index of the relation. Nouns and deter-
miners have referential indices (conventionally written with an
x), while adjectives and verbs have event indices (written with
an e).
2The HCONS feature has as value a list of qeq constraints
(equality modulo quantifiers), which function is to express that
the label of a relation is equal to a handle in an argument posi-
tion (without unifying them).
(3)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
pp-adj_mtr
IN
?
?
?
?
?
?
?
?
?
?
?
?
?
RELS
?
[
LBL h1 , ARG0 x1
]
[
LBL h0 , ARG0 e0 ,
ARG1 ext , ARG2 x1
]
[
ARG0 x1 , RSTR hr
]
?
HCONS
?[
HARG hr , LARG h1
]?
?
?
?
?
?
?
?
?
?
?
?
?
?
OUT|RELS
?[
LBL h0 , ARG0 e0 ,
ARG1 ext
]?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
3 Procedure
We are using GIZA++ (Och and Ney, 2003) and
Anymalign (Lardilleux and Lepage, 2009) to gener-
ate phrase tables from a collection of four Japanese
English parallel corpora and one bilingual dictio-
nary. The corpora are the Tanaka Corpus (2,930,132
words: Tanaka (2001)), the Japanese Wordnet Cor-
pus (3,355,984 words: Bond et al (2010)), the
Japanese Wikipedia corpus (7,949,605),3 and the
Kyoto University Text Corpus with NICT transla-
tions (1,976,071 words: Uchimoto et al (2004)).
The dictionary is Edict, a Japanese English dictio-
nary (3,822,642 words: Breen (2004)). The word
totals include both English and Japanese words.
We divided the corpora into development, test,
and training data, and extracted the transfer rules
from the training data. The training data of the four
corpora together with the Edict dictionary form a
parallel corpus of 20 million words (9.6 million En-
glish words and 10.4 million Japanese words). The
Japanese text is tokenized and lemmatized with the
MeCab morphological analyzer (Kudo et al, 2004),
and the English text is tokenized and lemmatized
with the Freeling analyzer (Padr? et al, 2010), with
MWE, quantities, dates and sentence segmentation
turned off.
When applying GIZA++ and Anymalign to the
lemmatized parallel corpus they produced phrase ta-
bles with 10,812,423 and 5,765,262 entries, respec-
tively, running GIZA++ with the default MOSES
settings and Anymalign for approximately 16 hours.
3The Japanese-English Bilingual Corpus of Wikipedia?s Ky-
oto Articles: http://alaginrc.nict.go.jp/WikiCorpus/
index_E.html
93
We filtered out the entries with an absolute fre-
quency of 1,4 and which had more than 4 words on
the Japanese side or more than 3 words on the En-
glish side. This left us with 6,040,771 Moses entries
and 3,435,176 Anymalign entries. We then checked
against the Jacy lexicon on the Japanese side and the
ERG lexicon on the English side to ensure that the
source and the target could be parsed/generated by
the MT system. Finally, we filtered out entries with a
translation probability, P(English|Japanese), of less
than 0.1. This gave us 1,376,456 Moses entries and
234,123 Anymalign entries. These were all phrase
table entries with a relatively high probability, con-
taining lexical items known both to the parser and
the generator.
For each of these phrase table entries, we looked
up the lexemes on either side in the Jacy/ERG lexi-
cons, and represented them with the semantic predi-
cate (and their syntactic category).5 Ambiguous lex-
emes were represented with a list of predicates. We
represented each possible surface rule with a list of
all possible semantic predicate rules. So a possible
surface rule with two (two times) ambiguous lexi-
cal items would give four possible semantic rules, a
possible surface rule with three (two times) ambigu-
ous lexical items would give eight possible seman-
tic rules, and so on. A total of 53,960,547 possible
semantic rules were created. After filtering out se-
mantic transfer rules containing English predicates
of probability less than 0.2 compared to the most
frequent predicate associated with the same surface
form, this number was reduced to 26,875,672.6 Each
of these rules consists of two ordered lists of seman-
tic predicates (one for Japanese and one for English).
From these possible semantic transfer rules, we
extracted transfer rules that fitted nine different pat-
4The absolute frequency number can, according to Adrien
Lardilleux (p.c.), be thought of as a confidence score. The
larger, the more accurate and reliable the translation probabili-
ties. 1 is the lowest score.
5As shown in (2), predicates reflect the syntactic category of
the lexical item by means of an infix, e.g. ?_n_? for noun.
6We used a profile of the English training data from the
Tanaka Corpus and the Japanese Wordnet Corpus, parsed with
the ERG grammar, to find the probability of each English pred-
icate, given its surface form. For example the word sleep is
assigned the predicate "_sleep_n_1_rel" 103 times, the predi-
cate "_sleep_v_1_rel" 89 times, and "_sleep_v_in_rel" 2 times.
Hence, semantic transfer rules containing the first two are ac-
cepted, while rules conataining the last are filtered out.
terns. We extracted 81,690 rules from the Moses en-
tries, and 52,344 rules from the Anymalign entries.
The total number of rules extracted was 97,478.
(36,556 rules overlapped.) Once the rule templates
have been selected and the thresholds set, the entire
process is automatic.
The distribution of the extracted rules over the
nine patterns is shown in Table 1.
In the first three patterns, we would simply see if
the predicates had the appropriate ?_n_? and ?_a_?
infixes in them (for nouns and adjectives respec-
tively). 82,651 rules fitted these patterns and were
accepted as transfer rules. The last six patterns were
slightly more complex, and are described below.
3.1 PP? adjective
Japanese PPs headed by the postposition? no ?of?
often correspond to an adjective in English as illus-
trated in (4).
(4) a. ??
small.size
?
of
small
b. ??
music
?
of
musical
In order to extract transfer rules that fit this pat-
tern, we checked for possible semantic rules hav-
ing two predicates on the Japanese side and one on
the English side. The first Japanese predicate would
have the infix ?_n_? (be a noun), and the second
would be ?_no_p_rel? (the predicate of the postpo-
sition ?). The sole English predicate would have
the infix ?_a_? (be an adjective).
3.2 PP? PP
Japanese PPs headed by the postposition ? de
?with/by/in/on/at? are, given certain NP comple-
ments, translated into English PPs headed by the
preposition ?by? (meaning ?by means of?) where the
prepositional object does not have a determiner, as
illustrated in (5).
(5) ????
taxi
?
DE
by taxi
By checking for possible semantic transfer rules
fitting the pattern noun + de_p_rel on the Japanese
94
Input Output Moses Anymalign Merged rules
noun + noun ? noun + noun 34,691 23,333 38,529
noun + noun ? adj + noun 21,129 13,198 23,720
noun + noun ? noun 11,824 12,864 20,402
PP ? adj 753 372 1,022
PP ? PP 131 24 146
verb + NP ? verb + NP 9,985 1,926 10,256
noun + adj ? adj 544 243 566
postp + noun + verb ? verb 1,821 173 1,921
PP + verb ? verb 812 211 916
Total 81,690 52,344 97,478
Table 1: Transfer rule patterns.
side, and the pattern by_p_rel and noun on the En-
glish side, we created PP to PP transfer rules where,
in addition to the predicates stemming from the lex-
ical items, the English determiner was set to the
empty determiner (udef_q_rel). The resulting trans-
fer rule for (5) is illustrated in (6).
(6)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
pp_pp_mtr
IN
?
[
PRED _de_p_rel
]
[
PRED udef_q_rel
]
[
PRED _takushii_n_rel
]
?
OUT
?
[
PRED _by_p_means_rel
]
[
PRED udef_q_rel
]
[
PRED _taxi_n_1_rel
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
With this particular pattern we get transfer rules
which prevent us from generating all possible trans-
lations of ? (?with?, ?by?, ?on?, ?in?, or ?at?), and
keeps the quantifier unexpressed.
There are many other possible PP?PP patterns,
such as??? start in/on/at/to ?in the beginning?.
We started with one well known idiomatic English
type, but should learn many more.
3.3 Verb + NP? Verb + NP
Japanese MWEs fitting the pattern noun + object
marker (?) + verb usually are translated into En-
glish MWEs fitting one out of three verb + NP pat-
terns, illustrated in (7). In (7a), the NP has an unex-
pressed quantifier. The English pattern in these cases
will be verb + noun. In (7b), the NP has an indef-
inite article. The English pattern will then be verb
+ _a_q_rel + noun. And in (7c), the NP has defi-
nite article. The English pattern will then be verb +
_the_q_rel + noun.
(7) a. ???
tenisu
tennis
?
wo
ACC
?
shi
do
??
masu
POLITE
play tennis
b. ??
seikei
living
?
wo
ACC
???
tateru
stand up
make a living
c. ??
seme
blame
?
wo
ACC
??
ou
bear
take the blame
By adding these rules to the transfer grammar, we
avoid generating sentences such as I play the ten-
nis and He took a blame. In addition, we are able
to constrain the translations of the individual words,
greatly reducing the transfer search space
3.4 Noun + Adjective? Adjective
Japanese has a multiword expression pattern that is
not found in English. In this pattern, noun +? (ga)
+ adjective usually correspond to English adjectives,
as shown in (8). The pattern is an example of a dou-
ble subject construction. The Japanese adjective has
its subject provided by a noun, but still takes an ex-
ternal subject. Our transfer rule takes this external
95
subject and links it to the subject of the English ad-
jective.
(8) X
X
X
ga
ga
ga
?
se
NOM
?
ga
height
??
takai
NOM high
X is tall
With the new rules, the transfer grammar now cor-
rectly translates (9) as She is very intelligent. and
not Her head is very good., which is the translation
produced by the system without the new multiword
rules. Notice the fact that the adverb modifying the
adjective in Japanese is also modifying the adjective
in English.
(9) ??
kanojo
She
?
wa
TOPIC
??
taihen
very
?
atama
head
?
ga
NOM
??
yoi
good
?
.
.
She is very intelligent.
Because of the flexibility of the rule based sys-
tem, we can also parse, translate and generate many
variants of this, including those where the adverb
comes in the middle of the MWE, or where a dif-
ferent topic marker is used as in (10). We learn the
translation equivalences from text n-grams, but then
match them to complex patterns, thus taking advan-
tage of the ease of processing of simple text, but
still apply them flexibly, with the power of the deep
grammar.
(10) ??
kanojo
She
?
mo
FOCUS
?
atama
head
?
ga
NOM
??
taihen
very
??
yoi
good
?
.
.
She is also very intelligent.
She is very intelligent also.
3.5 Postp + Noun + Verb? Verb / PP + Verb
? Verb
Japanese has two MWE patterns consisting of a
postposition, a noun, and a verb, corresponding to
a verb in English. The first is associated with the
postposition? no ?of? (see (11)), and the second is
associated with the postposition ? ni ?in/on/at/to?
(see (12)).
(11) ??
rekishi
history
?
no
of
??
benkyou
study
?
wo
ACC
??
suru
make
study history
(12) ??
kingyo
goldfish
?
ni
in/on/at/to
??
esa
feed
?
wo
ACC
??
yaru
give
feed the goldfish
In (11), the postposition? no ?of?, the noun?
? benkyou ?study?, and the verb?? suru ?make?
are translated as study, while in (12), the postposi-
tion ? ni ?in/on/at/to?, the noun ?? esa ?feed?,
and the verb?? yaru ?give? are translated as feed.
In both MWE patterns, the noun is marked with the
object marker? wo. The two patterns have differ-
ent analysis: In (11), which has the no-pattern, the
postposition attaches to the noun, and the object of
the postposition?? rekishi ?history? functions as
a second subject of the verb. In (12), which has the
ni-pattern, the postposition attaches to the verb, and
the object of the postposition?? kingyo ?goldfish?
is a part of a PP. Given the different semantic rep-
resentations assigned to the two MWE patterns, we
have created two transfer rule types. We will have a
brief look at the transfer rule type for the no transla-
tion pattern, illustrated in (13).7
(13)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
p+n+arg12_arg12_mtr
IN
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
RELS
?
[
LBL h2 , ARG0 event,
ARG1 x3 , ARG2 x2
]
[
LBL h2 , ARG0 x3
]
[
ARG0 x3 , RSTR h3
]
[
LBL h1 , ARG0 e1 ,
ARG1 x1 , ARG2 x3
]
?
HCONS
?[
HARG h3 , LARG h2
]?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
OUT|RELS
?[
LBL h1 , ARG0 e1 ,
ARG1 x1 , ARG2 x2
]?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
The input of the p+n+arg12_arg12_mtr transfer
rule type consists of (i) a postposition relation, (ii)
a noun relation, (iii) a quantifier (of the the noun),
7The transfer rule type for the ni translation pattern
(pp+arg12_arg12_mtr) is identical to the transfer rule type for
the no translation pattern except from the linking of the postpo-
sition in the input.
96
and (iv) a verb relation (listed as they appear on the
RELS list). The output relation is a verb relation. No-
tice that the ARG1 of the input verb relation is reen-
tered as ARG1 of the output relation ( x1 ), and the
ARG2 of the input postposition relation is reentered
as ARG2 of the output relation ( x2 ). The output re-
lation is also given the same LBL and ARG0 value
as the input verb relation. In this way, the Japanese
MWE is collapsed into one English relation while
semantic links to the rest of the semantic representa-
tion are maintained.
3.6 Summary
Out of the 26,875,672 possible semantic predicate
rules, we extracted 97,478 rules that fitted one of the
nine patterns. These rules were then included in the
transfer grammar of the MT system.
4 Results
The impact of the MWE transfer rules on the MT
system is illustrated in Table 2.
We compare two versions of the system, one with
automatically extracted MWE rules and one with-
out. They both have hand-written MWE and single
word rules as well as automatically extracted sin-
gle word rules extracted from Edict by Nichols et al
(2007).
The additional rules in + MWE are those pro-
duced in Section 3. The system was tested on held
out sections of the Tanaka Corpus (sections 003 to
005). As can be seen from the results, the overall
system is still very much a research prototype, the
coverage being only just over 20%.
Adding the new rules gave small but consistent
increases in both end-to-end coverage (19.3% to
20.1%) and translation quality (17.80% to 18.18%)
measured with NEVA (Forsbom, 2003).8
When we look only at the 105 sentences whose
translations were changed by the new rules the
NEVA increased from 17.1% to 21.36%. Investigat-
ing the effects on development data, we confirmed
that when the new MWE rules hit, they almost al-
ways improved the translation. However, there is
still a problem of data-sparseness, we are missing
8NEVA is an alternative to BLEU that is designed to provide
a more meaningful sentence-level score for short references. It
is calculated identically to BLEU, but leaving out the log and
exponent calculations. We find it correlates highly with BLEU.
instances of rule-types as well as missing many po-
tential rule types.
As an example of the former, we have a pattern
for verb+NP ? verb+NP, but were unable to learn
????? jihi wo negau ?beg for mercy: lit. ask
for compassion?. We had one example in the train-
ing data, and this was not enough to get over our
threshold. As an example of the latter, we do not
currently learn any rules for Adverb+Verb?Verb al-
though this is a common pattern.
5 Discussion and Further Work
The transfer rules learned here are based on co-
occurrence data from corpora and a Japanese-to-
English dictionary. Many of the translations learned
are in fact compositional, especially for the com-
pound noun and verb-object patterns. For exam-
ple, ? ? ?? ana-wo horu ?dig hole? ? dig
a whole would have been translated using existing
rules. In this case the advantage of the MWE rule is
that it reduces the search space, so the system does
not have to consider less likely translations such as
carve the shortages. More interestingly, many of the
rules find non-compositional translations, or those
where the structure cannot be translated word for
word. Some of these are also idiomatic in the source
and target language. One of our long term goals is
to move these expressions into the source and tar-
get grammars. Currently, both Jacy and the ERG
have idiom processing (based on Copestake et al,
2002), but there are few idiomatic entries in their
lexicons. Bilingual data can be a good source for
identifying these monolingual idioms, as it makes
the non-compositionality explicit. An example of
a rule that uses the current idiom machinery is the
(hand-built) rule N-ga chie-wo shiboru ?N squeezes
knowledge??N racks N?s brains, where the subject
is co-indexed with a possessive pronoun modifying
the object: I/You rack my/your brains. Adding such
expressions to the monolingual grammars simplifies
the transfer rules and makes the grammars more use-
ful for other tasks.
In this paper we only presented results for nine
major multi-word transfer rule types. These were
those that appeared often in the training and devel-
opment data. We can straightforwardly extend this
in two ways: by extending the number of rule types
97
Version Parse Transfer Generation Total NEVA (%) F1
coverage coverage coverage coverage
?MWE 3614/4500 1647/3614 870/1647 870/4500 17.80 0.185
(0 rules) (80.3%) (45.6%) (52.8%) (19.3%)
+ adj/n 3614/4500 1704/3614 900/1704 900/4500 17.99 0.189
(83,217 rules) (80.3%) (47.1%) (52.8%) (20.0%)
+ PP 3614/4500 1659/3614 877/1659 877/4500 17.88 0.187
(1,168 rules) (80.3%) (45.9%) (52.9%) (19.5%)
+ verb 3614/4500 1688/3614 885/1688 885/4500 17.89 0.186
(13,093 rules) (80.3%) (46.7%) (52.4%) (19.7%)
+ MWE 3614/4500 1729/3614 906/1729 906/4500 18.18 0.190
(97,478 rules) (80.3%) (47.8%) (52.4%) (20.1%)
Table 2: Coverage of the MT system before and after adding the MWE transfer rules.
and by extending the number of rule instances.
Shirai et al (2001) looked at examples in a
65,500-entry English-Japanese lexicon and esti-
mated that there were at least 80 multi-word
Japanese patterns that translated to a single word
in English. As we are also going from multi-word
to multi-word we expect that there will be even
more than this. Currently, adding another pattern is
roughly an hour?s work (half to make the rule-type in
the transfer engine, half to make the rule matcher in
the rule builder). To add another 100 patterns is thus
6 weeks work. Almost certainly this can be speeded
up by sharing information between the templates.
We therefore estimate that we can greatly reduce the
sparseness of rule-types with four weeks work.
To improve the coverage of rule instances, we
need to look at more data, such as that aligned by
Utiyama and Takahashi (2003).
Neither absolute frequency nor estimated transla-
tion probability give reliable thresholds for deter-
mining whether rules are good or not. Currently
we are investigating two solutions. One is feedback
cleaning, where we investigate the impact of each
new rule and discard those that degrade translation
quality, following the general idea of Imamura et al
(2003). The second is the more traditional human-
in-the loop: presenting each rule and a series of rele-
vant translation pairs to a human and asking them to
judge if it is good or not. Ultimately, we would like
to extend this approach to crowd source the deci-
sions. There are currently two very successful online
collaborative Japanese-English projects (Edict and
Tatoeba, producing lexical entries and multilingual
examples respectively) which indicates that there is
a large pool of interested knowledgeable people.
Finally, we are working in parallel to qualitatively
improve the MWE rules in two ways. The first is to
extend rules using semantic classes, not just words.
This would mean we would need fewer rules, but
each rule would be more powerful. Of course, many
rules are very idiomatic and should trigger on actual
lexemes, but there are many, such as ?????
himei wo negau ?beg for mercy? which allow some
variation ? in this case there are at least three differ-
ent verbs that are commonly used. At a lower level
we need to improve our handling of orthographic
variants so that a rule can match on different forms
of the same word, rather than requiring several rules.
We are working together with the Japanese WordNet
to achieve these goals.
The second approach is to learn complex rules
directly from the parallel text, in a similar way to
(Jellinghaus, 2007) or (Way, 1999). This will be
necessary to catch rules that our templates do not
include, but it is very easy to over-fit the rules to the
translation data. For this reason, we are still con-
straining rules with templates.
98
Resource Availability
The MWE expression rules made here and the ma-
chine translation system that uses them are avail-
able through an open source code repository. In-
stallation details can be found at http://wiki.
delph-in.net/moin/LogonInstallation. The
code to make the rules is undergoing constant re-
vision, when it settles down we intend to also add it
to the repository.
6 Conclusion
This paper presented a procedure for extracting
transfer rules for multiword expressions from paral-
lel corpora for use in a rule based Japanese-English
MT system. We showed that adding the multi-
word rules improves translation coverage (19.3%
to 20.1%) and translation quality (17.8% to 18.2%
NEVA). We show how we can further improve by
learning even more rules.
Acknowledgments
We would like to thank the members of the LO-
GON, and DELPH-IN collaborations for their support
and encouragement. In addition we would like to
thank the developers and maintainers of the other
resources we used in our project, especially JMDict,
Tatoeba, Anymalign and Moses. This project was
supported in part by Nanyang Technological Univer-
sity (through a start-up grant on ?Automatically de-
termining meaning by comparing a text to its trans-
lation?).
References
Francis Bond, Hitoshi Isahara, Kiyotaka Uchimoto,
Takayuki Kuribayashi, and Kyoko Kanzaki. 2010.
Japanese WordNet 1.0. In 16th Annual Meeting of
The Association for Natural Language Process-
ing, pages A5?3. Tokyo.
Francis Bond, Stephan Oepen, Eric Nichols, Dan
Flickinger, Erik Velldal, and Petter Haugereid.
2011. Deep open source machine translation. Ma-
chine Translation. (Special Issue on Open source
Machine Translation, to appear).
James W. Breen. 2004. JMDict: a Japanese-
multilingual dictionary. In Coling 2004 Workshop
on Multilingual Linguistic Resources, pages 71?
78. Geneva.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statistical
machine translation to larger corpora and longer
phrases. In 43nd Annual Meeting of the Associa-
tion for Computational Linguistics: ACL-2005.
Ann Copestake, Dan Flickinger, Carl J. Pol-
lard, and Ivan A. Sag. 2005. Minimal Re-
cursion Semantics: an introduction. Re-
search on Language and Computation, 3(4):281?
332. URL http://lingo.stanford.edu/sag/
papers/copestake.pdf.
Ann Copestake, Fabre Lambeau, Aline Villavicen-
cio, Francis Bond, Timothy Baldwin, Ivan Sag,
and Dan Flickinger. 2002. Multiword expres-
sions: Linguistic precision and reusability. In
Proceedings of the Third International Confer-
ence on Language Resources and Evaluation
(LREC 2002), pages 1941?7. Las Palmas, Canary
Islands.
Eva Forsbom. 2003. Training a super model look-
alike: Featuring edit distance, n-gram occurrence,
and one reference translation. In In Proceedings
of the Workshop on Machine Translation Evalua-
tion. Towards Systemizing MT Evaluation.
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and
Hiromi Nakaiwa. 1991. Toward an MT system
without pre-editing ? effects of new methods
in ALT-J/E ?. In Third Machine Translation
Summit: MT Summit III, pages 101?106. Wash-
ington DC. URL http://xxx.lanl.gov/abs/
cmp-lg/9510008.
Kenji Imamura, Eiichiro Sumita, and Yuji Mat-
sumoto. 2003. Feedback cleaning of machine
translation rules using automatic evaluation. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics, pages
447?454. Association for Computational Lin-
guistics, Sapporo, Japan. URL http://www.
aclweb.org/anthology/P03-1057.
Michael Jellinghaus. 2007. Automatic Acquisition of
Semantic Transfer Rules for Machine Translation.
Master?s thesis, Universit?t des Saarlandes.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying Conditional Random Fields to
Japanese Morphological Analysis. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP
99
2004, pages 230?237. Association for Computa-
tional Linguistics, Barcelona, Spain.
Adrien Lardilleux and Yves Lepage. 2009.
Sampling-based multilingual alignment. In
Proceedings of Recent Advances in Natural
Language Processing (RANLP 2009), pages
214?218. Borovets, Bulgaria.
Yoshihiro Matsuo, Satoshi Shirai, Akio Yokoo, and
Satoru Ikehara. 1997. Direct parse tree translation
in cooperation with the transfer method. In Daniel
Joneas and Harold Somers, editors, New Methods
in Language Processing, pages 229?238. UCL
Press, London.
Eric Nichols, Francis Bond, Darren Scott Appling,
and Yuji Matsumoto. 2007. Combining resources
for open source machine translation. In The
11th International Conference on Theoretical and
Methodological Issues in Machine Translation
(TMI-07), pages 134?142. Sk?vde.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Stephan Oepen, Erik Velldal, Jan Tore L?nning,
Paul Meurer, and Victoria Rosen. 2007. Towards
hybrid quality-oriented machine translation. on
linguistics and probabilities in MT. In 11th Inter-
national Conference on Theoretical and Method-
ological Issues in Machine Translation: TMI-
2007, pages 144?153.
Llu?s Padr?, Miquel Collado, Samuel Reese, Marina
Lloberes, and Irene Castell?n. 2010. Freeling 2.1:
Five years of open-source language processing
tools. In Proceedings of 7th Language Resources
and Evaluation Conference (LREC 2010). La Val-
letta. (http://nlp.lsi.upc.edu/freeling.
Satoshi Shirai, Kazuhide Yamamoto, and Kazutaka
Takao. 2001. Construction of a dictionary to
translate japanese phrases into one english word.
In Proceedings of ICCPOL?2001 (19th Interna-
tional Conference on Computer Processing of
Oriental Languages, pages 3?8. Seoul.
Yasuhito Tanaka. 2001. Compilation of a mul-
tilingual parallel corpus. In Proceedings of
PACLING 2001, pages 265?268. Kyushu.
(http://www.colips.org/afnlp/archives/
pacling2001/pdf/tanaka.pdf).
Kiyotaka Uchimoto, Yujie Zhang, Kiyoshi Sudo,
Masaki Murata, Satoshi Sekine, and Hitoshi
Isahara. 2004. Multilingual aligned parallel
treebank corpus reflecting contextual informa-
tion and its applications. In Gilles S?ras-
set, editor, COLING 2004 Multilingual Linguis-
tic Resources, pages 57?64. COLING, Geneva,
Switzerland. URL http://acl.ldc.upenn.
edu/W/W04/W04-2208.bib.
Masao Utiyama and Mayumi Takahashi. 2003.
English-Japanese translation alignment data.
http://www2.nict.go.jp/x/x161/members/
mutiyama/align/index.html.
Andy Way. 1999. A hybrid architecture for robust
MT using LFG-DOP. Journal of Experimental
and Theoretical Artificial Intelligence, 11. Special
Issue on Memory-Based Language Processing.
Setsuo Yamada, Kenji Imamura, and Kazuhide Ya-
mamoto. 2002. Corpus-assisted expansion of
manual mt knowledge. In Ninth International
Conference on Theoretical and Methodological
Issues in Machine Translation: TMI-2002, pages
199?208. Keihanna, Japan.
100
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 67?75,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Extracting Semantic Transfer Rules from Parallel Corpora
with SMT Phrase Aligners
Petter Haugereid and Francis Bond
Linguistics and Multilingual Studies
Nanyang Technological University
petterha@ntu.edu.sg bond@ieee.org
Abstract
This paper presents two procedures for ex-
tracting transfer rules from parallel corpora
for use in a rule-based Japanese-English MT
system. First a ?shallow? method where
the parallel corpus is lemmatized before it is
aligned by a phrase aligner, and then a ?deep?
method where the parallel corpus is parsed by
deep parsers before the resulting predicates
are aligned by phrase aligners. In both pro-
cedures, the phrase tables produced by the
phrase aligners are used to extract semantic
transfer rules. The procedures were employed
on a 10 million word Japanese English paral-
lel corpus and 190,000 semantic transfer rules
were extracted.
1 Introduction
Just like syntactic and semantic information finds its
way into SMT models and contribute to improved
quality of SMT systems, rule-based systems bene-
fit from the inclusion of statistical models, typically
in order to rank the output of the components in-
volved. In this paper, we present another way of im-
proving RBMT systems with the help of SMT tools.
The basic idea is to learn transfer rules from paral-
lel texts: first creating alignments of predicates with
the help of SMT phrase aligners and then extracting
semantic transfer rules from these. We discuss two
procedures for creating the alignments. In the first
procedure the parallel corpus is lemmatized before
it is aligned with two SMT phrase aligners. Then
the aligned lemmas are mapped to predicates with
the help of the lexicons of the parsing grammar and
the generating grammar. Finally, the transfer rules
are extracted from the aligned predicates. In the sec-
ond procedure, the parallel corpus is initially parsed
by the parsing grammar and the generating gram-
mar. The grammars produce semantic representa-
tions, which are represented as strings of predicates.
This gives us a parallel corpus of predicates, about
a third of the size of the original corpus, which we
feed the phrase aligners. The resulting phrase tables
with aligned predicates are finally used for extrac-
tion of semantic transfer rules.
The two procedures complement each other. The
first procedure is more robust and thus learns from
more examples although the resulting rules are less
reliable. Here we extract 127,000 semantic transfer
rules. With the second procedure, which is more ac-
curate but less robust, we extract 113,000 semantic
transfer rules. The union of the procedures gives a
total of 190,000 unique rules for the Japanese En-
glish MT system Jaen.
2 Semantic Transfer
Jaen is a rule-based machine translation system em-
ploying semantic transfer rules. The medium for the
semantic transfer is Minimal Recursion Semantics,
MRS (Copestake et al, 2005). The system consists
of the two HPSG grammars: JACY, which is used
for the parsing of the Japanese input (Siegel and
Bender, 2002) and the ERG, used for the generation
of the English output (Flickinger, 2000). The third
component of the system is the transfer grammar,
which transfers the MRS representation produced by
the Japanese grammar into an MRS representation
that the English grammar can generate from: Jaen
(Bond et al, 2011).
At each step of the translation process, the output
67
Source
Language
Analysis ff
MRS
-







Bitext
?







Grammar
??







Treebank
Controller
Reranker ff
MRS
- Target
Language
Generation







Grammar
??







TreebankffSL?TL
Semantic
Transfer
6MRS
?
Interactive Use
6?
Batch Processing
6?
Figure 1: Architecture of the Jaen MT system.
is ranked by stochastic models. In the default con-
figuration, only the 5 top ranked outputs at each step
are kept, so the maximum number of translations is
125 (5x5x5). There is also a final reranking using a
combined model (Oepen et al, 2007).
The architecture of the MT system is illustrated in
Figure 1, where the contribution of the transfer rule
extraction from parallel corpora is depicted by the
arrow going from Bitext to Semantic Transfer.
Most of the rules in the transfer grammar are
simple predicate changing rules, like the rule for
mapping the predicate ?_hon_n_rel? onto the predi-
cate ?_book_v_1_rel?. Other rules are more com-
plex, and transfers many Japanese relations into
many English relations. In all, there are 61 types
of transfer rules, the most frequent being the rules
for nouns translated into nouns (44,572), noun noun
compounds translated into noun noun compounds
(38,197), and noun noun compounds translated into
adjective plus noun (27,679). 31 transfer rule types
have less than 10 instances. The most common rule
types are given in Table 1.1
1Some of the rule types are extracted by only one ex-
traction method. This holds for the types n_adj+n_mtr,
n+n+n_n+n_mtr, n+n_n_mtr, pp+np_np+pp_mtr, and
arg1+pp_arg1+pp_mtr, adj_pp_mtr, and preposition_mtr.
The lemmatized extraction method extracts rules for triple
compounds n+n+n_n+n. This is currently not done with
the semantic extraction method, since a template for a triple
compound would include 8 relations (each noun also has a
quantifier and there are two compound relations in between),
and the number of input relations are currently limited to 5 (but
can be increased). The rest of the templates are new, and they
have so far only been successfully integrated with the semantic
extraction method.
The transfer grammar has a core set of 1,415
hand-written transfer rules, covering function
words, proper nouns, pronouns, time expressions,
spatial expressions, and the most common open
class items. The rest of the transfer rules (190,356
unique rules) are automatically extracted from par-
allel corpora.
The full system is available from http:
//moin.delph-in.net/LogonTop (different
components have different licenses, all are open
source, mainly LGPL and MIT).
3 Two methods of rule extraction
The parallel corpus we use for rule extraction is
a collection of four Japanese English parallel cor-
pora and one bilingual dictionary. The corpora
are the Tanaka Corpus (2,930,132 words: Tanaka,
2001), the Japanese Wordnet Corpus (3,355,984
words: Bond, Isahara, Uchimoto, Kuribayashi, and
Kanzaki, 2010), the Japanese Wikipedia corpus
(7,949,605 words),2 and the Kyoto University Text
Corpus with NICT translations (1,976,071 words:
Uchimoto et al, 2004). The dictionary is Edict
(3,822,642 words: Breen, 2004). The word totals
include both English and Japanese words.
The corpora were divided into into development,
test, and training data. The training data from the
four corpora plus the bilingual dictionary was used
for rule extraction. The combined corpus used for
rule extraction consists of 9.6 million English words
and 10.4 million Japanese words (20 million words
in total).
3.1 Extraction from a lemmatized parallel
corpus
In the first rule extraction procedure we extracted
transfer rules directly from the surface lemmas of
the parallel text. The four parallel corpora were
tokenized and lemmatized, for Japanese with the
MeCab morphological analyzer (Kudo et al, 2004),
and for English with the Freeling analyzer (Padr?
et al, 2010), with MWE, quantities, dates and sen-
tence segmentation turned off. (The bilingual dic-
tionary was not tokenized and lemmatized, since the
entries in the dictionary are lemmas).
2The Japanese-English Bilingual Corpus of Wikipedia?s
Kyoto Articles: http://alaginrc.nict.go.jp/
WikiCorpus/index_E.html.
68
Rule type Hand Lemma Pred Intersect Union Total
noun_mtr 64 32,033 31,575 19,100 44,508 44,572
n+n_n+n_mtr 0 32,724 18,967 13,494 38,197 38,197
n+n_adj+n_mtr 0 22,777 15,406 10,504 27,679 27,679
arg12+np_arg12+np_mtr 0 9,788 1,774 618 10,944 10,944
arg1_v_mtr 22 8,325 1,031 391 8,965 8,987
pp_pp_mtr 2 146 8,584 19 8,711 8,713
adjective_mtr 27 4,914 4,034 2,183 6,765 6,792
arg12_v_mtr 50 4,720 1,846 646 5,920 5,970
n_adj+n_mtr 1 - 4,695 - 4,695 4,696
n+n_n_mtr 0 2,591 3,273 1,831 4,033 4,033
n+n+n_n+n_mtr 0 3,380 - - 3,376 3,376
n+adj-adj-mtr 2 633 2,586 182 3,037 3,039
n_n+n_mtr 1 - 2,229 - 2,229 2,230
pp-adj_mtr 27 1,008 971 1 1,978 2,005
p+n+arg12_arg12_mtr 1 1,796 101 35 1,862 1,863
pp+np_np+pp_mtr 0 - 1,516 - 1,516 1,516
pp+arg12_arg12_mtr 0 852 62 26 888 888
arg1+pp_arg1+pp_mtr 1 - 296 - 296 297
monotonic_mtr 139 - - - - 139
adj_pp_mtr 0 - 112 - 112 112
preposition_mtr 53 - 34 - 34 87
arg123_v_mtr 3 30 14 8 36 39
Table 1: Most common mtr rule types. The numbers in the Hand column show the number of hand-written rules
for each type. The numbers in the Lemma column, show the number of rules extracted from the lemmatized parallel
corpus. The numbers in the Pred column show the number of rules extracted from the semantic parallel corpus. The
Intersect column, shows the number of intersecting rules of Lemma and Pred, and the Union column show the number
of distinct rules of Lemma and Pred.
We then used MOSES (Koehn et al, 2007) and
Anymalign (Lardilleux and Lepage, 2009) to align
the lemmatized parallel corpus. We got two phrase
tables with 10,812,423 and 5,765,262 entries, re-
spectively. MOSES was run with the default set-
tings, and Anymalign ran for approximately 16
hours.
We selected the entries that had (i) a translation
probability, P(English|Japanese) of more than 0.1,3
(ii) an absolute frequency of more than 1,4 (iii) fewer
than 5 lemmas on the Japanese side and fewer than 4
3This number is set based on a manual inspection of the
transfer rules produced. The output for each transfer rule tem-
plate is inspected, and for some of the templates, in particular
the multi-word expression templates, the threshold is set higher.
4The absolute frequency number can, according to Adrien
Lardilleux (p.c.), be thought of as a confidence score. The
larger, the more accurate and reliable the translation probabili-
ties. 1 is the lowest score.
lemmas on the English side,5 and (iv) lexical entries
for all lemmas in Jacy for Japanese and the ERG for
English. This gave us 2,183,700 Moses entries and
435,259 Anymalign entries, all phrase table entries
with a relatively high probability, containing lexical
items known both to the parser and the generator.
The alignments were a mix of one-to-one-or-
many and many-to-one-or-many. For each lemma
in each alignment, we listed the possible predicates
according to the lexicons of the parsing grammar
(Jacy) and the generating grammar (ERG). Since
many lemmas are ambiguous, we often ended up
with many semantic alignments for each surface
alignment. If a surface alignment contained 3 lem-
mas with two readings each, we would get 8 (2x2x2)
semantic alignments. However, some of the seman-
5These numbers are based on the maximal number of lem-
mas needed for the template matching on either side.
69
tic relations associated with a lemma had very rare
readings. In order to filter out semantic alignments
with such rare readings, we parsed the training cor-
pus and made a list of 1-grams of the semantic rela-
tions in the highest ranked output. Only the relations
that could be linked to a lemma with a probability
of more than 0.2 were considered in the semantic
alignment. The semantic alignments were matched
against 16 templates. Six of the templates are simple
one-to-one mapping templates:
1. noun ? noun
2. adjective ? adjective
3. adjective ? intransitive verb
4. intransitive verb ? intransitive verb
5. transitive verb ? transitive verb
6. ditransitive verb ? ditransitive verb
The rest of the templates have more than one
lemma on the Japanese side and one or more lem-
mas on the English side. In all, we extracted 126,964
rules with this method. Some of these are relatively
simple, such as 7 which takes a noun compound and
translates it into a single noun, or 8 which takes a
VP and translates it into a VP (without checking for
compositionality, if it is a common pattern we will
make a rule for it).
7. n+n? n
(1) ?
minor
???-?
test
??-?
had
?
I had a quiz.
8. arg12+np? arg12+np_mtr
(2) ??
that
??-?
job
??-??-?
finished
?
I finished the job.
Other examples, such as 9 are more complex, here
the rule takes a Japanese noun-adjective combina-
tion and translates it to an adjective, with the exter-
nal argument in Japanese (the so-called second sub-
ject) linked to the subject of the English adjective.
Even though we are applying the templates to learn
rules to lemma n-grams, in the translation system
these rules apply to the semantic representation, so
they can apply to a wide variety of syntactic vari-
ations (we give an example of a relative clause be-
low).
9. n+adj? adj
(3) ?-?
previous
?-?
winter
?-?
snow
???-?
much-be
?
Previous winter was snowy.
(4) ?-?
snow
??
much
?
winter
??-?
was
?
It was a snowy winter.
Given the ambiguity of the lemmas used for the
extraction of transfer rules, we were forced to fil-
ter semantic relations that have a low probability in
order to avoid translations that do not generalize.
One consequence of this is that we were not building
rules that should have been built in cases where an
ambiguous lemma has one dominant reading, and
one or more less frequent, but plausible, readings.
Another consequence is that we were building rules
where the dominant reading is used, but where a less
frequent reading is correct. The method is not very
precise since it is based on simple 1-gram counts,
and we are not considering the context of the indi-
vidual lemma. A way to improve the quality of the
assignment of the relation to the lemma would be to
use a tagger or a parser. However, instead of going
down that path, we decided to parse the whole par-
allel training corpus with the parsing grammar and
the generation grammar of the MT system and pro-
duce a parallel corpus of semantic relations instead
of lemmas. In this way, we use the linguistic gram-
mars as high-precision semantic taggers.
3.2 Extraction from a parallel corpus of
predicates
The second rule extraction procedure is based on a
parallel corpus of semantic representations, rather
than lemmatized sentences. We parsed the train-
ing corpus (1,578,602 items) with the parsing gram-
mar (Jacy) and the generation grammar (ERG) of
the MT system, and got a parse with both grammars
for 630,082 items. The grammars employ statistical
models trained on treebanks in order to select the
most probable analysis. For our semantic corpus,
70
we used the semantic representation of the highest
ranked analysis on either side.
The semantic representation produced by the
ERG for the sentence The white dog barks is given in
Figure 2. The relations in the MRSs are represented
in the order they appear in the analysis.6 In the se-
mantic parallel corpus we kept the predicates, e.g.
_the_q_rel, _white_a_1_rel, and so on, but we did
not keep the information about linking. For verbs,
we attached information about the valency. Verbs
that were analyzed as intransitive, like bark in Fig-
ure 2, were represented with a suffix 1x, where 1
indicates argument 1 and x indicates a referential
index: _bark_v_1_rel@1x. If a verb was analyzed
as being transitive or ditransitive, this would be re-
flected in the suffix: _give_v_1_rel@1x2x3x. The
item corresponding to The white dog barks in the se-
mantic corpus would be _the_q_rel _white_a_1_rel
_dog_n_1_rel _bark_v_1_rel@1x.
The resulting parallel corpus of semantic rep-
resentations consists of 4,712,301 relations for
Japanese and 3,806,316 relations for English. This
means that the size of the semantic parallel corpus
is a little more than a third of the lemmatized paral-
lel corpus. The grammars used for parsing are deep
linguistic grammars, and they do not always perform
very well on out of domain data, like for example the
Japanese Wikipedia corpus. One way to increase the
coverage of the grammars would be to include ro-
bustness rules. This would decrease the reliability
of the assignment of semantic relations, but still be
more reliable than simply using 1-grams to assign
the relation.
The procedure for extracting semantic transfer
rules from the semantic parallel corpus is similar
to the procedure for extraction from the lemmatized
corpus. The major difference is that the semantic
corpus is disambiguated by the grammars.
As with the lemmatized corpus, the semantic par-
allel corpus was aligned with MOSES and Anyma-
lign. They produced 4,830,000 and 4,095,744 align-
ments respectively. Alignments with more than 5
relations on either side and with a probability of
less than 0.01 were filtered out.7 This left us with
6Each predicate has the character span of the corresponding
word(s) attached.
7A manual inspection of the rules produced by the template
matching showed that most of the rules produced for several of
4,898,366 alignments, which were checked against
22 rule templates.8 This produced 112,579 rules,
which is slightly fewer than the number of rules
extracted from the lemmatized corpus (126,964).
49,187 of the rules overlap with the rules extracted
from the lemmatized corpus, which gives us a total
number of unique rules of 190,356. The distribution
of the rules is shown in Table 1.
Some of the more complex transfer
rules types like p+n+arg12_arg12_mtr and
pp+arg12_arg12_mtr were extracted in far greater
numbers from the lemmatized corpus than from
the corpus of semantic representations. This is
partially due to the fact that the method involving
the lemmatized corpus is more robust, which means
that the alignments are done on 3 times as much
data as the method involving the corpus of semantic
predicates. Another reason is that the number
of items that need to be aligned to match these
kinds of multi-word templates is larger when the
rules are extracted from the corpus of semantic
representations. (For example, a noun relation
always has a quantifier binding it, even if there is no
particular word expressing the quantifier.) Since the
number of items to be aligned is bigger, the chance
of getting an alignment with a high probability that
matches the template becomes smaller.
One of the transfer rule templates (pp_pp_mtr)
generates many more rules with the method in-
volving the semantic predicates than the method
involving lemmas. This is because we restricted
the rule to only one preposition pair (_de_p_rel
? _by_p_means_rel) with the lemmatized corpus
method, while all preposition pairs are accepted with
the semantic predicate method since the confidence
in the output of this method is higher.
4 Experiment and Results
In order to compare the methods for rule extraction,
we made three versions of the transfer grammar, one
including only the rules extracted from the lemma-
the templates were good, even with a probability as low as 0.01.
For some of the templates, the threshold was set higher.
8The reason why the number of rule templates is higher with
this extraction method, is that the confidence in the results is
higher. This holds in particular for many-to-one rules, were the
quality of the rules extracted with from the lemmatized corpus
is quite low.
71
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
mrs
LTOP h1 h
INDEX e2 e
RELS
?
?
?
?
?
?
?
?
_the_q_rel<0:3>
LBL h3 h
ARG0 x5 x
RSTR h6 h
BODY h4 h
?
?
?
?
?
?
?
,
?
?
?
?
?
_white_a_1_rel<4:9>
LBL h7 h
ARG0 e8 e
ARG1 x5
?
?
?
?
?
,
?
?
?
_dog_n_1_rel<10:13>
LBL h7
ARG0 x5
?
?
?,
?
?
?
?
?
_bark_v_1_rel<14:20>
LBL h9 h
ARG0 e2
ARG1 x5
?
?
?
?
?
?
HCONS
?
?
?
?
qeq
HARG h6
LARG h7
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 2: MRS of The white dog barks
tized corpus (Lemm), one including only the rules
extracted from the corpus of semantic representa-
tions (Pred), and one including the union of the two
(Combined). In the Combined grammar, the Lemm
rules with a probability lower than 0.4 were filtered
out if the input relation(s) are already translated by
either handwritten rules or Pred rules since the con-
fidence in the Lemm rules is lower.
Since the two methods for rule extraction involve
different sets of templates, we also made two ver-
sions of the transfer grammar including only the 15
templates used in both Lemm and Pred. These were
named LemmCore and PredCore.
The five versions of the transfer grammar were
tested on sections 003, 004, and 005 of the Tanaka
Corpus (4,500 test sentences), and the results are
shown in Table 2. The table shows how the ver-
sions of Jaen performs with regard to parsing (con-
stant), transfer, generation, and overall coverage. It
also shows the NEVA9 scores of the highest ranked
translated sentences (NEVA), and the highest NEVA
score of the 5 highest ranked translations (Oracle).
The F1 is calculated based on the overall coverage
and the NEVA.
The coverage of Lemm and Pred is the same;
20.8%, but Pred gets a higher NEVA score than
Lemm (21.11 vs. 18.65), and the F1 score is one
percent higher. When the Lemm and Pred rules are
combined in Combined, the coverage is increased
by almost 6%. This increase is due to the fact that
the Lemm and Pred rule sets are relatively compli-
9NEVA (N-gram EVAluation: Forsbom (2003)) is a modi-
fied version of BLEU.
mentary. Although the use of the Lemm and Pred
transfer grammars gives the same coverage (20.8%),
only 648 (14.4%) of the test sentences are translated
by both systems. The NEVA score of Combined is
between that of Lemm and Pred while the F1 score
beats both Lemm and Pred.
When comparing the core versions of Lemm and
Pred, LemmCore and PredCore, we see the same
trend, namely that coverage is about the same and
the NEVA score is higher when the Pred rules are
used.
644 of the test sentences were translated by all
versions of the transfer grammar (Lemm, Pred, and
Combined). Table 3 shows how the different ver-
sions of Jaen perform on these sentences. The re-
sults show that the quality of the transfer rules ex-
tracted from the MRS parallel corpus is higher than
the quality of the transfer rules based on the lemma-
tized parallel corpus. It also shows that there is a
small decrease of quality when the rules from the
lemmatized parallel corpus are added to the rules
from the MRS corpus.
Version NEVA
Lemmatized 20.44
MRS 23.55
Lemma + MRS 23.04
Table 3: NEVA scores of intersecting translations
The two best-performing versions of JaEn, Pred
and Combined, were compared to MOSES (see Ta-
ble 4 and Table 5). The BLEU scores were calcu-
lated with multi-bleu.perl, and the METEOR
72
Parsing Transfer Generation Overall NEVA Oracle F1
LemmCore 3590/4500 1661/3590 930/1661 930/4500 18.65 22.99 19.61
79.8% 46.3% 56.0% 20.7%
Lemm 3590/4500 1674/3590 938/1674 938/4500 18.65 22.99 19.69
79.8% 46.6% 56.0% 20.8%
PredCore 3590/4500 1748/3590 925/1748 925/4500 20.40 24.81 20.48
79.8% 48.7% 52.9% 20.6%
Pred 3590/4500 1782/3589 937/1782 937/4500 21.11 25.75 20.96
79.8% 49.7% 52.6% 20.8%
Combined 3590/4500 2184/3589 1194/2184 1194/4500 19.77 24.00 22.66
79.8% 60.9% 54.7% 26.5%
Table 2: Evaluation of the Tanaka Corpus Test Data
scores were calculated with meteor-1.3.jar
using default settings.10 The human score is a direct
comparison, an evaluator11 was given the Japanese
source, a reference translation and the output from
the two systems, randomly presented as A or B.
They then indicated which they preferred, or if the
quality was the same (in which case each system
gets 0.5). All the translations, including the refer-
ence translations, were tokenized and lower-cased.
In both comparisons, MOSES gets better BLEU and
METEOR scores, while the Jaen translation is pre-
ferred by the human evaluator in 58 out of 100 cases.
BLEU METEOR HUMAN
JaEn First 16.77 28.02 58
MOSES 30.19 31.98 42
Table 4: BLEU Comparison of Jaen loaded with the
Combined rules, and MOSES (1194 items)
BLEU METEOR HUMAN
JaEn 18.34 29.02 58
MOSES 31.37 32.14 42
Table 5: BLEU Comparison of Jaen loaded with the Pred
rules, and MOSES (936 items)
The two systems make different kinds of mis-
takes. The output of Jaen is mostly grammatical,
10The METEOR evaluation metric differs from BLEU in that
it does not only give a score for exact match, but it also gives
partial scores for stem, synonym, and paraphrase matches.
11A Japanese lecturer at NTU, trilingual in English, Japanese
and Korean, not involved in the development of this system, but
with experience in Japanese/Korean MT research.
but it may not always make sense. An example of a
nonsense translation from Jaen is given in (5).12
(5) S: ???????????
R: We eat fish raw.
M: We eat fish raw.
J: We eat fish in the camcorder.
Jaen sometimes gets the arguments wrong:
(6) S: ???????????
R: He was elected president.
M: He was elected president.
J: The president chose him.
The output of Moses on the other hand is more
likely to lack words in the translation, and it is also
more likely to be ungrammatical. A translation with
a missing word is shown in (7).
(7) S: ??????????????
R: The curtains were drawn slowly.
M: The curtain was slowly.
J: The curtain was drawn slowly.
Missing words become extra problematic when a
negation is not transferred:
(8) S: ????????????
R: We shouldn?t have any prejudice.
M: You should have a bias.
J: I shouldn?t have prejudice.
Sometimes the Moses output is lacking so many
words that it is impossible to follow the meaning:
12The examples below are taken from the development data
of the Tanaka Corpus. ?S? stands for ?Source?, ?R? stands for
?Reference translation?, ?M? stands for ?Moses translation,? and
?J? stands for ?Jaen translation.?
73
(9) S: ???????????????
R: Our brains control our activities.
M: The brain to us.
J: The brain is controlling our activities.
Also the output of Moses is more likely to be un-
grammatical, as illustrated in (10) and (11).
(10) S: ?????????????
R: I have a deep love for Japan.
M: I is devoted to Japan.
J: I am deeply loving Japan.
(11) S: ?????????????
R: She wrung the towel dry.
M: She squeezed pressed the towel.
J: She wrung the towel hard.
5 Discussion
In order to get a system with full coverage, Jaen
could be used with Moses as a fallback. This would
combine the precision of the rule-based system with
the robustness of Moses. The coverage and the qual-
ity of Jaen itself can be extended by using more
training data. Our experience is that this holds even
if the training data is from a different domain. By
adding training data, we are incrementally adding
rules to the system. We still build the rules we built
before, plus some more rules extracted from the new
data. Learning rules that are not applicable for the
translation task does not harm or slow down the sys-
tem. Jaen has a rule pre-selection program which,
before each translation task selects the applicable
rules. When the system does a batch translation of
1,500 sentences, the program selects about 15,000 of
the 190,000 automatically extracted rules, and only
these will be loaded. Rules that have been learned
but are not applicable are not used.13
We can also extend the system by adding more
transfer templates. So far, we are using 23 templates,
and by adding new templates for multi-word expres-
sions, we can increase the precision.
The predicate alignments produced from the par-
allel corpus of predicates are relatively precise since
the predicates are assigned by the grammars. This
allows us to extract transfer rules from alignments
13The pre-selection program speeds up the system by a factor
of three.
that are given a low probability (down to 0.01) by
the aligner.
We would also like to get more from the data we
have, by making the parser more robust. Two ap-
proaches that have been shown to work with other
grammars is making more use of morphological in-
formation (Adolphs et al, 2008) or adding robust-
ness rules (Cramer and Zhang, 2010).
6 Conclusion
We have shown how semantic transfer rules can be
learned from parallel corpora that have been aligned
in SMT phrase tables. We employed two strategies.
The first strategy was to lemmatize the parallel cor-
pus and use SMT aligners to create phrase tables of
lemmas. We then looked up the relations associated
with the lemmas using the lexicons of the parser and
generator. This gave us a phrase table of aligned
relations. We were able to extract 127,000 rules
by matching the aligned relations with 16 semantic
transfer rule templates.
The second strategy was to parse the parallel cor-
pus with the parsing grammar and the generating
grammar of the MT system. This gave us a paral-
lel corpus of predicates, which, because of lack of
coverage of the grammars, was about a third the size
of the full corpus. The parallel corpus of predicates
was aligned with SMT aligners, and we got a sec-
ond phrase table of aligned relations. We extracted
113,000 rules by matching the alignments against 22
rule templates. These transfer rules produced the
same number of translation as the rules produced
with the first strategy (20.8%), but they proved to
be more precise.
The two rule extraction methods complement
each other. About 30% of the sentences translated
with one rule set are not translated by the other. By
merging the two rule sets into one, we increased the
coverage of the system to 26.6%. A human evalua-
tor preferred Jaen?s translation to that of Moses for
58 out of a random sample of 100 translations.
References
Peter Adolphs, Stephan Oepen, Ulrich Callmeier,
Berthold Crysmann, Dan Flickinger, and Bernd
Kiefer. 2008. Some fine points of hybrid natu-
ral language parsing. In European Language Re-
74
sources Association (ELRA), editor, Proceedings
of the Sixth International Language Resources
and Evaluation (LREC?08), pages 1380?1387.
Marrakech, Morocco.
Francis Bond, Hitoshi Isahara, Kiyotaka Uchimoto,
Takayuki Kuribayashi, and Kyoko Kanzaki. 2010.
Japanese WordNet 1.0. In 16th Annual Meeting of
the Association for Natural Language Processing,
pages A5?3. Tokyo.
Francis Bond, Stephan Oepen, Eric Nichols, Dan
Flickinger, Erik Velldal, and Petter Haugereid.
2011. Deep open source machine transla-
tion. Machine Translation, 25(2):87?105.
URL http://dx.doi.org/10.1007/
s10590-011-9099-4, (Special Issue on
Open source Machine Translation).
James W. Breen. 2004. JMDict: a Japanese-
multilingual dictionary. In Coling 2004 Workshop
on Multilingual Linguistic Resources, pages 71?
78. Geneva.
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal Recursion Seman-
tics. An introduction. Research on Language and
Computation, 3(4):281?332.
Bart Cramer and Yi Zhang. 2010. Constraining
robust constructions for broad-coverage parsing
with precision grammars. In Proceedings of
COLING-2010, pages 223?231. Beijing.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6(1):15?28. (Special Issue on Effi-
cient Processing with HPSG).
Eva Forsbom. 2003. Training a super model look-
alike: Featuring edit distance, n-gram occurrence,
and one reference translation. In In Proceedings
of the Workshop on Machine Translation Evalua-
tion. Towards Systemizing MT Evaluation.
Philipp Koehn, Wade Shen, Marcello Federico,
Nicola Bertoldi, Chris Callison-Burch, Brooke
Cowan, Chris Dyer, Hieu Hoang, Ondrej Bo-
jar, Richard Zens, Alexandra Constantin, Evan
Herbst, Christine Moran, and Alexandra Birch.
2007. Moses: Open source toolkit for statistical
machine translation. In Proceedings of the ACL
2007 Interactive Presentation Sessions. Prague.
URL http://www.statmt.org/moses/.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying Conditional Random Fields to
Japanese Morphological Analysis. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP
2004, pages 230?237. Association for Computa-
tional Linguistics, Barcelona, Spain.
Adrien Lardilleux and Yves Lepage. 2009.
Sampling-based multilingual alignment. In
Proceedings of Recent Advances in Natural
Language Processing (RANLP 2009), pages
214?218. Borovets, Bulgaria.
Stephan Oepen, Erik Velldal, Jan Tore L?nning,
Paul Meurer, and Victoria Rosen. 2007. Towards
hybrid quality-oriented machine translation. on
linguistics and probabilities in MT. In 11th Inter-
national Conference on Theoretical and Method-
ological Issues in Machine Translation: TMI-
2007, pages 144?153.
Llu?s Padr?, Miquel Collado, Samuel Reese, Ma-
rina Lloberes, and Irene Castell?n. 2010. Freel-
ing 2.1: Five years of open-source language pro-
cessing tools. In Proceedings of 7th Language
Resources and Evaluation Conference (LREC
2010). La Valletta. (http://nlp.lsi.upc.
edu/freeling.
Melanie Siegel and Emily M. Bender. 2002. Effi-
cient deep processing of Japanese. In Proceed-
ings of the 3rd Workshop on Asian Language Re-
sources and International Standardization at the
19th International Conference on Computational
Linguistics, pages 1?8. Taipei.
Yasuhito Tanaka. 2001. Compilation of a multilin-
gual parallel corpus. In Proceedings of PACLING
2001, pages 265?268. Kyushu. (http:
//www.colips.org/afnlp/archives/
pacling2001/pdf/tanaka.pdf).
Kiyotaka Uchimoto, Yujie Zhang, Kiyoshi Sudo,
Masaki Murata, Satoshi Sekine, and Hitoshi
Isahara. 2004. Multilingual aligned paral-
lel treebank corpus reflecting contextual in-
formation and its applications. In Gilles
S?rasset, editor, COLING 2004 Multilingual
Linguistic Resources, pages 57?64. COLING,
Geneva, Switzerland. URL http://acl.
ldc.upenn.edu/W/W04/W04-2208.bib.
75
