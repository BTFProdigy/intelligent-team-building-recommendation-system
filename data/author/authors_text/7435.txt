Proceedings of the 43rd Annual Meeting of the ACL, pages 565?572,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Instance-based Sentence Boundary Determination by Optimization for
Natural Language Generation
Shimei Pan and James C. Shaw
IBM T. J. Watson Research Center
19 Skyline Drive
Hawthorne, NY 10532, USA
{shimei,shawjc}@us.ibm.com
Abstract
This paper describes a novel instance-
based sentence boundary determination
method for natural language generation
that optimizes a set of criteria based on
examples in a corpus. Compared to exist-
ing sentence boundary determination ap-
proaches, our work offers three signifi-
cant contributions. First, our approach
provides a general domain independent
framework that effectively addresses sen-
tence boundary determination by balanc-
ing a comprehensive set of sentence com-
plexity and quality related constraints.
Second, our approach can simulate the
characteristics and the style of naturally
occurring sentences in an application do-
main since our solutions are optimized
based on their similarities to examples
in a corpus. Third, our approach can
adapt easily to suit a natural language gen-
eration system?s capability by balancing
the strengths and weaknesses of its sub-
components (e.g. its aggregation and re-
ferring expression generation capability).
Our final evaluation shows that the pro-
posed method results in significantly bet-
ter sentence generation outcomes than a
widely adopted approach.
1 Introduction
The problem of sentence boundary determination in
natural language generation exists when more than
one sentence is needed to convey multiple concepts
and propositions. In the classic natural language
generation (NLG) architecture (Reiter, 1994), sen-
tence boundary decisions are made during the sen-
tence planning stage in which the syntactic struc-
ture and wording of sentences are decided. Sentence
boundary determination is a complex process that
directly impacts a sentence?s readability (Gunning,
1952), its semantic cohesion, its syntactic and lex-
ical realizability, and its smoothness between sen-
tence transitions. Sentences that are too complex are
hard to understand, so are sentences lacking seman-
tic cohesion and cross-sentence coherence. Further
more, bad sentence boundary decisions may even
make sentences unrealizable.
To design a sentence boundary determination
method that addresses these issues, we employ an
instance-based approach (Varges and Mellish, 2001;
Pan and Shaw, 2004). Because we optimize our so-
lutions based on examples in a corpus, the output
sentences can demonstrate properties, such as simi-
lar sentence length distribution and semantic group-
ing similar to those in the corpus. Our approach
also avoids problematic sentence boundaries by op-
timizing the solutions using all the instances in the
corpus. By taking a sentence?s lexical and syntac-
tic realizability into consideration, it can also avoid
sentence realization failures caused by bad sentence
boundary decisions. Moreover, since our solution
can be adapted easily to suit the capability of a natu-
ral language generator, we can easily tune the algo-
rithm to maximize the generation quality. To the best
of our knowledge, there is no existing comprehen-
sive solution that is domain-independent and pos-
sesses all the above qualities. In summary, our work
offers three significant contributions:
1. It provides a general and flexible sentence
565
boundary determination framework which
takes a comprehensive set of sentence com-
plexity and quality related criteria into consid-
eration and ensures that the proposed algorithm
is sensitive to not only the complexity of the
generated sentences, but also their semantic co-
hesion, multi-sentence coherence and syntactic
and lexical realizability.
2. Since we employ an instance-based method,
the proposed solution is sensitive to the style
of the sentences in the application domain in
which the corpus is collected.
3. Our approach can be adjusted easily to suit
a sentence generation system?s capability and
avoid some of its known weaknesses.
Currently, our work is embodied in a multimodal
conversation application in the real-estate domain in
which potential home buyers interact with the sys-
tem using multiple modalities, such as speech and
gesture, to request residential real-estate informa-
tion (Zhou and Pan, 2001; Zhou and Chen, 2003;
Zhou and Aggarwal, 2004). After interpreting the
request, the system formulates a multimedia pre-
sentation, including automatically generated speech
and graphics, as the response (Zhou and Aggarwal,
2004). The proposed sentence boundary determi-
nation module takes a set of propositions selected
by a content planner and passes the sentence bound-
ary decisions to SEGUE (Pan and Shaw, 2004), an
instance-based sentence generator, to formulate the
final sentences. For example, our system is called
upon to generate responses to a user?s request: ?Tell
me more about this house.? Even though not all of
the main attributes of a house (more than 20) will be
conveyed, it is clear that a good sentence boundary
determination module can greatly ease the genera-
tion process and improve the quality of the output.
In the rest of the paper, we start with a discussion
of related work, and then describe our instance-base
approach to sentence boundary determination. Fi-
nally, we present our evaluation results.
2 Related Work
Existing approaches to sentence boundary determi-
nation typically employ one of the following strate-
gies. The first strategy uses domain-specific heuris-
tics to decide which propositions can be combined.
For example, Proteus (Davey, 1979; Ritchie, 1984)
produces game descriptions by employing domain-
specific sentence scope heuristics. This approach
can work well for a particular application, however,
it is not readily reusable for new applications.
The second strategy is to employ syntactic, lex-
ical, and sentence complexity constraints to con-
trol the aggregation of multiple propositions (Robin,
1994; Shaw, 1998). These strategies can generate
fluent complex sentences, but they do not take other
criteria into consideration, such as semantic cohe-
sion. Further more, since these approaches do not
employ global optimization as we do, the content of
each sentence might not be distributed evenly. This
may cause dangling sentence problem (Wilkinson,
1995).
Another strategy described in Mann and
Moore(1981) guided the aggregation process by
using an evaluation score that is sensitive to the
structure and term usage of a sentence. Similar to
our approach, they rely on search to find an optimal
solution. The main difference between this approach
and ours is that their evaluation score is computed
based on preference heuristics. For example, all
the semantic groups existing in a domain have to
be coded specifically in order to handle semantic
grouping. In contrast, in our framework, the score is
computed based on a sentence?s similarity to corpus
instances, which takes advantage of the naturally
occurring semantic grouping in the corpus.
Recently, Walker (2002) and Stent (2004) used
statistical features derived from corpus to rank gen-
erated sentence plans. Because the plan ranker was
trained with existing examples, it can choose a plan
that is consistent with the examples. However, de-
pending on the features used and the size of the train-
ing examples, it is unclear how well it can capture
patterns like semantic grouping and avoid problems
likes dangling sentences.
3 Examples
Before we describe our approach in detail, we start
with a few examples from the real-estate domain
to demonstrate the properties of the proposed ap-
proach.
First, sentence complexity impacts sentence
boundary determination. As shown in Table 1, af-
ter receiving a user?s request (U1) for the details of a
house, the content planner asked the sentence plan-
ner to describe the house with a set of attributes in-
cluding its asking price, style, number of bedrooms,
number of bathrooms, square footage, garage, lot
size, property tax, and its associated town and school
566
Example Turn Sentence
E1 U1 Tell me more about this house
S1 This is a 1 million dollar 3 bedroom, 2 bathroom, 2000 square foot colonial
with 2 acre of land, 2 car garage, annual taxes 8000 dollars in Armonk
and in the Byram Hills school district.
S2 This is a 1 million dollar house. This is a 3 bedroom house. This is a 2 bathroom
house. This house has 2000 square feet. This house has 2 acres of land.
This house has 2 car garage. This is a colonial house. The annual taxes are 8000 dollars.
This house is in Armonk. This house is in the Byram Hills school district.
S3 This is a 3 bedroom, 2 bathroom, 2000 square foot colonial located in Armonk
with 2 acres of land. The asking price is 1 million dollar and the annual taxes
are 8000 dollars. The house is located in the Byram Hills School District.
E2 S4 This is a 1 million dollar 3 bedroom house. This is a 2 bathroom house with
annual taxes of 8000 dollars.
S5 This is a 3 bedroom and 2 bathroom house. Its price is 1 million dollar and
its annual taxes are 8000 dollars.
E3 S6 The tax rate of the house is 3 percent.
S7 The house has an asphalt roof.
E4 S8 This is a 3 bedroom, 2 bathroom colonial with 2000 square feet and 2 acres of land.
S9 The house has 2 bedrooms and 3 bathrooms. This house is a colonial.
It has 2000 square feet. The house is on 2 acres of land.
Table 1: Examples
district name. Without proper sentence boundary
determination, a sentence planner may formulate a
single sentence to convey all the information, as in
S1. Even though S1 is grammatically correct, it
is too complex and too exhausting to read. Simi-
larly, output like S2, despite its grammatical correct-
ness, is choppy and too tedious to read. In contrast,
our instance-based sentence boundary determination
module will use examples in a corpus to partition
those attributes into several sentences in a more bal-
anced manner (S3).
Semantic cohesion also influences the quality of
output sentences. For example, in the real-estate
domain, the number of bedrooms and number of
bathrooms are two closely related concepts. Based
on our corpus, when both concepts appear, they al-
most always conveyed together in the same sen-
tence. Given this, if the content planner wants to
convey a house with the following attributes: price,
number of bedrooms, number of bathrooms, and
property tax, S4 is a less desirable solution than S5
because it splits these concepts into two separate
sentences. Since we use instance-based sentence
boundary determination, our method generates S5 to
minimize the difference from the corpus instances.
Sentence boundary placement is also sensitive to
the syntactic and lexical realizability of grouped
items. For example, if the sentence planner asks the
surface realizer to convey two propositions S6 and
S7 together in a sentence, a realization failure will
be triggered because both S6 and S7 only exist in
the corpus as independent sentences. Since neither
of them can be transformed into a modifier based on
the corpus, S6 and S7 cannot be aggregated in our
system. Our method takes a sentence?s lexical and
syntactic realizability into consideration in order to
avoid making such aggregation request to the sur-
face realizer in the first place.
A generation system?s own capability may also
influence sentence boundary determination. Good
sentence boundary decisions will balance a system?s
strengths and weaknesses. In contrast, bad decisions
will expose a system?s venerability. For example, if
a sentence generator is good at performing aggre-
gations and weak on referring expressions, we may
avoid incoherence between sentences by preferring
aggregating more attributes in one sentence (like in
S8) rather than by splitting them into multiple sen-
tences (like in S9).
In the following, we will demonstrate how our ap-
proach can achieve all the above goals in a unified
instance-based framework.
4 Instance-based boundary determination
Instance-based generation automatically creates
sentences that are similar to those generated by hu-
mans, including their way of grouping semantic con-
tent, their wording and their style. Previously, Pan
and Shaw (2004) have demonstrated that instance-
based learning can be applied successfully in gen-
erating new sentences by piecing together existing
words and segments in a corpus. Here, we want to
demonstrate that by applying the same principle, we
can make better sentence boundary decisions.
567
The key idea behind the new approach is to find a
sentence boundary solution that minimizes the ex-
pected difference between the sentences resulting
from these boundary decisions and the examples in
the corpus. Here we measure the expected differ-
ence based a set of cost functions.
4.1 Optimization Criteria
We use three sentence complexity and quality re-
lated cost functions as the optimization criteria: sen-
tence boundary cost, insertion cost and deletion cost.
Sentence boundary cost (SBC): Assuming P is
a set of propositions to be conveyed and S is a col-
lection of example sentences selected from the cor-
pus to convey P . Then we say P can be realized
by S with a sentence boundary cost that is equal to
(|S| ? 1) ? SBC in which |S| is the number of sen-
tences and SBC is the sentence boundary cost. To
use a specific example from the real-estate domain,
the input P has three propositions:
p
1
. House1 has-attr (style=colonial).
p
2
. House1 has-attr(bedroom=3).
p
3
. House1 has-attr(bathroom=2).
One solution, S, contains 2 sentences:
s
1
. This is a 3 bedroom, 2 bathroom house.
s
2
. This is a colonial house.
Since only one sentence boundary is involved, S is a
solution containing one boundary cost. In the above
example, even though both s
1
and s
2
are grammati-
cal sentences, the transition from s
1
to s
2
is not quite
smooth. They sound choppy and disjointed. To pe-
nalize this, whenever there is a sentence break, there
is a SBC. In general, the SBC is a parameter that is
sensitive to a generation system?s capability such as
its competence in reference expression generation.
If a generation system does not have a robust ap-
proach for tracking the focus across sentences, it is
likely to be weak in referring expression generation
and adding sentence boundaries are likely to cause
fluency problems. In contrast, if a generation sys-
tem is very capable in maintaining the coherence be-
tween sentences, the proper sentence boundary cost
would be lower.
Insertion cost: Assume P is the set of propo-
sitions to be conveyed, and Ci is an instance in
the corpus that can be used to realize P by insert-
ing a missing proposition pj to Ci, then we say P
can be realized using Ci with an insertion cost of
icost(CH , pj), in which CH is the host sentence in
the corpus containing proposition pj . Using an ex-
ample from our real-estate domain, assume the input
P=(p
2
, p
3
, p
4
), where
p
4
. House1 has-attr (square footage=2000).
Assume Ci is a sentence selected from the cor-
pus to realize P : ?This is 3 bedroom 2 bathroom
house?. Since Ci does not contain p4, p4 needs to
be added. We say that P can be realized using Ci
by inserting a proposition p
4
with an insertion cost
of icost(CH , p4), in which CH is a sentence in the
corpus such as ?This is a house with 2000 square
feet.?
The insertion cost is influenced by two main fac-
tors: the syntactic and lexical insertability of the
proposition pj and a system?s capability in aggre-
gating propositions. For example, if in the corpus,
the proposition pj is always realized as an indepen-
dent sentence and never as a modifier, icost(?, pj)
should be extremely high, which effectively pro-
hibit pj from becoming a part of another sen-
tence. icost(?, pj) is defined as the minimum in-
sertion cost among all the icost(CH , pj). Currently
icost(CH , pj) is computed dynamically based on
properties of corpus instances. In addition, since
whether a proposition is insertable depends on how
capable an aggregation module can combine propo-
sitions correctly into a sentence, the insertion cost
should be assigned high or low accordingly.
Deletion cost: Assume P is a set of input proposi-
tions to be conveyed and Ci is an instance in the cor-
pus that can be used to convey P by deleting an un-
needed proposition pj in Ci. Then, we say P can be
realized using Ci with a deletion cost dcost(Ci, pj).
As a specific example, assuming the input is P=(p
2
,
p
3
, p
4
), Ci is an instance in the corpus ?This is a
3 bedroom, 2 bathroom, 2000 square foot colonial
house.? In addition to the propositions p
2
, p
3
and
p
4
, Ci also conveys a proposition p1. Since p1 is
not needed when conveying P , we say that P can be
realized using Ci by deleting proposition p1 with a
deletion cost of dcost(Ci, p1).
The deletion cost is affected by two main fac-
tors as well: first the syntactic relation between
pj and its host sentence. Given a new instance
Ci, ?This 2000 square foot 3 bedroom, 2 bathroom
house is a colonial?, deleting p
1
, the main object
568
of the verb, will make the rest of the sentence in-
complete. As a result, dcost(Ci, p1) is very expen-
sive. In contrast, dcost(Ci, p4) is low because the
resulting sentence is still grammatically sound. Cur-
rently dcost(Ci, pj) is computed dynamically based
on properties of corpus instances. Second, the ex-
pected performance of a generation system in dele-
tion also impacts the deletion cost. Depending on
the sophistication of the generator to handle various
deletion situations, the expected deletion cost can
be high if the method employed is naive and error
prone, or is low if the system can handle most cases
accurately.
Overall cost: Assume P is the set of propositions
to be conveyed and S is the set of instances in the
corpus that are chosen to realize P by applying a set
of insertion, deletion and sentence breaking opera-
tions, the overall cost of the solution
Cost(P ) =
?
C
i
(Wi ?
?
j
icost(CHj , pj)
+Wd ?
?
k
dcost(Ci, pk))
+(Nb ? 1) ? SBC
in which Wi, Wd and SBC are the insertion weight,
deletion weight and sentence boundary cost; Nb is
the number of sentences in the solution, Ci is a cor-
pus instance been selected to construct the solution
and CHj is the host sentence that proposition pj be-
longs.
4.2 Algorithm: Optimization based on overall
cost
We model the sentence boundary determination pro-
cess as a branch and bound tree search problem. Be-
fore we explain the algorithm itself, first a few no-
tations. The input P is a set of input propositions
chosen by the content planner to be realized. ? is
the set of all possible propositions in an application
domain. Each instance Ci in the corpus C is repre-
sented as a subset of ?. Assume S is a solution to
P , then it can be represented as the overall cost plus
a list of pairs like (Cis, Ois), in which Cis is one
of the instances selected to be used in that solution,
Ois is a set of deletion, insertion operations that can
be applied to Cis to transform it to a subsolution Si.
To explain this representation further, we use a spe-
cific example in which P=(a, d, e, f), ?=(a, b, c, d,
e, f g, h, i). One of the boundary solution S can be
represented as
S = (Cost(S), (S1, S2))
S
1
= (C
1
= (a, b, d, i), delete(b, i)),
S
2
= (C
2
= (e), insert(f as in C
3
= (f, g)))
Cost(S) = Wd ? (dcost(C1, b) + dcost(C1, i)) +
Wi ? icost(C3, f) + 1 ? SBC
in which C
1
and C
2
are two corpus instances se-
lected as the bases to formulate the solution and C
3
is the host sentence containing proposition f .
The general idea behind the instance-based
branch and bound tree search algorithm is that given
an input, P , for each corpus instance Ci, we con-
struct a search branch, representing all possible
ways to realize the input using the instance plus
deletions, insertions and sentence breaks. Since
each sentence break triggers a recursive call to
our sentence boundary determination algorithm, the
complexity of the algorithm is NP-hard. To speed up
the process, for each iteration, we prune unproduc-
tive branches using an upper bound derived by sev-
eral greedy algorithms. The details of our sentence
boundary determination algorithm, sbd(P ), are de-
scribed below. P is the set of input propositions.
1. Set the current upper bound, UB, to the mini-
mum cost of solutions derived by greedy algo-
rithms, which we will describe later. This value
is used to prune unneeded branches to make the
search more efficient.
2. For each instance Ci in corpus C in which (Ci?
P ) 6= ?, loop from step 3 to 9. The goal here
is to identify all the useful corpus instances for
realizing P .
3. Delete all the propositions pj ? D in which
D = Ci ? P (D contains propositions in Ci
but not exist in P) with cost Costd(P ) = Wd ?
?
P
j
?D dcost(Ci, pj). This step computes the
deletion operators and their associated costs.
4. Let I = P ? Ci (I contains propositions in P
but not in Ci). For each subset Ej ? I (Ej in-
cludes ? and I itself), iterate through step 5 to
9. These steps figure out all the possible ways
to add the missing propositions, including in-
serting into the instance Ci and separating the
rest as independent sentence(s).
569
5. Generate a solution in which ?pk ? Ej , insert
pk to Ci. All the propositions in Q = I ? Ej
will be realized in different sentences, thus in-
curring a SBC.
6. We update the cost Cost(P ) to
Costd(P ) + Wi ?
?
p
k
?E
j
icost(?, pk)+
SBC + Cost(Q)
in which Cost(Q) is the cost of sbd(Q) which
recursively computes the best solution for input
Q and Q ? P . To facilitate dynamic program-
ming, we remember the best solution for Q de-
rived by sbd(Q) in case Q is used to formulate
other solutions.
7. If the lower bound for Cost(P) is greater than
the established upper bound UB, prune this
branch.
8. Using the notation described in the beginning
of Sec. 4.2, we update the current solution to
sbd(P ) = (Cost(P ), (Ci, delete
?p
j
?D(pj),
insert
?p
k
?E
j
(pk)))
?
sbd(Q)
in which
?
is an operator that composes two
partial solutions.
9. If sbd(P) is a complete solution (either Q is
empty or have a known best solution) and
Cost(P ) < UB, update the upper bound
UB = Cost(P ).
10. Output the solution with the lowest overall cost.
To establish the initial UB for pruning, we use the
minimum of the following three bounds. In general,
the tighter the UB is, the more effective the pruning
is.
Greedy set partition: we employ a greedy set
partition algorithm in which we first match the set
S ? P with the largest |S|. Repeat the same process
for P ? where P ? = P ? S. The solution cost is
Cost(P ) = (N ? 1) ? SBC , and N is the number
of sentences in the solution. The complexity of this
computation is O(|P |), where |P | is the number of
propositions in P .
Revised minimum set covering: we employ a
greedy minimum set covering algorithm in which
we first find the set S in the corpus that maximizes
the overlapping of propositions in the input P . The
unwanted propositions in S ? P are deleted. As-
sume P ? = P ? S, repeat the same process to P?
until P ? is empty. The only difference between this
and the previous approach is that S here might not
be a subset of P . The complexity of this computa-
tion is O(|P |).
One maximum overlapping sentence: we first
identify the instance Ci in corpus that covers the
maximum number of propositions in P . To arrive
at a solution for P , the rest of the propositions not
covered by Ci are inserted into Ci and all the un-
wanted propositions in Ci are deleted. The cost of
this solution is
Wd ?
?
p
j
?D
dcost(Ci, pj) + Wi ?
?
p
k
?I
icost(?, pk)
in which D includes proposition in Ci but not in P ,
and I includes propositions in P but not in Ci.
Currently, we update UB only after a complete
solution is found. It is possible to derive better UB
by establishing the upper bound for each partial so-
lution, but the computational overhead might not
justify doing so.
4.3 Approximation Algorithm
Even with pruning and dynamic programming, the
exact solution still is very expensive computation-
ally. Computing exact solution for an input size
of 12 propositions has over 1.6 millions states and
takes more than 30 minutes (see Figure 1). To make
the search more efficient for tasks with a large num-
ber of propositions in the input, we naturally seek
a greedy strategy in which at every iteration the al-
gorithm myopically chooses the next best step with-
out regard for its implications on future moves. One
greedy search policy we implemented explores the
branch that uses the instance with maximum over-
lapping propositions with the input and ignores all
branches exploring other corpus instances. The in-
tuition behind this policy is that the more overlap
an instance has with the input, the less insertions or
sentence breaks are needed.
Figure 1 and Figure 2 demonstrate the trade-
off between computation efficiency and accuracy.
In this graph, we use instances from the real-
estate corpus with size 250, we vary the input sen-
tence length from one to twenty and the results
shown in the graphs are average value over sev-
eral typical weight configurations ((Wd,Wi,SBC)=
570
(1,3,5),(1,3,7),(1,5,3),(1,7,3),(1,1,1)). Figure 2 com-
pares the quality of the solutions when using exact
solutions versus approximation. In our interactive
multimedia system, we currently use exact solution
for input size of 7 propositions or less and switch to
greedy for any larger input size to ensure sub-second
performance for the NLG component.
0
20
40
60
80
100
120
140
160
180
200
2 4 6 8 9 10 12 14 16 18 20
# of Propositions in Input
Ex
ec
ut
io
n 
Ti
m
e 
(Se
co
nd
s)
Greedy
Exact
Figure 1: Speed difference between exact solutions
and approximations
0
2
4
6
8
10
12
14
16
18
20
2 4 6 8 9 10 12 14 16 18 20
# of Proposition in Input
Co
st Greedy
Exact
Figure 2: Cost difference between exact solutions
and approximations
Measures Ours B-3 B-6
Dangling sentence (7) 0 100% 100%
Split Semantic Group 1% 61% 21%
Realization Failure 0 56% 72%
Fluency 59% 4% 8%
Table 2: Comparisons
5 Evaluations
To evaluate the quality of our sentence boundary de-
cisions, we implemented a baseline system in which
boundary determination of the aggregation module
is based on a threshold of the maximum number
of propositions allowed in a sentence (a simplified
version of the second strategy in Section 2. We
have tested two threshold values, the average (3) and
maximum (6) number of propositions among cor-
pus instances. Other sentence complexity measures,
such as the number of words and depth of embed-
ding are not easily applicable for our comparison
because they require the propositions to be realized
first before the boundary decisions can be made.
We tune the relative weight of our approach to
best fit our system?s capability. Currently, the
weights are empirically established to Wd = 1,
Wi = 3 and SBC = 3. Based on the output gen-
erated from both systems, we derive four evaluation
metrics:
1. Dangling sentences: We define dangling sen-
tences as the short sentences with only one
proposition that follow long sentences. This
measure is used to verify our claim that because
we use global instead of local optimization,
we can avoid generating dangling sentences by
making more balanced sentence boundary de-
cisions. In contrast, the baseline approaches
have dangling sentence problem when the in-
put proposition is 1 over the multiple of the
threshold values. The first row of Table 2 shows
that when the input proposition length is set
to 7, a pathological case, among the 200 input
proposition sets randomly generated, the base-
line approach always produce dangling sen-
tences (100%). In contrast, our approach al-
ways generates more balanced sentences (0%).
2. Semantic group splitting. Since we use an
instance-based approach, we can maintain the
semantic cohesion better. To test this, we
randomly generated 200 inputs with up to 10
propositions containing semantic grouping of
both the number of bedrooms and number of
bathrooms. The second row, Split Semantic
Group, in Table 2 shows that our algorithm can
maintain semantic group much better than the
baseline approach. Only in 1% of the output
sentences, our algorithm generated number of
bedrooms and number of bathrooms in separate
sentences. In contrast, the baseline approaches
did much worse (61% and 21%).
3. Sentence realization failure. This measure is
used to verify that since we also take a sen-
tence?s lexical and syntactical realizability into
consideration, our sentence boundary decisions
will result in less sentence realization failures.
571
An realization failure occurs when the aggre-
gation module failed to realize one sentence
for all the propositions grouped by the sentence
boundary determination module. The third row
in Table 2, Realization Failure, indicates that
given 200 randomly generated input proposi-
tion sets with length from 1 to 10, howmany re-
alization happened in the output. Our approach
did not have any realization failure while for the
baseline approaches, there are 56% and 72%
outputs have one or more realization failures.
4. Fluency. This measure is used to verify our
claim that since we also optimize our solutions
based on boundary cost, we can reduce incoher-
ence across multiple sentences. Given 200 ran-
domly generated input propositions with length
from 1 to 10, we did a blind test and presented
pairs of generated sentences to two human sub-
jects randomly and asked them to rate which
output is more coherent. The last row, Flu-
ency, in Table 2 shows how often the human
subjects believe that a particular algorithm gen-
erated better sentences. The output of our al-
gorithm is preferred for more than 59% of the
cases, while the baseline approaches are pre-
ferred 4% and 8%, respectively. The other per-
centages not accounted for are cases where the
human subject felt there is no significant differ-
ence in fluency between the two given choices.
The result from this evaluation clearly demon-
strates the superiority of our approach in gener-
ating coherent sentences.
6 Conclusion
In the paper, we proposed a novel domain indepen-
dent instance-based sentence boundary determina-
tion algorithm that is capable of balancing a com-
prehensive set of generation capability, sentence
complexity, and quality related constraints. This
is the first domain-independent algorithm that pos-
sesses many desirable properties, including balanc-
ing a system?s generation capabilities, maintaining
semantic cohesion and cross sentence coherence,
and preventing severe syntactic and lexical realiza-
tion failures. Our evaluation results also demon-
strate the superiority of the approach over a rep-
resentative domain independent sentence boundary
solution.
References
Anthony C. Davey. 1979. Discourse Production. Edin-
burgh University Press, Edinburgh.
Robert Gunning. 1952. The Technique of Clear Writing.
McGraw-Hill.
William C. Mann and James A. Moore. 1981. Computer
generation of multiparagraph English text. American
Journal of Computational Linguistics, 7(1):17?29.
Shimei Pan and James Shaw. 2004. SEGUE: A hy-
brid case-based surface natural language generator. In
Proc. of ICNLG, Brockenhurst, U.K.
Ehud Reiter. 1994. Has a consensus NL generation
architecture appeared, and is it psycholinguistically
plausible? In Proc. of INLG, Kennebunkport, Maine.
Graeme D. Ritchie. 1984. A rational reconstruction of
the Proteus sentence planner. In Proc. of the COLING
and the ACL, Stanford, CA.
Jacques Robin. 1994. Automatic generation and revi-
sion of natural language summaries providing histori-
cal background. In Proc. of the Brazilian Symposium
on Artificial Intelligence, Fortaleza, CE, Brazil.
James Shaw. 1998. Segregatory coordination and ellipsis
in text generation. In Proc. of the COLING and the
ACL., Montreal, Canada.
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex in-
formation presentation in spoken dialog systems. In
Proc. of the ACL, Barcelona, Spain.
Sebastian Varges and Chris Mellish. 2001. Instance-
based natural language generation. In Proc. of the
NAACL, Pittsburgh, PA.
Marilyn Walker, Owen Rambow, and Monica Rogati.
2002. Training a sentence planner for spoken dialogue
using boosting. Computer Speech and Language.
John Wilkinson. 1995. Aggregation in natural language
generation: Another look. Co-op work term report,
Dept. of Computer Science, University of Waterloo.
Michelle Zhou and Vikram Aggarwal. 2004. An
optimization-based approach to dynamic data content
selection in intelligent multimedia interfaces. In Proc.
of the UIST, Santa Fe, NM.
Michelle X. Zhou and Min Chen. 2003. Automated
generation of graphic sketches by example. In IJCAI,
Acapulco, Mexico.
Michelle X. Zhou and Shimei Pan. 2001. Automated
authoring of coherent multimedia discourse in conver-
sation systems. In ACM Multimedia, Ottawa, Canada.
572
Generating Referring Quantified Expressions 
J ames  Shaw and Kath leen  McKeown 
Dept .  of Computer  Sc ience 
Co lumbia  Un ivers i ty  
New York,  NY  10027, USA 
shaw,  kathy*cs ,  co lumbia ,  edu 
, : ,  ~ .*~ . 
Abst ract  
In this paper, we describe how quantifiers can be 
generated in a text generation system. By taking 
advantage of discourse and ontological information, 
quantified expressions can replace entities in a text, 
making the text more fluent and concise. In ad- 
dition to avoiding ambiguities between distributive 
and collective readings in universal quantification 
generation, we will also show how different scope 
orderings between universal and existential quanti- 
tiers will result in different quantified expressions in
our algorithm. 
1 In t roduct ion  
To convey information concisely and fluently, text 
generation systems often perform opportunistic text 
planning (Robin, 1995; Mellish et al, 1998) and em- 
ploy advanced linguistic constructions such as ellip- 
sis (Shaw, 1998). But a system can also take ad- 
vantage of quantification and ontological informa- 
tion to generate concise references to entities at the 
discourse level. For example, a sentence such as 
"'The patient has an infusion line in each arm." is 
a more concise version of "The patient has an in- 
fusion line ir~ his left arm. The patient has an in- 
fusion line in his right arm." Quantification is an 
active research topic in logic, language, and philoso- 
phy(Carpenter, 1997; de Swart. 1998). Since nat- 
ural language understanding systems need to ob- 
tain as few interpretations as possible from text, 
researchers have studied quantifier scope ambigu- 
ity extensively (Woods~ 1978;-Grosz et al, 1987; 
Hobbs and Shieber, 1987: Pereira, 1990; Moran and 
Pereira, 1992: Park, 1995). Research in quantifica- 
tion interpretation first transforms a sentence into 
predicate logic, raises tim quantifiers to the senten- 
tial level, and permutes these quantifiers {o obtain 
as many readings as possible relaled to quantifier 
scoping. Then, invalid readings are eliminated using 
various consl raints. 
Ambiguity in quantified expressions i caused by 
two main culprits. The  first type of ambiguity in- 
volves the distributive reading versus the collective 
reading. In universal quantification, a referring ex- 
100 
pression refers to multiple entities. There is a po- 
tential ambiguity between whether the aggregated 
entities acted individually (distributive) or acted to- 
gether as one (collective). Under the distributive 
reading, the sentence "All the nurses inspected the 
patient." implies that each nurse individually in- 
spected the patient. Under the collective reading, 
the nurses inspected the patient ogether as a group. 
The other ambiguity in quantification i volves mul- 
tiple quantifiers in the same sentence. The sentence 
"A nurse inspected each patient." has two possi- 
ble quantifier scope orderings. In Vpatient3nurse, 
the universal quantifier V has wide scope, outscop- 
ing the existential quantifier 3. This ordering means 
that each patient is inspected by a nurse, who might 
not be the same in each case. In the other scope 
order, 3nurseVpatient, a single, particular nurse in- 
spected every patient. In both types of ambiguities, 
a generation system should make the desired reading 
clear. 
Fortunately, the difficulties of quantifier scope dis- 
ambiguation faced by the understanding conmmnity 
do not apply to text generation. For generation, the 
problem is the reverse: given an unambiguous rep- 
resentation of a set of facts as input, how can it 
generate a quantified sentence that unambiguously 
conveys the intended meaning? In this paper, we 
propose an algorithm which selects an appropriate 
quantified expression to refer .to a set of entities us- 
ing discourse and ontological knowledge. The algo- 
rithm first identifies the entities for quantification i
;the input :propositions. Then an- appropriate con- 
cept in the ontology is selected to refer to these en- 
tities. Using discourse and ontological information, 
the system determines if quantification is appropri- 
ate and if it is, which particular quantifier to use 
to minimize the anabiguity between distributive and 
collective readings. More importantly, when there 
are multiple quantifiers hi the same sentence, the al- 
gorithm generates different expressions for differen~ 
scope orderings. In this work, we focus on generat- 
ing referring quantified expressions for entities which 
have been mentioned before in the discourse or can 
be inferred from an ontology. There are quantified 
expressions that do not refer to particular entities 
in a domain or discourse, such as generics (i.e. "All 
whales are mammals."), or negatives (i.e., "The pa- 
tient has no allergies."). The synthesis of such quan- 
tifiers is currently performed in earlier stages.of the 
((TYPE EVENT) 
(PRED ((PRED receive) (ID idl))) 
(ARGi ((PRED patient) (ID ptl))) 
(ARG2 ((PRED aprotinin) (ID apl))) 
generation process. (MODS ((PRED after) (ID id2) 
. In the next section;we..vdll..~orapaxe ou_r~.approach . . . . .  . : .  . .  tTYRE_TIME).. ............... 
with previous work in the generation of quantified 
expressions. In Section 3, we will describe the appli- 
cation where the need for concise output motivated ) ) )  
our research in quantification. The algorithm for 
generating universal quantifiers is detailed in Sec- 
tion 4, including how the system handles ambiguity 
between distributive and collective readings. Sec- 
tion 5 describes how our algorithm generates en- 
tences with multiple quantifiers. 
2 Related Work 
Because a quantified expression refers to multiple 
entities in a domain, our work can be categorized as 
referring expression generation (Dale, 1992; Reiter 
and Dale, 1992; Horacek, 1997). Previous work in 
this area did not address the generation of quantified 
expressions directly. In this paper, we are interested 
in how to systematically derive quantifiers from in- 
put propositions, discourse history, and ontological 
information. Recent work on the generation ofquan- 
tifiers (Gailly, 1988; Creaney, 1996; Creaney, 1999) 
follows the analysis viewpoint, discussing scope ana- 
biguities extensively. Though our algorithm gener- 
ates different sentences for different scope orderings, 
we do not achieve this through scoping operations as 
they did. Creaney also discussed various imprecise 
quantifiers, such as some, at least ,  and at most. 
In regards to generating eneric quantified expres- 
sions, (Knott et al, 1997) has proposed an algorithm 
for generating defeasible, but informative descrip- 
tions for objects in nmseums. 
Other researchers (van Eijck and Alshawi, 1992; 
Copestake t al., 1999) proposed representations i  a 
machine translation setting which allow underspec- 
ification in regard to quantifier scope. Our work is 
different, in that we perform quantification directly 
on the instance-based representation btained from 
database tuples. Our input .does not have the in - . .  
formation about which entities are quantified as is 
the case in machine translation, where the quanti- 
tiers are already specified in the input from a source 
language. 
3 The  Application Domain 
We implemented our quantification algorithm as 
part of MAGIC (Dalai et al, 1996: McKeown et 
al., 1997). MAGIC automatically generates multi- 
media briefings to describe the post-operative sta- 
tus of a patient after undergoing Coronary Artery 
Bypass Graft, surgery. The system embodies a stan- 
. f . . . . . . .  
(ARG2 ((PRED critical-point) 
(NAME intubation) (IDcl))) 
Figure h The predicate-argument structure of 
"After intubation, a patient received aprotinin." 
dard text generation system architecture with three 
modules (Rambow and Korelsky, 1992): a content 
planner, a sentence planner, and a linguistic realizer. 
Once the bypass urgery is finished, information that 
is automatically collected during surgery such as 
blood pressure, heart rate, and medications given, 
is sent to a domain=specific medical inference mod- 
ule. Based on the medical inferences and schemas 
(McKeown, 1985), the content planner determines 
the information to convey and the order to convey 
it. 
The sentence planner takes a set of propositions 
(or predicate-argument structures) with rhetorical 
relations from the content planner and uses linguistic 
information to make decisions about how to convey 
the propositions fluently. Each proposition is repre- 
sented as a feature structure (Kaplan and Bresnan, 
1982; Kay, 1979) similar to the one shown in Fig- 
ure 1. The sentence planner's responsibilities include 
referring expression generation, clause aggregation, 
and lexical choice (Wanner and How, 1996). Then 
the aggregated predicate-argument structure is sent 
to FUF/SURGE (Elhadad and Robin, 1992), a lin- 
guistic realizer which t.ransforms the lexicalized se- 
inantic specification i to a string. The quantification 
algorithm is implemented in the sentence planner. 
4 Quantification Algorithm 
in this:,work, weprefergenerating expressions with 
universal quantifiers over conjunction because, as- 
suming that the users and the system have tile same 
domain model, the universally quantified expres- 
sions are more concise and they represent the same 
amount of information as the expression with con- 
joined entities. In contrast,, when given a conjunc- 
tion of entities and an expression with a cardinal 
quantifier, the system, by default, would use the 
conjunction if the conjoined entities can be distin- 
guished at the surface level. This is because once 
the system generates a cardinal quantifier when the 
universal quantification does not hold, such as "three 
101 
patients", it is impossible for the hearer to recover 
the identities of these patients based on the con- 
text. The default heuristics to prefer universal quan- 
tifier over conjunction over cardinal quantifier can 
be superseded by directives f romthe  contentplan- 
ner which are application specific. 
The input to our quaatif ica~omalgorit;hm is a set 
of predicate-argument structures after the referr ing 
expression module selected the properties to identify 
the entities (Dale, 1992; Dale and Reiter, 1995), but 
without carrying out the assignment of quantifiers. 
Our quantification algorithm first identifies the set 
of distinct entities which can be quantified in the 
input propositions. A generalization of the entities 
in the ontology is selected to potentially replace the 
references to these entities. If universal quantifica- 
tion is possible, then the replacement is made and 
the system must select which particular quantifier 
to use. In our system, we have six realizations for 
universal quantifiers: each, every,  a l l  1, both, the, 
and any, and two for existential quantifiers: the in- 
definite article, a/an, and cardinal n. 
4.1 Ident i fy  Themat ic  Ro les  wi th  Dist inct  
Ent i t ies  
Our algorithm identifies the roles containing distinct 
entities among the input propositions as candidates 
for universal and existential quantification. Suppose 
the system is given two propositions imilar to the 
one in Figure 1, "After intubation, Alice received 
aprotinin" and "After start of bypass, Alice received 
aprotinin", each with four roles - PRED, ARG1, 
ARG2, and MODS-TIME. By computing similarity 
anaong entities in the same role, the system deter- 
mines that the entities in ARG1, PRED, and ARG2 
are identical in each role, and only the entities in 
MODS-TIME are different. Based on this result, 
the distinct entities in MODS-TIME, "after intuba- 
tion" and "after start of bypass", are candidates for 
quantificat ion. 
4.2 Genera l i zat ion  and Quant i f icat ion 
We used the axioms in Figure 2 to determine if 
the distinct entities can be universally or existen- 
tially quantified. Though the axioms are similar to 
those used in Generalized Quantifier (Barwise and 
Cooper, 1981; Zwarts, 1983; de Swart, 1998). the 
semantics of set X and set D are different. In the 
previous step. the entities in set X have been iden- 
tified. To compute set D in Figure 2. we introduce 
a concept, Class-X. Class-X is a generalization of 
the distinct entities in set X. Quantification can re- 
place the distinct entities in the propositions with 
a reference to their type restricled by a quantifier. 
accessing discourse and ontological information .to 
provide a context. Our ontology is implemented in
l a l i  is rea l ized as "a l i  the" .  
102 
? bo th :  ID  - X \ [  = 0 and I x I  = 2, can have col- 
lective reading 
? every, a l l ,  the: ID-XI = 0 and IX\[ > 2, can 
have collective reading 
? each: I D - X\[ = 0 and IXI _> 2, only distribu- 
-- ? tive reading . . . . . . . .  
? any: \ ]D-  X\] = 0, when under the scope of 
negation 
? a /an :  IDnXl > 0 and Ixl = 1 
? n (cardinal): IOnXl > 0 and \[Xl = n 
Figure 2: Axioms of the quantifiers discussed in this 
paper. 
CLASSIC(Borgida et al, 1989) and is a subset of 
WordNet(Miller et al, 1990) and an online medical 
dictionary (Cimino et al, 1994) designed to support 
multiple applications across the medical institution. 
Given the entities in set X, queries in CLASSIC de- 
termine the class of each instance and its ancestors 
in the ontology. Based on this information, the gen- 
eralization algorithm identifies Class-X by comput- 
ing the most specific class which covers all the enti- 
ties. Earlier work (Passonneau et al, 1996) provided 
a framework for balancing specificity and verbosity 
in selecting appropriate concepts for generalization. 
However, given the precision needed in medical re- 
ports, our generalization procedure selects the most 
specific class. 
Set D represents the set of instances of Class-X in 
a context. Our system currently computes et D for 
three different contexts: 
e discourse: Previous references can provide an 
appropriate context for universal quantification. 
For example, if "Alice" and "Bob" were men- 
tioned in the previous entence, the system can 
refer t.o them as "both patients" in the current 
sentence. 
? domain ontology: The domain ontology pro- 
vides a closed world from which we can obtain 
't-he set D by matching all the instances of a 
concept in the knowledge base, such as "'ev- 
ery patient". In addition, certain concepts in 
the ontology have limited types. For example, 
knowing that cell savers, platelets and packed 
red blood cells are the only possible types of 
blood products in the ontology, the quantified 
expression "every blood product" can be used 
instead of referring to each entity. 
? domain knowledge: The possessor of the dis- 
tinct entities in a role might contain a maximum 
number of instances allowed for Class-X. For ex- 
ample, because a person has only two arms, the tinguishable xpressions at surface level. A more 
entities "the patient's left arm" and "the pa- developed pragmatic module is needed before quan- 
tient's right arm" can be referred to as "each tifiers such as some, raps'e, a t  leas t ,  and few, can 
arm". be systematically generated. Indiscriminate applica- 
tion of imprecise quantification can result in- vague 
The computation of set D can also involve interac- or inappropriate text in our domain, such as "The 
tions with a referring expression m0dule(Dale aad~ ~-:patient~rec~ived.~some 61ood~produetS:"'-v.ia-our~e~P - 
Reiter, 1995). For example, instead of the expres- plication, knowing exactly what blood products are 
sion "Alice and Bob" and "both patients" covered 
by the current algorithm, by interacting with a refer- 
ring expression module, the system might determine 
that "both CABG patients operated on this morn- 
ing by Dr. Rose" is a clearer expression to refer to 
the entities. Though this is desirable, we did not 
incorporate this capability into our system. 
Although the is often used to indicate a generic 
reference (i.e., "The lion is the king of jungle."), in 
English, the can also be used as an unmarked uni- 
versal quantifier when its head noun is plural, such 
as "the patients." Like the quantifier a l l ,  the can 
be both distributive and collective. However, the 
cannot always replace a l l  as a universal quantifier. 
the cannot be used when universal quantification is
based on the domain ontology. For example, it is 
not obvious that the quantified expression in "John 
received the blood products." refers to "each blood 
product" in the ontology. Although unmarked uni- 
versal quantifiers can be used to refer to body parts, 
as in "The lines include an IV in the arms.", the ex- 
pression is ambiguous between the distributive and 
collective readings. Of the three contexts discussed 
above, the system occationally generates the instead 
of every and both in a discourse context, yielding 
more natural output. 
When the computed set D matches et X exactly 
(ID - X I = 0), a quantified expression with either 
each, a l l ,  every, both, the, and any, replaces the 
entities in set X. 
4.3 Select ing a Par t i cu la r  Quant i f ie r  
In general, the universal quantification of a partic- 
ular type of entity, such as "every patient", refers 
to all such entities in a context. As a result, read- 
ers can recover what a universally quantified expres- 
sion refers to. In contrast, readers cannot pinpoint 
which entity has been refei'red to. in an existentially . 
quantified expression, such as "a patient." or "two 
patients". Because a universally quantified expres- 
sion preserves original semantics and is more con- 
cise than listing each entity, it is the focus of our 
quantificalion algorithm. The universal quantifiers 
hlaplemented in our system include the six possible 
realizations of V in English: every, a l l .  each. both. 
the, and any. The only existential quantifiers im- 
plemented in our system are the singular indefinite 
quantifier, a/an. and cardinal quantifiers, n. They 
are used in sentences with multiple quantifiers and 
when the entities being referred to do not have dis- 
used is very important. To avoid generating such 
inappropriate sentences, the system only performs 
generalization on the entities which can be univer- 
sally quantified. If the distinct entities cannot be 
universally quantified, the system will realize these 
entities using coordinated conjunction. 
Once the system decides that a universally quan- 
tified expression can be used to replace the entities 
in set X, it must select which universal quantifier. 
Because our sentence planner opportunistically com- 
bines distinct entries from separate database ntries 
for conciseness, it is not the case that these aggre- 
gated entities acted together (the collective read- 
ing). Given such input, the referring expression for 
aggregated entities should have only the distribu- 
tive reading 2. The universal quantifier, each, al- 
ways imposes a distributive reading when applied. 
In general, each requires a "matching" between the 
domain of the quantifier and the objects referred 
to(McCawley, 1981, pp. 37). In our algorithm, this 
matching process is exactly what happened, thus it 
is the default universal quantifier in our algorithm. 
Of course, indiscriminate use of each can result in 
awkward sounding text. For example, tile sentence 
"Every patient is awake" sounds more natural than 
"Each patient is awake." However, since quantified 
expressions with the universal quantifiers a l l  and 
every 3 can have collective readings (Vendler, 1967; 
McCawley, 1981), our system generates every and 
a l l  under two conditions when the collective read- 
ing is unlikely. First if the proposition is a state, as 
opposed to an event, we assume only the distribu- 
tive reading is possible 4. The quantifier every is 
used in "Ever.q patient tmd.taehycardia.'" because 
the proposition is a state proposition and contains 
the predicate has-attribute, an attributive relation. 
. . . . .  2For our  system to  generate noun-phrases.wivh ,col}eetive 
readings, the quantification process must be performed at the 
content planner level not in the clause aggregation module. 
3every is also distributive, but it stresses completeness or
rather, exhaustiveness(Vendler, 1967). The sentence "John 
took a picture of everyone in the room."  is ambiguous while 
"John took a picture os t each person in the room." is not. 
4There are cases where state propositions do have dis- 
teibuted readings (e.g., "Mountains urround the village." ). 
Sentences with collective readings are bandied earlier in the 
content planner and thus, this type of problem does not occur 
at this point in our system. Though .this observation seems to 
be true in our medical application, when implementing quan- 
tifiers in a new domain, we can limit this assumption to only 
the subset of state relations for which it holds. 
103 
Second, when the concept being universally quan- 
tified is marked as having a distributive reading in 
the lexicon, such as the concept episode, quantifiers 
every will be used instead of each. These quanti- 
tiers make the quantified sentences more natural be- 
cause they do not pick out the redundant distribu- 
tive meaning. . . . . . .  .~ . -: ....... =~:: .... ~" :~; 
The use of prepositions can also affect which quan- 
tifier to use. For example, "A f te r  all the episodes, 
the patient received obutamine" is ambiguous in re- 
gards to whether the dobutamine is given once dur- 
ing the surgery, or given after each episode. In con- 
trast, the sentence " In  all the episodes, the patient 
received dobutamine." does not have this problem. 
The current system looks at the particular preposi- 
tion (i.e., "before", "after", or "in") before selecting 
the appropriate quantifier. 
4.4 Examples of a Single Quantifier 
Given the four propositions, "After intubation, 
Mrs. Doe had tachycardia", "After skin incision, 
Mrs. Doe had tachycardia", "After start of bypass, 
Mrs. Doe had tachycardia',  and "After coming off 
bypass, Mrs. Doe had tachycardia.", the algorithm 
first identifies roles with similar entities, ARG1, 
PRED, ARG2 and removes them from further quan- 
tification processing while the distinct entities in the 
role MODS-TIME, "after intubation", "after skin in- 
cision", "after start of bypass", and "after coming off 
bypass", are further processed for universal quantifi- 
cation. The role MODS-TIME is further separated 
into two smaller roles, one role with the preposi- 
tions and the other role with different critical points. 
Since the prepositions are all the same, universal 
quantification is only applied to the distinct entities 
in set X, in this case, the four critical points. Queries 
to the CLASSIC ontology indicate that the enti- 
ties in set X, "intubation", "skin-incision", "start- 
of-bypass", and "conaing-off-bypass" match all the 
possible types of the concept c r i t i ca l -po in t ,  sat- 
isfying the domain ontology context in Section 4.2. 
Since set D and set X match exactly, generalization 
and universal quantification can be used to replace 
the references to these entities: "After each criti- 
cal point, Mrs. Doe had tachycardia." The system 
currently does not.perfor.m generMization omeJ~tities 
which failed the univeral quantification test.. In such 
cases, a sentence with conjunction will be generated, 
i.e., "After intubation and skin incision, Mrs. Doe 
had tachycardia." 
In addition to every,  the system generates both 
when the number of entities in set X is two. In 
our application, both is used as a universal quanti- 
tier under discourse context: "Alice had q)isodes of 
bradycardia b@)re inductio1~ and start of bypass, h~ 
both episodes, she received Cefazolin and Phen!lle- 
phrine. " 
When a universal quantifier is under the govern- 
104 
ment of negation, each, a l l ,  every  and both are in- 
appropriate, and any should be used instead. Given 
that the patient went on bypass without compli- 
cations, the system should generate "The patient 
went on bypass without any  problem." In contrast ,  
"The patient went on bypass without every  prob- 
/em.V=-~as-~ a,:differeut.-~meani~g; -,Our, :,system-cur=. 
rently uses any as a universal quantifier when the 
universal quantification is under the government of 
negation, such as "The patient denied any drug al- 
lergy.", or "Her hypertension was controlled without 
any medication." Currently, the generation of nega- 
tion sentences about surgery problems and allergies 
are handled in the content planner. They are not 
synthesized from multiple negation sentences: "The 
patient is not allergic to aspirin. The paitent is not 
allergic to penicillin..." 
5 Generation of Multiple Quantifiers 
When there are two distinct roles across the proposi- 
tions, the algorithm tries to use a universal quantifier 
for one role and an existential quantifier for another. 
To generate sentences with 33, both entities being 
referred to must have no proper names; this triggers 
the use of existential quantifiers. We intentionally 
ignore the cases where two universal quantifiers are 
generated in the same sentence. The likelihood for 
input specifying sentences with W to a text genera- 
tion system is slim. 
When generating multiple quantifiers in the same 
sentence, we differentiate between cases where there 
is or isn't a dependency between the two distinct 
roles. Two roles are independent of each other when 
one is not a modifier of the other. For example, 
the roles ARG1 and ARG2 in a proposition are in- 
dependent. In "Each patient is given a high sever- 
ity rating", performing universal quantification on 
the patients (ARG3) is a separate decision from 
the existential quantification of the severity ratings 
(ARG2). Similarly, in "An abnormal lab result was 
seen in each patient with hypertension after bypass". 
the quantification operations on the abnormal ab 
results and the patients can be performed indepen- 
dently. 
.... When there isa dependency 'between theroles be- 
ing quantified, the quantification process of each role 
might interact because modifiers restrict the range 
of the entities being modified. We found that when 
universal quantification occurs in the MODS role, 
the quantification of PRED and MODS can be per- 
formed independently, just as in the cases withou! 
dependency. Given the input propositions "Alice has 
I I<I in Alice's left arm. Alice has IV-2 in Alice's 
right arm. ", the distinct roles are ARG2 "IV-i" and 
"IV-T',  and ARG2-MODS "in Alice's left arm" and 
"in Alice's right arm". The ARG2-MODS is uni- 
versally quantified based on domain knowledge that 
? Roles without dependency, V Role-l,3 Role-2 
Each patient is given a high severity rating. 
? Roles without dependency, 3 Role-l, 'v' Role-2 
An abnormal lab result was seen in each patient 
geon's name is likely to be known, and the in-  
put is likely to be "Dr. Rose operated on Alice", 
"Dr~ Rose operated on Bob", and "Dr. Rose oper- 
ated on Chris". Given these three propositions, the 
entities in ARG1 and PRED are identical, and only 
with hypertension after bypass, the distinct entities in ARG2, "Alice", "Bob" and 
*Roles  with depend~flcy, V PRED,-3 MODS ............. Ghns:,~-~ d.~be:;~qua,atffied.... ~Wilih-,-am:a;ppropriate 
context, the sentence "Dr. Rose operated on each Every patient with a balloon pump had hyper- 
tension. 
? Roles with dependency, 3 PRED, V MODS 
Alice has an IV in each arm,. 
Figure 3: Sentences with two quantifiers 
a patient is a human and a human has a left arm 
and a right arm. In this example, "an IV in each 
arm", the decision to generate universal and exis- 
tential quantified expressions are independent. But 
in "Every patient with a balloon pump had hyperten- 
sion", the existentially quantified expression "with a 
balloon pump" is a restrictive modifier of its head. In 
this case, the set D does not include all the patients, 
but only the patients "with a balloon pump". When 
computing set D for universal quantification, the al- 
gorithm takes this extra restriction into account by 
eliminating all patients without such a restriction. 
Once a role is universally quantified and the other is 
existentially quantified, our algorithm replaces both 
roles with the corresponding quantified expressions. 
Figure 3 shows the sentences with multiple quanti- 
tiers generated by applying our algorithm. 
5.1 Ambiguity Revisited 
In Section 4.3, we described how to minimize the 
ambiguity between distributive and collective read- 
ings when generating universal quantitiers. What 
about the scope ambiguity when there are muhiple 
quantifiers in the same sentence? If we look at the 
roles which are being universally and existentially 
quantified in our examples in Figure 3, it is inter- 
esting to note that the universal quantifiers always 
have wider scope than the existential quantifiers. In 
the first, example, ,the.scope: order is Vpatient~high- 
severity-rating, the second example is Vpatient31ab- 
result, the third is Vpatient3balloon-pump, and the 
fourth is Varm3IV. The scope orderings are all V3. 
\Vhat happens if a sentence contains an existen- 
tial quantifier which has a wider scope than a uni- 
versal quantifier? In "A suryeon operated on each 
patient.", tile normal reading is Vpatienl3surgeon. 
13ut~ if the existentially quantified noun phrase 
"'a surgeon" refers to tile same surgeon, as in 
3surgeonVpatient. tlle system would generate "(A 
particular/The same) surgeon operated on each pa- 
tient." In an applied generation system, the sur- 
patient" will be generated. If the name of the sur- 
geon is not available but the identifiers for the sur- 
geon entities across the propositions are the same, 
the system will generate "The same surgeon oper- 
ated on each patient." As this example indicates, 
when 3 has a wider scope than V, the first step in 
our algorithm (described in Section 4.1), identify- 
ing roles with distinct entities, would eliminate the 
roles with identical entities from further quantifica- 
tion processing. Based on our algorithm, the sen- 
tences with 3V readings are taken care of by the first 
step, identifying roles with distinct entities, while V3 
cases are handled by quantification operations for 
multiple roles, as described in Section 5. 
In Section 4.3, we mentioned that it is important 
to know exactly what blood products are used in 
our application. As a result, the system would not 
generate the sentence "Each patient received a blood 
product." when the input propositions are "Alice re- 
ceived packed red blood cells. Bob received platelets. 
Chris received platelets." Even though tim conjoined 
entities can be generalized to "blood product", this 
quantification operation would violate our precondi- 
tion for using existential quantifiers: the descriptions 
for each of the conjoined entities must be indistin- 
guishable. Here, one is "red blood cells" and tile oth- 
ers are "platelets". Given these three propositions, 
the system would generate "Alice received packed 
red blood cells, and Bob and Chris, platelets." based 
on the algorithm described in (Shaw. 1998). If in 
our domain the input propositions could be "'Al- 
ice received blood-product-1. Bob received blood- 
product-2. Chris received blood-product-2.", where 
each instance of blood-product-n could be realized 
as "blood product", then the system would generate 
"Each patient received a blood product." since the 
description of conj0ined entities are not dist~inguish - 
able at the surface level. 
6 Conc lus ion  
We have described the quantification operators that 
can make the text more concise while preserving the 
original semantics in the input propositions. Though 
we would like to incorporate imprecise quantifiers 
such as few. many, some into our system because 
they have potential to drastically reduce the text. 
further, these quantifiers do not, have the desired 
property ill which the readers can recover the exact. 
entities in the input propositions. The property of 
105 
preserving the original semantics i very important 
since it guarantees that even though the surface x- 
pressions are modified, the information is preserved. 
This property allows the operators to be domain in- 
dependent and reusable in different natural language 
Norman Creaney. 1999. Generating quantified logi: 
cal forms from raw data. In Proe. of the ESSLLI- 
99 Workshop on the Generation of Nominal Ex- 
pressions. 
M. Dalal~ S. Feiner, K. McKeown, D. Jordan, 
generation systems. B. Allen, and Y. alSafadi. 1996. MAGIC: An 
We have described: an. algo_r.itlma :which.sy.stemati .............. e:~cpertimeeataL:aystem..for: genetattiag~ .multimedia 
cally derives quantifiers from input propositions, dis- 
course history and ontological information. We iden- 
tified three types of information from the discourse 
and ontology to determine if a universal quantifier 
can be applied. We also minimnized the ambiguity 
between distributive and collective readings by se- 
lecting an appropriate universal quantifier. Most 
importantly, for multiple quantifiers in the same sen- 
tence, we have shown how our algorithm generates 
different quantifed expressions for different scope or- 
derings. 
7 Acknowledgement  
We would like to thank anonymous reviewers for 
valuable comments. The research is supported in 
part by the National Library of Medicine under 
grant LM06593-02 and the Columbia University 
Center for Advanced Technology in High Perfor- 
mance Computing and Communications in Health- 
care (funded by the New York State Science and 
Technology Foundation). Any opinions, findings, or 
recommendations expressed in this paper are those 
of the authors and do not necessarily reflect the 
views of the above agencies. 
Re ferences  
Jon Barwise and Robin Cooper. 1981. Generalized 
quantifiers and natural anguage. Linguistics and 
Philosophy, 4:159-219. 
Alexander Borgida, Ronald Brachman, Deborah 
McGuinness, and Lori Alperin Resnick. 1989. 
CLASSIC: A structural data model for objects. 
In A CM SIGMOD International Conference on 
Management of Data. 
Bob Carpenter. 1997. Type-LogicaISemanties. MIT 
Press, Cambridge, Massachusetts. 
James J. Cimino, Paul D. Clayton, George Hripc- 
sak, and Stephen B. Johnson,: 1994. Knowledge-. 
based approaches to the maintenance of a large 
controlled medical terminology. The Journal of 
the American Medical lnformatics Association, 
1(1):35-50. 
Ann Copestake, Dan Flickinger, Ivan A. Sag. and 
Carl J. Pollard. 1999. Minimal recursion seman- 
tics: An introduction. Manuscript available via 
ht tp://lingo.stan ford.edu/pubs.ht ml.
Norman Creaney. 1996. An algorithm for generat- 
ing q~rantifiers. In Proc. of the 8th International 
Workshop on Natural Language Generation, Sus- 
sex, UK. 
briefings about post-bypass patient status. In 
Proc. 1996 AMIA Annual Fall Syrup, pages 684- 
688, Washington, DC, October 26-30. 
Robert Dale and Ehud Reiter. 1995. Computational 
interpretations of the gricean maxims in the gener- 
ation of referring expressions. Cognitive Science, 
19:233-263. 
Robert Dale. 1992. Generating Referring Expres- 
sions: Constructing Descriptions in a Domain of 
Objects and Processes. MIT Press, Cambridge, 
MA. 
Henriette de Swart. 1998. Introduction to Natural 
Language Semantics. CSLI Publications. 
Michael Elhadad and Jacques Robin. 1992. Con- 
trolling content realization with functional unifi- 
cation grammars. In Aspects of Automated Nat- 
ural Language Generation, Lecture Notes in Ar- 
tificial Intelligence, 587, pages 89-104. Springer- 
Verlag, Berlin, April. 
Pierre-Joseph Gailly. 1988. Expressing quantifier 
scope in French generation. In Proceedings of the 
12th International Conference on Computational 
Linguistics (COLING-88), volumne 1, pages 182- 
184, Budapest, August 22-27,. 
Barbara J. Grosz, Douglas E. Appelt, Paul A. 
Martin, and Fernando C. N. Pereira. 1987. 
TEAM: An experiment in the design of trans- 
portable natural-language interfaces. Artificial 
Intelligence, 32(2):173-243, May. 
Jerry Hobbs and Stuart Shieber. 1987. An algo- 
rithm for generating quantifier scopings. Compu- 
tational Linguistics, 13(1-2):47-63, January-June. 
Helmut Horacek. 1997. An algorithm for generating 
referential descriptions with flexible interfaces. In 
Proc. of the 35th ACL and 8th EACL, pages 206 
213. 
Ronald M. Kaplan and Joan Bresnan. 1982. 
..... ,-Lexical-functional,granmaar:, A formal system for 
grammatical representation. I  Joan Bresnan, ed- 
itor, The Mental Representation of Grammatical 
Relations, chapter 4. MIT Press. 
Martin Kay. 1979. Functional grammar. In Proceed- 
ings of the 5th Annual Meeting of the Berkeley 
Linguistic Society, pages 142-158, Berkeley, CA, 
February 17-19, 
Alistair Knott, Mick O'Donnell, Jon Oberlander. 
and Chris Mellish. 1997. Defeasible rules in con- 
tent selection and text structuring. In Proc. of 
the 6th European Workshop on Natural Language 
Generation, Duisburg, Germany. 
106 
James D. McCawley. 1981. Everything that linguists 
have always wanted to know about logic (but were 
ashamed to ask). University of Chicago Press. 
Kathleen MeKeown, Shimei Pan, James Shaw, 
Desmond Jordan, and Barry Allen. 1997. Lan- 
guage Engine, pages 11-38. MIT Press, Cam- 
bridge, MA. 
Zeno Vendler. 1967. Each and every, any and all. 
In Linguistics in Philosophy, pages 70-96. Cornell 
University Press, Ithaca and London. 
guage generation for multimedia healthcare brief- Leo Wanner and Eduard Hovy. 1996. The Health- 
ings. In Proc. of the Fifth:~'-AGl~: Cort\[,,~xm A:NL P, ~. -~?.:~. Doe~,.sentence, :planner.. qw:Proc~.:.of 4he~.Sth :.fnter- 
pages 277-282. 
Kathleen R. McKeown. 1985. Tezt Generation: Us- 
ing Discourse Strategies and Focus Constraints to 
Generate Natural Language Tezt. Cambridge Uni- 
versity Press, Cambridge. 
Chris Mellish, Mick O'Donnell, Jon Oberlander, and 
Alistair Knott. 1998. An architecture for oppor- 
tunistic text generation. In Proc. of the 9th Inter- 
national Workshop on Natural Language Genera- 
tion., pages 28-37. 
George Miller, Richard Beckwith, Christiane Fell- 
baum, Derek Gross, and Katherine Miller. 1990. 
Five papers on WordNet. CSL Report 43, Cogni- 
tive Science Laboratory, Princeton University. 
Douglas B. Moran and Fernando C. N. Pereira. 
1992. Quantifier scoping. In Hiyan Alshawi, ed- 
itor, The Core Language Engine, pages 149-172. 
MIT Press, Cambridge, MA. 
Jong C. Park. 1995. Quantifier scope and con- 
stituency. In Pvoc. of the 33rd ACL, pages 205- 
212. 
Rebecca Passonneau, Karen Kukich, Vasileios Hatzi- 
vassiloglou, Larry Lefkowitz, and Hongyan Jing. 
1996. Generating summaries of work flow di- 
agrams. In Proc. of the International Confer- 
ence on Natural Language Processing and Indus- 
trial Applications, pages 204-210, New Brunswick, 
Canada. University of Moncton. 
Fernando C. N. Pereira. 1990. Categorial semantics 
and scoping. Computational Linguistics, 16( 1): 1- 
10. 
Owen Rainbow and Tanya Korelsky. 1992. Applied 
text generation. In Proceedings of the Third A CL 
Conference on Applied Natural Language Process- 
ing, pages 40-47, Trento, Italy. 
Ehud Reiter and Robert Dale. 1992. A fast algo- 
rithm for the generation of referring expressions. 
In Proceedings of the I4th International Con- 
ference on Computational Linguistics (COLING- 
92), pages 232-238, Nantes, France. 
Jacques Robin. t995. Revision-Based Generation of 
Natural Language Sum maries Providing Historical 
Background. Ph.D. thesis, Columbia University. 
James Shaw. 1998. Segregatory coordination and el- 
lipsis in text generation, tn Proc. of the 17th COL- 
I.'VG and the 36th .4m~ual Meeting of the ACL.. 
pages 1220-1226. 
Jan van Eijck and Hiyan Alshawi. 1992. Logical 
forms. In Hiyan Alshawi, editor, The Core Lan- 
national Workshop on Natural Language Genera- 
tion, pages 1-10, Sussex, UK. 
William A. Woods. 1978. Semantics and quantifi- 
cation in natural language question answering. In 
Advances in Computers, volume 17, pages 1-87. 
Academic Press. 
Frans Zwarts. 1983. Determiners: a relational 
perspective. In A. ter Meulen, editor, Studies 
in model-theoretic semantics, pages 37-62. Dor- 
drecht: Forts. 
107 
  		
	
 	