A Framework for MT and Multilingual NLG Systems Based on 
Uniform Lexico-Structural Processing 
Benoit Lavoie 
CoGenTex, Inc. 
840 Hanshaw Road 
Ithaca, NY 
USA, 14850 
benoit@cogentex.com 
Richard Kittredge 
CoGenTex, Inc. 
840 Hanshaw Road 
Ithaca, NY 
USA, 14850 
richard @ cogentex.com 
Tanya Korelsky 
CoGenTex, Inc. 
840 Hanshaw Road 
Ithaca, NY 
USA, 14850 
tanya @ cogentex.com 
Owen Rambow *
ATT Labs-Research, B233 
180 Park Ave, PO Box 971 
Florham Park, NJ 
USA, 07932 
rambow @research.att.com 
Abstract 
In this paper we describe an implemented 
framework for developing monolingual or 
multilingual natural language generation 
(NLG) applications and machine translation 
(MT) applications. The framework 
demonstrates a uniform approach to 
generation and transfer based on declarative 
lexico-structural transformations of 
dependency structures of syntactic or 
conceptual levels ("uniform lexico-structural 
processing"). We describe how this 
framework has been used in practical NLG 
and MT applications, and report he lessons 
learned. 
1 Introduction 
In this paper we present a linguistically 
motivated framework for uniform lexico- 
structural processing. It has been used for 
transformations of conceptual and syntactic 
structures during generation i monolingual nd 
multilingual natural language generation (NLG) 
and for transfer in machine translation (MT). 
Our work extends directions taken in systems 
such as Ariane (Vauquois and Boitet, 1985), 
FoG (Kittredge and Polgu6re, 1991), JOYCE 
(Rainbow and Korelsky, 1992), and LFS 
(Iordanskaja et al, 1992). Although it adopts 
the general principles found in the above- 
mentioned systems, the approach presented in 
this paper is more practical, and we believe, 
would eventually integrate better with emerging 
statistics-based approaches toMT. 
* The work performed on the framework by this co- 
author was done while at CoGenTex, Inc. 
The framework consists of a portable Java 
environment for building NLG or MT 
applications by defining modules using a core 
tree transduction engine and single declarative 
ASCII specification language for conceptual or 
syntactic dependency tree structures 1 and their 
transformations. Developers can define new 
modules, add or remove modules, or modify 
their connections. Because the processing of the 
transformation engine is restricted to 
transduction of trees, it is computationally 
efficient. 
Having declarative rules facilitates their reuse 
when migrating from one programming 
environment toanother; if the rules are based on 
functions pecific to a programming language, 
the implementation f these functions might no 
longer be available in a different environment. 
In addition, having all lexical information and 
all rules represented eclaratively makes it 
relatively easy to integrate into the framework 
techniques for generating some of the rules 
automatically, for example using corpus-based 
methods. The declarative form of 
transformations makes it easier to process them, 
compare them, and cluster them to achieve 
proper classification and ordering. 
1 In this paper, we use the term syntactic dependency 
(tree) structure as defined in the Meaning-Text 
Theory (MTT; Mel'cuk, 1988). However, we 
extrapolate from this theory when we use the term 
conceptual dependency (tree) structure, which has no 
equivalent in MTT (and is unrelated to Shank's CD 
structures proposed inthe 1970s). 
60 
Thus, the framework represents a generalized 
processing environment that can be reused in 
different ypes of natural language processing 
(NLP) applications. So far the framework has 
been used successfully to build a wide variety of 
NLG and MT applications in several limited 
domains (meteorology, battlefield messages, 
object modeling) and for different languages 
(English, French, Arabic, and Korean). 
In the next sections, we present the design of the 
core tree transduction module (Section 2), 
describe the representations that it uses (Section 
3) and the linguistic resources (Section 4). We 
then discuss the processing performed by the 
tree transduction module (Section 5) and its 
instantiation for different applications (Section 
6). Finally, we discuss lessons learned from 
developing and using the framework (Section 7) 
and describe the history of the framework 
comparing it to other systems (Section 8). 
2 The Framework's Tree Transduction Module 
The core processing engine of the framework is 
a generic tree transduction module for lexico- 
structural processing, shown in Figure 1. The 
module has dependency stuctures as input and 
output, expressed in the same tree formalism, 
although not necessarily at the same level (see 
Section 3). This design facilitates the pipelining 
of modules for stratificational transformation. I  
fact, in an application, there are usually several 
instantiations of this module. 
The transduction module consists of three 
processing steps: lexico-structural pre- 
processing, main lexico-structural processing, 
and lexico-structural post-processing. Each of 
these steps is driven by a separate grammar, and 
all three steps draw on a common feature data 
base and lexicon. The grammars, the lexicon 
and the feature data base are referred to as the 
linguistic resources (even if they sometimes 
apply to a conceptual representation). All 
linguistic resources are represented in a 
declarative manner. An instantiation of the tree 
transduction module consists of a specification 
of the linguistic resources. 
Input Dependency Structure 
~ L exlco-Structural Preproce~ing 
Intermediate Dependency StructttreL_~ 
Lexico-Structm'al Processing 
Intermediate + Dependency Structure 
~ Lexico-Structural 
Postprocessing 
Output / /~  
Dependency SUucturc 
i 
Figure 1: Design of the Tree Transduction Module 
3 The Framework's Representations 
The representations used by all instantiations of
the tree transduction module in the framework 
are dependency tree structures. The main 
characteristics of all the dependency tree 
structures are: 
? A dependency tree is unordered (in contrast 
with phrase structure trees, there is no 
ordering between the branches of the tree). 
? All the nodes in the tree correspond to 
lexemes (i.e., lexical heads) or concepts 
depending on the level of representation. I  
contrast with a phrase structure 
representation, there are no phrase-structure 
nodes labeled with nonterminal symbols. 
Labelled arcs indicate the dependency 
relationships between the lexemes. 
The first of these characteristics makes a 
dependency tree structure a very useful 
representation for MT and multilingual NLG, 
since it gives linguists a representation that 
allows them to abstract over numerous cross- 
linguistic divergences due to language specific 
ordering (Polgu~re, 1991). 
We have implemented 4 different types of 
dependency tree structures that can be used for 
NLG, MT or both: 
? Deep-syntactic structures (DSyntSs); 
? Surface syntactic structures (SSyntSs); 
61 
? Conceptual structures (ConcSs); 
? Parsed syntactic structures (PSyntSs). 
The DSyntSs and SSyntSs correspond closely to 
the equivalent structures of the Meaning-Text 
Theory (MTT; Mel'cuk, 1988): both structures 
are unordered syntactic representations, but a 
DSyntS only includes full meaning-bearing 
lexemes while a SSyntS also contains function 
words such as determiners, auxiliaries, and 
strongly governed prepositions. In the 
implemented applications, the DSyntSs are the 
pivotal representations involved in most 
transformations, as this is also often the case in 
practice in linguistic-based MT (Hutchins and 
Somers, 1997). Figure 2 illustrates a DSyntS 
from a meteorological application, MeteoCogent 
(Kittredge and Lavoie, 1998), represented using 
the standard graphical notation and also the 
RealPro ASCII notation used internally in the 
framework (Lavoie and Rambow, 1997). As 
Figure 2 illustrates, there is a straightforward 
mapping between the graphical notation and the 
ASCII notation supported in the framework. 
This also applies for all the transformation rules 
in the framework which illustrates the 
declarative nature of our approach, 
I 1 
LOW 
-5 TO 
't 
LOw 
( 
A'I~R -5 
ATTR TO 
( 
il HIGH 
( 
A'\]I~R 20 
) 
) 
) 
Low -S to high 20 
Figure 2: DSyntS (Graphical nd ASCII Notation) 
The ConcSs correspond to the standard frame- 
like structures used in knowledge representation, 
with labeled arcs corresponding to slots. We 
have used them only for a very limited 
meteorological domain (in MeteoCogent), and 
we imagine that they will typically be defined in 
a domain-specific manner. 
Figure 3 illustrates the mapping between an 
interlingua defined as a ConcS and a 
corresponding English DSyntS. This example, 
also taken from MeteoCogent, illustrates that the 
conceptual interlingua in NLG can be closer to a 
database representation f domain data than to 
its linguistic representations. 
As mentioned in (Polgu~re, 1991), the high level 
of abstraction of the ConcSs makes them a 
suitable interlingua for multilingual NLG since 
they bridge the semantic discrepancies between 
languages, and they can be produced easily from 
the domain data. However, most off-the-shelf 
parsers available for MT produce only syntactic 
structures, thus the DSyntS level is often more 
suitable for transfer. 
Cones  
#TEMPERATURE 
Low -5 to Mlgh 20 
DS~tS  
LOW 
-5 TO 
ItlGH 
Figure 3: ConcS Interlingua nd English DSyntS 
Finally, the PSyntSs correspond to the parser 
outputs represented using RealPro's dependency 
structure formalism. The PSyntSs may not be 
valid directly for realization or transfer since 
they may contain unsupported features or 
dependency relations. However, the PSyntSs 
are represented in a way to allow the framework 
to convert hem into valid DSyntS via lexico- 
structural processing. This conversion is done 
via conversion grammars customized for each 
parser. There is a practical need to convert one 
syntactic formalism to another and so far we 
have implemented converters for three off-the- 
shelf parsers (Palmer et al, 1998). 
4 The Framework's Linguistic Resources 
As mentioned previously, the framework is 
composed of instantiations of the tree 
62 
transduction module shown in Figure 1. Each 
module has the following resources: 
? Feature Data-Base: This consists of the 
feature system defining available features 
and their possible values in the module. 
? Lexicon: This consists of the available 
lexemes or concepts, depending on whether 
the module works at syntactic or conceptual 
level. Each lexeme and concept is defined 
with its features, and may contain specific 
lexico-structural ules: transfer rules for MT, 
mapping rules to the next level of 
representation for surface realization of 
DSyntS or lexicalization of ConcS. 
? Main Grammar: This consists of the lexico- 
structural mapping rules that apply at this 
level and which are not lexeme- or concept- 
specific (e.g. DSynt-rules for the DSynt- 
module, Transfer-rules for the Transfer 
module, etc.) 
? Preprocessing rammar: This consists of 
the lexico-structural mapping rules for 
transforming the input structures in order to 
make them compliant with the main 
grammar, if this is necessary. Such rules are 
used to integrate new modules together 
when discrepancies in the formalism need to 
be fixed. This grammar can also be used 
for adding default features (e.g. setting the 
default number of nouns to singular) or for 
applying default transformations (e.g. 
replacing non meaning-bearing lexemes 
with features). 
Postprocessing rammar: This consists of 
lexico-structural mapping rules for 
transforming the output structures before 
they can be processed by the next module. 
As for the preprocessing rules, these rules 
can be used to fix some discrepancies 
between modules. 
Our representation f the lexicon at the lexical 
level (as opposed to conceptual) is similar to the 
one found in RealPro. Figure 4 shows a 
specification for the lexeme SELL. This lexeme 
is defined as a verb of regular morphology with 
two lexical-structural mappings, the first one 
introducing the preposition TO for its 3 r? actant, 
and the preposition FOR for its 4 th actant: (a 
seller) X1 sells (merchandise) X2 to (a buyer) 
X3 for (a price) X4. What is important is that 
each mapping specifies a transformation 
between structures at different levels of 
representation but that are represented in one 
and the same representation formalism (DSyntS 
and SSyntS in this case). As we will see 
below, grammar ules are also expressed in a 
similar way. 
LEX~ME: SELL 
CATEGORY:  verb  
FEATURES:  \[ \] 
GOV-PATTERN: \ [  
DSYNT-RULE:  
SELL ( I I I  $X3 ) 
<- -> 
SELL 
( complet ive2  TO 
( p repos i t iona l  $X3 ) ) 
DSYNT-RULE : 
SELL ( IV $X4 ) 
<- -> 
SELL 
( complet ive3  FOR 
( p repos i t iona l  $X4 ) 
\] 
MORPHOLOGY:  \[ 
( \[ tense :past  \] so ld  \[ inv 
( \[ mood:past -par t  \] so ld  \[ inv 
( \[ \] sel l  \[ reg 
\] 
Figure 4: Specification ofLexeme SELL 
At the conceptual level, the conceptual lexicon 
associates lexical-structural mapping with 
concepts in a similar way. Figure 5 illustrates 
the mapping at the deep-syntactic level 
associated with the concept #TEMPERATURE. 
Except for the slight differences in the labelling, 
this type of specification is similar to the one 
used on the lexical level. The first mapping rule 
corresponds to one of the lexico-structural 
transformations u ed to convert he interlingual 
ConcS of Figure 3 to the corresponding DSyntS. 
ZONCEPT:  #TEMPERATURE 
5EXICAL:  \[ 
L~-RULE:  
#TEMPERATURE ( #min imum SX 
#maxim~ $Y 
<- -> 
LOW ( ATTR $X 
ATTR TO 
( II H IGH 
( ATTR SY ) ) ) 
LEX-RULE:  
#TEMPERATURE ( #min im~ SX 
<- -> 
LOW ( ATTR $X ) 
LEX-RULE:  
#TEMPE~TURE ( #max imum $X 
<- -> 
H IGH ( ATTR SX ) 
\] 
Figure 5: Specification ofConcept #TEMPERATURE 
63 
Note that since each lexicon entry can have 
more than one lexical-structural mapping rule, 
the list of these rules represents a small grammar 
specific to this lexeme or concept. 
Realization grammar ules of the main grammar 
include generic mapping rules (which are not 
lexeme-specific) such as the DSyntS-rule 
illustrated in Figure 6, for inserting a determiner. 
DSYNT-RULE:  
$X  \[ c lass :noun ar t i c le :de f  \] 
$X  ( determinat ive  THE ) 
Figure 6: Deep-Syntactic Rule for Determiner Insertion 
The lexicon formalism has also been extended to 
implement lexeme-specific lexico-structural 
transfer rules. Figure 7 shows the lexico- 
structural transfer of the English verb lexeme 
MOVE to French implemented for a military 
and weather domain (Nasr et al, 1998): 
Cloud will move into the western regions. 
Des nuages envahiront les rdgions ouest. 
They moved the assets forward. 
-.9 lls ont amen~ les ressources vers l 'avant. 
The 79 dcg moves forward. 
---~ La 79 dcg avance  vers l'avant. 
A disturbance will move north of Lake Superior. 
--~ Une perturbation se diplacera au nord du lac 
supdrieur. 
LEXEME : MO~'E 
CATEGORY : verb 
FEATORES : \[ \] 
TRANSFER: \[ 
TRANSFER-RULE: 
MOVE 
I ATTR INTO \ [ c lass :prepos i t ion \ ]  
( II SXl ) ) 
.-.> 
E2~VAH IR \[class:verb\] 
( II SX1 ) 
TRANSFER-RULE : 
MOVE 
( II $X2 ) 
AMENER \[class:verb\] 
\[ II $X2 ) 
TRANSFER-RULE: 
MOVE 
( ATTR SX \[Iexe~e:FORWARD class:adverb\] ) 
AVANCER 
( ATTR SX ) 
TRANSFER-RULE : 
MOVE 
<--> 
DEPLACER \[class:verb refl:?\] 
\] 
Figure 7: Lexico-Structural Transfer of English Lexerne 
MOVE to French 
More general exico-structural rules for transfer 
can also be implemented using our grammar rule 
formalism. Figure 8 gives an English-French 
transfer ule applied to a weather domain for the 
transfer of a verb modified by the adverb 
ALMOST: 
It almost rained. 
--o II a fail l i  pleuvoir. 
TRANSFER-RULE:  
SX  \[ c lass :verb  \] 
( ATTR ALMOST ) 
<- -> 
FA ILL IR  \[ c lass :verb  \] 
( I I  SX  \[ mood: in f  \] ) 
Figure 8: English to French Lexico-Structural 
Transfer Rule with Verb Modifier ALMOST 
More details on how the structural divergences 
described in (Dorr, 1994) can be accounted for 
using our formalism can be found in (Nasr et 
al., 1998). 
5 The Rule Processing 
Before being processed, the rules are first 
compiled and indexed for optimisation. Each 
module applies the following processing. 
The rules are assumed to be ordered from most 
specific to least specific. The application of the 
rules to the structures i  top-down in a recursive 
way from the f'n-st rule to the last. For the main 
grammar, before applying a grammar ule to a 
given node, dictionary lookup is carried out in 
order to first apply the lexeme- or concept- 
specific rules associated with this node. These 
are also assumed to be ordered from the most 
specific to the least specific. 
If a lexico-structural transformation involves 
switching a governor node with one of its 
dependents in the tree, the process is reapplied 
with the new node governor. When no more 
rules can be applied, the same process is applied 
to each dependent of the current governor. 
When all nodes have been processed, the 
processing is completed, 
6 Using the Framework to build Applications 
Figure 9 shows how different instantiations of 
the tree transduction module can be combined to 
64 
build NLP applications. The diagram does not 
represent a particular system, but rather shows 
the kind of transformations that have been 
implemented using the framework, and how they 
interact. Each arrow represents one type of 
processing implemented by an instantiation of 
the tree transduction module. Each triangle 
represents a different level of representation. 
Scope of the 
Framework 
~Conversion bl 
Parsed 
PSyntS LI 
Parsing 
Sentence 
PI "ng 
C'?nezoa~ 1 
~ e Transfer ~_~ , Co.verMon 
D$ ntS LI 
~SyntS ~ealizalion 
/ \  
SSyntS LI SSyntS 1.2 ~ yntS ealization 
A DSyntS L2 Parsed DSym51 PSyntS L2 
Realiza~o~ 
SSym~ Realizatio parsin 
Input Generated Generated Input 
Sentence LI Sentence LI Sentence 1.2 Sentence L2. 
I concS Concepmd suar.tm~ SSyntS Suffaee:Syntnetlc su'uet~'e 
os~ts t~sy~ac~ Psy~s ~d:~n~c 
Figure 9: Scope of the Framework's Transformations 
For example, in Figure 9, starting with the 
"Input Sentence LI" and passing through 
Parsing, Conversion, Transfer, DSyntS 
Realization and SSyntS Realization to 
"Generated Sentence L2" we obtain an Ll-to-L2 
MT system. Starting with "Sentence Planning" 
and passing through DSyntS Realization, and 
SSyntS Realization (including linearization and 
inflection) to "Generated Sentence LI", we 
obtain a monolingual NLG system for L1. 
So far the framework has been used successfully 
for building a wide variety of applications in 
different domains and for different languages: 
NLG: 
? Realization of English DSyntSs via SSyntS 
level for the domains of meteorology 
(MeteoCogent; Kittredge and Lavoie, 1998) 
and object modeling (ModelExplainer; 
Lavoie et al, 1997). 
? Generation of English text from conceptual 
interlingua for the meteorology domain 
(MeteoCogent). (The design of the 
interlingua can also support he generation 
of French but this functionality has not yet 
been implemented.) 
MT: 
? Transfer on the DSyntS level and realization 
via SSyntS level for English--French, 
English--Arabic, English---Korean and 
Korean--English. Translation in the 
meteorology and battlefield omains (Nasr 
et al, 1998). 
? Conversion of the output structures from 
off-the-shelf English, French and Korean 
parsers to DSyntS level before their 
processing by the other components in the 
framework (Palmer et al, 1998). 
7 Lessons Learned Using the Framework 
Empirical results obtained from the applications 
listed in Section 6 have shown that the approach 
used in the framework is flexible enough and 
easily portable to new domains, new languages, 
and new applications. Moreover, the time spent 
for development was relatively short compared 
to that formerly required in developing similar 
types of applications. Finally, as intended, the 
limited computational power of the transduction 
module, as well as careful implementation, 
including the compilation of declarative 
linguistic knowledge to Java, have ensured 
efficient run-time behavior. For example, in the 
MT domain we did not originally plan for a 
separate conversion step from the parser output 
to DSyntS. However, it quickly became apparent 
that there was a considerable gap between the 
output of the parsers we were using and the 
DSyntS representation that was required, and 
furthermore, that we could use the tree 
transduction module to quickly bridge this gap. 
Nevertheless, our tree transduction-based 
approach has some important limitations. In 
particular, the framework requires the developer 
of the transformation rules to maintain them and 
specify the order in which the rules must be 
applied. For a small or a stable grammar, this 
does not pose a problem. However, for large or 
rapidly changing grammar (such as a transfer 
grammar in MT that may need to be adjusted 
when switching from one parser to another), the 
65 
burden of the developer's task may be quite 
heavy. In practice, a considerable amount of 
time can be spent in testing a grammar after its 
revision. 
Another major problem is related to the 
maintenance of both the grammar and the 
lexicon. On several occasions during the 
development of these resources, the developer in 
charge of adding lexical and grammatical data 
must make some decisions that are domain 
specific. For example, in MT, writing transfer 
rules for terms that can have several meanings or 
uses, they may simplify the problem by 
choosing a solution based on the context found 
in the current corpus, which is a perfectly natural 
strategy. However, later, when porting the 
transfer esources to other domains, the chosen 
strategy may need to be revised because the 
context has changed, and other meanings or uses 
are found in the new corpora. Because the 
current approach is based on handcrafted rules, 
maintenance problems of this sort cannot be 
avoided when porting the resources to new 
domains. 
An approach such as the one described in (Nasr 
et al, 1998; and Palmer and al., 1998) seems to 
be solving a part of the problem when it uses 
corpus analysis techniques for automatically 
creating a first draft of the lexical transfer 
dictionary using statistical methods. However, 
the remaining work is still based on handcrafting 
because the developer must refine the rules 
manually. The current framework offers no 
support for merging handcrafted rules with new 
lexical rules obtained statistically while 
preserving the valid handcrafted changes and 
deleting the invalid ones. In general, a better 
integration of linguistically based and statistical 
methods during all the development phases is 
greatly needed. 
8 History of the Framework and Comparison 
with Other Systems 
The framework represents a generalization of 
several predecessor NLG systems based on 
Meaning-Text Theory: FoG (Kittredge and 
Polgu~re, 1991), LFS (Iordanskaja et al, 1992), 
and JOYCE (Rambow and Korelsky, 1992). 
The framework was originally developed for the 
realization of deep-syntactic structures in NLG 
(Lavoie and Rambow, 1997). It was later 
extended for generation of deep-syntactic 
structures from conceptual interlingua (Kittredge 
and Lavoie, 1998). Finally, it was applied to 
MT for transfer between deep-syntactic 
structures of different languages (Palmer et al, 
1998). The current framework encompasses the 
full spectrum of such transformations, i.e. from 
the processing of conceptual structures to the 
processing of deep-syntactic structures, either 
for NLG or MT. 
Compared to its predecessors (Fog, LFS, 
JOYCE), our approach as obvious advantages 
in uniformity, declarativity and portability. The 
framework has been used in a wider variety of 
domains, for more languages, and for more 
applications (NLG as well as MT). The 
framework uses the same engine for all the 
transformations at all levels because all the 
syntactic and conceptual structures are 
represented asdependency tree structures. 
In contrast, the predecessor systems were not 
designed to be rapidly portable. These systems 
used programming languages or scripts for the 
implementation f the transformation rules, and 
used different ypes of processing at different 
levels of representation. For instance, in LFS 
conceptual structures were represented as 
graphs, whereas syntactic structures were 
represented as trees which required different 
types of processing at these two levels. 
Our approach also has some disadvantages 
compared with the systems mentioned above. 
Our lexico-structural transformations are far 
less powerful than those expressible using an 
arbitrary programming language. In practice, 
the formalism that we are using for expressing 
the transformations is inadequate for long-range 
phenomena (inter-sentential or intra-sentential), 
including syntactic phenomena such as long- 
distance wh-movement and discourse 
phenomena such as anaphora nd ellipsis. The 
formalism could be extended to handle intra- 
sentential syntactic effects, but inter-sentential 
discourse phenomena probably require 
procedural rules in order to access lexemes in 
66
other sentences. In fact, LFS and JOYCE 
include a specific module for elliptical structure 
processing. 
Similarly, the limited power of the tree 
transformation rule formalism distinguishes the 
framework from other NLP frameworks based 
on more general processing paradigms uch as 
unification of FUF/SURGE in the generation 
domain (Elhadad and Robin, 1992). 
9 Status 
The framework is currently being improved in 
order to use XML-based specifications for 
representing the dependency structures and the 
transformation rules in order to offer a more 
standard development environment and to 
facilitate the framework extension and 
maintenance. 
Acknowledgements 
A first implementation of the framework (C++ 
processor and ASCII formalism for expressing 
the lexico-structural transformation rules) 
applied to NLG was developed under SBIR 
F30602-92-C-0015 awarded by USAF Rome 
Laboratory. The extensions to MT were 
developed under SBIR DAAL01-97-C-0016 
awarded by the Army Research Laboratory. The 
Java implementation and general improvements 
of the framework were developed under SBIR 
DAAD17-99-C-0008 awarded by the Army 
Research Laboratory. We are thankful to Ted 
Caldwell, Daryl McCullough, Alexis Nasr and 
Mike White for their comments and criticism on 
the work reported in this paper. 
References 
Dorr, B. J. (1994) Machine translation divergences: 
A formal description and proposed solution. In 
Computational Linguistics, vol. 20, no. 4, pp. 597- 
635. 
Elhadad, M. and Robin, J. (1992) Controlling 
Content Realization with Functional Unification 
Grammars. In Aspects of Automated Natural 
Language Generation, Dale, R., Hovy, E., Rosner, 
D. and Stock, O. Eds., Springer Verlag, pp. 89- 
104. 
Hutchins, W. J. and Somers, H. L. (1997) An 
Introduction to Machine Translation. Academic 
Press, second edition. 
Iordanskaja, L., Kim, M., Kittredge, R., Lavoie, B. 
and Polgu6re, A. (1992) Generation of Extended 
Bilingual Statistical Reports. In Proceedings of the 
15th International Conference on Computational 
Linguistics, Nantes, France, pp. 1019-1023. 
Kittredge, R. and Lavoie, B. (1998) MeteoCogent: A
Knowledge-Based Tool For Generating Weather 
Forecast Texts. In Proceedings of the American 
Meteorological Society AI Conference (AMS-98), 
Phoenix, Arizona, pp. 80--83. 
Kittredge, R. and Polgu~re, A. (1991) Dependency 
Grammars for Bilingual Text Generation: Inside 
FoG's Stratificational Models. In Proceedings of 
the International Conference on Current Issues in 
Computational Linguistics, Penang, Malaysia, pp. 
318-330. 
Lavoie, B. (1995) Interlingua for Bilingual Statistical 
Reports. In Notes of IJCAI-95 Workshop on 
Multilingual Text Generation, Montr6al, Canada, 
pp. 84---94. 
Lavoie, B. and Rambow, O. (1997) A Fast and 
Portable Realizer for Text Generation Systems. In 
Proceedings of the Fifth Conference on Applied 
Natural Language Processing, Washington, DC., 
pp. 265-268. 
Lavoie, B., Rambow, O. and Reiter, E. (1997) 
Customizable Descriptions of Object-Oriented 
Models. In Proceedings of the Fifth Conference on 
Applied Natural Language Processing, 
Washington, DC., pp. 253-256. 
Mel'cuk, I. (1988) Dependency Syntax. State 
University of New York Press, Albany, NY. 
Nasr, A., Rambow, O., Palmer, M. and Rosenzweig, 
J. (1998) Enriching lexical transfer with cross- 
linguistic semantic features. In Proceedings of the 
Interlingua Workshop at the MT Summit, San 
Diego, California. 
Palmer, M., Rambow, O. and Nasr, A. (1998) Rapid 
Prototyping of Domain-Specific Machine 
Translation Systems. In Proceedings of the Third 
Conference on Machine Translation in the 
Americas (AMTA-98), PA, USA, pp. 95-102. 
Polgu6re, A. (1991) Everything has not been said 
about interlinguae: the case of multi-lingual text 
generation system. In Proc. of Natural Language 
Processing Pacific Rim Symposium, Singapore. 
Rambow, O. and Korelsky, T. (1992) Applied Text 
Generation. In Proceedings of the 6th International 
Workshop on Natural Language Generation, 
Trento, Italy, pp. 40--47. 
Vauquois, B. and Boitet C. (1985) Automated 
translation at Grenoble University. In 
Computational Linguistics, Vol. 11, pp. 28-36. 
67 
Towards Translingual Information Access 
using Portable Information Extraction 
Michael White, Claire Cardie, Chung-hye Han, Nari Kim, # 
Benoit Lavoie, Martha Palmer, Owen Rainbow,* Juntae Yoon 
CoGenTex, Inc. 
Ithaca, NY, USA 
\[mike,benoit.owen\] 
@cogentex.com 
Institute for Research in 
Cognitive Science 
University of Pennsylvania 
Philadelphia, PA, USA 
chunghye@babel, ling. upenn, edu 
\[ nari, mpalmer, j tyoon } 
@linc. cis.upenn.edu 
Dept. of Computer Science 
Cornell University 
Ithaca, NY, USA 
cardie@cs, cornell, edu 
Abstract 
We report on a small study undertaken to 
demonstrate the feasibility of combining 
portable information extraction with MT in 
order to support translingual information 
access. After describing the proposed 
system's usage scenario and system design, 
we describe our investigation of transferring 
information extraction techniques developed 
for English to Korean. We conclude with a 
brief discussion of related MT issues we plan 
to investigate in future work. 
1 Introduction 
In this paper, we report on a small study 
undertaken to demonstrate the feasibility of 
combining portable information extraction with 
MT in order to support ranslingual information 
access. The goal of our proposed system is to 
better enable analysts to perform information 
filtering tasks on foreign language documents. 
This effort was funded by a SBIR Phase I award 
from the U.S. Army Research Lab, and will be 
pursued further under the DARPA TIDES 
initiative. 
Information extraction (IE) systems are 
designed to extract specific types of information 
from natural language texts. In order to achieve 
acceptable accuracy, IE systems need to be 
tuned for a given topic domain. Since this 
domain tuning can be labor intensive, recent IE 
research has focused on developing learning 
algorithms for training IE system components 
(cf. Cardie, 1997, for a survey). To date, 
however, little work has been done on IE 
systems for languages other than English 
(though cf. MUC-5, 1994, and MUC-7, 1998, 
for Japanese IE systems); and, to our knowledge, 
none of the available techniques for the core task 
of learning information extraction patterns have 
been extended or evaluated for multilingual 
information extraction (though again cf. MUC-7, 
1998, where the use of learning techniques for 
the IE subtasks of named entity recognition and 
coreference r solution are described). 
Given this situation, the primary objective of 
our study was to demonstrate he feasibility of 
using portable--i.e., easily trainable--IE 
technology on Korean documents, focusing on 
techniques for learning information extraction 
patterns. Secondary objectives of the study were 
to elaborate the analyst scenario and system 
design. 
2 Analyst Scenario 
Figure 1 illustrates how an intelligence analyst 
might use the proposed system: 
? The analyst selects one or more Korean 
documents in which to search for 
information (this step not shown). 
# Current affiliation: Konan Technology, Inc., Korea, nari@konantech.co.kr 
* Current affiliation: A'IT Labs-Research, Florham Park, NJ, USA, rambow@research.att.com 
31 
Ouery  
Find Report 
Event: Nest !!lg ........... 
sourcn:l . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ~ ~ 
sate :  I ................. ' ......................... ' ................. i 
Locat Ion: I~u.'~h K..e~.e..a.; ..................................... j I~ 
Part clpant : I .................................................................. i 
Iseun:i~'North Korea" AND "missiles" i 
I 
Response  to  Ouery  
The reports Indicate 2 meetings held In South Korea on the 
issues of North Korea anti missiles: 
Sources Translated Extracts 
Joon,ap~l A ~ ~  
. . . .  I ,4 ~t ln ,  g ~# ,#~=1# o,1 Apf J l  ~ sLYout tP~ I10t 
,!nerF, orea j /ine? ~t~wn Saoul end Tokyo for  the 
Noes / - -  t~Q I ela~gen?? d~/tu~tlons ~uc/t eg Alottl I  Kofgm'~ 
Trans la t ion  o f  Korean  Source  Repor t  
\[Joongang Dally\] 
Korean. Japanese H in i s ters  Discuss NK Po l i cy  
The tmo ministers ~9rsed that any further launching of a 
missile by North Korean would undermine the security of 
~Northeast Asia and the Korea, the United States and Japan 
should take Joint steps against the North Korean missile 
threat. 
}-long requested that Koeura cork to normalize Japan's 
relations with North Korea. rather than cutting channels 
of dialogue bet#men the two countries. 
Koeura said that i f  North Korea continues Its missile 
testing, the Japanese government will definitely stop 
making contributions to KEDO. 
The tee ministers also tentatively agreed that J~anese 
primo minister Kslzo Obuchl should make a state visit  to 
Korea on or around Nerch 20. 
Korean  Source  Repor t  
E t -~ "~I -D lX i '~  ~oo ~ Cll~o" 
oj_a, xd~. ~ ~.\]Ol D IXF~ ~F ~,~FI,,t ~'-9-, ~.~OF ~t~l.~. ~t l  
ud~Otl ~l . : , r t}  ~\]l~i/ ~ol~.-E.II .?-INto ?,,toiSF.~. ~t.-Ol-~ 8-.~01 
~XlI~II= = ~ZISH LDFPI~_ ~C.F~ uH~C3 ~-~-  ~.1-~..~ OF-..It~ 
~01 ~cF.  
x~.~ ~.~OI ~l,.Lt~ EH~=  ~S lO I  ...~CI.~ ~.~_o~ ~It~,/~F 
a~_tOI LO~O KILL= ~0~OPj ~-~/~1 )H~F ~dXl~ 8~9F 
Figure 1 
The analyst selects one or more scenario 
template, to activate in the query. Each 
scenario template corresponds to a specific 
type of event. Available scenario templates 
might include troop movements, acts of 
violence, meetings and negotiathms, 
protests, etc. In Figure 1, the selected event 
is of type meeting (understood broadly). 
The analyst fills in the available slots of the 
selected scenario template in order to restrict 
the search to the information considered to 
be relevant. In Figure 1, the values specified 
in the scenario template indicate that the 
information to f'md is about meetings having 
as location South Korea and as issue North 
Korea and missiles. The analyst also 
32  
specifies what information s/he wants to be 
reported when information matching the 
query is found. In Figure 1, the selected 
boxes under the Report column indicate that 
all information found satisfying the query 
should be reported except for the meeting 
participants. 1 
? Once the analyst submits the query for 
evaluation, the system searches the input 
documents for information matching the 
query. As a result, a hypertext document is 
generated describing the information 
matching the query as well as the source of 
this information. Note that the query 
contains English keywords that are 
automatically translated into Korean prior to 
matching. The extracted information is 
presented in English after being translated 
from Korean. In Figure 1, the generated 
hypertext response indicates two documents 
in the input set that matched the query 
totally or in part. Each summary in the 
response includes just the translations of the 
extracted information that the analyst 
requested to be reported. 
? For each document extract matching the 
analyst query, the analyst can obtain a 
complete machine translation of the Korean 
document where the match was found, and 
where the matched information is 
highlighted. Working with a human 
translator, the analyst can also verify the 
accuracy of the reported information by 
accessing the documents in their original 
language. 
3 System Design 
Figure 2 shows the high-level design of the 
system. It consists of the following components: 
? The User Interface. The browser-based 
interface is for entering queries and 
displaying the resulting presentations. 
? The Portable Information Extractor (PIE) 
component. The PIE component uses the 
While in this example the exclusion of participant 
information in the resulting report is rather artificial, 
in general a scenario template may contain many 
different ypes of information, not all of which are 
likely to interest an analyst at once. 
Extraction Pattem Library - -  which 
contains the set of extraction patterns 
learned in the lab, one set per scenario 
template - -  to extract specific types of 
information from the input Korean 
documents, once parsed. 
? The Ranker component. This component 
ranks the extracted information returned by 
the PIE component according to how well it 
matches the keyword restrictions in the 
query. The MT component's English-to- 
Korean Transfer Lexicon is used to map the 
English keywords to corresponding Korean 
ones. When the match falls below a user- 
? configurable threshold, the extracted 
information is filtered out. 
? The MT component. The MT component 
(cf. Lavoie et al, 2000) translates the 
extracted Korean phrases or sentences into 
corresponding English ones. 
? The Presentation Generator component. 
This component generates well-organized, 
easy-to-read hypertext presentations by 
organizing and formatting the ranked 
extracted information. It uses existing NLG 
components, including the Exemplars text 
planning framework (White and Caldwell, 
1998) and the RealPro syntactic realizer 
(Lavoie and Rainbow, 1997). 
In our feasibility study, the majority of the effort 
went towards developing the PIE component, 
described in the next section. This component 
was implemented in a general way, i.e. in a way 
that we would expect to work beyond the 
specific training/test corpus described below. In 
contrast, we only implemented initial versions of 
the User Interface, Ranker and Presentation 
Generator components, in order to demonstrate 
the system concept; that is, these initial versions 
were only intended.to work with our training/test 
corpus, and will require considerable further 
development prior to reaching operational status. 
For the MT component, we used an early 
version of the lexical transfer-based system 
currently under development in an ongoing 
SBIR Phase II project (cf. Nasr et al, 1997; 
Palmer et al, 1998; Lavoie et al, 2000), though 
with a limited lexicon specifically for translating 
the slot fillers in our training/test corpus. 
33 
Korean Documents 
Parser 
Tagged l 
Korean Documents ( LexiconK?rean 1
~ Syntactic . . . . . .  Eaglish Grammar Structure (English) RealPro 
English Lexicon / ' S~'ntactic Realizer Sentence (English) 
t Parsed Document ~ ::i~i?~'~vii~i? ' .~:Qi~I~:i~-'-iL \[:!::ili:::.:: ~t r~.  :::::::::::::::::::::::: 
Extracted Information \[ 
(Korean) 
Ordered Extracted 
Information(Korean) 
Parsed Document \] Machine "lYanslation I ( 
~l Component (MT) 
Ordered Extracted 
Information (English) 
User Input Data Presentation (E glish) 
Information Extraction 
Query (English) 1 
i : rla0 Inf0rntauonl 
English-Korean 7 
Transfer Lexicon J
Korean-English 
Transfer Lexicon ) 
T 
Miiiiii ii 
? 
Presentation (English) 
End user Document Processing Knowledge base 
component component 
D (C)OTS component 
\[\]Component created in Phase I 
\[\]Component created or improved in Phase II 
Figure 2 
4 Portable Information Extraction 
4.1 Scenario Template and Training/Fest 
Corpus 
For our Phase I feasibility demonstration, we 
chose a minimal scenario template for meeting 
and negotiation events consisting of one or more 
participant slots plus optional date and location 
slots. 2 We then gathered a small corpus of thirty 
articles by searching for articles containing 
"North Korea" and one or more of about 15 
keywords. The first two sentences (with a few 
exceptions) were then annotated with the slots to 
be extracted, leading to a total of 51 sentences 
containing 47 scenario templates and 89 total 
2 In the end, we did not use the 'issue' slot shown in 
Figure 1, as it contained more complex Idlers than 
those that ypically have been handled in IE systems. 
correct slots. Note that in a couple of cases 
more than one template was given for a single 
long sentence. 
When compared to the MUC scenario 
template task, our extraction task was 
considerably simpler, for the following reasons: 
* The answer keys only contained information 
that could be found within a single sentence, 
i.e. the answer keys did not require merging 
information across entences. 
? The answer keys did not require anaphoric 
references to be resolved, and we did not 
deal with conjuncts eparately. 
? We did not attempt o normalize dates or 
remove appositives from NPs. 
4.2 Extraction Pattern Learning 
For our feasibility study, we chose to follow the 
AutoSlog (Lehnert et al, 1992; Riloff, 1993) 
approach to extraction pattern acquisition. In 
this approach, extraction patterns are acquired 
34 
i. E: 
K: 
<target-np>=<subject> <active voice verb> 
<participant> MET 
<target-np>=<subject> <active voice verb> 
<John-i> MANNASSTA 
<John-nom>'MET 
2. E: 
K: 
<target-np>=<subject> <verb> <infinitive> 
<participant> agreed to MEET 
<target-np>=<subject> <verbl-ki- lo> <verb2> 
<John-un> MANNA-ki- lo hapuyhayssta 
<John-nom> MEET-ki- lo agreed 
(-ki: nominalization ending, -io: an adverbial postposition) 
Figure 3 
via a one-shot general-to-specific learning 
algorithm designed specifically for the 
information extraction task. 3 The learning 
algorithm is straightforward and depends only 
on the existence of a (partial) parser and a small 
set of general inguistic patterns that direct the 
creation of specific patterns. As a training 
corpus, it requires a set of texts with noun 
phrases annotated with the slot type to be 
extracted. 
To adapt the AutoSlog approach to Korean, 
we first devised Korean equivalents of the 
English patterns, two of which are shown in 
Figure 3. It turned out that for our corpus, we 
could collapse some of these patterns, though 
some new ones were also needed. In the end we 
used just nine generic patterns. 
Important issues that arose in adapting the 
approach were (1) greater flexibility in word 
order and heavier reliance on morphological 
cues in Korean, and (2) the predominance of 
light verbs (verbs with little semantic ontent of 
their own) and aspectual verbs in the chosen 
domain. We discuss these issues in the next two 
sections. 
4.3 Korean Parser 
We used Yoon's hybrid statistical Korean parser 
(Yoon et al, 1997, 1999; Yoon, 1999) to process 
the input sentences prior to extraction. The 
parser incorporates a POS tagger and 
3 For TIDES, we plan to use more sophisticated 
learning algorithms, as well as active learning 
techniques, such as those described in Thompson et 
al. (1999). 
morphological nalyzer and yields a dependency 
representation as its output? The use of a 
dependency representation e abled us to handle 
the greater flexibility in word order in Korean. 
To facilitate pattern matching, we wrote a 
simple program to convert he parser's output o 
XML form. During the XML conversion, two 
simple heuristics were applied, one to recover 
implicit subjects, and another to correct a 
recurring misanalysis of noun compounds. 
4.4 Trigger Word Filtering and 
Generalization 
In the newswire corpus we looked at, meeting 
events were rarely described with the verb 
'mannata' ('to meet'). Instead, they were 
usually described with a noun that stands for 
'meeting' and a light or aspectual verb, for 
example, 'hoyuy-lul kacta' ('to have a meeting') 
or 'hoyuy-lul machita' ('to finish a meeting'). 
In order to acquire extraction patterns that made 
appropriate use of such collocations, we decided 
to go beyond the AutoSlog approach and 
explicitly group trigger words (such as 'hoyuy') 
into classes, and to likewise group any 
collocations, such as those involving light verbs 
or aspectual verbs. To fmd collocations for the 
trigger words, we reviewed a Korean lexical co- 
occurrence base which was constructed from a 
corpus of 40 million words (Yoon et al, 1997). 
We then used the resulting specification to filter 
the learned patterns to just those containing the 
4 Overall dependency precision is reported to be 
89.4% (Yoon, 1999). 
35 
. - !  
trigger words or trigger word collocations, as 
well as to generalize the patterns to the word 
class level. Because the number of tr:igger 
words is small, this specification can be done 
quickly, and soon pays off in terms of time 
saved in manually filtering the learned patterns. 
4.5 Results 
In testing our approach, we obtained overall 
results of 79% recall and 67% precision in a 
hold-one-out cross validation test. In a cross 
validation test, one repeatedly divides a corpus 
into different raining and test sets, averaging the 
results; in the hold-one-out version, the system 
is tested on a held-out example after being 
trained on the rest. In the IE setting, the recall 
measure is the number of correct slots found 
divided by the total number of correct slots, 
while the precision measure is the number of 
correct slots found divided by the total number 
of slots found. 
While direct comparisons with the MUC 
conference results cannot be made for the 
reasons we gave above, we nevertheless 
consider these results quite promising, as these 
scores exceed the best scores reported at MUC-6 
on the scenario template task. 5 
Table 1: Hold-One-Out Cross Validation 
Slots Recall Precision 
All 79% 67% 
Participant 75% 84% 
Date/Location 86% 54% 
Table2: Hold-One-OutCross Validat~n 
wi~outGeneralizafion 
Slots Recall Precision 
All 61% 64% 
Participant 57% 81% 
Date/Location 67% 52% 
A breakdown by slot is shown in Table 1. We 
may note that precision is low for date and 
location slots because we used a simplistic 
sentence-level merge, rather than dependencies. 
To measure the impact of our approach to 
generalization, we may compare the results in 
5 
http://www.nist.gov/itl/div894/894.02/related_project 
s/tipster/muc.htm 
Table 1 with those shown in Table 2, where 
generalization is not used. As can be seen, the 
generalization step adds substantially to overall 
recall. 
To illustrate the effect of generalization, 
consider the pattern to extract he subject NP of 
the light verb 'kac (hold)' when paired with an 
object NP headed by the noun 'hyepsang 
(negotiation)'. Since this pattern only occurs 
once in our corpus, the slot is not successfully 
extracted in the cross-validation test without 
generalization. However, since this example 
does fall under the more generalized pattern of 
extracting the subject NP of a verb in the light 
verb class when paired with an object NP 
headed by a noun the 'hoytam-hyepsang' class, 
the slot is successfully extracted in the cross- 
validation test using the generalized patterns. 
Cases like these are the source of the 18% boost 
in recall of participant slots, from 57% to 75%. 
5 Discussion 
Our feasibility study has focused our attention 
on several questions concerning the interaction 
of IE and MT, which we hope to pursue under 
the DARPA TIDES initiative. One question is 
the extent o which slot filler translation is more 
practicable than general-purpose MT; one would 
expect to achieve much higher quality on slot 
fillers, as they are typically relatively brief noun 
phrases, and instantiation of a slot implies a 
degree of semantic lassification. On the other 
hand, one might find that higher quality is 
required in order to take translated phrases out 
of their original context. Another question is 
how to automate the construction of bilingual 
lexicons. An important issue here will be how 
to combine information from different sources, 
given that automatically acquired lexical 
information is apt to be less reliable, though 
domain-specific. 
Acknowledgements 
Our thanks go to Richard Kittredge and Tanya 
Korelsky for helpful comments and advice. This 
work was supported by ARL contract DAAD 17- 
99-C-0005. 
36 
References 
Cardie, C. (1997). Empirical Methods in Information 
Extraction. AI Magazine 18(4):65-79. 
Lavoie, B. and Rambow, O. (1997). RealPro - -  A 
fast, portable sentence realizer. In Proceedings of 
the Conference on Applied Natural Language 
Processing (ANLP'97), Washington, DC. 
Lavoie, B., Korelsky, T., and Rambow, O. (2000). A 
Framework for MT and Multilingual NLG Systems 
Based on Uniform Lexico-Structural Processing. 
To appear in Proceedings of the Sixth Conference 
on Applied Natural Language Processing (ANLP- 
2000), Seattle, WA. 
Lehnert, W., Cardie, C., Fisher, D., McCarthy, J., 
Riloff, E., and Soderland, S. (1992). University of 
Massachusetts: Description of the CIRCUS system 
as used in MUC-4. In Proceedings of the Fourth 
Message Understanding Conference (MUC-4), 
pages 282-288, San Mateo, CA. Morgan 
Kaufmann. 
MUC-5 (1994). Proceedings of the Fifth Message 
Understanding Conference (MUC-5). Morgan 
Kaufmann, San Mateo, CA. 
MUC-7 (1998). Proceedings of the Seventh Message 
Understanding Conference (MUC-7). Morgan 
Kaufmann, San Francisco, CA. 
Nasr, A., Rambow, O., Palmer, M., and Rosenzweig, 
J. (1997). Enriching lexical transfer with cross- 
linguistic semantic features. In Proceedings of the 
lnterlingua Workshop at the MT Summit, San 
Diego, CA. 
Palmer, M., Rambow, O., and Nasr, A. (1998). 
Rapid prototyping of domain-specific machine 
translation systems. In Machine Translation and 
the Information Soup - Proceedings of the Third 
Conference of the Association for Machine 
Translation in the Americas AMTA'98, Springer 
Verlag (Lecture Notes in Artificial Intelligence No. 
1529), Berlin. 
Riloff, E. (1993). Automatically constructing a
dictionary for information exlxaction tasks. In 
Proceedings of the Eleventh National Conference 
on Artificial Intelligence, pages 811-816, 
Washington, DC. AAAI Press / MIT Press. 
Thompson, C. A., Califf, M. E., and Mooney, R. J. 
(1999). Active learning for natural language 
parsing and information extraction. In Proceedings 
of the Sixteenth International Machine Learning 
Conference (1CML-99), Bled, Slovenia. 
White, M. and Caldwell, T. (1998). EXEMPLARS: A 
practical, extensible framework for dynamic text 
generation. In Proceedings of the 8th International 
Workshop on Natural Language Generation, 
Niagara-on-the-Lake, Ontario. 
Yoon, J. (1999). Efficient dependency parsing based 
on three types of chunking and lexical association. 
Submitted. 
Yoon, J., Choi, K.-S., and Song, M. (1999). Three 
types of chunking in Korean and dependency 
analysis based on lexical association. In 
Proceedings of lCCPOL. 
Yoon, J., Kim, S., and Song, M. (1997). New parsing 
method using global association table. In 
Proceedings of the 5th International Workshop on 
Parsing Technology. 
37 
Inducing Lexico-Structural Transfer Rules from Parsed Bi-texts
Benoit Lavoie, Michael White, and Tanya Korelsky
CoGenTex, Inc.
840 Hanshaw Road
Ithaca, NY 14850, USA
benoit,mike,tanya@cogentex.com
Abstract
This paper describes a novel approach
to inducing lexico-structural transfer
rules from parsed bi-texts using syn-
tactic pattern matching, statistical co-
occurrence and error-driven filtering.
We present initial evaluation results and
discuss future directions.
1 Introduction
This paper describes a novel approach to inducing
transfer rules from syntactic parses of bi-texts and
available bilingual dictionaries. The approach
consists of inducing transfer rules using the four
major steps described in more detail below: (i)
aligning the nodes of the parses; (ii) generating
candidate rules from these alignments; (iii) order-
ing candidate rules by co- occurrence; and (iv) ap-
plying error-driven filtering to select the final set
of rules.
Our approach is based on lexico-structural
transfer (Nasr et. al., 1997), and extends recent
work reported in (Han et al, 2000) about Korean
to English transfer in particular. Whereas Han et
al. focus on high quality domain-specific transla-
tion using handcrafted transfer rules, in this work
we instead focus on automating the acquisition of
such rules.
Our approach can be considered a generaliza-
tion of syntactic approaches to example-based
machine translation (EBMT) such as (Nagao,
1984; Sato and Nagao, 1990; Maruyama and
Watanabe, 1992). While such approaches use
syntactic transfer examples during the actual
transfer of source parses, our approach instead
uses syntactic transfer examples to induce general
transfer rules that can be compiled into a transfer
dictionary for use in the actual translation process.
Our approach is similar to the recent work of
(Meyers et al, 1998) where transfer rules are also
derived after aligning the source and target nodes
of corresponding parses. However, it also differs
from (Meyers et al, 1998) in several important
points. The first difference concerns the content
of parses and the resulting transfer rules; in (Mey-
ers et al, 1998), parses contain only lexical labels
and syntactic roles (as arc labels), while our ap-
proach uses parses containing lexical labels, syn-
tactic roles, and any other syntactic information
provided by parsers (tense, number, person, etc.).
The second difference concerns the node align-
ment; in (Meyers et al, 1998), the alignment of
source and target nodes is designed in a way that
preserves node dominancy in the source and tar-
get parses, while our approach does not have such
restriction. One of the reasons for this difference
is due to the different language pairs under study;
(Meyers et al, 1998) deals with two languages
that are closely related syntactically (Spanish and
English) while we are dealing with languages that
syntactically are quite divergent, Korean and En-
glish (Dorr, 1994). The third difference is in the
process of identification of transfer rules candi-
dates; in (Meyers et al, 1998), the identification
is done by using the exact tree fragments in the
source and target parse that are delimited by the
alignment, while we use all source and target tree
sub-patterns matching a subset of the parse fea-
tures that satisfy a customizable set of alignment
constraints and attribute constraints. The fourth
third difference is in the level of abstraction of
transfer rules candidates; in (Meyers et al, 1998),
the source and target patterns of each transfer rule
are fully lexicalized (except possibly the terminal
nodes), while in our approach the nodes of trans-
fer rules do not have to be lexicalized.
Section 2 describes our approach to trans-
fer rules induction and its integration with data
preparation and evaluation. Section 3 describes
the data preparation process and resulting data.
Section 4 describes the transfer induction process
in detail. Section 5 describes the results of our ini-
tial evaluation. Finally, Section 6 concludes with
a discussion of future directions.
2 Overall Approach
In its most general form, our approach to transfer
rules induction includes three different processes,
data preparation, transfer rule induction and eval-
uation. An overview of each process is provided
below; further details are provided in subsequent
sections.
The data preparation process creates the fol-
lowing resources from the bi-texts:
? A training set and a test set of source and
target parses for the bi-texts, post-processed
into a syntactic dependency representation.
? A baseline transfer dictionary, which may in-
clude (depending upon availability) lexical
transfer rules extracted from the bi-texts us-
ing statistical methods, lexical transfer rules
from existing bilingual dictionaries, and/or
handcrafted lexico-structural transfer rules.
The transfer induction process induces lexico-
structural transfer rules from the training set of
corresponding source and target parses that, when
added to the baseline transfer dictionary, produce
transferred parses that are closer to the corre-
sponding target parses. The transfer induction
process has the following steps:
? Nodes of the corresponding source and tar-
get parses are aligned using the baseline
transfer dictionary and some heuristics based
on the similarity of part-of-speech and syn-
tactic context.
? Transfer rule candidates are generated based
on the sub-patterns that contain the corre-
sponding aligned nodes in the source and tar-
get parses.
? The transfer rule candidates are ordered
based on their likelihood ratios.
? The transfer rule candidates are filtered, one
at a time, in the order of the likelihood ra-
tios, by removing those rule candidates that
do not produce an overall improvement in
the accuracy of the transferred parses.
The evaluation process has the following steps:
? Both the baseline transfer dictionary and the
induced transfer dictionary (i.e., the baseline
transfer dictionary augmented with the in-
duced transfer rules) are applied to the test
set in order to produce two sets of transferred
parses, the baseline set and the (hopefully)
improved induced set. For each set, the dif-
ferences between the transferred parses and
target parses are measured, and the improve-
ment in tree accuracy is calculated.
? After performing syntactic realization on the
baseline set and the induced set of trans-
ferred parses, the differences between the
resulting translated strings and the target
strings are measured, and the improvement
in string accuracy is calculated.
? For a subset of the translated strings, human
judgments of accuracy and grammaticality
are gathered, and the correlations between
the manual and automatic scores are calcu-
lated, in order to assess the meaningfulness
of the automatic measures.
3 Data Preparation
3.1 Parsing the Bi-texts
In our experiments to date, we have used a cor-
pus consisting of a Korean dialog of 4183 sen-
tences and their English human translations. We
ran off-the-shelf parsers on each half of the cor-
pus, namely the Korean parser developed by Yoon
et al (1997) and the English parser developed by
Collins (1997). Neither parser was trained on our
corpus.
We automatically converted the phrase struc-
ture output of the Collins parser into the syntac-
tic dependency representation used by our syn-
tactic realizer, RealPro (Lavoie and Rambow,
1997). This representation is based on the deep-
syntactic structures (DSyntS) of Meaning-Text
Theory (Mel?c?uk, 1988). The important features
of a DSyntS are as follows:
? a DSyntS is an unordered tree with labeled
nodes and labeled arcs;
? a DSyntS is lexicalized, meaning that the
nodes are labeled with lexemes (uninflected
words) from the target language;
? a DSyntS is a dependency structure and not a
phrase- structure structure: there are no non-
terminal nodes, and all nodes are labeled
with lexemes;
? a DSyntS is a syntactic representation,
meaning that the arcs of the tree are la-
beled with syntactic relations such as SUB-
JECT (represented in DSyntSs as I), rather
than conceptual or semantic relations such as
AGENT;
? a DSyntS is a deep syntactic representation,
meaning that only meaning-bearing lexemes
are represented, and not function words.
Since the output of the Yoon parser is quite sim-
ilar, with the exception of its treatment of syn-
tactic relations, we have used its output as is.
The DSyntS representations for two correspond-
ing Korean1 and English sentences are illustrated
in Figure 1.
In examining the outputs of the two parsers
on our corpus, we found that about half of the
parse pairs contained incorrect dependency as-
signments, incomplete lemmatization or incom-
plete parses. To reduce the impact of such pars-
ing errors in our initial experiments, we have pri-
marily focused on a higher quality subset of 1763
sentence pairs that were selected according to the
following criteria:
? Parse pairs where the source or target parse
contained more than 10 nodes were rejected,
1Korean is represented in romanized format in this paper.
(S1) {i} {Ci-To-Reul} {Ta-Si} {Po-Ra}.
this + map-accusative + again + look-imp
(D1) {po} [class=vbma ente={ra}] (
s1 {ci-to} [class=nnin2 ppca={reul}] (
s1 {i} [class=ande]
)
s1 {ta-si} [class=adco2]
)
(S2) Look at the map again.
(D2) look [class=verb mood=imp] (
attr at [class=preposition] (
ii map [class=common_noun article=def]
)
attr again [class=adverb]
)
Figure 1: Syntactic dependency representations
for corresponding Korean and English sentences
since these usually contained more parse er-
rors than smaller parses.
? Parse pairs where the source or target parse
contained non-final punctuation were re-
jected; this criterion was based on our ob-
servation that in most such cases, the source
or target parses contained only a fragment
of the original sentence content (i.e., one or
both parsers only parsed what was on one
side of an intra-sentential punctuation mark).
We divided this higher quality subset into train-
ing and test sets by randomly choosing 50% of
the 1763 higher quality parse pairs (described in
Section 3.1) for inclusion in the training set, re-
serving the remaining 50% for the test set. The
average numbers of parse nodes in the training set
and test set were respectively 6.91 and 6.11 nodes.
3.2 Creating the Baseline Transfer
Dictionary
In the general case, any available bilingual dic-
tionaries can be combined to create the base-
line transfer dictionary. These dictionaries may
include lexical transfer dictionaries extracted
from the bi-texts using statistical methods, exist-
ing bilingual dictionaries, or handcrafted lexico-
structural transfer dictionaries. If probabilistic in-
formation is not already associated with the lexi-
cal entries, log likelihood ratios can be computed
and added to these entries based on the occur-
rences of these lexical items in the parse pairs.
In our initial experiments, we decided to focus
on the scenario where the baseline transfer dic-
@KOREAN:
{po} [class=vbma] (
s1 $X [ppca={reul}]
)
@ENGLISH:
look [class=verb] (
attr at [class=preposition] (
ii $X
)
)
@-2xLOG_LIKELIHOOD: 12.77
Figure 2: Transfer rule for English lexicalization
and preposition insertion
@KOREAN:
$X [class=vbma ente={ra}]
@ENGLISH:
$X [class=verb mood=imp]
@-2xLOG_LIKELIHOOD: 33.37
Figure 3: Transfer rule for imperative forms
tionary is created from lexical transfer entries ex-
tracted from the bi-texts using statistical methods.
To simulate this scenario, we created our baseline
transfer dictionary by taking the lexico-syntactic
transfer dictionary developed by Han et al (2000)
for this corpus and removing the (more general)
rules that were not fully lexicalized. Starting with
this purely lexical baseline transfer dictionary en-
abled us to examine whether these more general
rules could be discovered through induction.
4 Transfer Rule Induction
The induced lexico-structural transfer rules are
represented in a formalism similar to the one de-
scribed in Nasr et al (1997), and extended to also
include log likelihood ratios. Figures 2 and 3
illustrate two entry samples that can be used to
transfer a Korean syntactic representation for ci-
to-reul po-ra to an English syntactic representa-
tion for look at the map. The first rule lexicalizes
the English predicate and inserts the correspond-
ing preposition while the second rule inserts the
English imperative attribute. This formalism uses
notation similar to the syntactic dependency nota-
tion shown in Figure 1, augmented with variable
arguments prefixed with $ characters.
4.1 Aligning the Parse Nodes
To align the nodes in the source and target parse
trees, we devised a new dynamic programming
alignment algorithm that performs a top-down,
bidirectional beam search for the least cost map-
ping between these nodes. The algorithm is pa-
rameterized by the costs of (1) aligning two nodes
whose lexemes are not found in the baseline trans-
fer dictionary; (2) aligning two nodes with dif-
fering parts of speech; (3) deleting or inserting a
node in the source or target tree; and (4) aligning
two nodes whose relative locations differ.
To determine an appropriate part of speech cost
measure, we first extracted a small set of parse
pairs that could be reliably aligned using lexical
matching alone, and then based the cost measure
on the co-occurrence counts of the observed parts
of speech pairings. The remaining costs were set
by hand.
As a result of the alignment process, alignment
id attributes (aid) are added to the nodes of the
parse pairs. Some nodes may be in alignment
with no other node, such as English prepositions
not found in the Korean DSyntS.
4.2 Generating Rule Candidates
Candidate transfer rules are generated using three
data sources:
? the training set of aligned source and target
parses resulting from the alignment process;
? a set of alignment constraints which identify
the subtrees of interest in the aligned source
and target parses (Section 4.2.1);
? a set of attribute constraints which determine
what parts of the aligned subtrees to include
in the transfer rule candidates? source and
target patterns (Section 4.2.2).
The alignment and attribute constraints are nec-
essary to keep the set of candidate transfer rules
manageable in size.
4.2.1 Alignment constraints
Figure 4 shows an example alignment constraint.
This constraint, which matches the structural pat-
terns of the transfer rule illustrated in Figure 2,
uses the aid alignment attribute to indicate that
@KOREAN:
$X1 [aid=$1] (
$R1 $X2 [aid=$2]
)
@ENGLISH:
$Y1 [aid=$1] (
$R2 $Y2 (
$R3 $Y3 [aid=$2]
)
)
Figure 4: Alignment constraint
in a Korean and English parse pair, any source
and target sub-trees matching this alignment con-
straint (where $X1 and $Y1 are aligned or have
the same attribute aid values and where $X2 and
$Y3 are aligned) can be used as a point of depar-
ture for generating transfer rule candidates. We
suggest that alignment constraints such as this one
can be used to define most of the possible syntac-
tic divergences between languages (Dorr, 1994),
and that only a handful of them are necessary for
two given languages (we have identified 11 gen-
eral alignment constraints necessary for Korean to
English transfer so far).
4.2.2 Attribute constraints
Attribute constraints are used to limit the space
of possible transfer rule candidates that can be
generated from the sub-trees satisfying the align-
ment constraints. Candidate transfer rules must
satisfy all of the attribute constraints. Attribute
constraints can be divided into two types:
? independent attribute constraints, whose
scope covers only one part of a candidate
transfer rule and which are the same for the
source and target parts;
? concurrent attribute constraints, whose
scope extends to both the source and target
parts of a candidate transfer rule.
The examples of an independent attribute con-
straint and of a concurrent attribute constraint are
given in Figure 5 and Figure 6 respectively. As
with the alignment constraints, we suggest that a
relatively small number of attribute constraints is
necessary to generate most of the desired rules for
a given language pair.
Each node of a candidate transfer rule must have its relation
attribute (relationship with its governor) specified if it is an
internal node, otherwise this relation must not be specified:
e.g.
 $X1 ( $R $X2 )
Figure 5: Independent attribute constraint
In a candidate transfer rule, inclusion of the lexemes of two
aligned nodes must be done concurrently:
e.g.
$X [aid=$1]
and
$Y [aid=$1]
e.g.
 [aid=$1]
and
 [aid=$1]
Figure 6: Concurrent attribute constraint
4.3 Ordering Rule Candidates
In the next step, transfer rule candidates are or-
dered as follows: first, by their log likelihood ra-
tios (Manning and Schutze, 1999: 172-175); sec-
ond, any transfer rule candidates with the same
log likelihood ratio are ordered by their speci-
ficity.
4.3.1 Rule ordering by log likelihood ratio
We calculate the log likelihood ratio, log ?, ap-
plied to a transfer rule candidate as indicated in
Figure 7. Note that log ? is a negative value,
and following (Manning and Schutze, 1999), we
assign -2 log ? to the transfer rule. Note also
that in the definitions of C1, C2, and C12 we are
currently only considering one occurrence or co-
occurrence of the source and/or target patterns per
parse pair, while in general there could be more
than one; in our initial experiments these defini-
tions have sufficed.
4.3.2 Rule ordering by specificity
If two or more candidate transfer rules have the
same log likelihood ratio, ties are broken by a
specificity heuristic, with the result that more gen-
eral rules are ordered ahead of more specific ones.
The specificity of a rule is defined to be the fol-
lowing sum: the number of attributes found in
the source and target patterns, plus 1 for each for
log ? =
logL(C12, C1, p) + logL(C2 ? C12, N ? C1, p)
? logL(C12, C1, p1)? logL(C2?C12, N ?C1, p2)
where, not counting attributes aid,
? C1 = number of source parses containing at least one
occurrence of C?s source pattern
? C2 = number of target parses containing at least one
occurrence of C?s target pattern
? C12 = number of source and target parse pairs contain-
ing at least one co-occurrence of C?s source pattern
and C?s target pattern satisfying the alignment con-
straints
? N = number of source and target parse pairs
? P = C2/N ;
? P1 = C12/C1;
? P2 = (C2 ? C12)/(N ? C1);
? L(k, n, x) = xk(1? x)n?k
Figure 7: Log likelihood ratios for transfer rule
candidates
each lexeme attribute and for each dependency re-
lationship. In our initial experiments, this simple
heuristic has been satisfactory.
4.4 Filtering Rule Candidates
Once the candidate transfer rules have been or-
dered, error-driven filtering is used to select those
that yield improvements over the baseline trans-
fer dictionary. The algorithm works as follows.
First, in the initialization step, the set of accepted
transfer rules is set to just those appearing in the
baseline transfer dictionary, and the current er-
ror rate is established by applying these transfer
rules to all the source structures and calculating
the overall difference between the resulting trans-
ferred structures and the target parses. Then, in a
single pass through the ordered list of candidates,
each transfer rule candidate is tested to see if it
reduces the error rate. During each iteration, the
candidate transfer rule is provisionally added to
the current set of accepted rules and the updated
set is applied to all the source structures. If the
overall difference between the transferred struc-
tures and the target parses is lower than the cur-
rent error rate, then the candidate is accepted and
@KOREAN:
{po} [class=vbma ente={ra}] (
s1 $X [ppca={reul}]
)
@ENGLISH:
look [class=verb mood=imp] (
attr at [class=preposition] (
ii $X
)
)
@-2xLOG_LIKELIHOOD: 11.40
Figure 8: Transfer rule for English imperative
with lexicalization and preposition insertion
the current error rate is updated; otherwise, the
candidate is rejected and removed from the cur-
rent set.
4.5 Discussion of Induced Rules
Experimentation with the training set of 882 parse
pairs described in Section 3.1 produced 12467
source and target sub-tree pairs using the align-
ment constraints, from which 20569 transfer rules
candidate were generated and 7565 were accepted
after filtering. We expect that the number of
accepted rules per parse pair will decrease with
larger training sets, though this remains to be ver-
ified.
The rule illustrated in Figure 3 was accepted as
the 65th best transfer rule with a log likelihood
ratio of 33.37, and the rule illustrated in Figure 2
was accepted as the 189th best transfer rule can-
didate with a log likelihood ratio of 12.77. An ex-
ample of a candidate transfer rule that was not ac-
cepted is the one that combines the features of the
two rules mentioned above, illustrated in Figure 8.
This transfer rule candidate had a lower log like-
lihood ratio of 11.40; consequently, it is only con-
sidered after the two rules mentioned above, and
since it provides no further improvement upon
these two rules, it is filtered out.
In an informal inspection of the top 100 ac-
cepted transfer rules, we found that most of them
appear to be fairly general rules that would nor-
mally be found in a general syntactic-based trans-
fer dictionary. In looking at the remaining rules,
we found that the rules tended to become increas-
ingly corpus-specific.
5 Initial Evaluation
5.1 Results
In an initial evaluation of our approach, we ap-
plied both the baseline transfer dictionary and
the induced transfer dictionary (i.e., the baseline
transfer dictionary augmented with the transfer
rules induced from the training set) to the test half
of the 1763 higher quality parse pairs described in
Section 3.1, in order to produce two sets of trans-
ferred parses, the baseline set and the induced set.
For each set, we then calculated tree accuracy re-
call and precision measures as follows:
Tree accuracy recall The tree accuracy recall
for a transferred parse and a correspond-
ing target parse is determined the by C/Rq,
where C is the total number of features (at-
tributes, lexemes and dependency relation-
ships) that are found in both the nodes of
the transferred parse and in the correspond-
ing nodes in the target parse, and Rq is the
total number of features found in the nodes
of the target parse. The correspondence be-
tween the nodes of the transferred parse and
the nodes of the target parse is determined
with alignment information obtained using
the technique described in Section 4.1.
Tree accuracy precision The tree accuracy pre-
cision for a transferred parse and a corre-
sponding target parse is determined the by
C/Rt, where C is the total number of fea-
tures (attributes, lexemes and dependency
relationships) that are found in both the
nodes of the transferred parse and in the cor-
responding nodes in the target parse, and Rt
is the total number of features found in the
nodes of the transferred parse.
Table 1 shows the tree accuracy results, where
the f-score is equally weighted between recall and
precision. The results illustrated in Table 1 indi-
cate that the transferred parses obtained using in-
duction were moderately more similar to the tar-
get parses than the transferred parses obtained us-
ing the baseline transfer, with about 15 percent
improvement in the f-score.
Recall Precision F-Score
Baseline 37.77 46.81 41.18
Induction 55.35 58.20 55.82
Table 1: Tree accuracy results
5.2 Discussion
At the time of writing, the improvements in tree
accuracy do not yet appear to yield apprecia-
ble improvements in realization results. While
our syntactic realizer, RealPro, does produce rea-
sonable surface strings from the target depen-
dency trees, despite occasional errors in parsing
the target strings and converting the phrase struc-
ture trees to dependency trees, it appears that the
tree accuracy levels for the transferred parses will
need to be higher on average before the improve-
ments in tree accuracy become consistently visi-
ble in the realization results. At present, the fol-
lowing three problems represent the most impor-
tant obstacles we have identified to achieving bet-
ter end-to-end results:
? Since many of the test sentences require
transfer rules for which there are no similar
cases in the set of training sentences, it ap-
pears that the relatively small size of our cor-
pus is a significant barrier to better results.
? Some performance problems with the cur-
rent implementation have forced us to make
use of a perhaps overly strict set of alignment
and attribute constraints. With an improved
implementation, it may be possible to find
more valuable rules from the same training
data.
? A more refined treatment of rule conflicts is
needed in order to allow multiple rules to
access overlapping contexts, while avoiding
the introduction of multiple translations of
the same content in certain cases.
6 Conclusion and Future Directions
In this paper we have presented a novel approach
to transfer rule induction based on syntactic pat-
tern co-occurrence in parsed bi-texts. In an initial
evaluation on a relatively small corpus, we have
shown that the induced syntactic transfer rules
from Korean to English lead to a modest increase
in the accuracy of transferred parses when com-
pared to the target parses. In future work, we
hope to demonstrate that a combination of consid-
ering a larger set of transfer rule candiates, refin-
ing our treatment of rule conflicts, and making use
of more training data will lead to further improve-
ments in tree accuracy, and, following syntactic
realization, will yield to significant improvements
in end-to-end results.
Acknowledgements
We thank Richard Kittredge for helpful discus-
sion, Daryl McCullough and Ted Caldwell for
their help with evaluation, and Chung-hye Han,
Martha Palmer, Joseph Rosenzweig and Fei Xia
for their assistance with the handcrafted Korean-
English transfer dictionary and the conversion of
phrase structure parses to syntactic dependency
representations. This work has been partially sup-
ported by DARPA TIDES contract no. N66001-
00-C-8009.
References
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Meeting of the Association for Computational
Linguistics (ACL?97), Madrid, Spain.
Bonnie Dorr. 1994. Machine translation divergences:
A formal description and proposed solution. Com-
putational Linguistics, 20(4):597?635.
C. Han, B. Lavoie, M. Palmer, O. Rambow, R. Kit-
tredge, T. Korelsky, N. Kim, and M. Kim. 2000.
Handling structural divergences and recovering
dropped arguments in a Korean-English machine
translation system. In Proceedings of the Fourth
Conference on Machine Translation in the Ameri-
cas (AMTA?00), Misin Del Sol, Mexico.
Benoit Lavoie and Owen Rambow. 1997. RealPro ?
a fast, portable sentence realizer. In Proceedings of
the Conference on Applied Natural Language Pro-
cessing (ANLP?97), Washington, DC.
C. D. Manning and H. Schutze. 1999. Foundations
of Statistical Natural Language Processing. MIT
Press.
H. Maruyama and H. Watanabe. 1992. Tree cover
search algorithm for example-based translation. In
Proceedings of the Fourth International Conference
on Theoretical and Methodological Issues in Ma-
chine Translation (TMI?92), pages 173?184.
Y. Matsumoto, H. Hishimoto, and T. Utsuro. 1993.
Structural matching of parallel texts. In Proceed-
ings of the 31st Annual Meetings of the Association
for Computational Linguistics (ACL?93), pages 23?
30.
Igor Mel?c?uk. 1988. Dependency Syntax. State Uni-
versity of New York Press, Albany, NY.
A. Meyers, R. Yangarber, R. Grishman, C. Macleod,
and A. Moreno-Sandoval. 1998. Deriving transfer
rules from dominance-preserving alignments. In
Proceedings of COLING-ACL?98, pages 843?847.
Makoto Nagao. 1984. A framework of a mechan-
ical translation between Japenese and English by
analogy principle. In A. Elithorn and R. Banerji,
editors, Artificial and Human Intelligence. NATO
Publications.
Alexis Nasr, Owen Rambow, Martha Palmer, and
Joseph Rosenzweig. 1997. Enriching lexical trans-
fer with cross-linguistic semantic features. In Pro-
ceedings of the Interlingua Workshop at the MT
Summit, San Diego, California.
S. Sato and M. Nagao. 1990. Toward memory-
based translation. In Proceedings of the 13th Inter-
national Conference on Computational Linguistics
(COLING?90), pages 247?252.
Fei Xia and Martha Palmer. 2001. Converting depen-
dency structures to phrase structures. In Notes of
the First Human Language Technology Conference,
San Diego, California.
J. Yoon, S. Kim, and M. Song. 1997. New parsing
method using global association table. In Proceed-
ings of the 5th International Workshop on Parsing
Technology.
Learning Domain-Specific Transfer Rules:
An Experiment with Korean to English Translation
Benoit Lavoie, Michael White, and Tanya Korelsky
CoGenTex, Inc.
840 Hanshaw Road
Ithaca, NY 14850, USA
benoit,mike,tanya@cogentex.com
Abstract
We describe the design of an MT system that em-
ploys transfer rules induced from parsed bitexts
and present evaluation results. The system learns
lexico-structural transfer rules using syntactic pat-
tern matching, statistical co-occurrence and error-
driven filtering. In an experiment with domain-
specific Korean to English translation, the approach
yielded substantial improvements over three base-
line systems.
1 Introduction
In this paper, we describe the design of an MT
system that employs transfer rules induced from
parsed bitexts and present evaluation results for Ko-
rean to English translation. Our approach is based
on lexico-structural transfer (Nasr et. al., 1997),
and extends recent work reported in (Han et al,
2000) about Korean to English transfer in particular.
Whereas Han et al focus on high quality domain-
specific translation using handcrafted transfer rules,
in this work we instead focus on automating the ac-
quisition of such rules.
The proposed approach is inspired by example-
based machine translation (EBMT; Nagao, 1984;
Sato and Nagao, 1990; Maruyama and Watanabe,
1992) and is similar to the recent works of (Mey-
ers et al, 1998) and (Richardson et al, 2001) where
transfer rules are also derived after aligning the
source and target nodes of corresponding parses.
However, while (Meyers et al, 1998) and (Richard-
son et al, 2001) only consider parses and rules
with lexical labels and syntactic roles, our approach
uses parses containing any syntactic information
provided by parsers (lexical labels, syntactic roles,
tense, number, person, etc.), and derives rules con-
sisting of any source and target tree sub-patterns
matching a subset of the parse features. A more de-
tailed description of the differences can be found in
(Lavoie et. al., 2001).
2 Overall Runtime System Design
Our Korean to English MT runtime system relies on
the following off-the-shelf software components:
Korean parser For parsing, we used the wide cov-
erage syntactic dependency parser for Korean
developed by (Yoon et al, 1997). The parser
was not trained on our corpus.
Transfer component For transfer of the Korean
parses to English structures, we used the same
lexico-structural transfer framework as (Lavoie
et al, 2000).
Realizer For surface realization of the transferred
English syntactic structures, we used the Re-
alPro English realizer (Lavoie and Rambow,
1997).
The training of the system is described in the next
two sections.
3 Data Preparation
3.1 Parses for the Bitexts
In our experiments, we used a parallel corpus de-
rived from bilingual training manuals provided by
the U.S. Defense Language Institute. The corpus
consists of a Korean dialog of 4,183 sentences about
battle scenario message traffic and their English hu-
man translations.
The parses for the Korean sentences were ob-
tained using Yoon?s parser, as in the runtime sys-
tem. The parses for the English human transla-
(S1) {i} {Ci-To-Reul} {Ta-Si} {Po-Ra}.
this + map-accusative + again + look-imp
(D1) {po} [class=vbma ente={ra}] (
s1 {ci-to} [class=nnin2 ppca={reul}] (
s1 {i} [class=ande] )
s1 {ta-si} [class=adco2] )
(S2) Look at the map again.
(D2) look [class=verb mood=imp] (
attr at [class=preposition] (
ii map [class=common_noun article=def] )
attr again [class=adverb] )
Figure 1: Syntactic dependency representations for
corresponding Korean and English sentences
Korean English
Avg. sentence size 9.08 13.26
Avg. parse size 8.96 10.77
Table 1: Average sizes for sentences and parses in
corpus
tions were derived from an English Tree Bank de-
veloped in (Han et al, 2000). To enable the sur-
face realization of the English parses via RealPro,
we automatically converted the phrase structures
of the English Tree Bank into deep-syntactic de-
pendency structures (DSyntSs) of the Meaning-Text
Theory (MTT) (Mel?c?uk, 1988) using Xia?s con-
verter (Xia and Palmer, 2001) and our own conver-
sion grammars. The realization results of the re-
sulting DSyntSs for our training corpus yielded a
unigram and bigram accuracy (f-score) of approxi-
mately 95% and 90%, respectively.
A DSyntS is an unordered tree where all nodes
are meaning-bearing and lexicalized. Since the out-
put of the Yoon parser is quite similar, we have used
its output as is. The syntactic dependency represen-
tations for two corresponding Korean1 and English
sentences are shown in Figure 1.
3.2 Training and Test Sets of Parse Pairs
The average sentence lengths (in words) and parse
sizes (in nodes) for the 4,183 Korean and English
sentences in our corpus are given in Table 1.
In examining the Korean parses, we found that
many of the larger parses, especially those contain-
ing intra-sentential punctuation, had incorrect de-
pendency assignments, incomplete lemmatization
or were incomplete parses. In examining the En-
glish converted parses, we found that many of
1Korean is represented in romanized format in this paper.
Korean English
Avg. sentence size 6.43 9.36
Avg. parse size 6.43 7.34
Table 2: Average sizes for sentences and parses in
training set
Korean English
Avg. sentence size 7.04 10.58
Avg. parse size 7.02 8.56
Table 3: Average sizes for sentences and parses in
test set
the parses containing intra-sentential punctuation
marks other than commas had incorrect dependency
assignments, due to the limitations of our conver-
sion grammars. Consequently, in our experiments
we have primarily focused on a higher quality sub-
set of 1,483 sentence pairs, automatically selected
by eliminating from the corpus all parse pairs where
one of the parses contained more than 11 content
nodes or involved problematic intra-sentential punc-
tuation.
We divided this higher quality subset into training
and test sets. For the test set, we randomly selected
50 parse pairs containing at least 5 nodes each. For
the training set, we used the remaining 1,433 parse
pairs. The average sentence lengths and parse sizes
for the training and test sets are represented in Ta-
bles 2 and 3.
3.3 Creating the Baseline Transfer Dictionary
In our system, transfer dictionaries contain Ko-
rean to English lexico-structural transfer rules de-
fined using the formalism described in (Nasr et.
al., 1997), extended to include log likelihood ra-
tios (Manning and Schutze, 1999: 172-175). Sam-
ple transfer rules are illustrated in Section 4. The
simplest transfer rules consist of direct lexical map-
pings, while the most complex may contain source
and target syntactic patterns composed of multiple
nodes defined with lexical and/or syntactic features.
Each transfer rule is assigned a log likelihood ratio
calculated using the training parse set.
To create the baseline transfer dictionary for our
experiments, we had three bilingual dictionary re-
sources at our disposal:
A corpus-based handcrafted dictionary: This
dictionary was manually assembled by (Han et
al., 2000) for the same corpus used here. Note,
Korean English
Lexical coverage 92.18% 90.17%
Table 4: Concurrent lexical coverage of training set
by baseline dictionary
however, that it was developed for different
parse representations, and with an emphasis
primarily on the lexical coverage of the source
parses, rather than the source and target parse
pairs.
A corpus-based extracted dictionary: This dic-
tionary was automatically created from our
corpus by the RALI group from the University
of Montreal. Since the extraction heuristics
did not handle the rich morphological suffixes
of Korean, the extraction results contained
inflected words rather than lexemes.
A wide coverage dictionary: This dictionary of
70,300 entries was created by Systran, without
regard to our corpus.
We processed and combined these resources as
follows:
 First, we replaced the inflected words with un-
inflected lexemes using Yoon?s morphological
analyzer and a wide coverage English morpho-
logical database (Karp and Schabes, 1992).
 Second, we merged all morphologically an-
alyzed entries after removing all non-lexical
features, since these features generally did not
match those found in the parses.
 Third, we matched the resulting transfer dictio-
nary entries with the training parse set, in order
to determine for each entry all possible part-of-
speech instantiations and dependency relation-
ships. For each distinct instantiation, we cal-
culated a log likelihood ratio.
 Finally, we created a baseline dictionary us-
ing the instantiated rules whose source patterns
had the best log likelihood ratios.
Table 4 illustrates the concurrent lexical coverage
of the training set using the resulting baseline dictio-
nary, i.e. the percentage of nodes covered by rules
whose source and target patterns both match. Note
that since the baseline dictionary contained some
noise, we allowed induced rules to override ones in
the baseline dictionary where applicable.
@KOREAN:
{po} [class=vbma] (
s1 $X [ppca={reul}] )
@ENGLISH:
look [class=verb] (
attr at [class=preposition] (
ii $X ))
@-2xLOG_LIKELIHOOD: 12.77
Figure 2: Transfer rule for English lexicalization
and preposition insertion
4 Transfer Rule Induction
The transfer rule induction process has the follow-
ing steps described below (additional details can
also be found in (Lavoie et. al., 2001)):
 Nodes of the corresponding source and target
parses are aligned using the baseline transfer
dictionary and some heuristics based on the
similarity of part-of-speech and syntactic con-
text.
 Transfer rule candidates are generated based
on the sub-patterns that contain the corre-
sponding aligned nodes in the source and target
parses.
 The transfer rule candidates are ordered based
on their likelihood ratios.
 The transfer rule candidates are filtered, one at
a time, in the order of the likelihood ratios, by
removing those rule candidates that do not pro-
duce an overall improvement in the accuracy of
the transferred parses.
Figures 2 and 3 show two sample induced rules.
The rule formalism uses notation similar to the
syntactic dependency notation shown in Figure 1,
augmented with variable arguments prefixed with
$ characters. These two lexico-structural rules can
be used to transfer a Korean syntactic representation
for ci-to-reul po-ra to an English syntactic represen-
tation for look at the map. The first rule lexicalizes
the English predicate and inserts the corresponding
preposition while the second rule inserts the English
imperative attribute.
4.1 Aligning the Parse Nodes
To align the nodes in the source and target parse
trees, we devised a new dynamic programming
@KOREAN:
$X [class=vbma ente={ra}]
@ENGLISH:
$X [class=verb mood=imp]
@-2xLOG_LIKELIHOOD: 33.37
Figure 3: Transfer rule for imperative forms
alignment algorithm that performs a top-down, bidi-
rectional beam search for the least cost mapping be-
tween these nodes. The algorithm is parameterized
by the costs of (1) aligning two nodes whose lex-
emes are not found in the baseline transfer dictio-
nary; (2) aligning two nodes with differing parts of
speech; (3) deleting or inserting a node in the source
or target tree; and (4) aligning two nodes whose rel-
ative locations differ.
To determine an appropriate part of speech cost
measure, we first extracted a small set of parse pairs
that could be reliably aligned using lexical matching
alone, and then based the cost measure on the co-
occurrence counts of the observed parts of speech
pairings. The remaining costs were set by hand.
As a result of the alignment process, alignment id
attributes (aid) are added to the nodes of the parse
pairs. Some nodes may be in alignment with no
other node, such as English prepositions not found
in the Korean DSyntS.
4.2 Generating Rule Candidates
Candidate transfer rules are generated by extracting
source and target tree sub-patterns from the aligned
parse pairs using the two set of constraints described
below.
4.2.1 Alignment constraints
Figure 4 shows an example alignment constraint.
This constraint, which matches the structural pat-
terns of the transfer rule illustrated in Figure 2, uses
the aid alignment attribute to indicate that in a Ko-
rean and English parse pair, any source and target
sub-trees matching this alignment constraint (where
$X1 and $Y1 are aligned, i.e. have the same aid at-
tribute values, and where $X2 and $Y3 are aligned)
can be used as a point of departure for generat-
ing transfer rule candidates. We suggest that align-
ment constraints such as this one can be used to de-
fine most of the possible syntactic divergences be-
tween languages (Dorr, 1994), and that only a hand-
ful of them are necessary for two given languages
(we have identified 11 general alignment constraints
@KOREAN:
$X1 [aid=$1] (
$R1 $X2 [aid=$2] )
@ENGLISH:
$Y1 [aid=$1] (
$R2 $Y2 (
$R3 $Y3 [aid=$2] ) )
Figure 4: Alignment constraint
Each node of a candidate transfer rule must have
its relation attribute (relationship with its governor)
specified if it is an internal node, otherwise this relation
must not be specified:
e.g.
 $X1 ( $R $X2 )
Figure 5: Independent attribute constraint
necessary for Korean to English transfer so far).
4.2.2 Attribute constraints
Attribute constraints are used to limit the space of
possible transfer rule candidates that can be gen-
erated from the sub-trees satisfying the alignment
constraints. Candidate transfer rules must satisfy all
of the attribute constraints. Attribute constraints can
be divided into two types:
 independent attribute constraints, whose scope
covers only one part of a candidate transfer rule
and which are the same for the source and tar-
get parts;
 concurrent attribute constraints, whose scope
extends to both the source and target parts of a
candidate transfer rule.
Figures 5 and 6 give examples of an indepen-
dent attribute constraint and of a concurrent attribute
constraint. As with the alignment constraints, we
suggest that a relatively small number of attribute
constraints is necessary to generate most of the de-
sired rules for a given language pair.
4.3 Ordering Rule Candidates
In the next step, transfer rule candidates are ordered
as follows. First, they are sorted by their decreasing
log likelihood ratios. Second, if two or more can-
didate transfer rules have the same log likelihood
ratio, ties are broken by a specificity heuristic, with
the result that more general rules are ordered ahead
In a candidate transfer rule, inclusion of the lexemes of
two aligned nodes must be done concurrently:
e.g.
$X [aid=$1]
and
$Y [aid=$1]
e.g.
 [aid=$1]
and
 [aid=$1]
Figure 6: Concurrent attribute constraint
of more specific ones. The specificity of a rule is
defined to be the following sum: the number of at-
tributes found in the source and target patterns, plus
1 for each for each lexeme attribute and for each de-
pendency relationship. In our initial experiments,
this simple heuristic has been satisfactory.
4.4 Filtering Rule Candidates
Once the candidate transfer rules have been ordered,
error-driven filtering is used to select those that yield
improvements over the baseline transfer dictionary.
The algorithm works as follows. First, in the initial-
ization step, the set of accepted transfer rules is set
to just those appearing in the baseline transfer dic-
tionary. The current error rate is also established, by
applying these transfer rules to all the source struc-
tures and calculating the overall difference between
the resulting transferred structures and the target
parses, using a tree accuracy recall and precision
measure (determined by comparing the features and
dependency relationships in the transferred parses
and corresponding target parses). Then, in a sin-
gle pass through the ordered list of candidates, each
transfer rule candidate is tested to see if it reduces
the error rate. During each iteration, the candidate
transfer rule is provisionally added to the current set
of accepted rules and the updated set is applied to
all the source structures. If the overall difference be-
tween the transferred structures and the target parses
is lower than the current error rate, then the can-
didate is accepted and the current error rate is up-
dated; otherwise, the candidate is rejected and re-
moved from the current set.
4.5 Discussion of Induced Rules
In our experiments, the alignment constraints
yielded 22,881 source and target sub-tree pairs from
the training set of 1,433 parse pairs. Using the at-
@KOREAN:
{po} [class=vbma ente={ra}] (
s1 $X [ppca={reul}] )
@ENGLISH:
look [class=verb mood=imp] (
attr at [class=preposition] (
ii $X ) )
@-2xLOG_LIKELIHOOD: 11.40
Figure 7: Transfer rule for English imperative with
lexicalization and preposition insertion
tribute constraints, an initial list of 801,674 trans-
fer rule candidates was then generated from these
sub-tree pairs. The initial list was subsequently re-
duced to 32,877 unique transfer rule candidates by
removing duplicates and by eliminating candidates
that had the same source pattern as another candi-
date with a better log likelihood ratio. After filter-
ing, 2,133 of these transfer rule candidates were ac-
cepted. We expect that the number of accepted rules
per parse pair will decrease with larger training sets,
though this remains to be verified.
The rule illustrated in Figure 3 was accepted as
the 65th best transfer rule with a log likelihood ra-
tio of 33.37, and the rule illustrated in Figure 2 was
accepted as the 189th best transfer rule candidate
with a log likelihood ratio of 12.77. An example
of a candidate transfer rule that was not accepted is
the one that combines the features of the two rules
mentioned above, illustrated in Figure 7. This trans-
fer rule candidate had a lower log likelihood ratio of
11.40; consequently, it is only considered after the
two rules mentioned above, and since it provides no
further improvement upon these two rules, it is fil-
tered out.
In an informal inspection of the top 100 accepted
transfer rules, we found that most of them appear
to be fairly general rules that would normally be
found in a general syntactic-based transfer dictio-
nary. In looking at the remaining rules, we found
that the rules tended to become increasingly corpus-
specific.
The induction results were obtained using a Java
implementation of the induction component. Mi-
crosoft SQL Server was used to count and dedupli-
cate the rule candidates. The data preparation and
induction processes took about 12 hours on a 300
MHz PC with 256 MB RAM.
5 Evaluation
5.1 Systems Compared
Babelfish As a first baseline, we used Babelfish
from Systran, a commercial large coverage MT
system supporting Korean to English transla-
tion. This system was not trained on our cor-
pus.
GIZA++/RW As a second baseline, we used an
off-the-shelf statistical MT system, consisting
of the ISI ReWrite Decoder (Germann et al,
2001) together with a translation model pro-
duced by GIZA++ (Och and Ney, 2000) and
a language model produced by the CMU Sta-
tistical Language Modeling Toolkit (Clarkson
and Rosenfeld, 1997). This system was trained
on our corpus only.
Lex Only As a third baseline, we used our system
with the baseline transfer dictionary as the sole
transfer resource.
Lex+Induced We compared the three baseline sys-
tems against our complete system, using the
baseline transfer dictionary augmented with
the induced transfer rules.
We ran each of the four systems on the test set
of 50 Korean sentences described in Section 3.2
and compared the resulting translations using the
automatic evaluation and the human evaluation de-
scribed below.
5.2 Automatic Evaluation Results
For the automatic evaluation, we used the Bleu met-
ric from IBM (Papineni et al, 2001). The Bleu
metric combines several modified N-gram precision
measures (N = 1 to 4), and uses brevity penalties to
penalize translations that are shorter than the refer-
ence sentences.
Table 5 shows the Bleu N-gram precision scores
for each of the four systems. Our system
(Lex+Induced) had better precision scores than each
of the baseline systems, except in the case of 4-
grams, where it slightly trailed Babelfish. The sta-
tistical baseline system (GIZA++/RW) performed
poorly, as might have been expected given the small
amount of training data.
Table 6 shows the Bleu overall precision scores.
Our system (Lex+Induced) improved substantially
over both the Lex Only and Babelfish baseline sys-
tems. The score for the statistical baseline system
(GIZA++/RW) is not meaningful, due to the ab-
sence of 3-gram and 4-gram matches.
System 1-g Prec 2-g Prec 3-g Prec 4-g Prec
Babelfish 0.3814 0.1207 0.0467 0.0193
GIZA++/RW 0.1894 0.0173 0.0 0.0
Lex Only 0.4234 0.1252 0.0450 0.0145
Lex+Induced 0.4725 0.1618 0.0577 0.0185
Table 5: Bleu N-gram precision scores
System Bleu Score
Babelfish 0.0802
GIZA++/RW NA
Lex Only 0.0767
Lex+Induced 0.0950
Table 6: Bleu overall precision scores
5.3 Human Evaluation Results
For the human evaluation, we asked two English
native speakers to rank the quality of the transla-
tion results produced by the Babelfish, Lex Only
and Lex+Induced, with preference given to fidelity
over fluency. (The translation results of the statisti-
cal system were not yet available when the evalua-
tion was performed.) A rank of 1 was assigned to
the best translation, a rank of two to the second best
and a rank of 3 to the third, with ties allowed.
Table 7 shows the pairwise comparisons of the
three systems. The top section indicates that the
Babelfish and Lex Only baseline systems are es-
sentially tied, with neither system preferred more
frequently than the other. In contrast, the middle
and bottom sections show that our system improves
substantially over both baseline systems; most strik-
ingly, our system (Lex+Induced) was preferred al-
most 20% more frequently than the Babelfish base-
line (46% to 27%, with ties 27% of the time).
System Pair Comparison Result
Babelfish better than Lex Only 37%
Lex Only better than Babelfish 36%
Babelfish same as Lex Only 27%
Babelfish better than Lex+Induced 27%
Lex+Induced better than Babelfish 46%
Babelfish same as Lex+Induced 27%
Lex Only better than Lex+Induced 18%
Lex+Induced better than Lex Only 41%
Lex Only same as Lex+Induced 41%
Table 7: Human evaluation results
6 Conclusion and Future Work
In this paper we have described the design of an MT
system based on lexico-structural transfer rules in-
duced from parsed bitexts. In a small scale exper-
iment with Korean to English translation, we have
demonstrated a substantial improvement over three
baseline systems, including a nearly 20% improve-
ment in the preference rate for our system over Ba-
belfish (which was not trained on our corpus). Al-
though our experimentation was aimed at Korean
to English translation, we believe that our approach
can be readily applied to other language pairs.
It remains for future work to explore how well
the approach would fare with a much larger train-
ing corpus. One foreseeable problem concerns the
treatment of lengthy training sentences: since the
number of transfer rule candidates generated grows
exponentially with the size of the parse tree pairs,
refinements will be necessary in order to make use
of complex sentences; one option might be to auto-
matically chunk longer sentences into smaller units.
Acknowledgements
We thank Richard Kittredge for helpful discussion, Daryl Mc-
Cullough and Ted Caldwell for their help with evaluation and
Fei Xia for her assistance with the automatic conversion of
the phrase structure parses to syntactic dependency represen-
tations. We also thank Chung-hye Han, Chulwoo Park, Martha
Palmer, and Joseph Rosenzweig for the handcrafted Korean-
English transfer dictionary, and Graham Russell for the corpus-
based extracted transfer dictionary. This work has been par-
tially supported by DARPA TIDES contract no. N66001-00-C-
8009.
References
Philip Clarkson and Ronald Rosenfeld. 1997. Statistical Lan-
guage Modeling Using the CMU-Cambridge Toolkit. In
Proceedings of Eurospeech?97.
Bonnie Dorr. 1994. Machine translation divergences: A for-
mal description and proposed solution. Computational Lin-
guistics, 20(4):597?635.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu
and Kenji Yamada. 2001. Fast Decoding and Optimal De-
coding for Machine Translation. In Proceedings of ACL?01,
Toulouse, France.
Chung hye Han, Benoit Lavoie, Martha Palmer, Owen
Rambow, Richard Kittredge, Tanya Korelsky, Nari Kim,
and Myunghee Kim. 2000. Handling Structural Diver-
gences and Recovering Dropped Arguments in a Korean-
English Machine Translation System. In Proceedings of the
Fourth Conference on Machine Translation in the Americas
(AMTA?00), Mision Del Sol, Mexico.
Daniel Karp and Yves Schabes. 1992. A Wide Coverage
Public Domain Morphological Analyzer for English. In
Proceedings of the Fifteenth International Conference on
Computational Linguistics (COLING?92), pages 950?955,
Nantes, France.
Benoit Lavoie and Owen Rambow. 1997. RealPro ? A Fast,
Portable Sentence Realizer. In Proceedings of the Confer-
ence on Applied Natural Language Processing (ANLP?97),
Washington, DC.
Benoit Lavoie, Richard Kittredge, Tanya Korelsky, and Owen
Rambow. 2000. A framework for MT and multilingual
NLG systems based on uniform lexico-structural process-
ing. In Proceedings of ANLP/NAACL 2000, Seattle, Wash-
ington.
Benoit Lavoie, Michael White, and Tanya Korelsky. 2001. In-
ducing Lexico-Structural Transfer Rules from Parsed Bi-
texts. In Proceedings of the ACL 2001 Workshop on
Data-driven Machine Translation, pages 17?24, Toulouse,
France.
Christopher D. Manning and Hinrich Schutze. 1999. Foun-
dations of Statistical Natural Language Processing. MIT
Press.
H. Maruyama and H. Watanabe. 1992. Tree cover Search
Algorithm for Example-Based Translation. In Proceedings
of the Fourth International Conference on Theoretical and
Methodological Issues in Machine Translation (TMI?92),
pages 173?184.
Yuji Matsumoto, Hiroyuki Hishimoto, and Takehito Utsuro.
1993. Structural Matching of Parallel Texts. In Proceedings
of the 31st Annual Meetings of the Association for Compu-
tational Linguistics (ACL?93), pages 23?30.
Igor Mel?c?uk. 1988. Dependency Syntax. State University of
New York Press, Albany, NY.
Adam Meyers, Roman Yangarber, Ralph Grishman, Catherine
Macleod, and Antonio Moreno-Sandoval. 1998. Deriving
Transfer Rules from Dominance-Preserving Alignments. In
Proceedings of COLING-ACL?98, pages 843?847.
Makoto Nagao. 1984. A framework of a mechanical transla-
tion between Japenese and English by analogy principle. In
A. Elithorn and R. Banerji, editors, Artificial and Human
Intelligence. NATO Publications.
Alexis Nasr, Owen Rambow, Martha Palmer, and Joseph
Rosenzweig. 1997. Enriching lexical transfer with cross-
linguistic semantic features. In Proceedings of the Interlin-
gua Workshop at the MT Summit, San Diego, California.
Franz Josef Och and Hermann Ney. 2000. Improved Statistical
Alignment Models. In Proceedings of ACL?00, pages 440?
447, Hongkong, China.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2001. Bleu: a Method for Automatic Evaluation of
Machine Translation. In IBM technical report, RC22176.
Stephen D. Richardson, William B. Dolan, Arul Menezes, and
Monica Corston-Oliver. 2001. Overcoming the Customiza-
tion Bottleneck using Example-based MT. In Proceedings
of the ACL 2001 Workshop on Data-driven Machine Trans-
lation, pages 9?16, Toulouse, France.
Satoshi Sato and Makoto Nagao. 1990. Toward Memory-
based Translation. In Proceedings of the 13th International
Conference on Computational Linguistics (COLING?90),
pages 247?252.
Fei Xia and Martha Palmer. 2001. Converting Dependency
Structures to Phrase Structures. In Notes of the First Human
Language Technology Conference, San Diego, California.
Juntae Yoon, Seonho Kim, and Mansuk Song. 1997. New
parsing method using global association table. In Proceed-
ings of the 5th International Workshop on Parsing Technol-
ogy.
Question Answering Using Ontological Semantics 
Stephen BEALE, Benoit LAVOIE, Marjorie MCSHANE, Sergei NIRENBURG,Tanya KORELSKY 
 
Institute for Language and Information 
Technologies (ILIT-UMBC) 
1000 Hilltop Circle 
Baltimore, MD, USA 21250 
{sbeale,marge,sergei}@umbc.edu 
CoGenTex, Inc. 
840 Hanshaw Rd, Suite 1 
Ithaca, NY, USA, 14850 
{benoit,tanya}@cogentext.com 
 
Abstract 
This paper describes the initial results of an 
experiment in integrating knowledge-based 
text processing with real-world reasoning in a 
question answering system. Our MOQA 
?meaning-oriented question answering? 
system seeks answers to questions not in open 
text but rather in a structured fact repository 
whose elements are instances of ontological 
concepts extracted from the text meaning 
representations (TMRs) produced by the 
OntoSem text analyzer. The query 
interpretation and answer content formulation 
modules of MOQA use the same knowledge 
representation substrate and the same static 
knowledge resources as the ontological 
semantic (OntoSem) semantic text analyzer. 
The same analyzer is used for deriving the 
meaning of questions and of texts from which 
the fact repository content is extracted. 
Inference processes in question answering rely 
on ontological scripts (complex events) that 
also support reasoning for purely NLP-related 
purposes, such as ambiguity resolution in its 
many guises. 
1 The Task 
People would have no problem answering 
questions like Has Tony Hall met with Umid 
Medhat Mubarak? ? provided they know who 
these two people are and have witnessed such a 
meeting or read about it. Even in the absence of 
overt evidence about such a meeting, people might 
conclude ? based on additional knowledge they 
might have about the protagonists ? that such a 
meeting could or might have taken place. Some 
current automatic question-answering (QA) 
systems might be able to answer such a question if 
they found a sentence like Tony Hall met with 
(saw, talked with) Umid Medhat Mubarak on July 
3, 2003 in Baghdad  in some text. But what if the 
text data was more typical, like, for instance, the 
following two excerpts: 
  
April 18, 2000  Associated Press. Representative 
Tony Hall, a Democrat from Ohio, arrived in 
Baghdad on a four-day visit to investigate the 
plight of Iraqi people under sanctions aimed at 
forcing the government of Iraq to give up its 
weapons of mass destruction? 
 
Umid Medhat Mubarak returned to Baghdad on 
April 17, 2000 after a visit to Jordan and plans to 
meet with a visiting US politician. 
 
To the best of our knowledge, no current system 
can input the above texts and return a reasoned 
response about the likelihood of a meeting between 
Tony Hall and Mubarak. But in a realistic 
environment there are even further complications. 
What if the first text was in English and the second 
in Arabic? Will the system be able to make even a 
tentative connection between Tony Hall (in the 
first text) and US politician (in the second)? What 
if the reference to US politician was omitted; i.e. if 
the second text contained only the information that 
Umid Medhat Mubarak was in Baghdad on April 
17, 2000? The system would have to infer the 
possibility of a meeting on the basis of knowledge 
about (at least) the social and professional 
background of the protagonists and the times 
involved. 
This paper describes a system that is able to 
make connections and inferences such as the 
above. Its most important properties are question 
answering against structured data stored in a fact 
repository (FR) and the fact that it uses the same 
processing machinery and knowledge resources a) 
to process texts for conversion into facts, b) to 
understand questions and c) to find answers to 
questions. We describe the underlying technology 
that supports such a capability, including the 
production of text meaning representations 
(TMRs), reference and date resolution, fact 
extraction and retrieval, and event scripts that 
allow us to infer (with some degree of probability) 
certain events or states not directly stated in any 
text. 
2 The Environment for QA 
Our question answering system consists of four 
main and one auxiliary processing modules (see 
Figure 1). The question analysis module takes as 
input the text of a user?s question and produces its 
text meaning representation (TMR, see below 
for an illustration) that contains representations of 
instances of ontological concepts to which the 
input refers plus speaker-attitude and 
communicative information. The TMR is input to 
the question interpretation module that interprets 
the question in terms of its type and transforms it 
into a formal query against the fact repository or 
the ontology (see below). (Note that the format of 
the database query is the same as that of the 
question TMR. In general, all internal 
representations of knowledge in our system, both 
elements of knowledge support and results of 
actual processing, are compatible with the content 
and format of the ontology and fact repository.)  
a
p
c
t
m
t
f
a
k
t
c
w
i
C
s
W
m
g
a
p
q
i
implementation simply returns fragments of facts 
(and fact reasoning chains) that answer the initial 
question. In the future, natural language generation 
will be employed to produce textual responses. 
In order to answer complex questions in context, 
a system must extract, manipulate and generate the 
meaning of natural language texts. Question 
answering against a structured knowledge base, 
especially when the latter contains interpretable 
knowledge elements (e.g., instances of events and 
objects defined in an ontology, not uninterpreted 
text strings), can attain better results than QA that 
works by manipulating templates filled with 
snippets of actual texts ? at the least because of the 
added benefit of disambiguation and reference 
resolution. The prerequisite for such a system is 
the existence of a structured knowledge base used 
as a source of answers to questions. In a real 
application, the knowledge must be ample and 
dynamic, so that the knowledge resources must be 
constantly and promptly augmented. This is not 
practical if knowledge is acquired entirely by 
people. Automating structured knowledge 
acquisition from open text is, therefore, a 
necessary condition for the success of an advanced 
QA application. The CSK module of our system is Figure 1. The top-level architecture of the 
system. 
Thus, text meaning representation in our 
pproach ?doubles? as the basis for reasoning 
rocesses. The query serves as input to answer 
ontent determination. This latter module uses 
he knowledge resources of the system to infer the 
ost preferred answer, once again, formulated in 
he TMR metalanguage. If an answer cannot be 
ound, the system has the option to call the 
uxiliary module for creating structured 
nowledge, CSK. The CSK module works also in 
he background mode, using text sources to 
ontinuously update the fact repository (in the 
ork reported here there has been some human 
nvolvement in the process of TMR production for 
SK; we plan to study the degradation of the 
ystem when used in a fully automatic mode). 
hen called by the answer content determination 
odule, the CSK module analyzes texts and 
enerates entries in the fact repository that help to 
nswer the original question. The text analysis 
rocess in this module is the same as that used in 
uestion analysis. The final module in the system 
s answer formulation. The current 
a step toward this functionality, albeit not yet in a 
fully automatic way. At this point, we rely on 
TMRs that are obtained automatically but 
improved through human interaction (see 
Nirenburg et al 2004 for details). Note that fully 
automatic methods for creating structured 
knowledge of a quality even remotely approaching 
that needed to support realistic QA do not at this 
point exist. Few of the numerous current and 
recent machine learning and statistical processing 
experiments in NLP deal with the analysis of 
meaning at all; and those that do address partial 
tasks (e.g., determining case role fillers in terms of 
undisambiguated text elements in Gildea and 
Jurafsky 2002) in a rather ?knowledge-lean? 
manner. The results are very far away indeed from 
either good quality or good coverage, either in 
terms of phenomena and text. We believe that our 
approach, using as it does statistical as well as 
recorded-knowledge evidence for extracting, 
representing and manipulating meaning is the most 
practical and holds the most promise for the future. 
Indeed, it is not even as expensive as many people 
believe. 
3 The Knowledge Support Infrastructure 
The process of deriving TMRs from text is 
implemented in our Ontosem text analyzer. 
Semantic analysis in OntoSem is described in 
some detail in Nirenburg and Raskin 2004; 
Nirenburg et al, 2004; Beale et al 1995, 1996, 
2003; Mahesh et al 1997. Our description here 
will be necessarily brief.  Also note that the 
analysis process is described here as if it were a 
strict pipeline architecture; in reality, semantic 
analysis is used to inform and disambiguate 
syntactic analysis, for example, in cases of 
prepositional phrase attachment.  
Text analysis in OntoSem relies on the results of 
a battery of pre-semantic text processing modules. 
The preprocessor module deals with mark-up in 
the input text, finds boundaries of sentences and 
words, recognizes dates, numbers, named entities 
a l 
a
g
t
e
l
s
o
c
a
c
o
(
t
c
d
d
i
a
m
a
produced in OntoSem using a variety of 
?microtheories,? to produce extended TMRs. At 
both steps, the analyzer has to deal with ambiguity, 
incongruity between the input and the expectations 
recorded in the static knowledge sources (SKSs), 
unknown words, and non-literal language. In a 
recent evaluation, the basic analyzer was shown to 
carry out word sense disambiguation at over 90% 
and semantic dependency determination at 87% on 
the basis of correct syntactic analysis and on 
sentences of an average length of over 25 words 
with 1.33 unknown words on average per input 
sentence (see Nirenburg et al, 2004). While not nd acronyms and performs morphologicanalysis. Once the morphological analyzer has 
enerated the citation forms for word forms in a 
ext, the system can activate the relevant lexical 
ntries in its lexicons, including the onomasticon (a 
exicon of proper names). Figure 2 presents a 
ample of preprocessor output.  
Figure 2: Sample preprocessor output 
Figure 3: Sample parser output, in graphical
The task of syntactic analysis (see Figure 3) in 
ntological semantics is, essentially, to determine 
lause-level dependency structures for an input text 
nd assign grammatical categories to clause 
onstituents (that is, establish subjects, direct 
bjects, oblique objects and adjuncts). 
 
Semantic analysis proper uses the information 
mutual constraints) in the active lexicon entries, 
he ontology and the results of earlier processing to 
arry out, at the first step, word sense 
isambiguation and establish basic semantic 
ependencies in the text. The results are recorded 
n basic TMRs (see below). At the next step, the 
nalyzer determines the values of the various 
odalities, aspect, time, speech acts, speaker 
ttitudes and other knowledge elements that are 
perfect, these results show promise as training data 
for machine learning work.  
The OntoSem ontology provides a 
metalanguage for describing the meaning of the 
lexical units in a language as well as for the 
specification of meaning encoded in TMRs. The 
ontology contains specifications of 
concepts corresponding to classes of 
things and events in the world. It is a 
collection of frames, or named sets of 
property-value pairs, organized into a 
hierarchy with multiple inheritance. The 
expressive power of the ontology and the TMR is 
enhanced by multivalued fillers for properties, 
implemented using the ?facets? DEFAULT, SEM, 
VALUE, and RELAXABLE-TO, among others. At the 
time of this writing, the ontology contains about 
6,000 concepts (events, objects and properties), 
with, on average, 16 properties each. Temporally 
and causally related events are encoded as values 
of a complex event?s HAS-EVENT-AS-PART 
property. These are essentially scripts that provide 
information that is very useful in general reasoning 
as well as reasoning for NLP (e.g., Schank and 
Abelson 1977, Lin and Hovy 2000, Clark and 
Porter 2000). We use scripts in the answer content 
determination module of the question answering 
system. Figure 4 illustrates a rather simple script 
that supports reasoning for our example question 
answering session.  
     The OntoSem lexicon contains not only 
semantic information, it also supports 
morphological and syntactic analysis. 
Semantically, it specifies what concept, concepts, 
property or properties of concepts defined in the 
ontology must be instantiated in the TMR to 
account for the meaning of a given lexical unit of 
input. At the time of writing, the latest version of 
the English semantic lexicon includes over 12,000 
handcrafted entries. These entries cover some of 
the most complex lexical material in the language 
? ?closed-class? grammatical lexemes such as 
conjunctions, prepositions, pronouns, auxiliary and 
modal verbs, etc. as well as about 3,000 of the 
For lack of space, we will not be able to discuss 
all the representational and descriptive devices 
used in the lexicon or the variety of ways in which 
semantic information in the lexicon and the 
ontology can interact. See Nirenburg and Raskin 
(2004, Chapters 7 and 8) for a discussion.  
MEET-WITH     
  (AGENT (VALUE $VAR1))     
  (THEME (VALUE $VAR2))     
  (LOCATION (VALUE $VAR3))     
  (TIME(VALUE $VAR4))     
 
 PRECONDITIONS     
  (AND         
  (LOCATION           
    (DOMAIN (VALUE $VAR1))           
    (RANGE (VALUE $VAR3))           
    (TIME (VALUE $VAR4)))        
  (LOCATION          
    (DOMAIN (VALUE $VAR2))         
    (RANGE (VALUE $VAR3))          
    (TIME (VALUE $VAR4))))     
 
 EFFECTS      
 (SPEECH-ACT        
   (AGENT (VALUE $VAR1))          
   (BENEFICIARY (VALUE $VAR2)))       
 (SPEECH-ACT        
   (AGENT (VALUE $VAR2))       
   (BENEFICIARY (VALUE $VAR1))) 
 
 
COME   
  (AGENT (VALUE $VAR1))   
  (DESTINATION (VALUE $VAR2))     
 
 EFFECTS     
 (LOCATION     
    (DOMAIN (VALUE $VAR1))      
   (RANGE (VALUE $VAR2))) 
 
 
LOCATION  
  (DOMAIN (VALUE $VAR1))  
  (RANGE (VALUE $VAR2))    
 
 EFFECT-OF     
 (COME       
   (AGENT (VALUE $VAR1))       
   (DESTINATION (VALUE $VAR2))) 
 
Figure 4: A sample script,  
presented in a simplified  
presentation format. 
The English onomasticon (lexicon of proper 
names) currently contains over 350,000 entries 
semantically linked to ontological concepts; it is 
increasing in size daily by means of semi-
automated knowledge-extraction methods. 
The TMR (automatically generated but shown 
here in a simplified presentation format) for a short 
sentence (He asked the UN to authorize the 
war) from a recently processed text about Colin 
Powell is presented below. The numbers associated 
with the ontological concepts indicate instances of 
those concepts: e.g., REQUEST-ACTION-69 means 
the 69th time that the concept REQUEST-ACTION has 
been instantiated in the world model used for, and 
extended during, the processing of this text or 
corpus.  
 
REQUEST-ACTION-69  
    AGENT   HUMAN-72  
    THEME   ACCEPT-70  
    BENEFICIARY   ORGANIZATION-71  
    SOURCE-ROOT-WORD  ask  
    TIME     (< (FIND-ANCHOR-TIME))  
ACCEPT-70  
   THEME   WAR-73  
   THEME-OF   REQUEST-ACTION-69  
   SOURCE-ROOT-WORD   authorize 
ORGANIZATION-71  
   HAS-NAME   UNITED-NATIONS 
   BENEFICIARY-OF     REQUEST-ACTION-69  
   SOURCE-ROOT-WORD  UN 
HUMAN-72  
   HAS-NAME  COLIN-POWELL 
   AGENT-OF   REQUEST-ACTION-69  
   SOURCE-ROOT-WORD   he ; ref. resolution done 
WAR-73  
   THEME-OF     ACCEPT-70  
    SOURCE-ROOT-WORD  war  
most frequent main verbs. We illustrate the 
structure of the lexicon entry on the example of the 
first verbal sense of alert: 
 
alert-v1      
  cat    v   
  morph  regular     
  ex     "He alerted us to the danger" The above says that there is a REQUEST-ACTION 
event whose agent is HUMAN-72 (Colin Powell), 
whose beneficiary is ORGANIZATION-71 (United 
Nations) and whose THEME is an ACCEPT event. 
That ACCEPT event, in turn, has the THEME WAR-
73. Note that the concept ACCEPT is not the same 
as the English word accept: its human-oriented 
definition in the ontology as ?To agree to carry out 
an action, fulfill a request, etc?, which fits well 
here.  
  syn-struc 
    subject   $var1          
 root    "alert" 
     indirectobject  $var2  
     pp   (opt +) 
    root  "to"  
  object $var3          
  sem-struc 
     WARN      
    agent   ^$var1   
      beneficiary  ^$var2 
      theme   ^$var3 
The Fact Repository contains a list of 
remembered instances of ontological concepts. For 
 
ex the 
co ies 
for
on
rep
dif
ins
ea
typ
are
un
en
4 
de
mo
Int
Fig
ex
qu
an
an
The TMR of the question, the result of the 
Question Analysis module, is displayed in the ample, whereas the ontology contains 
ncept CITY, the fact repository contains entr
 London, Paris and Rome; and whereas the 
tology contains the concept WAR, the fact 
ository contains the entry WWII. The main 
ference between an ontological concept and its 
tance is the nature of the fillers of properties for 
ch. In the former, the fillers of properties are, 
ically, overridable constraints; in the latter, they 
 actual values (when known), or they are left 
filled when not known. A simple fact repository 
try is illustrated below: 
 
HUMAN-33599 
 NAME George W. Bush  
 ALIAS  
 George Bush,  
Figure 5: Querying about a known person.
  President Bush,  
 George W,  
 the president of the United States,  
 the US president 
 SOCIAL-ROLE  PRESIDENT 
 GENDER        male 
 NATIONALITY        NATION-213 ;(USA)  
 DATE-OF-BIRTH July 6, 1946 
 spouse  human-33966 ;Laura Bush 
The Question Answering Modules 
Referring back to Figure 1, we now will briefly 
scribe the three central question answering 
dules of Question Analysis, Question 
erpretation and Answer Content Determination.  
ure 5 shows a question answering session that 
emplifies these three stages. The user enters a 
estion in the ?Natural Language Query? text box 
d clicks on the ?Submit? button. The OntoSem 
alyzer is then invoked to analyze the question. 
Query Analysis Details box. Obviously, not many 
of the details can be seen in these figures, but in 
the interface, the user can scroll through the TMR 
output. We will, in fact, be integrating our existing 
TMR and Fact graphical browsers into this 
interface in the near future. The results of the next 
module, Question Interpretation, are then displayed 
in the Query Paraphrase box. From there, the fact 
repository is queried, an answer is returned 
(perhaps utilizing the inference techniques to be 
described), and the supporting fact (or fact 
reasoning chain) is displayed in the Answer Details 
box. Below we present three example sessions, the 
third of which will be the basis for the more 
detailed description of the three modules. 
For this discussion, we concentrate on the 
following three sentences that have been processed 
by the CSK module and the facts that were derived 
from them and stored in the fact repository (the 
facts are represented using instances of ontological 
concepts, of course): 
 
1. Tony Hall Met with Umid Medhat Mubarak. 
2. Tony Hall arrived in Baghdad (on Thursday). 
3. Ali Atwa was in Baghdad (on April 18 and 
later). 
 
In Figure 5, the question is ?Who is Tony Hall?? 
In its current state, the system simply finds and 
responds with the stored fact about Tony Hall, 
which includes the information about his arrival 
(the COME-1003 event derived from sentence 2) 
and the MEET-WITH-1001 event (derived from 
sentence 1). Figure 6 shows the results of the 
query, ?Did Tony Hall meet with Umid Medhat 
Mubarak?? A matching 
fact was found in the FR 
(derived from sentence 
1) and is displayed in the 
Answer Details. Figure 7 
presents a more complex 
case. The query is ?Did 
Tony Hall meet with Ali 
Atwa?? The FR contains 
no fact that can directly 
answer this question. 
The system uses facts 
from sentences 2 and 3 
to return the possible 
inference MEET-WITH-
??? (the tentative nature 
of the inference is 
marked by the -??? 
appended as the instance 
number; and in the 
Figure 6: Querying about a known event. 
future, we will also generate a numerical value 
reflecting the confidence level of the inferences 
and sub-inferences). The supporting reasoning 
chain is also displayed. We will now use this 
example to discuss the three main question 
answering modules. 
The Question Analysis module takes the input 
text question and produces the TMR using the 
resources described in section 3 above. Figure 8 
illustrates the resulting TMR in a graphical 
browser. (One can inspect the content of the 
various concept instances ? e.g., OBJECT-56 ? by 
clicking on graphical objects representing them). 
The main thing to point out is that the TMR 
instantiates a MEET-WITH event which is the basis 
for querying the FR, itself comprised of facts 
represented by ontological concept instances. 
The Question Interpretation module then 
derives the canonical form of an 
input question TMR (determining 
question type and reformulating 
the content of an actual question 
in a standard format), and applies 
reference resolution. The answer 
displayed in Figure 7 involves 
reformulating the query Did $X 
meet with $Y? to Find meetings 
involving $X and $Y or more 
particularly, Find meetings where 
a person named $X is AGENT and 
a person named $Y is 
BENEFICIARY, and meetings 
where a person named $Y is 
AGENT and a person named $X is 
BENEFICIARY. Such a query can 
be specified as a standard DBMS 
query for the actual search. The knowledge that we 
are dealing with people and knowledge of their 
names is used to help resolve the references 
between the instances Tony Hall 
and Ali Atwa appearing in the 
query and the references to Tony 
Hall and Ali Atwa that may be 
stored in the fact repository. We 
will report on the actual methods 
of question interpretation and 
reference resolution we use 
separately.  
    The Answer Content 
Determination module is 
invoked next. The possible 
queries constructed in the 
previous module are processed. 
First, direct queries are attempted. 
If an answer is found, it is returned directly. In this 
example, no fact directly states that Tony Hall met 
Ali Atwa. Scripts are then activated which allow us 
to reason about the question. In the script of Figure 
4, the preconditions of a MEET-WITH event include 
both participants having to be in the same place at 
the same time. This will invoke a series of queries 
that will determine if Tony Hall and Ali Atwa were 
indeed in the same location at the same time. In 
general, if the preconditions of an event are 
satisfied, we infer that the event itself possibly 
took place. In this case, the fact that Ali Atwa was 
in Baghdad is present in the FR by virtue of 
sentence 3 above. Using this knowledge, the 
system seeks to prove that Tony Hall was also in 
Baghdad at the same time. Once again, there is no 
direct fact that states this. However, the facts about 
Tony Hall include information that he arrived in 
Baghdad at a certain time (at the present time, we 
do not attempt to match the times of the facts, 
although this will be a focus of our ongoing work). 
Figure 7: Querying about an unknown 
event. 
Matching times ofThis information is represented 
by the COME-1003 fact. We can look up COME in 
the script of Figure 4 and see that one effect of a 
COME event is that the agent?s location becomes 
the destination of the COME event. In general, we 
can use known facts to infer additional facts about 
their effects. In this case, we can infer that Tony 
Hall was, in fact, in Baghdad, which, in turn, 
allows us to make the top level inference that he 
might have met with Ali Atwa, who we previously 
determined was also in Baghdad. We are aware 
that the conjecture about the possible meeting 
should involve additional knowledge of the 
background and histories of the participants (e.g., 
if a cobbler and a head of state are in the same 
p a 
p g 
o e 
o
r
i
T
i
?
?
?
e
p
e
direct the inference process ? even we are fully 
aware of the abductive, defeasible nature of this 
knowledge.  
The inference steps described above were 
directed by the information in the MEET-WITH and 
COME scripts. Also, known facts about one of the 
participants, Tony Hall, were used to direct queries 
to support a possible inference. Obviously, much 
work remains to be done. We must populate a large 
fact repository that should include a large 
collection of facts about individuals as well as 
places, organizations and event instances. At this 
time, we are starting to use our TMR production 
environment for extracting facts. We hope to be 
able to report on the progress of this work at the 
workshop.  
5 Conclusion 
We have presented the first experiment with a 
knowledge-based QA system in which text 
processing is integrated with reasoning on the basis 
of shared knowledge and processing 
infrastructures. Indeed, the same processing and 
knowledge resources in the system carry out 
reasoning for the purposes of QA and reasoning 
that is necessary to create a high-quality 
unambiguous text meaning representation itself. 
While this is just a small experiment, we have 
specific and, we believe, realistic plans for scaling 
this system up ? through automatic population of 
the fact repository, semi-automatic enlargement of 
the lexicons and the ontology and expansion of the 
inventory of scripts.  
Figure 8. Sample output viewed through the 
TMR browser 
We believe that integrating a comprehensive 
throughput system for an advanced application, 
even one in which some of the modules are still on 
a relatively small scale, is a very important kind of 
work in our field. It tackles real problems head on, lace at the same time, that does not imply 
otential meeting between them). We are workin
n enhancing the knowledge (centered on th
ntological MEET-WITH script) to improve such 
eckoning.  
In a separate article in preparation, we will go 
nto much more detail about the reasoning process. 
here are obviously many additional issues, 
ncluding: 
 events. Our time resolution meaning 
procedures enable this; 
 Assigning probabilities to inferences. For 
example, if two people were in the same room, 
the possibility of their meeting is much higher 
than if they were in the same country; 
 Controlling the inference process. 
 
With regard to this last issue, the OntoSem 
nvironment provides a useful mechanism. In 
articular, the scripts that we are developing 
ncode expectations and are meant to constrain and 
without resorting to a rather defeatist ? though 
quite common in today?s NLP ? claim that certain 
goals are infeasible.  
References  
S. Beale, S. Nirenburg and K. Mahesh. 1995. 
Semantic analysis in the Mikrokosmos machine 
translation project. In Proceedings of the 2nd 
Symposium on Natural Language Processing, 
Kaset Sart University, Bangkok, Thailand.   
S. Beale, S. Nirenburg and K. Mahesh. 1996. 
Hunter-Gatherer: Three search techniques 
integrated for natural language semantics. In 
Proceedings of the 13th National Conference on 
Artificial Intelligence. Portland, OR.  
S. Beale, S. Nirenburg and M. McShane. 
2003. Just-in-time grammar. In Proceedings 
HLT-NAACL-2003, Edmonton, Canada. 
P. Clark and B. Porter. 2000. $RESTAURANT Ren-
visited: A KM implementation of a compositional 
approach. Technical Report, AI Lab, University 
of Texas at Austin.  
D. Gildea and D. Jurafsky. 2002. Automatic 
labeling of semantic roles. Computational 
Linguistics 28(3). 245-288.  
C. Lin and E. H. Hovy. 2000. The automated 
acquisition of topic signatures for text 
summarization. In Proceedings of the COLING 
Workshop on Text Summarization. Strasbourg, 
France. 
K. Mahesh, S. Nirenburg and S. Beale. 1997. If 
you have it, flaunt it: Using full ontological 
knowledge for word sense disambiguation. In 
Proceedings of Theoretical and Methodological 
Issues in Machine Translation (TMI-97). Santa 
Fe, NM. 
S. Nirenburg and V. Raskin. 2004. Ontological 
Semantics.  MIT Press. 
S. Nirenburg, M. McShane and S. Beale.  2004.  
Evaluating the performance of OntoSem.  In 
Proceedings ACL Workshop on Text Meaning 
and Interpretation, Barcelona. 
R. Schank and R. Abelson. 1977. Scripts, plans, 
goals, and understanding. Hillsdale, NJ: 
Erlbaum. 
 
