Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412?418,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Language Independent Connectivity Strength Features
for Phrase Pivot Statistical Machine Translation
Ahmed El Kholy, Nizar Habash
Center for Computational Learning Systems, Columbia University
{akholy,habash}@ccls.columbia.edu
Gregor Leusch, Evgeny Matusov
Science Applications International Corporation
{gregor.leusch,evgeny.matusov}@saic.com
Hassan Sawaf
eBay Inc.
hsawaf@ebay.com
Abstract
An important challenge to statistical ma-
chine translation (SMT) is the lack of par-
allel data for many language pairs. One
common solution is to pivot through a
third language for which there exist par-
allel corpora with the source and target
languages. Although pivoting is a robust
technique, it introduces some low quality
translations. In this paper, we present two
language-independent features to improve
the quality of phrase-pivot based SMT.
The features, source connectivity strength
and target connectivity strength reflect the
quality of projected alignments between
the source and target phrases in the pivot
phrase table. We show positive results (0.6
BLEU points) on Persian-Arabic SMT as
a case study.
1 Introduction
One of the main issues in statistical machine trans-
lation (SMT) is the scarcity of parallel data for
many language pairs especially when the source
and target languages are morphologically rich. A
common SMT solution to the lack of parallel data
is to pivot the translation through a third language
(called pivot or bridge language) for which there
exist abundant parallel corpora with the source
and target languages. The literature covers many
pivoting techniques. One of the best performing
techniques, phrase pivoting (Utiyama and Isahara,
2007), builds an induced new phrase table between
the source and target. One of the main issues of
this technique is that the size of the newly cre-
ated pivot phrase table is very large (Utiyama and
Isahara, 2007). Moreover, many of the produced
phrase pairs are of low quality which affects the
translation choices during decoding and the over-
all translation quality. In this paper, we introduce
language independent features to determine the
quality of the pivot phrase pairs between source
and target. We show positive results (0.6 BLEU
points) on Persian-Arabic SMT.
Next, we briefly discuss some related work. We
then review two common pivoting strategies and
how we use them in Section 3. This is followed by
our approach to using connectivity strength fea-
tures in Section 4. We present our experimental
results in Section 5.
2 Related Work
Many researchers have investigated the use of piv-
oting (or bridging) approaches to solve the data
scarcity issue (Utiyama and Isahara, 2007; Wu and
Wang, 2009; Khalilov et al, 2008; Bertoldi et al,
2008; Habash and Hu, 2009). The main idea is to
introduce a pivot language, for which there exist
large source-pivot and pivot-target bilingual cor-
pora. Pivoting has been explored for closely re-
lated languages (Hajic? et al, 2000) as well as un-
related languages (Koehn et al, 2009; Habash and
Hu, 2009). Many different pivot strategies have
been presented in the literature. The following
three are perhaps the most common.
The first strategy is the sentence translation
technique in which we first translate the source
sentence to the pivot language, and then translate
the pivot language sentence to the target language
412
(Khalilov et al, 2008).
The second strategy is based on phrase pivot-
ing (Utiyama and Isahara, 2007; Cohn and Lap-
ata, 2007; Wu and Wang, 2009). In phrase pivot-
ing, a new source-target phrase table (translation
model) is induced from source-pivot and pivot-
target phrase tables. Lexical weights and transla-
tion probabilities are computed from the two trans-
lation models.
The third strategy is to create a synthetic source-
target corpus by translating the pivot side of
source-pivot corpus to the target language using an
existing pivot-target model (Bertoldi et al, 2008).
In this paper, we build on the phrase pivoting
approach, which has been shown to be the best
with comparable settings (Utiyama and Isahara,
2007). We extend phrase table scores with two
other features that are language independent.
Since both Persian and Arabic are morphologi-
cally rich, we should mention that there has been
a lot of work on translation to and from morpho-
logically rich languages (Yeniterzi and Oflazer,
2010; Elming and Habash, 2009; El Kholy and
Habash, 2010a; Habash and Sadat, 2006; Kathol
and Zheng, 2008). Most of these efforts are fo-
cused on syntactic and morphological processing
to improve the quality of translation.
To our knowledge, there hasn?t been a lot of
work on Persian and Arabic as a language pair.
The only effort that we are aware of is based
on improving the reordering models for Persian-
Arabic SMT (Matusov and Ko?pru?, 2010).
3 Pivoting Strategies
In this section, we review the two pivoting strate-
gies that are our baselines. We also discuss how
we overcome the large expansion of source-to-
target phrase pairs in the process of creating a
pivot phrase table.
3.1 Sentence Pivoting
In sentence pivoting, English is used as an inter-
face between two separate phrase-based MT sys-
tems; Persian-English direct system and English-
Arabic direct system. Given a Persian sentence,
we first translate the Persian sentence from Per-
sian to English, and then from English to Arabic.
3.2 Phrase Pivoting
In phrase pivoting (sometimes called triangulation
or phrase table multiplication), we train a Persian-
to-Arabic and an English-Arabic translation mod-
els, such as those used in the sentence pivoting
technique. Based on these two models, we induce
a new Persian-Arabic translation model.
Since we build our models on top of Moses
phrase-based SMT (Koehn et al, 2007), we need
to provide the same set of phrase translation prob-
ability distributions.1 We follow Utiyama and Isa-
hara (2007) in computing the probability distribu-
tions. The following are the set of equations used
to compute the lexical probabilities (?) and the
phrase probabilities (pw)
?(f |a) =?
e
?(f |e)?(e|a)
?(a|f) =?
e
?(a|e)?(e|f)
pw(f |a) =
?
e
pw(f |e)pw(e|a)
pw(a|f) =
?
e
pw(a|e)pw(e|f)
where f is the Persian source phrase. e is
the English pivot phrase that is common in both
Persian-English translation model and English-
Arabic translation model. a is the Arabic target
phrase.
We also build a Persian-Arabic reordering table
using the same technique but we compute the re-
ordering weights in a similar manner to Henriquez
et al (2010).
As discussed earlier, the induced Persian-
Arabic phrase and reordering tables are very large.
Table 1 shows the amount of parallel corpora
used to train the Persian-English and the English-
Arabic and the equivalent phrase table sizes com-
pared to the induced Persian-Arabic phrase table.2
We introduce a basic filtering technique dis-
cussed next to address this issue and present some
baseline experiments to test its performance in
Section 5.3.
3.3 Filtering for Phrase Pivoting
The main idea of the filtering process is to select
the top [n] English candidate phrases for each Per-
sian phrase from the Persian-English phrase ta-
ble and similarly select the top [n] Arabic target
phrases for each English phrase from the English-
Arabic phrase table and then perform the pivot-
ing process described earlier to create a pivoted
1Four different phrase translation scores are computed in
Moses? phrase tables: two lexical weighting scores and two
phrase translation probabilities.
2The size of the induced phrase table size is computed but
not created.
413
Training Corpora Phrase Table
Translation Model Size # Phrase Pairs Size
Persian-English ?4M words 96,04,103 1.1GB
English-Arabic ?60M words 111,702,225 14GB
Pivot Persian-Arabic N/A 39,199,269,195 ?2.5TB
Table 1: Translation Models Phrase Table comparison in terms of number of line and sizes.
Persian-Arabic phrase table. To select the top can-
didates, we first rank all the candidates based on
the log linear scores computed from the phrase
translation probabilities and lexical weights mul-
tiplied by the optimized decoding weights then we
pick the top [n] pairs.
We compare the different pivoting strategies
and various filtering thresholds in Section 5.3.
4 Approach
One of the main challenges in phrase pivoting is
the very large size of the induced phrase table.
It becomes even more challenging if either the
source or target language is morphologically rich.
The number of translation candidates (fanout) in-
creases due to ambiguity and richness (discussed
in more details in Section 5.2) which in return
increases the number of combinations between
source and target phrases. Since the only criteria
of matching between the source and target phrase
is through a pivot phrase, many of the induced
phrase pairs are of low quality. These phrase pairs
unnecessarily increase the search space and hurt
the overall quality of translation.
To solve this problem, we introduce two
language-independent features which are added to
the log linear space of features in order to deter-
mine the quality of the pivot phrase pairs. We call
these features connectivity strength features.
Connectivity Strength Features provide two
scores, Source Connectivity Strength (SCS) and
Target Connectivity Strength (TCS). These two
scores are similar to precision and recall metrics.
They depend on the number of alignment links be-
tween words in the source phrase to words of the
target phrase. SCS and TSC are defined in equa-
tions 1 and 2 where S = {i : 1 ? i ? S} is the
set of source words in a given phrase pair in the
pivot phrase table and T = {j : 1 ? j ? T}
is the set of the equivalent target words. The
word alignment between S and T is defined as
A = {(i, j) : i ? S and j ? T }.
SCS = |A||S| (1)
TCS = |A||T | (2)
We get the alignment links by projecting the
alignments of source-pivot to the pivot-target
phrase pairs used in pivoting. If the source-target
phrase pair are connected through more than one
pivot phrase, we take the union of the alignments.
In contrast to the aggregated values represented
in the lexical weights and the phrase probabilities,
connectivity strength features provide additional
information by counting the actual links between
the source and target phrases. They provide an
independent and direct approach to measure how
good or bad a given phrase pair are connected.
Figure 1 and 2 are two examples (one good, one
bad) Persian-Arabic phrase pairs in a pivot phrase
table induced by pivoting through English.3 In the
first example, each Persian word is aligned to an
Arabic word. The meaning is preserved in both
phrases which is reflected in the SCS and TCS
scores. In the second example, only one Persian
word in aligned to one Arabic word in the equiv-
alent phrase and the two phrases conveys two dif-
ferent meanings. The English phrase is not a good
translation for either, which leads to this bad pair-
ing. This is reflected in the SCS and TCS scores.
5 Experiments
In this section, we present a set of baseline ex-
periments including a simple filtering technique to
overcome the huge expansion of the pivot phrase
table. Then we present our results in using connec-
tivity strength features to improve Persian-Arabic
pivot translation quality.
3We use the Habash-Soudi-Buckwalter Arabic transliter-
ation (Habash et al, 2007) in the figures with extensions for
Persian as suggested by Habash (2010).
414
Persian: "A?tmAd"myAn"dw"k?wr " " "? ?"??"()"?"?%$#"?,-. ?"" " " " " " " " " " " " "?trust"between"the"two"countries?"English: "trust"between"the"two"countries"
Arabic:" "Al?q?"byn"Aldwltyn " " " "? /012?52?2$3"34"? ?"" " " " " " " " " " " " "?the"trust"between"the"two"countries?"
Figure 1: An example of strongly connected Persian-Arabic phrase pair through English. All Persian
words are connected to one or more Arabic words. SCS=1.0 and TCS=1.0.
Persian: "AyjAd"cnd"?rkt"m?trk " " " "? 0/.+?",+*(")'&"?$#"? ?"" " " " " " " " " " " " "?Establish"few"joint"companies?"English: "joint"ventures"
Arabic:" "b?D"?rkAt"AlmqAwlAt"fy"Albld" "? 123"?<=>&";:"?89"?6?",+5"? ?"" " " " " " " " " " " " "?Some"construcBon"companies"in"the"country?"
Figure 2: An example of weakly connected Persian-Arabic phrase pairs through English. Only one
Persian word is connected to an Arabic word. SCS=0.25 and TCS=0.2.
5.1 Experimental Setup
In our pivoting experiments, we build two SMT
models. One model to translate from Persian to
English and another model to translate from En-
glish to Arabic. The English-Arabic parallel cor-
pus is about 2.8M sentences (?60M words) avail-
able from LDC4 and GALE5 constrained data. We
use an in-house Persian-English parallel corpus of
about 170K sentences and 4M words.
Word alignment is done using GIZA++ (Och
and Ney, 2003). For Arabic language model-
ing, we use 200M words from the Arabic Giga-
word Corpus (Graff, 2007) together with the Ara-
bic side of our training data. We use 5-grams
for all language models (LMs) implemented us-
ing the SRILM toolkit (Stolcke, 2002). For En-
glish language modeling, we use English Giga-
word Corpus with 5-gram LM using the KenLM
toolkit (Heafield, 2011).
All experiments are conducted using the Moses
phrase-based SMT system (Koehn et al, 2007).
We use MERT (Och, 2003) for decoding weight
4LDC Catalog IDs: LDC2005E83, LDC2006E24,
LDC2006E34, LDC2006E85, LDC2006E92, LDC2006G05,
LDC2007E06, LDC2007E101, LDC2007E103,
LDC2007E46, LDC2007E86, LDC2008E40, LDC2008E56,
LDC2008G05, LDC2009E16, LDC2009G01.
5Global Autonomous Language Exploitation, or GALE,
is a DARPA-funded research project.
optimization. For Persian-English translation
model, weights are optimized using a set 1000 sen-
tences randomly sampled from the parallel cor-
pus while the English-Arabic translation model
weights are optimized using a set of 500 sen-
tences from the 2004 NIST MT evaluation test
set (MT04). The optimized weights are used for
ranking and filtering (discussed in Section 3.3).
We use a maximum phrase length of size 8
across all models. We report results on an in-
house Persian-Arabic evaluation set of 536 sen-
tences with three references. We evaluate using
BLEU-4 (Papineni et al, 2002) and METEOR
(Lavie and Agarwal, 2007).
5.2 Linguistic Preprocessing
In this section we present our motivation and
choice for preprocessing Arabic, Persian, English
data. Both Arabic and Persian are morphologi-
cally complex languages but they belong to two
different language families. They both express
richness and linguistic complexities in different
ways.
One aspect of Arabic?s complexity is its vari-
ous attachable clitics and numerous morphologi-
cal features (Habash, 2010). We follow El
Kholy and Habash (2010a) and use the PATB to-
kenization scheme (Maamouri et al, 2004) in our
415
experiments. We use MADA v3.1 (Habash and
Rambow, 2005; Habash et al, 2009) to tokenize
the Arabic text. We only evaluate on detokenized
and orthographically correct (enriched) output fol-
lowing the work of El Kholy and Habash (2010b).
Persian on the other hand has a relatively sim-
ple nominal system. There is no case system and
words do not inflect with gender except for a few
animate Arabic loanwords. Unlike Arabic, Persian
shows only two values for number, just singular
and plural (no dual), which are usually marked by
either the suffix A?+ +hA and sometimes 	?@+ +An,
or one of the Arabic plural markers. Verbal mor-
phology is very complex in Persian. Each verb
has a past and present root and many verbs have
attached prefix that is regarded part of the root.
A verb in Persian inflects for 14 different tense,
mood, aspect, person, number and voice combina-
tion values (Rasooli et al, 2013). We use Perstem
(Jadidinejad et al, 2010) for segmenting Persian
text.
English, our pivot language, is quite different
from both Arabic and Persian. English is poor
in morphology and barely inflects for number and
tense, and for person in a limited context. English
preprocessing simply includes down-casing, sepa-
rating punctuation and splitting off ??s?.
5.3 Baseline Evaluation
We compare the performance of sentence pivot-
ing against phrase pivoting with different filtering
thresholds. The results are presented in Table 2. In
general, the phrase pivoting outperforms the sen-
tence pivoting even when we use a small filtering
threshold of size 100. Moreover, the higher the
threshold the better the performance but with a di-
minishing gain.
Pivot Scheme BLEU METEOR
Sentence Pivoting 19.2 36.4
Phrase Pivot F100 19.4 37.4
Phrase Pivot F500 20.1 38.1
Phrase Pivot F1K 20.5 38.6
Table 2: Sentence pivoting versus phrase pivoting
with different filtering thresholds (100/500/1000).
We use the best performing setup across the rest
of the experiments.
5.4 Connectivity Strength Features
Evaluation
In this experiment, we test the performance of
adding the connectivity strength features (+Conn)
to the best performing phrase pivoting model
(Phrase Pivot F1K).
Model BLEU METEOR
Sentence Pivoting 19.2 36.4
Phrase Pivot F1K 20.5 38.6
Phrase Pivot F1K+Conn 21.1 38.9
Table 3: Connectivity strength features experi-
ment result.
The results in Table 3 show that we get a
nice improvement of ?0.6/0.5 (BLEU/METEOR)
points by adding the connectivity strength fea-
tures. The differences in BLEU scores between
this setup and all other systems are statistically
significant above the 95% level. Statistical signif-
icance is computed using paired bootstrap resam-
pling (Koehn, 2004).
6 Conclusion and Future Work
We presented an experiment showing the effect of
using two language independent features, source
connectivity score and target connectivity score,
to improve the quality of pivot-based SMT. We
showed that these features help improving the
overall translation quality. In the future, we plan
to explore other features, e.g., the number of the
pivot phases used in connecting the source and tar-
get phrase pair and the similarity between these
pivot phrases. We also plan to explore language
specific features which could be extracted from
some seed parallel data, e.g., syntactic and mor-
phological compatibility of the source and target
phrase pairs.
Acknowledgments
The work presented in this paper was possible
thanks to a generous research grant from Science
Applications International Corporation (SAIC).
The last author (Sawaf) contributed to the effort
while he was at SAIC. We would like to thank M.
Sadegh Rasooli and Jon Dehdari for helpful dis-
cussions and insights into Persian. We also thank
the anonymous reviewers for their insightful com-
ments.
416
References
Nicola Bertoldi, Madalina Barbaiani, Marcello Fed-
erico, and Roldano Cattoni. 2008. Phrase-based
statistical machine translation with pivot languages.
Proceeding of IWSLT, pages 143?149.
Trevor Cohn and Mirella Lapata. 2007. Ma-
chine translation by triangulation: Making ef-
fective use of multi-parallel corpora. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 728.
Ahmed El Kholy and Nizar Habash. 2010a. Ortho-
graphic and Morphological Processing for English-
Arabic Statistical Machine Translation. In Proceed-
ings of Traitement Automatique du Langage Naturel
(TALN-10). Montre?al, Canada.
Ahmed El Kholy and Nizar Habash. 2010b. Tech-
niques for Arabic Morphological Detokenization
and Orthographic Denormalization. In Proceed-
ings of the seventh International Conference on Lan-
guage Resources and Evaluation (LREC), Valletta,
Malta.
Jakob Elming and Nizar Habash. 2009. Syntactic
Reordering for English-Arabic Phrase-Based Ma-
chine Translation. In Proceedings of the EACL 2009
Workshop on Computational Approaches to Semitic
Languages, pages 69?77, Athens, Greece, March.
David Graff. 2007. Arabic Gigaword 3, LDC Cat-
alog No.: LDC2003T40. Linguistic Data Consor-
tium, University of Pennsylvania.
Nizar Habash and Jun Hu. 2009. Improving Arabic-
Chinese Statistical Machine Translation using En-
glish as Pivot Language. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 173?181, Athens, Greece, March.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphologi-
cal Disambiguation in One Fell Swoop. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL?05), pages 573?
580, Ann Arbor, Michigan.
Nizar Habash and Fatiha Sadat. 2006. Arabic Pre-
processing Schemes for Statistical Machine Transla-
tion. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Companion Vol-
ume: Short Papers, pages 49?52, New York City,
USA.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den
Bosch and A. Soudi, editors, Arabic Computa-
tional Morphology: Knowledge-based and Empiri-
cal Methods. Springer.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Khalid
Choukri and Bente Maegaard, editors, Proceedings
of the Second International Conference on Arabic
Language Resources and Tools. The MEDAR Con-
sortium, April.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Jan Hajic?, Jan Hric, and Vladislav Kubon. 2000. Ma-
chine Translation of Very Close Languages. In Pro-
ceedings of the 6th Applied Natural Language Pro-
cessing Conference (ANLP?2000), pages 7?12, Seat-
tle.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, UK.
Carlos Henriquez, Rafael E. Banchs, and Jose? B.
Marin?o. 2010. Learning reordering models for sta-
tistical machine translation with a pivot language.
Amir Hossein Jadidinejad, Fariborz Mahmoudi, and
Jon Dehdari. 2010. Evaluation of PerStem: a sim-
ple and efficient stemming algorithm for Persian. In
Multilingual Information Access Evaluation I. Text
Retrieval Experiments, pages 98?101.
Andreas Kathol and Jing Zheng. 2008. Strategies for
building a Farsi-English smt system from limited re-
sources. In Proceedings of the 9th Annual Confer-
ence of the International Speech Communication As-
sociation (INTERSPEECH2008), pages 2731?2734,
Brisbane, Australia.
M. Khalilov, Marta R. Costa-juss, Jos A. R. Fonollosa,
Rafael E. Banchs, B. Chen, M. Zhang, A. Aw, H. Li,
Jos B. Mario, Adolfo Hernndez, and Carlos A. Hen-
rquez Q. 2008. The talp & i2r smt systems for iwslt
2008. In International Workshop on Spoken Lan-
guage Translation. IWSLT 2008, pg. 116?123.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-
pher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Christopher Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public.
Philipp Koehn, Alexandra Birch, and Ralf Steinberger.
2009. 462 machine translation systems for europe.
Proceedings of MT Summit XII, pages 65?72.
Philipp Koehn. 2004. Statistical significance tests for-
machine translation evaluation. In Proceedings of
the Empirical Methods in Natural Language Pro-
cessing Conference (EMNLP?04), Barcelona, Spain.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: An
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, pages 228?231, Prague, Czech Repub-
lic.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus.
417
In NEMLAR Conference on Arabic Language Re-
sources and Tools, pages 102?109, Cairo, Egypt.
Evgeny Matusov and Selc?uk Ko?pru?. 2010. Improv-
ing reordering in statistical machine translation from
farsi. In AMTA The Ninth Conference of the Associ-
ation for Machine Translation in the Americas, Den-
ver, Colorado, USA.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?52.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160?167. As-
sociation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?318,
Philadelphia, PA.
Mohammad Sadegh Rasooli, Manouchehr Kouhestani,
and Amirsaeid Moloodi. 2013. Development of
a Persian syntactic dependency treebank. In The
2013 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies (NAACL HLT), At-
lanta, USA.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), volume 2, pages 901?904, Denver,
CO.
Masao Utiyama and Hitoshi Isahara. 2007. A com-
parison of pivot methods for phrase-based statistical
machine translation. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
484?491, Rochester, New York, April. Association
for Computational Linguistics.
Hua Wu and Haifeng Wang. 2009. Revisiting pivot
language approach for machine translation. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 154?162, Suntec, Singapore, August.
Association for Computational Linguistics.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based sta-
tistical machine translation from english to turkish.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 454?
464, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
418
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 237?249,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Filtering Antonymous, Trend-Contrasting, and Polarity-Dissimilar
Distributional Paraphrases for Improving Statistical Machine Translation
Yuval Marton
T.J. Watson Research Center
IBM
yymarton@us.ibm.com
Ahmed El Kholy and Nizar Habash
Center for Computational Learning Systems
Columbia University
{akholy,habash}@ccls.columbia.edu
Abstract
Paraphrases are useful for statistical machine
translation (SMT) and natural language pro-
cessing tasks. Distributional paraphrase gen-
eration is independent of parallel texts and
syntactic parses, and hence is suitable also
for resource-poor languages, but tends to erro-
neously rank antonyms, trend-contrasting, and
polarity-dissimilar candidates as good para-
phrases. We present here a novel method
for improving distributional paraphrasing by
filtering out such candidates. We evalu-
ate it in simulated low and mid-resourced
SMT tasks, translating from English to two
quite different languages. We show statisti-
cally significant gains in English-to-Chinese
translation quality, up to 1 BLEU from non-
filtered paraphrase-augmented models (1.6
BLEU from baseline). We also show that
yielding gains in translation to Arabic, a mor-
phologically rich language, is not straightfor-
ward.
1 Introduction
Paraphrase recognition and generation has proven
useful for various natural language processing
(NLP) tasks, including statistical machine transla-
tion (SMT), information retrieval, query expansion,
document summarization, and natural language gen-
eration. We concentrate here on phrase-level (as
opposed to sentence-level) paraphrasing for SMT.
Paraphrasing is useful for SMT as it increases trans-
lation coverage ? a persistent problem, even in large-
scale systems. Two common approaches are ?pivot?
and distributional paraphrasing. Pivot paraphrasing
translates phrases of interest to other languages and
back (Callison-Burch et al, 2006; Callison-Burch,
2008). It relies on parallel texts (or translation
phrase tables) in various languages, which are typ-
ically scarce, and hence limit its applicability. Dis-
tributional paraphrasing (Marton et al, 2009) gener-
ates paraphrases using a distributional semantic dis-
tance measure computed over a large monolingual
corpus.1 Monolingual corpora are relatively easy
and inexpensive to collect, but distributional seman-
tic distance measures are known to rank antonymous
and polarity-dissimilar phrasal candidates high. We
therefore attempt to identify and filter out such ill-
suited paraphrase candidates.
A phrase pair may have a varying degree of
antonymy, beyond the better-known complete op-
posites (hot / cold) and contradictions (did / did
not), e.g., weaker contrasts (hot / cool), contrast-
ing trends (covered / reduced coverage), or senti-
ment polarity (happy / sad). Information extrac-
tion, opinion mining and sentiment analysis litera-
ture has been grappling with identifying such pairs
(Pang and Lee, 2008), e.g., in order to distinguish
positive and negative reviews or comments, or to de-
tect contradictions (Marneffe et al, 2008; Voorhees,
2008). We transfer some of the insights, data and
techniques to the area of paraphrasing and SMT. We
distributionally expand a small seed set of antonyms
in an unsupervised manner, following Mohammad
et al (2008). We then present a method for fil-
tering antonymous and polarity-dissimilar distribu-
tional paraphrases using the expanded antonymous
list and a list of negators (e.g., cannot) and trend-
decreasing words (reduced). We evaluate the im-
pact of our approach in a SMT setting, where non-
1Other variants use a lexical resource in conjunction with
the monolingual corpus (Mirkin et al, 2009; Marton, 2010).
237
baseline translation models are augmented with dis-
tributional paraphrases. We show gains of up to
1 BLEU relative to non-filtered models (1.6 BLEU
from non-augmented baselines) in English-Chinese
models trained on small and medium-large size data,
but lower to no gains in English-Arabic. The small
training size simulates resource-poor languages.
The rest of this paper is organized as follows:
We describe distributional paraphrase generation in
Section 2, antonym discovery in Section 3, and
paraphrase-augmented SMT in Section 4. We then
report experimental results in Section 5, and discuss
the implications in Section 6. We survey related
work in Section 7, and conclude with future work
in Section 8.
2 Distributional Paraphrases
Our method improves on the method presented in
Marton et al (2009). Using a non-annotated mono-
lingual corpus, our method constructs distributional
profiles (DP; a.k.a. context vectors) of focal words
or phrases. Each DPphr is a vector containing log-
likelihood ratios of the focal phrase phr and each
word w in the corpus. Given a paraphrase candidate
phrase cand, the semantic distance between phr and
cand is calculated using the cosine of their respec-
tive DPs (McDonald, 2000). For details on DPs and
distributional measures, see Weeds et al (2004) and
Turney and Pantel (2010).
The search of the corpus for paraphrase candi-
dates is performed in the following manner:
1. For each focal phrase phr, build distributional
profile DPphr.
2. Gather contexts: for each occurrence of phr,
keep surrounding (left and right) context L R.
3. For each such context, gather paraphrase can-
didates cand which occur between L and R in
other locations in the training corpus, i.e., all
cand such that L cand R occur in the corpus.
4. For each candidate cand, build a profile
DPcand and measure profile similarity between
DPcand and DPphr.
5. Rank all cand according to the profile similar-
ity score.
6. Filter out every candidate cand that textually
entails phr: This is approximated by filtering
cand if its words all appear in phr in the same
order. For example, if phr is spoken softly, then
spoken very softly would be filtered out.
7. Filter out every candidate cand that is antony-
mous to phr (See Algorithm 1 below).
8. Output k-best remaining candidates above a
certain similarity score threshold t.
Most of the steps above are similar to, and have
been elaborated in, Marton et al (2009). Due to
space limitations, we concentrate on the main novel
element here, which is the antonym filtering step,
detailed below. Antonyms (largely speaking) are op-
posites, terms that contrast in meaning, such as hot /
cold. Negators are terms such as not and lost, which
often flip the meaning of the word or phrase that fol-
lows or contains them, e.g., confidence / lost confi-
dence. Details on obtaining their definitions and on
obtaining the antonymous pair list and the negator
list are given in Section 3.
Algorithm 1 Antonymous candidate filtering
Given an antonymous pair list, a negator list, and a
phrase-paraphrase candidate (phr-cand) pair list,
for all phr-cand pairs do
for all words w in phr do
if w is also in cand, and there is a negator up
to two words before it in either phr or cand
(but not both!) then
filter out this pair
if w, ant is an antonymous pair, and ant is
in cand, and there is no negator up to two
words before w and ant, or there is such a
negator before both then
filter out this pair
3 Antonyms, Trends, Sentiment Polarity
Native speakers of a language are good at deter-
mining whether two words are antonyms (hot?cold,
ascend?descend, friend?foe) or not (penguin?clown,
cold?chilly, boat?rudder) (Cruse, 1986; Lehrer and
Lehrer, 1982; Deese, 1965). Strict antonyms apart,
there are also many word pairs that exhibit some de-
gree of contrast in meaning, for example, lukewarm?
cold, ascend?slip, and fan?enemy (Mohammad et
al., 2008). Automatically identifying such con-
trasting word pairs has many uses including detect-
ing and generating paraphrases (The lion caught
the gazel / The gazel could not escape the lion)
238
and detecting contradictions (Marneffe et al, 2008;
Voorhees, 2008) (The inhabitants of Peru are well
off / the inhabitants of Peru are poor). Of course,
such ?contradictions? may be a result of differing
sentiment, new information, non-coreferent men-
tions, or genuinely contradictory statements. Iden-
tifying paraphrases and contradictions are in turn
useful in effectively re-ranking target language hy-
potheses in machine translation, and for re-ranking
query responses in information retrieval. Identifying
contrasting word pairs (or short phrase pairs) is also
useful for detecting humor (Mihalcea and Strappar-
ava, 2005), as satire and jokes tend to have contra-
dictions and oxymorons. Lastly, it is useful to know
which words contrast a focal word, even if only to
filter them out. For example, in the automatic cre-
ation of a thesaurus it is necessary to distinguish
near-synonyms from contrasting word pairs. Distri-
butional similarity measures typically fail to do so.
Instances of strong contrast are recorded to some
extent in manually created dictionaries, but hun-
dreds of thousands of other contrasting pairs are not.
Further, antonyms can be of many kinds such as
those described in Section 3.1 below. We use the
Mohammad et al (2008) method to automatically
generate a large list of contrasting word pairs, which
are used to identify false paraphrases. Their method
is briefly described in Section 3.2.
3.1 Kinds of antonyms
Antonyms can be classified into different kinds.
A detailed description of one such classification can
be found in Cruse (1986) (Chapters 9, 10, and 11),
where the author describes complementaries (open?
shut, dead?alive), gradable adjective pairs (long?
short, slow?fast) (further classified into polar, over-
lapping, and equipollent antonyms), directional op-
posites (up?down, north?south), (further classified
into antipodals, counterparts, and reversives), re-
lational opposites (husband?wife, predator?prey),
indirect converses (give?receive, buy?pay), con-
gruence variants (huge?little, doctor?patient), and
pseudo opposites (black?white). It should be
noted, however, that even though contrasting word
pairs and antonyms have long been studied by
linguists, lexicographers, and others, experts do
not always agree on the scope of antonymy and
the kinds of contrasting word pairs. Some lex-
ical relations have also received attention at the
Educational Testing Services (ETS). They clas-
sify antonyms into contradictories (alive?dead,
masculine?feminine), contraries (old?young, happy-
sad), reverses (attack?defend, buy?sell), direction-
als (front?back, left?right), incompatibles (happy?
morbid, frank?hypocritical), asymmetric contraries
(hot?cool, dry?moist), pseudoantonyms (popular?
shy, right?bad), and defectives (default?payment,
limp?walk) (Bejar et al, 1991).
As mentioned earlier, in addition to antonyms,
there are other meaning-contrasting phenomena, or
other ways to classify them, such as contrasting
trends and sentiment polarity. They all may have
varying degrees of contrast in meaning. Hereafter
we sometime broadly refer to all of these as antony-
mous phrases. The antonymous phrase pair genera-
tion algorithm that we use here does not employ any
antonym-subclass-specific techniques.
3.2 Detecting antonyms
Mohammad et al (2008) used a Roget-like the-
saurus, co-occurrence statistics, and a seed set of
antonyms to identify the degree of antonymy be-
tween two words, and generate a list of antony-
mous words. The thesaurus divides the vocabulary
into about a thousand coarse categories. Each cat-
egory has, on average, about a hundred closely re-
lated words. (A word with more than one sense,
is listed in more than one category.) Mohammad
et al first determine pairs of thesaurus categories
that are contrasting in meaning. A category pair
is said to be contrasting if it has a seed antonym
pair. A list of seed antonyms is compiled using 16
affix patterns such as X and unX (clear?unclear)
and X and disX (honest?dishonest). Once a con-
trasting category pair is identified, all the word pairs
across the two categories are considered to have con-
trasting meaning. The strength of co-occurrence
(as measured by pointwise mutual information) be-
tween two contrasting word pairs is taken to be the
degree of antonymy. This is based on the distri-
butional hypothesis of antonyms, which states that
antonymous pairs tend to co-occur in text more of-
ten than chance. Co-occurrence counts are made
from the British National Corpus (BNC) (Burnard,
2000). The approach attains more than 80% accu-
racy on GRE-style closest opposite questions.
239
3.3 Detecting negators
The General Inquirer (GI) (Stone et al, 1966) has
11,788 words labeled with 182 categories of word
tags, such as positive and negative semantic orien-
tation, pleasure, pain, and so on.2 Two of the GI
categories, NOTLW and DECREAS, contain terms
that negate the meaning of what follows (Choi and
Cardie, 2008; Kennedy and Inkpen, 2005). These
terms (with limited added inflection variation) form
our list of negators.
4 Paraphrase-Augmented SMT
Augmenting the source side of SMT phrase tables
with paraphrases of out-of-vocabulary (OOV) items
was introduced by Callison-Burch et al (2006),
and was adopted practically ?as-is? in consequent
work (Callison-Burch, 2008; Marton et al, 2009;
Marton, 2010). Given an OOV source-side phrase
f , if the translation model has a rule ?f ?, e? whose
source side is a paraphrase f ? of f , then a new rule
?f, e? is added, with an extra weighted log-linear
feature, whose value for the new rule is the similar-
ity score between f and f ? (computed as a function
of the pivot translation probabilities or the distribu-
tional semantic distance of the respective DPs). We
follow the same line here:
h(e, f) =
?
???????
???????
asim(DPf ? , If phrase table entry (e, f)
DPf ) is generated from (e, f ?)
using monolingually-
derived paraphrases.
1 Otherwise.
(1)
where the definition of asim is repeated below. As
noted in that previous work, it is possible to con-
struct a new translation rule from f to e via more
than one pair of source-side phrase and its para-
phrase; e.g., if f1 is a paraphrase of f , and so is f2,
and both f1, f2 translate to the same e, then both lead
to the construction of the new rule translating f to e,
but with potentially different feature scores. In order
to leverage on these paths and resolve feature value
conflicts, an aggregated similarity measure was ap-
plied: For each paraphrase f of source-side phrases
2http://www.wjh.harvard.edu/?inquirer
fi with similarity scores sim(fi, f),
asimi = asimi?1+(1?asimi?1) sim(fi, f) (2)
where asim0 = 0. We only augment the phrase
table with a single rule from f to e, and in it are the
feature values of the phrase fi for which sim(fi, f)
was the highest.
5 Experiment
5.1 System and Parameters
We augmented translation models with para-
phrases based on distributional semantic distance
measures, with our novel antonym-filtering, and
without it. We tested all models in English-
to-Chinese and English-to-Arabic translation, aug-
menting the models with translation rules for un-
known English phrases. We also contrasted these
models with non-augmented baseline models.
For baseline we used the phrase-based SMT sys-
tem Moses (Koehn et al, 2007), with the default
model features: 1. phrase translation probability,
2. reverse phrase translation probability, 3. lexical
translation probability, 4. reverse lexical translation
probability, 5. word penalty, 6. phrase penalty, 7. six
lexicalized reordering features, 8. distortion cost,
and 9. language model (LM) probability. We used
Giza++ (Och and Ney, 2000) for word alignment.
All features were weighted in a log-linear frame-
work (Och and Ney, 2002). Feature weights were
set with minimum error rate training (Och, 2003) on
a tuning set using BLEU (Papineni et al, 2002) as the
objective function. Test results were evaluated using
BLEU and TER (Snover et al, 2006): The higher
the BLEU score, the better the result; the lower the
TER score, the better the result. This is denoted
with BLEU? and TER? in Table 1. Statistical signif-
icance of model output differences was determined
using Koehn (2004)?s test on the objective function
(BLEU).
The paraphrase-augmented models were created
as described in Section 4. We used the same data
and parameter settings as in Marton (2010).3 We
used cosine distance over DPs of log-likelihood ra-
tios (McDonald, 2000), built with a sliding win-
3Data preprocessing and paraphrasing code slightly differ
from those used in Marton et al (2009) and Marton (2010), and
hence scores are not exactly the same across these publications.
240
dow of size ?6, a sampling threshold of 10000 oc-
currences, and a maximal paraphrase length of 6
tokens. We applied a paraphrase score threshold
t = 0.05; a dynamic context length (the short-
est non-stoplisted left context L occurring less than
512 times in the corpus, and similarly for R); para-
phrasing of OOV unigrams; filtering paraphrase can-
didates occurring less than 25 times in the corpus
(inspired by McDonald, 2000); and allowing up to
k = 100 best paraphrases per phrase. We tuned
the weights of each model (non-augmented base-
line, unigram-augmented, and unigram-augmented-
filtered) with a separate minimum error rate training.
We explored here augmenting OOV unigrams,
although our paraphrasing and antonym filtering
methods can be applied to longer n-grams with no
further modifications. However, preliminary experi-
ments showed that longer n-grams require additional
provisions in order to yield gains.
5.2 Data
In order to take advantage of the English antonym
resource, we chose English as the source language
for the translation task. We chose Chinese as
the translation target language in order to compare
with Marton (2010), and for the same reasons it was
chosen there: It is quite different from English (e.g.,
in word order), and four reference translation were
available from NIST. We chose Arabic as another
target language, because it is different from both
English and Chinese, and richer morphologically,
which introduces additional challenges.
English-Chinese: For training we used the
LDC Sinorama and FBIS tests (LDC2005T10 and
LDC2003E14), and segmented the Chinese side
with the Stanford Segmenter (Tseng et al, 2005).
After tokenization and filtering, this bitext contained
231,586 lines (6.4M + 5.1M tokens). We trained a
trigram language model on the Chinese side, with
the SRILM toolkit (Stolcke, 2002), using the mod-
ified Kneser-Ney smoothing option. We followed
the split in Marton (2010), and constructed the re-
duced set of about 29,000 sentence pairs. The pur-
pose of creating this subset model was to simulate a
resource-poor language. We trained separate trans-
lation models, using either the subset or the full-size
training dataset.
For weight tuning we used the Chinese-English
NIST MT 2005 evaluation set. In order to use it for
the reverse translation direction (English-Chinese),
we arbitrarily chose the first English reference set
as the tuning ?source?, and the Chinese source as a
single ?reference translation?. For testing we used
the English-Chinese NIST MT evaluation 2008 test
set with its four reference translations.
English-Arabic: We use an English-Arabic par-
allel corpus of about 135k sentences (4 million
words) and a subset of 30K sentences (one mil-
lion words) for the translation models? training data.
The sentences were extracted from Arabic News
(LDC2004T17), eTIRR (LDC2004E72), English
translation of Arabic Treebank (LDC2005E46),
and Ummah (LDC2004T18).4 For Arabic pre-
processing, we follow previously reported best to-
kenization scheme (TB)5 and orthographic word
normalization condition (Reduced) when translat-
ing from English to Arabic (El Kholy and Habash,
2010b). MADA (Habash and Rambow, 2005) is
used to pre-process the Arabic text for the translation
model and 5-gram language model (LM). As a post-
processing step, we jointly denormalize and deto-
kenize the text to produce the final Arabic output.
Following El Kholy and Habash (2010a), we use
their best detokenization technique, T+R+LM. The
technique crucially utilizes a lookup table (T), map-
ping tokenized forms to detokenized forms, based
on our MADA-fied LM. Alternatives are given con-
ditional probabilities, P (detokenized|tokenized).
Tokenized words absent from the tables are deto-
kenized using deterministic rules (R), as a backoff
strategy. We use a 5-gram untokenized LM and
the disambig utility in the SRILM toolkit to de-
cide among different alternatives. Word alignment
is done using GIZA++, as in English-Chinese sys-
tem. We use lemma-based alignment which consis-
tently yields superior results to surface-based align-
ment (El Kholy and Habash, 2010b). For LM, we
use 200M words from the Arabic Gigaword Corpus
(LDC2007T40) together with the Arabic side of our
training data.
All experiments were conducted using Moses
here as well. We used a maximum phrase length
4All are available from the Linguistic Data Consortium
(LDC) http://www.ldc.upenn.edu
5TB: Penn Arabic Tree Bank tokenization scheme
241
of size 8 tokens. Weight optimization was done us-
ing a set of 300 sentences from the NIST MT 2004
Arabic-English evaluation test set (MT04). The tun-
ing was based on tokenized Arabic without detok-
enization. Testing was done on the NIST Arabic-
English MT05 and MEDAR 2010 English-Arabic
four-reference evaluation sets. For both tuning on
MT04 and testing on MT05, since we need the re-
verse English-Arabic direction, we chose one En-
glish reference translation as the ?source?, and the
Arabic as a single ?reference?. We evaluated using
BLEU and TER here too.
English paraphrases: We augmented the base-
line models with paraphrases generated as described
above, using a monolingual text of over 516M to-
kens, consisting of the BNC and English Gigaword
documents from 2004 and 2008 (LDC2009T13),
pre-processed to remove punctuation and to conflate
numbers, dates, months, days of week, and alphanu-
meric tokens to their respective classes.
5.3 Results
English-Chinese: Results are given in Table 1.
Augmenting SMT phrase tables with paraphrases of
OOV unigrams resulted in gains of 0.6-0.7 BLEU
points for both subset and full models, but TER
scores were worse (higher) for the full model. Aug-
menting same models with same paraphrases filtered
for antonyms resulted in further gains of 1.6 and 1
BLEU points for both subset and full models, respec-
tively, relative to the respective baselines. The TER
scores of the antonym filtered models were also as
good or better (lower) than those of the baselines.
reduced size large size
model BLEU? TER? BLEU? TER?
baseline 15.8 69.2 21.8 63.8
aug-1gram 16.4B 68.9 22.5B 64.4
aug-1gram-ant-filt 17.4BD 68.7 22.8BD 63.7
Table 1: English-Chinese scores. B/D = statistically significant
w.r.t. (B)aseline or (D)istributional 1gram model, using Koehn
(2004)?s statistical significance.
English-Arabic: Results are given in columns 1-7
of Table 2. On the MT05 test set, the 135k-sentence
aug-1gram model outperformed its baseline in both
BLEU and TER scores. The lemmatized variants
of the scores showed higher or same gains. Since
only one entry was antonym-filtered here, we do
not provide separate scores for aug-1gram-ant-filt.
Surprisingly, for the reduced 30k models, all scores
(BLEU, TER, and even their lemmatized variants) of
the augmented 1gram model were somewhat worse
than the baseline?s, and those of the antonym-filtered
model were the worst. we also ran a 4-reference test
(Medar) to see whether the single MT05 reference
was problematic, but results were similar. We exam-
ine possible reasons for this in the next section.
6 Discussion
Filtering quality: Our filtering technique is based
on antonymous pair and negator lists that were ex-
panded distributionally from seed sets. Therefore,
they are noisy. From a small random sample (Ta-
ble 3) it seems that only about 10% of filtered cases
should not have been filtered; of the rest, 50% were
strongly antonymous, 25% mildly so, and 15% were
siblings (co-hypernyms) in a natural categorical hi-
erarchy or otherwise noisy paraphrases filtered due
to a noisy antonym pair. Negators in the unigrams?
paraphrase candidates were rare.
English-Chinese: Our paraphrase filtering tech-
nique yielded an additional 1 BLEU point gain
over the non-filtered paraphrase-augmented reduced
model (totaling 1.6 BLEU over baseline). The re-
duced and large augmented models? phrase table
size increased by about 27% and 4%, respectively ?
and antonym filtering did not change these numbers
by much (see left side of Table 4). Therefore, the dif-
ference in performance between the filtered and non-
filtered systems is unlikely to be quantitative (phrase
table size). The out of vocabulary (OOV) rate of the
29k subset model is somewhat high (see Table 4),
especially for the test set; but only after these exper-
iments were completed did we peek at the test set
for calculating these statistics, and in any case, we
should not be guided by such information in choos-
ing the test set. At first glance it may seem surpris-
ing that only 0.4% of the paraphrase candidates of
the English OOV unigrams (248 candidates) were
filtered by our procedure, and that it accounted for
as much as 1 BLEU in the reduced set. (For English-
Arabic only 0.6%, or 23 candidates, were filtered).
Leaving the estimation of antonymous phrase detec-
tion recall for the future, we note that these num-
242
BLEU Lemm. Brev. Ref/Sys TER Lemm. Unigram Lemma Match Analysis
? BLEU penal. ratio ? TER Exact Match Lemma-only Unmatchable Total
30k-sentence (1M word) training dataset models
MT05 baseline 23.6 31.3 99.2 1.008 57.6 47.3 15614 55.4% 4055 14.4% 8550 30.3% 28219
aug-1gram 23.2 30.8 99.9 1.001 58.8 48.4 15387 54.2% 4195 14.8% 8831 31.1% 28413
aug-1gram-ant-filt 23.2 30.8 99.9 1.001 58.8 48.3 15387 54.2% 4195 14.8% 8831 31.1% 28413
MEDAR baseline 13.6 18.7 93.6 1.066 67.6 61.3 4924 53.0% 1563 16.8% 2800 30.1% 9287
aug-1gram 12.9 18.3 94.2 1.060 68.9 62.3 4894 52.0% 1710 18.2% 2815 29.9% 9419
aug-1gram-ant-filt 12.9 18.3 94.2 1.060 69.0 62.3 4891 51.9% 1715 18.2% 2815 29.9% 9421
135k-sentence (4M word) training dataset models
MT05 baseline 25.8 33.5 99.2 1.008 55.7 45.3 16115 57.1% 3999 14.2% 8128 28.8% 28242
aug-1gram 26.4 34.3B 99.5 1.005 55.1 44.7 16156 57.1% 4068 14.4% 8089 28.6% 28313
aug-1gram-ant-filt 26.4 34.3B 99.5 1.005 55.0 44.6 16153 57.1% 4090 14.5% 8068 28.5% 28311
MEDAR baseline 17.1 23.1 94.7 1.054 65.1 58.6 5483 57.7% 1577 16.6% 2438 25.7% 9498
aug-1gram 17.2 23.5 95.3 1.048 65.1 58.6 5586 58.1% 1606 16.7% 2424 25.2% 9616
aug-1gram-ant-filt 17.2 23.5 95.3 1.048 65.1 58.6 5586 58.1% 1606 16.7% 2424 25.2% 9616
Table 2: English-Arabic translation scores and analysis for NIST MT05 and MEDAR test sets. B = statistically significant w.r.t.
(B)aseline using Koehn (2004)?s statistical significance test.
bers from English are not directly comparable to the
Chinese side: they relate to paraphrase candidates
and not phrase table entries; they relate to types and
not tokens; each OOV English word may translate
to one or more Chinese words, each of which may
comprise of one or more characters; and last but not
least, the BLEU score we use is character-based.
phrase ||| paraphrase ||| score comments
absence ||| occupation ||| 0.06 mild
absence ||| presence ||| 0.33 good
backwards ||| forwards ||| 0.21 good
wooden ||| plastic lawn ||| 0.12 sibling
dump ||| dispose of ||| 0.41 bad
cooler ||| warm ||| 0.45 mild
diminished ||| increased ||| 0.23 good
minor ||| serious ||| 0.42 good
relic ||| youth activist in the ||| 0.12 harmless
dive ||| rise ||| 0.15 good
argue ||| also recognize ||| 0.05 mild
bother ||| waste time ||| 0.79 bad
dive ||| climb ||| 0.17 good
moonlight ||| spring ||| 0.05 harmless
sharply ||| slightly ||| 0.60 good
substantial ||| meager ||| 0.14 good
warmer ||| cooler ||| 0.72 good
tough ||| delicate ||| 0.07 good
tiny ||| mostly muslim ||| 0.06 mild
softly ||| deep ||| 0.06 mild
Table 3: Random filtering examples
While individual unigram to 4gram scores for the
augmented models were lower than the baseline?s,
filtered model?s unigram and bigram scores were
lower or similar to the baseline?s, and their trigram
and 4gram scores were higher than the baseline?s.
We intend to further investigate the cause for this
pattern, and its effect on translation quality, with the
help of a native Chinese speaker ? and on BLEU, to-
gether with the brevity penalty ? in the future.
English-Arabic: The most striking fact is the set of
differences between the language pairs: In English-
Chinese, we see gains with distributional paraphrase
augmentation, and further gains when antonymous
and contrasting paraphrase candidates are filtered
out. But in the 30k-sentence English-Arabic models,
paraphrase augmentation actually degrades perfor-
mance, even in lemma scores. It has been observed
before that BLEU (and similarly TER) is not ideal
for evaluation of contributions of this sort (Callison-
Burch et al, 2006). Therefore we conducted both
manual and focused automatic analysis, including
OOV statistics and unigram lemma match analysis6
6Unigram lemma match analysis is a classification of all the
words in the translation hypothesis (against the translation ref-
erence) into: (a) exact match, which is equal to simple unigram
precision, (b) lemma-only match, which counts words that can
only be matched at the lemma level, and (c) unmatchable.
243
between the system output and the reference trans-
lation.
Table 4 shows that the OOV rates for English-
Arabic are lower than English-Chinese. But if they
were negligible, we would not expect to see gains
(or in fact any change) in either model size, contrary
to fact. It is interesting to point out that our trans-
lation model augmentation technique handles about
50% of the (non-digit, non-punctuation) OOV words
in all models (except for only half that in the 135k
model, which still showed gains).
Another concern is that the current maximal para-
phrase length (6 tokens) may be too far from the
paraphrasee?s length (unigram), resulting in lower
quality. However, a closer examination of the
length difference evident through the BLEU brevity
penalty and the reference:system-output length ra-
tio (columns 4-5 of Table 2), reveals that the dif-
ferences are small and inconsistent; on average, the
brevity penalty difference accounts for roughly 0.1
absolute BLEU points and 0.2 absolute lemmatized
BLEU points of the respective differences.7
Last, Modern Standard Arabic is a morphologi-
cally rich language: It has many inflected forms for
most verbs, and several inflected forms for nouns,
adjectives and other parts of speech ? and complex
syntactic agreement patterns showing these inflec-
tions. It might be the case that the inflected Arabic
LM model might not serve well the augmented mod-
els, since they include translation rules that are more
likely to be ?off? inflection-wise (e.g., showing un-
grammatical syntactic agreement or simply an ac-
ceptable choice that differs from the reference). Pre-
sumably, the smaller the training set, the larger this
problem, since there would be fewer rules and hence
smaller variety of inflected forms per similar core
meaning. The unigram lemma match analysis and
lemma scores? statistics (Table 2) support this con-
cern. In the 30k model, lemma-only match seems
to even further increase, at the expense of the exact
word-form match. Possible solutions include using
a lemma-based LM, or another LM that is adjusted
to this sort of inflection-wise ?off? text.
7These values are computed by subtracting the difference
between two BLEU scores from the difference between the same
two BLEU scores without the effect of brevity penalty (i.e., each
divided by its brevity penalty).
Error Analysis We conducted an error analysis of
our Arabic 30k system using part of the MT05 test
set. That set had 571 OOV types, out of which,
we were able to augment phrases for 196 OOV
types. The majority of OOV words were proper
nouns (67.8%), with the rest being mostly nouns, ad-
jectives and verbs (in the order of their frequency).
Among the OOVs for which we augmented phrases,
the proper noun ratio was smaller than the full set
(45.4% relative). We selected a random sample of
50 OOV words, and examined their translations in
the MT05 test set. The analysis considered all the
OOV word occurrences (96 sentences). We classi-
fied each OOV translation in the augmented system
and the augmented-filtered system as follows:
a1 correct (and in reference)
a2 correct (morphological variation)
a3 acceptable translation into a synonym
a4 acceptable translation into a hypernym
b1 wrong translation into a hypernym
b2 co-hypernym: a sibling in a psychologically
natural category hierarchy
b3 antonymous, trend-contrasting, or polarity dis-
similar meaning
c1 wrong proper-noun translation (sibling)
c2 wrong proper-noun translation (other)
d wrong translation for other reasons
Both the augmented and augmented-filtered system
had 27.1% correct cases (category a). Only one-
quarter of these were exact matches with the refer-
ence (category a1) that can be captured by BLEU.
Incorrect proper-noun translation (category c) was
the biggest error (augmented model: 33.3%, filtered
model: 37.5%); within this category, sibling mis-
translations (category c1), e.g., Buddhism is trans-
lated as Islam, were the majority (over half in aug-
mented model, and about two-thirds in the filtered
model). Proper nouns seem to be a much bigger
problem for translation into Arabic than into Chi-
nese in our sets. Category b mis-translations ap-
peared in 20.8% of the time (equally in augmented
and filtered). Almost half of these were sibling mis-
translations (category b2), e.g., diamond translated
as gold. Only two OOV translations in our sam-
ple were antonymous (category b3). It is possible,
therefore, that our Arabic sets do not give room for
our filtering method to be effective. In one case,
the verb deepen (reference translation

???

K) is mis-
244
translated as summit (

??

?). In the other case, the
adjective cool (political relations), whose reference
translation uses a figure of speech periods of tension
(QK?J? @ 	?? H@Q


	
?), is mistranslated as good (

?YJ
k. ),
which carries the opposite sentiment. The rest of
category b involve hypernyms (b1), such as trans-
lating the OOV word telecom into company (

??Q??? @).
Overall, the filtered model did not behave signifi-
cantly differently from its augmented counterpart.
Chinese-Arabic score difference: We conjecture
that another possible reason for the different score
gain patterns between the two language pairs is the
fact that in Chinese, many words that are siblings-in-
meaning share a character, which doesn?t necessar-
ily have a stand-alone meaning; therefore, character-
based BLEU was able to give credit to such para-
phrases on the Chinese side, which was not case for
the word-based BLEU on the Arabic side.
7 Related Work
This paper brings together several sub-areas:
SMT, paraphrase generation, distributional seman-
tic distance measures, and antonym-related work.
Therefore we can only briefly survey the most rel-
evant work here. Our work can be viewed as an ex-
tension of the line of research that seeks to augment
translation tables with automatically generated para-
phrases of OOV words or phrases in a fashion sim-
ilar to Section 4: Callison-Burch et al (2006) use
pivoting technique (translating to other languages
and back) in order to generate paraphrases, and the
pivot translation probability as their similarity score;
Callison-Burch (2008) filters such paraphrases using
syntactic parsing information; Marton et al (2009)
use distributional paraphrasing technique that ap-
plies distributional semantic distance measure for
the paraphrase score; Marton (2010) applies a lexi-
cal resource / corpus-based hybrid semantic distance
measure for the paraphrase score instead, approxi-
mating word senses; here, we apply a distributional
semantic distance measure that is similar to Marton
et al (2009), with the main difference being the fil-
tering of the resulting paraphrases for antonymity.
Other work on augmentating SMT: Habash and
Hu (2009) show, pivoting via a trilingual parallel
text, that using English as a pivot language be-
tween Chinese and Arabic outperforms translation
using a direct Chinese-Arabic bilingual parallel text.
Other attempts to reduce the OOV rate by augment-
ing the phrase table?s source side include Habash
(2009), providing an online tool for paraphrasing
OOV phrases by lexical and morphological expan-
sion of known phrases and dictionary terms ? and
transliteration of proper names.
Bond et al (2008) also pivot for paraphrasing.
They improve SMT coverage by using a manually
crafted monolingual HPSG grammar for generating
meaning and grammar-preserving paraphrases. This
grammar allows for certain word reordering, lexical
substitutions, contractions, and ?typo? corrections.
Onishi et al (2010), Du et al (2010), and others,
pivot-paraphrase the input, and represent the para-
phrases in a lattice format, decoding it with Moses.
Work on paraphrase generation: Barzilay and
McKeown (2001) extract paraphrases from a mono-
lingual parallel corpus, containing multiple transla-
tions of the same source. However, monolingual
parallel corpora are extremely rare and small. Dolan
et al (2004) use edit distance for paraphrasing.
Max (2009) and others take the context of the para-
phrased word?s occurrence into account. Zhao et al
(2008) apply SMT-style decoding for paraphrasing,
using several log linear weighted resources while
Zhao et al (2009) filter out paraphrase candidates
and weight paraphrase features according to the de-
sired NLP task. Chevelu et al (2009) introduce
a new paraphrase generation tool based on Monte-
Carlo sampling. Mirkin et al (2009), inter alia,
frame paraphrasing as a special, symmetrical case of
(WordNet-based) textual entailment. See Madnani
and Dorr (2010) for a good paraphrasing survey.
Work on measuring distributional semantic dis-
tance: For one survey of this rich topic, see Weeds
et al (2004) and Turney and Pantel (2010). We
use here cosine of log-likelihood ratios (McDonald,
2000). A recent paper (Kazama et al, 2010) advo-
cates a Bayesian approach, making rare terms have
lower strength of association, as a by-product of re-
lying on their probabilistic Expectation.
Work on detecting antonyms: Our work with
antonyms can be thought of as an application-based
extension of the (Mohammad et al, 2008) method.
Some of the earliest computational work in this
area is by Lin et al (2003) who used patterns
245
model e2z:29k e2z:232k e2a:30k e2a:135k
phrase table baseline vocab. (# source-side types) 13916 34825 24371 49854
phrase table entries: baseline 1996k 13045k 2606k 12344k
phrase table entries: aug-1gram 2543k 127.38% 13615k 104.37% 2635k 101.09% *12373k 100.23%
phrase table entries: aug-1gram-ant-filt 2542k 127.35% 13615k 104.37% 2635k 101.09% *12373k 100.23%
OOV types in tune (% tune types) 1097 21.58% 451 8.87% 141 7.31% 84 4.35%
OOV tokens in tune (% tune tokens) 2138 6.10% 917 2.62% 193 2.18% 115 1.30%
OOV types in test (% test types) 2473 33.59% 1227 16.66% 574 12.42% 339 7.34%
OOV tokens in test (% test tokens) 4844 10.40% 2075 4.46% 992 2.83% 544 1.55%
tune OOV token decrease in aug-1gram/ant-filt 1343 27.73% 510 24.58% 79 7.96% 28 5.15%
tune OOV type decrease in aug-1gram/ant-filt 646 58.89% 203 45.01% 60 42.55% 22 26.19%
test OOV token decrease in aug-1gram /ant-filt 2776 57.31% 996 48.00% 460 46.37% 127 23.35%
test OOV type decrease in aug-1gram/ant-filt 1394 56.37% 585 47.68% 246 42.86% 76 22.42%
Table 4: Out-of-vocabulary (OOV) word rates and phrase table sizes for all model sizes and language pairs. e2z = English-Chinese;
e2a = English-Arabic. The statistics marked with * in the top-right cell are identical, see ?5.3.
such as ?from X to Y ? and ?either X or Y ? to
distinguish between antonymous and similar word
pairs. Harabagiu et al (2006) detected antonyms
by determining if their WordNet synsets are con-
nected by the hypernymy?hyponymy links and ex-
actly one antonymy link. Turney (2008) proposed a
supervised method to solve word analogy questions
that require identifying synonyms, antonyms, hyper-
nyms, and other lexical-semantic relations between
word pairs.
8 Conclusions and Future Work
We presented here a novel method for filtering out
antonymous phrasal paraphrase candidates, adapted
from sentiment analysis literature, and tested in sim-
ulated low- and mid-resourced SMT tasks from En-
glish to two quite different languages. We used an
antonymous word pair list extracted distributionally
by extending a seed list. Then, the extended list, to-
gether with a negator list and a novel heuristic, were
used to filter out antonymous paraphrase candidates.
Finally, SMT models were augmented with the fil-
tered paraphrases, yielding English-Chinese transla-
tion improvements of up to 1 BLEU from the corre-
sponding non-filtered paraphrase-augmented model
(up to 1.6 BLEU from the corresponding baseline
model). Our method proved effective for mod-
els trained on both reduced and mid-large English-
Chinese parallel texts. The reduced models sim-
ulated ?low density? languages by limiting the
amount of the training text.
We also showed for the first time transla-
tion gains for English-Arabic with paraphrase-
augmented (non-filtered) models. However, Ara-
bic, and presumably other morphologically rich lan-
guages, may require more complex models in order
to benefit from our filtering method.
Our antonym detection and filtering method is
distributional and heuristic-based; hence it is noisy.
We suspect that OOV terms in larger models tend
to be harder to paraphrase (judging by the differ-
ence from the reduced models, and the lower OOV
rate), and also harder to filter paraphrase candidates
of (due to the lower paraphrase quality, which might
not even include sufficiently distributionally similar
candidates, antonymous or otherwise). In the future,
we intend to improve our method, so that it can be
used to improve also the quality of models trained
on even larger parallel texts.
Last, we intend to extend our method beyond un-
igrams, limit paraphrase length to the vicinity of the
paraphrasee?s length, and improve our inflected Ara-
bic generation technique, so it can handle this novel
type of augmented data well.
Acknowledgments
Part of this work was done while the first author
was at Columbia University. The second author was
funded through a Google research award. The au-
thors wish to thank Saif Mohammad for providing
his data and for useful discussion, and also thank the
anonymous reviewers for their useful feedback.
246
References
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceed-
ings of the Association for Computational Linguistics
(ACL).
Isaac I. Bejar, Roger Chaffin, and Susan Embretson.
1991. Cognitive and Psychometric Analysis of Ana-
logical Problem Solving. Springer-Verlag, New York,
NY.
Francis Bond, Eric Nichols, Darren Scott Appling, and
Michael Paul. 2008. Improving statistical machine
translation by paraphrasing the training data. In Pro-
ceedings of IWSLT, Hawai?i, USA.
Lou Burnard. 2000. Reference Guide for the British
National Corpus (World Edition). Oxford University
Computing Services.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics (NAACL).
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), Waikiki,
Hawai?i.
Jonathan Chevelu, Thomas Lavergne, Yves Lepage, and
Thierry Moudenc. 2009. Introduction of a new para-
phrase generation tool based on monte-carlo sampling.
In Proceedings of the 47th Annual Meeting of the As-
sociation for Computational Linguistics (ACL) - the
4th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natural
Language Processing (IJCNLP) Short Papers, pages
249?252, Suntec, Singapore.
Yejin Choi and Claire Cardie. 2008. Learning with
compositional semantics as structural inference for
subsentential sentiment analysis. In Proceedings of
Empirical Methods in Natural Language Processing
(EMNLP), Waikiki, Hawaii.
David A. Cruse. 1986. Lexical semantics. Cambridge
University Press.
James Deese. 1965. The structure of associations in lan-
guage and thought. The Johns Hopkins Press.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of the 20th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), Geneva,
Switzerland.
Jinhua Du, Jie Jiang, and Andy Way. 2010. Facilitating
translation using source language paraphrase lattices.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), pages
420?429, MIT, Massachusetts, USA.
Ahmed El Kholy and Nizar Habash. 2010a. Techniques
for Arabic Morphological Detokenization and Ortho-
graphic Denormalization. In Proceedings of the sev-
enth International Conference on Language Resources
and Evaluation (LREC), Valletta, Malta.
Ahmed El Kholy and Nizar Habash. 2010b. Ortho-
graphic and Morphological Processing for English-
Arabic Statistical Machine Translation. In In Actes
de Traitement Automatique des Langues Naturelles
(TALN), Montreal, Canada.
Nizar Habash and Jun Hu. 2009. Improving Arabic-
Chinese statistical machine translation using English
as pivot language. In Proceedings of the 4th EACL
Workshop on Statistical Machine Translation, pages
173?181, Athens, Greece.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphological
Disambiguation in One Fell Swoop. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 573?580, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Nizar Habash. 2009. REMOOV: A tool for online han-
dling of out-of-vocabulary words in machine transla-
tion. In Proceedings of the 2nd International Con-
ference on Arabic Language Resources and Tools
(MEDAR), Cairo, Egypt.
Sanda M. Harabagiu, Andrew Hickl, and Finley Laca-
tusu. 2006. Lacatusu: Negation, contrast and contra-
diction in text processing. In Proceedings of the 23rd
National Conference on Artificial Intelligence (AAAI),
Boston, MA.
Junichi Kazama, Stijn De Saeger, Kow Kuroda, Masaki
Murata, and Kentaro Torisawa. 2010. A Bayesian
method for robust estimation of distributional similar-
ities. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 247?256, Uppsala, Sweden.
Alistair Kennedy and Diana Inkpen. 2005. Sentiment
classification of movie and product reviews using con-
textual valence shifters. COMPUTATIONAL INTEL-
LIGENCE, pages 110?125.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
the Annual Meeting of the Association for Com-
putational Linguistics (ACL) demonstration session,
Prague, Czech Republic.
Philipp Koehn. 2004. Statistical significance tests for
247
machine translation evaluation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Adrienne Lehrer and K. Lehrer. 1982. Antonymy. Lin-
guistics and Philosophy, 5:483?501.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou.
2003. Identifying synonyms among distributionally
similar words. In Proceedings of the 18th Interna-
tional Joint Conference on Artificial Intelligence (IJ-
CAI), pages 1492?1493, Acapulco, Mexico.
Nitin Madnani and Bonnie Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Computational Linguistics, 36(3).
Marie-Catherine de Marneffe, Anna Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Columbus, OH.
Yuval Marton, Chris Callison-Burch, and Philip Resnik.
2009. Improved statistical machine translation using
monolingually-derived paraphrases. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), Singapore.
Yuval Marton. 2010. Improved statistical machine trans-
lation using monolingual text and a shallow lexical re-
source for hybrid phrasal paraphrase generation. In
Proceedings of the Ninth Conference of the Associa-
tion for Machine Translation in the Americas (AMTA),
Denver, Colorado.
Aurelien Max. 2009. Sub-sentential paraphrasing by
contextual pivot translation. In Proceedings of the
47th Annual Meeting of the Association for Compu-
tational Linguistics (ACL) - the 4th International
Joint Conference on Natural Language Processing of
the Asian Federation of Natural Language Processing
(IJCNLP) - Workshop on Applied Textual Inference,
pages 18?26, Singapore. Suntec.
Scott McDonald. 2000. Environmental determinants of
lexical processing effort. Ph.D. thesis, University of
Edinburgh.
Rada Mihalcea and Carlo Strapparava. 2005. Making
computers laugh: Investigations in automatic humor
recognition. In Proceedings of the Conference on Hu-
man Language Technology and Empirical Methods in
Natural Language Processing, pages 531?538, Van-
couver, Canada.
Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido Da-
gan, Marc Dymetman, and Idan Szpektor . 2009.
Source-language entailment modeling for translating
unknown terms. In Proceedings of the 47th Annual
Meeting of the Association for Computational Linguis-
tics (ACL) - the 4th International Joint Conference
on Natural Language Processing of the Asian Federa-
tion of Natural Language Processing (IJCNLP), pages
791?799, Suntec, Singapore.
Saif Mohammad, Bonnie Dorr, and Codie Dunn. 2008.
Computing word-pair antonymy. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 982?991, Waikiki,
Hawaii.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 440?447.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of ACL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the ACL, pages 160?167.
Takashi Onishi, Masao Utiyama, and Eiichiro Sumita.
2010. Paraphrase lattice for statistical machine trans-
lation. In Proceedings of the Association for Computa-
tional Linguistics (ACL) Short Papers, pages 1?5, Up-
psala, Sweden.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1?2):1?135.
Kishore Papineni, Salim Roukos, Todd Ward, John Hen-
derson, and Florence Reeder. 2002. Corpus-based
comprehensive and diagnostic MT evaluation: Initial
Arabic, Chinese, French, and Spanish results. In Pro-
ceedings of the ACL Human Language Technology
Conference, pages 124?127, San Diego, CA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas, pages 223?231, Cambridge, MA.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
volume 2, pages 901?904.
Philip Stone, Dexter C. Dunphy, Marshall S. Smith,
Daniel M. Ogilvie, and associates. 1966. The General
Inquirer: A Computer Approach to Content Analysis.
The MIT Press.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter. In Fourth
SIGHAN Workshop on Chinese Language Processing.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Articial Intelligence Research, 37:141?188.
Peter Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In
Proceedings of the 22nd International Conference
248
on Computational Linguistics (COLING), pages 905?
912, Manchester, UK.
Ellen M Voorhees. 2008. Contradictions and jus-
tifications: Extensions to the textual entailment task.
In Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), Colum-
bus, OH.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional sim-
ilarity. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING),
pages 1015?1021, Geneva, Switzerland.
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and
Sheng Li. 2008. Combining multiple resources to
improve smt-based paraphrasing model. In Proceed-
ings of the Association for Computational Linguis-
tics (ACL)Human Language Technology (HLT), pages
1021?1029, Columbus, Ohio, USA.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of the 47th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL) - the 4th
International Joint Conference on Natural Language
Processing of the Asian Federation of Natural Lan-
guage Processing (IJCNLP), pages 834?842, Suntec,
Singapore.
249
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 90?94,
Utica, May 2012. c?2012 Association for Computational Linguistics
Rich Morphology Generation
Using Statistical Machine Translation
Ahmed El Kholy and Nizar Habash
Center for Computational Learning Systems, Columbia University
475 Riverside Drive New York, NY 10115
{akholy,habash}@ccls.columbia.edu
Abstract
We present an approach for generation of mor-
phologically rich languages using statistical
machine translation. Given a sequence of lem-
mas and any subset of morphological features,
we produce the inflected word forms. Testing
on Arabic, a morphologically rich language,
our models can reach 92.1% accuracy starting
only with lemmas, and 98.9% accuracy if all
the gold features are provided.
1 Introduction
Many natural language processing (NLP) applica-
tions, such as summarization and machine trans-
lation (MT), require natural language generation
(NLG). Generation for morphologically rich lan-
guages, which introduce a lot of challenges for NLP
in general, has gained a lot of attention recently, es-
pecially in the context of statistical MT (SMT). The
common wisdom for handling morphological rich-
ness is to reduce the complexity in the internal ap-
plication models and then generate complex word
forms in a final step.
In this paper,1 we present a SMT-based approach
for generation of morphologically rich languages.
Given a sequence of lemmas and any subset of mor-
phological features, we produce the inflected word
forms. The SMT model parameters are derived from
a parallel corpus mapping lemmas and morphologi-
cal features to the inflected word forms.
As a case study, we focus on Arabic, a mor-
phologically rich language. Our models can reach
92.1% accuracy starting only with tokenized lem-
mas and predicting some features, up from 55.0%
accuracy without inflecting the lemmas. If all of the
gold morphological features are provided as input,
our best model achieves 98.9% accuracy.
1This work was funded by a Google research award.
2 Related Work
In the context of morphological generation for MT,
the state-of-the-art factored machine translation ap-
proach models morphology using generation factors
in the translation process (Koehn et al, 2007). One
of the limitations of factored models is that gen-
eration is based on the word level not the phrase
level and the context is only captured through a lan-
guage model. Minkov et al (2007) and Toutanova et
al. (2008) model translation and morphology inde-
pendently for English-Arabic and English-Russian
MT. They use a maximum entropy model to predict
inflected word forms directly. Clifton and Sarkar
(2011) use a similar approach for English-Finnish
MT where they predict morpheme sequences. Un-
like both approaches, we generate the word forms
from the deeper representation of lemmas and fea-
tures.
As for using SMT in generation, there are many
previous efforts. Wong and Mooney (2007) use
SMT methods for tactical NLG. They learn through
SMT to map meaning representations to natural lan-
guage. Quirk et al (2004) apply SMT tools to gen-
erate paraphrases of input sentences in the same lan-
guage. Both of these efforts target English, a mor-
phologically poor language. Our work is conceptu-
ally closer to Wong and Mooney (2007), except that
we focus on the question of morphological genera-
tion and our approach includes an optional feature
prediction component. In a related publication, we
integrate our generation model as part of end-to-end
English-Arabic SMT (El Kholy and Habash, 2012).
In that work, we make use of English features in the
Arabic morphology prediction component, e.g., En-
glish POS and parse trees.
90
3 Arabic Challenges
Arabic is a morphologically complex language. One
aspect of Arabic?s complexity is its orthography
which often omits short vowel diacritics. As a re-
sult, ambiguity is rampant. Another aspect is the
various attachable clitics which include conjunction
proclitics, e.g., +? w+ ?and?, particle proclitics, e.g.,
+? l+ ?to/for?, the definite article +?@ Al+ ?the?, and
the class of pronominal enclitics, e.g., ??+ +hm
?their/them?. Beyond these clitics, Arabic words
inflect for person (PER), gender (GEN), number
(NUM), aspect (ASP), mood (MOD), voice (VOX),
state (STT) and case (CAS). Arabic inflectional
features are realized as affixes as well as templatic
changes, e.g., broken plurals.2
These three phenomena, optional diacritics, at-
tachable clitics and the large inflectional space, lead
to thousands of inflected forms per lemma and a high
degree of ambiguity: about 12 analyses per word,
typically corresponding to two lemmas on average
(Habash, 2010). The Penn Arabic Treebank (PATB)
tokenization scheme (Maamouri et al, 2004), which
we use in all our experiments, separates all clitics
except for the determiner clitic Al+ (DET). As such
we consider the DET as an additional morphological
feature.
Arabic has complex morpho-syntactic agreement
rules in terms of GEN, NUM and definiteness. Ad-
jectives agree with nouns in GEN and NUM but plu-
ral irrational nouns exceptionally take feminine sin-
gular adjectives. Moreover, verbs agree with sub-
jects in GEN only in VSO order while they agree
in GEN and NUM in SVO order (Alkuhlani and
Habash, 2011). The DET in Arabic is used to dis-
tinguish different syntactic constructions such as the
possessive or adjectival modification. These agree-
ment rules make the generation of correctly inflected
forms in context a challenging task.
4 Approach
In this section, we discuss our approach in gener-
ating Arabic words from Arabic lemmas (LEMMA)
using a pipeline of three steps.
1. (Optional)Morphology Prediction of linguis-
tic features to inflect LEMMAs.
2The Arabic NLP tools we use in this paper do not model all
templatic inflectional realizations.
Tokens w+ s+ yktbwn +hA
POS conj fut part verb pron
Lemma wa sa katab hA
Features na,na,na, na,na,na, 3rd,masc,pl, 3rd,fem,sg,
na,na,na, na,na,na, imp,act,ind, na,na,na,
na,na,na, na,na,na, na,na,na, na,na,na,
Figure 1: An example A?+ 	K?J.

J?K
+?+? w+s+yktbwn +hA ?and
they will write it?. Features? order of presentation is: PER, GEN,
NUM, ASP, VOX, MOD, DET, CAS, and STT. The value ?na? is
for ?not-applicable?.
2. Morphology Generation of inflected Arabic
tokens from LEMMAs and any subset of Ara-
bic linguistic features.
3. Detokenization of inflected Arabic tokens into
surface Arabic words.
Morphology generation is the main contribution
of this paper which in addition to detokenization rep-
resents an end-to-end inflection generator. The mor-
phology prediction step is an optional step that com-
plements the whole process by enriching the input
of the morphology generation step with one or more
predicted morphological features.
We follow numerous previously published efforts
on the value of tokenization for Arabic NLP tasks
(Badr et al, 2008; El Kholy and Habash, 2010). We
use the best performing tokenization scheme (PATB)
in machine translation in all our experiments and fo-
cus on the question of how to generate Arabic in-
flected words from LEMMAs and features. Figure 1
shows an example of a tokenized word and its de-
composition into a LEMMA and morphological fea-
tures.
Morphology Prediction This optional step takes
a sequence of LEMMAs and tries to enrich them
by predicting one or more morphological features.
It is implemented using a supervised discriminative
learning model, namely Conditional Random Fields
(CRF) (Lafferty et al, 2001). Table 1 shows the
accuracy of the CRF module on a test set of 1000
sentences compared to using the most common fea-
ture value baseline. Some features, such as CAS and
STT are harder to predict but they also have very low
baseline values. GEN, DET and NUM have a mod-
erate prediction accuracy while ASP, PER, VOX and
MOD have high prediction accuracy (but also very
high baselines). This task is similar to POS tagging
91
Predicted Baseline Prediction
Feature Accuracy% Accuracy%
Case (CAS) 42.87 70.39
State (STT) 42.85 76.93
Gender (GEN) 67.42 84.17
Determiner (DET) 59.71 85.41
Number (NUM) 70.61 87.31
Aspect (ASP) 90.38 92.10
Person (PER) 85.71 92.80
Voice (VOX) 90.38 93.70
Mood (MOD) 90.38 93.80
Table 1: Accuracy (%) of feature prediction starting from Ara-
bic lemmas (LEMMA). The second column shows the baseline
for prediction using the most common feature value. The third
column is the prediction accuracy using CRF.
except that it starts with lemmas as opposed to in-
flected forms (Habash and Rambow, 2005; Alkuh-
lani and Habash, 2012). As such, we expect it to
perform worse than a comparable POS tagging task.
For example, Habash and Rambow (2005) report
98.2% and 98.8% for GEN and NUM, respectively,
compared to our 84.2% and 87.3%.
In the context of a specific application, the per-
formance of the prediction could be improved us-
ing information other than the context of provided
LEMMAs. For example, in MT, source language lex-
ical, syntactic and morphological information could
be used in the prediction module (El Kholy and
Habash, 2012).
The morphology prediction step produces a lat-
tice with all the possible feature values each having
an associated confidence score. We filter out options
with very low confidence scores to control the expo-
nential size of the lattice when combining more than
one feature. We tried some experiments using only
one or two top values but got lower performance.
The morphology generation step takes the lattice and
decides on the best target inflection.
Morphology Generation This step is imple-
mented using a SMT model that translates from a
deeper linguistic representation to a surface repre-
sentation. The model parameters are derived from a
parallel corpus mapping LEMMAs plus morphologi-
cal features to Arabic inflected forms. The model is
monotonic and there is neither reordering nor word
deletion/addition. We plan to consider these varia-
tions in the future. The main advantage of this ap-
proach is that it only needs monolingual data which
is abundant.
The morphology generation step can take a se-
quence of LEMMAs and a subset of morphological
features directly or after enriching the sequence with
one or more morphological features using the mor-
phology prediction step.
Detokenization Since we work on tokenized Ara-
bic, we use a detokenization step which simply
stitches the words and clitics together as a post-
processing step after morphology generation. We
use the best detokenization technique presented by
El Kholy and Habash (2010).
5 Evaluation
Evaluation Setup All of the training data we use
is available from the Linguistic Data Consortium
(LDC).3 For SMT training and language modeling
(LM), we use 200M words from the Arabic Giga-
word corpus (LDC2007T40). We use 5-grams for
all LMs implemented using the SRILM toolkit (Stol-
cke, 2002).
MADA+TOKAN (Habash and Rambow, 2005;
Habash et al, 2009) is used to preprocess the
Arabic text for generation and language modeling.
MADA+TOKAN tokenizes, lemmatizes and selects
all morphological features in context.
All generation experiments are conducted using
the Moses phrase-based SMT system (Koehn et al,
2007). The decoding weight optimization is done
using a set of 300 Arabic sentences from the 2004
NIST MT evaluation test set (MT04). The tuning is
based on tokenized Arabic without detokenization.
We use a maximum phrase length of size 4. We
report results on the Arabic side of the 2005 NIST
MT evaluation set (MT05), our development set.
We use the Arabic side of MT06 NIST data set for
blind test. We evaluate using BLEU-1 and BLEU-4
(Papineni et al, 2002). BLEU is a precision-based
evaluation metric commonly used in MT research.
Given the way we define our generation task to ex-
clude reordering and word deletion/addition, BLEU-
1 can be interpreted as a measure of word accuracy.
BLEU-4 is the geometric mean of unigram, bigram,
trigram and 4-gram precision.4 Since Arabic text
3http://www.ldc.upenn.edu
4n-gram precision is the number of test n-word sequences
that appear in the reference divided by the number of all possi-
ble n-word sequences in the test.
92
is generally written without diacritics, we evaluate
on undiacritized text only. In the future, we plan
to study generation into diacritized Arabic, a more
challenging goal.
Baseline We conducted two baseline experiments.
First, as a degenerate baseline, we only used deto-
kenization to generate the inflected words from
LEMMAs. Second, we used a generation step be-
fore detokenization to generate the inflected tokens
from LEMMAs. The BLEU-1/BLEU-4 scores of
the two baselines on the MT05 set are 55.04/24.51
and 91.54/82.19. We get a significant improvement
(?35% BLEU-1 & ?58% BLEU-4) by using the
generation step before detokenization.
Generation with Gold Features We built several
SMT systems translating from LEMMAs plus one or
more morphological features to Arabic inflected to-
kens. We then use the detokenization step to recom-
bine the tokens and produce the surface words.
Table 2 shows the BLEU scores for MT05 set as
LEMMAs plus different morphological features and
their combinations. We greedily combined the fea-
tures based on the performance of each feature sep-
arately. Features with higher performance are com-
bined first. As expected, the more features are in-
cluded the better the results. Oddly, when we add
the POS to the feature combination, the performance
drops. That could be explained by the redundancy in
information provided by the POS given all the other
features and the added sparsity.
Although STT and MOD features hurt the per-
formance when added incrementally to the feature
combination, removing them from the complete fea-
ture set led to a drop in performance. We suspect
that there are synergies in combining different fea-
tures. We plan to investigate this point extensively
in the future. BLEU scores are very high because
the input is golden in terms of word order, lemma
choice and features. These scores should be seen as
the upper limit of our model?s performance. Most of
the errors are detokenization and word form under-
specification errors.
Generation with Predicted Features We com-
pare results of generation with a variety of predicted
features (see Table 3). The results show that us-
ing predicted features can help improve the gener-
ation quality over the baseline in some cases, e.g.,
Gold Generation Input BLEU-1% BLEU-4%
LEMMA 91.54 82.19
LEMMA+MOD 91.70 82.44
LEMMA+ASP 92.09 83.26
LEMMA+PER 92.09 83.34
LEMMA+VOX 92.33 83.70
LEMMA+CAS 92.71 84.34
LEMMA+STT 93.92 86.55
LEMMA+DET 93.97 86.62
LEMMA+NUM 93.91 86.89
LEMMA+GEN 94.33 87.32
LEMMA+GEN+NUM 95.67 91.16
++DET 97.88 95.76
++STT 97.73 95.39
++CAS 98.13 96.35
++VOX 98.19 96.47
++PER 98.24 96.59
++ASP 98.85 98.08
++MOD 98.85 98.06
LEMMA + All Features + POS 98.82 98.01
Table 2: Results of generation from gold Arabic lemmas
(LEMMA) plus different sets of morphological features. Results
are in (BLEU-1 & BLEU-4) on our MT05 set. ?++? means the
feature is added to the set of features in the previous row.
when the LEMMAs are enriched with CAS, ASP,
PER, VOX or MOD features. Our best performer is
LEMMA+MOD. Unlike gold features, greedily com-
bining predicted features hurts the performance and
the more features added the worse the performance.
One explanation is that each feature is predicted in-
dependently which may lead to incompatible feature
values. In the future, we plan to investigate ways
of combining features that could help performance
such predicting more than one feature together and
filtering out bad feature combinations. The feature
prediction accuracy (Table 1) does not always cor-
relate with the generation performance, e.g., CAS
has lower accuracy than GEN, but has a relatively
higher BLEU score. This may be due to the fact that
some features are mostly realized as diacritics (CAS)
which are ignored in our evaluation.
Blind Test Set To validate our results, we ap-
plied different versions of our system to a blind
test set (MT06). Our results are as follows
(BLEU-1/BLEU-4): detokenization without inflec-
tion (55.64/24.92), generation from LEMMAs only
(86.70/72.69), generation with gold MOD feature
(87.00/73.31), and generation with predicted MOD
feature (86.96/73.29). These numbers are generally
93
Generation Input BLEU-1% BLEU-4%
Baseline (LEMMA) 91.54 82.19
LEMMA+GEN 89.23 78.37
LEMMA+NUM 91.14 81.35
LEMMA+STT 91.16 81.70
LEMMA+DET 91.18 81.78
LEMMA+CAS 91.60 82.43
LEMMA+ASP 91.94 83.07
LEMMA+PER 91.97 83.10
LEMMA+VOX 91.99 83.18
LEMMA+MOD 92.05 83.26
LEMMA+MOD+VOX 91.76 82.73
++PER 91.57 82.32
++ASP 91.07 81.32
++CAS 89.71 78.68
Table 3: Results of generation from LEMMA plus different sets
of predicted morphological features. Results are in (BLEU-1 &
BLEU-4) on our MT05 set. ?++? means the feature is added to
the set of features in the previous row.
lower than our development set, but the trends and
conclusions are consistent.
6 Conclusion and Future Work
We present a SMT-based approach to generation of
morphologically rich languages. We evaluate our
approach under a variety of settings for Arabic. In
the future, we plan to improve the quality of feature
prediction and test our approach on other languages.
References
Sarah Alkuhlani and Nizar Habash. 2011. A Corpus
for Modeling Morpho-Syntactic Agreement in Arabic:
Gender, Number and Rationality. In Proc. of ACL?11,
Portland, OR.
Sarah Alkuhlani and Nizar Habash. 2012. Identifying
Broken Plurals, Irregular Gender, and Rationality in
Arabic Text. In Proc. of EACL?12, Avignon, France.
Ibrahim Badr, Rabih Zbib, and James Glass. 2008. Seg-
mentation for English-to-Arabic Statistical Machine
Translation. In Proc. of ACL?08, Columbus, OH.
Ann Clifton and Anoop Sarkar. 2011. Combin-
ing morpheme-based machine translation with post-
processing morpheme prediction. In Proc. of ACL?11,
Portland, OR.
Ahmed El Kholy and Nizar Habash. 2010. Orthographic
and Morphological Processing for English-Arabic Sta-
tistical Machine Translation. In Proc. of TALN?10,
Montre?al, Canada.
Ahmed El Kholy and Nizar Habash. 2012. Translate,
Predict or Generate: Modeling Rich Morphology in
Statistical Machine Translation. In Proc. of EAMT?12,
Trento, Italy.
Nizar Habash and Owen Rambow. 2005. Arabic To-
kenization, Part-of-Speech Tagging and Morphologi-
cal Disambiguation in One Fell Swoop. In Proc. of
ACL?05, Ann Arbor, MI.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, pos tag-
ging, stemming and lemmatization. Proc. of MEDAR,
Cairo, Egypt.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-
pher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Christopher Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In Proc. of ACL?07, Prague, Czech Re-
public.
J. Lafferty, A. McCallum, and F.C.N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proc. of
the International Conference on Machine Learning.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus. In
Proc. of NEMLAR?04, Cairo, Egypt.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proc. of ACL?07, Prague, Czech Re-
public.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proc. of ACL?02,
Philadelphia, PA.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Dekang Lin and Dekai Wu, editors, Proc.
of EMNLP?04, Barcelona, Spain.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proc. of ICSLP?02, Den-
ver, CO.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proc. of ACL?08, Columbus,
OH.
Yuk Wah Wong and Raymond Mooney. 2007. Gen-
eration by inverting a semantic parser that uses sta-
tistical machine translation. In Proc. of NAACL?07,
Rochester, NY.
94
