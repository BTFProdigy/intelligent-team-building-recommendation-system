Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 729?739,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
The Inside-Outside Recursive Neural Network model for
Dependency Parsing
Phong Le and Willem Zuidema
Institute for Logic, Language, and Computation
University of Amsterdam, the Netherlands
{p.le,zuidema}@uva.nl
Abstract
We propose the first implementation of
an infinite-order generative dependency
model. The model is based on a new
recursive neural network architecture, the
Inside-Outside Recursive Neural Network.
This architecture allows information to
flow not only bottom-up, as in traditional
recursive neural networks, but also top-
down. This is achieved by computing
content as well as context representations
for any constituent, and letting these rep-
resentations interact. Experimental re-
sults on the English section of the Uni-
versal Dependency Treebank show that
the infinite-order model achieves a per-
plexity seven times lower than the tradi-
tional third-order model using counting,
and tends to choose more accurate parses
in k-best lists. In addition, reranking with
this model achieves state-of-the-art unla-
belled attachment scores and unlabelled
exact match scores.
1 Introduction
Estimating probability distributions is the core is-
sue in modern, data-driven natural language pro-
cessing methods. Because of the traditional defi-
nition of discrete probability
Pr(A) ?
the number of times A occurs
the size of event space
counting has become a standard method to tackle
the problem. When data are sparse, smoothing
techniques are needed to adjust counts for non-
observed or rare events. However, successful use
of those techniques has turned out to be an art. For
instance, much skill and expertise is required to
create reasonable reduction lists for back-off, and
to avoid impractically large count tables, which
store events and their counts.
An alternative to counting for estimating prob-
ability distributions is to use neural networks.
Thanks to recent advances in deep learning, this
approach has recently started to look very promis-
ing again, with state-of-the-art results in senti-
ment analysis (Socher et al., 2013), language mod-
elling (Mikolov et al., 2010), and other tasks. The
Mikolov et al. (2010) work, in particular, demon-
strates the advantage of neural-network-based ap-
proaches over counting-based approaches in lan-
guage modelling: it shows that recurrent neu-
ral networks are capable of capturing long histo-
ries efficiently and surpass standard n-gram tech-
niques (e.g., Kneser-Ney smoothed 5-gram).
In this paper, keeping in mind the success of
these models, we compare the two approaches.
Complementing recent work that focused on such
a comparison for the case of finding appropriate
word vectors (Baroni et al., 2014), we focus here
on models that involve more complex, hierarchical
structures. Starting with existing generative mod-
els that use counting to estimate probability distri-
butions over constituency and dependency parses
(e.g., Eisner (1996b), Collins (2003)), we develop
an alternative based on recursive neural networks.
This is a non-trivial task because, to our knowl-
edge, no existing neural network architecture can
be used in this way. For instance, classic recur-
rent neural networks (Elman, 1990) unfold to left-
branching trees, and are not able to process ar-
bitrarily shaped parse trees that the counting ap-
proaches are applied to. Recursive neural net-
works (Socher et al., 2010) and extensions (Socher
et al., 2012; Le et al., 2013), on the other hand,
do work with trees of arbitrary shape, but pro-
cess them in a bottom-up manner. The probabil-
ities we need to estimate are, in contrast, defined
by top-down generative models, or by models that
require information flows in both directions (e.g.,
the probability of generating a node depends on
the whole fragment rooted at its just-generated sis-
729
Figure 1: Inner (i
p
) and outer (o
p
) representations
at the node that covers constituent p. They are vec-
torial representations of p?s content and context,
respectively.
ter).
To tackle this problem, we propose a new ar-
chitecture: the Inside-Outside Recursive Neural
Network (IORNN) in which information can flow
not only bottom-up but also top-down, inward and
outward. The crucial innovation in our architec-
ture is that every node in a hierarchical structure
is associated with two vectors: one vector, the in-
ner representation, representing the content under
that node, and another vector, the outer represen-
tation, representing its context (see Figure 1). In-
ner representations can be computed bottom-up;
outer representations, in turn, can be computed
top-down. This allows information to flow in
any direction, depending on the application, and
makes the IORNN a natural tool for estimating
probabilities in tree-based generative models.
We demonstrate the use of the IORNN by ap-
plying it to an ?-order generative dependency
model which is impractical for counting due to
the problem of data sparsity. Counting, instead, is
used to estimate a third-order generative model as
in Sangati et al. (2009) and Hayashi et al. (2011).
Our experimental results show that our new model
not only achieves a seven times lower perplex-
ity than the third-order model, but also tends to
choose more accurate candidates in k-best lists. In
addition, reranking with this model achieves state-
of-the-art scores on the task of supervised depen-
dency parsing.
The outline of the paper is following. Firstly, we
give an introduction to Eisner?s generative model
in Section 2. Then, we present the third-order
model using counting in Section 3, and propose
the IORNN in Section 4. Finally, in Section 5 we
show our experimental results.
2 Eisner?s Generative Model
Eisner (1996b) proposed a generative model for
dependency parsing. The generation process is
top-down: starting at the ROOT, it generates
left dependents and then right dependents for the
ROOT. After that, it generates left dependents and
right dependents for each of ROOT?s dependents.
The process recursively continues until there is no
further dependent to generate. The whole process
is captured in the following formula
P (T (H)) =
L
?
l=1
P
(
H
L
l
|C
H
L
l
)
P
(
T (H
L
l
)
)
?
R
?
r=1
P
(
H
R
r
|C
H
R
r
)
P
(
T (H
R
r
)
)
(1)
whereH is the current head, T (N) is the fragment
of the dependency parse rooted in N , and C
N
is
the context in which N is generated. H
L
, H
R
are
respectively H?s left dependents and right depen-
dents, plus EOC (End-Of-Children), a special to-
ken to indicate that there are no more dependents
to generate. Thus, P (T (ROOT )) is the proba-
bility of generating the entire dependency struc-
ture T . We refer to ?H
L
l
, C
H
L
l
?, ?H
R
r
, C
H
R
r
? as
?events?, and ?C
H
L
l
?, ?C
H
R
r
? as ?conditioning con-
texts?.
In order to avoid the problem of data sparsity,
the conditioning context in which a dependent D
is generated should capture only part of the frag-
ment generated so far. Based on the amount of
information that contexts hold, we can define the
order of a generative model (see Hayashi et al.
(2011, Table 3) for examples)
? first-order: C
1
D
contains the head H ,
? second-order: C
2
D
contains H and the just-
generated sibling S,
? third-order: C
3
D
contains H , S, the sibling S
?
before S (tri-sibling); or H , S and the grand-
head G (the head of H) (grandsibling) (the
fragment enclosed in the blue doted contour
in Figure 2),
? ?-order: C
?
D
contains all of D?s ancestors,
theirs siblings, and its generated siblings (the
fragment enclosed in the red dashed contour
in Figure 2).
In the original models (Eisner, 1996a), each de-
pendent D is a 4-tuple ?dist, w, c, t?
? dist(H,D) the distance between D and its
headH , represented as one of the four ranges
1, 2, 3-6, 7-?.
730
Figure 2: Example of different orders of context of ?diversified?. The blue dotted shape corresponds
to the third-order outward context, while the red dashed shape corresponds to the?-order left-to-right
context. The green dot-dashed shape corresponds to the context to compute the outer representation.
? word(D) the lowercase version of the word
of D,
? cap(D) the capitalisation feature of the word
of D (all letters are lowercase, all letters are
uppercase, the first letter is uppercase, the
first letter is lowercase),
? tag(D) the POS-tag of D,
Here, to make the dependency complete,
deprel(D), the dependency relation of D (e.g.,
SBJ, DEP), is also taken into account.
3 Third-order Model with Counting
The third-order model we suggest is similar to
the grandsibling model proposed by Sangati et
al. (2009) and Hayashi et al. (2011). It defines
the probability of generating a dependent D =
?dist, d, w, c, t? as the product of the distance-
based probability and the probabilities of gener-
ating each of its components (d, t, w, c, denoting
dependency relation, POS-tag, word and capitali-
sation feature, respectively). Each of these prob-
abilities is smoothed using back-off according to
the given reduction lists (as explained below).
P (D|C
D
)
= P (dist(H,D), dwct(D)|H,S,G, dir)
= P (d(D)|H,S,G, dir)
reduction list:
tw(H), tw(S), tw(G), dir
tw(H), tw(S), t(G), dir{
tw(H), t(S), t(G), dir
t(H), tw(S), t(G), dir
t(H), t(S), t(G), dir
? P (t(D)|d(D), H, S,G, dir)
reduction list:
d(D), dtw(H), t(S), dir
d(D), d(H), t(S), dir
d(D), d(D), dir
? P (w(D)|dt(D), H, S,G, dir)
reduction list:
dtw(H), t(S), dir
dt(H), t(S), dir
? P (c(D)|dtw(D), H, S,G, dir)
reduction list:
tw(D), d(H), dir
tw(D), dir
? P (dist(H,D)|dtwc(D), H, S,G, dir) (2)
reduction list:
dtw(D), dt(H), t(S), dir
dt(D), dt(H), t(S), dir
The reason for generating the dependency rela-
tion first is based on the similarity between rela-
tion/dependent and role/filler: we generate a role
and then choose a filler for that role.
Back-off The back-off parameters are identi-
cal to Eisner (1996b). To estimate the proba-
bility P (A|context) given a reduction list L =
(l
1
, l
2
, ..., l
n
) of context, let
p
i
=
{
count(A,l
i
)+0.005
count(l
i
)+0.5
if i = n
count(A,l
i
)+3p
i+1
count(l
i
)+3
otherwise
then P (A|context) = p
1
.
4 The Inside-Outside Recursive Neural
Network
In this section, we first describe the Recur-
sive Neural Network architecture of Socher et
al. (2010) and then propose an extension we
call the Inside-Outside Recursive Neural Network
(IORNN). The IORNN is a general architecture
for trees, which works with tree-based genera-
tive models including those employed by Eisner
(1996b) and Collins (2003). We then explain how
to apply the IORNN to the?-order model. Note
that for the present paper we are only concerned
with the problem of computing the probability of
731
Figure 3: Recursive Neural Network (RNN).
a tree; we assume an independently given parser is
available to assign a syntactic structure, or multi-
ple candidate structures, to an input string.
4.1 Recursive Neural Network
The architecture we propose can best be under-
stood as an extension of the Recursive Neural Net-
works (RNNs) proposed by Socher et al. (2010),
that we mentioned above. In order to see how
an RNN works, consider the following example.
Assume that there is a constituent with parse tree
(p
2
(p
1
x y) z) (Figure 3), and that x,y, z ? R
n
are the (inner) representations of the three words
x, y and z, respectively. We use a neural network
which consists of a weight matrix W
1
? R
n?n
for
left children and a weight matrix W
2
? R
n?n
for
right children to compute the vector for a parent
node in a bottom up manner. Thus, we compute
p
1
as follows
p
1
= f(W
1
x + W
2
y + b)
where b is a bias vector and f is an activation
function (e.g., tanh or logistic). Having computed
p
1
, we can then move one level up in the hierarchy
and compute p
2
:
p
2
= f(W
1
p
1
+ W
2
z + b)
This process is continued until we reach the root
node. The RNN thus computes a single vector
for each node p in the tree, representing the con-
tent under that node. It has in common with log-
ical semantics that representations for compounds
(here xyz) are computed by recursively applying a
composition function to meaning representations
of the parts. It is difficult to characterise the ex-
pressivity of the resulting system in logical terms,
but recent work suggests it is surprisingly power-
ful (e.g., Kanerva (2009)).
Figure 4: Inside-Outside Recursive Neural Net-
work (IORNN). Black rectangles correspond to in-
ner representations, white rectangles correspond
to outer representations.
4.2 IORNN
We extend the RNN-architecture by adding a sec-
ond vector to each node, representing the context
of the node, shown as white rectangles in figure 4.
The job of this second vector, the outer represen-
tation, is to summarize all information about the
context of node p so that we can either predict its
content (i.e., predict an inner representation), or
pass on this information to the daughters of p (i.e.,
compute outer representations of these daughters).
Outer representations thus allow information to
flow top-down.
We explain the operation of the resulting Inside-
Outside Recursive Neural Network in terms of the
same example parse tree (p
2
(p
1
x y) z) (see Fig-
ure 4). Each node u in the syntactic tree carries
two vectors o
u
and i
u
, the outer representation and
inner representation of the constituent that is cov-
ered by the node.
Computing inner representations Inner repre-
sentations are computed from the bottom up. We
assume for every word w an inner representation
i
w
? R
n
. The inner representation of a non-
terminal node, say p
1
, is given by
i
p
1
= f(W
i
1
i
x
+ W
i
2
i
y
+ b
i
)
where W
i
1
,W
i
2
are n ? n real matrices, b
i
is a
bias vector, and f is an activation function, e.g.
tanh. (This is the same as the computation of
non-terminal vectors in the RNNs.) The inner rep-
resentation of a parent node is thus a function of
the inner representations of its children.
Computing outer representations Outer repre-
sentations are computed from the top down. For a
node which is not the root, say p
1
, the outer repre-
732
sentation is given by
o
p
1
= g(W
o
1
o
p
2
+ W
o
2
i
z
+ b
o
)
where W
o
1
,W
o
2
are n ? n real matrices, b
o
is a
bias vector, and g is an activation function. The
outer representation of a node is thus a function of
the outer representation of its parent and the inner
representation of its sisters.
If there is information about the external context
of the utterance that is being processed, this infor-
mation determines the outer representation of the
root node o
root
. In our first experiments reported
here, no such information was assumed to be avail-
able. In this case, a random value o
?
is chosen at
initialisation and assigned to the root nodes of all
utterances; this value is then adjusted by the learn-
ing process discussed below.
Training Training the IORNN is to minimise an
objective function J(?) which depends on the pur-
pose of usage where ? is the set of parameters. To
do so, we compute the gradient ?J/?? and ap-
ply the gradient descent method. The gradient is
effectively computed thanks to back-propagation
through structure (Goller and K?uchler, 1996). Fol-
lowing Socher et al. (2013), we use AdaGrad
(Duchi et al., 2011) to update the parameters.
4.3 The?-order Model with IORNN
The RNN and IORNN are defined for context-
free trees. To apply the IORNN architecture to
dependency parses we need to adapt the defini-
tions somewhat. In particular, in the generative
dependency model, every step in the generative
story involves the decision to generate a specific
word while the span of the subtree that this word
will dominate only becomes clear when all depen-
dents are generated. We therefore introduce par-
tial outer representation as a representation of the
current context of a word in the generative pro-
cess, and compute the final outer representation
only when all its siblings have been generated.
Consider an example of head h and its depen-
dents x, y (we ignore directions for simplicity) in
Figure 5. Assume that we are in the state in the
generative process where the generation of h is
complete, i.e. we know its inner and outer rep-
resentations i
h
and o
h
. Now, when generating h?s
first dependent x (see Figure 5-a), we first com-
pute x?s partial outer representation (representing
its context at this stage in the process), which is
a function of the outer representation of the head
(representing the head?s context) and the inner rep-
resentation of the head (representing the content of
the head word):
?
o
1
= f(W
hi
i
h
+ W
ho
o
h
+ b
o
)
where W
hi
,W
ho
are n ? n real matrices, b
o
is a
bias vector, f is an activation function.
With the context of the first dependent deter-
mined, we can proceed and generate its content.
For this purpose, we assume a separate weight ma-
trix W, trained (as explained below) to predict a
specific word given a (partial) outer representa-
tion. To compute a proper probability for word
x, we use the softmax function:
softmax(x,
?
o
1
) =
e
u(x,
?
o
1
)
?
w?V
e
u(w,
?
o
1
)
where
[
u(w
1
,
?
o
1
), ..., u(w
|V |
,
?
o
1
)
]
T
= W
?
o
1
+ b
and V is the set of all possible dependents.
Note that since o
h
, the outer representation of
h, represents the entire dependency structure gen-
erated up to that point,
?
o
1
is a vectorial represen-
tation of the ?-order context generating the first
dependent (like the fragment enclosed in the red
dashed contour in Figure 2). The softmax func-
tion thus estimates the probability P (D = x|C
?
D
).
The next step, now that x is generated, is to
compute the partial outer representation for the
second dependent (see Figure 5-b)
?
o
2
= f(W
hi
i
h
+ W
ho
o
h
+ W
dr(x)
i
x
+ b
o
)
where W
dr(x)
is a n ? n real matrix specific for
the dependency relation of x with h.
Next y is generated (using the softmax function
above), and the partial outer representation for the
third dependent (see Figure 5-c) is computed:
?
o
3
= f(W
hi
i
h
+ W
ho
o
h
+
1
2
(
W
dr(x)
i
x
+ W
dr(y)
i
y
)
+ b
o
)
Since the third dependent is the End-of-
Children symbol (EOC), the process of generat-
ing dependents for h stops. We can then return
to x and y to replace the partial outer represen-
tations with complete outer representations
1
(see
1
According to the IORNN architecture, to compute the
outer representation of a node, the inner representations of
the whole fragments rooting at its sisters must be taken into
account. Here, we replace the inner representation of a frag-
ment by the inner representation of its root since the meaning
of a phrase is often dominated by the meaning of its head.
733
Figure 5: Example of applying IORNN to dependency parsing. Black, grey, white boxes are respectively
inner, partial outer, and outer representations. For simplicity, only links related to the current computation
are drawn (see text).
Figure 5-d,e):
o
x
= f(W
hi
i
h
+ W
ho
o
h
+ W
dr(y)
i
y
+ b
o
)
o
y
= f(W
hi
i
h
+ W
ho
o
h
+ W
dr(x)
i
x
+ b
o
)
In general, if u is the first dependent of h then
?
o
u
= f(W
hi
i
h
+ W
ho
o
h
+ b
o
)
otherwise
?
o
u
= f(W
hi
i
h
+ W
ho
o
h
+ b
o
+
1
|
?
S(u)|
?
v?
?
S(u)
W
dr(v)
i
v
)
where
?
S(u) is the set of u?s sisters generated be-
fore it. And, if u is the only dependent of h (ig-
noring EOC) then
o
u
= f(W
hi
i
h
+ W
ho
o
h
+ b
o
)
otherwise
o
u
= f(W
hi
i
h
+ W
ho
o
h
+ b
o
+
1
|S(u)|
?
v?S(u)
W
dr(v)
i
v
)
where S(u) is the set of u?s sisters.
We then continue this process to generate de-
pendents for x and y until the process stops.
Inner Representations In the calculation of the
probability of generating a word, described above,
we assumed inner representations of all possible
words to be given. These are, in fact, themselves a
function of vector representations for the words (in
our case, the word vectors are initially borrowed
from Collobert et al. (2011)), the POS-tags and
capitalisation features. That is, the inner represen-
tation at a node h is given by:
i
h
= f (W
w
w
h
+ W
p
p
h
+ W
c
c
h
)
where W
w
? R
n?d
w
, W
p
? R
n?d
p
, W
c
?
R
n?d
c
, w
h
is the word vector of h, and p
h
, c
h
are
respectively binary vectors representing the POS-
tag and capitalisation feature of h.
Training Training this IORNN is to minimise
the following objective function which is the reg-
ularised cross-entropy
J(?) =?
1
m
?
T?D
?
w?T
log(P (w|
?
o
w
))
+
1
2
(
?
W
??
W
?
2
+ ?
L
??
L
?
2
)
where D is the set of training dependency parses,
m is the number of dependents; ?
W
, ?
L
are
the weight matrix set and the word embeddings
(? = (?
W
, ?
L
)); ?
W
, ?
L
are regularisation hyper-
parameters.
Implementation We decompose a dependent D
into four features: dependency relation, POS-tag,
lowercase version of word, capitalisation feature
of word. We then factorise P (D|C
?
D
) similarly to
Section 3, where each component is estimated by
a softmax function.
5 Experiments
In our experiments, we convert the Penn Treebank
to dependencies using the Universal dependency
annotation (McDonald et al., 2013)
2
; this yields
a dependency tree corpus we label PTB-U. In or-
der to compare with other systems, we also ex-
periment with an alternative conversion using the
head rules of Yamada and Matsumoto (2003)
3
;
this yields a dependency tree corpus we label PTB-
YM. Sections 2-21 are used for training, section
22 for development, and section 23 for testing. For
the PTB-U, the gold POS-tags are used. For the
PTB-YM, the development and test sets are tagged
by the Stanford POS-tagger
4
trained on the whole
2
https://code.google.com/p/uni-dep-tb/
3
http://stp.lingfil.uu.se/
?
nivre/
research/Penn2Malt.html
4
http://nlp.stanford.edu/software/
tagger.shtml
734
Perplexity
3rd-order model 1736.73
?-order model 236.58
Table 1: Perplexities of the two models on PTB-
U-22.
training data, whereas 10-way jackknifing is used
to generate tags for the training set.
The vocabulary for both models, the third-order
model and the ?-order model, is taken as a list
of words occurring more than two times in the
training data. All other words are labelled ?UN-
KNOWN? and every digit is replaced by ?0?. For
the IORNN used by the ?-order model, we set
n = 200, and define f as the tanh activation func-
tion. We initialise it with the 50-dim word embed-
dings from Collobert et al. (2011) and train it with
the learning rate 0.1, ?
W
= 10
?4
, ?
L
= 10
?10
.
5.1 Perplexity
We firstly evaluate the two models on PTB-U-22
using the perplexity-per-word metric
ppl(P ) = 2
?
1
N
?
T?D
log
2
P (T )
where D is a set of dependency parses, N is the
total number of words. It is worth noting that,
the better P estimates the true distribution P
?
of
D, the lower its perplexity is. Because Eisner?s
model with the dist(H,D) feature (Equation 2)
is leaky (the model allocates some probability to
events that can never legally arise), this feature is
discarded (only in this experiment).
Table 1 shows results. The perplexity of the
third-order model is more than seven times higher
than the?-order model. This reflects the fact that
data sparsity is more problematic for counting than
for the IORNN.
To investigate why the perplexity of the third-
order model is so high, we compute the percent-
ages of events extracted from the development
set appearing more than twice in the training set.
Events are grouped according to the reduction lists
in Equation 2 (see Table 2). We can see that re-
ductions at level 0 (the finest) for dependency re-
lations and words seriously suffer from data spar-
sity: more than half of the events occur less than
three times, or not at all, in the training data. We
thus conclude that counting-based models heavily
rely on carefully designed reduction lists for back-
off.
back-off level d t w c
0 47.4 61.6 43.7 87.7
1 69.8 98.4 77.8 97.3
2 76.0, 89.5 99.7
3 97.9
total 76.1 86.6 60.7 92.5
Table 2: Percentages of events extracted from
PTB-U-22 appearing more than twice in the train-
ing set. Events are grouped according to the reduc-
tion lists in Equation 2. d, t, w, c stand for depen-
dency relation, POS-tag, word, and capitalisation
feature.
5.2 Reranking
In the second experiment, we evaluate the two
models in the reranking framework proposed by
Sangati et al. (2009) on PTB-U. We used the MST-
Parser (with the 2nd-order feature mode) (McDon-
ald et al., 2005) to generate k-best lists. Two
evaluation metrics are labelled attachment score
(LAS) and unlabelled attachment score (UAS), in-
cluding punctuation.
Rerankers Given D(S), a k-best list of parses
of a sentence S, we define the generative reranker
T
?
= arg max
T?D(S)
P (T (ROOT ))
which is identical to Sangati et al. (2009).
Moreover, as in many mixture-model-based ap-
proaches, we define the mixture reranker as a com-
bination of the generative model and the MST dis-
criminative model (Hayashi et al., 2011)
T
?
= arg max
T?D(S)
? logP (T (ROOT ))+(1??)s(S, T )
where s(S, T ) is the score given by the MST-
Parser, and ? ? [0, 1].
Results Figure 6 shows UASs of the generative
reranker on the development set. The MSTParser
achieves 92.32% and the Oracle achieve 96.23%
when k = 10. With the third-order model, the
generative reranker performs better than the MST-
Parser when k < 6 and the maximum improve-
ment is 0.17%. Meanwhile, with the ?-order
model, the generative reranker always gains higher
UASs than the MSTParser, and with k = 6, the
difference reaches 0.7%. Figure 7 shows UASs of
the mixture reranker on the same set. ? is opti-
mised by searching with the step-size 0.005. Un-
surprisingly, we observe improvements over the
735
Figure 6: Performance of the generative reranker
on PTB-U-22.
Figure 7: Performance of the mixture reranker on
PTB-U-22. For each k, ? was optimized with the
step-size 0.005.
LAS UAS
MSTParser 89.97 91.99
Oracle (k = 10) 93.73 96.24
Generative reranker with
3rd-order (k = 3) 90.27 (+0.30) 92.27 (+0.28)
?-order (k = 6) 90.76 (+0.79) 92.83 (+0.84)
Mixture reranker with
3rd-order (k = 6) 90.62 (+0.65) 92.62 (+0.63)
?-order (k = 9) 91.02 (+1.05) 93.08 (+1.09)
Table 3: Comparison based on reranking on PTB-
U-23. The numbers in the brackets are improve-
ments over the MSTParser.
generative reranker as the mixture reranker can
combine the advantages of the two models.
Table 3 shows scores of the two rerankers on the
test set with the parameters tuned on the develop-
ment set. Both the rerankers, either using third-
order or ?-order models, outperform the MST-
Parser. The fact that both gain higher improve-
ments with the ?-order model suggests that the
IORNN surpasses counting.
Figure 9: F1-scores of binned HEAD distance
(PTB-U-23).
5.3 Comparison with other systems
We first compare the mixture reranker using the
?-order model against the state-of-the-art depen-
dency parser TurboParser (with the full mode)
(Martins et al., 2013) on PTB-U-23. Table 4 shows
LASs and UASs. When taking labels into account,
the TurboParser outperforms the reranker. But
without counting labels, the two systems perform
comparably, and when ignoring punctuation the
reranker even outperforms the TurboParser. This
pattern is also observed when the exact match met-
rics are used (see Table 4). This is due to the fact
that the TurboParser performs significantly better
than the MSTParser, which generates k-best lists
for the reranker, in labelling: the former achieves
96.03% label accuracy score whereas the latter
achieves 94.92%.
One remarkable point is that reranking with
the ?-order model helps to improve the exact
match scores 4% - 6.4% (see Table 4). Because
the exact match scores correlate with the ability
to handle global structures, we conclude that the
IORNN is able to capture?-order contexts. Fig-
ure 8 shows distributions of correct-head accuracy
over CPOS-tags and Figure 9 shows F1-scores of
binned HEAD distance. Reranking with the ?-
order model is clearly helpful for all CPOS-tags
and dependent-to-head distances, except a minor
decrease on PRT.
We compare the reranker against other systems
on PTB-YM-23 using the UAS metric ignoring
punctuation (as the standard evaluation for En-
glish) (see Table 5). Our system performs slightly
better than many state-of-the-art systems such as
Martins et al. (2013) (a.k.a. TurboParser), Zhang
and McDonald (2012), Koo and Collins (2010).
It outperforms Hayashi et al. (2011) which is a
reranker using a combination of third-order gen-
erative models with a variational model learnt
736
LAS (w/o punc) UAS (w/o punc) LEM (w/o punc) UEM (w/o punc)
MSTParser 89.97 (90.54) 91.99 (92.82) 32.37 (34.19) 42.80 (45.24)
w. ?-order (k = 9) 91.02 (91.51) 93.08 (93.84) 37.58 (39.16) 49.17 (51.53)
TurboParser 91.56 (92.02) 93.05 (93.70) 40.65 (41.72) 48.05 (49.83)
Table 4: Comparison with the TurboParser on PTB-U-23. LEM and UEM are respectively the labelled
exact match score and unlabelled exact match score metrics. The numbers in brackets are scores com-
puted excluding punctuation.
Figure 8: Distributions of correct-head accuracy over CPOS-tags (PTB-U-23).
System UAS
Huang and Sagae (2010) 92.1
Koo and Collins (2010) 93.04
Zhang and McDonald (2012) 93.06
Martins et al. (2013) 93.07
Bohnet and Kuhn (2012) 93.39
Reranking
Hayashi et al. (2011) 92.89
Hayashi et al. (2013) 93.12
MST+?-order (k = 12) 93.12
Table 5: Comparison with other systems on PTB-
YM-23 (excluding punctuation).
on the fly; performs equally with Hayashi et al.
(2013) which is a discriminative reranker using the
stacked technique; and slightly worse than Bohnet
and Kuhn (2012), who develop a hybrid transition-
based and graphical-based approach.
6 Related Work
Using neural networks to process trees was first
proposed by Pollack (1990) in the Recursive Au-
toassociative Memory model which was used for
unsupervised learning. Socher et al. (2010) later
introduced the Recursive Neural Network archi-
tecture for supervised learning tasks such as syn-
tactic parsing and sentiment analysis (Socher et
al., 2013). Our IORNN is an extension of
the RNN: the former can process trees not only
bottom-up like the latter but also top-down.
Elman (1990) invented the simple recurrent
neural network (SRNN) architecture which is ca-
pable of capturing very long histories. Mikolov
et al. (2010) then applied it to language mod-
elling and gained state-of-the-art results, outper-
forming the the standard n-gram techniques such
as Kneser-Ney smoothed 5-gram. Our IORNN
architecture for dependency parsing bears a re-
semblance to the SRNN in the sense that it can
also capture long ?histories? in context represen-
tations (i.e., outer representations in our terminol-
ogy). Moreover, the IORNN can be seen as a gen-
eralization of the SRNN since a left-branching tree
is equivalent to a chain and vice versa.
The idea of letting parsing decisions depend
on arbitrarily long derivation histories is also ex-
plored in Borensztajn and Zuidema (2011) and
is related to parsing frameworks that allow arbi-
trarily large elementary trees (e.g., Scha (1990),
O?Donnell et al. (2009), Sangati and Zuidema
(2011), and van Cranenburgh and Bod (2013)).
Titov and Henderson (2007) were the first
proposing to use deep networks for dependency
parsing. They introduced a transition-based gen-
erative dependency model using incremental sig-
moid belief networks and applied beam pruning
for searching best trees. Differing from them,
our work uses the IORNN architecture to rescore
k-best candidates generated by an independent
737
graph-based parser, namely the MSTParser.
Reranking k-best lists was introduced by
Collins and Koo (2005) and Charniak and Johnson
(2005). Their rerankers are discriminative and for
constituent parsing. Sangati et al. (2009) proposed
to use a third-order generative model for reranking
k-best lists of dependency parses. Hayashi et al.
(2011) then followed this idea but combined gen-
erative models with a variational model learnt on
the fly to rerank forests. In this paper, we also
followed Sangati et al. (2009)?s idea but used an
?-order generative model, which has never been
used before.
7 Conclusion
In this paper, we proposed a new neural network
architecture, the Inside-Outside Recursive Neural
Network, that can process trees both bottom-up
and top-down. The key idea is to extend the RNN
such that every node in the tree has two vectors
associated with it: an inner representation for its
content, and an outer representation for its context.
Inner and outer representations of any constituent
can be computed simultaneously and interact with
each other. This way, information can flow top-
down, bottom-up, inward and outward. Thanks to
this property, by applying the IORNN to depen-
dency parses, we have shown that using an ?-
order generative model for dependency parsing,
which has never been done before, is practical.
Our experimental results on the English section
of the Universal Dependency Treebanks show that
the ?-order generative model approximates the
true dependency distribution better than the tradi-
tional third-order model using counting, and tends
to choose more accurate parses in k-best lists.
In addition, reranking with this model even out-
performs the state-of-the-art TurboParser on unla-
belled score metrics.
Our source code is available at: github.
com/lephong/iornn-depparse.
Acknowledgments
We thank Remko Scha and three anonymous re-
viewers for helpful comments.
References
Marco Baroni, Georgiana Dinu, and Germ?an
Kruszewski. 2014. Don?t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, volume 1.
Bernd Bohnet and Jonas Kuhn. 2012. The best of
both worlds: a graph-based completion model for
transition-based parsers. In Proceedings of the 13th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 77?87.
Association for Computational Linguistics.
Gideon Borensztajn and Willem Zuidema. 2011.
Episodic grammar: a computational model of the
interaction between episodic and semantic memory
in language processing. In Proceedings of the 33d
Annual Conference of the Cognitive Science Soci-
ety (CogSci?11), pages 507?512. Lawrence Erlbaum
Associates.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 173?180.
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25?66.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational lin-
guistics, 29(4):589?637.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, pages 2121?2159.
Jason M. Eisner. 1996a. An empirical comparison of
probability models for dependency grammar. Tech-
nical report, University of Pennsylvania Institute for
Research in Cognitive Science.
Jason M Eisner. 1996b. Three new probabilistic mod-
els for dependency parsing: An exploration. In Pro-
ceedings of the 16th conference on Computational
linguistics-Volume 1, pages 340?345. Association
for Computational Linguistics.
Jeffrey L. Elman. 1990. Finding structure in time.
Cognitive science, 14(2):179?211.
Christoph Goller and Andreas K?uchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In International
Conference on Neural Networks. IEEE.
Katsuhiko Hayashi, Taro Watanabe, Masayuki Asa-
hara, and Yuji Matsumoto. 2011. Third-order
variational reranking on packed-shared dependency
738
forests. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 1479?1488. Association for Computational
Linguistics.
Katsuhiko Hayashi, Shuhei Kondo, and Yuji Mat-
sumoto. 2013. Efficient stacked dependency pars-
ing by forest reranking. Transactions of the Associ-
ation for Computational Linguistics, 1(1):139?150.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1077?
1086. Association for Computational Linguistics.
Pentti Kanerva. 2009. Hyperdimensional comput-
ing: An introduction to computing in distributed rep-
resentation with high-dimensional random vectors.
Cognitive Computation, 1(2):139?159.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1?11. Association for
Computational Linguistics.
Phong Le, Willem Zuidema, and Remko Scha. 2013.
Learning from errors: Using vector-based composi-
tional semantics for parse reranking. In Proceedings
Workshop on Continuous Vector Space Models and
their Compositionality (at ACL 2013). Association
for Computational Linguistics.
Andre Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the turbo: Fast third-order non-
projective turbo parsers. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
617?622, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of
dependency parsers. In Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 91?98. Association for Computa-
tional Linguistics.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Os-
car T?ackstr?om, et al. 2013. Universal dependency
annotation for multilingual parsing. Proceedings of
ACL, Sofia, Bulgaria.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Timothy J O?Donnell, Noah D Goodman, and Joshua B
Tenenbaum. 2009. Fragment grammar: Exploring
reuse in hierarchical generative processes. Techni-
cal report, Technical Report MIT-CSAIL-TR-2009-
013, MIT.
Jordan B. Pollack. 1990. Recursive distributed repre-
sentations. Artificial Intelligence, 46(1):77105.
Federico Sangati and Willem Zuidema. 2011. Ac-
curate parsing with compact tree-substitution gram-
mars: Double-DOP. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMLNP?11), pages 84?95. Association
for Computational Linguistics.
Federico Sangati, Willem Zuidema, and Rens Bod.
2009. A generative re-ranking model for depen-
dency parsing. In Proceedings of the 11th Inter-
national Conference on Parsing Technologies, pages
238?241.
Remko Scha. 1990. Taaltheorie en taaltechnolo-
gie; competence en performance. In R. de Kort
and G.L.J. Leerdam, editors, Computertoepassin-
gen in de Neerlandistiek, pages 7?22. LVVN,
Almere, the Netherlands. English translation at
http://iaaa.nl/rs/LeerdamE.html.
Richard Socher, Christopher D. Manning, and An-
drew Y. Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, Seattle, Washington, USA, October.
Ivan Titov and James Henderson. 2007. A latent vari-
able model for generative dependency parsing. In
Proceedings of the 10th International Conference on
Parsing Technologies, pages 144?155.
Andreas van Cranenburgh and Rens Bod. 2013. Dis-
continuous parsing with an efficient and accurate
DOP model. In Proceedings of the International
Conference on Parsing Technologies (IWPT?13).
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of International Conference
on Parsing Technologies (IWPT), pages 195?206.
Hao Zhang and Ryan McDonald. 2012. Generalized
higher-order dependency parsing with cube prun-
ing. In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 320?331. Association for Computational Lin-
guistics.
739
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 11?19,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Learning from errors: Using vector-based compositional semantics for
parse reranking
Phong Le, Willem Zuidema, Remko Scha
Institute for Logic, Language, and Computation
University of Amsterdam, the Netherlands
{p.le,zuidema,scha}@uva.nl
Abstract
In this paper, we address the problem of
how to use semantics to improve syntac-
tic parsing, by using a hybrid reranking
method: a k-best list generated by a sym-
bolic parser is reranked based on parse-
correctness scores given by a composi-
tional, connectionist classifier. This classi-
fier uses a recursive neural network to con-
struct vector representations for phrases in
a candidate parse tree in order to classify
it as syntactically correct or not. Tested on
the WSJ23, our method achieved a statisti-
cally significant improvement of 0.20% on
F-score (2% error reduction) and 0.95% on
exact match, compared with the state-of-
the-art Berkeley parser. This result shows
that vector-based compositional semantics
can be usefully applied in syntactic pars-
ing, and demonstrates the benefits of com-
bining the symbolic and connectionist ap-
proaches.
1 Introduction
Following the idea of compositionality in formal
semantics, compositionality in vector-based se-
mantics is also based on the principle of composi-
tionality, which says that ?The meaning of a whole
is a function of the meanings of the parts and of
the way they are syntactically combined? (Partee,
1995). According to this principle, composing the
meaning of a phrase or sentence requires a syntac-
tic parse tree, which is, in most current systems,
given by a statistical parser. This parser, in turn, is
trained on syntactically annotated corpora.
However, there are good reasons to also con-
sider information flowing in the opposite direc-
tion: from semantics to syntactic parsing. Per-
formance of parsers trained and evaluated on the
Penn WSJ treebank has reached a plateau, as many
ambiguities cannot be resolved by syntactic infor-
mation alone. Further improvements in parsing
may depend on the use of additional sources of in-
formation, including semantics. In this paper, we
study the use of semantics for syntactic parsing.
The currently dominant approach to syntactic
parsing is based on extracting symbolic grammars
from a treebank and defining appropriate proba-
bility distributions over the parse trees that they
license (Charniak, 2000; Collins, 2003; Klein
and Manning, 2003; Petrov et al, 2006; Bod et
al., 2003; Sangati and Zuidema, 2011; van Cra-
nenburgh et al, 2011). An alternative approach,
with promising recent developments (Socher et
al., 2010; Collobert, 2011), is based on us-
ing neural networks. In the present paper, we
combine the ?symbolic? and ?connectionist? ap-
proaches through reranking: a symbolic parser
is used to generate a k-best list which is then
reranked based on parse-correctness scores given
by a connectionist compositional-semantics-based
classifier.
The idea of reranking is motivated by anal-
yses of the results of state-of-the-art symbolic
parsers such as the Brown and Berkeley parsers,
which have shown that there is still considerable
room for improvement: oracle results on 50-best
lists display a dramatic improvement in accuracy
(96.08% vs. 90.12% on F-score and 65.56% vs.
37.22% on exact match with the Berkeley parser).
This suggests that parsers that rely on syntactic
corpus-statistics, though not sufficient by them-
selves, may very well serve as a basis for sys-
tems that integrate other sources of information by
means of reranking.
One important complementary source of infor-
mation is the semantic plausibility of the con-
stituents of the syntactically viable parses. The ex-
ploitation of that kind of information is the topic
of the research we report here. In this work,
we follow up on a proposal by Mark Steedman
11
(1999), who suggested that the realm of seman-
tics lacks the clearcut hierarchical structures that
characterise syntax, and that semantic information
may therefore be profitably treated by the clas-
sificatory mechanisms of neural nets?while the
treatment of syntactic structures is best left to sym-
bolic parsers. We thus developed a hybrid system,
which parses its input sentences on the basis of a
symbolic probabilistic grammar, and reranks the
candidate parses based on scores given by a neural
network.
Our work is inspired by the work of Socher and
colleagues (2010; 2011). They proposed a parser
using a recursive neural network (RNN) for en-
coding parse trees, representing phrases in a vec-
tor space, and scoring them. Their experimental
result (only 1.92% lower than the Stanford parser
on unlabelled bracket F-score for sentences up to a
length of 15 words) shows that an RNN is expres-
sive enough for syntactic parsing. Additionally,
their qualitative analysis indicates that the learnt
phrase features capture some aspects of phrasal se-
mantics, which could be useful to resolve semantic
ambiguity that syntactical information alone can
not. Our work in this paper differs from their work
in that we replace the parsing task by a reranking
task, and thus reduce the object space significantly
to a set of parses generated by a symbolic parser
rather than the space of all parse trees. As a result,
we can apply our method to sentences which are
much longer than 15 words.
Reranking a k-best list is not a new idea.
Collins (2000), Charniak and Johnson (2005), and
Johnson and Ural (2010) have built reranking sys-
tems with performances that are state-of-the-art.
In order to achieve such high F-scores, those
rerankers rely on a very large number of features
selected on the basis of expert knowledge. Unlike
them, our feature set is selected automatically, yet
the reranker achieved a statistically significant im-
provement on both F-score and exact match.
Closest to our work is Menchetti et al (2005)
and Socher et al (2013): both also rely on sym-
bolic parsers to reduce the search space and use
RNNs to score candidate parses. However, our
work differs in the way the feature set for rerank-
ing is selected. In their methods, only the score at
the tree root is considered whereas in our method
the scores at all internal nodes are taken into ac-
count. Selecting the feature set like that gives us a
flexible way to deal with errors accumulated from
the leaves to the root.
Figure 1 shows a diagram of our method. First,
a parser (in this paper: the Berkeley parser) is used
to generate k-best lists of the Wall Street Jour-
nal (WSJ) sections 02-21. Then, all parse trees in
these lists and the WSJ02-21 are preprocessed by
marking head words, binarising, and performing
error-annotation (Section 2). After that, we use
the annotated trees to train our parse-correctness
classifier (Section 3). Finally, those trees and the
classifier are used to train the reranker (Section 4).
2 Experimental Setup
The experiments presented in this paper have the
following setting. We use the WSJ corpus with
the standard splits: sections 2-21 for training, sec-
tion 22 for development, and section 23 for test-
ing. The latest implementation (version 1.7) of the
Berkeley parser1 (Petrov et al, 2006) is used for
generating 50-best lists. We mark head words and
binarise all trees in the WSJ and the 50-best lists
as in Subsection 2.1, and annotate them as in Sub-
section 2.2 (see Figure 2).
2.1 Preprocessing Trees
We preprocess trees by marking head words and
binarising the trees. For head word marking,
we used the head finding rules of Collins (1999)
which are implemented in the Stanford parser.
To binarise a k-ary branching, e.g. P ?
C1 ... H ... Ck where H is the top label of the
head constituent, we use the following method. If
H is not the left-most child, then
P ? C1 @P ; @P ? C2 ... H ... Ck
otherwise,
P ? @P Ck ; @P ? H ... Ck?1
where @P , which is called extra-P , now is the
head of P . We then apply this transformation
again on the children until we reach terminal
nodes. In this way, we ensure that every internal
node has one head word.
2.2 Error Annotation
We annotate nodes (as correct or incorrect) as fol-
lows. Given a parse tree T in a 50-best list and
a corresponding gold-standard tree G in the WSJ,
1https://code.google.com/p/berkeleyparser
12
Figure 1: An overview of our method.
Figure 2: Example for preprocessing trees. Nodes marked with (*) are labelled incorrect whereas the
other nodes are labelled correct.
we first attempt to align their terminal nodes ac-
cording to the following criterion: a terminal node
t is aligned to a terminal node g if they are at
the same position counting from-left-to-right and
they have the same label. Then, a non-terminal
node P [wh] with children C1, ..., Ck is aligned to
a gold-standard non-terminal node P ?[w?h] with
children C?1 , ..., C
?
l (1 ? k, l ? 2 in our case)
if they have the same word head, the same syn-
tactical category, and their children are all aligned
in the right order. In other words, the following
conditions have to be satisfied
P = P ? ; wh = w?h ; k = l
Ci is aligned to C?i , for all i = 1..k
Aligned nodes are annotated as correct whereas
the other nodes are annotated as incorrect.
3 Parse-Correctness Classification
This section describes how a neural network
is used to construct vector representations for
phrases given parse trees and to identify if those
trees are syntactically correct or not. In order to
encode tree structures, we use an RNN2 (see Fig-
ure 3 and Figure 4) which is similar to the one
proposed by Socher and colleagues (2010). How-
ever, unlike their RNN, our RNN can handle unary
branchings, and also takes head words and syntac-
tic tags as input. It is worth noting that, although
we can use some transformation to remove unary
branchings, handling them is helpful in our case
because the system avoids dealing with so many
syntactic tags that would result from the transfor-
2The first neural-network approach attempting to operate
and represent compositional, recursive structure is the Recur-
sive Auto-Associative Memory network (RAAM), which was
proposed by Pollack (1988). In order to encode a binary tree,
the RAAM network contains three layers: an input layer for
two daughter nodes, a hidden layer for their parent node, and
an output layer for their reconstruction. Training the network
is to minimise the reconstruction error such that we can de-
code the information captured in the hidden layer to the orig-
inal tree form. Our RNN differs from the RAAM network in
that its output layer is not for reconstruction but for classifi-
cation.
13
mation. In addition, using a new set of weight ma-
trices for unary branchings makes our RNN more
expressive without facing the problem of sparsity
thanks to a large number of unary branchings in
the treebank.
Figure 3: An RNN attached to the parse tree
shown in the top-right of Figure 2. All unary
branchings share a set of weight matrices, and all
binary branchings share another set of weight ma-
trices (see Figure 4).
An RNN processes a tree structure by repeat-
edly applying itself at each internal node. Thus,
walking bottom-up from the leaves of the tree to
the root, we compute for every node a vector based
on the vectors of its children. Because of this
process, those vectors have to have the same di-
mension. It is worth noting that, because informa-
tion at leaves, i.e. lexical semantics, is composed
according to a given syntactic parse, what a vec-
tor at each internal node captures is some aspects
of compositional semantics of the corresponding
phrase. In the remainder of this subsection, we
describe in more detail how to construct composi-
tional vector-based semantics geared towards the
parse-correctness classification task.
Similar to Socher et al (2010), and Col-
lobert (2011), given a string of words (w1, ..., wl),
we first compute a string of vectors (x1, ..., xl)
representing those words by using a look-up table
(i.e., word embeddings) L ? Rn?|V |, where |V | is
the size of the vocabulary and n is the dimension-
ality of the vectors. This look-up table L could
be seen as a storage of lexical semantics where
each column is a vector representation of a word.
Hence, let bi be the binary representation of word
wi (i.e., all of the entries of bi are zero except the
one corresponding to the index of the word in the
dictionary), then
xi = Lbi ? Rn (1)
We also encode syntactic tags by binary vectors
but put an extra bit at the end of each vector to
mark if the corresponding tag is extra or not (i.e.,
@P or P ).
Figure 4: Details about our RNN for a unary
branching (top) and a binary branching (bottom).
The bias is not shown for the simplicity.
Then, given a unary branching P [wh]? C, we
can compute the vector at the node P by (see Fig-
ure 4-top)
p = f
(
Wuc+Whxh +W?1x?1 +
W+1x+1 +Wttp + bu
)
where c, xh are vectors representing the child C
and the head word, x?1, x+1 are the left and right
neighbouring words of P , tp encodes the syn-
tactic tag of P , Wu,Wh,W?1,W+1 ? Rn?n,
Wt ? Rn?(|T |+1), |T | is the size of the set of
syntactic tags, bu ? Rn, and f can be any acti-
vation function (tanh is used in this case). With
a binary branching P [wh] ? C1 C2, we simply
change the way the children?s vectors added (see
Figure 4-bottom)
p = f
(
Wb1c1 +Wb2c2 +Whxh +W?1x?1 +
W+1x+1 +Wttp + bb
)
Finally, we put a sigmoid neural unit on the
top of each internal node (except pre-terminal
nodes because we are not concerned with POS-
tagging) to detect the correctness of the subparse
tree rooted at that node
y = sigmoid(Wcatp+ bcat) (2)
where Wcat ? R1?n, bcat ? R.
14
3.1 Learning
The error on a parse tree is computed as the sum
of classification errors of all subparses. Hence, the
learning is to minimise the objective
J(?) =
1
N
?
T
?
(y(?),t)?T
1
2
(t? y(?))2 + ????2
(3)
where ? are the model parameters, N is the num-
ber of trees, ? is a regularisation hyperparameter,
T is a parse tree, y(?) is given by Equation 2, and
t is the class of the corresponding subparse (t = 1
means correct). The gradient ?J?? is computed ef-
ficiently thanks to backpropagation through the
structure (Goller and Kuchler, 1996). L-BFGS
(Liu and Nocedal, 1989) is used to minimise the
objective function.
3.2 Experiments
We implemented our classifier in Torch73 (Col-
lobert et al, 2011a), which is a powerful Matlab-
like environment for machine learning. In order to
save time, we only trained the classifier on 10-best
parses of WSJ02-21. The training phase took six
days on a computer with 16 800MHz CPU-cores
and 256GB RAM. The word embeddings given by
Collobert et al (2011b)4 were used as L in Equa-
tion 1. Note that these embeddings, which are the
result of training a language model neural network
on the English Wikipedia and Reuters, have been
shown to capture many interesting semantic simi-
larities between words.
We tested the classifier on the development
set WSJ22, which contains 1, 700 sentences, and
measured the performance in positive rate and
negative rate
pos-rate =
#true pos
#true pos +#false neg
neg-rate =
#true neg
#true neg +#false pos
The positive/negative rate tells us the rate at which
positive/negative examples are correctly labelled
positive/negative. In order to achieve high per-
formance in the reranking task, the classifier must
have a high positive rate as well as a high nega-
tive rate. In addition, percentage of positive exam-
ples is also interesting because it shows the unbal-
ancedness of the data. Because the accuracy is not
3http://www.torch.ch/
4http://ronan.collobert.com/senna/
a reliable measurement when the dataset is highly
unbalanced, we do not show it here. Table 1, Fig-
ure 5, and Figure 6 show the classification results.
pos-rate (%) neg-rate (%) %-Pos
gold-std 75.31 - 1
1-best 90.58 64.05 71.61
10-best 93.68 71.24 61.32
50-best 95.00 73.76 56.43
Table 1: Classification results on the WSJ22 and
the k-best lists.
Figure 5: Positive rate, negative rate, and percent-
age of positive examples w.r.t. subtree depth.
3.3 Discussion
Table 1 shows the classification results on the
gold-standard, 1-best, 10-best, and 50-best lists.
The positive rate on the gold-standard parses,
75.31%, gives us the upper bound of %-pos when
this classifier is used to yield 1-best lists. On the 1-
best data, the classifier missed less than one tenth
positive subtrees and correctly found nearly two
third of the negative ones. That is, our classi-
fier might be useful for avoiding many of the mis-
takes made by the Berkeley parser, whilst not in-
troducing too many new mistakes of its own. This
fact gave us hope to improve parsing performance
when using this classifier for reranking.
Figure 5 shows positive rate, negative rate, and
percentage of positive examples w.r.t. subtree
depth on the 50-best data. We can see that the pos-
itive rate is inversely proportional to the subtree
depth, unlike the negative rate. That is because the
15
Figure 6: Positive rate, negative rate, and percentage of positive samples w.r.t. syntactic categories
(excluding POS tags).
deeper a subtree is, the lower the a priori likeli-
hood that the subtree is positive (we can see this
in the percentage-of-positive-example curve). In
addition, deep subtrees are difficult to classify be-
cause uncertainty is accumulated when propagat-
ing from bottom to top.
4 Reranking
In this section, we describe how we use the above
classifier for the reranking task. First, we need to
represent trees in one vector space, i.e., ?(T ) =
(
?1(T ), ..., ?v(T )
)
for an arbitrary parse tree T .
Collins (2000), Charniak and Johnson (2005), and
Johnson and Ural (2010) set the first entry to the
model score and the other entries to the number of
occurrences of specific discrete hand-chosen prop-
erties (e.g., how many times the word pizza comes
after the word eat) of trees. We here do the same
with a trick to discretize results from the classifier:
we use a 2D histogram to store predicted scores
w.r.t. subtree depth. This gives us a flexible way to
penalise low score subtrees and reward high score
subtrees w.r.t. the performance of the classifier at
different depths (see Subsection 3.3). However,
unlike the approaches just mentioned, we do not
use any expert knowledge for feature selection; in-
stead, this process is fully automatic.
Formally speaking, a vector feature ?(T ) is
computed as following. ?1(T ) is the model score
(i.e., max-rule-sum score) given by the parser,
(
?2(T ), ..., ?v(T )
)
is the histogram of a set of
(y, h) where y is given by Equation 2 and h is the
depth of the corresponding subtree. The domain
of y (i.e., [0, 1]) is split into ?y equal bins whereas
the domain of h (i.e., {1, 2, 3, ...}) is split into ?h
bins such that the i-th (i < ?h) bin corresponds to
subtrees of depth i and the ?h-th bin corresponds
to subtrees of depth equal or greater than ?h. The
parameters ?y and ?h are then estimated on the de-
velopment set.
After extracting feature vectors for parse trees,
we then find a linear ranking function
f(T ) = w>?(T )
such that
f(T1) > f(T2) iff fscore(T1) > fscore(T2)
where fscore(.) is the function giving F-score, and
w ? Rv is a weight vector, which is efficiently
estimated by SVM ranking (Yu and Kim, 2012).
SVM was initially used for binary classification.
Its goal is to find the hyperplane which has the
largest margin to best separate two example sets. It
was then proved to be efficient in solving the rank-
ing task in information retrieval, and in syntactic
parsing (Shen and Joshi, 2003; Titov and Hender-
son, 2006). In our experiments, we used SVM-
16
Rank5 (Joachims, 2006), which runs extremely
fast (less than two minutes with about 38, 000 10-
best lists).
4.1 Experiments
Using the classifier in Section 3, we implemented
the reranker in Torch7, trained it on WSJ02-21.
We used WSJ22 to estimate the parameters ?y and
?h by the grid search and found that ?y = 9 and
?h = 4 yielded the best F-score.
Table 2 shows the results of our reranker on
50-best WSJ23 given by the Berkeley parser, us-
ing the standard evalb. Our method improves
0.20% on F-score for sentences with all length,
and 0.22% for sentences with ? 40 words.
These differences are statistically significant6 with
p-value < 0.003. Our method also improves ex-
act match (0.95% for all sentences as well as for
sentences with ? 40 words).
Parser LR LP LF EX
all
Berkeley parser 89.98 90.25 90.12 37.22
This paper 90.10 90.54 90.32 38.17
Oracle 95.94 96.21 96.08 65.56
? 40 words
Berkeley parser 90.43 90.70 90.56 39.65
This paper 90.57 91.01 90.78 40.50
Oracle 96.47 96.73 96.60 68.51
Table 2: Reranking results on 50-best lists on
WSJ23 (LR is labelled recall, LP is labelled pre-
cision, LF is labelled F-score, and EX is exact
match.)
Table 3 shows the comparison of the three
parsers that use the same hybrid reranking ap-
proach. On F-score, our method performed 0.1%
lower than Socher et al (2013), and 1.5% better
than Menchetti et al (2005). However, our method
achieved the least improvement on F-score over its
corresponding baseline. That could be because our
baseline parser (i.e., the Berkeley parser) performs
much better than the other two baseline parsers;
and hence, detecting errors it makes on candidate
parse trees is more difficult.
5www.cs.cornell.edu/people/tj/svm light/svm rank.html
6We used the ?Significance testing for evalua-
tion statistics? software (http://www.nlpado.de/ sebas-
tian/software/sigf.shtml) given by Pado? (2006).
Parser LF (all) K-best
parser
Menchetti et
al. (2005)
88.8 (0.6) Collins
(1999)
Socher et
al. (2013)
90.4 (3.8) PCFG Stan-
ford parser
This paper 90.3 (0.2) Berkeley
parser
Table 3: Comparison of parsers using the same hy-
brid reranking approach. The numbers in the blan-
kets indicate the improvements on F-score over the
corresponding baselines (i.e., the k-best parsers).
5 Conclusions
This paper described a new reranking method
which uses semantics in syntactic parsing: a sym-
bolic parser is used to generate a k-best list which
is later reranked thanks to parse-correctness scores
given by a connectionist compositional-semantics-
based classifier. Our classifier uses a recursive
neural network, like Socher et al, (2010; 2011), to
not only represent phrases in a vector space given
parse trees, but also identify if these parse trees are
grammatically correct or not.
Tested on WSJ23, our method achieved a
statistically significant improvement on F-score
(0.20%) as well as on exact match (0.95%).
This result, although not comparable to the re-
sults reported by Collins (2000), Charniak and
Johnson (2005), and Johnson and Ural (2010),
shows an advantage of using vector-based com-
positional semantics to support available state-of-
the-art parsers.
One of the limitations of the current paper is the
lack of a qualitative analysis of how learnt vector-
based semantics has affected the reranking results.
Therefore, the need for ?compositional seman-
tics? in syntactical parsing may still be doubted.
In future work, we will use vector-based seman-
tics together with non-semantic features (e.g., the
ones of Charniak and Johnson (2005)) to find out
whether the semantic features are truly helpful or
they just resemble non-semantic features.
Acknowledgments
We thank two anonymous reviewers for helpful
comments.
17
References
Rens Bod, Remko Scha, and Khalil Sima?an. 2003.
Data-Oriented Parsing. CSLI Publications, Stan-
ford, CA.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173?180. Association for Computational Lin-
guistics.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics, pages 132?139. Association for
Computational Linguistics.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the In-
ternational Workshop on Machine Learning (then
Conference), pages 175?182.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational lin-
guistics, 29(4):589?637.
Ronan Collobert, Koray Kavukcuoglu, and Cle?ment
Farabet. 2011a. Torch7: A matlab-like environment
for machine learning. In BigLearn, NIPS Workshop.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011b. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Ronan Collobert. 2011. Deep learning for efficient
discriminative parsing. In International Conference
on Artificial Intelligence and Statistics (AISTATS).
Christoph Goller and Andreas Kuchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In IEEE Inter-
national Conference on Neural Networks, volume 1,
pages 347?352. IEEE.
Thorsten Joachims. 2006. Training linear SVMs in lin-
ear time. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery
and data mining, pages 217?226. ACM.
Mark Johnson and Ahmet Engin Ural. 2010. Rerank-
ing the Berkeley and Brown parsers. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 665?668. As-
sociation for Computational Linguistics.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, Volume 1, pages 423?430. Asso-
ciation for Computational Linguistics.
Dong C Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical programming, 45(1-3):503?528.
Sauro Menchetti, Fabrizio Costa, Paolo Frasconi, and
Massimiliano Pontil. 2005. Wide coverage natu-
ral language processing using kernel methods and
neural networks for structured data. Pattern Recogn.
Lett., 26(12):1896?1906, September.
Sebastian Pado?, 2006. User?s guide to sigf: Signifi-
cance testing by approximate randomisation.
Barbara Partee. 1995. Lexical semantics and compo-
sitionality. In L. R. Gleitman and M. Liberman, ed-
itors, Language. An Invitation to Cognitive Science,
volume 1, pages 311?360. MIT Press, Cambridge,
MA.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 433?
440. Association for Computational Linguistics.
Jordan B Pollack. 1988. Recursive auto-associative
memory. Neural Networks, 1:122.
Federico Sangati and Willem Zuidema. 2011. Ac-
curate parsing with compact tree-substitution gram-
mars: Double-DOP. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 84?95. Association for Computa-
tional Linguistics.
Libin Shen and Aravind K Joshi. 2003. An SVM based
voting algorithm with application to parse reranking.
In Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 9?16. Association for Computational Linguis-
tics.
Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop.
Richard Socher, Cliff C Lin, Andrew Y Ng, and
Christopher D Manning. 2011. Parsing natural
scenes and natural language with recursive neu-
ral networks. In Proceedings of the 26th Inter-
national Conference on Machine Learning (ICML),
volume 2.
18
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing With Composi-
tional Vector Grammars. In Proceedings of the ACL
conference (to appear).
Mark Steedman. 1999. Connectionist sentence
processing in perspective. Cognitive Science,
23(4):615?634.
Ivan Titov and James Henderson. 2006. Loss mini-
mization in parse reranking. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing, pages 560?567. Association
for Computational Linguistics.
Andreas van Cranenburgh, Remko Scha, and Federico
Sangati. 2011. Discontinuous Data-Oriented Pars-
ing: A mildly context-sensitive all-fragments gram-
mar. In Proceedings of the Second Workshop on Sta-
tistical Parsing of Morphologically Rich Languages,
pages 34?44. Association for Computational Lin-
guistics.
Hwanjo Yu and Sungchul Kim. 2012. SVM tutorial:
Classification, regression, and ranking. In Grzegorz
Rozenberg, Thomas Ba?ck, and Joost N. Kok, ed-
itors, Handbook of Natural Computing, volume 1,
pages 479?506. Springer.
19
