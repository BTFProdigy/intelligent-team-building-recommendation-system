Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 207?215,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Linguistic Profiling based on General?purpose Features and Native
Language Identification
Andrea Cimino, Felice Dell?Orletta, Giulia Venturi and Simonetta Montemagni
Istituto di Linguistica Computazionale ?Antonio Zampolli? (ILC?CNR)
ItaliaNLP Lab - www.italianlp.it
via G. Moruzzi, 1 ? Pisa (Italy)
{name.surname}@ilc.cnr.it
Abstract
In this paper, we describe our approach to na-
tive language identification and discuss the re-
sults we submitted as participants to the First
NLI Shared Task. By resorting to a wide set of
general?purpose features qualifying the lexi-
cal and grammatical structure of a text, rather
than to ad hoc features specifically selected
for the NLI task, we achieved encouraging re-
sults, which show that the proposed approach
is general?purpose and portable across differ-
ent tasks, domains and languages.
1 Introduction
Since the seminal work by Koppel et al (2005),
within the Computational Linguistics community
there has been a growing interest in the NLP?based
Native Language Identification (henceforth, NLI)
task. However, so far, due to the unavailability
of balanced and wide?coverage benchmark corpora
and the lack of evaluation standards it has been dif-
ficult to compare the results achieved for this task
with different methods and techniques (Tetreault et
al., 2013). The First Shared Task on Native Lan-
guage Identification (Tetreault et al, 2013) can be
seen as an answer to the above mentioned problems.
In this paper, we describe our approach to na-
tive language identification and discuss the results
we submitted as participants to the First NLI Shared
Task. Following the guidelines by the Shared Task
Organizers based on the previous literature on this
topic, Native Language Identification is tackled as
a text classification task combining NLP?enabled
feature extraction and machine learning: see e.g.
Tetreault et al (2013) and Brooke and Hirst (2012).
Interestingly, the same methodological paradigm is
shared by other tasks like e.g. author recognition and
verification (see e.g. van Halteren (2004), author-
ship attribution (see Juola (2008) for a survey), genre
identification (Mehler et al, 2011) as well as read-
ability assessment (see Dell?Orletta et al (2011a) for
an updated survey), all relying on feature extraction
from automatically parsed texts and state?of?the?art
machine learning algorithms. Besides obvious dif-
ferences at the level of the typology of selected lin-
guistic features and of learning techniques, these dif-
ferent tasks share a common approach to the prob-
lems they tackle: i.e. they succeed in determining
the language variety, the author, the text genre or the
level of readability of a text by exploiting the distri-
bution of different types of linguistic features auto-
matically extracted from texts.
Our approach to NLI relies on multi?level lin-
guistic analysis, covering morpho?syntactic tagging
and dependency parsing. In the NLI literature, the
range of features used is wide and includes char-
acteristics of the linguistic structure underlying the
L2 text, encoded in terms of sequences of charac-
ters, words, grammatical categories or of syntac-
tic constructions, as well as of the document struc-
ture: note however that, in most part of the cases,
the exploited features are task?specific. In our ap-
proach, we decided to resort to a wide set of fea-
tures ranging across different levels of linguistic de-
scription (i.e. lexical, morpho?syntactic and syntac-
tic) without any a priori selection: the same set of
features was successfully exploited in NLI?related
tasks, i.e. focusing on the linguistic form rather than
207
the content of texts, such as readability assessment
(Dell?Orletta et al, 2011a) or the classification of
textual genres (Dell?Orletta et al, 2012).
The exploitation of general features qualifying the
lexical and grammatical structure of a text, rather
than ad hoc features specifically selected for the task
at hand, is not the only peculiarity of our approach
to NLI. Following Biber (1993), we start from the
assumption that ?linguistic features from all levels
function together as underlying dimensions of vari-
ation?. This choice stems from studies on linguis-
tic variation, in particular from Biber and Conrad
(2009) who claim that linguistic varieties ? called
?registers? from a functional perspective ? differ ?in
their characteristic distributions of pervasive linguis-
tic features, not the single occurrence of an indi-
vidual feature?. This is to say that by carrying out
the linguistic analysis of collections of essays each
written by different L1 native speakers, we need to
quantify the extent to which a given feature occurs
in each collection, in order to reconstruct the lin-
guistic profile underlying each L1 collection: dif-
ferences lie at the level of the distribution of linguis-
tic features, which can be common and pervasive in
some L1 collections but comparatively rare in oth-
ers. This approach is the basis of so?called ?linguis-
tic profiling? of texts, within which ?the occurrences
of a large number of linguistic features in a text, ei-
ther individual items or combinations of items, are
counted? (van Halteren, 2004) with the final aim of
reconstructing the profile of a text.
We carried out native language identification in
two steps. The first step consisted of the identifi-
cation of the set of linguistic features characteriz-
ing the essays written by different L1 native speak-
ers, i.e. the linguistic profiling of the different sec-
tions of TOEFL11 corpus (Blanchard et al, 2013)
distributed as training and development data. In
the second step, the features which turned out to
have highly discriminative power were used for the
classification of essays written by different L1 na-
tive speakers. Essay classification has been carried
out by experimenting with different approaches: i.e.
a single?classifier method and two different multi?
model ensemble approaches.
The paper is organised as follows: after introduc-
ing the set of used linguistic features (Section 2),
Section 3 illustrates a selection of the linguistic
profiling results obtained with respect to the train-
ing section of the TOEFL11 corpus; Section 4 de-
scribes the different classification approaches we
followed and the feature selection process; in Sec-
tion 5 achieved results are reported and discussed.
2 Features
In this study, we focused on a wide set of features
ranging across different levels of linguistic descrip-
tion. Differing from previous work on NLI, no a
priori selection of features was carried out. Instead
of focusing on particular classes of errors or on dif-
ferent types of stylistic idiosyncrasies, we took into
account a wide range of features which are typically
used in studies focusing on the ?form? of a text,
e.g. on issues of genre, style, authorship or read-
ability. As previously pointed out, this represents a
peculiarity of our approach. This choice makes the
selected features language?independent, domain?
independent and reusable across different types of
tasks, as empirically demonstrated in Dell?Orletta
et al (2011a) where the same set of features has
been successfully exploited for readability assess-
ment, and in Dell?Orletta et al (2012) where the fea-
tures have been used for the classification of differ-
ent types of textual genre. Note that in both cases the
language dealt with was Italian: for the NLI Shared
Task we had to specialize the feature extraction pro-
cess with respect to the English language as well as
to the annotation scheme used to represent the un-
derlying linguistic structure.
The whole set of features we started with is de-
scribed below, organised into four main categories:
namely, raw text and lexical features as well as
morpho-syntactic and syntactic features. This pro-
posed four?fold partition closely follows the differ-
ent levels of linguistic analysis automatically car-
ried out on the text being evaluated, i.e. tokeniza-
tion, lemmatization, morpho-syntactic tagging and
dependency parsing.
2.1 Raw and Lexical Text Features
Sentence Length, calculated as the average number
of words per sentence.
Word Length, calculated as the average number of
characters per word.
Document Length, calculated as the total number
208
of words per document.
Character bigrams.
Word n-grams, including both unigrams and bi-
grams.
Type/Token Ratio: the Type/Token Ratio (TTR) is
a measure of vocabulary variation which has shown
to be a helpful measure of lexical variety within
a text as well as style marker in an authorship at-
tribution scenario: a text characterized by a low
type/token ratio will contain a great deal of repeti-
tion whereas a high type/token ratio reflects vocabu-
lary richness and variation. Due to its sensitivity to
sample size, TTR has been computed for text sam-
ples of equivalent length (the first 50 tokens).
2.2 Morpho?syntactic Features
Coarse grained Part-Of-Speech n-grams: distri-
bution of unigrams and bigrams of coarse?grained
PoS, corresponding to the main grammatical cate-
gories (e.g. noun, verb, adjective, etc.).
Fine grained Part-Of-Speech n-grams: distribu-
tion of unigrams and bigrams of fine?grained PoS,
which represent subdivisions of the coarse?grained
tags (e.g. the class of nouns is subdivided into proper
vs common nouns, verbs into main verbs, gerund
forms, past particles, etc.).
Verbal chunks: distribution of sequences of verbal
PoS (also including adverbs). This feature can be
seen as a proxy to capture different aspects of verbal
predication, with particular attention to idiosyncratic
usages of verbal mood, tense, person and adverbial
modification.
Lexical density: ratio of content words (verbs,
nouns, adjectives and adverbs) to the total number
of lexical tokens in a text.
2.3 Syntactic Features
Dependency types n-grams: distribution of uni-
grams and bigrams of dependency types calculated
with respect to i) the hierarchical parse tree structure
and ii) the surface linear ordering of words.
Dependency triples: distribution of triplets repre-
senting a dependency relation consisting of a syn-
tactic head (h), the dependency relation type (t) and
the dependent (d). Two different variants of this fea-
ture are distinguished, based on the fact that either
the coarse?grained PoS or the word?form of h and d
is considered: we will refer to the former as Coarse
grained Part-Of-Speech dependency triples and to
the latter as Lexical dependency triples. In both
cases, the relative ordering of h and d, i.e. whether h
precedes or follows d at the level of the linear order-
ing of words within the sentence, is also considered.
Dependency Subtrees: distribution of dependency
subtrees consisting of a dependency relation (repre-
sented as the dependency triple {h, t, d}), the head
father and the dependency relation linking the two.
As in the previous case, two different variants of this
feature are distinguished, based on the fact that ei-
ther the coarse grained PoS or the word?forms of
the nodes in the dependency subtree are considered.
Parse tree depth features: this set of features is
meant to capture different aspects of the parse tree
depth and includes: a) the depth of the whole parse
tree, calculated in terms of the longest path from
the root of the dependency tree to some leaf; b)
the average depth of embedded complement ?chains?
governed by a nominal head and including either
prepositional complements or nominal and adjecti-
val modifiers; c) the probability distribution of em-
bedded complement ?chains? by depth. These fea-
tures represent reliable indicators of sentence com-
plexity, as stated by, among others, Yngve (1960),
Frazier (1985) and Gibson (1998), and they can thus
allow capturing specific difficulties of L2 learners.
Coarse grained Part-Of-Speech of sentence root:
this feature refers to coarse grained POS of the syn-
tactic root of a sentence.
Arity of verbal predicates: this feature refers to
the number of dependencies (corresponding to either
subcategorized arguments or modifiers) governed by
the same verbal head. In the NLI context, it can al-
low capturing improper verbal usage by L2 learners
due to language transfer (e.g. with pro?drop lan-
guages as L1).
Subordination features: this set of features is
meant to capture different aspects of the use of sub-
ordination and includes: a) the distribution of sub-
ordinate vs main clauses; b) the average depth of
?chains? of embedded subordinate clauses and c)
the probability distribution of embedded subordinate
clauses ?chains? by depth. Similarly to parse tree
depth, this set of features can be taken to reflect the
structural complexity of sentences and can thus be
indicative of specific difficulties of L2 learners.
Length of dependency links: measured in terms
209
of the words occurring between the syntactic head
and the dependent. This is another feature which
reflects the syntactic complexity of sentences (Lin,
1996; Gibson, 1998) and which can be successfully
exploited to capture syntactic idiosyncracies of L2
learners due to L1 interferences.
2.4 Other features
Two further features have been considered for NLI
purposes, which were included in the distributed
datasets. For each document, we have also consid-
ered i) the English language proficiency level (high,
medium, or low) based on human assessment by lan-
guage specialists, and ii) the topic of the essays.
3 Linguistic Profiling of TOEFL11 Corpus
In this section, we illustrate the results of linguis-
tic profiling carried out on the training and devel-
opment sets extracted from the TOEFL11 corpus.
This corpus, described in Blanchard et al (2013),
contains 1,100 essays per 11 languages (for a to-
tal of 12,100 essays) sampled as evenly as possi-
ble from 8 prompts (i.e., topics) along with score
levels (low/medium/high) for each essay. The con-
sidered L1s are: Arabic, Chinese, French, German,
Hindi, Italian, Japanese, Korean, Spanish, Telugu,
and Turkish. For the specific purposes of the NLI
Shared Task, a total of 9,900 essays has been dis-
tributed as training data (900 essays per L1), 1,100
as development data (100 per L1) and the remaining
1,100 essays have been used as test data.
We started from the automatic linguistic annota-
tion of training and development data whose output
has been searched for with respect to the features il-
lustrated in Section 2.
3.1 Linguistic Pre?processing
Both training and development data were au-
tomatically morpho-syntactically tagged by the
POS tagger described in Dell?Orletta (2009) and
dependency?parsed by the DeSR parser using
Multi?Layer Perceptron as learning algorithm (At-
tardi et al, 2009), a state?of?the?art linear?time
Shift?Reduce dependency parser. Feature extraction
is carried out against the output of the multi?level
automatic linguistic analysis carried out during the
pre?processing stage: lexical and grammatical pat-
terns corresponding to the wide typology of selected
features are looked for within each annotation layer
and quantified.
3.2 Linguistic Profiling
Generally speaking, linguistic profiling makes it
possible to identify (groups of) texts which are sim-
ilar, at least with respect to the ?profiled? features
(van Halteren, 2004). In what follows we report
the results of linguistic profiling obtained with re-
spect to the 11 L1 sub?corpora considered in this
study. Figure 1 shows the results obtained with re-
spect to a selection of the features described in Sec-
tion 2. These results refer to the combined training
and development data sets: note, however, that we
also calculated the values of these features in the two
datasets separately and it turned out that they do not
vary significantly between the two sets. This fact
can be taken as a proof both of the reliability of our
approach to linguistic profiling and of the relevance
of these features for NLI purposes.
Starting from raw textual features (Figures 1(a)
and 1(b)), both average sentence length and aver-
age word length vary significantly across L1s. In
particular, if on the one hand the essays written by
Arabic and Spanish L1 speakers contain the shortest
words and the longest sentences, on the other hand
the Hindi and Telugu L1 essays are characterized by
the longest words; the L1 Japanese and Korean cor-
pora contain the shortest sentences.
Let us focus now on the distribution of unigrams
of coarse grained Parts?Of?Speech. If we consider
the distributions of determiners and nouns, two fea-
tures typically used for NLI purposes (Wong and
Dras, 2009) which also represent stylistic markers
associated with different linguistic varieties (Biber
and Conrad, 2009), it can be noticed (see Fig-
ures 1(c) and 1(d)) that for Japanese and Korean the
essays show the lowest percentage of determiners,
while for Hindi and Telugu they are characterized
by the highest percentage of nouns.
For what concerns syntactic features, we observe
that essays by Japanese and Korean speakers are
characterized by quite a different distribution with
respect to the other L1 corpora. In particular, they
show the shallowest parse trees, the shortest depen-
dency links as well as the shortest ?chains? of em-
bedded complements governed by a nominal head.
On the other hand, the essays by Spanish and Ara-
210
(a) Average word length (b) Average sentence length
(c) Distribution of Determiners (d) Distribution of Nouns
(e) Average parse tree depth (f) Average depth of embedded complement ?chains?
(g) Average length of the longest dependency link (h) Arity of verbal predicates
Figure 1: Results of linguistic profiling carried out on the combined training and development sections of the TOEFL11
corpus.
211
bic speakers contain the deepest parse trees, for Ital-
ian and Spanish we observe the longest dependency
links and for Hindi and Telugu the longest sequences
of embedded complements. Moreover, while the
essays by Italians are characterised by the highest
value of arity of verbal predicates, for Hindi, Telugu
and Korean essays much lower values are recorded.
Interestingly, these linguistic profiling results
show similar trends across the 11 languages at dif-
ferent levels of linguistic analysis. For instance, it
can be noted that Japanese and Korean or Italian
and Spanish, which belong to two different language
families, show similar distributions of features. Sim-
ilarities have also been recorded in the sub?corpora
by Hindi and Telugu speakers, even if these lan-
guages do not belong to the same family; we can
hypothesize that this might originate from language
contact phenomena.
4 System Description
4.1 Machine Learning Classifier
Our approach to Native Language Identification has
been implemented in a software prototype, i.e. a
classifier operating on mopho?syntactically tagged
and dependency parsed texts which assigns to each
document a score expressing its probability of be-
longing to a given L1 class. The highest score rep-
resents to the most probable class. Given a set of
features and a training corpus, the classifier creates a
statistical model using the feature statistics extracted
from the training corpus. This model is used in the
classification of unseen documents. The set of fea-
tures and the machine learning algorithm can be pa-
rameterized through a configuration file.
For each feature, we have implemented three dif-
ferent variants, depending on whether the feature
value is encoded in terms of: i ) presence/absence
of the feature (binary variant), ii ) the normalized
frequency (normalized frequency variant), and iii )
the normalized tf*idf value (normalized tf*idf vari-
ant). Since the binary feature variant outperformed
the other two, in all the experiments carried out on
the development set reported in Section 5 we illus-
trate the results obtained using this variant only. This
is in line with the results obtained by Brooke and
Hirst (2012) and Tetreault et al (2013). According
to (Brooke and Hirst, 2012), a possible explanation
is that ?in these relatively short texts, there is high
variability in normalized frequencies, and a simpler
metric, by having less variability, is easier for the
classifier to leverage?. Support Vector Machines
(SVM) using LIBSVM (Chang and Lin, 2001) and
Maximum Entropy (ME) using MaxEnt1 have been
used as machine learning algorithms.
We experimented two classification approaches: a
single classifier method and two ensemble systems,
combining the output of several classifiers.
The single classifier uses the set of features re-
sulting from the feature selection process described
in Section 4.2 and the SVM using linear kernel as
machine learning algorithm. This choice is due to
the fact that in all the experiments the linear SVM
outperformed the SVM using polynomial kernel.
There are two possible explanations for this fact,
namely: a) the number of features is much higher
than the number of training instances, accordingly
it might not be necessary to map data to a higher
dimensional space, therefore the nonlinear mapping
does not improve the performance; b) Weston et al
(2000) showed that SVMs can indeed suffer in high
dimensional spaces where many features are irrele-
vant. Note that in Section 5, we report the results of
this classifier using different sets of features corre-
sponding to the lexical, morpho?syntactic and syn-
tactic levels of linguistic analysis.
The two ensemble systems combine the outputs
of the component classifiers following two different
strategies. The first one is based on the majority vot-
ing method (henceforth, VoteComb ): the combina-
tion strategy is seen as a classical voting problem
where for each essay is assigned the L1 class that
has been selected from the majority of classifiers. In
case of ties, the L1 class predicted from the best indi-
vidual model (as resulting from the experiments car-
ried out on the development set) is selected. The sec-
ond strategy combines the outputs of the component
classifiers via another classifier (henceforth referred
to as meta?classifier): we will refer to this second
strategy as ClassComb. The meta?classifier uses as
a feature the probability score predicted from each
component classifier for each L1 class. Differently
from the component classifiers, the meta?classifier
is based on polynomial kernel SVM. In both en-
1https://github.com/lzhang10/maxent#readme
212
semble systems, the component classifiers use linear
SVM and ME as machine learning algorithms and
exploit different sets of features among the ones re-
sulting from the feature selection process described
below.
4.2 Features Selection Process
Since our approach to NLI relies on a wide num-
ber of general?purpose features, a feature selection
process was necessary in order to prune irrelevant
and redundant features which could negatively af-
fect the classification results. The selection process
starts taking into account all the n features described
in Section 2. In each iteration, for each feature fi we
generate a configuration ci such that fi is disabled
and all the other features are enabled. When an it-
eration finishes, we obtain for each ci a correspond-
ing accuracy score score(ci) which is computed as
the average of the accuracy obtained by the classi-
fier on the development set (ad) and on an internal
development set (ai), corresponding to the 10% of
the training set, used in order to reduce the overfit-
ting risk. Being cb the best configuration among all
the ci configurations, if score(cb) ? of the accuracy
scores resulting from the previous iterations the pro-
cess stops. Otherwise:
1. store in F the pair ?fb, disabled? ;
2. for each configuration ci, if score(ci) ? of the
accuracy scores resulting from the previous it-
erations, we store in F the pair ?fi, enabled?;
3. set C = ?cb, score(cb)?
where F is a map containing elements
feature ? {disabled, enabled} and C is a
pair that contains the current best configuration cb
and the corresponding score score(cb). In each
iteration, we consider only the features which do
not occur in F . At the initialization step F is empty
and C contains the configuration where all the
considered features are enabled.
In spite of the fact that the described selection
process does not guarantee to obtain the global opti-
mum, it however permitted us to obtain an improve-
ment of about 8% with respect to the starting model
indiscriminately using all features.
Table 1 lists the features resulting from the fea-
ture selection process. It can be noted that some
Lexical features:
Word n-grams
Morpho?syntactic features:
Coarse grained Part-Of-Speech unigrams
Fine grained Part-Of-Speech bigrams
Syntactic features:
Dependency types unigrams
Lexical dependency triples
Parse tree depth features
Coarse grained Part-Of-Speech of sentence root
Arity of verbal predicates
Subordination features
Length of dependency links
Table 1: Features resulting from the feature selection pro-
cess.
of them coincide with those typically used for NLI
purposes: this is the case of n?grams of words,
Parts-Of-Speech and syntactic dependencies. Inter-
estingly, to our knowledge, other features such as ar-
ity of verbal predicates, length of dependency links
as well as subordination and parse tree depth fea-
tures have not been used for NLI so far, in spite of
their being widely exploited in the syntactic com-
plexity literature (as discussed in Section 2).
5 Results
Table 2 reports the overall Accuracy achieved with
the different classifier models in the NLI classifi-
cation task on the official test set as well as the
F-measure score recorded for each L1 class. The
first two lines show the accuracies of the two com-
bination models, while the last three report the re-
sults obtained by the single classifier using i) the set
of features resulting by the features selection pro-
cess (Best Single), ii) the selected lexical features
only (see Table 1) (Lexical ) and iii) the lexical and
morpho?syntactic features (Lex+Morph ).
The two combination models outperform all
the single model classifiers: note that ClassComb
achieved much better results with respect to Vote-
Comb. By comparing these results with the F-
measure scores obtained on the distributed develop-
ment data (see Table 3), it can be seen that the rank-
ing of the scores achieved by the different classifiers
remains the same even if on the test data we obtained
a performance of -2,2% with respect to the develop-
213
Accuracy ARA CHI FRE GER HIN ITA JAP KOR SPA TEL TUR
ClassComb 77,9 73,8 77,5 83,2 87,3 71,1 86,0 78,8 74,2 70,8 76,2 78,0
VoteComb 77,2 74,3 77,0 80,0 87,0 72,8 81,6 79,6 73,8 67,7 77,6 77,6
Best Single 76,6 71,9 77,6 75,8 85,7 73,2 82,0 80,0 74,0 69,0 76,9 76,5
Lex+Morph 76,4 77,2 76,2 78,6 85,9 72,1 80,4 76,8 71,9 68,0 76,4 76,4
Lexical 76,2 71,1 76,5 79,0 87,6 74,5 80,8 77,7 70,8 66,7 79,2 73,4
Table 2: Classification results of different classifiers on official test data.
ment test set.
Let us consider now the results obtained by the
single model classifiers. In all cases the Best Single
outperforms the other two models demonstrating the
reliability of the features selection process and that
a combination of lexical, morpho?syntactic and syn-
tactic features leads to better results.
Although the best performing model is the Class-
Comb, this is not true for all the 11 languages. In
Table 2, the best results for each L1 are bolded. In-
terestingly, even though Lexical is the worst model,
it is the best performing one for three L1s while the
best model, i.e. ClassComb, for five only.
It can be noted that with respect to the devel-
opment data set the syntactic features used by the
Best Single model allow an increment of +1% as
opposed to the Lexical model: this represents a
much higher increase if compared with the result
obtained on the test data, which is +0,4%. This is
an unexpected result since the feature selection de-
scribed in Section 4.2 was carried out on an internal
development set in order to prevent the risk of over-
fitting on the distributed development data.
Classifier Accuracy
ClassComb 80,1
VoteComb 79,3
Best Single 78,8
Lex+Morph 78,2
Lexical 77,8
Table 3: Classification results of different classifiers on
distributed development data.
6 Conclusion
In this paper, we reported our participation results
to the First Native Language Identification Shared
Task. By resorting to a wide set of general?
purpose features qualifying the lexical and grammat-
ical structure of a text, rather than to ad hoc fea-
tures specifically selected for the task at hand, we
achieved encouraging results. After a feature se-
lection process, new features which to our knowl-
edge have never been exploited so far for NLI pur-
poses turned out to contribute significantly to the
task. Interestingly, the same set of features we
started from has been previously successfully ex-
ploited in other related tasks, such as readability
assessment and genre classification, operating on
the Italian language. The obtained results suggest
that our approach is general?purpose and portable
across different domains and languages. Further di-
rections of research currently include: i) comparison
of results obtained with general purpose features and
with NLI?specific features (e.g. typical errors or dif-
ferent types of stylistic idiosyncrasies specific to L2
learners), with a view to combining them to achieve
better results; ii) design and development of new en-
semble classification methods as well as new fea-
ture selection methods considering not only classes
of features but also individual features; iii) testing
our approach to NLI on different L2s (e.g. Italian) .
References
Giuseppe Attardi, Felice Dell?Orletta, Maria Simi and
Joseph Turian. 2009. Accurate dependency parsing
with a stacked multilayer perceptron. In Proceedings
of EVALITA, Evaluation of NLP and Speech Tools for
Italian, Reggio Emilia, Italy.
Douglas Biber. 1993. Using Register?diversified Cor-
pora for General Language Studies. Computational
Linguistics Journal, 19(2): 219?241.
Douglas Biber and Susan Conrad. 2009. Genre, Register,
Style. Cambridge: CUP.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Educational Testing
Service.
214
Julian Brooke and Graeme Hirst. 2012. Robust, Lexical-
ized Native Language Identification. In Proceedings
of COLING 2012, Mumbai, India, 391?408.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM:
a library for support vector machines. Software avail-
able at http://www.csie.ntu.edu.tw/ cjlin/libsvm
Walter Daelemans. 2012. Explanation in Computational
Stylometry. In A. Gelbukh (ed.) CICLing 2012, Part
II, LNCS 7817, Springer?Verlag, 451?462.
Felice Dell?Orletta. 2009. Ensemble system for Part-of-
Speech tagging. In Proceedings of Evalita?09, Eval-
uation of NLP and Speech Tools for Italian, Reggio
Emilia, December.
Felice Dell?Orletta, Simonetta Montemagni and Giulia
Venturi. 2011a. READ-IT: Assessing Readability of
Italian Texts with a View to Text Simplification. In
Proceedings of the Workshop on ?Speech and Lan-
guage Processing for Assistive Technologies? (SLPAT
2011), Edinburgh, July 30, 73?83.
Felice Dell?Orletta, Simonetta Montemagni and Giulia
Venturi. 2012. Genre?oriented Readability Assess-
ment: a Case Study. In Proceedings of the Workshop
on Speech and Language Processing Tools in Educa-
tion (SLP-TED), 91?98.
Lyn Frazier. 1985. Syntactic complexity. In D.R.
Dowty, L. Karttunen and A.M. Zwicky (eds.), Natural
Language Parsing, Cambridge University Press, Cam-
bridge, UK.
Edward Gibson. 1998. Linguistic complexity: Locality
of syntactic dependencies. In Cognition, 68(1), pp. 1?
76.
Patrick Juola. 2008. Authorship Attribution. Now Pub-
lishers Inc.
Moshe Koppel, Jonathan Schler and Kfir Zigdon. 2005.
Automatically determining an anonymous author?s na-
tive language. In Intelligence and Security Informat-
ics, vol. 3495, LNCS, Springer?Verlag, 209?217.
Dekan Lin. 1996. On the structural complexity of natural
language sentences. In Proceedings of COLING 1996,
pp. 729?733.
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the Errors of Data-Driven Dependency Parsing
Models. In Proceedings of EMNLP-CoNLL, 2007,
122?131.
Alexander Mehler, Serge Sharoff and Marina Santini
(Eds.). 2011. Genres on the Web. Computational
Models and Empirical Studies. Springer Series: Text,
Speech and Language Technology.
Sze?Meng Jojo Wong and Mark Dras. 2009. Contrastive
Analysis and Native Language Identification. In Pro-
ceedings of the Australasian Language Technology As-
sociation Workshop.
Hans van Halteren. 2004. Linguistic profiling for author
recognition and verification. In Proceedings of the
Association for Computational Linguistics (ACL04),
200?207.
Joel Tetreault, Daniel Blanchard, Aoife Cahill and Mar-
tin Chodorow. 2012. Native Tongues, Lost and
Found: Resources and Empirical Evaluations in Na-
tive Language Identification. In Proceedings of COL-
ING 2012, Mumbai, India, 2585?2602.
Joel Tetreault, Daniel Blanchard and Aoife Cahill. 2013.
Summary Report on the First Shared Task on Native
Language Identification. In Proceedings of the Eighth
Workshop on Building Educational Applications Us-
ing NL, Atlanta, GA, USA.
Victor H.A. Yngve. 1960. Amodel and an hypothesis for
language structure. In Proceedings of the American
Philosophical Society, 444?466.
Jason Weston, Sayan Mukherjee, Oliver Chapelle, Mas-
similiano Pontil, Tomaso Poggio and Vladimir Nau-
movich Vapnik. 2000. Feature selection for SVMs. In
Advances in Neural Information Processing Systems
13, MIT Press, 668?674.
215
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 163?173,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Assessing the Readability of Sentences: Which Corpora and Features?
Felice Dell?Orletta

, Martijn Wieling
??
, Andrea Cimino

, Giulia Venturi

and Simonetta Montemagni


Istituto di Linguistica Computazionale ?Antonio Zampolli? (ILC?CNR)
ItaliaNLP Lab - www.italianlp.it
{name.surname}@ilc.cnr.it
?
Department of Humanities Computing, University of Groningen, The Netherlands
?
Department of Quantitative Linguistics, University of T?ubingen, Germany
wieling@gmail.com
Abstract
The paper investigates the problem of
sentence readability assessment, which is
modelled as a classification task, with a
specific view to text simplification. In par-
ticular, it addresses two open issues con-
nected with it, i.e. the corpora to be used
for training, and the identification of the
most effective features to determine sen-
tence readability. An existing readabil-
ity assessment tool developed for Italian
was specialized at the level of training cor-
pus and learning algorithm. A maximum
entropy?based feature selection and rank-
ing algorithm (grafting) was used to iden-
tify to the most relevant features: it turned
out that assessing the readability of sen-
tences is a complex task, requiring a high
number of features, mainly syntactic ones.
1 Introduction
Over the last ten years, work on automatic read-
ability assessment employed sophisticated NLP
techniques (such as syntactic parsing and statisti-
cal language modeling) to capture highly complex
linguistic features, and used statistical machine
learning to build readability assessment tools. A
variety of different NLP?based approaches has
been proposed so far in the literature, differing
at the level of the number of identified readabil-
ity classes, the typology of features taken into ac-
count, the intended audience of the texts under
evaluation, or the application within which read-
ability assessment is carried out, etc.
Research focused so far on readability assess-
ment at the document level. However, as pointed
out by Skory and Eskenazi (2010), methods devel-
oped perform well when the task is characterizing
the readability level of an entire document, while
they are unreliable for short texts, including single
sentences. Yet, for specific applications, assessing
the readability level of individual sentences would
be desirable. This is the case, for instance, for text
simplification: in current approaches, text read-
ability is typically assessed with respect to the en-
tire document, while text simplification is carried
out at the sentence level, as e.g. done in Alu??sio
et al. (2010), Bott and Saggion (2011) and Inui et
al. (2003). By decoupling the readability assess-
ment and simplification processes, the impact of
simplification operations on the overall readabil-
ity level of a given text may not always be clear.
With sentence?based readability assessment, this
is expected to be no longer a problem. Sentence
readability assessment thus represents an open is-
sue in the literature which is worth being further
explored. To our knowledge, the only attempts
in this direction are represented by Dell?Orletta et
al. (2011) and Sj?oholm (2012) for the Italian and
Swedish languages respectively, followed more
recently by Vajjala and Meurers (2014) dealing
with English.
In this paper, we tackle the challenge of assess-
ing the readability of individual sentences as a first
step towards text simplification. The task is mod-
elled as a classification task, with the final aim
of shedding light on two open issues connected
with it, namely the reference corpora to be used
for training (i.e. collections of sentences classified
according to their readability level), and the iden-
tification of the most effective features to deter-
mine sentence readability. For what concerns the
former, sentence readability assessment poses the
remarkable issue of classifying sentences accord-
ing to their difficulty: if all sentences occurring in
simplified texts can be assumed to be easy?to?read
sentences, the reverse does not necessarily hold
since not all sentences occurring in complex texts
are to be assumed difficult?to?read. This fact has
important implications at the level of the composi-
tion of the corpora to be used for training. The sec-
163
ond issue is concerned with whether and to what
extent the features playing a significant role in the
assessment of readability at the sentence level co-
incide with those exploited at the level of docu-
ment. In particular, the following research ques-
tions are addressed:
1. in assessing sentence readability, is it bet-
ter to use a small gold standard training cor-
pus of manually classified sentences or a
much bigger training corpus automatically
constructed from readability?tagged docu-
ments possibly containing misclassified sen-
tences?
2. which are the features maximizing sentence
readability assessment?
3. to what extent do important features for sen-
tence readability classification match those
playing a role in the document readability
classification?
We will try to answer these questions by work-
ing on Italian, which is a less?resourced language
as far as readability is concerned. To this end,
READ?IT (Dell?Orletta et al., 2011; Dell?Orletta
et al., 2014), which represents the first NLP?based
readability assessment tool for Italian, was spe-
cialized in different respects, namely at the level of
the training corpus and of the learning algorithm;
to investigate questions 2. and 3. above, a maxi-
mum entropy?based feature selection and ranking
algorithm (i.e. grafting) was selected. The specific
target audience of readers addressed in this study
is represented by people characterised by low lit-
eracy skills and/or by mild cognitive impairment.
The paper is organized as follows: Section 2 de-
scribes the background literature, Section 3 intro-
duces our approach to the task, in terms of used
corpora, features and learning algorithm. Finally,
Sections 4 and 5 describe the experimental setting
and discuss achieved results.
2 Background
In spite of the acknowledged need of perform-
ing readability assessment at the sentence level,
so far very few attempts have been made to sys-
tematically investigate the issues and challenges
concerned with the readability assessment of sen-
tences (as opposed to documents). The first two
studies in this direction focused on languages
other than English, namely Italian (Dell?Orletta
et al., 2011) and Swedish (Sj?oholm, 2012). In
both cases, the authors start from the assump-
tion that while all sentences occurring in simpli-
fied texts can be assumed to be easy?to?read sen-
tences, the reverse is not true, since not all sen-
tences occurring in complex texts are difficult?to?
read. This has important consequences at the level
of the evaluation of sentence classification results:
i.e. erroneous readability assessments within the
class of difficult?to?read texts may either corre-
spond to those easy?to?read sentences occurring
within complex texts or represent real classifi-
cation errors. To overcome this problem in the
readability assessment of individual sentences, a
notion of distance with respect to easy-to-read
sentences was introduced by Dell?Orletta et al.
(2011). Focusing on English, a similar issue is
addressed more recently by Vajjala and Meur-
ers (2014) who developed a binary sentence clas-
sifier trained on Wikipedia and Simple English
Wikipedia: they showed that the low accuracy ob-
tained by their classifier stems from the incorrect
assumption that all Wikipedia sentences are more
complex than the Simple Wikipedia ones.
Besides readability, sentence?based analyses
are reported in the literature for related tasks: for
instance, in a text simplification scenario by Drn-
darevi?c et al. (2013), Alu??sio et al. (2008),
?
Stajner
and Saggion (2013) and Barlacchi and Tonelli
(2013); or to predict writing quality level by Louis
and Nenkova (2013). Sheikha and Inkpen (2012)
report the results of both document? and sentence?
based classification in the different but related task
of assessing formal vs. informal style of a docu-
ment/sentence. For students learning English, An-
dersen et al. (2013) made a self?assessment and
tutoring system available which was able to assign
a quality score for each individual sentence they
write: this provides automated feedback on learn-
ers? writing.
A further important issue, largely investigated
in previous readability assessment studies, is the
identification of linguistic factors playing a role
in assessing the readability of documents. If tra-
ditional readability metrics (see e.g., Kincaid et
al. (1975)) typically rely on raw text characteris-
tics, such as word and sentence length, the new
NLP?based readability indices exploit wider sets
of features ranging across different linguistic lev-
els. Starting from Schwarm and Ostendorf (2005)
and Heilman et al. (2007), the role of syntactic
164
features in this task was considered, and more re-
cently, the role of discourse features (e.g., dis-
course topic, discourse cohesion and coherence)
has also been taken into account (see e.g., Barzi-
lay and Lapata (2008), Pitler and Nenkova (2008),
Kate et al. (2010), Feng et al. (2010) and Tonelli
et al. (2012)). Many of these studies also explored
the usefulness of features belonging to individual
levels of linguistic description in predicting text
readability. For example, Feng et al. (2010) sys-
tematically evaluated a wide range of features and
compared the results of different statistical classi-
fiers trained on different classes of features. Sim-
ilarly, the correlation between level?specific fea-
tures has been calculated by Pitler and Nenkova
(2008) with respect to human readability judg-
ments, and by Franc?ois and Fairon (2012) with
respect to readability levels. In both cases, the
classes of features which turned out to be highly
correlated with readability judgments were used
in a readability assessment tool to test their effi-
cacy. Note, however, that in all cases the predic-
tive power of the selected features was evaluated
at the document level only.
3 Our Approach
In this section, we introduce the main ingredi-
ents of our approach to sentence readability as-
sessment, corpora used for training and testing,
selected features and the learning and feature se-
lection algorithm.
3.1 Corpora
We relied on two different corpora: a newspaper
corpus, La Repubblica (henceforth, Rep), and an
easy?to?read newspaper, Due Parole (henceforth,
2Par). 2Par includes articles specifically written
by Italian linguists experts in text simplification
for an audience of adults with a rudimentary lit-
eracy level or with mild intellectual disabilities
(Piemontese, 1996), which represents the target
audience of this study. The two corpora ? selected
as representative of complex vs. simplified texts
within the journalistic genre ? differ significantly
with respect to the distribution of features typi-
cally correlated with text complexity (Dell?Orletta
et al., 2011) and thus represent reliable training
datasets. However, whereas such a distinction is
valid as far as documents are concerned, it appears
to be a simplistic generalization when the focus is
on sentences. In other words, whereas we can con-
sider all sentences of 2Par as easy?to?read, not all
Rep sentences are expected to be difficult?to?read.
From this it follows that whereas the internal com-
position of 2Par is homogeneous at the sentence
level, this is not the case for Rep.
To overcome this asymmetry and in particular
to assess the impact of the noise in the Rep train-
ing corpus, we constructed different training sets
differing in size and internal composition, going
from a noisy set which assumes all Rep sentences
to be difficult?to?read to a clean but smaller set
in which the easy?to?read sentences occurring in
Rep were manually filtered out. These training
sets were used in different experiments whose re-
sults are reported in Section 4.2.
The corpus containing only difficult?to?read
sentences was manually built by annotating Rep
sentences according to their readability (i.e. easy
vs. difficult). The annotation process was car-
ried out by two annotators with a background in
computational linguistics. In order to assess the
reliability of their judgements, we started with a
small annotation experiment: the two annotators
were provided with the same 5 articles from the
Rep corpus (for a total of 107 sentences) and were
asked to extract the difficult?to?read sentences (as
opposed to both easy?to?read and not?easy?to?
classify sentences). The first annotator carried out
the task in 5 minutes and 46 seconds, while the
second annotator took 9 minutes and 8 seconds.
The two annotators agreed on the classification of
81 difficult?to?read sentences out of 107 consid-
ered ones (in particular, the first annotator iden-
tified 90 difficult?to?read?sentences and the sec-
ond one 93 sentences). The agreement between
the two annotators was calculated in terms of pre-
cision, by taking one of the annotation sets as the
gold standard and the other as response: on aver-
age, we obtained a precision of 0.88 in the retrieval
of sentences definitely classified as difficult?to?
read. Given the high level of agreement, the two
annotators were asked to select difficult sentences
from two sets of distinct Rep articles. This re-
sulted in a set of 1,745 difficult?to?read sentences
which were used together with a random selection
of easy?to?read sentences from 2Par for training
and testing.
1
1
The collection can be downloaded from
www.italianlp.it/?page id=22.
165
Feature Ranking position Feature Ranking position
Sent. class. Doc. class. Sent. class. Doc. class.
Raw text features:
[1] Sentence length 1 1 [2] Word length 2 2
Lexical features:
[3] Word types in the Basic Italian Vocabu-
lary
14 42 [6] ?High availability words? 21 22
[4] ?Fundamental words? 10 9 [7] TTR (form) 7
[5] ?High usage words? 22 38 [8] TTR (lemma) 53
Morpho?syntactic features:
[9] Adjective 46 [26] Aux. verb ? inf. mood 64
[10] Adverb 29 59 [27] Aux. verb ? part. mood 51
[11] Article 49 25 [28] Aux. verb ? subj. mood 55
[12] Conjunction 40 [29] Main verb ? cond. mood 40 43
[13] Determiner 43 54 [30] Main verb ? ger. mood 48 48
[14] Interjection [31] Main verb ? imp. mood 37 57
[15] Noun 12 19 [32] Main verb ? indic. mood 16 11
[16] Number 65 44 [33] Main verb ? inf. mood 13 13
[17] Predeterminer [34] Main verb ? part. mood 26 28
[18] Preposition 61 [35] Main verb ? subj. mood 46 32
[19] Pronoun 27 30 [36] Modal verb - inf. mood 54 56
[20] Punctuation 35 [37] Modal verb ? cond. mood 41 36
[21] Residual [38] Modal verb ? imp. mood
[22] Verb 63 34 [39] Modal verb ? indic. mood 18 23
[23] Lexical density 34 33 [40] Modal verb ? part. mood
[24] Aux. verb ? cond. mood 59 60 [41] Modal verb ? subj. mood 60 58
[25] Aux. verb ? indic. mood 17 17
Syntactic features:
[42] Argument 62 [65] Sentence root 35 62
[43] Auxiliary 70 [66] Subject 39 52
[44] Clitic 63 [67] Subordinate clause 64
[45] Complement 28 29 [68] Temporal complement 45 55
[46] Concatenation 66 [69] Temporal modifier
[47] Conjunct in a disjunctive compound 58 67 [70] Temporal predicate
[48] Conjunct linked by a copulative con-
junction
38 37 [71] Parse tree depth 5 4
[49] Copulative conjunction 31 39 [72] Embedded complement ?chains? 8 24
[50] Determiner 50 26 [73] Verbal Root 6 3
[51] Direct object 44 27 [74] Arity of verbal predicates 3 15
[52] Disjunctive conjunction 57 68 [75] Pre?verbal subject 4 12
[53] Indirect complement/object 66 [76] Post?verbal subject 25 16
[54] Locative complement 52 51 [77] Pre?verbal object 36 41
[55] Locative modifier [78] Post?verbal object 9 21
[56] Locative predicate [79] Main clauses 23 14
[57] Modal verb 61 [80] Subordinate clauses 42 45
[58] Modifier 20 47 [81] Subordinate clauses in pre?verbal posi-
tion
32 10
[59] Negative 56 69 [82] Subordinate clauses in post?verbal po-
sition
19 20
[60] Passive subject [83] ?Chains? of embedded subordinate
clauses
11 5
[61] Predicative complement 49 [84] Finite complement clauses 30 18
[62] Preposition [85] Infinitive clauses 53 50
[63] Punctuation 24 31 [86] Length of dependency links 15 8
[64] Relative modifier 47 65 [87] Maximum length of dependency links 7 6
Table 1: Typology of features and ranking position in sentence and document readability assessment
experiments. Only about 14 features are needed for an adequate model of document readability, whereas
this number increases to 30 for sentence readability (marked in boldface). Features which were not
selected during ranking have no rank.
3.2 Linguistic Features
The set of features used in the experiments re-
ported in this paper is wide, spanning across dif-
ferent levels of linguistic analysis. They can
be broadly classified into four main classes, as
reported in Table 1: raw text features, lexical
features, morpho?syntactic features and syntactic
features, shortly described below.
2
2
For an exhaustive discussion including the motivations
underlying this selection of features, the interested reader is
Raw text features (Features [1?2] in Table 1)
refer to those features typically used within tra-
ditional readability metrics and include sentence
length, calculated as the average number of words
per sentence, and word length, calculated as the
average number of characters per words.
The cover category of lexical features (Features
[3?8] in Table 1) includes features referring to
referred to Dell?Orletta et al. (2011, 2014) where these fea-
tures were successfully used for assessing the readability of
Italian texts.
166
both the internal composition of the vocabulary
and the lexical richness of the text. For what con-
cerns the former, the Basic Italian Vocabulary by
De Mauro (2000) was taken as a reference re-
source, including a list of 7000 words highly fa-
miliar to native speakers of Italian. In particular,
we consider: a) the percentage of all unique words
(types) on this reference list occurring in the text,
and b) the internal distribution of the occurring ba-
sic Italian vocabulary words into the usage classi-
fication classes of ?fundamental words? (very fre-
quent words), ?high usage words? (frequent words)
and ?high availability words? (relatively lower fre-
quency words referring to everyday life). Lexical
richness of texts is monitored by computing the
Type/Token Ratio (TTR), which refers to the ratio
between the number of lexical types and the num-
ber of tokens within a text. Due to its sensitivity
to sample size, this feature is computed for text
samples of equivalent length.
The set of morpho?syntactic features (Features
[9?41] in Table 1) is aimed at capturing differ-
ent aspects of the linguistic structure affecting in
one way or another the readability of a text. They
range from the probability distribution of part?
of?speech (POS) types, to the lexical density of
the text, calculated as the ratio of content words
(verbs, nouns, adjectives and adverbs) to the to-
tal number of lexical tokens in a text. This class
also includes features referring to the distribution
of verbs by mood and/or tense, which can be seen
as a language?specific feature exploiting the pre-
dictive power of the Italian rich morphology.
The set of syntactic features (Features [42?87]
in Table 1) captures different aspects of the syntac-
tic structure which are taken as reliable indicators
for automatic readability assessment, namely:
? the unconditional probability of syntactic de-
pendency types, e.g. subject, direct object,
modifier, etc. (Features 42?70);
? parse tree depth features (71?72), going from
the depth of the whole parse tree, calculated
in terms of the longest path from the root
of the dependency tree to some leaf, to a
more specific feature referring to the aver-
age depth of embedded complement ?chains?
governed by a nominal head and including
either prepositional complements or nominal
and adjectival modifiers;
? verbal predicate features (73?78) aimed at
capturing different aspects of the behaviour
of verbal predicates: they range from the
number of verbal roots with respect to num-
ber of all sentence roots occurring in a text,
to more specific features such as the arity
of verbs, meant as the number of instanti-
ated dependency links sharing the same ver-
bal head (covering both arguments and modi-
fiers) and the relative ordering of subject and
object with respect to the verbal head;
? as subordination is widely acknowledged
to be an index of structural complexity
in language, subordination features (79?
85) include: the distribution of subordinate
vs. main clauses; for subordinates, the dis-
tribution of infinitives vs finite complement
clauses, their relative ordering with respect
to the main clause and the average depth of
?chains? of embedded subordinate clauses;
? the length of dependency links is another
characteristic connected with the syntactic
complexity of sentences. Features 86?87
measure dependency length in terms of the
words occurring between the syntactic head
and the dependent: they focus on all depen-
dency links vs. maximum dependency links
only.
3.3 Model Training and Feature Ranking
Given the twofold goal of this study, i.e. re-
liably assessing sentence readability and finding
the most predictive features undelying it, we used
GRAFTING (Perkins et al., 2003), as this approach
allows to train a maximum entropy model while si-
multaneously including incremental feature selec-
tion. The method uses a gradient?based heuristic
to select the most promising feature (to add to the
set of selected features S), and then performs a full
weight optimization over all features in S. This
process is repeated until a certain stopping crite-
rion is reached. As the grafting approach we use
integrates the l
1
regularization (preventing overfit-
ting), features are only included (i.e. have a non-
zero weight) when the reduction of the objective
function is greater than a certain treshold. In our
case, the l
1
prior we use was selected on the basis
of evaluating maximum entropy models with vary-
ing l
1
values (range: 1e-11, 1e-10, ..., 0.1, 1) via
10?fold cross validation. We used TINYEST
3
, a
3
http://github.com/danieldk/tinyest
167
grafting-capable maximum entropy parameter es-
timator for ranking tasks (de Kok, 2011; de Kok,
2013), to select the features and estimate their
weights. Whereas our task is not a ranking task,
but rather a binary classification problem, we were
able to model it as a ranking task by assigning a
high score (1) to difficult?to?read sentences and a
low score (0) to easy?to?read sentences. Conse-
quently, a sentence having a score < 0.5 was in-
terpreted as an easy?to?read sentence, whereas a
sentence which was assigned a score ? 0.5 was
interpreted to be a difficult?to?read sentence.
4 Experiments and Results
4.1 Experimental Setup
In all experiments, the corpora were automatically
tagged by the part?of?speech tagger described
in Dell?Orletta (2009) and dependency?parsed by
the DeSR parser (Attardi, 2006) using Support
Vector Machines as learning algorithm. We de-
vised two different experiments, aimed at explor-
ing the research questions investigated in this pa-
per. To this end, READ?IT was adapted by inte-
grating a specialized training corpus and a maxi-
mum entropy?based feature selection and ranking
algorithm (i.e. grafting).
Experiment 1
This experiment, investigating the first research
question, is aimed at identifying what is the most
effective training data for sentence readability as-
sessment. In particular, the goal is to compare
the results on the basis of using a small set of
gold standard data with respect to a (potentially
larger, but) noisy data set (i.e. without manual re-
vision) where every Rep sentence was assumed to
be difficult?to?read. In particular, the comparison
involved four datasets:
? a collection of gold standard data consisting
of 1,310 easy?to?read sentences randomly
extracted from the 2Par corpus and 1,310
manually selected difficult?to-read sentences
from the Rep corpus;
? a large and unbalanced collection of uncor-
rected data consisting of the whole 2Par cor-
pus (3,910 easy?to?read sentences) and the
whole Rep corpus (8,452 sentences, classi-
fied a priori as difficult?to?read);
? a balanced collection of uncorrected sen-
tences, consisting of 3,910 sentences from
2Par and 3,910 sentences from Rep;
? a balanced collection of uncorrected sen-
tences having the same size as the gold stan-
dard dataset, namely 1,310 sentences from
2Par and 1,310 sentences from Rep.
To assess similarities and differences at the level
of the different corpora used for training in this
experiment, in Table 2 we report a selection of
linguistic features (see Section 3.2) characterizing
the four datasets with respect to the whole 2Par
corpus.We can observe that 2Par differs from all
four Rep corpora for all reported features, and that
the four Rep corpora show similar trends. Inter-
estingly, however, the Rep Gold corpus is almost
always the most distant one from 2Par (i.e. at the
level of sentence length, word length, distribution
of adjectives and subjects, average length of de-
pendency links and parse tree depth).
On the basis of the four Rep datasets, four mod-
els were built which we evaluated using a held?
out test set consisting of 435 sentences from 2Par
and 435 manually classified difficult?to?read sen-
tences from Rep. Using the grafting method, we
calculated the classification score for each sen-
tence in our test set on the basis of an increasing
number of features (ranging from 1 to all non-zero
weighted features for the specific dataset): sen-
tences with a score below 0.5 were classified as
easy?to?read, whereas sentences having a score
greater or equal to 0.5 were classified as difficult?
to?read. This procedure was repeated for each of
the four models.
Experiment 2
The second experiment is aimed at answering our
second and third research questions, focusing on
the features relevant for sentence readability, and
the relationship of those features with document
readability classification. For this purpose, we
compared sentence? and document?based read-
ability classification results. In particular, we com-
pared the features used by the sentence?based
readability model trained on the gold standard
data and the features used by the document?based
model trained on Rep and 2Par. With respect
to the document classification, we used a cor-
pus of 638 documents (319 extracted from 2Par
representating easy?to?read texts, and 319 ex-
tracted from Rep representing difficult?to?read
texts) with 20% of the documents constituting the
held?out test set.
168
Features Rep Unbalan. large Rep Balan. small Rep Balan. large Rep Gold 2Par
Sentence length 24.98 26.03 25.26 28.14 18.66
Word length 5.14 5.24 5.14 5.28 5
?Fundamental words? 75.05% 75.08% 74.83% 74.99% 76.38
Adjective 6.19% 6.25% 6.36% 6.42% 6.03%
Noun 25.65% 27.09% 25.74% 26.10% 29.13%
Subject 4.62% 4.75% 4.64% 4.42% 6%
Max. length of dependency links 9.73 10.13 9.85 10.98 7.67
Parse tree depth 6.18 6.57 6.30 6.83 5.2
Table 2: Distribution of some linguistic features in Rep and 2Par training data
Accuracy Precision (all ft)
Training data 2 ft 10 ft 30 ft 50 ft all ft Easy Difficult
Unbalanced large 50 63.7 74.9 78.4 78.9 (85 ft) 69.2 88.5
Balanced small 64 67.9 79.2 80.8 82.5 (82 ft) 82.5 82.5
Balanced large 63.9 70.6 79.7 81.0 82.3 (85 ft) 83.0 81.6
Gold data 65.6 69.8 79.9 81.3 83.7 (66 ft) 84.8 82.5
Table 3: Sentence classification results using four training datasets and a varying number of features
4.2 Which Training Corpus for Sentence
Classification?
Table 3 reports the results for the sentence classi-
fication task using the four training datasets de-
scribed above. Results are reported in terms of
both overall accuracy (calculated as the proportion
of correct answers against all answers) and preci-
sion within each readability class (when using all
features), defined as the number of easy or diffi-
cult sentences correctly identified as such (in their
respective columns).
Accuracy was computed for all training models
tested using an increasing number of features (2,
10, 30, 50 and all features) as resulting from the
GRAFTING?based ranking and detailed in Table 1.
Note that the first two features correspond in all
cases to the traditional readability features of sen-
tence length and word length. The classification
model trained on the small gold standard dataset
turned out to almost always outperform all other
models: it achieved the best accuracy (83.7%) us-
ing a relatively small number of features (66), and
also for a fixed number of features (i.e. 2, 30
and 50). Only when using the top?10 features,
the uncorrected balanced large dataset slightly out-
performed the gold standard dataset. The accu-
racy when using the unbalanced dataset for train-
ing was always significantly (p < 0.05) worse (us-
ing McNemar?s test) than the accuracy based on
the other training data. The only other significant
difference existed between the balanced small and
large dataset for 10 features. All other differences
are non?significant.
It is also interesting to note that in the results
reported in column 2 ft of Table 3 a significant
difference is observed when comparing the accu-
racy achieved using the unbalanced large data set
with that achieved with the gold standard data: i.e.
about 15.5 percentage points of difference for the
2 ft model against 3 ? 6% using higher numbers
of features. This result originates from the fact that
the unbalanced corpus contains to a larger extent
sentences which are short and complex at the same
time whose correct readability assessment requires
linguistically?grounded features (see below).
The last two columns of Table 3 report preci-
sion results for easy? vs. difficult?to?read sen-
tences for each of the four training datasets (all
features). It is clear that for the class of difficult?
to?read sentences the highest precision (88.5%) is
obtained when using the whole 2Par and Rep cor-
pora for training (i.e. unbalanced large), whereas
for the class of easy?to?read sentences the best
precision results (84.8%) are obtained with the
system trained on the gold standard dataset. In-
terestingly, the worst precision results (69.2%) are
reported for the class of easy?to?read sentences
with the unbalanced large training data set.
These results suggest that the advantages of us-
ing the gold standard data over the uncorrected
training data sets are limited. From this it fol-
lows that treating the whole Rep corpus as a col-
lection of difficult?to?read sentences is not com-
pletely unjustified: this is in line with the satisfac-
tory results reported by Dell?Orletta et al. (2011)
where Rep was used for training a sentence read-
169
ability classifier without any manual filtering of
sentences. Nevertheless, the results of this ex-
periment demonstrate that readability assessment
accuracy and in particular the precision in identi-
fying easy?to?read sentences can be improved by
using a manually selected training dataset. Bal-
ancing the size of larger but potentially noisy (i.e.
without manual revision) data sets appears to cre-
ate a positive trade?off between accuracy and pre-
cision for both classes, thus representing a viable
alternative to the construction of a gold standard
dataset.
4.3 Sentence vs. Document Classification:
which and how many features?
To identify the typology of features needed for
sentence readability assessment and compare them
to those needed for assessing document read-
ability, we compared the results obtained by the
grafting?based feature selection in the sentence
classification task (using the gold standard dataset
for training, see Table 3) to those obtained in the
document classification task whose accuracy on
the test set is reported in Table 4 for increasing
numbers of features selected via GRAFTING.
Train. data 2 ft 10 ft 30 ft 50 ft 70 ft (all)
Rep - 2Par 80 93.3 96.6 96.6 95
Table 4: Accuracy of document classification for
a varying number of features
By comparing the document classification re-
sults with respect to those obtained for sentences,
it can be noticed that the best accuracy is achieved
using a set of 30 features: in contrast to sentence
classification where adding features keeps increas-
ing the performance, more features do not appear
to help for document classification. Sentence read-
ability classification thus seems to be a more com-
plex task, requiring a higher amount of features.
This trend emerges more clearly in Figures 1(a)
and 1(b), where the classification results on the
training set (using 10?fold cross?validation) and
the held?out test set are visualized for increas-
ing amounts of features selected via GRAFTING.
As Figure 1(a) shows, the document classifica-
tion task requires about 14 features after which
the performance appears to stabilize (97.4% accu-
racy for the ten?fold cross-validation and 96.7%
for the held?out test set). In contrast, Figure
1(b) shows that sentence classification requires at
least 30 features (83.4% accuracy for the ten?fold
cross-validation and 79.9% for the test set).
Noticeable differences can also be observed in
the typology of features playing a prominent role
in the two tasks. For each feature taken into ac-
count, Table 1 reports its ranking as resulting from
sentence? and document?based classification ex-
periments (columns ?Sent. class.? and ?Doc.
class.? respectively). Note that in interpreting
the rank associated with each feature it should
be considered that in sentence? and document?
classification the number of required features is
significantly different, i.e. 30 and 14 respectively:
this is to say that approximately the same rank as-
sociated to the same feature does not entail a com-
parable role across the two classification tasks.
As already pointed out, for both sentences and
documents raw text features (i.e. Sentence length
and Word length) turned out to be the top features,
leading however to significantly different results:
i.e. 80% accuracy for documents vs. 65% for
sentences. Among the remaining features, graft-
ing results show that syntactic features do play
a central role in both sentence? and document?
based readability assessment: many of these are
highly ranked, with some differences. Syntactic
features playing a similar role in both readabil-
ity classification tasks include: Verbal root [73],
Parse tree depth [71], ?Chains? of embedded sub-
ordinate clauses [83] and Max. length of depen-
dency links [87], covering important aspects of
syntactic complexity such as depth of the syntactic
dependency (sub?)tree and length of dependency
links. Features that are mainly useful for sentence
readability turned out to be Arity of verbal pred-
icates [74], Pre?verbal subject [75], Post?verbal
object [78] and Embedded complement ?chains?
[72], which can all be seen as representing local
features referring to sentence parts. The feature
Subordinate clauses in pre?verbal position [81],
focusing on the global distribution of pre?verbal
subordinate clauses within the document, is rele-
vant for document classification only. It is interest-
ing to note that features capturing different facets
of the same phenomenon can play quite a different
role for assessing the readability of sentences vs.
documents: this is the case of dependency length,
measured in terms of the words occurring between
the syntactic head and the dependent, where fea-
ture [86] refers to the average length of all de-
pendency links and [87] to the average length of
170
(a) Document classification (b) Sentence classification
Figure 1: Document vs Sentence classification results
maximum dependency links from each sentence.
Whereas [86] plays a similar role for sentences
and documents (in both cases it is a middle rank
feature), [87] is a global feature playing a more
prominent role in document classification.
At the morpho?syntactic level, the feature rank-
ing is more comparable. However, it is interest-
ing to note that very few morpho?syntactic fea-
tures were selected by the feature selection pro-
cess: this is particularly true for document classi-
fication. This can follow from the fact that these
features can be considered as proxies of the syn-
tactic structure which in these experiments was
represented through specific features: in this situ-
ation, the grafting process preferred syntactic fea-
tures over morpho?syntactic ones, in spite of the
lower accuracy of the dependency parser with re-
spect to the part?of?speech tagger. Interestingly,
this result is in contrast with what reported by
Falkenjack and J?onsson (2014) for what concerns
document readability assessment, who claim that
an optimal subset of text features for readability
based document classification does not need fea-
tures induced via parsing. Among the morpho?
syntactic features, it appears that verbal features
play an important role: this can follow both by the
language dealt with which is a a morphologically
rich language, and by the fact that these features
do not have a counterpart at the syntactic level.
Lexical features show a much more mixed re-
sult. Type?Token Ratio (TTR) is only important
for document classification, whereas most of the
other features are important for sentence readabil-
ity, but not for document readability (with the ex-
ception of the presence of ?fundamental words? of
the Basic Italian Vocabulary).
5 Discussion
In this study we have focused on three research
questions. First, we asked which type of train-
ing corpus is best to assess sentence readability.
Whereas we found that using a set of manually
selected complex sentences was better than using
a simple corpus?based distinction, the extra ef-
fort needed to construct the training corpus might
not be worthwhile as observed improvements were
quite modest. However, we did not consider a
more sensitive measure of the difficulty of a sen-
tence (such as a number ranging between 0 and
1), and this might be able to offer a more sub-
stantial improvement (at the cost of needing more
time to create the training material). Of course,
when the goal is to identify the best features for
assessing sentence readability, it does make sense
to have high?quality training data to prevent se-
lecting inadequate features. The second research
question involved identifying which features were
most useful for assessing sentence readability. Be-
sides raw text features, syntactic but also morpho?
syntactic features turned out to play a central role
to achieve adequate performance. The third re-
search question investigated the overlap between
the features needed for document and sentence
readability classification. Whereas there certainly
was overlap between the top features (with dif-
ferent levels of performance), most of the fea-
tures had a different rank across the two tasks,
with local features being more predictive for sen-
tence classification and global ones for documents.
This suggests that the sentence readability task is
more complex than assessing document readabil-
ity, given that there is much less information avail-
able for a sentence than for a document.
171
Acknowledgments
The research reported in this paper was carried out
in the framework of the Short Term Mobility pro-
gram of international exchanges funded by CNR
(Italy). We thank Dani?el de Kok for his help in
applying TINYEST to our data and Giulia Benotto
for her help in manual revision of training data.
References
Sandra M. Alu??sio, Lucia Specia, Thiago A.S. Pardo,
Erick G. Maziero, and Renata P.M. Fortes. 2008.
Towards brazilian portuguese automatic text simpli-
fication systems. In Proceedings of the Eighth ACM
Symposium on Document Engineering, pages 240?
248.
Sandra Alu??sio, Lucia Specia, Caroline Gasperin, and
Carolina Scarton. 2010. Readability assessment for
text simplification. In Proceedings of the NAACL
HLT 2010 Fifth Workshop on Innovative Use of NLP
for Building Educational Applications, pages 1?9.
?istein E. Andersen, Helen Yannakoudakis, Fiona
Barker, and Tim Parish. 2013. Developing and
testing a self-assessment and tutoring system. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 32?41.
Giuseppe Attardi. 2006. Experiments with a multi-
language non-projective dependency parser. In Pro-
ceedings of the Tenth Conference on Computational
Natural Language Learning (CoNLL-X ?06), New
York City, New York, pages 166?170.
Gianni Barlacchi and Sara Tonelli. 2013. Ernesta: A
sentence simplification tool for children?s stories in
italian. In Proceedings of the 14th Conferences on
Computational Linguistics and Natural Language
Processing (CICLing 2013), pages 476?487.
Regina Barzilay and Mirella Lapata. 2008. Model-
ing local coherence: An entity-based approach. vol-
ume 34.
Stefan Bott and Horacio Saggion. 2011. An un-
supervised alignment algorithm for text simplifica-
tion corpus construction. In Proceedings of the
Workshop on Monolingual Text-To-Text Generation,
pages 20?26.
Dani?el de Kok. 2011. Discriminative features in
reversible stochastic attribute-value grammars. In
Proceedings of the EMNLP Workshop on Language
Generation and Evaluation, pages 54?63. Associa-
tion for Computational Linguistics.
Dani?el de Kok. 2013. Reversible Stochastic Attribute-
Value Grammars. Ph.D. thesis, Rijksuniversiteit
Groningen.
Tullio De Mauro. 2000. Il dizionario della lingua ital-
iana. Paravia, Torino.
Felice Dell?Orletta, Simonetta Montemagni, and Giu-
lia Venturi. 2011. Read-it: Assessing readability
of italian texts with a view to text simplification. In
Proceedings of the Workshop on Speech and Lan-
guage Processing for Assistive Technologies (SLPAT
2011), pages 73?83.
Felice Dell?Orletta, Simonetta Montemagni, and Giulia
Venturi. 2014. Assessing document and sentence
readability in less resourced languages and across
textual genres. In International Journal of Applied
Linguistics (ITL). Special Issue on Readability and
Text Simplification. To appear.
Felice Dell?Orletta. 2009. Ensemble system for part-
of-speech tagging. In Proceedings of Evalita?09,
Evaluation of NLP and Speech Tools for Italian,
Reggio Emilia, December.
Biljana Drndarevi?c, Sanja
?
Stajner, Stefan Bott, Susana
Bautista, and Horacio Saggion. 2013. Automatic
text simplification in spanish: A comparative evalu-
ation of complementing modules. In Computational
Linguistics and Intelligent Text Processing, pages
488?500. Springer Berlin Heidelberg.
Johan Falkenjack and Arne J?onsson. 2014. Classify-
ing easy-to-read texts without parsing. In Proceed-
ings of the Proceedings of the 3rd Workshop on Pre-
dicting and Improving Text Readability for Target
Reader Populations (PITR), Gothenburg, Sweden.
Association for Computational Linguistics.
Lijun Feng, Martin Jansche, Matt Huenerfauth, and
No?emie Elhadad. 2010. A comparison of features
for automatic readability assessment. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics (COLING 2010), pages 276?
284.
Thomas Franc?ois and C?edrick Fairon. 2012. An ?AI
readability? formula for french as a foreign lan-
guage. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, Jeju Island, Korea, pages 466?477.
Michael J. Heilman, Kevyn Collins, and Jamie Callan.
2007. Combining lexical and grammatical features
to improve readability measures for first and second
language texts. In Proceedings of the Human Lan-
guage Technology Conference, pages 460?467.
Kentaro Inui, Atsushi Fujita, Tetsuro Takahashi, Ryu
Iida, and Tomoya Iwakura. 2003. Text simplifica-
tion for reading assistance: A project note. In Pro-
ceedings of the Second International Workshop on
Paraphrasing, pages 9?16.
Rohit J. Kate, Xiaoqiang Luo, Siddharth Patwardhan,
Martin Franz, Radu Florian, Raymond J. Mooney,
Salim Roukos, and Chris Welty. 2010. Learning to
172
predict readability using diverse linguistic features.
In oceedings of the 23rd International Conference
on Computational Linguistics, pages 546?554.
J. Peter Kincaid, Lieutenant Robert P. Fishburne,
Richard L. Rogers, and Brad S. Chissom. 1975.
Derivation of new readability formulas for navy
enlisted personnel. In Research Branch Report,
Millington, TN: Chief of Naval Training, pages 8?
75.
Annie Louis and Ani Nenkova. 2013. A corpus of sci-
ence journalism for analysing writing quality. vol-
ume 4.
Simon Perkins, Kevin Lacker, and James Theiler.
2003. Grafting: Fast, incremental feature selection
by gradient descent in function space. The Journal
of Machine Learning Research, 3:1333?1356.
Maria Emanuela Piemontese. 1996. Capire e farsi
capire. Teorie e tecniche della scrittura controllata.
Tecnodid, Napoli.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 186?195.
Sarah E. Schwarm and Mari Ostendorf. 2005. Reading
level assessment using support vector machines and
statistical language models. In Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics (ACL 05), pages 523?530.
Fadi Abu Sheikha and Diana Inkpen. 2012. Learning
to classify documents according to formal and infor-
mal style. volume 8.
Johan Sj?oholm. 2012. Probability as readability: A
new machine learning approach to readability as-
sessment for written Swedish. LiU Electronic Press,
Master thesis.
Adam Skory and Maxine Eskenazi. 2010. Predicting
cloze task quality for vocabulary training. In Pro-
ceedings of the NAACL HLT 2010 Fifth Workshop
on Innovative Use of NLP for Building Educational
Applications, pages 49?56.
Sara Tonelli, Ke Tran Manh, and Emanuele Pianta.
2012. Making readability indices readable. In Pro-
ceedings of the First Workshop on Predicting and
Improving Text Readability for Target Reader Popu-
lations, pages 40?48.
Sowmya Vajjala and Detmar Meurers. 2014. On as-
sessing the reading level of individual sentences for
text simplification. In Proceedings of the 14th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL-14), Gothen-
burg, Sweden. Association for Computational Lin-
guistics.
Sanja
?
Stajner and Horacio Saggion. 2013. Readabil-
ity indices for automatic evaluation of text simplifi-
cation systems: A feasibility study for spanish. In
Proceedings of the International Joint Conference
on Natural Language Processing.
173
