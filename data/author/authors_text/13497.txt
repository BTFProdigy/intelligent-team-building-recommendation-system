Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 439?446,
Beijing, August 2010
Hierarchical Phrase-based Machine Translation with Word-based
Reordering Model
Katsuhiko Hayashi*, Hajime Tsukada**
Katsuhito Sudoh**, Kevin Duh**, Seiichi Yamamoto*
*Doshisha University
katsuhiko-h@is.naist.jp, seyamamo@mail.doshisha.ac.jp
**NTT Communication Science Laboratories
tsukada, sudoh, kevinduh@cslab.kecl.ntt.co.jp
Abstract
Hierarchical phrase-based machine trans-
lation can capture global reordering with
synchronous context-free grammar, but
has little ability to evaluate the correctness
of word orderings during decoding. We
propose a method to integrate word-based
reordering model into hierarchical phrase-
based machine translation to overcome
this weakness. Our approach extends the
synchronous context-free grammar rules
of hierarchical phrase-based model to in-
clude reordered source strings, allowing
efficient calculation of reordering model
scores during decoding. Our experimen-
tal results on Japanese-to-English basic
travel expression corpus showed that the
BLEU scores obtained by our proposed
system were better than those obtained by
a standard hierarchical phrase-based ma-
chine translation system.
1 Introduction
Hierarchical phrase-based machine translation
(Chiang, 2007; Watanabe et al, 2006) is one of
the promising statistical machine translation ap-
proaches (Brown et al, 1993). Its model is for-
mulated by a synchronous context-free grammar
(SCFG) which captures the syntactic information
between source and target languages. Although
the model captures global reordering by SCFG,
it does not explicitly introduce reordering model
to constrain word order. In contrast, lexicalized
reordering models (Tillman, 2004; Koehn et al,
2005; Nagata et al, 2006) are extensively used
for phrase-based translation. These lexicalized re-
ordering models cannot be directly applied to hi-
erarchical phrased-based translation since the hi-
erarchical phrase representation uses nonterminal
symbols.
To handle global reordering in phrase-based
translation, various preprocessing approaches
have been proposed, where the source sentence
is reordered to target language order beforehand
(Xia and McCord, 2004; Collins et al, 2005; Li et
al., 2007; Tromble and Eisner, 2009). However,
preprocessing approaches cannot utilize other in-
formation in the translation model and target lan-
guage model, which has been proven helpful in
decoding.
This paper proposes a method that incorpo-
rates word-based reordering model into hierarchi-
cal phrase-based translation to constrain word or-
der. In this paper, we adopt the reordering model
originally proposed by Tromble and Eisner (2009)
for the preprocessing approach in phrase-based
translation. To integrate the word-based reorder-
ing model, we added a reordered source string
into the right-hand-side of SCFG?s rules. By this
extension, our system can generate the reordered
source sentence as well as target sentence and is
able to efficiently calculate the score of the re-
ordering model. Our method utilizes the transla-
tion model and target language model as well as
the reordering model during decoding. This is an
advantage of our method over the preprocessing
approach.
The remainder of this paper is organized as
follows. Section 2 describes the concept of our
approach. Section 3 briefly reviews our pro-
posed method on hierarchical phrase-based ma-
439
Standard SCFG X ?< X1 wa jinsei no X2 da , X1 is X2 of life>
SCFG (move-to-front) X ?< X1 wa jinsei no X2 da , wa X1 da X2 no jinsei , X1 is X2 of life>
SCFG (attach) X ?< X1 wa jinsei no X2 da , X1 wa da X2 no jinsei , X1 is X2 of life>
Table 1: A Japanese-to-English example of various SCFG?s rule representations. Japanese words are
romanized. Our proposed representation of rules has reordered source string to generate reordered
source sentence S? as well as target sentence T . The ?move-to-front? means Tromble and Eisner (2009)
?s algorithm and the ?attach? means Al-Onaizan and Papineni (2006) ?s algorithm.
chine translation model. We experimentally com-
pare our proposed system to a standard hierarchi-
cal phrase-based system on Japanese-to-English
translation task in Section 4. Then we discuss on
related work in Section 5 and conclude this paper
in Section 6.
2 The Concept of Our Approach
The preprocessing approach (Xia and McCord,
2004; Collins et al, 2005; Li et al, 2007; Tromble
and Eisner, 2009) splits translation procedure into
two stages:
S ? S? ? T (1)
where S is a source sentence, S? is a reordered
source sentence with respect to the word order of
target sentence T . Preprocessing approach has the
very deterministic and hard decision in reorder-
ing. To overcome the problem, Li et al (2007)
proposed k-best appoach. However, even with a
k-best approach, it is difficult to generate good hy-
potheses S? by using only a reordering model.
In this paper, we directly integrated the reorder-
ing model into the decoder in order to use the
reordering model together with other information
in the hierarchical phrase-based translation model
and target language model. Our approach is ex-
pressed as the following equation.
S ? (S? , T ). (2)
Our proposed method generates the reordered
source sentence S? by SCFG and evaluates the
correctness of the reorderings using a word-based
reordering model of S? which will be introduced
in section 3.4.
Figure 1: A derivation tree for Japanse-to-English
translation.
3 Hierarchical Phrase-based Model
Extension
3.1 Hierarchical Phrase-based Model
Hierarchical phrase-based model (Chiang, 2007)
induces rules of the form
X ?< ?, ?,?, w > (3)
where X is a non-terminal symbol, ? is a se-
quence string of non-terminals and source termi-
nals, ? is a sequence string of non-terminals and
target terminals. ? is a one-to-one correspon-
dence for the non-terminals appeared in ? and ?.
Given a source sentence S, the translation task
under this model can be expressed as
T? = T
(
argmax
D:S(D)=S
w(D)
)
(4)
where D is a derivation and w(D) is a score of
the derivation. Decoder seeks a target sentence
440
Figure 2: Reordered source sentence generated by
our proposed system.
T (D) which has the highest score w(D). S(D)
is a source sentence under a derivation D. Fig-
ure 1 shows the example of Japanese-to-English
translation by hierarchical phrase-based machine
translation model.
3.2 Rule Extension
To generate reordered source sentence S? as well
as target sentence T , we extend hierarchical
phrase rule expressed in Equation 3 to
X ?< ?, ?? , ?,?, w > (5)
where ?? is a sequence string of non-terminals and
source terminals, which is reordered ? with re-
spect to the word order of target string ?. The
reason why we add ?? to rules is to efficiently cal-
culate the reordering model scores. If each rule
does not have ?? , the decoder need to keep word
alignments because we cannot know word order
of S? without them. The calculation of reorder-
ing model scores using word alignments is very
wasteful when decoding.
The translation task under our model extends
Equation 4 to the following equation:
T? = (S?? , T? ) = (S? , T )
(
argmax
D:S(D)=S
w(D)
)
. (6)
Our system generates the reordered source sen-
tence S? as well as target sentence T . Figure 2
shows the generated reordered source sentence S?
Uni-gram Features
sr, s-posr
sr
s-posr
sl, s-posl
sl
s-posl
Bi-gram Features
sr, s-posr, sl, s-posl
s-posr, sl, s-posl
sr, sl, s-posl
sr, s-posr, s-posl
sr, s-posr, sl
sr, sl
s-posr, s-posl
Table 2: Features used by Word-based Reordering
Model. pos means part-of-speech tag.
when translating the example of Figure 1. Note
that the structure of S? is the same as that of target
sentence T . The decoder generates both Figure 2
and the right hand side of Figure 1, allowing us to
score both global and local word reorderings.
To add ?? to rules, we permuted ? into ?? after
rule extraction based on Grow-diag-final (Koehn
et al, 2005) alignment by GIZA++ (Och and Ney,
2003). To do this permutation on rules, we ap-
plied two methods. One is the same algorithm
as Tromble and Eisner (2009), which reorders
aligned source terminals and nonterminals in the
same order as that of target side and moves un-
aligned source terminals to the front of aligned
terminals or nonterminals (move-to-front). The
other is the same algorithm as AI-Onaizan and
Papineni (2006), which differs from Tromble and
Eisner?s approach in attaching unaligned source
terminals to the closest prealigned source termi-
nals or nonterminals (attach). This extension of
adding ?? does not increase the number of rules.
Table 1 shows a Japanese-to-English example
of the representation of rules for our proposed sys-
tem. Japanese words are romanized. Suppose that
source-side string is (X1 wa jinsei no X2 da) and
target-side string is (X1 is X2 of life) and their
word alignments are a=((jinsei , life) , (no , of)
, (da , is)). Source-side aligned words and non-
terminal symbols are sorted into the same order of
target string. Source-side unaligned word (wa) is
moved to the front or right of the prealigned sym-
bol (X1).
441
Surrounding Word Pos Features
s-posr, s-posr + 1, s-posl ? 1, s-posl
s-posr ? 1, s-posr, s-posl ? 1, s-posl
s-posr, s-posr + 1, s-posl, s-posl + 1
s-posr ? 1, s-posr, s-posl, s-posl + 1
Table 3: The Example of Context Features
3.3 Word-based Reordering Model
We utilize the following score(S?) as a feature for
the word-based reordering model. This is incor-
polated into the log-linear model (Och and Ney,
2002) of statistical machine translation.
score(S?) =
?
i,j:1?i<j?n
B[s?i, s
?
j ] (7)
B[s?l, s
?
r] = ? ? ?(s
?
l, s
?
r) (8)
where n is the length of reordered source sen-
tence S? (= (s?1 . . . s
?
n)), ? is a weight vector and
? is a vector of features. This reordering model,
which is originally proposed by Tromble and Eis-
ner (2009), can assign a score to any possible per-
mutation of source sentences. Intuitively B[s?l, s
?
r]
represents the score of ordering s?l before s
?
r; the
higher the value, the more we prefer word s?l oc-
curs before s?r. Whether S
?
l should occur before S
?
r
depends on how often this reordering occurs when
we reorder the source to target sentence order.
To train B, we used binary feature functions
? as used in (Tromble and Eisner, 2009), which
were introduced for dependency parsing by Mc-
Donald et al (2005). Table 2 shows the kind
of features we used in our experiments. We did
not use context features like surrounding word pos
features in Table 3 because they were not useful in
our preliminary experiments and propose an effi-
cient implementation described in the next section
in order to calculate this reordering model when
decoding. To train the parameter ?, we used the
perceptron algorithm following Tromble and Eis-
ner (2009).
3.4 Integration to Cube Pruning
CKY parsing and cube-pruning are used for de-
coding of hierarchical phrase-based model (Chi-
ang, 2007). Figure 3 displays that hierarchical
phrase-based decoder seeks new span [1,7] items
Figure 3: Creating new items from subitems and
rules, that have a span [1,7] in source sentence.
with rules, utilizing subspan [1,3] items and sub-
span [4,7] items. In this example, we use 2-gram
language model and +LM decoding. uni(?) means
1-gram language model cost for heuristics and in-
teraction usually means language model cost that
cannot be calculated offline. Here, we introduce
our two implementations to calculate word-based
reordering model scores in this decoding algo-
rithm.
First, we explain a naive implementation shown
in the left side of Figure 4. This algorithm per-
forms the same calculation of reordering model as
that of language model. Each item keeps a part of
reordered source sentence. The reordering score
of new item can be calculated as interaction cost
when combining subitems with the rule.
The right side of Figure 4 shows our pro-
posed implementation. This implementation can
be adopted to decoding only when we do not use
context features like surrounding word pos fea-
tures in Table 3 (and consider a distance between
words in features). If a span is given, the reorder-
ing scores of new item can be calculated for each
rule, being independent from the word order of
reordered source segment of a subitem. So, the
reordering model scores can be calculated for all
rules with spans by using a part of the input source
sentence before sorting them for cube pruning.
We expect this sorting of rules with reordering
442
Figure 4: The ?naive? and ?proposed? implementation to calculate the reordering cost of new items.
model scores will have good influence on cube
pruning. The right hand side of Figure 4 shows
the diffrence between naive and proposed imple-
mentation (S? is not shown to allow for a clear pre-
sentation). Note the difference is in where/when
the reordering scores are inserted: together with
the N -gram scores in the case of naive implemen-
tation; incorpolated into sorted rules for the pro-
posed implementation.
4 Experiment
4.1 Purpose
To reveal the effectiveness of integrating the re-
ordering model into decoder, we compared the
following setups:
? baseline: a standard hierarchical phrase-
based machine translation (Hiero) system.
? preprocessing: applied Tromble and Eisner?s
approach, then translate by Hiero system.
? Hiero system + reordering model: integrated
reordering model into Hiero system.
We used the Joshua Decoder (Li and Khudanpur,
2008) as the baseline Hiero system. This decoder
uses a log-linear model with seven features, which
consist of N -gram language model PLM (T ), lex-
ical translation model Pw(?|?), Pw(?|?), rule
translation model P (?|?), P (?|?), word penalty
and arity penalty.
The ?Hiero + Reordering model? system has
word-based reordering model as an additional fea-
ture to baseline features. For this approach, we
use two systems. One has ?move-to-front? sys-
tem and the other is ?attach? system explained in
Section 3.2. We implemented our proposed algo-
rithm in Section 3.4 to both ?Hiero + Reordering
model? systems. As for beam width, we use the
same setups for each system.
4.2 Data Set
Data Sent. Word. Avg. leng
Training ja 200.8K 2.4M 12.0
en 200.8K 2.3M 11.5
Development ja 1.0K 10.3K 10.3
en 1.0K 9.8K 9.8
Test ja 1.0K 14.2K 14.2
en 1.0K 13.5K 13.5
Table 4: The Data statistics
For experiments we used a Japanese-English
basic travel expression corpus (BTEC). Japanese
word order is linguistically very different from
English and we think Japanese-English pair is
a very good test bed for evaluating reordering
model.
443
XXXXXXXXXXXSystem
Metrics BLEU PER
Baseline (Hiero) 28.09 39.68
Preprocessing 17.32 45.27
Hiero + move-to-front 28.85 39.89
Hiero + attach 29.25 39.43
Table 5: BLEU and PER scores on the test set.
Our training corpus contains about 200.8k sen-
tences. Using the training corpus, we extracted
hierarchical phrase rules and trained 4-gram lan-
guage model and word-based reordering model.
Parameters were tuned over 1.0k sentences (devel-
opment data) with single reference by minimum
error rate training (MERT) (Och, 2003). Test data
consisted of 1.0k sentences with single reference.
Table 4 shows the condition of corpus in detail.
4.3 Results
Table 5 shows the BLEU (Papineni et al, 2001)
and PER (Niesen et al, 2000) scores obtained by
each system. The results clearly indicated that
our proposed system with word-based reorder-
ing model (move-to-front or attach) outperformed
baseline system on BLEU scores. In contrast,
there is no significant improvement from baseline
on PER. This suggests that the improvement of
BLEU mainly comes from reordering. In our ex-
periment, preprocessing approach resulted in very
poor scores.
4.4 Discussion
Table 6 displays examples showing the cause of
the improvements of our system with reordering
model (attach) comparing to baseline system. We
can see that the outputs of our system are more
fluent than those of baseline system because of re-
ordering model.
As a further analysis, we calculated the BLEU
scores of Japanese S? predicted from reorder-
ing model against true Japanese S? made from
GIZA++ alignments, were only 26.2 points on de-
velopment data. We think the poorness mainly
comes from unaligned words since they are un-
tractable for the word-based reordering model.
Actually, Japanese sentences in our training data
include 34.7% unaligned words. In spite of the
poorness, our proposed method effectively utilize
this reordering model in contrast to preprocessing
approach.
5 Related Work
Our approach is similar to preprocessing approach
(Xia and McCord, 2004; Collins et al, 2005; Li
et al, 2007; Tromble and Eisner, 2009) in that it
reorders source sentence in target order. The dif-
ference is this sentence reordering is done in de-
coding rather than in preprocessing.
A lot of studies on lexicalized reordering (Till-
man, 2004; Koehn et al, 2005; Nagata et al,
2006) focus on the phrase-based model. These
works cannnot be directly applied to hierarchi-
cal phrase-based model because of the difference
between normal phrases and hierarchical phrases
that includes nonterminal symbols.
Shen et al (2008,2009) proposed a way to inte-
grate dependency structure into target and source
side string on hierarchical phrase rules. This ap-
proach is similar to our approach in extending the
formalism of rules on hierarchical phrase-based
model in order to consider the constraint of word
order. But, our approach differs from (Shen et al,
2008; Shen et al, 2009) in that syntax annotation
is not necessary.
6 Conclusion and Future Work
We proposed a method to integrate word-based
reordering model into hierarchical phrase-based
machine translation system. We add ?? into the
hiero rules, but this does not increase the num-
ber of rules. So, this extension itself does not af-
fect the search space of decoding. In this paper
we used Tromble and Eisner?s reordering model
for our method, but various reordering model can
be incorporated to our method, for example S?
N -gram language model. Our experimental re-
sults on Japanese-to-English task showed that our
system outperformed baseline system and prepro-
cessing approach.
In this paper we utilize ?? only for reorder-
ing model. However, it is possible to use ?? for
other modeling, for example we can use it for
rule translation probabilities P (?? |?), P (?|??) for
additional feature functions. Of course, we can
444
S america de seihin no hanbai wo hajimeru keikaku ga ari masu ka . kono tegami wa koukuubin de nihon made ikura kakari masu ka .
TB sales of product in america are you planning to start ? this letter by airmail to japan . how much is it ?
TP are you planning to start products in the u.s. ? how much does it cost to this letter by airmail to japan ?
R do you plan to begin selling your products in the u.s. ? how much will it cost to send this letter by air mail to japan ?
Table 6: Examples of outputs for input sentence S from baseline system TB and our proposed sys-
tem (attach) TP . R is a reference. The underlined portions have equivalent meanings and show the
reordering differences.
also utilize reordered target sentence T ? for vari-
ous modeling as well. Addtionally we plan to use
S? for MERT because we hypothesize the fluent
S? leads to fluent T .
References
AI-Onaizan, Y. and K. Papineni. 2006. Distortion
models for statistical machine translation. In Proc.
the 44th ACL, pages 529?536.
Brown, P. F., S. A. D. Pietra, V. D. J. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical ma-
chine translation: Parameter estimation. Computa-
tional Linguitics, 19:263?312.
Chiang, D., K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
Proc. NAACL, pages 216?226.
Chiang, D. 2007. Hierachical phrase-based transla-
tion. Computational Linguitics, 33:201?228.
Collins, M., P. Koehn, and I. Kucerova. 2005. Clause
restructuring for statistical machine translation. In
Proc. the 43th ACL, pages 531?540.
Collins, M. 2002. Discriminative training methods for
hidden markov models. In Proc. of EMNLP.
Freund, Y. and R. E. Schapire. 1996. Experiments
with a new boosting algorithm. In Proc. of the 13th
ICML, pages 148?156.
Koehn, P., A. Axelrod, A-B. Mayne, C. Callison-
Burch, M. Osborne, and D. Talbot. 2005. Ed-
inburgh system description for 2005 iwslt speech
translation evaluation. In Proc. the 2nd IWSLT.
Li, Z. and S. Khudanpur. 2008. A scalable decoder
for parsing-based machine translation with equiv-
alent language model state maintenance. In Proc.
ACL SSST.
Li, C-H., D. Zhang, M. Li, M. Zhou, K. Li, and
Y. Guan. 2007. A probabilistic approach to syntax-
based reordering for statistical machine translation.
In Proc. the 45th ACL, pages 720?727.
McDonald, R., K. Crammer, and F. Pereira. 2005.
Spanning tree methods for discriminative training of
dependency parsers. In Thechnical Report MS-CIS-
05-11, UPenn CIS.
Nagata, M., K. Saito, K. Yamamoto, and K. Ohashi.
2006. A clustered global phrase reordering model
for statistical machine translation. In COLING-
ACL, pages 713?720.
Niesen, S., F.J. Och, G. Leusch, and H. Ney. 2000.
An evaluation tool for machine translation: Fast
evaluation for mt research. In Proc. the 2nd In-
ternational Conference on Language Resources and
Evaluation.
Och, F. J. and H. Ney. 2002. Discriminative train-
ing and maximum entropy models for statistical ma-
chine translation. In Proc. the 40th ACL, pages 295?
302.
Och, F. and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29:19?51.
Och, F. J. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. the 41th ACL,
pages 160?167.
Papineni, K. A., S. Roukos, T. Ward, and W-J. Zhu.
2001. Bleu: a method for automatic evaluation of
machine translation. In Proc. the 39th ACL, pages
311?318.
Shen, L., J. Xu, and R. Weischedel. 2008. A new
string-to-dependency machine translation algorithm
with a target dependency language model. In Proc.
ACL, pages 577?585.
Shen, L., J. Xu, B. Zhang, S. Matsoukas, and
R. Weischedel. 2009. Effective use of linguistic and
contextual information for statistical machine trans-
lation. In Proc. EMNLP, pages 72?80.
Tillman, C. 2004. A unigram orientation model
for statistical machine translation. In Proc. HLT-
NAACL, pages 101?104.
Tromble, R. and J. Eisner. 2009. Learning linear
ordering problems for better translation. In Proc.
EMNLP, pages 1007?1016.
445
Watanabe, T., H. Tsukada, and H. Isozaki. 2006. Left-
to-right target generation for hierarchical phrase-
based translation. In Proc. COLING-ACL, pages
777?784.
Xia, F. and M. McCord. 2004. Improving a statis-
tical mt system with automatically learned rewrite
patterns. In Proc. the 18th ICON, pages 508?514.
446
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1479?1488,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Third-order Variational Reranking on Packed-Shared Dependency Forests
Katsuhiko Hayashi?, Taro Watanabe?, Masayuki Asahara?, Yuji Matsumoto?
?Nara Insutitute of Science and Technology
Ikoma, Nara, 630-0192, Japan
?National Institute of Information and Communications Technology
Sorakugun, Kyoto, 619-0289, Japan
{katsuhiko-h,masayu-a,matsu}@is.naist.jp
taro.watanabe@nict.go.jp
Abstract
We propose a novel forest reranking algorithm
for discriminative dependency parsing based
on a variant of Eisner?s generative model. In
our framework, we define two kinds of gener-
ative model for reranking. One is learned from
training data offline and the other from a for-
est generated by a baseline parser on the fly.
The final prediction in the reranking stage is
performed using linear interpolation of these
models and discriminative model. In order to
efficiently train the model from and decode
on a hypergraph data structure representing a
forest, we apply extended inside/outside and
Viterbi algorithms. Experimental results show
that our proposed forest reranking algorithm
achieves significant improvement when com-
pared with conventional approaches.
1 Introduction
Recently, much of research on statistical parsing
has been focused on k-best (or forest) reranking
(Collins, 2000; Charniak and Johnson, 2005; Huang,
2008). Typically, reranking methods first generate
a list of top-k candidates (or a forest) from a base-
line system, then rerank the candidates with arbi-
trary features that are intractable within the baseline
system. In the reranking framework, the baseline
system is usually modeled with a generative model,
and a discriminative model is used for reranking.
Sangati et al (2009) reversed the usual order of the
two models for dependency parsing by employing
a generative model to rescore the k-best candidates
provided by a discriminative model. They use a vari-
ant of Eisner?s generative model C (Eisner, 1996b;
Eisner, 1996a) for reranking and extend it to capture
higher-order information than Eisner?s second-order
generative model. Their reranking model showed
large improvements in dependency parsing accu-
racy. They reported that the discriminative model is
very effective at filtering out bad candidates, while
the generative model is able to further refine the se-
lection among the few best candidates.
In this paper, we propose a forest generative
reranking algorithm, opposed to Sangati et al
(2009)?s approach which reranks only k-best candi-
dates. Forests usually encode better candidates more
compactly than k-best lists (Huang, 2008). More-
over, our reranking uses not only a generative model
obtained from training data, but also a sentence spe-
cific generative model learned from a forest. In the
reranking stage, we use linearly combined model
of these models. We call this variational rerank-
ing model. The model proposed in this paper is
factored in the third-order structure, therefore, its
non-locality makes it difficult to perform the rerank-
ing with an usual 1-best Viterbi search. To solve
this problem, we also propose a new search algo-
rithm, which is inspired by the third-order dynamic
programming parsing algorithm (Koo and Collins,
2010). This algorithm enables us an exact 1-best
reranking without any approximation. We summa-
rize our contributions in this paper as follows.
? To extend k-best to forest generative reranking.
? We introduce variational reranking which is a
combination approach of generative reranking
and variational decoding (Li et al, 2009).
? To obtain 1-best tree in the reranking stage, we
1479
propose an exact 1-best search algorithm with
the third-order model.
In experiments on English Penn Treebank data,
we show that our proposed methods bring signif-
icant improvement to dependency parsing. More-
over, our variational reranking framework achieves
consistent improvement, compared to conventional
approaches, such as simple k-best and forest-based
generative reranking algorithms.
2 Dependency Parsing
Given an input sentence x ? X , the task of statis-
tical dependency parsing is to predict output depen-
dencies y? for x. The task is usually modeled within a
discriminative framework, defined by the following
equation:
y? = argmax
y?Y
s(x, y)
= argmax
y?Y
?? ? F(y, x) (1)
where Y is the output space, ? is a parameter vector,
and F() is a set of feature functions.
We denote a set of candidates as G(x). By using
G(x), the conditional probability p(y|x) is typically
derived as follows:
p(y|x) = e
??s(x,y)
Z(x) =
e??s(x,y)?
y?G(x) e??s(x,y)
(2)
where s(x, y) is the score function shown in Eq.1
and ? is a scaling factor to adjust the sharpness of
the distribution and Z(x) is a normarization factor.
2.1 Hypergraph Representation
We propose to encode many hypotheses in a com-
pact representation called dependency forest. While
there may be exponentially many dependency trees,
the forest represents them in polynomial space. A
dependency forest (or tree) can be defined as a hy-
pergraph data strucureHG (Tu et al, 2010).
Figure 1 shows an example of a hypergraph for a
dependency tree. A shaded hyperedge e is defined
as the following form:
e : ?(I1,2, girl3,5,with5,8), saw1,8?.
.
.
.top0,8
..
.
..saw1,8:V
.
.
. .I1,2:N .. . . . .
.
..girl3,5:N
..a3,4:D ..
. .
.
..with5,8:P
..
.
..telescope6,8:N
. ..a6,7:D ..
. .
. .
.e . . . . .
. .
Figure 1: An example of dependency tree for a sentence
?I saw a girl with a telescope?.
The node saw1,8 is a head node of e. The nodes, I1,2,
girl3,5 and with5,8, are tail nodes of e. The hyper-
edge e is an incoming edge for saw1,8 and outgoing
edge for each of I1,2, girl3,5 and with5,81.
More formally, HG(x) of a forest is a pair
?V,E?, where V is a set of nodes and E is a set
of hyperedges. Given a length m sentence x =
(w1 . . . wm), each node v ? V is in the form of
wi,j (= (wi . . . wj+1)) which denotes that a word
w dominates the substring from positions i to j. In
our implementation, each word is paired with POS-
tag tag(w). We denote the root node of dependency
tree y as top. Each hyperedge e ? E is a pair
?tails(e), head(e)?, where head(e) ? V is the head
and tails(e) ? V + are its dependants. For nota-
tional brevity of algorithmic description, we do not
distinguish left and right tails in the definition, but,
our implementation implicitly distinguishes left tails
tailsL(e) and right tails tailsR(e). We define the set
of incoming edges of a node v as IE(v) and the set
of outgoing edges of a node v as OE(v).
3 Forest Reranking
3.1 Generative Model for Reranking
Given a node v in a dependency tree y, the left and
right children are generated as two separate Markov
sequences, each conditioned on ancestral and sibling
information (context). Like a variation of Eisner?s
generative model C (Eisner, 1996b; Eisner, 1996a),
1In Figure 1, according to custom of dependency tree
description, the direction of hyperedge is written as from
head to tail nodes. However, in this paper, ?incoming? and
?outgoing? have the same meanings as those in (Huang, 2006).
1480
Table 1: An event list of tri-sibling model whose event
space is v|h, sib, tsib, dir, extracted from hyperedge e in
Figure 1. EOC is an end symbol of sequence.
event space
I | saw NONE NONE L
EOC | saw I NONE L
girl | saw NONE NONE R
with | saw girl NONE R
EOC | saw with girl R
the probability of our model q is defined as follows:
q(v) =
|tailsL(e)|?
l=1
q(vl|C(vl)) ? q(vl)
?
|tailsR(e)|?
r=1
q(vr|C(vr)) ? q(vr) (3)
where |tailsL(e)| and |tailsR(e)| are the number of
left and right children of v, vl and vr are the left and
right child of position l and r in each side. C(v) is
a context event space of v. We explain the context
event space later in more detail. The probability of
the entire dependency tree y is recursively computed
by q(y(top)) where y(top) denotes a top node of y.
The probability q(v|C(v)) is dependent on a con-
text space C(v) for a node v. We define two kinds of
context spaces. First, we define a tri-sibling model
whose context space consists of the head node, sib-
ling node, tri-sibling node and direction of a node
v:
q1(v|C(v)) = q1(v|h, sib, tsib, dir) (4)
where h, sib and tsib are head, sibling and tri-sibling
node of v, and dir is a direction of v from h. Table
1 shows an example of an event list of the tri-sibling
model, which is extracted from hyperedge e in Fig-
ure 1. EOC indicates the end of the left or right child
sequence. This is factored in a tri-sibling structure
shown in the left side of Figure 2.
Eq.4 is further decomposed into a product of the
form consisting of three terms:
q1(v|h, sib, tsib, dir) (5)
= q1(dist(v, h), wrd(v), tag(v)|h, sib, tsib, dir)
= q1(tag(v)|h, sib, tsib, dir)
?q1(wrd(v)|tag(v), h, sib, tsib, dir)
?q1(dist(v, h)|wrd(v), tag(v), h, sib, tsib, dir)
where tag(v) and wrd(v) are the POS-tag and word
of v and dist(v, h) is the distance between positions
of v and h. The values of dist(v, h) are partitioned
into 4 categories: 1, 2, 3 ? 6, 7 ??.
Second, following Sangati et al (2009), we define
a grandsibling model whose context space consists
of the head node, sibling node, grandparent node and
direction of a node v.
q2(v|C(v)) = q2(v|h, sib, g, dir) (6)
where g is a grandparent node of v. Analogous to
Eq.5, Eq.6 is decomposed into three terms:
q2(v|h, sib, g, dir) (7)
= q2(dist(v, h), wrd(v), tag(v)|h, sib, g, dir)
= q2(tag(v)|h, sib, g, dir)
?q2(wrd(v)|tag(v), h, sib, g, dir)
?q2(dist(v, h)|wrd(v), tag(v), h, sib, g, dir)
where notations are the same as those in Eq.5 with
the exception of tri-sibling tsib and grandparent g.
This model is factored in a grandsibling structure
shown in the right side of Figure 2.
The direct estimation of tri-sibling and grandsib-
ling models from a corpus suffers from serious data
sparseness issues. To overcome this, Eisner (1996a)
proposed a back-off strategy which reduces the con-
ditioning of a model. We show the reductions list
for each term of two models in Table 2. The usage
of reductions list is identical to Eisner (1996a) and
readers may refer to it for further details.
The final prediction is performed using a log-
linear interpolated model. It interpolates the base-
line discriminative model and two (tri-sibling and
grandsibling) generative models.
y? = argmax
y?G(x)
2?
n=1
log qn(top(y))?n
+ log p(y|x)?base (8)
where ? are parameters to adjust the weight of each
term in prediction. These parameters are tuned using
MERT algorithm (Och, 2003) on development data
using a criterion of accuracy maximization. The rea-
son why we chose MERT is that it effectively tunes
dense parameters with a line search algorithm.
1481
Table 2: Reduction lists for tri-sibling and grandsibling models: wt(), w() and t() mean word and POS-tag, word,
POS-tag for a node. d indicates the direction. The first reduction on the list keeps all or most of the original condition;
later reductions throw away more and more of this information.
tri-sibling grandsibling
1-st term 2-nd term 3-rd term 1-st term 2-nd term 3-rd term
wt(h),wt(sib),wt(tsib),d wt(h),t(sib),d wt(v),t(h),t(sib),d wt(h),wt(sib),wt(g),d wt(h),t(sib),d wt(v),t(h),t(sib),d
wt(h),wt(sib),t(tsib),d t(h),t(sib),d t(v),t(h),t(sib),d wt(h),wt(sib),t(g),d t(h),t(sib),d t(v),t(h),t(sib),d
t(h),wt(sib),t(tsib),d ? ? t(h),wt(sib),t(g),d ? ?wt(h),t(sib),t(tsib),d wt(h),t(sib),t(g),d
t(h),t(sib),t(tsib),d ? ? t(h),t(sib),t(g),d ? ?
..h .tsib .sib .v ..g .h .sib .v
Figure 2: The left side denotes tri-sibling structure and
the right side denotes grandsibling structure.
Table 3: A summarization of the model factorization and
order
first-order McDonald et al (2005)
second-order Eisner (1996a)
(sibling) McDonald et al (2005)
third-order tri-sibling model
(tri-sibling) Model 2 (Koo and Collins, 2010)
third-order grandsibling model (Sangati et al, 2009)
(grandsibling) Model 1 (Koo and Collins, 2010)
3.2 Exact Search Algorithm
Our baseline discriminative model uses first- and
second-order features provided in (McDonald et al,
2005; McDonald and Pereira, 2006). Therefore,
both our tri-sibling model and baseline discrimina-
tive model integrate local features that are factored
in one hyperedge. On the other hand, the grandsib-
ling model has non-local features because the grand-
parent is not factored in one hyperedge. We sum-
marize the order of each model in Table 3. Our
reranking models are generative versions of Koo and
Collins (2010)?s third-order factorization model.
Non-locality of weight function makes it difficult
to perform the search of Eq.8 with an usual exact
Viterbi 1-best algorithm. One solution to resolve
the intractability is an approximate k-best Viterbi
search. For a constituent parser, Huang (2008) ap-
plied cube pruning techniques to forest reranking
with non-local features. Cube pruning is originally
proposed for the decoding of statistical machine
translation (SMT) with an integrated n-gram lan-
guage model (Chiang, 2007). It is an approximate
k-best Viterbi search algorithm using beam search
and lazy computation (Huang and Chiang, 2005).
In the case of a dependency parser, Koo and
Collins (2010) proposed dynamic-programming-
based third-order parsing algorithm, which enumer-
ates all grandparents with an additional loop. Our
hypergraph based search algorithm for Eq.8 share
the same spirit to their third-order parsing algo-
rithm since the grandsibling model is similar to their
model 1 in that it is factored in grandsibling struc-
ture. Algorithm 1 shows the search algorithm. This
is almost the same bottom-up 1-best Viterbi algo-
rithm except an additional loop in line 4. Line 4 ref-
erences outgoing edge e? of node h from a set of out-
going edges OE(h). tails(e) contains a node v, the
sibling node sib and tri-sibling node tsib of v, more-
over, the head of e? (head(e?)) is the grandparent for
v and sib. Thus, in line 5, we can capture tri-sibling
and grandsibling information and compute the cur-
rent inside estimate of Eq.8.
In our actual implementation, each score of com-
ponents in Eq.8 is represented as a cost. This is writ-
ten as a shortest path search algorithm with a tropi-
cal (real) semiring framework (Mohri, 2002; Huang,
2006). Therefore,? denotes the min operater and?
denotes the + operater. The function f is defined as
follows:
f(d(v1, e), . . . , d(v|e|, e))) =
|e|?
i=1
d(vi, e) (9)
where d(vi, e) denotes the current estimate of the
best cost for a pair of node vi and a hyperedge e.? sums the best cost of a pair of a sub span node
and hyperedge e. Each ctsib and cgsib in line 5 and
7 indicates the cost of tri-sibling and grandsibling
1482
Algorithm 1 Exact DP-Search Algorithm(HG(x))
1: for h ? V in bottom-up topological order do
2: for e ? IE(h) do
3: // tails(e) is {v1, . . . , v|e|
}.
4: for e? ? OE(h) do
5: d(h, e?) = ?f(d(v1, e), . . . , d(v|e|, e)) ? we ? ctsib(h, tails(e)) ? cgsib(head(e?), h, tails(e))
6: if h == top then
7: d(h) = ?f(d(v1, e), . . . , d(v|e|, e)) ? we ? ctsib(h, tails(e))
model. we indicates the cost of hyperedge e com-
puted from a baseline discriminative model. Lines
6-7 denote the calculation of the best cost for a top
node. We do not compute the cost of the grandsib-
ling model when h is top node because top node has
no outgoing edges.
Our baseline k-best second-order parser is imple-
mented using Huang and Chiang (2005)?s algorithm
2 whose time complexity is O(m3+mk log k). Koo
and Collins (2010)?s third-order parser has O(m4)
time complexity and is theoretically slower than our
baseline k-best parser for a long sentence. Our
search algorithm is based on the third-order parsing
algorithm, but, the search space is previously shrank
by a baseline parser?s k-best approximation and a
forest pruning algorithm presented in the next sec-
tion. Therefore, the time efficiency of our reranking
is unimpaired.
3.3 Forest Pruning
Charniak and Johnson (2005) and Huang (2008)
proposed forest pruning algorithms to reduce the
size of a forest. Huang (2008)?s pruning algo-
rithm uses a 1-best Viterbi inside/outside algorithm
to compute an inside probability ?(v) and an out-
side probability ?(v), while Charniak and Johnson
(2005) use the usual inside/outside algorithm.
In our experiments, we use Charniak and Johnson
(2005)?s forest pruning criterion because the varia-
tional model needs traditional inside/outside proba-
bilities for its ML estimation. We prune away all
hyperedges that have score < ? for a threshold ?.
score = ??(e)?(top) . (10)
Following Huang (2008), we also prune away nodes
with all incoming and outgoing hyperedges pruned.
4 Variational Reranking Model
In place of a maximum a posteriori (MAP) decision
based on Eq.2, the minimum Bayes risk (MBR) deci-
sion rule (Titov and Henderson, 2006) is commonly
used and defined as following equation:
y? = argmin
y?G(x)
?
y??G(x)
loss(y, y?)p(y?|x) (11)
where loss(y, y?) represents a loss function2. As an
alternative to the MBR decision rule, Li et al (2009)
proposed a variational decision rule that rescores
candidates with an approximate distribution q? ? Q.
y? = argmax
y?G(x)
q?(y) (12)
where q? minimizes the KL divergence KL(p||q)
q? = argmin
q?Q
KL(p||q)
= argmax
q?Q
?
y?G(x)
p log q (13)
where each p and q represents p(y|x) and q(y). For
SMT systems, q? is modeled by n-gram language
model over output strings. While the decoding based
on q? is an approximation of intractable MAP de-
coding3, it works as a rescoring function for candi-
dates generated from a baseline model. Here, we
propose to apply the variational decision rule to de-
pendency parsing. For dependency parsing, we can
choose to model q? as the tri-sibling and grandsib-
ling generative models in section 3.
2In case of dependency parsing, Titov and Henderson (2006)
proposed that a loss function is simply defined using a depen-
dency attachment score.
3In SMT, a marginalization of all derivations which yield
a paticular translation needs to be carried out for each trans-
lation. This makes the MAP decoding NP-hard in SMT. This
variational approximate framework can be applied to other tasks
collapsing spurious ambiguity, such as latent-variable parsing
(Matsuzaki et al, 2005).
1483
Algorithm 2 DP-ML Estimation(HG(x))
1: run inside and outside algorithm onHG(x)
2: for v ? V do
3: for e ? IE(v) do
4: ctsib = pe ? ?(v)/?(top)
5: for u ? tails(e) do
6: ctsib = ctsib ? ?(u)
7: for e? ? IE(u) do
8: cgsib = pe ? pe? ? ?(v)/?(top)
9: for u? ? tails(e) \ u do
10: cgsib = cgsib ? ?(u?)
11: for u?? ? tails(e?) do
12: cgsib = cgsib ? ?(u??)
13: for u?? ? tails(e?) do
14: c2(u??|C(u??))+ = cgsib
15: c2(C(u??))+ = cgsib
16: for u ? tails(e) do
17: c1(u|C(u))+ = ctsib
18: c1(C(u))+ = ctsib
19: MLE estimate q?1 , q?2 using formula Eq.14
4.1 ML Estimation from a Forest
q?(v|C(v)) is estimated from a forest using a max-
imum likelihood estimation (MLE). The count of
events is no longer an integer count, but an expected
count under p, which is formulated as follows:
q?(v|C(v)) = c(v|C(v))c(C(v))
=
?
y p(y|x)cv|C(v)(y)?
y p(y|x)cC(v)(y)
(14)
where ce(y) is the number of event e in y. The es-
timation of Eq.14 can be efficiently performed on a
hypergraph data structureHG(x) of a forest.
Algorithm 2 shows the estimation algorithm.
First, it runs the inside/outside algorithm onHG(x).
We denote inside weight for a node v as ?(v) and
outside weight as ?(v). For each hyperedge e, we
denote ctsib as the posterior weight for computing
expected count c1 of events in the tri-sibling model
q?1 . Lines 16-18 compute c1 for all events occuring
in a hyperedge e.
The expected count c2 needed for the estimation
of grandsibling model q?2 is extracted in lines 7-15.
c2 for a grandsibling model must be extracted over
two hyperedges e and e? because it needs grandpar-
ent information. Lines 8-12 show the algorithm to
compute the posterior weight cgsib of e and e?, which
 92
 93
 94
 95
 96
 97
 98
 99
 100
 0  200  400  600  800  1000  1200  1400
Un
la
be
le
d 
Ac
cu
ra
cy
the number of hyperedges per sentence
p=0.001
k=20
k=100
"kbest"
"forest"
Figure 3: The relationship between tha data size (the
number of hyperedges) and oracle scores on develop-
ment data: Forests encode candidates with high accuracy
scores more compactly than k-best lists.
is similar to that to compute the posterior weight
of rules of tree substitution grammars used in tree-
based MT systems (Mi and Huang, 2008). Lines
13-15 compute expected counts c2 of events occur-
ing over two hyperedges e and e?. Finally, line 19
estimates q?1 and q?2 using the form in Eq.14.
Li et al (2009) assumes n-gram locality of the
forest to efficiently train the model, namely, the
baseline n-gram model has larger n than that of vari-
ational n-gram model. In our case, grandsibling lo-
cality is not embedded in the forest generated from
the baseline parser. Therefore, we need to reference
incoming hyperedges of tail nodes in line 7.
y? of Eq.12 may be locally appropriate but glob-
ally inadequate because q? only approximates p.
Therefore, we log-linearly combine q? with a global
generative model estimated from the training data
and the baseline discriminative model.
y? = argmax
y?G(x)
2?
n=1
log qn(top(y))?n
+
2?
n=1
log q?n(top(y))?
?
n
+ log p(y|x)?base (15)
Algorithm1 is also applicable to the decoding of
Eq.15. Note that this framework is a combination of
variational decoding and generative reranking. We
call this framework variational reranking.
1484
Table 4: The statistics of forests and 20-best lists on de-
velopment data: this shows the average number of hyper-
edges and nodes per sentence and oracle scores.
forest 20-best
pruning threshold ? = 10?3 ?
ave. num of hyperedges 180.67 255.04
ave. num of nodes 135.74 491.42
oracle scores 98.76 96.78
5 Experiments
Experiments are performed on English Penn Tree-
bank data. We split WSJ part of the Treebank into
sections 02-21 for training, sections 22 for develop-
ment, sections 23 for testing. We use Yamada and
Matsumoto (2003)?s head rules to convert phrase
structure to dependency structure. We obtain k-best
lists and forests generated from the baseline discrim-
inative model which has the same feature set as pro-
vided in (McDonald et al, 2005), using the second-
order Eisner algorithms. We use MIRA for training
as it is one of the learning algorithms that achieves
the best performance in dependency parsing. We set
the scaling factor ? = 1.0.
We also train a generative reranking model from
the training data. To reduce the data sparseness
problem, we use the back-off strategy proposed in
(Eisner, 1996a). Parameters ? are trained using
MERT (Och, 2003) and for each sentence in the de-
velopment data, 300-best dependency trees are ex-
tracted from its forest. Our variational reranking
does not need much time to train the model be-
cause the training is performed over not the train-
ing data (39832 sentences) but the development data
(1700 sentences)4. After MERT was performed un-
til the convergence, the variational reranking finally
achieved a 94.5 accuracy score on development data.
5.1 k-best Lists vs. Forests
Figure 3 shows the relationship between the size of
data structure (the number of hyperedges) and accu-
racy scores on development data. Obviously, forests
can encode a large number of potential candidates
more compactly than k-best lists. This means that
4To generate forests, sentences are parsed only once before
the training. MERT is performed over the forests. We can also
apply a more efficient hypergraph MERT algorithm (Kumar et
al., 2009) to the training than a simple MERT algorithm.
for reranking, there is more possibility of selecting
good candidates in forests than k-best lists.
Table 4 shows the statistics of forests and 20-
best lists on development data. This setting, thresh-
old ? = 10?3 for pruning, is also used for testing.
Forests, which have an average of 180.67 hyper-
edges per sentence, achieve oracle score of 98.76,
which is about 1.0% higher than the 96.78 oracle
score of 20-best lists with 255.04 hyperedges per
sentence. Though the size of forests is smaller than
that of k-best lists, the oracle scores of forests are
much higher than those of k-best lists.
5.2 The Performance of Reranking
First, we compare the performance of variational de-
coding with that of MBR decoding. The results are
shown in Table 5. Variational decoding outperforms
MBR decodings. However, compared with base-
line, the gains of variational and MBR decoding are
small. Second, we also compare the performance of
variational reranking with k-best and forest gener-
ative reranking algorithms. Table 6 shows that our
variational reranking framework achieves the high-
est accuracy scores.
Being different from the decoding framework,
reranking achieves significant improvements. This
result is intuitively reasonable because the rerank-
ing model obtained from training data has the ability
to select a globally consistent candidate, while the
variational approximate model obtained from a for-
est only supports selecting a locally consistent can-
didate. On the other hand, the fact that variational
reranking achieves the best results clearly indicates
that the combination of sentence specific generative
model and that obtained from training data is suc-
cessful in selecting both locally and globally appro-
priate candidate from a forest.
Table 7 shows the parsing time (on 2.66GHz
Quad-Core Xeon) of the baseline k-best, generative
reranking and variational reranking parsers (java im-
plemented). The variational reranking parser con-
tains the following procedures.
1. k-best forest creation (baseline)
2. Estimation of variational model
3. Forest pruning
4. Search with the third-order model
Our reranking parser incurred little overhead to the
1485
Table 5: The comparison of the decoding frameworks:
MBR decoding seeks a candidate which has the high-
est accuracy scores over a forest (Kumar et al, 2009).
Variational decoding is performed based on Eq.8.XXXXXXXXXXDecoding
Eval Unlabeled
baseline 91.9
MBR (8-best forest) 91.99
Variational (8-best forest) 92.17
Table 6: The comparison of the reranking frameworks:
Generative means k-best or forest reranking algorithm
based on a generative model estimated from a corpus.
Variational reranking is performed based on Eq.15.XXXXXXXXXXReranking
Eval Unlabeled
Generative (8-best) 92.66
Generative (8-best forest) 92.72
Variational (8-best forest) 92.87
Table 7: The parsing time (CPU second per sentence) and
accuracy score of the baseline k-best, generative rerank-
ing and variational reranking parsers
k baseline generative variational
2 0.09 (91.9) +0.03 (92.67) +0.05 (92.76)
4 0.1 (91.9) +0.05 (92.68) +0.09 (92.81)
8 0.13 (91.9) +0.06 (92.72) +0.11 (92.87)
16 0.18 (91.9) +0.07 (92.75) +0.12 (92.89)
32 0.29 (91.9) +0.07 (92.73) +0.13 (92.89)
64 0.54 (91.9) +0.08 (92.72) +0.15 (92.87)
Table 8: The comparison of tri-sibling and grandsibling
models: the performance of the grandsibling model out-
performs that of the tri-sibling model.PPPPPPPModel
Eval Unlabeled
tri-sibling 92.63
grandsibling 92.74
baseline parser in terms of runtime. This means that
our reranking parser can parse sentences at reason-
able times.
5.3 The Effects of Third-order Factors and
Error Analysis
From results in section 5.2, our variational rerank-
ing model achieves higher accuracy scores than the
others. To analyze the factors that improve accu-
racy scores, we further investigate whether varia-
tional reranking is performed better with the tri-
sibling or grandsibling model. Table 8 indicates that
grandsibling model achieves a larger gain than that
of tri-sibling model. Table 9 shows the examples
whose accuracy scores improved by the grandsib-
ling model. For example, the dependency relation-
ship from Verb to Noun phrase was corrected by our
proposed model.
On the other hand, many errors remain still in
Table 10: Comparison of our best result (using 16-best
forests) with other best-performing Systems on the whole
section 23
Parser English
McDonald et al (2005) 90.9
McDonald and Pereira (2006) 91.5
Koo et al (2008) standard 92.02
Huang and Sagae (2010) 92.1
Koo and Collins (2010) model1 93.04
Koo and Collins (2010) model2 92.93
this work 92.89
Koo et al (2008) semi-sup 93.16
Suzuki et al (2009) 93.79
our results. In our experiments, 48% of sentences
which contain errors have Prepositionalword errors.
In fact, well-known PP-Attachment is a problem to
be solved for natural language parsers. Other re-
maining errors are caused by symbols such as .,:??().
45% sentences contain such a dependency mistake.
Adding features to solve these problems may poten-
tially improve our parser more.
5.4 Comparison with Other Systems
Table 10 shows the comparison of the performance
of variational reranking (16-best forests) with that of
other systems. Our method outperforms supervised
parsers with second-order features, and achieves
comparable results compared to a parser with third-
order features (Koo and Collins, 2010). We can not
directly compare our method with semi-supervised
parsers such as Koo et al (2008)?s semi-sup and
Suzuki et al (2009), because ours does not use addi-
tional unlabeled data for training. The model trained
from unlabeled data can be easily incorporated into
our reranking framework. We plan to investigate
semi-supervised learning in future work.
1486
Table 9: Examples of outputs for input sentence No.148 and No.283 in section 23 from baseline and variational
reranking parsers. The underlined portions show the effect of the grandsibling model.
sent (No.148) A quick turnaround is crucial to Quantum because its cash requirements remain heavy .
correct 3 3 4 0 4 5 6 4 11 11 12 8 12 4
baseline 3 3 4 0 4 5 6 4 11 11 8 8 12 4
proposed 3 3 4 0 4 5 6 4 11 11 12 8 12 4
sent (No.283) Many called it simply a contrast in styles .
correct 2 0 2 6 6 2 6 7 2
baseline 2 0 2 2 6 2 6 7 2
proposed 2 0 2 6 6 2 6 7 2
6 Related Work
Collins (2000) and Charniak and Johnson (2005)
proposed a reranking algorithm for constituent
parsers. Huang (2008) extended it to a forest rerank-
ing algorithm with non-local features. Our frame-
work is for a dependency parser and the decoding in
the reranking stage is done with an exact 1-best dy-
namic programming algorithm. Sangati et al (2009)
proposed a k-best generative reranking algorithm for
dependency parsing. In this paper, we use a similar
generative model, but combined with a variational
model learned on the fly. Moreover, our framework
is applicable to forests, not k-best lists.
Koo and Collins (2010) presented third-order de-
pendency parsing algorithm. Their model 1 is de-
fined by an enclosing grandsibling for each sibling
or grandchild part used in Carreras (2007). Our
grandsibling model is similar to the model 1, but
ours is defined by a generative model. The decod-
ing in the reranking stage is also similar to the pars-
ing algorithm of their model 1. In order to capture
grandsibling factors, our decoding calculates inside
probablities for not the current head node but each
pair of the node and its outgoing edges.
Titov and Henderson (2006) reported that the
MBR approach could be applied to a projective de-
pendency parser. In the field of SMT, for an approx-
imation of MAP decoding, Li et al (2009) proposed
variational decoding and Kumar et al (2009) pre-
sented hypergraph MBR decoding. Our variational
model is inspired by the study of Li et al (2009) and
we apply it to a dependency parser in order to select
better candidates with third-order information. We
also propose an efficient algorithm to estimate the
non-local third-order model structure.
7 Conclusions
In this paper, we propose a novel forest reranking
algorithm for dependency parsing. Our reranking
algorithm is a combination approach of generative
reranking and variational decoding. The search al-
gorithm in the reranking stage can be performed
using dynamic programming algorithm. Our vari-
ational reranking is aimed at selecting a candidate
from a forest, which is correct both in local and
global. Our experimental results show more signif-
icant improvements than conventional approaches,
such as k-best and forest generative reranking.
In the future, we plan to investigate more ap-
propriate generative models for reranking. PP-
Attachment is one of the most difficult problems
for a natural language parser. We plan to exam-
ine to model such a complex structure (granduncle)
(Goldberg and Elhadad, 2010) or higher-order struc-
ture than third-order for reranking which is compu-
tationally expensive for a baseline parser. As we
mentioned in Section 5.4, we also plan to incorpo-
rate semi-supervised learning into our framework,
which may potentially improve our reranking per-
formance.
Acknowledgments
Wewould like to thank GrahamNeubig andMasashi
Shimbo for their helpful comments and to the anony-
mous reviewers for their effort of reviewing our pa-
per and giving valuable comments. This work was
supported in part by Grant-in-Aid for Japan Society
for the Promotion of Science (JSPS) Research Fel-
lowship for Young Scientists.
1487
References
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. the CoNLL-
EMNLP, pages 957?961.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proc. the 43rd ACL, pages 173?180.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33:201?228.
M. Collins. 2000. Discriminative reranking for natural
language parsing. In Proc. the ICML.
J. M. Eisner. 1996a. An empirical comparison of prob-
ability models for dependency grammar. In Technical
Report, pages 1?18.
J. M. Eisner. 1996b. Three new probabilistic models for
dependency parsing: An exploration. In Proc. the 16th
COLING, pages 340?345.
Y. Goldberg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In Proc. the HLT-NAACL, pages 742?750.
L. Huang and D. Chiang. 2005. Better k-best parsing. In
Proc. the IWPT, pages 53?64.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proc. the ACL,
pages 1077?1086.
L. Huang. 2006. Dynamic programming al-
gorithms in semiring and hypergraph frame-
works. Qualification Exam Report, pages 1?19.
http://www.cis.upenn.edu/ lhuang3/wpe2/.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In Proc. the 46th ACL,
pages 586?594.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. the 48th ACL, pages 1?11.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proc. the ACL,
pages 595?603.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Effi-
cient minimum error rate training and minimum bayes-
risk decoding for translation hypergraphs and lattices.
In Proc. the 47th ACL, pages 163?171.
Z. Li, J. Eisner, and S. Khudanpur. 2009. Variational
decoding for statistical machine translation. In Proc.
the 47th ACL, pages 593?601.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic cfg with latent annotations. In Proc. the ACL, pages
75?82.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
EACL, pages 81?88.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
the 43rd ACL, pages 91?98.
H. Mi and L. Huang. 2008. Forest-based translation rule
extraction. In Proceedings of EMNLP, pages 206?
214.
M. Mohri. 2002. Semiring framework and algorithms
for shortest-distance problems. Automata, Languages
and Combinatorics, 7:321?350.
F. J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. the 41st ACL, pages
160?167.
F. Sangati, W. Zuidema, and R. Bod. 2009. A generative
re-ranking model for dependency parsing. In Proc. the
11th IWPT, pages 238?241.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009.
An empirical study of semi-supervised structured con-
ditional models for dependency parsing. In Proc. the
EMNLP, pages 551?560.
I. Titov and J. Henderson. 2006. Bayes risk minimiza-
tion in natural language parsing. In Technical Report,
pages 1?9.
Z. Tu, Y. Liu, Y. Hwang, Q. Liu, and S. Lin. 2010. De-
pendency forest for statistical machine translation. In
Proc. the 23rd COLING, pages 1092?1100.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
the IWPT, pages 195?206.
1488
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1382?1386,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Shift-Reduce Word Reordering for Machine Translation
Katsuhiko Hayashi?, Katsuhito Sudoh, Hajime Tsukada, Jun Suzuki, Masaaki Nagata
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
?hayashi.katsuhiko@lab.ntt.co.jp
Abstract
This paper presents a novel word reordering
model that employs a shift-reduce parser for
inversion transduction grammars. Our model
uses rich syntax parsing features for word re-
ordering and runs in linear time. We apply it to
postordering of phrase-based machine trans-
lation (PBMT) for Japanese-to-English patent
tasks. Our experimental results show that our
method achieves a significant improvement
of +3.1 BLEU scores against 30.15 BLEU
scores of the baseline PBMT system.
1 Introduction
Even though phrase-based machine translation
(PBMT) (Koehn et al, 2007) and tree-based MT
(Graehl and Knight, 2004; Chiang, 2005; Galley
et al, 2006) systems have achieved great success,
many problems remain for distinct language pairs,
including long-distant word reordering.
To improve such word reordering, one promis-
ing way is to separate it from the translation pro-
cess as preordering (Collins et al, 2005; DeNero
and Uszkoreit, 2011) or postordering (Sudoh et al,
2011; Goto et al, 2012). Many studies utilize a rule-
based or a probabilistic model to perform a reorder-
ing decision at each node of a syntactic parse tree.
This paper presents a parser-based word reorder-
ing model that employs a shift-reduce parser for in-
version transduction grammars (ITG) (Wu, 1997).
To the best of our knowledge, this is the first study
on a shift-reduce parser for word reordering.
The parser-based reordering approach uses rich
syntax parsing features for reordering decisions.
Our propoesd method can also easily define such
.
Source-
ordered Target
Sentence (HFE)
Source Sen-
tence (J)
Target Sen-
tence (E)
reordering
Figure 1: A description of the postordering MT system.
non-local features as theN -gram words of reordered
strings. Even when using these non-local features,
the complexity of the shift-reduce parser does not
increase at all due to give up achieving an optimal
solution. Therefore, it works much more efficient.
In our experiments, we apply our proposed
method to postordering for J-to-E patent tasks be-
cause their training data for reordering have little
noise and they are ideal for evaluating reordering
methods. Although our used J-to-E setups need
a language-dependent scheme and we describe our
proposed method as a J-to-E postordering method,
the key algorithm is language-independent and it can
be applicable to preordering as well as postordering
if the training data for reordering are available.
2 Postordering by Parsing
As shown in Fig.1, postordering (Sudoh et al, 2011)
has two steps; the first is a translation step that trans-
lates an input sentence into source-ordered transla-
tions. The second is a reordering step in which the
translations are reordered in the target language or-
der. The key to postordering is the second step.
Goto et al (2012) modeled the second step by
parsing and created training data for a postordering
parser using a language-dependent rule called head-
finalization. The rule moves syntactic heads of a
lexicalized parse tree of an English sentence to the
1382
.S(saw)
. .VP(saw)
. .PP(with)
. .NP(telescope)
. .N(telescope)
.telescope.
D(a)
.a
.
PR(with)
.with
.
VP(saw)
. .NP(girl)
. .N(girl)
.girl.
D(a)
.a
.
V(saw)
.saw
.
NP(I)
.N(I)
.I
.
. .mita. .wo.shoujyo.de.bouenkyo. .wawatashi
.S(saw)
. .VP#(saw)
. .VP#(saw)
. .V(saw)
.saw
.
NP(wo)?a/an?
. .WO(wo)
.wo
N(girl)
.girl.
PP#(with)
. .PR(with)
.with
NP(telescope)?a/an?
.N(telescope)
.telescope
.
NP(wa)?no articles?
. .WA(wa)
.wa.
N(I)
.I
.
. .mita. .wo.shoujyo.de.bouenkyo. .wawatashi
Figure 2: An example of the head-finzalizaton process for an English-Japanese sentence pair: the left-hand side tree
is the original English tree, and the right-hand side tree is its head-final English tree.
end of the corresponding syntactic constituents. As
a result, the terminal symbols of the English tree are
sorted in a Japanese-like order. In Fig.2, we show an
example of head-finalization and a tree on the right-
hand side is a head-finalized English (HFE) tree of
an English tree on the left-hand side. We annotate
each parent node of the swapped edge with # sym-
bol. For example, a nonterminal symbol PP#(with)
shows that a noun phrase ?a/an telescope? and a
word ?with? are inverted.
For better word alignments, Isozaki et al (2012)
also deleted articles ?the? ?a? ?an? from English be-
cause Japanese has no articles, and inserted Japanese
particles ?ga? ?wo? ?wa? into English sentences.
We privilege the nonterminals of a phrase modified
by a deleted article to determine which ?the? ?a/an?
or ?no articles? should be inserted at the front of the
phrase. Note that an original English sentence can
be recovered from its HFE tree by using # symbols
and annotated articles and deleting Japanese parti-
cles.
As well as Goto et al (2012), we solve postorder-
ing by a parser whose model is trained with a set
of HFE trees. The main difference between Goto et
al. (2012)?s model and ours is that while the former
simply used the Berkeley parser (Petrov and Klein,
2007), our shift-reduce parsing model can use such
non-local task specific features as theN -gram words
of reordered strings without sacrificing efficiency.
Our method integrates postediting (Knight and
Chander, 1994) with reordering and inserts articles
into English translations by learning an additional
?insert? action of the parser. Goto et al (2012)
solved the article generation problem by using an
N -gram language model, but this somewhat compli-
cates their approach. Compared with other parsers,
one advantage of the shift-reduce parser is to easily
define such additional operations as ?insert?.
HFE trees can be defined as monolingual ITG
trees (DeNero and Uszkoreit, 2011). Our monolin-
gual ITG G is a tuple G = (V, T, P, I, S) where V
is a set of nonterminals, T is a set of terminals, P
is a set of production rules, I is a set of nontermi-
nals on which ?the? ?a/an? or ?no articles? must be
determined, and S is the start symbol.
Set P consists of terminal production rules that
are responsible for generating word w(? T ):
X ? w
and binary production rules in two forms:
X ? YZ
X# ? YZ
where X, X#, Y and Z are nonterminals. On
the right-hand side, the second rule generates two
phrases Y and Z in the reverse order. In our experi-
ments, we removed all unary production rules.
3 Shift-Reduce Parsing
Given an input sentence w1 . . . wn, the shift-reduce
parser uses a stack of partial derivations, a buffer of
input words, and a set of actions to build a parse tree.
The following is the parser?s configuration:
? : ?i, j, S? : pi
where ? is the step size, S is a stack of elements
s0, s1, . . . , i is the leftmost span index of the stack
1383
top element s0, j is an index of the next input word
of the buffer, and pi is a set of predictor states1.
Each stack element has at least the following com-
ponents of its partial derivation tree:
s = {H, h, wleft, wright, a}
where H is a root nonterminal or a part-of-speech tag
of the subtree, h is a head index of H, a is a variable
to which ?the? ?a/an? ?no articles? or null are as-
signed, and wleft, wright are the leftmost and right-
most words of phrase H. When referring to compo-
nent ?, we use a s.? notation.
Our proposed system has 4 actions shift-X, insert-
x, reduce-MR-X and reduce-SR-X.
The shift-X action pushes the next input word
onto the stack and assigns a part-of-speech tag X to
the word. The deduction step is as follows:
X ? wj ? P
p
? ?? ?
? : ?i, j, S|s?0? : pi
? + 1 : ?j, j + 1, S|s?0|s0)? : {p}
where s0 is {X, j, wj , wj , null}.
The insert-x action determines whether to gener-
ate ?the? ?a/an? or ?no articles? (= x):
s?0.X ? I ? (s?0.a ?= ?the? ? s?0.a ?= ?a/an?)
? : ?i, j, S|s?0)? : pi
? + 1 : ?i, j, S|s0? : pi
where s?0 is {X, h, wleft, wright, a} and s0 is
{X, h, wleft, wright, x} (i ? h, left, right < j).
The side condition prevents the parser from inserting
articles into phrase X more than twice. During pars-
ing, articles are not explicitly inserted into the input
string: they are inserted into it when backtracking to
generate a reordered string after parsing.
The reduce-MR-X action has a deduction rule:
X ? Y Z ? P ? q ? pi
q
? ?? ?
: ?k, i, S|s?2|s?1? : pi? ? : ?i, j, S|s?1|s?0? : pi
? + 1 : ?k, j, S|s?2|s0? : pi?
1Since our notion of predictor states is identical to that in
(Huang and Sagae, 2010), we omit the details here.
s0.wh ? s0.th s0.H s0.H ? s0.th s0.wh ? s0.H
s1.wh ? s1.th s1.H s1.H ? s1.th s1.wh ? s1.H
s2.th ? s2.H s2.wh ? s2.H q0.w q1.w q2.w
s0.tl ? s0.L s0.wl ? s0.L s1.tl ? s1.L s1.wl ? s1.L
s0.wh ? s0.H ? s1.wh ? s1.H s0.H ? s1.wh s0.wh ? s1.H
s0.H ? s1.H s0.wh ? s0.H ? q0.w s0.H ? q0.w
s1.wh ? s1.H ? q0.w s1.H ? q0.w s1.th ? q0.w ? q1.w
s0.wh ? s0.H ? s1.H ? q0.w s0.H ? s1.wh ? s1.H ? q0.w
s0.H ? s1.H ? q0.w s0.th ? s1.th ? q0.w
s0.wh ? s1.H ? q0.w ? q1.w s0.H ? q0.w ? q1.w
s0.th ? q0.w ? q1.w s0.wh ? s0.H ? s1.H ? s2.H
s0.H ? s1.wh ? s1.H ? s2.H s0.H ? s1.H ? s2.wh ? s2.H
s0.H ? s1.H ? s2.H s0.th ? s1.th ? s2.th
s0.H ? s0.R ? s0.L s1.H ? s1.R ? s1.L s0.H ? s0.R ? q0.w
s0.H ? s0.L ? s1.H s0.H ? s0.L ? s1.wh s0.H ? s1.H ? s1.L
s0.wh ? s1.H ? s1.R
s0.wleft ? s1.wright s0.tleft ? s1.tright
s0.wright ? s1.wleft s0.tright ? s1.tleft
s0.a ? s0.wleft s0.a ? s0.tleft s0.a ? s0.wleft ? s1.wright
s0.a ? s0.tleft ? s1.tright s0.a ? s0.wh s0.a ? s0.th
Table 1: Feature templates: s.L and s.R denote the left
and right subnodes of s. l and r are head indices of L and
R. q denotes a buffer element. t is a part-of-speech tag.
where s?0 is {Z, h0, wleft0, wright0, a0} and s?1 is
{Y, h1, wleft1, wright1, a1}. The action generates s0
by combining s?0 and s?1 with binary rule X?Y Z:
s0 = {X, h0, wleft1, wright0, a1}.
New nonterminal X is lexicalized with head word
wh0 of right nonterminal Z. This action expands Y
and Z in a straight order. The leftmost word of
phrase X is set to leftmost word wleft1 of Y, and the
rightmost word of phrase X is set to rightmost word
wright0 of Z. Variable a is set to a1 of Y.
The difference between reduce-MR-X and
reduce-SR-X actions is new stack element s0. The
reduce-SR-X action generates s0 by combining s?0
and s?1 with binary rule X# ?Y Z:
s0 = {X#, h0, wleft0, wright1, a0}.
This action expands Y and Z in a reverse order, and
the leftmost word of X# is set to wleft0 of Z, and the
rightmost word of X# is set to wright1 of Y. Variable
a is set to a0 of Z.
We use a linear model that is discriminatively
trained with the averaged perceptron (Collins and
Roark, 2004). Table 1 shows the feature templates
used in our experiments and we call the features in
the bottom two rows ?non-local? features.
1384
train dev test9 test10
# of sent. 3,191,228 2,000 2,000 2,300
ave. leng. (J) 36.4 36.6 37.0 43.1
ave. leng. (E) 33.3 33.3 33.7 39.6
Table 2: NTCIR-9 and 10 data statistics.
4 Experiments
4.1 Experimental Setups
We conducted experiments for NTCIR-9 and 10
patent data using a Japanese-English language pair.
Mecab2 was used for the Japanese morphological
analysis. The data are summarized in Table 2.
We used Enju (Miyao and Tsujii, 2008) for pars-
ing the English training data and converted parse
trees into HFE trees by a head-finalization scheme.
We extracted grammar rules from all the HFE trees
and randomly selected 500,000 HFE trees to train
the shift-reduce parser.
We used Moses (Koehn et al, 2007) with lexical-
ized reordering and a 6-gram language model (LM)
trained using SRILM (Stolcke et al, 2011) to trans-
late the Japanese sentences into HFE sentences.
To recover the English sentences, our shift-reduce
parser reordered only the 1-best HFE sentence. Our
strategy is much simpler than Goto et al (2012)?s
because they used a linear inteporation of MT cost,
parser cost and N -gram LM cost to generate the best
English sentence from the n-best HFE sentences.
4.2 Main Results
The main results in Table 3 indicate our method was
significantly better and faster than the conventional
PBMT system. Our method also ourperformed Goto
et al (2012)?s reported systems as well as a tree-
based (moses-chart) system3. Our proposed model
with ?non-local? features (w/ nf.) achieved gains
against that without the features (w/o nf.). Further
feature engineering may improve the accuracy more.
4.3 Analysis
We show N -gram precisions of PBMT (dist=6,
dist=20) and proposed systems in Table 5. The re-
sults clearly show that improvements of 1-gram pre-
2https://code.google.com/p/mecab/
3All the data and the MT toolkits used in our experiments
are the same as theirs.
test9 test10
BLEU RIBES BLEU RIBES
HFE w/ art. 28.86 73.45 29.9 73.52
proposed 32.93 76.68 33.25 76.74
w/o art. 19.86 75.62 20.17 75.63
N -gram 32.15 76.52 32.28 76.46
Table 4: The effects of article generation: ?w/o art.? de-
notes evaluation scores for translations of the best system
(?proposed?) in Table 3 from which articles are removed.
?HFE w/ art.? system used HFE data with articles and
generated them by MT system and the shift-reduce parser
performed only reordering. ?N -gram? system inserted
articles into the translations of ?w/o art.? by Goto et al
(2012)?s article generation method.
(1?4)-gram precision
moses (dist=6) 67.1 / 36.9 / 20.7 / 11.5
moses (dist=20) 67.7 / 38.9 / 23.0 / 13.7
proposed 68.9 / 40.6 / 25.7 / 16.7
Table 5: N -gram precisions of moses (dist=6, dist=20)
and proposed systems for test9 data.
cisions are the main factors that contribute to bet-
ter performance of our proposed system than PBMT
systems. It seems that the gains of 1-gram presicions
come from postediting (article generation).
In table 4, we show the effectiveness of our joint
reordering and postediting approach (?proposed?).
The ?w/o art.? results clearly show that generating
articles has great effects on MT evaluations espe-
cially for BLEU metric. Comparing ?proposed? and
?HFEw/ art.? systems, these results show that poste-
diting is much more effective than generating arti-
cles by MT. Our joint approach also outperformed
?N -gram? postediting system.
5 Conclusion
We proposed a shift-reduce word ordering model
and applied it to J-to-E postordering. Our experi-
mental results indicate our method can significantly
improve the performance of a PBMT system.
Future work will investigate our method?s use-
fulness on various language datasets. We plan to
study more general methods that use word align-
ments to embed swap information in trees (Galley
et al, 2006).
1385
test9 test10
BLEU RIBES time (sec.) BLEU RIBES time (sec.)
PBMT (dist=6) 27.1 67.76 2.66 27.92 68.13 3.18
PBMT (dist=12) 29.55 69.84 4.15 30.03 69.88 4.93
PBMT (dist=20) 29.98 69.87 6.22 30.15 69.43 7.19
Tree-based MT** (Goto et al, 2012) 29.53 69.22 ? ? ? ?
PBMT (dist=20)** (Goto et al, 2012) 30.13 68.86 ? ? ? ?
Goto et al (2012)** 31.75 72.57 ? ? ? ?
PBMT (dist=0) + proposed w/o nf. (beam=12) 32.59 76.35 1.46 + 0.01 32.83 76.44 1.7 + 0.01
PBMT (dist=0) + proposed w/o nf. (beam=48) 32.61 76.58 1.46 + 0.06 32.86 76.6 1.7 + 0.06
PBMT (dist=0) + proposed w/ nf. (beam=12) 32.91 76.38 1.46 + 0.01 33.15 76.53 1.7 + 0.02
PBMT (dist=0) + proposed w/ nf. (beam=48) 32.93 76.68 1.46 + 0.07 33.25 76.74 1.7 + 0.07
Table 3: System comparison: time represents the average second per sentence. ** denotes ?not our experiments?.
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 263?270.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, page 111.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 531?540.
John DeNero and Jakob Uszkoreit. 2011. Inducing sen-
tence structure from parallel corpora for reordering. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 193?203.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics, pages
961?968.
Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2012.
Post-ordering by parsing for japanese-english statisti-
cal machine translation. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics, pages 311?316.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proc. HLT-NAACL, pages 105?112.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1077?1086.
Hideki Isozaki, Jun Suzuki, Hajime Tsukada, Masaaki
Nagata, Sho Hoshino, and Yusuke Miyao. 2012.
HPSG-based preprocessing for English-to-Japanese
translation. ACM Transactions on Asian Language In-
formation Processing (TALIP), 11(3).
Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In Proceedings of the
National Conference on Artificial Intelligence, pages
779?779.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, et al 2007. Moses: Open source toolkit for sta-
tistical machine translation. In Proceedings of the 45th
Annual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 177?180.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest
models for probabilistic hpsg parsing. Computational
Linguistics, 34(1):35?80.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human language tech-
nologies 2007: the conference of the North American
chapter of the Association for Computational Linguis-
tics, pages 404?411.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. Srilm at sixteen: Update and outlook.
In Proceedings of IEEE Automatic Speech Recognition
and Understanding Workshop.
Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Post-ordering in
statistical machine translation. In Proc. MT Summit.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
1386
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 657?665,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Head-driven Transition-based Parsing with Top-down Prediction
Katsuhiko Hayashi?, Taro Watanabe?, Masayuki Asahara?, Yuji Matsumoto?
?Nara Institute of Science and Technology
Ikoma, Nara, 630-0192, Japan
?National Institute of Information and Communications Technology
Sorakugun, Kyoto, 619-0289, Japan
?National Institute for Japanese Language and Linguistics
Tachikawa, Tokyo, 190-8561, Japan
katsuhiko-h@is.naist.jp, taro.watanabe@nict.go.jp
masayu-a@ninjal.ac.jp, matsu@is.naist.jp
Abstract
This paper presents a novel top-down head-
driven parsing algorithm for data-driven pro-
jective dependency analysis. This algorithm
handles global structures, such as clause and
coordination, better than shift-reduce or other
bottom-up algorithms. Experiments on the
English Penn Treebank data and the Chinese
CoNLL-06 data show that the proposed algo-
rithm achieves comparable results with other
data-driven dependency parsing algorithms.
1 Introduction
Transition-based parsing algorithms, such as shift-
reduce algorithms (Nivre, 2004; Zhang and Clark,
2008), are widely used for dependency analysis be-
cause of the efficiency and comparatively good per-
formance. However, these parsers have one major
problem that they can handle only local information.
Isozaki et al (2004) pointed out that the drawbacks
of shift-reduce parser could be resolved by incorpo-
rating top-down information such as root finding.
This work presents an O(n2) top-down head-
driven transition-based parsing algorithm which can
parse complex structures that are not trivial for shift-
reduce parsers. The deductive system is very similar
to Earley parsing (Earley, 1970). The Earley predic-
tion is tied to a particular grammar rule, but the pro-
posed algorithm is data-driven, following the current
trends of dependency parsing (Nivre, 2006; McDon-
ald and Pereira, 2006; Koo et al, 2010). To do the
prediction without any grammar rules, we introduce
a weighted prediction that is to predict lower nodes
from higher nodes with a statistical model.
To improve parsing flexibility in deterministic
parsing, our top-down parser uses beam search al-
gorithm with dynamic programming (Huang and
Sagae, 2010). The complexity becomes O(n2 ? b)
where b is the beam size. To reduce prediction er-
rors, we propose a lookahead technique based on a
FIRST function, inspired by the LL(1) parser (Aho
and Ullman, 1972). Experimental results show that
the proposed top-down parser achieves competitive
results with other data-driven parsing algorithms.
2 Definition of Dependency Graph
A dependency graph is defined as follows.
Definition 2.1 (Dependency Graph) Given an in-
put sentence W = n0 . . .nn where n0 is a spe-
cial root node $, a directed graph is defined as
GW = (VW , AW ) where VW = {0, 1, . . . , n} is a
set of (indices of) nodes and AW ? VW ? VW is a
set of directed arcs. The set of arcs is a set of pairs
(x, y) where x is a head and y is a dependent of x.
x ?? l denotes a path from x to l. A directed graph
GW = (VW , AW ) is well-formed if and only if:
? There is no node x such that (x, 0) ? AW .
? If (x, y) ? AW then there is no node x? such
that (x?, y) ? AW and x? ?= x.
? There is no subset of arcs {(x0, x1), (x1, x2),
. . . , (xl?1, xl)} ? AW such that x0 = xl.
These conditions are refered to ROOT, SINGLE-
HEAD, and ACYCLICITY, and we call an well-
formed directed graph as a dependency graph.
Definition 2.2 (PROJECTIVITY) A dependency
graph GW = (VW , AW ) is projective if and only if,
657
input: W = n0 . . .nn
axiom(p0): 0 : ?1, 0, n + 1,n0? : ?
predx:
state p
? ?? ?
? : ?i, h, j, sd|...|s0? :
? + 1 : ?i, k, h, sd?1|...|s0|nk? : {p}
?k : i ? k < h
predy:
state p
? ?? ?
? : ?i, h, j, sd|...|s0? :
? + 1 : ?i, k, j, sd?1|...|s0|nk? : {p}
?k : i ? k < j ? h < i
scan:
? : ?i, h, j, sd|...|s0? : pi
? + 1 : ?i + 1, h, j, sd|...|s0? : pi
i = h
comp:
state q
? ?? ?
: ? , h?, j?, s?d|...|s?0? : pi?
state p
? ?? ?
? : ?i, h, j, sd|...|s0? : pi
? + 1 : ?i, h?, j?, s?d|...|s?1|s?0ys0? : pi?
q ? pi, h < i
goal: 3n : ?n + 1, 0, n + 1, s0? : ?
Figure 1: The non-weighted deductive system of top-down dependency parsing algorithm: means ?take anything?.
for every arc (x, y) ? AW and node l in x < l < y
or y < l < x, there is a path x ?? l or y ?? l.
The proposed algorithm in this paper is for projec-
tive dependency graphs. If a projective dependency
graph is connected, we call it a dependency tree,
and if not, a dependency forest.
3 Top-down Parsing Algorithm
Our proposed algorithm is a transition-based algo-
rithm, which uses stack and queue data structures.
This algorithm formally uses the following state:
? : ?i, h, j, S? : pi
where ? is a step size, S is a stack of trees sd|...|s0
where s0 is a top tree and d is a window size for
feature extraction, i is an index of node on the top
of the input node queue, h is an index of root node
of s0, j is an index to indicate the right limit (j ?
1 inclusive) of predy, and pi is a set of pointers to
predictor states, which are states just before putting
the node in h onto stack S. In the deterministic case,
pi is a singleton set except for the initial state.
This algorithm has four actions, predictx(predx),
predicty(predy), scan and complete(comp). The
deductive system of the top-down algorithm is
shown in Figure 1. The initial state p0 is a state ini-
tialized by an artificial root node n0. This algorithm
applies one action to each state selected from appli-
cable actions in each step. Each of three kinds of
actions, pred, scan, and comp, occurs n times, and
this system takes 3n steps for a complete analysis.
Action predx puts nk onto stack S selected from
the input queue in the range, i ? k < h, which is
to the left of the root nh in the stack top. Similarly,
action predy puts a node nk onto stack S selected
from the input queue in the range, h < i ? k < j,
which is to the right of the root nh in the stack top.
The node ni on the top of the queue is scanned if it
is equal to the root node nh in the stack top. Action
comp creates a directed arc (h?, h) from the root h?
of s?0 on a predictor state q to the root h of s0 on a
current state p if h < i 1.
The precondition i < h of action predx means
that the input nodes in i ? k < h have not been
predicted yet. Predx, scan and predy do not con-
flict with each other since their preconditions i < h,
i = h and h < i do not hold at the same time.
However, this algorithm faces a predy-comp con-
flict because both actions share the same precondi-
tion h < i, which means that the input nodes in
1 ? k ? h have been predicted and scanned. This
1In a single root tree, the special root symbol $0 has exactly
one child node. Therefore, we do not apply comp action to a
state if its condition satisfies s1.h = n0 ? ? ?= 3n? 1.
658
step state stack queue action state information
0 p0 $0 I1 saw2 a3 girl4 ? ?1, 0, 5? : ?
1 p1 $0|saw2 I1 saw2 a3 girl4 predy ?1, 2, 5? : {p0}
2 p2 saw2|I1 I1 saw2 a3 girl4 predx ?1, 1, 2? : {p1}
3 p3 saw2|I1 saw2 a3 girl4 scan ?2, 1, 2? : {p1}
4 p4 $0|I1xsaw2 saw2 a3 girl4 comp ?2, 2, 5? : {p0}
5 p5 $0|I1xsaw2 a3 girl4 scan ?3, 2, 5? : {p0}
6 p6 I1xsaw2|girl4 a3 girl4 predy ?3, 4, 5? : {p5}
7 p7 girl4|a3 a3 girl4 predx ?3, 3, 4? : {p6}
8 p8 girl4|a3 girl4 scan ?4, 3, 4? : {p6}
9 p9 I1xsaw2|a3xgirl4 girl4 comp ?4, 4, 5? : {p5}
10 p10 I1xsaw2|a3xgirl4 scan ?5, 4, 5? : {p5}
11 p11 $0|I1xsaw2ygirl4 comp ?5, 2, 5? : {p0}
12 p12 $0ysaw2 comp ?5, 0, 5? : ?
Figure 2: Stages of the top-down deterministic parsing process for a sentence ?I saw a girl?. We follow a convention
and write the stack with its topmost element to the right, and the queue with its first element to the left. In this example,
we set the window size d to 1, and write the descendants of trees on stack elements s0 and s1 within depth 1.
parser constructs left and right children of a head
node in a left-to-right direction by scanning the head
node prior to its right children. Figure 2 shows an
example for parsing a sentence ?I saw a girl?.
4 Correctness
To prove the correctness of the system in Figure
1 for the projective dependency graph, we use the
proof strategy of (Nivre, 2008a). The correct deduc-
tive system is both sound and complete.
Theorem 4.1 The deductive system in Figure 1 is
correct for the class of dependency forest.
Proof 4.1 To show soundness, we show that Gp0 =
(VW , ?), which is a directed graph defined by the
axiom, is well-formed and projective, and that every
transition preserves this property.
? ROOT: The node 0 is a root in Gp0 , and the
node 0 is on the top of stack of p0. The two pred
actions put a word onto the top of stack, and
predict an arc from root or its descendant to
the child. The comp actions add the predicted
arcs which include no arc of (x, 0).
? SINGLE-HEAD: Gp0 is single-head. A node
y is no longer in stack and queue after a comp
action creates an arc (x, y). The node y cannot
make any arc (x?, y) after the removal.
? ACYCLICITY: Gp0 is acyclic. A cycle is cre-
ated only if an arc (x, y) is added when there
is a directed path y ?? x. The node x is no
longer in stack and queue when the directed
path y ?? x was made by adding an arc (l, x).
There is no chance to add the arc (x, y) on the
directed path y ?? x.
? PROJECTIVITY: Gp0 is projective. Projec-
tivity is violated by adding an arc (x, y) when
there is a node l in x < l < y or y < l < x
with the path to or from the outside of the span
x and y. When predy creates an arc relation
from x to y, the node y cannot be scanned be-
fore all nodes l in x < l < y are scanned and
completed. When predx creates an arc rela-
tion from x to y, the node y cannot be scanned
before all nodes k in k < y are scanned and
completed, and the node x cannot be scanned
before all nodes l in y < l < x are scanned
and completed. In those processes, the node l
in x < l < y or y < l < x does not make a
path to or from the outside of the span x and y,
and a path x ?? l or y ?? l is created. 2
To show completeness, we show that for any sen-
tence W , and dependency forest GW = (VW , AW ),
there is a transition sequence C0,m such that Gpm =
GW by an inductive method.
? If |W | = 1, the projective dependency graph
for W is GW = ({0}, ?) and Gp0 = GW .
? Assume that the claim holds for sentences with
length less or equal to t, and assume that
|W | = t + 1 and GW = (VW , AW ). The sub-
graph GW ? is defined as (VW ? t, A?t) where
659
..
.s2 h
.
.. . .
. .
.
.
.s1 h
.. . .
.. . . .
..s1.l
.. . . .
.. . .
.. . . .
..s1.r
. .. . .
.
.
.s0 h
.. . .
.. . . .
..s0.l
.. . . .
.. . .
.. . . .
..s0.r
. .. . .
Figure 3: Feature window of trees on stack S: The win-
dow size d is set to 2. Each x.h, x.l and x.r denotes root,
left and right child nodes of a stack element x.
A?t = AW ?{(x, y)|x = t? y = t}. If GW is
a dependency forest, then GW ? is also a depen-
dency forest. It is obvious that there is a transi-
tion sequence for constructing GW except arcs
which have a node t as a head or a dependent2.
There is a state pq = q : ?i, x, t + 1? :
for i and x (0 ? x < i < t + 1). When
x is the head of t, predy to t creates a state
pq+1 = q + 1 : ?i, t, t + 1? : {pq}. At least one
node y in i ? y < t becomes the dependent of
t by predx and there is a transition sequence
for constructing a tree rooted by y. After con-
structing a subtree rooted by t and spaned from
i to t, t is scaned, and then comp creates an
arc from x to t. It is obvious that the remaining
transition sequence exists. Therefore, we can
construct a transition sequence C0,m such that
Gpm = GW . 2
The deductive sysmtem in Figure 1 is both sound and
complete. Therefore, it is correct. 2
5 Weighted Parsing Model
5.1 Stack-based Model
The proposed algorithm employs a stack-based
model for scoring hypothesis. The cost of the model
is defined as follows:
cs(i, h, j, S) = ?s ? fs,act(i, h, j, S) (1)
where ?s is a weight vector, fs is a feature function,
and act is one of the applicable actions to a state ? :
?i, h, j, S? : pi. We use a set of feature templates of
(Huang and Sagae, 2010) for the model. As shown
in Figure 3, left children s0.l and s1.l of trees on
2This transition sequence is defined for GW ? , but it is pos-
sible to be regarded as the definition for GW as long as the
transition sequence is indifferent from the node t.
Algorithm 1 Top-down Parsing with Beam Search
1: input W = n0, . . . ,nn
2: start? ?1, 0, n + 1,n0?
3: buf [0]? {start}
4: for ?? 1 . . . 3n do
5: hypo? {}
6: for each state in buf [?? 1] do
7: for act?applicableAct(state) do
8: newstates?actor(act, state)
9: addAll newstates to hypo
10: add top b states to buf [?] from hypo
11: return best candidate from buf [3n]
stack for extracting features are different from those
of Huang and Sagae (2010) because in our parser the
left children are generated from left to right.
As mentioned in Section 1, we apply beam search
and Huang and Sagae (2010)?s DP techniques to
our top-down parser. Algorithm 1 shows the our
beam search algorithm in which top most b states
are preserved in a buffer buf [?] in each step. In
line 10 of Algorithm 1, equivalent states in the step
? are merged following the idea of DP. Two states
?i, h, j, S? and ?i?, h?, j?, S?? in the step ? are equiv-
alent, notated ?i, h, j, S? ? ?i?, h?, j?, S??, iff
fs,act(i, h, j, S) = fs,act(i?, h?, j?, S?). (2)
When two equivalent predicted states are merged,
their predictor states in pi get combined. For fur-
ther details about this technique, readers may refer
to (Huang and Sagae, 2010).
5.2 Weighted Prediction
The step 0 in Figure 2 shows an example of predic-
tion for a head node ?$0?, where the node ?saw2? is
selected as its child node. To select a probable child
node, we define a statistical model for the prediction.
In this paper, we integrate the cost from a graph-
based model (McDonald and Pereira, 2006) which
directly models dependency links. The cost of the
1st-order model is defined as the relation between a
child node c and a head node h:
cp(h, c) = ?p ? fp(h, c) (3)
where ?p is a weight vector and fp is a features func-
tion. Using the cost cp, the top-down parser selects
a probable child node in each prediction step.
When we apply beam search to the top-down
parser, then we no longer use ? but ? on predx and
660
..
.h
..l1 . .. . . . ..ll . ..r1 . .. . . . ..rm
Figure 4: An example of tree structure: Each h, l and r
denotes head, left and right child nodes.
predy in Figure 1. Therefore, the parser may predict
many nodes as an appropriate child from a single
state, causing many predicted states. This may cause
the beam buffer to be filled only with the states, and
these may exclude other states, such as scanned or
completed states. Thus, we limit the number of pre-
dicted states from a single state by prediction size
implicitly in line 10 of Algorithm 1.
To improve the prediction accuracy, we introduce
a more sophisticated model. The cost of the sibling
2nd-order model is defined as the relationship be-
tween c, h and a sibling node sib:
cp(h, sib, c) = ?p ? fp(h, sib, c). (4)
The 1st- and sibling 2nd-order models are the same
as McDonald and Pereira (2006)?s definitions, ex-
cept the cost factors of the sibling 2nd-order model.
The cost factors for a tree structure in Figure 4 are
defined as follows:
cp(h,?, l1) +
l?1
?
y=1
cp(h, ly, ly+1)
+cp(h,?, r1) +
m?1
?
y=1
cp(h, ry, ry+1).
This is different from McDonald and Pereira (2006)
in that the cost factors for left children are calcu-
lated from left to right, while those in McDonald and
Pereira (2006)?s definition are calculated from right
to left. This is because our top-down parser gener-
ates left children from left to right. Note that the
cost of weighted prediction model in this section is
incrementally calculated by using only the informa-
tion on the current state, thus the condition of state
merge in Equation 2 remains unchanged.
5.3 Weighted Deductive System
We extend deductive system to a weighted one, and
introduce forward cost and inside cost (Stolcke,
1995; Huang and Sagae, 2010). The forward cost is
the total cost of a sequence from an initial state to the
end state. The inside cost is the cost of a top tree s0
in stack S. We define these costs using a combina-
tion of stack-based model and weighted prediction
model. The forward and inside costs of the combi-
nation model are as follows:
{
cfw = cfws + cfwp
cin = cins + cinp
(5)
where cfws and cins are a forward cost and an inside
cost for stack-based model, and cfwp and cinp are a for-
ward cost and an inside cost for weighted prediction
model. We add the following tuple of costs to a state:
(cfws , cins , cfwp , cinp ).
For each action, we define how to efficiently cal-
culate the forward and inside costs3 , following Stol-
cke (1995) and Huang and Sagae (2010)?s works. In
either case of predx or predy,
(cfws , , cfwp , )
(cfws + ?, 0, cfwp + cp(s0.h,nk), 0)
where
? =
{
?s ? fs,predx(i, h, j, S) if predx
?s ? fs,predy(i, h, j, S) if predy
(6)
In the case of scan,
(cfws , cins , cfwp , cinp )
(cfws + ?, cins + ?, cfwp , cinp )
where
? = ?s ? fs,scan(i, h, j, S). (7)
In the case of comp,
(c?fws , c?
in
s , c?
fw
p , c?
in
p ) (cfws , cins , cfwp , cinp )
(c?fws + cins + ?, c?ins + cins + ?,
c?fwp + cinp + cp(s?0.h, s0.h),
c?inp + cinp + cp(s?0.h, s0.h))
where
? = ?s ? fs,comp(i, h, j, S) + ?s ? fs,pred ( , h?, j?, S?).
(8)
3For brevity, we present the formula not by 2nd-order model
as equation 4 but a 1st-order one for weighted prediction.
661
Pred takes either predx or predy. Beam search is
performed based on the following linear order for
the two states p and p? at the same step, which have
(cfw, cin) and (c?fw, c?in) respectively:
p ? p? iff cfw < c?fw or cfw = c?fw ? cin < c?in. (9)
We prioritize the forward cost over the inside cost
since forward cost pertains to longer action sequence
and is better suited to evaluate hypothesis states than
inside cost (Nederhof, 2003).
5.4 FIRST Function for Lookahead
Top-down backtrack parser usually reduces back-
tracking by precomputing the set FIRST(?) (Aho and
Ullman, 1972). We define the set FIRST(?) for our
top-down dependency parser:
FIRST(t?) = {ld.t|ld ? lmdescendant(Tree, t?)
Tree ? Corpus} (10)
where t? is a POS-tag, Tree is a correct depen-
dency tree which exists in Corpus, a function
lmdescendant(Tree, t?) returns the set of the leftmost
descendant node ld of each nodes in Tree whose
POS-tag is t?, and ld.t denotes a POS-tag of ld.
Though our parser does not backtrack, it looks ahead
when selecting possible child nodes at the prediction
step by using the function FIRST. In case of predx:
?k : i ? k < h ? ni.t ? FIRST(nk.t)
state p
? ?? ?
? : ?i, h, j, sd|...|s0? :
? + 1 : ?i, k, h, sd?1|...|s0|nk? : {p}
where ni.t is a POS-tag of the node ni on the top of
the queue, and nk.t is a POS-tag in kth position of
an input nodes. The case for predy is the same. If
there are no nodes which satisfy the condition, our
top-down parser creates new states for all nodes, and
pushes them into hypo in line 9 of Algorithm 1.
6 Time Complexity
Our proposed top-down algorithm has three kinds
of actions which are scan, comp and predict. Each
scan and comp actions occurs n times when parsing
a sentence with the length n. Predict action also oc-
curs n times in which a child node is selected from
a node sequence in the input queue. Thus, the algo-
rithm takes the following times for prediction:
n + (n? 1) + ? ? ? + 1 =
n
?
i
i = n(n + 1)2 . (11)
As n2 for prediction is the most dominant factor, the
time complexity of the algorithm is O(n2) and that
of the algorithm with beam search is O(n2 ? b).
7 Related Work
Alshawi (1996) proposed head automaton which
recognizes an input sentence top-down. Eisner
and Satta (1999) showed that there is a cubic-time
parsing algorithm on the formalism of the head
automaton grammars, which are equivalently con-
verted into split-head bilexical context-free gram-
mars (SBCFGs) (McAllester, 1999; Johnson, 2007).
Although our proposed algorithm does not employ
the formalism of SBCFGs, it creates left children
before right children, implying that it does not have
spurious ambiguities as well as parsing algorithms
on the SBCFGs. Head-corner parsing algorithm
(Kay, 1989) creates dependency tree top-down, and
in this our algorithm has similar spirit to it.
Yamada and Matsumoto (2003) applied a shift-
reduce algorithm to dependency analysis, which is
known as arc-standard transition-based algorithm
(Nivre, 2004). Nivre (2003) proposed another
transition-based algorithm, known as arc-eager al-
gorithm. The arc-eager algorithm processes right-
dependent top-down, but this does not involve the
prediction of lower nodes from higher nodes. There-
fore, the arc-eager algorithm is a totally bottom-up
algorithm. Zhang and Clark (2008) proposed a com-
bination approach of the transition-based algorithm
with graph-based algorithm (McDonald and Pereira,
2006), which is the same as our combination model
of stack-based and prediction models.
8 Experiments
Experiments were performed on the English Penn
Treebank data and the Chinese CoNLL-06 data. For
the English data, we split WSJ part of it into sections
02-21 for training, section 22 for development and
section 23 for testing. We used Yamada and Mat-
sumoto (2003)?s head rules to convert phrase struc-
ture to dependency structure. For the Chinese data,
662
time accuracy complete root
McDonald05,06 (2nd) 0.15 90.9, 91.5 37.5, 42.1 ?
Koo10 (Koo and Collins, 2010) ? 93.04 ? ?
Hayashi11 (Hayashi et al, 2011) 0.3 92.89 ? ?
2nd-MST? 0.13 92.3 43.7 96.0
Goldberg10 (Goldberg and Elhadad, 2010) ? 89.7 37.5 91.5
Kitagawa10 (Kitagawa and Tanaka-Ishii, 2010) ? 91.3 41.7 ?
Zhang08 (Sh beam 64) ? 91.4 41.8 ?
Zhang08 (Sh+Graph beam 64) ? 92.1 45.4 ?
Huang10 (beam+DP) 0.04 92.1 ? ?
Huang10? (beam 8, 16, 32+DP) 0.03, 0.06, 0.10 92.3, 92.27, 92.26 43.5, 43.7, 43.8 96.0, 96.0, 96.1
Zhang11 (beam 64) (Zhang and Nivre, 2011) ? 93.07 49.59 ?
top-down? (beam 8, 16, 32+pred 5+DP) 0.07, 0.12, 0.22 91.7, 92.3, 92.5 45.0, 45.7, 45.9 94.5, 95.7, 96.2
top-down? (beam 8, 16, 32+pred 5+DP+FIRST) 0.07, 0.12, 0.22 91.9, 92.4, 92.6 45.0, 45.3, 45.5 95.1, 96.2, 96.6
Table 1: Results for test data: Time measures the parsing time per sentence in seconds. Accuracy is an unlabeled
attachment score, complete is a sentence complete rate, and root is a correct root rate. ? indicates our experiments.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  10  20  30  40  50  60  70
pa
rs
in
g 
tim
e 
(cp
u s
ec
)
length of input sentence
"shift-reduce"
"2nd-mst"
"top-down"
Figure 5: Scatter plot of parsing time against sentence
length, comparing with top-down, 2nd-MST and shift-
reduce parsers (beam size: 8, pred size: 5)
we used the information of words and fine-grained
POS-tags for features. We also implemented and ex-
perimented Huang and Sagae (2010)?s arc-standard
shift-reduce parser. For the 2nd-order Eisner-Satta
algorithm, we used MSTParser (McDonald, 2012).
We used an early update version of averaged per-
ceptron algorithm (Collins and Roark, 2004) for
training of shift-reduce and top-down parsers. A
set of feature templates in (Huang and Sagae, 2010)
were used for the stack-based model, and a set of
feature templates in (McDonald and Pereira, 2006)
were used for the 2nd-order prediction model. The
weighted prediction and stack-based models of top-
down parser were jointly trained.
8.1 Results for English Data
During training, we fixed the prediction size and
beam size to 5 and 16, respectively, judged by pre-
accuracy complete root
oracle (sh+mst) 94.3 52.3 97.7
oracle (top+sh) 94.2 51.7 97.6
oracle (top+mst) 93.8 50.7 97.1
oracle (top+sh+mst) 94.9 55.3 98.1
Table 2: Oracle score, choosing the highest accuracy
parse for each sentence on test data from results of top-
down (beam 8, pred 5) and shift-reduce (beam 8) and
MST(2nd) parsers in Table 1.
accuracy complete root
top-down (beam:8, pred:5) 90.9 80.4 93.0
shift-reduce (beam:8) 90.8 77.6 93.5
2nd-MST 91.4 79.3 94.2
oracle (sh+mst) 94.0 85.1 95.9
oracle (top+sh) 93.8 84.0 95.6
oracle (top+mst) 93.6 84.2 95.3
oracle (top+sh+mst) 94.7 86.5 96.3
Table 3: Results for Chinese Data (CoNLL-06)
liminary experiments on development data. After
25 iterations of perceptron training, we achieved
92.94 unlabeled accuracy for top-down parser with
the FIRST function and 93.01 unlabeled accuracy
for shift-reduce parser on development data by set-
ting the beam size to 8 for both parsers and the pre-
diction size to 5 in top-down parser. These trained
models were used for the following testing.
We compared top-down parsing algorithm with
other data-driven parsing algorithms in Table 1.
Top-down parser achieved comparable unlabeled ac-
curacy with others, and outperformed them on the
sentence complete rate. On the other hand, top-
down parser was less accurate than shift-reduce
663
No.717 Little Lily , as Ms. Cunningham calls7 herself in the book , really was14 n?t ordinary .
shift-reduce 2 7 2 2 6 4 14 7 7 11 9 7 14 0 14 14 14
2nd-MST 2 14 2 2 6 7 4 7 7 11 9 2 14 0 14 14 14
top-down 2 14 2 2 6 7 4 7 7 11 9 2 14 0 14 14 14
correct 2 14 2 2 6 7 4 7 7 11 9 2 14 0 14 14 14
No.127 resin , used to make garbage bags , milk jugs , housewares , toys and meat packaging25 , among other items .
shift-reduce 25 9 9 13 11 15 13 25 18 25 25 25 25 25 25 25 7 25 25 29 27 4
2nd-MST 29 9 9 13 11 15 13 29 18 29 29 29 29 25 25 25 29 25 25 29 7 4
top-down 7 9 9 13 11 15 25 25 18 25 25 25 25 25 25 25 13 25 25 29 27 4
correct 7 9 9 13 11 15 25 25 18 25 25 25 25 25 25 25 13 25 25 29 27 4
Table 4: Two examples on which top-down parser is superior to two bottom-up parsers: In correct analysis, the boxed
portion is the head of the underlined portion. Bottom-up parsers often mistake to capture the relation.
parser on the correct root measure. In step 0, top-
down parser predicts a child node, a root node of
a complete tree, using little syntactic information,
which may lead to errors in the root node selection.
Therefore, we think that it is important to seek more
suitable features for the prediction in future work.
Figure 5 presents the parsing time against sen-
tence length. Our proposed top-down parser is the-
oretically slower than shift-reduce parser and Fig-
ure 5 empirically indicates the trends. The domi-
nant factor comes from the score calculation, and
we will leave it for future work. Table 2 shows
the oracle score for test data, which is the score
of the highest accuracy parse selected for each sen-
tence from results of several parsers. This indicates
that the parses produced by each parser are differ-
ent from each other. However, the gains obtained by
the combination of top-down and 2nd-MST parsers
are smaller than other combinations. This is because
top-down parser uses the same features as 2nd-MST
parser, and these are more effective than those of
stack-based model. It is worth noting that as shown
in Figure 5, our O(n2?b) (b = 8) top-down parser is
much faster than O(n3) Eisner-Satta CKY parsing.
8.2 Results for Chinese Data (CoNLL-06)
We also experimented on the Chinese data. Fol-
lowing English experiments, shift-reduce parser was
trained by setting beam size to 16, and top-down
parser was trained with the beam size and the predic-
tion size to 16 and 5, respectively. Table 3 shows the
results on the Chinese test data when setting beam
size to 8 for both parsers and prediction size to 5 in
top-down parser. The trends of the results are almost
the same as those of the English results.
8.3 Analysis of Results
Table 4 shows two interesting results, on which top-
down parser is superior to either shift-reduce parser
or 2nd-MST parser. The sentence No.717 contains
an adverbial clause structure between the subject
and the main verb. Top-down parser is able to han-
dle the long-distance dependency while shift-reudce
parser cannot correctly analyze it. The effectiveness
on the clause structures implies that our head-driven
parser may handle non-projective structures well,
which are introduced by Johansonn?s head rule (Jo-
hansson and Nugues, 2007). The sentence No.127
contains a coordination structure, which it is diffi-
cult for bottom-up parsers to handle, but, top-down
parser handles it well because its top-down predic-
tion globally captures the coordination.
9 Conclusion
This paper presents a novel head-driven parsing al-
gorithm and empirically shows that it is as practi-
cal as other dependency parsing algorithms. Our
head-driven parser has potential for handling non-
projective structures better than other non-projective
dependency algorithms (McDonald et al, 2005; At-
tardi, 2006; Nivre, 2008b; Koo et al, 2010). We are
in the process of extending our head-driven parser
for non-projective structures as our future work.
Acknowledgments
We would like to thank Kevin Duh for his helpful
comments and to the anonymous reviewers for giv-
ing valuable comments.
664
References
A. V. Aho and J. D. Ullman. 1972. The Theory of Pars-
ing, Translation and Compiling, volume 1: Parsing.
Prentice-Hall.
H. Alshawi. 1996. Head automata for speech translation.
In Proc. the ICSLP.
G. Attardi. 2006. Experiments with a multilanguage
non-projective dependency parser. In Proc. the 10th
CoNLL, pages 166?170.
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proc. the 42nd ACL.
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the Association for Com-
puting Machinery, 13(2):94?102.
J. M. Eisner and G. Satta. 1999. Efficient parsing for
bilexical context-free grammars and head automaton
grammars. In Proc. the 37th ACL, pages 457?464.
Y. Goldberg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In Proc. the HLT-NAACL, pages 742?750.
K. Hayashi, T. Watanabe, M. Asahara, and Y. Mat-
sumoto. 2011. The third-order variational rerank-
ing on packed-shared dependency forests. In Proc.
EMNLP, pages 1479?1488.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proc. the 48th
ACL, pages 1077?1086.
H. Isozaki, H. Kazawa, and T. Hirao. 2004. A determin-
istic word dependency analyzer enhanced with prefer-
ence learning. In Proc. the 21st COLING, pages 275?
281.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
Proc. NODALIDA.
M. Johnson. 2007. Transforming projective bilexical
dependency grammars into efficiently-parsable CFGs
with unfold-fold. In Proc. the 45th ACL, pages 168?
175.
M. Kay. 1989. Head driven parsing. In Proc. the IWPT.
K. Kitagawa and K. Tanaka-Ishii. 2010. Tree-based de-
terministic dependency parsing ? an application to
nivre?s method ?. In Proc. the 48th ACL 2010 Short
Papers, pages 189?193, July.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. the 48th ACL, pages 1?11.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proc. EMNLP, pages
1288?1298.
D. McAllester. 1999. A reformulation of eisner and
satta?s cubic time parser for split head automata gram-
mars. http://ttic.uchicago.edu/ dmcallester/.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
EACL, pages 81?88.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. HLT-EMNLP, pages 523?
530.
R. McDonald. 2012. Minimum spanning tree parser.
http://www.seas.upenn.edu/ strctlrn/MSTParser.
M.-J. Nederhof. 2003. Weighted deductive parsing
and knuth?s algorithm. Computational Linguistics,
29:135?143.
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proc. the IWPT, pages 149?160.
J. Nivre. 2004. Incrementality in deterministic depen-
dency parsing. In Proc. the ACL Workshop Incremen-
tal Parsing: Bringing Engineering and Cognition To-
gether, pages 50?57.
J. Nivre. 2006. Inductive Dependency Parsing. Springer.
J. Nivre. 2008a. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34:513?553.
J. Nivre. 2008b. Sorting out dependency parsing. In
Proc. the CoTAL, pages 16?27.
A. Stolcke. 1995. An efficient probabilistic context-free
parsing algorithm that computes prefix probabilities.
Computational Linguistics, 21(2):165?201.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
the IWPT, pages 195?206.
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
vestigating and combining graph-based and transition-
based dependency parsing using beam-search. In
Proc. EMNLP, pages 562?571.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proc.
the 49th ACL, pages 188?193.
665
Transactions of the Association for Computational Linguistics, 1 (2013) 139?150. Action Editor: Joakim Nivre.
Submitted 12/2012; Revised 3/2013; Published 5/2013. c?2013 Association for Computational Linguistics.
Efficient Stacked Dependency Parsing by Forest Reranking
Katsuhiko Hayashi and Shuhei Kondo and Yuji Matsumoto
Graduate School of Information Science Nara Institute of Science and Technology
8916-5, Takayama, Ikoma, Nara 630-0192, Japan
{katsuhiko-h,shuhei-k,matsu}@is.naist.jp
Abstract
This paper proposes a discriminative for-
est reranking algorithm for dependency pars-
ing that can be seen as a form of efficient
stacked parsing. A dynamic programming
shift-reduce parser produces a packed deriva-
tion forest which is then scored by a discrim-
inative reranker, using the 1-best tree output
by the shift-reduce parser as guide features in
addition to third-order graph-based features.
To improve efficiency and accuracy, this pa-
per also proposes a novel shift-reduce parser
that eliminates the spurious ambiguity of arc-
standard transition systems. Testing on the
English Penn Treebank data, forest reranking
gave a state-of-the-art unlabeled dependency
accuracy of 93.12.
1 Introduction
There are two main approaches of data-driven de-
pendency parsing ? one is graph-based and the other
is transition-based.
In the graph-based approach, global optimiza-
tion algorithms find the highest-scoring tree with
locally factored models (McDonald et al, 2005).
While third-order graph-based models achieve state-
of-the-art accuracy, it has O(n4) time complexity
for a sentence of length n. Recently, some prun-
ing techniques have been proposed to improve the
efficiency of third-order models (Rush and Petrov,
2012; Zhang and McDonald, 2012).
The transition-based approach usually employs
the shift-reduce parsing algorithm with linear-time
complexity (Nivre, 2008). It greedily chooses the
transition with the highest score and the result-
ing transition sequence is not always globally op-
timal. The beam search algorithm improves pars-
ing flexibility in deterministic parsing (Zhang and
Clark, 2008; Zhang and Nivre, 2011), and dy-
namic programming makes beam search more effi-
cient (Huang and Sagae, 2010).
There is also an alternative approach that in-
tegrates graph-based and transition-based models
(Sagae and Lavie, 2006; Zhang and Clark, 2008;
Nivre and McDonald, 2008; Martins et al, 2008).
Martins et al (2008) formulated their approach as
stacking of parsers where the output of the first-stage
parser is provided to the second as guide features. In
particular, they used a transition-based parser for the
first stage and a graph-based parser for the second
stage. The main drawback of this approach is that
the efficiency of the transition-based parser is sacri-
ficed because the second-stage employs full parsing.
This paper proposes an efficient stacked pars-
ing method through discriminative reranking with
higher-order graph-based features, which works on
the forests output by the first-stage dynamic pro-
gramming shift-reduce parser and integrates non-
local features efficiently with cube-pruning (Huang
and Chiang, 2007). The advantages of our method
are as follows:
? Unlike the conventional stacking approach, the
first-stage shift-reduce parser prunes the search
space of the second-stage graph-based parser.
? In addition to guide features, the second-stage
graph-based parser can employ the scores of
the first-stage parser which cannot be incorpo-
139
axiom(c0) : 0 : (0, 1,w0) : ?
goal(c2n) : 2n : (0, n, s0) : ?
shift :
state p? ?? ?
? : ( , j, sd|sd?1| . . . |s1|s0) :
? + 1 : (j, j + 1, sd?1|sd?2| . . . |s0|wj) : (p) i < n
reduce? :
state p? ?? ?
: (i, j, s?d|s?d?1| . . . |s?1|s?0) : pi?
state q? ?? ?
? : (j, k, sd|sd?1| . . . |s1|s0) : pi
? + 1 : (i, k, s?d|s?d?1| . . . |s?1|s?0?s0) : pi?
s?0.h.w ?= w0 ? p ? pi
reduce? :
state p? ?? ?
: (i, j, s?d|s?d?1| . . . |s?1|s?0) : pi?
state q? ?? ?
? : (j, k, sd|sd?1| . . . |s1|s0) : pi
? + 1 : (i, k, s?d|s?d?1| . . . |s?1|s?0?s0) : pi?
p ? pi
Figure 1: The arc-standard transition-based dependency parsing system with dynamic programming: means ?take
anything?. a?b denotes that a tree b is attached to a tree a.
rated in standard graph-based models.
? In contrast to joint transition-based/graph-
based approaches (Zhang and Clark, 2008;
Bohnet and Kuhn, 2012) which require a large
beam size and make dynamic programming im-
practical, our two-stage approach can integrate
both models with little loss of efficiency.
In addition, the elimination of spurious ambiguity
from the arc-standard shift-reduce parser improves
the efficiency and accuracy of our approach.
2 Arc-Standard Shift-Reduce Parsing
We use a beam search shift-reduce parser with dy-
namic programming as our baseline system. Fig-
ure 1 shows it as a deductive system (Shieber et al,
1995). A state is defined as the following:
? : (i, j, sd|sd?1| . . . |s1|s0) : pi
where ? is the step size, [i, j] is the span of the top-
most stack element s0, and sd|sd?1| . . . |s1 shows
a stack with d elements at the top, where d is the
window size used for defining features. The ax-
iom is initialized with an input sentence of length n,
x = w0 . . .wn where w0 is a special root symbol $0.
The system takes 2n steps for a complete analysis.
pi is a set of pointers to the predictor states, each of
which is the state just before shifting the root word
s0.h.t ? s0.lc.t ? s0.lc2.t s0.h.t ? s0.rc.t ? s0.rc2.t
s1.h.t ? s1.lc.t ? s1.lc2.t s1.h.t ? s1.rc.t ? s1.rc2.t
s0.h.t ? s0.lc.t ? s0.lc2.t ? q0.t
s0.h.t ? s0.rc.t ? s0.rc2.t ? q0.t
s0.h.t ? s1.h.t ? q0.t ? q1.t
s0.h.w ? s1.h.t ? q0.t ? q1.t
Table 1: Additional feature templates for shift-reduce
parsers: q denotes input queue. h, lc and rc are head, left-
most child and rightmost child of a stack element s. lc2
and rc2 denote the second leftmost and rightmost chil-
dren. t and w are a part-of-speech (POS) tag and a word.
of s0 into stack1. Dynamic programming merges
equivalent states in the same step if they have the
same feature values. We add the feature templates
shown in Table 1 to Huang and Sagae (2010)?s fea-
ture templates.
Dynamic programming not only makes the shift-
reduce parser with beam search more efficient but
also produces a packed forest that encodes an expo-
nential number of dependency trees. A packed de-
pendency forest can be represented by a weighted
(directed) hypergraph. A weighted hypergraph is a
pair H = ?V,E?, where V is the set of vertices and
E is the set of hyperedges. Each hyperedge e ? E
is a tuple e = ?T (e), h(e), fe?, where h(e) ? V is
1Huang and Sagae (2010)?s dynamic programming is based
on a notion of a push computation (Kuhlmann et al, 2011). The
details are out of scope here and readers may refer to the paper.
140
X($)saw0, 7
IXher(saw)1, 7
IXher(saw)1, 4
IX(saw)1, 3
X(I)1, 2
I
X(saw)2, 3
saw
X(her)3, 4
her
IXher,with(saw)1, 7
Xwith(her)3, 7
Xman(with)4, 7
X(with)4, 5
with
aX(man)5, 7
X(a)5, 6
a
X(man)6, 7
man
Figure 2: An example of packed dependency (derivation) forest: each vertex has information about the topmost stack
element of the corresponding state to it.
its head vertex, T (e) ? V + is an ordered list of tail
vertices, and fe is a weight for e.
Figure 2 shows an example of a packed forest.
Each binary hyperedge corresponds to a reduce ac-
tion, and each leaf vertex corresponds to a shift ac-
tion. Each vertex also corresponds to a state, and
parse histories on the states can be encoded into the
vertices. In the example, information about the top-
most stack element is attached to the corresponding
vertex marked with a non-terminal symbol X.
Weights are omitted in the example. In practice,
we attach each reduction weight to the correspond-
ing hyperedge, and add the shift weight to the reduc-
tion weight when a shifted word is reduced.
3 Arc-Standard Shift-Reduce Parsing
without Spurious Ambiguity
One solution to remove spurious ambiguity in the
arc-standard transition system is to give priority to
the construction of left arcs over that of right arcs
(or vice versa) like Eisner (1997). For example, an
Earley dependency parser (Hayashi et al, 2012) at-
taches all left dependents to a word before right de-
pendents. The parser uses a scan action to stop the
construction of left arcs.
We apply this idea to the arc-standard transition
system and show the resulting transition system in
Figure 3. We introduce the ? symbol to indicate that
the root node of the topmost element on the stack
has not been scanned yet. The shift and reduce? ac-
tions can be used only when the root of the topmost
element on the stack has already been scanned, and
all left arcs are always attached to the head before
the head is scanned.
The arc-standard shift-reduce parser without spu-
rious ambiguity takes 3n steps to finish parsing, and
the additional n scan actions add surplus vertices
and (unary) hyperedges to a packed forest. How-
ever, it is easy to remove them from the packed for-
est because the consequent state of a scan action has
a unique antecedent state and all the hyperedges go-
ing out from a vertex corresponding to the conse-
quent state can be attached to the vertex correspond-
ing to the antecedent state. The scan weight of the
removed unary hyperedge is added to each weight of
the hyperedges attached to the antecedent.
4 Experiments (Spurious Ambiguity vs.
Non-Spurious Ambiguity)
We conducted experiments on the English Penn
Treebank (PTB) data to compare spurious and non-
spurious shift-reduce parsers. We split the WSJ
part of PTB into sections 02-21 for training, sec-
tion 22 for development, and section 23 for test. We
used the head rules (Yamada and Matsumoto, 2003)
to convert phrase structure to dependency structure.
141
axiom(c0) : 0 : (0, 1,w0) : ?
goal(c3n) : 3n : (0, n, s0) : ?
shift :
state p? ?? ?
? : ( , j, sd|sd?1| . . . |s1|s0) :
? + 1 : (j, j + 1, sd?1|sd?2| . . . |s0|w?j ) : (p)
j < n
scan : ? : (i, j, sd|sd?1| . . . |s1|s
?
0) : pi
? + 1 : (i, j, sd|sd?1| . . . |s1|s0) : pi
reduce? :
state p? ?? ?
: (i, j, s?d|s?d?1| . . . |s?0|s?0) : pi?
state q? ?? ?
? : (j, k, sd|sd?1| . . . |s1|s?0) : pi
? + 1 : (i, k, s?d|s?d?1| . . . |s?1|s?0?s?0) : pi?
s?0.h.w ?= w0 ? p ? pi
reduce? :
state p? ?? ?
: (i, j, s?d|s?d?1| . . . |s?1|s?0) : pi?
state q? ?? ?
? : (j, k, sd|sd?1| . . . |s1|s0) : pi
? + 1 : (i, k, s?d|s?d?1| . . . |s?1|s?0?s0) : pi?
p ? pi
Figure 3: The dynamic programming arc-standard transition-based deductive system without spurious ambiguity: the
symbol represents that the root node of the topmost element on the stack has not been scanned yet.
8 16 32 64 128
spurious UAS (w/o punc.) 92.5 (93.5) 92.7 (93.6) 92.6 (93.6) 92.6 (93.6) 92.6 (93.6)
dev. sec. (per sent.) 0.01 0.017 0.03 0.06 0.13
non-sp. UAS (w/o punc.) 92.5 (93.6) 92.6 (93.6) 92.6 (93.6) 92.6 (93.6) 92.6 (93.6)sec. (per sent.) 0.01 0.018 0.03 0.07 0.13
spurious UAS (w/o punc.) 92.7 (93.3) 92.7 (93.3) 92.7 (93.3) 92.8 (93.3) 92.8 (93.3)
test sec. (per sent.) 0.01 0.017 0.03 0.06 0.13
non-sp. UAS (w/o punc.) 92.8 (93.4) 92.9 (93.5) 92.9 (93.5) 92.9 (93.5) 92.9 (93.5)sec. (per sent.) 0.01 0.018 0.03 0.06 0.13
Table 2: Unlabeled accuracy scores (UAS) and parsing times (+forest dumping times, second per sentence) for parsing
development (WSJ22) and test (WSJ23) data with spurious shift-reduce and proposed shift-reduce parser (non-sp.)
using several beam sizes.
We used an early update version of the averaged per-
ceptron algorithm (Collins and Roark, 2004; Huang
et al, 2012) to train two shift-reduce dependency
parsers with beam size of 12.
Table 2 shows experimental results of parsing the
development and test datasets with each of the spu-
rious and non-spurious shift-reduce parsers using
several beam sizes. Parsing accuracies were eval-
uated by unlabeled accuracy scores (UAS) with and
without punctuations. The parsing times were mea-
sured on an Intel Core i7 2.8GHz. The average cpu
time (per sentence) includes that of dumping packed
forests. This result indicates that the non-spurious
parser achieves better accuracies than the spurious
beam size 8 32 128
% of distinct trees (10) 93.5 94.8 95.0
% of distinct trees (100) 81.8 84.9 87.2
% of distinct trees (1000) 70.6 73.1 77.6
% of distinct trees (10000) 62.1 64.3 65.6
Table 3: The percentages of distinct dependency trees in
10, 100, 1000 and 10000 best trees extracted from spuri-
ous forests with several beam sizes.
parser without loss of efficiency.
Figure 4 shows oracle unlabeled accuracies of
spurious k-best lists, non-spurious k-best lists, spu-
rious forests, and non-spurious forests. We extract
an oracle tree from each packed forest using the for-
142
 0.96
 0.965
 0.97
 0.975
 0.98
 0.985
 0.99
 0.995
 0  500  1000  1500  2000  2500  3000
o
ra
cl
e 
un
la
be
le
d 
ac
cu
ra
cy
ave. # of hyperedges
beam 16
beam 64
beam 64
"kbest"
"forest"
"non-sp-kbest"
"non-sp-forest"
Figure 4: Each plot shows oracle unlabeled accuracies of
spurious k-best lists, spurious forests, and non-spurious
forests. The oracle accuracies are evaluated using UAS
with punctuations.
est oracle algorithm (Huang, 2008). Both forests
produce much better results than the k-best lists, and
non-spurious forests have almost the same oracle ac-
curacies as spurious forests.
However, as shown in Table 3, spurious forests
encode a number of non-unique dependency trees
while all dependency trees in non-spurious forests
are distinct from each other.
5 Forest Reranking
5.1 Discriminative Reranking Model
We define a reranking model based on the graph-
based features as the following:
y? = argmax
y?H
? ? fg(x, y) (1)
where ? is a weight vector, fg is a feature vector (g
indicates ?graph-based?), x is the input sentence, y
is a dependency tree and H is a dependency for-
est. This model assumes a hyperedge factorization
which induces a decomposition of the feature vector
as the following:
? ? fg(x, y) =
?
e?y
? ? fg,e(e). (2)
The search problem can be solved by simply using
the (generalized) Viterbi algorithm (Klein and Man-
ning, 2001). When using non-local features, the hy-
peredge factorization is redefined to the following:
? ? fg(x, y) =
?
e?y
? ? fg,e(e) + ? ? fg,e,N (e) (3)
where fg,e,N is a non-local feature vector. Though
the cube-pruning algorithm (Huang and Chiang,
2007) is an approximate decoding technique based
on a k-best Viterbi algorithm, it can calculate the
non-local scores efficiently.
The baseline score can be taken into the reranker
as a linear interpolation:
y? = argmax
y?H
? ? sctr(x, y) + ? ? fg(x, y) (4)
where sctr is the score from the baseline parser (tr in-
dicates ?transition-based?), and ? is a scaling factor.
5.2 Features for Discriminative Model
5.2.1 Local Features
While the inference algorithm is a simple Viterbi
algorithm, the discriminative model can use all tri-
sibling features and some grand-sibling features2
(Koo and Collins, 2010) as a local scoring factor in
addition to the first- and sibling second-order graph-
based features. This is because the first stage shift-
reduce parser uses features described in Section 2
and this information can be encoded into vertices of
a hypergraph.
The reranking model also uses guide features ex-
tracted from the 1-best tree predicted by the first
stage shift-reduce parser. We define the guide fea-
tures as first-order relations like those used in Nivre
and McDonald (2008) though our parser handles
only unlabeled and projective dependency struc-
tures. We summarize the features for discriminative
reranking model as the following:
? First- and second-order features: these features
are the same as those used in MST parser3.
? Grand-child features: we define tri-gram POS
features with POS tags of grand parent, parent,
and rightmost or leftmost child.
? Tri-sibling features: we define tri-gram features
with three POS-tags of child, sibling, and tri-
sibling. We also define tri-gram features with
one word and two POS tags of the above.
2The grand-child and grand-sibling features can be used
only when interacting with the leftmost or rightmost child and
sibling. In case of local reranking, we did not use grand-sibling
features because in our experiments, they were not effective.
3http://www.seas.upenn.edu/?strctlrn/
MSTParser/MSTParser.html
143
? Guide feaures: we define a feature indicating
whether an arc from a child to its parent is
present in the 1-best tree predicted by the first-
stage shift-reduce parser, conjoined with the
POS tags of the parent and child.
? PP-Attachment features: when a parent word is
a preposition, we define tri-gram features with
the parent word and POS tags of grand parent
and the rightmost child.
5.2.2 Non-local Features
To define richer features as a non-local factor, we
extend a local reranking algorithm by augmenting
each k-best item with all child vertices of its head
vertex4. Information about all children enables the
reranker to calculate the following features when re-
ducing the head vertex:
? Grand-child features: we define tri-gram fea-
tures with one word and two POS tags of grand
parent, parent, and child.
? Grand-sibling features: we define 4-gram POS
features with POS tags of grand parent, parent,
child and sibling. We also define coordination
features with POS tags of grand parent, parent
and child when the sibling word is a coordinate
conjunction.
? Valency features: we define a feature indicat-
ing the number of children of a head, conjoined
with each of its word and POS tag.
When using non-local features, we removed the lo-
cal grand-child features from the model.
5.3 Oracle for Discriminative Training
A discriminative reranking model is trained on
packed forests by using their oracle trees as the cor-
rect parse. More accurate oracles are essential to
train a discriminative reranking model well.
While large size forests have much more accurate
oracles than small size forests, large forests have too
many hyperedges to train a discriminative model on
them, as shown in Figure 4. The usual forest rerank-
ing algorithms (Huang, 2008; Hayashi et al, 2011)
4If each item is augmented with richer information, even
features based on the entire subtree can be defined.
remove low quality hyperedges from large forests by
using inside-outside forest pruning.
However, producing large forests and pruning
them is computationally very expensive. Instead, we
propose a simpler method to produce small forests
which have more accurate oracles by forcing the
beam search shift-reduce parser to keep the correct
state in the beam buffer. As a result, the correct tree
will always be encoded in a packed forest.
6 Experiments (Discriminative Reranking)
6.1 Experimental Setting
Following (Huang, 2008), the training set (WSJ02-
21) is split into 20 folds, and each fold is parsed by
each of the spurious and non-spurious shift-reduce
parsers using beam size 12 with the model trained
on sentences from the remaining 19 folds, dumping
the outputs as packed forests.
The reranker is modeled by either equation (1) or
(4). By our preliminary experiments using develop-
ment data (WSJ22), we modeled the reranker with
equation (1) when training, and with equation (4)
when testing5 (i.e., the scores of the first-stage parser
are not considered during training of the reranking
model). This prevents the discriminative reranking
features from under-training (Sutton et al, 2006;
Hollingshead and Roark, 2008).
A discriminative reranking model is trained on
the packed forests by using the averaged percep-
tron algorithm with 5 iterations. When training non-
local reranking models, we set k-best size of cube-
pruning to 5.
For dumping packed forests for test data, spurious
and non-spurious shift-reduce parsers are trained by
the averaged perceptron algorithm. In all experi-
ments on English data, we fixed beam size to 12 for
training both parsers.
6.2 Test with Gold POS tags
We show the comparison of dumped spurious and
non-spurious packed forests for training data in Ta-
ble 4. Both oracle accuracies are 100.0 due to the
5The scaling factor ? was tuned by minimum error rate
training (MERT) algorithm (Och, 2003) using development
data. The MERT algorithm is suited to tune low-dimensional
parameters. The ? was set to about 1.2 in case of local rerank-
ing, and to about 1.5 in case of non-local reranking.
144
system w/ rerank. sec. (per sent.) UAS (w/o punc.)
sr (12) ? 0.011 92.8 (93.3)
(8) w/ local 0.009 + 0.0056 93.03 (93.69)
(12) w/ local 0.011 + 0.0079 93.03 (93.68)
(32) w/ local 0.03 + 0.019 93.07 (93.67)
(64) w/ local 0.06 + 0.039 93.0 (93.61)
(12, k=3) w/ non-local 0.011 + 0.0085 93.17 (93.78)
(64, k=3) w/ non-local 0.06 + 0.046 93.19 (93.78)
non-sp sr (12) ? 0.012 92.9 (93.5)
(8) w/ local 0.01 + 0.005 93.05 (93.73)
(12) w/ local 0.012 + 0.0074 93.21 (93.87)
(32) w/ local 0.031 + 0.0184 93.22 (93.84)
(64) w/ local 0.061 + 0.0375 93.23 (93.83)
(12, k=3) w/ non-local 0.012 + 0.0083 93.28 (93.9)
(64, k=3) w/ non-local 0.061 + 0.045 93.39 (93.96)
Table 7: Unlabeled accuracy scores and cpu times per sentence (parsing+reranking) when parsing and reranking test
data (WSJ23) with gold POS tags: shift-reduce parser is denoted as sr (beam size, k: k-best size of cube pruning).
sp. non-sp.
ave. # of hyperedges 141.9 133.3
ave. # of vertices 199.1 187.6
ave. % of distinct trees 82.5 100.0
1-best UAS w/ punc. 92.5 92.6
oracle UAS w/ punc. 100.0 100.0
Table 4: Comparison of spurious (sp.) and non-spurious
(non-sp.) forests: each forest is produced by baseline
and proposed shift-reduce parsers using beam size 12 for
39832 training sentences with gold POS tags.
method described in Section 5.3. The 1-best accu-
racy of the non-spurious forests is higher than that
of the spurious forests. As we expected, the results
show that there are many non-unique dependency
trees in the spurious forests. The spurious forests
also get larger than the non-spurious forests.
Table 5 shows how long the training on spurious
and non-spurious forests took on an Opteron 8356
2.3GHz. It is clear from the results that training on
non-spurious forests is more efficient than that on
spurious forests.
Table 6 shows the statistics of spurious and non-
spurious packed forests dumped by shift-reduce
parsers using beam size 12 for test data. The trends
are similar to those for training data shown in Ta-
ble 4. We show the results of the forest rerank-
ing algorithms for test data in Table 7. Each spu-
rious and non-spurious shift-reduce parser produces
reranker pre-comp. training
spurious 16.4 min. 34.9 min.
non-spurious 15.5 min. 32.9 min.
spurious non-local 17.3 min. 64.3 min.
non-spurious non-local 16.2 min. 60.3 min.
Table 5: Training times on both spurious and non-
spurious packed forests (beam 12): pre-comp. denotes
cpu time for feature extraction and attaching features to
all hyperedges. The non-local models were trained set-
ting k-best size of cube-pruning to 5, and non-local fea-
tures were calculated on-the-fly while training.
packed forests using four beam sizes 8, 12, 32, and
64. The reranking on non-spurious forests achieves
better accuracies and is slightly faster than that on
spurious forests consistently.
6.3 Test with Automatic POS tags
To compare the proposed reranking system with
other systems, we evaluate its parsing accuracy on
test data with automatic POS tags. We used the Stan-
ford POS tagger6 with a model trained on sections
02-21 to tag development and test data, and used
10-way jackknifing to tag training data. The tagging
accuracies on training, development, and test data
were 97.1, 97.2, and 97.5.
Table 8 lists the accuracy and parsing speed of
6http://nlp.stanford.edu/software/
tagger.shtml
145
sp. non-sp.
ave. # of hyperedges 127.0 119.1
ave. # of vertices 178.6 168.5
ave. % of distinct trees 82.4 100.0
1-best UAS w/ punc. 92.8 92.9
oracle UAS w/ punc. 97.0 97.0
Table 6: Comparison of spurious (sp.) and non-spurious
(non-sp.) forests: each forest is produced by baseline and
proposed shift-reduce parsers using beam size 12 for test
data (WSJ23) with gold POS tags.
system tok./sec. UAS w/o punc.
sr (12) 2130 92.5
w/ local (12) 1290 92.8
non-sp sr (12) 1950 92.6
w/ local (12) 1300 92.98
w/ non-local (12, k=1) 1280 93.1
w/ non-local (12, k=3) 1180 93.12
w/ non-local (12, k=12) 1060 93.12
Huang10 sr (8) 782 92.1
Rush12 sr (16) 4780 92.5
Rush12 sr (64) 1280 92.7
Koo10 ? 93.04
Rush12 third 20 93.3
Rush12 vine 4400 93.1
H-Zhang12 third 50 92.81
H-Zhang12 (label) 220 93.06
Y-Zhang11 (64, label) 680 92.9
Bohnet12 (80, label) 120 93.39
Table 8: Comparison with other systems: the results were
evaluated on testing data (WSJ23) with automatic POS
tags: label means labeled dependency parsing and the cpu
times of our systems were taken on Intel Core i7 2.8GHz.
our proposed systems together with results from re-
lated work. The parsing times are reported in to-
kens/second for comparison. Note that, however,
the difference of the parsing time does not represent
the efficiency of the algorithm directly because each
system was implemented in different programming
language and the times were measured on different
environments.
The accuracy of local reranking on non-spurious
forests is the best among unlabeled shift-reduce
parsers, but slightly behind the third-order graph-
based systems (Koo and Collins, 2010; Zhang and
McDonald, 2012; Rush and Petrov, 2012). It is
likely that the difference comes from the fact that
our local reranking model can define only some of
the grand-child related features.
w/ guide. w/o guide.PPPPPPPfeature
UAS 92.98 92.86
Linear (first) 89,330 89,215
CorePos (first) 1,047,948 1,053,796
TwoObs (first) 1,303,911 1,325,990
Sibling (second) 290,291 292,849
Trip (second) 19,333 19,267
Grand-child 16,975 16,951
Guide 4,934 ?
Tri-sibling 277,770 279,720
PP-Attachment 32,695 32,993
total 3,083,187 3,110,781
Table 9: Accuracy and the number of non-zero weighted
features of the local reranking models with and without
guide features: the first- and second-order features are
named for MSTParser.
To define all grand-child features and other non-
local features, we also experimented with the non-
local reranking algorithm on non-spurious packed
forests. It achieved almost the same accuracy as the
previous third-order graph-based algorithms. More-
over, the computational overhead is very small when
setting k-best size of cube-pruning small.
6.4 Analysis
One advantage of our reranking approach is that
guide features can be defined as in stacked parsing.
To analyze the effect of the guide features on parsing
accuracy, we remove the guide features from base-
line reranking models with and without non-local
features used in Section 6.3. The results are shown
in Table 9 and 10. The parsing accuracies of the
baseline reranking models are better than those of
the models without guide features though the num-
ber of guide features is not large. Additionally, each
model with guide features is smaller than that with-
out guide features. This indicates that stacking has a
good effect on training the models.
To further investigate the effects of guide features,
we tried to define unlabeled versions of the second-
order guide features used in (Martins et al, 2008;
McClosky et al, 2012). However, these features did
not produce good results, and investigation to find
the cause is an important future work.
We also examined parsing errors in more de-
tail. Table 11 shows root and sentence complete
rates of three systems, the non-spurious shift-reduce
146
w/ guide. w/o guide.PPPPPPPfeature
UAS 93.12 93.04
Linear (first) 88,634 88,934
CorePos (first) 1,035,897 1,045,242
TwoObs (first) 1,274,834 1,301,103
Sibling (second) 284,341 288,796
Trip (second) 19,201 19,219
Guide 4,916 ?
Tri-sibling 272,418 276,025
PP-Attachment 32,085 32,577
Grand-child 718,064 730,663
Grand-sibling 72,865 73,103
Valency 49,262 49,677
total 3,852,517 3,905,339
Table 10: Accuracy and the number of non-zero weighted
features of the non-local reranking models with and with-
out guide features: the first- and second-order features are
named for MSTParser.
system UAS root comp.
non-sp sr 92.6 95.8 45.6
local 92.98 96.1 48.1
non-local 93.12 96.3 48.2
Table 11: Unlabeled accuracy, root correct rate, and sen-
tence complete rate: these scores are measured on test
data (WSJ23) without punctuations.
parser, local reranking, and non-local reranking.
The two reranking systems outperform the shift-
reduce parser significantly, and the non-local rerank-
ing system is the best among them.
Part of the difference between the shift-reduce
parser and reranking systems comes from the correc-
tion of coordination errors. Table 12 shows the head
correct rate, recall, precision, F-measure and com-
plete rate of coordination structures, by which we
mean the head and siblings of a token whose POS
tag is CC. The head correct rate denotes how cor-
rect a head of the CC token is. The recall, precision,
F-measure are measured by counting arcs between
the head and siblings. When the head of the CC to-
ken is incorrect, all arcs of the coordination structure
are counted as incorrect. Therefore, the recall, preci-
sion, F-measure are greatly affected by the head cor-
rect rate, and though the complete rate of non-local
reranking is higher than that of local reranking, the
results of the first three measures are lower.
non-sp sr local non-local
head correct 87.73 88.97 88.83
recall 82.38 84.35 84.11
precision 83.07 84.57 83.98
F-measure 82.72 84.46 84.05
comp. 62.92 64.52 65.18
Table 12: Head correct rate, recall, precision, F-measure,
and complete rate of coordination strutures: these are
measured on test data (WSJ23).
system recall precision F-measure
non-sp sr 91.58 92.5 92.04
local 91.96 92.95 92.45
non-local 92.44 93.07 92.75
Table 13: Recall, precision, and F-measure of grand-child
structures whose grand parent is an artificial root symbol:
these are measured on test data (WSJ23).
We assume that the improvements of non-local
reranking over the others can be mainly attributed
to the better prediction of the structures around the
sentence root because most of the non-local features
are useful for predicting these structures. Table 13
shows the recall, precision and F-measure of grand-
child structures whose grand parent is a sentence
root symbol $. The results support the above as-
sumption. The root correct rate directly influences
on prediction of the overall structures of a sentence,
and it is likely that the reduction of root prediction
errors brings better results.
6.5 Experiments on Chinese
We also experiment on the Penn Chinese Treebank
(CTB5). Following Huang and Sagae (2010), we
split it into training (secs 001-815 and 1001-1136),
development (secs 886-931 and 1148-1151), and
test (secs 816-885 and 1137-1147) sets, and use the
head rules of Zhang and Clark (2008). The training
set is split into 10 folds to dump packed forests for
training of reranking models.
We set the beam size of both spurious and non-
spurious parsers to 12, and the number of perceptron
training iterations to 25 for the parsers and to 8 for
both rerankers. Table 14 shows the results for the
test sets. As we expected, reranking on non-spurious
forests outperforms that on spurious forests.
147
system UAS root comp.
sr (12) 85.3 78.6 33.4
w/ non-local (12, k=3) 85.8 79.4 34.2
non-sp sr (12) 85.3 78.4 33.7
w/ non-local (12, k=3) 85.9 79.6 34.3
Table 14: Results on Chinese Treebank data (CTB5):
evaluations are performed without punctuations.
7 Related Works
7.1 How to Handle Spurious Ambiguity
The graph-based approach employs Eisner and Satta
(1999)?s algorithm where spurious ambiguities are
eliminated by the notion of split head automaton
grammars (Alshawi, 1996).
However, the arc-standard transition-based parser
has the spurious ambiguity problem. Cohen et al
(2012) proposed a method to eliminate the spurious
ambiguity of shift-reduce transition systems. Their
method covers existing systems such as the arc-
standard and non-projective transition-based parsers
(Attardi, 2006). Our system copes only with the pro-
jective case, but is simpler than theirs and we show
its efficacy empirically through some experiments.
The arc-eager shift-reduce parser also has a spuri-
ous ambiguity problem. Goldberg and Nivre (2012)
addressed this problem by not only training with a
canonical transition sequence but also with alternate
optimal transitions that are calculated dynamically
for a current state.
7.2 Methods to Improve Dependency Parsing
Higher-order features like third-order dependency
relations are essential to improve dependency pars-
ing accuracy (Koo and Collins, 2010; Rush and
Petrov, 2012; Zhang and McDonald, 2012). A
reranking approach is one effective solution to intro-
duce rich features to a parser model in the context of
constituency parsing (Charniak and Johnson, 2005;
Huang, 2008).
Hall (2007) applied a k-best maximum spanning
tree algorithm to non-projective dependency analy-
sis, and showed that k-best discriminative rerank-
ing improves parsing accuracy in several languages.
Sangati et al (2009) proposed a k-best dependency
reranking algorithm using a third-order generative
model, and Hayashi et al (2011) extended it to a
forest algorithm. Though forest reranking requires
some approximations such as cube-pruning to inte-
grate non-local features, it can explore larger search
space than k-best reranking.
The stacking approach (Nivre and McDonald,
2008; Martins et al, 2008) uses the output of one
dependency parser to provide guide features for an-
other. Stacking improves the parsing accuracy of
second stage parsers on various language datasets.
The joint graph-based and transition-based approach
(Zhang and Clark, 2008; Bohnet and Kuhn, 2012)
uses an arc-eager shift-reduce parser with a joint
graph-based and transition-based model. Though
it improves parsing accuracy significantly, the large
beam size of the shift-reduce parser harms its effi-
ciency. Sagae and Lavie (2006) showed that com-
bining the outputs of graph-based and transition-
based parsers can improve parsing accuracies.
8 Conclusion
We have presented a discriminative forest reranking
algorithm for dependency parsing. This can be seen
as a kind of joint transition-based and graph-based
approach because the first-stage parser is a shift-
reduce parser and the second-stage reranker uses a
graph-based model.
Additionally, we have proposed a dynamic pro-
gramming arc-standard transition-based dependency
parser without spurious ambiguity, along with a
heuristic that encodes the correct tree in the output
packed forest for reranker training, and shown that
forest reranking works well on packed forests pro-
duced by the proposed parser.
To improve the accuracy of reranking, we will en-
gage in feature engineering. We need to further in-
vestigate effective higher-order guide and non-local
features. It also seems promising to extend the un-
labeled reranker to a labeled one because labeled in-
formation often improves unlabeled accuracy.
In this paper, we adopt a reranking approach, but
a rescoring approach is more promising to improve
efficiency because it does not have the overhead of
dumping packed forests.
Acknowledgments
We would like to thank the anonymous reviewers
for their valuable comments. This work was partly
148
supported by Grant-in-Aid for Japan Society for the
Promotion of Science (JSPS) Research Fellowship
for Young Scientists.
References
H. Alshawi. 1996. Head automata for speech translation.
In Proc. the ICSLP.
G. Attardi. 2006. Experiments with a multilanguage
non-projective dependency parser. In Proc. of the 10th
Conference on Natural Language Learning, pages
166?170.
B. Bohnet and J. Kuhn. 2012. The best of bothworlds ?
a graph-based completion model for transition-based
parsers. In Proceedings of the 13th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 77?87.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 173?180.
S. B. Cohen, C. Go?mez-Rodr??guez, and G. Satta. 2012.
Elimination of spurious ambiguity in transition-based
dependency parsing. Technical report.
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics (ACL?04).
J. M. Eisner and G. Satta. 1999. Efficient parsing for
bilexical context-free grammars and head automaton
grammars. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguistics,
pages 457?464.
J. Eisner. 1997. Bilexical grammars and a cubic-time
probabilistic parser. In Proceedings of the 5th Inter-
national Workshop on Parsing Technologies (IWPT),
pages 54?65.
Y. Goldberg and J. Nivre. 2012. A dynamic oracle for
arc-eager dependency parsing. In Proceedings of the
24rd International Conference on Computational Lin-
guistics (Coling 2012).
K. Hall. 2007. K-best spanning tree parsing. In Proceed-
ings of the 45th Annual Meeting of the Association of
Computational Linguistics, pages 392?399.
K. Hayashi, T. Watanabe, M. Asahara, and Y. Mat-
sumoto. 2011. The third-order variational reranking
on packed-shared dependency forests. In Proceedings
of the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1479?1488.
K. Hayashi, T. Watanabe, M. Asahara, and Y. Mat-
sumoto. 2012. Head-driven transition-based parsing
with top-down prediction. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics, pages 657?665.
K. Hollingshead and B. Roark. 2008. Reranking with
baseline system scores and ranks as features. In
CSLU-08-001, Center for Spoken Language Under-
standing, Oregon Health and Science University.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 144?151.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics (ACL?10), pages 1077?1086.
L. Huang, S. Fayong, and Y. Guo. 2012. Structured per-
ceptron with inexact search. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 142?151.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics, pages 586?594.
D. Klein and C. D. Manning. 2001. Parsing and hyper-
graphs. In Proceedings of the 7th International Work-
shop on Parsing Technologies.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?10), pages 1?11.
M. Kuhlmann, C. Go?mez-Rodr??guez, and G. Satta. 2011.
Dynamic programming algorithms for transition-
based dependency parsers. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics, pages 673?682.
Andre? F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking dependency parsers. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing, pages 157?166.
D.McClosky, W. Che, M. Recasens, M.Wang, R. Socher,
and C. D. Manning. 2012. Stanfords system for pars-
ing the english web. In Proceedings of First Work-
shop on Syntactic Analysis of Non-Canonical Lan-
guage (SANCL) at NAACL 2012.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05), pages
91?98.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL-08: HLT, pages 950?958.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34:513?553.
149
F. J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. the 41st ACL, pages
160?167.
A. Rush and S. Petrov. 2012. Vine pruning for effi-
cient multi-pass dependency parsing. In Proceedings
of the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 498?507.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proc. HLT, pages 129?132.
F. Sangati, W. Zuidema, and R. Bod. 2009. A generative
re-ranking model for dependency parsing. In Proceed-
ings of the 11th International Conference on Parsing
Technologies (IWPT?09), pages 238?241.
S. M. Shieber, Y. Schabes, and F. C. N. Pereira. 1995.
Principles and implementation of deductive parsing. J.
Log. Program., 24(1&2):3?36.
C. Sutton, M. Sindelar, and A. McCallum. 2006. Reduc-
ing weight undertraining in structured discriminative
learning. In Conference on Human Language Tech-
nology and North American Association for Computa-
tional Linguistics (HLT-NAACL).
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Pro-
ceedings of the 10th International Conference on Pars-
ing Technologies (IWPT?03), pages 195?206.
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
vestigating and combining graph-based and transition-
based dependency parsing using beam-search. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 562?571.
H. Zhang and R. McDonald. 2012. Generalized higher-
order dependency parsing with cube pruning. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 320?331.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 188?193.
150
