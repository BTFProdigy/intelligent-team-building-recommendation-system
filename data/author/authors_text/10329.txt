R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 519 ? 529, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Web-Based Unsupervised Learning for Query 
Formulation in Question Answering 
Yi-Chia Wang1, Jian-Cheng Wu2, Tyne Liang1, and Jason S. Chang2 
1
 Dep. of Computer and Information Science, National Chiao Tung University,  
1001 Ta Hsueh Rd., Hsinchu, Taiwan 300, R.O.C. 
rhyme.cis92g@nctu.edu.tw, tliang@cis.nctu.edu.tw 
2
 Dep. of Computer Science, National Tsing Hua University,  
101, Section 2 Kuang Fu Road, Hsinchu, Taiwan 300, R.O.C. 
d928322@oz.nthu.edu.tw, jschang@cs.nthu.edu.tw 
Abstract. Converting questions to effective queries is crucial to open-domain 
question answering systems. In this paper, we present a web-based 
unsupervised learning approach for transforming a given natural-language 
question to an effective query. The method involves querying a search engine 
for Web passages that contain the answer to the question, extracting patterns 
that characterize fine-grained classification for answers, and linking these 
patterns with n-grams in answer passages. Independent evaluation on a set of 
questions shows that the proposed approach outperforms a naive keyword-
based approach in terms of mean reciprocal rank and human effort. 
1   Introduction 
An automated question answering (QA) system receives a user?s natural-language 
question and returns exact answers by analyzing the question and consulting a large 
text collection [1, 2]. As Moldovan et al [3] pointed out, over 60% of the QA errors 
can be attributed to ineffective question processing, including query formulation and 
query expansion.  
A naive solution to query formulation is using the keywords in an input question as 
the query to a search engine. However, it is possible that the keywords may not appear 
in those answer passages which contain answers to the given question. For example, 
submitting the keywords in ?Who invented washing machine?? to a search engine like 
Google may not lead to retrieval of answer passages like ?The inventor of the automatic 
washer was John Chamberlain.? In fact, by expanding the keyword set (?invented?, 
?washing?, ?machine?) with ?inventor of,? the query to a search engine is effective in 
retrieving such answer passages as the top-ranking pages. Hence, if we can learn how to 
associate a set of questions (e.g. (?who invented ???) with effective keywords or 
phrases (e.g. ?inventor of?) which are likely to appear in answer passages, the search 
engine will have a better chance of retrieving pages containing the answer. 
In this paper, we present a novel Web-based unsupervised learning approach to 
handling question analysis for QA systems. In our approach, training-data questions 
are first analyzed and classified into a set of fine-grained categories of question 
520 Y.-C. Wang et al 
patterns. Then, the relationships between the question patterns and n-grams in answer 
passages are discovered by employing a word alignment technique. Finally, the best 
query transforms are derived by ranking the n-grams which are associated with a 
specific question pattern. At runtime, the keywords in a given question are extracted 
and the question is categorized. Then the keywords are expanded according the 
category of the question. The expanded query is the submitted to a search engine in 
order to bias the search engine to return passages that are more likely to contain 
answers to the question. Experimental results indicate the expanded query indeed 
outperforms the approach of directly using the keywords in the question. 
2   Related Work 
Recent work in Question Answering has attempted to convert the original input 
question into a query that is more likely to retrieve the answers. Hovy et al [2] utilized 
WordNet hypernyms and synonyms to expand queries to increase recall. Hildebrandt et 
al. [4] looked up in a pre-compiled knowledge base and a dictionary to expand a 
definition question. However, blindly expanding a word using its synonyms or 
dictionary gloss may cause undesirable effects. Furthermore, it is difficult to determine 
which of many related word senses should be considered when expanding the query.  
Radev et al [5] proposed a probabilistic algorithm called QASM that learns the best 
query expansion from a natural language question. The query expansion takes the 
form of a series of operators, including INSERT, DELETE, REPLACE, etc., to 
paraphrase a factual question into the best search engine query by applying 
Expectation Maximization algorithm. On the other hand, Hermjakob et al [6] 
described an experiment to observe and learn from human subjects who were given a 
question and asked to write queries which are most effective in retrieving the answer 
to the question. First, several randomly selected questions are given to users to 
?manually? generate effective queries that can bias Web search engines to return 
answers. The questions, queries, and search results are then examined to derive seven 
query reformulation techniques that can be used to produce queries similar to the ones 
issued by human subjects. 
In a study closely related to our work, Agichtein et al [7] presented Tritus system 
that automatically learns transforms of wh-phrases (e.g. expanding ?what is? to 
?refers to?) by using FAQ data. The wh-phrases are restricted to sequences of 
function word beginning with an interrogative, (i.e. who, what, when, where, why, 
and how).  These wh-phrases tend to coarsely classify questions into a few types. 
Tritus uses heuristic rules and thresholds of term frequencies to learn transforms. 
In contrast to previous work, we rely on a mathematical model trained on a set of 
questions and answers to learn how to transform the question into an effective query. 
Transformations are learned based on a more fine-grained question classification 
involving the interrogative and one or more content words. 
3   Transforming Question to Query 
The method is aimed at automatically learning of the best transforms that turn a given 
natural language question into an effective query by using the Web as corpus. To that 
 Web-Based Unsupervised Learning for Query Formulation in Question Answering 521 
end, we first automatically obtain a collection of answer passages (APs) as the 
training corpus from the Web by using a set of (Q, A) pairs. Then we identify the 
question pattern for each Q by using statistical and linguistic information. Here, a 
question pattern Qp is defined as a question word plus one or two keywords that are 
related to the question word. Qp represents the question intention and it can be treated 
as a preference indicative for fine-grained type of named entities. Finally, we decide 
the transforms Ts for each Qp by choosing those phrases in the APs that are 
statistically associated with Qp and adjacent to the answer A. 
Table 1. An example of converting a question (Q) with its answer (A) to a SE query and 
retrieving answer passages (AP) 
(Q, A) AP 
Bungalow For Rent in Islamabad, Capital 
Pakistan. Beautiful Big House For ? What is the capital of Pakistan?  Answer:( Islamabad) 
(k1, k2,?, kn, A) 
Islamabad is the capital of Pakistan. Current 
time, ? 
capital, Pakistan, Islamabad ?the airport which serves Pakistan's capital Islamabad, ? 
3.1   Search the Web for Relevant Answer Passages 
For training purpose, a large amount of question/answer passage pairs are mined from 
the Web by using a set of question/answer pairs as seeds.  
More formally, we attempt to retrieve a set of (Q, AP) pairs on the Web for training 
purpose, where Q stands for a natural language question, and AP is a passage 
containing at least one keyword in Q and A (the answer to Q). The seed data (Q, A) 
pairs can be acquired from many sources, including trivia game Websites, TREC QA 
Track benchmarks, and files of Frequently Asked Questions (FAQ). The output of 
this training-data gathering process is a large collection of (Q, AP) pairs. We describe 
the procedure in details as follows: 
1. For each (Q, A) pair, the keywords k1, k2,?, kn are extracted from Q by removing 
stopwords. 
2. Submit (k1, k2,?, kn, A) as a query to a search engine SE. 
3. Download the top n summaries returned by SE. 
4. Separate sentences in the summaries, and remove HTML tags, URL, special 
character references (e.g., ?&lt;?). 
5. Retain only those sentences which contain A and some ki. 
Consider the example of gathering answer passages from the Web for the (Q, A) 
pair where Q = ?What is the capital of Pakistan?? and A = ?Islamabad.? See Table 1 
for the query submitted to a search engine and potential answer passages returned. 
3.2   Question Analysis 
This subsection describes the presented identification of the so-called ?question 
pattern? which is critical in categorizing a given question and transforming the 
question into a query. 
522 Y.-C. Wang et al 
Formally, a ?question pattern? for any question is defined as following form: 
question-word  head-word+ 
where ?question-word? is one of the interrogatives (Who/What/Where/When/How) 
and the ?head-word? represents the headwords in the subsequent chunks that tend to 
reflect the intended answer more precisely. If the first headword is a light verb, an 
additional headword is needed. For instance, ?who had hit? is a reasonable question 
pattern for ?Who had a number one hit in 1984 with ?Hello???, while ?who had? 
seems to be too coarse. 
In order to determine the appropriate question pattern for each question, we 
examined and analyzed a set of questions which are part-of-speech (POS) tagged and 
phrase-chunked. With the help of a set of simple heuristic rules based on POS and 
chunk information, fine-grained classification of questions can be carried out 
effectively. 
Question Pattern Extraction 
After analyzing recurring patterns and regularity in quizzes on the Web, we designed 
a simple procedure to recognize question patterns. The procedure is based on a small 
set of prioritized rules. 
The question word which is one of the wh-words (?who,? ?what,? ?when,? 
?where,? ?how,? or ?why?) tagged as determiner or adverbial question word. 
According to the result of POS tagging and phrase chunking, we further decide the 
main verb and the voice of the question. Then, we apply the following expanded rules 
to extract words to form question patterns: 
Rule 1: Question word in a chunk of length more than one (see Example (1) in Table 2). 
Qp = question word + headword in the same chunk 
Rule 2: Question word followed by a light verb and Noun Phrase(NP) or 
Prepositional Phrase(PP) chunk (Example (2)). 
Qp = question word + light verb +headword in the following NP or PP chunk 
Rule 3: Question word followed immediately by a verb (Example (3)).  
Qp = question word + headword in the following Verb Phrase(VP) or NP chunk 
Rule 4: Question word followed by a passive VP (Example (4)).  
Qp = Question word + ?to be? + headword in the passive VP chunk 
Rule 5: Question word followed by the copulate ?to be? and an NP (Example (5)).  
Qp = Question word + ?to be? + headword in the next NP chunk 
Rule 6: If none of the above rules are applicable, the question pattern is the question 
word. 
By exploiting linguistic information of POS and chunks, we can easily form the 
question pattern. These heuristic rules are intuitive and easy to understand. Moreover, 
the fact that these patterns which tend to recur imply that they are general and it is 
easy to gather training data accordingly. These question patterns also indicate a 
preference for the answer to be classified with a fine-grained type of proper nouns. In 
 Web-Based Unsupervised Learning for Query Formulation in Question Answering 523 
the next section, we describe how we exploit these patterns to learn the best question-
to-query transforms. 
Table 2. Example questions and question patterns (of words shown in bold) 
(1) Which female singer performed the first song on Top of the Pops? 
(2) Who in 1961 made the first space flight? 
(3) Who painted ?The Laughing Cavalier?? 
(4) What is a group of geese called? 
(5) What is the second longest river in the world? 
3.3   Learning Best Transforms 
This section describes the procedure for learning transforms Ts which convert the 
question pattern Qp into bigrams in relevant APs. 
Word Alignment Across Q and AP 
We use word alignment techniques developed for statistical machine translation to 
find out the association between question patterns in Q and bigrams in AP. The reason 
why we use bigrams in APs instead of unigrams is that bigrams tend to have more 
unique meaning than single words and are more effective in retrieving relevant 
passages. 
We use Competitive Linking Algorithm [8] to align a set of (Q, AP) pairs. The 
method involves preprocessing steps for each (Q, AP) pair so as to filter useless 
information: 
1. Perform part-of-speech tagging on Q and AP. 
2. Replace all instances of A with the tag <ANS> in APs to indicate the location of 
the answers. 
3. Identify the question pattern, Qp and keywords which are not a named entity. We 
denote the question pattern and keywords as q1, q2, ..., qn. 
4. Convert AP into bigrams and eliminate bigrams with low term frequency (tf) or 
high document frequency (df). Bigrams composed of two function words are also 
removed, resulting in bigrams a1, a2, ..., am. 
We then align q?s and a?s via Competitive Linking Algorithm (CLA) procedure as 
follows: 
Input: A collection C of (Q; A) pairs, where (Q; A) = (q1 = Qp , q2, q3, ..., qn ; a1, a2, 
..., am) 
Output: Best alignment counterpart a?s for all q?s in C 
1. For each pair of (Q; A) in C and for all qi and aj in each pair of C, calculate LLR(qi, 
aj), logarithmic likelihood ratio (LLR) between qi and aj, which reflects their 
statistical association. 
2. Discard (q, a) pairs with a LLR value lower than a threshold. 
524 Y.-C. Wang et al 
3. For each pair of (Q; A) in C and for all qi and aj therein, carry out Steps 4-7: 
4. Sort list of (qi, aj) in each pair of (Q ; A) by decreasing LLR value. 
5. Go down the list and select a pair if it does not conflict with previous selection. 
6. Stop when running out of pairs in the list. 
7. Produce the list of aligned pairs for all Qs and APs. 
8. Tally the counts of aligning (q, a). 
9. Select top k bigrams, t1, t2, ..., tk, for every question pattern or keyword q. 
The LLR statistics is generally effective in distinguishing related terms from 
unrelated ones. However, if two terms occur frequently in questions, their alignment 
counterparts will also occur frequently, leading to erroneous alignment due to indirect 
association. CLA is designed to tackle the problem caused by indirect association. 
Therefore, if we only make use of the alignment counterpart of the question pattern, 
we can keep the question keywords in Q so as to reduce the errors caused by indirect 
association. For instance, the question ?How old was Bruce Lee when he died?? Our 
goal is to learn the best transforms for the question pattern ?how old.? In other words, 
we want to find out what terms are associated with ?how old? in the answer passages. 
However, if we consider the alignment counterparts of ?how old? without considering 
those keyword like ?died,? we run the risk of getting ?died in? or ?is dead? rather than 
?years old? and ?age of.? If we have sufficient data for a specific question pattern like 
?how long,? we will have more chances to obtain alignment counterparts that are 
effective terms for query expansion. 
Distance Constraint and Proximity Ranks 
In addition to the association strength implied with alignment counts and co-
occurrence, the distance of the bigrams to the answer should also be considered. We 
observe that terms in the answer passages close to the answers intuitively tend to be 
useful in retrieving answers. Thus, we calculate the bigrams appearing in a window of 
three words appearing on both sides of the answers to provide additional constraints 
for query expansion. 
Combing Alignment and Proximity Ranks 
The selection of the best bigrams as the transforms for a specific question pattern is 
based on a combined rank of alignment count and proximity count. It takes the 
average of these two counts to re-rank bigrams. The average rank of a bigram b is  
Rankavg (b) = (Rankalign (b)+ Rankprox (b))/2, 
where Rankalign (b) is the rank of b?s alignment count and Rankprox (b) is the rank of 
b?s proximity count. The n top-ranking bigrams  for a specific type of question will be 
chosen to transform the question pattern into query terms. For the question pattern 
?how old,? the candidate bigrams with alignment ranks, co-occurring ranks, and 
average ranks are shown in Table 3. 
 Web-Based Unsupervised Learning for Query Formulation in Question Answering 525 
Table 3. Average rank calculated from for the bigram counterparts of ?how old? 
Bigrams Alignment Rank Proximity Rank Avg. Rank Final Rank 
age of 1 1 1 1 
years old 2 2 2 2 
ascend the 3 - - - 
throne in 4 3 3.5 3 
the youngest 3 - - - 
? ? ? ? ? 
3.4   Runtime Transformation of Questions 
At runtime, a given question Q submitted by a user is converted into one or more 
keywords and a question pattern, which is subsequently expanded in to a sequence of 
query terms based on the transforms obtained at training. 
We follow the common practice of keyword selection in formulating Q into a 
query: 
? Function words are identified and discarded. 
? Proper nouns that are capitalized or quoted are treated as a single search term with 
quotes. 
Additionally, we expand the question patterns based on alignment and proximity 
considerations: 
? The question pattern Qp is identified according to the rules (in Section 3.2) and is 
expanded to be a disjunction (sequence of ORs) of Qp?s headword and n top-
ranking bigrams (in section 3.3) 
? The query will be a conjunction (sequence of ANDs) of expanded Qp, proper 
names, and remaining keywords. Except for the expanded Qp, all other proper 
names and keywords will be in the original order in the given question for the best 
results. 
Table 4. An example of transformation from question into query 
Question 
How old was Bruce Lee when he died? 
Question pattern Proper noun Keyword 
how old 
Transformation 
age of, years old 
?Bruce Lee? died 
Expanded query 
Boolean query: ( ?old? OR ?age of? OR ?years old? ) AND ?Bruce Lee? AND ?died? 
Equivalent Google query: (old || ?age of? || ?years old?) ?Bruce Lee? died 
526 Y.-C. Wang et al 
For example, formulating a query for the question ?How old was Bruce Lee when 
he died?? will result in a question pattern ?how old.? Because there is a proper noun 
?Bruce Lee? in the question and a remaining keyword ?died,? the query becomes  
?( ?old? OR ?age of? OR ?years old? ) AND ?Bruce Lee? AND ?died.?? Table 4 lists the 
query formulating for the example question.  
4   Experiments and Evaluation 
The proposed method is implemented by using the Web search engine, Google, as the 
underlying information retrieval system. The experimental results are also justified 
with assessing the effectiveness of question classification and query expansion. 
We used a POS tagger and chunker to perform shallow parsing of the questions 
and answer passages. The tagger was developed using the Brown corpus and 
WordNet. The chunker is built from the shared CoNLL-2000 data provided by 
CoNLL-2000. The shared task CoNLL-2000 provides a set of training and test data 
for chunks. The chunker we used produces chunks with an average precision rate of 
about 94%. 
4.1   Evaluation of Question Patterns 
The 200 questions from TREC-8 QA Track provide an independent evaluation of how 
well the proposed method works for question pattern extraction works. We will also 
give an error analysis. 
Table 5. Evaluation results of question pattern extraction 
 Two ?good? labels At least one ?good? label 
Precision (%) 86 96 
Table 6. The first five questions with question patterns and judgment 
Question Question pattern Judgment 
Who is the author of the book, "The Iron 
Lady: A Biography of Margaret Thatcher"? Who-author good 
What was the monetary value of the Nobel 
Peace Prize in 1989? What value good  
What does the Peugeot company manufacture? What do 
manufacture good 
How much did Mercury spend on advertising 
in 1993?      How much good 
What is the name of the managing director of 
Apricot Computer? What name bad 
 Web-Based Unsupervised Learning for Query Formulation in Question Answering 527 
Two human judges both majoring in Foreign Languages were asked to assess the 
results of question pattern extraction and give a label to each extracted question 
pattern. A pattern will be judged as ?good? if it clearly expresses the answer 
preference of the question; otherwise, it is tagged as ?bad.? The precision rate of 
extraction for these 200 questions is shown in Table 5. The second column indicates 
the precision rate when both of two judges agree that an extracted question pattern is 
?good.? In addition, the third column indicates the rate of those question patterns that 
are found to be ?good? by either judge. The results imply that the proposed pattern 
extraction rules are general, since they are effective even for questions independent of 
the training and development data. Table 6 shows evaluation results for ?two ?good? 
labels? of the first five questions. 
We summarize the reasons behind these bad patterns: 
? Incorrect part-of-speech tagging and chunking 
? Imperative questions such as ?Name the first private citizen to fly in space.? 
? Question patterns that are not specific enough 
For instance, the system produces ?what name? for ?What is the name of the 
chronic neurological autoimmune disease which ? ??, while the judges suggested 
that ?what disease.?. Indeed, some of the patterns extracted can be modified to meet 
the goal of being more fine-grained and indicative of a preference to a specific type of 
proper nouns or terminology. 
4.2   Evaluation of Query Expansion 
We implemented a prototype of the proposed method called Atlas (Automatic 
Transform Learning by Aligning Sentences of question and answer). To develop the 
system of Atlas, we gathered seed training data of questions and answers from a trivia 
game website, called QuizZone1. We collected the questions posted in June, 2004 on 
QuizZone and obtained 3,851 distinct question-answer pairs. We set aside the first 45 
questions for testing and used the rest for training. For each question, we form a query 
with question keywords and the answer and submitted the query to Google to retrieve 
top 100 summaries as the answer passages. In all, we collected 95,926 answer passages.  
At training time, we extracted a total of 338 distinct question patterns from 3,806 
questions. We aligned these patterns and keywords with bigrams in the 95,926 answer 
passages, identified the locations of the answers, and obtained the bigrams appearing 
within a distance of 3 of the answers. At runtime, we use the top-ranking bigram to 
expand each question pattern. If no such bigrams are found, we use only the keyword 
in the question patterns. The expanded terms for question pattern are placed at the 
beginning of the query.  
We submitted forty-five keyword queries and the same number of expanded 
queries generated by Atlas for the test questions to Google and obtained ten returned 
summaries for evaluation. For the evaluation, we use three indicators to measure the 
performance. The first indicator is the mean reciprocal rank (MRR) of the first 
relevant document (or summary) returned. If the r-th document (summary) returned is 
the one with the answer, then the reciprocal rank of the document (summary) is 1/r. 
                                                          
1
 QuizZone (http://www.quiz-zone.co.uk) 
528 Y.-C. Wang et al 
The mean reciprocal rank is the average reciprocal rank of all test questions. The 
second indicator of effective query is the recall at R document retrieved (Recall at R). 
The last indicator measures the human effort (HE) in finding the answer. HE is 
defined as the least number of passages needed to be viewed for covering all the 
answers to be returned from the system. 
The average length of these test questions is short. We believe the proposed 
question expansion scheme helps those short sentences, which tend to be less 
effective in retrieving answers. We evaluated the expanded queries against the same 
measures for summaries returned by simple keyword queries. Both batches of 
returned summaries for the forty-five questions were verified by two human judges. 
As shown in Table 7, the MRR produced by keyword-based scheme is slightly lower 
than the one yielded by the presented query expansion scheme. Nevertheless, such 
improvement is encouraging by indicating the effectiveness of the proposed method. 
Table 8 lists the comparisons in more details. It is found that our method is 
effective in bringing the answers to the top 1 and top 2 summaries as indicated by the 
high Recall of 0.8 at R = 2. In addition, Table 8 also shows that less user?s efforts are 
needed by using our approach. That is, for each question, the average of summaries 
required to be viewed by human beings goes down from 2.7 to 2.3. 
In the end, we found that those bigrams containing a content word and a function 
word  turn out to be very effective. For instance, our method tends to favor transforms 
Table 7. Evaluation results of MRR 
Performances MRR 
GO (Direct keyword query for Google) 0.64 
AT+GO (Atlas expanded query for Google) 0.69 
Table 8. Evaluation Result of Recall at R and Human Effort 
Rank count Recall at R Rank GO AT+GO GO AT+GO 
1 25 26 0.56 0.58 
2 6 10 0.69 0.80 
3 5 3 0.80 0.87 
4 0 1 0.80 0.89 
5 1 1 0.82 0.91 
6 2 0 0.87 0.91 
7 1 0 0.89 0.91 
8 2 0 0.93 0.91 
9 0 1 0.93 0.93 
10 0 0 0.93 0.93 
No answers 3 3 
Human Effort 122 105 
 
# of questions 45 45 
HE per question 2.7 2.3 
 
 Web-Based Unsupervised Learning for Query Formulation in Question Answering 529 
such as ?who invented? to bigrams such as ?invented by,? ?invent the,? and ?inventor 
of.? This contrasts to conventional wisdom of using a stoplist of mostly function 
words and excluding them from consideration in a query. Our experiment also shows 
a function word as part of a phrasal term seems to be very effective, for it indicate an 
implied relation with the answer.  
5   Conclusion and Future Work 
In this paper, we introduce a method for learning query transformations that improves 
the ability to retrieve passages with answers using the Web as corpus. The method 
involves question classification and query transformations using a learning-based 
approach. We also describe the experiment with over 3,000 questions indicates that 
satisfactory results were achieved. The experimental results show that the proposed 
method provides effective query expansion that potentially can lead to performance 
improvement for a question answering system. 
A number of future directions present themselves. First, the patterns learned from 
answer passages acquired on the Web can be refined and clustered to derive a 
hierarchical classification of questions for more effective question classification. Second, 
different question patterns, like ?who wrote? and ?which author?, should be treated as the 
same in order to cope with data sparseness and improve system performance. On the 
other hand, an interesting direction is the generating pattern transformations that contain 
the answer extraction patterns for different types of questions.  
References 
1. Ittycheriah, A., Franz, M., Zhu, W.-J., and Rathaparkhi, A. 2000. IBM?s statistical 
question answering system. In Proceedings of the TREC-9 Question Answering Track, 
Gaithersburg, Maryland. 
2. Hovy, E., Gerber, L., Hermjakob, U., Junk, M., and Lin, C.-Y. 2000. Question answering 
in Webclopedia. In Proceedings of the TREC-9 Question Answering Track, Gaithersburg, 
Maryland. 
3. Moldovan D., Pasca M., Harabagiu S., & Surdeanu M. 2002. Performance Issues and error 
Analysis in an Open-Domain Question Answering System. In Proceedings of the 40th 
Annual Meeting of ACL, Philadelphia, Pennsylvania. 
4. Hildebrandt, W., Katz, B., & Lin, J. 2004. Answering definition questions with multiple 
knowledge sources. In Proceedings of the 2004 Human Language Technology Conference 
and the North American Chapter of the Association for Computational. 
5. Radev, D. R., Qi, H., Zheng, Z., Blair-Goldensohn, S., Fan, Z. Z. W., and Prager, J. M. 
2001. Mining the web for answers to natural language questions. In Proceedings of the 
International Conference on Knowledge Management (CIKM-2001), Atlanta, Georgia. 
6. Hermjakob, U., Echihabi, A., and Marcu, D. 2002. Natural Language Based 
Reformulation Resource and Web Exploitation for Question Answering. In Proceeding of 
TREC-2002, Gaithersburg, Maryland. 
7. Agichtein, E., Lawrence, S., and Gravano, L. Learning to find answers to questions on the 
Web. 2003. In ACM Transactions on Internet Technology (TOIT), 4(2):129-162. 
8. Melamed, I. D. 1997. A Word-to-Word Model of Translational Equivalence. In 
Proceedings of the 35st Annual Meeting of ACL, Madrid, Spain. 
9. Yi-Chia Wang, Jian-Cheng Wu, Tyne Liang, and Jason S. Chang. 2004. Using the Web as 
Corpus for Un-supervised Learning in Question Answering, Proceedings of Rocling 2004, 
Taiwan. 
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 742 ? 753, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Anaphora Resolution for Biomedical Literature by 
Exploiting Multiple Resources 
Tyne Liang and Yu-Hsiang Lin 
National Chiao Tung University, Department of Computer and Information Science, 
Hsinchu, Taiwan 300, ROC 
{tliang, gis91534}@cis.nctu.edu.tw 
Abstract. In this paper, a resolution system is presented to tackle nominal and 
pronominal anaphora in biomedical literature by using rich set of syntactic and 
semantic features. Unlike previous researches, the verification of semantic as-
sociation between anaphors and their antecedents is facilitated by exploiting 
more outer resources, including UMLS, WordNet, GENIA Corpus 3.02p and 
PubMed. Moreover, the resolution is implemented with a genetic algorithm on 
its feature selection. Experimental results on different biomedical corpora 
showed that such approach could achieve promising results on resolving the 
two common types of anaphora.  
1   Introduction 
Correct identification of antecedents for an anaphor is essential in message under-
standing systems as well as knowledge acquisition systems. For example, efficient 
anaphora resolution is needed to enhance protein interaction extraction from biomedi-
cal literature by mining more protein entity instances which are represented with 
pronouns or general concepts. 
In biomedical literature, pronominal and nominal anaphora are the two common 
types of anaphora. In past literature, different strategies to identify antecedents of  
an anaphor have been presented by using syntactic, semantic and pragmatic clues.  
For example, grammatical roles of noun phrases were used in [9] [10]. In addition to 
the syntactic information, statistical information like co-occurring patterns obtained 
from a corpus is employed during antecedent finding in [3]. However, a large corpus 
is needed for acquiring sufficient co-occurring patterns and for dealing with data 
sparseness.  
On the other hand, outer resources, like WordNet1, are applied in [4][12][15] and 
proved to be helpful to improve the system like the one described in [12] where ani-
macy information is exploited by analyzing the hierarchical relation of nouns and 
verbs in the surrounding context learned from WordNet. Nevertheless, using Word-
Net alne for acquiring semantic information is not sufficient for solving unknown 
words. To tackle this problem, a richer resource, the Web, was exploited in [16] 
                                                          
1
 http://wordnet.princeton.edu/ 
 Anaphora Resolution for Biomedical Literature by Exploiting Multiple Resources 743 
where anaphoric information is mined from Google search results at the expense of 
less precision. 
The domain-specific ontologies like UMLS2 (Unified Medical Language System) 
has been employed in [2] in such a way that frequent semantic types associated to 
agent (subject) and patient (object) role of subject-action or action-object patterns can 
be extracted. The result showed such kind of patterns could gain increase in both 
precision (76% to 80%) and recall (67% to 71%). On the other hand, Kim and Park 
[11] built their BioAR to relate protein names to SWISS-Prot entries by using the 
centering theory presented by [7] and salience measures by [2]. 
In this paper, a resolution system is presented for tackling both nominal anaphora 
and pronominal anaphora in biomedical literature by using various kinds of syntactic 
and semantic features. Unlike previous approaches, our verification of the semantic 
association between anaphors and their antecedents is facilitated with the help of both 
general domain and domain-specific resources. For example, the semantic type check-
ing for resolving nominal anaphora can be done by the domain ontology UMLS and 
PubMed3, the search engine for MEDLINE databases. Here, UMLS is used not only 
for tagging the semantic type for the noun phrase chunks if they are in UMLS, but 
also for generating the key lexicons for each type so that we can use them to tag those 
chunks if they are not in UMLS. If no type information can be obtained from an 
chunk, then its type finding will be implemented through the web mining of PubMed. 
On the other hand, the domain corpus, GENIA 3.02p corpus [20] is exploited while 
we solve the semantic type checking for pronominal anaphora. With simple weight 
calculation, the key SA/AO (subject-action or action-object) patterns for each type 
can be mined from the corpus and they turn out to be helpful in resolution. Beside the 
semantic type agreement, the implicit resemblance between an anaphor and its ante-
cedents is another evidence useful for verifying the semantic association. Hence, the 
general domain thesaurus, WordNet, which supporting more relationship between 
concepts and subconcepts, is also employed to enhance the resemblance extraction.  
The presented resolution system is constructed on a basis of a salience grading. In 
order to boost the system, we implemented a simple genetic algorithm on its selection 
of the rich feature set. The system was developed on the small evaluation corpus 
MedStract 4 . Nevertheless, we constructed a larger test corpus (denoted as ?100-
MEDLINE?) so that more instances of anaphors can be resolved. Experimental results 
show that our resolution on MedStract can yield 92% and 78% F-Scores on resolving 
pronominal and nominal anaphora respectively. Promising results were also obtained 
on the larger corpus in terms of 87.43% and 80.61% F-scores on resloving pronomi-
nal and nominal anaphora respectively. 
2   Anaphora Resolution  
Figure 1 is the overview of the presented architecture, including the extraction of 
biomedical SA/AO patterns and semantic type lexicons in background processing 
                                                          
2
 http://www.nlm.nih.gov/research/umls/ 
3
 http://www.pubmedcentral.nih.gov/ 
4
 http://www.medstract.org/ 
744 T. Liang and Y.-H. Lin 
(indicated with dotted lines), as well as the document processing, anaphor recognition 
and antecedent selection in foreground processing (indicated with solid lines). 
 
Fig. 1. System architecture overview 
2.1   Syntactic Information Extraction  
Being important features for anaphora resolution, syntactic information, like POS tags 
and base NP chunks, is extracted from each document by using the Tagger5. Mean-
while, each NP will be tagged with its grammatical role, namely, ?Oblique?, ?Direct 
object?, ?Indirect object?, or ?Subject? by using the following rules which were 
adopted from [22] by adding rules 5 and 6. 
 
                                                          
5
 http://tamas.nlm.nih.gov/tagger.html 
Pronominal Anaphor 
Syntactic Information 
Extraction 
Anaphor Recognition 
Antecedent Selection 
Texts 
Document Processing 
 
Nominal Anaphor 
Semantic Type Lexi-
con Extraction 
Key  
lexicons for 
each type  
PubMed  
Search  
Results Salience Grading 
WordNet 
2.0 
Semantically-tagged 
SA/AO Extraction 
UMLS 
2003AC 
Metathesaurus 
GENIA 
Corpus 
3.02p 
Semantic 
SA/AO 
Patterns 
Number Agreement 
Check 
Rule1: Prep NP (Oblique) 
Rule2: Verb NP (Direct object) 
Rule3:  Verb [NP]+ NP (Indirect object) 
Rule4: NP (Subject) [?,[^Verb], ?|Prep NP]* Verb 
Rule5: NP1 Conjunction NP2 (Role is same as NP1) Conjunction] 
Rule6:  [Conjunction] NP1 ( Role is same as NP2 ) Conjunction NP2 
 Anaphora Resolution for Biomedical Literature by Exploiting Multiple Resources 745 
Rules 5 and 6 are presented for dealing those plural anaphors in such a way that the 
syntactic agreement between the first antecedent and its anaphora is used to find other 
antecedents. For example, without rules 5 and 6, ?anti-CD4 mAb? in Example 1 will 
not be found when resolving anaphora ?they?. 
 
2.2   Semantic Information Extraction 
Beside the syntactic clues, the semantic agreement between an anaphor and its ante-
cedents can also facilitate anaphora resolution in domain-specific literature. In this 
paper, the semantic information for each target noun phrase chunk can be extracted 
with the help of the domain ontology, UMLS, which supports the semantic type for 
the chunk. However, the semantic types for those chunks which are not in UMLS are 
needed to be predicted. Therefore we need to extract the key lexicons from UMLS for 
each semantic type in background processing and use them to tag unknown chunk 
with predicted types. On the other hand, the semantic type checking for pronominal 
anaphors is done through the extraction of the key verbs for each semantic type. 
Hence, a domain corpus GENIA 3.02p is exploited in background processing.   
2.2.1   Key Lexicons for Each Semantic Type 
For each UMLS semantic type, its key lexicons are mined as the following steps in 
Figure 2: 
 
Fig. 2. Procedure to mine key lexicons for each semantic type 
A. Collect all UMLS concepts and their corresponding synonyms as type 
lexicon candidates.  
B. Tokenize the candidates. For example, concept ?interleukin-2? has 
synonyms ?Costimulator?, ?Co-Simulator?, ?IL 2?, and ?interleukine 2?. 
Then ?interleukin?, ?costimulator?, ?simulator?, ?IL?, and ?interleukine? 
will be treated as lexicon candidates.  
C. For each candidate, calculate its weight wij for each type by using Eq. 
(1) which takes into account its concentration and distribution. A prede-
fined threshold is given for the final selection of the candidates.  
)1(1
,
ij
i
ji twcMax
w
w ?=  
wi,j :  score of word i in semantic type j 
wi :   count of word i in semantic type j 
Max cj :  Max count of word k in semantic type j 
twi :   count of semantic types that word i occurs in 
Example1: ?Whereas different anti-CD4 mAb or HIV-1 gp120 could all 
trigger activation of the ..., they differed?? 
746 T. Liang and Y.-H. Lin 
2.2.2   Semantic SA/AO Patterns 
As indicated previously in Section 2.2, the semantic type checking for pronominal 
anaphors can be done through the extraction of the co-occurring SA/AO patterns 
extracted from GENIA 3.02p. We tagged each base noun phrase chunk from the cor-
pus with its grammatical role and tagged it with UMLS-semantic type. Then we used 
Eq. 2 to score each pattern. At resolution, an antecedent candidate is concerned if its 
scores are greater than a given threshold. Table 1 is an example to show the key lexi-
cons and verbs for two semantic types when the semantically-typed chunk is tagged 
with the role of subject. 
)2()(.
1
)(
),(),(
jj
ji
ji
verbtypesofNoverbfrequency
verbtypefrequency
verbtypescore ?=
 
Table 1. Some key lexicons and verbs for two semantic types 
Semantic types key lexicons for each type key verbs for each type 
Amino Acid, 
Peptide, or  
Protein 
protein, product, cerevisiae, 
endonuclease, kinase, antigen, 
receptor, synthase, reductase, 
arabidopsis 
bind, function, derive, raise, 
attenuate, abolish, present, 
signal, localize, release 
Gene or Genome gene, oncogenes activate, compare, locate, 
regulate, remain, transcribe, 
encode, distribute, indicate, 
occupy 
2.3   Anaphora Recognition 
Anaphor recognition is to recognize the target anaphors by filtering strategies. Pro-
nominal anaphora recognition is done by filtering pleonastic-it instances by using the 
set of hand-craft rules presented in [12]. On two corpora, namely, Medstract and the 
new 100-Medline corpus, 100% recognition accuracy was achieved. The remaining 
noun phrases indicated with ?it?, ?its?, ?itself?, ?they?, ?them?, ?themselves? or ?their? 
are considered as pronominal anaphor. Others like ?which? and ?that? used in relative 
clauses are treated as pronominal anaphors and are resolved by the following rules.  
Rule 1: ?that? is treated as pleonastic-that if it is paired with pleonastic-it. 
Rule 2: For a relative clause with ?which? or ?that?, the antecedents will be the 
noun phrases preceding to ?which? or ?that?. 
On the other hand, noun phrases shown with ?either?, ?this?, ?both?, ?these?, ?the?, 
and ?each? are considered as nominal anaphor candidates. Nominal anaphora recogni-
tion is approached by filtering those anaphor candidates, which have no referent ante-
cedents or which have antecedents but not in the target biomedical semantic types. 
Following are two rules used to filter out those non-target nominal anaphors.  
Rule 1: Filter out those anaphor candidates if they are not tagged with one of the 
target UMLS semantic types (the same types in [2]) 
Rule 2: Filter out ?this? or ?the? + proper nouns with capital letters or numbers. 
 Anaphora Resolution for Biomedical Literature by Exploiting Multiple Resources 747 
We treated all other anaphors indicated with ?this? or ?the + singular-NP? as singu-
lar anaphors which have one antecedent only. Others are treated as plural nominal 
anaphors and their numbers of antecedents are shown in Table 2. At antecedent selec-
tion, we can discard those candidates whose numbers differ from the corresponding 
anaphors.  
Table 2. Number of Antecedents 
Anaphor Antecedents # 
Either 2 
Both 2 
Each Many 
They, Their, Them, Themselves Many 
The +Number+ noun Number 
Those +Number+ noun Number 
These +Number+ noun Number 
2.4   Antecedent Selection 
2.4.1   Salience Grading 
The antecedent selection is based on the salience grading as shown in Table 3 in 
which seven features, including syntactic and semantic information, are concerned. 
Table 3. Salience grading for candidate antecedents 
Features Score 
F1 
recency 
0, if in two sentences away from anaphor 
1, if in one sentence away from anaphor 
2, if in same sentence as anaphor 0-2 
F2 Subject and Object Preference 1 
F3 Grammatical function agreement 1 
F4 Number Agreement 1 
F5 Semantic Longest Common Subsequence 0 to 3 
F6 Semantic Type Agreement -1 to +2 
F7 Biomedical antecedent preference -2 if not or +2 
The first feature F1 is recency which measures the distance between an anaphor 
and candidate antecedents in number of sentences. From the statistics of the two cor-
pora, most of antecedents and their corresponding anaphors are within in two sentence 
distance, so a window size for finding antecedent candidates is set to be two sentences 
in the proposed system. The second feature F2 concerns the grammatical roles that an  
 
748 T. Liang and Y.-H. Lin 
anaphor plays in a sentence. Since many anaphors are subjects or objects so antece-
dents with such grammatical tags are preferred. Furthermore, the antecedent candi-
dates will receive more scores if they have grammatical roles (feature F3) or number 
agreement (feature F4) with their anaphors.  
On the other hand, features 5, 6, and 7 are related to semantic association. Feature 
5 concerns the fact that the anaphor and its antecedents are semantical variants of 
each other, so antecedents will receive different scores (as shown below) on the basis 
of their variation: 
 
Following are examples to show the cases: 
 
 
Fig. 3. Procedure to find semantic types for antecedent candidates 
If the antecedent can be found by UMLS,  
Then record its semantic types;  
Else If the antecedent contains the mined key lexicons of the anaphor?s se-
mantic type, then record the semantic type;  
Else mine the semantic type by web mining in such a way that searching 
PubMed by issuing {anaphor Ana, antecedent Ai } pair and apply-
ing Eq. 3 to grade its semantic agreement for Ai. 
)3(3.010)(#
),(#
1)()( ????
?
???
?
?+?=
i
i
ii Acontainingpagesof
AAnacontainingpagesof
AScoreAScore  
Example 2 
case 1: total match: 
<anaphor: each inhibitor, antecedent: PAH alkyne metabolism-based in-
hibitors> 
case 2: partial match: 
<Anaphor: both receptor types, antecedent: the ETB receptor antagonist 
BQ788> 
case 3: component match by using WordNet 2.0: 
<Anaphor: this protein (hyponym: growth factor), antecedent: Cleavage 
and polyadenylation specificity factor> 
If  there is total match of the semantic lexicons between an antecedent?s head 
word  and its anaphor  
Then salience score = salience score + 3 
Else If any antecedent component, other than head word, is matched  
   with its anaphor 
 Then salience score = salience score + 2 
 Else If  any antecedent component is matched with its anaphor?s  
             hyponym by WordNet 2.0 
         Then  salience score = salience score + 1 
 Anaphora Resolution for Biomedical Literature by Exploiting Multiple Resources 749 
Feature 6 is the semantic type agreement between anaphors and antecedents. As 
described in figure 3, the type finding for each antecedent can implemented with the 
help of UMLS. When there is no type information can be obtained from an antece-
dent, the type finding can be implemented with the help of PubMed, and the grading 
on such antecedent will be as Eq. 3. Feature 7 is biomedical antecedent preference. 
That is an antecedent which can be tagged with UMLS or the key lexicons database 
will receive more score.  
2.4.2   Antecedent Selection Strategies 
The noun phrases which precede a recognized anaphor in the range of two sentences 
will be treated as candidates and will be assigned with zero at initial state by the pre-
sented salience grader. Antecedents can be selected by the following strategies. 
(1) Best First: select antecedents with the highest salience score that is greater 
than a threshold  
(2) Nearest First: select the nearest antecedents whose salience value is greater 
than a given threshold 
For plural anaphors, their antecedents are selected as follows: 
(1) If the number of the antecedents is known, then select the same number of 
top-score antecedents.  
(2) If the number of antecedents is unknown, then select those antecedent candi-
dates whose scores are greater than a threshold and whose grammatical pat-
terns are the same as the top-score candidate. 
2.5   Experiments and Analysis 
As mentioned in previous sections, a larger corpus was used for testing the proposed 
system. The corpus, denoted as ?100-Medline?, contains 100 MEDLINE abstracts 
including 43 abstracts (denoted as ?43-Genia? in Table 6) randomly selected from 
GENIA 3.02p and another 57 abstracts (denoted as ?57-PubMed? in Table 6) collected 
from the search results of PubMed (by issuing ?these proteins? and ?these receptors? in 
order to acquire more anaphor instances). There is no common abstract in the public 
MedStract and the new corpus. Table 4 shows the statistics of pronominal and nomi-
nal anaphors for each corpus. 
Table 4. Statistics of anaphor and antecedent pairs 
 Abstracts Sentences Pronominal   instances 
Nominal 
instances Total 
MedStract 32 268 26 47 73 
43-GENIA 43 479 98 63 161 
57-PubMed 57 565 69 118 187 
     The proposed approach was verified with experiments in two ways. One is to in-
vestigate the impact of the features which are concerned in the resolution. Another is 
to compare different resolution approaches. In order to boost our system, a simple 
750 T. Liang and Y.-H. Lin 
generic algorithm is implemented to yield the best set of features by choosing best 
parents to produce offspring.  
In the initial state, we chose features (10 chromosomes), and chose crossover fea-
ture to produce offspring randomly. We calculated mutations for each feature in each 
chromosome, and evaluated chromosome with maximal F-Score. Top 10 chromo-
somes were chosen for next generation and the algorithm terminated if two contigu-
ous generations did not increase the F-score. The time complexity associated with 
such approach is O(MN) where M is the number of candidate antecedents, N is num-
ber of anaphors. 
Table 5. F-Score of Medstract and 100-Medlines 
  Medstract 100-Medlines 
  Nominal Pronominal Nominal Pronominal 
P R F P R F P R F P R F 
33/56 33/47   23/26 23/26   130/184 130/178   145/167 145/167   Total 
Features 58.93 70.21 64.08 88.46 88.46 88.46 70.65 73.34 71.33 86.82 86.82 86.82 
F5, F6, F7 All-F5 F5, F6, F7 All-F5 
P R F P R F P R F P R F 
37/47 37/47   24/26 24/26   156/212 156/178   146/167 146/167   Genetic 
Features 78.72 78.72 78.72 92.31 92.31 92.31 73.58 87.64 80.61 87.43 87.43 87.43 
Table 6. Feature impact experiments 
 Medstract 43-GENIA 57-PubMed 
 Nominal Pronominal Nominal Pronominal Nominal Pronominal 
All 64.08% 88.46% 67.69% 93.58% 73.28% 76.81% 
All ? F1 61.05% 73.08% 60.14% 83.87% 75.44% 75.36% 
All ? F2 65.96% 88.00% 70.22% 93.58% 78.40% 76.81% 
All ? F3 72.00% 80.77% 69.68% 84.46% 73.45% 76.81% 
All ? F4 64.65% 81.48% 68.33% 91.54% 73.73% 76.81% 
All ? F5 48.00% 92.31% 52.55% 93.58% 56.59% 78.26% 
All ? F6 44.04% 88.46% 46.42% 81.63% 57.14% 78.26% 
All ? F7 38.26% 59.26% 47.19% 71.96% 60.44% 50.72% 
Table 5 shows that anaphora resolution implemented with the genetic algorithm 
indeed achieves higher F-scores than the one when all features are concerned. Table 
5 also shows that the semantic features play more important role than the syntactic 
features for nominal anaphora resolution. Similar results can be also found in Table 
6 where the impact of each feature is justified. Moreover, Table 6 indicates that the 
pronominal anaphora resolution on 43-Genia is better than that on the other two 
corpora. It implies that the mined SA/AO patterns from GENIA 3.02p corpus are 
 Anaphora Resolution for Biomedical Literature by Exploiting Multiple Resources 751 
helpful for pronominal anaphora resolution. Moreover, Table 7 proves that the key 
lexicons mined from UMLS for semantic type finding indeed enhance anaphora 
resolution, yet a slight improvement is found with the usage of PubMed search 
results. One of the reasons is few unknown instances in our corpora.   
On the other hand, comparisons with evaluation corpus, Medstract, were shown 
in Table 8 where the best-first strategy yielded higher F-score than the results by the 
nearest-first strategy. It also shows that the best-first strategy with the best selection 
by genetic approach achieves higher F-scores than the approach presented in [2]. 
Table 7. Impacts of the mined semantic lexicons and the use of PubMed 
 With semantic lexicons w/o semantic lexicons 
 Medstract. 100-Medlines Medstract. 100-Medlines 
With PubMed 78% 80.62% 59% 72.16% 
Without PubMed 76% 80.13% 58% 71.33% 
Table 8. Comparisons among different strategies on Medstract 
  Best-First Nearest-First  Casta?o et al [2] 
F-score Nominal Pronominal Nominal Pronominal Nominal Pronominal 
Total  
Features 64.08% 88.46% 50.49% 73.47%     
F5, F6, F7 All - F5 F5, F6, F7 All-(F2,F5) F4, F5, F6 F4, F6, F7 Genetic 
Features 78.72% 92.31% 61.18% 79.17% 74.40% 75.23% 
3   Conclusion 
In this paper, the resolution for pronominal and nominal anaphora in biomedical lit-
erature is addressed. The resolution is constructed with a salience grading on various 
kinds of syntactic and semantic features. Unlike previous researches, we exploit more 
resources, including both domain-specific and general thesaurus and corpus, to verify 
the semantic association between anaphors and their antecedents. Experimental re-
sults on different corpora prove that the semantic features provided with the help of 
the outer resources indeed can enhance anaphora resolution. Compared to other ap-
proaches, the presented best-first strategy with the genetic-algorithm based feature 
selection can achieve the best resolution on the same evaluation corpus.  
References 
1. Baldwin, B.: CogNIAC: high precision coreference with limited knowledge and linguistic 
resources. In Proceedings of the ACL'97/EACL'97 workshop on Operational factors in 
practical, robust anaphora resolution (1997) 38-45 
2. Casta?o, J., Zhang J., Pustejovsky, H.: Anaphora Resolution in Biomedical Literature. In 
International Symposium on Reference Resolution (2002) 
752 T. Liang and Y.-H. Lin 
3. Dagan, I., Itai, A.: Automatic processing of large corpora for the resolution of anaphora 
references. In Proceedings of the 13th International Conference on Computational Linguis-
tics (COLING'90) Vol. III (1990) 1-3 
4. Denber, M.: Automatic resolution of anaphora in English. Technical report, Eastman Ko-
dak Co. (1998) 
5. Gaizauskas, R., Demetriou, G., Artymiuk, P.J., Willett, P.: Protein Structures and Informa-
tion Extraction from Biological Texts: The PASTA System.  Bioinformatics (2003) 
6. Gasperin, C., Vieira R.: Using word similarity lists for resolving indirect anaphora. In 
ACL Workshop on Reference Resolution and its Applications, Barcelona (2004) 
7. Grosz, B.J., Joshi, A.K., Weinstein, S.: Centering: A framework for modelling the local 
coherence of discourse. Computational Linguistics (1995) 203-225 
8. Hahn, U., Romacker, M.: Creating Knowledge Repositories from Biomedical Re-
ports:The MEDSYNDIKATE Text Mining System. In Pacific Symposium on Biocom-
puting (2002)  
9. Hobbs, J.: Pronoun resolution, Research Report 76-1. Department of Computer Science, 
City College, City University of New York, August (1976) 
10. Kennedy, C., Boguraev, B.: Anaphora for everyone: Pronominal anaphora resolution 
without a parser. In Proceedings of the 16th International Conference on Computational 
Linguistics (1996) 113-118 
11. Kim, J., Jong, C.P.: BioAR: Anaphora Resolution for Relating Protein Names to Proteome 
Database Entries. ACL Workshop on Reference Resolution and its Applications Barcelona 
Spain (2004) 79-86 
12. Liang, T., Wu, D.S.: Automatic Pronominal Anaphora Resolution in English Texts.  
Computational Linguistics and Chinese Language Processing Vol.9, No.1 (2004) 21-40 
13. Mitkov, R.: Robust pronoun resolution with limited knowledge. In Proceedings of the 18th 
International Conference on Computational Linguistics (COLING'98)/ACL'98 Conference 
Montreal Canada (1998) 869-875 
14. Mitkov, R.: Anaphora Resolution: The State of the Art. Working paper (Based on the 
COLING'98/ACL'98 tutorial on anaphora resolution) (1999) 
15. Mitkov, R., Evans, R., Orasan, C.: A new fully automatic version of Mitkov's knowledge-
poor pronoun resolution method. In Proceedings of CICLing- 2000 Mexico City Mexico 
(2002) 
16. Modjeska, Natalia, Markert, K., Nissim, M.: Using the Web in Machine Learning for 
Other-Anaphora Resolution. In Proceedings of the Conference on Empirical Methods in 
Natural Language Processing (EMNLP2003) Sapporo Japan 
17. Navarretta, C.: An Algorithm for Resolving Individual and Abstract Anaphora in Danish 
Texts and Dialogues. ACL Workshop on Reference Resolution and its Applications Barce-
lona, Spain (2004)  95-102 
18. Ng, V., Cardie, C.: Improving Machine Learning Approaches to Coreference Resolution. 
In Proceedings of the 40th Annual Meeting of the Association for Computational Linguis-
tics, Association for Computational Linguistics (2002) 
19. Oh, I.S., Lee, J.S., Moon, B.R.: Hybrid Genetic Algorithms for Feature Selection. IEEE 
Transactions on pattern analysis and machine Vol. 26. No. 11 (2004) 
20. Ohta, T., Tateisi, Y., Kim, J.D., Lee, S.Z., Tsujii, J.: GENIA corpus: A Semantically An-
notated Corpus in Molecular Biology Domain. In  Proceedings of the ninth International 
Conference on Intelligent Systems for Molecular Biology (ISMB 2001) poster session 
(2001) 68 
 Anaphora Resolution for Biomedical Literature by Exploiting Multiple Resources 753 
21. Pustejovsky, J., Rumshisky, A., Casta?o, J.: Rerendering Semantic Ontologies: Automatic 
Extensions to UMLS through Corpus Analytics. LREC 2002 Workshop on Ontologies and 
Lexical Knowledge Bases (2002) 
22. Siddharthan, A.: Resolving Pronouns Robustly: Plumbing the Depths of Shallowness. In 
Proceedings of the Workshop on Computational Treatments of Anaphora, 11th Conference 
of the European Chapter of the Association for Computational Linguistics (EACL 2003) 
(2003) 7-14 
23. Yang, X., Su, J., Zhou, G., Tan, C.L.: Improving Pronoun Resolution by Incorporating 
Coreferential Information of Candidates. In Proceedings of ACL 2004 (2004) 127-134 
