Learning to Rank Answers to Non-Factoid
Questions fromWeb Collections
Mihai Surdeanu?
Stanford University
Massimiliano Ciaramita??
Google Inc.
Hugo Zaragoza?
Yahoo! Research
This work investigates the use of linguistically motivated features to improve search, in par-
ticular for ranking answers to non-factoid questions. We show that it is possible to exploit
existing large collections of question?answer pairs (from online social Question Answering sites)
to extract such features and train ranking models which combine them effectively. We investigate
a wide range of feature types, some exploiting natural language processing such as coarse word
sense disambiguation, named-entity identification, syntactic parsing, and semantic role label-
ing. Our experiments demonstrate that linguistic features, in combination, yield considerable
improvements in accuracy. Depending on the system settings we measure relative improvements
of 14% to 21% in Mean Reciprocal Rank and Precision@1, providing one of the most compelling
evidence to date that complex linguistic features such as word senses and semantic roles can have
a significant impact on large-scale information retrieval tasks.
1. Introduction
The problem of Question Answering (QA) has received considerable attention in the
past few years. Nevertheless, most of the work has focused on the task of factoid
QA, where questions match short answers, usually in the form of named or numerical
entities. Thanks to international evaluations organized by conferences such as the Text
REtrieval Conference (TREC) and the Cross Language Evaluation Forum (CLEF) Work-
shop, annotated corpora of questions and answers have become available for several
languages, which has facilitated the development of robust machine learning models
for the task.1
? Stanford University, 353 Serra Mall, Stanford, CA 94305?9010. E-mail: mihais@stanford.edu.
?? Google Inc., Brandschenkestrasse 110, CH?8002 Zu?rich, Switzerland. E-mail: massi@google.com.
? Yahoo! Research, Avinguda Diagonal 177, 8th Floor, 08018 Barcelona, Spain.
E-mail: hugoz@yahoo-inc.com.
1 TREC: http://trec.nist.gov; CLEF: http://www.clef-campaign.org.
The primary part of this work was carried out while all authors were working at Yahoo! Research.
Submission received: 1 April 2010; revised submission received: 11 September 2010; accepted for publication:
23 November 2010.
? 2011 Association for Computational Linguistics
Computational Linguistics Volume 37, Number 2
Table 1
Sample content from Yahoo! Answers.
High Q: How do you quiet a squeaky door?
Quality A: Spray WD-40 directly onto the hinges of the door. Open and close the door
several times. Remove hinges if the door still squeaks. Remove any rust,
dirt or loose paint. Apply WD-40 to removed hinges. Put the hinges back,
open and close door several times again.
High Q: How does a helicopter fly?
Quality A: A helicopter gets its power from rotors or blades. So as the rotors turn,
air flows more quickly over the tops of the blades than it does below.
This creates enough lift for flight.
Low Q: How to extract html tags from an html documents with c++?
Quality A: very carefully
The situation is different once one moves beyond the task of factoid QA. Com-
paratively little research has focused on QA models for non-factoid questions such
as causation, manner, or reason questions. Because virtually no training data is avail-
able for this problem, most automated systems train either on small hand-annotated
corpora built in-house (Higashinaka and Isozaki 2008) or on question?answer pairs
harvested from Frequently Asked Questions (FAQ) lists or similar resources (Soricut
and Brill 2006; Riezler et al 2007; Agichtein et al 2008). None of these situations is
ideal: The cost of building the training corpus in the former setup is high; in the latter
scenario the data tend to be domain-specific, hence unsuitable for the learning of open-
domain models, and for drawing general conclusions about the underlying scientific
problems.
On the other hand, recent years have seen an explosion of user-generated content
(or social media). Of particular interest in our context are community-driven question-
answering sites, such as Yahoo! Answers, where users answer questions posed by other
users and best answers are selected manually either by the asker or by all the partici-
pants in the thread.2 The data generated by these sites have significant advantages over
other Web resources: (a) they have a high growth rate and they are already abundant;
(b) they cover a large number of topics, hence they offer a better approximation of open-
domain content; and (c) they are available for many languages. Community QA sites,
similar to FAQs, provide a large number of question?answer pairs. Nevertheless, these
data have a significant drawback: they have high variance of quality (i.e., questions
and answers range from very informative to completely irrelevant or even abusive).
Table 1 shows some examples of both high and low quality content from the Yahoo!
Answers site.
In this article we investigate two important aspects of non-factoid QA:
1. Is it possible to learn an answer-ranking model for non-factoid questions, in a
completely automated manner, using data available in on-line social QA sites?
This is an interesting question because a positive answer indicates that a
plethora of training data are readily available to researchers and system
2 http://answers.yahoo.com.
352
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
developers working on natural language processing, information retrieval,
and machine learning.
2. Which features and models are more useful in this context, that is, ample but
noisy data? For example: Are similarity models as effective as models that
learn question-to-answer transformations? Does syntactic and semantic
information help?
Social QA sites are the ideal vehicle to investigate such questions. Questions posted
on these sites typically have a correct answer that is selected manually by users. As-
suming that all the other candidate answers are incorrect (we discuss this assumption
in Section 5), it is trivial to automatically organize these data into a format ready for
discriminative learning, namely, the pair question?correct answer generates one posi-
tive example and all other answers for the same question are used to generate negative
examples. This allows one to use the collection in a completely automated manner to
learn answer ranking models.
The contributions of our investigation are the following:
1. We introduce and evaluate many linguistic features for answer re-ranking.
Although several of these features have been introduced in previous work,
some are novel in the QA context, for example, syntactic dependencies
and semantic role dependencies with words generalized to semantic tags.
Most importantly, to the best of our knowledge this is the first work that
combines all these features into a single framework. This allows us to
investigate their comparative performance in a formal setting.
2. We propose a simple yet powerful representation for complex linguistic
features, that is, we model syntactic and semantic information as bags of
syntactic dependencies or semantic role dependencies and build similarity
and translation models over these representations. To address sparsity,
we incorporate a back-off approach by adding additional models where
lexical elements in these structures are generalized to semantic tags.
These models are not only simple to build, but, as our experiments
indicate, they perform at least as well as complex, dedicated models such
as tree kernels.
3. We are the first to evaluate the impact of such linguistic features in a
large-scale setting that uses real-world noisy data. The impact on QA of
some of the features we propose has been evaluated before, but these
experiments were either on editorialized data enhanced with gold
semantic structures (e.g., the Wall Street Journal corpus with semantic
roles from PropBank [Bilotti et al 2007]), or on very few questions
(e.g., 413 questions from TREC 12 [Cui et al 2005]). On the other hand,
we evaluate on over 25,000 questions, and each question has up to
100 candidate answers from Yahoo! Answers. All our data are processed
with off-the-shelf natural language (NL) processors.
The article is organized as follows. We describe our approach, including all the
features explored for answer modeling, in Section 2. We introduce the corpus used in
our empirical analysis in Section 3. We detail our experiments and analyze the results
353
Computational Linguistics Volume 37, Number 2
in Section 4. Section 5 discusses current shortcomings of our system and proposes
solutions. We overview related work in Section 6 and conclude the article in Section 7.
2. Approach
Figure 1 illustrates our QA architecture. The processing flow is the following. First, the
answer retrieval component extracts a set of candidate answers A for a question Q
from a large collection of answers, C, provided by a community-generated question-
answering site. The retrieval component uses a state-of-the-art information retrieval
(IR) model to extract A given Q. The second component, answer ranking, assigns to
each answer Ai ? A a score that represents the likelihood that Ai is a correct answer
for Q, and ranks all answers in descending order of these scores. In our experiments,
the collection C contains all answers previously selected by users of a social QA site
as best answers for non-factoid questions of a certain type (e.g., ?How to? questions).
The entire collection of questions, Q, is split into a training set and two held-out sets:
a development one used for parameter tuning, and a testing one used for the formal
evaluation.
Our architecture follows closely the architectures proposed in the TREC QA track
(see, e.g., Voorhees 2001). For efficiency reasons, most participating systems split the
answer extraction phase into a retrieval phase that selected likely answer snippets
using shallow techniques, followed by a (usually expensive) answer ranking phase
that processes only the candidates proposed by the retrieval component. Due to this
separation, such architectures can scale to collections of any size. We discuss in Section 6
how related work has improved this architecture further?for example, by adding
query expansion terms from the translation models back to answer retrieval (Riezler
et al 2007).
The focus of this work, however, is on the re-ranking model implemented in the
answer ranking component. We call this model FMIX?from feature mix?because the
proposed scoring function is a linear combination of four different classes of features
Figure 1
Architecture of our QA framework.
354
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
(detailed in Section 2.2). To accommodate and combine all these feature classes, our
QA approach combines three types of machine learning methodologies (as highlighted
in Figure 1): the answer retrieval component uses unsupervised IR models, the an-
swer ranking is implemented using discriminative learning, and finally, some of the
ranking features are produced by question-to-answer translation models, which use
class-conditional generative learning. To our knowledge, this combined approach is
novel in the context of QA. In the remainder of the article, we will use the FMIX
function to answer the research objectives outlined in the Introduction. To answer the
first research objective we will compare the quality of the rankings provided by this
component against the rankings generated by the IRmodel used for answer retrieval. To
answer the second research objective we will analyze the contribution of the proposed
feature set to this function.
We make some simplifying assumptions in this study. First, we will consider only
manner questions, and in particular only ?How to? questions. This makes the corpus
more homogeneous and more focused on truly informational questions (as opposed
to social questions such as ?Why don?t girls like me??, or opinion questions such as
?Who will win the next election??, both of which are very frequent in Yahoo! Answers).
Second, we concentrate on the task of answer-re-ranking, and ignore all other modules
needed in a complete on-line social QA system. For example, we ignore the problem
of matching questions to questions, very useful when retrieving answers in a FAQ or
a QA collection (Jeon, Croft, and Lee 2005), and we ignore all ?social? features such as
the authority of users (Jeon et al 2006; Agichtein et al 2008). Instead, we concentrate on
matching answers and on the different textual features. Hence, the document collection
used in our experiments contains only answers, without the corresponding questions
answered. Furthermore, we concentrate on the re-ranking phase and we do not explore
techniques to improve the recall of the initial retrieval phase (by methods of query
expansion, for example). Such aspects are complementary to our work, and can be
investigated separately.
2.1 Representations of Content
One of our main interests in using very large data sets was to show that complex lin-
guistic features can improve rankingmodels if they are correctly combinedwith simpler
features, in particular using discriminative learning methods on a particular task. For
this reason we explore several forms of textual representation going beyond the bag
of words. In particular, we generate our features over four different representations
of text:
Words (W): This is the traditional IR view where the text is seen as a bag of words.
n-grams (N): The text is represented as a bag of word n-grams, where n ranges from two
up to a given length (we discuss structure parameters in the following).
Dependencies (D): The text is converted to a bag of syntactic dependency chains. We
extract syntactic dependencies in the style of the CoNLL-2007 shared task using the
syntactic processor described in Section 3.3 From the tree of syntactic dependencies we
extract all the paths up to a given length following modifier-to-head links. The top part
3 http://depparse.uvt.nl/depparse-wiki/SharedTaskWebsite.
355
Computational Linguistics Volume 37, Number 2
Figure 2
Sample syntactic dependencies and semantic tags.
Figure 3
Sample semantic proposition.
of Figure 2 shows a sample corpus sentence with the actual syntactic dependencies ex-
tracted by our syntactic processor. The figure indicates that this representation captures
important syntactic relations, such as subject?verb (e.g., helicopter
SBJ??? gets) or object-
verb (e.g., power
OBJ??? gets).
Semantic Roles (R): The text is represented as a bag of predicate?argument rela-
tions extracted using the semantic parser described in Section 3. The parser follows the
PropBank notations (Palmer, Gildea, and Kingsbury 2005), that is, it assigns semantic
argument labels to nodes in a constituent-based syntactic tree. Figure 3 shows an exam-
ple. The figure shows that the semantic proposition corresponding to the predicate gets
includes A helicopter as the Arg0 argument (Arg0 stands for agent), its power as the Arg1
argument (or patient), and from rotors or blades as Arg2 (or instrument). Semantic roles
have the advantage that they extract meaning beyond syntactic representations (e.g., a
syntactic subject may be either an agent or a patient in the actual proposition). We con-
vert the semantic propositions detected by our parser into semantic dependencies using
the same approach as Surdeanu et al (2008), that is, we create a semantic dependency
between each predicate and the syntactic head of every one of its arguments. These
dependencies are labeled with the label of the corresponding argument. For example,
the semantic dependency that includes the Arg0 argument in Figure 3 is represented as
gets
Arg0
??? helicopter. If the syntactic constituent corresponding to a semantic argument is
356
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
a prepositional phrase (PP), we convert it to a bigram that includes the preposition and
the head word of the attached phrase. For example, the tuple for Arg2 in the example is
represented as gets
Arg2
??? from-rotors.
In all representations we remove structures where either one of the elements is a
stop word and convert the remaining words to their WordNet lemmas.4
The structures we propose are highly configurable. In this research, we investigate
this issue along three dimensions:
Degree of lexicalization:We reduce the sparsity of the proposed structures by replacing
the lexical elements with semantic tags which might provide better generalization. In
this article we use two sets of tags, the first consisting of coarse WordNet senses, or
supersenses (WNSS) (Ciaramita and Johnson 2003), and the second of named-entity
labels extracted from the Wall Street Journal corpus. We present in detail the tag sets
and the processors used to extract them in Section 3. For an overview, we show a sample
annotated sentence in the bottom part of Figure 2.
Labels of relations: Both dependency and predicate?argument relations can be labeled
or unlabeled (e.g., gets
Arg0
??? helicopter versus gets? helicopter). We make this distinction
in our experiments for two reasons: (a) removing relation labels reduces the model
sparsity because fewer elements are created, and (b) performing relation recognition
without classification is simpler than performing the two tasks, so the corresponding
NL processors might be more robust in the unlabeled-relation setup.
Structure size: This parameter controls the size of the generated structures, namely,
number of words in n-grams or dependency chains, or number of elements in the
predicate?argument tuples. Nevertheless, in our experiments we did not see any im-
provements from structure sizes larger than two. In the experiments reported in this
article, all the structures considered are of size two, that is, we use bigrams, dependency
chains of two elements, and tuples of one predicate and one semantic argument.
2.2 Features
We explore a rich set of features inspired by several state-of-the-art QA systems
(Harabagiu et al 2000; Magnini et al 2002; Cui et al 2005; Soricut and Brill 2006; Bilotti
et al 2007; Ko, Mitamura, and Nyberg 2007). To the best of our knowledge this is the
first work that: (a) adapts all these features for non-factoid answer ranking, (b) combines
them in a single scoring model, and (c) performs an empirical evaluation of the different
feature families and their combinations.
For clarity, we group the features into four sets: features that model the similarity
between questions and answers (FG1), features that encode question-to-answer trans-
formations using a translation model (FG2), features that measure keyword density and
frequency (FG3), and features that measure the correlation between question?answer
pairs and other collections (FG4). Wherever applicable, we explore different syntactic
and semantic representations of the textual content, as introduced previously. We next
explain in detail each of these feature groups.
4 http://wordnet.princeton.edu.
357
Computational Linguistics Volume 37, Number 2
FG1: Similarity Features. We measure the similarity between a question Q and an
answer A using the length-normalized BM25 formula (Robertson and Walker 1997),
which computes the score of the answer A as follows:
BM25(A) =
|Q|
?
i=0
(k1 + 1)tf
A
i (k3 + 1)tf
Q
i
(K+ tf Ai )(k3 + tf
Q
i )
log(idfi) (1)
where tf Ai and tf
Q
i are the frequencies of the question term i in A and Q, and idfi is
the inverse document frequency of term i in the answer collection. K is the length-
normalization factor:
K = k1((1? b)+ b|A|/avg len)
where avg len is the average answer length in the collection. For all the constants in the
formula (b, k1, and k3) we use values reported optimal for other IR collections (b = 0.75,
k1 = 1.2, and k3 = 1, 000).
We chose this similarity formula because, of all the IR models we tried, it provided
the best ranking at the output of the answer retrieval component. For completeness
we also include in the feature set the value of the tf ? idf similarity measure. For both
formulas we use the implementations available in the Terrier IR platform with the
default parameters.5
To understand the contribution of our syntactic and semantic processors we com-
pute the similarity features for different representations of the question and answer
content, ranging from bag of words to semantic roles. We detail these representations in
Section 2.1.
FG2: Translation Features. Berger et al (2000) showed that similarity-based models
are doomed to perform poorly for QA because they fail to ?bridge the lexical chasm?
between questions and answers. One way to address this problem is to learn question-
to-answer transformations using a translation model (Berger et al 2000; Echihabi and
Marcu 2003; Soricut and Brill 2006; Riezler et al 2007). In our model, we incorporate this
approach by adding the probability that the question Q is a translation of the answer
A, P(Q|A), as a feature. This probability is computed using IBM?s Model 1 (Brown et al
1993):
P(Q|A) =
?
q?Q
P(q|A) (2)
P(q|A) = (1? ?)Pml(q|A)+ ?Pml(q|C) (3)
Pml(q|A) =
?
a?A
(T(q|a)Pml(a|A)) (4)
where the probability that the question term q is generated from answer A, P(q|A),
is smoothed using the prior probability that the term q is generated from the entire
collection of answers C, Pml(q|C). ? is the smoothing parameter. Pml(q|C) is computed
5 http://ir.dcs.gla.ac.uk/terrier.
358
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
using the maximum likelihood estimator. To mitigate sparsity, we set Pml(q|C) to a small
value for out-of-vocabulary words.6 Pml(q|A) is computed as the sum of the probabilities
that the question term q is a translation of an answer term a, T(q|a), weighted by the
probability that a is generated fromA. The translation table for T(q|a) is computed using
the EM algorithm implemented in the GIZA++ toolkit.7
Translation models have one important limitation when used for retrieval tasks:
They do not guarantee that the probability of translating a word to itself, that is, T(w|w),
is high (Murdock and Croft 2005). This is a problem for QA, where word overlap
between question and answer is a good indicator of relevance (Moldovan et al 1999).
We address this limitation with a simple algorithm: we set T(w|w) = 0.5 and re-scale
the other T(w?|w) probabilities for all other words w? in the vocabulary to sum to 0.5, to
guarantee that
?
w? T(w
?|w) = 1. This has the desired effect that T(w|w) becomes larger
than any other T(w?|w). Our initial experiments proved empirically that this is essential
for good performance.
As prior work indicates, tuning the smoothing parameter ? is also crucial for the
performance of translation models, especially in the context of QA (Xue, Jeon, and
Croft 2008). We tuned the ? parameter independently for each of the translation models
introduced as follows: (a) for a smaller subset of the development corpus introduced
in Section 3 (1,500 questions) we retrieved candidate answers using our best retrieval
model (BM25); (b) we implemented a simple re-ranking model using as the only feature
the translation model probability; and (c) we explored a large range of values for ?
and selected the one that maximizes the mean reciprocal rank (MRR) of the re-ranking
model. This process selected a wide range of values for the ? parameter for the different
translation models (e.g., 0.09 for the translation model over labeled syntactic depen-
dencies, and 0.43 for the translation model over labeled semantic role dependencies).
Similarly to the previous feature group, we add translation-based features for the
different text representations detailed in Section 2.1. By moving beyond the bag-of-
words representation we hope to learn relevant transformations of structures, for ex-
ample, from the squeaky? door dependency to spray?WD-40 in the Table 1 example.
FG3: Density and Frequency Features. These features measure the density and fre-
quency of question terms in the answer text. Variants of these features were used
previously for either answer or passage ranking in factoid QA (Moldovan et al 1999;
Harabagiu et al 2000). Tao and Zhai (2007) evaluate a series of proximity-based mea-
sures in the context of information retrieval.
Same word sequence: Computes the number of non-stop question words that are
recognized in the same order in the answer.
Answer span: The largest distance (in words) between two non-stop question words in
the answer. We compute multiple variants of this feature, where we count: (a) the total
number of non-stop words in the span, or (b) the number of non-stop nouns.
Informativeness: Number of non-stop nouns, verbs, and adjectives in the answer text
that do not appear in the question.
6 We used 1E-9 for the experiments in this article.
7 http://www.fjoch.com/GIZA++.html.
359
Computational Linguistics Volume 37, Number 2
Same sentencematch:Number of non-stop question termsmatched in a single sentence
in the answer. This feature is added both unnormalized and normalized by the question
length.
Overall match: Number of non-stop question terms matched in the complete answer.
All these features are computed as raw counts and as normalized counts (dividing
the count by the question length, or by the answer length in the case of Answer span).
The last two features (Same sentence match and Overall match) are computed for all
text representations introduced, including syntactic and semantic dependencies (see
Section 2.1).
Note that counting the number of matched syntactic dependencies is essentially
a simplified tree kernel for QA (e.g., see Moschitti et al 2007) matching only trees of
depth 2. We also include in this feature group the following tree-kernel features.
Tree kernels: Tomodel larger syntactic structures that are shared between questions and
answers we compute the tree kernel values between all question and answer sentences.
We implemented a dependency-tree kernel based on the convolution kernels proposed
by Collins and Duffy (2001). We add as features the largest value measured between
any two individual sentences, as well as the average of all computed kernel values for
a given question and answer. We compute tree kernels for both labeled and unlabeled
dependencies, and for both lexicalized trees and for trees where words are generalized
to their predicted WNSS or named-entity tags (when available).
FG4: Web Correlation Features. Previous work has shown that the redundancy of a
large collection (e.g., the Web) can be used for answer validation (Brill et al 2001;
Magnini et al 2002). In the same spirit, we add features that measure the correlation
between question?answer pairs and large external collections:
Web correlation:Wemeasure the correlation between the question?answer pair and the
Web using the Corrected Conditional Probability (CCP) formula of Magnini et al (2002):
CCP(Q,A) = hits(Q+ A)/(hits(Q) hits(A)2/3) (5)
where hits returns the number of page hits from a search engine. The hits procedure
constructs a Boolean query from the given set of terms, represented as a conjunction of
all the corresponding keywords. For example, for the second question in Table 1, hits(Q)
uses the Boolean query: helicopter AND fly.
It is notable that this formula is designed for Web-based QA, that is, the conditional
probability is adjusted with 1/hits(A)2/3 to reduce the number of cases when snippets
containing high-frequency words are marked as relevant answers. This formula was
shown to perform best for the task of QA (Magnini et al 2002). Nevertheless, this
formula was designed for factoid QA, where both the question and the exact answer
have a small number of terms. This is no longer true for non-factoid QA. In this context
it is likely that the number of hits returned forQ, A, orQ+ A is zero given the large size
of the typical question and answer. To address this issue, wemodified the hits procedure
to include a simple iterative query relaxation algorithm:
1. Assign keyword priorities using a set of heuristics inspired by
Moldovan et al (1999). The complete priority detection algorithm
is listed in Table 2.
360
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
Table 2
Keyword priority heuristics.
Step Keyword type Priority
(a) Non-stop keywords within quotes 8
(b) Non-stop keywords tagged as proper nouns 7
(c) Contiguous sequences of 2+ adjectives as nouns 6
(d) Contiguous sequences of 2+ nouns 5
(e) Adjectives not assigned in step (c) 4
(f) Nouns not assigned in steps (c) or (d) 3
(g) Verbs and adverbs 2
(h) Non-stop keywords not assigned in the previous steps 1
2. Fetch the number of page hits using the current query.
3. If the number of hits is larger than zero, stop; otherwise discard the set of
keywords with the smallest priority in the current query and repeat from
step 2.
Query-log correlation: As in Ciaramita, Murdock, and Plachouras (2008), we also com-
pute the correlation between question?answer pairs from a search-engine query-log
corpus of more than 7.5 million queries, which shares roughly the same time stamp
with the community-generated question?answer corpus. Using the query-log correla-
tion between two snippets of text was shown to improve performance for contextual
advertising, that is, linking a user?s query to the description of an ad (Ciaramita,
Murdock, and Plachouras 2008). In this work, we adapt this idea to the task of QA.
However, because it is not clear which correlation metric performs best in this context,
we compute both the Pointwise Mutual Information (PMI) and chi square (?2) associ-
ation measures between each question?answer word pair in the query-log corpus. The
largest and the average values are included as features, as well as the number of QA
word pairs which appear in the top 10, 5, and 1 percentile of the PMI and ?2 word pair
rankings.
We replicate all features that can be computed for different content representations
using every independent representation and parameter combination introduced in
Section 2.1. For example, we compute similarity scores (FG1) for 16 different repre-
sentations of question/answer content, produced by different parametrizations of the
four different generic representations (W, N, D, R). One important exception to this
strategy are the translation-model features (FG2). Because our translation models aim
to learn both lexical and structural transformations between questions and answers,
it is important to allow structural variations in the question/answer representations.
In this article, we implement a simple and robust approximation for this purpose: For
translation models we concatenate all instances of structured representations (N, D, R)
with the corresponding bag-of-words representation (W). This allows the translation
models to learn some combined lexical and structural transformation (e.g., from the
dependency squeaky? door dependency to the tokenWD-40). All in all, replicating our
features for all the different content representations yields 137 actual features to be used
for learning.
361
Computational Linguistics Volume 37, Number 2
2.3 Ranking Models
Our approach is agnostic with respect to the actual learning model. To emphasize this,
we experimented with two learning algorithms. First, we implemented a variant of the
ranking Perceptron proposed by Shen and Joshi (2005). In this framework the ranking
problem is reduced to a binary classification problem. The general idea is to exploit the
pairwise preferences induced from the data by training on pairs of patterns, rather than
independently on each pattern. Given a weight vector ?, the score for a pattern x (a
candidate answer) is given by the inner product between the pattern and the weight
vector:
f?(x) = ?x,?? (6)
However, the error function depends on pairwise scores. In training, for each pair
(xi, xj) ? A, the score f?(xi ? xj) is computed; note that if f is an inner product f?(xi ?
xj) = f?(xi)? f?(xj). In this framework one can define suitable margin functions that
take into account different levels of relevance; for example, Shen and Joshi (2005)
propose g(i, j) = ( 1i ?
1
j ), where i and j are the rank positions of xi and xj. Because in
our case there are only two relevance levels we use a simpler sign function yi,j, which
is negative if i > j and positive otherwise; yi,j is then scaled by a positive rate ? found
empirically on the development data. In the presence of numbers of possible rank levels
appropriate margin functions can be defined. During training, if f?(xi ? xj) ? yi,j?, an
update is performed as follows:
?t+1 = ?t + (xi ? xj)yi,j? (7)
We notice, in passing, that variants of the perceptron including margins have been
investigated before; for example, in the context of uneven class distributions (see Li et al
2002). It is interesting to notice that such variants have been found to be competitive
with SVMs in terms of performance, while being more efficient (Li et al 2002; Surdeanu
and Ciaramita 2007). The comparative evaluation from our experiments are consistent
with these findings. For regularization purposes, we use as a final model the average of
all Perceptron models posited during training (Freund and Schapire 1999).
We also experimented with SVM-rank (Joachims 2006), which is an instance of
structural SVM?a family of Support Vector Machine algorithms that model structured
outputs (Tsochantaridis et al 2004)?specifically tailored for ranking problems.8 SVM-
rank optimizes the area under a ROC curve. The ROC curve is determined by the true
positive rate vs. the false positive rate for varying values of the prediction threshold,
thus providing a metric closely related to Mean Average Precision (MAP).
3. The Corpus
The corpus is extracted from a sample of the U.S. Yahoo! Answers questions and
answers. We focus on the subset of advice or ?how to? questions due to their fre-
quency, quality, and importance in social communities. Nevertheless, our approach
8 http://www.cs.cornell.edu/People/tj/svm light/svm rank.html.
362
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
is independent of the question type. To construct our corpus, we implemented the
following successive filtering steps:
Step 1: From the full corpus we keep only questions that match the regular
expression:
how (to|do|did|does|can|would|could|should)
and have an answer selected as best either by the asker or by the
participants in the thread. The outcome of this step is a set of
364,419 question?answer pairs.
Step 2: From this corpus we remove the questions and answers of dubious
quality. We implement this filter with a simple heuristic by keeping
only questions and answers that have at least four words each, out
of which at least one is a noun and at least one is a verb. The
rationale for this step is that genuine answers to ?how to? questions
should have a minimal amount of structure, approximated by the
heuristic. This step filters out questions like How to be excellent? and
answers such as I don?t know. The outcome of this step forms our
answer collection C. C contains 142,627 question?answer pairs.
This corpus is freely available through the Yahoo! Webscope
program.9
Arguably, all these filters could be improved. For example, the first step can be
replaced by a question classifier (Li and Roth 2006). Similarly, the second step can be
implemented with a statistical classifier that ranks the quality of the content using
both the textual and non-textual information available in the database (Jeon et al 2006;
Agichtein et al 2008). We plan to further investigate these issues, which are not the
main object of this work.
The data was processed as follows. The text was split at the sentence level, token-
ized and POS tagged, in the style of the Wall Street Journal Penn TreeBank (Marcus,
Santorini, and Marcinkiewicz 1993). Each word was morphologically simplified using
the morphological functions of the WordNet library. Sentences were annotated with
WNSS categories, using the tagger of Ciaramita and Altun (2006), which annotates
text with a 46-label tagset.10 These tags, defined by WordNet lexicographers, provide
a broad semantic categorization for nouns and verbs and include labels for nouns such
as food, animal, body, and feeling, and for verbs labels such as communication, contact,
and possession. We chose to annotate the data with this tagset because it is less biased
towards a specific domain or set of semantic categories than, for example, a named-
entity tagger. Using the same tagger as before we also annotated the text with a named-
entity tagger trained on the BBNWall Street Journal (WSJ) Entity Corpus which defines
105 categories for entities, nominal concepts, and numerical types.11 See Figure 2 for a
sample sentence annotated with these tags.
Next, we parsed all sentences with the dependency parser of Attardi et al (2007).12
We chose this parser because it is fast and it performed very well in the domain adap-
tation shared task of CoNLL 2007. Finally, we extracted semantic propositions using
9 You can request the corpus by email at research-data-requests@yahoo-inc.com. More information
about this corpus can be found at: http://www.yr-bcn.es/MannerYahooAnswers.
10 http://sourceforge.net/projects/supersensetag.
11 LDC catalog number LDC2005T33.
12 http://sourceforge.net/projects/desr.
363
Computational Linguistics Volume 37, Number 2
the SwiRL semantic parser of Surdeanu et al (2007).13 SwiRL starts by syntactically
analyzing the text using a constituent-based full parser (Charniak 2000) followed by a
semantic layer, which extracts PropBank-style semantic roles for all verbal predicates in
each sentence.
It is important to realize that the output of all mentioned processing steps is noisy
and contains plenty of mistakes, because the data have huge variability in terms of
quality, style, genres, domains, and so forth. In terms of processing speed, both the
semantic tagger of Ciaramita and Altun and the Attardi et al parser process 100+
sentences/second. The SwiRL system is significantly slower: On average, it parses less
than two sentences per second. However, recent research showed that this latter task
can be significantly sped up without loss of accuracy (Ciaramita et al 2008).
We used 60% of the questions for training, 20% for development, and 20% for test-
ing. Our ranking model was tuned strictly on the development set for feature selection
(described later) and the ? parameter of the translation models. The candidate answer
set for a given question is composed of one positive example, that is, its corresponding
best answer, and as negative examples all the other answers retrieved in the top N by
the retrieval component.
4. Experiments
We used several measures to evaluate our models. Recall that we are using an initial
retrieval engine to select a pool of N answer candidates (Figure 1), which are then re-
ranked. This couples the performance of the initial retrieval engine and the re-rankers.
We tried to de-couple them in our performance measures, as follows. We note that if
the initial retrieval engine does not rank the correct answer in the pool of top N results,
it is impossible for any re-ranker to do well. We therefore follow the approach of Ko
et al (2007) and define performance measures only with respect to the subset of pools
which contain the correct answer for a given N.
This complicates slightly the typical notions of recall and precision. Let us callQ the
set of all queries in the collection and QN the subset of queries for which the retrieved
answer pool of size N contains the correct answer. We will then use the following
performance measure definitions:
Retrieval Recall@N: The usual recall definition:
|QN|
|Q| . This is equal for all re-rankers.
Re-ranking Precision@1: Average Precision@1 over the QN set, where the Precision@1
of a query is defined as 1 if the correct answer is re-ranked into the first position,
0 otherwise.
Re-ranking MRR: MRR over theQN set, where the reciprocal rank is the inverse of the
rank of the correct answer.
Note that as N gets larger, QN grows in size, increasing the Retrieval Recall@N but
also increasing the difficulty of the task for the re-ranker, and therefore decreasing Re-
ranking Precision@1 and Re-ranking MRR.
During training of the FMIX re-ranker, the presentation of the training instances is
randomized, which defines a randomized training protocol producing different models
with each permutation of the data. We exploit this property to estimate the variance on
the experimental results by reporting the average performance of 10 different models,
together with an estimate of the standard deviation.
13 http://swirl-parser.sourceforge.net.
364
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
Table 3
Re-ranking evaluation for Perceptron and SVM-rank. Improvement indicates relative
improvement over the baseline.
N = 15 N = 25 N = 50 N = 100
Retrieval Recall@N 29.04% 32.81% 38.09% 43.42%
Re-ranking Precision@1
Baseline 41.48 36.74 31.66 27.75
FMIX (Perceptron) 49.87?0.03 44.48?0.03 38.53?0.11 33.72?0.05
FMIX (SVM-rank) 49.48 44.10 38.18 33.52
Improvement (Perceptron) +20.22% +21.06% +21.69% +21.51%
Improvement (SVM-rank) +19.28% +20.03% +20.59% +20.79%
Re-ranking MRR
Baseline 56.12 50.31 43.74 38.53
FMIX (Perceptron) 64.16?0.01 58.20?0.01 51.19?0.07 45.29?0.05
FMIX (SVM-rank) 63.81 57.89 50.93 45.12
Improvement (Perceptron) +14.32% +15.68% +17.03% +17.54%
Improvement (SVM-rank) +13.70% +15.06% +16.43% +17.10%
The initial retrieval engine used to select the pool of candidate answers is the BM25
score as described earlier. This is also our baseline re-ranker. We will compare this to the
FMIX re-ranker using all features or using subsets of features.
4.1 Overall Results
Table 3 and Figure 4 show the results obtained using FMIX and the baseline for in-
creasing values of N. We report results for Perceptron and SVM-rank using the optimal
feature set for each (we discuss feature selection in the next sub-section).
Looking at the first column in Table 3 we see that a good bag-of-words representa-
tion alone (BM25 in this case) can achieve 41.5% Precision@1 (for the 29.0% of queries for
Figure 4
Re-ranking evaluation; precision-recall curve.
365
Computational Linguistics Volume 37, Number 2
which the retrieval engine can find an answer in the top N = 15 results). These baseline
results are interesting because they indicate that the problem is not hopelessly hard, but
it is far from trivial. In principle, we see much room for improvement over bag-of-words
methods. Indeed, the FMIX re-ranker greatly improves over the baseline. For example,
the FMIX approach using Perceptron yields a Precision@1 of 49.9%, a 20.2% relative
increase.
SettingN to a higher valuewe see recall increase at the expense of precision. Because
recall depends only on the retrieval engine and not on the re-ranker, what we are
interested in is the relative performance of our re-rankers for increasing numbers of
N. For example, setting N = 100 we observe that the BM25 re-ranker baseline obtains
27.7% Precision@1 (for the 43.4% of queries for which the best answer is found in the
top N = 100). For this same subset, the FMIX re-ranker using Perceptron obtains 33.7%
Precision@1, a 21.5% relative improvement over the baseline model.
The FMIX system yields a consistent and significant improvement for all values
of N, regardless of the type of learning algorithm used. As expected, as N grows the
precision of both re-rankers decreases, but the relative improvement holds or increases.
This can be seen most clearly in Figure 4 where re-ranking Precision and MRR are
plotted against retrieval Recall. Recalling that the FMIX model was trained only once,
using pools of N = 15, we can note that the training framework is stable at increasing
sizes of N.
Table 3 and Figure 4 show that the two FMIX variants (Perceptron and SVM-rank)
yield scores that are close (e.g., Precision@1 scores are within 0.5% of each other). We
hypothesize that the small difference between the two different learning models is
caused by our greedy tuning procedures (described in the next section), which converge
to slightly different solutions due to the different learning algorithms. Most importantly,
the fact that we obtain analogous results with two different learningmodels underscores
the robustness of our approach and of our feature set.
These overall results provide strong evidence that: (a) readily available and scalable
NLP technology can be used to improve lexical matching and translation models for
retrieval and QA tasks, (b) we can use publicly available online QA collections to
investigate features for answer ranking without the need for costly human evaluation,
and (c) we can exploit large and noisy on-line QA collections to improve the accuracy of
answer ranking systems. In the remainder of this section we analyze the performance
of the different features.
4.2 Contribution of Feature Groups
In order to gain some insights about the effectiveness of the different features groups,
we carried out a greedy feature selection procedure. We implemented similar processes
for Perceptron and SVM-rank, to guarantee that our conclusions are not biased by a
particular learning model.
4.2.1 Perceptron. We initialized the feature selection process with a single feature that
replicates the baseline model (BM25 applied to the bag-of-words [W] representation).
Then the algorithm incrementally adds to the feature set the single feature that provides
the highest MRR improvement in the development partition. The process stops when
no features yield any improvement. Note that this is only a heuristic process, and needs
to be interpreted with care. For example, if two features were extremely correlated, the
algorithm would choose one at random and discard the other. Therefore, if a feature is
366
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
missing from the selection process it means that it is either useless, or strongly correlated
with other features in the list.
Table 4 summarizes the outcome of this feature selection process. Where applicable,
we show within parentheses the text representation for the corresponding feature: W
for words, N for n-grams, D for syntactic dependencies, and R for semantic roles. We
use subscripts to indicate if the corresponding representation is fully lexicalized (no
subscript), or its elements are replaced by WordNet supersenses (WNSS) or named-
entity tags (WSJ). Where applicable, we use the l superscript to indicate if the cor-
responding structures are labeled. No superscript indicates unlabeled structures. For
example, DWNSS stands for unlabeled syntactic dependencies where the participating
tokens are replaced by their WordNet supersense; RlWSJ stands for semantic tuples of
predicates and labeled arguments with the words replaced with the corresponding WSJ
named-entity tags.
The table shows that, although the features selected span all the four feature groups
introduced, the lion?s share is taken by the translation features (FG2): 75% of the MRR
improvement is achieved by these features. The frequency/density features (FG3) are
responsible for approximately 16% of the improvement. The rest is due to the query-log
correlation features (FG4). This indicates that, even though translation models are the
most useful, it is worth exploring approaches that combine several strategies for answer
ranking.
As we noted before, many features may be missing from this list simply because
they are strongly correlated with others. For example most similarity features (FG1) are
correlated with BM25(W); for this reason the selection process does not choose a FG1
feature until iteration 9. On the other hand, some features do not provide a useful signal
Table 4
Summary of the model selection process using Perceptron.
Iteration Feature Set Group MRR P@1 (%)
0 BM25(W) FG1 56.09 41.14
1 + translation(R) FG2 61.18 46.33
2 + translation(N) FG2 62.49 47.97
3 + overall match(DWNSS) FG3 63.07 48.93
4 + translation(W) FG2 63.27 49.12
5 + query-log avg(PMI) FG4 63.57 49.56
6 + overall match(W) FG3 63.72 49.74
7 + overall match(W), normalized by Q size FG3 63.82 49.89
8 + same word sequence, normalized by Q size FG3 63.90 49.94
9 + BM25(N) FG1 63.98 50.00
10 + informativeness: verb count FG3 64.16 49.97
11 + query-log max(PMI) FG4 64.37 50.28
12 + same sentence match(W) FG3 64.42 50.40
13 + overall match(NWSJ) FG3 64.49 50.51
14 + query-log max(?2) FG4 64.56 50.59
15 + same word sequence FG3 64.66 50.72
16 + BM25(RWSJ) FG1 64.68 50.78
17 + translation(RlWSJ) FG2 64.71 50.75
18 + answer span, normalized by A size FG3 64.76 50.80
19 + query-log top10(?2) FG4 64.89 51.06
20 + tree kernel(DWSJ) FG3 64.93 51.07
21 + translation(RWNSS) FG2 64.95 51.16
367
Computational Linguistics Volume 37, Number 2
at all. A notable example in this class is theWeb-based CCP feature, which was designed
originally for factoid answer validation and does not adapt well to our problem. To test
this, we learned a model with BM25 and the Web-based CCP feature only, and this
model did not improve over the baseline model at all. We hypothesize that because the
length of non-factoid answers is typically significantly larger than in the factoid QA
task, we have to discard a large part of the query when computing hits(Q+ A) to reach
non-zero counts. This means that the final hit counts, hence the CCP value, are generally
uncorrelated with the original (Q,A) tuple.
One interesting observation is that two out of the first three features chosen by
our model selection process use information from the NLP processors. The first feature
selected is the translation probability computed between the R representation (unla-
beled semantic roles) of the question and the answer. This feature alone accounts for
57% of the measured MRR improvement. This is noteworthy: Semantic roles have been
shown to improve factoid QA, but to the best of our knowledge this is the first result
demonstrating that semantic roles can improve ad hoc retrieval (on a large set of non-
factoid open-domain questions). We also find noteworthy that the third feature chosen
measures the number of unlabeled syntactic dependencies with words replaced by their
WNSS labels that are matched in the answer. Overall, the features that use the output of
NL processors account for 68% of the improvement produced by our model over the IR
baseline. These results provide empirical evidence that natural language analysis (e.g.,
coarse word sense disambiguation, syntactic parsing, and semantic role labeling) has a
positive contribution to non-factoid QA, even in broad-coverage noisy settings based
on Web data. To our knowledge, this had not been shown before.
Finally, we note that tree kernels provide minimal improvement: A tree kernel
feature is selected only in iteration 20 and the MRR improvement is only 0.04 points.
One conjecture is that, due to the sparsity and the noise of the data, matching trees of
depth higher than 2 is highly uncommon. Hence matching immediate dependencies
is a valid approximation of kernels in this setup. Another possible explanation is that
because the syntactic trees produced by the parser contain several mistakes, the tree
kernel, which considers matches between an exponential number of candidate sub-
trees, might be particularly unreliable on noisy data.
4.2.2 SVM-rank. For SVM-rank we employed a tuning procedure similar to the one used
for the Perceptron that implements both feature selection and tuning of the regularizer
parameter C. We started with the baseline feature alone and greedily added one feature
at a time. In each iteration we added the feature that provided the best improvement.
The procedure continues to evaluate all available features, until no improvement is
observed. For this step we set the regularizer parameter to 1.0, a value which provided
a good tradeoff between accuracy and speed as evaluated in an initial experiment.
The selection procedure generated 12 additional features. At this point, using only the
selected features, we fine-tuned the regularization parameter C across a wide spectrum
of possible values. This can be useful because in SVM-rank the interpretation of C is
slightly different than in standard SVM, specifically Csvm = Crank/m, where m is the
number of queries, or questions in our case. Therefore, an optimal value can depend
crucially on the target data. The final value selected by this search procedure was equal
to 290, although performance is relatively stable with values between 1 and 100,000. As
a final optimization step, we continued the feature selection routine, starting from the
13 features already chosen and C = 290. This last step selected six additional features.
A further attempt at fine-tuning the C parameter did not provide any improvements.
368
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
This process is summarized in Table 5, using the same notations as Table 4. Al-
though the features selected by SVM-rank are slightly different than the ones chosen
by the Perceptron, the conclusions drawn are the same as before: Features generated
by NL processors provide a significant boost on top of the IR model. Similarly to the
Perceptron, the first feature chosen by the selection procedure is a translation probabil-
ity computed over semantic role dependencies (labeled, unlike the Perceptron, which
prefers unlabeled dependencies). This feature alone accounts for 33.3% of the measured
MRR improvement. This further enforces our observation that semantic roles improve
retrieval performance for complex tasks such as our non-factoid QA exercise. All in all,
13 out of the 18 selected features, responsible for 70% of the total MRR improvement,
use information from the NL processors.
4.3 Contribution of Natural Language Structures
One of the conclusions of the previous analysis is that features based on natural lan-
guage processing are important for the problem of QA. This observation deserves a
more detailed analysis. Table 6 shows the performance of our first three feature groups
when they are applied to each of the content representations and incremental combina-
tions of representations. In this table, for simplicity we merge features from labeled
and unlabeled representations. For example, R indicates that features are extracted
from both labeled (Rl) and unlabeled (R) semantic role representations. The g subscript
indicates that the lexical terms in the corresponding representation are separately gener-
alized toWNSS andWSJ labels. For example,Dgmerges features generated fromDWNSS,
Table 5
Summary of the model selection process using SVM-rank.
Iteration Feature Set Group MRR P@1 (%)
0 BM25(W) FG1 56.09 41.12
1 + translation(Rl) FG2 59.02 43.99
2 + answer span FG3 60.31 45.05
3 + translation(W) FG2 61.16 46.13
4 + translation(R) FG2 61.65 46.77
5 + overall match(D) FG3 62.85 48.57
6 + translation(RlWSJ) FG2 63.05 48.78
7 + translation(NWSJ) FG2 63.23 48.88
8 + translation(DlWSJ) FG2 63.47 49.21
9 + query-log max(?2) FG4 63.64 49.35
10 + translation(D) FG2 63.77 49.53
11 + translation(N) FG2 63.85 49.66
12 + overall match(NWSJ) FG3 64.03 49.93
+ C fine tuning 64.49 50.43
13 + BM25(DWSJ) FG1 64.49 50.43
14 + BM25(N) FG1 64.74 50.71
15 + tf ? idf(DWNSS) FG1 64.74 50.60
16 + answer span in nouns FG3 64.74 50.60
17 + tf ? idf(Rl) FG1 64.84 50.89
18 + translation(Dl) FG2 64.88 50.91
369
Computational Linguistics Volume 37, Number 2
Table 6
Contribution of natural language structures in each feature group. Scores are MRR changes of
the Perceptron on the development set over the baseline model (FG1 with W), for N = 15. The
best scores for each feature group (i.e., column in the table), are marked in bold.
FG1 FG2 FG3
W ? +4.18 ?6.80
N ?13.97 +2.49 ?13.63
Ng ?18.65 +3.63 ?15.57
D ?15.15 +1.48 ?15.39
Dg ?19.31 +3.41 ?18.18
R ?27.61 +0.33 ?27.82
Rg ?28.29 +3.46 ?26.74
W +N +1.46 +5.20 ?4.36
W +N +Ng +1.51 +5.33 ?4.31
W +N +Ng +D +1.56 +5.78 ?4.31
W +N +Ng +D +Dg +1.56 +5.85 ?4.21
W +N +Ng +D +Dg + R +1.58 +6.12 ?4.28
W +N +Ng +D +Dg + R + Rg +1.65 +6.29 ?4.28
DWSJ, D
l
WNSS, and D
l
WSJ. For each cell in the table, we use only the features from the
corresponding feature group and representation to avoid the correlation with features
from other groups. We generate each best model using the same feature selection
process described above.
The top part of the table indicates that all individual representations perform worse
than the bag-of-words representation (W) in every feature group. The differences range
from less than one MRR point (e.g., FG2[Rg] versus FG2[W]), to over 28 MRR points
(e.g., FG1[Rg] versus FG1[W]). Such a large difference is justified by the fact that for
feature groups FG1 and FG3 we compute feature values using only the corresponding
structures (e.g., only semantic roles), which could be very sparse. For example, there
are questions in our corpus where our SRL system does not detect any semantic propo-
sition. Because translation models merge all structured representations with the bag-
of-word representation, they do not suffer from this sparsity problem. Furthermore, on
their own, FG3 features are significantly less powerful than FG1 or FG2 features. This
explains why models using FG3 features fail to improve over the baseline. Regardless
of these differences, the analysis indicates that in our noisy setting the bag-of-words
representation outperforms any individual structured representation.
However, the bottom part of the table tells a more interesting story: The second
part of our analysis indicates that structured representations provide complementary
information to the bag-of-words representation. Even the combination of bag of words
with the simplest n-gram structures (W + N) always outperforms the bag-of-words
representation alone. But the best results are always obtained when the combination
includes more natural language structures. The improvements are relatively small, but
remarkable (e.g., see FG2) if we take into account the significant scale and settings of the
evaluation. The improvements yielded by natural language structures are statistically
significant for all feature groups. This observation correlates well with the analysis
shown in Tables 4 and 5, which shows that features using semantic (R) and syntactic
(D) representations contribute the most on top of the IR model (BM25(W)).
370
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
5. Error Analysis and Discussion
Similar to most re-ranking systems, our system improves the answer quality for some
questions while decreasing it for others. Table 7 lists the percentage of questions from
our test set that are improved (i.e., the correct answer is ranked higher after re-ranking),
worsened (i.e., the correct answer is ranked lower), and unchanged (i.e., the position of
the correct answer does not change after re-ranking). The table indicates that, regard-
less of the number of candidate answers for re-ranking (N), the number of improved
questions is approximately twice the number of worsened questions. This explains the
consistent improvements in P@1 and MRR measured for various values of N. As N
increases, the number of questions that are improved also grows, which is an expected
consequence of having more candidate answers to re-rank. However, the percentage
of improved questions grows at a slightly lower rate than the percentage of worsened
questions. This indicates that choosing the ideal number of candidate answers to re-
rank requires a trade-off: On the one hand, having more candidate answers increases
the probability of capturing the correct answer in the set; on the other hand, it also
increases the probability of choosing an incorrect answer due to the larger number
of additional candidates. For our problem, it seems that re-ranking using values of N
much larger than 100 would not yield significant benefits over smaller values of N.
This analysis is consistent with the experiments reported in Table 3 where we did not
measure significant growth in P@1 or MRR for N larger than 50.
Although Table 7 gives the big picture of the behavior of our system, it is important
to look at actual questions that are improved or worsened by the re-ranking model in
order to understand the strengths and weaknesses of our system. Table 8 lists some
representative questions where the re-ranking model brings the correct answer to the
top position. For every question we list: (a) the correct answer and its position as given
by the baseline IR Model (?Baseline?) and the re-ranking model (?Re-ranking?); and (b)
the answer that was ranked by the baseline model in the first position and its position
after re-ranking.
Generally, Table 8 indicates that our model performs considerably better than the
bag-of-words IRmodel. For example, we boost the rank of answers that share structures
with the question: for example, the cook ? grouse syntactico-semantic dependency for
the second sample question or make ? call and see ? number for the third example.
Modeling structures is important especially for questions with minimal context, that is,
short length and common terms, like the third sample question. Due to the structure-
based translation models and/or the generalizations to supersenses or named-entity
labels, our model can match structures even when they are not identical. For example,
Table 7
Percentage of questions in the test set that are improved/worsened/unchanged after re-ranking.
This experiment used the Perceptron model.
Better (%) Worse (%) Unchanged (%)
N = 10 33.98 16.81 49.21
N = 15 36.76 18.56 44.68
N = 25 39.64 20.68 39.68
N = 50 42.95 23.30 33.75
N = 100 45.18 25.28 29.54
371
Computational Linguistics Volume 37, Number 2
Table 8
Examples of questions improved by our re-ranking model. URLs were replaced with <URL> in
answer texts. Some non-relevant text was replaced with <...> to save space. The remaining text
maintains the original capitalization and spelling. Non-stop question terms are emphasized in
the answers.
How would you rank the top 5 NFL teams? Do your rankings depend on the outcome
of the Colts Vs. Pats Game?
Baseline Re-ranking Correct? Answer Text
2 1 yes Ok. How can you think the Chargers are better than the Colts, Ravens, and
the Broncos? As for the Rankings, this is what I got: 1. Colts (they actually beat
a good team) 2. Bears (for now, they are going to lose to the Giants) 3. Patriots
(the winner of Cotls vs Patriots should be the next #1) 4. Broncos (They lost
on a field goal) 5. Ravens (all about the D)
1 2 no Basically in sport rankings you will look at the how they perform their
wins against their loss or their stats. Ranking is the process of positioning
individuals, groups or businesses on a ordinal scale in relation to others.
A list arranged in this way is said to be in rank order. Some examples: In
many sports, individuals or teams are given rankings, generally by the sport?s
governing body. In football (soccer), national teams are ranked in the FIFA
World Rankings. In snooker, players are ranked using the Snooker world
rankings. In ice hockey, national teams are ranked in the IIHF World Ranking.
In golf, the top male golfers are ranked using the Official World Golf Rankings
how can i cook grouse quick with normal household spices w/o going out to buy stuff?
Baseline Re-ranking Correct? Answer Text
10 1 yes I?ve never cooked a grouse, but poultry is poultry... You could salt it and pepper
it, put some flour on a plate and roll it in the flour to coat it lightly, then heat
a few tablespoons of olive oil in a skillet and pan-fry it. (If you have no olive
oil, use a little vegetable oil plus a pat of butter ? the oil is to keep the butter
from burning.) Squeeze a few drops of lemon juice over it if you want. Or:
Skip the flour. Salt and pepper the grouse. Pan-fry it in a little olive oil. When
it?s looking close to ready, pour in a little white wine which will sizzle and
reduce down to a sauce. If you?ve got some fresh or dried herbs (rosemary,
thyme, parsley) you could sprinkle a pinch of that in.
1 2 no Well, a grouse is a prey animal. If there was a decline in the population
of grouse, then the animals that usually prey on the grouse - coyotes, owls,
etc - would probably start eating other prey animals, like the pheasants and
squirrels.
how did I do for make a call and that the other dont see my number?
Baseline Re-ranking Correct? Answer Text
2 1 yes to make a call so that the other person cant see the number... dial *67 and wait
for the three beeps.. then dial the number
1 2 no Oneday out of the blue call her. If u dont have her number, when u see her ask
her if she wanted to go out oneday then get her number. When u talk on the
phone get to know her. But dont ask her out too soon because she may not
feel the same way. After a couple of days or weeks taking to her let her know
how u felt about her since the first time u met her.
372
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
Table 8
(continued)
how can i find a veterinary college with dorms?
Baseline Re-ranking Correct? Answer Text
14 1 yes <...> I would say not to look for a specific school of veterinarianmedicine but
rather find a creditable University that offers a degree such as Pre-Vet. Then
from there you can attend graduate school to finish up to become a doctor in
that field. Most major universities will have this degree along with dorms. In
my sources you can see that this is just one of many major universities that
offer Pre-vet medicine.
1 7 no Hi there... here?s an instructional video by Cornell University Feline Health
Center - College of VeterinaryMedicine on how to pill cats: <URL>
how to handle commission splits with partners in Real estate?
Baseline Re-ranking Correct? Answer Text
5 1 yes My company splits the commissions all evenly. However many various
agents/brokers are involved (or even think they are involved), it gets split
further. Keeps everyone happy. No one complains that someone gets ?more?.
1 3 no You will find information regarding obtaining a real estate license in Okla-
homa at the Oklahoma Real Estate Commission?s website (<URL>) Good luck!
for the fourth question, find? college can bematched to look? school if the structures are
generalized toWordNet supersenses. Translation models are crucial to fetching answers
rich in terms related to question concepts. For example, for the first question, our model
boosts the position of the correct answer due to the large numbers of concepts that
are related to NFL, Colts, and Pats: Ravens, Broncos, Patriots, and so forth. In the second
example, our model ranks on the first position the answer containing many concepts
related to cook: salt, pepper, flour, tablespoons, oil, skillet, and so on. In the last example,
our model is capable of associating the bigram real estate to agent and broker. Without
these associationsmany answers are lost to false positives provided by the bag-of-words
similarity models. For example, in the first and last examples in the table, the answers
selected by the baseline model contain more matches of the questions terms than the
correct answers extracted by our model.
All in all, this analysis proves that non-factoid QA is a complex problem where
many phenomena must be addressed. The key for success does not seem to be a unique
model, but rather a combination of approaches each capable of addressing different
facets of the problem. Our model makes a step forward towards this goal, mainly
through concept expansion and the exploration of syntactico-semantic structures. Nev-
ertheless, our model is not perfect. To understand where FMIX fails we performed
a manual error analysis on 50 questions where FMIX performs worse than the IR
baseline and we identified seven error classes. Table 9 lists the distribution of these error
classes and Table 10 lists sample questions and answers from each class. Note that the
percentage values listed in Table 9 sum up to more than 100% because the error classes
are not exclusive. We now detail each of these error classes.
373
Computational Linguistics Volume 37, Number 2
Table 9
Distribution of error classes in questions where FMIX (Perceptron) performs worse.
COMPLEX INFERENCE 38%
ELLIPSIS 36%
ALSO GOOD 18%
REDIRECTION 10%
ANSWER QUALITY 4%
SPELLING 2%
CLARIFICATION 2%
COMPLEX INFERENCE: This is the most common class of errors (38%). Questions in this
class could theoretically be answered by an automated system but such a system would
require complex reasoning mechanisms, large amounts of world knowledge, and dis-
course understanding. For example, to answer the first question in Table 10, a system
would have to understand that confronting or being supportive are forms of dealing with
a person. To answer the second question, the system would have to know that creating
a CD at what resolution you need supersedes making a low resolution CD. Our approach
captures some simple inference rules through translationmodels but fails to understand
complex implications such as these.
ELLIPSIS: This class of errors is not necessarily a fault of our approach but is rather
caused by the problem setting. Because in a social QA site each answer responds to a
specific question, discourse ellipsis (i.e., omitting the context set by the question in the
answer text) is common. This makes some answers (e.g., the third answer in Table 10)
ambiguous, hence hard to retrieve automatically. This affects 36% of the questions
analyzed.
ALSO GOOD: It is a common phenomenon in Yahoo! Answers that a question is asked
several times by different users, possibly in a slightly different formulation. To enable
our large scale automatic evaluation, we considered an answer as correct only if it was
chosen as the ?best answer? for the corresponding question. So in our setting, ?best
answers? from equivalent questions are marked as incorrect. This causes 18% of the
?errors? of the re-ranking model. One example is the fourth question in Table 10, where
the answer selected by our re-ranking model is obviously also correct. It is important
to note that at testing time we do not have access to the questions that generated the
candidate answers for the current test question, that is, the system does not know
which questions are answered by the answers in the ALSO GOOD section of Table 10.
So the answers in the ALSO GOOD category are not selected based on the similarity of
the corresponding queries, but rather, based on better semantic matching between test
question and candidate answer.
REDIRECTION: Some answers (10% of the questions analyzed) do not directly answer a
question but rather redirect the user to relevant URLs (see the fifth question in Table 10).
Because we do not extract the text behind URLs in the answer content, such questions
are virtually impossible to answer using our approach.
ANSWER QUALITY: For a small number of the questions analyzed (4%) the choice of ?best
answer? is dubious (see the sixth example in Table 10). This is to be expected in a
social QA site, where the selection of best answers is not guaranteed to be optimal.
Nevertheless, the relatively small number of such cases is unlikely to influence the
quality of the evaluation.
374
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
Table 10
Examples of questions in each error class. The corresponding error class is listed on the left side
of the question text. We list the answer ranked at the top position by FMIX only where relevant
(e.g., the ALSO GOOD category). URLs were replaced with <URL> in answer texts. Some
non-relevant text was replaced with <...> to save space. The remaining text maintains the
original capitalization and spelling.
COMPLEX INFERENCE how to deal with a person in denial with M.P.D.?
Baseline Re-ranking Correct? Answer Text
1 6 yes First, i would find out if MPD has been diagnosed by a pro-
fessional. In current terminology, MPD is considered a part
of Dissociative Personality Disorder. In any case, it would be
up to the professionals to help this person because you could
cause further problems by confronting this person with what
you think the problem is. If this person is a family member,
you could ask for a consultation with the psychiatric profes-
sional who is treating him/her. Please, please, just do you
best to be supportive without being confrontational since that
might make things even worse for that person.
COMPLEX INFERENCE How do I make a low resolution CD of 100 phtos that were shot at
8 megapixels?
Baseline Re-ranking Correct? Answer Text
1 4 yes you can use picasa2 from google: <URL> is free. In picasa
you can do ?GiftCD? create a CD with you picture at what
resolution you need (including original size)
ELLIPSIS How do mineral ions affect the biology and survival of a pond organism?
Baseline Re-ranking Correct? Answer Text
1 3 yes Some mineral ions are fertilizer and will increase vegetative
growth while others are poisons.
ALSO GOOD How to learn the British accent?
Baseline Re-ranking Correct? Answer Text
4 5 yes Get a dictionary where there is a pronunciation guide which
gives the pronunciation in British English. Watch british
movies and imitate what you can. Then just practice, practice
practice. But before you go about learning accents, slangs or
dialects, make sure you brush up on your basic grammar.
<...>
3 1 no You can do one of two things: first, go to a local bookstore, like
Barnes and Noble. They sell cd?s with different accents from
around the world, accompanied by a book that phonetically
spells the words. This is designed for actors/actresses who
need to learn different accents. Also, go rent a bunch of british
movies, or watch british television. Continually pause and
repeat common phrases and words.
375
Computational Linguistics Volume 37, Number 2
Table 10
(continued)
REDIRECTION How can I build an easy lean-to shed out of scrap wood and skids?
Baseline Re-ranking Correct? Answer Text
6 15 yes the pallet shed... <URL> building a wood shed from pallets...
<URL> good ideas from those who?ve been there...<URL> pics. of
the shed... <URL> nice pics. <URL> taking pallets apart... and other
tips... <URL> <...>
ANSWER QUALITY How do make a Naruto AMV? Can you show me how? I need the
website or program and the exact directions.?
Baseline Re-ranking Correct? Answer Text
2 94 yes i?m not an expert. but i sure do like Naruto. i?ll wait for answers
too
SPELLING how does aliquid expansion boiler thrrmosstat work?
Baseline Re-ranking Correct? Answer Text
2 4 yes the liquid expands inside the thermostat when the liquid reaches
the shutoff temp or pressure it will shut off the boiler preventing
boiler explosions
CLARIFICATION how could you combine your styles and personalities effectively to
produce the best paper?
Baseline Re-ranking Correct? Answer Text
29 1 yes Your question is not clear. Are you asking about writing styles?
it also depends on what kind of paper you are writing? Your
question cannot be answered without more info.
SPELLING: Two percent (2%) of the error cases analyzed are caused by spelling errors
(e.g., the seventh example in Table 10). Because these errors are relatively infrequent,
they are not captured by our translation models, and our current system does not
include any other form of spelling correction.
CLARIFICATION: Another 2% of the questions inspected manually had answers that
pointed to errors or ambiguities in the question text rather than responding to the given
question (see the last example in Table 10). These answers are essentially correct but
they require different techniques to be extracted: Our assumption is that questions are
always correct and sufficient for answer extraction.
6. Related Work
There is a considerable amount of previous work in several related areas. First, we will
discuss related work with respect to the features and models used in this research; most
of this work is to be found in the factoid QA community, where the most sophisticated
376
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
QA selection and re-ranking algorithms have been developed. We then review existing
work in non-factoid QA; we will see that in this area there is much less work, and the
emphasis has been so far in query re-writing and scalability using relatively simple
features andmodels. Finally wewill discuss relatedwork in the area of community-built
(social) QA sites. Although we do not exploit the social aspect of our QA collection, this
is complementary to our work and would be a natural extension. Table 11 summarizes
aspects of the different approaches discussed in this section, highlighting the differences
and similarities with our current work.
Our work borrows ideas from many of the papers mentioned in this section, es-
pecially for feature development; indeed our work includes matching features as well
as translation and retrieval models, and operates at the lexical level, the parse tree
Table 11
Comparison of some of the characteristics of the related work cited. Task: Document Retrieval
(DRet), Answer Extraction (Ex) or Answer Re-ranking or Selection (Sel).Queries: factoid (Fact)
or non-factoid (NonFact). Features: lexical (L), n-grams (Ngr), collocations (Coll), paraphrases
(Para), POS, syntactic dependency tree (DT), syntactic constituent tree (CT), named entities (NE),
WordNet Relations (WNR), WordNet supersenses (WNSS), semantic role labeling (SRL), causal
relations (CR), query classes (QC), query-log co-ocurrences (QLCoOcc).Models: bag-of-words
scoring (BOW), tree matching (TreeMatch), linear (LM), log-linear (LLM), statistical learning
(kernel) (SL), probabilistic grammar (PG), statistical machine translation (SMT), query likelihood
language model (QLLM).Development and Evaluation: data sizes used, expressed as number
of queries/number of query?answer pairs (i.e., sum of all candidate answers per question).
Data: type of source used for feature construction, training and/or evaluation. Question marks
are place holders for information not available or not applicable in the corresponding work.
Publication Task Queries Features Models Devel Eval Data
Agichtein et al DRet NonFact L, Ngr, Coll BOW, LM ?/10K 50/100 WWW
(2001)
Echihabi and Sel Fact, L, Ngr, Coll, SMT 4.6K/100K 1K/300K? TREC, KM,
Marcu (2003) NonFact DT, NE, WWW
WN
Higashinaka and Sel NonFact L, WN, SRL, SL 1K/500K 1K/500K WHYQA
Isozaki (2008) (Why) CR
Punyakanok et al Sel Fact L, POS, DT, TreeMatch ?/400 TREC13
(2004) NE, QC
Riezler et al DRet NonFact L, Ngr, Para SMT 10M/10M 60/1.2K WWW, FAQ
(2007)
Soricut and Brill DRet, NonFact L, Ngr, Coll BOW, SMT 1M/? 100/? WWW, FAQ
(2006) Sel,
Ex
Verberne et al Sel NonFact CT, WN, BOW, LLM same as eval 186/28K Webclopedia,
(2010) (Why) Para Wikipedia
Wang et al (2007) Sel Fact L, POS, DT, LLM, PG 100/1.7K 200/1.7K TREC13
NE, WNR,
Hyb
Xue et al (2008) DRet, NonFact, L, Coll SMT, QLLM 1M/1M 50/? SocQA,
Sel Fact TREC9
This work Sel NonFact L, Ngr, POS, TreeMatch, 112K/1.6M 28K/up to SocQA,
(How) DT, SRL, BOW, SMT, 2.8M QLog
NE, WN, SL
WNSS,
Hyb,
QLCoOcc
377
Computational Linguistics Volume 37, Number 2
level, as well as the level of semantic roles, named entities, and lexical semantic classes.
However, to the best of our knowledge no previous work in QA has evaluated the use
of so many types of features concurrently, nor has it built so many combinations of these
features at different levels. Furthermore, we employ unsupervised methods, generative
methods, and supervised learning methods. This is made possible by the choice of the
task and the data collection, another novelty of our work which should enable future
research in complex linguistic features for QA and ranking.
Factoid QA. Within the statistical machine translation community there has been
much research on the issue of automatically learning transformations (at the lexical,
syntactical, and semantical level). Some of this work has been applied to automated
QA systems, mostly for factoid questions. For example, Echihabi and Marcu (2003)
presented a noisy-channel approach (IBM model 4) adapted for the task of QA. The
features used included lexical and parse-tree elements as well as some named entities
(such as dates). They use a dozen heuristic rules to heavily reduce the feature space and
choose a single representation mode for each of the tokens in the queries (for example:
?terms overlapping with the question are preserved as surface text?) and learn language
models on the resulting representation. We extend Echihabi and Marcu by considering
deeper semantic representations (such as SRL andWNSS), but instead of using selection
heuristics we learn models from each of the full representations (as well as from some
hybrid representations) and then combine them using discriminant learning techniques.
Punyakanok, Roth, and Yih (2004) attempted a more comprehensive use of the
parse tree information, computing a similarity score between question and answer
parse trees (using a distance function based on approximate tree matching algorithms).
This is an unsupervised approach, which is interesting especially when coupled with
appropriate distances. Shen and Joshi (2005) extend this idea with a supervised learning
approach, training dependency tree kernels to compute the similarity. In our work we
also used this type of feature, although we show that, in our context, features based on
dependency tree kernels are subsumed by simpler features that measure the overlap
of binary dependencies. Another alternative is proposed by Cui et al (2005), where
significant words are aligned and similarity measures (based on mutual information of
correlations) are then computed on the resulting dependency paths. Shen and Klakow
(2006) extend this using a dynamic time warping algorithm to improve the alignment
for approximate question phrase mapping, and learn a Maximum Entropy model to
combine the obtained scores for re-ranking. Wang, Smith, andMitamura (2007) propose
to use a probabilistic quasi-synchronous grammar to learn the syntactic transformations
between questions and answers. We extend the work of Cui et al by considering paths
within and across different representations beyond dependency trees, although we do
not investigate the issue of alignment specifically?instead we use standard statistical
translation models for this.
Non-factoid QA. The previous works dealt with the problem of selection, that is,
finding the single sentence that correctly answers the question out of a set of candidate
documents. A related problem in QA is that of retrieval: selecting potentially relevant
documents or sentences prior to the selection phase. This problem is closer to gene-
ral document retrieval and it is therefore easier to generalize to the non-factoid domain.
Retrieval algorithms tend to be much simpler than selection algorithms, however, in
part due to the need for speed, but also because there has been little previous evidence
that complex algorithms or deeper linguistic analysis helps at this stage, especially in
the context of non-factoid questions.
378
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
Previous work addressed the task by learning transformations between questions
and answers and using them to improve retrieval. All these works use only lexical
features. For example, Agichtein et al (2001) learned lexical transformations (from the
original question to a set of Web search queries, from ?what is a? to ?the term?, ?stands
for?, etc.) which are likely to retrieve good candidate documents in commercial Web
search engines; they applied this successfully to large-scale factoid and non-factoid QA
tasks. Murdock and Croft (2005) study the problem of candidate sentence retrieval for
QA and show that a lexical translation model can be exploited to improve factoid QA.
Xue, Jeon, and Croft (2008) show that a linear interpolation of translation models and
a query likelihood language model outperforms each individual model for a QA task
that is independent of the question type. In the same space, Riezler et al (2007) develop
SMT-based query expansionmethods and use them for retrieval from FAQpages. In our
work we did not address the issue of query expansion and re-writing directly: While
our re-ranking approach is limited to the recall of the retrieval model, these methods of
query transformation could be used in a complementary manner to improve the recall.
Even more interesting would be to couple the two approaches in an efficient manner;
this remains as future work.
There has also been some work in the problem of selection for non-factoid ques-
tions. Girju (2003) extracts non-factoid answers by searching for certain semantic struc-
tures (e.g., causation relations as answers to causation questions). We generalized this
methodology (in the form of semantic roles) and evaluated it systematically. Soricut
and Brill (2006) develop a statistical model by extracting (in an unsupervised manner)
QA pairs from one million FAQs obtained from the Web. They show how different
statistical models may be used for the problems of ranking, selection, and extraction
of non-factoid QAs on the Web; due to the scale of their problem they only consider lex-
ical n-grams and collocations, however. More recent work has showed that structured
retrieval improves answer ranking for factoid questions: Bilotti et al (2007) showed that
matching predicate?argument frames constructed from the question and the expected
answer types improves answer ranking. Cui et al (2005) learned transformations of
dependency paths from questions to answers to improve passage ranking. All these
approaches use similarity models at their core because they require the matching of
the lexical elements in the search structures, however. On the other hand, our approach
allows the learning of full transformations from question structures to answer structures
using translation models applied to different text representations.
The closest work to ours is that of Higashinaka and Isozaki (2008) and Verberne
et al (2010), both on Why questions. Higashinaka et al consider a wide range of
semantic features by exploiting WordNet and gazetteers, semantic role labeling, and
extracted causal relations. Verberne et al exploit syntactic information from constituent
trees, WordNet synonymy sets and relatedness measures, and paraphrases. As in our
models, both these works combine these features using discriminative learning tech-
niques and apply the learned models to re-rank answers to non-factoid questions (Why
type questions). Their features, however, are based on counting matches or events
defined heuristically. We have extended this approach in several ways. First, we use a
much larger feature set that includes correlation and transformation-based features and
five different content representations. Second, we use generative (translation) models
to learn transformation functions before they are combined by the discriminant learner.
Finally, we carry out training and evaluation at a much larger scale.
Content from community-built question?answer sites can be retrieved by searching
for similar questions already answered (Jeon, Croft, and Lee 2005) and ranked using
meta-data information like answerer authority (Jeon et al 2006; Agichtein et al 2008).
379
Computational Linguistics Volume 37, Number 2
Here we show that the answer text can be successfully used to improve answer ranking
quality. Our method is complementary to the earlier approaches. It is likely that an
optimal retrieval engine from social media would combine all three methodologies.
Moreover, our approach might have applications outside of social media (e.g., for open-
domainWeb-based QA), because the rankingmodel built is based only on open-domain
knowledge and the analysis of textual content.
7. Conclusions
In this work we describe an answer ranking system for non-factoid questions built
using a large community-generated question?answer collection. We show that the best
ranking performance is obtained when several strategies are combined into a single
model. We obtain the best results when similarity models are aggregated with features
that model question-to-answer transformations, frequency and density of content, and
correlation of QA pairs with external collections. Although the features that model
question-to-answer transformations provide the most benefits, we show that the com-
bination is crucial for improvement. Further, we show that complex linguistic features,
most notably semantic role dependencies and semantic labels derived from WordNet
senses, yield a statistically significant performance increase on top of the traditional
bag-of-words and n-gram representations. We obtain these results using only off-the-
shelf NL processors that were not adapted in any way for our task. As a side effect, our
experiments prove that we can effectively exploit large amounts of availableWeb data to
do research on NLP for non-factoid QA systems, without any annotation or evaluation
cost. This provides an excellent framework for large-scale experimentation with various
models that otherwise might be hard to understand or evaluate.
As implications of our work, we expect the outcome of our investigation to help
several applications, such as retrieval from social media and open-domain QA on the
Web. On social media, for example, our system should be combined with a component
that searches for similar questions already answered; the output of this ensemble can
possibly be filtered further by a content-quality module that explores ?social? features
such as the authority of users, and so on. Although we do not experiment on Wikipedia
or news sites in this work, one can view our data as a ?worse-case scenario,? given its
ungrammaticality and annotation quality. It seems reasonable to expect that training our
model on cleaner data (e.g., fromWikipedia or news), would yield even better results.
This work can be extended in several directions. First, answers that were not se-
lected as best, but were marked as good by a minority of voters, could be incorporated
in the training data, possibly introducing a graded notion of relevance. This wouldmake
the learning problemmore interesting andwould also provide valuable insights into the
possible pitfalls of user-annotated data. It is not clear if more data, but of questionable
quality, is beneficial. Another interesting problem concerns the adaptation of the re-
ranking model trained on social media to collections from other genres and/or domains
(news, blogs, etc.). To our knowledge, this domain adaptation problem for QA has not
been investigated yet.
References
Agichtein, Eugene, Carlos Castillo, Debora
Donato, Aristides Gionis, and Gilad Mishne.
2008. Finding high-quality content in social
media, with an application to community-
based question answering. In Proceedings of
the Web Search and Data Mining Conference
(WSDM), pages 183?194, Stanford, CA.
Agichtein, Eugene, Steve Lawrence, and Luis
Gravano. 2001. Learning search engine
specific query transformations for question
answering. In Proceedings of the World
380
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
Wide Web Conference, pages 169?178,
Hong Kong.
Attardi, Giuseppe, Felice Dell?Orletta, Maria
Simi, Atanas Chanev, and Massimiliano
Ciaramita. 2007. Multilingual dependency
parsing and domain adaptation using
DeSR. In Proceedings of the Shared Task
of the Conference on Computational
Natural Language Learning (CoNLL),
pages 1112?1118, Prague.
Berger, Adam, Rich Caruana, David Cohn,
Dayne Freytag, and Vibhu Mittal. 2000.
Bridging the lexical chasm: Statistical
approaches to answer finding. In
Proceedings of the 23rd Annual International
ACM SIGIR Conference on Research &
Development on Information Retrieval,
pages 192?199, Athens, Greece.
Bilotti, Matthew W., Paul Ogilvie, Jamie
Callan, and Eric Nyberg. 2007. Structured
retrieval for question answering. In
Proceedings of the 30th Annual International
ACM SIGIR Conference on Research &
Development on Information Retrieval,
pages 351?358, Amsterdam.
Brill, Eric, Jimmy Lin, Michele Banko,
Susan Dumais, and Andrew Ng. 2001.
Data-intensive question answering.
In Proceedings of the Text REtrieval
Conference (TREC), pages 393?400,
Gaithersburg, MD, USA.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Charniak, Eugene. 2000. A maximum-
entropy-inspired parser. In Proceedings of
the North American Chapter of the Association
for Computational Linguistics (NAACL),
pages 132?139, Seattle, WA.
Ciaramita, Massimiliano and Yasemin
Altun. 2006. Broad coverage sense
disambiguation and information
extraction with a supersense sequence
tagger. In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 594?602,
Sidney.
Ciaramita, Massimiliano, Giuseppe Attardi,
Felice Dell?Orletta, and Mihai Surdeanu.
2008. Desrl: A linear-time semantic role
labeling system. In Proceedings of the Shared
Task of the 12th Conference on Computational
Natural Language Learning (CoNLL-2008),
pages 258?262, Manchester.
Ciaramita, Massimiliano and Mark Johnson.
2003. Supersense tagging of unknown
nouns in WordNet. In Proceedings of the
2003 Conference on Empirical Methods in
Natural Language Processing,
pages 168?175, Sapporo.
Ciaramita, Massimiliano, Vanessa Murdock,
and Vassilis Plachouras. 2008. Semantic
associations for contextual advertising.
Journal of Electronic Commerce
Research?Special Issue on Online
Advertising and Sponsored Search, 9(1):1?15.
Collins, Michael and Nigel Duffy. 2001.
Convolution kernels for natural language.
In Proceedings of the Neural Information
Processing Systems Conference (NIPS),
pages 625?632, Vancouver, Canada.
Cui, Hang, Renxu Sun, Keya Li, Min-Yen
Kan, and Tat-Seng Chua. 2005. Question
answering passage retrieval using
dependency relations. In Proceedings of the
28th Annual International ACM SIGIR
Conference on Research & Development in
Information Retrieval, pages 400?407,
Salvador.
Echihabi, Abdessamad and Daniel Marcu.
2003. A noisy-channel approach to
question answering. In Proceedings of the
41st Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 16?23, Sapporo.
Freund, Yoav and Robert E. Schapire. 1999.
Large margin classification using the
perceptron algorithm.Machine Learning,
37:277?296.
Girju, Roxana. 2003. Automatic detection of
causal relations for question answering. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics
(ACL), Workshop on Multilingual
Summarization and Question Answering,
pages 76?83, Sapporo.
Harabagiu, Sanda, Dan Moldovan, Marius
Pasca, Rada Mihalcea, Mihai Surdeanu,
Razvan Bunescu, Roxana Girju, Vasile Rus,
and Paul Morarescu. 2000. Falcon:
Boosting knowledge for answer engines.
In Proceedings of the Text REtrieval
Conference (TREC), pages 479?487,
Gaithersburg, MD.
Higashinaka, Ryuichiro and Hideki Isozaki.
2008. Corpus-based question answering
for why-questions. In Proceedings of the
Third International Joint Conference on
Natural Language Processing (IJCNLP),
pages 418?425, Hyderabad.
Jeon, Jiwoon, W. Bruce Croft, and
Joon Hoo Lee. 2005. Finding similar
questions in large question and answer
archives. In Proceedings of the ACM
Conference on Information and Knowledge
381
Computational Linguistics Volume 37, Number 2
Management (CIKM), pages 84?90,
Bremen.
Jeon, Jiwoon, W. Bruce Croft, Joon Hoo Lee,
and Soyeon Park. 2006. A framework to
predict the quality of answers with
non-textual features. In Proceedings of the
29th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 228?235,
Seattle, WA.
Joachims, Thorsten. 2006. Training linear
svms in linear time. In KDD ?06:
Proceedings of the 12th ACM SIGKDD
International Conference on Knowledge
Discovery and Data Mining, pages 217?226,
New York, NY.
Ko, Jeongwoo, Teruko Mitamura, and Eric
Nyberg. 2007. Language-independent
probabilistic answer ranking for question
answering. In Proceedings of the 45th
Annual Meeting of the Association for
Computational Linguistics, pages 784?791,
Prague.
Li, Xin and Dan Roth. 2006. Learning
question classifiers: The role of semantic
information. Natural Language Engineering,
12:229?249.
Li, Yaoyong, Hugo Zaragoza, Ralf Herbrich,
John Shawe-Taylor, and Jaz S. Kandola.
2002. The perceptron algorithm with
uneven margins. In Proceedings of the
Nineteenth International Conference on
Machine Learning, pages 379?386,
Sidney.
Magnini, Bernardo, Matteo Negri, Roberto
Prevete, and Hristo Tanev. 2002.
Comparing statistical and content-based
techniques for answer validation on the
web. In Proceedings of the VIII Convegno
AI*IA, Siena, Italy.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
Moldovan, Dan, Sanda Harabagiu, Marius
Pasca, Rada Mihalcea, Richard Goodrum,
Roxana Girju, and Vasile Rus. 1999.
Lasso?a tool for surfing the answer net. In
Proceedings of the Text REtrieval Conference
(TREC), pages 175?183, Gaithersburg, MD.
Moschitti, Alessandro, Silvia Quarteroni,
Roberto Basili, and Suresh Manandhar.
2007. Exploiting syntactic and shallow
semantic kernels for question/answer
classification. In Proceedings of the 45th
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 776?783, Prague.
Murdock, Vanessa and W. Bruce Croft. 2005.
A translation model for sentence retrieval.
In Proceedings of the Conference on Human
Language Technology and Empirical Methods
in Natural Language Processing,
pages 684?691, Vancouver.
Palmer, Martha, Daniel Gildea, and Paul
Kingsbury. 2005. The proposition bank: An
annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106
Punyakanok, Vasin, Dan Roth, and Wen-tau
Yih. 2004. Mapping dependencies trees:
An application to question answering.
Proceedings of AI&Math 2004, pages 1?10,
Fort Lauderdale, FL.
Riezler, Stefan, Alexander Vasserman,
Ioannis Tsochantaridis, Vibhu Mittal,
and Yi Liu. 2007. Statistical machine
translation for query expansion in
answer retrieval. In Proceedings of the
45th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 464?471, Prague.
Robertson, Stephen and Stephen G. Walker.
1997. On relevance weights with little
relevance information. In Proceedings of
the Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 16?24,
New York, NY.
Shen, Dan and Dietrich Klakow. 2006.
Exploring correlation of dependency
relation paths for answer extraction. In
Proceedings of the 21st International
Conference on Computational Linguistics and
the 44th Annual Meeting of the Association for
Computational Linguistics, pages 889?896,
Sydney.
Shen, Libin and Aravind K. Joshi. 2005.
Ranking and reranking with perceptron.
Machine Learning. Special Issue on Learning
in Speech and Language Technologies,
60(1):73?96.
Soricut, Radu and Eric Brill. 2006. Automatic
question answering using the Web:
Beyond the factoid. Journal of Information
Retrieval?Special Issue on Web Information
Retrieval, 9(2):191?206.
Surdeanu, Mihai and Massimiliano
Ciaramita. 2007. Robust information
extraction with perceptrons. In Proceedings
of the NIST 2007 Automatic Content
Extraction Workshop (ACE07), College Park,
MD. Available at: http://www.surdeanu.
name/mihai/papers/ace07a.pdf.
Surdeanu, Mihai, Richard Johansson, Adam
Meyers, Lluis Marquez, and Joakim Nivre.
2008. The CoNLL-2008 shared task on joint
parsing of syntactic and semantic
382
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
dependencies. In Proceedings of the
Conference on Computational Natural
Language Learning (CoNLL), pages 159?177,
Manchester.
Surdeanu, Mihai, Lluis Marquez, Xavier
Carreras, and Pere R. Comas. 2007.
Combination strategies for semantic role
labeling. Journal of Artificial Intelligence
Research, 29:105?151.
Tao, Tao and ChengXiang Zhai. 2007. An
exploration of proximity measures in
information retrieval. In Proceedings of the
30th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 259?302,
Amsterdam.
Tsochantaridis, Ioannis, Thomas Hofmann,
Thorsten Joachims, and Yasemin Altun.
2004. Support vector machine learning for
interdependent and structured output
spaces. In ICML ?04: Proceedings of the
Twenty-First International Conference on
Machine Learning, pages 104?111,
New York, NY.
Verberne, Suzan, Lou Boves, Nelleke
Oostdijk, and Peter-Arno Coppen. 2010.
What is not in the bag of words for
why-qa? Computational Linguistics,
36(2):229?245.
Voorhees, Ellen M. 2001. Overview of the
TREC-9 question answering track. In
Proceedings of the Text REtrieval Conference
(TREC) TREC-9 Proceedings, pages 1?15,
Gaithersburg, MD.
Wang, Mengqiu, Noah A. Smith, and Teruko
Mitamura. 2007. What is the Jeopardy
model? A quasi-synchronous grammar for
QA. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural
Language Processing and Computational
Natural Language Learning, pages 22?32,
Prague.
Xue, Xiaobing, Jiwoon Jeon, and W. Bruce
Croft. 2008. Retrieval models for question
and answer archives. In Proceedings of the
Annual ACM SIGIR Conference on Research
and Development in Information Retrieval,
pages 475?482, Singapore.
383

Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 1?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The CoNLL-2009 Shared Task:
Syntactic and Semantic Dependencies in Multiple Languages
Jan Hajic?? Massimiliano Ciaramita? Richard Johansson? Daisuke Kawahara?
Maria Anto`nia Mart???? Llu??s Ma`rquez?? Adam Meyers?? Joakim Nivre?? Sebastian Pado???
Jan ?Ste?pa?nek? Pavel Stran?a?k? Mihai Surdeanu?? Nianwen Xue?? Yi Zhang??
?: Charles University in Prague, {hajic,stepanek,stranak}@ufal.mff.cuni.cz
?: Google Inc., massi@google.com
?: University of Trento, johansson@disi.unitn.it
?: National Institute of Information and Communications Technology, dk@nict.go.jp
??: University of Barcelona, amarti@ub.edu
??: Technical University of Catalonia, Barcelona, lluism@lsi.upc.edu
??: New York University, meyers@cs.nyu.edu
??: Uppsala University and Va?xjo? University, joakim.nivre@lingfil.uu.se
??: Stuttgart University, pado@ims.uni-stuttgart.de
??: Stanford University, mihais@stanford.edu
??: Brandeis University, xuen@brandeis.edu
??: Saarland University, yzhang@coli.uni-sb.de
Abstract
For the 11th straight year, the Conference
on Computational Natural Language Learn-
ing has been accompanied by a shared task
whose purpose is to promote natural language
processing applications and evaluate them in
a standard setting. In 2009, the shared task
was dedicated to the joint parsing of syntac-
tic and semantic dependencies in multiple lan-
guages. This shared task combines the shared
tasks of the previous five years under a unique
dependency-based formalism similar to the
2008 task. In this paper, we define the shared
task, describe how the data sets were created
and show their quantitative properties, report
the results and summarize the approaches of
the participating systems.
1 Introduction
Every year since 1999, the Conference on Com-
putational Natural Language Learning (CoNLL)
launches a competitive, open ?Shared Task?. A
common (?shared?) task is defined and datasets are
provided for its participants. In 2004 and 2005, the
shared tasks were dedicated to semantic role label-
ing (SRL) in a monolingual setting (English). In
2006 and 2007 the shared tasks were devoted to
the parsing of syntactic dependencies, using corpora
from up to 13 languages. In 2008, the shared task
(Surdeanu et al, 2008) used a unified dependency-
based formalism, which modeled both syntactic de-
pendencies and semantic roles for English. The
CoNLL-2009 Shared Task has built on the 2008 re-
sults by providing data for six more languages (Cata-
lan, Chinese, Czech, German, Japanese and Span-
ish) in addition to the original English1. It has thus
naturally extended the path taken by the five most
recent CoNLL shared tasks.
As in 2008, the CoNLL-2009 shared task com-
bined dependency parsing and the task of identify-
ing and labeling semantic arguments of verbs (and
other parts of speech whenever available). Partici-
pants had to choose from two tasks:
? Joint task (syntactic dependency parsing and
semantic role labeling), or
? SRL-only task (syntactic dependency parses
have been provided by the organizers, using
state-of-the art parsers for the individual lan-
guages).
1There are some format changes and deviations from the
2008 task data specification; see Sect. 2.3
1
In contrast to the previous year, the evaluation data
indicated which words were to be dealt with (for the
SRL task). In other words, (predicate) disambigua-
tion was still part of the task, whereas the identi-
fication of argument-bearing words was not. This
decision was made to compensate for the significant
differences between languages and between the an-
notation schemes used.
The ?closed? and ?open? challenges have been
kept from last year as well; participants could have
chosen one or both. In the closed challenge, systems
had to be trained strictly with information contained
in the given training corpus; in the open challenge,
systems could have been developed making use of
any kind of external tools and resources.
This paper is organized as follows. Section 2 de-
fines the task, including the format of the data, the
evaluation metrics, and the two challenges. A sub-
stantial portion of the paper (Section 3) is devoted
to the description of the conversion and develop-
ment of the data sets in the additional languages.
Section 4 shows the main results of the submitted
systems in the Joint and SRL-only tasks. Section 5
summarizes the approaches implemented by partic-
ipants. Section 6 concludes the paper. In all sec-
tions, we will mention some of the differences be-
tween last year?s and this year?s tasks while keeping
the text self-contained whenever possible; for details
and observations on the English data, please refer to
the overview paper of the CoNLL-2008 Shared Task
(Surdeanu et al, 2008) and to the references men-
tioned in the sections describing the other languages.
2 Task Definition
In this section we provide the definition of the shared
task; after introducing the two challenges and the
two tasks the participants were to choose, we con-
tinue with the format of the shared task data, fol-
lowed by a description of the evaluation metrics
used.
For three of the languages (Czech, English and
German), out-of-domain data (OOD) have also been
prepared for the final evaluation, following the same
guidelines and formats.
2.1 Closed and Open Challenges
Similarly to the CoNLL-2005 and CoNLL-2008
shared tasks, this shared task evaluation is separated
into two challenges:
Closed Challenge The aim of this challenge was to
compare performance of the participating systems in
a fair environment. Systems had to be built strictly
with information contained in the given training cor-
pus, and tuned with the development section. In
addition, the lexical frame files (such as the Prop-
Bank and NomBank for English, the valency dictio-
nary PDT-Vallex for Czech etc.) were provided and
may have been used. These restrictions mean that
outside parsers (not trained by the participants? sys-
tems) could not be used. However, we did provide
the output of a single, state-of-the-art dependency
parser for each language so that participants could
build a SRL-only system (using the provided parses
as inputs) within the closed challenge (as opposed to
the 2008 shared task).
Open Challenge Systems could have been devel-
oped making use of any kind of external tools and
resources. The only condition was that such tools or
resources must not have been developed with the an-
notations of the test set, both for the input and output
annotations of the data. In this challenge, we were
interested in learning methods which make use of
any tools or resources that might improve the per-
formance. The comparison of different systems in
this setting may not be fair, and thus ranking of sys-
tems is not necessarily important.
2.2 Joint and SRL-only tasks
In 2008, systems participating in the open challenge
could have used state-of-the-art parsers for the syn-
tactic dependency part of the task. This year, we
have provided the output of these parsers for all the
languages in an uniform way, thus allowing an or-
thogonal combination of the two tasks and the two
challenges. For the SRL-only task, participants in
the closed challenge simply had to use the provided
parses only.
Despite the provisions for the SRL-only task, we
are more interested in the approaches and results of
the Joint task. Therefore, primary system ranking is
provided for the Joint task while additional measures
2
are computed for various combinations of parsers
and SRL methods across the tasks and challenges.
2.3 Data Format
The data format used in this shared task has been
based on the CoNLL-2008 shared task, with some
differences. The data follows these general rules:
? The files contain sentences separated by a blank
line.
? A sentence consists of one or more tokens and
the information for each token is represented on
a separate line.
? A token consists of at least 14 fields. The fields
are separated by one or more whitespace char-
acters (spaces or tabs). Whitespace characters
are not allowed within fields.
The data is thus a large table with whitespace-
separated fields (columns). The fields provided in
the data are described in Table 1. They are identical
for all languages, but they may differ in contents;
for example, some fields might not be filled for all
the languages provided (such as the FEAT or PFEAT
fields).
For the SRL-only task, participants have been
provided will all the data but the PRED and
APREDs, which they were supposed to fill in with
their correct values. However, they did not have
to determine which tokens are predicates (or more
precisely, which are the argument-bearing tokens),
since they were marked by ?Y? in the FILLPRED
field.
For the Joint task, participants could not (in ad-
dition to the PRED and APREDs) see the gold-
standard nor the predicted syntactic dependencies
(HEAD, PHEAD) and their labels (DEPREL, PDE-
PREL). These syntactic dependencies were also to
be filled by participants? systems.
In both tasks, participants have been free to
use any other data (columns) provided, except the
LEMMA, POS and FEAT columns (to get more ?re-
alistic? results using only their automatically pre-
dicted variants PLEMMA, PPOS and PFEAT).
Besides the corpus proper, predicate dictionaries
have been provided to participants in order to be able
to properly match the predicates to the tokens in the
corpus; their contents could have been used e.g. as
features for the PRED/APREDs predictions (or even
for the syntactic dependencies, i.e., for filling in the
PHEAD and PDEPREL fields).
The system of filling-in the APREDs follows
the 2008 pattern; for each argument-bearing token
(predicate), a new APREDn column is created in the
order in which the predicate token is encountered
within the sentence (i.e., based on its ID seen as a
numerical value). Then, for each token in the sen-
tence, the value in the intersection of the APREDn
column and the token row is either left unfilled
(if the token is not an argument), or a predicate-
argument label(s) is(are) filled in.
The differences between the English-only 2008
task and this year?s multilingual task can be briefly
summarized as follows:
? only ?split?2 lemmas and forms have been pro-
vided in the English datasets (for the other lan-
guages, original tokenization from the respec-
tive treebanks has been used);
? rich morphological features have been added
wherever available;
? syntactic dependencies by state-of-the-art
parsers have been provided (for the SRL-only
task);
? multiple semantic labels for a single token have
been allowed (and properly evaluated) in the
APREDs columns;
? predicates have been pre-identified and marked
in both the training and test data;
? some of the fields (e.g. the APREDx) and val-
ues (ARG0? A0 etc.) have been renamed.
2.4 Evaluation Measures
It was required that participants submit results in all
seven languages in the chosen task and in any of (or
both) the challenges. Submission of out-of-domain
data files has been optional.
The main evaluation measure, according to which
systems are primarily compared, is the Joint task,
2Splitting of forms and lemmas in English has been intro-
duced in the 2008 shared task to match the tokenization con-
vention for the arguments in NomBank.
3
Field # Name Description
1 ID Token counter, starting at 1 for each new sentence
2 FORM Form or punctuation symbol (the token; ?split? for English)
3 LEMMA Gold-standard lemma of FORM
4 PLEMMA Automatically predicted lemma of FORM
5 POS Gold-standard POS (major POS only)
6 PPOS Automatically predicted major POS by a language-specific tagger
7 FEAT Gold-standard morphological features (if applicable)
8 PFEAT Automatically predicted morphological features (if applicable)
9 HEAD Gold-standard syntactic head of the current token (ID or 0 if root)
10 PHEAD Automatically predicted syntactic head
11 DEPREL Gold-standard syntactic dependency relation (to HEAD)
12 PDEPREL Automatically predicted dependency relation to PHEAD
13 FILLPRED Contains ?Y? for argument-bearing tokens
14 PRED (sense) identifier of a semantic ?predicate? coming from a current token
15... APREDn Columns with argument labels for each semantic predicate (in the ID order)
Table 1: Description of the fields (columns) in the data provided. The values of columns 9, 11 and 14 and above are
not provided in the evaluation data; for the Joint task, columns 9?12 are also empty in the evaluation data.
closed challenge, Macro F1 score. However, scores
can also be computed for a number of other condi-
tions:
? Task: Joint or SRL-only
? Challenge: open or closed
? Domain: in-domain data (IDD, separated from
training corpus) or out-of-domain data (OOD)
Joint task participants are also evaluated separately
on the syntactic dependency task (labeled attach-
ment score, LAS). Finally, systems competing in
both tasks are compared on semantic role labeling
alone, to assess the impact of the the joint pars-
ing/SRL task compared to an SRL-only task on pre-
parsed data.
Finally, as an explanatory measure, precision and
recall of the semantic labeling task have been com-
puted and tabulated.
We have decided to omit several evaluation fig-
ures that were reported in previous years, such as the
percentage of completely correct sentences (?Exact
Match?), unlabeled scores, etc. With seven lan-
guages, two tasks (plus two challenges, and the
IDD/OOD distinction), there are enough results to
get lost even as it is.
2.4.1 Syntactic Dependency Measures
The LAS score is defined similarly as in the pre-
vious shared tasks, as the percentage of tokens for
which a system has predicted the correct HEAD and
DEPREL columns. The unlabeled attachment score
(UAS), i.e., the percentage of tokens with correct
HEAD regardless if the DEPREL is correct, has not
been officially computed this year. No precision and
recall measures are applicable, since all systems are
supposed to output a single dependency with a single
label (see also below the footnote to the description
of the combined score).
2.4.2 Semantic Labeling Measures
The semantic propositions are evaluated by con-
verting them to semantic dependencies, i.e., we cre-
ate n semantic dependencies from every predicate
to its n arguments. These dependencies are labeled
with the labels of the corresponding arguments. Ad-
ditionally, we create a semantic dependency from
each predicate to a virtual ROOT node. The latter
dependencies are labeled with the predicate senses.
This approach guarantees that the semantic depen-
dency structure conceptually forms a single-rooted,
connected (but not necessarily acyclic) graph. More
importantly, this scoring strategy implies that if a
system assigns the incorrect predicate sense, it still
receives some points for the arguments correctly as-
signed. For example, for the correct proposition:
verb.01: A0, A1, AM-TMP
the system that generates the following output for
the same argument tokens:
4
verb.02: A0, A1, AM-LOC
receives a labeled precision score of 2/4 because two
out of four semantic dependencies are incorrect: the
dependency to ROOT is labeled 02 instead of 01
and the dependency to the AM-TMP is incorrectly la-
beled AM-LOC. Using this strategy we compute pre-
cision, recall, and F1 scores for semantic dependen-
cies (labeled only).
For some languages (Czech, Japanese) there may
be more than one label in a given argument position;
for example, this happens in Czech in special cases
of reciprocity when the same token serves as two or
more arguments to the same predicate. The scorer
takes this into account and considers such cases to
be (as if) multiple predicate-argument relations for
the computation of the evaluation measures.
For example, for the correct proposition:
v1f1: ACT|EFF, ADDR
the system that generates the following output for
the same argument tokens:
v1f1: ACT, ADDR|PAT
receives a labeled precision score of 3/4 because
the PAT is incorrect and labeled recall 3/4 be-
cause the EFF is missing (should the ACT|EFF and
ADDR|PAT be taken as atomic values, the scores
would then be zero).
2.4.3 Combined Syntactic and Semantic Score
We combine the syntactic and semantic measures
into one global measure using macro averaging. We
compute macro precision and recall scores by aver-
aging the labeled precision and recall for semantic
dependencies with the LAS for syntactic dependen-
cies:3
LMP = Wsem ? LPsem + (1?Wsem) ? LAS (1)
LMR = Wsem ? LRsem + (1 ?Wsem) ? LAS (2)
where LMP is the labeled macro precision and
LPsem is the labeled precision for semantic depen-
dencies. Similarly, LMR is the labeled macro re-
call and LRsem is the labeled recall for semantic
dependencies. Wsem is the weight assigned to the
3We can do this because the LAS for syntactic dependen-
cies is a special case of precision and recall, where the predicted
number of dependencies is equal to the number of gold depen-
dencies.
semantic task.4 The macro labeled F1 score, which
was used for the ranking of the participating sys-
tems, is computed as the harmonic mean of LMP
and LMR.
3 Data
The unification of the data formats for the various
languages appeared to be a challenge in itself. We
will briefly describe the processes of the conversion
of the existing treebanks in the seven languages of
the CoNLL-2009 shared task. In many instances,
the original treebanks had to be not only converted
format-wise, but also merged with other resources in
order to generate useful training and testing data that
fit the task description.
3.1 The Input Corpora
The data used as the input for the transformations
aimed at arriving at the data contents and format de-
scribed in Sect. 2.3 are described in (Taule? et al,
2008), (Xue and Palmer, 2009), (Hajic? et al, 2006),
(Surdeanu et al, 2008), (Burchardt et al, 2006) and
(Kawahara et al, 2002).
In the subsequent sections, the procedures for the
data conversion for the individual languages are de-
scribed. The data has been collected by the main
organization site and checked for format errors, and
repackaged for distribution.
There were three packages of the data distributed
to the participants: Trial, Training plus Develop-
ment, and Evaluation. The Trial data were rather
small, just to give the feeling of the format and
languages involved. A visual representation of the
Trial data was also created to make understanding
of the data easier. Any data in the same format
can be transformed and displayed in the Tree Editor
TrEd5 (Pajas and ?Ste?pa?nek, 2008) with the CoNLL
2009 Shared Task extension that can be installed
from within the editor. A sample visualization of an
English sentence after its conversion to the shared
task format (Sect. 2.3) is in Fig. 1.
Due to licensing requirements, every package of
the data had to be split into two portions. One
portion (Catalan, German, Japanese, and Spanish
data) was published on the task?s webpage for down-
4We assign equal weight to the two tasks, i.e., Wsem = 0.5.
5http://ufal.mff.cuni.cz/?pajas/tred
5
$QG
'(3 &&
VRPHWLPHV
703 5%
D
102' '7
UHSXWDEOH
102' --
FKDULW\
6%- 11
ZLWK
102' ,1
D
102' '7
KRXVHKROG
102' 11
QDPH QDPH
302' 11
JHWV JHW
5227 9%=
XVHG XVH
9& 9%1
DQG
&225' &&
GRHV
&21- 9%=
Q
W
$'9 5%
HYHQ
$'9 5%
NQRZ NQRZ
9& 9%
LW
2%- 353

3  
$0703$0703$0703
 

$$$$

 
 

$
 

 

$



$01(*

$0$'9
 


$

Figure 1: Visualisation of the English sentence ?And sometimes a reputable charity with a houshold name gets used
and doesn?t even know it.? (Penn Treebank, wsj 0559) showing jointly the labeled syntactic and semantic depen-
dencies. The basic tree shape comes from the syntactic dependencies; syntactic labels and POS tags are on the 2nd
line at each node. Semantic dependencies which do not follow the syntactic ones use dotted lines. Predicate senses
in parentheses (use:01, ...) follow the word label. SRLs (A0, AM-TMP, ...) are on the last line. Please note that
multiple semantic dependencies (e.g., there are four for charity: A0? know, A1? gets, A1? used, A1? name)
and self-dependencies (name) appear in this sentence.
load, the other portion (Czech, English, and Chinese
data) was invoiced and distributed by the Linguistic
Data Consortium under a special agreement free of
charge.
Distribution of the Evaluation package was a bit
more complicated, because there were two types of
the packages - one for the Joint task and one for the
SRL-only task. Every participant had to subscribe
to one of the two tasks; subsequently, they obtained
the appropriate data (again, from the webpage and
LDC).
Prior to release, each data file was checked to
eliminate errors. The following test were carried
out:
? For every sentence, number of PREDs rows
matches the number of APREDs columns.
? The first line of each file is never empty, while
the last line always is.
? The first character on a non-empty line is al-
ways a digit, the last one is never a whitespace.
? The number of empty lines (i.e. the number
of sentences) equals the number of lines begin-
ning with ?1?.
? The data contain no spaces nor double tabs.
Some statistics on the data can be seen in Ta-
bles 2, 3 and 4. Whereas the training sizes of the
data have not been that different as they were e.g.
for the 2007 shared task on multilingual dependency
parsing (Nivre et al, 2007)6, substantial differences
existed in the distribution of the predicates and ar-
guments, the input features, the out-of-vocabulary
rates, and other statistical characteristics of the data.
Data sizes have been relatively uniform in all the
datasets, with Japanese having the smallest dataset
6http://nextens.uvt.nl/depparse-wiki/
DataOverview
6
containing data for SRL annotation training. To
compensate at least for the dependency parsing part,
an additional, large Japanese corpus with syntactic
dependency annotation has been provided.
The average sentence length, the vocabulary sizes
for FORM and LEMMA fields and the OOV rates
characterize quite naturally the properties of the re-
spective languages (in the domain of the training and
evaluation data). It is no surprise that the FORM
OOV rate is the highest for Czech, a highly inflec-
tional language, and that the LEMMA OOV rate is
the highest for German (as a consequence of keeping
compounds as a single lemma). The other statistics
also reflect (to a large extent) the annotation speci-
fication and conventions used for the original tree-
banks and/or the result of the conversion process to
the unified CoNLL-2009 Shared Task format.
Starting with the POS and FEAT fields, it can be
seen that Catalan, Czech and Spanish use only the
12 major part-of-speech categories as values of the
POS field (with richly populated FEAT field); En-
glish and Chinese are the opposite extreme, disre-
garding the use of the FEAT field completely and
coding everything as a POS value. While for Chi-
nese this is quite understandable, English follows the
PTB tradition in this respect. German and Japanese
use relatively rich set of values in both the POS and
FEAT fields.
For the dependency relations (DEPREL), all
the languages use a similarly-sized set except for
Japanese, which only encodes the distinction be-
tween a root and a dependent node (and some in-
frequent special ones).
Evaluation data are over 10% of the size of the
training data for Catalan, Chinese, Czech, Japanese
and Spanish and roughly 5% for English and Ger-
man.
Table 3 shows the distribution of the five most fre-
quent dependency relations (determined as part of
the subtask of syntactic parsing). With the exception
of Japanese, which essentially does not label depen-
dency relations at this level, all the other languages
show little difference in this distribution. For exam-
ple, the unconditioned probability of ?subjects? is
almost the same for all the six other languages (be-
tween 6 and 8 percent). The probability mass cov-
ered by the first five most frequent DEPRELs is also
almost the same (again, except for Japanese), sug-
gesting that the labeling task might have similar dif-
ficulty7. The most skewed one is for Czech (after
Japanese).
Table 4 shows similar statistics for the argument
labels (PRED/APREDs); it also adds the average
number of arguments per ?predicate? token, since
this is part of the SRL task8. It is apparent from the
comparison of the ?Total? rows in this table and Ta-
ble 3 that the first five argument labels cover more
that their syntactic counterparts. For example, the
arguments A0-A4 account for all but 3% of all ar-
guments labels, whereas Spanish and Catalan have
much more rich set of argument labels, with a high
entropy of the most-frequent-label distribution.
3.2 Catalan and Spanish
The Catalan and Spanish datasets (Taule? et al, 2008)
were generated from the AnCora corpora9 through
an automatic conversion process from a constituent-
based formalism to dependencies (Civit et al, 2006).
AnCora corpora contain about half million words
for Catalan and Spanish annotated with syntactic
and semantic information. Text sources for the Cata-
lan corpus are EFE news agency (?75Kw), ACN
Catalan news agency (?225Kw), and ?El Perio?dico?
newspaper (?200Kw). The Spanish corpus comes
from the Lexesp Spanish balanced corpus (?75Kw),
the EFE Spanish news agency (?225Kw), and the
Spanish version of ?El Perio?dico? (?200Kw). The
subset from ?El Perio?dico? corresponds to the same
news in Catalan and Spanish, spanning from January
to December 2000.
Linguistic annotation is the same in both lan-
guages and includes: PoS tags with morphologi-
cal features (gender, number, person, etc.), lemma-
tization, syntactic dependencies (syntactic func-
tions), semantic dependencies (arguments and the-
matic roles), named entities and predicate semantic
classes (Lexical Semantic Structure, LSS). Tag sets
are shared by the two languages.
If we take into account the complete PoS tags,
7Yes, this is overgeneralization since this distribution does
not condition on the features, dependencies etc. But as a rough
measure, it often correlates well with the results.
8A number below 1 means there are some argument-bearing
words (often nouns) which have no arguments in the particular
sentence in which they appear.
9http://clic.ub.edu/ancora
7
Characteristic Catalan Chinese Czech English German Japanese Spanish
Training data size (sentences) 13200 22277 38727 39279 36020 4393a 14329
Training data size (tokens) 390302 609060 652544 958167 648677 112555a 427442
Avg. sentence length (tokens) 29.6 27.3 16.8 24.4 18.0 25.6 29.8
Tokens with argumentsb (%) 9.6 16.9 63.5 18.7 2.7 22.8 10.3
DEPREL types 50 41 49 69 46 5 49
POS types 12 41 12 48 56 40 12
FEAT types 237 1 1811 1 267 302 264
FORM vocabulary size 33890 40878 86332 39782 72084 36043 40964
LEMMA vocabulary size 24143 40878 37580 28376 51993 30402 26926
Evaluation data size (sent.) 1862 2556 4213 2399 2000 500 1725
Evaluation data size (tokens) 53355 73153 70348 57676 31622 13615 50630
Evaluation FORM OOVc 5.40 3.92 7.98/8.62d 1.58/3.76d 7.93/7.57d 6.07 5.63
Evaluation LEMMA OOVc 4.14 3.92 3.03/4.29d 1.08/2.30d 5.83/7.36d 5.21 3.69
Table 2: Elementary data statistics for the CoNLL-2009 Shared Task languages. The data themselves, the original
treebanks they were derived from and the conversion process are described in more detail in sections 3.2-3.7. All
evaluation data statistics are derived from the in-domain evaluation data.
aThere were additional 33257 sentences (839947 tokens) available for syntactic dependency parsing of Japanese; the type and
vocabulary statistics are computed using this larger dataset.
bPercentage of tokens with FILLPRED=?Y?.
cPercentage of FORM/LEMMA tokens not found in the respective vocabularies derived solely from the training data.
dOOV percentage for in-domain/out-of-domain data.
DEPREL Catalan Chinese Czech English German Japanese Spanish
sn 0.16 COMP 0.21 Atr 0.26 NMOD 0.27 NK 0.31 D 0.93 sn 0.16
spec 0.15 NMOD 0.14 AuxP 0.10 P 0.11 PUNC 0.14 ROOT 0.04 spec 0.15
Labels f 0.11 ADV 0.10 Adv 0.10 PMOD 0.10 MO 0.12 P 0.03 f 0.12
sp 0.09 UNK 0.09 Obj 0.07 SBJ 0.07 SB 0.07 A 0.00 sp 0.08
suj 0.07 SBJ 0.08 Sb 0.06 OBJ 0.06 ROOT 0.06 I 0.00 suj 0.08
Total 0.58 0.62 0.59 0.61 0.70 1.00 0.59
Table 3: Unigram probability for the five most frequent DEPREL labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five dependency labels shown.
APRED Catalan Chinese Czech English German Japanese Spanish
arg1-pat 0.22 A1 0.30 RSTR 0.30 A1 0.37 A0 0.40 GA 0.33 arg1-pat 0.20
arg0-agt 0.18 A0 0.27 PAT 0.18 A0 0.25 A1 0.39 WO 0.15 arg0-agt 0.19
Labels arg1-tem 0.15 ADV 0.20 ACT 0.17 A2 0.12 A2 0.12 NO 0.15 arg1-tem 0.15
argM-tmp 0.08 TMP 0.07 APP 0.06 AM-TMP 0.06 A3 0.06 NI 0.09 arg2-atr 0.08
arg2-atr 0.08 DIS 0.04 LOC 0.04 AM-MNR 0.03 A4 0.01 DE 0.06 argM-tmp 0.08
Total 0.71 0.91 0.75 0.83 0.97 0.78 0.70
Avg. 2.25 2.26 0.88 2.20 1.97 1.71 2.26
Table 4: Unigram probability for the five most frequent APRED labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five argument labels shown. The ?Avg.? line
shows the average number of arguments per predicate or other argument-bearing token (i.e. for those marked by
FILLPRED=?Y?).
8
AnCora has 280 different labels. Considering only
the main syntactic categories, the tag set is reduced
to 47 tags. The syntactic tag set consists of 50 dif-
ferent syntactic functions. Regarding semantic ar-
guments, we distinguish Arg0, Arg1, Arg2, Arg3,
Arg4, ArgM, and ArgL. The first five tags are num-
bered from less to more obliqueness with respect
to the verb, ArgM corresponds to adjuncts. The
list of thematic roles consists of 20 different labels:
AGT (Agent), AGI (Induced Agent), CAU (Cause),
EXP (Experiencer), SCR (Source), PAT (Patient),
TEM (Theme), ATR (Attribute), BEN (Beneficiary),
EXT (Extension), INS (Instrument), LOC (Loca-
tive), TMP (Time), MNR (Manner), ORI (Origin),
DES (Goal), FIN (Purpose), EIN (Initial State), EFI
(Final State), and ADV (Adverbial). Each argument
position can map onto specific thematic roles. By
way of example, Arg1 can be PAT, TEM or EXT. For
Named Entities, we distinguish six types: Organiza-
tion, Person, Location, Date, Number, and Others.
An incremental process guided the annotation of
AnCora, since semantics depends on morphosyntax,
and syntax relies on morphology. This procedure
made it possible to check, correct, and complete
the previous annotations, thus guaranteeing the final
quality of the corpora and minimizing the error rate.
The annotation process was carried out sequentially
from lower to upper layers of linguistic description.
All resulting layers are independent of each other,
thus making easier the data management. The ini-
tial annotation was performed manually for syntax,
semiautomatically in the case of arguments and the-
matic roles, and fully automatically for PoS (Mart??
et al, 2007; Ma`rquez et al, 2007).
The Catalan and Spanish AnCora corpora were
straightforwardly translated into the CoNLL-2009
shared task formatting (information about named
entities was skipped in this process). The resulting
Catalan corpus (including training, development and
test partitions) contains 16,786 sentences with an av-
erage length of 29.59 lexical tokens per sentence.
Long sentences abound in this corpus. For instance,
10.73% of the sentences are longer than 50 tokens,
and 4.42% are longer than 60. The corpus con-
tains 47,537 annotated predicates (2.83 predicates
per sentence, on average) with 107,171 arguments
(2.25 arguments per predicate, on average). From
the latter, 73.89% correspond to core arguments and
26.11% to adjuncts. Numbers for the Spanish cor-
pus are comparable in all aspects: 17,709 sentences
with 29.84 lexical tokens on average (11.58% of the
sentences longer than 50 tokens, 4.07% longer than
60); 54,075 predicates (3.05 per sentence, on aver-
age) and 122,478 arguments (2.26 per predicate, on
average); 73.34% core arguments and 26.66% ad-
juncts.
The following are important features of the Cata-
lan and Spanish corpora in the CoNLL-2009 shared
task setting: (1) all dependency trees are projective;
(2) no word can be the argument of more than one
predicate in a sentence; (3) semantic dependencies
completely match syntactic dependency structures
(i.e., no new edges are introduced by the semantic
structure); (4) only verbal predicates are annotated
(with exceptional cases referring to words that can
be adjectives and past participles); (5) the corpus is
segmented so multi-words, named entities, temporal
expressions, compounds, etc. are grouped together;
and (6) segmentation also accounts for elliptical pro-
nouns (there are marked as empty lexical tokens ?_?
with a pronoun POS tag).
Finally, the predicted columns (PLEMMA,
PPOS, and PFEAT) have been generated with the
FreeLing Open source suite of Language Analyz-
ers10. Accuracy in PLEMMA and PPOS columns
is above 95% for the two languages. PHEAD
and PDEPREL columns have been generated using
MaltParser11. Parsing accuracy (LAS) is above 86%
for the the two languages.
3.3 Chinese
The Chinese Corpus for the 2009 CoNLL Shared
Task was generated by merging the Chinese Tree-
bank (Xue et al, 2005) and the Chinese Proposition
Bank (Xue and Palmer, 2009) and then converting
the constituent structure to a dependency formalism
as specified in the CoNLL Shared Task. The Chi-
nese data used in the shared task is based on Chinese
Treebank 6.0 and the Chinese Proposition Bank 2.0,
both of which are publicly available via the Linguis-
tic Data Consortium.
The Chinese Treebank Project originated at Penn
and was later moved to University of Colorado at
10http://www.lsi.upc.es/?nlp/freeling
11http://w3.msi.vxu.se/?jha/maltparser
9
Boulder. Now it is the process of being to moved
to Brandeis University. The data sources of the Chi-
nese Treebank range from Xinhua newswire (main-
land China), Hong Kong news, and Sinorama Maga-
zine (Taiwan). More recently under DARPA GALE
funding it has been expanded to include broadcast
news, broadcast conversation, news groups and web
log data. It currently has over one million words
and is fully segmented, POS-tagged and annotated
with phrase structure. The version of the Chinese
Treebank used in this shared task, CTB 6.0, includes
newswire, magazine articles, and transcribed broad-
cast news 12. The training set has 609,060 tokens,
the development set has 49,620 tokens, and the test
set has 73,153 tokens.
The Chinese Proposition Bank adds a layer of se-
mantic annotation to the syntactic parses in the Chi-
nese Treebank. This layer of semantic annotation
mainly deals with the predicate-argument structure
of Chinese verbs and their nominalizations. Each
major sense (called frameset) of a predicate takes a
number of core arguments annotated with numeri-
cal labels Arg0 through Arg5 which are defined in
a predicate-specific manner. The Chinese Proposi-
tion Bank also annotates adjunctive arguments such
as locative, temporal and manner modifiers of the
predicate. The version of the Chinese Propbank used
in this CoNLL Shared Task is CPB 2.0, but nominal
predicates are excluded because the annotation is in-
complete.
Since the Chinese Treebank is annotated with
constituent structures, the conversion and merging
procedure converts the constituent structures to de-
pendencies by identifying the head for each con-
stituent in a parse tree and making its sisters its de-
pendents. The Chinese Propbank pointers are then
shifted from the entire constituent to the head of that
constituent. The conversion procedure identifies the
head by first exploiting the structural information
in the syntactic parse and detecting six broad cate-
gories of syntactic relations that hold between the
head and its dependents (predication, modification,
complementation, coordination, auxiliary, and flat)
and then designating the head based on these rela-
tions. In particular, the first conjunct of a coordina-
12A small number of files were taken out of the CoNLL
shared task data due to conversion problems and time con-
straints to fix them.
tion structure is designated as the head and the heads
of the other conjuncts are the conjunctions preced-
ing them. The conjunctions all ?modify? the first
conjunct.
3.4 Czech
For the training, development and evaluation data,
Prague Dependency Treebank 2.0 was used (Hajic?
et al, 2006). For the out-of-domain evaluation data,
part of the Czech side of the Prague Czech-English
Dependency Treebank (version 2, under construc-
tion) was used13, see also ( ?Cmejrek et al, 2004). For
the OOD data, no manual annotation of LEMMA,
POS, and FEAT existed, so the predicted values
were used. The same conversion procedure has been
applied to both sources.
The FORM column was created from the form
element of the morphological layer, not from the
?token? from the word-form layer. Therefore, most
typos, errors in word segmentation and tokenization
are corrected and numerals are normalized.
The LEMMA column was created from the
lemma element of the morphological layer. Only
the initial string of the element was used, so there is
no distinction between homonyms. However, some
components of the detailed lemma explanation were
incorporated into the FEAT column (see below).
The POS column was created form the morpho-
logical tag element, its first character more pre-
cisely.
The FEAT column was created from the remain-
ing characters of the tag element. In addition, the
special feature ?Sem? corresponds to a semantic fea-
ture of the lemma.
For the HEAD and DEPREL columns, the PDT
analytical layer was used. The DEPREL was taken
from the analytic function (the afun node at-
tribtue). There are 27 possible values for afun el-
ement: Pred, Pnom, AuxV, Sb, Obj, Atr, Adv,
Atv, AtvV, Coord, Apos, ExD, and a number
of auxiliary and ?double-function? labels. The first
nine of these are the ?most interesting? from the
point of view of the shared task, since they relate to
semantics more closely than the rest (at least from
the linguistic point of view). The HEAD is a pointer
to its parent, which means the PDT?s ord attribute
13http://ufal.mff.cuni.cz/pedt
10
(within-sentence ID / word position number) of the
parent. If a node is a member of a coordination
or apposition (is_member element), its DEPREL
obtains the _M suffix. The parenthesis annotation
(is_parenthesis_root element) was ignored.
The PRED and APREDs columns were created
from the tectogrammatical layer of PDT 2.0 and the
valency lexicon PDT-Vallex according to the follow-
ing rules:
? Every line corresponding to an analytical node
referenced by a lexical reference (a/lex.rf)
from the tectogrammatical layer has a PRED
value filled. If the referring non-generated
tectogrammatical node (is_generated not
equal to 1) has a valency frame assigned
(val_frame.rf), the value of PRED is the
identifier of the frame. Otherwise, it is set to
the same value as the LEMMA column.
? For every tectogrammatical node, a corre-
sponding analytical node is searched for:
1. If the tectogrammatical node is not
generated and has a lexical reference
(a/lex.rf), the referenced node is
taken.
2. Otherwise, if the tectogrammatical node
has a coreference (coref_text.rf or
coref_gram.rf) or complement refer-
ence (compl.rf) to a node that has an
analytical node assigned (by 1. or 2.), the
assigned node is taken.
APRED columns are filled with respect to the
following correspondence: for a tectogrammatical
node P and its effective child C with functor F, the
column for P?s corresponding analytical node at the
row for C?s corresponding analytical node is filled
with F. Some nodes can thus have several functors
in one APRED column, separated by a vertical bar
(see Sect. 2.4.2).
PLEMMA, PPOS and PFEAT were gener-
ated by the (cross-trained) morphological tagger
MORCE (Spoustova? et al, 2009), which gives full
combined accuracy (PLEMMA+PPOS+PFEAT)
slightly under 96%.
PHEAD and PDEPREL were generated by
the (cross-trained) MST parser for Czech (Chu?
Liu/Edmonds algorithm, (McDonald et al, 2005)),
which has typical dependency accuracy around
85%.
The valency lexicon, converted from (Hajic? et al,
2003), has four columns:
1. lemma (can occur several times in the lexicon,
with different frames)
2. frame identifier (as found in the PRED column)
3. list of space-separated actants and obligatory
members of the frame
4. example(s)
The source of the out-of-domain data uses an
extended valency lexicon (because of out-of-
vocabulary entries). For simplicity, the extended
lexicon was not provided; instead, such words were
not marked as predicates in the OOD data (their
FILLPRED was set to ?_?) and thus not evaluated.
3.5 English
The English corpus is almost identical to the cor-
pus used in the closed challenge in the CoNLL-2008
shared task evaluation (Surdeanu et al, 2008). This
corpus was generated through a process that merges
several input corpora and converts them from the
constituent-based formalism to dependencies. The
following corpora were used as input to the merging
procedure:
? Penn Treebank 3 ? The Penn Treebank 3 cor-
pus (Marcus et al, 1994) consists of hand-
coded parses of the Wall Street Journal (test,
development and training) and a small subset
of the Brown corpus (W. N. Francis and H.
Kucera, 1964) (test only).
? BBN Pronoun Coreference and Entity Type
Corpus ? BBN?s NE annotation of the Wall
Street Journal corpus (Weischedel and Brun-
stein, 2005) takes the form of SGML inline
markup of text, tokenized to be completely
compatible with the Penn Treebank annotation.
For the CoNLL-2008 shared task evaluation,
this corpus was extended by the task organizers
to cover the subset of the Brown corpus used as
a secondary testing dataset. From this corpus
we only used NE boundaries to derive NAME
11
dependencies between NE tokens, e.g., we cre-
ate a NAME dependency from Mary to Smith
given the NE mention Mary Smith.
? Proposition Bank I (PropBank) ? The Prop-
Bank annotation (Palmer et al, 2005) classifies
the arguments of all the main verbs in the Penn
Treebank corpus, other than be. Arguments are
numbered (Arg0, Arg1, . . .) based on lexical
entries or frame files. Different sets of argu-
ments are assumed for different rolesets. De-
pendent constituents that fall into categories in-
dependent of the lexical entries are classified as
various types of adjuncts (ArgM-TMP, -ADV,
etc.).
? NomBank ? NomBank annotation (Meyers et
al., 2004) uses essentially the same framework
as PropBank to annotate arguments of nouns.
Differences between PropBank and NomBank
stem from differences between noun and verb
argument structure; differences in treatment of
nouns and verbs in the Penn Treebank; and dif-
ferences in the sophistication of previous re-
search about noun and verb argument structure.
Only the subset of nouns that take arguments
are annotated in NomBank and only a subset of
the non-argument siblings of nouns are marked
as ArgM.
The complete merging process and the conversion
from the constituent representation to dependencies
is detailed in (Surdeanu et al, 2008).
The main difference between the 2008 and 2009
version of the corpora is the generation of word lem-
mas. In the 2008 version the only lemmas pro-
vided were predicted using the built-in lemmatizer
in WordNet (Fellbaum, 1998) based on the most fre-
quent sense for the form and the predicted part-of-
speech tag. These lemmas are listed in the 2009
corpus under the PLEMMA column. The LEMMA
column in the 2009 version of the corpus contains
lemmas generated using the same algorithm but us-
ing the correct Treebank part-of-speech tags. Addi-
tionally, the PHEAD and PDEPREL columns were
generated using MaltParser14, similarly to the open
challenge corpus in the CoNLL 2008 shared task.
14http://w3.msi.vxu.se/?nivre/research/
MaltParser.html
3.6 German
The German in-domain dataset is based on the an-
notated verb instances of the SALSA corpus (Bur-
chardt et al, 2006), a total of around 40k sen-
tences15. SALSA provides manual semantic role
annotation on top of the syntactically annotated
TIGER newspaper corpus, one of the standard Ger-
man treebanks. The original SALSA corpus uses se-
mantic roles in the FrameNet paradigm. We con-
structed mappings between FrameNet frame ele-
ments and PropBank argument positions at the level
of frame-predicate pairs semi-automatically. For the
frame elements of each frame-predicate pair, we first
identified the semantically defined PropBank Arg-
0 and Arg-1 positions. To do so, we annotated a
small number of very abstract frame elements with
these labels (Agent, Actor, Communicator as Arg-
0, and Theme, Effect, Message as Arg-1) and per-
colated these labels through the FrameNet hierar-
chy, adding further manual labels where necessary.
Then, we used frequency and grammatical realiza-
tion information to map the remaining roles onto
higher-numbered Arg roles. We considerably sim-
plified the annotations provided by SALSA, which
use a rather complex annotation scheme. In partic-
ular, we removed annotation for multi-word expres-
sions (which may be non-contiguous), annotations
involving multiple frames for the same predicate
(metaphors, underspecification), and inter-sentence
roles.
The out-of-domain dataset was taken from a study
on the multi-lingual projection of FrameNet annota-
tion (Pado and Lapata, 2005). It is sampled from
the EUROPARL corpus and was chosen to maxi-
mize the lexical coverage, i.e., it contains of a large
number of infrequent predicates. Both syntactic and
semantic structure were annotated manually, in the
TIGER and SALSA format, respectively. Since it
uses a simplified annotation schemes, we did not
have to discard any annotation.
For both datasets, we converted the syntactic
TIGER (Brants et al, 2002) representations into de-
pendencies with a similar set of head-finding rules
used for the preparation of the CoNLL-X shared task
German dataset. Minor modifications (for the con-
15Note, however, that typically not all predicates in each sen-
tence are annotated (cf. Table 2).
12
version of person names and coordinations) were
made to achieve better consistency with datasets
of other languages. Since the TIGER annotation
allows non-contiguous constituents, the resulting
dependencies can be non-projective. Secondary
edges were discarded in the conversion. As for the
automatically constructed features, we used Tree-
Tagger (Schmid, 1994) to produce the PLEMMA
and PPOS columns, and the Morphisto morphol-
ogy (Zielinski and Simon, 2008) for PFEAT.
3.7 Japanese
For Japanese, we used the Kyoto University Text
Corpus (Kawahara et al, 2002), which consists of
approximately 40k sentences taken from Mainichi
Newspapers. Out of them, approximately 5k sen-
tences are annotated with syntactic and semantic de-
pendencies, and are used the training, development
and test data of this year?s shared task. The remain-
ing sentences, which are annotated with only syntac-
tic dependencies, are provided for the training cor-
pus of syntactic dependency parsers.
This corpus adopts a dependency structure repre-
sentation, and thus the conversion to the CoNLL-
2009 format was relatively straightforward. How-
ever, since the original dependencies are annotated
on the basis of phrases (Japanese bunsetsu), we
needed to automatically convert the original annota-
tions to word-based ones using several criteria. We
used the following basic criteria: the words except
the last word in a phrase depend on the next (right)
word, and the last word in a phrase basically depends
on the head word of the governing phrase.
Semantic dependencies are annotated for both
verbal predicates and nominal predicates. The se-
mantic roles (APRED columns) consist of 41 sur-
face cases, many of which are case-marking post-
positions such as ga (nominative), wo (accusative)
and ni (dative). Semantic frame discrimination is not
annotated, and so the PRED column is the same as
the LEMMA column. The original corpus contains
coreference annotations and inter-sentential seman-
tic dependencies, such as inter-sentential zero pro-
nouns and bridging references, but we did not use
these annotations, which are not the target of this
year?s shared task.
To produce the PLEMMA, PPOS and PFEAT
columns, we used the morphological analyzer JU-
MAN 16 and the dependency and case structure an-
alyzer KNP 17. To produce the PHEAD and PDE-
PREL columns, we used the MSTParser 18.
4 Submissions and Results
Participants uploaded the results through the shared
task website, and the official evaluation was per-
formed centrally. Feedback was provided if any for-
mal problems were encountered (for a list of checks,
see the previous section). One submission had to
be rejected because only English results were pro-
vided. After the evaluation period had passed, the
results were anonymized and published on the web.
A total of 20 systems participated in the closed
challenge; 13 of them in the Joint task and seven in
the SRL-only task. Two systems participated in the
open challenge (Joint task). Moreover, 17 systems
provided output in the out-of-domain part of the task
(11 in the OOD Joint task and six in the OOD SRL-
only task).
The main results for the core task - the Joint task
(dependency syntax and semantic relations) in the
context of the closed challenge - are summarized and
ranked in Table 5.
The largest number of systems can be compared
in the SRL results table (Table 6), where all the sys-
tems have been evaluated solely on the SRL perfor-
mance regardless whether they participated in the
Joint or SRL-only task. However, since the results
might have been influenced by the supplied parser,
separate ranking is provided for both types of the
systems.
Additional breakdown of the results (open chal-
lenge, precision and recall tables for the semantic
labeling task, etc.) are available from the CoNLL-
2009 Shared Task website19.
5 Approaches
Table 7 summarizes the properties of the systems
that participated in the closed the open challenges.
16http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
17http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
18http://sourceforge.net/projects/
mstparser
19http://ufal.mff.cuni.cz/conll2009-st
13
Rank System Average Catalan Chinese Czech English German Japanese Spanish
1 Che 82.64 81.84 76.38 83.27 87.00 82.44 85.65 81.90
2 Chen 82.52 83.01 76.23 80.87 87.69 81.22 85.28 83.31
3 Merlo 82.14 82.66 76.15 83.21 86.03 79.59 84.91 82.43
4 Bohnet 80.85 80.44 75.91 79.57 85.14 81.60 82.51 80.75
5 Asahara 78.43 75.91 73.43 81.43 86.40 69.84 84.86 77.12
6 Brown 77.27 77.40 72.12 75.66 83.98 77.86 76.65 77.21
7 Zhang 76.49 75.00 73.42 76.93 82.88 73.76 78.17 75.25
8 Dai 73.98 72.09 72.72 67.14 81.89 75.00 80.89 68.14
9 Lu Li 73.97 71.32 65.53 75.85 81.92 70.93 80.49 71.72
10 Llu??s 71.49 56.64 66.18 75.95 81.69 72.31 81.76 65.91
11 Vallejo 70.81 73.75 67.16 60.50 78.19 67.51 77.75 70.78
12 Ren 67.81 59.42 75.90 60.18 77.83 65.77 77.63 57.96
13 Zeman 51.07 49.61 43.50 57.95 50.27 49.57 57.69 48.90
Table 5: Official results of the Joint task, closed challenge. Teams are denoted by the last name (first name added
only where needed) of the author who registered for the evaluation data. Results are sorted in descending order of the
language-averaged macro F1 score on the closed challenge Joint task. Bold numbers denote the best result for a given
language.
Rank Rank in task System Average Catalan Chinese Czech English German Japanese Spanish
1 1 (SRLonly) Zhao 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.46
2 2 (SRLonly) Nugues 80.31 80.01 78.60 85.41 85.63 79.71 76.30 76.52
3 1 (Joint) Chen 79.96 80.10 76.77 82.04 86.15 76.19 78.17 80.29
4 2 (Joint) Che 79.94 77.10 77.15 86.51 85.51 78.61 78.26 76.47
5 3 (Joint) Merlo 78.42 77.44 76.05 86.02 83.24 71.78 77.23 77.19
6 3 (SRLonly) Meza-Ruiz 77.46 78.00 77.73 75.75 83.34 73.52 76.00 77.91
7 4 (Joint) Bohnet 76.00 74.53 75.29 79.02 80.39 75.72 72.76 74.31
8 5 (Joint) Asahara 75.65 72.35 74.17 84.69 84.26 63.66 77.93 72.50
9 6 (Joint) Brown 72.85 72.18 72.43 78.02 80.43 73.40 61.57 71.95
10 7 (Joint) Dai 70.78 66.34 71.57 75.50 78.93 67.43 71.02 64.64
11 8 (Joint) Zhang 70.31 67.34 73.20 78.28 77.85 62.95 64.71 67.81
12 9 (Joint) Lu Li 69.72 66.95 67.06 79.08 77.17 61.98 69.58 66.23
13 4 (SRLonly) Baoli Li 69.26 74.06 70.37 57.46 69.63 67.76 72.03 73.54
14 10 (Joint) Vallejo 68.95 70.14 66.71 71.49 75.97 61.01 68.82 68.48
15 5 (SRLonly) Moreau 66.49 65.60 67.37 71.74 72.14 66.50 57.75 64.33
16 11 (Joint) Llu??s 63.06 46.79 59.72 76.90 75.86 62.66 71.60 47.88
17 6 (SRLonly) Ta?ckstro?m 61.27 57.11 63.41 71.05 67.64 53.42 54.74 61.51
18 7 (SRLonly) Lin 57.18 61.70 70.33 60.43 65.66 59.51 23.78 58.87
19 12 (Joint) Ren 56.69 41.00 72.58 62.82 67.56 54.31 58.73 39.80
20 13 (Joint) Zeman 32.14 24.19 34.71 58.13 36.05 16.44 30.13 25.36
Table 6: Official results of the semantic labeling, closed challenge, all systems. Teams are denoted by the last name
(first name added only where needed) of the author who registered for the evaluation data. Results are sorted in
descending order of the semantic labeled F1 score (closed challenge). Bold numbers denote the best result for a given
language. Separate ranking is provided for SRL-only systems.
The second column of the table highlights the over-
all architectures. We used + to indicate that the
components are sequentially connected. The lack of
a + sign indicates that the corresponding tasks are
performed jointly.
It is perhaps not surprising that most of the obser-
vations from the 2008 shared task still hold; namely,
the best systems overall do not use joint learning or
optimization (the best such system was placed third
in the Joint task, and there were only four systems
where the learning methodology can be considered
?joint?).
Therefore, most of the observations and conclu-
sions from 2008 shared task hold as well for the
current results. For details, we will leave it to the
reader to interpret the architectures and methods
14
O
v
er
a
ll
D
D
D
PA
PA
PA
Jo
in
t
M
L
Sy
st
em
a
A
rc
h.
b
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
c
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
Le
a
rn
in
g/
O
pt
.
M
et
ho
ds
Zh
ao
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
/g
lo
ba
l
se
ar
ch
(S
R
L-
o
n
ly
)
M
E
N
u
gu
es
(P
C+
A
I+
A
C)
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
be
am
se
ar
ch
+
re
ra
n
ki
n
g
(S
R
L-
o
n
ly
)
L2
-
re
gu
la
riz
ed
lin
.
re
gr
es
sio
n
Ch
en
P
+
PC
+
A
I+
A
C
gr
ap
h
pa
rt
ia
lly
M
ST
C
L
/E
cl
as
s
n
o
gr
ee
dy
(?)
n
o
M
E
Ch
e
D
+
PC
+
A
IC
gr
ap
h
n
o
M
ST
H
O
E
cl
as
s
n
o
IL
P
n
o
SV
M
,
M
E
M
er
lo
D
PA
IC
+
D
ge
n
er
at
iv
e,
tr
an
s
n
o
be
am
se
ar
ch
tr
an
s
n
o
be
am
se
ar
ch
sy
n
ch
ro
n
iz
ed
de
riv
at
io
n
IS
B
N
M
ez
a-
R
u
iz
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
M
ar
ko
v
LN
n
o
Cu
tti
n
g
Pl
an
e
(S
R
L-
o
n
ly
)
M
IR
A
B
o
hn
et
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
C
+
re
ar
ra
n
ge
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
IR
A
)
A
sa
ha
ra
D
+
PI
C
+
A
IC
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
n
-
be
st
re
la
x
.
n
o
pe
rc
ep
tr
o
n
D
ai
D
+
PC
+
A
C
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
pr
o
b
ite
ra
tiv
e
M
E
Zh
an
g
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
E
cl
as
s
n
o
cl
as
sifi
ca
tio
n
n
o
M
IR
A
,
M
E
Lu
Li
D
+
(P
C
||
A
IC
)
gr
ap
h
fo
r
ea
ch
la
n
g.
M
ST
C
L
/E
,
M
ST
E
cl
as
s
n
o
gr
ee
dy
n
o
M
E
B
ao
li
Li
PC
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
,
kN
N
,
M
E
Va
lle
jod
[D
+
P+
A
]C
+
D
I
cl
as
s
n
o
re
ra
n
ki
n
g
cl
as
s
n
o
re
ra
n
ki
n
g
u
n
ifi
ed
la
be
ls
M
B
L
M
o
re
au
D
+
PI
+
Cl
u
st
er
in
g
+
A
I+
A
C
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
CR
F
(S
R
L-
o
n
ly
)
CR
F
Ll
u
??s
D
+
D
A
IC
+
PC
gr
ap
h
n
o
M
ST
E
gr
ap
h
n
o
M
ST
E
ye
s,
M
ST
E
Av
g.
Pe
rc
ep
tr
o
n
Ta?
ck
st
ro?
m
D
+
PI
+
A
I
+
A
C
+
Co
n
st
ra
in
tS
at
isf
ac
tio
n
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
R
en
D
+
PC
+
A
IC
tr
an
s
n
o
gr
ee
dy
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
al
t),
M
E
Ze
m
an
D
I+
D
C+
PC
+
A
I+
A
C
tr
an
s
n
o
gr
ee
dy
w
ith
he
u
ris
tic
s
cl
as
s
n
o
gr
ee
dy
n
o
co
o
cc
u
rr
en
ce
Ta
bl
e
7:
Su
m
m
ar
y
o
fs
ys
te
m
ar
ch
ite
ct
u
re
s
fo
r
th
e
Co
N
LL
-
20
09
sh
ar
ed
ta
sk
;
al
ls
ys
te
m
s
ar
e
in
cl
u
de
d.
SR
L-
o
n
ly
sy
st
em
s
do
n
o
t
ha
v
e
th
e
D
co
lu
m
n
s
an
d
th
e
Jo
in
t
Le
ar
in
g/
O
pt
.
co
lu
m
n
s
fil
le
d
in
.
Th
e
sy
st
em
s
ar
e
so
rt
ed
by
th
e
se
m
an
tic
la
be
le
d
F 1
sc
o
re
av
er
ag
ed
o
v
er
al
lt
he
la
n
gu
ag
es
(sa
m
e
as
in
Ta
bl
e
6).
O
n
ly
th
e
sy
st
em
s
th
at
ha
v
e
a
co
rr
es
po
n
di
n
g
pa
pe
r
in
th
e
pr
o
ce
ed
in
gs
ar
e
in
cl
u
de
d.
A
cr
o
n
ym
s
u
se
d:
D
-
sy
n
ta
ct
ic
de
pe
n
de
n
ci
es
,
P
-
pr
ed
ic
at
e,
A
-
ar
gu
m
en
t,
I-
id
en
tifi
ca
tio
n
,
C
-
cl
as
sifi
ca
tio
n
.
O
v
er
a
ll
a
rc
h.
st
an
ds
fo
r
th
e
co
m
pl
et
e
sy
st
em
ar
ch
ite
ct
u
re
;D
A
rc
h.
st
an
ds
fo
r
th
e
ar
ch
ite
ct
u
re
o
ft
he
sy
n
ta
ct
ic
pa
rs
er
;D
C
o
m
b.
in
di
ca
te
s
if
th
e
fin
al
pa
rs
er
o
u
tp
u
tw
as
ge
n
er
at
ed
u
sin
g
pa
rs
er
co
m
bi
n
at
io
n
;D
In
fe
re
n
ce
st
an
ds
fo
r
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
sy
n
ta
ct
ic
pa
rs
in
g;
PA
A
rc
h.
st
an
ds
th
e
ty
pe
o
fa
rc
hi
te
ct
u
re
u
se
d
fo
r
PA
IC
;P
A
C
o
m
b.
in
di
ca
te
s
if
th
e
PA
o
u
tp
u
t
w
as
ge
n
er
at
ed
th
ro
u
gh
sy
st
em
co
m
bi
n
at
io
n
;P
A
In
fe
re
n
ce
st
an
ds
fo
r
th
e
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
PA
IC
;J
o
in
tL
ea
rn
in
g/
O
pt
.
in
di
ca
te
s
if
so
m
e
fo
rm
o
fjo
in
tl
ea
rn
in
g
o
r
o
pt
im
iz
at
io
n
w
as
im
pl
em
en
te
d
fo
r
th
e
sy
n
ta
ct
ic
+
se
m
an
tic
gl
o
ba
lt
as
k;
M
L
M
et
ho
ds
lis
ts
th
e
M
L
m
et
ho
ds
u
se
d
th
ro
u
gh
o
u
tt
he
co
m
pl
et
e
sy
st
em
.
a
A
u
th
o
rs
o
ft
w
o
sy
st
em
s:
?
B
ro
w
n
?
an
d
?
Li
n
?
di
dn
?
ts
u
bm
it
a
pa
pe
r,
so
th
ei
r
sy
st
em
s?
ar
ch
ite
ct
u
re
s
ar
e
u
n
kn
ow
n
.
b T
he
sy
m
bo
l+
in
di
ca
te
s
se
qu
en
tia
lp
ro
ce
ss
in
g
(ot
he
rw
ise
,
pa
ra
lle
l/jo
in
t).
Th
e
||
m
ea
n
s
th
at
se
v
er
al
di
ffe
re
n
ta
rc
hi
te
ct
u
re
s
sp
an
n
in
g
m
u
lti
pl
e
su
bt
as
ks
ra
n
in
pa
ra
lle
l.
c
M
ST
C
L
/E
as
u
se
d
by
M
cD
o
n
al
d
(20
05
),
M
ST
C
by
Ca
rr
er
as
(20
07
),M
ST
E
by
Ei
sn
er
(20
00
),
M
ST
H
O
E
=
M
ST
E
w
ith
hi
gh
er
-
o
rd
er
fe
at
u
re
s
(si
bl
in
gs
+
al
lg
ra
n
dc
hi
ld
re
n
).
d T
he
sy
st
em
u
n
ifi
es
th
e
sy
n
ta
ct
ic
an
d
se
m
an
tic
la
be
ls
in
to
o
n
e
la
be
l,
an
d
tr
ai
n
s
cl
as
sifi
er
s
o
v
er
th
em
.
It
is
th
u
s
di
ffi
cu
lt
to
sp
lit
th
e
sy
st
em
ch
ar
ac
te
ris
tic
in
to
a
?
D
?
/?
PA
?
pa
rt
.
15
when comparing Table 7 with the Tables 5 and 6).
6 Conclusion
This year?s task has been demanding in several re-
spects, but certainly the most difficulty came from
the fact that participants had to tackle all seven lan-
guages. It is encouraging that despite this added af-
fort the number of participating systems has been
almost the same as last year (20 vs. 22 in 2008).
There are several positive outcomes from this
year?s enterprise:
? we have prepared a unified format and data for
several very different lanaguages, as a basis
for possible extensions towards other languages
and unified treatment of syntactic depenndecies
and semantic role labeling across natural lan-
guages;
? 20 participants have produced SRL results for
all seven languages, using several different
methods, giving hope for a combined system
with even substantially better performance;
? initial results have been provided for three lan-
guages on out-of-domain data (being in fact
quite close to the in-domain results).
Only four systems tried to apply what can be de-
scribed as joint learning for the syntactic and seman-
tic parts of the task. (Morante et al, 2009) use a true
joint learning formulation that phrases syntactico-
semantic parsing as a series of classification where
the class labels are concatenations of syntactic and
semantic edge labels. They predict (a), the set of
syntactico-semantic edge labels for each pair of to-
kens; (b), the set of incoming syntactico-semantic
edge labels for each individual token; and (c), the
existence of an edge between each pair of tokens.
Subsequently, they combine the (possibly conflict-
ing) output of the three classifiers by a ranking ap-
proach to determine the most likely structure that
meets all well-formedness constraints. (Llu??s et al,
2009) present a joint approach based on an exten-
sion of Eisner?s parser to accommodate also seman-
tic dependency labels. This architecture is similar
to the one presented by the same authors in the past
edition, with the extension to a second-order syn-
tactic parsing and a particular setting for Catalan
and Spanish. (Gesmundo et al, 2009) use an in-
cremental parsing model with synchronous syntac-
tic and semantic derivations and a joint probability
model for syntactic and semantic dependency struc-
tures. The system uses a single input queue but two
separate stacks and synchronizes syntactic and se-
mantic derivations at every word. The synchronous
derivations are modeled with an Incremental Sig-
moid Belief Network that has latent variables for
both syntactic and semantic states and connections
from syntax to semantics and vice versa. (Dai et
al., 2009) designed an iterative system to exploit
the inter-connections between the different subtasks
of the CoNLL shared task. The idea is to decom-
pose the joint learning problem into four subtasks
? syntactic dependency identification, syntactic de-
pendency labeling, semantic dependency identifica-
tion and semantic dependency labeling. The initial
step is to use a pipeline approach to use the input of
one subtask as input to the next, in the order speci-
fied. The iterative steps then use additional features
that are not available in the initial step to improve the
accuracy of the overall system. For example, in the
iterative steps, semantic information becomes avail-
able as features to syntactic parsing, so on and so
forth.
Despite these results, it is still not clear whether
joint learning has a significant advantage over other
approaches (and if yes, then for what languages). It
is thus necessary to carefully plan the next shared
tasks; it might be advantageous to bring up a sim-
ilar task in the future once again, and/or couple it
with selected application(s). There, (we hope) the
benefits of the dependency representation combined
with semantic roles the way we have formulated it
in 2008 and 2009 will really show up.
Acknowledgments
We would like to thank the Linguistic Data Consor-
tium, mainly to Denise DiPersio, Tony Casteletto
and Christopher Cieri for their help and handling
of invoicing and distribution of the data for which
LDC has a license. For all of the trial, training and
evaluation data they had to act a very short notice.
All the data has been at the participants? disposal
(again) free of charge. We are grateful to all of them
for LDC?s continuing support of the CoNLL Shared
16
Tasks.
We would also like to thank organizers of the pre-
vious four shared tasks: Sabine Buchholz, Xavier
Carreras, Ryan McDonald, Amit Dubey, Johan Hall,
Yuval Krymolowski, Sandra Ku?bler, Erwin Marsi,
Jens Nilsson, Sebastian Riedel and Deniz Yuret.
This shared task would not have been possible with-
out their previous effort.
We also acknowledge the support of the M?SMT
of the Czech Republic, projects MSM0021620838
and LC536; the Grant Agency of the Academy of
sciences of the Czech Republic 1ET201120505 (for
Jan Hajic?, Jan ?Ste?pa?nek and Pavel Stran?a?k).
Llu??s Ma`rquez and M. Anto`nia Mart?? partici-
pation was supported by the Spanish Ministry of
Education and Science, through the OpenMT and
TextMess research projects (TIN2006-15307-C03-
02, TIN2006-15265-C06-06).
The following individuals directly contributed to
the Chinese Treebank (in alphabetic order): Meiyu
Chang, Fu-Dong Chiou, Shizhe Huang, Zixin Jiang,
Tony Kroch, Martha Palmer, Mitch Marcus, Fei
Xia, Nianwen Xue. The contributors to the Chi-
nese Proposition Bank include (in alphabetic order):
Meiyu Chang, Gang Chen, Helen Chen, Zixin Jiang,
Martha Palmer, Zhiyi Song, Nianwen Xue, Ping Yu,
Hua Zhong. The Chinese Treebank and the Chinese
Proposition Bank were funded by DOD, NSF and
DARPA.
Adam Meyers? work on the shared task has been
supported by the NSF Grant IIS-0534700 ?Structure
Alignment-based MT.?
We thank the Mainichi Newspapers for the per-
mission of distributing the sentences of the Kyoto
University Text Corpus for this shared task.
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
EMNLP-CoNLL 2007, pages 957?961, June. Prague,
Czech Republic.
Montserrat Civit, M. Anto`nia Mart??, and Nu?ria Buf??.
2006. Cat3LB and Cast3LB: from constituents to
dependencies. In Proceedings of the 5th Interna-
tional Conference on Natural Language Processing,
FinTAL, pages 141?153, Turku, Finland. Springer Ver-
lag, LNAI 4139.
Qifeng Dai, Enhong Chen, and Liu Shi. 2009. An it-
erative approach for joint dependency parsing and se-
mantic role labeling. In Proceedings of the 13th Con-
ference on Computational Natural Language Learning
(CoNLL-2009), June 4-5, Boulder, Colorado, USA.
June 4-5.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Tehcnologies, pages 29?62. Kluwer Academic
Publishers.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press, Cambridge.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the 13th Conference on
Computational Natural Language Learning (CoNLL-
2009), June 4-5, Boulder, Colorado, USA. June 4-5.
Jan Hajic?, Jarmila Panevova?, Zden?ka Ures?ova?, Alevtina
Be?mova?, Veronika Kola?r?ova?- ?Rezn??c?kova??, and Petr
Pajas. 2003. PDT-VALLEX: Creating a Large-
coverage Valency Lexicon for Treebank Annotation.
In J. Nivre and E. Hinrichs, editors, Proceedings of The
Second Workshop on Treebanks and Linguistic Theo-
ries, pages 57?68, Vaxjo, Sweden. Vaxjo University
Press.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan ?Ste?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k ?Zabokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Xavier Llu??s, Stefan Bott, and Llu??s Ma`rquez. 2009.
A second-order joint eisner model for syntactic and
semantic dependency parsing. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA. June 4-5.
17
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
Llu??s Ma`rquez, Luis Villarejo, M. Anto`nia Mart??, and
Mariona Taule?. 2007. SemEval-2007 Task 09: Mul-
tilevel semantic annotation of catalan and spanish.
In Proceedings of the 4th International Workshop on
Semantic Evaluations (SemEval-2007), pages 42?47,
Prague, Czech Republic.
M. Anto`nia Mart??, Mariona Taule?, Llu??s Ma`rquez, and
Manu Bertran. 2007. Anotacio?n semiautoma?tica
con papeles tema?ticos de los corpus CESS-ECE.
Procesamiento del Lenguaje Natural, SEPLN Journal,
38:67?76.
Ryan McDonald, Fernando Pereira, Jan Hajic?, and Kiril
Ribarov. 2005. Non-projective dependency parsing
using spanning tree algortihms. In Proceedings of
NAACL-HLT?05, Vancouver, Canada, pages 523?530.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank Project: An Interim Report. In NAACL/HLT
2004 Workshop Frontiers in Corpus Annotation,
Boston.
Roser Morante, Vincent Van Asch, and Antal van den
Bosch. 2009. A simple generative pipeline approach
to dependency parsing and semantic role labeling. In
Proceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL-2009), Boulder,
Colorado, USA. June 4-5.
Joakim Nivre, Johann Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The conll 2007 shared task on depen-
dency parsing. In Proceedings of the EMNLP-CoNLL
2007 Conference, pages 915?932, Prague, Czech Re-
public.
Sebastian Pado and Mirella Lapata. 2005. Cross-lingual
projection of role-semantic information. In Proceed-
ings of the Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing (HLT/EMNLP-2005), pages 859?
866, Vancouver, BC.
Petr Pajas and Jan ?Ste?pa?nek. 2008. Recent advances in
a feature-rich framework for treebank annotation. In
The 22nd International Conference on Computational
Linguistics - Proceedings of the Conference (COL-
ING?08), pages 673?680, Manchester.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the European ACL Cenference EACL?09,
Athens, Greece.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008), pages 159?177.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Martin ?Cmejrek, Jan Cur???n, Jan Hajic?, Jir??? Havelka,
and Vladislav Kubon?. 2004. Prague Czech-English
Dependency Treebank: Syntactically Anntoated Re-
sources for Machine Translation. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation (LREC-2004), pages 1597?
1600, Lisbon, Portugal.
W. N. Francis and H. Kucera. 1964. Brown Corpus Man-
ual of Information to accompany A Standard Corpus
of Present-Day Edited American English, for use with
Digital Computers. Revised 1971, Revised and Am-
plified 1979, available at www.clarinet/brown.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Technical report,
Lin- guistic Data Consortium.
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Nianwen Xue, Fei Xia, Fu Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Andrea Zielinski and Christian Simon. 2008. Morphisto:
An open-source morphological analyzer for german.
In Proceedings of the Conference on Finite State Meth-
ods in Natural Language Processing.
18
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 49?57,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
An Analysis of Bootstrapping for the Recognition of Temporal Expressions
Jordi Poveda
TALP Research Center
Technical University of Catalonia (UPC)
Barcelona, Spain
jpoveda@lsi.upc.edu
Mihai Surdeanu
NLP Group
Stanford University
Stanford, CA
mihais@stanford.edu
Jordi Turmo
TALP Research Center
Technical University of Catalonia (UPC)
Barcelona, Spain
turmo@lsi.upc.edu
Abstract
We present a semi-supervised (bootstrapping)
approach to the extraction of time expression
mentions in large unlabelled corpora. Because
the only supervision is in the form of seed
examples, it becomes necessary to resort to
heuristics to rank and filter out spurious pat-
terns and candidate time expressions. The
application of bootstrapping to time expres-
sion recognition is, to the best of our knowl-
edge, novel. In this paper, we describe one
such architecture for bootstrapping Informa-
tion Extraction (IE) patterns ?suited to the
extraction of entities, as opposed to events or
relations? and summarize our experimental
findings. These point out to the fact that a
pattern set with a good increase in recall with
respect to the seeds is achievable within our
framework while, on the other side, the de-
crease in precision in successive iterations is
succesfully controlled through the use of rank-
ing and selection heuristics. Experiments are
still underway to achieve the best use of these
heuristics and other parameters of the boot-
strapping algorithm.
1 Introduction
The problem of time expression recognition refers
to the identification in free-format natural language
text of the occurrences of expressions that denote
time. Time-denoting expressions appear in a great
diversity of forms, beyond the most obvious ab-
solute time or date references (e.g. 11pm, Febru-
ary 14th, 2005): time references that anchor on an-
other time (three hours after midnight, two weeks be-
fore Christmas), expressions denoting durations (a
few months), expressions denoting recurring times
(every third month, twice in the hour), context-
dependent times (today, last year), vague references
(somewhere in the middle of June, the near future)
or times that are indicated by an event (the day G.
Bush was reelected). This problem is a subpart of
a task called TERN (Temporal Expression Recog-
nition and Normalization), where temporal expres-
sions are first identified in text and then its intended
temporal meaning is represented in a canonical for-
mat. TERN was first proposed as an independent
task in the 2004 edition of the ACE conferences1.
The most widely used standard for the annotation of
temporal expressions is TIMEX (Ferro et al, 2005).
The most common approach to temporal expres-
sion recognition in the past has been the use of
hand-made grammars to capture the expressions (see
(Wiebe et al, 1998; Filatova and Hovy, 2001; Sa-
quete et al, 2004) for examples), which can then
be easily expanded with additional attributes for the
normalization task, based on computing distance
and direction (past or future) with respect to a ref-
erence time. This approach achieves an F1-measureof approximately 85% for recognition and normal-
ization. The use of machine learning techniques ?
mainly statistical? for this task is a more recent
development, either alongside the traditional hand-
grammar approach to learn to distinguish specific
difficult cases (Mani and Wilson, 2000), or on its
own (Hacioglu et al, 2005). The latter apply SVMs
to the recognition task alone, using the output of sev-
eral human-made taggers as additional features for
the classifier, and report an F1-measure of 87.8%.
1http://www.nist.gov/speech/tests/ace/
49
Bootstrapping techniques have been used for such
diverse NLP problems as: word sense disambigua-
tion (Yarowsky, 1995), named entity classification
(Collins and Singer, 1999), IE pattern acquisition
(Riloff, 1996; Yangarber et al, 2000; Yangarber,
2003; Stevenson and Greenwood, 2005), document
classification (Surdeanu et al, 2006), fact extraction
from the web (Pas?ca et al, 2006) and hyponymy re-
lation extraction (Kozareva et al, 2008).
(Yarowsky, 1995) used bootstrapping to train de-
cision list classifiers to disambiguate between two
senses of a word, achieving impressive classification
accuracy. (Collins and Singer, 1999) applied boot-
strapping to extract rules for named entity (NE) clas-
sification, seeding the sytem with a few handcrafted
rules. Their main innovation was to split training
in two alternate stages: during one step, only con-
textual rules are sought; during the second step, the
new contextual rules are used to tag further NEs and
these are used to produce new spelling rules.
Bootstrapping approaches are employed in
(Riloff, 1996), (Yangarber et al, 2000), (Yangarber,
2003), and (Stevenson and Greenwood, 2005)
in order to find IE patterns for domain-specific
event extraction. (Pas?ca et al, 2006) employ a
bootstrapping process to extract general facts from
the Web, viewed as two-term relationships (e.g
[Donald Knuth, 1938] could be an instance of
a ?born in year? relationship). (Surdeanu et al,
2006) used bootstrapping co-trained with an EM
classifier in order to perform topic classification
of documents based on the presence of certain
learned syntactic-semantic patterns. In (Kozareva
et al, 2008), bootstrapping is applied to finding
new members of certain class of objects (i.e. an
?is-a? relationship), by providing a member of the
required class as seed and using a ?such as? type of
textual pattern to locate new instances.
The recognition of temporal expressions is cru-
cial for many applications in NLP, among them: IE,
Question Answering (QA) and Automatic Summa-
rization (for the temporal ordering of events). Work
on slightly supervised approaches such as bootstrap-
ping is justified by the large availability of unla-
belled corpora, as opposed to tagged ones, from
which to learn models for recognition.
2 Architecture
Figure 1 illustrates the building blocks of the algo-
rithm and their interactions, along with input and
output data.
The inputs to the bootstrapping algorithm are the
unlabelled training corpus and a file of seed ex-
amples. The unlabelled corpus is a large collec-
tion of documents which has been tokenized, POS
tagged, lemmatized, and syntactically analyzed for
basic syntactic constituents (shallow parsing) and
headwords. The second input is a set of seed exam-
ples, consisting of a series of token sequences which
we assume to be correct time expressions. The seeds
are supplied without additional features, and without
context information.
Our bootstrapping algorithm works with two al-
ternative views of the same target data (time expres-
sions), that is: patterns and examples (i.e. an in-
stance of a pattern in the corpus). A pattern is a gen-
eralized representation that can match any sequence
of tokens meeting the conditions expressed in the
pattern (these can be morphological, semantic, syn-
tactic and contextual). An example is an actual can-
didate occurrence of a time expression. Patterns are
generated from examples found in the corpus and,
in its turn, new examples are found by searching
for matches of new patterns. Both patterns and ex-
amples may carry contextual information, that is, a
window of tokens left and right of the candidate time
expression.
Output examples and output patterns are the out-
puts of the bootstrapping process. Both the set of
output examples and the set of output patterns are
increased with each new iteration, by adding the new
candidate examples (respectively, patterns) that have
been ?accepted? during the last iteration (i.e. those
that have passed the ranking and selection step).
Initially, a single pass through the corpus is per-
formed in order to find occurrences of the seeds in
the text. Thus, we bootstrap an initial set of exam-
ples. From then on, the bootstrapping process con-
sists of a succession of iterations with the following
steps:
1. Ranking and selection of examples: Each ex-
ample produced during any of the previous it-
erations, 0 to i ? 1, is assigned a score (rank-
ing). The top n examples are selected to grow
the set of output examples (selection) and will
50
Figure 1: Block diagram of bootstrapping algorithm
be used for the next step. The details are given
in Section 4.2.
2. Generation of candidate patterns: Candidate
patterns for the current iteration are generated
from the selected examples of the previous step
(discussed in Section 3).
3. Ranking and selection of candidate patterns:
Each pattern from the current iteration is as-
signed a score and the top m patterns are se-
lected to grow the set of output patterns and to
be used in the next step (discussed in Section
4.1). This step also involves a process of analy-
sis of subsumptions, performed simultaneously
with selection, in which the set of selected pat-
terns is examined and those that are subsumed
by other patterns are discarded.
4. Search for instances of the selected patterns:
The training corpus is traversed, in order to
search for instances (matches) of the selected
patterns, which, together with the accepted ex-
amples from all previous iterations, will form
the set of candidate examples for iteration i+1.
Also, in order to relax the matching of pat-
terns to corpus tokens and of token forms among
themselves, the matching of token forms is case-
insensitive, and all the digits in a token are gen-
eralized to a generic digit marker (for instance,
?12-23-2006? is internally rewritten as ?@@-@@-
@@@@?).
Even though our architecture is built on a tradi-
tional boostrapping approach, there are several ele-
ments that are novel, at least in the context of tem-
poral expression recognition: a) our pattern repre-
sentation incorporates full syntax and distributional
semantics in a unified model (see Section 3); b) our
pattern ranking/selection approach includes a sub-
sumption model to limit redundancy; c) the formu-
lae in our example ranking/selection approach are
designed to work with variable-length expressions
that incorporate a context.
3 Pattern representation
Patterns capture both the sequence of tokens that
integrate a potential time expression (i.e. a time
expression mention), and information from the left
and right context where it occurs (up to a bounded
length). Let us call prefix the part of the pattern that
represents the left context, infix the part that repre-
sents a potential time expression mention and postfix
the part that represents the right context.
The EBNF grammar that encodes our pattern rep-
resentation is given in Figure 2. Patterns are com-
posed of multiple pattern elements (PEs). A pattern
element is the minimal unit that is matched against
the tokens in the text, and a single pattern element
can match to one or several tokens, depending on
the pattern element type. A pattern is considered to
match a sequence of tokens in the text when: first,
all the PEs from the infix are matched (this gives the
potential time expression mention) and, second, all
the PEs from the prefix and the postfix are matched
(this gives the left and right context information for
the new candidate example, respectively). There-
fore, patterns with a larger context window are more
restrictive, because all of the PEs in the prefix and
the postfix have to be matched (on top of the infix)
for the pattern to yield a match.
We distinguish among token-level generalizations
51
pattern ::= prefix SEP infix SEP postfix SEP
(modifiers)*
prefix ::= (pattern-elem)*
infix ::= (pattern-elem)+
postfix ::= (pattern-elem)*
pattern-elem ::= FORM "(" token-form ")" |
SEMCLASS "(" token-form ")" |
POS "(" pos-tag ")" | LEMMA "(" lemma-form ")" |
SYN "(" syn-type "," head ")" |
SYN-SEM "(" syn-type "," head ")"
modifiers ::= COMPLETE-PHRASE
Figure 2: The EBNF Grammar for Patterns
(i.e. PEs) and chunk-level generalizations. The for-
mer have been generated from the features of a sin-
gle token and will match to a single token in the text.
The latter have been generated from and match to a
sequence of tokens in the text (e.g. a basic syntactic
chunk). Patterns are built from the following types
of PEs (which can be seen in the grammar from Fig-
ure 2):
1. Token form PEs: The more restrictive, only
match a given token form.
2. Semantic class PEs: Match tokens (sometimes
multiwords) that belong to a given semantic
similarity class. This concept is defined below.
3. POS tag PEs: Match tokens with a given POS.
4. Lemma PEs: Match tokens with a given
lemma.
5. Syntactic chunk PEs: Match a sequence of to-
kens that is a syntactic chunk of a given type
(e.g. NP) and whose headword has the same
lemma as indicated.
6. Generalized syntactic PEs: Same as the previ-
ous, but the lemma of the headword may be any
in a given semantic similarity class.
The semantic similarity class of a word is defined
as the word itself plus a group of other semanti-
cally similar words. For computing these, we em-
ploy Lin?s corpus of pairwise distributional similari-
ties among words (nouns, verbs and adjectives) (Lin,
1998), filtered to include only those words whose
similarity value is above both an absolute (highest
n) and relative (to the highest similarity value in the
class) threshold. Even after filtering, Lin?s similari-
ties can be ?noisy?, since the corpus has been con-
structed relying on purely statistical means. There-
fore, we are employing in addition a set of manu-
ally defined semantic classes (hardcoded lists) sen-
sitive to our domain of temporal expressions, such
that these lists ?override? the Lin?s similarity cor-
pus whenever the semantic class of a word present
in them is involved. The manually defined semantic
classes include: the written form of cardinals; ordi-
nals; days of the week (plus today, tomorrow and
yesterday); months of the year; date trigger words
(e.g. day, week); time trigger words (e.g. hour, sec-
ond); frequency adverbs (e.g. hourly, monthly); date
adjectives (e.g. two- day, @@-week-long); and time
adjectives (e.g. three-hour, @@-minute-long).
We use a dynamic window for the amount of con-
text that is encoded into a pattern, that is, we gen-
erate all the possible patterns with the same infix,
and anything between 0 and the specified length of
the context window PEs in the prefix and the postfix,
and let the selection step decide which variations get
accepted into the next iteration.
The modifiers field in the pattern representa-
tion has been devised as an extension mecha-
nism. Currently the only implemented mod-
ifier is COMPLETE-PHRASE, which when at-
tached to a pattern, ?rounds? the instance (i.e.
candidate time expression) captured by its infix
to include the closest complete basic syntactic
chunk (e.g. ?LEMMA(end) LEMMA(of) SEM-
CLASS(January)? would match ?the end of De-
cember 2009? instead of only ?end of December?
against the text ?. . . By the end of December 2009,
. . . ?). This modifier was implemented in view of the
fact that most temporal expressions correspond with
whole noun phrases or adverbial phrases.
From the above types of PEs, we have built the
following types of patterns:
1. All-lemma patterns (including the prefix and
postfix).
2. All-semantic class patterns.
3. Combinations of token form with sem. class.
4. Combinations of lemma with sem. class.
5. All-POS tag patterns.
6. Combinations of token form with POS tag.
7. Combinations of lemma with POS tag.
8. All-syntactic chunk patterns.
9. All-generalized syntactic patterns.
4 Ranking and selection of patterns and
learning examples
4.1 Patterns
For the purposes of this section, let us define the
control set C as being formed by the seed examples
plus all the selected examples over the previous it-
erations (only the infix considered, not the context).
52
Note that, except for the seed examples, this is only
assumed correct, but cannot be guaranteed to be cor-
rect (unsupervised). In addition, let us define the in-
stance set Ip of a candidate pattern p as the set of
all the instances of the pattern found in a fraction of
the unlabelled corpus (only infix of the instance con-
sidered). Each candidate pattern pat is assigned two
partial scores:
1. A frequency-based score freq sc(p) that mea-
sures the coverage of the pattern in (a section
of) the unsupervised corpus:
freq sc(p) = Card(Ip ? C)
2. A precision score prec sc(p) that evaluates the
precision of the pattern in (a section of) the un-
supervised corpus, measured against the con-
trol set:
prec sc(p) = Card(Ip?C)Card(Ip)
These two scores are computed only against a
fraction of the unlabelled corpus for time effi-
ciency. There remains an issue with whether multi-
sets (counting each repeated instance several times)
or normal sets (counting them only once) should be
used for the instance sets Ip. Our experiments indi-
cate that the best results are obtained by employing
multisets for the frequency-based score and normal
sets for the precision score.
Given the two partial scores above, we have tried
three different strategies for combining them:
? Multiplicative combination: ?1 log(1 +
freq sc(p)) + ?2 log(2 + prec sc(p))
? The strategy suggested in (Collins and Singer,
1999): Patterns are first filtered by imposing
a threshold on their precision score. Only for
those patterns that pass this first filter, their final
score is considered to be their frequency-based
score.
? The strategy suggested in (Riloff, 1996):{ prec sc(p) ? log(freq sc(p)) if prec sc(p) ? thr
0 otherwise
4.1.1 Analysis of subsumptions
Intertwined with the selection step, an analysis of
subsumptions is performed among the selected pat-
terns, and the patterns found to be subsumed by oth-
ers in the set are discarded. This is repeated until ei-
ther a maximum ofm patterns with no subsumptions
among them are selected, or the list of candidate pat-
terns is exhausted, whichever happens first. The pur-
pose of this analysis of subsumptions is twofold: on
the one hand, it results in a cleaner output pattern
set by getting rid of redundant patterns; on the other
hand, it improves temporal efficiency by reducing
the number of patterns being handled in the last step
of the algorithm (i.e. searching for new candidate
examples).
In our scenario, a pattern p1 with instance set Ip1
is subsumed by a pattern p2 with instance set Ip2
if Ip1 ? Ip2 . We make a distinction among ?theo-
retical? and ?empirical? subsumptions. Theoretical
subsumptions are those that can be justified based on
theoretical grounds alone, from observing the form
of the patterns. Empirical subsumptions are those
cases where in fact one pattern subsumes another ac-
cording to the former definition, but this could only
be detected by having calculated their respective in-
stance sets a priori, which beats one of the purposes
of the analysis of subsumptions ?namely, tempo-
ral efficiency?. We are only dealing with theoreti-
cal subsumptions here. A pattern theoretically sub-
sumes another pattern when either of these condi-
tions occur:
? The first pattern is identical to the second, ex-
cept that the first has fewer contextual PEs in
the prefix and/or the postfix.
? Part or all of the PEs of the first pattern are
identical to the corresponding PEs in the sec-
ond pattern, except for the fact that they are
of a more general type (element-wise); the re-
maining PEs are identical. To this end, we have
defined a partial order of generality in the PE
types (see section 3), as follows:
FORM ? LEMMA ? SEMCLASS; FORM ? POS;
SYN ? SYN-SEMC
? Both the above conditions (fewer contextual
PEs and of a more general type) happen at the
same time.
4.2 Learning Examples
An example is composed of the tokens which have
been identified as a potential time expression (which
we shall call the infix) plus a certain amount of left
and right context (from now on, the context) en-
coded alongside the infix. For ranking and selecting
53
examples, we first assign a score and select a num-
ber n of distinct infixes and, in a second stage, we
assign a score to each context of appearance of an
infix and select (at most) m contexts per infix. Our
scoring system for the infixes is adapted from (Pas?ca
et al, 2006). Each distinct infix receives three par-
tial scores and the final score for the infix is a linear
combination of these, with the ?i being parameters:
?1sim sc(ex) + ?2pc sc(ex) + ?3ctxt sc(ex)
1. A similarity-based score (sim sc(ex)), which
measures the semantic similarity (as per the
Lin?s similarity corpus (Lin, 1998)) of the
infix with respect to set of ?accepted? output
examples from all previous iterations plus the
initial seeds. If w1, . . . , wn are the tokens in
the infix (excluding stopwords); ej,1, . . . , ej,mj
are the tokens in the j-th example of the set
E of seed plus output examples; and sv(x, y)
represents a similarity value, the similarity
Sim(wi) of the i-th word of the infix wrt
the seeds and output is given by Sim(wi) =?|E|
j=1 max(sv(wi, ej,1), . . . , sv(wi, ej,mj )),
and the similarity-based score of an in-
fix containing n words is given by?n
i=1 log(1+Sim(wi))
n .
2. A phrase-completeness score (pc sc(ex)),
which measures the likelihood that the infix
is a complete time expression and not merely
a part of one, over the entire set of candidate
example: count(INFIX)count(?INFIX?)
3. A context-based score (ctxt sc(ex)), intended
as a measure of the infix?s relevance. For each
context (up to a length) where this infix appears
in the corpus, the frequency of the word with
maximum relative frequency (over the words
in all the infix?s contexts) is taken. The sum
is then scaled by the relative frequency of this
particular infix.
Apart from the score associated with the infix,
each example (i.e. infix plus a context) receives
two additional frequency scores for the left and right
context part of the example respectively. Each of
these is given by the relative frequency of the token
with maximum frequency of that context, computed
over all the tokens that appear in all the contexts of
all the candidate examples. For each selected infix,
the m contexts with best score are selected.
5 Experiments
5.1 Experimental setup
As unsupervised data for our experiments, we use
the NW (newswire) category of LDC?s ACE 2005
Unsupervised Data Pool, containing 456 Mbytes of
data in 204K documents for a total of over 82 mil-
lion tokens. Simultaneously, we use a much smaller
labelled corpus (where the correct time expressions
are tagged) to measure the precision, recall and F1-measure of the pattern set learned by the bootstrap-
ping process. This is the ACE 2005 corpus, contain-
ing 550 documents with 257K tokens and approx.
4650 time expression mentions. The labelled corpus
is split in two halves: one half is used to obtain the
initial seed examples from among the time expres-
sions found therein; the other half is used for eval-
uation. We are requiring that a pattern captures the
target time expression mention exactly (no misalign-
ment allowed at the boundaries), in order to count it
as a precision or recall hit.
We will also be interested in measuring the gain
in recall, that is, the difference between the recall
in the best iteration and the initial recall given by
the seeds. Also important is the number of iter-
ations after which the bootstrapping process con-
verges. In the case where the same F1- measuremark is achieved in two experimental settings, ear-
lier convergence of the algorithm will be prefered.
Otherwise, better F1 and gain in recall are the pri-mary goals.
In order to start with a set of seeds with high pre-
cision, we select them automatically, imposing that
a seed time expression must have precision above a
certain value (understood as the percentage, of all
the appearances of the sequence of tokens in the su-
pervised corpus, those in which it is tagged as a cor-
rect time expression). In the experiments presented
below, this threshold for precision of the seeds is
90% ?in the half of the supervised corpus reserved
for extraction of seeds?. From those that pass this
filter, the ones that appear with greater frequency are
selected. For time expressions that have an identi-
cal digit pattern (e.g. two dates ?@@ December?
or two years ?@@@@?, where @ stands for any
digit), only one seed is taken. This approach sim-
ulates the human domain expert, which typically is
the first step in bootstrapping IE models
54
Unless specifically stated otherwise, all the exper-
iments presented below share the following default
settings:
? Only the first 2.36 Mbytes of the unsupervised
corpus are used (10 Mbytes after tokenization
and feature extraction), that is 0.5% of the
available data. This is to keep the execution
time of experiments low, where multiple exper-
iments need to be run to optimize a certain pa-
rameter.
? We use the Collins and Singer strategy (see
section 4.1) with a precision threshold of 0.50
for sub-score combination in pattern selection.
This strategy favours patterns with slightly
higher precision.
? The maximum length of prefix and postfix is 1
and 0 elements, respectively. This was deter-
mined experimentally.
? 100 seed examples are used (out of a maximum
of 605 available).
? In the ranking of examples, the ?i weights for
the three sub- scores for infixes are 0.5 for
the ?similarity-based score?, 0.25 for ?phrase-
completeness? and 0.25 for ?context-based
score?.
? In the selection of examples, the maximum
number of new infixes accepted per iteration is
200, with a maximum of 50 different contexts
per infix. In the selection of patterns, the max-
imum number of new accepted patterns per it-
eration is 5000 (although this number is never
reached due to the analysis of subsumptions).
? In the selection of patterns, multisets are used
for computing the instance set of a pattern
for the frequency-based score and normal sets
for the precision score (determined experimen-
tally).
? The POS tag type of generalization (pattern el-
ement) has been deactivated, that is, neither all-
POS patterns, nor patterns that are combina-
tions of POS PEs with another are generated.
After an analysis of errors, it was observed that
POS generalizations (because of the fact that
they are not lexicalized like, for instance, the
syntactic PEs with a given headword) give rise
to a considerable number of precision errors.
? All patterns are generated with COMPLETE-
PHRASE modifier automatically attached. It
was determined experimentally that it was best
to use this heuristic in all cases (see section 3).
5.2 Variation of the number of seeds
We have performed experiments using 1, 5, 10, 20,
50, 100, 200 and 500 seeds. The general trends ob-
served were as follows. The final precision (when
the bootstrapping converges) decreases more or less
monotonically as the number of seeds increases, al-
though there are slight fluctuations; besides, the dif-
ference in this respect between using few seeds (20
to 50) or more (100 to 200) is of only around 3%.
However, a big leap can be observed in moving from
200 to 500 seeds, where both the initial precision
(of the seeds) and final precision (at point of con-
vergence) drop by 10% wrt to using 200 seeds. The
final recall increases monotonically as the number
of seeds increases?since more supervised informa-
tion is provided?. The final F1-measure first in-creases and then decreases with an increasing num-
ber of seeds, with an optimum value being reached
somewhere between the 50 and 100 seeds.
The largest gain in recall (difference between re-
call of the seeds and recall at the point of con-
vergence) is achieved with 20 seeds, for a gain
of 16.38% (initial recall is 20.08% and final is
36.46%). The best mark in F1-measure is achievedwith 100 seeds, after 6 iterations: 60.43% (the final
precision is 69.29% and the final recall is 53.58%;
the drop in precision is 6.5% and the gain in recall is
14.28%). Figure 3 shows a line plot of precision vs
recall for these experiments. This experiment sug-
gests that the problem of temporal expression recog-
nition can be captured with minimal supervised in-
formation (100 seeds) and larger amounts of unsu-
pervised information.
Figure 3: Effect of varying the number of seeds
55
5.3 Variation of the type of generalizations
used in patterns
In these experiments, we have defined four differ-
ents sets of generalizations (i.e. types of pattern ele-
ments among those specified in section 3) to evalu-
ate how semantic and syntactic generalizations con-
tribute to performance of the algorithm. These four
experiments are labelled as follows: NONE includes
only PEs of the LEMMA type; SYN includes PEs
of the lemma type and of the not-generalized syn-
tactic chunk (SYN) type; SEM includes PEs of the
lemma type and of the semantic class (SEMCLASS)
type, as well as combinations of lemma with SEM-
CLASS PEs; and lastly, SYN+SEM includes every-
thing that both SYN and SEM experiments include,
plus PEs of the generalized syntactic chunk (SYN-
SEMC) type.
One can observe than neither type of generaliza-
tion, syntactic or semantic, is specially ?effective?
when used in isolation (only a 3.5% gain in recall in
both cases). It is only the combination of both types
that gives a good gain in recall (14.28% in the case
of this experiment). Figure 4 shows a line plot of this
experiment. The figure indicates that the problem of
temporal expression recognition, even though appar-
ently simple, requires both syntactic and semantic
information for efficient modeling.
Figure 4: Effect of using syntactic and/or semantic gen-
eralizations
5.4 Variation of the size of unsupervised data
used
We performed experiments using increasing
amounts of unsupervised data for training in the
bootstrapping: 1, 5, 10, 50 and 100 Mbytes of
preprocessed corpus (tokenized and with feature
extraction). The amounts of plain text data are
roughly a fifth part, respectively. The objective
of these experiments is to determine whether
performance improves as the amount of training
data is increased. The number of seeds passed to
the bootstrapping is 68. The maximum number of
new infixes (the part of an example that contains a
candidate time expression) accepted per iteration
has been increased from 200 to 1000, because it
was observed that larger amounts of unsupervised
training data need a greater number of selection
?slots? in order to render an improvement (that is, a
more ?reckless? bootstrapping), otherwise they will
fill up all the allowed selection slots.
The observed effect is that both the drop in preci-
sion (from the initial iteration to the point of conver-
gence) and the gain in recall improve more or less
consistently as a larger amount of training data is
taken, or otherwise the same recall point is achieved
in an earlier iteration. These improvements are nev-
ertheless slight, in the order of between 0.5% and
2%. The biggest improvement is observed in the 100
Mbytes experiment, where recall after 5 iterations is
6% better than in the 50 Mbytes experiment after 7
iterations. The drop in precision in the 100 Mbytes
experiment is 13.05%, for a gain in recall of 21.36%
(final precision is 71.02%, final recall 52.84% and
final F1 60.59%). Figure 5 shows a line plot of thisexperiment. This experiment indicates that increas-
ing amounts of unsupervised data can be used to im-
prove the performance of our model, but the task is
not trivial.
Figure 5: Effect of varying the amount of unsupervised
training data
6 Conclusions and future research
We have presented a slightly supervised algorithm
for the extraction of IE patterns for the recognition
56
of time expressions, based on bootstrapping, which
introduces a novel representation of patterns suited
to this task. Our experiments show that with a rel-
atively small amount of supervision (50 to 100 ini-
tial correct examples or seeds) and using a combina-
tion of syntactic and semantic generalizations, it is
possible to obtain an improvement of around 15%-
20% in recall (with regard to the seeds) and F1-measure over 60% learning exclusively from unla-
belled data. Furthermore, using increasing amounts
of unlabelled training data (of which there is plenty
available) is a workable way to obtain small im-
provements in performance, at the expense of train-
ing time. Our current focus is on addressing specific
problems that appear on inspection of the precision
errors in test, which can improve both precision and
recall to a degree. Future planned lines of research
include using WordNet for improving the semantic
aspects of the algorithm (semantic classes and simi-
larity), and studying forms of combining the patterns
obtained in this semi-supervised approach with su-
pervised learning.
References
M. Collins and Y. Singer. 1999. Unsupervised mod-
els for named entity classification. In Proceedings of
the Joint SIGDAT Conference on Empirical Methods
in Natural Language Processing and Very Large Cor-
pora, pages 100?110, College Park, MD. ACL.
L. Ferro, L. Gerber, I. Mani, B. Sundheim, and G. Wil-
son. 2005. Tides 2005 standard for the annotation of
temporal expressions. Technical report, MITRE Cor-
poration.
E. Filatova and E. Hovy. 2001. Assigning time-stamps to
event-clauses. In Proceedings of the 2001 ACL Work-
shop on Temporal and Spatial Information Processing,
pages 88?95.
K. Hacioglu, Y. Chen, and B. Douglas. 2005. Automatic
time expression labelling for english and chinese text.
In Proc. of the 6th International Conference on Intel-
ligent Text Processing and Computational Linguistics
(CICLing), pages 548?559. Springer.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Seman-
tic class learning from the web with hyponym pattern
linkage graphs. In Proc. of the Association for Com-
putational Linguistics 2008 (ACL-2008:HLT), pages
1048?1056.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proceedings of the 17th International
Conference on Computational Linguistics and the 36th
Annual Meeting of the Association for Computational
Linguistics (COLING-ACL-98), pages 768?774, Mon-
treal, Quebec. ACL.
I. Mani and G. Wilson. 2000. Robust temporal process-
ing of news. In Proceedings of the 38th Annual Meet-
ing of the Association for Computational Linguistics,
pages 69?76, Morristown, NJ, USA. ACL.
M. Pas?ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Names and similarities on the web: Fact extrac-
tion in the fast lane. In Proceedings of the 21th In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the ACL, pages 809?816.
ACL.
E. Riloff. 1996. Automatically generating extraction pat-
terns from untagged text. In Proceedings of the Thir-
teenth National Conference on Artificial Intelligence
(AAAI-96), pages 1044?1049. AAAI/MIT Press.
E. Saquete, R. Mun?oz, and P. Mart??nez-Barco. 2004.
Event ordering using terseo system. In Proc. of the
9th International Conference on Application of Natu-
ral Language to Information Systems (NLDB), pages
39?50. Springer.
M. Stevenson and M. Greenwood. 2005. A semantic
approach to IE pattern induction. In Proceedings of
the 43rd Meeting of the Association for Computational
Linguistics, pages 379?386. ACL.
M. Surdeanu, J. Turmo, and A. Ageno. 2006. A hybrid
approach for the acquisition of information extraction
patterns. In Proceedings of the EACL 2006 Workshop
on Adaptive Text Extraction and Mining (ATEM 2006).
ACL.
J. M. Wiebe, T. P. O?Hara, T. Ohrstrom-Sandgren, and
K. J. McKeever. 1998. An empirical approach to tem-
poral reference resolution. Journal of Artificial Intelli-
gence Research, 9:247?293.
R. Yangarber, R. Grishman, P. Tapanainen, and
S. Hutunen. 2000. Automatic acquisition of domain
knowledge for information extraction. In Proceedings
of the 18th International Conference of Computational
Linguistics, pages 940?946.
R. Yangarber. 2003. Counter-training in discovery of
semantic patterns. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics. ACL.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association
for Computational Linguistics, pages 189?196, Cam-
bridge, MA. ACL.
57
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 644?651, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Robust Combination Strategy for Semantic Role Labeling
Llu??s Ma`rquez, Mihai Surdeanu, Pere Comas, and Jordi Turmo
Technical University of Catalunya
Barcelona, Spain
{lluism,surdeanu,pcomas,turmo}@lsi.upc.edu
Abstract
This paper focuses on semantic role la-
beling using automatically-generated syn-
tactic information. A simple and robust
strategy for system combination is pre-
sented, which allows to partially recover
from input parsing errors and to signif-
icantly boost results of individual sys-
tems. This combination scheme is also
very flexible since the individual systems
are not required to provide any informa-
tion other than their solution. Extensive
experimental evaluation in the CoNLL-
2005 shared task framework supports our
previous claims. The proposed architec-
ture outperforms the best results reported
in that evaluation exercise.
1 Introduction
The task of Semantic Role Labeling (SRL), i.e.
the process of detecting basic event structures
such as who did what to whom, when and where,
has received considerable interest in the past few
years (Gildea and Jurafsky, 2002; Surdeanu et al,
2003; Xue and Palmer, 2004; Pradhan et al, 2005a;
Carreras and Ma`rquez, 2005). It was shown that
the identification of such event frames has a signif-
icant contribution for many Natural Language Pro-
cessing (NLP) applications such as Information Ex-
traction (Surdeanu et al, 2003) and Question An-
swering (Narayanan and Harabagiu, 2004).
Most current SRL approaches can be classified
in one of two classes: approaches that take ad-
vantage of complete syntactic analysis of text, pi-
oneered by Gildea and Jurafsky (2002), and ap-
proaches that use partial syntactic analysis, cham-
pioned by previous evaluations performed within
the Conference on Computational Natural Language
Learning (CoNLL) (Carreras and Ma`rquez, 2004).
The wisdom extracted from this volume of work in-
dicates that full syntactic analysis has a significant
contribution to the SRL performance, when using
hand-corrected syntactic information.
On the other hand, when only automatically-
generated syntax is available, the quality of the in-
formation provided through full syntax decreases
because the state-of-the-art of full parsing is less
robust and performs worse than the tools used for
partial syntactic analysis. Under such real-world
conditions, the difference between the two SRL ap-
proaches (with full or partial syntax) is not that high.
More interestingly, the two SRL strategies perform
better for different semantic roles. For example,
models that use full syntax recognize better agent
and theme roles, whereas models based on partial
syntax are better at recognizing explicit patient roles,
which tend to be farther from the predicate and accu-
mulate more parsing errors (Ma`rquez et al, 2005).
The above observations motivate the work pre-
sented in this paper. We introduce a novel semantic
role labeling approach that combines several indi-
vidual SRL systems. Intuitively, our approach can
be separated in two stages: a candidate generation
phase, where the solutions provided by several indi-
vidual models are combined into a pool of candidate
arguments, and an inference phase, where the candi-
dates are filtered using a binary classifier, and possi-
644
The luxury auto maker last year sold 1,214 cars in the U.S.
PPNP
VPNPNP
S
ARG0 ARGM?TMP P ARG1 ARGM?LOC
Figure 1: Sample PropBank sentence.
ble conflicts with domain knowledge constraints are
resolved to obtain the final solution.
For robustness, the inference model uses only
global attributes extracted from the solutions pro-
vided by the individual systems, e.g., the sequence
of role labels generated by each system for the cur-
rent predicate. We do not use any attributes spe-
cific to the individual models, not even the confi-
dence assigned by the individual classifiers. Besides
simplicity, the consequence of this decision is that
our approach does not impose any restrictions on the
individual SRL strategies, as long as one solution
is provided for each predicate. On the other hand,
probabilistic inference processes, which have been
successfully used for SRL (Koomen et al, 2005),
mandate that each individual candidate argument be
associated with its raw activation, or confidence, in
the given model. However, this information is not
directly available in two out of three of our individ-
ual models, which classify argument chunks and not
entire arguments.
Despite its simplicity, our approach obtains en-
couraging results: the combined system outperforms
any of the individual systems and, using exactly the
same data, it is also competitive with the best SRL
systems that participated in the latest CoNLL shared
task evaluation (Carreras and Ma`rquez, 2005).
2 Semantic Corpora
In this paper we report results using PropBank, an
approximately one-million-word corpus annotated
with predicate-argument structures (Kingsbury et
al., 2002). To date, PropBank addresses mainly
predicates lexicalized by verbs and a small num-
ber of predicates lexicalized by verb nominalizations
and adjectives.
The arguments of each predicate are numbered se-
quentially from ARG0 to ARG5. Generally, ARG0
stands for agent, ARG1 for theme or direct ob-
ject, and ARG2 for indirect object, benefactive or
instrument, but mnemonics tend to be verb spe-
cific. Additionally, predicates might have ?adjunc-
tive arguments?, referred to as ARGMs. For example,
ARGM-LOC indicates a locative and ARGM-TMP in-
dicates a temporal. Figure 1 shows a sample sen-
tence where one predicate (?sold?) has 4 arguments.
In a departure from ?traditional? SRL approaches
that train on the hand-corrected syntactic trees as-
sociated with PropBank, we do not use any syn-
tactic information from PropBank. Instead, we
develop our models using automatically-generated
syntax and named-entity (NE) labels, made avail-
able by the CoNLL shared task evaluation (Carreras
and Ma`rquez, 2005). From the CoNLL data, our
individual models based on full syntactic analysis
use the trees generated by the Charniak parser. The
partial-syntax model uses the chunk? i.e. basic syn-
tactic phrase ? labels and clause boundaries. All in-
dividual models make use of the provided NE labels.
Following the CoNLL-2005 setting we evaluated
our system also on a fresh test set, derived from the
Brown corpus. This second evaluation allows us to
re-enforce our robustness claim.
3 Approach Overview
The proposed architecture, summarized in Figure 2,
consists of two stages: a candidate generation phase
and an inference stage.
In the candidate generation step, we merge the so-
lutions of three individual SRL models into a unique
pool of candidate arguments. The proposed models
range from complete reliance on full parsing to us-
ing only partial syntactic information. The first two
models, Model 1 and 2, are developed as sequential
taggers (using the BIO tagging scheme) on a shared
framework. The major difference between the two
models is that Model 1 uses only partial syntactic
information (basic phrases and clause boundaries),
whereas Model 2 uses complete syntactic informa-
tion. To maximize diversity, Model 3 implements
a different strategy: it models only arguments that
map into exactly one syntactic constituent. Section 4
details all three individual models.
The inference stage starts with candidate filtering,
645
Candidate Filtering
Reliance on full syntax
Model 1 Model 2 Model 3
Conflict Resolution
Inference
Candidate
Generation
Figure 2: Architecture of the proposed system.
which reduces the number of candidate arguments
in the pool using a single binary classifier. Using
this classifier?s confidence values and a number of
domain-specific constraints, e.g. no two arguments
can overlap, the conflict resolution component en-
forces the consistency of the final solution using a
straightforward greedy strategy. The complete in-
ference model is detailed in Section 5.
4 Individual SRL Models
Models 1 and 2. These models approach SRL as
a sequential tagging task. In a pre-process step, the
input syntactic structures are traversed in order to
select a subset of constituents organized sequentially
(i.e. non embedding). Model 1 makes use only of
the partial tree defined by base chunks and clause
boundaries, while Model 2 explores full parse trees.
Precisely, the sequential tokens are selected as fol-
lows. First, the input sentence is splitted into dis-
joint segments by considering the clause boundaries
given by the syntactic structure. Second, for each
segment, the set of top-most non-overlapping syn-
tactic constituents completely falling inside the seg-
ment are selected as tokens. Note that this strategy
provides a set of sequential tokens covering the com-
plete sentence. Also, it is independent of the syn-
tactic annotation explored, given it provides clause
boundaries ? see (Ma`rquez et al, 2005) for more
details.
Due to this pre-processing stage, the upper-bound
recall figures are 95.67% for Model 1 and 90.32%
for Model 2 using the datasets defined in Section 6.
The nodes selected are labeled with B-I-O tags
(depending if they are at the beginning, inside, or
outside of a predicate argument) and they are con-
verted into training examples by considering a rich
set of features, mainly borrowed from state-of-the-
art systems. These features codify properties from:
(a) the argument constituent, (b) the target predicate,
Constituent type and head: extracted using common head-
word rules. If the first element is a PP chunk, then the
head of the first NP is extracted.
First and last words and POS tags of the constituent.
POS sequence: if it is less than 5 tags long.
2/3/4-grams of the POS sequence.
Bag-of-words of nouns, adjectives, and adverbs.
TOP sequence: sequence of types of the top-most syntactic
elements in the constituent (if it is less than 5 elements long).
In the case of full parsing this corresponds to the right-hand
side of the rule expanding the constituent node.
2/3/4-grams of the TOP sequence.
Governing category as in (Gildea and Jurafsky, 2002).
NamedEnt, indicating if the constituent embeds or
strictly matches a named entity along with its type.
TMP, indicating if the constituent embeds or strictly matches
a temporal keyword (extracted from AM-TMP arguments of
the training set).
Previous and following words and POS of the constituent.
The same features characterizing focus constituents are
extracted for the two previous and following tokens, provided
they are inside the clause boundaries of the codified region.
Table 1: Constituent structure features: Models 1/2
Predicate form, lemma, and POS tag.
Chunk type and type of verb phrase in which verb is
included: single-word or multi-word.
The predicate voice. We currently distinguish five voice
types: active, passive, copulative, infinitive, and progressive.
Binary flag indicating if the verb is a start/end of a clause.
Sub-categorization rule, i.e. the phrase structure rule that
expands the predicate immediate parent.
Table 2: Predicate structure features: Models 1/2
and (c) the distance between the argument and pred-
icate. The three feature sets are listed in Tables 1, 2,
and 3, respectively.1
Regarding the learning algorithm, we used gener-
alized AdaBoost with real-valued weak classifiers,
which constructs an ensemble of decision trees of
fixed depth (Schapire and Singer, 1999). We con-
sidered a one-vs-all decomposition into binary prob-
lems to address multi-class classification. AdaBoost
binary classifiers are then used for labeling test se-
quences, from left to right, using a recurrent sliding
window approach with information about the tag as-
signed to the preceding token. This tagging module
enforces some basic constraints, e.g., BIO correct
structure, arguments cannot overlap with clause nor
chunk boundaries, discard ARG0-5 arguments not
present in PropBank frames for a certain verb, etc.
1Features extracted from partial parsing and Named Entities
are common to Model 1 and 2, while features coming from full
parse trees only apply to Model 2.
646
Relative position, distance in words and chunks, and level of
embedding (in #clause-levels) with respect to the constituent.
Constituent path as described in (Gildea and Jurafsky, 2002)
and all 3/4/5-grams of path constituents beginning at the
verb predicate or ending at the constituent.
Partial parsing path as described in (Carreras et al, 2004)
and all 3/4/5-grams of path elements beginning at the
verb predicate or ending at the constituent.
Syntactic frame as described by Xue and Palmer (2004)
Table 3: Predicate?constituent features: Models 1/2
The syntactic label of the candidate constituent.
The constituent head word, suffixes of length 2, 3, and 4,
lemma, and POS tag.
The constituent content word, suffixes of length 2, 3, and
4, lemma, POS tag, and NE label. Content words, which
add informative lexicalized information different from
the head word, were detected using the heuristics
of (Surdeanu et al, 2003).
The first and last constituent words and their POS tags.
NE labels included in the candidate phrase.
Binary features to indicate the presence of temporal cue
words, i.e. words that appear often in AM-TMP phrases
in training.
For each TreeBank syntactic label we added a feature to
indicate the number of such labels included in the
candidate phrase.
The sequence of syntactic labels of the constituent
immediate children.
The phrase label, head word and POS tag of the
constituent parent, left sibling, and right sibling.
Table 4: Constituent structure features: Model 3
Model 3. The third individual SRL model makes
the strong assumption that each predicate argument
maps to one syntactic constituent. For example, in
Figure 1 ARG0 maps to a noun phrase, ARGM-LOC
maps to a prepositional phrase etcetera. This as-
sumption holds well on hand-corrected parse trees
and simplifies significantly the SRL process because
only one syntactic constituent has to be correctly
classified in order to recognize one semantic argu-
ment. On the other hand, this approach is limited
when using automatically-generated syntactic trees.
For example, only slightly over 91% of the argu-
ments can be mapped to one of the syntactic con-
stituents produced by the Charniak parser.
Using a bottom-up approach, Model 3 maps each
argument to the first syntactic constituent that has
the exact same boundaries and then climbs as high as
possible in the tree across unary production chains.
We currently ignore all arguments that do not map
to a single syntactic constituent.
The predicate word and lemma.
The predicate voice. Same definition as Models 1 and 2.
A binary feature to indicate if the predicate is frequent
(i.e., it appears more than twice in the training data) or not.
Sub-categorization rule. Same def. as Models 1 and 2.
Table 5: Predicate structure features: Model 3
The path in the syntactic tree between the argument phrase
and the predicate as a chain of syntactic labels along with
the traversal direction (up or down).
The length of the above syntactic path.
The number of clauses (S* phrases) in the path.
The number of verb phrases (VP) in the path.
The subsumption count, i.e. the difference between the
depths in the syntactic tree of the argument and predicate
constituents. This value is 0 if the two phrases share the
same parent.
The governing category, which indicates if NP
arguments are dominated by a sentence (typical for
subjects) or a verb phrase (typical for objects).
We generalize syntactic paths with more than 3
elements using two templates:
(a) Arg ? Ancestor ? Ni ? Pred, where Arg is the
argument label, Pred is the predicate label, Ancestor
is the label of the common ancestor, and Ni is instantiated
with all the labels between Pred and Ancestor in
the full path; and
(b) Arg ? Ni ? Ancestor ? Pred, where Ni is
instantiated with all the labels between Arg and
Ancestor in the full path.
The surface distance between the predicate and the
argument phrases encoded as: the number of tokens, verb
terminals (VB*), commas, and coordinations (CC) between
the argument and predicate phrases, and a binary feature to
indicate if the two constituents are adjacent.
A binary feature to indicate if the argument starts with a
predicate particle, i.e. a token seen with the RP* POS
tag and directly attached to the predicate in training.
Table 6: Predicate?constituent features: Model 3
Once the mapping process completes, Model 3
extracts a rich set of lexical, syntactic, and seman-
tic features. Tables 4, 5, and 6 present these features
organized in the same three categories as the previ-
ous Models 1 and 2 ? see (Surdeanu and Turmo,
2005) for more details.
Similarly with Models 1 and 2, Model 3 trains
one-vs-all classifiers using AdaBoost for the most
common argument labels. To reduce the sample
space, Model 3 selects training examples (both posi-
tive and negative) only from: (a) the first clause that
includes the predicate, or (b) from phrases that ap-
pear to the left of the predicate in the sentence. More
than 98% of the argument constituents fall into one
of these classes.
At prediction time the classifiers are combined us-
ing a simple greedy technique that iteratively assigns
647
to each predicate the argument classified with the
highest confidence. For each predicate we consider
as candidates all AM attributes, but only numbered
attributes indicated in the corresponding PropBank
frame. Additionally, this greedy strategy enforces a
limited number of domain knowledge constraints in
the generated solution: (a) arguments can not over-
lap in any form, and (b) no duplicate arguments are
allowed for ARG0-5.
5 The Inference Model
The most important component of our inference
model is candidate filtering, which decides if a can-
didate argument should be maintained in the global
solution or not. Candidate filtering is implemented
as a single binary classifier that uses only features
extracted from the solutions provided by the individ-
ual systems. For robustness, we do not use any fea-
tures that are specific to any of the individual mod-
els, nor the confidence value of their classifiers.
Table 7 lists the features extracted from each can-
didate argument by the filtering classifier. For sim-
plicity we have focused only on attributes that: (a)
are readily available in the solutions proposed by the
individual classifiers, and (b) allow the gathering of
simple and robust statistics. For example, the fil-
tering classifier might learn that a candidate is to be
trusted if: (a) two individual systems proposed it, (b)
if its label is ARG2 and it was generated by Model 1,
or (c) if it was proposed by Model 2 within a certain
argument sequence.
The candidate arguments that pass the filtering
stage are incorporated in the global solution by the
conflict resolution module, which enforces several
domain specific constraints. We have currently im-
plemented two constraints: (a) arguments can not
overlap or embed other arguments; and (b) no du-
plicate arguments are allowed for the numbered ar-
guments ARG0-5. Theoretically, the set of con-
straints can be extended with any other rules, but in
our particular case, we know that some constraints,
e.g. providing only arguments indicated in the cor-
responding PropBank frame, are already guaranteed
by the individual models. Conflicts are solved with
a straightforward greedy strategy: the pool of candi-
date arguments is inspected in descending order of
the confidence values assigned by the filtering clas-
The label of the candidate argument.
The number of systems that generated an argument with
this label and span.
The unique ids, e.g. M1 and M2, of all the systems that
generated an argument with this label and span.
The argument sequence for this predicate for all the systems
that generated an argument with this label and span. For
example, the argument sequence for the proposition
illustrated in Figure 1 is: ARG0 - ARGM-TMP - P -
ARG1 - ARGM-LOC.
The number and unique ids of all the systems that generated
an argument with the same span but different label.
The number and unique ids of all the systems that generated
an argument included in the current argument.
The number and unique ids of all the systems that generated
an argument that contains the current argument.
The number and unique ids of all the systems that generated
an argument that overlaps the current argument.
Table 7: Features used by the candidate filtering
classifier.
sifier, and candidates are appended to the global so-
lution only if they do not violate any of the domain
constraints with the arguments already selected. Our
inference system currently has a sequential architec-
ture, i.e. no feedback is sent from the conflict reso-
lution module to candidate filtering.
6 Experimental Results
We trained the individual models using the complete
CoNLL-2005 training set (PropBank/TreeBank sec-
tions 2 to 21). All models were developed using
AdaBoost with decision trees of depth 4 (i.e. each
branch may represent a conjunction of at most 4 ba-
sic features). Each classification model was trained
for up to 2,000 rounds.
We applied some simplifications to keep training
times and memory requirements inside admissible
bounds: (a) we have limited the number of nega-
tive examples in Model 3 to the first 500,000; (b)
we have trained only the most frequent argument la-
bels: top 41 for Model 1, top 35 for Model 2, and
top 24 for Model 3; and (c) we discarded all features
occurring less than 15 times in the training set.
The models were tuned on a separate develop-
ment partition (TreeBank section 24) and evaluated
on two corpora: (a) TreeBank section 23, which
consists of Wall Street Journal (WSJ) documents,
and (b) on three sections of the Brown corpus, se-
mantically annotated by the PropBank team for the
CoNLL 2005 shared task evaluation. Table 8 sum-
648
WSJ PProps Precision Recall F?=1
Model 1 48.45% 78.76% 72.44% 75.47 ?0.8
Model 2 52.04% 79.65% 74.92% 77.21 ?0.8
Model 3 45.28% 80.32% 72.95% 76.46 ?0.6
Brown PProps Precision Recall F?=1
Model 1 30.85% 67.72% 58.29% 62.65 ?2.1
Model 2 36.44% 71.82% 64.03% 67.70 ?1.9
Model 3 29.48% 72.41% 59.67% 65.42 ?2.1
Table 8: Overall results of the individual models on
the WSJ and Brown test sets.
marizes the results of the three models on the WSJ
and Brown corpora. In that table we include the
percentage of perfect propositions detected by each
model (?PProps?), i.e. predicates recognized with
all their arguments, the overall precision, recall, and
F?=1 measure2.
The results summarized in Table 8 indicate that
all individual systems have a solid performance. Al-
though none of them would rank in the top 3 in this
year?s CoNLL evaluation (Carreras and Ma`rquez,
2005), their performance is comparable to the best
individual systems presented at that evaluation exer-
cise3. As expected, the models based on full parsing
(2 and 3) perform better than the model based on
partial syntax. But, interestingly, the difference is
not large (e.g., less than 2 points in F?=1 in the WSJ
corpus), evincing that having base syntactic chunks
and clause boundaries is enough to obtain a compet-
itive performance with a simple system.
Consistently with other systems evaluated on the
Brown corpus, all our models experience a severe
performance drop in this corpus, due to the lower
performance of the linguistic processors.
6.1 Performance of Combination Systems
We have trained the candidate filtering binary classi-
fier on one third of the training partition. Its training
data was generated using individual models trained
on the other two thirds of the training partition. The
classifier was developed using Support Vector Ma-
chines (SVM) with a polynomial kernel of degree 2.
We trained combined models for all 4 possible com-
binations of our 3 individual models.
2The significance intervals for the F1 measure have been ob-
tained using bootstrap resampling (Noreen, 1989). F1 rates out-
side of these intervals are assumed to be significantly different
from the related F1 rate (p < 0.05).
3The best performing SRL systems at CoNLL were a com-
bination of several subsystems. See section 7 for details.
Table 9 summarizes the performance of the com-
bined systems on the WSJ and Brown corpora.4
The combined systems are compared against a base-
line combination system, which merges all the argu-
ments generated by the individual systems. For con-
flict resolution, the baseline uses the greedy strategy
introduced in Section 5, but using as argument or-
dering criterion a radix sort that orders the candidate
arguments in descending order of: number of mod-
els that agreed on this argument; argument length in
tokens; and performance of the individual system5.
Table 9 indicates that our combination strategy is
always successful: the results of all combined sys-
tems improve upon their individual models and they
are better the baseline when using the same num-
ber of individual models. As expected, the highest
scoring combined system includes all three individ-
ual models. Its F?=1 measure is 2.35 points higher
than the best individual model (Model 2) in the WSJ
test set and 1.30 points higher in the Brown test
set. Somewhat surprisingly, the highest percentage
of perfect propositions is not obtained by the over-
all best combination, but by the system that com-
bines the two models based on full parsing (Models
2 and 3). This happens because Model 1 is the weak-
est performing of the bunch, hence its arguments,
while providing useful information to the filtering
classifier, decrease the number of perfect proposi-
tions when selected.
We consider these results encouraging given the
simplicity of our inference model and the limited
amount of training data used to train the candidate
filtering classifier. Additionally, they compare fa-
vorably with respect to the best performing systems
at CoNLL-2005 shared task (see Section 7).
6.2 Upper Limit of the Combination Strategy
To explore the potential of our approach we have
constructed a hypothetical system where our candi-
date filtering module is replaced with a perfect clas-
sifier that selects only correct arguments and dis-
cards all others. Table 10 lists the results obtained
on the WSJ and Brown corpora by this hypothetical
system using all three individual models.
4For conciseness, in Table 9 we introduced the notation
M1+2+3 to indicate the combination of Models 1, 2, and 3
5This combination produced the highest-scoring baseline
model.
649
WSJ PProps Prec. Recall F?=1
M1+2 51.30% 81.30% 74.13% 77.55 ?0.7
M1+3 47.26% 81.21% 73.36% 77.08 ?0.8
M2+3 52.65% 81.55% 75.30% 78.30 ?0.7
M1+2+3 51.64% 84.89% 74.87% 79.56 ?0.7
baseline 51.09% 77.29% 78.67% 77.98 ?0.7
Brown PProps Prec. Recall F?=1
M1+2 35.95% 73.70% 62.93% 67.89 ?2.0
M1+3 28.98% 72.83% 58.84% 65.09 ?2.2
M2+3 37.06% 73.89% 63.30% 68.18 ?2.2
M1+2+3 34.20% 78.66% 61.46% 69.00 ?2.1
baseline 33.58% 67.66% 66.01% 66.82 ?1.8
Table 9: Overall results of the combination models
on the WSJ and Brown test sets.
Perfect props Precision Recall F?=1
WSJ 70.76% 99.12% 85.22% 91.64
Brown 51.87% 99.63% 74.32% 85.14
Table 10: Performance upper limit on the test sets.
Table 10 indicates that the upper limit of proposed
approach is relatively high: the F?=1 of this hy-
pothetical system is over 12 points higher than our
best combined system in the WSJ test set, and over
16 points higher in the Brown corpus. These re-
sults indicate that the potential of our combination
strategy is high, especially when compared with re-
ranking strategies, which are limited to the perfor-
mance of the best complete solution in the candidate
pool. By allowing the re-combination of arguments
from the individual candidate solutions we raise this
threshold significantly. Table 11 lists the contribu-
tion of the individual models to this upper limit on
the WSJ corpus. For conciseness, we list only the
?core? numbered arguments. ?? of 3? indicates the
percentage of correct arguments where all 3 mod-
els agreed, ?? of 2? indicates the percentage of cor-
rect arguments where any 2 models agreed, and the
other columns indicate the percentage of correct ar-
guments detected by a single model. Table 11 indi-
cates that, as expected, two or more individual mod-
els agreed on a large percentage of the correct argu-
ments. Nevertheless, a significant number of correct
arguments, e.g. over 22% of ARG3, come from a
single individual system. This proves that, in order
to achieve maximum performance, one has to look
beyond simple voting strategies that favor arguments
with high agreement between individual systems.
? of 3 ? of 2 M1 M2 M3
ARG0 80.45% 12.10% 3.47% 2.14% 1.84%
ARG1 69.82% 17.83% 7.45% 2.77% 2.13%
ARG2 56.04% 22.32% 12.20% 4.95% 4.49%
ARG3 56.03% 21.55% 12.93% 5.17% 4.31%
ARG4 65.85% 20.73% 6.10% 2.44% 4.88%
Table 11: Contribution of the individual systems to
the upper limit, for ARG0?ARG4 in the WSJ test set.
WSJ Brown
PProps F?=1 PProps F?=1
koomen 53.79% 79.44 ?0.8 32.34% 67.75 ?1.8
haghighi 56.52% 78.45 ?0.8 37.06% 67.71 ?2.0
pradhan 50.14% 77.37 ?0.7 36.44% 67.07 ?2.0
Table 12: Results of the best combined systems at
CoNLL-2005.
7 Related Work
The best performing systems at the CoNLL-2005
shared task included a combination of different base
subsystems to increase robustness and to gain cover-
age and independence from parse errors. Therefore,
they are closely related to the work of this paper.
Table 12 summarizes their results under exactly the
same experimental setting.
Koomen et al (2005) used a 2 layer architecture
similar to ours. The pool of candidates is generated
by running a full syntax SRL system on alternative
input information (Collins parsing, and 5-best trees
from Charniak?s parser). The combination of can-
didates is performed in an elegant global inference
procedure as constraint satisfaction, which, formu-
lated as Integer Linear Programming, can be solved
efficiently. Interestingly, the generalized inference
layer allows to include in the objective function,
jointly with the candidate argument scores, a num-
ber of linguistically-motivated constraints to obtain
a coherent solution. Differing from the strategy pre-
sented in this paper, their inference layer does not
include learning. Also, they require confidence val-
ues from individual classifiers. This is the best per-
forming system at CoNLL-2005.
Haghighi et al (2005) implemented a double re-
ranking model on top of the base SRL models to se-
lect the most probable solution among a set of can-
didates. The re-ranking is performed, first, on a set
of n-best solutions obtained by the base system run
on a single parse tree, and, then, on the set of best-
candidates coming from the n-best parse trees. The
650
re-ranking approach allows to define global complex
features applying to complete candidate solutions to
train the rankers. The main drawback, compared to
our approach, is that re-ranking does not permit to
combine different solutions since it is forced to se-
lect a complete candidate solution. This fact implies
that the performance upper limit strongly depends
on the ability of the base model to generate the com-
plete correct solution in the set of n-best candidates.
Finally, Pradhan et al (2005b) followed a stack-
ing approach by learning two individual systems
based on full syntax, whose outputs are used to
generate features to feed the training stage of a fi-
nal chunk-by-chunk SRL system. Although the fine
granularity of the chunking-based system allows to
recover from parsing errors, we find this combina-
tion scheme quite ad-hoc because it forces to break
argument candidates into chunks in the last stage.
8 Conclusions
This paper introduces a novel, robust combination
strategy for semantic role labeling. Our approach
is separated in two stages: a candidate generation
phase, which combines the solutions generated by
several individual models into a pool of candidate ar-
guments, followed by a simple inference model that
filters the candidate arguments using a single binary
classifier and then enforces an arbitrary number of
domain-specific constraints.
The proposed approach has several advantages.
First, because it combines the solutions provided by
the individual models, the inference model can re-
cover from errors produced in the generation phase.
Second, due to the diversity of the individual models
employed, the candidate pool contains a high per-
centage of the correct arguments. And lastly, our
approach is flexible and robust: it can incorporate
any SRL model in the candidate generation stage
because it does not require that the individual SRL
models provide any information, e.g. classification
confidence values, other than an argument solution.
Our results are better than the state of the art us-
ing automatically-generated syntactic information.
These results are encouraging considering the sim-
plicity of the proposed approach.
Acknowledgments
This research has been partially supported by the
European Commission (CHIL project, IP-506909).
Mihai Surdeanu is a research fellow within the
Ramo?n y Cajal program of the Spanish Ministry of
Education and Science.
References
X. Carreras and L. Ma`rquez. 2004. Introduction to the CoNLL-
2004 shared task: Semantic role labeling. In Proceedings of
CoNLL 2004.
X. Carreras and L. Ma`rquez. 2005. Introduction to the CoNLL-
2005 Shared Task: Semantic Role Labeling. In Proceedings
of CoNLL-2005.
X. Carreras, L. Ma`rquez, and G. Chrupa?a. 2004. Hierarchical
recognition of propositional arguments with perceptrons. In
Proceedings of CoNLL 2004 shared task.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3).
A. Haghighi, K. Toutanova, and C. Manning. 2005. A joint
model for semantic role labeling. In Proceedings of CoNLL-
2005 shared task.
P. Kingsbury, M. Palmer, and M. Marcus. 2002. Adding se-
mantic annotation to the Penn Treebank. In Proceedings of
the Human Language Technology Conference.
P. Koomen, V. Punyakanok, D. Roth, and W. Yih. 2005. Gen-
eralized inference with multiple semantic role labeling sys-
tems. In Proceedings of CoNLL-2005 shared task.
L. Ma`rquez, P. Comas, J. Gime?nez, and N. Catala`. 2005. Se-
mantic role labeling as sequential tagging. In Proceedings of
CoNLL-2005 shared task.
S. Narayanan and S. Harabagiu. 2004. Question answering
based on semantic structures. In International Conference
on Computational Linguistics (COLING 2004).
E. W. Noreen. 1989. Computer-Intensive Methods for Testing
Hypotheses. John Wiley & Sons.
S. Pradhan, K. Hacioglu, V. Krugler, W. Ward, J. H. Martin, and
D. Jurafsky. 2005a. Support vector learning for semantic
argument classification. Machine Learning, to appear.
S. Pradhan, K. Hacioglu, W. Ward, J. H. Martin, and D. Juraf-
sky. 2005b. Semantic role chunking combining complemen-
tary syntactic views. In Proceedings of CoNLL-2005.
R. E. Schapire and Y. Singer. 1999. Improved boosting algo-
rithms using confidence-rated predictions. Machine Learn-
ing, 37(3).
M. Surdeanu and J. Turmo. 2005. Semantic role labeling using
complete syntactic analysis. In Proceedings of CoNLL-2005
shared task.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth. 2003.
Using predicate-argument structures for information extrac-
tion. In Proceedings of ACL 2003.
N. Xue and M. Palmer. 2004. Calibrating features for semantic
role labeling. In Proceedings of EMNLP-2004.
651
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 221?224, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Labeling Using Complete Syntactic Analysis
Mihai Surdeanu
Technical University of Catalunya
surdeanu@lsi.upc.edu
Jordi Turmo
Technical University of Catalunya
turmo@lsi.upc.edu
Abstract
In this paper we introduce a semantic role
labeling system constructed on top of the
full syntactic analysis of text. The la-
beling problem is modeled using a rich
set of lexical, syntactic, and semantic at-
tributes and learned using one-versus-all
AdaBoost classifiers.
Our results indicate that even a simple ap-
proach that assumes that each semantic ar-
gument maps into exactly one syntactic
phrase obtains encouraging performance,
surpassing the best system that uses par-
tial syntax by almost 6%.
1 Introduction
Most current semantic role labeling (SRL) ap-
proaches can be classified in one of two classes:
approaches that take advantage of complete syntac-
tic analysis of text, pioneered by (Gildea and Juraf-
sky, 2002), and approaches that use partial syntac-
tic analysis, championed by the previous CoNLL
shared task evaluations (Carreras and Ma`rquez,
2004).
However, to the authors? knowledge, a clear anal-
ysis of the benefits of using full syntactic analysis
versus partial analysis is not yet available. On one
hand, the additional information provided by com-
plete syntax should intuitively be useful. But, on
the other hand, the state-of-the-art of full parsing
is known to be less robust and perform worse than
the tools used for partial syntactic analysis, which
would decrease the quality of the information pro-
vided. The work presented in this paper contributes
to this analysis by introducing a model that is en-
tirely based on the full syntactic analysis of text,
generated by a real-world parser.
2 System Description
2.1 Mapping Arguments to Syntactic
Constituents
Our approach maps each argument label to one syn-
tactic constituent, using a strategy similar to (Sur-
deanu et al, 2003). Using a bottom-up approach,
we map each argument to the first phrase that has the
exact same boundaries and climb as high as possible
in the syntactic tree across unary production chains.
Unfortunately, this one-to-one mapping between
semantic arguments and syntactic constituents is not
always possible. One semantic argument may be
mapped to many syntactic constituents due to: (a)
intrinsic differences between the syntactic and se-
mantic representations, and (b) incorrect syntactic
structure. Figure 1 illustrates each one of these sit-
uations: Figure 1 (a) shows a sentence where each
semantic argument correctly maps to one syntac-
tic constituent; Figure 1 (b) illustrates the situation
where one semantic argument correctly maps to two
syntactic constituents; and Figure 1 (c) shows a one-
to-many mapping caused by an incorrect syntactic
structure: argument A0 maps to two phrases, the ter-
minal ?by? and the noun phrase ?Robert Goldberg?,
due to the incorrect attachment of the last preposi-
tional phrase, ?at the University of California?.
Using the above observations, we separate one-
221
rising consumer prices
VBG NN NNS
NP
P A1
developed by Robert Goldberg at the University of California
NP
PPNP
NP
PP
VP
P A0 AM?LOC
The luxury auto maker last year sold 1,214 cars in the U.S.
PPNP
VPNPNP
S
A0 A1PAM?TMP AM?LOC
(b)(a) (c)
Figure 1: Mapping semantic arguments to syntactic constituents: (a) correct one?to-one mapping; (b) correct
one-to-many mapping; (c) one-to-many mapping due to incorrect syntax.
(a) (b) (c)
Training 96.06% 2.49% 1.45%
Development 91.36% 4.83% 3.81%
Table 1: Distribution of semantic arguments accord-
ing to their mapping to syntactic constituents ob-
tained with the Charniak parser: (a) one-to-one, (b)
one-to-many, all syntactic constituents have same
parent, (c) one-to-many, syntactic constituents have
different parents.
to-many mappings in two classes: (a) when the syn-
tactic constituents mapped to the semantic argument
have the same parent (Figure 1 (b)) the mapping is
correct and/or could theoretically be learned by a
sequential SRL strategy, and (b) when the syntac-
tic constituents mapped to the same argument have
different parents, the mapping is generally caused
by incorrect syntax. Such cases are very hard to be
learned due to the irregularities of the parser errors.
Table 1 shows the distribution of semantic argu-
ments into one of the above classes, using the syn-
tactic trees provided by the Charniak parser. For the
results reported in this paper, we model only one-
to-one mappings between semantic arguments and
syntactic constituents. A subset of the one-to-many
mappings are addressed with a simple heuristic, de-
scribed in Section 2.4.
2.2 Features
The features incorporated in the proposed model
are inspired from the work of (Gildea and Juraf-
sky, 2002; Surdeanu et al, 2003; Pradhan et al,
2005; Collins, 1999) and can be classified into five
classes: (a) features that capture the internal struc-
ture of the candidate argument, (b) features extracted
The syntactic label of the candidate constituent.
The constituent head word, suffixes of length 2, 3, and 4,
lemma, and POS tag.
The constituent content word, suffixes of length 2, 3, and
4, lemma, POS tag, and NE label. Content words, which
add informative lexicalized information different from
the head word, were detected using the heuristics
of (Surdeanu et al, 2003).
The first and last constituent words and their POS tags.
NE labels included in the candidate phrase.
Binary features to indicate the presence of temporal cue
words, i.e. words that appear often in AM-TMP phrases
in training.
For each TreeBank syntactic label we added a feature to
indicate the number of such labels included in the
candidate phrase.
The sequence of syntactic labels of the constituent
immediate children.
Table 2: Argument structure features
The phrase label, head word and POS tag of the
constituent parent, left sibling, and right sibling.
Table 3: Argument context features
from the argument context, (c) features that describe
properties of the target predicate, (d) features gener-
ated from the predicate context, and (e) features that
model the distance between the predicate and the ar-
gument. These five feature sets are listed in Tables 2,
3, 4, 5, and 6.
2.3 Classifier
The classifiers used in this paper were devel-
oped using AdaBoost with confidence rated predic-
tions (Schapire and Singer, 1999). AdaBoost com-
bines many simple base classifiers or rules (in our
case decision trees of depth 3) into a single strong
classifier using a weighted-voted scheme. Each base
classifier is learned sequentially from weighted ex-
amples and the weights are dynamically adjusted ev-
ery learning iteration based on the behavior of the
222
The predicate word and lemma.
The predicate voice. We currently distinguish five voice
types: active, passive, copulative, infinitive, and progressive.
A binary feature to indicate if the predicate is frequent - i.e.
it appears more than twice in the training partition - or not.
Table 4: Predicate structure features
Sub-categorization rule, i.e. the phrase structure rule that
expands the predicate immediate parent, e.g.
NP? VBG NN NNS for the predicate in Figure 1 (b).
Table 5: Predicate context features
The path in the syntactic tree between the argument phrase
and the predicate as a chain of syntactic labels along with
the traversal direction (up or down).
The length of the above syntactic path.
The number of clauses (S* phrases) in the path.
The number of verb phrases (VP) in the path.
The subsumption count, i.e. the difference between the
depths in the syntactic tree of the argument and predicate
constituents. This value is 0 if the two phrases share the
same parent.
The governing category, which indicates if NP
arguments are dominated by a sentence (typical for
subjects) or a verb phrase (typical for objects).
We generalize syntactic paths with more than 3
elements using two templates:
(a) Arg ? Ancestor ? Ni ? Pred, where Arg is the
argument label, Pred is the predicate label, Ancestor
is the label of the common ancestor, and Ni is instantiated
with all the labels between Pred and Ancestor in
the full path; and
(b) Arg ? Ni ? Ancestor ? Pred, where Ni is
instantiated with all the labels between Arg and
Ancestor in the full path.
The surface distance between the predicate and the
argument phrases encoded as: the number of tokens, verb
terminals (VB*), commas, and coordinations (CC) between
the argument and predicate phrases, and a binary feature to
indicate if the two constituents are adjacent.
A binary feature to indicate if the argument starts with a
predicate particle, i.e. a token seen with the RP* POS
tag and directly attached to the predicate in training.
Table 6: Predicate-argument distance features
previously learned rules.
We trained one-vs-all classifiers for the top 24
most common arguments in training (including
R-A* and C-A*). For simplicity we do not la-
bel predicates. Following the strategy proposed
by (Carreras et al, 2004) we select training exam-
ples (both positive and negative) only from: (a) the
first S* phrase that includes the predicate, or (b)
from phrases that appear to the left of the predicate
in the sentence. More than 98% of the arguments
fall into one of these classes.
At prediction time the classifiers are combined us-
ing a simple greedy technique that iteratively assigns
to each predicate the argument classified with the
highest confidence. For each predicate we consider
as candidates all AM attributes, but only numbered
attributes indicated in the corresponding PropBank
frame.
2.4 Argument Expansion Heuristics
We address arguments that should map to more
than one terminal phrase with the following post-
processing heuristic: if an argument is mapped to
one terminal phrase, its boundaries are extended
to the right to include all terminal phrases that are
not already labeled as other arguments for the same
predicate. For example, after the system tags ?con-
sumer? as the beginning of an A1 argument in Fig-
ure 1, this heuristic extends the right boundary of
the A1 argument to include the following terminal,
?prices?.
To handle inconsistencies in the treatment of
quotes in parsing we added a second heuristic: argu-
ments are expanded to include preceding/following
quotes if the corresponding pairing quote is already
included in the argument constituent.
3 Evaluation
3.1 Data
We trained our system using positive examples ex-
tracted from all training data available. Due to mem-
ory limitations on our development machines we
used only the first 500,000 negative examples. In the
experiments reported in this paper we used the syn-
tactic trees generated by the Charniak parser. The
results were evaluated for precision, recall, and F1
using the scoring script provided by the task orga-
nizers.
3.2 Results and Discussion
Table 7 presents the results obtained by our system.
On the WSJ data, our results surpass with almost 6%
the results obtained by the best SRL system that used
partial syntax in the CoNLL 2004 shared task eval-
uation (Hacioglu et al, 2004). Even though these
numbers are not directly comparable (this year?s
shared task offers more training data), we consider
these results encouraging given the simplicity of
our system (we essentially model only one-to-one
223
Precision Recall F?=1
Development 79.14% 71.57% 75.17
Test WSJ 80.32% 72.95% 76.46
Test Brown 72.41% 59.67% 65.42
Test WSJ+Brown 79.35% 71.17% 75.04
Test WSJ Precision Recall F?=1
Overall 80.32% 72.95% 76.46
A0 87.09% 85.21% 86.14
A1 79.80% 72.23% 75.83
A2 74.74% 58.38% 65.55
A3 83.04% 53.76% 65.26
A4 77.42% 70.59% 73.85
A5 0.00% 0.00% 0.00
AM-ADV 57.82% 46.05% 51.27
AM-CAU 49.38% 54.79% 51.95
AM-DIR 62.96% 40.00% 48.92
AM-DIS 72.19% 76.25% 74.16
AM-EXT 60.87% 43.75% 50.91
AM-LOC 64.19% 52.34% 57.66
AM-MNR 63.90% 44.77% 52.65
AM-MOD 98.09% 93.28% 95.63
AM-NEG 96.15% 97.83% 96.98
AM-PNC 55.22% 32.17% 40.66
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 79.17% 73.41% 76.18
R-A0 84.85% 87.50% 86.15
R-A1 75.00% 71.15% 73.03
R-A2 60.00% 37.50% 46.15
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 68.00% 80.95% 73.91
R-AM-MNR 30.00% 50.00% 37.50
R-AM-TMP 60.81% 86.54% 71.43
V 0.00% 0.00% 0.00
Table 7: Overall results (top) and detailed results on
the WSJ test (bottom).
mappings between semantic arguments and syntac-
tic constituents). Only 0.14% out of the 75.17% F
measure obtained on the development partition are
attributed to the argument expansion heuristics in-
troduced in Section 2.4.
4 Conclusions
This paper describes a semantic role labeling sys-
tem constructed on top of the complete syntactic
analysis of text. We model semantic arguments that
map into exactly one syntactic phrase (about 90%
of all semantic arguments in the development set)
using a rich set of lexical, syntactic, and semantic
attributes. We trained AdaBoost one-versus-all clas-
sifiers for the 24 most common argument types. Ar-
guments that map to more than one syntactic con-
stituent are expanded with a simple heuristic in a
post-processing step.
Our results surpass with almost 6% the results ob-
tained by best SRL system that used partial syntax in
the CoNLL 2004 shared task evaluation. Although
the two evaluations are not directly comparable due
to differences in training set size, the current results
are encouraging given the simplicity of our proposed
system.
5 Acknowledgements
This research has been partially funded by the Euro-
pean Union project ?Computers in the Human Inter-
action Loop? (CHIL - IP506909). Mihai Surdeanu is
a research fellow within the Ramo?n y Cajal program
of the Spanish Ministry of Education and Science.
We would also like to thank Llu??s Ma`rquez and
Xavi Carreras for the help with the AdaBoost classi-
fier, for providing the set of temporal cue words, and
for the many motivating discussions.
References
X. Carreras and L. Ma`rquez. 2004. Introduction to the CoNLL-
2004 shared task: Semantic role labeling. In Proceedings of
CoNLL 2004 Shared Task.
X. Carreras, L. Ma`rquez, and G. Chrupa?a. 2004. Hierarchical
recognition of propositional arguments with perceptrons. In
Proceedings of CoNLL 2004 Shared Task.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. PhD Dissertation, University of Penn-
sylvania.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3).
K. Hacioglu, S. Pradhan, W. Ward, J. H. Martin, and D. Ju-
rafsky. 2004. Semantic role labeling by tagging syntactic
chunks. In Proceedings of CoNLL 2004 Shared Task.
S. Pradhan, K. Hacioglu, V. Krugler, W. Ward, J. H. Martin,
and D. Jurafsky. 2005. Support vector learning for semantic
argument classification. To appear in Journal of Machine
Learning.
R. E. Schapire and Y. Singer. 1999. Improved boosting algo-
rithms using confidence-rated predictions. Machine Learn-
ing, 37(3).
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth. 2003.
Using predicate-argument structures for information extrac-
tion. In Proceedings of ACL 2003.
224
A Hybrid Approach for the Acquisition of
Information Extraction Patterns
Mihai Surdeanu, Jordi Turmo, and Alicia Ageno
Technical University of Catalunya
Barcelona, Spain
{surdeanu,turmo,ageno}@lsi.upc.edu
Abstract
In this paper we present a hybrid ap-
proach for the acquisition of syntactico-
semantic patterns from raw text. Our
approach co-trains a decision list learner
whose feature space covers the set of all
syntactico-semantic patterns with an Ex-
pectation Maximization clustering algo-
rithm that uses the text words as attributes.
We show that the combination of the two
methods always outperforms the decision
list learner alone. Furthermore, using a
modular architecture we investigate sev-
eral algorithms for pattern ranking, the
most important component of the decision
list learner.
1 Introduction
Traditionally, Information Extraction (IE) identi-
fies domain-specific events, entities, and relations
among entities and/or events with the goals of:
populating relational databases, providing event-
level indexing in news stories, feeding link discov-
ery applications, etcetera.
By and large the identification and selective ex-
traction of relevant information is built around a
set of domain-specific linguistic patterns. For ex-
ample, for a ?financial market change? domain
one relevant pattern is <NOUN fall MONEY
to MONEY>. When this pattern is matched on
the text ?London gold fell $4.70 to $308.35?, a
change of $4.70 is detected for the financial in-
strument ?London gold?.
Domain-specific patterns are either hand-
crafted or acquired automatically (Riloff, 1996;
Yangarber et al, 2000; Yangarber, 2003; Steven-
son and Greenwood, 2005). To minimize annota-
tion costs, some of the latter approaches use lightly
supervised bootstrapping algorithms that require
as input only a small set of documents annotated
with their corresponding category label. The focus
of this paper is to improve such lightly supervised
pattern acquisition methods. Moreover, we focus
on robust bootstrapping algorithms that can han-
dle real-world document collections, which con-
tain many domains.
Although a rich literature covers bootstrap-
ping methods applied to natural language prob-
lems (Yarowsky, 1995; Riloff, 1996; Collins and
Singer, 1999; Yangarber et al, 2000; Yangar-
ber, 2003; Abney, 2004) several questions remain
unanswered when these methods are applied to
syntactic or semantic pattern acquisition. In this
paper we answer two of these questions:
(1) Can pattern acquisition be improved with
text categorization techniques?
Bootstrapping-based pattern acquisition algo-
rithms can also be regarded as incremental text
categorization (TC), since in each iteration docu-
ments containing certain patterns are assigned the
corresponding category label. Although TC is ob-
viously not the main goal of pattern acquisition
methodologies, it is nevertheless an integral part of
the learning algorithm: each iteration of the acqui-
sition algorithm depends on the previous assign-
ments of category labels to documents. Hence, if
the quality of the TC solution proposed is bad, the
quality of the acquired patterns will suffer.
Motivated by this observation, we introduce a
co-training-based algorithm (Blum and Mitchell,
1998) that uses a text categorization algorithm as
reinforcement for pattern acquisition. We show,
using both a direct and an indirect evaluation, that
the combination of the two methodologies always
improves the quality of the acquired patterns.
48
(2) Which pattern selection strategy is best?
While most bootstrapping-based algorithms fol-
low the same framework, they vary significantly
in what they consider the most relevant patterns in
each bootstrapping iteration. Several approaches
have been proposed in the context of word sense
disambiguation (Yarowsky, 1995), named entity
(NE) classification (Collins and Singer, 1999),
pattern acquisition for IE (Riloff, 1996; Yangarber,
2003), or dimensionality reduction for text catego-
rization (TC) (Yang and Pedersen, 1997). How-
ever, it is not clear which selection approach is
the best for the acquisition of syntactico-semantic
patterns. To answer this question, we have im-
plemented a modular pattern acquisition architec-
ture where several of these ranking strategies are
implemented and evaluated. The empirical study
presented in this paper shows that a strategy previ-
ously proposed for feature ranking for NE recogni-
tion outperforms algorithms designed specifically
for pattern acquisition.
The paper is organized as follows: Sec-
tion 2 introduces the bootstrapping framework
used throughout the paper. Section 3 introduces
the data collections. Section 4 describes the di-
rect and indirect evaluation procedures. Section 5
introduces a detailed empirical evaluation of the
proposed system. Section 6 concludes the paper.
2 The Pattern Acquisition Framework
In this section we introduce a modular pattern ac-
quisition framework that co-trains two different
views of the document collection: the first view
uses the collection words to train a text categoriza-
tion algorithm, while the second view bootstraps
a decision list learner that uses all syntactico-
semantic patterns as features. The rules acquired
by the latter algorithm, of the form p ? y, where
p is a pattern and y is a domain label, are the out-
put of the overall system. The system can be cus-
tomized with several pattern selection strategies
that dramatically influence the quality and order
of the acquired rules.
2.1 Co-training Text Categorization and
Pattern Acquisition
Given two views of a classification task, co-
training (Blum and Mitchell, 1998) bootstraps a
separate classifier for each view as follows: (1)
it initializes both classifiers with the same small
amount of labeled data (i.e. seed documents in our
case); (2) it repeatedly trains both classifiers us-
ing the currently labeled data; and (3) after each
learning iteration, the two classifiers share all or a
subset of the newly labeled examples (documents
in our particular case).
The intuition is that each classifier provides
new, informative labeled data to the other classi-
fier. If the two views are conditional independent
and the two classifiers generally agree on unla-
beled data they will have low generalization error.
In this paper we focus on a ?naive? co-training ap-
proach, which trains a different classifier in each
iteration and feeds its newly labeled examples to
the other classifier. This approach was shown to
perform well on real-world natural language prob-
lems (Collins and Singer, 1999).
Figure 1 illustrates the co-training framework
used in this paper. The feature space of the
first view contains only lexical information, i.e.
the collection words, and uses as classifier Ex-
pectation Maximization (EM) (Dempster et al,
1977). EM is actually a class of iterative algo-
rithms that find maximum likelihood estimates of
parameters using probabilistic models over incom-
plete data (e.g. both labeled and unlabeled docu-
ments) (Dempster et al, 1977). EM was theoret-
ically proven to converge to a local maximum of
the parameters? log likelihood. Furthermore, em-
pirical experiments showed that EM has excellent
performance for lightly-supervised text classifica-
tion (Nigam et al, 2000). The EM algorithm used
in this paper estimates its model parameters us-
ing the Naive Bayes (NB) assumptions, similarly
to (Nigam et al, 2000). From this point further,
we refer to this instance of the EM algorithm as
NB-EM.
The feature space of the second view contains
the syntactico-semantic patterns, generated using
the procedure detailed in Section 3.2. The second
learner is the actual pattern acquisition algorithm
implemented as a bootstrapped decision list clas-
sifier.
The co-training algorithm introduced in this pa-
per interleaves one iteration of the NB-EM algo-
rithm with one iteration of the pattern acquisition
algorithm. If one classifier converges faster (e.g.
NB-EM typically converges in under 20 iterations,
whereas the acquisition algorithms learns new pat-
terns for hundreds of iterations) we continue boot-
strapping the other classifier alone.
2.2 The Text Categorization Algorithm
The parameters of the generative NB model, ??, in-
clude the probability of seeing a given category,
49
pattern
Initialize
acquisition
Labeled seed documents
Unlabeled documents Iteration
NB?EM Patternacquisition
iteration
Pattern
acquisition
terminated?
NB?EM
converged?
Ranking
method
Initialize
NB?EM
No
Yes
Patterns
Yes
No
Figure 1: Co-training framework for pattern acquisition.
1. Initialization:
? Initialize the set of labeled examples with n la-
beled seed documents of the form (di, yi). yi is
the label of the ith document di. Each docu-
ment di contains a set of patterns {pi1, pi2, ..., pim}.
? Initialize the list of learned rules R = {}.
2. Loop:
? For each label y, select a small set of pattern
rules r = p ? y, r /? R.
? Append all selected rules r to R.
? For all non-seed documents d that contain a
pattern in R, set label(d) = argmaxp,y strength(p, y).
3. Termination condition:
? Stop if no rules selected or maximum number
of iterations reached.
Figure 2: Pattern acquisition meta algorithm
P (c|??), and the probability of seeing a word given
a category, P (w|c; ??). We calculate both simi-
larly to Nigam (2000). Using these parameters,
the word independence assumption typical to the
Naive Bayes model, and the Bayes rule, the prob-
ability that a document d has a given category c is
calculated as:
P (c|d; ??) = P (c|??)P (d|c; ??)
P (d|??)
(1)
= P (c|??)?
|d|
i=1P (wi|c; ??)
?q
j=1 P (cj |??)?
|d|
i=1P (wi|cj ; ??)
(2)
2.3 The Pattern Acquisition Algorithm
The lightly-supervised pattern acquisition algo-
rithm iteratively learns domain-specific IE pat-
terns from a small set of labeled documents and
a much larger set of unlabeled documents. Dur-
ing each learning iteration, the algorithm acquires
a new set of patterns and labels more documents
based on the new evidence. The algorithm output
is a list R of rules p ? y, where p is a pattern
in the set of patterns P , and y a category label in
Y = {1...k}, k being the number of categories in
the document collection. The list of acquired rules
R is sorted in descending order of rule importance
to guarantee that the most relevant rules are ac-
cessed first. This generic bootstrapping algorithm
is formalized in Figure 2.
Previous studies called the class of algorithms
illustrated in Figure 2 ?cautious? or ?sequential?
because in each iteration they acquire 1 or a small
set of rules (Abney, 2004; Collins and Singer,
1999). This strategy stops the algorithm from be-
ing over-confident, an important restriction for an
algorithm that learns from large amounts of unla-
beled data. This approach was empirically shown
to perform better than a method that in each itera-
tion acquires all rules that match a certain criterion
(e.g. the corresponding rule has a strength over a
certain threshold).
The key element where most instances of this
algorithm vary is the select procedure, which de-
cides which rules are acquired in each iteration.
Although several selection strategies have been
previously proposed for various NLP problems, to
our knowledge no existing study performs an em-
pirical analysis of such strategies in the context of
acquisition of IE patterns. For this reason, we im-
plement several selection methods in our system
(described in Section 2.4) and evaluate their per-
formance in Section 5.
The label of each collection document is given
by the strength of its patterns. Similarly to (Collins
and Singer, 1999; Yarowsky, 1995), we define the
strength of a pattern p in a category y as the pre-
cision of p in the set of documents labeled with
category y, estimated using Laplace smoothing:
strength(p, y) = count(p, y) + count(p) + k (3)
where count(p, y) is the number of documents la-
beled y containing pattern p, count(p) is the over-
all number of labeled documents containing p, and
k is the number of domains. For all experiments
presented here we used  = 1.
Another point where acquisition algorithms dif-
fer is the initialization procedure: some start with a
small number of hand-labeled documents (Riloff,
1996), as illustrated in Figure 2, while others start
with a set of seed rules (Yangarber et al, 2000;
Yangarber, 2003). However, these approaches are
conceptually similar: the seed rules are simply
used to generate the seed documents.
This paper focuses on the framework introduced
in Figure 2 for two reasons: (a) ?cautious? al-
50
gorithms were shown to perform best for several
NLP problems (including acquisition of IE pat-
terns), and (b) it has nice theoretical properties:
Abney (2004) showed that, regardless of the selec-
tion procedure, ?sequential? bootstrapping algo-
rithms converge to a local minimum of K, where
K is an upper bound of the negative log likelihood
of the data. Obviously, the quality of the local
minimum discovered is highly dependent of the
selection procedure, which is why we believe an
evaluation of several pattern selection strategies is
important.
2.4 Selection Criteria
The pattern selection component, i.e. the select
procedure of the algorithm in Figure 2, consists of
the following: (a) for each category y all patterns
p are sorted in descending order of their scores in
the current category, score(p, y), and (b) for each
category the top k patterns are selected. For all
experiments in this paper we have used k = 3.
We provide four different implementations for the
pattern scoring function score(p, y) according to
four different selection criteria.
Criterion 1: Riloff
This selection criterion was developed specifically
for the pattern acquisition task (Riloff, 1996) and
has been used in several other pattern acquisition
systems (Yangarber et al, 2000; Yangarber, 2003;
Stevenson and Greenwood, 2005). The intuition
behind it is that a qualitative pattern is yielded by a
compromise between pattern precision (which is a
good indicator of relevance) and pattern frequency
(which is a good indicator of coverage). Further-
more, the criterion considers only patterns that are
positively correlated with the corresponding cate-
gory, i.e. their precision is higher than 50%. The
Riloff score of a pattern p in a category y is for-
malized as:
score(p, y) =
{
prec(p, y) log(count(p, y)),
if prec(p, y) > 0.5;
0, otherwise.
(4)
prec(p, y) = count(p, y)count(p) (5)
where prec(p, y) is the raw precision of pattern p
in the set of documents labeled with category y.
Criterion 2: Collins
This criterion was used in a lightly-supervised NE
recognizer (Collins and Singer, 1999). Unlike the
previous criterion, which combines relevance and
frequency in the same scoring function, Collins
considers only patterns whose raw precision is
over a hard threshold T and ranks them by their
global coverage:
score(p, y) =
{
count(p), if prec(p, y) > T ;
0, otherwise. (6)
Similarly to (Collins and Singer, 1999) we used
T = 0.95 for all experiments reported here.
Criterion 3: ?2 (Chi)
The ?2 score measures the lack of independence
between a pattern p and a category y. It is com-
puted using a two-way contingency table of p and
y, where a is the number of times p and y co-occur,
b is the number of times p occurs without y, c is
the number of times y occurs without p, and d is
the number of times neither p nor y occur. The
number of documents in the collection is n. Sim-
ilarly to the first criterion, we consider only pat-
terns positively correlated with the corresponding
category:
score(p, y) =
{
?2(p, y), if prec(p, y) > 0.5;
0, otherwise. (7)
?2(p, y) = n(ad? cb)
2
(a + c)(b + d)(a + b)(c + d) (8)
The ?2 statistic was previously reported to be
the best feature selection strategy for text catego-
rization (Yang and Pedersen, 1997).
Criterion 4: Mutual Information (MI)
Mutual information is a well known information
theory criterion that measures the independence of
two variables, in our case a pattern p and a cate-
gory y (Yang and Pedersen, 1997). Using the same
contingency table introduced above, the MI crite-
rion is estimated as:
score(p, y) =
{
MI(p, y), if prec(p, y) > 0.5;
0, otherwise. (9)
MI(p, y) = log P (p ? y)P (p)? P (y) (10)
? log na(a + c)(a + b) (11)
3 The Data
3.1 The Document Collections
For all experiments reported in this paper we used
the following three document collections: (a) the
AP collection is the Associated Press (year 1999)
subset of the AQUAINT collection (LDC catalog
number LDC2002T31); (b) the LATIMES collec-
tion is the Los Angeles Times subset of the TREC-
5 collection1; and (c) the REUTERS collection is
the by now classic Reuters-21578 text categoriza-
tion collection2.
1http://trec.nist.gov/data/docs eng.html
2http://trec.nist.gov/data/reuters/reuters.html
51
Collection # of docs # of categories # of words # of patterns
AP 5000 7 24812 140852
LATIMES 5000 8 29659 69429
REUTERS 9035 10 12905 36608
Table 1: Document collections used in the evaluation
Similarly to previous work, for the REUTERS
collection we used the ModApte split and selected
the ten most frequent categories (Nigam et al,
2000). Due to memory limitations on our test ma-
chines, we reduced the size of the AP and LA-
TIMES collections to their first 5,000 documents
(the complete collections contain over 100,000
documents).
The collection words were pre-processed as fol-
lows: (i) stop words and numbers were discarded;
(ii) all words were converted to lower case; and
(iii) terms that appear in a single document were
removed. Table 1 lists the collection characteris-
tics after pre-processing.
3.2 Pattern Generation
In order to extract the set of patterns available in
a document, each collection document undergoes
the following processing steps: (a) we recognize
and classify named entities3, and (b) we generate
full parse trees of all document sentences using a
probabilistic context-free parser.
Following the above processing steps, we ex-
tract Subject-Verb-Object (SVO) tuples using a se-
ries of heuristics, e.g.: (a) nouns preceding active
verbs are subjects, (b) nouns directly attached to a
verb phrase are objects, (c) nouns attached to the
verb phrase through a prepositional attachment are
indirect objects. Each tuple element is replaced
with either its head word, if its head word is not
included in a NE, or with the NE category oth-
erwise. For indirect objects we additionally store
the accompanying preposition. Lastly, each tuple
containing more than two elements is generalized
by maintaining only subsets of two and three of its
elements and replacing the others with a wildcard.
Table 2 lists the patterns extracted from one
sample sentence. As Table 2 hints, the system
generates a large number of candidate patterns. It
is the task of the pattern acquisition algorithm to
extract only the relevant ones from this complex
search space.
4 The Evaluation Procedures
4.1 The Indirect Evaluation Procedure
The goal of our evaluation procedure is to measure
the quality of the acquired patterns. Intuitively,
3We identify six categories: persons, locations, organiza-
tions, other names, temporal and numerical expressions.
Text The Minnesota Vikings beat the Arizona
Cardinals in yesterday?s game.
Patterns s(ORG) v(beat)
v(beat) o(ORG)
s(ORG) o(ORG)
v(beat) io(in game)
s(ORG) io(in game)
o(ORG) io(in game)
s(ORG) v(beat) o(ORG)
s(ORG) v(beat) io(in game)
v(beat) o(ORG) io(in game)
Table 2: Patterns extracted from one sample sentence. s
stands for subject, v for verb, o for object, and io for indirect
object.
the learned patterns should have high coverage and
low ambiguity. We indirectly measure the quality
of the acquired patterns using a text categorization
strategy: we feed the acquired rules to a decision-
list classifier, which is then used to classify a new
set of documents. The classifier assigns to each
document the category label given by the first rule
whose pattern matches. Since we expect higher-
quality patterns to appear higher in the rule list,
the decision-list classifier never changes the cate-
gory of an already-labeled document.
The quality of the generated classification is
measured using micro-averaged precision and re-
call:
P =
?q
i=1 TruePositivesi
?q
i=1(TruePositivesi + FalsePositivesi)
(12)
R =
?q
i=1 TruePositivesi
?q
i=1(TruePositivesi + FalseNegativesi)
(13)
where q is the number of categories in the docu-
ment collection.
For all experiments and all collections with the
exception of REUTERS, which has a standard
document split for training and testing, we used 5-
fold cross validation: we randomly partitioned the
collections into 5 sets of equal sizes, and reserved
a different one for testing in each fold.
We have chosen this evaluation strategy because
this indirect approach was shown to correlate well
with a direct evaluation, where the learned patterns
were used to customize an IE system (Yangarber
et al, 2000). For this reason, much of the fol-
lowing work on pattern acquisition has used this
approach as a de facto evaluation standard (Yan-
garber, 2003; Stevenson and Greenwood, 2005).
Furthermore, given the high number of domains
and patterns (we evaluate on 25 domains), an eval-
uation by human experts is extremely costly. Nev-
ertheless, to show that the proposed indirect eval-
uation correlates well with a direct evaluation, two
human experts have evaluated the patterns in sev-
eral domains. The direct evaluation procedure is
described next.
52
4.2 The Direct Evaluation Procedure
The task of manually deciding whether an ac-
quired pattern is relevant or not for a given domain
is not trivial, mainly due to the ambiguity of the
patterns. Thus, this process should be carried out
by more than one expert, so that the relevance of
the ambiguous patterns can be agreed upon. For
example, the patterns s(ORG) v(score) o(goal) and
s(PER) v(lead) io(with point) are clearly relevant
only for the sports domain, whereas the patterns
v(sign) io(as agent) and o(title) io(in DATE) might
be regarded as relevant for other domains as well.
The specific procedure to manually evaluate the
patterns is the following: (1) two experts sepa-
rately evaluate the acquired patterns for the con-
sidered domains and collections; and (2) the re-
sults of both evaluations are compared. For any
disagreement, we have opted for a strict evalua-
tion: all the occurrences of the corresponding pat-
tern are looked up in the collection and, whenever
at least one pattern occurrence belongs to a docu-
ment assigned to a different domain than the do-
main in question, the pattern will be considered as
not relevant.
Both the ambiguity and the high number of
the extracted patterns have prevented us from per-
forming an exhaustive direct evaluation. For this
reason, only the top (most relevant) 100 patterns
have been evaluated for one domain per collection.
The results are detailed in Section 5.2.
5 Experimental Evaluation
5.1 Indirect Evaluation
For a better understanding of the proposed ap-
proach we perform an incremental evaluation:
first, we evaluate only the various pattern selection
criteria described in Section 2.4 by disabling the
NB-EM component. Second, using the best selec-
tion criteria, we evaluate the complete co-training
system.
In both experiments we initialize the system
with high-precision manually-selected seed rules
which yield seed documents with a coverage of
10% of the training partitions. The remaining 90%
of the training documents are maintained unla-
beled. For all experiments we used a maximum of
400 bootstrapping iterations. The acquired rules
are fed to the decision list classifier which assigns
category labels to the documents in the test parti-
tions.
Evaluation of the pattern selection criteria
Figure 3 illustrates the precision/recall charts
of the four algorithms as the number of patterns
made available to the decision list classifier in-
creases. All charts show precision/recall points
starting after 100 learning iterations with 100-
iteration increments. It is immediately obvious
that the Collins selection criterion performs sig-
nificantly better than the other three criteria. For
the same recall point, Collins yields a classifica-
tion model with much higher precision, with dif-
ferences ranging from 5% in the REUTERS col-
lection to 20% in the AP collection.
Theorem 5 in (Abney, 2002) provides a theo-
retical explanation for these results: if certain in-
dependence conditions between the classifier rules
are satisfied and the precision of each rule is larger
than a threshold T , then the precision of the final
classifier is larger than T . Although the rule inde-
pendence conditions are certainly not satisfied in
our real-world evaluation, the above theorem in-
dicates that there is a strong relation between the
precision of the classifier rules on labeled data and
the precision of the final classifier. Our results pro-
vide the empirical proof that controling the preci-
sion of the acquired rules (i.e. the Collins crite-
rion) is important.
The Collins criterion controls the recall of the
learned model by favoring rules with high fre-
quency in the collection. However, since the other
two criteria do not use a high precision thresh-
old, they will acquire more rules, which translates
in better recall. For two out of the three collec-
tions, Riloff and Chi obtain a slightly better recall,
about 2% higher than Collins?, albeit with a much
lower precision. We do not consider this an im-
portant advantage: in the next section we show
that co-training with the NB-EM component fur-
ther boosts the precision and recall of the Collins-
based acquisition algorithm.
The MI criterion performs the worst of the four
evaluated criteria. A clue for this behavior lies in
the following equivalent form for MI: MI(p, y) =
logP (p|y)?logP (p). This formula indicates that,
for patterns with equal conditional probabilities
P (p|y), MI assigns higher scores to patterns with
lower frequency. This is not the desired behavior
in a TC-oriented system.
Evaluation of the co-training system
Figure 4 compares the performance of the
stand-alone pattern acquisition algorithm (?boot-
strapping?) with the performance of the acquisi-
tion algorithm trained in the co-training environ-
53
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.15  0.2  0.25  0.3  0.35  0.4  0.45  0.5  0.55
Pr
ec
is
io
n
Recall
collins
riloff
chi
mi
(a)
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.1  0.15  0.2  0.25  0.3  0.35  0.4
Pr
ec
is
io
n
Recall
collins
riloff
chi
mi
(b)
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 0.1  0.15  0.2  0.25  0.3  0.35  0.4  0.45
Pr
ec
is
io
n
Recall
collins
riloff
chi
mi
(c)
Figure 3: Performance of the pattern acquisition algorithm for various pattern selection strategies and multiple collections:
(a) AP, (b) LATIMES, and (c) REUTERS
ment (?co-training?). For both setups we used the
best pattern selection criterion for pattern acqui-
sition, i.e. the Collins criterion. To put things in
perspective, we also depict the performance ob-
tained with a baseline system, i.e. the system con-
figured to use the Riloff pattern selection criterion
and without the NB-EM algorithm (?baseline?).
To our knowledge, this system, or a variation of
it, is the current state-of-the-art in pattern acqui-
sition (Riloff, 1996; Yangarber et al, 2000; Yan-
garber, 2003; Stevenson and Greenwood, 2005).
All algorithms were initialized with the same seed
rules and had access to all documents.
Figure 4 shows that the quality of the learned
patterns always improves if the pattern acquisi-
tion algorithm is ?reinforced? with EM. For the
same recall point, the patterns acquired in the
co-training environment yield classification mod-
els with precision (generally) much larger than
the models generated by the pattern acquisition
algorithm alone. When using the same pat-
tern acquisition criterion, e.g. Collins, the dif-
ferences between the co-training approach and
the stand-alone pattern acquisition method (?boot-
strapping?) range from 2-3% in the REUTERS
collection to 20% in the LATIMES collection.
These results support our intuition that the sparse
pattern space is insufficient to generate good clas-
sification models, which directly influences the
quality of all acquired patterns.
Furthermore, due to the increased coverage of
the lexicalized collection views, the patterns ac-
quired in the co-training setup generally have bet-
ter recall, up to 11% higher in the LATIMES col-
lection.
Lastly, the comparison of our best system (?co-
training?) against the current state-of-the-art (our
?baseline?) draws an even more dramatic picture:
Collection Domain Relevant Relevant Initial
patterns patterns inter-expert
baseline co-training agreement
AP Sports 22% 68% 84%
LATIMES Financial 67% 76% 70%
REUTERS Corporate 38% 46% 66%
Acquisitions
Table 3: Percentage of relevant patterns for one domain per
collection by the baseline system (Riloff) and the co-training
system.
for the same recall point, the co-training system
obtains a precision up to 35% higher for AP and
LATIMES, and up to 10% higher for REUTERS.
5.2 Direct Evaluation
As stated in Section 4.2, two experts have man-
ually evaluated the top 100 acquired patterns for
one different domain in each of the three collec-
tions. The three corresponding domains have been
selected intending to deal with different degrees of
ambiguity, which are reflected in the initial inter-
expert agreement. Any disagreement between ex-
perts is solved using the algorithm introduced in
Section 4.2. Table 3 shows the results of this di-
rect evaluation. The co-training approach outper-
forms the baseline for all three collections. Con-
cretely, improvements of 9% and 8% are achieved
for the Financial and the Corporate Acquisitions
domains, and 46%, by far the largest difference, is
found for the Sports domain in AP. Table 4 lists
the top 20 patterns extracted by both approaches
in the latter domain. It can be observed that for
the baseline, only the top 4 patterns are relevant,
the rest being extremely general patterns. On the
other hand, the quality of the patterns acquired by
our approach is much higher: all the patterns are
relevant to the domain, although 7 out of the 20
might be considered ambiguous and according to
the criterion defined in Section 4.2 have been eval-
uated as not relevant.
54
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.3  0.35  0.4  0.45  0.5  0.55  0.6
Pr
ec
is
io
n
Recall
co-training
bootstrapping
baseline
(a)
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.2  0.25  0.3  0.35  0.4  0.45
Pr
ec
is
io
n
Recall
co-training
bootstrapping
baseline
(b)
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 0.15  0.2  0.25  0.3  0.35  0.4  0.45
Pr
ec
is
io
n
Recall
co-training
bootstrapping
baseline
(c)
Figure 4: Comparison of the bootstrapping pattern acquisition algorithm with the co-training approach: (a) AP, (b) LATIMES,
and (c) REUTERS
Baseline Co-training
s(he) o(game) v(win) o(title)
v(miss) o(game) s(I) v(play)
v(play) o(game) s(he) v(game)
v(play) io(in LOC) s(we) v(play)
v(go) o(be) v(miss) o(game)
s(he) v(be) s(he) v(coach)
s(that) v(be) v(lose) o(game)
s(I) v(be) s(I) o(play)
s(it) v(go) o(be) v(make) o(play)
s(it) v(be) v(play) io(in game)
s(I) v(think) v(want) o(play)
s(I) v(know) v(win) o(MISC)
s(I) v(want) s(he) o(player)
s(there) v(be) v(start) o(game)
s(we) v(do) s(PER) o(contract)
v(do) o(it) s(we) o(play)
s(it) o(be) s(team) v(win)
s(we) v(are) v(rush) io(for yard)
s(we) v(go) s(we) o(team)
s(PER) o(DATE) v(win) o(Bowl)
Table 4: Top 20 patterns acquired from the Sports domain
by the baseline system (Riloff) and the co-training system for
the AP collection. The correct patterns are in bold.
6 Conclusions
This paper introduces a hybrid, lightly-supervised
method for the acquisition of syntactico-semantic
patterns for Information Extraction. Our approach
co-trains a decision list learner whose feature
space covers the set of all syntactico-semantic
patterns with an Expectation Maximization clus-
tering algorithm that uses the text words as at-
tributes. Furthermore, we customize the decision
list learner with up to four criteria for pattern se-
lection, which is the most important component of
the acquisition algorithm.
For the evaluation of the proposed approach we
have used both an indirect evaluation based on
Text Categorization and a direct evaluation where
human experts evaluated the quality of the gener-
ated patterns. Our results indicate that co-training
the Expectation Maximization algorithm with the
decision list learner tailored to acquire only high
precision patterns is by far the best solution. For
the same recall point, the proposed method in-
creases the precision of the generated models up
to 35% from the previous state of the art. Further-
more, the combination of the two feature spaces
(words and patterns) also increases the coverage
of the acquired patterns. The direct evaluation of
the acquired patterns by the human experts vali-
dates these results.
References
S. Abney. 2002. Bootstrapping. In Proceedings of the 40th
Annual Meeting of the Association for Computational Lin-
guistics.
S. Abney. 2004. Understanding the Yarowsky algorithm.
Computational Linguistics, 30(3).
A. Blum and T. Mitchell. 1998. Combining labeled and un-
labeled data with co-training. In Proceedings of the 11th
Annual Conference on Computational Learning Theory.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In Proceedings of EMNLP.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Max-
imum likelihood from incomplete data via the EM algo-
rithm. Journal of the Royal Statistical Society, Series B,
39(1).
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. 2000.
Text classification from labeled and unlabeled documents
using EM. Machine Learning, 39(2/3).
E. Riloff. 1996. Automatically generating extraction patterns
from untagged text. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence (AAAI-96).
M. Stevenson and M. Greenwood. 2005. A semantic ap-
proach to ie pattern induction. In Proceedings of the 43rd
Meeting of the Association for Computational Linguistics.
Y. Yang and J. O. Pedersen. 1997. A comparative study
on feature selection in text categorization. In Proceed-
ings of the Fourteenth International Conference on Ma-
chine Learning.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hutunen.
2000. Automatic acquisition of domain knowledge for in-
formation extraction. In Proceedings of the 18th Interna-
tional Conference of Computational Linguistics (COLING
2000).
R. Yangarber. 2003. Counter-training in discovery of se-
mantic patterns. In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics (ACL
2003).
D. Yarowsky. 1995. Unsupervised word sense disambigua-
tion rivaling supervised methods. In Proceedings of the
33rd Annual Meeting of the Association for Computa-
tional Linguistics.
55
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 181?185, New York City, June 2006. c?2006 Association for Computational Linguistics
Projective Dependency Parsing with Perceptron
Xavier Carreras, Mihai Surdeanu and Llu??s Ma`rquez
TALP Research Centre ? Software Department (LSI)
Technical University of Catalonia (UPC)
Campus Nord - Edifici Omega, Jordi Girona Salgado 1?3, E-08034 Barcelona
{carreras,surdeanu,lluism}@lsi.upc.edu
Abstract
We describe an online learning depen-
dency parser for the CoNLL-X Shared
Task, based on the bottom-up projective
algorithm of Eisner (2000). We experi-
ment with a large feature set that mod-
els: the tokens involved in dependencies
and their immediate context, the surface-
text distance between tokens, and the syn-
tactic context dominated by each depen-
dency. In experiments, the treatment of
multilingual information was totally blind.
1 Introduction
We describe a learning system for the CoNLL-X
Shared Task on multilingual dependency parsing
(Buchholz et al, 2006), for 13 different languages.
Our system is a bottom-up projective dependency
parser, based on the cubic-time algorithm by Eisner
(1996; 2000). The parser uses a learning function
that scores all possible labeled dependencies. This
function is trained globally with online Perceptron,
by parsing training sentences and correcting its pa-
rameters based on the parsing mistakes. The features
used to score, while based on the previous work in
dependency parsing (McDonald et al, 2005), intro-
duce some novel concepts such as better codification
of context and surface distances, and runtime infor-
mation from dependencies previously parsed.
Regarding experimentation, the treatment of mul-
tilingual data has been totally blind, with no spe-
cial processing or features that depend on the lan-
guage. Considering its simplicity, our system
achieves moderate but encouraging results, with an
overall labeled attachment accuracy of 74.72% on
the CoNLL-X test set.
2 Parsing and Learning Algorithms
This section describes the three main components of
the dependency parsing: the parsing model, the pars-
ing algorithm, and the learning algorithm.
2.1 Model
Let 1, . . . , L be the dependency labels, defined be-
forehand. Let x be a sentence of n words, x1 . . . xn.
Finally, let Y(x) be the space of well-formed depen-
dency trees for x. A dependency tree y ? Y(x) is a
set of n dependencies of the form [h,m, l], where
h is the index of the head word (0 ? h ? n,
where 0 means root), m is the index of the modi-
fier word (1 ? m ? n), and l is the dependency
label (1 ? l ? L). Each word of x participates as a
modifier in exactly one dependency of y.
Our dependency parser, dp, returns the maximum
scored dependency tree for a sentence x:
dp(x,w) = argmax
y?Y(x)
?
[h,m,l]?y
sco([h,m, l], x, y,w)
In the formula, w is the weight vector of the
parser, that is, the set of parameters used to score de-
pendencies during the parsing process. It is formed
by a concatenation of L weight vectors, one for each
dependency label, w = (w1, . . . ,wl, . . . ,wL). We
assume a feature extraction function, ?, that repre-
sents an unlabeled dependency [h,m] in a vector of
D features. Each of the wl has D parameters or
dimensions, one for each feature. Thus, the global
181
weight vector w maintains L ? D parameters. The
scoring function is defined as follows:
sco([h,m, l], x, y,w) = ?(h,m, x, y) ? wl
Note that the scoring of a dependency makes use
of y, the tree that contains the dependency. As de-
scribed next, at scoring time y just contains the de-
pendencies found between h and m.
2.2 Parsing Algorithm
We use the cubic-time algorithm for dependency
parsing proposed by Eisner (1996; 2000). This pars-
ing algorithm assumes that trees are projective, that
is, dependencies never cross in a tree. While this as-
sumption clearly does not hold in the CoNLL-X data
(only Chinese trees are actually 100% projective),
we chose this algorithm for simplicity. As it will be
shown, the percentage of non-projective dependen-
cies is not very high, and clearly the error rates we
obtain are caused by other major factors.
The parser is a bottom-up dynamic programming
algorithm that visits sentence spans of increasing
length. In a given span, from word s to word e, it
completes two partial dependency trees that cover
all words within the span: one rooted at s and the
other rooted at e. This is done in two steps. First, the
optimal dependency structure internal to the span is
chosen, by combining partial solutions from inter-
nal spans. This structure is completed with a depen-
dency covering the whole span, in two ways: from
s to e, and from e to s. In each case, the scoring
function is used to select the dependency label that
maximizes the score.
We take advantage of this two-step processing to
introduce features for the scoring function that rep-
resent some of the internal dependencies of the span
(see Section 3 for details). It has to be noted that
the parsing algorithm we use does not score depen-
dencies on top of every possible internal structure.
Thus, by conditioning on features extracted from y
we are making the search approximative.
2.3 Perceptron Learning
As learning algorithm, we use Perceptron tailored
for structured scenarios, proposed by Collins (2002).
In recent years, Perceptron has been used in a num-
ber of Natural Language Learning works, such as in
w = 0
for t = 1 to T
foreach training example (x, y) do
y? = dp(x,w)
foreach [h,m, l] ? y\y? do
wl = wl + ?(h,m, x, y?)
foreach [h,m, l] ? y?\y do
wl = wl ? ?(h,m, x, y?)
return w
Figure 1: Pseudocode of the Perceptron Algorithm. T is a
parameter that indicates the number of epochs that the algorithm
cycles the training set.
partial parsing (Carreras et al, 2005) or even depen-
dency parsing (McDonald et al, 2005).
Perceptron is an online learning algorithm that
learns by correcting mistakes made by the parser
when visiting training sentences. The algorithm is
extremely simple, and its cost in time and memory
is independent from the size of the training corpora.
In terms of efficiency, though, the parsing algorithm
must be run at every training sentence.
Our system uses the regular Perceptron working
in primal form. Figure 1 sketches the code. Given
the number of languages and dependency types in
the CoNLL-X exercise, we found prohibitive to
work with a dual version of Perceptron, that would
allow the use of a kernel function to expand features.
3 Features
The feature extraction function, ?(h,m, x, y), rep-
resents in a feature vector a dependency from word
positions m to h, in the context of a sentence x and a
dependency tree y. As usual in discriminative learn-
ing, we work with binary indicator features: if a cer-
tain feature is observed in an instance, the value of
that feature is 1; otherwise, the value is 0. For con-
venience, we describe ? as a composition of several
base feature extraction functions. Each extracts a
number of disjoint features. The feature extraction
function ?(h,m, x, y) is calculated as:
?token(x, h, ?head?) + ?tctx(x, h, ?head?) +
?token(x,m, ?mod?) + ?tctx(x,m, ?mod?) +
?dep(x,mmdh,m) + ?dctx(x,mmdh,m) +
?dist(x,mmdh,m) + ?runtime(x, y, h,m, dh,m)
where ?token extracts context-independent token
features, ?tctx computes context-based token fea-
tures, ?dep computes context-independent depen-
182
?token(x, i, type)
type ? w(xi)
type ? l(xi)
type ? cp(xi)
type ? fp(xi)
foreach(ms): type ?ms(xi)
type ? w(xi) ? cp(xi)
foreach(ms): type ? w(xi) ?ms(xi)
?tctx(x, i, type)
?token(x, i? 1, type ? string(i? 1))
?token(x, i? 2, type ? string(i? 2))
?token(x, i+ 1, type ? string(i+ 1))
?token(x, i+ 2, type ? string(i+ 2))
type ? cp(xi) ? cp(xi?1)
type ? cp(xi) ? cp(xi?1) ? cp(xi?2)
type ? cp(xi) ? cp(xi+1)
type ? cp(xi) ? cp(xi+1) ? cp(xi+2)
Table 1: Token features, both context-independent (?token)
and context-based (?tctx). type - token type, i.e. ?head? or
?mod?, w - token word, l - token lemma, cp - token coarse part-
of-speech (POS) tag, fp - token fine-grained POS tag, ms -
token morpho-syntactic feature. The ? operator stands for string
concatenation.
?dep(x, i, j,dir)
dir ? w(xi) ? cp(xi) ? w(xj) ? cp(xj)
dir ? cp(xi) ? w(xj) ? cp(xj)
dir ? w(xi) ? w(xj) ? cp(xj)
dir ? w(xi) ? cp(xi) ? cp(xj)
dir ? w(xi) ? cp(xi) ? w(xj)
dir ? w(xi) ? w(xj)
dir ? cp(xi) ? cp(xj)
?dctx(x, i, j,dir)
dir ? cp(xi) ? cp(xi+1) ? cp(xj?1) ? cp(xj)
dir ? cp(xi?1) ? cp(xi) ? cp(xj?1) ? cp(xj)
dir ? cp(xi) ? cp(xi+1) ? cp(xj) ? cp(xj+1)
dir ? cp(xi?1) ? cp(xi) ? cp(xj) ? cp(xj+1)
Table 2: Dependency features, both context-independent
(?dep) and context-based (?dctx), between two points i and j,
i < j. dir - dependency direction: left to right or right to left.
dency features, ?dctx extracts contextual depen-
dency features, ?dist calculates surface-distance fea-
tures between the two tokens, and finally, ?runtime
computes dynamic features at runtime based on the
dependencies previously built for the given interval
during the bottom-up parsing. mmdh,m is a short-
hand for a triple of numbers: min(h,m), max(h,m)
and dh,m (a sign indicating the direction, i.e., +1 if
m < h, and ?1 otherwise).
We detail the token features in Table 1, the depen-
dency features in Table 2, and the surface-distance
features in Table 3. Most of these features are in-
spired by previous work in dependency parsing (Mc-
Donald et al, 2005; Collins, 1999). What is impor-
?dist(x, i, j,dir)
foreach(k ? (i, j)): dir ? cp(xi) ? cp(xk) ? cp(xj)
number of tokens between i and j
number of verbs between i and j
number of coordinations between i and j
number of punctuations signs between i and j
Table 3: Surface distance features between points i and j. Nu-
meric features are discretized using ?binning? to a small number
of intervals.
?runtime(x,y,h,m,dir)
let l1, . . . , lS be the labels of dependencies
in y that attach to h and are found from m to h.
foreach i, 1? i?S : dir ? cp(xh) ? cp(xm) ? li
if S?1 , dir ? cp(xh) ? cp(xm) ? l1
if S?2 , dir ? cp(xh) ? cp(xm) ? l1 ? l2
if S?3 , dir ? cp(xh) ? cp(xm) ? l1 ? l2 ? l3
if S?4 , dir ? cp(xh) ? cp(xm) ? l1 ? l2 ? l3 ? l4
if S=0 , dir ? cp(xh) ? cp(xm) ? null
if 0<S?4 , dir ? cp(xh) ? cp(xm) ? regular
if S>4 , dir ? cp(xh) ? cp(xm) ? big
Table 4: Runtime features of y between m and h.
tant for the work presented here is that we construct
explicit feature combinations (see above tables) be-
cause we configured our linear predictors in primal
form, in order to keep training times reasonable.
While the features presented in Tables 1, 2, and 3
are straightforward exploitations of the training data,
the runtime features (?runtime) take a different, and
to our knowledge novel in the proposed framework,
approach: for a dependency from m to h, they rep-
resent the dependencies found between m and h
that attach also to h. They are described in detail
in Table 4. As we have noted above, these fea-
tures are possible because of the parsing scheme,
which scores a dependency only after all dependen-
cies spanned by it are scored.
4 Experiments and Results
We experimented on the 13 languages proposed
in the CoNLL-X Shared Task (Hajic? et al, 2004;
Simov et al, 2005; Simov and Osenova, 2003; Chen
et al, 2003; Bo?hmova? et al, 2003; Kromann, 2003;
van der Beek et al, 2002; Brants et al, 2002;
Kawata and Bartels, 2000; Afonso et al, 2002;
Dz?eroski et al, 2006; Civit and Mart??, 2002; Nilsson
et al, 2005; Oflazer et al, 2003; Atalay et al, 2003).
Our approach to deal with many different languages
was totally blind: we did not inspect the data to mo-
tivate language-specific features or processes.
183
We did feature filtering based on frequency
counts. Our feature extraction patterns, that ex-
ploit both lexicalization and combination, gener-
ate millions of feature dimensions, even with small
datasets. Our criterion was to use at most 500,000
different dimensions in each label weight vector. For
each language, we generated all possible features,
and then filtered out most of them according to the
counts. Depending on the number of training sen-
tences, our counts cut-offs vary from 3 to 15.
For each language, we held out from training data
a portion of sentences (300, 500 or 1000 depend-
ing on the total number of sentences) and trained a
model for up to 20 epochs in the rest of the data. We
evaluated each model on the held out data for differ-
ent number of training epochs, and selected the op-
timum point. Then, we retrained each model on the
whole training set for the selected number of epochs.
Table 5 shows the attachment scores obtained
by our system, both unlabeled (UAS) and labeled
(LAS). The first column (GOLD) presents the LAS
obtained with a perfect scoring function: the loss in
accuracy is related to the projectivity assumption of
our parsing algorithm. Dutch turns out to be the
most non-projective language, with a loss in accu-
racy of 5.44%. In our opinion, the loss in other lan-
guages is relatively small, and is not a major limita-
tion to achieve a high performance in the task. Our
system achieves an overall LAS of 74.72%, with
substantial variation from one language to another.
Turkish, Arabic, Dutch, Slovene and Czech turn out
to be the most difficult languages for our system,
with accuracies below 70%. The easiest language
is clearly Japanese, with a LAS of 88.13%, followed
by Chinese, Portuguese, Bulgarian and German, all
with LAS above 80%.
Table 6 shows the contribution of base feature ex-
traction functions. For four languages, we trained
models that increasingly incorporate base functions.
It can be shown that all functions contribute to a bet-
ter score. Contextual features (?3) bring the system
to the final order of performance, while distance (?4)
and runtime (?) features still yield substantial im-
provements.
5 Analysis and Conclusions
It is difficult to explain the difference in performance
across languages. Nevertheless, we have identified
GOLD UAS LAS
Bulgarian 99.56 88.81 83.30
Arabic 99.76 72.65 60.94
Chinese 100.0 88.65 83.68
Czech 97.78 77.44 68.82
Danish 99.18 85.67 79.74
Dutch 94.56 71.39 67.25
German 98.84 85.90 82.41
Japanese 99.16 90.79 88.13
Portuguese 98.54 87.76 83.37
Slovene 98.38 77.72 68.43
Spanish 99.96 80.77 77.16
Swedish 99.64 85.54 78.65
Turkish 98.41 70.05 58.06
Overall 98.68 81.19 74.72
Table 5: Results of the system on test data. GOLD: labeled
attachment score using gold scoring functions; the loss in ac-
curacy is caused by the projectivity assumption made by the
parser. UAS : unlabeled attachment score. LAS : labeled at-
tachment score, the measure to compare systems in CoNLL-X.
Bulgarian is excluded from overall scores.
?1 ?2 ?3 ?4 ?
Turkish 33.02 48.00 55.33 57.16 58.06
Spanish 12.80 53.80 68.18 74.27 77.16
Portuguese 47.10 64.74 80.89 82.89 83.37
Japanese 38.78 78.13 86.87 88.27 88.13
Table 6: Labeled attachment scores at increasing feature con-
figurations. ?1 uses only ?token at the head and modifier. ?2
extends ?1 with ?dep. ?3 incorporates context features, namely
?tctx at the head and modifier, and ?dctx. ?4 extends ?3 with
?dist. Finally, the final feature extraction function ? increases
?4 with ?runtime.
four generic factors that we believe caused the most
errors across all languages:
Size of training sets: the relation between the
amount of training data and performance is strongly
supported in learning theory. We saw the same re-
lation in this evaluation: for Turkish, Arabic, and
Slovene, languages with limited number of train-
ing sentences, our system obtains accuracies below
70%. However, one can not argue that the training
size is the only cause of errors: Czech has the largest
training set, and our accuracy is also below 70%.
Modeling large distance dependencies: even
though we include features to model the distance
between two dependency words (?dist), our analy-
sis indicates that these features fail to capture all the
intricacies that exist in large-distance dependencies.
Table 7 shows that, for the two languages analyzed,
the system performance decreases sharply as the dis-
tance between dependency tokens increases.
184
to root 1 2 3 ? 6 >= 7
Spanish 83.04 93.44 86.46 69.97 61.48
Portuguese 90.81 96.49 90.79 74.76 69.01
Table 7: F?=1 score related to dependency token distance.
Modeling context: many attachment decisions, e.g.
prepositional attachment, depend on additional con-
text outside of the two dependency tokens. To ad-
dress this issue, we have included in our model fea-
tures to capture context, both static (?dctx and ?tctx)
and dynamic (?runtime). Nevertheless, our error
analysis indicates that our model is not rich enough
to capture the context required to address complex
dependencies. All the top 5 focus words with the
majority of errors for Spanish and Portuguese ? ?y?,
?de?, ?a?, ?en?, and ?que? for Spanish, and ?em?,
?de?, ?a?, ?e?, and ?para? for Portuguese ? indicate
complex dependencies such as prepositional attach-
ments or coordinations.
Projectivity assumption: Dutch is the language
with most crossing dependencies in this evaluation,
and the accuracy we obtain is below 70%.
On the Degree of Lexicalization We conclude the
error analysis of our model with a look at the de-
gree of lexicalization in our model. A quick analy-
sis of our model on the test data indicates that only
34.80% of the dependencies for Spanish and 42.94%
of the dependencies for Portuguese are fully lexical-
ized, i.e. both the head and modifier words appear
in the model feature set (see Table 8). There are
two reasons that cause our model to be largely un-
lexicalized: (a) in order to keep training times rea-
sonable we performed heavy filtering of all features
based on their frequency, which eliminates many
lexicalized features from the final model, and (b)
due to the small size of most of the training cor-
pora, most lexicalized features simply do not ap-
pear in the testing section. Considering these re-
sults, a reasonable question to ask is: how much
are we losing because of this lack of lexical infor-
mation? We give an approximate answer by ana-
lyzing the percentage of fully-lexicalized dependen-
cies that are correctly parsed by our model. As-
suming that our model scales well, the accuracy on
fully-lexicalized dependencies is an indication for
the gain (or loss) to be had from lexicalization. Our
model parses fully-lexicalized dependencies with an
Fully One token Fully
lexicalized unlexicalized unlexicalized
Spanish 34.80% 54.77% 10.43%
Portuguese 42.94% 49.26% 7.80%
Table 8: Degree of dependency lexicalization.
accuracy of 74.81% LAS for Spanish (2.35% lower
than the overall score) and of 83.77% LAS for Por-
tuguese (0.40% higher than the overall score). This
analysis indicates that our model has limited gains
(if any) from lexicalization.
In order to improve the quality of our dependency
parser we will focus on previously reported issues
that can be addressed by a parsing model: large-
distance dependencies, better modeling of context,
and non-projective parsing algorithms.
Acknowledgements
This work was partially funded by the European Union Com-
mission (PASCAL - IST-2002-506778) and Spanish Ministry
of Science and Technology (TRANGRAM - TIN2004-07925-
C03-02). Mihai Surdeanu was supported by a Ramon y Cajal
fellowship of the latter institution.
References
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski. 2006.
CoNLL-X shared task on multilingual dependency parsing.
In Proc. of the Tenth Conf. on Computational Natural Lan-
guage Learning (CoNLL-X). SIGNLL.
X. Carreras, Llu??s Ma`rquez, and J. Castro. 2005. Filtering-
ranking perceptron learning for partial parsing. Machine
Learning, 1?3(60):41?71.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
M. Collins. 2002. Discriminative training methods for hidden
markov models: Theory and experiments with perceptron al-
gorithms. In Proc. of EMNLP-2002.
J. Eisner. 1996. Three new probabilistic models for depen-
dency parsing: An exploration. In Proc. of the 16th Intern.
Conf. on Computational Linguistics (COLING).
J. Eisner. 2000. Bilexical grammars and their cubic-time pars-
ing algorithms. In H. C. Bunt and A. Nijholt, editors, New
Developments in Natural Language Parsing, pages 29?62.
Kluwer Academic Publishers.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-
margin training of dependency parsers. In Proc. of the 43rd
Annual Meeting of the ACL.
185
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 426?429,
Prague, June 2007. c?2007 Association for Computational Linguistics
UPC: Experiments with Joint Learning within SemEval Task 9
Llu??s Ma`rquez, Llu??s Padro?, Mihai Surdeanu, Luis Villarejo
Technical University of Catalonia
{lluism,padro,surdeanu,luisv}@lsi.upc.edu
1 Introduction
This paper describes UPC?s participation in the
SemEval-2007 task 9 (Ma`rquez et al, 2007).1 We
addressed all four subtasks using supervised learn-
ing. The paper introduces several novel issues:
(a) for the SRL task, we propose a novel re-
ranking algorithm based on the re-ranking Percep-
tron of Collins and Duffy (2002); and (b) for the
same task we introduce a new set of global features
that extract information not only at proposition level
but also from the complete set of frame candidates.
We show that in the SemEval setting, i.e., small
training corpora, this approach outperforms previ-
ous work. Additionally, we added NSD and NER
information in the global SRL model but this exper-
iment was unsuccessful.
2 Named Entity Recognition
For the NER subtask we recognize first strong NEs,
followed by weak NE identification. Any single to-
ken with the np0000, W, or Z PoS tag is consid-
ered a strong entity and is classified using the (At-
serias et al, 2006) implementation of a multi-label
AdaBoost.MH algorithm, with a configuration sim-
ilar to the NE classification module of Carreras et
al. (2003). The classifier yields predictions for four
classes (person, location, organization, misc). En-
tities with NUM and DAT are detected separately
solely based on POS tags.
The features used by the strong NE classifier
model a [-3,+3] context around the focus word, and
include bag-of-words, positional lexical features,
1Two of the authors of this paper, Llu??s Ma`rquez and Luis
Villarejo, are organizers of the SemEval-2007 task 9.
PoS tags, orthographic features, as well as features
indicating whether the focus word, some of its com-
ponents, or some word in the context are included in
external gazetteers or trigger words files.
The second step starts by selecting all noun
phrases (np) that cover a span of more than one to-
ken and include a strong NE as weak entity candi-
dates. This strategy covers more than 95% of the
weak NEs. A second AdaBoost.MH classifier is
then applied to decide the right class for the noun
phrase among the possible six (person, location, or-
ganization, misc, number, date) plus a NONE class
indicating that the noun phrase is not a weak NE.
The features used for weak NE classification are:
(1) simple features ? length in tokens, head word,
lemma, and POS of the np, syntactic function of the
np (if any), minimum and maximum number of np
nodes in the path from the candidate noun phrase to
any of the strong NEs included in it, and number and
type of the strong NEs predicted by the first?level
classifier that fall inside the candidate; (2) bag of
content words inside the candidate; and (3) pattern-
based features, consisting in codifying the sequence
of lexical tokens spanned by the candidate according
to some generalizations. When matching, tokens are
generalized to: the POS tag (in case of np0000,
W, Z, and punctuation marks), trigger-word of class
X, word-in-gazetteer of class X, and strong-NE of
type X, predicted by the first level classifier. The
rest of words are abstracted to a common form (?w?
standing for a single word and ?w+? standing for a
sequence of n > 1 words). Beginning and end of the
span are also codified explicitly in the pattern?based
features. Finally, to avoid sparsity, only paths of up
426
to length 6 are codified as features. Also, for each
path, n?grams of length 2, 3 and 4 are considered.
We filter out features that occur less than 10 times.
3 Noun Sense Disambiguation
We have approached the NSD subtask using su-
pervised learning. In particular, we used SVMlight
(Joachims, 1999), which is a freely available imple-
mentation of Support Vector Machines (SVM).
We trained binary SVM classifiers for every sense
of words with more than 15 examples in the training
set and a probability distribution over its senses in
which no sense is above 90%. The words not cov-
ered by the SVM classifiers are disambiguated using
the most frequent sense (MFS) heuristic. The MFS
was calculated from the relative frequencies in the
training corpus. To the words that do not appear in
the training corpus we assigned the first WordNet
sense.
We used a fairly regular set of features from the
WSD literature. We included: (1) a bag of con-
tent words appearing in a ?10-word window; (2) a
bag of content words appearing in the clause of the
target word; (3) {1, . . . , n}?grams of POS tags and
lemmas in a ?n-word window (n is 3 for POS and
2 for lemmas); (4) unigrams and bigrams of (POS-
tag,lemma) pairs in a?2-word window; and (5) syn-
tactic features, i.e., label of the syntactic constituent
from which the target noun is the head, syntactic
function of that constituent (if any), and the verb.
Regarding the empirical setting, we filtered out
features occurring less than 3 times, we used linear
SVMs with a 0.5 value for the C regularization pa-
rameter (trade-off between training error and mar-
gin), and we applied one-vs-all binarization.
4 Semantic Role Labeling
The SRL approach deployed here implements a re-
ranking strategy that selects the best argument frame
for each predicate from the top N frames generated
by a base model. We describe the two models next.
4.1 The Local Model
The local (i.e., base) model is an adaption of Model
3 of Ma`rquez et al (2005). This SRL approach
maps each frame argument to one syntactic con-
stituent and trains one-vs-all AdaBoost (Schapire
and Singer, 1999) classifiers to jointly identify and
classify constituents in the full syntactic tree of the
sentence as arguments. The model was adapted to
the languages and corpora used in the SemEval eval-
uations by removing the features that were specific
either to English or PropBank (governing category,
content word, and temporal cue words) and adding
several new features: (a) syntactic function features
? the syntactic functions available in the data often
point to specific argument labels (e.g., SUJ usually
indicates an Arg0); and (b) back-off features for
syntactic labels and POS tags ? for the features that
include POS tags or syntactic labels we add a back-
off version of the feature where the POS tags and
syntactic labels are reduced to a small set.
In addition to feature changes we modified the
candidate filtering heuristic: we select as candidates
only syntactic constituents that are immediate de-
scendents of S phrases that include the correspond-
ing predicate (for both languages, over 99.6% of the
candidates match this constraint).
4.2 The Global Model
We base our re-ranking approach on a variant of the
re-ranking Perceptron of Collins and Duffy (2002).
We modify the original algorithm in two ways to
make it more robust to the small training set avail-
able: (a) instead of comparing the score of the cor-
rect frame only with that of the best candidate for
each frame, we sequentially compare it with the
score of each candidate in order to acquire more in-
formation, and (b) we learn not only when the pre-
diction is incorrect but also when the prediction is
not confident enough.
The algorithm is listed in Algorithm 1: w is the
vector of model parameters, h generates the feature
vector for one example, and xij denotes the jth can-
didate for the ith frame in the training data. xi1,
which denotes the ?correct? candidate for frame i, is
selected to maximize the F1 score for each frame.
The algorithm sequentially inspects all candidates
for each frame and learns when the difference be-
tween the scores of the correct and the current candi-
date is less than a threshold ? . During testing we use
the average of all acquired model vectors, weighted
by the number of iterations they survived in train-
ing. We tuned all system parameters through cross-
validation on the training data. For both languages
we set ? = 10 (we do not normalize feature vectors)
427
Algorithm 1: Re-ranking Perceptron
w = ~0
for i = 1 to n do
for j = 2 to ni do
if w ? h(xij) > w ? h(xi1)? ? then
w? w + h(xi1)? h(xij)
and the number of training epochs to 2.
With respect to the features used, we focus only
on global features that can be extracted indepen-
dently of the local models. We show in Section 6
that this approach performs better on the small
SemEval corpora than approaches that include fea-
tures from the local models. We group the features
into two sets: (a) features that extract information
from the whole candidate set, and (b) features that
model the structure of each candidate frame:
Features from the whole candidate set:
(1) Position of the current candidate in the whole set.
Frame candidates are generated using the dynamic
programming algorithm of Toutanova et al (2005),
and then sorted in descending order of the log prob-
ability of the whole frame (i.e., the sum of all ar-
gument log probabilities as reported by the local
model). Hence, smaller positions indicate candi-
dates that the local model considers better.
(2) For each argument in the current frame, we store
its number of repetitions in the whole candidate set.
The intuition is that an argument that appears in
many candidate frames is most likely correct.
Features from each candidate frame:
(3) The complete sequence of argument labels, ex-
tended with the predicate lemma and voice, similar
to Toutanova et al (2005).
(4) Maximal overlap with a frame from the verb lex-
icon. Both the Spanish and Catalan TreeBanks con-
tain a static lexicon that lists the accepted sequences
of arguments for the most common verbs. For each
candidate frame, we measure the maximal overlap
with the lexicon frames for the given verb and use
the precision, recall, and F1 scores as features.
(5) Average probability (from the local model) of all
arguments in the current frame.
(6) For each argument label that repeats in the cur-
rent frame, we add combinations of the predicate
lemma, voice, argument label, and the number of
label repetitions as features. The intuition is that ar-
gument repetitions typically indicate an error (even
if allowed by the domain constraints).
5 Semantic Class Detection
The semantic class detection subtask has been per-
formed using a naive cascade of heuristics: (1) the
predicted frame for each verb is compared with the
frames present in the provided verbal lexicon, and
the class of the lexicon frame with the largest num-
ber of matching arguments is chosen; (2) if there is
more than one verb with the maximum score, the
first one in the lexicon (i.e., the most frequent) is
used; (3) if the focus verb is not found in the lexicon,
its most frequent class in the training corpus is used;
(4) if the verb does not appear in the training data,
the most frequent class overall (D2) is assigned. The
results obtained on the training corpus are 81.1% F1
for Spanish and 86.6% for Catalan. As a baseline,
assigning the most frequent class for each verb (or
D2 if not seen in training), yields F1 values of 48.1%
for Spanish and 64.0% for Catalan.
6 Results and Discussion
Table 1 lists the results of our system on the Se-
mEval test data. Our results are encouraging con-
sidering the size of the training corpus (e.g., the En-
glish PropBank is 10 times larger than the corpus
used here) and the complexity of the problem (e.g.,
the NER task includes both weak and strong entities;
the SRL task contains 33 core arguments for Span-
ish vs. 6 for English). We analyze the behavior of
our system next.
The first issue that deserves further analysis is the
contribution of our global SRL model. We list the
results of this analysis in Table 2 as improvements
over the local SRL model. We report results for 6
corpora: the 4 test corpora and the 2 training cor-
pora, where the results are generated through 5-fold
cross validation. The first block in the table shows
the contribution of our best re-ranking model. The
second block shows the results of a re-ranking model
using our best feature set but the original re-ranking
Perceptron of Collins and Duffy (2002). The third
block shows the performance of our re-ranking al-
gorithm configured with the features proposed by
Toutanova et al (2005). We draw several conclu-
sions from this experiment: (a) our re-ranking model
428
NER NSD SRL SC
P R F1 P R F1 P R F1 F1
ca.CESS-ECE 79.92% 76.63% 78.24 87.47% 87.47% 87.47 82.16% 70.05% 75.62 85.71
es.CESS-ECE 72.53% 68.48% 70.45 83.30% 83.30% 83.30 86.24% 75.58% 80.56 87.74
ca.3LB 82.04% 79.42% 80.71 85.69% 85.53% 85.61 86.36% 85.30% 85.83 87.35
es.3LB 62.03% 53.85% 57.65 88.14% 88.14% 88.14 82.23% 80.78% 81.50 76.01
Table 1: Official results on the test data. Due to space constraints, we show only the F1 score for SC.
Re-ranking Collins Toutanova
P R F1 P R F1 P R F1
ca.train +1.87 +1.79 +1.83 +1.56 +1.48 +1.52 -6.81 -6.67 -6.73
es.train +3.16 +3.12 +3.14 +2.96 +2.93 +2.95 -6.51 -6.96 -6.75
ca.CESS-ECE +0.77 +0.66 +0.71 +0.99 +0.84 +0.91 -8.11 -6.29 -7.10
es.CESS-ECE +1.85 +1.94 +1.91 +1.45 +1.85 +1.68 -10.84 -8.46 -9.54
ca.3LB +1.58 +1.47 +1.53 +1.48 +1.39 +1.44 -7.71 -7.57 -7.64
es.3LB +2.57 +2.83 +2.71 +2.71 +2.91 +2.82 -10.53 -11.95 -11.26
Table 2: Analysis of the re-ranking model for SRL.
using only global information always outperforms
the local model, with F1 score improvements rang-
ing from 0.71 to 3.14 points; (b) the re-ranking Per-
ceptron proposed here performs better than the orig-
inal algorithm, but the improvement is minimal; and
(c) the feature set proposed here achieve significant
better performance on the SemEval corpora than the
set proposed by Toutanova et al, which never im-
proves over the local model. The model configured
with the Toutanova et al feature set performs mod-
estly because the features are too sparse for the small
SemEval corpora (e.g., all features from the local
model are included, concatenated with the label of
the corresponding argument). On the other hand, we
replicate the behavior of the local model just with
feature (1), and furthermore, all the other 5 global
features proposed have a positive contribution.
In a second experiment we investigated simple
strategies for model combination. We incorporated
NER and NSD information in the re-ranking model
for SRL as follows: for each frame argument, we
add features that concatenate the predicate lemma,
the argument label, and the NER or NSD labels for
the argument head word (we add features both with
and without the predicate lemma). We used only the
best NER/NSD labels from the local models. To re-
duce sparsity, we converted word senses to coarser
classes based on the corresponding WordNet seman-
tic files. This new model boosts the F1 score of our
best re-ranking SRL model with an average of 0.13
points on two corpora (es.3LB and ca.CESS-ECE),
but it reduces the F1 of our best SRL model with an
average of 0.17 points on the other 4 corpora. We
can conclude that, in the current setting, NSD and
NER do not bring useful information to the SRL
problem. However, it is soon to state that problem
combination is not useful. To have a conclusive an-
swer one will have to investigate true joint learning
of the three subtasks.
References
J. Atserias, B. Casas, E. Comelles, M. Gonza`lez, L. Padro?, and
M. Padro?. 2006. Freeling 1.3: Syntactic and semantic ser-
vices in an open-source NLP library. In Proc. of LREC.
X. Carreras, L. Ma`rquez, and L. Padro?. 2003. A simple named
entity extractor using AdaBoost. In CoNLL 2003 Shared
Task Contribution.
M. Collins and N. Duffy. 2002. New ranking algorithms for
parsing and tagging: Kernels over discrete structures, and
the voted perceptron. In Proc. of ACL.
T. Joachims. 1999. Making large-scale SVM learning practi-
cal, Advances in Kernel Methods - Support Vector Learning.
MIT Press, Cambridge, MA.
L. Ma`rquez, M. Surdeanu, P. Comas, and J. Turmo. 2005. A
robust combination strategy for semantic role labeling. In
Proc. of EMNLP.
L. Ma`rquez, M.A. Mart??, M. Taule?, and L. Villarejo. 2007.
SemEval-2007 task 09: Multilevel semantic annotation of
Catalan and Spanish. In Proc. of SemEval-2007, the 4th
Workshop on Semantic Evaluations. Association for Com-
putational Linguistics.
R.E. Schapire and Y. Singer. 1999. Improved boosting algo-
rithms using confidence-rated predictions. Machine Learn-
ing, 37(3).
K. Toutanova, A. Haghighi, and C. Manning. 2005. Joint learn-
ing improves semantic role labeling. In Proc. of ACL.
429
 	
Using Predicate-Argument Structures for Information Extraction
Mihai Surdeanu and Sanda Harabagiu and John Williams and Paul Aarseth
Language Computer Corp.
Richardson, Texas 75080, USA
mihai,sanda@languagecomputer.com
Abstract
In this paper we present a novel, cus-
tomizable IE paradigm that takes advan-
tage of predicate-argument structures. We
also introduce a new way of automatically
identifying predicate argument structures,
which is central to our IE paradigm. It is
based on: (1) an extended set of features;
and (2) inductive decision tree learning.
The experimental results prove our claim
that accurate predicate-argument struc-
tures enable high quality IE results.
1 Introduction
The goal of recent Information Extraction (IE)
tasks was to provide event-level indexing into news
stories, including news wire, radio and television
sources. In this context, the purpose of the HUB
Event-99 evaluations (Hirschman et al, 1999) was
to capture information on some newsworthy classes
of events, e.g. natural disasters, deaths, bombings,
elections, financial fluctuations or illness outbreaks.
The identification and selective extraction of rele-
vant information is dictated by templettes. Event
templettes are frame-like structures with slots rep-
resenting the event basic information, such as main
event participants, event outcome, time and loca-
tion. For each type of event, a separate templette
is defined. The slots fills consist of excerpts from
text with pointers back into the original source mate-
rial. Templettes are designed to support event-based
browsing and search. Figure 1 illustrates a templette
defined for ?market changes? as well as the source
of the slot fillers.
<MARKET_CHANGE_PRI199804281700.1717?1>:=
CURRENT_VALUE:      $308.45
LOCATION:                London
DATE:                        daily   
INSTRUMENT:             London [gold]
AMOUNT_CHANGE:    fell [$4.70] cents
London gold  fell $4.70 cents to $308.35
Time for our daily market report from NASDAQ. 
Figure 1: Templette filled with information about a
market change event.
To date, some of the most successful IE tech-
niques are built around a set of domain relevant lin-
guistic patterns based on select verbs (e.g. fall, gain
or lose for the ?market change? topic). These pat-
terns are matched against documents for identifying
and extracting domain-relevant information. Such
patterns are either handcrafted or acquired automat-
ically. A rich literature covers methods of automati-
cally acquiring IE patterns. Some of the most recent
methods were reported in (Riloff, 1996; Yangarber
et al, 2000).
To process texts efficiently and fast, domain pat-
terns are ideally implemented as finite state au-
tomata (FSAs), a methodology pioneered in the
FASTUS IE system (Hobbs et al, 1997). Although
this paradigm is simple and elegant, it has the dis-
advantage that it is not easily portable from one do-
main of interest to the next.
In contrast, a new, truly domain-independent IE
paradigm may be designed if we know (a) predicates
relevant to a domain; and (b) which of their argu-
ments fill templette slots. Central to this new way
of extracting information from texts are systems that
label predicate-argument structures on the output of
full parsers. One such augmented parser, trained on
data available from the PropBank project has been
recently presented in (Gildea and Palmer, 2002).
In this paper we describe a domain-independent IE
paradigm that is based on predicate-argument struc-
tures identified automatically by two different meth-
ods: (1) the statistical method reported in (Gildea
and Palmer, 2002); and (2) a new method based
on inductive learning which obtains 17% higher F-
score over the first method when tested on the same
data. The accuracy enhancement of predicate argu-
ment recognition determines up to 14% better IE re-
sults. These results enforce our claim that predicate
argument information for IE needs to be recognized
with high accuracy.
The remainder of this paper is organized as fol-
lows. Section 2 reports on the parser that produces
predicate-argument labels and compares it against
the parser introduced in (Gildea and Palmer, 2002).
Section 3 describes the pattern-free IE paradigm and
compares it against FSA-based IE methods. Section
4 describes the integration of predicate-argument
parsers into the IE paradigm and compares the re-
sults against a FSA-based IE system. Section 5 sum-
marizes the conclusions.
2 Learning to Recognize
Predicate-Argument Structures
2.1 The Data
Proposition Bank or PropBank is a one mil-
lion word corpus annotated with predicate-
argument structures. The corpus consists of
the Penn Treebank 2 Wall Street Journal texts
(www.cis.upenn.edu/   treebank). The PropBank
annotations, performed at University of Pennsyl-
vania (www.cis.upenn.edu/   ace) were described
in (Kingsbury et al, 2002). To date PropBank has
addressed only predicates lexicalized by verbs,
proceeding from the most to the least common
verbs while annotating verb predicates in the
corpus. For any given predicate, a survey was made
to determine the predicate usage and if required, the
usages were divided in major senses. However, the
senses are divided more on syntactic grounds than
VPNP
S
VP
PP
NP
Big Board floor traders
ARG0
byassailed
P
wasThe futures halt
ARG1
Figure 2: Sentence with annotated arguments
semantic, under the fundamental assumption that
syntactic frames are direct reflections of underlying
semantics.
The set of syntactic frames are determined by
diathesis alternations, as defined in (Levin, 1993).
Each of these syntactic frames reflect underlying
semantic components that constrain allowable ar-
guments of predicates. The expected arguments
of each predicate are numbered sequentially from
Arg0 to Arg5. Regardless of the syntactic frame
or verb sense, the arguments are similarly labeled
to determine near-similarity of the predicates. The
general procedure was to select for each verb the
roles that seem to occur most frequently and use
these roles as mnemonics for the predicate argu-
ments. Generally, Arg0 would stand for agent,
Arg1 for direct object or theme whereas Arg2 rep-
resents indirect object, benefactive or instrument,
but mnemonics tend to be verb specific. For
example, when retrieving the argument structure
for the verb-predicate assail with the sense ?to
tear attack? from www.cis.upenn.edu/   cotton/cgi-
bin/pblex fmt.cgi, we find Arg0:agent, Arg1:entity
assailed and Arg2:assailed for. Additionally, the ar-
gument may include functional tags from Treebank,
e.g. ArgM-DIR indicates a directional, ArgM-LOC
indicates a locative, and ArgM-TMP stands for a
temporal.
2.2 The Model
In previous work using the PropBank corpus,
(Gildea and Palmer, 2002) proposed a model pre-
dicting argument roles using the same statistical
method as the one employed by (Gildea and Juraf-
sky, 2002) for predicting semantic roles based on the
FrameNet corpus (Baker et al, 1998). This statis-
tical technique of labeling predicate argument oper-
ates on the output of the probabilistic parser reported
in (Collins, 1997). It consists of two tasks: (1) iden-
tifying the parse tree constituents corresponding to
arguments of each predicate encoded in PropBank;
and (2) recognizing the role corresponding to each
argument. Each task can be cast a separate classifier.
For example, the result of the first classifier on the
sentence illustrated in Figure 2 is the identification
of the two NPs as arguments. The second classifier
assigns the specific roles ARG1 and ARG0 given the
predicate ?assailed?.
? POSITION (pos) ? Indicates if the constituent appears  
before or after the the predicate in the sentence.
? VOICE (voice) ? This feature distinguishes between
active or passive voice for the predicate phrase.
are preserved.
of the evaluated phrase. Case and morphological information
? HEAD WORD (hw) ? This feature contains the head word
? PARSE TREE PATH (path): This feature contains the path
in the parse tree between the predicate phrase and the 
argument phrase, expressed as a sequence of nonterminal
labels linked by direction symbols (up or down), e.g.
? PHRASE TYPE (pt): This feature indicates the syntactic
NP for ARG1 in Figure 2.
type of the phrase labeled as a predicate argument, e.g.
noun phrases only, and it indicates if the NP is dominated
by a sentence phrase (typical for subject arguments with
active?voice predicates), or by a verb phrase (typical 
for object arguments).
? GOVERNING CATEGORY (gov) ? This feature applies to
? PREDICATE WORD ? In our implementation this feature
consists of two components: (1) VERB: the word itself with the
case and morphological information preserved; and 
(2) LEMMA which represents the verb normalized to lower 
case and infinitive form. 
NP    S    VP    VP for ARG1 in Figure 2.
Figure 3: Feature Set 1
Statistical methods in general are hindered by the
data sparsity problem. To achieve high accuracy
and resolve the data sparsity problem the method
reported in (Gildea and Palmer, 2002; Gildea and
Jurafsky, 2002) employed a backoff solution based
on a lattice that combines the model features. For
practical reasons, this solution restricts the size of
the feature sets. For example, the backoff lattice
in (Gildea and Palmer, 2002) consists of eight con-
nected nodes for a five-feature set. A larger set of
features will determine a very complex backoff lat-
tice. Consequently, no new intuitions may be tested
as no new features can be easily added to the model.
In our studies we found that inductive learning
through decision trees enabled us to easily test large
sets of features and study the impact of each feature
BOOLEAN NAMED ENTITY FLAGS ? A feature set comprising: 
PHRASAL VERB COLOCATIONS ? Comprises two features:
? pvcSum: the frequency with which a verb is immediately followed by
? pvcMax: the frequency with which a verb is followed by its
                   any preposition or particle.
                   predominant preposition or particle. 
? neOrganization: set to 1 if an organization is recognized in the phrase
? neLocation: set to 1 a location is recognized in the phrase
? nePerson: set to 1 if a person name is recognized in the phrase
? neMoney: set to 1 if a currency expression is recognized in the phrase
? nePercent: set to 1 if a percentage expression is recognized in the phrase
? neTime: set to 1 if a time of day expression is recognized in the phrase
? neDate: set to 1 if a date temporal expression is recognized in the phrase 
word from the constituent, different from the head word.
? CONTENT WORD (cw) ? Lexicalized feature that selects an informative 
PART OF SPEECH OF HEAD WORD (hPos) ? The part of speech tag of
the head word.
PART OF SPEECH OF CONTENT WORD (cPos) ?The part of speech 
tag of the content word.
NAMED ENTITY CLASS OF CONTENT WORD (cNE) ? The class of 
the named entity that includes the content word
Figure 4: Feature Set 2
in NP
last June
PP to VP
be VP
declared
VPSBAR
Sthat
VP
occurred NP
yesterday(a) (b) (c)
Figure 5: Sample phrases with the content word dif-
ferent than the head word. The head words are indi-
cated by the dashed arrows. The content words are
indicated by the continuous arrows.
on the augmented parser that outputs predicate ar-
gument structures. For this reason we used the C5
inductive decision tree learning algorithm (Quinlan,
2002) to implement both the classifier that identifies
argument constituents and the classifier that labels
arguments with their roles.
Our model considers two sets of features: Feature
Set 1 (FS1): features used in the work reported in
(Gildea and Palmer, 2002) and (Gildea and Juraf-
sky, 2002) ; and Feature Set 2 (FS2): a novel set of
features introduced in this paper. FS1 is illustrated
in Figure 3 and FS2 is illustrated in Figure 4.
In developing FS2 we used the following obser-
vations:
Observation 1:
Because most of the predicate arguments are
prepositional attachments (PP) or relative clauses
(SBAR), often the head word (hw) feature from
FS1 is not in fact the most informative word in
H1: if phrase type is PP then
select the right?most child
Example: phrase = "in Texas", cw = "Texas"
ifH2: phrase type is SBAR then
select the left?most sentence (S*) clause
Example: phrase = "that occurred yesterday", cw = "occurred"
if thenH3: phrase type is VP 
if there is a VP child then
else select the head word
select the left?most VP child
Example: phrase = "had placed", cw = "placed"
ifH4: phrase type is ADVP then
select the right?most child not IN or TO
Example: phrase = "more than", cw = "more"
ifH5: phrase type is ADJP then
select the right?most adjective, verb,
noun, or ADJP
Example: phrase = "61 years old", cw = "old"
H6: for for all other phrase types do
select the head word
Example: phrase = "red house", cw = "house"
Figure 6: Heuristics for the detection of content
words
the phrase. Figure 5 illustrates three examples of
this situation. In Figure 5(a), the head word of
the PP phrase is the preposition in, but June is at
least as informative as the head word. Similarly,
in Figure 5(b), the relative clause is featured only
by the relative pronoun that, whereas the verb oc-
curred should also be taken into account. Figure 5(c)
shows another example of an infinitive verb phrase,
in which the head word is to, whereas the verb de-
clared should also be considered. Based on these
observations, we introduced in FS2 the CONTENT
WORD (cw), which adds a new lexicalization from
the argument constituent for better content repre-
sentation. To select the content words we used the
heuristics illustrated in Figure 6.
Observation 2:
After implementing FS1, we noticed that the hw
feature was rarely used, and we believe that this hap-
pens because of data sparsity. The same was noticed
for the cw feature from FS2. Therefore we decided
to add two new features, namely the parts of speech
of the head word and the content word respectively.
These features are called hPos and cPos and are
illustrated in Figure 4. Both these features generate
an implicit yet simple backoff solution for the lexi-
calized features HEAD WORD (hw) and CONTENT
WORD (cw).
Observation 3:
Predicate arguments often contain names or other
expressions identified by named entity (NE) recog-
nizers, e.g. dates, prices. Thus we believe that
this form of semantic information should be intro-
duced in the learning model. In FS2 we added the
following features: (a) the named entity class of
the content word (cNE); and (b) a set of NE fea-
tures that can take only Boolean values grouped as
BOOLEAN NAMED ENTITY FEATURES and defined
in Figure 4. The cNE feature helps recognize the ar-
gument roles, e.g. ARGM-LOC and ARGM-TMP,
when location or temporal expressions are identi-
fied. The Boolean NE flags provide information
useful in processing complex nominals occurring in
argument constituents. For example, in Figure 2
ARG0 is featured not only by the word traders but
also by ORGANIZATION, the semantic class of the
name Big Board.
Observation 4:
Predicate argument structures are recognized accu-
rately when both predicates and arguments are cor-
rectly identified. Often, predicates are lexicalized by
phrasal verbs, e.g. put up, put off. To identify cor-
rectly the verb particle and capture it in the structure
of predicates instead of the argument structure, we
introduced two collocation features that measure the
frequency with which verbs and succeeding prepo-
sitions cooccurr in the corpus. The features are pvc-
Sum and pvcMax and are defined in Figure 4.
2.3 The Experiments
The results presented in this paper were obtained
by training on Proposition Bank (PropBank) release
2002/7/15 (Kingsbury et al, 2002). Syntactic infor-
mation was extracted from the gold-standard parses
in TreeBank Release 2. As named entity information
is not available in PropBank/TreeBank we tagged
the training corpus with NE information using an
open-domain NE recognizer, having 96% F-measure
on the MUC61 data. We reserved section 23 of Prop-
Bank/TreeBank for testing, and we trained on the
rest. Due to memory limitations on our hardware,
for the argument finding task we trained on the first
150 KB of TreeBank (about 11% of TreeBank), and
1The Message Understanding Conferences (MUC) were IE
evaluation exercises in the 90s. Starting with MUC6 named
entity data was available.
for the role assignment task on the first 75 KB of
argument constituents (about 60% of PropBank an-
notations).
Table 1 shows the results obtained by our induc-
tive learning approach. The first column describes
the feature sets used in each of the 7 experiments
performed. The following three columns indicate
the precision (P), recall (R), and F-measure (   )2
obtained for the task of identifying argument con-
stituents. The last column shows the accuracy (A)
for the role assignment task using known argument
constituents. The first row in Table 1 lists the re-
sults obtained when using only the FS1 features.
The next five lines list the individual contributions
of each of the newly added features when combined
with the FS1 features. The last line shows the re-
sults obtained when all features from FS1 and FS2
were used.
Table 1 shows that the new features increase the
argument identification F-measure by 3.61%, and
the role assignment accuracy with 4.29%. For the
argument identification task, the head and content
word features have a significant contribution for the
task precision, whereas NE features contribute sig-
nificantly to the task recall. For the role assignment
task the best features from the feature set FS2 are
the content word features (cw and cPos) and the
Boolean NE flags, which show that semantic infor-
mation, even if minimal, is important for role clas-
sification. Surprisingly, the phrasal verb collocation
features did not help for any of the tasks, but they
were useful for boosting the decision trees. Deci-
sion tree learning provided by C5 (Quinlan, 2002)
has built in support for boosting. We used it and
obtained improvements for both tasks. The best F-
measure obtained for argument constituent identifi-
cation was 88.98% in the fifth iteration (a 0.76% im-
provement). The best accuracy for role assignment
was 83.74% in the eight iteration (a 0.69% improve-
ment)3. We further analyzed the boosted trees and
noticed that phrasal verb collocation features were
mainly responsible for the improvements. This is
the rationale for including them in the FS2 set.
We also were interested in comparing the results
2 	



3These results, listed also on the last line of Table 2, dif-
fer from those in Table 1 because they were produced after the
boosting took place.
Features Arg P Arg R Arg  Role A
FS1 84.96 84.26 84.61 78.76
FS1 + hPos 92.24 84.50 88.20 79.04
FS1 + cw, cPos 92.19 84.67 88.27 80.80
FS1 + cNE 83.93 85.69 84.80 79.85
FS1 + NE flags 87.78 85.71 86.73 81.28
FS1 + pvcSum + 84.88 82.77 83.81 78.62
pvcMax
FS1 + FS2 91.62 85.06 88.22 83.05
Table 1: Inductive learning results for argument
identification and role assignment
Model Implementation Arg  Role A
Statistical (Gildea and Palmer) - 82.8
This study 71.86 78.87
Decision Trees FS1 84.61 78.76
FS1 + FS2 88.98 83.74
Table 2: Comparison of statistical and decision tree
learning models
of the decision-tree-based method against the re-
sults obtained by the statistical approach reported
in (Gildea and Palmer, 2002). Table 2 summarizes
the results. (Gildea and Palmer, 2002) report the re-
sults listed on the first line of Table 2. Because no F-
scores were reported for the argument identification
task, we re-implemented the model and obtained the
results listed on the second line. It looks like we
had some implementation differences, and our re-
sults for the argument role classification task were
slightly worse. However, we used our results for the
statistical model for comparing with the inductive
learning model because we used the same feature ex-
traction code for both models. Lines 3 and 4 list the
results of the inductive learning model with boosting
enabled, when the features were only from FS1, and
from FS1 and FS2 respectively. When comparing
the results obtained for both models when using only
features from FS1, we find that almost the same re-
sults were obtained for role classification, but an en-
hancement of almost 13% was obtained when recog-
nizing argument constituents. When comparing the
statistical model with the inductive model that uses
all features, there is an enhancement of 17.12% for
argument identification and 4.87% for argument role
recognition.
Another significant advantage of our inductive
learning approach is that it scales better to un-
Document(s)
POS
Tagger
NPB
Identifier
Dependency
Parser
Named Entity Recognizer
Entity
Coreference
Document(s) Named Entity
Recognizer
Phrasal
Parser (FSA) Combiner (FSA)
Entity
Coreference
Event
Recognizer (FSA)
Event
Coreference
Event
Merging
Template(s)
Pred/Arg
Identification Predicate Arguments
Mapping
into Template Slots
Event
Coreference
Event
Merging
Template(s)Full Parser
(b)
(a)
Figure 7: IE architectures: (a) Architecture based on predicate/argument relations; (b) FSA-based IE system
known predicates. The statistical model introduced
in Gildea and Jurafsky (2002) uses predicate lex-
ical information at most levels in the probability
lattice, hence its scalability to unknown predicates
is limited. In contrast, the decision tree approach
uses predicate lexical information only for 5% of the
branching decisions recorded when testing the role
assignment task, and only for 0.01% of the branch-
ing decisions seen during the argument constituent
identification evaluation.
3 The IE Paradigm
Figure 7(a) illustrates an IE architecture that em-
ploys predicate argument structures. Documents are
processed in parallel to: (1) parse them syntactically,
and (2) recognize the NEs. The full parser first per-
forms part-of-speech (POS) tagging using transfor-
mation based learning (TBL) (Brill, 1995). Then
non-recursive, or basic, noun phrases (NPB) are
identified using the TBL method reported in (Ngai
and Florian, 2001). At last, the dependency parser
presented in (Collins, 1997) is used to generate the
full parse. This approach allows us to parse the sen-
tences with less than 40 words from TreeBank sec-
tion 23 with an F-measure slightly over 85% at an
average of 0.12 seconds/sentence on a 2GHz Pen-
tium IV computer.
The parse texts marked with NE tags are passed to
a module that identifies entity coreference in docu-
ments, resolving pronominal and nominal anaphors
and normalizing coreferring expressions. The parses
are also used by a module that recognizes predi-
cate argument structures with any of the methods
described in Section 2.
For each templette modeling a different do-
main a mapping between predicate arguments and
templette slots is produced. Figure 8 illus-
trates the mapping produced for two Event99 do-
INSTRUMENTARG1 and MARKET_CHANGE_VERB 
ARG2 and (MONEY or PERCENT or NUMBER or QUANTITY) and
MARKET_CHANGE_VERB AMOUNT_CHANGE
MARKET_CHANGE_VERB CURRENT_VALUE
(PERSON and ARG0 and DIE_VERB) or
(PERSON and ARG1 and KILL_VERB) DECEASED
(ARG0 and KILL_VERB) or
(ARG1 and DIE_VERB) AGENT_OF_DEATH
(ARGM?TMP and ILNESS_NOUN) or
KILL_VERB or DIE_VERB MANNER_OF_DEATH
ARGM?TMP and DATE DATE
(ARGM?LOC or ARGM?TMP) and
LOCATION LOCATION
(a)
(b)
(ARG4 or ARGM_DIR) and NUMBER and 
Figure 8: Mapping rules between predicate ar-
guments and templette slots for: (a) the ?market
change? domain, and (b) the ?death? domain
mains. The ?market change? domain monitors
changes (AMOUNT CHANGE) and current values
(CURRENT VALUE) for financial instruments (IN-
STRUMENT). The ?death? domain extracts the de-
scription of the person deceased (DECEASED), the
manner of death (MANNER OF DEATH), and, if ap-
plicable, the person to whom the death is attributed
(AGENT OF DEATH).
To produce the mappings we used training data
that consists of: (1) texts, and (2) their correspond-
ing filled templettes. Each templette has pointers
back to the source text similarly to the example pre-
sented in Figure 1. When the predicate argument
structures were identified, the mappings were col-
lected as illustrated in Figure 9. Figure 9(a) shows
an interesting aspect of the mappings. Although the
role classification of the last argument is incorrect (it
should have been identified as ARG4), it is mapped
into the CURRENT-VALUE slot. This shows how the
mappings resolve incorrect but consistent classifica-
tions. Figure 9(b) shows the flexibility of the system
to identify and classify constituents that are not close
to the predicate phrase (ARG0). This is a clear ad-
5 1/4
ARG2
34 1/2to
ARGM?DIR
flewThe space shuttle Challenger apart over Florida like a billion?dollar confetti killing six astronauts
NP VP
S
NP
PP
NP
fellNorwalk?based Micro Warehouse
ARG1
NP
ADVP PP PP S
VP
VPNP
S
ARG0 P ARG1
INSTRUMENT AMOUNT_CHANGE CURRENT_VALUE AGENT_OF_DEATH MANNER_OF_DEATH DECEASED
Mappings
(a) (b)
Figure 9: Predicate argument mapping examples for: (a) the ?market change? domain, and (b) the ?death?
domain
vantage over the FSA-based system, which in fact
missed the AGENT-OF-DEATH in this sentence. Be-
cause several templettes might describe the same
event, event coreference is processed and, based on
the results, templettes are merged when necessary.
The IE architecture in Figure 7(a) may be com-
pared with the IE architecture with cascaded FSA
represented in Figure 7(b) and reported in (Sur-
deanu and Harabagiu, 2002). Both architectures
share the same NER, coreference and merging
modules. Specific to the FSA-based architec-
ture are the phrasal parser, which identifies simple
phrases such as basic noun or verb phrases (some
of them domain specific), the combiner, which
builds domain-dependent complex phrases, and the
event recognizer, which detects the domain-specific
Subject-Verb-Object (SVO) patterns. An example
of a pattern used by the FSA-based architecture
is:   DEATH-CAUSE KILL-VERB PERSON  , where
DEATH-CAUSE may identify more than 20 lexemes,
e.g. wreck, catastrophe, malpractice, and more than
20 verbs are KILL-VERBS, e.g. murder, execute, be-
head, slay. Most importantly, each pattern must rec-
ognize up to 26 syntactic variations, e.g. determined
by the active or passive form of the verb, relative
subjects or objects etc. Predicate argument struc-
tures offer the great advantage that syntactic vari-
ations do not need to be accounted by IE systems
anymore.
Because entity and event coreference, as well as
templette merging will attempt to recover from par-
tial patterns or predicate argument recognitions, and
our goal is to compare the usage of FSA patterns
versus predicate argument structures, we decided to
disable the coreference and merging modules. This
explains why in Figure 7 these modules are repre-
System Market Change Death
Pred/Args Statistical 68.9% 58.4%
Pred/Args Inductive 82.8% 67.0%
FSA 91.3% 72.7%
Table 3: Templette F-measure (    ) scores for the
two domains investigated
System Correct Missed Incorrect
Pred/Args Statistical 26 16 3
Pred/Args Inductive 33 9 2
FSA 38 4 2
Table 4: Number of event structures (FSA patterns
or predicate argument structures) matched
sented with dashed lines.
4 Experiments with The Integration of
Predicate Argument Structures in IE
To evaluate the proposed IE paradigm we selected
two Event99 domains: ?market change?, which
tracks changes in stock indexes, and ?death?, which
extracts all manners of human deaths. These do-
mains were selected because most of the domain in-
formation can be processed without needing entity
or event coreference. Moreover, one of the domains
(market change) uses verbs commonly used in Prop-
Bank/TreeBank, while the other (death) uses rela-
tively unknown verbs, so we can also evaluate how
well the system scales to verbs unseen in training.
Table 3 lists the F-scores for the two domains.
The first line of the Table lists the results obtained
by the IE architecture illustrated in Figure 7(a) when
the predicate argument structures were identified by
the statistical model. The next line shows the same
results for the inductive learning model. The last
line shows the results for the IE architecture in Fig-
ure 7(b). The results obtained by the FSA-based IE
were the best, but they were made possible by hand-
crafted patterns requiring an effort of 10 person days
per domain. The only human effort necessary in
the new IE paradigm was imposed by the genera-
tion of mappings between arguments and templette
slots, accomplished in less than 2 hours per domain,
given that the training templettes are known. Addi-
tionally, it is easier to automatically learn these map-
pings than to acquire FSA patterns.
Table 3 also shows that the new IE paradigm per-
forms better when the predicate argument structures
are recognized with the inductive learning model.
The cause is the substantial difference in quality
of the argument identification task between the two
models. The Table shows that the new IE paradigm
with the inductive learning model achieves about
90% of the performance of the FSA-based system
for both domains, even though one of the domains
uses mainly verbs rarely seen in training (e.g. ?die?
appears 5 times in PropBank).
Another way of evaluating the integration of pred-
icate argument structures in IE is by comparing the
number of events identified by each architecture. Ta-
ble 4 shows the results. Once again, the new IE
paradigm performs better when the predicate argu-
ment structures are recognized with the inductive
learning model. More events are missed by the sta-
tistical model which does not recognize argument
constituents as well the inductive learning model.
5 Conclusion
This paper reports on a novel inductive learning
method for identifying predicate argument struc-
tures in text. The proposed approach achieves over
88% F-measure for the problem of identifying argu-
ment constituents, and over 83% accuracy for the
task of assigning roles to pre-identified argument
constituents. Because predicate lexical information
is used for less than 5% of the branching decisions,
the generated classifier scales better than the statisti-
cal method from (Gildea and Palmer, 2002) to un-
known predicates. This way of identifying pred-
icate argument structures is a central piece of an
IE paradigm easily customizable to new domains.
The performance degradation of this paradigm when
compared to IE systems based on hand-crafted pat-
terns is only 10%.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998.
The Berkeley FrameNet Project. In Proceedings of COL-
ING/ACL ?98:86-90,. Montreal, Canada.
Eric Brill. 1995. Transformation-Based Error-Driven Learning
and Natural Language Processing: A Case Study in Part of
Speech Tagging. Computational Linguistics.
Michael Collins. 1997. Three Generative, Lexicalized Mod-
els for Statistical Parsing. In Proceedings of the 35th An-
nual Meeting of the Association for Computational Linguis-
tics (ACL 1997):16-23, Madrid, Spain.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic Labeling
of Semantic Roles. Computational Linguistics, 28(3):245-
288.
Daniel Gildea and Martha Palmer. 2002. The Necessity of
Parsing for Predicate Argument Recognition. In Proceed-
ings of the 40th Meeting of the Association for Computa-
tional Linguistics (ACL 2002):239-246, Philadelphia, PA.
Lynette Hirschman, Patricia Robinson, Lisa Ferro, Nancy Chin-
chor, Erica Brown, Ralph Grishman, Beth Sundheim 1999.
Hub-4 Event99 General Guidelines and Templettes.
Jerry R. Hobbs, Douglas Appelt, John Bear, David Israel,
Megumi Kameyama, Mark E. Stickel, and Mabry Tyson.
1997. FASTUS: A Cascaded Finite-State Transducer for Ex-
tracting Information from Natural-Language Text. In Finite-
State Language Processing, pages 383-406, MIT Press,
Cambridge, MA.
Paul Kingsbury, Martha Palmer, and Mitch Marcus. 2002.
Adding Semantic Annotation to the Penn TreeBank. In Pro-
ceedings of the Human Language Technology Conference
(HLT 2002):252-256, San Diego, California.
Beth Levin. 1993. English Verb Classes and Alternations a
Preliminary Investigation. University of Chicago Press.
Grace Ngai and Radu Florian. 2001. Transformation-
Based Learning in The Fast Lane. In Proceedings of the
North American Association for Computational Linguistics
(NAACL 2001):40-47.
Ross Quinlan. 2002. Data Mining Tools See5 and C5.0.
http://www.rulequest.com/see5-info.html.
Ellen Riloff and Rosie Jones. 1996. Automatically Generating
Extraction Patterns from Untagged Text. In Proceedings of
the Thirteenth National Conference on Artificial Intelligence
(AAAI-96)):1044-1049.
Mihai Surdeanu and Sanda Harabagiu. 2002. Infrastructure for
Open-Domain Information Extraction In Proceedings of the
Human Language Technology Conference (HLT 2002):325-
330.
Roman Yangarber, Ralph Grishman, Pasi Tapainen and Silja
Huttunen, 2000. Automatic Acquisition of Domain Knowl-
edge for Information Extraction. In Proceedings of the
18th International Conference on Computational Linguistics
(COLING-2000): 940-946, Saarbrucken, Germany.
The Role of Lexico-Semantic Feedback in Open-Domain Textual
Question-Answering
Sanda Harabagiu, Dan Moldovan
Marius Pas?ca, Rada Mihalcea, Mihai Surdeanu,
Ra?zvan Bunescu, Roxana G??rju, Vasile Rus and Paul Mora?rescu
Department of Computer Science and Engineering
Southern Methodist University
Dallas, TX 75275-0122
 
sanda  @engr.smu.edu
Abstract
This paper presents an open-domain
textual Question-Answering system
that uses several feedback loops to en-
hance its performance. These feedback
loops combine in a new way statistical
results with syntactic, semantic or
pragmatic information derived from
texts and lexical databases. The paper
presents the contribution of each feed-
back loop to the overall performance of
76% human-assessed precise answers.
1 Introduction
Open-domain textual Question-Answering
(Q&A), as defined by the TREC competitions1 ,
is the task of identifying in large collections of
documents a text snippet where the answer to
a natural language question lies. The answer
is constrained to be found either in a short (50
bytes) or a long (250 bytes) text span. Frequently,
keywords extracted from the natural language
question are either within the text span or in
its immediate vicinity, forming a text para-
graph. Since such paragraphs must be identified
throughout voluminous collections, automatic
and autonomous Q&A systems incorporate an
index of the collection as well as a paragraph
retrieval mechanism.
Recent results from the TREC evaluations
((Kwok et al, 2000) (Radev et al, 2000) (Allen
1The Text REtrieval Conference (TREC) is a series of
workshops organized by the National Institute of Standards
and Technology (NIST), designed to advance the state-of-
the-art in information retrieval (IR)
et al, 2000)) show that Information Retrieval (IR)
techniques alone are not sufficient for finding an-
swers with high precision. In fact, more and more
systems adopt architectures in which the seman-
tics of the questions are captured prior to para-
graph retrieval (e.g. (Gaizauskas and Humphreys,
2000) (Harabagiu et al, 2000)) and used later in
extracting the answer (cf. (Abney et al, 2000)).
When processing a natural language question two
goals must be achieved. First we need to know
what is the expected answer type; in other words,
we need to know what we are looking for. Sec-
ond, we need to know where to look for the an-
swer, e.g. we must identify the question keywords
to be used in the paragraph retrieval.
The expected answer type is determined based
on the question stem, e.g. who, where or how
much and eventually one of the question concepts,
when the stem is ambiguous (for example what),
as described in (Harabagiu et al, 2000) (Radev et
al., 2000) (Srihari and Li, 2000). However finding
question keywords that retrieve all candidate an-
swers cannot be achieved only by deriving some
of the words used in the question. Frequently,
question reformulations use different words, but
imply the same answer. Moreover, many equiv-
alent answers are phrased differently. In this pa-
per we argue that the answer to complex natural
language questions cannot be extracted with sig-
nificant precision from large collections of texts
unless several lexico-semantic feedback loops are
allowed.
In Section 2 we survey the related work
whereas in Section 3 we describe the feedback
loops that refine the search for correct answers.
Section 4 presents the approach of devising key-
word alternations whereas Section 5 details the
recognition of question reformulations. Section 6
evaluates the results of the Q&A system and Sec-
tion 7 summarizes the conclusions.
2 Related work
Mechanisms for open-domain textual Q&A were
not discovered in the vacuum. The 90s witnessed
a constant improvement of IR systems, deter-
mined by the availability of large collections of
texts and the TREC evaluations. In parallel, In-
formation Extraction (IE) techniques were devel-
oped under the TIPSTER Message Understand-
ing Conference (MUC) competitions. Typically,
IE systems identify information of interest in a
text and map it to a predefined, target represen-
tation, known as template. Although simple com-
binations of IR and IE techniques are not practical
solutions for open-domain textual Q&A because
IE systems are based on domain-specific knowl-
edge, their contribution to current open-domain
Q&A methods is significant. For example, state-
of-the-art Named Entity (NE) recognizers devel-
oped for IE systems were readily available to be
incorporated in Q&A systems and helped recog-
nize names of people, organizations, locations or
dates.
Assuming that it is very likely that the answer
is a named entity, (Srihari and Li, 2000) describes
a NE-supported Q&A system that functions quite
well when the expected answer type is one of the
categories covered by the NE recognizer. Un-
fortunately this system is not fully autonomous,
as it depends on IR results provided by exter-
nal search engines. Answer extractions based on
NE recognizers were also developed in the Q&A
presented in (Abney et al, 2000) (Radev et al,
2000) (Gaizauskas and Humphreys, 2000). As
noted in (Voorhees and Tice, 2000), Q&A sys-
tems that did not include NE recognizers per-
formed poorly in the TREC evaluations, espe-
cially in the short answer category. Some Q&A
systems, like (Moldovan et al, 2000) relied both
on NE recognizers and some empirical indicators.
However, the answer does not always belong
to a category covered by the NE recognizer. For
such cases several approaches have been devel-
oped. The first one, presented in (Harabagiu et
al., 2000), the answer type is derived from a large
answer taxonomy. A different approach, based on
statistical techniques was proposed in (Radev et
al., 2000). (Cardie et al, 2000) presents a method
of extracting answers as noun phrases in a novel
way. Answer extraction based on grammatical
information is also promoted by the system de-
scribed in (Clarke et al, 2000).
One of the few Q&A systems that takes into
account morphological, lexical and semantic al-
ternations of terms is described in (Ferret et al,
2000). To our knowledge, none of the cur-
rent open-domain Q&A systems use any feed-
back loops to generate lexico-semantic alterna-
tions. This paper shows that such feedback loops
enhance significantly the performance of open-
domain textual Q&A systems.
3 Textual Q&A Feedback Loops
Before initiating the search for the answer to a
natural language question we take into account
the fact that it is very likely that the same ques-
tion or a very similar one has been posed to the
system before, and thus those results can be used
again. To find such cached questions, we measure
the similarity to the previously processed ques-
tions and when a reformulation is identified, the
system returns the corresponding cached correct
answer, as illustrated in Figure 1.
When no reformulations are detected, the
search for answers is based on the conjecture that
the eventual answer is likely to be found in a
text paragraph that (a) contains the most repre-
sentative question concepts and (b) includes a tex-
tual concept of the same category as the expected
answer. Since the current retrieval technology
does not model semantic knowledge, we break
down this search into a boolean retrieval, based
on some question keywords and a filtering mech-
anism, that retains only those passages containing
the expected answer type. Both the question key-
words and the expected answer type are identified
by using the dependencies derived from the ques-
tion parse.
By implementing our own version of the pub-
licly available Collins parser (Collins, 1996), we
also learned a dependency model that enables the
mapping of parse trees into sets of binary rela-
tions between the head-word of each constituent
and its sibling-words. For example, the parse tree
of TREC-9 question Q210: ?How many dogs pull
a sled in the Iditarod ?? is:
JJ
S
Iditarod
VP
NP
PP
NP
NNPDTINNN
NP
DTVBPNNS
NP
manyHow
WRB
dogs pull a sled in the
For each possible constituent in a parse tree,
rules first described in (Magerman, 1995) and
(Jelinek et al, 1994) identify the head-child and
propagate the head-word to its parent. For the
parse of question Q210 the propagation is:
NP (sled)
DT NN DTIN
manyHow
WRB
dogs
NNSJJ
NP (dogs)
VBP
pull a sled in the Iditarod
NNP (Iditarod)
NP (Iditarod)
PP (Iditarod)
NP (sled)
VP (pull)
S (pull)
When the propagation is over, head-modifier
relations are extracted, generating the following
dependency structure, called question semantic
form in (Harabagiu et al, 2000).
dogs IditarodCOUNT pull sled
In the structure above, COUNT represents the
expected answer type, replacing the question stem
?how many?. Few question stems are unambigu-
ous (e.g. who, when). If the question stem is am-
biguous, the expected answer type is determined
by the concept from the question semantic form
that modifies the stem. This concept is searched
in an ANSWER TAXONOMY comprising several
tops linked to a significant number of WordNet
noun and verb hierarchies. Each top represents
one of the possible expected answer types imple-
mented in our system (e.g. PERSON, PRODUCT,
NUMERICAL VALUE, COUNT, LOCATION). We
encoded a total of 38 possible answer types.
In addition, the question keywords used for
paragraph retrieval are also derived from the ques-
tion semantic form. The question keywords are
organized in an ordered list which first enumer-
ates the named entities and the question quota-
tions, then the concepts that triggered the recogni-
tion of the expected answer type followed by all
adjuncts, in a left-to-right order, and finally the
question head. The conjunction of the keywords
represents the boolean query applied to the doc-
ument index. (Moldovan et al, 2000) details the
empirical methods used in our system for trans-
forming a natural language question into an IR
query.
Answer Semantic Form
No
No
Yes
Lexical 
Alternations
Semantic
Alternations
Question Semantic Form
Answer Logical Form
S-UNIFICATIONS
Expected Answer Type 
Question Logical Form
ABDUCTIVE   PROOF
in paragraph
No
Yes
No
Yes
LOOP 2
Filter out paragraph
Expected Answer Type
Question Keywords
Min<Number Paragraphs<Max No
LOOP 1Index
Yes LOOP 3
Yes
PARSE
	



Retrieval
Cached Questions
Cached Answers
    
Question
REFORMULATION
Figure 1: Feedbacks for the Answer Search.
It is well known that one of the disadvantages
of boolean retrieval is that it returns either too
many or too few documents. However, for ques-
tion answering, this is an advantage, exploited by
the first feedback loop represented in Figure 1.
Feedback loop 1 is triggered when the number of
retrieved paragraphs is either smaller than a min-
imal value or larger than a maximal value deter-
mined beforehand for each answer type. Alterna-
tively, when the number of paragraphs is within
limits, those paragraphs that do not contain at
least one concept of the same semantic category
as the expected answer type are filtered out. The
remaining paragraphs are parsed and their depen-
dency structures, called answer semantic forms,
are derived.
Feedback loop 2 illustrated in Figure 1 is acti-
vated when the question semantic form and the
answer semantic form cannot by unified. The uni-
fication involves three steps:
 Step 1: The recognition of the expected answer
type. The first step marks all possible concepts
that are answer candidates. For example, in the
case of TREC -9 question Q243: ?Where did the
ukulele originate ??, the expected answer type is
LOCATION. In the paragraph ?the ukulele intro-
duced from Portugal into the Hawaiian islands?
contains two named entities of the category LO-
CATION and both are marked accordingly.
 Step 2: The identification of the question con-
cepts. The second step identifies the question
words, their synonyms, morphological deriva-
tions or WordNet hypernyms in the answer se-
mantic form.
 Step 3: The assessment of the similarities of
dependencies. In the third step, two classes of
similar dependencies are considered, generating
unifications of the question and answer semantic
forms:Proceedings of the 12th Conference of the European Chapter of the ACL, pages 246?254,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Company-Oriented Extractive Summarization of Financial News?
Katja Filippova?, Mihai Surdeanu?, Massimiliano Ciaramita?, Hugo Zaragoza?
?EML Research gGmbH ?Yahoo! Research
Schloss-Wolfsbrunnenweg 33 Avinguda Diagonal 177
69118 Heidelberg, Germany 08018 Barcelona, Spain
filippova@eml-research.de,{mihais,massi,hugoz}@yahoo-inc.com
Abstract
The paper presents a multi-document sum-
marization system which builds company-
specific summaries from a collection of fi-
nancial news such that the extracted sen-
tences contain novel and relevant infor-
mation about the corresponding organiza-
tion. The user?s familiarity with the com-
pany?s profile is assumed. The goal of
such summaries is to provide information
useful for the short-term trading of the cor-
responding company, i.e., to facilitate the
inference from news to stock price move-
ment in the next day. We introduce a
novel query (i.e., company name) expan-
sion method and a simple unsupervized al-
gorithm for sentence ranking. The sys-
tem shows promising results in compari-
son with a competitive baseline.
1 Introduction
Automatic text summarization has been a field of
active research in recent years. While most meth-
ods are extractive, the implementation details dif-
fer considerably depending on the goals of a sum-
marization system. Indeed, the intended use of the
summaries may help significantly to adapt a par-
ticular summarization approach to a specific task
whereas the broadly defined goal of preserving rel-
evant, although generic, information may turn out
to be of little use.
In this paper we present a system whose goal is
to extract sentences from a collection of financial
?This work was done during the first author?s internship
at Yahoo! Research. Mihai Surdeanu is currently affiliated
with Stanford University (mihais@stanford.edu).
Massimiliano Ciaramita is currently at Google
(massi@google.com).
news to inform about important events concern-
ing companies, e.g., to support trading (i.e., buy or
sell) the corresponding symbol on the next day, or
managing a portfolio. For example, a company?s
announcement of surpassing its earnings? estimate
is likely to have a positive short-term effect on its
stock price, whereas an announcement of job cuts
is likely to have the reverse effect. We demonstrate
how existing methods can be extended to achieve
precisely this goal.
In a way, the described task can be classified
as query-oriented multi-document summarization
because we are mainly interested in information
related to the company and its sector. However,
there are also important differences between the
two tasks.
? The name of the company is not a query,
e.g., as it is specified in the context of the
DUC competitions1, and requires an exten-
sion. Initially, a query consists exclusively
of the ?symbol?, i.e., the abbreviation of the
name of a company as it is listed on the stock
market. For example, WPO is the abbrevia-
tion used on the stock market to refer to The
Washington Post?a large media and educa-
tion company. Such symbols are rarely en-
countered in the news and cannot be used to
find all the related information.
? The summary has to provide novel informa-
tion related to the company and should avoid
general facts about it which the user is sup-
posed to know. This point makes the task
related to update summarization where one
has to provide the user with new information
1http://duc.nist.gov; since 2008 TAC: http:
//www.nist.gov/tac.
246
given some background knowledge2. In our
case, general facts about the company are as-
sumed to be known by the user. Given WPO,
we want to distinguish between The Wash-
ington Post is owned by The Washington Post
Company, a diversified education and media
company and The Post recently went through
its third round of job cuts and reported an
11% decline in print advertising revenues for
its first quarter, the former being an example
of background information whereas the lat-
ter is what we would like to appear in the
summary. Thus, the similarity to the query
alone is not the decisive parameter in com-
puting sentence relevance.
? While the summaries must be specific for a
given organization, important but general fi-
nancial events that drive the overall market
must be included in the summary. For exam-
ple, the recent subprime mortgage crisis af-
fected the entire economy regardless of the
sector.
Our system proceeds in the three steps illus-
trated in Figure 1. First, the company symbol is
expanded with terms relevant for the company, ei-
ther directly ? e.g., iPod is directly related to Apple
Inc. ? or indirectly ? i.e., using information about
the industry or sector the company operates in. We
detail our symbol expansion algorithm in Section
3. Second, this information is used to rank sen-
tences based on their relatedness to the expanded
query and their overall importance (Section 4). Fi-
nally, the most relevant sentences are re-ranked
based on the degree of novelty they carry (Section
5).
The paper makes the following contributions.
First, we present a new query expansion tech-
nique which is useful in the context of company-
dependent news summarization as it helps identify
sentences important to the company. Second, we
introduce a simple and efficient method for sen-
tence ranking which foregrounds novel informa-
tion of interest. Our system performs well in terms
of the ROUGE score (Lin & Hovy, 2003) com-
pared with a competitive baseline (Section 6).
2 Data
The data we work with is a collection of financial
news consolidated and distributed by Yahoo! Fi-
2See the DUC 2007 and 2008 update tracks.
nance3 from various sources4. Each story is la-
beled as being relevant for a company ? i.e., it
appears in the company?s RSS feed ? if the story
mentions either the company itself or the sector the
company belongs to. Altogether the corpus con-
tains 88,974 news articles from a period of about
5 months (148 days). Some articles are labeled
as being relevant for several companies. The total
number of (company name, news collection) pairs
is 46,444.
The corpus is cleaned of HTML tags, embed-
ded graphics and unrelated information (e.g., ads,
frames) with a set of manually devised rules. The
filtering is not perfect but removes most of the
noise. Each article is passed through a language
processing pipeline (described in (Atserias et al,
2008)). Sentence boundaries are identified by
means of simple heuristics. The text is tokenized
according to Penn TreeBank style and each to-
ken lemmatized using Wordnet?s morphological
functions. Part of speech tags and named entities
(LOC, PER, ORG, MISC) are identified by means
of a publicly available named-entity tagger5 (Cia-
ramita & Altun, 2006, SuperSense). Apart from
that, all sentences which are shorter than 5 tokens
and contain neither nouns nor verbs are sorted out.
We apply the latter filter as we are interested in
textual information only. Numeric information
contained, e.g., in tables can be easily and more
reliably obtained from the indices tables available
online.
3 Query Expansion
In company-oriented summarization query expan-
sion is crucial because, by default, our query con-
tains only the symbol, that is the abbreviation of
the name of the company. Unfortunately, exist-
ing query expansion techniques which utilize such
knowledge sources as WordNet or Wikipedia are
not useful for symbol expansion. WordNet does
not include organizations in any systematic way.
Wikipedia covers many companies but it is unclear
how it can be used for expansion.
3http://finance.yahoo.com
4http://biz.yahoo.com, http://www.
seekingalpha.com, http://www.marketwatch.
com, http://www.reuters.com, http://www.
fool.com, http://www.thestreet.com, http:
//online.wsj.com, http://www.forbes.com,
http://www.cnbc.com, http://us.ft.com,
http://www.minyanville.com
5http://sourceforge.net/projects/
supersensetag
247
Expansion
Query
Expanded
Query
Relatedness
to Query
Filtering
Relevant
Sentences
Ranking
Novelty
Company
Profile
Yahoo! Finance
Symbol
Summary
News
Figure 1: System architecture
Intuitively, a good expansion method should
provide us with a list of products, or properties,
of the company, the field it operates in, the typi-
cal customers, etc. Such information is normally
found on the profile page of a company at Yahoo!
Finance6. There, so called ?business summaries?
provide succinct and financially relevant informa-
tion about the company. Thus, we use business
summaries as follows. For every company sym-
bol in our collection, we download its business
summary, split it into tokens, remove all words
but nouns and verbs which we then lemmatize.
Since words like company are fairly uninforma-
tive in the context of our task, we do not want to
include them in the expanded query. To filter out
such words, we compute the company-dependent
TF*IDF score for every word on the collection of
all business summaries:
score(w) = tfw,c ? log
?
N
cfw
?
(1)
where c is the business summary of a company,
tfw,c is the frequency of w in c, N is the total
number of business summaries we have, cfw is
the number of summaries that contain w. This
formula penalizes words occurring in most sum-
maries (e.g., company, produce, offer, operate,
found, headquarter, management). At the mo-
ment of running the experiments, N was about
3,000, slightly less than the total number of sym-
6http://finance.yahoo.com/q/pr?s=AAPL
where the trading symbol of any company can be used
instead of AAPL.
bols because some companies do not have a busi-
ness summary on Yahoo! Finance. It is impor-
tant to point out that companies without a business
summary are usually small and are seldom men-
tioned in news articles: for example, these compa-
nies had relevant news articles in only 5% of the
days monitored in this work.
Table 1 gives the ten high scoring words for
three companies (Apple Inc. ? the computer and
software manufacture, Delta Air Lines ? the air-
line, and DaVita ? dyalisis services). Table 1
shows that this approach succeeds in expanding
the symbol with terms directly related to the com-
pany, e.g., ipod for Apple, but also with more gen-
eral information like the industry or the company
operates in, e.g., software and computer for Apple.
All words whose TF*IDF score is above a certain
threshold ? are included in the expanded query (?
was tuned to a value of 5.0 on the development
set).
4 Relatedness to Query
Once the expanded query is generated, it can be
used for sentence ranking. We chose the system of
Otterbacher et al (2005) as a a starting point for
our approach and also as a competitive baseline
because it has been successfully tested in a simi-
lar setting?it has been applied to multi-document
query-focused summarization of news documents.
Given a graph G = (S,E), where S is the set
of all sentences from all input documents, and E is
the set of edges representing normalized sentence
similarities, Otterbacher et al (2005) rank all sen-
248
AAPL DAL DVA
apple air dialysis
music flight davita
mac delta esrd
software lines kidney
ipod schedule inpatient
computer destination outpatient
peripheral passenger patient
movie cargo hospital
player atlanta disease
desktop fleet service
Table 1: Top 10 scoring words for three companies
tence nodes based on the inter-sentence relations
as well as the relevance to the query q. Sentence
ranks are found iteratively over the set of graph
nodes with the following formula:
r(s, q) = ?
rel(s|q)
P
t?S rel(t|q)
+(1??)
X
t?S
sim(s, t)
P
v?S sim(v, t)
r(t, q) (2)
The first term represents the importance of a sen-
tence defined in respect to the query, whereas the
second term infers the importance of the sentence
from its relation to other sentences in the collec-
tion. ? ? (0, 1) determines the relative importance
of the two terms and is found empirically. Another
parameter whose value is determined experimen-
tally is the sentence similarity threshold ? , which
determines the inclusion of a sentence in G. Ot-
terbacher et al (2005) report 0.2 and 0.95 to be
the optimal values for ? and ? respectively. These
values turned out to produce the best results also
on our development set and were used in all our
experiments. Similarity between sentences is de-
fined as the cosine of their vector representations:
sim(s, t) =
P
w?s?t weight(w)
2
q
P
w?s weight(w)2 ?
q
P
w?t weight(w)2
(3)
weight(w) = tfw,sidfw,S (4)
idfw,S = log
( |S| + 1
0.5 + sfw
)
(5)
where tfw,s is the frequency of w in sentence s,
|S| is the total number of sentences in the docu-
ments from which sentences are to be extracted,
and sfw is the number of sentences which contain
the word w (all words in the documents as well
as in the query are stemmed and stopwords are re-
moved from them). Relevance to the query is de-
fined in Equation (6) which has been previously
used for sentence retrieval (Allan et al, 2003):
rel(s|q) =
X
w?q
log(tfw,s + 1) ? log(tfw,q + 1) ? idfw,S (6)
where tfw,x stands for the number of times w ap-
pears in x, be it a sentence (s) or the query (q). If
a sentence shares no words other than stopwords
with the query, the relevance becomes zero. Note
that without the relevance to the query part Equa-
tion 2 takes only inter-sentence similarity into ac-
count and computes the weighted PageRank (Brin
& Page, 1998).
In defining the relevance to the query, in Equa-
tion (6), words which do not appear in too many
sentences in the document collection weigh more.
Indeed, if a word from the query is contained in
many sentences, it should not count much. But it
is also true that not all words from the query are
equally important. As it has been mentioned in
Section 3, words like product or offer appear in
many business summaries and are equally related
to any company. To penalize such words, when
computing the relevance to the query, we multiply
the relevance score of a given word w with the in-
verted document frequency of w on the corpus of
business summaries Q ? idfw,Q:
idfw,Q = log
( |Q|
qfw
)
(7)
We also replace tfw,s with the indicator function
s(w) since it has been reported to be more ad-
equate for sentences, in particular for sentence
alignment (Nelken & Shieber, 2006):
s(w) =
{
1 if s contains w
0 otherwise
(8)
Thus, the modified formula we use to compute
sentence ranks is as follows:
rel(s|q) =
X
w?q
s(w) ? log(tfw,q + 1) ? idfw,S ? idfw,Q (9)
We call these two ranking algorithms that use
the formula in (2) OTTERBACHER and QUERY
WEIGHTS, the difference being the way the rel-
evance to the query is computed: (6) or (9). We
use the OTTERBACHER algorithm as a baseline in
the experiments reported in Section 6.
249
5 Novelty Bias
Apart from being related to the query, a good sum-
mary should provide the user with novel infor-
mation. According to Equation (2), if there are,
say, two sentences which are highly similar to the
query and which share some words, they are likely
to get a very high score. Experimenting with the
development set, we observed that sentences about
the company, such as e.g., DaVita, Inc. is a lead-
ing provider of kidney care in the United States,
providing dialysis services and education for pa-
tients with chronic kidney failure and end stage re-
nal disease, are ranked high although they do not
contribute new information. However, a non-zero
similarity to the query is indeed a good filter of the
information related to the company and to its sec-
tor and can be used as a prerequisite of a sentence
to be included in the summary. These observations
motivate our proposal for a ranking method which
aims at providing relevant and novel information
at the same time.
Here, we explore two alternative approaches to
add the novelty bias to the system:
? The first approach bypasses the relatedness
to query step introduced in Section 4 com-
pletely. Instead, this method merges the dis-
covery of query relatedness and novelty into
a single algorithm, which uses a sentence
graph that contains edges only between sen-
tences related to the query, (i.e., sentences for
which rel(s|q) > 0). All edges connecting
sentences which are unrelated to the query
are skipped in this graph. In this way we limit
the novelty ranking process to a subset of sen-
tences related to the query.
? The second approach models the problem
in a re-ranking architecture: we take the
top ranked sentences after the relatedness-to-
query filtering component (Section 4) and re-
rank them using the novelty formula intro-
duced below.
The main difference between the two approaches
is that the former uses relatedness-to-query and
novelty information but ignores the overall impor-
tance of a sentence as given by the PageRank al-
gorithm in Section 4, while the latter combines all
these aspects ?i.e., importance of sentences, relat-
edness to query, and novelty? using the re-ranking
architecture.
To amend the problem of general information
ranked inappropriately high, we modify the word-
weighting formula (4) so that it implements a nov-
elty bias, thus becoming dependent on the query.
A straightforward way to define the novelty weight
of a word would be to draw a line between the
?known? words, i.e., words appearing in the busi-
ness summary, and the rest. In this approach all
the words from the business summary are equally
related to the company and get the weight of 0:
weight(w) =
{
0 if Q contains w
tfw,sidfw,S otherwise
(10)
We call this weighting scheme SIMPLE. As
an alternative, we also introduce a more elab-
orate weighting procedure which incorporates
the relatedness-to-query (or rather distance from
query) in the word weight formula. Intuitively, the
more related to the query a word is (e.g., DaVita,
the name of the company), the more familiar to the
user it is and the smaller its novelty contribution
is. If a word does not appear in the query at all, its
weight becomes equal to the usual tfw,sidfw,S :
weight(w) =
 
1 ? tfw,q ? idfw,QP
wi?q
tfwi,q ? idfwi,Q
!
? tfw,sidfw,S (11)
The overall novelty ranking formula is based
on the query-dependent PageRank introduced in
Equation (2). However, since we already incorpo-
rate the relatedness to the query in these two set-
tings, we focus only on related sentences and thus
may drop the relatedness to the query part from
(2):
r?(s, q) = ? + (1 ? ?)
?
t?S
sim(s, t, q)
?
u?S sim(t, u, q)
(12)
We set ? to the same value as in OTTERBACHER.
We deliberately set the sentence similarity thresh-
old ? to a very low value (0.05) to prevent the
graph from becoming exceedingly bushy. Note
that this novelty-ranking formula can be equally
applied in both scenarios introduced at the begin-
ning of this section. In the first scenario, S stands
for the set of nodes in the graph that contains only
sentences related to the query. In the second sce-
nario, S contains the highest ranking sentences
detected by the relatedness-to-query component
(Section 4).
250
5.1 Redundancy Filter
Some sentences are repeated several times in the
collection. Such repetitions, which should be
avoided in the summary, can be filtered out ei-
ther before or after the sentence ranking. We ap-
ply a simple repetition check when incrementally
adding ranked sentences to the summary. If a sen-
tence to be added is almost identical to the one
already included in the summary, we skip it. Iden-
tity check is done by counting the percentage of
non-stop word lemmas in common between two
sentences. 95% is taken as the threshold.
We do not filter repetitions before the rank-
ing has taken place because often such repetitions
carry important and relevant information. The re-
dundancy filter is applied to all the systems de-
scribed as they are equally prone to include repe-
titions.
6 Evaluation
We randomly selected 23 company stock names,
and constructed a document collection for each
containing all the news provided in the Yahoo! Fi-
nance news feed for that company in a period of
two days (the time period was chosen randomly).
The average length of a news collection is about
600 tokens. When selecting the company names,
we took care of not picking those which have only
a few news articles for that period of time. This
resulted into 9.4 news articles per collection on av-
erage. From each of these, three human annotators
independently selected up to ten sentences. All an-
notators had average to good understanding of the
financial domain. The annotators were asked to
choose the sentences which could best help them
decide whether to buy, sell or retain stock for the
company the following day and present them in
the order of decreasing importance. The anno-
tators compared their summaries of the first four
collections and clarified the procedure before pro-
ceeding with the other ones. These four collec-
tions were then later used as a development set.
All summaries ? manually as well as automat-
ically generated ? were cut to the first 250 words
which made the summaries 10 words shorter on
average. We evaluated the performance automat-
ically in terms of ROUGE-2 (Lin & Hovy, 2003)
using the parameters and following the methodol-
ogy from the DUC events. The results are pre-
sented in Table 2. We also report the 95% confi-
dence intervals in brackets. As in DUC, we used
METHOD ROUGE-2
Otterbacher 0.255 (0.226 - 0.285)
Query Weights 0.289 (0.254 - 0.324)
Novelty Bias (simple) 0.315 (0.287 - 0.342)
Novelty Bias 0.302 (0.277 - 0.329)
Manual 0.472 (0.415 - 0.531)
Table 2: Results of the four extraction methods
and human annotators
jackknife for each (query, summary) pair and com-
puted a macro-average to make human and au-
tomatic results comparable (Dang, 2005). The
scores computed on summaries produced by hu-
mans are given in the bottom line (MANUAL) and
serve as upper bound and also as an indicator for
the inter-annotator agreement.
6.1 Discussion
From Table 2 follows that the modifications we
applied to the baseline are sensible and indeed
bring an improvement. QUERY WEIGHTS per-
forms better than OTTERBACHER and is in turn
outperformed by the algorithms biased to novel in-
formation (the two NOVELTY systems). The over-
lap between the confidence intervals of the base-
line and the simple version of the novelty algo-
rithm is minimal (0.002).
It is remarkable that the achieved improvement
is due to a more balanced relatedness to the query
ranking (9), as well as to the novelty bias re-
ranking. The fact that the simpler novelty weight-
ing formula (10) produced better results than the
more elaborated one (11) requires a deeper anal-
ysis and a larger test set to explain the difference.
Our conjecture so far is that the SIMPLE approach
allows for a better combination of both novelty
and relatedness to query. Since the more complex
novelty ranking formula penalizes terms related
to the query (Equation (11)), it favors a scenario
where novelty is boosted in detriment of related-
ness to query, which is not always realistic.
It is important to note that, compared with the
baseline, we did not do any parameter tuning for
? and the inter-sentence similarity threshold. The
improvement between the system of Otterbacher
et al (2005) and our best model is statistically
significant.
251
6.2 System Combination
Recall from Section 5 that the motivation for pro-
moting novel information came from the fact that
sentences with background information about the
company obtained very high scores: they were re-
lated but not novel. The sentences ranked by OT-
TERBACHER or QUERY WEIGHTS required a re-
ranking to include related and novel sentences in
the summary. We checked whether novelty re-
ranking brings an improvement if added on top
of a system which does not have a novelty bias
(baseline or QUERY WEIGHTS) and compared it
with the setting where we simply limit the novelty
ranking to all the sentences related to the query
(NOVELTY SIMPLE and NOVELTY). In the simi-
larity graph, we left only edges between the first
30 sentences from the ranked list produced by
one of the two algorithms described in Section 4
(OTTERBACHER or QUERY WEIGHTS). Then we
ranked the sentences biased to novel information
the same way as described in Section 5. The re-
sults are presented in Table 3. What we evalu-
ate here is whether a combination of two methods
performs better than the simple heuristics of dis-
carding edges between sentences unrelated to the
query.
METHOD ROUGE-2
Otterbacher + Novelty simple 0.280 (0.254 - 0.306)
Otterbacher + Novelty 0.273 (0.245 - 0.301)
Query Weights + Novelty simple 0.275 (0.247 - 0.302)
Query Weights + Novelty 0.265 (0.242 - 0.289)
Table 3: Results of the combinations of the four
methods
From the four possible combinations, there is
an improvement over the baseline only (0.255 vs.
0.280 resp. 0.273). None of the combinations per-
forms better than the simple novelty bias algo-
rithm on a subset of edges. This experiment sug-
gests that, at least in the scenario investigated here
(short-term monitoring of publicly-traded compa-
nies), novelty is more important than relatedness
to query. Hence, the simple novelty bias algo-
rithm, which emphasizes novelty and incorporates
relatedness to query only through a loose con-
straint (rel(s|q) > 0) performs better than com-
plex models, which are more constrained by the
relatedness to query.
7 Related Work
Summarization has been extensively investigated
in recent years and to date there exists a multi-
tude of very different systems. Here, we review
those that come closest to ours in respect to the
task and that concern extractive multi-document
query-oriented summarization. We also mention
some work on using textual news data for stock
indices prediction which we are aware of.
Stock market prediction: Wu?thrich et al
(1998) were among the first who introduced an au-
tomatic stock indices prediction system which re-
lies on textual information only. The system gen-
erates weighted rules each of which returns the
probability of the stock going up, down or remain-
ing steady. The only information used in the rules
is the presence or absence of certain keyphrases
provided by a human expert who ?judged them
to be influential factors potentially moving stock
markets?. In this approach, training data is re-
quired to measure the usefulness of the keyphrases
for each of the three classes. More recently, Ler-
man et al (2008) introduced a forecasting system
for prediction markets that combines news anal-
ysis with a price trend analysis model. This ap-
proach was shown to be successful for the fore-
casting of public opinion about political candi-
dates in such prediction markets. Our approach
can be seen as a complement to both these ap-
proaches, necessary especially for financial mar-
kets where the news typically cover many events,
only some related to the company of interest.
Unsupervized summarization systems extract
sentences whose relevance can be inferred from
the inter-sentence relations in the document col-
lection. In (Radev et al, 2000), the centroid of
the collection, i.e., the words with the highest
TF*IDF, is considered and the sentences which
contain more words from the centroid are ex-
tracted. Mihalcea & Tarau (2004) explore sev-
eral methods developed for ranking documents
in information retrieval for the single-document
summarization task. Similarly, Erkan & Radev
(2004) apply in-degree and PageRank to build a
summary from a collection of related documents.
They show that their method, called LexRank,
achieves good results. In (Otterbacher et al, 2005;
Erkan, 2006) the ranking function of LexRank is
extended to become applicable to query-focused
summarization. The rank of a sentence is deter-
mined not just by its relation to other sentences in
252
the document collection but also by its relevance
to the query. Relevance to the query is defined as
the word-based similarity between query and sen-
tence.
Query expansion has been used for improv-
ing information retrieval (IR) or question answer-
ing (QA) systems with mixed results. One of the
problems is that the queries are expanded word
by word, ignoring the context and as a result the
extensions often become inadequate7. However,
Riezler et al (2007) take the entire query into ac-
count when adding new words by utilizing tech-
niques used in statistical machine translation.
Query expansion for summarization has not yet
been explored as extensively as in IR or QA.
Nastase (2008) uses Wikipedia and WordNet for
query expansion and proposes that a concept can
be expanded by adding the text of all hyper-
links from the first paragraph of the Wikipedia
article about this concept. The automatic eval-
uation demonstrates that extracting relevant con-
cepts from Wikipedia leads to better performance
compared with WordNet: both expansion systems
outperform the no-expansion version in terms of
the ROUGE score. Although this method proved
helpful on the DUC data, it seems less appropriate
for expanding company names. For small compa-
nies there are short articles with only a few links;
the first paragraphs of the articles about larger
companies often include interesting rather than
relevant information. For example, the text pre-
ceding the contents box in the article about Apple
Inc. (AAPL) states that ?Fortune magazine named
Apple the most admired company in the United
States?8. The link to the article about the For-
tune magazine can be hardly considered relevant
for the expansion of AAPL. Wikipedia category
information, which has been successfully used in
some NLP tasks (Ponzetto & Strube, 2006, inter
alia), is too general and does not help discriminate
between two companies from the same sector.
Our work suggests that query expansion is
needed for summarization in the financial domain.
In addition to previous work, we also show that an-
other key factor for success in this task is detecting
and modeling the novelty of the target content.
7E.g., see the proceedings of TREC 9, TREC 10: http:
//trec.nist.gov.
8Checked on September 17, 2008.
8 Conclusions
In this paper we presented a multi-document
company-oriented summarization algorithm
which extracts sentences that are both relevant for
the given organization and novel to the user. The
system is expected to be useful in the context of
stock market monitoring and forecasting, that is,
to help the trader predict the move of the stock
price for the given company. We presented a
novel query expansion method which works par-
ticularly well in the context of company-oriented
summarization. Our sentence ranking method is
unsupervized and requires little parameter tuning.
An automatic evaluation against a competitive
baseline showed supportive results, indicating that
the ranking algorithm is able to select relevant
sentences and promote novel information at the
same time.
In the future, we plan to experiment with po-
sitional features which have proven useful for
generic summarization. We also plan to test the
system extrinsically. For example, it would be of
interest to see if a classifier may predict the move
of stock prices based on a set of features extracted
from company-oriented summaries.
Acknowledgments: We would like to thank the
anonymous reviewers for their helpful feedback.
References
Allan, James, Courtney Wade & Alvaro Bolivar
(2003). Retrieval and novelty detection at the
sentence level. In Proceedings of the 26th An-
nual International ACM SIGIR Conference on
Research and Development in Information Re-
trieval Toronto, On., Canada, 28 July ? 1 Au-
gust 2003, pp. 314?321.
Atserias, Jordi, Hugo Zaragoza, Massimiliano
Ciaramita & Giuseppe Attardi (2008). Se-
mantically annotated snapshot of the English
Wikipedia. In Proceedings of the 6th Interna-
tional Conference on Language Resources and
Evaluation, Marrakech, Morocco, 26 May ? 1
June 2008.
Brin, Sergey & Lawrence Page (1998). The
anatomy of a large-scale hypertextual web
search engine. Computer Networks and ISDN
Systems, 30(1?7):107?117.
Ciaramita, Massimiliano & Yasemin Altun
(2006). Broad-coverage sense disambiguation
253
and information extraction with a supersense
sequence tagger. In Proceedings of the 2006
Conference on Empirical Methods in Natural
Language Processing, Sydney, Australia,
22?23 July 2006, pp. 594?602.
Dang, Hoa Trang (2005). Overview of DUC
2005. In Proceedings of the 2005 Document
Understanding Conference held at the Human
Language Technology Conference and Confer-
ence on Empirical Methods in Natural Lan-
guage Processing, Vancouver, B.C., Canada, 9?
10 October 2005.
Erkan, Gu?nes? (2006). Using biased random walks
for focused summarization. In Proceedings
of the 2006 Document Understanding Confer-
ence held at the Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics,,
New York, N.Y., 8?9 June 2006.
Erkan, Gu?nes? & Dragomir R. Radev (2004).
LexRank: Graph-based lexical centrality as
salience in text summarization. Journal of Arti-
ficial Intelligence Research, 22:457?479.
Lerman, Kevin, Ari Gilder, Mark Dredze & Fer-
nando Pereira (2008). Reading the markets:
Forecasting public opinion of political candi-
dates by news analysis. In Proceedings of
the 22st International Conference on Computa-
tional Linguistics, Manchester, UK, 18?22 Au-
gust 2008, pp. 473?480.
Lin, Chin-Yew & Eduard H. Hovy (2003). Au-
tomatic evaluation of summaries using N-gram
co-occurrence statistics. In Proceedings of the
Human Language Technology Conference of the
North American Chapter of the Association for
Computational Linguistics, Edmonton, Alberta,
Canada, 27 May ?1 June 2003, pp. 150?157.
Mihalcea, Rada & Paul Tarau (2004). Textrank:
Bringing order into texts. In Proceedings of the
2004 Conference on Empirical Methods in Nat-
ural Language Processing, Barcelona, Spain,
25?26 July 2004, pp. 404?411.
Nastase, Vivi (2008). Topic-driven multi-
document summarization with encyclopedic
knowledge and activation spreading. In Pro-
ceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, Hon-
olulu, Hawaii, 25?27 October 2008. To appear.
Nelken, Rani & Stuart M. Shieber (2006). To-
wards robust context-sensitive sentence align-
ment for monolingual corpora. In Proceedings
of the 11th Conference of the European Chapter
of the Association for Computational Linguis-
tics, Trento, Italy, 3?7 April 2006, pp. 161?168.
Otterbacher, Jahna, Gu?nes? Erkan & Dragomir
Radev (2005). Using random walks for
question-focused sentence retrieval. In Pro-
ceedings of the Human Language Technology
Conference and the 2005 Conference on Empir-
ical Methods in Natural Language Processing,
Vancouver, B.C., Canada, 6?8 October 2005,
pp. 915?922.
Ponzetto, Simone Paolo & Michael Strube (2006).
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Pro-
ceedings of the Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics,
New York, N.Y., 4?9 June 2006, pp. 192?199.
Radev, Dragomir R., Hongyan Jing & Malgorzata
Budzikowska (2000). Centroid-based summa-
rization of mutliple documents: Sentence ex-
traction, utility-based evaluation, and user stud-
ies. In Proceedings of the Workshop on Au-
tomatic Summarization at ANLP/NAACL 2000,
Seattle, Wash., 30 April 2000, pp. 21?30.
Riezler, Stefan, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal & Yi Liu (2007).
Statistical machine translation for query expan-
sion in answer retrieval. In Proceedings of
the 45th Annual Meeting of the Association for
Computational Linguistics, Prague, Czech Re-
public, 23?30 June 2007, pp. 464?471.
Wu?thrich, B, D. Permunetilleke, S. Leung, V. Cho,
J. Zhang & W. Lam (1998). Daily prediction of
major stock indices from textual WWW data. In
In Proceedings of the 4th International Confer-
ence on Knowledge Discovery and Data Mining
- KDD-98, pp. 364?368.
254
Proceedings of ACL-08: HLT, pages 719?727,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Learning to Rank Answers on Large Online QA Collections
Mihai Surdeanu, Massimiliano Ciaramita, Hugo Zaragoza
Barcelona Media Innovation Center, Yahoo! Research Barcelona
mihai.surdeanu@barcelonamedia.org, {massi,hugo}@yahoo-inc.com
Abstract
This work describes an answer ranking engine
for non-factoid questions built using a large
online community-generated question-answer
collection (Yahoo! Answers). We show how
such collections may be used to effectively
set up large supervised learning experiments.
Furthermore we investigate a wide range of
feature types, some exploiting NLP proces-
sors, and demonstrate that using them in com-
bination leads to considerable improvements
in accuracy.
1 Introduction
The problem of Question Answering (QA) has re-
ceived considerable attention in the past few years.
Nevertheless, most of the work has focused on the
task of factoid QA, where questions match short an-
swers, usually in the form of named or numerical en-
tities. Thanks to international evaluations organized
by conferences such as the Text REtrieval Confer-
ence (TREC)1 or the Cross Language Evaluation Fo-
rum (CLEF) Workshop2, annotated corpora of ques-
tions and answers have become available for several
languages, which has facilitated the development of
robust machine learning models for the task.
The situation is different once one moves beyond
the task of factoid QA. Comparatively little research
has focused on QA models for non-factoid ques-
tions such as causation, manner, or reason questions.
Because virtually no training data is available for
this problem, most automated systems train either
1http://trec.nist.gov
2http://www.clef-campaign.org
Q: How do you quiet a squeaky door?
A: Spray WD-40 directly onto the hinges
of the door. Open and close the door
several times. Remove hinges if the
door still squeaks. Remove any rust,
dirt or loose paint. Apply WD-40 to
High removed hinges. Put the hinges back,
Quality open and close door several times again.
Q: How to extract html tags from an html
Low documents with c++?
Quality A: very carefully
Table 1: Sample content from Yahoo! Answers.
on small hand-annotated corpora built in house (Hi-
gashinaka and Isozaki, 2008) or on question-answer
pairs harvested from Frequently Asked Questions
(FAQ) lists or similar resources (Soricut and Brill,
2006). None of these situations is ideal: the cost
of building the training corpus in the former setup
is high; in the latter scenario the data tends to be
domain-specific, hence unsuitable for the learning of
open-domain models.
On the other hand, recent years have seen an ex-
plosion of user-generated content (or social media).
Of particular interest in our context are community-
driven question-answering sites, such as Yahoo! An-
swers3, where users answer questions posed by other
users and best answers are selected manually either
by the asker or by all the participants in the thread.
The data generated by these sites has significant ad-
vantages over other web resources: (a) it has a high
growth rate and it is already abundant; (b) it cov-
ers a large number of topics, hence it offers a better
3http://answers.yahoo.com
719
approximation of open-domain content; and (c) it is
available for many languages. Community QA sites,
similar to FAQs, provide large number of question-
answer pairs. Nevertheless, this data has a signifi-
cant drawback: it has high variance of quality, i.e.,
answers range from very informative to completely
irrelevant or even abusive. Table 1 shows some ex-
amples of both high and low quality content.
In this paper we address the problem of answer
ranking for non-factoid questions from social media
content. Our research objectives focus on answering
the following two questions:
1. Is it possible to learn an answer ranking model
for complex questions from such noisy data?
This is an interesting question because a posi-
tive answer indicates that a plethora of training
data is readily available to QA researchers and
system developers.
2. Which features are most useful in this sce-
nario? Are similarity models as effective as
models that learn question-to-answer transfor-
mations? Does syntactic and semantic infor-
mation help? For generality, we focus only on
textual features extracted from the answer text
and we ignore all meta data information that is
not generally available.
Notice that we concentrate on one component of a
possible social-media QA system. In addition to
answer ranking, a complete system would have to
search for similar questions already answered (Jeon
et al, 2005), and rank content quality using ?social?
features such as the authority of users (Jeon et al,
2006; Agichtein et al, 2008). This is not the focus of
our work: here we investigate the problem of learn-
ing an answer ranking model capable of dealing with
complex questions, using a large number of, possi-
ble noisy, question-answer pairs. By focusing exclu-
sively on textual content we increase the portability
of our approach to other collections where ?social?
features might not available, e.g., Web search.
The paper is organized as follows. We describe
our approach, including all the features explored for
answer modeling, in Section 2. We introduce the
corpus used in our empirical analysis in Section 3.
We detail our experiments and analyze the results in
Section 4. We overview related work in Section 5
and conclude the paper in Section 6.
AnswerCollection
Answers
Translation
Features
Web Correlation
FeaturesFeatures
Similarity
Answer
RankingQ AnswerRetrieval
(unsupervised)
(discriminative learning)
(class?conditional learning)
Features
Density/Frequency
Figure 1: System architecture.
2 Approach
The architecture of the QA system analyzed in the
paper, summarized in Figure 1, follows that of the
most successful TREC systems. The first com-
ponent, answer retrieval, extracts a set of candi-
date answers A for question Q from a large col-
lection of answers, C, provided by a community-
generated question-answering site. The retrieval
component uses a state-of-the-art information re-
trieval (IR) model to extract A given Q. Since
our focus is on exploring the usability of the an-
swer content, we do not perform retrieval by find-
ing similar questions already answered (Jeon et al,
2005), i.e., our answer collection C contains only
the site?s answers without the corresponding ques-
tions answered.
The second component, answer ranking, assigns
to each answer Ai ? A a score that represents
the likelihood that Ai is a correct answer for Q,
and ranks all answers in descending order of these
scores. The scoring function is a linear combina-
tion of four different classes of features (detailed in
Section 2.2). This function is the focus of the pa-
per. To answer our first research objective we will
compare the quality of the rankings provided by this
component against the rankings generated by the IR
model used for answer retrieval. To answer the sec-
ond research objective we will analyze the contri-
bution of the proposed feature set to this function.
Again, since our interest is in investigating the util-
ity of the answer textual content, we use only infor-
mation extracted from the answer text when learn-
ing the scoring function. We do not use any meta
information (e.g., answerer credibility, click counts,
etc.) (Agichtein et al, 2008; Jeon et al, 2006).
Our QA approach combines three types of ma-
chine learning methodologies (as highlighted in Fig-
ure 1): the answer retrieval component uses un-
720
supervised IR models, the answer ranking is im-
plemented using discriminative learning, and fi-
nally, some of the ranking features are produced
by question-to-answer translation models, which use
class-conditional learning.
2.1 Ranking Model
Learning with user-generated content can involve
arbitrarily large amounts of data. For this reason
we choose as a ranking algorithm the Perceptron
which is both accurate and efficient and can be
trained with online protocols. Specifically, we im-
plement the ranking Perceptron proposed by Shen
and Joshi (2005), which reduces the ranking prob-
lem to a binary classification problem. The general
intuition is to exploit the pairwise preferences in-
duced from the data by training on pairs of patterns,
rather than independently on each pattern. Given a
weight vector ?, the score for a pattern x (a candi-
date answer) is simply the inner product between the
pattern and the weight vector:
f?(x) = ?x, ?? (1)
However, the error function depends on pairwise
scores. In training, for each pair (xi,xj) ? A,
the score f?(xi ? xj) is computed; note that if f
is an inner product f?(xi?xj) = f?(xi)? f?(xj).
Given a margin function g(i, j) and a positive rate ? ,
if f?(xi ? xj) ? g(i, j)? , an update is performed:
?t+1 = ?t + (xi ? xj)?g(i, j) (2)
By default we use g(i, j) = (1i ? 1j ), as a mar-
gin function, as suggested in (Shen and Joshi, 2005),
and find ? empirically on development data. Given
that there are only two possible ranks in our set-
ting, this function only generates two possible val-
ues. For regularization purposes, we use as a final
model the average of all Perceptron models posited
during training (Freund and Schapire, 1999).
2.2 Features
In the scoring model we explore a rich set of features
inspired by several state-of-the-art QA systems. We
investigate how such features can be adapted and
combined for non-factoid answer ranking, and per-
form a comparative feature analysis using a signif-
icant amount of real-world data. For clarity, we
group the features into four sets: features that model
the similarity between questions and answers (FG1),
features that encode question-to-answer transfor-
mations using a translation model (FG2), features
that measure keyword density and frequency (FG3),
and features that measure the correlation between
question-answer pairs and other collections (FG4).
Wherever applicable, we explore different syntactic
and semantic representations of the textual content,
e.g., extracting the dependency-based representation
of the text or generalizing words to their WordNet
supersenses (WNSS) (Ciaramita and Altun, 2006).
We detail each of these feature groups next.
FG1: Similarity Features
We measure the similarity between a question
Q and an answer A using the length-normalized
BM25 formula (Robertson and Walker, 1997). We
chose this similarity formula because, out of all the
IR models we tried, it provided the best ranking at
the output of the answer retrieval component. For
completeness we also include in the feature set the
value of the tf ?idf similarity measure. For both for-
mulas we use the implementations available in the
Terrier IR platform4 with the default parameters.
To understand the contribution of our syntactic
and semantic processors we compute the above sim-
ilarity features for five different representations of
the question and answer content:
Words (W) - this is the traditional IR view where the
text is seen as a bag of words.
Dependencies (D) - the text is represented as a bag
of binary syntactic dependencies. The relative syn-
tactic processor is detailed in Section 3. Dependen-
cies are fully lexicalized but unlabeled and we cur-
rently extract dependency paths of length 1, i.e., di-
rect head-modifier relations (this setup achieved the
best performance).
Generalized dependencies (Dg) - same as above, but
the words in dependencies are generalized to their
WNSS, if detected.
Bigrams (B) - the text is represented as a bag of bi-
grams (larger n-grams did not help). We added this
view for a fair analysis of the above syntactic views.
Generalized bigrams (Bg) - same as above, but the
words are generalized to their WNSS.
4http://ir.dcs.gla.ac.uk/terrier
721
In all these representations we skip stop words
and normalize all words to their WordNet lemmas.
FG2: Translation Features
Berger et al (2000) showed that similarity-based
models are doomed to perform poorly for QA be-
cause they fail to ?bridge the lexical chasm? be-
tween questions and answers. One way to address
this problem is to learn question-to-answer trans-
formations using a translation model (Berger et al,
2000; Echihabi and Marcu, 2003; Soricut and Brill,
2006; Riezler et al, 2007). In our model, we in-
corporate this approach by adding the probability
that the question Q is a translation of the answer A,
P (Q|A), as a feature. This probability is computed
using IBM?s Model 1 (Brown et al, 1993):
P (Q|A) =
?
q?Q
P (q|A) (3)
P (q|A) = (1? ?)Pml(q|A) + ?Pml(q|C) (4)
Pml(q|A) =
?
a?A
(T (q|a)Pml(a|A)) (5)
where the probability that the question term q is
generated from answer A, P (q|A), is smoothed us-
ing the prior probability that the term q is gen-
erated from the entire collection of answers C,
Pml(q|C). ? is the smoothing parameter. Pml(q|C)
is computed using the maximum likelihood estima-
tor. Pml(q|A) is computed as the sum of the proba-
bilities that the question term q is a translation of an
answer term a, T (q|a), weighted by the probability
that a is generated from A. The translation table for
T (q|a) is computed using the EM-based algorithm
implemented in the GIZA++ toolkit5.
Similarly with the previous feature group, we
add translation-based features for the five differ-
ent text representations introduced above. By
moving beyond the bag-of-word representation we
hope to learn relevant transformations of structures,
e.g., from the ?squeaky? ? ?door? dependency to
?spray? ? ?WD-40? in the Table 1 example.
FG3: Density and Frequency Features
These features measure the density and frequency
of question terms in the answer text. Variants of
these features were used previously for either an-
swer or passage ranking in factoid QA (Moldovan
et al, 1999; Harabagiu et al, 2000).
5http://www.fjoch.com/GIZA++.html
Same word sequence - computes the number of non-
stop question words that are recognized in the same
order in the answer.
Answer span - the largest distance (in words) be-
tween two non-stop question words in the answer.
Same sentence match - number of non-stop question
terms matched in a single sentence in the answer.
Overall match - number of non-stop question terms
matched in the complete answer.
These last two features are computed also for the
other four text representations previously introduced
(B, Bg, D, and Dg). Counting the number of
matched dependencies is essentially a simplified
tree kernel for QA (e.g., see (Moschitti et al,
2007)) matching only trees of depth 2. Experiments
with full dependency tree kernels based on several
variants of the convolution kernels of Collins and
Duffy (2001) did not yield improvements. We con-
jecture that the mistakes of the syntactic parser may
be amplified in tree kernels, which consider an ex-
ponential number of sub-trees.
Informativeness - we model the amount of informa-
tion contained in the answer by counting the num-
ber of non-stop nouns, verbs, and adjectives in the
answer text that do not appear in the question.
FG4: Web Correlation Features
Previous work has shown that the redundancy of
a large collection (e.g., the web) can be used for an-
swer validation (Brill et al, 2001; Magnini et al,
2002). In the same spirit, we add features that mea-
sure the correlation between question-answer pairs
and large external collections:
Web correlation - we measure the correlation be-
tween the question-answer pair and the web using
the Corrected Conditional Probability (CCP) for-
mula of Magnini et al (2002): CCP (Q,A) =
hits(Q + A)/(hits(Q) hits(A)2/3) where hits re-
turns the number of page hits from a search engine.
When a query returns zero hits we iteratively relax it
by dropping the keyword with the smallest priority.
Keyword priorities are assigned using the heuristics
of Moldovan et al (1999).
Query-log correlation - as in (Ciaramita et al, 2008)
we also compute the correlation between question-
answer pairs and a search-engine query-log cor-
pus of more than 7.5 million queries, which shares
722
roughly the same time stamp with the community-
generated question-answer corpus. We compute the
Pointwise Mutual Information (PMI) and Chi square
(?2) association measures between each question-
answer word pair in the query-log corpus. The
largest and the average values are included as fea-
tures, as well as the number of QA word pairs which
appear in the top 10, 5, and 1 percentile of the PMI
and ?2 word pair rankings.
3 The Corpus
The corpus is extracted from a sample of the U.S.
Yahoo! Answers logs. In this paper we focus on
the subset of advice or ?how to? questions due to
their frequency and importance in social communi-
ties.6 To construct our corpus, we implemented the
following successive filtering steps:
Step 1: from the full corpus we keep only questions
that match the regular expression:
how (to|do|did|does|can|would|could|should)
and have an answer selected as best either by
the asker or by the participants in the thread.
The outcome of this step is a set of 364,419
question-answer pairs.
Step 2: from the above corpus we remove the questions
and answers of obvious low quality. We im-
plement this filter with a simple heuristic by
keeping only questions and answers that have
at least 4 words each, out of which at least 1 is
a noun and at least 1 is a verb. This step filters
out questions like ?How to be excellent?? and
answers such as ?I don?t know?. The outcome
of this step forms our answer collection C. C
contains 142,627 question-answer pairs.7.
Arguably, all these filters could be improved. For
example, the first step can be replaced by a question
classifier (Li and Roth, 2005). Similarly, the second
step can be implemented with a statistical classifier
that ranks the quality of the content using both the
textual and non-textual information available in the
database (Jeon et al, 2006; Agichtein et al, 2008).
We plan to further investigate these issues which are
not the main object of this work.
6Nevertheless, the approach proposed here is independent
of the question type. We will explore answer ranking for other
non-factoid question types in future work.
7The data will be available through the Yahoo! Webscope
program (research-data-requests@yahoo-inc.com).
The data was processed as follows. The text was
split at the sentence level, tokenized and PoS tagged,
in the style of the Wall Street Journal Penn Tree-
Bank (Marcus et al, 1993). Each word was morpho-
logically simplified using the morphological func-
tions of the WordNet library8. Sentences were an-
notated with WNSS categories, using the tagger of
Ciaramita and Altun (2006)9, which annotates text
with a 46-label tagset. These tags, defined by Word-
Net lexicographers, provide a broad semantic cat-
egorization for nouns and verbs and include labels
for nouns such as food, animal, body and feeling,
and for verbs labels such as communication, con-
tact, and possession. Next, we parsed all sentences
with the dependency parser of Attardi et al (2007)10.
It is important to realize that the output of all men-
tioned processing steps is noisy and contains plenty
of mistakes, since the data has huge variability in
terms of quality, style, genres, domains etc., and do-
main adaptation for the NLP tasks involved is still
an open problem (Dredze et al, 2007).
We used 60% of the questions for training, 20%
for development, and 20% for test. The candidate
answer set for a given question is composed by one
positive example, i.e., its corresponding best answer,
and as negative examples all the other answers re-
trieved in the top N by the retrieval component.
4 Experiments
We evaluate our results using two measures: mean
Precision at rank=1 (P@1) ? i.e., the percentage of
questions with the correct answer on the first posi-
tion ? and Mean Reciprocal Rank (MRR) ? i.e., the
score of a question is 1/k, where k is the position
of the correct answer. We use as baseline the output
of our answer retrieval component (Figure 1). This
component uses the BM25 criterion, the highest per-
forming IR model in our experiments.
Table 2 lists the results obtained using this base-
line and our best model (?Ranking? in the table) on
the testing partition. Since we are interested in the
performance of the ranking model, we evaluate on
the subset of questions where the correct answer is
retrieved by answer retrieval in the top N answers
(similar to Ko et al (2007)). In the table we report
8http://wordnet.princeton.edu
9sourceforge.net/projects/supersensetag
10http://sourceforge.net/projects/desr
723
MRR P@1
N = 10 N = 15 N = 25 N = 50 N = 10 N = 15 N = 25 N = 50
recall@N 26.25% 29.04% 32.81% 38.09% 26.25% 29.04% 32.81% 38.09%
Baseline 61.33 56.12 50.31 43.74 45.94 41.48 36.74 31.66
Ranking 68.72?0.01 63.84?0.01 57.76?0.07 50.72?0.01 54.22?0.01 49.59?0.03 43.98?0.09 37.99?0.01
Improvement +12.04% +13.75% +14.80% +15.95% +18.02% +19.55% +19.70% +19.99%
Table 2: Overall results for the test partition.
results for several N values. For completeness, we
show the percentage of questions that match this cri-
terion in the ?recall@N? row.
Our ranking model was tuned strictly on the de-
velopment set (i.e., feature selection and parame-
ters of the translation models). During training, the
presentation of the training instances is randomized,
which generates a randomized ranking algorithm.
We exploit this property to estimate the variance in
the results produced by each model and report the
average result over 10 trials together with an esti-
mate of the standard deviation.
The baseline result shows that, for N = 15,
BM25 alone can retrieve in first rank 41% of the
correct answers, and MRR tells us that the correct
answer is often found within the first three answers
(this is not so surprising if we remember that in this
configuration only questions with the correct answer
in the first 15 were kept for the experiment). The
baseline results are interesting because they indicate
that the problem is not hopelessly hard, but it is far
from trivial. In principle, we see much room for im-
provement over bag-of-word methods.
Next we see that learning a weighted combina-
tion of features yields consistently marked improve-
ments: for example, for N = 15, the best model
yields a 19% relative improvement in P@1 and 14%
in MRR. More importantly, the results indicate that
the model learned is stable: even though for the
model analyzed in Table 2 we used N = 15 in train-
ing, we measure approximately the same relative im-
provement as N increases during evaluation.
These results provide robust evidence that: (a) we
can use publicly available online QA collections to
investigate features for answer ranking without the
need for costly human evaluation, (b) we can exploit
large and noisy online QA collections to improve the
accuracy of answer ranking systems and (c) readily
available and scalable NLP technology can be used
Iter. Feature Set MRR P@1
0 BM25(W) 56.06 41.12%
1 + translation(Bg) 61.13 46.24%
2 + overall match(D) 62.50 48.34%
3 + translation(W) 63.00 49.08%
4 + query-log avg(?2) 63.50 49.63%
5 + answer span
normalized by A size 63.71 49.84%
6 + query-log max(PMI) 63.87 50.09%
7 + same word sequence 63.99 50.23%
8 + translation(B) 64.03 50.30%
9 + tfidf(W) 64.08 50.42%
10 + same sentence match(W) 64.10 50.42%
11 + informativeness:
verb count 64.18 50.36%
12 + tfidf(B) 64.22 50.36%
13 + same word sequence
normalized by Q size 64.33 50.54%
14 + query-log max(?2) 64.46 50.66%
15 + same sentence match(W)
normalized by Q size 64.55 50.78%
16 + query-log avg(PMI) 64.60 50.88%
17 + overall match(W) 64.65 50.91%
Table 3: Summary of the model selection process.
to improve lexical matching and translation models.
In the remaining of this section we analyze the per-
formance of the different features.
Table 3 summarizes the outcome of our automatic
greedy feature selection process on the development
set. Where applicable, we show within parentheses
the text representation for the corresponding feature.
The process is initialized with a single feature that
replicates the baseline model (BM25 applied to the
bag-of-words (W) representation). The algorithm
incrementally adds to the feature set the feature that
provides the highest MRR improvement in the de-
velopment partition. The process stops when no fea-
tures yield any improvement. The table shows that,
while the features selected span all the four feature
groups introduced, the lion?s share is taken by the
translation features: approximately 60% of the MRR
724
W B Bg D Dg W + W + W + B + W + B + Bg
B B + Bg Bg + D D + Dg
FG1 (Similarity) 0 +1.06 -2.01 +0.84 -1.75 +1.06 +1.06 +1.06 +1.06
FG2 (Translation) +4.95 +4.73 +5.06 +4.63 +4.66 +5.80 +6.01 +6.36 +6.36
FG3 (Frequency) +2.24 +2.33 +2.39 +2.27 +2.41 +3.56 +3.56 +3.62 +3.62
Table 4: Contribution of NLP processors. Scores are MRR improvements on the development set.
improvement is achieved by these features. The fre-
quency/density features are responsible for approx-
imately 23% of the improvement. The rest is due
to the query-log correlation features. This indicates
that, even though translation models are the most
useful, it is worth exploring approaches that com-
bine several strategies for answer ranking.
Note that if some features do not appear in Table 3
it does not necessarily mean that they are useless.
In some cases such features are highly correlated
with features previously selected, which already ex-
ploited their signal. For example, most similarity
features (FG1) are correlated. Because BM25(W)
is part of the baseline model, the selection process
chooses another FG1 feature only much later (iter-
ation 9) when the model is significantly changed.
On the other hand, some features do not provide a
useful signal at all. A notable example in this class
is the web-based CCP feature, which was designed
originally for factoid answer validation and does not
adapt well to our problem. Because the length of
non-factoid answers is typically significantly larger
than in the factoid QA task, we have to discard a
large part of the query when computing hits(Q+A)
to reach non-zero counts. This means that the final
hit counts, hence the CCP value, are generally un-
correlated with the original (Q,A) tuple.
One interesting observation is that the first two
features chosen by our model selection process use
information from the NLP processors. The first cho-
sen feature is the translation probability computed
between the Bg question and answer representations
(bigrams with words generalized to their WNSS
tags). The second feature selected measures the
number of syntactic dependencies from the question
that are matched in the answer. These results pro-
vide empirical evidence that coarse semantic disam-
biguation and syntactic parsing have a positive con-
tribution to non-factoid QA, even in broad-coverage
noisy settings based on Web data.
The above observation deserves a more detailed
analysis. Table 4 shows the performance of our first
three feature groups when they are applied to each
of the five text representations or incremental com-
binations of representations. For each model cor-
responding to a table cell we use only the features
from the corresponding feature group and represen-
tation to avoid the correlation with features from
other groups. We generate each best model using
the same feature selection process described above.
The left part of Table 4 shows that, generally, the
models using representations that include the output
of our NLP processors (Bg, D and Dg) improve over
the baseline (FG1 and W).11 However, comparable
improvements can be obtained with the simpler bi-
gram representation (B). This indicates that, in terms
of individual contributions, our NLP processors can
be approximated with simpler n-gram models in this
task. Hence, is it fair to say that syntactic and se-
mantic analysis is useful for such Web QA tasks?
While the above analysis seems to suggest a neg-
ative answer, the right-hand side of Table 4 tells a
more interesting story. It shows that the NLP anal-
ysis provides complementary information to the n-
gram-based models. The best models for the FG2
and FG3 feature groups are obtained when combin-
ing the n-gram representations with the representa-
tions that use the output of the NLP processors (W +
B + Bg + D). The improvements are relatively small,
but remarkable (e.g., see FG2) if we take into ac-
count the significant scale of the evaluation. This
observation correlates well with the analysis shown
in Table 3, which shows that features using semantic
(Bg) and syntactic (D) representations contribute the
most on top of the IR model (BM25(W)).
11The exception to this rule are the models FG1(Bg) and
FG1(Dg). This is caused by the fact that the BM25 formula
is less forgiving with errors of the NLP processors (due to the
high idf scores assigned to bigrams and dependencies), and the
WNSS tagger is the least robust component in our pipeline.
725
5 Related Work
Content from community-built question-answer
sites can be retrieved by searching for similar ques-
tions already answered (Jeon et al, 2005) and
ranked using meta-data information like answerer
authority (Jeon et al, 2006; Agichtein et al, 2008).
Here we show that the answer text can be success-
fully used to improve answer ranking quality. Our
method is complementary to the above approaches.
In fact, it is likely that an optimal retrieval engine
from social media should combine all these three
methodologies. Moreover, our approach might have
applications outside of social media (e.g., for open-
domain web-based QA), because the ranking model
built is based only on open-domain knowledge and
the analysis of textual content.
In the QA literature, answer ranking for non-
factoid questions has typically been performed by
learning question-to-answer transformations, either
using translation models (Berger et al, 2000; Sori-
cut and Brill, 2006) or by exploiting the redundancy
of the Web (Agichtein et al, 2001). Girju (2003) ex-
tracts non-factoid answers by searching for certain
semantic structures, e.g., causation relations as an-
swers to causation questions. In this paper we com-
bine several methodologies, including the above,
into a single model. This approach allowed us to per-
form a systematic feature analysis on a large-scale
real-world corpus and a comprehensive feature set.
Recent work has showed that structured retrieval
improves answer ranking for factoid questions:
Bilotti et al (2007) showed that matching predicate-
argument frames constructed from the question and
the expected answer types improves answer ranking.
Cui et al (2005) learned transformations of depen-
dency paths from questions to answers to improve
passage ranking. However, both approaches use
similarity models at their core because they require
the matching of the lexical elements in the search
structures. On the other hand, our approach al-
lows the learning of full transformations from ques-
tion structures to answer structures using translation
models applied to different text representations.
Our answer ranking framework is closest in spirit
to the system of Ko et al (2007) or Higashinaka et
al. (2008). However, the former was applied only
to factoid QA and both are limited to similarity, re-
dundancy and gazetteer-based features. Our model
uses a larger feature set that includes correlation and
transformation-based features and five different con-
tent representations. Our evaluation is also carried
out on a larger scale. Our work is also related to that
of Riezler et al (2007) where SMT-based query ex-
pansion methods are used on data from FAQ pages.
6 Conclusions
In this work we described an answer ranking en-
gine for non-factoid questions built using a large
community-generated question-answer collection.
On one hand, this study shows that we can effec-
tively exploit large amounts of available Web data
to do research on NLP for non-factoid QA systems,
without any annotation or evaluation cost. This pro-
vides an excellent framework for large-scale experi-
mentation with various models that otherwise might
be hard to understand or evaluate. On the other hand,
we expect the outcome of this process to help sev-
eral applications, such as open-domain QA on the
Web and retrieval from social media. For example,
on the Web our ranking system could be combined
with a passage retrieval system to form a QA system
for complex questions. On social media, our system
should be combined with a component that searches
for similar questions already answered; this output
can possibly be filtered further by a content-quality
module that explores ?social? features such as the
authority of users, etc.
We show that the best ranking performance
is obtained when several strategies are combined
into a single model. We obtain the best results
when similarity models are aggregated with features
that model question-to-answer transformations, fre-
quency and density of content, and correlation of
QA pairs with external collections. While the fea-
tures that model question-to-answer transformations
provide most benefits, we show that the combination
is crucial for improvement.
Lastly, we show that syntactic dependency pars-
ing and coarse semantic disambiguation yield a
small, yet statistically significant performance in-
crease on top of the traditional bag-of-words and
n-gram representation. We obtain these results us-
ing only off-the-shelf NLP processors that were not
adapted in any way for our task.
726
References
G. Attardi, F. Dell?Orletta, M. Simi, A. Chanev and
M. Ciaramita. 2007. Multilingual Dependency Pars-
ing and Domain Adaptation using DeSR. Proc. of
CoNLL Shared Task Session of EMNLP-CoNLL 2007.
E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G.
Mishne. 2008. Finding High-Quality Content in So-
cial Media, with an Application to Community-based
Question Answering. Proc. of WSDM.
E. Agichtein, S. Lawrence, and L. Gravano. 2001.
Learning Search Engine Specific Query Transforma-
tions for Question Answering. Proc. of WWW.
A. Berger, R. Caruana, D. Cohn, D. Freytag, and V. Mit-
tal. 2000. Bridging the Lexical Chasm: Statistical
Approaches to Answer Finding. Proc. of SIGIR.
M. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. 2007.
Structured Retrieval for Question Answering. Proc. of
SIGIR.
E. Brill, J. Lin, M. Banko, S. Dumais, and A. Ng. 2001.
Data-Intensive Question Answering. Proc. of TREC.
P. Brown, S. Della Pietra, V. Della Pietra, R. Mercer.
1993. The Mathematics of Statistical Machine Trans-
lation: Parameter Estimation. Computational Linguis-
tics, 19(2).
M. Ciaramita and Y. Altun. 2006. Broad Coverage Sense
Disambiguation and Information Extraction with a Su-
persense Sequence Tagger. Proc. of EMNLP.
M. Ciaramita, V. Murdock and V. Plachouras. 2008. Se-
mantic Associations for Contextual Advertising. 2008.
Journal of Electronic Commerce Research - Special Is-
sue on Online Advertising and Sponsored Search, 9(1),
pp.1-15.
M. Collins and N. Duffy. 2001. Convolution Kernels for
Natural Language. Proc. of NIPS 2001.
H. Cui, R. Sun, K. Li, M. Kan, and T. Chua. 2005. Ques-
tion Answering Passage Retrieval Using Dependency
Relations. Proc. of SIGIR.
M. Dredze, J. Blitzer, P. Pratim Talukdar, K. Ganchev,
J. Graca, and F. Pereira. 2007. Frustratingly Hard
Domain Adaptation for Parsing. In Proc. of EMNLP-
CoNLL 2007 Shared Task.
A. Echihabi and D. Marcu. 2003. A Noisy-Channel Ap-
proach to Question Answering. Proc. of ACL.
Y. Freund and R.E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37, pp. 277-296.
R. Girju. 2003. Automatic Detection of Causal Relations
for Question Answering. Proc. of ACL, Workshop on
Multilingual Summarization and Question Answering.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,
M. Surdeanu, R. Bunescu, R. Girju, V. Rus, and P.
Morarescu. 2000. Falcon: Boosting Knowledge for
Answer Engines. Proc. of TREC.
R. Higashinaka and H. Isozaki. 2008. Corpus-based
Question Answering for why-Questions. Proc. of IJC-
NLP.
J. Jeon, W. B. Croft, and J. H. Lee. 2005. Finding Simi-
lar Questions in Large Question and Answer Archives.
Proc. of CIKM.
J. Jeon, W. B. Croft, J. H. Lee, and S. Park. 2006. A
Framework to Predict the Quality of Answers with
Non-Textual Features. Proc. of SIGIR.
J. Ko, T. Mitamura, and E. Nyberg. 2007. Language-
independent Probabilistic Answer Ranking. for Ques-
tion Answering. Proc. of ACL.
X. Li and D. Roth. 2005. Learning Question Classifiers:
The Role of Semantic Information. Natural Language
Engineering.
B. Magnini, M. Negri, R. Prevete, and H. Tanev. 2002.
Comparing Statistical and Content-Based Techniques
for Answer Validation on the Web. Proc. of the VIII
Convegno AI*IA.
M.P. Marcus, B. Santorini and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn TreeBank. Computational Linguis-
tics, 19(2), pp. 313-330.
D. Moldovan, S. Harabagiu, M. Pasca, R. Mihalcea, R.
Goodrum, R. Girju, and V. Rus. 1999. LASSO - A
Tool for Surfing the Answer Net. Proc. of TREC.
A. Moschitti, S. Quarteroni, R. Basili and S. Manand-
har. 2007. Exploiting Syntactic and Shallow Semantic
Kernels for Question/Answer Classification. Proc. of
ACL.
S. Robertson and S. Walker. 1997. On relevance Weights
with Little Relevance Information. Proc. of SIGIR.
R. Soricut and E. Brill. 2006. Automatic Question An-
swering Using the Web: Beyond the Factoid. Journal
of Information Retrieval - Special Issue on Web Infor-
mation Retrieval, 9(2).
L. Shen and A. Joshi. 2005. Ranking and Reranking
with Perceptron, Machine Learning. Special Issue on
Learning in Speech and Language Technologies, 60(1-
3), pp. 73-96.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal
and Y. Liu. 2007. Statistical Machine Translation
for Query Expansion in Answer Retrieval. In Proc.
of ACL.
727
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 492?501,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Multi-Pass Sieve for Coreference Resolution
Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangarajan, Nathanael Chambers,
Mihai Surdeanu, Dan Jurafsky, Christopher Manning
Computer Science Department
Stanford University, Stanford, CA 94305
{kr,heeyoung,sudarshn,natec,mihais,jurafsky,manning}@stanford.edu
Abstract
Most coreference resolution models determine
if two mentions are coreferent using a single
function over a set of constraints or features.
This approach can lead to incorrect decisions
as lower precision features often overwhelm
the smaller number of high precision ones. To
overcome this problem, we propose a simple
coreference architecture based on a sieve that
applies tiers of deterministic coreference mod-
els one at a time from highest to lowest preci-
sion. Each tier builds on the previous tier?s
entity cluster output. Further, our model prop-
agates global information by sharing attributes
(e.g., gender and number) across mentions in
the same cluster. This cautious sieve guar-
antees that stronger features are given prece-
dence over weaker ones and that each deci-
sion is made using all of the information avail-
able at the time. The framework is highly
modular: new coreference modules can be
plugged in without any change to the other
modules. In spite of its simplicity, our ap-
proach outperforms many state-of-the-art su-
pervised and unsupervised models on several
standard corpora. This suggests that sieve-
based approaches could be applied to other
NLP tasks.
1 Introduction
Recent work on coreference resolution has shown
that a rich feature space that models lexical, syn-
tactic, semantic, and discourse phenomena is cru-
cial to successfully address the task (Bengston and
Roth, 2008; Haghighi and Klein, 2009; Haghighi
and Klein, 2010). When such a rich representation
is available, even a simple deterministic model can
achieve state-of-the-art performance (Haghighi and
Klein, 2009).
By and large most approaches decide if two men-
tions are coreferent using a single function over all
these features and information local to the two men-
tions.1 This is problematic for two reasons: (1)
lower precision features may overwhelm the smaller
number of high precision ones, and (2) local infor-
mation is often insufficient to make an informed de-
cision. Consider this example:
The second attack occurred after some rocket firings
aimed, apparently, toward [the israelis], apparently in
retaliation. [we]?re checking our facts on that one. ...
the president, quoted by ari fleischer, his spokesman, is
saying he?s concerned the strike will undermine efforts
by palestinian authorities to bring an end to terrorist at-
tacks and does not contribute to the security of [israel].
Most state-of-the-art models will incorrectly link
we to the israelis because of their proximity and
compatibility of attributes (both we and the israelis
are plural). In contrast, a more cautious approach is
to first cluster the israelis with israel because the de-
monymy relation is highly precise. This initial clus-
tering step will assign the correct animacy attribute
(inanimate) to the corresponding geo-political
entity, which will prevent the incorrect merging with
the mention we (animate) in later steps.
We propose an unsupervised sieve-like approach
to coreference resolution that addresses these is-
1As we will discuss below, some approaches use an addi-
tional component to infer the overall best mention clusters for a
document, but this is still based on confidence scores assigned
using local information.
492
sues. The approach applies tiers of coreference
models one at a time from highest to lowest pre-
cision. Each tier builds on the entity clusters con-
structed by previous models in the sieve, guarantee-
ing that stronger features are given precedence over
weaker ones. Furthermore, each model?s decisions
are richly informed by sharing attributes across the
mentions clustered in earlier tiers. This ensures that
each decision uses all of the information available
at the time. We implemented all components in our
approach using only deterministic models. All our
components are unsupervised, in the sense that they
do not require training on gold coreference links.
The contributions of this work are the following:
? We show that a simple scaffolding framework
that deploys strong features through tiers of
models performs significantly better than a
single-pass model. Additionally, we propose
several simple, yet powerful, new features.
? We demonstrate how far one can get with sim-
ple, deterministic coreference systems that do
not require machine learning or detailed se-
mantic information. Our approach outperforms
most other unsupervised coreference models
and several supervised ones on several datasets.
? Our modular framework can be easily extended
with arbitrary models, including statistical or
supervised models. We believe that our ap-
proach also serves as an ideal platform for the
development of future coreference systems.
2 Related Work
This work builds upon the recent observation that
strong features outweigh complex models for coref-
erence resolution, in both supervised and unsuper-
vised learning setups (Bengston and Roth, 2008;
Haghighi and Klein, 2009). Our work reinforces this
observation, and extends it by proposing a novel ar-
chitecture that: (a) allows easy deployment of such
features, and (b) infuses global information that can
be readily exploited by these features or constraints.
Most coreference resolution approaches perform
the task by aggregating local decisions about pairs
of mentions (Bengston and Roth, 2008; Finkel and
Manning, 2008; Haghighi and Klein, 2009; Stoy-
anov, 2010). Two recent works that diverge from
this pattern are Culotta et al (2007) and Poon and
Domingos (2008). They perform coreference reso-
lution jointly for all mentions in a document, using
first-order probabilistic models in either supervised
or unsupervised settings. Haghighi and Klein (2010)
propose a generative approach that models entity
clusters explicitly using a mostly-unsupervised gen-
erative model. As previously mentioned, our work
is not constrained by first-order or Bayesian for-
malisms in how it uses cluster information. Ad-
ditionally, the deterministic models in our tiered
model are significantly simpler, yet perform gener-
ally better than the complex inference models pro-
posed in these works.
From a high level perspective, this work falls un-
der the theory of shaping, defined as a ?method of
successive approximations? for learning (Skinner,
1938). This theory is known by different names in
many NLP applications: Brown et al (1993) used
simple models as ?stepping stones? for more com-
plex word alignment models; Collins (1999) used
?cautious? decision list learning for named entity
classification; Spitkovsky et al (2010) used ?baby
steps? for unsupervised dependency parsing, etc. To
the best of our knowledge, we are the first to apply
this theory to coreference resolution.
3 Description of the Task
Intra-document coreference resolution clusters to-
gether textual mentions within a single document
based on the underlying referent entity. Mentions
are usually noun phrases (NPs) headed by nominal
or pronominal terminals. To facilitate comparison
with most of the recent previous work, we report re-
sults using gold mention boundaries. However, our
approach does not make any assumptions about the
underlying mentions, so it is trivial to adapt it to pre-
dicted mention boundaries (e.g., see Haghighi and
Klein (2010) for a simple mention detection model).
3.1 Corpora
We used the following corpora for development and
evaluation:
? ACE2004-ROTH-DEV2 ? development split
of Bengston and Roth (2008), from the corpus
used in the 2004 Automatic Content Extraction
(ACE) evaluation. It contains 68 documents
and 4,536 mentions.
2We use the same corpus names as (Haghighi and Klein,
2009) to facilitate comparison with previous work.
493
? ACE2004-CULOTTA-TEST ? partition of
ACE 2004 corpus reserved for testing by sev-
eral previous works (Culotta et al, 2007;
Bengston and Roth, 2008; Haghighi and Klein,
2009). It consists of 107 documents and 5,469
mentions.
? ACE2004-NWIRE ? the newswire subset of
the ACE 2004 corpus, utilized by Poon and
Domingos (2008) and Haghighi and Klein
(2009) for testing. It contains 128 documents
and 11,413 mentions.
? MUC6-TEST ? test corpus from the sixth
Message Understanding Conference (MUC-6)
evaluation. It contains 30 documents and 2,068
mentions.
We used the first corpus (ACE2004-ROTH-DEV)
for development. The other corpora are reserved for
testing. We parse all documents using the Stanford
parser (Klein and Manning, 2003). The syntactic in-
formation is used to identify the mention head words
and to define the ordering of mentions in a given
sentence (detailed in the next section). For a fair
comparison with previous work, we do not use gold
named entity labels or mention types but, instead,
take the labels provided by the Stanford named en-
tity recognizer (NER) (Finkel et al, 2005).
3.2 Evaluation Metrics
We use three evaluation metrics widely used in the
literature: (a) pairwise F1 (Ghosh, 2003) ? com-
puted over mention pairs in the same entity clus-
ter; (b) MUC (Vilain et al, 1995) ? which measures
how many predicted clusters need to be merged to
cover the gold clusters; and (c) B3 (Amit and Bald-
win, 1998) ? which uses the intersection between
predicted and gold clusters for a given mention to
mark correct mentions and the sizes of the the pre-
dicted and gold clusters as denominators for preci-
sion and recall, respectively. We refer the interested
reader to (X. Luo, 2005; Finkel and Manning, 2008)
for an analysis of these metrics.
4 Description of the Multi-Pass Sieve
Our sieve framework is implemented as a succes-
sion of independent coreference models. We first de-
scribe how each model selects candidate mentions,
and then describe the models themselves.
4.1 Mention Processing
Given a mention mi, each model may either decline
to propose a solution (in the hope that one of the
subsequent models will solve it) or deterministically
select a single best antecedent from a list of pre-
vious mentions m1, . . . , mi?1. We sort candidate
antecedents using syntactic information provided by
the Stanford parser, as follows:
Same Sentence ? Candidates in the same sentence
are sorted using left-to-right breadth-first traversal
of syntactic trees (Hobbs, 1977). Figure 1 shows an
example of candidate ordering based on this traver-
sal. The left-to-right ordering favors subjects, which
tend to appear closer to the beginning of the sentence
and are more probable antecedents. The breadth-
first traversal promotes syntactic salience by rank-
ing higher noun phrases that are closer to the top of
the parse tree (Haghighi and Klein, 2009). If the
sentence containing the anaphoric mention contains
multiple clauses, we repeat the above heuristic sep-
arately in each S* constituent, starting with the one
containing the mention.
Previous Sentence ? For all nominal mentions we
sort candidates in the previous sentences using right-
to-left breadth-first traversal. This guarantees syn-
tactic salience and also favors document proximity.
For pronominal mentions, we sort candidates in pre-
vious sentences using left-to-right traversal in or-
der to favor subjects. Subjects are more probable
antecedents for pronouns (Kertz et al, 2006). For
example, this ordering favors the correct candidate
(pepsi) for the mention they:
[pepsi] says it expects to double [quaker]?s
snack food growth rate. after a month-long
courtship, [they] agreed to buy quaker oats. . .
In a significant departure from previous work,
each model in our framework gets (possibly incom-
plete) clustering information for each mention from
the earlier coreference models in the multi-pass sys-
tem. In other words, each mention mi may already
be assigned to a cluster Cj containing a set of men-
tions: Cj = {m
j
1, . . . ,m
j
k}; mi ? Cj . Unassigned
mentions are unique members of their own cluster.
We use this information in several ways:
Attribute sharing ? Pronominal coreference reso-
lution (discussed later in this section) is severely af-
494
S	 ?
of	 ?
will	 ?
head	 ?
NP	 ?
Richard	 ?Levin	 ?
the	 ?Globaliza?on	 ?Studies	 ?Center	 ?
NP	 ?
NP	 ?
the	 ?Chancelor	 ?
NP	 ?
,	 ?
VP	 ?
NP	 ?
PP	 ?
this	 ?pres?gious	 ?university	 ?
NP	 ?
VP	 ?
#1	 ?
#2	 ?
#3	 ?
#4	 ?
Figure 1: Example of left-to-right breadth-first tree
traversal. The numbers indicate the order in which the
NPs are visited.
fected by missing attributes (which introduce pre-
cision errors because incorrect antecedents are se-
lected due to missing information) and incorrect at-
tributes (which introduce recall errors because cor-
rect links are not generated due to attribute mismatch
between mention and antecedent). To address this
issue, we perform a union of all mention attributes
(e.g., number, gender, animacy) in a given cluster
and share the result with all cluster mentions. If
attributes from different mentions contradict each
other we maintain all variants. For example, our
naive number detection assigns singular to the
mention a group of students and plural to five stu-
dents. When these mentions end up in the same clus-
ter, the resulting number attributes becomes the set
{singular, plural}. Thus this cluster can later
be merged with both singular and plural pronouns.
Mention selection ? Traditionally, a coreference
model attempts to resolve every mention in the text,
which increases the likelihood of errors. Instead, in
each of our models, we exploit the cluster informa-
tion received from the previous stages by resolving
only mentions that are currently first in textual order
in their cluster. For example, given the following or-
dered list of mentions, {m11, m
2
2, m
2
3, m
3
4, m
1
5, m
2
6},
where the superscript indicates cluster id, our model
will attempt to resolve only m22 and m
3
4. These two
are the only mentions that have potential antecedents
and are currently marked as the first mentions in
their clusters. The intuition behind this heuristic
is two-fold. First, early cluster mentions are usu-
ally better defined than subsequent ones, which are
likely to have fewer modifiers or are pronouns (Fox,
1993). Several of our models use this modifier infor-
mation. Second, by definition, first mentions appear
closer to the beginning of the document, hence there
are fewer antecedent candidates to select from, and
fewer opportunities to make a mistake.
Search Pruning ? Finally, we prune the search
space using discourse salience. We disable coref-
erence for first cluster mentions that: (a) are or start
with indefinite pronouns (e.g., some, other), or (b)
start with indefinite articles (e.g., a, an). One excep-
tion to this rule is the model deployed in the first
pass; it only links mentions if their entire extents
match exactly. This model is triggered for all nom-
inal mentions regardless of discourse salience, be-
cause it is possible that indefinite mentions are re-
peated in a document when concepts are discussed
but not instantiated, e.g., a sports bar below:
Hanlon, a longtime Broncos fan, thinks it is the perfect
place for [a sports bar] and has put up a blue-and-orange
sign reading, ?Wanted Broncos Sports Bar On This Site.?
. . . In a Nov. 28 letter, Proper states ?while we have no
objection to your advertising the property as a location
for [a sports bar], using the Broncos? name and colors
gives the false impression that the bar is or can be affili-
ated with the Broncos.?
4.2 The Modules of the Multi-Pass Sieve
We now describe the coreference models imple-
mented in the sieve. For clarity, we summarize them
in Table 1 and show the cumulative performance as
they are added to the sieve in Table 2.
4.2.1 Pass 1 - Exact Match
This model links two mentions only if they con-
tain exactly the same extent text, including modifiers
and determiners, e.g., the Shahab 3 ground-ground
missile. As expected, this model is extremely pre-
cise, with a pairwise precision over 96%.
4.2.2 Pass 2 - Precise Constructs
This model links two mentions if any of the con-
ditions below are satisfied:
Appositive ? the two nominal mentions are in an
appositive construction, e.g., [Israel?s Deputy De-
fense Minister], [Ephraim Sneh] , said . . . We
use the same syntactic rules to detect appositions as
Haghighi and Klein (2009).
495
Pass Type Features
1 N exact extent match
2 N,P appositive | predicate nominative | role appositive | relative pronoun | acronym | demonym
3 N cluster head match & word inclusion & compatible modifiers only & not i-within-i
4 N cluster head match & word inclusion & not i-within-i
5 N cluster head match & compatible modifiers only & not i-within-i
6 N relaxed cluster head match & word inclusion & not i-within-i
7 P pronoun match
Table 1: Summary of passes implemented in the sieve. The Type column indicates the type of coreference in each
pass: N ? nominal or P ? pronominal. & and | indicate conjunction and disjunction of features, respectively.
Predicate nominative ? the two mentions (nominal
or pronominal) are in a copulative subject-object re-
lation, e.g., [The New York-based College Board] is
[a nonprofit organization that administers the SATs
and promotes higher education] (Poon and Domin-
gos, 2008).
Role appositive ? the candidate antecedent is
headed by a noun and appears as a modifier in an
NP whose head is the current mention, e.g., [[ac-
tress] Rebecca Schaeffer]. This feature is inspired
by Haghighi and Klein (2009), who triggered it only
if the mention is labeled as a person by the NER.
We constrain this heuristic more in our work: we
allow this feature to match only if: (a) the mention
is labeled as a person, (b) the antecedent is animate
(we detail animacy detection in Pass 7), and (c) the
antecedent?s gender is not neutral.
Relative pronoun ? the mention is a relative pro-
noun that modifies the head of the antecedent NP,
e.g., [the finance street [which] has already formed
in the Waitan district].
Acronym ? both mentions are tagged as NNP and
one of them is an acronym of the other, e.g., [Agence
France Presse] . . . [AFP]. We use a simple acronym
detection algorithm, which marks a mention as an
acronym of another if its text equals the sequence
of upper case characters in the other mention. We
will adopt better solutions for acronym detection in
future work (Schwartz, 2003).
Demonym ? one of the mentions is a demonym of
the other, e.g., [Israel] . . . [Israeli]. For demonym
detection we use a static list of countries and their
gentilic forms from Wikipedia.3
All the above features are extremely precise. As
shown in Table 2 the pairwise precision of the sieve
3
http://en.wikipedia.org/wiki/List_of_adjectival_and_
demonymic_forms_of_place_names
after adding these features is over 95% and recall
increases 5 points.
4.2.3 Pass 3 - Strict Head Matching
Linking a mention to an antecedent based on the
naive matching of their head words generates a lot
of spurious links because it completely ignores pos-
sibly incompatible modifiers (Elsner and Charniak,
2010). For example, Yale University and Harvard
University have similar head words, but they are ob-
viously different entities. To address this issue, this
pass implements several features that must all be
matched in order to yield a link:
Cluster head match ? the mention head word
matches any head word in the antecedent clus-
ter. Note that this feature is actually more relaxed
than naive head matching between mention and an-
tecedent candidate because it is satisfied when the
mention?s head matches the head of any entity in the
candidate?s cluster. We constrain this feature by en-
forcing a conjunction with the features below.
Word inclusion ? all the non-stop4 words in the
mention cluster are included in the set of non-stop
words in the cluster of the antecedent candidate.
This heuristic exploits the property of discourse that
it is uncommon to introduce novel information in
later mentions (Fox, 1993). Typically, mentions
of the same entity become shorter and less infor-
mative as the narrative progresses. For example,
the two mentions in . . . intervene in the [Florida
Supreme Court]?s move . . . does look like very dra-
matic change made by [the Florida court] point to
the same entity, but the two mentions in the text be-
low belong to different clusters:
The pilot had confirmed . . . he had turned onto
4Our stop word list includes person titles as well.
496
MUC B3 Pairwise
Passes P R F1 P R F1 P R F1
{1} 95.9 31.8 47.8 99.1 53.4 69.4 96.9 15.4 26.6
{1,2} 95.4 43.7 59.9 98.5 58.4 73.3 95.7 20.6 33.8
{1,2,3} 92.1 51.3 65.9 96.7 62.9 76.3 91.5 26.8 41.5
{1,2,3,4} 91.7 51.9 66.3 96.5 63.5 76.6 91.4 27.8 42.7
{1,2,3,4,5} 91.1 52.6 66.7 96.1 63.9 76.7 90.3 28.4 43.2
{1,2,3,4,5,6} 89.5 53.6 67.1 95.3 64.5 76.9 88.8 29.2 43.9
{1,2,3,4,5,6,7} 83.7 74.1 78.6 88.1 74.2 80.5 80.1 51.0 62.3
Table 2: Cumulative performance on development (ACE2004-ROTH-DEV) as passes are added to the sieve.
[the correct runway] but pilots behind him say
he turned onto [the wrong runway].
Compatible modifiers only ? the mention?s mod-
ifiers are all included in the modifiers of the an-
tecedent candidate. This feature models the same
discourse property as the previous feature, but it fo-
cuses on the two individual mentions to be linked,
rather than their entire clusters. For this feature we
only use modifiers that are nouns or adjectives.
Not i-within-i ? the two mentions are not in an i-
within-i construct, i.e., one cannot be a child NP
in the other?s NP constituent (Haghighi and Klein,
2009).
This pass continues to maintain high precision
(91% pairwise) while improving recall significantly
(over 6 points pairwise and almost 8 points MUC).
4.2.4 Passes 4 and 5 - Variants of Strict Head
Passes 4 and 5 are different relaxations of the
feature conjunction introduced in Pass 3, i.e.,
Pass 4 removes the compatible modifiers
only feature, while Pass 5 removes the word
inclusion constraint. All in all, these two passes
yield an improvement of 1.7 pairwise F1 points,
due to recall improvements. Table 2 shows that the
word inclusion feature is more precise than
compatible modifiers only, but the latter
has better recall.
4.2.5 Pass 6 - Relaxed Head Matching
This pass relaxes the cluster head match heuris-
tic by allowing the mention head to match any word
in the cluster of the candidate antecedent. For ex-
ample, this heuristic matches the mention Sanders
to a cluster containing the mentions {Sauls, the
judge, Circuit Judge N. Sanders Sauls}. To maintain
high precision, this pass requires that both mention
and antecedent be labeled as named entities and the
types coincide. Furthermore, this pass implements
a conjunction of the above features with word
inclusion and not i-within-i. This pass
yields less than 1 point improvement in most met-
rics.
4.2.6 Pass 7 - Pronouns
With one exception (Pass 2), all the previous
coreference models focus on nominal coreference
resolution. However, it would be incorrect to say
that our framework ignores pronominal coreference
in the first six passes. In fact, the previous mod-
els prepare the stage for pronominal coreference by
constructing precise clusters with shared mention at-
tributes. These are crucial factors for pronominal
coreference.
Like previous work, we implement pronominal
coreference resolution by enforcing agreement con-
straints between the coreferent mentions. We use the
following attributes for these constraints:
Number ? we assign number attributes based on:
(a) a static list for pronouns; (b) NER labels: men-
tions marked as a named entity are considered sin-
gular with the exception of organizations, which can
be both singular or plural; (c) part of speech tags:
NN*S tags are plural and all other NN* tags are sin-
gular; and (d) a static dictionary from (Bergsma and
Lin, 2006).
Gender ? we assign gender attributes from static
lexicons from (Bergsma and Lin, 2006; Ji and Lin,
2009).
Person ? we assign person attributes only to pro-
nouns. However, we do not enforce this constraint
when linking two pronouns if one appears within
quotes. This is a simple heuristic for speaker de-
tection, e.g., I and she point to the same person in
497
?[I] voted my conscience,? [she] said.
Animacy ? we set animacy attributes using: (a)
a static list for pronouns; (b) NER labels, e.g.,
PERSON is animate whereas LOCATION is not; and
(c) a dictionary boostrapped from the web (Ji and
Lin, 2009).
NER label ? from the Stanford NER.
If we cannot detect a value, we set attributes to
unknown and treat them as wildcards, i.e., they can
match any other value.
This final model raises the pairwise recall of our
system almost 22 percentage points, with only an 8
point drop in pairwise precision. Table 2 shows that
similar behavior is measured for all other metrics.
After all passes have run, we take the transitive clo-
sure of the generated clusters as the system output.
5 Experimental Results
We present the results of our approach and other rel-
evant prior work in Table 3. We include in the ta-
ble all recent systems that report results under the
same conditions as our experimental setup (i.e., us-
ing gold mentions) and use the same corpora. We
exclude from this analysis two notable works that
report results only on a version of the task that in-
cludes finding mentions (Haghighi and Klein, 2010;
Stoyanov, 2010). The Haghighi and Klein (2009)
numbers have two variants: with semantics (+S)
and without (?S). To measure the contribution of
our multi-pass system, we also present results from a
single-pass variant of our system that uses all appli-
cable features from the multi-pass system (marked
as ?single pass? in the table).
Our sieve model outperforms all systems on
two out of the four evaluation corpora (ACE2004-
ROTH-DEV and ACE2004-NWIRE), on all met-
rics. On the corpora where our model is not best,
it ranks a close second. For example, in ACE2004-
CULOTTA-TEST our system has a B3 F1 score
only .4 points lower than Bengston and Roth (2008)
and it outperforms all unsupervised approaches. In
MUC6-TEST, our sieve?s B3 F1 score is 1.8 points
lower than Haghighi and Klein (2009) +S, but it out-
performs a supervised system that used gold named
entity labels. Finally, the multi-pass architecture al-
ways beats the equivalent single-pass system with
its contribution ranging between 1 and 4 F1 points
depending on the corpus and evaluation metric.
Our approach has the highest precision on all cor-
pora, regardless of evaluation metric. We believe
this is particularly useful for large-scale NLP appli-
cations that use coreference resolution components,
e.g., question answering or information extraction.
These applications can generally function without
coreference information so it is beneficial to provide
such information only when it is highly precise.
6 Discussion
6.1 Comparison to Previous Work
The sieve model outperforms all other systems on
at least two test sets, even though most of the other
models are significantly richer. Amongst the com-
parisons, several are supervised (Bengston and Roth,
2008; Finkel and Manning, 2008; Culotta et al,
2007). The system of Haghighi and Klein (2009)
+S uses a lexicon of semantically-compatible noun
pairs acquired transductively, i.e., with knowledge
of the mentions in the test set. Our system does
not rely on labeled corpora for training (like super-
vised approaches) nor access to corpora during test-
ing (like Haghighi and Klein (2009)).
The system that is closest to ours is Haghighi and
Klein (2009) ?S. Like us, they use a rich set of fea-
tures and deterministic decisions. However, theirs
is a single-pass model with a smaller feature set
(no cluster-level, acronym, demonym, or animacy
information). Table 3 shows that on the two cor-
pora where results for this system are available, we
outperform it considerably on all metrics. To un-
derstand if the difference is due to the multi-pass
architecture or the richer feature set we compared
(Haghighi and Klein, 2009) ?S against both our
multi-pass system and its single-pass variant. The
comparison indicates that both these contributions
help: our single-pass system outperforms Haghighi
and Klein (2009) consistently, and the multi-pass ar-
chitecture further improves the performance of our
single-pass system between 1 and 4 F1 points, de-
pending on the corpus and evaluation metric.
6.2 Semantic Head Matching
Recent unsupervised coreference work from
Haghighi and Klein (2009) included a novel
semantic component that matched related head
words (e.g., AOL is a company) learned from select
498
MUC B3 Pairwise
P R F1 P R F1 P R F1
ACE2004-ROTH-DEV
This work (sieve) 83.7 74.1 78.6 88.1 74.2 80.5 80.1 51.0 62.3
This work (single pass) 82.2 72.6 77.1 86.8 72.6 79.1 76.0 47.6 58.5
Haghighi and Klein (2009) ?S 78.3 70.5 74.2 84.0 71.0 76.9 71.3 45.4 55.5
Haghighi and Klein (2009) +S 77.9 74.1 75.9 81.8 74.3 77.9 68.2 51.2 58.5
ACE2004-CULOTTA-TEST
This work (sieve) 80.4 71.8 75.8 86.3 75.4 80.4 71.6 46.2 56.1
This work (single pass) 78.4 69.2 73.5 85.1 73.9 79.1 69.5 44.1 53.9
Haghighi and Klein (2009) ?S 74.3 66.4 70.2 83.6 71.0 76.8 66.4 38.0 48.3
Haghighi and Klein (2009) +S 74.8 77.7 79.6 79.6 78.5 79.0 57.5 57.6 57.5
Culotta et al (2007) ? ? ? 86.7 73.2 79.3 ? ? ?
Bengston and Roth (2008) 82.7 69.9 75.8 88.3 74.5 80.8 55.4 63.7 59.2
MUC6-TEST
This work (sieve) 90.5 68.0 77.7 91.2 61.2 73.2 90.3 53.3 67.1
This work (single pass) 89.3 65.9 75.8 90.2 58.8 71.1 89.5 50.6 64.7
Haghighi and Klein (2009) +S 87.2 77.3 81.9 84.7 67.3 75.0 80.5 57.8 67.3
Poon and Domingos (2008) 83.0 75.8 79.2 ? ? ? 63.0 57.0 60.0
Finkel and Manning (2008) +G 89.7 55.1 68.3 90.9 49.7 64.3 74.1 37.1 49.5
ACE2004-NWIRE
This work (sieve) 83.8 73.2 78.1 87.5 71.9 78.9 79.6 46.2 58.4
This work (single pass) 82.2 71.5 76.5 86.2 70.0 77.3 76.9 41.9 54.2
Haghighi and Klein (2009) +S 77.0 75.9 76.5 79.4 74.5 76.9 66.9 49.2 56.7
Poon and Domingos (2008) 71.3 70.5 70.9 ? ? ? 62.6 38.9 48.0
Finkel and Manning (2008) +G 78.7 58.5 67.1 86.8 65.2 74.5 76.1 44.2 55.9
Table 3: Results using gold mention boundaries. Where available, we show results for a given corpus grouped in
two blocks: the top block shows results of unsupervised systems and the bottom block contains supervised systems.
Bold numbers indicate best results in a given block. +/-S indicates if the (Haghighi and Klein, 2009) system in-
cludes/excludes their semantic component. +G marks systems that used gold NER labels.
wikipedia articles. They first identified articles
relevant to the entity mentions in the test set, and
then bootstrapped from known syntactic patterns
for apposition and predicate-nominatives in order to
learn a database of related head pairs. They show
impressive gains by using these learned pairs in
coreference decisions. This type of learning using
test set mentions is often described as transductive.
Our work instead focuses on an approach that
does not require access to the dataset beforehand.
We thus did not include a similar semantic compo-
nent in our system, given that running a bootstrap-
ping learner whenever a new data set is encountered
is not practical and, ultimately, reduces the usability
of this NLP component. However, our results show
that our sieve algorithm with minimal semantic in-
formation still performs as well as the Haghighi and
Klein (2009) system with semantics.
6.3 Flexible Architecture
The sieve architecture offers benefits beyond im-
proved accuracy. Its modular design provides a flex-
ibility for features that is not available in most su-
pervised or unsupervised systems. The sieve al-
lows new features to be seamlessly inserted with-
out affecting (or even understanding) the other com-
ponents. For instance, once a new high precision
feature (or group of features) is inserted as its own
stage, it will benefit later stages with more precise
clusters, but it will not interfere with their particu-
499
lar algorithmic decisions. This flexibility is in sharp
contrast to supervised classifiers that require their
models to be retrained on labeled data, and unsu-
pervised systems that do not offer a clear insertion
point for new features. It can be difficult to fully
understand how a system makes a single decision,
but the sieve allows for flexible usage with minimal
effort.
6.4 Error Analysis
Pronominal Nominal Proper Total
Pronominal 49 / 237 116 / 317 104 / 595 269 / 1149
Nominal 79 / 351 129 / 913 61 / 986 269 / 2250
Proper 51 / 518 15 / 730 38 / 595 104 / 1843
Total 179 / 1106 260 / 1960 203 / 2176 642 / 5242
Table 4: Number of pair-wise errors produced by the
sieve after transitive closure in the MUC6-TEST corpus.
Rows indicate mention types; columns are types of an-
tecedent. Each cell shows the number of precision/recall
errors for that configuration. The total number of gold
links in MUC6-TEST is 11,236.
Table 4 shows the number of incorrect pair-wise
links generated by our system on the MUC6-TEST
corpus. The table indicates that most of our er-
rors are for nominal mentions. For example, the
combined (precision plus recall) number of errors
for proper or common noun mentions is three times
larger than the number of errors made for pronom-
inal mentions. The table also highlights that most
of our errors are recall errors. There are eight times
more recall errors than precision errors in our output.
This is a consequence of our decision to prioritize
highly precise features in the sieve.
The above analysis illustrates that our next effort
should focus on improving recall. In order to under-
stand the limitations of our current system, we ran-
domly selected 60 recall errors (20 for each mention
type) and investigated their causes. Not surprisingly,
the causes are unique to each type.
For proper nouns, 50% of recall errors are due to
mention lengthening, mentions that are longer than
their earlier mentions. For example, Washington-
based USAir appears after USAir in the text, so our
head matching components skip it because their high
precision depends on disallowing new modifiers as
the discourse proceeds. When the mentions were re-
versed (as is the usual case), they match.
The common noun recall errors are very differ-
ent from proper nouns: 17 of the 20 random exam-
ples can be classified as semantic knowledge. These
errors are roughly evenly split between recognizing
categories of names (e.g., Gitano is an organization
name hence it should match the nominal antecedent
the company), and understanding hypernym rela-
tions like settlements and agreements.
Pronoun errors come in two forms. Roughly 40%
of these errors are attribute mismatches involving
sometimes ambiguous uses of gender and number
(e.g., she with Pat Carney). Another 40% are not se-
mantic or attribute-based, but rather simply arise due
to the order in which we check potential antecedents.
In all these situations, the correct links are missed
because the system chooses a closer (incorrect) an-
tecedent.
These four highlighted errors (lengthening, se-
mantics, attributes, ordering) add up to 77% of all
recall errors in the selected set. In general, each
error type is particular to a specific mention type.
This suggests that recall improvements can be made
by focusing on one mention type without aversely
affecting the others. Our sieve-based approach to
coreference uniquely allows for such new models to
be seamlessly inserted.
7 Conclusion
We presented a simple deterministic approach to
coreference resolution that incorporates document-
level information, which is typically exploited only
by more complex, joint learning models. Our sieve
architecture applies a battery of deterministic coref-
erence models one at a time from highest to low-
est precision, where each model builds on the pre-
vious model?s cluster output. Despite its simplicity,
our approach outperforms or performs comparably
to the state of the art on several corpora.
An additional benefit of the sieve framework is its
modularity: new features or models can be inserted
in the system with limited understanding of the other
features already deployed. Our code is publicly re-
leased5 and can be used both as a stand-alone coref-
erence system and as a platform for the development
of future systems.
5http://nlp.stanford.edu/software/
dcoref.shtml
500
The strong performance of our system suggests
the use of sieves in other NLP tasks for which a va-
riety of very high-precision features can be designed
and non-local features can be shared; likely candi-
dates include relation and event extraction, template
slot filling, and author name deduplication.
Acknowledgments
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0181. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the author(s) and do not
necessarily reflect the view of DARPA, AFRL, or
the US government.
Many thanks to Jenny Finkel for writing a reim-
plementation of much of Haghighi and Klein (2009),
which served as the starting point for the work re-
ported here. We also thank Nicholas Rizzolo and
Dan Roth for helping us replicate their experimen-
tal setup, and Heng Ji and Dekang Lin for providing
their gender lexicon.
References
B. Amit and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In MUC-7.
E. Bengston and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP.
S. Bergsma and D. Lin. 2006. Bootstrapping Path-Based
Pronoun Resolution. In ACL-COLING.
P.F. Brown, V.J. Della Pietra, S.A. Della Pietra, and R.L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19.
M. Collins and Y. Singer. 1999. Unsupervised models
for named entity classification. In EMNLP-VLC.
A. Culotta, M. Wick, R. Hall, and A. McCallum. 2007.
First-order probabilistic models for coreference reso-
lution. In NAACL-HLT.
M. Elsner and E. Charniak. 2010. The same-head heuris-
tic for coreference. In ACL.
J. Finkel, T. Grenager, and C. Manning. 2005. Incorpo-
rating non-local information into information extrac-
tion systems by Gibbs sampling. In ACL.
J. Finkel and C. Manning. 2008. Enforcing transitivity
in coreference resolution. In ACL.
B. A. Fox 1993. Discourse structure and anaphora:
written and conversational English. Cambridge Uni-
versity Press.
J. Ghosh. 2003. Scalable clustering methods for data
mining. Handbook of Data Mining, chapter 10, pages
247?277.
A. Haghighi and D. Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In EMNLP.
A. Haghighi and D. Klein. 2010. Coreference resolution
in a modular, entity-centered model. In HLT-NAACL.
J.R. Hobbs. 1977. Resolving pronoun references. Lin-
gua.
H. Ji and D. Lin. 2009. Gender and animacy knowl-
edge discovery from web-scale n-grams for unsuper-
vised person mention detection. In PACLIC.
L. Kertz, A. Kehler, and J. Elman. 2006. Grammatical
and Coherence-Based Factors in Pronoun Interpreta-
tion. In Proceedings of the 28th Annual Conference of
the Cognitive Science Society.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In ACL.
X. Luo. 2005. On coreference resolution performance
metrics. In HTL-EMNLP.
H. Poon and P. Domingos. 2008. Joint unsuper-
vised coreference resolution with Markov Logic. In
EMNLP.
A.S. Schwartz and M.A. Hearst. 2003. A simple
algorithm for identifying abbrevation definitions in
biomedical text. In Pacific Symposium on Biocomput-
ing.
B.F. Skinner. 1938. The behavior of organisms: An ex-
perimental analysis. Appleton-Century-Crofts.
V.I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010.
From baby steps to leapfrog: How ?less is more? in
unsupervised dependency parsing. In NAACL.
V. Stoyanov, N. Gilbert, C. Cardie, and E. Riloff. 2010.
Conundrums in noun phrase coreference resolution:
making sense of the state-of-the-art. In ACL-IJCNLP.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L.
Hirschman. 1995. A model-theoretic coreference
scoring scheme. In MUC-6.
501
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 455?465, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Multi-instance Multi-label Learning for Relation Extraction
Mihai Surdeanu?, Julie Tibshirani?, Ramesh Nallapati?, Christopher D. Manning?
? Stanford University, Stanford, CA 94305
{mihais,jtibs,manning}@stanford.edu
? Artificial Intelligence Center, SRI International
nallapat@ai.sri.com
Abstract
Distant supervision for relation extraction
(RE) ? gathering training data by aligning a
database of facts with text ? is an efficient ap-
proach to scale RE to thousands of different
relations. However, this introduces a challeng-
ing learning scenario where the relation ex-
pressed by a pair of entities found in a sen-
tence is unknown. For example, a sentence
containing Balzac and France may express
BornIn or Died, an unknown relation, or no re-
lation at all. Because of this, traditional super-
vised learning, which assumes that each ex-
ample is explicitly mapped to a label, is not
appropriate. We propose a novel approach
to multi-instance multi-label learning for RE,
which jointly models all the instances of a pair
of entities in text and all their labels using
a graphical model with latent variables. Our
model performs competitively on two difficult
domains.
1 Introduction
Information extraction (IE), defined as the task of
extracting structured information (e.g., events, bi-
nary relations, etc.) from free text, has received re-
newed interest in the ?big data? era, when petabytes
of natural-language text containing thousands of dif-
ferent structure types are readily available. How-
ever, traditional supervised methods are unlikely to
scale in this context, as training data is either lim-
ited or nonexistent for most of these structures. One
of the most promising approaches to IE that ad-
dresses this limitation is distant supervision, which
generates training data automatically by aligning a
DB =
(
BornIn(Barack Obama,United States)
EmployedBy(Barack Obama,United States)
)
Sentence Latent Label
Barack Obama is the 44th and current President
of the United States.
EmployedBy
Obama was born in the United States just as he
has always said.
BornIn
United States President Barack Obama meets
with Chinese Vice President Xi Jinping today.
EmployedBy
Obama ran for the United States Senate in 2004. ?
Figure 1: Training sentences generated through distant
supervision for a database containing two facts.
database of facts with text (Craven and Kumlien,
1999; Bunescu and Mooney, 2007).
In this paper we focus on distant supervision for
relation extraction (RE), a subproblem of IE that ad-
dresses the extraction of labeled relations between
two named entities. Figure 1 shows a simple exam-
ple for a RE domain with two labels. Distant super-
vision introduces two modeling challenges, which
we highlight in the table. The first challenge is
that some training examples obtained through this
heuristic are not valid, e.g., the last sentence in Fig-
ure 1 is not a correct example for any of the known
labels for the tuple. The percentage of such false
positives can be quite high. For example, Riedel
et al2010) report up to 31% of false positives in
a corpus that matches Freebase relations with New
York Times articles. The second challenge is that
the same pair of entities may have multiple labels
and it is unclear which label is instantiated by any
textual mention of the given tuple. For example, in
Figure 1, the tuple (Barack Obama, United States)
has two valid labels: BornIn and EmployedBy, each
(latently) instantiated in different sentences. In the
455
instance
...
instance
label
instance
label
...
object
Figure 2: Overview of multi-instance multi-label learn-
ing. To contrast, in traditional supervised learning there
is one instance and one label per object. For relation ex-
traction the object is a tuple of two named entities. Each
mention of this tuple in text generates a different instance.
Riedel corpus, 7.5% of the entity tuples in the train-
ing partition have more than one label.
We summarize this multi-instance multi-label
(MIML) learning problem in Figure 2. In this pa-
per we propose a novel graphical model, which we
called MIML-RE, that targets MIML learning for re-
lation extraction. Our work makes the following
contributions:
(a) To our knowledge, MIML-RE is the first RE ap-
proach that jointly models both multiple instances
(by modeling the latent labels assigned to instances)
and multiple labels (by providing a simple method to
capture dependencies between labels). For example,
our model learns that certain labels tend to be gener-
ated jointly while others cannot be jointly assigned
to the same tuple.
(b) We show that MIML-RE performs competitively
on two difficult domains.
2 Related Work
Distant supervision for IE was introduced by Craven
and Kumlien (1999), who focused on the ex-
traction of binary relations between proteins and
cells/tissues/diseases/drugs using the Yeast Protein
Database as a source of distant supervision. Since
then, the approach grew in popularity (Bunescu and
Mooney, 2007; Bellare and McCallum, 2007; Wu
and Weld, 2007; Mintz et al2009; Riedel et al
2010; Hoffmann et al2011; Nguyen and Moschitti,
2011; Sun et al2011; Surdeanu et al2011a).
However, most of these approaches make one or
more approximations in learning. For example,
most proposals heuristically transform distant super-
vision to traditional supervised learning (i.e., single-
instance single-label) (Bellare and McCallum, 2007;
Wu and Weld, 2007; Mintz et al2009; Nguyen
and Moschitti, 2011; Sun et al2011; Surdeanu
et al2011a). Bunescu and Mooney (2007) and
Riedel et al2010) model distant supervision for
relation extraction as a multi-instance single-label
problem, which allows multiple mentions for the
same tuple but disallows more than one label per ob-
ject. Our work is closest to Hoffmann et al2011).
They address the same problem we do (binary rela-
tion extraction) with a MIML model, but they make
two approximations. First, they use a deterministic
model that aggregates latent instance labels into a
set of labels for the corresponding tuple by OR-ing
the classification results. We use instead an object-
level classifier that is trained jointly with the clas-
sifier that assigns latent labels to instances and can
capture dependencies between labels. Second, they
use a Perceptron-style additive parameter update ap-
proach, whereas we train in a Bayesian framework.
We show in Section 5 that these approximations gen-
erally have a negative impact on performance.
MIML learning has been used in fields other than
natural language processing. For example, Zhou
and Zhang (2007) use MIML for scene classifica-
tion. In this problem, each image may be assigned
multiple labels corresponding to the different scenes
captured. Furthermore, each image contains a set of
patches, which forms the bag of instances assigned
to the given object (image). Zhou and Zhang pro-
pose two algorithms that reduce the MIML problem
to a more traditional supervised learning task. In
one algorithm, for example, they convert the task to
a multi-instance single-label problem by creating a
separate bag for each label. Due to this, the pro-
posed approach cannot model inter-label dependen-
cies. Moreover, the authors make a series of approx-
imations, e.g., they assume that each instance in a
bag shares the bag?s overall label. We instead model
all these issues explicitly in our approach.
In general, our approach belongs to the category
of models that learn in the presence of incomplete or
incorrect labels. There has been interest among ma-
chine learning researchers in the general problem of
noisy data, especially in the area of instance-based
learning. Brodley and Friedl (1999) summarize
past approaches and present a simple, all-purpose
method to filter out incorrect data before training.
While potentially applicable to our problem, this ap-
proach is completely general and cannot incorporate
our domain-specific knowledge about how the noisy
456
data is generated.
3 Distant Supervision for Relation Extraction
Here we focus on distant supervision for the ex-
traction of relations between two entities. We de-
fine a relation as the construct r(e1, e2), where r is
the relation name, e.g., BornIn in Figure 1, and e1
and e2 are two entity names, e.g., Barack Obama
and United States. Note that there are entity tu-
ples (e1, e2) that participate in multiple relations,
r1, . . . , ri. In other words, the tuple (e1, e2) is the
object illustrated in Figure 2 and the different rela-
tion names are the labels. We define an entity men-
tion as a sequence of text tokens that matches the
corresponding entity name in some text, and relation
mention (for a given relation r(e1, e2)) as a pair of
entity mentions of e1 and e2 in the same sentence.
Relation mentions thus correspond to the instances
in Figure 2.1 As the latter definition indicates, we
focus on the extraction of relations expressed in a
single sentence. Furthermore, we assume that entity
mentions are extracted by a different process, such
as a named entity recognizer.
We define the task of relation extraction as a func-
tion that takes as input a document collection (C), a
set of entity mentions extracted from C (E), a set of
known relation labels (L) and an extraction model,
and outputs a set of relations (R) such that any of the
relations extracted is supported by at least one sen-
tence in C. To train the extraction model, we use a
database of relations (D) that are instantiated at least
once in C. Using distant supervision, D is aligned
with sentences in C, producing relation mentions for
all relations in D.
4 Model
Our model assumes that each relation mention in-
volving an entity pair has exactly one label, but al-
lows the pair to exhibit multiple labels across differ-
ent mentions. Since we do not know the actual re-
lation label of a mention in the distantly supervised
setting, we model it using a latent variable z that
can take one of the k pre-specified relation labels
as well as an additional NIL label, if no relation is
expressed by the corresponding mention. We model
the multiple relation labels an entity pair can assume
1For this reason, we use relation mention and relation in-
stance interchangeably in this paper.
. . .
. . . . . .
. . .
Figure 3: MIML model plate diagram. We unrolled the
y plate to emphasize that it is a collection of binary clas-
sifiers (one per relation label), whereas the z classifier is
multi-class. Each z and yj classifier has an additional
prior parameter, which is omitted here for clarity.
using a multi-label classifier that takes as input the
latent relation types of the all the mentions involving
that pair. The two-layer hierarchical model is shown
graphically in Figure 3, and is described more for-
mally below. The model includes one multi-class
classifier (for z) and a set of binary classifiers (for
each yj). The z classifier assigns latent labels from
L to individual relation mentions or NIL if no rela-
tion is expressed by the mention. Each yj classifier
decides if relation j holds for the given entity tu-
ple, using the mention-level classifications as input.
Specifically, in the figure:
? n is the number of distinct entity tuples in D;
? Mi is the set of mentions for the ith entity pair;
? x is a sentence and z is the latent relation clas-
sification for that sentence;
? wz is the weight vector for the multi-class
mention-level classifier;
? k is the number of known relation labels in L;
? yj is the top-level classification decision for the
entity pair as to whether the jth relation holds;
? wj is the weight vector for the binary top-level
classifier for the jth relation.
Additionally, we define Pi (Ni) as the set of all
known positive (negative) relation labels for the ith
entity tuple. In this paper, we construct Ni as L\Pi,
but, in general, other scenarios are possible. For
example, both Sun et al2011) and Surdeanu et
457
al. (2011a) proposed models where Ni for the ith tu-
ple (e1, e2) is defined as: {rj | rj(e1, ek) ? D, ek 6=
e2, rj /? Pi}, which is a subset of L\Pi. That is, en-
tity e2 is considered a negative example for relation
rj (in the context of entity e1) only if rj exists in the
training data with a different value.
The addition of the object-level layer (for y) is an
important contribution of this work. This layer can
capture information that cannot be modeled by the
mention-level classifier. For example, it can learn
that two relation labels (e.g., BornIn and SpouseOf)
cannot be generated jointly for the same entity tu-
ple. So, if the z classifier outputs both these la-
bels for different mentions of the same tuple, the y
layer can cancel one of them. Furthermore, the y
classifiers can learn when two labels tend to appear
jointly, e.g., CapitalOf and Contained between two
locations, and use this occurrence as positive rein-
forcement for these labels. We discuss the features
that implement these ideas in Section 5.
4.1 Training
We train the proposed model using hard discrimina-
tive Expectation Maximization (EM). In the Expec-
tation (E) step we assign latent mention labels us-
ing the current model (i.e., the mention and relation
level classifiers). In the Maximization (M) step we
retrain the model to maximize the log likelihood of
the data using the current latent assignments.
In the equations that follow, we refer to
w1, . . . ,wk collectively as wy for compactness.
The vector zi contains the latent mention-level clas-
sifications for the ith entity pair, while yi represents
the corresponding set of gold-standard labels (that
is, y(r)i = 1 if r ? Pi, and y
(r)
i = 0 for r ? Ni.)
Using these notations, the log-likelihood of the data
is given by:
LL(wy,wz) =
n?
i=1
log p(yi|xi,wy,wz)
=
n?
i=1
log
?
zi
p(yi, zi|xi,wy,wz)
The joint probability in the inner summation can be
broken up into simpler parts:
p(yi, zi|xi,wy,wz)
= p(zi|xi,wz)p(yi|zi,wy)
=
?
m?Mi
p(z(m)i |x
(m)
i ,wz)
?
r?Pi?Ni
p(y(r)i |zi,w
(r)
y )
where the last step follows from conditional inde-
pendence. Thus the log-likelihood for this problem
is not convex (it includes a sum of products). How-
ever, we can still use EM, but the optimization fo-
cuses on maximizing the lower bound of the log-
likelihood, i.e., we maximize the above joint proba-
bility for each entity pair in the database. Rewriting
this probability in log space, we obtain:
log p(yi, zi|xi,wy,wz) (1)
=
?
m?Mi
log p(z(m)i |x
(m)
i ,wz)+
?
r?Pi?Ni
log p(y(r)i |zi,w
(r)
y )
The algorithm proceeds as follows.
E-step: In this step we infer the mention-level
classifications zi for each entity tuple, given all its
mentions, the gold labels yi, and current model, i.e.,
wz and wy weights. Formally, we seek to find:
zi
? = argmax
z
p(z|yi,xi,wy,wz)
However it is computationally intractable to con-
sider all vectors z as there is an exponential num-
ber of possible assignments, so we approximate and
consider each mention separately. Concretely,
p(z(m)i |yi,xi,wy,wz)
? p(yi, z
(m)
i |xi,wy,wz)
? p(z(m)i |x
(m)
i ,wz)p(yi|z
?
i,wy)
= p(z(m)i |x
(m)
i ,wz)
?
r?Pi?Ni
p(y(r)i |z
?
i,w
(r)
y )
where z?i contains the previously inferred mention
labels for group i, with the exception of compo-
nent m whose label is replaced by z(m)i . So for
i = 1, . . . , n, and for each m ?Mi we calculate:
z(m)?i =argmax
z
p(z|x(m)i ,wz)? (2)
?
r?Pi?Ni
p(y(r)i |z
?
i,w
(r)
y )
458
Intuitively, the above equation indicates that men-
tion labels are chosen to maximize: (a) the prob-
abilities assigned by the mention-level model; (b)
the probability that the correct relation labels are as-
signed to the corresponding tuple; and (c) the prob-
ability that the labels known to be incorrect are not
assigned to the tuple. For example, if a particular
mention label receives a high mention-level proba-
bility but it is known to be a negative label for that
tuple, it will receive a low overall score.
M-step: In this step we find wy,wz that maxi-
mize the lower bound of the log-likelihood, i.e., the
probability in equation (1), given the current assign-
ments for zi. From equation (1) it is clear that this
can be maximized separately with respect to wy and
wz. Intuitively, this step amounts to learning the
weights for the mention-level classifier (wz) and the
weights for each of the k top-level classifiers (wy).
The updates are given by:
w?z = argmax
w
n?
i=1
?
m?Mi
log p(z(m)?i |x
(m)
i ,w) (3)
w(r)?y = argmax
w
?
1?i?n s.t. r?Pi?Ni
log p(y(r)i |z
?
i ,w) (4)
Note that these are standard updates for logistic re-
gression. We obtained these weights using k + 1
logistic classifiers: one multi-class classifier for wz
and k binary classifiers for each relation label r ? L.
We implemented all using the L2-regularized logis-
tic regression from the publicly-downloadable Stan-
ford CoreNLP package.2 The main difference be-
tween the classifiers is how features are generated:
the mention-level classifier computes its features
based on xi, whereas the relation-level classifiers
generate features based on the current assignments
for zi and the corresponding relation label r. We
discuss the actual features used in our experiments
in Section 5.
4.2 Inference
Given an entity tuple, we obtain its relation labels as
follows. We first classify its mentions:
z(m)?i = argmaxz
p(z|x(m)i ,wz) (5)
2nlp.stanford.edu/software/corenlp.shtml
then decide on the final relation labels using the top-
level classifiers:
y(r)?i = arg max
y?{0,1}
p(y|z?i ,w
(r)
y ) (6)
4.3 Implementation Details
We discuss next several details that are crucial for
the correct implementation of the above model.
Initialization: Since EM is not guaranteed to con-
verge at the global maximum of the observed data
likelihood, it is important to provide it with good
starting values. In our context, the initial values are
labels assigned to zi, which are required to compute
equation (2) in the first iteration (z?i). We generate
these values using a local logistic regression classi-
fier that uses the same features as the mention-level
classifier in the joint model but treats each relation
mention independently. We train this classifier using
?traditional? distant supervision: for each relation in
the databaseD we assume that all the corresponding
mentions are positive examples for the correspond-
ing label (Mintz et al2009). Note that this heuris-
tic repeats relation mentions with different labels for
the tuples that participate in multiple relations. For
example, all the relation mentions in Figure 1 will
yield datums with both the EmployedBy and BornIn
labels. Despite this limitation, we found that this is
a better initialization heuristic than random assign-
ment.
For the second part of equation (2), we initial-
ize the relation-level classifier with a model that
replicates the at least one heuristic of Hoffmann et
al. (2011). Each w(r)y model has a single feature with
a high positive weight that is triggered when label r
is assigned to any of the mentions in z?i .
Avoiding overfitting: A na??ve implementation of
our approach leads to an unrealistic training scenario
where the z classifier generates predictions (in equa-
tion (2)) for the same datums it has seen in training
in the previous iteration. To avoid this overfitting
problem we used cross validation: we divided the
training tuples in K distinct folds and trained K dif-
ferent mention-level classifiers. Each classifier out-
puts p(z|x(m)i ,wz) for tuples in a given fold during
the E-step (equation (2)) and is trained (equation (3))
using tuples from all other folds.
459
At testing time, we compute p(z|x(m)i ,wz) in
equation (5) as the average of the probabilities of
the above set of mention classifiers:
p(z|x(m)i ,wz) =
?K
j=1 p(z|x
(m)
i ,w
j
z)
K
where wjz are the weights of the mention classifier
responsible for fold j. We found that this simple
bagging model performs slightly better in practice
(a couple of tenths of a percent) than training a sin-
gle mention classifier on the latent mention labels
generated in the last training iteration.
Inference during training: During the inference
process in the E-step, the algorithm incrementally
?flips? mention labels based on equation (2), for
each group of mentions Mi. Thus, z?i changes as the
algorithm progresses, which may impact the label
assigned to the remaining mentions in that group. To
avoid any potential bias introduced by the arbitrary
order of mentions as seen in the data, we randomize
each group Mi before we inspect its mentions.
5 Experimental Results
5.1 Data
We evaluate our algorithm on two corpora. The first
was developed by Riedel et al2010) by aligning
Freebase3 relations with the New York Times (NYT)
corpus. They used the Stanford named entity recog-
nizer (Finkel et al2005) to find entity mentions in
text and constructed relation mentions only between
entity mentions in the same sentence.
Riedel et al2010) observes that evaluating on
this corpus underestimates true extraction accuracy
because Freebase is incomplete. Thus, some re-
lations extracted during testing will be incorrectly
marked as wrong, simply because Freebase has no
information on them. To mitigate this issue, Riedel
et al2010) and Hoffman et al2011) perform a
second evaluation where they compute the accuracy
of labels assigned to a set of relation mentions that
they manually annotated. To avoid any potential an-
notation biases, we instead evaluate on a second cor-
pus that has comprehensive annotations generated
by experts for all test relations.
We constructed this second dataset using mainly
resources distributed for the 2010 and 2011 KBP
3freebase.com
shared tasks (Ji et al2010; Ji et al2011). We gen-
erated training relations from the knowledge base
provided by the task organizers, which is a subset
of the English Wikipedia infoboxes from a 2008
snapshot. Similarly to the corpus of Riedel et al
these infoboxes contain open-domain relations be-
tween named entities, but with a different focus.
For example, more than half of the relations in
the evaluation data are alternate names of organi-
zations or persons (e.g., org:alternate names) or re-
lations associated with employment and member-
ship (e.g., per:employee of) (Ji et al2011). We
aligned these relations against a document collec-
tion that merges two distinct sources: (a) the col-
lection provided by the shared task, which contains
approximately 1.5 million documents from a vari-
ety of sources, including newswire, blogs and tele-
phone conversation transcripts; and (b) a complete
snapshot of the English Wikipedia from June 2010.
During training, for each entity tuple (e1, e2), we
retrieved up to 50 sentences that contain both en-
tity mentions.4 We used Stanford?s CoreNLP pack-
age to find entity mentions in text and, similarly to
Riedel et al2010), we construct relation mention
candidates only between entity mentions in the same
sentence. We analyzed a set of over 2,000 relation
mentions and we found that 39% of the mentions
where e1 is an organization name and 36% of men-
tions where e1 is a person name do not express the
corresponding relation.
At evaluation time, the KBP shared task requires
the extraction of all relations r(e1, e2) given a query
that contains only the first entity e1. To accommo-
date this setup, we adjusted our sentence extraction
component to use just e1 as the retrieval query and
we kept up to 50 sentences that contain a mention
of the input entity for each evaluation query. For
tuning and testing we used the 200 queries from the
2010 and 2011 evaluations. We randomly selected
40 queries for development and used the remaining
160 for the formal evaluation.
To address the large number of negative examples
in training, Riedel et alubsampled them randomly
with a retention probability of 10%. For the KBP
corpus, we followed the same strategy, but we used
4Sentences were ranked using the similarity between their
parent document and the query that concatenates the two entity
names. We used the default Lucene similarity measure.
460
# of gold # of gold % of gold entity tuples % of gold entity tuples % of mentions that
relations relations with more than one label with multiple mentions in text do not express # of relation labels
in training in testing in training in training their relation
Riedel 4,700 1,950 7.5% 46.4% up to 31% 51
KBP 183,062 3,334 2.8% 65.1% up to 39% 41
Table 1: Statistics about the two corpora used in this paper. Some of the numbers for the Riedel dataset is from (Riedel
et al2010; Hoffmann et al2011).
a subsampling probability of 5% because this led to
the best results in development for all models.
Table 1 provides additional statistics about the
two corpora. The table indicates that having mul-
tiple mentions for an entity tuple is a very common
phenomenon in both corpora, and that having mul-
tiple labels per tuple is more common in the Riedel
dataset than KBP (7.5% vs. 2.8%).
5.2 Features
Our model requires two sets of features: one for the
mention classifier (z) and one for the relation clas-
sifier (y). In the Riedel dataset, we used the same
features as Riedel et al2010) and Hoffmann et
al. (2011) for the mention classifier. In the KBP
dataset, we used a feature set that was developed in
our previous work (Surdeanu et al2011b). These
features can be grouped in three classes: (a) features
that model the two entities, such as their head words;
(b) features that model the syntactic context of the
relation mention, such as the dependency path be-
tween the two entity mentions; and (c) features that
model the surface context, such as the sequence of
part of speech tags between the two entity mentions.
We used these features for all the models evaluated
on the KBP dataset.5
For the relation-level classifier, we developed two
feature groups. The first models Hoffmann et al
at least one heuristic using a single feature, which
is set to true if at least one mention in zi has the la-
bel r, which is modeled by the current relation clas-
sifier. The second group models the dependencies
between relation labels. This is implemented by a
set of |L| ? 1 features, where feature j is instan-
tiated whenever the label modeled (r) is predicted
jointly with another label rj (rj ? L, rj 6= r) in zi.
These features learn both positive and negative re-
inforcements between labels. For example, if labels
5To avoid an excessive number of features in the KBP exper-
iments, we removed features seen less than five times in train-
ing.
r1 and r2 tend to be generated jointly, the feature for
the corresponding dependency will receive a posi-
tive weight in the models for r1 and r2. Similarly, if
r1 and r2 cannot be generated jointly, the model will
assign a negative weight to feature 2 in r1?s classi-
fier and to feature 1 in r2?s classifier. Note that this
feature is asymmetric, i.e., feature 1 in r2?s classi-
fier may have a different value than feature 2 in r1?s
classifier, depending on the accuracy of the individ-
ual predictions for r1 and r2.
5.3 Baselines
We compare our approach against three models:
Mintz++ ? This is the model used to initialize the
mention-level classifier in our model. As discussed
in Section 4.3, this model follows the ?traditional?
distant supervision heuristic, similarly to (Mintz et
al., 2009). However, our implementation has several
advantages over the original model: (a) we model
each relation mention independently, whereas Mintz
et alollapsed all the mentions of the same entity
tuple into a single datum; (b) we allow multi-label
outputs for a given entity tuple at prediction time
by OR-ing the predictions for the individual rela-
tion mentions corresponding to the tuple (similarly
to (Hoffmann et al2011))6; and (c) we use the
simple bagging strategy described in Section 4.3 to
combine multiple models. Empirically, we observed
that these changes yield a significant improvement
over the original proposal. For this reason, we con-
sider this model a strong baseline on its own.
Riedel ? This is the ?at-least-once? model reported
in (Riedel et al2010), which had the best perfor-
mance in that work. This approach models the task
as a multi-instance single-label problem. Note that
this is the only model shown here that does not allow
multi-label outputs for an entity tuple.
6We also allow multiple labels per tuple at training time,
in which case we replicate the corresponding datum for each
label. However, this did not improve performance significantly
compared to selecting a single label per datum during training.
461
Hoffmann ? This is the ?MultiR? model, which per-
formed the best in (Hoffmann et al2011). This
models RE as a MIML problem, but learns using
a Perceptron algorithm and uses a deterministic ?at
least one? decision instead of a relation classifier.
We used Hoffman?s publicly released code7 for the
experiments on the Riedel dataset and our own im-
plementation for the KBP experiments.8
5.4 Results
We tuned all models using three-fold cross valida-
tion for the Riedel dataset and using the develop-
ment queries for the KBP dataset. MIML-RE has
two parameters that require tuning: the number of
EM epochs (T ) and the number of folds for the men-
tion classifiers (K).9 The values obtained after tun-
ing are T = 15,K = 5 for the Riedel dataset and
T = 8,K = 3 for KBP. Similarly, we tuned the
number of epochs for the Hoffmann model on the
KBP dataset, obtaining an optimal value of 20.
On the Riedel dataset we evaluate all models us-
ing standard precision and recall measures. For the
KBP evaluation we used the official KBP scorer,10
with two changes: (a) we score with the parame-
ter anydoc set to true, which configures the scorer
to accept relation mentions as correct regardless of
their supporting document; and (b) we score only
on the subset of gold relations that have at least one
mention in our sentences. The first decision is neces-
sary because the gold KBP answers contain support-
ing documents only from the corpus provided by the
organizers but we retrieve candidate answers from
multiple collections. The second is required because
the focus of this work is not on sentence retrieval but
on RE, which should be evaluated in isolation.11
Similarly to previous work, we report preci-
sion/recall curves in Figure 4. We evaluate two
variants of MIML-RE: one that includes all the
features for the y model, and another (MIML-RE
7cs.washington.edu/homes/raphaelh/mr/
8The decision to reimplement the Hoffmann model was a
practical one, driven by incompatibilities between their imple-
mentation and our KBP framework.
9We could also tune the prior parameters for both our model
and Mintz++, but we found in early experiments that the default
value of 1 yields the best scores for all priors.
10nlp.cs.qc.cuny.edu/kbp/2011/scoring.html
11Due to these changes, the scores reported in this paper are
not directly comparable with the shared task scores.
At-Least-One) which has only the at least one
feature. For all the Bayesian models implemented
here, we sorted the predicted relations by the noisy-
or score of the top predictions for their mentions.
Formally, we rank a relation r predicted for group i,
i.e., r ? y?i , using:
noisyOri(r) = 1?
?
m?Mi
(1? s(m)i (r))
where s(m)i (r) = p(r|x
(m)
i ,wz) if r = z
(m)?
i or 0 oth-
erwise. The noisy-or formula performs well for
ranking because it integrates model confidence (the
higher the probabilities, the higher the score) and re-
dundancy (the more mentions are predicted with a
label, the higher that label?s score). Note that the
above ranking score does not include the probability
of the relation classifier (equation (6)) for MIML-RE.
While we use equation (6) to generate y?i , we found
that the corresponding probabilities are too coarse
to provide a good ranking score. This is caused by
the fact that our relation-level classifier works with
a small number of (noisy) features. Lastly, for our
implementation of the Hoffmann et alodel, we
used their ranking heuristic (sorting predictions by
the maximum extraction score for that relation).
6 Discussion
Figure 4 indicates that MIML-RE generally outper-
forms the current state of the art. In the Riedel
dataset, MIML-RE has higher overall recall than the
Riedel et alodel, and, for the same recall point,
MIML-RE?s precision is between 2 and 15 points
higher. For most of the curve, our model obtains
better precision for the same recall point than the
Hoffmann model, which currently has the best re-
ported results on this dataset. The difference is as
high as 5 precision points around the middle of the
curve. The Hoffmann model performs better close to
the extremities of the curve (low/high recall). Nev-
ertheless, we argue that our model is more stable
than Hoffmann?s: MIML-RE yields a smoother pre-
cision/recall curve, without most of the depressions
seen in the Hoffmann results. In the KBP dataset,
MIML-RE performs consistently better than our im-
plementation of Hoffmann?s model, with higher pre-
cision values for the same recall point, and much
higher overall recall. We believe that these dif-
ferences are caused by our Bayesian framework,
462
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  0.05  0.1  0.15  0.2  0.25  0.3
Pr
ec
isi
on
Recall
Hoffmann
Riedel
Mintz++
MIML-RE
MIML-RE At-Least-One
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  0.05  0.1  0.15  0.2  0.25  0.3
Pr
ec
isi
on
Recall
Hoffmann (our implementation)
Mintz++
MIML-RE
MIML-RE At-Least-One
Figure 4: Results in the Riedel dataset (top) and the KBP dataset (bottom). The Hoffmann scores in the KBP dataset
were generated using our implementation. The other Hoffmann and Riedel results were taken from their papers.
which provides a more formal implementation of the
MIML problem.
Figure 4 also indicates that MIML-RE yields a con-
sistent improvement over Mintz++ (with the excep-
tion of a few points in the low-recall portion of the
KBP curves). The difference in precision for the
same recall point is as high as 25 precision points in
the Riedel dataset and up to 5 points in KBP. Over-
all, the best F1 score of MIML-RE is slightly over 1
point higher than the best F1 score of Mintz++ in
the Riedel dataset and 3 points higher in KBP. Con-
sidering that Mintz++ is a strong baseline and we
evaluate on two challenging domains, we consider
these results proof that the correct modeling of the
MIML scenario is beneficial.
Lastly, Figure 4 shows that MIML-RE outper-
forms its variant without label-dependency fea-
tures (MIML-RE At-Least-One) in the higher-
recall part of the curve in the Riedel dataset. The im-
provement is approximately 1 F1 point throughout
the last segment of the curve. The overall increase
in F1 was found to be significant (p = 0.0296) in a
one-sided, paired t-test over randomly sampled test
data. We see a smaller improvement in KBP (con-
centrated around the middle of the curve), likely be-
cause the number of entity tuples with multiple la-
bels in training is small (see Table 1). Neverthe-
less, this exercise shows that, when dependencies
between labels exist in a dataset, modeling them,
which can be trivially done in MIML-RE, is useful.
463
P R F1
Hoffmann (our implementation) 48.6 29.8 37.0
Mintz++ 43.8 36.8 40.0
MIML-RE 64.8 31.6 42.6
MIML-RE At-Least-One 56.1 32.5 41.1
Table 2: Results at the highest F1 point in the preci-
sion/recall curve on the dataset that contains groups with
at least 10 mentions.
In a similar vein, we tested the models previ-
ously described on a subset of the Riedel evalua-
tion dataset that only includes groups with at least
10 mentions. This corpus contains approximately
2% of the groups from the original testing partition,
out of which 90 tuples have at least one known label
and 1410 groups serve as negative examples.
For conciseness, we do not include the entire
precision/recall curves for this experiment, but sum-
marize them in Table 2, which lists the performance
peak (highest F1 score) for each of the models
investigated. The table shows that MIML-RE obtains
the highest F1 score overall, 1.5 points higher than
MIML-RE At-Least-One and 2.6 points higher
than Mintz++. More importantly, for approximately
the same recall point, MIML-RE obtains a precision
that is over 8 percentage points higher than that of
MIML-RE At-Least-One. A post-hoc inspection
of the results indicates that, indeed, MIML-RE suc-
cessfully eliminates undesired labels when two
(or more) incompatible labels are jointly assigned
to the same tuple. Take for example the tuple
(Mexico City, Mexico), for which the correct re-
lation is /location/administrative division/country.
MIML-RE At-Least-One incorrectly predicts
the additional /location/location/contains relation,
while MIML-RE does not make this prediction
because it recognizes that these two labels are in-
compatible in general: one location cannot both be
within another location and contain it. Indeed, ex-
amining the weights assigned to label-dependency
features in MIML-RE, we see that the model has
assigned a large negative weight to the depen-
dency feature between /location/location/contains
and /location/administrative division/country
for the /location/location/contains class. We
also observe positive dependencies between la-
bels. For example, MIML-RE learns that the
relations /people/person/place lived and /peo-
ple/person/place of birth tend to co-occur and
assigns a positive weight to this dependency feature
for the corresponding classes.
These results strongly suggest that when all as-
pects of the MIML scenario are present, our model
can successfully capture them and make use of the
additional structure to improve performance.
7 Conclusion
In this paper we showed that distant supervision
for RE, which generates training data by aligning a
database of facts with text, poses a distinct multi-
instance multi-label learning scenario. In this set-
ting, each entity pair to be modeled typically has
multiple instances in the text and may have multiple
labels in the database. This is considerably differ-
ent from traditional supervised learning, where each
instance has a single, explicit label.
We argued that this MIML scenario should be
formally addressed. We proposed, to our knowl-
edge, the first approach that models all aspects of the
MIML setting, i.e., the latent assignment of labels to
instances and dependencies between labels assigned
to the same entity pair.
We evaluated our model on two challenging do-
mains and obtained state-of-the-art results on both.
Our model performs well even when not all aspects
of the MIML scenario are common, and as seen in
the discussion, shows significant improvement when
evaluated on entity pairs with many labels or men-
tions. When all aspects of the MIML scenario are
present, our model is well-equipped to handle them.
The code and data used in the experiments re-
ported in this paper are available at: http://nlp.
stanford.edu/software/mimlre.shtml.
Acknowledgments
We gratefully acknowledge the support of Defense Ad-
vanced Research Projects Agency (DARPA) Machine
Reading Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181. Any
opinions, findings, and conclusion or recommendations
expressed in this material are those of the author(s) and
do not necessarily reflect the view of the DARPA, AFRL,
or the US government. We gratefully thank Raphael
Hoffmann and Sebastian Riedel for sharing their code
and data and for the many useful discussions.
464
References
Kedar Bellare and Andrew McCallum. 2007. Learn-
ing extractors from unlabeled text using relevant
databases. In Proceedings of the Sixth International
Workshop on Information Extraction on the Web.
Carla Brodley and Mark Friedl. 1999. Identifying mis-
labeled training data. Journal of Artificial Intelligence
Research (JAIR).
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal super-
vision. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh Inter-
national Conference on Intelligent Systems for Molec-
ular Biology.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics (ACL).
Heng Ji, Ralph Grishman, Hoa T. Dang, Kira Griffitt, and
Joe Ellis. 2010. Overview of the TAC 2010 knowl-
edge base population track. In Proceedings of the Text
Analytics Conference.
Heng Ji, Ralph Grishman, and Hoa T. Dang. 2011.
Overview of the TAC 2011 knowledge base popula-
tion track. In Proceedings of the Text Analytics Con-
ference.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics.
Truc Vien T. Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant supervi-
sion from external semantic repositories. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD ?10).
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New York University 2011 system for KBP slot
filling. In Proceedings of the Text Analytics Confer-
ence.
Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-
Closky, Angel X. Chang, Valentin I. Spitkovsky, and
Christopher D. Manning. 2011a. Stanford?s distantly-
supervised slot-filling system. In Proceedings of the
Text Analytics Conference.
Mihai Surdeanu, David McClosky, Mason R. Smith, An-
drey Gusev, and Christopher D. Manning. 2011b.
Customizing an information extraction system to a
new domain. In Proceedings of the Workshop on Re-
lational Models of Semantics, Portland, Oregon, June.
Fei Wu and Dan Weld. 2007. Autonomously semanti-
fying Wikipedia. In Proceedings of the International
Conference on Information and Knowledge Manage-
ment (CIKM).
Z.H. Zhou and M.L. Zhang. 2007. Multi-instance multi-
label learning with application to scene classification.
In Advances in Neural Information Processing Sys-
tems (NIPS).
465
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 489?500, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Joint Entity and Event Coreference Resolution across Documents
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai Surdeanu, Dan Jurafsky
Stanford University, Stanford, CA 94305
{heeyoung,recasens,angelx,mihais,jurafsky}@stanford.edu
Abstract
We introduce a novel coreference resolution
system that models entities and events jointly.
Our iterative method cautiously constructs
clusters of entity and event mentions using lin-
ear regression to model cluster merge opera-
tions. As clusters are built, information flows
between entity and event clusters through fea-
tures that model semantic role dependencies.
Our system handles nominal and verbal events
as well as entities, and our joint formulation
allows information from event coreference to
help entity coreference, and vice versa. In a
cross-document domain with comparable doc-
uments, joint coreference resolution performs
significantly better (over 3 CoNLL F1 points)
than two strong baselines that resolve entities
and events separately.
1 Introduction
Most coreference resolution systems focus on enti-
ties and tacitly assume a correspondence between
entities and noun phrases (NPs). Focusing on NPs
is a way to restrict the challenging problem of coref-
erence resolution, but misses coreference relations
like the one between hanged and his suicide in (1),
and between placed and put in (2).
1. (a) One of the key suspected Mafia bosses ar-
rested yesterday has hanged himself.
(b) Police said Lo Presti had hanged himself.
(c) His suicide appeared to be related to clan feuds.
2. (a) The New Orleans Saints placed Reggie Bush
on the injured list on Wednesday.
(b) Saints put Bush on I.R.
As (1c) shows, NPs can also refer to events, and
so corefer with phrases other than NPs (Webber,
1988). By being anchored in spatio-temporal dimen-
sions, events represent the most frequent referent of
verbal elements. In addition to time and location,
events are characterized by their participants or ar-
guments, which often correspond with discourse en-
tities. This two-way feedback between events and
their arguments (or entities) is the core of our ap-
proach. Since arguments play a key role in describ-
ing an event, knowing that two arguments corefer
is useful for finding coreference relations between
events, and knowing that two events corefer is use-
ful for finding coreference relations between enti-
ties. In (1), the coreference relation between One
of the key suspected Mafia bosses arrested yesterday
and Lo Presti can be found by knowing that their
predicates (i.e., has hanged and had hanged) core-
fer. On the other hand, the coreference relations be-
tween the arguments Saints and Bush in (2) helps
to determine the coreference relation between their
predicates placed and put.
In this paper, we take a holistic approach to coref-
erence. We annotate a corpus with cross-document
coreference relations for nominal and verbal men-
tions. We focus on both intra and inter-document
coreference because this scenario is at the same time
more challenging and more relevant to real-world
applications such as news aggregation. We use this
corpus to train a model that jointly addresses refer-
ences to both entities and events across documents.
The contributions of this work are the following:
? We introduce a novel approach for entity and
event coreference resolution. At the core of
489
our approach is an iterative algorithm that cau-
tiously constructs clusters of entity and event
mentions using linear regression to model clus-
ter merge operations. Importantly, our model
allows information to flow between clusters of
both types through features that model context
using semantic role dependencies.
? We annotate and release a new corpus with
coreference relations between both entities and
events across documents. The relations anno-
tated are both intra and inter-document, which
more accurately models real-world scenarios.
? We evaluate our cross-document coreference
resolution system on this corpus and show that
our joint approach significantly outperforms
two strong baselines that resolve entities and
events separately.
2 Related Work
Entity coreference resolution is a well studied prob-
lem with many successful techniques for identify-
ing mention clusters (Ponzetto and Strube, 2006;
Haghighi and Klein, 2009; Stoyanov et al2009;
Haghighi and Klein, 2010; Raghunathan et al2010;
Rahman and Ng, 2011, inter alia). Most of these
techniques focus on matching compatible noun pairs
using various syntactic and semantic features, with
efforts targeted toward improving features and clus-
tering models.
Prior work showed that models that jointly resolve
mentions across multiple entities result in better per-
formance than simply resolving mentions in a pair-
wise fashion (Denis and Baldridge, 2007; Poon and
Domingos, 2008; Wick et al2008; Lee et al2011,
inter alia). A natural extension is to perform coref-
erence jointly across both entities and events. Yet
there has been little attempt in this direction.
We know of only limited work that incorporates
event-related information in entity coreference, typ-
ically by incorporating the verbs in context as fea-
tures. For instance, Haghighi and Klein (2010) in-
clude the governor of the head of nominal mentions
as features in their model. Rahman and Ng (2011)
also used event-related information by looking at
which semantic role the entity mentions can have
and the verb pairs of their predicates. We confirm
that such features are useful but also show that the
complementary features for verbal mentions lead to
even better performance, especially when event and
entity clusters are jointly modeled.
Compared to the extensive work on entity coref-
erence, the related problem of event coreference re-
mains relatively under-explored, with minimal work
on how entity and event coreference can be con-
sidered jointly on an open domain. Early work on
event coreference for MUC (Humphreys et al1997;
Bagga and Baldwin, 1999) focused on scenario-
specific events. More recently, there have been
approaches that looked at event coreference for
wider domains. Chen and Ji (2009) proposed us-
ing spectral graph clustering to cluster events. Be-
jan and Harabagiu (2010) proposed a nonparamet-
ric Bayesian model for open-domain event resolu-
tion. However, most of this prior work focused only
on event coreference, whereas we address both en-
tities and events with a single model. Humphreys
et al1997) considered entities as well as events,
but due to the lack of a corpus annotated with event
coreference, their approach was only evaluated im-
plicitly in the MUC-6 template filling task. To our
knowledge, the only previous work that considered
entity and event coreference resolution jointly is
He (2007), but limited to the medical domain and
focused on just five semantic categories.
3 Architecture
Following the intuition introduced in Section 1, our
approach iteratively builds clusters of event and en-
tity mentions jointly. As more information becomes
available (e.g., finding out that two verbal mentions
have arguments that belong to the same entity clus-
ter), the features of both entity and event mentions
are re-generated, which prompts future clustering
operations. Our model follows a cautious (or ?baby
steps?) approach, which we previously showed to be
successful for entity coreference resolution (Raghu-
nathan et al2010; Lee et al2011). However,
unlike our previous work, which used deterministic
rules, in this paper we learn a coreference resolution
model using linear regression. Algorithm 1 summa-
rizes the flow of the proposed algorithm. We detail
its steps next. We describe the training procedure in
Section 4 and the features used in Section 5.
490
Algorithm 1: Joint Coreference Resolution
input : set of documents D
input : coreference model ?
// clusters of mentions:
E= {}1
// clusters of documents:
C = clusterDocuments(D)2
foreach document cluster c in C do3
// all mentions in one doc cluster:
M = extractMentions(c)4
// singleton mention clusters:
E ? = buildSingletonClusters(M)5
// high-precision deterministic sieves:
E ? = applyHighPrecisionSieves(E ?)6
// iterative event/entity coreference:
while ? e1, e2 ? E ?s.t. score(e1, e2,?) > 0.5 do7
(e1, e2) = arg max e1,e2?E? score(e1, e2,?)8
E ? = merge(e1, e2, E ?)9
// pronoun sieve:
E ? = applyPronounSieve(E ?)10
// append to global output:
E = E + E ?11
output : E
3.1 Document Clustering
Our approach starts with several steps that reduce
the search space for the actual coreference resolution
task. The first is document clustering, which clusters
the set of input documents (D) into a set of docu-
ment clusters (C). In the subsequent steps we only
cluster mentions that appear in the same document
cluster. We found this to be very useful in practice
because, in addition to reducing the search space, it
provides a word sense disambiguation mechanism
based on corpus-wide topics. For example, with-
out document clustering, our algorithm may decide
to cluster two mentions of the verb hit, but know-
ing that one belongs to a cluster containing earth-
quake reports and the other to a cluster with reports
on criminal activities, this decision can be avoided.1
Any non-parametric clustering algorithm can be
used in this step. In this paper, we used the algo-
rithm proposed by Surdeanu et al2005). This algo-
rithm is an Expectation Maximization (EM) variant
where the initial points (and the number of clusters)
are selected from the clusters generated by a hierar-
chical agglomerative clustering algorithm using ge-
1Since different mentions of the verb say in the same topic
might refer to different events, they are only merged if they have
coreferent arguments.
ometric heuristics. This algorithm performs well on
our data. For example, in the training dataset, only
two topics (handling different earthquake events) are
incorrectly merged into the same cluster.
3.2 Mention Extraction
In this step (4 in Algorithm 1) we extract nominal,
pronominal, and verbal mentions. We extract nom-
inal and pronominal mentions using the mention
identification component in the publicly download-
able Stanford coreference resolution system (Raghu-
nathan et al2010; Lee et al2011). We consider
as verbal mentions all words whose part of speech
starts with VB, with the exception of some auxil-
iary/copulative verbs (have, be and seem). For each
of the identified mentions we build a singleton clus-
ter (step 5 in Algorithm 1).
Crucially, we do not make a formal distinction be-
tween entity and event mentions. This distinction is
not trivial to implement (e.g., is the noun earthquake
an entity or an event mention?) and an imperfect
classification would negatively affect the following
coreference resolution. Instead, we simply classify
mentions into verbal or nominal, and use this dis-
tinction later during feature generation (Section 5).
To compare event nouns (e.g., development) with
verbal mentions, the ?derivationally related form?
relation in WordNet is used.
3.3 High-precision Entity Resolution Sieves
To further reduce the problem?s search space, in
step 6 of Algorithm 1 we apply a set of high-
precision filters from the Stanford coreference res-
olution system. This system is a collection of deter-
ministic models (or ?sieves?) for entity coreference
resolution that incorporate lexical, syntactic, seman-
tic, and discourse information. These sieves are ap-
plied from higher to lower precision. As clusters are
built, information such as mention gender and num-
ber is propagated across mentions in the same clus-
ter, which helps subsequent decisions. The Stanford
system obtained the highest score at the CoNLL-
2011 shared task on English coreference resolution.
For this step, we selected all the sieves from the
Stanford system with the exception of the pronoun
resolution sieve. All the remaining sieves (listed
in Table 1) have high precision because they em-
ploy linguistic heuristics with little ambiguity, e.g.,
491
High-precision sieves
Discourse processing sieve
Exact string match sieve
Relaxed string match sieve
Precise constructs sieve (e.g., appositives)
Strict head match sieves
Proper head noun match sieve
Relaxed head matching sieve
Table 1: Deterministic sieves in step 6 of Algorithm 1.
one sieve clusters together two entity mentions only
when they have the same head word. Note that all
these heuristics were designed for within-document
coreference. They work well in our context be-
cause we apply them in individual document clus-
ters, where the one-sense-per-discourse principle
still holds (Yarowsky, 1995).
Importantly, these sieves do not address verbal
mentions. That is, all verbal mentions are still in sin-
gleton clusters after this step. Furthermore, none of
these sieves use features that facilitate the joint reso-
lution of nominal and verbal mentions (e.g., features
from semantic role frames). All these limitations are
addressed next.
3.4 Iterative Entity/Event Resolution
In this stage (steps 7 ? 9 in Algorithm 1), we con-
struct entity and event clusters using a cautious or
?baby steps? approach. We use a single linear re-
gressor (?) to model cluster merge operations be-
tween both verbal and nominal clusters. Intuitively,
the linear regressor models the quality of the merge
operation, i.e., a score larger than 0.5 indicates that
more than half of the mention pairs introduced by
this merge are correct. We discuss the training pro-
cedure that yields this scoring function in Section 4.
In each iteration, we perform the merge operation
that has the highest score. Once two clusters are
merged (step 9) we regenerate all the mention fea-
tures to reflect the current clusters. We stop when no
merging operation with an overall benefit is found.
This iterative procedure is the core of our joint
coreference resolution approach. This algorithm
transparently merges both entity and event men-
tions and, importantly, allows information to flow
between clusters of both types as merge operations
take place. For example, assume that during iter-
ation i we merge the two hanged verbs in the first
example in Section 1 (because they have the same
lemma). Because of this merge, in iteration i+ 1 the
nominal mentions Lo Presti and One of the key sus-
pected Mafia bosses have the same semantic role for
verbs assigned to the same cluster. This is a strong
hint that these two nominal mentions belong to the
same cluster. Indeed, the feature that models this
structure received one of the highest weights in our
linear regression model (see Section 7).
3.5 Pronoun Sieve
Our approach concludes with the pronominal coref-
erence resolution sieve from the Stanford system.
This sieve is necessary because our current reso-
lution algorithm ignores mention ordering and dis-
tance (i.e., in step 7 we compare all clusters regard-
less of where their mentions appear in the text). As
previous work has proved, the structure of the text is
crucial for pronominal coreference (Hobbs, 1978).
For this reason, we handle pronouns outside of the
main algorithm block.
4 Training the Cluster Merging Model
Two observations drove our choice of model and
training algorithm. First, modeling the merge op-
eration as a classification task is not ideal, because
only a few of the resulting clusters are entirely cor-
rect or incorrect. In practice, most of the clusters
will contain some mention pairs that are correct and
some that are not. Second, generating training data
for the merging model is not trivial: a brute force
approach that looks at all the possible combinations
is exponential in the number of mentions. This is
both impractical and unnecessary, as some of these
combinations are unlikely to be seen in practice.
We address these observations with Algorithm 2.
The algorithm uses gold coreference labels to train a
linear regressor that models the quality of the clus-
ters produced by merge operations. We define the
quality score q of a new cluster as the percentage of
new mention pairs (i.e., not present in either one of
the clusters to be merged) that are correct:
q =
linkscorrect
linkscorrect + linksincorrect
(1)
where links(in)correct is the number of newly intro-
duced (in)correct pairwise mention links when two
clusters are merged.
492
Algorithm 2: Training Procedure
input : set of documents D
input : correct mention clusters G
C = clusterDocuments(D)1
// linear regression coreference model:
? = assignInitialWeights(C,G)2
// repeat for T epochs:
for t = 1 to T do3
// training data for linear regressor:
? = {}4
foreach document cluster c in C do5
M = extractMentions(c)6
E = buildSingletonClusters(M)7
E = applyHighPrecisionSieves(E)8
// gather training examples
// as clusters are built:
while ? e1, e2 ? Es.t. sco(e1, e2,?) > 0.5 do9
forall e?1, e
?
2 ? E do10
q = qualityOfMerge(e?1, e
?
2,G)11
? = append(e?1, e
?
2, q,?)12
(e1, e2) = arg max e1,e2?E sco(e1, e2,?)13
E = merge(e1, e2, E)14
// train using data from last epoch:
?? = trainLinearRegressor(?)15
// interpolate with older model:
? = ?? + (1? ?)??16
output : ?
We address the potential explosion in training data
size by considering only merge operations that are
likely to be inspected by the algorithm as it runs.
To achieve this, Algorithm 2 repeatedly runs the ac-
tual clustering algorithm (as given by the current
model ?) over the training dataset (steps 5 ? 14).2
When the algorithm iteratively constructs its clus-
ters (steps 9 ? 14), we generate training data from
all possible cluster pairs available during a particular
iteration (steps 10 ? 12). For each pair, we compute
its score using Equation 1 (step 11) and add it to the
training corpus ? (step 12). Note that this avoids in-
specting many of the possible cluster combinations:
once a cluster is built (e.g., during the previous iter-
ations or by the deterministic sieves in step 8), we
do not generate training data from its members, but
rather treat it as an atomic unit. On the other hand,
our approach generates more training data than on-
line learning, which trains using only the actual de-
cisions taken during inference in each iteration (i.e.,
2We skip the pronoun sieve here because it does not affect
the decisions taken during the iterative resolution steps.
the pair (e1, e2) in step 13).
After each epoch we have a new training cor-
pus ?, which we use to train the new linear regres-
sion model ?? (step 15), which is then interpolated
with the old one (step 16).
Our training procedure is similar in spirit to trans-
formation based learning (TBL) (Brill, 1995). Sim-
ilarly to TBL, our approach repeatedly applies the
model over the training data and attempts to mini-
mize the error rate of the current model. However,
while TBL learns rules that directly minimize the
current error rate, our approach achieves this indi-
rectly, by incorporating the reduction in error rate in
the score of the generated datums. This allows us
to fit a linear regression to this task, which, as dis-
cussed before, is a better model for this task.
Just like any hill-climbing algorithm, our ap-
proach has the risk of converging to a local max-
imum. To mitigate this risk, we do not initialize
our model ? with random weights, but rather use
hints from the deterministic sieves. This procedure
(listed in step 2) runs the high-precision sieves in-
troduced in Section 3.3 and, just like the data gen-
eration loop in Algorithm 2, creates training exam-
ples from the clusters available after every merge
operation. Since these deterministic models address
only nominal clusters, at the end we generate train-
ing data for events by inspecting all the pairs of sin-
gleton verbal clusters. Using this data, we train the
initial linear regression model.
We trained our model using L2 regularized linear
regression with a regularization coefficient of 1.0.
We did not tune the regularization coefficient. We
ran the training algorithm for 10 epochs, although
we observed minimal changes after three epochs.
We tuned the interpolation weight (?) to a value
of 0.7 using our development corpus.
5 Features
We list in Table 2 the features used by the lin-
ear regression model. As the table indicates, our
feature set relies heavily on semantic roles, which
were extracted using the SwiRL semantic role la-
beling (SRL) system (Surdeanu et al2007).3 Be-
cause SwiRL addresses only verbal predicates, we
extended it to handle nominal predicates. In this
3http://www.surdeanu.name/mihai/swirl/
493
Feature Name
Applies to
Entities (E)
or Events (V)
Description and Example
Entity Heads E
Cosine similarity of the head-word vectors of two clusters. The head-word vector
stores the head words of all mentions in a cluster and their frequencies. For example,
the vector for the three-mention cluster {Barack Obama, President Obama, US
president}, is {Obama:2, president:1}.
Event Lemmas V
Cosine similarity of the lemma vectors of two clusters. For example, the lemma
vector for the cluster {murdered, murders, hitting} is {murder:2, hit:1}.
Links between
Synonyms
E, V
The percentage of newly-introduced mention links after the merge that are WordNet
synonyms (Fellbaum, 1998). For example, when merging the following two clus-
ters, {hit, strike} and {strike, join, say}, two out of the six new links are between
words that belong to the same WordNet synset: (hit ? strike) and (strike ? strike).
Number of Coreferent
Arguments or
Predicates
E, V
The total number of shared arguments and predicates between mentions in the
two clusters. We use the cluster IDs of the corresponding arguments/predicates
to check for identity. For example, when comparing the event clusters {bought}
and {acquired}, extracted from the sentences [AMD]Arg0 bought [ATI]Arg1 and
[AMD]Arg0 acquired [ATI]Arg1, the value of this feature is 2 because the two men-
tions share one Arg0 and one Arg1 argument (assuming that the clusters {AMD,
AMD} and {ATI, ATI} were previously created). For entity clusters, this feature
counts the number of coreferent predicates. In addition to PropBank-style roles, for
event mentions we also include the closest left and right entity mentions in order to
capture any arguments missed by the SRL system.
Coreferent Arguments
in a Specific Role?
E, V
Indicator feature set to 1 if the two clusters have at least one coreferent argument in
a given role. We generate one variant of this feature for each argument label, e.g.,
Arg0, Arg1, etc. For example, the value of this feature for Arg0 for the clusters
{bought} and {acquired} in the above example is 1.
Coreferent Predicate in
a Specific Role?
E
Indicator feature set to 1 if the two clusters have at least one coreferent predicate for
a given role. For example, for the clusters {the man} and {the person}, extracted
from the sentences helped [the man]Arg1 and helped [the person]Arg1, the value of
this feature is 1 if the two helped verbs were previously clustered together.
2nd Order Similarity of
Mention Words
E
Cosine similarity of vectors containing words that are distributionally similar to
words in the cluster mentions. We built these vectors by extracting the top-ten
most-similar words in Dekang Lin?s similarity thesaurus (Lin, 1998) for all the
nouns/adjectives/verbs in a cluster. For example, for the singleton cluster {a new
home}, we construct this vector by expanding new and home to: {new:1, original:1,
old:1, existing:1, current:1, unique:1, modern:1, different:1, special:1, major:1,
small:1, home:1, house:1, apartment:1, building:1, hotel:1, residence:1, office:1,
mansion:1, school:1, restaurant:1, hospital:1 }.
Number; Animacy;
Gender; NE Label
E
Cosine similarity of number, gender, animacy, and NE label vectors. For example,
the number and gender vectors for the two-mention cluster {systems, a pen} are
Number = {singular:1, plural:1}, Gender = {neutral:2}.
Table 2: List of features used when comparing two clusters. If any of the two clusters contains a verbal mention we
consider the merge an operation between event (V) clusters; otherwise it is a merge between entity (E) clusters. We
append to all entity features the suffix Proper or Common based on the type of the head word of the first mention in
each of the two clusters. We use the suffix Proper only if both head words are proper nouns.
paper we used a single heuristic: the possessor of
a nominal event?s predicate is marked as its Arg0,
e.g., Logan is the Arg0 to run in Logan?s run.4
4A principled solution to this problem is to use an SRL sys-
tem for nominal predicates trained using NomBank (Meyers et
al., 2004). We will address this in future work.
494
We extracted named entity labels using the named
entity recognizer from the Stanford CoreNLP suite.
6 Evaluation
6.1 Corpus
The training and test data sets were derived from
the EventCorefBank (ECB) corpus5 created by Be-
jan and Harabagiu (2010) to study event coreference
since standard corpora such as OntoNotes (Pradhan
et al2007) contain a small number of annotated
event clusters. The ECB corpus consists of 482 doc-
uments from Google News clustered into 43 topics,
where a topic is described as a seminal event. The
reason for including comparable documents was to
increase the number of cross-document coreference
relations. Bejan and Harabagiu (2010) only anno-
tated a selection of events.
For the purpose of our study, we extended the
original corpus in two directions: (i) fully anno-
tated sentences, and (ii) entity coreference relations.
In addition, we removed relations other than coref-
erence (e.g., subevent, purpose, related, etc.) that
had been originally annotated. We revised and com-
pleted the original annotation by annotating every
entity and event in the sentences that were (partially)
annotated. The annotation was performed by four
experts, using the Callisto annotation tool.6 The
annotation guidelines and the generated corpus are
available here.7
Our annotation of the ECB corpus followed the
OntoNotes (Pradhan et al2007) standard for coref-
erence annotation, with a few extensions to handle
events. For nouns, we annotated full NPs (with all
modifiers), excluding appositive phrases and nomi-
nal predicates. Only premodifiers that were proper
nouns or possessive phrases were annotated. For
events, we annotated the semantic head of the verb
phrase. We extended the OntoNotes guidelines by
also annotating singletons (but we do not score
them; see below), and by including all events men-
tions (not only those mentioned at least once with an
NP). This required us to be specific with respect to:
5http://faculty.washington.edu/bejan/
data/ECB1.0.tar.gz
6http://callisto.mitre.org
7http://nlp.stanford.edu/pubs/
jcoref-corpus.zip
Training Dev Test Total
# Topics 12 3 28 43
# Documents 112 39 331 482
# Entities 459 46 563 1068
# Entity Mentions 1723 259 3465 5447
# Events 300 30 444 774
# Event Mentions 751 140 1642 2533
Table 3: Corpus statistics.
?ENTITY COREFID=?26?? A publicist ?/ENTITY? ?EVENT
COREFID=?4?? says ?/EVENT? ?ENTITY COREFID=?23??
Tara Reid ?/ENTITY? has ?EVENT COREFID=?3?? checked
?/EVENT? ?ENTITY COREFID=?23?? herself ?/ENTITY? ?EVENT
COREFID=?3*?? into ?/EVENT? ?ENTITY COREFID=?28?? rehab
?/ENTITY?.
Figure 1: Annotation example.
Light verbs Verbs such as give and make followed
by a noun (e.g., make an offer) were not anno-
tated, but the noun was.
Phrasal verbs We annotated the verb together with
the preposition or adverb (e.g., check in).
Idioms They were annotated with all their elements
(e.g., booze it up).
The first topic was annotated by all four anno-
tators as burn-in. Afterwards, annotation disagree-
ments were resolved between all annotators and the
next three topics were annotated again by all four an-
notators to measure agreement. Following Passon-
neau (2004), we computed an inter-annotator agree-
ment of ? = 0.55 (Krippendorff, 2004) on these
three topics, indicating moderate agreement among
the annotators. Given the complexity of the task, we
consider this to be a good score. For example, the
average of the CoNLL F1 between any two annota-
tors is 73.58, which is much higher than the system
scores reported in the literature.
After annotating the four topics, disagreements
were resolved again and all the documents in the
four topics were corrected to match the consensus.
The rest of the corpus was split between the four an-
notators, and each document was annotated by a sin-
gle annotator. Figure 1 shows an example. Table 3
shows the corpus statistics, including the training,
development (dev) and test set splits. The dev topics
were used for tuning the interpolation parameter ?
from Section 4.
495
MUC B3 CEAF-?4 BLANC
System R P F1 R P F1 R P F1 R P F1 CoNLL F1
Baseline 1
Wo/ SRL
Entity 47.4 72.3 57.2 44.1 82.7 57.5 42.5 21.9 28.9 60.1 78.3 64.8 47.9
Event 56.0 56.8 56.4 59.8 71.9 65.3 32.2 31.6 31.9 63.5 68.8 65.7 51.2
Both 49.9 75.4 60.0 44.9 83.9 58.5 46.2 23.3 31.0 60.9 81.2 66.1 49.8
Baseline 2
With SRL
Entity 52.7 73.0 61.2 48.6 80.8 60.7 41.8 24.1 30.6 63.4 78.4 68.2 50.8
Event 59.2 57.0 58.1 62.3 70.8 66.3 31.5 33.2 32.3 65.4 68.0 66.6 52.2
Both 54.5 76.4 63.7 48.7 82.6 61.3 46.3 25.5 32.9 63.9 81.1 69.2 52.6
This paper
Entity 60.7 70.6 65.2 55.5 74.9 63.7 39.3 29.5 33.7 66.9 79.6 71.5 54.2
Event 62.7 62.8 62.7 62.5 73.9 67.7 34.0 33.9 33.9 67.6 78.5 71.7 54.8
Both 61.2 75.9 67.8 53.9 79.0 64.1 45.2 30.0 35.8 67.1 82.2 72.3 55.9
Table 4: Performance of the two baselines and our model. We report scores for entity clusters, event clusters and the
complete task using five metrics.
6.2 Evaluation
We use five coreference evaluation metrics widely
used in the literature:
MUC (Vilain et al1995) Link-based metric which
measures how many predicted and gold clus-
ters need to be merged to cover the gold and
predicted clusters, respectively.
B3 (Bagga and Baldwin, 1998) Mention-based
metric which measures the proportion of over-
lap between predicted and gold clusters for a
given mention.
CEAF (Luo, 2005) Entity-based metric that, unlike
B3, enforces a one-to-one alignment between
gold and predicted clusters. We employ the
entity-based version of CEAF.
BLANC (Recasens and Hovy, 2011) Metric based
on the Rand index (Rand, 1971) that consid-
ers both coreference and non-coreference links
to address the imbalance between singleton and
coreferent mentions.
CoNLL F1 Average of MUC, B3, and CEAF-?4.
This was the official metric in the CoNLL-2011
shared task (Pradhan et al2011).
We followed the CoNLL-2011 evaluation methodol-
ogy, that is, we removed all singleton clusters, and
apposition/copular relations before scoring.
We evaluated the systems on three different set-
tings: only on entity clusters, only on event clus-
ters, and on the complete task, i.e., both entities and
events. Note that the gold corpus separates clusters
into entity and event clusters (see Table 3), but our
system does not make this distinction at runtime.
In order to compute the entity-only and event-only
scores in Table 4, we implemented the following
procedure: (a) when scoring entity clusters, we re-
moved all mentions that were found to be coreferent
with at least one gold event mention and not coref-
erent with any gold entity mentions; and (b) we per-
formed the opposite action when scoring event clus-
ters. This procedure is necessary because our men-
tion identification component is not perfect, i.e., it
generates mentions that do not exist in the gold an-
notation. Furthermore, this procedure is conserva-
tive with respect to the clustering errors of our sys-
tem, e.g., all spurious mentions that our system in-
cludes in a cluster with a gold entity mention are
considered for the entity score, regardless of their
gold type (event or entity).
6.3 Results
Table 4 compares the performance of our system
against two strong baselines that resolve entities and
events separately. Baseline 1 uses a modified Stan-
ford coreference resolution system after our doc-
ument clustering and mention identification steps.
Because the original Stanford system implements
only entity coreference, we extended it with an extra
sieve that implements lemma matching for events.
This additional sieve merges two verbal clusters
(i.e., clusters that contain at least one verbal men-
tion) or a verbal and a nominal cluster when at least
two lemmas of mention head words are the same be-
tween clusters, e.g., helped and the help.
The second baseline adds two more sieves to
Baseline 1. Both these sieves model entity and event
496
contextual information using semantic roles. The
first sieve merges two nominal clusters when two
mentions in the respective clusters have the same
head words and two mentions (possibly with dif-
ferent heads) modify with the same role label two
predicates that have the same lemma. For exam-
ple, this sieve merges the clusters {Obama, the pres-
ident} (seen in the text [Obama]Arg0 attended and
[the president]Arg1 was elected) and {Obama} (seen
in the text [Obama]Arg1 was elected), because they
share a mention with the same head word (Obama)
and two mentions modify with the same role (Arg1)
predicates with the same lemma (elect). The sec-
ond sieve implements the complementary action for
event clusters. That is, it merges two verbal clusters
when at least two mentions have the same lemma
and at least two mentions have semantic arguments
with the same role label and the same lemma.
7 Discussion
The first block in Table 4 indicates that lemma
matching is a strong baseline for event resolution.
Most of the event scores for Baseline 1 are actually
higher than the corresponding entity scores, which
were obtained using the highest ranked system at the
CoNLL-2011 shared task (Lee et al2011). Adding
contextual information using semantic roles (Base-
line 2) helps both entities and events. The CoNLL
F1 for Baseline 2 increases almost 3 points for enti-
ties and 1 point for events. This demonstrates that
local syntactico-semantic context is important for
coreference resolution even in a cross-document set-
ting and that the current state-of-the-art in SRL can
model this context accurately.
The best scores (almost unanimously) are ob-
tained by the model proposed in this paper, which
scores 3.4 CoNLL F1 points higher than Baseline 2
for entities, and 2.6 points higher for events. For the
complete task, our approach scores 3.3 CoNLL F1
points higher than Baseline 2, and 6.1 points higher
than Baseline 1. This demonstrates that a holistic
approach to coreference resolution improves the res-
olution of both entities and events more than models
that address aspects of the task separately. To fur-
ther understand our experiments, we listed the top
five entity/event features with the highest weights in
our model in Table 5. The table indicates that six out
of the ten features serve the purpose of passing infor-
Entity Feature Weight
Entity Heads ? Proper 1.10
Coreferent Predicate for ArgM-LOC ? Common 0.45
Entity Heads ? Common 0.36
Coreferent Predicate for Arg0 ? Proper 0.29
Coreferent Predicate for Arg2 ? Common 0.28
Event Feature Weight
Event Lemmas 0.45
Coreferent Argument for Arg1 0.19
Links between Synonym 0.16
Coreferent Argument for Arg2 0.13
Number of Coreferent Arguments 0.07
Table 5: Top five features with the highest weights.
mation between entity and event clusters. For exam-
ple, the ?Coreferent Argument for Arg1? feature is
triggered when two event clusters have Arg1 argu-
ments that already belong to the same entity cluster.
This allows information from previous entity coref-
erence operations to impact future merges of event
clusters. This is the crux of our iterative approach to
joint coreference resolution.
Finally, we performed an error analysis by man-
ually evaluating 100 errors. We distinguished nine
major types of errors. Their ratios together with a
description and an example are given in Table 6.
This work demonstrates that an approach that
jointly models entities and events is better for cross-
document coreference resolution. However, our
model can be improved. For example, document
clustering and coreference resolution can be solved
jointly, which we expect would improve both tasks.
Furthermore, our iterative coreference resolution
procedure (Algorithm 1) could be modified to ac-
count for mention ordering and distance, which
would allow us to include pronominal resolution in
our joint model, rather than addressing it with a sep-
arate deterministic sieve.
8 Conclusion
We have presented a holistic model for cross-
document coreference resolution that jointly solves
references to events and entities by handling both
nominal and verbal mentions. Our joint resolution
algorithm allows event coreference to help improve
entity coreference, and vice versa. In addition, our
iterative procedure, based on a linear regressor that
models the quality of cluster merges, allows each
497
Error Type (Ratio)
Description
Example
Pronoun resolution
(36%)
The pronoun is incorrectly resolved by the pronominal sieve of the Stanford deterministic entity
system. These errors include (only a small number of) event pronouns.
He said Timmons aimed and missed his target.
Semantics beyond
role frames
(20%)
The semantics of the coreference relation cannot be captured by role frames or WordNet.
Israeli forces on Tuesday killed at least 40 people . . . The Israeli army said the UN school in the
Jabaliya refugee camp was hit . . . and that the dead included a number of Hamas militants.
Arguments of
nominal events
(17%)
The arguments of two nominal events are not detected and thus not coreferred.
The attack on the school has caused widespread shock across Israel . . . while Israeli forces on
Tuesday killed at least 40 people during an attack on a United Nations-run school in Gaza.
Cascaded errors
(7%)
Entities or events are not coreferred due to errors in a previous merge iteration in the same
semantic frame. In the example below, we failed to link the two die verbs, which leads to the
listed entity error.
An Australian climber who survived two nights stuck on Mount Cook after seeing his brother
die . . . Dr Mark Vinar, 43, is presumed dead . . .
Initial high-precision
sieves
(6%)
An error made by the initial high-precision entity resolution sieves is propagated to our model.
Timmons told police he fired when he thought he saw someone in the other group reach for
a gun . . . 15-year-old Timmons was at the scene of the shooting and had a gun.
Phrasal verbs
(6%)
The meaning of a phrasal verb is not captured.
A relative unknown will take over the title role of Doctor Who . . . But the casting of Smith is
a stroke of genius.
Linear regression
(4%)
Recall error made by the regression model when the features are otherwise correct.
The Interior Department on Thursday issued ?revised? regulations . . . Interior Secretary Dirk
Kempthorne announced major changes . . .
Mention detection
(3%)
The mention detection module detects a spurious mention.
Police have arrested a man . . . in the parking lot crosswalk at Sam?s Club in Bloomington.
SRL
(1%)
The SRL system fails to label the semantic role. In this example, jail is detected as the ArgM-
MNR of hanged instead of ArgM-LOC.
A Mafia boss in Palermo hanged himself in jail.
Table 6: Error analysis. Mentions to be resolved are in bold face, correct antecedents are in italics, and our system?s
predictions are underlined.
merging state to benefit from the previous merged
entity and event mentions. This approach allows us
to start with a set of high-precision coreference rela-
tions and gradually add new ones to increase recall.
The experimental evaluation shows that our coref-
erence algorithm gives markedly better F1 for both
entities and events, outperforming two strong base-
lines that handle entities and events separately, mea-
sured by all the standard measures: MUC, B3,
CEAF-?4, BLANC and the official CoNLL-2011
metric. This is noteworthy since each measure has
been shown to place primary emphasis in evaluating
a different aspect of the coreference resolution task.
Our system is tailored for cross-document coref-
erence resolution on a corpus that contains news ar-
ticles that repeatedly report on a smaller number of
topics. This makes it particularly suitable for real-
world applications such as multi-document summa-
rization and cross-document information extraction.
We also release our labeled corpus to facilitate ex-
tensions and comparisons to our work.
Acknowledgements
We acknowledge the support of Defense Advanced Re-
search Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. Any opinions,
findings, and conclusion or recommendations expressed
in this material are those of the author(s) and do not nec-
essarily reflect the view of the DARPA, AFRL, or the US
government. MR is supported by a Beatriu de Pino?s post-
doctoral scholarship (2010 BP-A 00149) from Generali-
tat de Catalunya. AC is supported by a SAP Stanford
Graduate Fellowship. We also gratefully thank Cosmin
Bejan for sharing his code and the useful discussions.
498
References
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
the LREC 1998 Workshop on Linguistic Coreference,
pages 563?566.
Amit Bagga and Breck Baldwin. 1999. Cross-document
event coreference: Annotations, experiments, and ob-
servations. In Proceedings of the ACL 1999 Workshop
on Coreference and Its Applications, pages 1?8.
Cosmin Bejan and Sanda Harabagiu. 2010. Unsuper-
vised Event Coreference Resolution with Rich Lin-
guistic Features. In Proceedings of ACL 2010, pages
1412?1422.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case study
in part of speech tagging. Computational Linguistics,
21(4):543?565.
Zheng Chen and Heng Ji. 2009. Graph-based event
coreference resolution. In Proceedings of the ACL-
IJCNLP 2009 Workshop on Graph-based Methods for
Natural Language Processing, pages 54?57.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Proceedings of NAACL-
HLT 2007.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Proceedings of EMNLP 2009, pages 1152?1161.
Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Proceed-
ings of HLT-NAACL 2010, pages 385?393.
Tian He. 2007. Coreference Resolution on Entities and
Events for Hospital Discharge Summaries. Thesis,
Massachusetts Institute of Technology.
Jerry R. Hobbs. 1978. Resolving pronoun references.
Lingua, 44(4):311?338.
Kevin Humphreys, Robert Gaizauskas, and Saliha Az-
zam. 1997. Event coreference for information extrac-
tion. In Proceedings of the Workshop On Operational
Factors In Practical Robust Anaphora Resolution For
Unrestricted Texts, pages 75?81.
Klaus Krippendorff. 2004. Content Analysis: An In-
troduction to its Methodology. Sage, Thousand Oaks,
CA, second edition.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of CoNLL 2011: Shared Task, pages 28?34.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL 1998,
pages 768?774.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of HLT-EMNLP 2005,
pages 25?32.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank project: an interim report. In Proceedings of the
HLT-NAACL 2004 Workshop on Frontiers in Corpus
Annotation, pages 24?31.
Rebecca Passonneau. 2004. Computing reliability for
coreference annotation. In Proceedings of LREC
2004, pages 1503?1506.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of HLT-NAACL 2006, pages 192?199.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov logic. In
Proceedings of EMNLP 2008, pages 650?659.
Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Un-
restricted coreference: Identifying entities and events
in OntoNotes. In Proceedings of ICSC 2007, pages
446?453.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 shared task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
CoNLL 2011: Shared Task, pages 1?27.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Chris Manning. 2010. A multi-pass
sieve for coreference resolution. In Proceedings of
EMNLP 2010, pages 492?501.
Altaf Rahman and Vincent Ng. 2011. Coreference
resolution with world knowledge. In Proceedings of
ACL 2011, pages 814?824.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the American
Statistical Association, 66(336):846?850.
Marta Recasens and Eduard Hovy. 2011. BLANC: Im-
plementing the Rand index for coreference evaluation.
Natural Language Engineering, 17(4):485?510.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-art.
In Proceedings of ACL-IJCNLP 2009, pages 656?664.
Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 2005.
A hybrid unsupervised approach for document cluster-
ing. In Proceedings of KDD 2005, pages 685?690.
499
Mihai Surdeanu, Llu??s Ma`rquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination strategies for se-
mantic role labeling. Journal of Artificial Intelligence
Research, 29:105?151.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of MUC-6, pages 45?52.
Bonnie Lynn Webber. 1988. Discourse deixis: reference
to discourse segments. In Proceedings of ACL 1988,
pages 113?122.
Michael L. Wick, Khashayar Rohanimanesh, Karl
Schultz, and Andrew McCallum. 2008. A unified ap-
proach for schema matching, coreference and canoni-
calization. In Proceedings of KDD 2008, pages 722?
730.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of ACL 1995, pages 189?196.
500
Selectional Preferences for
Semantic Role Classification
Ben?at Zapirain?
University of the Basque Country
Eneko Agirre??
University of the Basque Country
Llu??s Ma`rquez?
Universitat Polite`cnica de Catalunya
Mihai Surdeanu?
University of Arizona
This paper focuses on a well-known open issue in Semantic Role Classification (SRC) research:
the limited influence and sparseness of lexical features. We mitigate this problem using models
that integrate automatically learned selectional preferences (SP). We explore a range of models
based on WordNet and distributional-similarity SPs. Furthermore, we demonstrate that the SRC
task is better modeled by SP models centered on both verbs and prepositions, rather than verbs
alone. Our experiments with SP-based models in isolation indicate that they outperform a lexical
baseline with 20 F1 points in domain and almost 40 F1 points out of domain. Furthermore, we
show that a state-of-the-art SRC system extended with features based on selectional preferences
performs significantly better, both in domain (17% error reduction) and out of domain (13%
error reduction). Finally, we show that in an end-to-end semantic role labeling system we obtain
small but statistically significant improvements, even though our modified SRC model affects
only approximately 4% of the argument candidates. Our post hoc error analysis indicates that
the SP-based features help mostly in situations where syntactic information is either incorrect or
insufficient to disambiguate the correct role.
? Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country.
E-mail: benat.zapirain@ehu.es.
?? Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country.
E-mail: e.agirre@ehu.es.
? UPC Campus Nord (Omega building), Jordi Girona 1?3, 08034 Barcelona, Catalonia.
E-mail: lluism@lsi.upc.edu.
? 1040 E. 4th Street, Tucson, AZ 85721. E-mail: msurdeanu@arizona.edu.
Submission received: 14 November 2011; revised submission received: 31 May 2012; accepted for publication:
15 August 2012.
doi:10.1162/COLI a 00145
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 3
1. Introduction
Semantic Role Labeling (SRL) is the problem of analyzing clause predicates in text by
identifying arguments and tagging them with semantic labels indicating the role they
play with respect to the predicate. Such sentence-level semantic analysis allows the
determination of who did what to whom, when and where, and thus characterizes the
participants and properties of the events established by the predicates. For instance,
consider the following sentence, in which the arguments of the predicate to send have
been annotated with their respective semantic roles.1
(1) [Mr. Smith]Agent sent [the report]Object [to me]Recipient [this morning]Temporal.
Recognizing these event structures has been shown to be important for a broad
spectrum of NLP applications. Information extraction, summarization, question
answering, machine translation, among others, can benefit from this shallow semantic
analysis at sentence level, which opens the door for exploiting the semantic relations
among arguments (Boas 2002; Surdeanu et al 2003; Narayanan and Harabagiu 2004;
Melli et al 2005; Moschitti et al 2007; Higashinaka and Isozaki 2008; Surdeanu,
Ciaramita, and Zaragoza 2011). In Ma`rquez et al (2008) the reader can find a broad
introduction to SRL, covering several historical and definitional aspects of the problem,
including also references to the main resources and systems.
State-of-the-art systems leverage existing hand-tagged corpora (Fillmore,
Ruppenhofer, and Baker 2004; Palmer, Gildea, and Kingsbury 2005) to learn supervised
machine learning systems, and typically perform SRL in two sequential steps:
argument identification and argument classification. Whereas the former is mostly a
syntactic recognition task, the latter usually requires semantic knowledge to be taken
into account. The semantic knowledge that most current systems capture from text is
basically limited to the predicates and the lexical units contained in their arguments,
including the argument head. These ?lexical features? tend to be sparse, especially
when the training corpus is small, and thus SRL systems are prone to overfit the
training data and generalize poorly to new corpora (Pradhan, Ward, and Martin 2008).
As a simplified example of the effect of sparsity, consider the following sentences
occurring in an imaginary training data set for SRL:
(2) [JFK]Patient was assassinated [in Dallas]Location
(3) [John Lennon]Patient was assassinated [in New York]Location
(4) [JFK]Patient was assassinated [in November]Temporal
(5) [John Lennon]Patient was assassinated [in winter]Temporal
All four sentences share the same syntactic structure, so the lexical features (i.e., the
words Dallas, New York, November, and winter) represent the most relevant knowledge
for discriminating between the Location and Temporal adjunct labels in learning.
1 For simplicity, in this paper we talk about arguments in the most general sense. Unless noted otherwise,
argument will refer to both core-arguments (Agent, Patient, Instrument, etc.) and adjuncts (Manner,
Temporal, Location, etc.).
632
Zapirain et al Selectional Preferences for Semantic Role Classification
The problem is that, as in the following sentences, for the same predicate, one may
encounter similar expressions with new words like Texas or December, which the
classifiers cannot match with the lexical features seen during training, and thus become
useless for classification:
(6) [Smith] was assassinated [in Texas]
(7) [Smith] was assassinated [in December]
This problem is exacerbated when SRL systems are applied to texts coming from
new domains where the number of new predicates and argument heads increases
considerably. The CoNLL-2004 and 2005 evaluation exercises on semantic role labeling
(Carreras and Ma`rquez 2004, 2005) reported a significant performance degradation
of around 10 F1 points when applied to out-of-domain texts from the Brown corpus.
Pradhan, Ward, and Martin (2008) showed that this performance degradation is
essentially caused by the argument classification subtask, and suggested the lexical
data sparseness as one of the main reasons.
In this work, we will focus on Semantic Role Classification (SRC), and we will show
that selectional preferences (SP) are useful for generalizing lexical features, helping
fight sparseness and domain shifts, and improving SRC results. Selectional preferences
try to model the kind of words that can fill a specific argument of a predicate, and
have been widely used in computational linguistics since the early days (Wilks 1975).
Both semantic classes from existing lexical resources like WordNet (Resnik 1993b) and
distributional similarity based on corpora (Pantel and Lin 2000) have been successfully
used for acquiring selectional preferences, and in this work we have used several of
those models.
The contributions of this work to the field of SRL are the following:
1. We formalize and implement a method that applies several selectional
preference models to Semantic Role Classification, introducing for the first
time the use of selectional preferences for prepositions, in addition to
selectional preferences for verbs.
2. We show that the selectional preference models are able to generalize
lexical features and improve role classification performance in a controlled
experiment disconnected from a complete SRL system. The positive effect
is consistently observed in all variants of WordNet and distributional
similarity measures and is especially relevant for out-of-domain data. The
separate learning of SPs for verbs and prepositions contributes
significantly to the improvement of the results.
3. We integrate the information of several SP models in a state-of-the-art SRL
system (SwiRL)2 and obtain significant improvements in semantic role
classification and, as a consequence, in the end-to-end SRL task. The key
for the improvement lies in the combination of the predictions provided
by SwiRL and the several role classification models based on selectional
preferences.
2 http://surdeanu.info/mihai/swirl/.
633
Computational Linguistics Volume 39, Number 3
4. We present a manual analysis of the output of the combined role
classification system. By observing a set of real examples, we categorized
and quantified the situations in which SP models tend to help role
classification. By inspecting also a set of negative cases, this analysis also
sheds light on the limitations of the current approach and identifies
opportunities for further improvements.
The use of selectional preferences for improving role classification was first pre-
sented in Zapirain, Agirre, and Ma`rquez (2009), and later extended in Zapirain et al
(2010) to a full-fledged SRC system. In the current paper, we provide more detailed
background information and details of the selectional preference models, as well as
complementary experiments on the integration in a full-fledged system. More impor-
tantly, we incorporate a detailed analysis of the output of the system, comparing it with
that of a state-of-the-art SRC system not using SPs.
The rest of the paper is organized as follows. Section 2 provides background on the
automatic acquisition of selectional preference, and its recent relation to the semantic
role labeling problem. In Section 3, the SP models investigated in this paper are ex-
plained in all their variants. The results of the SP models in laboratory conditions are
presented in Section 4. Section 5 describes the method for integrating the SP models in a
state-of-the-art SRL system and discusses the results obtained. In Section 6 the qualita-
tive analysis of the system output is presented, including a detailed discussion of several
examples. Finally, Section 7 concludes and outlines some directions for future research.
2. Background
The simplest model for generating selectional preferences would be to collect all heads
filling each role of the target predicate. This is akin to the lexical features used by current
SRL systems, and we refer to this model as the lexical model. More concretely, the
lexical model for verb-role selectional preferences consists of the list of words appearing
as heads of the role arguments of the predicate verb. This model can be extracted
automatically from the SRL training corpus using straightforward techniques. When
using this model for role classification, it suffices to check whether the head word of
the argument matches any of the words in the lexical model. The lexical model is the
baseline for our other SP models, all of which build on that model.
In order to generalize the lexical model, semantic classes can be used. Although in
principle any lexical resource listing semantic classes for nouns could be applied, most
of the literature has focused on the use of WordNet (Resnik 1993b). In the WordNet-
based model, the words occurring in the lexical model are projected over the semantic
hierarchy of WordNet, and the semantic classes which represent best those words are
selected. Given a new example, the SRC system has to check whether the new word
matches any of those semantic classes. For instance, in example sentences (2)?(5), the
semantic class <time period> covers both training examples for Temporal (i.e., November
and winter), and <geographical area> covers the examples for Location. When test
words Texas and December occur in Examples (6) and (7), the semantic classes to which
they belong can be used to tag the first as Location and the second as Temporal.
As an alternative to the use of WordNet, one can also apply automatically acquired
distributional similarity thesauri. Distributional similarity methods analyze the co-
occurrence patterns of words and are able to capture, for instance, that December is more
closely related to November than to Dallas (Grefenstette 1992). Distributional similarity is
typically used on-line (i.e., given a pair of words, their similarity is computed on the go),
634
Zapirain et al Selectional Preferences for Semantic Role Classification
but, in order to speed up its use, it has also been used to produce off-line a full thesauri,
storing, for every word, the weighted list of all outstanding similar words (Lin 1998).
In the Distributional similarity model, when test item Texas in Example (6) is to be
labeled, the higher similarity to Dallas and New York, in contrast to the lower similarity
to November and winter, would be used to label the argument with the Location role.
The automatic acquisition of selectional preferences is a well-studied topic in NLP.
Many methods using semantic classes and selectional preferences have been proposed
and applied to a variety of syntactic?semantic ambiguity problems, including syntactic
parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Martinez
2008; Koo, Carreras, and Collins 2008; Agirre et al 2011), word sense disambiguation
(Resnik 1993a; Agirre and Martinez 2001; McCarthy and Carroll 2003), pronoun res-
olution (Bergsma, Lin, and Goebel 2008) and named-entity recognition (Ratinov and
Roth 2009). In addition, selectional preferences have been shown to be effective to
improve the quality of inference and information extraction rules (Pantel et al 2007;
Ritter, Mausam, and Etzioni 2010). In some cases, the aforementioned papers do not
mention selectional preferences, but all of them use some notion of preferring certain
semantic types over others in order to accomplish their respective task.
In fact, one could use different notions of semantic types. In one extreme, we would
have a small set of coarse semantic classes. For instance, some authors have used the
26 so-called ?semantic fields? used to classify all nouns in WordNet (Agirre, Baldwin,
and Martinez 2008; Agirre et al 2011). The classification could be more fine-grained, as
defined by the WordNet hierarchy (Resnik 1993b; Agirre and Martinez 2001; McCarthy
and Carroll 2003), and other lexical resources could be used as well. Other authors have
used automatically induced hierarchical word classes, clustered according to occurrence
information from corpora (Koo, Carreras, and Collins 2008; Ratinov and Roth 2009).
On the other extreme, each word would be its own semantic class, as in the lexical
model, but one could also model selectional preference using distributional similarity
(Grefenstette 1992; Lin 1998; Pantel and Lin 2000; Erk 2007; Bergsma, Lin, and Goebel
2008). In this paper we will focus on WordNet-based models that use the whole hierarchy
and on distributional similarity models, and we will use the lexical model as baseline.
2.1 WordNet-Based Models
Resnik (1993b) proposed the modeling of selectional preferences using semantic classes
from WordNet and applied the model to tackle some ambiguity issues in syntax, such
as noun-compounds, coordination, and prepositional phrase attachment. Given two
alternative structures, Resnik used selectional preferences to choose the attachment
maximizing the fitness of the head to the selectional preferences of the attachment
points. This is similar to our task, but in our case we compare the target head to the selec-
tional preference models for each possible role label (i.e., given a verb and the head of an
argument, we need to find the role with the selectional preference that fits the head best).
In Resnik?s model, he first characterizes the restrictiveness of the selectional pref-
erence of an argument position r of a governing predicate p, noted as R(p, r). For that,
given a set of classes C from the WordNet nominal hierarchies, he takes the relative en-
tropy or Kullback-Leibler distance between the prior distribution P(C) and the posterior
distribution P(C|p, r):
R(p, r) =
?
c?C
P(c|p, r)logP(c|p, r)
P(c)
(1)
635
Computational Linguistics Volume 39, Number 3
The priors can be computed from any corpora, computing frequencies of classes
and using maximum likelihood estimates. The frequencies for classes cannot be directly
observed, but they can be estimated from the lexical frequencies of the nouns under
the class, as in Equation (2). Note that in WordNet, hypernyms (?hyp? for short)
correspond to superclass relations, and therefore hyp(n) returns all superclasses of
noun n.
freq(c) =
?
{n|c?hyp(n)}
freq(n) (2)
A complication arises because of the polysemy of nouns. If each occurrence of a
noun counted once in all classes that its senses belong to, polysemous nouns would
account for more probability mass than monosemous nouns, even if they occurred the
same number of times. As a solution, the frequency of polysemous nouns is split among
its senses uniformly. For instance, the probability of the class <time period> can be
estimated according to the frequencies of nouns like November, spring, and the rest of
nouns under it. November has a single sense, so every occurrence counts as 1, but spring
has six different senses, so each occurrence should only count as 0.16. Note that with
this method we are implicitly dealing with the word sense ambiguity problem. When
encountering a polysemous noun as an argument of a verb, we record the occurrence
of all of its senses. Given enough occurrences of nouns, the classes generalizing the
intended sense of the nouns will gather more counts than competing classes. In the
example, <time period> would have 1.16 compared with 0.16 <tool> (i.e., for the metal
elastic device meaning of spring). Researchers have used this fact to perform Word Sense
Disambiguation using selectional preferences (Resnik 1993a; Agirre and Martinez 2001;
McCarthy and Carroll 2003).
The posterior probability can be computed similarly, but it takes into account occur-
rences of the nouns in the required argument position of the predicate, and thus requires
a corpus annotated with roles.
The selectional preference of a predicate p and role r for a head w0 of any potential
argument, noted as SPRes(p, r, w0), is formulated as follows:3
SPRes(p, r, w0) = max
c0?hyp(w0 )
P(c0|p, r)log P(c0|p,r)P(c0)
R(p, r)
(3)
The numerator formalizes the goodness of fit for the best semantic class c0 that
contains w0. The hypernym (i.e., superclass) of w0 yielding the maximum value is
chosen. The denominator models how restrictive the selectional preference is for p and
r, as modeled in Equation (1).
Variations of Resnik?s idea to find a suitable level of generalization have been
explored in later years. Li and Abe (1998) applied the minimum-description length
principle. Alternatively, Clark and Weir (2002) devised a procedure to decide when a
class should be preferred rather than its children.
Brockmann and Lapata (2003) compared several class-based models (including
Resnik?s selectional preferences) on a syntactic plausibility judgment task for German.
3 We slightly modified the notation of Resnik (1993b) in order to be coherent with the formulae presented
in this paper.
636
Zapirain et al Selectional Preferences for Semantic Role Classification
The models return weights for (verb, syntactic function, noun) triples, and correla-
tion with human plausibility judgment is used for evaluation. Resnik?s selectional
preference scored best among WordNet-based methods (Li and Abe 1998; Clark and
Weir 2002). Despite its earlier publication, Resnik?s method is still the most popular
representative among WordNet-based methods (Pado?, Pado?, and Erk 2007; Erk, Pado?,
and Pado? 2010; Baroni and Lenci 2010). We also chose to use Resnik?s model in this
paper.
One of the disadvantages of the WordNet-based models, compared with the distri-
butional similarity models, is that they require that the heads are present in WordNet.
This limitation can negatively influence the coverage of the model, and also its general-
ization ability.
2.2 Distributional Similarity Models
Distributional similarity models assume that a word is characterized by the words it
co-occurs with. In the simplest model, co-occurring words are taken from a fixed-size
context window. Each word w would be represented by the set of words that co-occur
with it, T(w). In a more elaborate model, each word w would be represented as a vector
of words T(w) with weights, where Ti(w) corresponds to the weight of the ith word in
the vector. The weights can be calculated following a simple frequency of co-occurrence,
or using some other formula.
Then, given two words w and w0, their similarity can be computed using any simi-
larity measure between their co-occurrence sets or vectors. For instance, early work by
Grefenstette (1992) used the Jaccard similarity coefficient of the two sets T(w) and T(w0)
(cf. Equation (4) in Figure 1). Lee (1999) reviews a wide range of similarity functions,
including Jaccard and the cosine between two vectors T(w) and T(w0) (cf. Equation (5)
in Figure 1).
In the context of lexical semantics, the similarity measure defined by Lin (1998)
has been very successful. This measure (cf. Equation (6) in Figure 1) takes into account
syntactic dependencies (d) in its co-occurrence model. In this case, the set T(w) of co-
occurrences of w contains pairs (d,v) of dependencies and words, representing the fact
simJac(w, w0) =
|T(w) ? T(w0)|
|T(w) ? T(w0)|
(4)
simcos(w, w0) =
?n
i=1
Ti(w)Ti(w0)
?
?n
i=1
Ti(w)2
?
?n
i=1
Ti(w0)2
(5)
simLin(w, w0) =
?
(d,v)?T(w)?T(w0 )(I(w, d, v) + I(w0, d, v))
?
(d,v)?T(w) I(w, d, v) +
?
(d,v)?T(w0 ) I(w0, d, v)
(6)
Figure 1
Similarity measures used in the paper. Jac and cos stand for Jaccard and cosine similarity metrics.
T(w) is the set of words co-occurring with w, Ti(w) is the weight of the ith element of the vector
of words co-occurring with w, and I(w, d, v) is the mutual information between w and d, v.
637
Computational Linguistics Volume 39, Number 3
that the corpus contains an occurrence of w having dependency d with v. For instance,
if the corpus contains John loves Mary, then the pair (ncsubj, love) would be in the set
T for John. The measure uses information-theoretic principles, and I(w, d, v) represents
the information content of the triple (Lin 1998).
Although the use of co-occurrence vectors for words to compute similarity has been
standard practice, some authors have argued for more complex uses. Schu?tze (1998)
builds vectors for each context of occurrence of a word, combining the co-occurrence
vectors for each word in the context. The vectors for contexts were used to induce
senses and to improve information retrieval results. Edmonds (1997) built a lexical co-
occurrence network, and applied it to a lexical choice task. Chakraborti et al (2007)
used transitivity over co-occurrence relations, with good results on several classification
tasks. Note that all these works use second order and higher order to refer to their method.
In this paper, we will also use second order to refer to a new method which goes beyond
the usual co-occurrence vectors (cf. Section 3.3).
A full review of distributional models is out of the scope of this paper, as we are in-
terested in showing that some of those models can be used successfully to improve SRC.
Pado? and Lapata (2007) present a review of distributional models for word similarity,
and a study of several parameters that define a broad family of distributional similarity
models, including Jaccard and Lin. They provide publicly available software,4 which
we have used in this paper, as explained in the next section. Baroni and Lenci (2010)
present a framework for extracting distributional information from corpora that can be
used to build models for different tasks.
Distributional similarity models were first used to tackle syntactic ambiguity. For
instance, Pantel and Lin (2000) obtained very good results on PP-attachment using the
distributional similarity measure defined by Lin (1998). Distributional similarity was
used to overcome sparsity problems: Alongside the counts in the training data of the
target words, the counts of words similar to the target ones were also used. Although
not made explicit, Lin was actually using a distributional similarity model of selectional
preferences.
The application of distributional selectional preferences to semantic roles (as op-
posed to syntactic functions) is more recent. Gildea and Jurafsky (2002) are the only ones
applying selectional preferences in a real SRL task. They used distributional clustering
and WordNet-based techniques on a SRL task on FrameNet roles. They report a very
small improvement of the overall performance when using distributional clustering
techniques. In this paper we present complementary experiments, with a different role
set and annotated corpus (PropBank), a wider range of selectional preference models,
and the analysis of out-of-domain results.
Other papers applying semantic preferences in the context of semantic roles rely on
the evaluation of artificial tasks or human plausibility judgments. Erk (2007) introduced
a distributional similarity?based model for selectional preferences, reminiscent of that
of Pantel and Lin (2000). Her approach models the selectional preference SPsim(p, r, w0)
of an argument position r of governing predicate p for a possible head-word w0 as
follows:
SPsim(p, r, w0) =
?
w?Seen(p,r)
sim(w0, w) ? weight(p, r, w) (7)
4 http://www.coli.uni-saarland.de/?pado/dv/dv.html.
638
Zapirain et al Selectional Preferences for Semantic Role Classification
where sim(w0, w) is the similarity between the seen and potential heads, Seen(p, r) is the
set of heads of role r for predicate p seen in the training data set (as in the lexical model),
and weight(p, r, w) is the weight of the seen head word w. Our distributional model for
selectional preferences follows her formalization.
Erk instantiated the basic model with several corpus-based distributional similarity
measures, including Lin?s similarity, Jaccard, and cosine (Figure 1) among others, and
several implementations of the weight function such as the frequency. The quality of
each model instantiation, alongside Resnik?s model and an expectation maximization
(EM)-based clustering model, was tested in a pseudo-disambiguation task where the
goal was to distinguish an attested filler of the role and a randomly chosen word. The
results over 100 frame-specific roles showed that distributional similarities attain similar
error rates to Resnik?s model but better than EM-based clustering, with Lin?s formula
having the smallest error rate. Moreover, the coverage of distributional similarity mea-
sures was much better than Resnik?s. In a more recent paper, Erk, Pado?, and Pado? (2010)
extend the aforementioned work, including evaluation to human plausibility judgments
and a model for inverse selectional preferences.
In this paper we test similar techniques to those presented here, but we evaluate
selectional preference models in a setting directly related to semantic role classification,
namely, given a selectional preference model for a verb we find the role which fits
best the given head word. The problem is indeed qualitatively different from previous
work in that we do not have to choose among the head words competing for a role but
among selectional preferences of roles competing for a head word.
More recent work on distributional selectional preference has explored the use of
discriminative models (Bergsma, Lin, and Goebel 2008) and topical models (O? Se?aghdha
2010; Ritter, Mausam, and Etzioni 2010). These models would be a nice addition to those
implemented in this paper, and if effective, they would improve further our results with
respect to the baselines which don?t use selectional preferences.
Contrary to WordNet-based models, distributional preferences do not rely on a
hand-built resource. Their coverage and generalization ability depend on the corpus
from which the distributional similarity model was computed. This fact makes this
approach more versatile in domain adaptation scenarios, as more specific and test-set
focused generalization corpora could be used to modify, enrich, or even replace the
original corpus.
2.3 PropBank
In this work we use the semantic roles defined in PropBank. The Proposition Bank
(Palmer, Gildea, and Kingsbury 2005) emerged as a primary resource for research in
SRL. It provides semantic role annotation for all verbs in the Penn Treebank corpus.
PropBank takes a ?theory-neutral? approach to the designation of core semantic roles.
Each verb has a frameset listing its allowed role labelings in which the arguments are
designated by number (starting from 0). Each numbered argument is provided with an
English language description specific to that verb. The most frequent roles are Arg0 and
Arg1 and, generally, Arg0 stands for the prototypical agent and Arg1 corresponds to the
prototypical patient or theme of the proposition. The rest of arguments (Arg2 to Arg5)
do not generalize across verbs, that is, they have verb specific interpretations.
Apart from the core numbered roles, there are 13 labels to designate adjuncts:
AM-ADV (general-purpose), AM-CAU (cause), AM-DIR (direction), AM-DIS (dis-
course marker), AM-EXT (extent), AM-LOC (location), AM-MNR (manner), AM-MOD
639
Computational Linguistics Volume 39, Number 3
Table 1
Example of verb-role lexical SP models for write, listed in alphabetical order. Number of heads
indicates the number of head words attested, Unique heads indicates the number of distinct
head words attested, and Examples lists some of the heads in alphabetical order.
Verb-role Number of Unique Examples
heads heads
write-Arg0 98 84 Angrist anyone baker ball bank Barlow Bates ...
write-Arg1 97 69 abstract act analysis article asset bill book ...
write-Arg2 7 7 bank commander hundred jaguar Kemp member ...
write-AM-LOC 2 2 paper space
write-AM-TMP 1 1 month
(modal verb), AM-NEG (negation marker), AM-PNC (purpose), AM-PRD (predication),
AM-REC (reciprocal), and AM-TMP (temporal).
3. Selectional Preference Models for Argument Classification
Our approach for applying selectional preferences to semantic role classification is
discriminative. That is, the SP-based models provide a score for every possible role
label given a verb (or preposition), the head word of the argument, and the selectional
preferences for the verb (or preposition). These scores can be used to directly assign the
most probable role or to codify new features to train enriched semantic role classifiers.
In this section we first present all the variants for acquiring selectional preferences
used in our study, and then present the method to apply them to semantic role classifi-
cation. We selected several variants that have been successful in some previous works.
3.1 Lexical SP Model
In order to implement the lexical model we gathered all heads w of arguments filling
a role r of a predicate p and obtained freq(p, r, w) from the corresponding training data
(cf. Section 4.1). Table 1 shows a sample of the heads of arguments attested in the
corpus for the verb write. The lexical SP model can be simply formalized as follows:
SPlex(p, r, w0) = freq(p, r, w0) (8)
3.2 WordNet-Based SP Models
We instantiated the model based on (Resnik 1993b) presented in the previous sec-
tion (SPRes, cf. Equation (3)) using the implementation of Agirre and Martinez (2001).
Tables 2 and 3 show the synsets5 that generalize best the head words in Table 1
for write-Arg0 and write-Arg1, according to the weight assigned to those synsets by
Equation (1). According to this model, and following basic intuition, the words attested
as being Arg0s of write are best generalized by semantic classes such as living things,
5 The WordNet terminology for concepts is synset. In this paper we use concept, synset, and semantic class
interchangeably.
640
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 2
Excerpt from the selectional preferences for write-Arg0 according to SPRes, showing the synsets
that generalize best the head words in Table 1. Weight lists the weight assigned to those synsets
by Equation (1). Description includes the words and glosses in the synset.
Synset Weight Description
n#00002086 5.875 life form organism being living thing any living entity
n#00001740 5.737 entity something anything having existence (living or nonliving)
n#00009457 4.782 object physical object a physical (tangible and visible) entity;
n#00004123 4.351 person individual someone somebody mortal human soul
a human being;
Table 3
Excerpt from the selectional preferences for write-Arg1 according to SPRes, showing the synsets
that generalize best the head words in Table 1. Weight lists the weight assigned to those synsets
by Equation (1). Description includes the words and glosses in the synset.
Synset Weight Description
n#00019671 7.956 communication something that is communicated between people
or groups
n#04949838 4.257 message content subject matter substance what a communication
that . . .
n#00018916 3.848 relation an abstraction belonging to or characteristic of two entities
n#00013018 3.574 abstraction a concept formed by extracting common features
from examples
entities, physical objects, and human beings, whereas Arg1s by communication, mes-
sage, relation, and abstraction.
Resnik?s method performs well among Wordnet-based methods, but we realized
that it tends to overgeneralize. For instance, in Table 2, the concept for ?entity? (one of
the unique beginners of the WordNet hierarchy) has a high weight. This means that a
head like ?grant? would be assigned Arg0. In fact, any noun which is under concept
n#00001740 (entity) but not under n#04949838 (message) would be assigned Arg0. This
observation led us to speculate on an alternative method which would try to generalize
as little as possible.
Our intuition is that general synsets can fit several selectional preferences at the
same time. For instance, the <entity> class, as a superclass of most words, would be a
correct generalization for the selectional preferences of all agent, patient, and instrument
roles of a predicate like break. On the contrary, specific concepts are usually more useful
for characterizing selectional preferences, as in the <tool> class for the instrument role
of break. The priority of using specific synsets over more general ones is, thus, justified
in the sense that they may better represent the most relevant semantic characteristics of
the selectional preferences.
The alternative method (SPwn) is based on the depth of the concepts in the WordNet
hierarchy and the frequency of the nouns. The use of the depth in hierarchies to model
the specificity of concepts (the deeper the more specific) is not new (Rada et al 1989;
Sussna 1993; Agirre and Rigau 1996). Our method tries to be conservative with respect
to generalization: When we check which SP is a better fit for a given target head, we
always prefer the SP that contains the most specific generalization for the target head
(the lowest synset which is a hypernym of the target word).
641
Computational Linguistics Volume 39, Number 3
Table 4
Excerpt from the selectional preferences for write-Arg0 according to SPwn, showing from deeper
to shallower the synsets in WordNet which are connected to head words in Table 1. Depth lists
the depth of synsets in WordNet. Description includes the words and glosses in the synset.
Synset Depth Freq. Description
n#01967203 9 1 humanoid human being any living or extinct member of the . . .
n#07603319 8 1 spy undercover agent a secret agent hired by a state to . . .
n#07151308 8 1 woman a human female who does housework
n#06183656 8 1 Federal Reserve the central bank of the US
Table 5
Excerpt from the selectional preferences for write-Arg1 according to SPwn, showing from deeper
to shallower the synsets in WordNet which are connected to head words in Table 1. Depth lists
the depth of synsets in WordNet. Description includes the words and glosses in the synset.
Synset Depth Freq. Description
n#05403815 13 1 information formal accusation of a crime
n#05401516 12 1 accusation accusal a formal charge of wrongdoing brought . . .
n#04925620 11 1 charge complaint a pleading describing some wrong or offense
n#04891230 11 1 memoir an account of the author?s personal experiences
More concretely, we model selectional preferences as a multiset6 of synsets, storing
all hypernyms of the heads seen in the training data for a certain role of a given
predicate, that is:
Smul(p, r) =
?
w?Seen(p,r)
hyp(w) (9)
where Seen(p, r) are all the argument heads for predicate p and role r, and hyp(w) returns
all the synsets and hypernyms of w, including hypernyms of hypernyms recursively up
to the top synsets.
For any given synset s, let d(s) be the depth of the synset in the WordNet hierarchy,
and let 1Smul(p,r)(s) be the multiplicity function which returns how many times s is con-
tained in the multiset Smul(p, r). We define a partial order among synsets a, b ? Smul(p, r)
as follows: ord(a) > ord(b) iff d(a) > d(b) or d(a) = d(b) ? 1Smul(p,r)(a) > 1Smul(p,r)(b).
Tables 4 and 5 show the most specific synsets (according to their depth) for write-Arg0
and write-Arg1.
We can then measure the goodness of fit of the selectional preference for a word as
the rank in the partial order of the first hypernym of the head that is also present in the
selectional preference. For that, we introduce SPwn(p, r, w), which following the previous
notation is defined as:
SPwn(p, r, w) = arg max
s?hyp(w)?Smul(p,r)
ord(s) (10)
6 Multisets are similar to sets, but allow for repeated members.
642
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 6
Most similar words for Texas and December according to Lin (1998).
Texas Florida 0.249, Arizona 0.236, California 0.231, Georgia 0.221, Kansas 0.217,
Minnesota 0.214, Missouri 0.214, Michigan 0.213, Colorado 0.208, North
Carolina 0.207, Oklahoma 0.207, Arkansas 0.205, Alabama 0.205, Nebraska
0.201, Tennessee 0.197, New Jersey 0.194, Illinois 0.189, Virginia 0.188,
Kentucky 0.188, Wisconsin 0.188, Massachusetts 0.184, New York 0.183
December June 0.341, October 0.340, November 0.333, April 0.330, February 0.329,
September 0.328, July 0.323, January 0.322, August 0.317, may 0.305, March
0.250, Spring 0.147, first quarter 0.135, mid-December 0.131, month 0.130,
second quarter 0.129, mid-November 0.128, fall 0.125, summer 0.125,
mid-October 0.121, autumn 0.121, year 0.121, third quarter 0.119
In case of ties, the role coming first in alphabetical order would be returned. Note that,
similar to the Resnik model (cf. Section 2.1), this model implicitly deals with the word
ambiguity problem.
As with any other approximation to measure specificity of concepts, the use of
depth has some issues, as some deeply rooted stray synsets would take priority. For
instance, Table 4 shows that synset n#01967203 for human being is the deepest synset. In
practice, when we search the synsets of a target word in the SPwn models following Eq.
(10), the most specific synsets (specially stray synsets) are not found, and synsets higher
in the hierarchy are used.
3.3 Distributional SP Models
All our distributional SP models are based on Equation (7). We have used several vari-
ants for sim(w0, w), as presented subsequently, but in all cases, we used the frequency
freq(p, r, w) as the weight in the equation. Given the availability of public resources for
distributional similarity, rather than implementing sim(w0, w) afresh we used (1) the pre-
compiled similarity measures by Lin (1998),7 and (2) the software for semantic spaces
by Pado? and Lapata (2007).
In the first case, Lin computed the similarity numbers for an extensive vocabulary
based on his own similarity formula (cf. Equation (6) in Figure 1) run over a large
parsed corpus comprising journalism texts from different sources: WSJ (24 million
words), San Jose Mercury (21 million words) and AP Newswire (19 million words).
The resource includes, for each word in the vocabulary, its most similar words with
the similarity weight. In order to get the similarity for two words, we can check the
entry in the thesaurus for either word. We will refer to this similarity measure as
simpreLin. Table 6 shows the most similar words for Texas and December according to this
resource.
For the second case, we applied the software to the British National Corpus to
extract co-occurrences, using the optimal parameters as described in Pado? and Lapata
(2007, page 179): word-based space, medium context, log-likelihood association, and
7 http://www.cs.ualberta.ca/?lindek/downloads.htm.
643
Computational Linguistics Volume 39, Number 3
Table 7
Summary of distributional similarity measures used in this work.
Similarity measure Source
simcos cosine BNC
simJac Jaccard BNC
simLin Lin BNC
simpreLin Lin Pre-computed
simpreLin?cos cosine (2nd order) Pre-computed
simpreLin?Jac Jaccard (2nd order) Pre-computed
2,000 basis elements. We tested Jaccard, cosine, and Lin?s measure for similarity, yielding
simJac, simcos, and simLin, respectively.
In addition to measuring the similarity of two words directly, that is, using the co-
occurrence vectors of each word as in Section 2, we also tried a variant which we will
call second-order similarity. In this case each word is represented by a vector which
contains all similar words with weights, where those weights come from first order
similarity. That is, in order to obtain the second-order vector for word w, we need to
compute its first order similarity with all other words in the vocabulary. The second-
order similarity of two words is then computed according to those vectors. For this, we
just need to change the definition of T and T in the similarity formulas in Figure 1: Now
T(w) would return the list of words which are taken to be similar to w, and T(w) would
return the same list but as a vector with weights.
This approximation is computationally expensive, as we need to compute the
square matrix of similarities for all word pairs in the vocabulary, which is highly time-
consuming. Fortunately, the pre-computed similarity scores of Lin (1998) (which use
simLin) are readily available, and thus the second-order similarity vectors can be easily
computed. We used Jaccard and cosine to compute the similarity of the vectors, and we
will refer to these similarity measures as simpreLin?Jac and sim
pre
Lin?cos hereinafter. Due to the
computational complexity, we did not compute second order similarity for the semantic
space software of Pado? and Lapata (2007).
Table 7 summarizes all similarity measures used in this study, and the corpus or
pre-computed similarity list used to build them.
3.4 Selectional Preferences for Prepositions
All the previously described models have been typically applied to verb-role selectional
preferences for NP arguments. Applying them to general semantic role labeling may
not be straightforward, however, and may require some extensions and adaptations.
For instance, not all argument candidates are noun phrases. Common arguments with
other syntactic types include prepositional, adjectival, adverbial, and verb phrases. Any
candidate argument without a nominal head cannot be directly treated by the models
described so far.
644
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 8
Example of prep-role lexical models for the preposition from, listed in alphabetical order.
Prep-role Number of Unique Examples
heads heads
from-Arg0 32 30 Abramson agency association barrier cut ...
from-Arg1 173 118 accident ad agency appraisal arbitrage ...
from-Arg2 708 457 academy account acquisition activity ad ...
from-Arg3 396 165 activity advertising agenda airport ...
from-Arg4 5 5 europe Golenbock system Vizcaya west
from-AM-ADV 19 17 action air air conception datum everyone ...
from-AM-CAU 5 4 air air design experience exposure
from-AM-DIR 79 71 agency alberta amendment america arson ...
from-AM-LOC 20 17 agency area asia body bureau orlando ...
from-AM-MNR 29 28 agency Carey company earnings floor ...
from-AM-TMP 33 21 april august beginning bell day dec. half ...
A particularly interesting case is that of prepositional phrases.8 Prepositions define
relations between the preposition attachment point and the preposition complement.
Prepositions are ambiguous with respect to these relations, which allows us to talk
about preposition senses. The Preposition Project (Litkowski and Hargraves 2005, 2006)
is an effort that produced a detailed sense inventory for English prepositions, which
was later used in a preposition sense disambiguation task at SemEval-2007 (Litkowski
and Hargraves 2007). Sense labels are defined as semantic relations, similar to those of
semantic role labels. In a more recent work, Srikumar and Roth (2011) presented a joint
model for extended semantic role labeling in which they show that determining the
sense of the preposition is mutually related to the task of labeling the argument role of
the prepositional phrase. Following the previous work, we also think that prepositions
define implicit selectional preferences, and thus decided to explore the use of preposi-
tional preferences with the aim of improving the selection of the appropriate semantic
roles. Addressing other arguments with non-nominal heads has been intentionally left
for further work.
The most straightforward way of including prepositional information in SP models
would be to add the preposition as an extra parameter of the SP. Initial experiments
revealed sparseness problems with collecting the ?verb, preposition, NP-head, role?
4-tuples from the training set. A simpler approach consists of completely disregarding
the verb information while collecting the prepositional preferences. That is, the selec-
tional preference for a preposition p and role r is defined as the union of all nouns w
found as heads of noun phrases embedded in prepositional phrases headed by p and
labeled with semantic role r. Then, one can apply any of the variants described in the
previous sections to calculate SP(p, r, w). Table 8 shows a sample of the lexical model for
the preposition from, organized according to the roles it plays.
These simple prep-role preferences largely avoided the sparseness problem while
still being able to capture relevant information to distinguish the appropriate roles in
many PP arguments. In particular, they proved to be relevant to distinguish between
adjuncts of the type ?[in New York]Location? vs. ?[in Winter]Temporal.? Nonetheless, we
8 Prepositional phrase is the second most frequent type of syntactic constituent for semantic arguments
(13%), after noun phrases (45%).
645
Computational Linguistics Volume 39, Number 3
are aware that not taking into account verb information also introduces some lim-
itations. In particular, the simplification could damage the performance on PP core
arguments, which are verb-dependent.9 For instance, our prepositional preferences
would not be able to suggest appropriate roles for the following two PP arguments:
?increase [ from seven cents a share]Arg3? and ?receive [ from the funds]Arg2,? because
the two head nouns (cents and funds) are semantically very similar. Assigning the
correct roles in these cases clearly depends on the information carried by the verbs.
Arg3 is the starting point for the predicate increase, whereas Arg2 refers to the source for
receive.
Our perspective on making this simple definition of prep-role SPs was practical and
just a starting point to play with the argument preferences introduced by prepositions.
A more complex model, distinguishing between prepositional phrases in adjunct and
core argument positions, should be able to model the linguistics better yet aleviate the
sparseness problem, and would hopefully produce better results.
The combination scheme for applying verb-role and prep-role is also very simple.
Depending on the syntactic type of the argument we apply one or the other model, both
in learning and testing:
 When the argument is a noun phrase, we use verb-role selectional
preferences.
 When the argument is a prepositional phrase, we use prep-role
selectional preferences.
We thus use a straightforward method to combine both kinds of SPs. More complex
possibilities like doing mixtures of both SPs are left for future work.
3.5 Role Classification with SP Models
Selectional preference models can be directly used to perform role classification. Given
a target predicate p and noun phrase candidate argument with head w, we simply select
the role r of the predicate which best fits the head according to the SP model. This
selection rule is formalized as:
ROLE(p, w) = arg max
r?Roles(p)
SP(p, r, w) (11)
with Roles(p) being the set of all roles applicable to the predicate p, and SP(p, r, w)
the goodness of fit of the selectional preference model for the head w, which can be
instantiated with all the variants mentioned in the previous subsections, including
the lexical model (Equation (8)) WordNet-based SP models (Equations (3) and (10)),
and distributional SP models (Equation (7)), using different similarity models as in
Table 7. Ties were broken returning the role coming first according to alphabetical
order. Note that in the case of SPwn (Equation 10) we need to use arg min rather than
arg max.
9 The percentage of prepositional phrases in core argument position is 48%, slightly lower than in adjunct
position (52%).
646
Zapirain et al Selectional Preferences for Semantic Role Classification
Note that if the candidate argument is a prepositional phrase with preposition p?
and embedded NP head word w, the classification rule uses the prep-role SP model,
that is:
ROLE(p, p?, w) = arg max
r?Roles(p? )
SP(p?, r, w)
4. Experiments with Selectional Preferences in Isolation
In this section we evaluate the ability of selectional preference models to discriminate
among different roles. For that, SP models will be used in isolation, according to the clas-
sification rule in Equation (11), to predict role labels for a set of (predicate, argument-head)
pairs. That is, we are interested in the discriminative power of the semantic information
carried by the SPs, factoring out any other feature commonly used by the state-of-the-
art SRL systems. The data sets used and the experimental results are presented in the
following.
4.1 Data Sets
The data used in this work are the benchmark corpus provided by the CoNLL-2005
shared task on SRL (Carreras and Ma`rquez 2005). The data set, of over 1 million tokens,
comprises PropBank Sections 02?21 for training, and Sections 24 and 23 for develop-
ment and testing, respectively. The Selectional Preferences implemented in this study
are not able to deal with non-nominal argument heads, such us those of NEG, DIS,
MOD (i.e., SPs never predict NEG, DIS, or MOD roles); but, in order to replicate the
same evaluation conditions of typical PropBank-based SRL experiments all arguments
are evaluated. That is, our SP models don?t return any prediction for those, and the
evaluation penalizes them accordingly.
The predicate?role?head triples (p, r, w) for generalizing the selectional preferences
are extracted from the arguments of the training set, yielding 71,240 triples, from which
5,587 different predicate-role selectional preferences (p, r) are derived by instantiating
the different models in Section 3. Tables 9 and 10 show additional statistics about some
of the most (and least) frequent verbs and prepositions in these tuples.
The test set contains 4,134 pairs (covering 505 different predicates) to be classified
into the appropriate role label. In order to study the behavior on out-of-domain data,
we also tested on the PropBanked part of the Brown corpus (Marcus et al 1994). This
corpus contains 2,932 (p, w) pairs covering 491 different predicates.
4.2 Results
The performance of each selectional preference model is evaluated by calculating
the customary precision (P), recall (R), and F1 measures.10 For all experiments re-
ported in this paper, we checked for statistical significance using bootstrap resampling
(100 samples) coupled with one-tailed paired t-test (Noreen 1989). We consider a result
significantly better than another if it passes this test at the 99% confidence interval.
10 P = Correct/Predicted ? 100, R = Correct/Gold ? 100, where Correct is the number of correct predictions,
Predicted is the number of predictions, and Gold is the total number of gold annotations.
F1 = 2PR/(P + R) is the harmonic mean of P and R.
647
Computational Linguistics Volume 39, Number 3
Table 9
Statistics of the three most and least frequent verbs in the training set. Role frame lists the types
of arguments seen in training for each verb; Heads indicates the total number of arguments for
the verb; Heads per role shows the average number of head words for each role; and Unique
heads per role lists the average number of unique head words for each verb?s role.
Verb Role frame Heads Heads Unique heads
per role per role
say Arg0,Arg1,Arg3,AM-ADV, AM-LOC, 7,488 1,069 371
AM-MNR, AM-TMP, AM-LOC,AM-MNR
have Arg0,Arg1,AM-ADV,AM-LOC 3,487 498 189
AM-MNR,AM-NEG,AM-TMP
make Arg0,Arg1,Arg2,AM-ADV 2,207 315 143
AM-LOC,AM-MNR,AM-TMP
... ... ... ... ...
accrete Arg1 1 1 1
accede Arg0 1 1 1
absolve Arg0 1 1 1
Table 10
Statistics of the three most and least frequent prepositions in the training set. Role frame lists
the types of arguments seen in training for each preposition; Heads indicates the total number
of arguments for the preposition; Heads per role shows the average number of head words for
each role; and Unique heads per role lists the average number of unique head words for each
preposition?s role.
Preposition Role frame Heads Heads Unique heads
per role per role
in Arg0,Arg1,Arg2,Arg3,Arg4,Arg5 6,859 403 81
AM-ADV,AM-CAU,AM-DIR,AM-DIS,
AM-EXT,AM-LOC,AM-MNR,AM-NEG,
AM-PNC,AM-PRD,AM-TMP
to Arg0,Arg1,Arg2,Arg3,Arg4, 3,495 233 94
AM-ADV,AM-CAU,AM-DIR,AM-DIS,
AM-EXT,AM-LOC,AM-MNR,AM-PNC,
AM-PRD,AM-TMP
for Arg0,Arg1,Arg2,Arg3,Arg4, 2,935 225 74
AM-ADV,AM-CAU,AM-DIR,AM-DIS,
AM-LOC,AM-MNR,AM-PNC,AM-TMP
... ... ... ... ...
beside Arg2, AM-LOC 2 1 1
atop Arg2, AM-DIR 2 1 1
aboard AM-LOC 1 1 1
Tables 11 and 12 list the results of the various selectional preference models in
isolation. Table 11 shows the results for verb-role SPs, and Table 12 lists the results
for the combination of verb-role and preposition-role SPs as described in Section 3.4.11
It is worth noting that the results of Tables 11 and 12 are calculated over exactly the
11 Note that the results reported here are not identical to those we reported in Zapirain, Agirre, and
Ma`rquez (2009). The differences are two-fold: (a) in our previous experiments we discarded roles such
as MOD, DIS, and NEG, whereas here we evaluate on all roles, and (b) our previous work used only the
subset of the data that could be mapped to VerbNet (around 50%), whereas here we inspect all tuples.
648
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 11
Results for verb-role SPs in the development partition of WSJ, the test partition of WSJ, and the
Brown corpus. For each experiment, we show precision (P), recall (R), and F1. Values in boldface
font are the highest in the corresponding column. F1 values marked with ? are significantly
lower than the highest F1 score in the same column.
Verb-role SPs
Development WSJ Test Brown Test
P R F1 P R F1 P R F1
lexical 73.94 21.81 33.69? 70.75 26.66 39.43? 59.39 05.51 10.08?
SPRes 43.65 35.70 39.28? 45.07 37.11 40.71? 36.34 27.58 31.33?
SPwn 53.09 43.35 47.73? 55.44 45.58 50.03? 41.76 31.58 35.96?
SPsimLin 53.88 44.35 48.65? 52.27 45.13 48.66? 48.30 32.08 38.56?
SPsimJac 48.40 45.53 46.92? 48.85 46.38 47.58? 42.10 34.34 37.82?
SPsimcos 52.37 49.26 50.77? 53.13 50.44 51.75? 43.24 35.27 38.85?
SPsimpreLin
60.29 59.54 59.91 59.93 59.38 59.65 50.79 48.39 49.56
SPsimpreLin?Jac
60.56 56.97 58.71 61.76 58.63 60.16 51.97 42.39 46.69?
SPsimpreLin?cos
60.22 56.64 58.37 61.12 58.12 59.63 51.92 42.35 46.65?
Table 12
Results for combined verb-role and prep-role SPs in the development partition of WSJ, the test
partition of WSJ, and the Brown corpus. For each experiment, we show precision (P), recall (R),
and F1. Values in boldface font are the highest in the corresponding column. F1 values marked
with ? are significantly lower from the highest F1 score in the same column.
Preposition-role and Verb-role SPs
Development WSJ Test Brown Test
P R F1 P R F1 P R F1
lexical 82.05 39.17 53.02? 82.98 43.77 57.31? 68.47 13.60 22.69?
SPRes 63.72 53.09 57.93? 63.47 53.24 57.91? 55.12 44.15 49.03?
SPwn 71.72 59.68 65.15? 65.70 63.88 64.78? 60.08 48.10 53.43?
SPsimLin 63.84 54.58 58.85? 63.75 56.40 59.85? 54.27 39.96 46.04?
SPsimJac 61.75 61.13 61.44? 61.83 61.40 61.61? 55.42 53.45 54.42?
SPsimcos 64.81 64.17 64.49? 64.67 64.22 64.44? 56.56 54.54 55.53?
SPsimpreLin
67.78 67.10 67.44? 68.34 67.87 68.10? 58.43 56.35 57.37?
SPsimpreLin?Jac
69.90 69.20 69.55 70.82 70.33 70.57 62.37 60.15 61.24
SPsimpreLin?cos
69.47 68.78 69.12 70.28 69.80 70.04 62.36 60.14 61.23
same example set. PP arguments are treated by the verb-role SPs by just ignoring the
preposition and considering the head noun of the NP immediately embedded in the PP.
It is worth mentioning that none of the SP models is able to predict the role when
facing a head word missing from the model. This is especially noticeable in the lexical
model, which can only return predictions for words seen in the training data and is
649
Computational Linguistics Volume 39, Number 3
penalized in recall. WordNet based models, which have a lower word coverage com-
pared to distributional similarity?based models, are also penalized in recall.
In both tables, the lexical row corresponds to the baseline lexical match method.
The following rows correspond to the WordNet-based selectional preference models.
The distributional models follow, including the results obtained by the three similarity
formulas on the co-occurrences extracted from the BNC (simJac, simcos simLin), and the
results obtained when using Lin?s pre-computed similarities directly (simpreLin) and as a
second-order vector (simpreLin?Jac and sim
pre
Lin?cos).
First and foremost, this experiment proves that splitting SPs into verb- and
preposition-role SPs yields better results. The comparison of Tables 11 and 12 shows
that the improvements are seen for both precision and recall, but especially remarkable
for recall. The overall F1 improvement is of up to 10 points. Unless stated otherwise, the
rest of the analysis will focus on Table 12.
As expected, the lexical baseline attains a very high precision in all data sets, which
underscores the importance of the lexical head word features in argument classification.
Its recall is quite low, however, especially in Brown, confirming and extending Pradhan,
Ward, and Martin (2008), who also report a similar performance drop for argument
classification on out-of-domain data. All our selectional preference models improve
over the lexical matching baseline in recall, with up to 24 absolute percentage points
in the WSJ test data set and 47 absolute percentage points in the Brown corpus. This
comes at the cost of reduced precision, but the overall F-score shows that all selectional
preference models are well above the baseline, with up to 13 absolute percentage
points on the WSJ data sets and 39 absolute percentage points on the Brown data set.
The results, thus, show that selectional preferences are indeed alleviating the lexical
sparseness problem.12
As an example, consider the following head words of potential arguments of the
verb wear found in the test set: doctor, men, tie, shoe. None of these nouns occurred as
heads of arguments of wear in the training data, and thus the lexical feature would
be unable to predict any role for them. Using selectional preferences, we successfully
assigned the A0 role to doctor and men, and the A1 role to tie and shoe.
Regarding the selectional preference variants, WordNet-based and first-order distri-
butional similarity models attain similar levels of precision, but the former have lower
recall and F1. The performance loss on recall can be explained by the limited lexical
coverage of WordNet when compared with automatically generated thesauri. Examples
of words missing in WordNet include abbreviations (e.g., Inc., Corp.) and brand names
(e.g., Texaco, Sony).
The comparison of the WordNet-based models indicates that our proposal for a
lighter method of WordNet-based selectional preference was successful, as our simpler
variant performs better than Resnik?s method. In manual analysis, we realized that
Resnik?s model tends to always predict the most frequent roles whereas our model
covers a wider role selection. Resnik?s tendency to overgeneralize makes more frequent
roles cover all the vocabulary, and the weighting system penalizes roles with fewer
occurrences.
12 We verified that the lexical model shows higher classification accuracy than all the more elaborate SP
models on the subset of cases covered by both the lexical and the SP models. In this situation, if we aimed
at constructing the best role classifier with SPs alone we could devise a back-off strategy, in the style of
Chambers and Jurafsky (2010), which uses the predictions of the lexical model when present and one of
the SP models if not. As presented in Section 5, however, our main goal is to integrate these SP models
in a real end-to-end SRL system, so we keep their analysis as independent predictors for the moment.
650
Zapirain et al Selectional Preferences for Semantic Role Classification
The results for distributional models indicate that the SPs using Lin?s ready-made
thesaurus (simpreLin) outperforms Pado? and Lapata?s distributional similarity model (Pado?
and Lapata 2007) calculated over the BNC (simLin) in both Tables 11 and 12. This might
be due to the larger size of the corpus used by Lin, but also by the fact that Lin used a
newspaper corpus, compared with the balanced BNC corpus. Further work would be
needed to be more conclusive, and, if successful, could improve further the results of
some SP models.
Among the three similarity metrics using Pado? and Lapata?s software, the cosine
seems to perform consistently better. Regarding the comparison between first-order and
second-order using pre-computed similarity models, the results indicate that second-
order is best when using both the verb-role and prep-role models (cf. Table 12), although
the results for verb-roles are mixed (cf. Table 11). Jaccard seems to provide slightly better
results than cosine for second-order vectors.
In summary, the use of separate verb-role and prep-role models produces the best
results, and second-order similarity is highly competitive. As far as we know, this is
the first time that prep-role models and second-order models are applied to selectional
preference modeling.
5. Semantic Role Classification Experiments
In this section we advance the use of SP in SRL one step further and show that selec-
tional preferences are able to effectively improve performance of a state-of-the-art SRL
system. More concretely, we integrate the information of selectional preference models
in a SRL system and show significant improvements in role classification, especially
when applied to out-of-domain corpora.13
We will use some of the selectional preference models presented in the previous
section. We will focus on the combination of verb-role and prep-role models. Regarding
the similarity models, we will choose the best two performing models from each of
the three families that we tried, namely, the two WordNet models, the two best models
based on the BNC corpus (simJac,simcos), and the two best models based on Lin?s precom-
puted similarity metrics (sim2Jac,sim
2
cos). We left the exploration of other combinations for
future work.
5.1 Integrating Selectional Preferences in Role Classification
For these experiments, we modified the SwiRL SRL system, a state-of-the-art semantic
role labeling system (Surdeanu et al 2007). SwiRL ranked second among the systems
that did not implement model combination at the CoNLL-2005 shared task and fifth
overall (Carreras and Ma`rquez 2005). Because the focus of this section is on role classi-
fication, we modified the SRC component of SwiRL to use gold argument boundaries,
that is, we assume that semantic role identification works perfectly. Nevertheless, for a
realistic evaluation, all the features in the role classification model are generated using
actual syntactic trees generated by the Charniak parser (Charniak 2000).
The key idea behind our approach is model combination: We generate a battery of
base models using all resources available and we combine their outputs using multi-
ple strategies. Our pool of base models contains 13 different models: The first is the
13 The data sets used for the experiments reported in this section are exactly the ones described in
Section 4.1.
651
Computational Linguistics Volume 39, Number 3
unmodified SwiRL SRC, the next six are the selected SP models from the previous
section, and the last six are variants of SwiRL SRC. In each variant, the feature set of
the unmodified SwiRL SRC model is extended with a single feature that models the
choice of a given SP, for example, SRC+SPres contains an extra feature that indicates the
choice of Resnik?s SP model.14
We combine the outputs of these base models using two different strategies: (a)
majority voting, which selects the label predicted by most models, and (b) meta-
classification, which uses a supervised model to learn the strengths of each base model.
For the meta-classification model, we opted for a binary classification approach: First,
for each constituent we generate n data points, one for each distinct role label proposed
by the pool of base models; then we use a binary meta-classifier to label each candidate
role as either correct or incorrect. We trained the meta-classifier on the usual PropBank
training partition, using 10-fold cross-validation to generate outputs for the base
models that require the same training material. At prediction time, for each candidate
constituent we selected the role label that was classified as correct with the highest
confidence.
The binary meta-classifier uses the following set of features:
 Labels proposed by the base models, for example, the feature SRC+SPres=Arg0
indicates that the SRC+SPres base model proposed the Arg0 label. We add
13 such features, one for each base model. Intuitively, this feature allows
the meta-classifier to learn the strengths of each base model with respect
to role labels: SRC+SPres should be trusted for the Arg0 role, and so on.
 Boolean value indicating agreement with the majority vote, for example, the
feature Majority=true indicates that the majority of the base models
proposed the same label as the one currently considered by the
meta-classifier.
 Number of base models that proposed this data point?s label. To reduce sparsity,
for each number of base models, N, we generate N distinct features
indicating that the number of base models that proposed this label is
larger than k, where k ? [0, N). For example, if two base models proposed
the label under consideration, we generate the following two features:
BaseModelNumber>0 and BaseModelNumber>1. This feature provides finer
control over the number of votes received by a label than the majority
voter, for example, the meta-classifier can learn to trust a label if more
than two base models proposed it, even if the majority vote disagrees.
 List of actual base models that proposed this data point?s label. We store a
distinct feature for each base model that proposed the current label, and
also a concatenation of all these base model names. The latter feature is
designed to allow the meta-classifier to learn preferences for certain
combinations of base models. For example, if two base models, SPres and
SPwn, proposed the label under consideration, we generate three features:
Base=SPres, Base=SPwn, and Base=SPres+SPwn.
14 Adding more than one SP output as a feature in SwiRL?s SRC model did not improve performance in
development over the single-SP SRC model. Our conjecture is that the large number of features in SRC
has the potential to drown the SP-based features. This may be accentuated when there are more SP-based
features because their signal is divided among them due to their overlap. We have also tried to add the
input features of the SP models directly to the SRC model but this also proved to be unsuccessful during
development.
652
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 13
Results for the combination approaches. Accuracy shows the overall results. Core and Adj
contain F1 results restricted to the core numbered roles and adjuncts, respectively. SRC is
SwiRL?s standalone SRC model; +SPx stands for the SRC model extended with a feature given by
the corresponding SP model. Values in boldface font are the highest in the corresponding
column. Accuracy values marked with ? are significantly lower than the highest accuracy score
in the same column.
WSJ test Brown test
Acc. Core F1 Adj. F1 Acc. Core F1 Adj. F1
SRC 90.83? 93.25 81.31 79.52 84.42 57.76
+SPRes 90.76? 93.17 81.08 79.86? 84.52 59.24
+SPwn 90.56? 92.88 81.11 79.73? 84.26 59.69
+SPsimJac 90.86? 93.37 80.30 79.83? 84.43 59.54
+SPsimcos 90.87? 93.33 80.92 80.50? 85.14 60.16
+SPsimpreLin?Jac
90.95? 93.03 82.75 80.75? 85.62 59.63
+SPsimpreLin?cos
91.23? 93.78 80.56 80.48? 84.95 61.01
Meta-classifier 92.43 94.62 84.00 81.94 86.25 63.36
Voting 92.36 94.57 83.68 82.15 86.37 63.78
5.2 Results for Semantic Role Classification
Table 13 compares the performance of both combination approaches against the stand-
alone SRC model. In the table, the SRC+SP? models stand for SRC classifiers enhanced
with one feature from the corresponding SP. The meta-classifier shown in the table com-
bines the output of all the 13 base models introduced previously. We implemented the
meta-classifier using Support Vector Machines (SVMs)15 with a quadratic polynomial
kernel, and C = 0.01 (tuned in the development set).16 Lastly, Table 13 shows the results
of the voting strategy, over the same set of base models.
In the columns we show overall classification accuracy and F1 results for both core
arguments (Core) and adjunct arguments (Adj.). Note that for the overall SRC scores, we
report classification accuracy, defined as ratio of correct predictions over total number
of arguments to be classified. The reason for this is that the models in this section always
return a label for all arguments to be classified, and thus accuracy, precision, recall, and
F1 are all equal.
Table 13 indicates that four out of the six SRC+SP? models perform better than the
standalone SRC model in domain (WSJ), and all of them outperform SRC out of domain
(Brown). The improvements are small, however, and, generally, not statistically signifi-
cant. On the other hand, the meta-classifier outperforms the original SRC model both
in domain (17.4% relative error reduction; 1.60 points of accuracy improvement) and
out of domain (13.4% relative error reduction; 2.42 points of accuracy improvement),
and the differences are statistically significant. This experiment proves our claim that
SPs can be successfully used to improve semantic role classification. It also underscores
the fact that combining SRC and SPs is not trivial, however. Our hypothesis is that this
15 http://svmlight.joachims.org.
16 We have also trained the meta-classifier with other learning algorithms (e.g., logistic regression with
L2 regularization) and we obtained similar but slightly lower results.
653
Computational Linguistics Volume 39, Number 3
is caused by the large performance disparity (20 F1 points in domain and 18 out of
domain) between the original SRC model and the standalone SP methods.
Interestingly, the meta-classifier performs only marginally better than the voting ap-
proach in domain and slightly worse out of domain. We believe that this is another effect
of the above observation: Given the weaker SP-based features, the meta-classifier does
not learn much beyond a majority vote, which is exactly what the simpler, unsuper-
vised voting method models. Nevertheless, regardless of the combination method, this
experiment emphasizes that infusing SP information in the SRC task is beneficial.
Table 13 also shows that our approach yields consistent improvements for both
core and adjunct arguments. Out of domain, we see a bigger accuracy improvement
for adjunct arguments (6.02 absolute points) vs. core arguments (1.83 points, for the
voting model). This is to be expected, as most core arguments fall under the Arg0 and
Arg1 classes, which can typically be disambiguated based on syntactic information (i.e.,
subject vs. object). On the other hand, there are no syntactic hints for adjunct arguments,
so the system learns to rely more on SP information in this case.
Regarding the performance of individual combinations of SRC and SP methods
(e.g., SRC+SPRes), the differences among SP models in Table 13 are much smaller
than in Table 12. SPsimpreLin?cos and SPsim
pre
Lin?Jac
yield the best results in both cases, and
distributional methods are slightly stronger than WordNet-based methods. SPRes and
SPwn perform similarly when combined, with a small lead for Resnik?s method. The
smaller differences and changes in the rank among SP methods are due to the complex
interactions when combining SP models with the SRC system.
Table 14
Precision (P), recall (R), and F1 results per argument type for the standalone SRC model and
the meta-classifier, in the two test data sets (WSJ and Brown). Due to space limitations, the
AM- prefix has been dropped from the labels of all adjuncts. When classifying all arguments
(last row), the F1 score is an accuracy score because in this scenario P = R = F1. We checked for
statistical significance for the overall F1 scores (All row). Values in boldface font indicate the
highest F1 score in the corresponding row and block. F1 values marked with ? are significantly
lower than the corresponding highest F1 score.
WSJ test Brown test
SRC Meta-classifier SRC Meta-classifier
P R F1 P R F1 P R F1 P R F1
Arg0 93.6 96.7 95.1 95.1 97.4 96.2 87.6 89.3 88.4 89.4 91.0 90.2
Arg1 93.3 94.5 93.9 94.2 95.7 95.0 84.3 90.6 87.3 86.2 91.9 89.0
Arg2 86.0 82.6 84.3 87.8 87.4 87.6 52.7 56.8 54.7 55.9 59.9 57.8
Arg3 77.6 63.4 69.8 82.4 68.3 74.7 36.4 19.0 25.0 45.8 26.2 33.3
Arg4 86.8 78.6 82.5 89.5 81.0 85.0 59.4 34.5 43.7 67.9 34.5 45.8
Core 92.9 93.6 93.3 94.2 95.1 94.6 82.6 86.3 84.4 84.6 87.9 86.3
ADV 58.5 51.4 54.7 64.4 52.3 57.7 45.1 24.3 31.6 51.9 25.7 34.4
CAU 61.1 71.0 65.7 80.0 77.4 78.7 64.7 45.8 53.7 84.6 45.8 59.5
DIR 46.2 25.0 32.4 68.8 45.8 55.0 64.7 45.8 53.7 73.9 44.5 55.6
DIS 84.3 82.7 83.5 95.6 82.7 88.7 52.6 27.0 35.7 54.5 32.4 40.7
EXT 50.0 12.5 20.0 50.0 12.5 20.0 0.0 0.0 0.0 0.0 0.0 0.0
LOC 85.2 80.9 83.0 85.0 84.7 84.8 67.8 61.2 64.3 68.3 68.7 68.5
MNR 55.8 54.1 55.0 68.9 61.7 65.1 47.4 38.9 42.7 59.2 49.3 53.8
PNC 51.9 37.8 43.8 62.5 40.5 49.2 51.7 39.5 44.8 53.3 42.1 47.1
TMP 93.6 95.9 94.7 92.8 95.9 94.4 79.0 78.1 78.5 84.1 83.2 83.7
Adj 83.1 79.6 81.3 86.2 81.9 84.0 64.9 52.1 57.8 69.8 58.0 63.4
All ? ? 90.8? ? ? 92.4 ? ? 79.5? ? ? 81.9
654
Zapirain et al Selectional Preferences for Semantic Role Classification
Lastly, Table 14 shows a breakdown of the results by argument type for the orig-
inal SRC model and the meta-classifier (results are also presented over all numbered
arguments, Core, adjuncts, and Adj). This comparison emphasizes the previous obser-
vation that SPs are more useful for arguments that are independent of syntax than for
arguments that are usually tied to certain syntactic constructs (i.e., Arg0 and Arg1). For
example, in domain the meta-classifier improves Arg0 classification with 1.1 F1 points,
but it boosts the classification performance for causative arguments (AM-CAU) with 13
absolute points. A similar behavior is observed out of domain. For example, whereas
Arg0 classification is improved with 1.7 points, the classification of manner arguments
(AM-MNR) is improved by 11 points. All in all, with two exceptions, selectional prefer-
ences improve classification accuracy for all argument types, both in and out of domain.
The previous experiments showed that a meta-classifier (and a voting approach)
over a battery of base models improves over the performance of each individual clas-
sifier. Given that half of our base models are all relatively minor changes of the same
original classifier (SwiRL), however, it would be desirable to ensure that the overall
performance gain of the meta-classification system is due to the infusion of semantic
information that is missing in the baseline SRC, and not to a regularization effect coming
from the ensemble of classifiers. The qualitative analysis presented in Section 6 will
reinforce this hypothesis.
5.3 Results for End-to-End Semantic Role Labeling
Lastly, we investigate the contribution of SPs in an end-to-end SRL system. As discussed
before, our approach focuses on argument classification, a subtask of complete SRL,
because this component suffers in the presence of lexical data sparseness (Pradhan,
Ward, and Martin 2008). To understand the impact of SPs on the complete SRL task we
compared two SwiRL models: one that uses the original classification model (the SRC
line in Table 13) and another that uses our meta-classifier model (the Meta-classifier
line in Table 13). To implement this experiment we had to modify the publicly down-
loadable SwiRL model, which performs identification and classification jointly, using a
single multi-class model. We changed this framework to a pipeline model, which first
performs argument identification (i.e., is this constituent an argument or not?), followed
by argument classification (i.e., knowing that this constituent is an argument, what is
its label?).17 We used the same set of features as the original SwiRL system and the
original model to identify argument boundaries. This pipeline model allowed us to
easily plug in different classification models, which offers a simple platform to evaluate
the contribution of SPs in an end-to-end SRL system.
Table 15 compares the original SwiRL pipeline (SwiRL in the table) with the pipeline
model where the classification component was replaced with the meta-classifier previ-
ously introduced (SwiRL w/ meta). The latter model backs off to the original classifi-
cation model for candidates that are not covered by our current selectional preferences
(i.e., are not noun phrases or prepositional phrases containing a noun phrase as the
second child). We report results for the test partitions of WSJ and Brown in the same
table. Note that these results are not directly comparable with the results in Tables 13
and 14, because in those initial experiments we used gold argument boundaries whereas
17 This pipeline model performs slightly worse than the original SwiRL on the WSJ data and slightly better
on Brown.
655
Computational Linguistics Volume 39, Number 3
Table 15
Precision (P), recall (R), and F1 results per argument for the end-to-end semantic role labeling
task. We compared two models: the original SwiRL model and the one where the classification
component was replaced with the meta-classifier introduced at the beginning of the section. We
used the official CoNLL-2005 shared-task scorer to produce these results. We checked for
statistical significance for the overall F1 scores (All row). Values in boldface font indicate the
highest F1 score in the corresponding row and block. F1 values marked with ? are significantly
lower than the corresponding highest F1 score.
WSJ test Brown test
SwiRL SwiRL w/ meta SwiRL SwiRL w/ meta
P R F1 P R F1 P R F1 P R F1
Arg0 87.0 81.6 84.2 87.8 81.9 84.8 86.6 81.3 83.9 87.3 81.7 84.4
Arg1 79.1 71.8 75.3 79.4 72.1 75.6 70.2 64.6 67.3 71.1 65.2 68.0
Arg2 70.0 56.6 62.6 69.2 58.3 63.3 41.8 42.7 42.2 42.3 44.6 43.4
Arg3 72.4 43.9 54.7 72.6 44.5 55.2 36.4 12.9 19.0 34.6 14.5 20.5
Arg4 73.3 61.8 67.0 73.8 60.8 66.7 48.8 25.6 33.6 44.4 25.6 32.5
ADV 59.4 50.6 54.6 59.5 50.0 54.4 49.0 38.2 42.9 49.9 38.5 43.5
CAU 61.5 43.8 51.2 66.0 45.2 53.7 58.7 35.5 44.3 59.1 34.2 43.3
DIR 44.7 20.0 27.6 50.0 22.6 30.9 59.0 27.2 37.2 61.3 25.9 36.5
DIS 76.1 63.8 69.4 77.0 63.8 69.7 58.8 41.0 48.3 59.7 41.3 48.9
EXT 72.7 50.0 59.3 72.7 50.0 59.3 20.0 8.1 11.5 21.4 8.1 11.8
LOC 64.7 52.9 58.2 64.8 55.4 59.7 48.3 37.7 42.3 46.8 40.5 43.5
MNR 59.1 52.0 55.3 61.4 51.7 56.2 53.8 47.3 50.3 55.9 48.3 51.8
PNC 47.1 34.8 40.0 46.4 33.9 39.2 51.8 26.4 35.0 52.4 26.7 35.1
TMP 78.7 71.4 74.9 78.4 71.5 73.8 59.7 60.6 60.2 61.0 61.2 61.1
All 79.7 70.9 75.0? 80.0 71.3 75.4 71.8 64.2 67.8? 72.4 64.6 68.4
Table 15 shows results for an end-to-end model, which includes predicted argument
boundaries.
Table 15 shows that the use of selectional preferences improves overall results when
using predicted argument boundaries as well. Selectional preferences improve F1 scores
for four out of five core arguments in both WSJ and Brown, for six out of nine modifier
arguments in WSJ, and for seven out of nine modifier arguments in Brown. Notably, the
SPs improve results for the most common argument types (Arg0 and Arg1). All in all,
SPs yield a 0.4 F1 point improvement in WSJ and 0.6 F1 point improvement in Brown.
These improvements are small but they are statistically significant. We consider these re-
sults encouraging, especially considering that only a small percentage of arguments are
actually inspected by selectional preferences. This analysis is summarized in Table 16,
which lists how many argument candidates are inspected by the system in its different
stages. The table indicates that the vast majority of argument candidates are filtered
out by the argument identification component, which does not use SPs. Because of this,
even though approximately 50% of the role classification decisions can be reinforced
with SPs, only 4.5% and 3.6% of the total number of argument candidates in WSJ and
Brown, respectively, are actually inspected by the classification model that uses SPs.
6. Analysis and Discussion
We conducted a complementary manual analysis to further verify the usefulness of the
semantic information provided by the selectional preferences. We manually inspected
100 randomly selected classification cases, 50 examples in which the meta-classifier is
656
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 16
Counts for argument candidates for the two test partitions on the end-to-end semantic role
labeling task. The Predicted non-arguments line indicates how many candidate arguments are
classified as non-arguments by the argument identification classifier. The Incompatible with SPs
line indicates how many candidates were classified as arguments but cannot be modeled by our
current SPs (i.e., they are not noun phrases or prepositional phrases containing a noun phrase as
the second child). Lastly, the Compatible with SPs line lists how many candidates were both
classified as likely arguments and can be modeled by the SPs.
WSJ test Brown test
Predicted non-arguments 158,310 184,958
Incompatible with SPs 5,739 11,167
Compatible with SPs 7,691 7,867
Total 171,740 203,992
correct and the baseline SRC (SwiRL) is wrong, and 50 where the meta-classifier chooses
the incorrect classifier and the SRC is right. Interestingly, we observed that the majority
of cases have a clear linguistic interpretation, shedding light on the reasons why the
meta-classifier using SP information manages to correct some erroneous predictions of
the original SRC model, but also on the limitations of selectional preferences.
Regarding the success of the meta-classifier, the studied cases generally correspond
to low frequency verb?argument head pairs, in which the baseline SRC might have
had problems with generalization. In 29 of the cases (?58%), the syntactic information
is not enough to disambiguate the proper role, tends to indicate a wrong role label,
or it confuses the SRC because it contains errors. Most of the semantically based SP
predictions are correct, however, so the meta-classifier does select the correct role label.
In another 15 cases (?30%) the source of the baseline SRC error is not clear, but still,
several SP models suggest the correct role, giving the opportunity to the meta-classifier
to make the right choice. Finally, in the remaining six cases (?12%) a ?chance effect? is
observed: The failure of the baseline SRC model does not have a clear interpretation and,
moreover, most SP predictions are actually wrong. In these situations, several labels are
predicted with the same confidence, and the meta-classifier selects the correct one by
chance.
Figure 2 shows four real examples in which we see the importance of the infor-
mation provided by the selectional preferences. In example (a), the verb flash never
occurs in training with the argument head word news. The syntactic structure alone
strongly suggests Arg0, because the argument is an NP just to the left of a verb in active
form. This is probably why the baseline SRC incorrectly predicts Arg0. Some semantic
information is needed to know that the word news is not the agent of the predicate
(Arg0), but rather the theme (thing shining, Arg1). Selectional preferences make this
work perfectly, because all variants predict the correct label by signaling that news is
much more compatible with flash in Arg1 position rather than Arg0.
In example (b), the predicate promise expects a person as Arg1 (person promised to,
Recipient) and an action as Arg2 (promised action, Theme). Moreover, the presence of
Arg2 is obligatory. The syntactic structure is correct but does not provide the semantic
(Arg1 should be a person) or structural information (the assignment of Arg1 would have
required an additional Arg2) needed to select the appropriate role. SwiRL does not have
it either, and it assigns the incorrect Arg1 label. Most SP models correctly predict that
investigation is more similar to the heads of Arg2 arguments of promise than to the
heads of Arg1 arguments, however.
657
Computational Linguistics Volume 39, Number 3
(a) Several traders could be seen shaking their heads when (([the news]Arg0?Arg1)NP
( flashed)VP)S .
(b) Italian President Francesco Cossiga (promised ([a quick investigation into
whether Olivetti broke Cocom rules]Arg1?Arg2)NP)VP.
(c) Annual payments (will more than double ([from (a year ago)NP]TMP?Arg3)PP to
about $240 million ? ? ? )VP ? ? ?
(d) Procter & Gamble Co. plans to (begin ((testing (next month)NP)VP)S ([a superco.
detergent that ? ? ? washload]Arg0?Arg1)NP)VP .
Figure 2
Examples of incorrect SwiRL role assignments fixed by the meta-classifier. In each sentence, the
verb is emphasized in italics and the head word for the selectional preferences is boldfaced. The
argument under focus is marked within square brackets. x ? y means that the incorrect label x
assigned by the baseline SwiRL model is corrected into role label y by the combined system.
Finally, examples also contain simplified syntactic annotations from the test set predicted
syntactic layer, which are used for the discussion in the text.
In example (c) we see the application of prep-role selectional preferences. In that
sentence, the baseline SRC is likely confused by the content word feature of the PP
?from a year ago? (Surdeanu et al 2003). In PropBank, ?year? is a strong indicator
of a temporal adjunct (AM-TMP). The predicate double, however, describes the Arg3
argument as ?starting point? of the action and it is usually introduced by the preposition
from. This is very common also for other motion verbs (go, rise, etc.), resulting in the
from-Arg3 selectional preference containing a number of heads of temporal expressions,
in particular many more instances of the word year than the from-AM-TMP selectional
preference. As a consequence, the majority of SP models predict the correct Arg3 label.
Finally, example (d) highlights that selectional preferences increase robustness in
front of parsing errors. In this example, the NP ?a superco. detergent? is incorrectly
attached to ?begin? instead of the predicate testing by the syntactic parser. This produces
many incorrect features derived from syntax (syntactic frame, path, etc.) that may con-
fuse the baseline SRC model, which ends up producing an incorrect Arg0 assignment.
Most of the SP models, however, predict that detergent is not a plausible Agent for test
(?examiner?), but instead it fits best with the Arg1 position (?examined?).
Nevertheless, selectional preferences have a significant limitation: They do not
model syntactic structures, which often give strong hints for classification. In fact, the
vast majority of the situations where the meta-classifier performs worse than the origi-
nal SRC model are cases that are syntax-driven, hence situations that are incompletely
addressed by the current SP models. Even though the SRC and the SRC+SP models
have features that model syntax, they can be overwhelmed by the SP features and
standalone models, which leads to incorrect meta-classification results. Figure 3 shows a
few representative examples in this category. In the first example in the figure, the meta-
classifier changes the correctly assigned label Arg2 to Arg1, because most SP models
favor the Arg1 label for the argument ?test.? In the PropBank training corpus, however,
the argument following the verb fail is labeled Arg2 in 79% of the cases. Because the
SP models do not take into account syntax or positional information, this syntactic
preference is lost. Similarly, SPs do not model the fact that the verb buy is seldom
preceded by an Arg1 argument, or the argument immediately following the verb precede
tends to be Arg1, hence the incorrect classifications in Figure 3 (b) and (c). All these
658
Zapirain et al Selectional Preferences for Semantic Role Classification
(a) Some ?circuit breakers? installed after the October 1987 crash (failed ([their first
test ]Arg2?Arg1)NP)VP...
(b) Many fund managers argue that now?s ([the time]TMP?Arg1)NP (to buy)VP)S .
(c) Telephone volume was up sharply, but it was still at just half the level of the
weekend (preceding ([Black Monday ]Arg1?TMP)NP)VP .
Figure 3
Examples of incorrect assignments by the meta-classifier. In each sentence, the verb is
emphasized in italics and the head word for the selectional preferences is boldfaced. The
argument under focus is marked within square brackets. x ? y means that the correct
x label assigned by the baseline model is wrongly converted into y by the meta-classifier.
As in Figure 2, examples also contain simplified syntactic annotations taken from the test
set predicted syntactic layer.
examples are strong motivation for SP models that model both lexical and syntactic
preferences. We will address such models in future work.
7. Conclusions
Current systems usually perform SRL in two pipelined steps: argument identification
and argument classification. Whereas identification is mostly syntactic, classification
requires semantic knowledge to be taken into account. In this article we have shown
that the lexical heads seen in training data are too sparse to assign the correct role,
and that selectional preferences are able to generalize those lexical heads. In fact, we
show for the first time that the combination of the predictions of several selectional
preference models with a state-of-the-art SRC system yields significant improvements in
both in-domain and out-of-domain test sets. These improvements to role classification
translate into small but statistically significant improvements in an end-to-end semantic
role labeling system. We find these results encouraging considering that in the complete
semantic role labeling task only a small percentage of argument candidates are affected
by our modified role classification model. The experiments were carried out over the
well-known CoNLL-2005 data set, based on PropBank.
We applied several selectional preference models, based on WordNet and distribu-
tional similarity. Our experiments show that all models outperform the pure lexical
matching approach, with distributional methods performing better that WordNet-based
methods, and second-order similarity models being the best. In addition to the tradi-
tional selectional preferences for verbs, we introduce the use of selectional preferences
for prepositions, which are applied to classifying prepositional phrases. The combi-
nation of both types of selectional preferences improves over the use of selectional
preferences for verbs alone.
The analysis performed over the cases where the base SRC system and the com-
bined system differed showed that the selectional preferences are specially helpful when
syntactic information is either incorrect or insufficient to disambiguate the correct role.
The analysis also highlighted that the limitations of selectional preferences for modeling
syntactic structures introduce some errors in the combined model. Those errors could
be addressed if the SP models included some syntactic information.
Our research leaves the door open for tighter integration of semantic and syntactic
information for Semantic Role Labeling. We introduced selectional preferences in the
SRC system as simple features, but models which extend syntactic structures with
659
Computational Linguistics Volume 39, Number 3
selectional preferences (or vice versa) could overcome some of the errors that our system
introduced. Extending the use of selectional preferences to other syntactic types beyond
noun phrases and prepositional phrases would be also of interest. In addition, the
method for combining selectional preferences for verbs and prepositions was naive,
and we expect that a joint model of verb and preposition preferences for prepositional
phrases would improve results further. Finally, individual selectional preference meth-
ods could be improved and newer methods incorporated, which could further improve
the results.
Acknowledgments
The authors would like to thank the three
anonymous reviewers for their detailed
and insightful comments on the submitted
version of this manuscript, which helped
us to improve it significantly in this revision.
This work was partially funded by
the Spanish Ministry of Science and
Innovation through the projects OpenMT-2
(TIN2009-14675-C03) and KNOW2
(TIN2009-14715-C04-04). It also received
financial support from the Seventh
Framework Programme of the EU
(FP7/2007- 2013) under grant agreements
247762 (FAUST) and 247914 (MOLTO).
Mihai Surdeanu was supported by the Air
Force Research Laboratory (AFRL) under
prime contract no. FA8750-09-C-0181.
Any opinions, findings, and conclusion
or recommendations expressed in this
material are those of the authors and do
not necessarily reflect the view of the
Air Force Research Laboratory (AFRL).
References
Agirre, Eneko, Timothy Baldwin, and
David Martinez. 2008. Improving
parsing and PP attachment performance
with sense information. In Proceedings
of ACL-08: HLT, pages 317?325,
Columbus, OH.
Agirre, Eneko, Kepa Bengoetxea, Koldo
Gojenola, and Joakim Nivre. 2011.
Improving dependency parsing with
semantic classes. In Proceedings of the
49th Annual Meeting of the Association
for Computational Linguistics: Human
Language Technologies, pages 699?703,
Portland, OR.
Agirre, Eneko and David Martinez. 2001.
Learning class-to-class selectional
preferences. In Proceedings of the 2001
Workshop on Computational Natural
Language Learning (CoNLL-2001),
pages 1?8, Toulouse.
Agirre, Eneko and German Rigau. 1996.
Word sense disambiguation using
conceptual density. In Proceedings of the
16th Conference on Computational
Linguistics - Volume 1, COLING ?96,
pages 16?22, Stroudsburg, PA.
Baroni, Marco and Alessandro Lenci.
2010. Distributional memory: A general
framework for corpus-based semantics.
Computational Linguistics, 36(4):673?721.
Bergsma, Shane, Dekang Lin, and Randy
Goebel. 2008. Discriminative learning of
selectional preference from unlabeled text.
In Proceedings of EMNLP, pages 59?68,
Honolulu, HI.
Boas, H. C. 2002. Bilingual framenet
dictionaries for machine translation.
In Proceedings of the Third International
Conference on Language Resources and
Evaluation (LREC), pages 1,364?1,371,
Las Palmas de Gran Canaria.
Brockmann, Carsten and Mirella Lapata.
2003. Evaluating and combining
approaches to selectional preference
acquisition. In Proceedings of the 10th
Conference of the European Chapter of the
Association of Computational Linguistics
(EACL-2003), pages 27?34, Budapest.
Carreras, X. and L. Ma`rquez. 2004.
Introduction to the CoNLL-2004
Shared Task: Semantic Role Labeling.
In Proceedings of the Eighth Conference
on Computational Natural Language
Learning (CoNLL-2004), pages 89?97,
Boston, MA.
Carreras, X. and L. Ma`rquez. 2005.
Introduction to the CoNLL-2005
Shared Task: Semantic Role Labeling.
In Proceedings of the Ninth Conference
on Computational Natural Language
Learning (CoNLL-2005), pages 152?164,
Ann Arbor, MI.
Chakraborti, Sutanu, Nirmalie Wiratunga,
Robert Lothian, and Stuart Watt. 2007.
Acquiring word similarities with higher
order association mining. In Proceedings
of the 7th International Conference on
Case-Based Reasoning: Case-Based Reasoning
Research and Development, ICCBR ?07,
pages 61?76, Berlin.
660
Zapirain et al Selectional Preferences for Semantic Role Classification
Chambers, Nathanael and Daniel Jurafsky.
2010. Improving the use of pseudo-words
for evaluating selectional preferences. In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics,
pages 445?453, Uppsala, Sweden.
Charniak, E. 2000. A maximum-entropy
inspired parser. In Proceedings of the 1st
Meeting of the North American Chapter
of the Association for Computational
Linguistics (NAACL-2000), pages 132?139,
Seattle, WA.
Clark, Stephen and Stephen Weir. 2002.
Class-based probability estimation
using a semantic hierarchy. Computational
Linguistics, 28(2):187?206.
Edmonds, Philip. 1997. Choosing the word
most typical in context using a lexical
co-occurrence network. In Proceedings of the
35th Annual Meeting of the Association for
Computational Linguistics and Eighth
Conference of the European Chapter of the
Association for Computational Linguistics,
ACL ?98, pages 507?509, Stroudsburg, PA.
Erk, Katrin. 2007. A simple, similarity-based
model for selectional preferences. In
Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics
(ACL-2007), pages 216?223, Prague.
Erk, Katrin, Sebastian Pado?, and Ulrike Pado?.
2010. A flexible, corpus-driven model of
regular and inverse selectional preferences.
Computational Linguistics, 36(4):723?763.
Fillmore, C. J., J. Ruppenhofer, and C. F.
Baker. 2004. FrameNet and representing
the link between semantic and syntactic
relations. In Frontiers in Linguistics,
volume I of Language and Linguistics
Monograph Series B. Institute of Linguistics,
Academia Sinica, Taipei, pages 19?59.
Gildea, D. and D. Jurafsky. 2002. Automatic
labeling of semantic roles. Computational
Linguistics, 28(3):245?288.
Grefenstette, Gregory. 1992. Sextant:
Exploring unexplored contexts for
semantic extraction from syntactic
analysis. In ACL?92, pages 324?326,
Newark, DE.
Higashinaka, Ryuichiro and Hideki Isozaki.
2008. Corpus-based question answering
for why-questions. In Proceedings of the
Third International Joint Conference on
Natural Language Processing (IJCNLP),
pages 418?425, Hyderabad.
Hindle, Donald. 1990. Noun classification
from predicate-argument structures. In
Proceedings of the 28th Annual Meeting of the
Association for Computational Linguistics
(ACL-1990), pages 268?275, Pittsburgh, PA.
Koo, Terry, Xavier Carreras, and Michael
Collins. 2008. Simple semi-supervised
dependency parsing. In Proceedings
of ACL-08: HLT, pages 595?603,
Columbus, OH.
Lee, Lillian. 1999. Measures of distributional
similarity. In 37th Annual Meeting of the
Association for Computational Linguistics,
pages 25?32, College Park, MD.
Li, Hang and Naoki Abe. 1998. Generalizing
case frames using a thesaurus and the
MDL principle. Computational Linguistics,
24(2):217?244.
Lin, Dekang. 1998. Automatic retrieval
and clustering of similar words. In
Proceedings of the 36th Annual Meeting
of the Association for Computational
Linguistics and the 17th International
Conference on Computational Linguistics
(COLING-ACL-1998), pages 768?774,
Montreal.
Litkowski, K. C. and O. Hargraves. 2005. The
preposition project. In Proceedings of the
ACL-SIGSEM Workshop on the Linguistic
Dimensions of Prepositions and their Use in
Computational Linguistic Formalisms and
Applications, pages 171?179, Colchester.
Litkowski, K. C. and O. Hargraves. 2007.
SemEval-2007 Task 06: Word-sense
disambiguation of prepositions.
In Proceedings of the 4th International
Workshop on Semantic Evaluations
(SemEval-2007), pages 24?29, Prague.
Litkowski, Ken and Orin Hargraves.
2006. Coverage and inheritance in the
preposition project. In Prepositions ?06:
Proceedings of the Third ACL-SIGSEM
Workshop on Prepositions, pages 37?44,
Trento.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre,
Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The Penn
Treebank: Annotating predicate argument
structure. In Proceedings of the Workshop on
Human Language Technology (HLT-94),
pages 114?119, Plainsboro, NJ.
Ma`rquez, Llu??s, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008.
Semantic role labeling: An introduction to
the special issue. Computational Linguistics,
34(2):145?159.
McCarthy, Diana and John Carroll. 2003.
Disambiguating nouns, verbs, and
adjectives using automatically acquired
selectional preferences. Computational
Linguisties, 29:639?654.
Melli, Gabor, Yang Wang, Yudong Liu,
Mehdi M. Kashani, Zhongmin Shi,
661
Computational Linguistics Volume 39, Number 3
Baohua Gu, Anoop Sarkar, and Fred
Popowich. 2005. Description of SQUASH,
the SFU question answering summary
handler for the DUC-2005 summarization
task. In Proceedings of Document
Understanding Workshop, HLT/EMNLP
Annual Meeting, Vancouver.
Moschitti, Alessandro, Silvia Quarteroni,
Roberto Basili, and Suresh Manandhar.
2007. Exploiting syntactic and shallow
semantic kernels for question/answer
classification. In Proceedings of the
45th Conference of the Association for
Computational Linguistics (ACL),
pages 776?783, Prague.
Narayanan, S. and S. Harabagiu. 2004.
Question answering based on semantic
structures. In Proceedings of the 20th
International Conference on Computational
Linguistics (COLING), pages 693?701,
Geneva.
Noreen, E. W. 1989. Computer-Intensive
Methods for Testing Hypotheses:
An Introduction, Wiley.
O? Se?aghdha, Diarmuid. 2010. Latent
variable models of selectional preference.
In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics,
pages 435?444, Uppsala.
Pado?, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of
semantic space models. Computational
Linguistics, 33(2):161?199.
Pado?, Sebastian, Ulrike Pado?, and Katrin Erk.
2007. Flexible, corpus-based modelling
of human plausibility judgements. In
Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning (EMNLP-CoNLL-2007),
pages 400?409, Prague.
Palmer, M., D. Gildea, and P. Kingsbury.
2005. The proposition bank: An annotated
corpus of semantic roles. Computational
Linguistics, 31(1):71?105.
Pantel, Patrick, Rahul Bhagat, Bonaventura
Coppola, Timothy Chklovski, and
Eduard Hovy. 2007. ISP: Learning
inferential selectional preferences.
In Human Language Technologies 2007:
The Conference of the North American
Chapter of the Association for Computational
Linguistics; Proceedings of the Main
Conference, pages 564?571, Rochester, NY.
Pantel, Patrick and Dekang Lin. 2000. An
unsupervised approach to prepositional
phrase attachment using contextually
similar words. In Proceedings of the 38th
Annual Conference of the Association of
Computational Linguistics (ACL-2000),
pages 101?108, Hong Kong.
Pradhan, S., W. Ward, and J. H. Martin. 2008.
Towards robust semantic role labeling.
Computational Linguistics, 34(2):289?310.
Rada, R., H. Mili, E. Bicknell, and M. Blettner.
1989. Development and application of a
metric on semantic nets. IEEE Transactions
on Systems, Man, and Cybernetics,
19(1):17?30.
Ratinov, Lev and Dan Roth. 2009. Design
challenges and misconceptions in named
entity recognition. In Proceedings of the
Thirteenth Conference on Computational
Natural Language Learning (CoNLL-2009),
pages 147?155, Boulder, CO.
Resnik, Philip. 1993a. Selection and
Information: A Class-Based Approach to
Lexical Relationships. Ph.D. thesis,
University of Pennsylvania.
Resnik, Philip. 1993b. Semantic classes and
syntactic ambiguity. In Proceedings of the
Workshop on Human Language Technology,
pages 278?283, Morristown, NJ.
Ritter, Alan, Mausam, and Oren Etzioni.
2010. A latent Dirichlet alocation method
for selectional preferences. In Proceedings of
the 48th Annual Meeting of the Association for
Computational Linguistics, pages 424?434,
Uppsala.
Schu?tze, Hinrich. 1998. Automatic word
sense discrimination. Computational
Linguistics, 24(1):97?123.
Srikumar, V. and D. Roth. 2011. A joint
model for extended semantic role labeling.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP), pages 129?139, Edinburgh.
Surdeanu, M., S. Harabagiu, J. Williams,
and P. Aarseth. 2003. Using predicate-
argument structures for information
extraction. In Proceedings of the 41st Annual
Meeting of the Association for Computational
Linguistics (ACL-2003), pages 8?15,
Sapporo.
Surdeanu, Mihai, Massimiliano Ciaramita,
and Hugo Zaragoza. 2011. Learning to
rank answers to non-factoid questions
from Web collections. Computational
Linguistics, 37(2):351?383.
Surdeanu, Mihai, Llu??s Ma`rquez, Xavier
Carreras, and Pere R. Comas. 2007.
Combination strategies for semantic role
labeling. Journal of Artificial Intelligence
Research (JAIR), 29:105?151.
Sussna, Michael. 1993. Word sense
disambiguation for free-text indexing
using a massive semantic network. In
Proceedings of the Second International
662
Zapirain et al Selectional Preferences for Semantic Role Classification
Conference on Information and Knowledge
Management, CIKM ?93, pages 67?74,
New York, NY.
Wilks, Yorick. 1975. Preference semantics.
In E. L. Kaenan, editor, Formal Semantics of
Natural Language. Cambridge University
Press, Cambridge, MA, pages 329?348.
Zapirain, Ben?at, Eneko Agirre, and Llu??s
Ma`rquez. 2009. Generalizing over lexical
features: Selectional preferences for
semantic role classification. In Proceedings
of the Joint Conference of the 47th Annual
Meeting of the Association for Computational
Linguistics and the 4th International Joint
Conference on Natural Language Processing
(ACL-IJCNLP-2009), pages 73?76, Suntec.
Zapirain, Ben?at, Eneko Agirre, Llu??s
Ma`rquez, and Mihai Surdeanu. 2010.
Improving semantic role classification
with selectional preferences. In Proceedings
of the 11th Annual Conference of the North
American Chapter of the Association for
Computational Linguistics (NAACL HLT
2010), pages 373?376, Los Angeles, CA.
663

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 373?376,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Improving Semantic Role Classification with Selectional Preferences
Ben?at Zapirain, Eneko Agirre
IXA NLP Group
Basque Country Univ.
{benat.zapirain,e.agirre}@ehu.es
Llu??s Ma`rquez
TALP Research Center
Technical Univ. of Catalonia
lluism@lsi.upc.edu
Mihai Surdeanu
Stanford NLP Group
Stanford Univ.
mihais@stanford.edu
Abstract
This work incorporates Selectional Prefer-
ences (SP) into a Semantic Role (SR) Clas-
sification system. We learn separate selec-
tional preferences for noun phrases and prepo-
sitional phrases and we integrate them in a
state-of-the-art SR classification system both
in the form of features and individual class
predictors. We show that the inclusion of the
refined SPs yields statistically significant im-
provements on both in domain and out of do-
main data (14.07% and 11.67% error reduc-
tion, respectively). The key factor for success
is the combination of several SP methods with
the original classification model using meta-
classification.
1 Introduction
Semantic Role Labeling (SRL) is the process of
extracting simple event structures, i.e., ?who? did
?what? to ?whom?, ?when? and ?where?. Current
systems usually perform SRL in two pipelined steps:
argument identification and argument classification.
While identification is mostly syntactic, classifica-
tion requires semantic knowledge to be taken into
account. Semantic information is usually captured
through lexicalized features on the predicate and the
head?word of the argument to be classified. Since
lexical features tend to be sparse, SRL systems are
prone to overfit the training data and generalize
poorly to new corpora.
Indeed, the SRL evaluation exercises at CoNLL-
2004 and 2005 (Carreras and Ma`rquez, 2005) ob-
served that all systems showed a significant perfor-
mance degradation (?10 F1 points) when applied to
test data from a different genre of that of the training
set. Pradhan et al (2008) showed that this perfor-
mance degradation is essentially caused by the argu-
ment classification subtask, and suggested the lexi-
cal data sparseness as one of the main reasons. The
same authors studied the contribution of the different
feature types in SRL and concluded that the lexical
features were the most salient features in argument
classification (Pradhan et al, 2007).
In recent work, we showed (Zapirain et al, 2009)
how automatically generated selectional preferences
(SP) for verbs were able to perform better than pure
lexical features in a role classification experiment,
disconnected from a full-fledged SRL system. SPs
introduce semantic generalizations on the type of ar-
guments preferred by the predicates and, thus, they
are expected to improve results on infrequent and
unknown words. The positive effect was especially
relevant for out-of-domain data. In this paper we ad-
vance (Zapirain et al, 2009) in two directions:
(1) We learn separate SPs for prepositions and verbs,
showing improvement over using SPs for verbs
alone.
(2) We integrate the information of several SP mod-
els in a state-of-the-art SRL system (SwiRL1) and
show significant improvements in SR classifica-
tion. The key for the improvement lies in a meta-
classifier, trained to select among the predictions
provided by several role classification models.
2 SPs for SR Classification
SPs have been widely believed to be an impor-
tant knowledge source when parsing and perform-
ing SRL, especially role classification. Still, present
parsers and SRL systems use just lexical features,
which can be seen as the most simple form of SP,
1http://www.surdeanu.name/mihai/swirl/
373
where the headword needs to be seen in the training
data, and otherwise the SP is not satisfied. Gildea
and Jurafsky (2002) showed barely significant im-
provements in semantic role classification of NPs
for FrameNet roles using distributional clusters. In
(Erk, 2007) a number of SP models are tested in
a pseudo-task related to SRL. More recently, we
showed (Zapirain et al, 2009) that several methods
to automatically generate SPs generalize well and
outperform lexical match in a large dataset for se-
mantic role classification, but the impact on a full
system was not explored.
In this work we apply a subset of the SP meth-
ods proposed in (Zapirain et al, 2009). These meth-
ods can be split in two main families, depending on
the resource used to compute similarity: WordNet-
based methods and distributional methods. Both
families define a similarity score between a word
(the headword of the argument to be classified) and a
set of words (the headwords of arguments of a given
role).
WordNet-based similarity: One of the models
that we used is based on Resnik?s similarity mea-
sure (1993), referring to it as res. The other model is
an in-house method (Zapirain et al, 2009), referred
as wn, which only takes into account the depth of
the most common ancestor, and returns SPs that are
as specific as possible.
Distributional similarity: Following (Zapirain et
al., 2009) we considered both first order and second
order similarity. In first order similarity, the simi-
larity of two words was computed using the cosine
(or Jaccard measure) of the co-occurrence vectors of
the two words. Co-occurrence vectors where con-
structed using freely available software (Pado? and
Lapata, 2007) run over the British National Corpus.
We used the optimal parameters (Pado? and Lapata,
2007, p. 179). We will refer to these similarities as
simcos and simJac, respectively. In contrast, sec-
ond order similarity uses vectors of similar words,
i.e., the similarity of two words was computed us-
ing the cosine (or Jaccard measure) between the
thesaurus entries of those words in Lin?s thesaurus
(Lin, 1998). We refer to these as sim2cos and sim
2
Jac.
Given a target sentence with a verb and its argu-
ments, the task of SR classification is to assign the
correct role to each of the arguments. When using
SPs alone, we only use the headwords of the ar-
guments, and each argument is classified indepen-
dently of the rest. For each headword, we select the
role (r) of the verb (c) which fits best the head word
(w), where the goodness of fit (SPsim(v, r, w)) is
modeled using one of the similarity models above,
between the headword w and the headwords seen in
training data for role r of verb v. This selection rule
is formalized as follows:
Rsim(v, w) = arg max
r?Roles(v)
SPsim(v, r, w) (1)
In our previous work (Zapirain et al, 2009), we
modelled SPs for pairs of predicates (verbs) and ar-
guments, independently of the fact that the argu-
ment is a core argument (typically a noun) or an
adjunct argument (typically a prepositional phrase).
In contrast, (Litkowski and Hargraves, 2005) show
that prepositions have SPs of their own, especially
when functioning as adjuncts. We therefore decided
to split SPs according to whether the potential argu-
ment is a Prepositional Phrase (PP) or a Noun Phrase
(NP). For NPs, which tend to be core arguments2,
we use the SPs of the verb (as formalized above).
For PPs, which have an even distribution between
core and adjunct arguments, we use the SPs of the
prepositions alone, ignoring the verbs. Implementa-
tion wise, this means that in Eq. (1), we change v
for p, where p is the preposition heading the PP.
3 Experiments with SPs in isolation
In this section we evaluate the use of SPs for classi-
fication in isolation, i.e., we use formula 1, and no
other information. In addition we contrast the use
of both verb-role and preposition-role SPs, as com-
pared to the use of verb-role SPs alone.
The dataset used in these experiments (and in Sec-
tion 4) is the same as provided by the CoNLL-2005
shared task on SRL (Carreras and Ma`rquez, 2005).
This dataset comprises several sections of the Prop-
Bank corpus (news from the WSJ) as well as an ex-
tract of the Brown Corpus. Sections 02-21 are used
for generating the SPs and training, Section 00 for
development, and Section 23 for testing, as custom-
ary. The Brown Corpus is used for out-of-domain
testing, but due to the limited size of the provided
section, we extended it with instances from Sem-
Link3. Since the focus of this work is on argument
2In our training data, NPs are adjuncts only 5% of the times
3http://verbs.colorado.edu/semlink/
374
Verb-Role SPs Preposition-Role and Verb-Role SPs
WSJ-test Brown WSJ-test Brown
prec. rec. F1 prec. rec. F1 prec. rec. F1 prec. rec. F1
lexical 70.75 26.66 39.43 59.39 05.51 10.08 82.98 43.77 57.31 68.47 13.60 22.69
SPres 45.07 37.11 40.71 36.34 27.58 31.33 63.47 53.24 57.91 55.12 44.15 49.03
SPwn 55.44 45.58 50.03 41.76 31.58 35.96 65.70 63.88 64.78 60.08 48.10 53.43
SPsimJac 48.85 46.38 47.58 42.10 34.34 37.82 61.83 61.40 61.61 55.42 53.45 54.42
SPsimcos 53.13 50.44 51.75 43.24 35.27 38.85 64.67 64.22 64.44 56.56 54.54 55.53
SPsim2
Jac
61.76 58.63 60.16 51.97 42.39 46.69 70.82 70.33 70.57 62.37 60.15 61.24
SPsim2cos 61.12 58.12 59.63 51.92 42.35 46.65 70.28 69.80 70.04 62.36 60.14 61.23
Table 1: Results for SPs in isolation, left for verb SPs, and right both preposition and verb SPs.
Labels proposed by the base models
Number of base models that proposed this datum?s label
List of actual base models that proposed this datum?s label
Table 2: Features of the binary meta-classifier.
classification, we use the gold PropBank data to
identify argument boundaries. Considering that SPs
can handle only nominal arguments, in these exper-
iments we used only arguments mapped to NPs and
PPs containing a nominal head. From the training
sections, we extracted over 140K such arguments for
the supervised generation of SPs. The development
and test sections contain over 5K and 8K examples,
respectively, and the portion of the Brown Corpus
comprises an amount of 8.1K examples.
Table 1 lists the results of the different SPs in iso-
lation. The results reported in the left part of Table
1 are comparable to those we reported in (Zapirain
et al, 2009). The differences are due to the fact that
we do not discard roles like MOD, DIS, NEG and
that our previous work used only the subset of the
data that could be mapped to VerbNet (around 50%).
All in all, the table shows that splitting SPs into verb
and preposition SPs yields better results, both in pre-
cision and recall, improving F1 up to 10 points in
some cases.
4 Integrating SPs in a SRL system
For these experiments we modified SwiRL (Sur-
deanu et al, 2007): (a) we matched the gold bound-
aries against syntactic constituents predicted inter-
nally using the Charniak parser (Charniak, 2000);
and (b) we classified these constituents with their
semantic role using a modified version of SwiRL?s
feature set.
We explored two different strategies for integrat-
ing SPs in SwiRL. The first, obvious method is to
extend SwiRL?s feature set with features that model
the preferences of the SPs, i.e., for each SP model
SPi we add a feature whose value is Ri. The second
method combines SwiRL?s classification model and
our SP models using meta-classification. We opted
for a binary classification approach: first, for each
constituent we generate n datums, one for each dis-
tinct role label proposed by the pool of base models;
then we use a binary meta-classifier to label each
candidate role as correct or incorrect. Table 2 lists
the features of the meta-classifier. We trained the
meta-classifier on the usual PropBank training par-
tition, using cross-validation to generate outputs for
the base models that require the same training ma-
terial. At prediction time, for each candidate con-
stituent we selected the role label that was classified
as correct with the highest confidence.
Table 3 compares the performance of both
combination approaches against the standalone
SwiRL classifier. We show results for both core
arguments (Core), adjunct arguments (Arg) and
all arguments combined (All). In the table, the
SwiRL+SP? models stand for SwiRL classifiers
enhanced with one feature from the correspond-
ing SP. Adding more than one SP-based feature to
SwiRL did not improve results. Our conjecture
is that the SwiRL classifier enhanced with SP-
based features does not learn relevant weights for
these features because their signal is ?drowned? by
SwiRL?s large initial feature set and the correlation
between the different SPs. This observation moti-
vated the development of the meta-classifier. The
meta-classifier shown in the table combines the out-
put of the SwiRL+SP? models with the predictions
of SP models used in isolation. We implemented
the meta-classifier using Support Vector Machines
(SVM)4 with a quadratic polynomial kernel, and
4http://svmlight.joachims.org
375
WSJ-test Brown
Core Adj All Core Adj All
SwiRL 93.25 81.31 90.83 84.42 57.76 79.52
+SPRes 93.17 81.08 90.76 84.52 59.24 79.86
+SPwn 92.88 81.11 90.56 84.26 59.69 79.73
+SPsimJac 93.37 80.30 90.86 84.43 59.54 79.83
+SPsimcos 93.33 80.92 90.87 85.14 60.16 80.50
+SPsim2
Jac
93.03 82.75 90.95 85.62 59.63 80.75
+SPsim2cos 93.78 80.56 91.23 84.95 61.01 80.48
Meta 94.37 83.40 92.12 86.20 63.40 81.91
Table 3: Classification accuracy for the combination ap-
proaches. +SPx stands for SwiRL plus each SP model.
C = 0.01 (tuned in development).
Table 3 indicates that four out of the six
SwiRL+SP? models perform better than SwiRL in
domain (WSJ-test), and all of them outperform
SwiRL out of domain (Brown). However, the im-
provements are small and, generally, not statistically
significant. On the other hand, the meta-classifier
outperforms SwiRL both in domain (14.07% error
reduction) and out of domain (11.67% error reduc-
tion), and the differences are statistically signifi-
cant (measured using two-tailed paired t-test at 99%
confidence interval on 100 samples generated us-
ing bootstrap resampling). We also implemented
two unsupervised voting baselines, one unweighted
(each base model has the same weight) and one
weighted (each base model is weighted by its accu-
racy in development). However, none of these base-
lines outperformed the standalone SwiRL classifier.
This is further proof that, for SR classification, meta-
classification is crucial because it can learn the dis-
tinct specializations of the various base models.
Finally, Table 3 shows that our approach yields
consistent improvements for both core and adjunct
arguments. Out of domain, we see a bigger accuracy
improvement for adjunct arguments (5.64 absolute
points) vs. core arguments (1.78 points). This is
to be expected, as most core arguments fall under
the Arg0 and Arg1 classes, which can typically be
disambiguated based on syntactic information, i.e.,
subject vs. object. On the other hand, there are no
syntactic hints for adjunct arguments, so the system
learns to rely more on SP information in this case.
5 Conclusions
This paper is the first work to show that SPs improve
a state-of-the-art SR classification system. Sev-
eral decisions were crucial for success: (a) we de-
ployed separate SP models for verbs and preposi-
tions, which in conjunction outperform SP models
for verbs alone; (b) we incorporated SPs into SR
classification using a meta-classification approach
that combines eight base models, developed from
variants of a state-of-the-art SRL system and the
above SP models. We show that the resulting system
outperforms the original SR classification system for
arguments mapped to nominal or prepositional con-
stituents. The improvements are statistically sig-
nificant both on in-domain and out-of-domain data
sets.
Acknowledgments
This work was partially supported by projects KNOW-
2 (TIN2009-14715-C04-01 / 04), KYOTO (ICT-2007-
211423) and OpenMT-2 (TIN2009-14675C03)
References
X. Carreras and L. Ma`rquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic role labeling. In
Proc. of CoNLL.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of NAACL.
K. Erk. 2007. A simple, similarity-based model for se-
lectional preferences. In Proc. of ACL.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3).
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proc. of COLING-ACL.
K. Litkowski and O. Hargraves. 2005. The preposi-
tion project. In Proceedings of the Workshop on The
Linguistic Dimensions of Prepositions and their Use
in Computational Linguistic Formalisms and Applica-
tions.
S. Pado? and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2).
S. Pradhan, W. Ward, and J. Martin. 2007. Towards ro-
bust semantic role labeling. In Proc. of NAACL-HLT.
S. Pradhan, W. Ward, and J. Martin. 2008. Towards ro-
bust semantic role labeling. Computational Linguis-
tics, 34(2).
P. Resnik. 1993. Semantic classes and syntactic ambigu-
ity. In Proc. of HLT.
M. Surdeanu, L. Ma`rquez, X. Carreras, and P.R. Comas.
2007. Combination strategies for semantic role label-
ing. Journal of Artificial Intelligence Research, 29.
B. Zapirain, E. Agirre, and L. Ma`rquez. 2009. General-
izing over lexical features: Selectional preferences for
semantic role classification. In Proc. of ACL-IJCNLP.
376
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 649?652,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Ensemble Models for Dependency Parsing:
Cheap and Good?
Mihai Surdeanu and Christopher D. Manning
Computer Science Department
Stanford University, Stanford, CA 94305
{mihais,manning}@stanford.edu
Abstract
Previous work on dependency parsing used
various kinds of combination models but a
systematic analysis and comparison of these
approaches is lacking. In this paper we imple-
mented such a study for English dependency
parsing and find several non-obvious facts: (a)
the diversity of base parsers is more important
than complex models for learning (e.g., stack-
ing, supervised meta-classification), (b) ap-
proximate, linear-time re-parsing algorithms
guarantee well-formed dependency trees with-
out significant performance loss, and (c) the
simplest scoring model for re-parsing (un-
weighted voting) performs essentially as well
as other more complex models. This study
proves that fast and accurate ensemble parsers
can be built with minimal effort.
1 Introduction
Several ensemble models have been proposed for
the parsing of syntactic dependencies. These ap-
proaches can generally be classified in two cate-
gories: models that integrate base parsers at learn-
ing time, e.g., using stacking (Nivre and McDon-
ald, 2008; Attardi and Dell?Orletta, 2009), and ap-
proaches that combine independently-trained mod-
els only at parsing time (Sagae and Lavie, 2006; Hall
et al, 2007; Attardi and Dell?Orletta, 2009). In the
latter case, the correctness of the final dependency
tree is ensured by: (a) selecting entire trees proposed
by the base parsers (Henderson and Brill, 1999); or
(b) re-parsing the pool of dependencies proposed by
the base models (Sagae and Lavie, 2006). The lat-
ter approach was shown to perform better for con-
stituent parsing (Henderson and Brill, 1999).
While all these models achieved good perfor-
mance, the previous work has left several questions
Devel In domain Out of domain
LAS LAS UAS LAS UAS
MST 85.36 87.07 89.95 80.48 86.08
Malt?AE 84.24 85.96 88.64 78.74 84.18
Malt?CN 83.75 85.61 88.14 78.55 83.68
Malt?AS 83.74 85.36 88.06 77.23 82.39
Malt?AS 82.43 83.90 86.70 76.69 82.57
Malt?CN 81.75 83.53 86.17 77.29 83.02
Malt?AE 80.76 82.51 85.35 76.18 82.02
Table 1: Labeled attachment scores (LAS) and unlabeled at-
tachment scores (UAS) for the base models. The parsers are
listed in descending order of LAS in the development partition.
unanswered. Here we answer the following ques-
tions, in the context of English dependency parsing:
1. When combining models at parsing time, what
is the best scoring model for candidate depen-
dencies during re-parsing? Can a meta classi-
fier improve over unsupervised voting?
2. Are (potentially-expensive) re-parsing strate-
gies justified for English? What percentage of
trees are not well-formed if one switches to a
light word-by-word voting scheme?
3. How important is the integration of base parsers
at learning time?
4. How do ensemble models compare against
state-of-the-art supervised parsers?
2 Setup
In our experiments we used the syntactic dependen-
cies from the CoNLL 2008 shared task corpus (Sur-
deanu et al, 2008).
We used seven base parsing models in this paper:
six are variants of the Malt parser1 and the seventh
is the projective version of MSTParser that uses only
first-order features2 (or MST for short). The six Malt
1http://maltparser.org/
2http://sourceforge.net/projects/
mstparser/
649
Unweighted Weighted by Weighted by Weighted by Weighted by
POS of modifier label of dependency dependency length sentence length
# of parsers LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS
3 86.03 89.44 86.02 89.43 85.53 88.97 85.85 89.23 86.03 89.45
4 86.79 90.14 86.68 90.07 86.38 89.78 86.46 89.79 86.84 90.18
5 86.98 90.33 86.95 90.30 86.60 90.06 86.87 90.22 86.86 90.22
6 87.14 90.51 87.17 90.50 86.74 90.22 86.91 90.23 87.04 90.37
7 86.81 90.21 86.82 90.21 86.50 90.01 86.71 90.08 86.80 90.19
Table 2: Scores of unsupervised combination models using different voting strategies. The combined trees are assembled using a
word-by-word voting scheme.
parser variants are built by varying the parsing algo-
rithm (we used three parsing models: Nivre?s arc-
eager (AE), Nivre?s arc-standard (AS), and Coving-
ton?s non-projective model (CN)), and the parsing
direction (left to right (?) or right to left (?)), sim-
ilar to (Hall et al, 2007). The parameters of the Malt
models were set to the values reported in (Hall et
al., 2007). The MST parser was used with the de-
fault configuration. Table 1 shows the performance
of these models in the development and test parti-
tions.
3 Experiments
3.1 On scoring models for parser combination
The most common approach for combining
independently-trained models at parsing time is to
assign each candidate dependency a score based
on the number of votes it received from the base
parsers. Considering that parsers specialize in
different phenomena, these votes can be weighted
by different criteria. To understand the importance
of such weighting strategies we compare several
voting approaches in Table 2: in the ?unweighted?
strategy all votes have the same weight; in all other
strategies each vote is assigned a value equal to
the accuracy of the given parser in the particular
instance of the context considered, e.g., in the
?weighted by POS of modifier? model we use the
accuracies of the base models for each possible
part-of-speech (POS) tag of a modifier token. In
the table we show results as more base parsers are
added to the ensemble (we add parsers in the order
given by Table 1). The results in Table 2 indicate
that weighting strategies do not have an important
contribution to overall performance. The only
approach that outperforms the LAS score of the
unweighted voting model is the model that weighs
parsers by their accuracy for a given modifier POS
tag, but the improvement is marginal. On the other
POS(m) POS(m) ? POS(h) length(s)
MST 38 56 26
Malt?AE 0 6 6
Malt?CN 0 14 7
Malt?AS 0 61 0
Malt?AS 0 0 3
Malt?CN 0 9 0
Malt?AE 0 0 0
Table 3: Total number of minority dependencies with precision
larger than 50%, for different base parsers and most represen-
tative features (m - modifier, h - head, s - sentence). These
are counts of tokens, computed in the development corpus of
33,368 dependencies.
hand, the number of base parsers in the ensemble
pool is crucial: performance generally continues to
improve as more base parsers are considered. The
best ensemble uses 6 out of the 7 base parsers.3
It is often argued that the best way to re-score
candidate dependencies is not through voting but
rather through a meta-classifier that selects candi-
date dependencies based on their likelihood of be-
longing to the correct tree. Unlike voting, a meta-
classifier can combine evidence from multiple con-
texts (such as the ones listed in Table 2). However,
in our experiments such a meta-classifier4 did not
offer any gains over the much simpler unweighted
voting strategy. We explain these results as follows:
the meta-classifier can potentially help only when it
proposes dependencies that disagree with the major-
ity vote. We call such dependencies minority depen-
dencies.5 For a given parser and context instance
(e.g., a modifier POS), we define precision of mi-
nority dependencies as the ratio of minority depen-
dencies in this group that are correct. Obviously, a
3We drew similar conclusions when we replaced voting with
the re-parsing algorithms from the next sub-section.
4We implemented a L2-regularized logistic regression clas-
sifier using as features: identifiers of the base models, POS tags
of head and modifier, labels of dependencies, length of depen-
dencies, length of sentence, and combinations of the above.
5(Henderson and Brill, 1999) used a similar framework in
the context of constituent parsing and only three base parsers.
650
group of minority dependencies provides beneficial
signal only if its precision is larger than 50%. Ta-
ble 3 lists the total number of minority dependencies
in groups with precision larger than 50% for all our
base parsers and the most representative features.
The table shows that the number of minority depen-
dencies with useful signal is extremely low. All in
all, it accounts for less than 0.7% of all dependen-
cies in the development corpus.
3.2 On re-parsing algorithms
To guarantee that the resulting dependency tree is
well-formed, most previous work used the dynamic
programming algorithm of Eisner (1996) for re-
parsing (Sagae and Lavie, 2006; Hall et al, 2007).6
However, it is not clear that this step is necessary.
In other words, how many sentences are not well-
formed if one uses a simple word-by-word voting
scheme? To answer this, we analyzed the output
of our best word-by-word voting scheme (six base
parsers weighted by the POS of the modifier). The
results for both in-domain and out-of-domain test-
ing corpora are listed in Table 4. The table shows
that the percentage of badly-formed trees is rela-
tively large: almost 10% out of domain. This in-
dicates that the focus on algorithms that guarantee
well-formed trees is justified.
However, it is not clear how the Eisner algo-
rithm, which has runtime complexity of O(n3) (n
? number of tokens per sentence), compares against
approximate re-parsing algorithms that have lower
runtime complexity. One such algorithm was pro-
posed by Attardi and Dell?Orletta (2009). The al-
gorithm, which has a runtime complexity of O(n),
builds dependency trees using a greedy top-down
strategy, i.e., it starts by selecting the highest-scoring
root node, then the highest-scoring children, etc. We
compare these algorithms against the word-by-word
voting scheme in Table 5.7 The results show that
both algorithms pay a small penalty for guaranteeing
well-formed trees. This performance drop is statis-
tically significant out of domain. On the other hand,
the difference between the Eisner and Attardi algo-
rithms is not statistically significant out of domain.
6We focus on projective parsing algorithms because 99.6%
of dependencies in our data are projective (Surdeanu et al,
2008).
7Statistical significance was performed using Dan Bikel ran-
domized parsing evaluation comparator at 95% confidence.
In domain Out of domain
Zero roots 0.83% 0.70%
Multiple roots 3.37% 6.11%
Cycles 4.29% 4.23%
Total 7.46% 9.64%
Table 4: Percentage of badly-formed dependency trees when
base parsers are combined using a word-by-word voting
scheme. The different error classes do not sum up to the listed
total because the errors are not mutually exclusive.
In domain Out of domain
LAS UAS LAS UAS
Word by word 88.89 91.52 82.13? 87.51?
Eisner 88.83? 91.47? 81.99 87.32
Attardi 88.70 91.34 81.82 87.29
Table 5: Scores of different combination schemes. ? indicates
that a model is significantly different than the next lower ranked
model.
This experiment proves that approximate re-parsing
algorithms are a better choice for practical purposes,
i.e., ensemble parsing in domains different from the
training material of the base models.
3.3 On parser integration at learning time
Recent work has shown that the combination of
base parsers at learning time, e.g., through stacking,
yields considerable benefits (Nivre and McDonald,
2008; Attardi and Dell?Orletta, 2009). However, it
is unclear how these approaches compare against the
simpler ensemble models, which combine parsers
only at runtime. To enable such a comparison, we
reimplemented the best stacking model from (Nivre
and McDonald, 2008) ? MSTMalt ? which trains a
variant of the MSTParser that uses additional fea-
tures extracted from the output of a Malt parser.
In Table 6, we compare this stacking approach
against four variants of our ensemble models. The
superscript in the ensemble name indicates the run-
time complexity of the model (O(n3) or O(n)). The
cubic-time models use all base parsers from Table 1
and the Eisner algorithm for re-parsing. The linear-
time models use only Malt-based parsers and the
Attardi algorithm for re-parsing. The subscript in
the model names indicates the percentage of avail-
able base parsers used, e.g., ensemble350% uses only
the first three parsers from Table 1. These re-
sults show that MSTMalt is statistically equivalent
to an ensemble that uses MST and two Malt vari-
ants, and both our ensemble100% models are signifi-
cantly better than MSTMalt. While this comparison
is somewhat unfair (MSTMalt uses two base models,
whereas our ensemble models use at least three) it
651
In domain Out of domain
LAS UAS LAS UAS
ensemble3100% 88.83
? 91.47? 81.99? 87.32?
ensemble1100% 88.01
? 90.76? 80.78 86.55
ensemble350% 87.45 90.17 81.12 86.62
MSTMalt 87.45? 90.22? 80.25? 85.90?
ensemble150% 86.74 89.62 79.44 85.54
Table 6: Comparison of different combination strategies.
In domain Out of domain
LAS UAS LAS UAS
CoNLL 2008, #1 90.13? 92.45? 82.81? 88.19?
ensemble3100% 88.83
? 91.47? 81.99? 87.32?
CoNLL 2008, #2 88.14 90.78 80.80 86.12
ensemble1100% 88.01 90.76 80.78 86.55
Table 7: Comparison with state of the art parsers.
does illustrate that the advantages gained from com-
bining parsers at learning time can be easily sur-
passed by runtime combination models that have ac-
cess to more base parsers. Considering that variants
of shift-reduce parsers can be generated with min-
imal effort (e.g., by varying the parsing direction,
learning algorithms, etc.) and combining models at
runtime is simpler than combining them at learning
time, we argue that runtime parser combination is a
more attractive approach.
3.4 Comparison with the state of the art
In Table 7 we compare our best ensemble models
against the top two systems of the CoNLL-2008
shared task evaluation. The table indicates that our
best ensemble model ranks second, outperforming
significantly 19 other systems. The only model per-
forming better than our ensemble is a parser that
uses higher-order features and has a higher runtime
complexity (O(n4)) (Johansson and Nugues, 2008).
While this is certainly proof of the importance of
higher-order features, it also highlights a pragmatic
conclusion: in out-of-domain corpora, an ensemble
of models that use only first-order features achieves
performance that is within 1% LAS of much more
complex models.
4 Conclusions
This study unearthed several non-intuitive yet im-
portant observations about ensemble models for de-
pendency parsing. First, we showed that the diver-
sity of base parsers is more important than complex
learning models for parser combination, i.e., (a) en-
semble models that combine several base parsers at
runtime performs significantly better than a state-of-
the-art model that combines two parsers at learning
time, and (b) meta-classification does not outper-
form unsupervised voting schemes for the re-parsing
of candidate dependencies when six base models are
available. Second, we showed that well-formed de-
pendency trees can be guaranteed without signifi-
cant performance loss by linear-time approximate
re-parsing algorithms. And lastly, our analysis in-
dicates that unweighted voting performs as well as
weighted voting for the re-parsing of candidate de-
pendencies. Considering that different base models
are easy to generate, this work proves that ensemble
parsers that are both accurate and fast can be rapidly
developed with minimal effort.
Acknowledgments
This material is based upon work supported by the Air
Force Research Laboratory (AFRL) under prime contract
no. FA8750-09-C-0181. Any opinions, findings, and
conclusion or recommendations expressed in this mate-
rial are those of the authors and do not necessarily reflect
the view of the Air Force Research Laboratory (AFRL).
We thank Johan Hall, Joakim Nivre, Ryan McDonald,
and Giuseppe Attardi for their help in understanding de-
tails of their models.
References
G. Attardi and F. Dell?Orletta. 2009. Reverse revision
and linear tree combination for dependency parsing.
In Proc. of NAACL-HLT.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. of COL-
ING.
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi,
M. Nilsson, and M. Saers. 2007. Single malt or
blended? A study in multilingual parser optimization.
In Proc. of CoNLL Shared Task.
J. C. Henderson and E. Brill. 1999. Exploiting diversity
in natural language processing: Combining parsers. In
Proc. of EMNLP.
R. Johansson and P. Nugues. 2008. Dependency-based
syntactic semantic analysis with PropBank and Nom-
Bank. In Proc. of CoNLL Shared Task.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proc. of ACL.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proc. of NAACL-HLT.
M. Surdeanu, R. Johansson, A. Meyers, L. Marquez, and
J. Nivre. 2008. The CoNLL-2008 shared task on joint
parsing of syntactic and semantic dependencies. In
Proc. of CoNLL.
652
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1626?1635,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Event Extraction as Dependency Parsing
David McClosky, Mihai Surdeanu, and Christopher D. Manning
Department of Computer Science
Stanford University
Stanford, CA 94305
{mcclosky,mihais,manning}@stanford.edu
Abstract
Nested event structures are a common occur-
rence in both open domain and domain spe-
cific extraction tasks, e.g., a ?crime? event
can cause a ?investigation? event, which can
lead to an ?arrest? event. However, most cur-
rent approaches address event extraction with
highly local models that extract each event and
argument independently. We propose a simple
approach for the extraction of such structures
by taking the tree of event-argument relations
and using it directly as the representation in a
reranking dependency parser. This provides a
simple framework that captures global prop-
erties of both nested and flat event structures.
We explore a rich feature space that models
both the events to be parsed and context from
the original supporting text. Our approach ob-
tains competitive results in the extraction of
biomedical events from the BioNLP?09 shared
task with a F1 score of 53.5% in development
and 48.6% in testing.
1 Introduction
Event structures in open domain texts are frequently
highly complex and nested: a ?crime? event can
cause an ?investigation? event, which can lead to an
?arrest? event (Chambers and Jurafsky, 2009). The
same observation holds in specific domains. For ex-
ample, the BioNLP?09 shared task (Kim et al, 2009)
focuses on the extraction of nested biomolecular
events, where, e.g., a REGULATION event causes a
TRANSCRIPTION event (see Figure 1a for a detailed
example). Despite this observation, many state-
of-the-art supervised event extraction models still
extract events and event arguments independently,
ignoring their underlying structure (Bjo?rne et al,
2009; Miwa et al, 2010b).
In this paper, we propose a new approach for su-
pervised event extraction where we take the tree of
relations and their arguments and use it directly as
the representation in a dependency parser (rather
than conventional syntactic relations). Our approach
is conceptually simple: we first convert the origi-
nal representation of events and their arguments to
dependency trees by creating dependency arcs be-
tween event anchors (phrases that anchor events in
the supporting text) and their corresponding argu-
ments.1 Note that after conversion, only event an-
chors and entities remain. Figure 1 shows a sentence
and its converted form from the biomedical do-
main with four events: two POSITIVE REGULATION
events, anchored by the phrase ?acts as a costim-
ulatory signal,? and two TRANSCRIPTION events,
both anchored on ?gene transcription.? All events
take either protein entity mentions (PROT) or other
events as arguments. The latter is what allows for
nested event structures. Existing dependency pars-
ing models can be adapted to produce these seman-
tic structures instead of syntactic dependencies. We
built a global reranking parser model using multiple
decoders from MSTParser (McDonald et al, 2005;
McDonald et al, 2005b). The main contributions of
this paper are the following:
1. We demonstrate that parsing is an attractive ap-
proach for extracting events, both nested and
otherwise.
1While our approach only works on trees, we show how we
can handle directed acyclic graphs in Section 5.
1626
(a) Original sentence with nested events (b) After conversion to event dependencies
Figure 1: Nested events in the text fragment: ?. . . the HTLV-1 transactivator protein, tax, acts as a costim-
ulatory signal for GM-CSF and IL-2 gene transcription . . . ? Throughout this paper, bold text indicates
instances of event anchors and italicized text denotes entities (PROTEINs in the BioNLP?09 domain). Note
that in (a) there are two copies of each type of event, which are merged to single nodes in the dependency
tree (Section 3.1).
2. We propose a wide range of features for event
extraction. Our analysis indicates that fea-
tures which model the global event structure
yield considerable performance improvements,
which proves that modeling event structure
jointly is beneficial.
3. We evaluate on the biomolecular event corpus
from the the BioNLP?09 shared task and show
that our approach obtains competitive results.
2 Related Work
The pioneering work of Miller et al (1997) was
the first, to our knowledge, to propose parsing as
a framework for information extraction. They ex-
tended the syntactic annotations of the Penn Tree-
bank corpus (Marcus et al, 1993) with entity and
relation mentions specific to the MUC-7 evalua-
tion (Chinchor et al, 1997) ? e.g., EMPLOYEE OF
relations that hold between person and organization
named entities ? and then trained a generative pars-
ing model over this combined syntactic and seman-
tic representation. In the same spirit, Finkel and
Manning (2009) merged the syntactic annotations
and the named entity annotations of the OntoNotes
corpus (Hovy et al, 2006) and trained a discrimina-
tive parsing model for the joint problem of syntac-
tic parsing and named entity recognition. However,
both these works require a unified annotation of syn-
tactic and semantic elements, which is not always
feasible, and focused only on named entities and bi-
nary relations. On the other hand, our approach fo-
cuses on event structures that are nested and have
an arbitrary number of arguments. We do not need
a unified syntactic and semantic representation (but
we can and do extract features from the underlying
syntactic structure of the text).
Finkel and Manning (2009b) also proposed a
parsing model for the extraction of nested named en-
tity mentions, which, like this work, parses just the
corresponding semantic annotations. In this work,
we focus on more complex structures (events instead
of named entities) and we explore more global fea-
tures through our reranking layer.
In the biomedical domain, two recent papers pro-
posed joint models for event extraction based on
Markov logic networks (MLN) (Riedel et al, 2009;
Poon and Vanderwende, 2010). Both works propose
elegant frameworks where event anchors and argu-
ments are jointly predicted for all events in the same
sentence. One disadvantage of MLN models is the
requirement that a human expert develop domain-
specific predicates and formulas, which can be a
cumbersome process because it requires thorough
domain understanding. On the other hand, our ap-
proach maintains the joint modeling advantage, but
our model is built over simple, domain-independent
features. We also propose and analyze a richer fea-
ture space that captures more information on the
global event structure in a sentence. Furthermore,
since our approach is agnostic to the parsing model
used, it could easily be tuned for various scenarios,
e.g., models with lower inference overhead such as
shift-reduce parsers.
Our work is conceptually close to the recent
CoNLL shared tasks on semantic role labeling,
where the predicate frames were converted to se-
1627
Events	 ?to	 ?	 ?
Dependencies	 ?
Parser	 ?1	 ? ?	 ?	 ?
Reranker	 ?
Dependencies	 ?	 ?
to	 ?Events	 ?
Parser	 ?k	 ?
Dependencies	 ?	 ?
to	 ?Events	 ?
Event	 ?	 ?
Trigger	 ?
Recognizer	 ?
En?ty	 ?
Recognizer	 ?
Figure 2: Overview of the approach. Rounded rect-
angles indicate domain-independent components;
regular rectangles mark domain-specific modules;
blocks in dashed lines surround components not nec-
essary for the domain presented in this paper.
mantic dependencies between predicates and their
arguments (Surdeanu et al, 2008; Hajic et al, 2009).
In this representation the dependency structure is a
directed acyclic graph (DAG), i.e., the same node
can be an argument to multiple predicates, and there
are no explicit dependencies between predicates.
Due to this representation, all joint models proposed
for semantic role labeling handle semantic frames
independently.
3 Approach
Figure 2 summarizes our architecture. Our approach
converts the original event representation to depen-
dency trees containing both event anchors and entity
mentions, and trains a battery of parsers to recognize
these structures. The trees are built using event an-
chors predicted by a separate classifier. In this work,
we do not discuss entity recognition because in
the BioNLP?09 domain used for evaluation entities
(PROTEINs) are given (but including entity recog-
nition is an obvious extension of our model). Our
parsers are several instances of MSTParser2 (Mc-
Donald et al, 2005; McDonald et al, 2005b) con-
figured with different decoders. However, our ap-
proach is agnostic to the actual parsing models used
and could easily be adapted to other dependency
parsers. The output from the reranking parser is
2http://sourceforge.net/projects/mstparser/
converted back to the original event representation
and passed to a reranker component (Collins, 2000;
Charniak and Johnson, 2005), tailored to optimize
the task-specific evaluation metric.
Note that although we use the biomedical event
domain from the BioNLP?09 shared task to illustrate
our work, the core of our approach is almost do-
main independent. Our only constraints are that each
event mention be activated by a phrase that serves as
an event anchor, and that the event-argument struc-
tures be mapped to a dependency tree. The conver-
sion between event and dependency structures and
the reranker metric are the only domain dependent
components in our approach.
3.1 Converting between Event Structures and
Dependencies
As in previous work, we extract event structures at
sentence granularity, i.e., we ignore events which
span sentences (Bjo?rne et al, 2009; Riedel et al,
2009; Poon and Vanderwende, 2010). These form
approximately 5% of the events in the BioNLP?09
corpus. For each sentence, we convert the
BioNLP?09 event representation to a graph (repre-
senting a labeled dependency tree) as follows. The
nodes in the graph are protein entity mentions, event
anchors, and a virtual ROOT node. Thus, the only
words in this dependency tree are those which par-
ticipate in events. We create edges in the graph in
the following way. For each event anchor, we cre-
ate one link to each of its arguments labeled with the
slot name of the argument (for example, connecting
gene transcription to IL-2 with the label THEME in
Figure 1b). We link the ROOT node to each entity
that does not participate in an event using the ROOT-
LABEL dependency label. Finally, we link the ROOT
node to each top-level event anchor, (those which do
not serve as arguments to other events) again using
the ROOT-LABEL label. We follow the convention
that the source of each dependency arc is the head
while the target is the modifier.
The output of this process is a directed graph,
since a phrase can easily play a role in two or more
events. Furthermore, the graph may contain self-
referential edges (self-loops) due to related events
sharing the same anchor (example below). To guar-
antee that the output of this process is a tree, we
must post-process the above graph with the follow-
1628
ing three heuristics:
Step 1: We remove self-referential edges. An exam-
ple of these can be seen in the text ?the domain in-
teracted preferentially with underphosphorylated
TRAF2,? there are two events anchored by the same
underphosphorylated phrase, a NEGATIVE REGU-
LATION and a PHOSPHORYLATION event, and the
latter serves as a THEME argument for the former.
Due to the shared anchor, our conversion compo-
nent creates an self-referential THEME dependency.
By removing these edges, 1.5% of the events in the
training arguments are left without arguments, so we
remove them as well.
Step 2: We break structures where one argument par-
ticipates in multiple events, by keeping only the de-
pendency to the event that appears first in text. For
example, in the fragment ?by enhancing its inactiva-
tion through binding to soluble TNF-alpha receptor
type II,? the protein TNF-alpha receptor type II is
an argument in both a BINDING event (binding) and
in a NEGATIVE REGULATION event (inactivation).
As a consequence of this step, 4.7% of the events in
training are removed.
Step 3: We unify events with the same types an-
chored on the same anchor phrase. For example,
for the fragment ?Surface expression of intercellu-
lar adhesion molecule-1, P-selectin, and E-selectin,?
the BioNLP?09 annotation contains three distinct
GENE EXPRESSION events anchored on the same
phrase (expression), each having one of the proteins
as THEMEs. In such cases, we migrate all arguments
to one of the events, and remove the empty events.
21.5% of the events in training are removed in this
step (but no dependencies are lost).
Note that we do not guarantee that the resulting
tree is projective. In fact, our trees are more likely
to be non-projective than syntactic dependency trees
of English sentences, because in our representation
many nodes can be linked directly to the ROOT node.
Our analysis indicates that 2.9% of the dependencies
generated in the training corpus are non-projective
and 7.9% of the sentences contain at least one non-
projective dependency (for comparison, these num-
bers for the English Penn Treebank are 0.3% and
6.7%, respectively).
After parsing, we implement the inverse process,
i.e., we convert the generated dependency trees to
the BioNLP?09 representation. In addition to the
obvious conversions, this process implements the
heuristics proposed by Bjo?rne et al (2009), which
reverse step 3 above, e.g., we duplicate GENE EX-
PRESSION events with multiple THEME arguments.
The heuristics are executed sequentially in the given
order:
1. Since all non-BINDING events can have at
most one THEME argument, we duplicate non-
BINDING events with multiple THEME argu-
ments by creating one separate event for each
THEME.
2. Similarly, since REGULATION events accepts
only one CAUSE argument, we duplicate REG-
ULATION events with multiple CAUSE argu-
ments, obtaining one event per CAUSE.
3. Lastly, we implement the heuristic of Bjo?rne et
al. (2009) to handle the splitting of BINDING
events with multiple THEME arguments. This is
more complex because these events can accept
one or more THEMEs. In such situations, we
first group THEME arguments by the label of the
first Stanford dependency (Marneffe and Man-
ning, 2008) from the head word of the anchor
to this argument. Then we create one event for
each combination of THEME arguments in dif-
ferent groups.
3.2 Recognition of Event Anchors
For anchor detection, we used a multiclass classifier
that labels each token independently.3 Since over
92% of the anchor phrases in our evaluation domain
contain a single word, we simplify the task by re-
ducing all multi-word anchor phrases in the training
corpus to their syntactic head word (e.g., ?acts? for
the anchor ?acts as a costimulatory signal?).
We implemented this model using a logistic re-
gression classifier with L2 regularization over the
following features:
3We experimented with using conditional random fields as a
sequence labeler but did not see improvements in the biomed-
ical domain. We hypothesize that the sequence tagger fails to
capture potential dependencies between anchor labels ? which
are its main advantage over an i.i.d. classifier ? because anchor
words are typically far apart in text. This result is consistent
with observations in previous work (Bjo?rne et al, 2009).
1629
? Token-level: The form, lemma, and whether
the token is present in a gazetteer of known an-
chor words.4
? Surface context: The above token features ex-
tracted from a context of two words around the
current token. Additionally, we build token bi-
grams in this context window, and model them
with similar features.
? Syntactic context: We model all syntactic de-
pendency paths up to depth two starting from
the token to be classified. These paths are built
from Stanford syntactic dependencies (Marn-
effe and Manning, 2008). We extract token
features from the first and last token in these
paths. We also generate combination features
by concatenating: (a) the last token in each path
with the sequence of dependency labels along
the corresponding path; and (b) the word to be
classified, the last token in each path, and the
sequence of dependency labels in that path.
? Bag-of-word and entity count: Extracted
from (a) the entire sentence, and (b) a window
of five words around the token to be classified.
3.3 Parsing Event Structures
Given the entities and event anchors from the pre-
vious stages in the pipeline, the parser generates la-
beled dependency links between them. Many de-
pendency parsers are available and we chose MST-
Parser for its ability to produce non-projective and
n-best parses directly. MSTParser frames parsing
as a graph algorithm. To parse a sentence, MST-
Parser finds the tree covering all the words (nodes)
in the sentence (graph) with the largest sum of edge
weights, i.e., the maximum weighted spanning tree.
Each labeled, directed edge in the graph represents a
possible dependency between its two endpoints and
has an associated score (weight). Scores for edges
come from the dot product between the edge?s corre-
sponding feature vector and learned feature weights.
As a result, all features for MSTParser must be edge-
factored, i.e., functions of both endpoints and the la-
bel connecting them. McDonald et al (2006) ex-
tends the basic model to include second-order de-
pendencies (i.e., two adjacent sibling nodes and their
4These are automatically extracted from the training corpus.
parent). Both first and second-order modes include
projective and non-projective decoders.
Our features for MSTParser use both the event
structures themselves as well as the surrounding
English sentences which include them. By map-
ping event anchors and entities back to the original
text, we can incorporate information from the orig-
inal English sentence as well its syntactic tree and
corresponding Stanford dependencies. Both forms
of context are valuable and complementary. MST-
Parser comes with a large number of features which,
in our setup, operate on the event structure level
(since this is the ?sentence? from the parser?s point
of view). The majority of additional features that
we introduced take advantage of the original text as
context (primarily its associated Stanford dependen-
cies). Our system includes the following first-order
features:
? Path: Syntactic paths in the original sentence
between nodes in an event dependency (as in
previous work by Bjo?rne et al (2009)). These
have many variations including using Stanford
dependencies (?collapsed? and ?uncollapsed?)
or constituency trees as sources, optionally lex-
icalizing the path, and using words or relation
names along the path. Additionally, we include
the bucketed length of the paths.
? Original sentence words: Words from the full
English sentence surrounding and between the
nodes in event dependencies, and their buck-
eted distances. This additional context helps
compensate for how our anchor detection pro-
vides only the head word of each anchor, which
does not necessarily provide the full context for
event disambiguation.
? Graph: Parents, children, and siblings of
nodes in the Stanford dependencies graph
along with label of the edge. This provides ad-
ditional syntactic context.
? Consistency: Soft constraints on edges be-
tween anchors and their arguments (e.g., only
regulation events can have edges labeled with
CAUSE). These features fire if their constraints
are violated.
? Ontology: Generalized types of the end-
points of edges using a given type hierar-
chy (e.g., POSITIVE REGULATION is a COM-
1630
PLEX EVENT5 is an EVENT). Values of
this feature are coded with the types of each
of the endpoints on an edge, running over
the cross-product of types for each endpoint.
For instance, an edge between a BINDING
event anchor and a POSITIVE REGULATION
could cause this feature to fire with the val-
ues [head:EVENT, child:COMPLEX EVENT] or
[head:SIMPLE EVENT, child:EVENT].6 The lat-
ter feature can capture generalizations such as
?simple event anchors cannot take other events
as arguments.?
Both Consistency and Ontology feature classes in-
clude domain-specific information but can be used
on other domains under different constraints and
type hierarchies. When using second-order de-
pendencies, we use additional Path and Ontol-
ogy features. We include the syntactic paths be-
tween sibling nodes (adjacent arguments of the same
event anchor). These Path features are as above
but differentiated as paths between sibling nodes.
The second-order Ontology features use the type
hierarchy information on both sibling nodes and
their parent. For example, a POSITIVE REGULA-
TION anchor attached to a PROTEIN and a BINDING
event would produce an Ontology feature with the
value [parent:COMPLEX EVENT, child1:PROTEIN,
child2:SIMPLE EVENT] (among several other possi-
ble combinations).
To prune the number of features used, we employ
a simple entropy-based measure. Our intuition is
that good features should typically appear with only
one edge label.7 Given all edges enumerated during
training and their gold labels, we obtain a distribu-
tion over edge labels (df ) for each feature f . Given
this distribution and the frequency of a feature, we
can score the feature with the following:
score(f) = ?? log2
(
freq(f)
)
?H(df )
The ? parameter adjusts the relative weight of the
two components. The log frequency component fa-
vors more frequent features while the entropy com-
ponent favors features with low entropy in their edge
5We define complex events are those which can accept other
events are arguments. Simple events can only take PROTEINs.
6We omit listing the other two combinations.
7Labels include ROOT-LABEL, THEME, CAUSE, and NULL.
We assign the NULL label to edges which aren?t in the gold data.
label distribution. Features are pruned by accepting
all features with a score above a certain threshold.
3.4 Reranking Event Structures
When decoding, the parser finds the highest scoring
tree which incorporates global properties of the sen-
tence. However, its features are edge-factored and
thus unable to take into account larger contexts. To
incorporate arbitrary global features, we employ a
two-step reranking parser. For the first step, we ex-
tend our parser to output its n-best parses instead
of just its top scoring parse. In the second step, a
discriminative reranker rescores each parse and re-
orders the n-best list. Rerankers have been success-
fully used in syntactic parsing (Collins, 2000; Char-
niak and Johnson, 2005; Huang, 2008) and semantic
role labeling (Toutanova et al, 2008).
Rerankers provide additional advantages in our
case due to the mismatch between the dependency
structures that the parser operates on and their cor-
responding event structures. We convert the out-
put from the parser to event structures (Section 3.1)
before including them in the reranker. This al-
lows the reranker to capture features over the ac-
tual event structures rather than their original de-
pendency trees which may contain extraneous por-
tions.8 Furthermore, this lets the reranker optimize
the actual BioNLP F1 score. The parser, on the other
hand, attempts to optimize the Labeled Attachment
Score (LAS) between the dependency trees and con-
verted gold dependency trees. LAS is approximate
for two reasons. First, it is much more local than
the BioNLP metric.9 Second, the converted gold de-
pendency trees lose information that doesn?t transfer
to trees (specifically, that event structures are really
multi-DAGs and not trees).
We adapt the maximum entropy reranker from
Charniak and Johnson (2005) by creating a cus-
tomized feature extractor for event structures ? in
all other ways, the reranker model is unchanged. We
use the following types of features in the reranker:
? Source: Score and rank of the parse from the
8For instance, event anchors with no arguments could be
proposed by the parser. These event anchors are automatically
dropped by the conversion process.
9As an example, getting an edge label between an anchor
and its argument correct is unimportant if the anchor is missing
other arguments.
1631
Unreranked Reranked
Decoder(s) R P F1 R P F1
1P 65.6 76.7 70.7 68.0 77.6 72.5
2P 67.4 77.1 71.9 67.9 77.3 72.3
1N 67.5 76.7 71.8 ? ? ?
2N 68.9 77.1 72.7 ? ? ?
1P, 2P, 2N ? ? ? 68.5 78.2 73.1
(a) Gold event anchors
Unreranked Reranked
Decoder(s) R P F1 R P F1
1P 44.7 62.2 52.0 47.8 59.6 53.1
2P 45.9 61.8 52.7 48.4 57.5 52.5
1N 46.0 61.2 52.5 ? ? ?
2N 38.6 66.6 48.8 ? ? ?
1P, 2P, 2N ? ? ? 48.7 59.3 53.5
(b) Predicted event anchors
Table 1: BioNLP recall, precision, and F1 scores of individual decoders and the best decoder combination
on development data with the impact of event anchor detection and reranking. Decoder names include the
features order (1 or 2) followed by the projectivity (P = projective, N = non-projective).
decoder; number of different decoders produc-
ing the parse (when using multiple decoders).
? Event path: Path from each node in the event
tree up to the root. Unlike the Path features
in the parser, these paths are over event struc-
tures, not the syntactic dependency graphs from
the original English sentence. Variations of the
Event path features include whether to include
word forms (e.g., ?binds?), types (BINDING),
and/or argument slot names (THEME). We also
include the path length as a feature.
? Event frames: Event anchors with all their ar-
guments and argument slot names.
? Consistency: Similar to the parser Consis-
tency features, but capable of capturing larger
classes of errors (e.g., incorrect number or
types of arguments). We include the number of
violations from four different classes of errors.
To improve performance and robustness, features
are pruned as in Charniak and Johnson (2005): se-
lected features must distinguish a parse with the
highest F1 score in a n-best list, from a parse with a
suboptimal F1 score at least five times.
Rerankers can also be used to perform model
combination (Toutanova et al, 2008; Zhang et al,
2009; Johnson and Ural, 2010). While we use a sin-
gle parsing model, it has multiple decoders.10 When
combining multiple decoders, we concatenate their
n-best lists and extract the unique parses.
10We only have n-best versions of the projective decoders.
For the non-projective decoders, we use their 1-best parse.
4 Experimental Results
Our experiments use the BioNLP?09 shared task
corpus (Kim et al, 2009) which includes 800
biomedical abstracts (7,449 sentences, 8,597 events)
for training and 150 abstracts (1,450 sentences,
1,809 events) for development. The test set includes
260 abstracts, 2,447 sentences, and 3,182 events.
Throughout our experiments, we report BioNLP F1
scores with approximate span and recursive event
matching (as described in the shared task definition).
For preprocessing, we parsed all documents us-
ing the self-trained biomedical McClosky-Charniak-
Johnson reranking parser (McClosky, 2010). We
bias the anchor detector to favor recall, allowing the
parser and reranker to determine which event an-
chors will ultimately be used. When performing n-
best parsing, n = 50. For parser feature pruning,
? = 0.001.
Table 1a shows the performance of each of the de-
coders when using gold event anchors. In both cases
where n-best decoding is available, the reranker im-
proves performance over the 1-best parsers. We also
present the results from a reranker trained from mul-
tiple decoders which is our highest scoring model.11
In Table 1b, we present the output for the predicted
anchor scenario. In the case of the 2P decoder,
the reranker does not improve performance, though
the drop is minimal. This is because the reranker
chose an unfortunate regularization constant during
crossvalidation, most likely due to the small size of
the training data. In later experiments where more
11Including the 1N decoder as well provided no gains, possi-
bly because its outputs are mostly subsumed by the 2N decoder.
1632
data is available, the reranker consistently improves
accuracy (McClosky et al, 2011). As before, the
reranker trained from multiple decoders outperforms
unreranked models and reranked single decoders.
All in all, our best model in Table 1a scores 1 F1
point higher than the best system at the BioNLP?09
shared task, and the best model in Table 1b performs
similarly to the best shared task system (Bjo?rne et
al., 2009), which also scores 53.5% on development.
We show the effects of each system component
in Table 2. Note how our upper limit is 87.1%
due to our conversion process, which enforces the
tree constraint, drops events spanning sentences, and
performs approximate reconstruction of BINDING
events. Given that state-of-the-art systems on this
task currently perform in the 50-60% range, we are
not troubled by this number as it still allows for
plenty of potential.12 Bjo?rne et al (2009) list 94.7%
as the upper limit for their system. Considering
this relatively large difference, we find the results
in the previous table very encouraging. As in other
BioNLP?09 systems, our performance drops when
switching from gold to predicted anchor informa-
tion. Our decrease is similar to the one seen in
Bjo?rne et al (2009).
To show the potential of reranking, we provide or-
acle reranker scores in Table 3. An oracle reranker
picks the highest scoring parse from the available
parses. We limit the n-best lists to the top k parses
where k ? {1, 2, 10,All}. For single decoders,
?All? uses the entire 50-best list. For multiple de-
coders, the n-best lists are concatenated together.
The oracle score with multiple decoders and gold
anchors is only 0.4% lower than our upper limit (see
Table 2). This indicates that parses which could have
achieved that limit were nearly always present. Im-
proving the features in the reranker as well as the
original parsers will help us move closer to the limit.
With predicated anchors, the oracle score is about
13% lower but still shows significant potential.
Our final results on the test set, broken down by
class, are shown in Table 4. As with other systems,
complex events (e.g., REGULATION) prove harder
than simple events. To get a complex event cor-
rect, one must correctly detect and parse all events in
12Additionally, improvements such as document-level pars-
ing and DAG parsing would eliminate the need for much of the
approximate and lossy portions of the conversion process.
AD Parse RR Conv R P F1
X X X 45.9 61.8 52.7
X X X X 48.7 59.3 53.5
G X X 68.9 77.1 72.7
G X X X 68.5 78.2 73.1
G G G X 81.6 93.4 87.1
Table 2: Effect of each major component to the over-
all performance in the development corpus. Compo-
nents shown: AD ? event anchor detection; Parse
? best individual parsing model; RR ? reranking
multiple parsers; Conv ? conversion between the
event and dependency representations. ?G? indicates
that gold data was used; ?X? indicates that the actual
component was used.
n-best parses considered
Anchors Decoder(s) 1 2 10 All
Gold
1P 70.7 76.6 84.0 85.7
2P 71.8 77.5 84.8 86.2
1P, 2P, 2N ? ? ? 86.7
Predicted
1P 52.0 60.3 69.9 72.5
2P 52.7 60.7 70.1 72.5
1P, 2P, 2N ? ? ? 73.4
Table 3: Oracle reranker BioNLP F1 scores for
our n-best decoders and their combinations before
reranking on the development corpus.
the event subtree allowing small errors to have large
effects. Top systems on this task obtain F1 scores
of 52.0% at the shared task evaluation (Bjo?rne et
al., 2009) and 56.3% post evaluation (Miwa et al,
2010a). However, both systems are tailored to the
biomedical domain (the latter uses multiple syntac-
tic parsers), whereas our system has a design that is
virtually domain independent.
5 Discussion
We believe that the potential of our approach is
higher than what the current experiments show. For
example, the reranker can be used to combine not
only several parsers but also multiple anchor rec-
ognizers. This passes the anchor selection decision
to the reranker, which uses global information not
available to the current anchor recognizer or parser.
Furthermore, our approach can be adapted to parse
event structures in entire documents (instead of in-
1633
Event Class Count R P F1
Gene Expression 722 68.6 75.8 72.0
Transcription 137 42.3 51.3 46.4
Protein Catabolism 14 64.3 75.0 69.2
Phosphorylation 135 80.0 82.4 81.2
Localization 174 44.8 78.8 57.1
Binding 347 42.9 51.7 46.9
Regulation 291 23.0 36.6 28.3
Positive Regulation 983 28.4 42.5 34.0
Negative Regulation 379 29.3 43.5 35.0
Total 3,182 42.6 56.6 48.6
Table 4: Results in the test set broken by event class;
scores generated with the main official metric of ap-
proximate span and recursive event matching.
dividual sentences) by using a representation with a
unique ROOT node for all event structures in a doc-
ument. This representation has the advantage that
it maintains cross-sentence events (which account
for 5% of BioNLP?09 events), and it allows for
document-level features that model discourse struc-
ture. We plan to explore these ideas in future work.
One current limitation of the proposed model is
that it constrains event structures to map to trees. In
the BioNLP?09 corpus this leads to the removal of
almost 5% of the events, which generate DAGs in-
stead of trees. Local event extraction models (Bjo?rne
et al, 2009) do not have this limitation, because
their local decisions are blind to (and hence not
limited by) the global event structure. However,
our approach is agnostic to the actual parsing mod-
els used, so we can easily incorporate models that
can parse DAGs (Sagae and Tsujii, 2008). Addi-
tionally, we are free to incorporate any new tech-
niques from dependency parsing. Parsing using
dual-decomposition (Rush et al, 2010) seems espe-
cially promising in this area.
6 Conclusion
In this paper we proposed a simple approach for the
joint extraction of event structures: we converted
the representation of events and their arguments to
dependency trees with arcs between event anchors
and event arguments, and used a reranking parser to
parse these structures. Despite the fact that our ap-
proach has very little domain-specific engineering,
we obtain competitive results. Most importantly, we
showed that the joint modeling of event structures is
beneficial: our reranker outperforms parsing models
without reranking in five out of the six configura-
tions investigated.
Acknowledgments
The authors would like to thank Mark Johnson for
helpful discussions on the reranker component and
the BioNLP shared task organizers, Sampo Pyysalo
and Jin-Dong Kim, for answering questions. We
gratefully acknowledge the support of the Defense
Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-
C-0181. Any opinions, findings, and conclusion
or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect
the view of DARPA, AFRL, or the US government.
References
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting Complex Biological Events with Rich Graph-
Based Feature Sets. Proceedings of the Workshop on
BioNLP: Shared Task.
Nate Chambers and Dan Jurafsky. 2009. Unsupervised
Learning of Narrative Schemas and their Participants.
Proceedings of ACL.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 2005 Meeting of the As-
sociation for Computational Linguistics (ACL), pages
173?180
Nancy Chinchor. 1997. Overview of MUC-7. Pro-
ceedings of the Message Understanding Conference
(MUC-7).
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Machine Learning: Pro-
ceedings of the Seventeenth International Conference
(ICML 2000), pages 175?182.
Jenny R. Finkel and Christopher D. Manning. 2009.
Joint Parsing and Named Entity Recognition. Pro-
ceedings of NAACL.
Jenny R. Finkel and Christopher D. Manning. 2009b.
Nested Named Entity Recognition. Proceedings of
EMNLP.
Jan Hajic?, Massimiliano Ciaramita, Richard Johansson,
Daisuke Kawahara, Maria A. Marti, Lluis Marquez,
Adam Meyers, Joakim Nivre, Sebastian Pado, Jan
Stepanek, Pavel Stranak, Mihai Surdeanu, Nianwen
1634
Xue, and Yi Zhang. 2009. The CoNLL-2009 Shared
Task: Syntactic and Semantic Dependencies in Multi-
ple Languages. Proceedings of CoNLL.
Eduard Hovy, Mitchell P. Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% Solution. Proceedings of the NAACL-HLT.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586?594, Association for Com-
putational Linguistics.
Mark Johnson and Ahmet Engin Ural. 2010. Rerank-
ing the Berkeley and Brown Parsers. Proceedings of
NAACL.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of the BioNLP?09 Shared Task on Event Extrac-
tion. Proceedings of the NAACL-HLT 2009 Work-
shop on Natural Language Processing in Biomedicine
(BioNLP?09).
Mitchell P. Marcus, Beatrice Santorini, and Marry Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford Typed Hierarchies Represen-
tation. Proceedings of the COLING Workshop on
Cross-Framework and Cross-Domain Parser Evalua-
tion.
David McClosky. 2010. Any Domain Parsing: Auto-
matic Domain Adaptation for Natural Language Pars-
ing. PhD thesis, Department of Computer Science,
Brown University.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011. Event extraction as dependency pars-
ing in BioNLP 2011. In BioNLP 2011 Shared Task
(submitted).
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online Large-Margin Training of Dependency
Parsers. Proceedings of ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-projective Dependency Pars-
ing using Spanning Tree Algorithms. Proceedings of
HLT/EMNLP.
Ryan McDonald and Fernando Pereira. 2006. Online
Learning of Approximate Dependency Parsing Algo-
rithms. Proceedings of EACL.
Scott Miller, Michael Crystal, Heidi Fox, Lance
Ramshaw, Richard Schwartz, Rebecca Stone, and
Ralph Weischedel. 1997. BBN: Description of the
SIFT System as Used for MUC-7. Proceedings of the
Message Understanding Conference (MUC-7).
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010a. A Comparative Study of Syn-
tactic Parsers for Event Extraction. Proceedings of
the 2010 Workshop on Biomedical Natural Language
Processing.
Makoto Miwa, Rune Saetre, Jin-Dong Kim, and Jun?ichi
Tsujii. 2010b. Event Extraction with Complex Event
Classification Using Rich Features. Journal of Bioin-
formatics and Computational Biology, 8 (1).
Hoifung Poon and Lucy Vanderwende. 2010. Joint Infer-
ence for Knowledge Extraction from Biomedical Liter-
ature. Proceedings of NAACL.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A Markov Logic Approach
to Bio-Molecular Event Extraction. Proceedings of the
Workshop on BioNLP: Shared Task.
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. Proceedings of EMNLP.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-Reduce De-
pendency DAG Parsing. Proceedings of the COLING.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Lluis Marquez, and Joakim Nivre. 2008. The CoNLL-
2008 Shared Task on Joint Parsing of Syntactic and
Semantic Dependencies. Proceedings of CoNLL.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A Global Joint Model for Semantic
Role Labeling. Computational Linguistics 34(2).
Zhang, H. and Zhang, M. and Tan, C.L. and Li, H. 2009.
K-best combination of syntactic parsers. Proceedings
of Empirical Methods in Natural Language Process-
ing.
1635
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 977?986,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Discourse Complements Lexical Semantics
for Non-factoid Answer Reranking
Peter Jansen and Mihai Surdeanu
University of Arizona
Tucson, AZ, USA
{pajansen,msurdeanu}
@email.arizona.edu
Peter Clark
Allen Institute for Artificial Intelligence
Seattle, WA, USA
peterc@allenai.org
Abstract
We propose a robust answer reranking
model for non-factoid questions that inte-
grates lexical semantics with discourse in-
formation, driven by two representations
of discourse: a shallow representation cen-
tered around discourse markers, and a
deep one based on Rhetorical Structure
Theory. We evaluate the proposed model
on two corpora from different genres and
domains: one from Yahoo! Answers and
one from the biology domain, and two
types of non-factoid questions: manner
and reason. We experimentally demon-
strate that the discourse structure of non-
factoid answers provides information that
is complementary to lexical semantic sim-
ilarity between question and answer, im-
proving performance up to 24% (relative)
over a state-of-the-art model that exploits
lexical semantic similarity alone. We fur-
ther demonstrate excellent domain transfer
of discourse information, suggesting these
discourse features have general utility to
non-factoid question answering.
1 Introduction
Driven by several international evaluations and
workshops such as the Text REtrieval Conference
(TREC)
1
and the Cross Language Evaluation Fo-
rum (CLEF),
2
the task of question answering (QA)
has received considerable attention. However,
most of this effort has focused on factoid questions
rather than more complex non-factoid (NF) ques-
tions, such as manner, reason, or causation ques-
tions. Moreover, the vast majority of QA mod-
els explore only local linguistic structures, such
as syntactic dependencies or semantic role frames,
1
http://trec.nist.gov
2
http://www.clef-initiative.eu
which are generally restricted to individual sen-
tences. This is problematic for NF QA, where
questions are answered not by atomic facts, but
by larger cross-sentence conceptual structures that
convey the desired answers. Thus, to answer NF
questions, one needs a model of what these answer
structures look like.
Driven by this observation, our main hypothe-
sis is that the discourse structure of NF answers
provides complementary information to state-of-
the-art QA models that measure the similarity (ei-
ther lexical and/or semantic) between question and
answer. We propose a novel answer reranking
(AR) model that combines lexical semantics (LS)
with discourse information, driven by two rep-
resentations of discourse: a shallow representa-
tion centered around discourse markers and sur-
face text information, and a deep one based on
the Rhetorical Structure Theory (RST) discourse
framework (Mann and Thompson, 1988). To the
best of our knowledge, this work is the first to
systematically explore within- and cross-sentence
structured discourse features for NF AR. The con-
tributions of this work are:
1. We demonstrate that modeling discourse is
greatly beneficial for NF AR for two types
of NF questions, manner (?how?) and rea-
son (?why?), across two large datasets from
different genres and domains ? one from the
community question-answering (CQA) site
of Yahoo! Answers
3
, and one from a biology
textbook. Our results show statistically sig-
nificant improvements of up to 24% on top of
state-of-the-art LS models (Yih et al, 2013).
2. We demonstrate that both shallow and deep
discourse representations are useful, and, in
general, their combination performs best.
3. We show that discourse-based QA models us-
ing inter-sentence features considerably out-
3
http://answers.yahoo.com
977
perform single-sentence models when an-
swers span multiple sentences.
4. We demonstrate good domain transfer per-
formance between these corpora, suggesting
that answer discourse structures are largely
independent of domain, and thus broadly ap-
plicable to NF QA.
2 Related Work
The body of work on factoid QA is too broad to be
discussed here (see, e.g., the TREC workshops for
an overview). However, in the context of LS, Yih
et al (2013) recently addressed the problem of an-
swer sentence selection and demonstrated that LS
models, including recurrent neural network lan-
guage models (RNNLM), have a higher contribu-
tion to overall performance than exploiting syntac-
tic analysis. We extend this work by showing that
discourse models coupled with LS achieve the best
performance for NF AR.
The related work on NF QA is considerably
more scarce, but several trends are clear. First,
most NF QA approaches tend to use multiple sim-
ilarity models (information retrieval or alignment)
as features in discriminative rerankers (Riezler et
al., 2007; Higashinaka and Isozaki, 2008; Ver-
berne et al, 2010; Surdeanu et al, 2011). Sec-
ond, and more relevant to this work, all these ap-
proaches focus either on bag-of-word representa-
tions or linguistic structures that are restricted to
single sentences (e.g., syntactic dependencies, se-
mantic roles, or standalone discourse cue phrases).
Answering how questions using a single dis-
course marker, by, was previously explored by
Prager et al (2000), who searched for by followed
by a present participle (e.g. by *ing) to elevate an-
swer candidates in a ranking framework. Verberne
et al (2011) extracted 47 cue phrases such as be-
cause from a small collection of web documents,
and used the cosine similarity between an answer
candidate and a bag of words containing these cue
phrases as a single feature in their reranking model
for non-factoid why QA. Extending this, Oh et
al. (2013) built a classifier to identify causal re-
lations using a small set of cue phrases (e.g., be-
cause and is caused by). This classifier was then
used to extract instances of causal relations in an-
swer candidates, which were turned into features
in a reranking model for Japanense why QA.
In terms of discourse parsing, Verberne et al
(2007) conducted an initial evaluation of the util-
ity of RST structures to why QA by evaluating
Figure 1: Architecture of the reranking framework for QA.
performance on a small sample of seven WSJ ar-
ticles drawn from the RST Treebank (Carlson et
al., 2003). They later concluded that while dis-
course parsing appears to be useful for QA, auto-
mated discourse parsing tools are required before
this approach can be tested at scale (Verberne et
al., 2010). Inspired by this previous work and re-
cent work in discourse parsing (Feng and Hirst,
2012), our work is the first to systematically ex-
plore structured discourse features driven by sev-
eral discourse representations, combine discourse
with lexical semantic models, and evaluate these
representations on thousands of questions using
both in-domain and cross-domain experiments.
3 Approach
The proposed answer reranking component is em-
bedded in the QA framework illustrated in Figure
1. This framework functions in two distinct sce-
narios, which use the same AR model, but differ
in the way candidate answers are retrieved:
CQA: In this scenario, the task is defined as
reranking all the user-posted answers for a particu-
lar question to boost the community-selected best
answer to the top position. This is a commonly
used setup in the CQA community (Wang et al,
2009).
4
Thus, for a given question, all its answers
are fetched from the answer collection, and an ini-
tial ranking is constructed based on the cosine sim-
ilarity between theirs and the question?s lemma
vector representations, with lemmas weighted us-
ing tf.idf (Ch. 6, (Manning et al, 2008)).
4
Although most of these works use shallow textual fea-
tures and focus mostly on meta data, e.g., number of votes
for a particular answer. Here we use no meta data and rely
solely on linguistic features.
978
Traditional QA: In this scenario answers are
dynamically constructed from larger docu-
ments (Pasca, 2001). We use this setup to answer
questions from a biology textbook, where each
section is indexed as a standalone document, and
each paragraph in a given document is considered
as a candidate answer. We implemented the docu-
ment indexing and retrieval stage using Lucene
5
.
The candidate answers are scored using a linear
interpolation of two cosine similarity scores:
one between the entire parent document and
question (to model global context), and a second
between the answer candidate and question (for
local context).
6
Because the number of answer
candidates is typically large (e.g., equal to the
number of paragraphs in the textbook), we return
the N top candidates with the highest scores.
These answer candidates are then passed to the
answer reranking component, the focus of this
work. AR analyzes the candidates using more
expensive techniques to extract discourse and LS
features (detailed in ?4), and these features are
then used in concert with a learning framework to
rerank the candidates and elevate correct answers
to higher positions. For the learning framework,
we used SVM
rank
, a variant of Support Vector
Machines for structured output adapted to rank-
ing problems.
7
In addition to these features, each
reranker also includes a single feature containing
the score of each candidate, as computed by the
above candidate retrieval (CR) component.
8
4 Models and Features
We propose two separate discourse representation
schemes ? one shallow, centered around discourse
markers, and one deep, based on RST.
4.1 Discourse Marker Model
The discourse marker model (DMM) extracts
cross-sentence discourse structures centered
around a discourse marker. This extraction pro-
cess is illustrated in the top part of Figure 2. These
structures are represented using three components:
(1) A discourse marker from Daniel Marcu?s list
5
http://lucene.apache.org
6
We empirically observed that this combination of scores
performs better than using solely the cosine similarity be-
tween the answer and question.
7
http://www.cs.cornell.edu/people/tj/
svm_light/svm_rank.html
8
Including these scores as features in the reranker model
is a common strategy that ensures that the reranker takes ad-
vantage of the analysis already performed by the CR model.
(see Appendix B in Marcu (1997)), that serves as
a divisive boundary between sentences. Examples
of these markers include and, in, that, for, if, as,
not, by, and but; (2) two marker arguments, i.e.,
text segments before and after the marker, labeled
to indicate if they are related to the question text or
not; and (3) a sentence range around the marker,
which defines the length of these segments (e.g.,
?2 sentences). For example, a marker feature
may take the form of: QSEG BY OTHER SR2,
which means that the the marker by has been
detected in an answer candidate. Further, the text
preceeding by matches text from the question (and
is therefore labeled QSEG), while the text after by
differs considerably from the question text, and
is labeled OTHER. In this particular example, the
scope of this similarity matching occurs over a
span of ?2 sentences around the marker.
Note that our marker arguments are akin to
EDUs in RST, but, in this shallow representa-
tion, they are simply constructed around discourse
markers and bound by an arbitrary sentence range.
Argument Labels: We label marker arguments
based on their similarity to question content. If
text before or after a marker out to a given sen-
tence range matches the entire text of the ques-
tion (with a cosine similarity score larger than a
threshold), that argument takes on the label QSEG,
or OTHER otherwise. In this way the features are
only partially lexicalized with the discourse mark-
ers. Argument labels indicate only if lemmas from
the question were found in a discourse structure
present in an answer candidate, and do not speak
to the specific lemmas that were found. We show
in ?5 that these lightly lexicalized features perform
well in domain and transfer between domains. We
explore other argument labeling strategies in ?5.7.
Feature Values: Our reranking framework uses
real-valued features. The values of the discourse
features are the mean of the similarity scores (e.g.,
cosine similarity using tf.idf weighting) of the two
marker arguments and the corresponding question.
For example, the value of the QSEG BY QSEG SR1
feature in Figure 2 is the average of the cosine sim-
ilarities of the question text with the answer texts
before/after by out to a distance of one sentence
before/after the marker.
It is important to note that these discourse
features are more expressive than features based
on discourse markers alone (Higashinaka and
Isozaki, 2008; Verberne et al, 2010). First,
979
Figure 2: Top: Example feature generation for the discourse marker model, for one question (Q) and one answer candidate
(AC). Answer candidates are searched for discourse markers (italic) and question word matches (bold), which are used to
generate features both within-sentence (SR0), and ?1 sentence (SR1). The actual DMM exhaustively generates features for all
markers and all sentence ranges. Here we show just a few for brevity. Bottom: Example feature generation for the discourse
parser model using the output of an actual discourse parser. The DPM creates one feature for each individual discourse relation.
the argument sequences used here capture cross-
sentence discourse structures. Second, these fea-
tures model the intensity of the match between the
text surrounding the discourse structure and the
question text using both the assigned argument la-
bels and the feature values.
4.2 Discourse Parser Model
The discourse parser model (DPM) is based on the
RST discourse framework (Mann and Thompson,
1988). In RST, the text is segmented into a se-
quence of non-overlapping fragments called ele-
mentary discourse units (EDUs), and binary dis-
course relations recursively connect neighboring
units. Most relations are hypotactic, where one
of the units in the relation (the nucleus) is consid-
ered more important than the other (the satellite).
A few relations are paratactic, where both partici-
pants have equal importance. In the bottom part of
Figure 2, we show hypotactic relations as directed
arrows, from the nucleus to the satellite. In this
work, we construct the RST discourse trees using
the parser of Feng and Hirst (2012).
Relying on a proper discourse framework facil-
itates the modeling of the numerous implicit re-
lations that are not driven by discourse markers
(see Ch. 21 in Jurafsky and Martin (2009)). How-
ever, this also introduces noise because discourse
analysis is a complex task and discourse parsers
are not perfect. To mitigate this, we used a sim-
ple feature generation strategy, which creates one
feature for each individual discourse relation by
concatenating the relation type with the labels of
the discourse units participating in it. To this end,
for every relation, we extract the entire text dom-
inated by each of its arguments, and we gener-
ate labels for the two participants in the relation
using the same strategy as the DMM (based on
the similarity with the question content). Similar
to the DMM, these features take real values ob-
tained by averaging the cosine similarity of the ar-
guments with the question content.
9
Fig. 2 shows
several such features, created around two RST
Elaboration relations, indicating that the latter
sentences expand on the information at the begin-
ning of the answer. Other common relations in-
clude Attribution, Contrast, Background, and
Evaluation.
4.3 Lexical Semantics Model
Inspired by the work of Yih et al (2013), we in-
clude lexical semantics in our reranking model.
Several of their proposed models rely on propri-
etary data; here we focus on LS models that rely
on open-source data and frameworks. In particu-
lar, we use the recurrent neural network language
model (RNNLM) of Mikolov et al (2013; 2010).
Like any language model, a RNNLM estimates the
probability of observing a word given the preced-
ing context, but, in this process, it learns word
embeddings into a latent, conceptual space with
a fixed number of dimensions. Consequently, re-
lated words tend to have vectors that are close to
each other in this space.
We derive two LS measures from these vec-
tors, which are then are included as features in
the reranker. The first is a measure of the over-
all LS similarity of the question and answer can-
9
We investigated more complex features, e.g., by explor-
ing depths of two and three in the discourse tree, and also
models that relied on tree kernels over these trees, but none
improved upon this simple representation. This suggests that,
in the domains explored here, there is a degree of noise intro-
duced by the discourse parser, and the simple features pro-
posed here are the best strategy to avoid overfitting on it.
980
didate, which is computed as the cosine similarity
between the two composite vectors of the ques-
tion and the answer candidate. These composite
vectors are assembled by summing the vectors for
individual question (or answer candidate) words,
and re-normalizing this composite vector to unit
length. Both this overall similarity score, as well
as the average pairwise cosine similarity between
each word in the question and answer candidate,
serve as features.
5 Experiments
5.1 Data
To test the utility of our approach, we experi-
mented with the two QA scenarios introduced in
?3 using the following two datasets:
Yahoo! Answers Corpus (YA): Yahoo! An-
swers
10
is an open domain community-generated
QA site, with questions and answers that span for-
mal and precise to informal and ambiguous lan-
guage. Due to the speed limitations of the dis-
course parser, we randomly drew 10,000 QA pairs
from the corpus of how questions described by
Surdeanu et al (2011) using their filtering crite-
ria, with the additional criterion that answers had
to contain at least four community-generated an-
swers, one of which was voted as the top answer.
The number of answers to each question ranged
from 4 to over 50, with the average 9.
11
Biology Textbook Corpus (Bio): This corpus fo-
cuses on the domain of cellular biology, and con-
sists of 185 how and 193 why questions hand-
crafted by a domain expert. Each question has
one or more gold answers identified in Campbell?s
Biology (Reece et al, 2011), a popular under-
graduate text. The entire biology text (at para-
graph granularity) serves as the possible set of an-
swers. Note that while our system retrieves an-
swers at paragraph granularity, the expert was not
constrained in any way during the annotation pro-
cess, so gold answers might be smaller than a para-
graph or span multiple paragraphs. This compli-
cates evaluation metrics on this dataset (see ?5.3).
10
http://answers.yahoo.com
11
Note that our experimental setup, i.e., reranking all the
answers provided for each question, is different from that of
Surdeanu et al For each question, they retrieved candidate
answers from all answers voted as best for some question in
the collection. The setup in this paper, commonly used in the
CQA community (Wang et al, 2009), is more relevant here
because it includes both high and low quality answers.
For the YA CQA corpora, 50% of QA pairs
were used for training, 25% for development, and
25% for test. Because of the small size of the
Bio corpus, it was evaluated using 5-fold cross-
validation, with three folds for training, one for
development, and one for test.
The following additional resources were used:
DiscourseMarkers: A set of 75 high-frequency
12
single-word discourse markers were extracted
from Marcu?s (1997) list of cue phrases, and used
for feature generation in DMM. These discourse
markers are extremely common in the answer cor-
pora ? for example, the YA corpus contains an av-
erage of 7 markers per answer.
Discourse Trees: We generated all discourse trees
using the parser of Feng and Hirst (2012). For
YA, we parsed entire answers. For Bio, we parsed
individual paragraphs. Note that, because these
domains are considerably different from the RST
Treebank, the parser fails to produce a tree on
a large number of answer candidates: 6.2% for
YA, and 41.1% for Bio. In these situations, we
constructed artificial discourse trees using a right-
attachment heuristic and a single relation label X.
Lexical Semantics: We trained two different
RNNLMs for this work. First, for the YA exper-
iments we trained an open-domain RNNLM us-
ing the entire Gigaword corpus of approximately
4G words.
13
For the Bio experiments, we trained
a domain specific RNNLM over a concatenation
of the textbook and a subset of Wikipedia spe-
cific to biology. The latter was created by ex-
tracting: (a) pages matching a word/phrase in a
glossary of biology (derived from the textbook);
plus (b) pages hyperlinked from (a) that are also
tagged as being in a small set of (hand-selected)
biology-related categories. The combined dataset
contains 7.7M words. For all RNNLMs we used
200-dimensional vectors.
5.2 Hyper Parameter Tuning
The following hyper parameters were tuned using
grid search to maximize P@1 on each develop-
ment partition: (a) the segment matching thresh-
olds that determine the minimum cosine simi-
larity between an answer segment and a ques-
tion for the segment to be labeled QSEG; and (b)
12
We selected all cue phrases with more than 100 occur-
rences in the Brown corpus.
13
LDC catalog number LDC2012T21
981
P@1 MRR
# Model/Features P@1 Impr. MRR Impr.
YA Corpus
1 Random Baseline 14.29 26.12
2 CR Baseline 19.57 43.14
3 CR + DMM 24.05
?
+23% 46.40
?
+8%
4 CR + DPM 24.29
?
+24% 46.81
?
+9%
5 CR + DMM + DPM 24.81
?
+27% 47.10
?
+9%
6 CR + LS Baseline 26.57 49.31
7 CR + LS + DMM 29.29
?
+10% 50.99
?
+3%
8 CR + LS + DPM 28.73
?
+8% 50.77
?
+3%
9 CR + LS + DMM + DPM 30.49
?
+15% 51.89
?
+5%
Bio HOW
10 CR Baseline 24.12 32.90
11 CR + DMM 29.88
?
+24% 38.88
?
+18%
12 CR + DPM 28.93
?
+20% 37.75
?
+15%
13 CR + DMM + DPM 30.43
?
+26% 39.28
?
+19%
14 CR + LS Baseline 25.35 33.79
15 CR + LS + DMM 30.09
?
+19% 39.04
?
+16%
16 CR + LS + DPM 28.50 +12% 37.58
?
+11%
17 CR + LS + DMM + DPM 30.68
?
+21% 39.44
?
+17%
Bio WHY
18 CR Baseline 28.62 38.25
19 CR + DMM 38.01
?
+33% 46.39
?
+21%
20 CR + DPM 38.62
?
+35% 46.85
?
+23%
21 CR + DMM + DPM 39.36
?
+38% 47.64
?
+25%
22 CR + LS Baseline 31.73 39.89
23 CR + LS + DMM 38.60
?
+22% 46.41
?
+16%
24 CR + LS + DPM 39.45
?
+24% 47.38
?
+19%
25 CR + LS + DMM + DPM 39.32
?
+24% 47.86
?
+20%
Table 1: Overall results across three datasets. The improve-
ments in each section are computed relative to their respective
baseline (CR or CR + LS). Bold font indicates the best score
in a given column.
?
indicates that a score is significantly bet-
ter (p < 0.05) than the score of the corresponding baseline.
All significance tests were implemented using one-tailed non-
parametric bootstrap resampling using 10,000 iterations.
SVM
rank
?s regularization parameter C. For all ex-
periments, the sentence range parameter (SRx) for
DMM ranged from 0 (within sentence) to ?3 sen-
tences.
14
5.3 Evaluation Metrics
For YA, we used the standard implementations for
P@1 and mean reciprocal rank (MRR) (Manning
et al, 2008). In the Bio corpus, because answer
candidates are not guaranteed to match gold anno-
tations exactly, these metrics do not immediately
apply. We adapted them to this dataset by weigh-
ing each answer by its overlap with gold answers,
where overlap is measured as the highest F1 score
between the candidate and a gold answer. Thus,
P@1 reduces to this F1 score for the top answer.
For MRR, we used the rank of the candidate with
the highest overlap score, weighed by the inverse
of the rank. For example, if the best answer for a
question appears at rank 2 with an F1 score of 0.3,
the corresponding MRR score is 0.3/2.
14
This was only limited to reduce the combinatorial expan-
sion of feature generation, and in principle could be set much
broader.
5.4 Overall Results
Table 1 analyzes the performance of the proposed
reranking model on the three datasets and against
two baselines. The first baseline sorts the can-
didate answers in descending order of the scores
produced by the candidate retrieval (CR) module.
The second baseline (CR + LS) trains a rerank-
ing model without discourse, using just the CR
and LS features. For YA, we include an addi-
tional baseline that selects an answer randomly.
We list multiple versions of the proposed rerank-
ing model, broken down by the features used. For
Bio, we retrieved the top 20 answer candidates in
CR. At this setting, the oracle performance (i.e.,
the performance with perfect reranking of the 20
candidates) was 69.6% P@1 for Bio HOW, and
72.3% P@1 for Bio WHY. These relatively low
oracle scores, which serve as a performance ceil-
ing for our approach, highlight the difficulty of the
task. For YA, we used all answers provided for
each given question. For all experiments we used
a linear SVM kernel.
15
Examining Table 1, several trends are clear.
Both discourse models significantly increase both
P@1 and MRR performance over all baselines
broadly across genre, domain, and question types.
More specifically, DMM and DPM show similar
performance benefits when used individually, but
their combination generally outperforms the indi-
vidual models, illustrating the fact that the two
models capture related but different discourse in-
formation. This is a motivating result for discourse
analysis, especially considering that the discourse
parser was trained on a domain different from the
corpora used here.
Lexical semantic features increase performance
for all settings, but demonstrate far more utility
to the open-domain YA corpus. This disparity
is likely due to the difficulty in assembling LS
training data at an appropriate level for the bi-
ology corpus, contrasted with the relative abun-
dance of large scale open-domain lexical seman-
tic resources. For the YA corpus, where lexical
semantics showed the most benefit, simply adding
15
The performance of all models can ultimately be in-
creased by using more sophisticated learning frameworks,
and considering more answer candidates in CR (for Bio).
For example, SVMs with polynomial kernels of degree two
showed approximately half a percent (absolute) performance
gain over the linear kernel. However, this came at the ex-
pense of an experiment runtime about an order of magni-
tude larger. Experiments with more answer candidates in Bio
showed similar trends to the results reported.
982
Q How does myelination affect action potentials?
A
baseline
The major selective advantage of myelination is its space ef-
ficiency. A myelinated axon 20 microns in diameter has a
conduction speed faster than that of a squid giant axon [. . . ].
Furthermore, more than 2,000 of those myelinated axons can
be packed into the space occupied by just one giant axon.
A
rerank
A nerve impulse travels [. . . ] to the synaptic terminals by
propagation of a series action potentials along the axon. The
speed of conduction increases [. . . ] with myelination. Action
potentials in myelinated axons jump between the nodes of
Ranvier, a process called saltatory conduction.
Table 2: An example question from the Biology corpus
where the correct answer is elevated to the top position by
the discourse model. A
baseline
is the top answer proposed by
the CR + LS baseline, which is incorrect, whereas A
rerank
is
the correct answer boosted to the top after reranking. [. . . ]
indicates non-essential text that was removed for space.
LS features to the CR baseline increases baseline
P@1 performance from 19.57 to 26.57, a +36%
relative improvement. Most importantly, compar-
ing lines 5 and 9 with their respective baselines
(lines 2 and 6, respectively) indicates that LS is
largely orthogonal to discourse. Line 5, the top-
performing model with discourse but without LS
outperforms the CR baseline by +5.24 absolute
P@1 improvement. Similarly, line 9, the top-
performing model that combines discourse with
LS has a +5.69 absolute P@1 improvement over
the CR + LS baseline. That this absolute perfor-
mance increase is nearly identical indicates that
LS features are complementary to and additive
with the full discourse model. Indeed, an analy-
sis of the questions improved by discourse vs. LS
(line 5 vs. 6) showed that the intersection of the
two sets is low (approximately a third of each set).
Finally, while the discourse models perform
well for HOW or manner questions, performance
on Bio WHY corpus suggests that reason ques-
tions are particularly amenable to discourse anal-
ysis. Relative improvements on WHY questions
reach +38% (without LS) and +24% (with LS),
with absolute performance on these non-factoid
questions jumping from 28% to nearly 40% P@1.
Table 2 shows one example where discourse
helps boost the correct answer to the top posi-
tion. In this example, the correct answer con-
tains multiple Elaboration relations that are both
cross sentence (e.g., between the first two sen-
tences) and intra-sentence (e.g., between the first
part of the second sentence and the phrase ?with
myelination?). Model features associated with
Elaboration relations are ranked highly by the
learned model. In contrast, the answer preferred
by the baseline contains mostly Joint relations,
Range Bio HOW Bio WHY YA
CR + LS + DMM + DPM
within-sentence +0.8% +8.4% +13.1%
full model +21.0%
?
+23.9%
?
+14.8%
Table 3: Relative P@1 performance increase over the CR
+ LS baseline for a model containing only intra-sentence fea-
tures, compared to the full model.
which ?represent the lack of a rhetorical relation
between the two nuclei? (Mann and Thompson,
1988) and have very small weights in the model.
5.5 Intra vs. Inter-sentence Features
To tease apart the relative contribution of dis-
course features that occur only within a single
sentence versus features that span multiple sen-
tences, we examined the performance of the full
model when using only intra-sentence features,
i.e., SR0 features for DMM, and features based on
discourse relations where both EDUs appear in the
same sentence for DPM, versus the full intersen-
tence models. The results are shown in Table 3.
For the Bio corpus where answer candidates
consist of entire paragraphs of a biology text, over-
all performance is dominated by inter-sentence
discourse features. Conversely, for YA, a large
proportion of performance comes from features
that span only a single sentence. This is caused
by the fact that YA answers are far shorter and
of variable grammatical quality, with 39% of an-
swer candidates consisting of only a single sen-
tence, and 57% containing two or fewer sentences.
All in all, this experiment emphasizes that model-
ing both intra- and inter-sentence discourse (where
available) is beneficial for non-factoid QA.
5.6 Domain Transfer
Because these discourse models appear to cap-
ture high-level information about answer struc-
tures, we hypothesize that the models should make
use of many of the same discourse features, even
when training on data from different domains. Ta-
ble 4 shows that of the highest-weighted SVM
features learned when training models for HOW
questions on YA and Bio, many are shared (e.g.,
56.5% of the features in the top half of both DPMs
are shared), suggesting that a core set of discourse
features may be of utility across domains.
To test the generality of these features, we per-
formed a transfer study where the full model was
trained and tuned on the open-domain YA cor-
pus, then evaluated as is on Bio HOW. This is
983
Model Top 10% Top 25% Top 50%
DMM 20.2% 33.2% 49.4%
DPM 22.2% 39.1% 56.5%
Table 4: Percentage of top features with the highest SVM
weights that are shared between Bio HOW and YA models.
a somewhat radical setup, where the target cor-
pus has both a different genre (formal text vs.
CQA) and different domain (biology vs. open do-
main). These experiments were performed in sev-
eral groups: both with and without LS features, as
well as using either a single SVM or an ensem-
ble model that linearly interpolates the predictions
of two SVM classifiers (one each for DMM and
DPM).
16
The results are summarized in Table 5.
The transferred models always outperform the
baselines, but only the ensemble model?s improve-
ment is statistically significant. This confirms ex-
isting evidence that ensemble models perform bet-
ter cross-domain because they overfit less (Domin-
gos, 2012; Hastie et al, 2009). The ensemble
model without LS (third line) has a nearly identi-
cal P@1 score as the equivalent in-domain model
(line 13 in Table 1), while slightly surpassing in-
domain MRR performance. To the best of our
knowledge, this is one of the most striking demon-
strations of domain transfer in answer ranking
for non-factoid QA, and highlights the generality
of these discourse features in identifying answer
structures across domains and genres.
The results of the transferred models that in-
clude LS features are slightly lower, but still ap-
proach statistical significance for P@1 and are sig-
nificant for MRR. We hypothesize that the limited
transfer observed for models with LS compared to
their counterparts without LS is due to the dispar-
ity in the size and utility of the biology LS training
data compared to the open-domain LS resources.
The open-domain YA model learns to place more
weight on LS features, which are unable to provide
the same utility in the biology domain.
5.7 Integrating Discourse and LS
So far, we have treated LS and discourse as dis-
tinct features in the reranking model, However,
given that LS features greatly improve the CR
baseline, we hypothesize that a natural extension
16
The interpolation parameter was tuned on the YA devel-
opment corpus. The in-domain performance of the ensemble
model is similar to that of the single classifier in both YA and
Bio HOW so we omit these results here for simplicity.
P@1 MRR
Model/Features P@1 Impr. MRR Impr.
Transfer: YA? Bio HOW
CR Baseline 24.12 32.90
CR + DMM + DPM 27.13 +13% 36.36? +11%
(CR + DMM) ? 30.10
?
+25% 39.62
?
+20%
(CR + DPM)
CR + LS Baseline 25.35 33.79
CR + LS + DMM + DPM 25.79 +2% 35.58 +5%
(CR + LS + DMM) ? 29.54? +17% 38.68
?
+15%
(CR + LS + DPM)
Table 5: Transfer performance from YA to Bio HOW for
single classifiers and ensembles (denoted with a ?). ? indi-
cates approaching statistical significance with p = 0.07 or
0.06.
to the discourse models would be to make use of
LS similarity (in addition to the traditional infor-
mation retrieval similarity) to label discourse seg-
ments. For example, for the question ?How do
cells replicate??, answer discourse segments con-
taining LS associates of cell and replicate, e.g., nu-
cleus, membrane, genetic, and duplicate, should
be considered as related to the question (i.e., be
labeled QSEG). We implemented two such mod-
els, denoted DMM
LS
and DPM
LS
, by replacing
the component that assigns argument labels with
one that relies on LS. Specifically, as in ?4.3, we
compute the cosine similarity between the com-
posite LS vectors of the question text and each
marker argument (in DMM) or EDU (in DPM),
and label the corresponding answer segment QSEG
if this score is higher than a threshold, or OTHER
otherwise. This way, the DMM and DPM features
jointly capture discourse structures and semantic
similarity between answer segments and question.
To test this, we use the YA corpus, which has
the best-performing LS model. Because we are
adding two new discourse models, we now tune
four segment matching thresholds, one for each
of the DMM, DPM, DMM
LS
, and DPM
LS
mod-
els.
17
The results are shown in Table 6. These re-
sults demonstrate that incorporating LS in the dis-
course models further increases performance for
all configurations, nearly doubling the relative per-
formance benefits over models that do not inte-
grate LS and discourse (compare with lines 6?9
of Table 1). For example, the last model in the
table, which combines four discourse representa-
tions, improves P@1 by 24%, whereas the equiv-
alent model without this integration (line 9 in Ta-
ble 1) outperforms the baseline by only 15%.
17
These hyperparameters were tuned on the development
corpus, and were found to be stable over broad ranges.
984
P@1 MRR
Model Features P@1 Impr. MRR Impr.
CR + LS Baseline 26.57 49.31
CR + LS + DMM + DMM
LS
32.41
?
+22% 53.55
?
+9%
CR + LS + DPM + DPM
LS
31.21
?
+18% 52.50
?
+7%
CR + LS + DMM + DPM + 32.93
?
+24% 53.91
?
+9%
DMM
LS
+ DPM
LS
Table 6: YA results with integrated discourse and LS.
5.8 Error Analysis
We performed an error analysis of the full QA
model (CR + LS + DMM + DPM) across the en-
tire Bio corpus (lines 17 and 25 from Table 1). We
chose the Bio setup for this analysis because it is
more complex than the CQA one: here gold an-
swers may have a granularity completely differ-
ent from what the system choses as best answers
(in our particular case, the QA system is currently
limited to answers consisting of single paragraphs,
whereas gold answers may be of any size).
Here, 94 of the 378 Bio HOW and WHY ques-
tions have improved answer scores, while 36 have
reduced performance relative to the CR baseline.
Of these 36 questions where answer scores de-
creased, nearly two thirds were directly related to
the paragraph granularity of the candidate answer
retrieval (see ?5.1):
Same Subsection (50%): In these cases, the
model selected an on-topic answer paragraph in
the same subsection of the textbook as a gold an-
swer. Often times this paragraph directly preceded
or followed the gold answer.
Answer Window Size (14%): Here, both the CR
and full model chose a paragraph containing a dif-
ferent gold answer. However, as discussed, gold
answers may unevenly straddle paragraph bound-
aries, and the paragraph chosen by the model hap-
pened to have a somewhat lower overlap with its
gold answer than the one chosen by the baseline.
Similar Topic (25%): The model chose a para-
graph that had a similar topic to the question, but
doesn?t answer the question. These are challeng-
ing errors, often associated with short questions
(e.g. ?How does HIV work??) that provide few
keywords. In these cases, discourse features tend
to dominate, and shift the focus towards answers
that have many discourse structures deemed rel-
evant. For example, for the above question, the
model chose a paragraph containing many dis-
course structures positively correlated with high-
quality answers, but which describes the origins
of HIV instead of how the virus enters a cell.
Similar Words, Different Topic (8%): The
model chose a paragraph that had many of the
same words as the question, but is on a different
topic. For example, for the question ?How are fos-
sil fuels formed, and why do they contain so much
energy??, the model selected an answer that men-
tions fossil fuels in a larger discussion of human
ecological footprints. Here, the matching of both
keywords and discourse structures shifted the an-
swer towards a different, incorrect topic.
Finally, in one case (3%), the model identified
an answer paragraph that contained a gold answer,
but was missed by the domain expert annotator.
In summary, this analysis suggests that, for the
majority of errors, the QA system selects an an-
swer that is both topical and adjacent to a gold an-
swer selected by the domain expert. This suggests
that most errors are minor and are driven by cur-
rent limitations of our answer boundary selection
mechanism, rather than the inherent limitations of
the discourse model.
6 Conclusions
This work focuses on two important aspects of an-
swer reranking for non-factoid QA: similarity be-
tween question and answer content, and answer
structure. While the former has been addressed
with a variety of lexical-semantic models, the lat-
ter has received little attention. Here we show
how to model answer structures using discourse
and how to integrate the two aspects into a holis-
tic framework. Empirically we show that model-
ing answer discourse structures is complementary
to modeling lexical semantic similarity and that
the best performance is obtained when they are
tightly integrated. We evaluate the proposed ap-
proach on multiple genres and question types and
obtain benefits of up to 24% relative improvement
over a strong baseline that combines information
retrieval and lexical semantics. We further demon-
strate that answer discourse structures are largely
independent of domain and transfer well, even be-
tween radically different datasets.
This work is open source and available at:
http://nlp.sista.arizona.edu/releases/
acl2014.
Acknowledgements
We thank the Allen Institute for Artificial Intelli-
gence for funding this work. We would also like
to thank the three anonymous reviewers for their
helpful comments and suggestions.
985
References
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2003. Building a Discourse-Tagged
Corpus in the Framework of Rhetorical Structure
Theory. In Jan van Kuppevelt and Ronnie Smith,
editors, Current Directions in Discourse and Dia-
logue, pages 85?112. Kluwer Academic Publishers.
Pedro Domingos. 2012. A few useful things to know
about machine learning. Communications of the
ACM, 55(10).
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
discourse parsing with rich linguistic features. In
Proceedings of the Association for Computational
Linguistics.
Trevor Hastie, Robert Tibshirani, and Jerome Fried-
man. 2009. The Elements of Statistical Learn-
ing: Data Mining, Inference, and Prediction, Sec-
ond Edition. Springer.
Ryuichiro Higashinaka and Hideki Isozaki. 2008.
Corpus-based question answering for why-
questions. In Proceedings of the Proceedings of the
Third International Joint Conference on Natural
Language Processing (IJCNLP), pages 418?425,
Hyderabad, India.
Dan Jurafsky and James H. Martin. 2009. Speech
and Language Processing, Second Edition. Prentice
Hall.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to Information
Retrieval. Cambridge University Press.
Daniel Marcu. 1997. The Rhetorical Parsing, Summa-
rization, and Generation of Natural Language Texts.
Ph.D. thesis, University of Toronto.
Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan
Cernocky, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Pro-
ceedings of the 11th Annual Conference of the In-
ternational Speech Communication Association (IN-
TERSPEECH 2010).
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word repre-
sentations in vector space. In Proceedings of the
International Conference on Learning Representa-
tions (ICLR).
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Motoki Sano, Stijn De Saeger, and Kiyonori Ohtake.
2013. Why-question answering using intra- and
inter-sentential causal relations. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 1733?1743, Sofia, Bulgaria, August. Associa-
tion for Computational Linguistics.
Marius Pasca. 2001. High-Performance, Open-
Domain Question Answering from Large Text Col-
lections. Ph.D. thesis, Southern Methodist Univer-
sity.
John Prager, Eric Brown, Anni Coden, and Dragomir
Radev. 2000. Question-answering by predictive an-
notation. In Proceedings of the 23rd annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, SIGIR ?00,
pages 184?191, New York, NY, USA. ACM.
J.B. Reece, L.A. Urry, M.L. Cain, S.A. Wasserman,
and P.V. Minorsky. 2011. Campbell Biology. Pear-
son Benjamin Cummings.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007.
Statistical machine translation for query expansion
in answer retrieval. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 464?471, Prague, Czech
Republic.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-
factoid questions from web collections. Computa-
tional Linguistics, 37(2):351?383.
Susan Verberne, Lou Boves, Nelleke Oostdijk, Peter-
Arno Coppen, et al 2007. Discourse-based an-
swering of why-questions. Traitement Automatique
des Langues, Discours et document: traitements au-
tomatiques, 47(2):21?41.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and
Peter-Arno Coppen. 2010. What is not in the bag
of words for why-qa? Computational Linguistics,
36(2):229?245.
Suzan Verberne, Hans Halteren, Daphne Theijssen,
Stephan Raaijmakers, and Lou Boves. 2011. Learn-
ing to rank for why-question answering. Inf. Retr.,
14(2):107?132, April.
Xin-Jing Wang, Xudong Tu, Dan Feng, and Lei Zhang.
2009. Ranking community answers by modeling
question-answer relationships via analogical reason-
ing. In Proceedings of the Annual ACM SIGIR Con-
ference.
Wen-tau Yih, Ming-Wei Chang, Christopher Meek, and
Andrzej Pastusiak. 2013. Question answering using
enhanced lexical semantic models. In Proceedings
of the 51st Annual Meeting of the Association for
Computational Linguistics (ACL).
986
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55?60,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
The Stanford CoreNLP Natural Language Processing Toolkit
Christopher D. Manning
Linguistics & Computer Science
Stanford University
manning@stanford.edu
Mihai Surdeanu
SISTA
University of Arizona
msurdeanu@email.arizona.edu
John Bauer
Dept of Computer Science
Stanford University
horatio@stanford.edu
Jenny Finkel
Prismatic Inc.
jrfinkel@gmail.com
Steven J. Bethard
Computer and Information Sciences
U. of Alabama at Birmingham
bethard@cis.uab.edu
David McClosky
IBM Research
dmcclosky@us.ibm.com
Abstract
We describe the design and use of the
Stanford CoreNLP toolkit, an extensible
pipeline that provides core natural lan-
guage analysis. This toolkit is quite widely
used, both in the research NLP community
and also among commercial and govern-
ment users of open source NLP technol-
ogy. We suggest that this follows from
a simple, approachable design, straight-
forward interfaces, the inclusion of ro-
bust and good quality analysis compo-
nents, and not requiring use of a large
amount of associated baggage.
1 Introduction
This paper describe the design and development of
Stanford CoreNLP, a Java (or at least JVM-based)
annotation pipeline framework, which provides
most of the common core natural language pro-
cessing (NLP) steps, from tokenization through to
coreference resolution. We describe the original
design of the system and its strengths (section 2),
simple usage patterns (section 3), the set of pro-
vided annotators and how properties control them
(section 4), and how to add additional annotators
(section 5), before concluding with some higher-
level remarks and additional appendices. While
there are several good natural language analysis
toolkits, Stanford CoreNLP is one of the most
used, and a central theme is trying to identify the
attributes that contributed to its success.
2 Original Design and Development
Our pipeline system was initially designed for in-
ternal use. Previously, when combining multiple
natural language analysis components, each with
their own ad hoc APIs, we had tied them together
with custom glue code. The initial version of the
Tokeniza)on*
Sentence*Spli0ng*
Part4of4speech*Tagging*
Morphological*Analysis*
Named*En)ty*Recogni)on*
Syntac)c*Parsing*
Other*Annotators*
Coreference*Resolu)on**
Raw*text*
Execu)
on*Flow
* Annota)on*Object*
Annotated*text*
(tokenize)*
(ssplit)*
(pos)*
(lemma)*
(ner)*
(parse)*
(dcoref)*
(gender, sentiment)!
Figure 1: Overall system architecture: Raw text
is put into an Annotation object and then a se-
quence of Annotators add information in an analy-
sis pipeline. The resulting Annotation, containing
all the analysis information added by the Annota-
tors, can be output in XML or plain text forms.
annotation pipeline was developed in 2006 in or-
der to replace this jumble with something better.
A uniform interface was provided for an Annota-
tor that adds some kind of analysis information to
some text. An Annotator does this by taking in an
Annotation object to which it can add extra infor-
mation. An Annotation is stored as a typesafe het-
erogeneous map, following the ideas for this data
type presented by Bloch (2008). This basic archi-
tecture has proven quite successful, and is still the
basis of the system described here. It is illustrated
in figure 1. The motivations were:
? To be able to quickly and painlessly get linguis-
tic annotations for a text.
? To hide variations across components behind a
common API.
? To have a minimal conceptual footprint, so the
system is easy to learn.
? To provide a lightweight framework, using plain
Java objects (rather than something of heav-
ier weight, such as XML or UIMA?s Common
Analysis System (CAS) objects).
55
In 2009, initially as part of a multi-site grant
project, the system was extended to be more easily
usable by a broader range of users. We provided
a command-line interface and the ability to write
out an Annotation in various formats, including
XML. Further work led to the system being re-
leased as free open source software in 2010.
On the one hand, from an architectural perspec-
tive, Stanford CoreNLP does not attempt to do ev-
erything. It is nothing more than a straightforward
pipeline architecture. It provides only a Java API.
1
It does not attempt to provide multiple machine
scale-out (though it does provide multi-threaded
processing on a single machine). It provides a sim-
ple concrete API. But these requirements satisfy
a large percentage of potential users, and the re-
sulting simplicity makes it easier for users to get
started with the framework. That is, the primary
advantage of Stanford CoreNLP over larger frame-
works like UIMA (Ferrucci and Lally, 2004) or
GATE (Cunningham et al., 2002) is that users do
not have to learn UIMA or GATE before they can
get started; they only need to know a little Java.
In practice, this is a large and important differ-
entiator. If more complex scenarios are required,
such as multiple machine scale-out, they can nor-
mally be achieved by running the analysis pipeline
within a system that focuses on distributed work-
flows (such as Hadoop or Spark). Other systems
attempt to provide more, such as the UIUC Cu-
rator (Clarke et al., 2012), which includes inter-
machine client-server communication for process-
ing and the caching of natural language analyses.
But this functionality comes at a cost. The system
is complex to install and complex to understand.
Moreover, in practice, an organization may well
be committed to a scale-out solution which is dif-
ferent from that provided by the natural language
analysis toolkit. For example, they may be using
Kryo or Google?s protobuf for binary serialization
rather than Apache Thrift which underlies Cura-
tor. In this case, the user is better served by a fairly
small and self-contained natural language analysis
system, rather than something which comes with
a lot of baggage for all sorts of purposes, most of
which they are not using.
On the other hand, most users benefit greatly
from the provision of a set of stable, robust, high
1
Nevertheless, it can call an analysis component written in
other languages via an appropriate wrapper Annotator, and
in turn, it has been wrapped by many people to provide Stan-
ford CoreNLP bindings for other languages.
quality linguistic analysis components, which can
be easily invoked for common scenarios. While
the builder of a larger system may have made over-
all design choices, such as how to handle scale-
out, they are unlikely to be an NLP expert, and
are hence looking for NLP components that just
work. This is a huge advantage that Stanford
CoreNLP and GATE have over the empty tool-
box of an Apache UIMA download, something
addressed in part by the development of well-
integrated component packages for UIMA, such
as ClearTK (Bethard et al., 2014), DKPro Core
(Gurevych et al., 2007), and JCoRe (Hahn et al.,
2008). However, the solution provided by these
packages remains harder to learn, more complex
and heavier weight for users than the pipeline de-
scribed here.
These attributes echo what Patricio (2009) ar-
gued made Hibernate successful, including: (i) do
one thing well, (ii) avoid over-design, and (iii)
up and running in ten minutes or less! Indeed,
the design and success of Stanford CoreNLP also
reflects several other of the factors that Patricio
highlights, including (iv) avoid standardism, (v)
documentation, and (vi) developer responsiveness.
While there are many factors that contribute to the
uptake of a project, and it is hard to show causal-
ity, we believe that some of these attributes ac-
count for the fact that Stanford CoreNLP is one of
the more used NLP toolkits. While we certainly
have not done a perfect job, compared to much
academic software, Stanford CoreNLP has gained
from attributes such as clear open source licens-
ing, a modicum of attention to documentation, and
attempting to answer user questions.
3 Elementary Usage
A key design goal was to make it very simple to
set up and run processing pipelines, from either
the API or the command-line. Using the API, run-
ning a pipeline can be as easy as figure 2. Or,
at the command-line, doing linguistic processing
for a file can be as easy as figure 3. Real life is
rarely this simple, but the ability to get started us-
ing the product with minimal configuration code
gives new users a very good initial experience.
Figure 4 gives a more realistic (and complete)
example of use, showing several key properties of
the system. An annotation pipeline can be applied
to any text, such as a paragraph or whole story
rather than just a single sentence. The behavior of
56
Annotator pipeline = new StanfordCoreNLP();
Annotation annotation = new Annotation(
"Can you parse my sentence?");
pipeline.annotate(annotation);
Figure 2: Minimal code for an analysis pipeline.
export StanfordCoreNLP_HOME /where/installed
java -Xmx2g -cp $StanfordCoreNLP_HOME/*
edu.stanford.nlp.StanfordCoreNLP
-file input.txt
Figure 3: Minimal command-line invocation.
import java.io.*;
import java.util.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.trees.TreeCoreAnnotations.*;
import edu.stanford.nlp.util.*;
public class StanfordCoreNlpExample {
public static void main(String[] args) throws IOException {
PrintWriter xmlOut = new PrintWriter("xmlOutput.xml");
Properties props = new Properties();
props.setProperty("annotators",
"tokenize, ssplit, pos, lemma, ner, parse");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
Annotation annotation = new Annotation(
"This is a short sentence. And this is another.");
pipeline.annotate(annotation);
pipeline.xmlPrint(annotation, xmlOut);
// An Annotation is a Map and you can get and use the
// various analyses individually. For instance, this
// gets the parse tree of the 1st sentence in the text.
List<CoreMap> sentences = annotation.get(
CoreAnnotations.SentencesAnnotation.class);
if (sentences != null && sentences.size() > 0) {
CoreMap sentence = sentences.get(0);
Tree tree = sentence.get(TreeAnnotation.class);
PrintWriter out = new PrintWriter(System.out);
out.println("The first sentence parsed is:");
tree.pennPrint(out);
}
}
}
Figure 4: A simple, complete example program.
annotators in a pipeline is controlled by standard
Java properties in a Properties object. The most
basic property to specify is what annotators to run,
in what order, as shown here. But as discussed be-
low, most annotators have their own properties to
allow further customization of their usage. If none
are specified, reasonable defaults are used. Run-
ning the pipeline is as simple as in the first exam-
ple, but then we show two possibilities for access-
ing the results. First, we convert the Annotation
object to XML and write it to a file. Second, we
show code that gets a particular type of informa-
tion out of an Annotation and then prints it.
Our presentation shows only usage in Java, but
the Stanford CoreNLP pipeline has been wrapped
by others so that it can be accessed easily from
many languages, including Python, Ruby, Perl,
Scala, Clojure, Javascript (node.js), and .NET lan-
guages, including C# and F#.
4 Provided annotators
The annotators provided with StanfordCoreNLP
can work with any character encoding, making use
of Java?s good Unicode support, but the system
defaults to UTF-8 encoding. The annotators also
support processing in various human languages,
providing that suitable underlying models or re-
sources are available for the different languages.
The system comes packaged with models for En-
glish. Separate model packages provide support
for Chinese and for case-insensitive processing of
English. Support for other languages is less com-
plete, but many of the Annotators also support
models for French, German, and Arabic (see ap-
pendix B), and building models for further lan-
guages is possible using the underlying tools. In
this section, we outline the provided annotators,
focusing on the English versions. It should be
noted that some of the models underlying annota-
tors are trained from annotated corpora using su-
pervised machine learning, while others are rule-
based components, which nevertheless often re-
quire some language resources of their own.
tokenize Tokenizes the text into a sequence of to-
kens. The English component provides a PTB-
style tokenizer, extended to reasonably handle
noisy and web text. The corresponding com-
ponents for Chinese and Arabic provide word
and clitic segmentation. The tokenizer saves the
character offsets of each token in the input text.
cleanxml Removes most or all XML tags from
the document.
ssplit Splits a sequence of tokens into sentences.
truecase Determines the likely true case of tokens
in text (that is, their likely case in well-edited
text), where this information was lost, e.g., for
all upper case text. This is implemented with
a discriminative model using a CRF sequence
tagger (Finkel et al., 2005).
pos Labels tokens with their part-of-speech (POS)
tag, using a maximum entropy POS tagger
(Toutanova et al., 2003).
lemma Generates the lemmas (base forms) for all
tokens in the annotation.
gender Adds likely gender information to names.
ner Recognizes named (PERSON, LOCATION,
ORGANIZATION, MISC) and numerical
(MONEY, NUMBER, DATE, TIME, DU-
RATION, SET) entities. With the default
57
annotators, named entities are recognized
using a combination of CRF sequence taggers
trained on various corpora (Finkel et al., 2005),
while numerical entities are recognized using
two rule-based systems, one for money and
numbers, and a separate state-of-the-art system
for processing temporal expressions (Chang
and Manning, 2012).
regexner Implements a simple, rule-based NER
over token sequences building on Java regular
expressions. The goal of this Annotator is to
provide a simple framework to allow a user to
incorporate NE labels that are not annotated in
traditional NL corpora. For example, a default
list of regular expressions that we distribute
in the models file recognizes ideologies (IDE-
OLOGY), nationalities (NATIONALITY), reli-
gions (RELIGION), and titles (TITLE).
parse Provides full syntactic analysis, including
both constituent and dependency representa-
tion, based on a probabilistic parser (Klein and
Manning, 2003; de Marneffe et al., 2006).
sentiment Sentiment analysis with a composi-
tional model over trees using deep learning
(Socher et al., 2013). Nodes of a binarized tree
of each sentence, including, in particular, the
root node of each sentence, are given a senti-
ment score.
dcoref Implements mention detection and both
pronominal and nominal coreference resolution
(Lee et al., 2013). The entire coreference graph
of a text (with head words of mentions as nodes)
is provided in the Annotation.
Most of these annotators have various options
which can be controlled by properties. These can
either be added to the Properties object when cre-
ating an annotation pipeline via the API, or spec-
ified either by command-line flags or through a
properties file when running the system from the
command-line. As a simple example, input to the
system may already be tokenized and presented
one-sentence-per-line. In this case, we wish the
tokenization and sentence splitting to just work by
using the whitespace, rather than trying to do any-
thing more creative (be it right or wrong). This can
be accomplished by adding two properties, either
to a properties file:
tokenize.whitespace: true
ssplit.eolonly: true
in code:
/** Simple annotator for locations stored in a gazetteer. */
package org.foo;
public class GazetteerLocationAnnotator implements Annotator {
// this is the only method an Annotator must implement
public void annotate(Annotation annotation) {
// traverse all sentences in this document
for (CoreMap sentence:annotation.get(SentencesAnnotation.class)) {
// loop over all tokens in sentence (the text already tokenized)
List<CoreLabel> toks = sentence.get(TokensAnnotation.class);
for (int start = 0; start < toks.size(); start++) {
// assumes that the gazetteer returns the token index
// after the match or -1 otherwise
int end = Gazetteer.isLocation(toks, start);
if (end > start) {
for (int i = start; i < end; i ++) {
toks.get(i).set(NamedEntityTagAnnotation.class,"LOCATION");
}
}
}
}
}
}
Figure 5: An example of a simple custom anno-
tator. The annotator marks the words of possibly
multi-word locations that are in a gazetteer.
props.setProperty("tokenize.whitespace", "true");
props.setProperty("ssplit.eolonly", "true");
or via command-line flags:
-tokenize.whitespace -ssplit.eolonly
We do not attempt to describe all the properties
understood by each annotator here; they are avail-
able in the documentation for Stanford CoreNLP.
However, we note that they follow the pattern of
being x.y, where x is the name of the annotator
that they apply to.
5 Adding annotators
While most users work with the provided annota-
tors, it is quite easy to add additional custom an-
notators to the system. We illustrate here both how
to write an Annotator in code and how to load it
into the Stanford CoreNLP system. An Annotator
is a class that implements three methods: a sin-
gle method for analysis, and two that describe the
dependencies between analysis steps:
public void annotate(Annotation annotation);
public Set<Requirement> requirementsSatisfied();
public Set<Requirement> requires();
The information in an Annotation is updated in
place (usually in a non-destructive manner, by
adding new keys and values to the Annotation).
The code for a simple Annotator that marks loca-
tions contained in a gazetteer is shown in figure 5.
2
Similar code can be used to write a wrapper Anno-
tator, which calls some pre-existing analysis com-
ponent, and adds its results to the Annotation.
2
The functionality of this annotator is already provided by
the regexner annotator, but it serves as a simple example.
58
While building an analysis pipeline, Stanford
CoreNLP can add additional annotators to the
pipeline which are loaded using reflection. To pro-
vide a new Annotator, the user extends the class
edu.stanford.nlp.pipeline.Annotator
and provides a constructor with the signature
(String, Properties). Then, the user adds
the property
customAnnotatorClass.FOO: BAR
to the properties used to create the pipeline. If
FOO is then added to the list of annotators, the
class BAR will be loaded to instantiate it. The
Properties object is also passed to the constructor,
so that annotator-specific behavior can be initial-
ized from the Properties object. For instance, for
the example above, the properties file lines might
be:
customAnnotatorClass.locgaz: org.foo.GazetteerLocationAnnotator
annotators: tokenize,ssplit,locgaz
locgaz.maxLength: 5
6 Conclusion
In this paper, we have presented the design
and usage of the Stanford CoreNLP system, an
annotation-based NLP processing pipeline. We
have in particular tried to emphasize the proper-
ties that we feel have made it successful. Rather
than trying to provide the largest and most engi-
neered kitchen sink, the goal has been to make it
as easy as possible for users to get started using
the framework, and to keep the framework small,
so it is easily comprehensible, and can easily be
used as a component within the much larger sys-
tem that a user may be developing. The broad us-
age of this system, and of other systems such as
NLTK (Bird et al., 2009), which emphasize acces-
sibility to beginning users, suggests the merits of
this approach.
A Pointers
Website: http://nlp.stanford.edu/software/
corenlp.shtml
Github: https://github.com/stanfordnlp/CoreNLP
Maven: http://mvnrepository.com/artifact/edu.
stanford.nlp/stanford-corenlp
License: GPL v2+
Stanford CoreNLP keeps the models for ma-
chine learning components and miscellaneous
other data files in a separate models jar file. If you
are using Maven, you need to make sure that you
list the dependency on this models file as well as
the code jar file. You can do that with code like the
following in your pom.xml. Note the extra depen-
dency with a classifier element at the bottom.
<dependency>
<groupId>edu.stanford.nlp</groupId>
<artifactId>stanford-corenlp</artifactId>
<version>3.3.1</version>
</dependency>
<dependency>
<groupId>edu.stanford.nlp</groupId>
<artifactId>stanford-corenlp</artifactId>
<version>3.3.1</version>
<classifier>models</classifier>
</dependency>
B Human language support
We summarize the analysis components supported
for different human languages in early 2014.
Annotator Ara- Chi- Eng- Fre- Ger-
bic nese lish nch man
Tokenize X X X X X
Sent. split X X X X X
Truecase X
POS X X X X X
Lemma X
Gender X
NER X X X
RegexNER X X X X X
Parse X X X X X
Dep. Parse X X
Sentiment X
Coref. X
C Getting the sentiment of sentences
We show a command-line for sentiment analysis.
$ cat sentiment.txt
I liked it.
It was a fantastic experience.
The plot move rather slowly.
$ java -cp "*" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators
tokenize,ssplit,pos,lemma,parse,sentiment -file sentiment.txt
Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/
english-left3words/english-left3words-distsim.tagger ... done [1.0 sec].
Adding annotator lemma
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/
englishPCFG.ser.gz ... done [1.4 sec].
Adding annotator sentiment
Ready to process: 1 files, skipped 0, total 1
Processing file /Users/manning/Software/stanford-corenlp-full-2014-01-04/
sentiment.txt ... writing to /Users/manning/Software/
stanford-corenlp-full-2014-01-04/sentiment.txt.xml {
Annotating file /Users/manning/Software/stanford-corenlp-full-2014-01-04/
sentiment.txt [0.583 seconds]
} [1.219 seconds]
Processed 1 documents
Skipped 0 documents, error annotating 0 documents
Annotation pipeline timing information:
PTBTokenizerAnnotator: 0.0 sec.
WordsToSentencesAnnotator: 0.0 sec.
POSTaggerAnnotator: 0.0 sec.
59
MorphaAnnotator: 0.0 sec.
ParserAnnotator: 0.4 sec.
SentimentAnnotator: 0.1 sec.
TOTAL: 0.6 sec. for 16 tokens at 27.4 tokens/sec.
Pipeline setup: 3.0 sec.
Total time for StanfordCoreNLP pipeline: 4.2 sec.
$ grep sentiment sentiment.txt.xml
<sentence id="1" sentimentValue="3" sentiment="Positive">
<sentence id="2" sentimentValue="4" sentiment="Verypositive">
<sentence id="3" sentimentValue="1" sentiment="Negative">
D Use within UIMA
The main part of using Stanford CoreNLP within
the UIMA framework (Ferrucci and Lally, 2004)
is mapping between CoreNLP annotations, which
are regular Java classes, and UIMA annotations,
which are declared via XML type descriptors
(from which UIMA-specific Java classes are gen-
erated). A wrapper for CoreNLP will typically de-
fine a subclass of JCasAnnotator ImplBase whose
process method: (i) extracts UIMA annotations
from the CAS, (ii) converts UIMA annotations to
CoreNLP annotations, (iii) runs CoreNLP on the
input annotations, (iv) converts the CoreNLP out-
put annotations into UIMA annotations, and (v)
saves the UIMA annotations to the CAS.
To illustrate part of this process, the ClearTK
(Bethard et al., 2014) wrapper converts CoreNLP
token annotations to UIMA annotations and saves
them to the CAS with the following code:
int begin = tokenAnn.get(CharacterOffsetBeginAnnotation.class);
int end = tokenAnn.get(CharacterOffsetEndAnnotation.class);
String pos = tokenAnn.get(PartOfSpeechAnnotation.class);
String lemma = tokenAnn.get(LemmaAnnotation.class);
Token token = new Token(jCas, begin, end);
token.setPos(pos);
token.setLemma(lemma);
token.addToIndexes();
where Token is a UIMA type, declared as:
<typeSystemDescription>
<name>Token</name>
<types>
<typeDescription>
<name>org.cleartk.token.type.Token</name>
<supertypeName>uima.tcas.Annotation</supertypeName>
<features>
<featureDescription>
<name>pos</name>
<rangeTypeName>uima.cas.String</rangeTypeName>
</featureDescription>
<featureDescription>
<name>lemma</name>
<rangeTypeName>uima.cas.String</rangeTypeName>
</featureDescription>
</features>
</typeDescription>
</types>
</typeSystemDescription>
References
Steven Bethard, Philip Ogren, and Lee Becker. 2014.
ClearTK 2.0: Design patterns for machine learning
in UIMA. In LREC 2014.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media.
Joshua Bloch. 2008. Effective Java. Addison Wesley,
Upper Saddle River, NJ, 2nd edition.
Angel X. Chang and Christopher D. Manning. 2012.
SUTIME: A library for recognizing and normalizing
time expressions. In LREC 2012.
James Clarke, Vivek Srikumar, Mark Sammons, and
Dan Roth. 2012. An NLP Curator (or: How I
learned to stop worrying and love NLP pipelines).
In LREC 2012.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
an architecture for development of robust HLT
applications. In ACL 2002.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC 2006, pages 449?454.
David Ferrucci and Adam Lally. 2004. UIMA: an
architectural approach to unstructured information
processing in the corporate research environment.
Natural Language Engineering, 10:327?348.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In ACL 43, pages 363?370.
I. Gurevych, M. M?uhlh?auser, C. M?uller, J. Steimle,
M. Weimer, and T. Zesch. 2007. Darmstadt knowl-
edge processing repository based on UIMA. In
First Workshop on Unstructured Information Man-
agement Architecture at GLDV 2007, T?ubingen.
U. Hahn, E. Buyko, R. Landefeld, M. M?uhlhausen,
Poprat M, K. Tomanek, and J. Wermter. 2008. An
overview of JCoRe, the Julie lab UIMA component
registry. In LREC 2008.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Suzanna Becker, Sebastian
Thrun, and Klaus Obermayer, editors, Advances in
Neural Information Processing Systems, volume 15,
pages 3?10. MIT Press.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, 39(4).
Anthony Patricio. 2009. Why this project is success-
ful? https://community.jboss.org/wiki/
WhyThisProjectIsSuccessful.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In EMNLP 2013, pages 1631?1642.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In NAACL 3, pages 252?259.
60
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 121?131,
Dublin, Ireland, August 23-24 2014.
Extracting Latent Attributes from Video Scenes Using Text as
Background Knowledge
Anh Tran, Mihai Surdeanu, Paul Cohen
University of Arizona
{trananh, msurdeanu, prcohen}@email.arizona.edu
Abstract
We explore the novel task of identify-
ing latent attributes in video scenes, such
as the mental states of actors, using
only large text collections as background
knowledge and minimal information about
the videos, such as activity and actor types.
We formalize the task and a measure of
merit that accounts for the semantic re-
latedness of mental state terms. We de-
velop and test several largely unsupervised
information extraction models that iden-
tify the mental states of human partici-
pants in video scenes. We show that these
models produce complementary informa-
tion and their combination significantly
outperforms the individual models as well
as other baseline methods.
1 Introduction
?Labeling a narrowly avoided vehicular
manslaughter as approach(car, person) is
missing something.?
1
The recognition of ac-
tivities, participants, and objects in videos has
advanced considerably in recent years (Li et al.,
2010; Poppe, 2010; Weinland et al., 2011; Yang
and Ramanan, 2011; Ng et al., 2012). However,
identifying latent attributes of scenes, such as the
mental states of human participants, has not been
addressed. Latent attributes matter: If a video
surveillance system detects one person chasing
another, the response from law enforcement
should be radically different if the people are
happy (e.g., children playing) or afraid and angry
(e.g., a person running from an assailant).
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
1
James Donlon, former manager of DARPA?s Mind?s Eye
program, personal communication.
Attributes that are latent in visual representa-
tions are often explicit in textual representations.
This suggests a novel method for inferring latent
attributes: Use explicit features of videos to query
text corpora, and from the resulting texts extract
attributes that are latent in the videos, such as men-
tal states. The contributions of this work are:
1: We formalize the novel task of latent attribute
identification from video scenes, focusing on the
identification of actors? mental states. The input
for the task is contextual information about the
scene, such as detections about the activity (e.g.,
chase) and actor types (e.g., policeman or child),
and the output is a distribution over mental state
labels. We show that gold standard annotations
for this task can be reliably generated using crowd
sourcing. We define a novel evaluation measure,
called constrained weighted similarity-aligned F
1
score, that accounts for both the differences be-
tween mental state distributions and the seman-
tic relatedness of mental state terms (e.g., partial
credit is given for irate when the target is angry).
2: We propose several robust and largely unsuper-
vised information extraction (IE) models for iden-
tifying the mental state labels of human partici-
pants in a scene, given solely the activity and actor
types: a lexical semantic (LS) model that extracts
mental state labels that are highly similar to the
context of the scene in a latent, conceptual vector
space; and an information retrieval (IR) model that
identifies labels commonly appearing in sentences
related to the explicit scene context. We show that
these models are complementary and their combi-
nation performs better than either model, alone.
3: Furthermore, we show that an event-centric
model that focuses on the mental state labels of
the participants in the relevant event (identified us-
ing syntactic patterns and coreference resolution)
outperforms the above shallower models.
121
2 Related Work
As far as we know, the task proposed here is novel.
We can, however, review work relevant to each
part of the problem and our solution. Mental
state inference is often formulated as a classifica-
tion problem, where the goal is to predict target
mental state labels based on low-level sensory in-
put data. Most solutions try to learn classification
models based on large amounts of training data,
while some require human engineering of domain
knowledge. Hidden Markov Models (HMMs) and
Dynamic Bayesian Networks (DBNs) are popular
representations because they can model the tem-
poral evolution of mental states. For instance, the
mental states of students can be inferred from un-
intentional body gestures using a DBN (Abbasi et
al., 2009). Likewise, an HMM can also be used
to model the emotional states of humans (Liu and
Wang, 2011). Some solutions combine HMMs
and DBNs in a Bayesian inference framework to
yield a multi-layer representation that can do real-
time inference of complex mental and emotional
states (El Kaliouby and Robinson, 2004; Baltru-
saitis et al., 2011). Our work differs from these
approaches in several ways: It is mostly unsuper-
vised, multi-modal, and requires little training.
Relevant video processing technology includes
object detection (e.g., (Felzenszwalb et al., 2008)),
person detection, and pose detection (e.g., (Yang
and Ramanan, 2011)). Many tracking algo-
rithms have been developed, such as group track-
ing (McKenna et al., 2000), tracking by learn-
ing appearances (Ramanan et al., 2007), and
tracking in 3D space (Giebel et al., 2004; Brau
et al., 2013). For human action recognition,
current state-of-the-art techniques are capable of
achieving near perfect performance on the com-
monly used KTH Actions dataset (Schuldt et al.,
2004) and high performance rates on other more
challenging datasets (O?Hara and Draper, 2012;
Sadanand and Corso, 2012).
To extract mental state information from texts,
one might use any or all of the technologies of
natural language processing, so a complete review
of relevant technologies is impossible, here. Of
immediate relevance is the work of de Marneffe
et al. (2010), which identified the latent meaning
behind scalar adjectives (e.g., which ages people
have in mind when talking about ?little kids?).
The authors learned these meanings by extract-
ing scalars, such as children?s ages, that were
commonly collocated with phrases, such as ?lit-
tle kids,? in web documents. Mohtarami et al.
(2011) tried to infer yes/no answers from indirect
yes/no question-answer pairs (IQAPs) by predict-
ing the uncertainty of sentiment adjectives in in-
direct answers. Their method employs antonyms,
synonyms, word sense disambiguation as well as
the semantic association between the sentiment
adjectives that appear in the IQAP to assign a de-
gree of certainty to each answer. Sokolova and La-
palme (2011) further showed how to learn a model
for predicting the opinions of users based on their
written contents, such as reviews and product de-
scriptions, on the Web. Gabbard et al. (2011)
found that coreference resolution can significantly
improve the recall rate of relations extraction with-
out much expense to the precision rate.
Our work builds on these efforts by combining
information retrieval, lexical semantics, and event
extraction to extract latent scene attributes.
3 Data
For the experiments in this paper, we focus solely
on videos containing chase scenes. Chases often
invoke clear mental state inferences, and depend-
ing on context can suggest very different mental
state distributions for the actors involved.
3.1 Video Corpus
We compiled a video dataset of 26 chase videos
found on the Web. Of these, five involve police
officers, seven involve children, four show sports-
related scenes, and twelve describe different chase
scenarios involving civilian adults (two videos in-
volve children playing sports). The average dura-
tion of the dataset is 8.8 seconds with a range of
[4, 18]. Most videos involve a single chaser and a
single chasee (a person being chased) while a few
have several chasers and/or chasees.
For each video, we used Amazon Mechanical
Turk (MTurk) to identify both the actors and their
mental states. Each worker was asked to view a
video in its entirety before answering some ques-
tions about the scene. We give no prior training to
the workers. The questions were carefully phrased
to apply to all participants of a particular role, for
example all chasers (if there are more than one).
We also ask obvious validation questions about the
participants in each role (e.g., are the chasers run-
ning towards the camera?) and use the answers to
these questions to filter out poor responses. In gen-
122
eral, we found that most responses were good and
only a few incomplete submissions were rejected.
In the first experiment, we asked MTurk work-
ers to select the actor types and various other de-
tections from a predefined list of tags. This label-
ing task is a proxy for a computer vision detection
system that functions at a human level of perfor-
mance. Indeed, we restricted the actor type labels
to a set that can be reasonably expected from auto-
matic detection algorithms: person, police officer,
child, and (non-human) object. For instance, po-
lice officers often wear distinctive color uniforms
that can be learned using the Felzenszwalb detec-
tor (Felzenszwalb et al., 2008), whereas children
can be reliably differentiated by their heights un-
der a 3D-tracking model (Brau et al., 2013). Each
video was annotated by three different workers
and the union of their annotations is produced.
The overall accuracy of the annotation was excel-
lent. The MTurk workers correctly identified the
important actors in every video.
Next, we collected a gold standard list of mental
state labels for each video by asking MTurk work-
ers to identify all applicable mental state adjec-
tives for the actors involved. We used a text-box
to allow for free-form input. Studies have shown
that people of different cultures can perceive emo-
tions very differently, and having forced choice
options cannot always capture their true percep-
tion (Gendron et al., 2014). Therefore, we did not
restrict the response of the workers in any way.
Workers could abstain from answering if they felt
the video was too ambiguous. Each video was
evaluated by ten different workers. We converted
each term provided to the closest adjective form
if possible. Terms with no equivalent adjective
forms were left in place. On rare occasions, work-
ers provided sentence descriptions despite being
asked for single-word adjectives. These sentences
were either removed, or collapsed into a single
word if appropriate. The overall quality of the an-
notations was good and generally followed com-
mon intuition. Asides from the frequently used
terms, we also received some colorful (yet infor-
mative) descriptions, like incredulous and vindic-
tive. In general, chases involving police scenar-
ios often contained violent and angry states while
chases involving children received more cheerful
labels. There were unexpected descriptions, such
as annoy for a playful chase between two children.
Upon review of the video, we agreed that one child
did indeed look annoyed. Thus, the resulting de-
scriptions were subjective, but very few were hard
to rationalize. By aggregating the answers from
the workers, we generated a gold standard distri-
bution of mental state terms for each video.
2
3.2 Text Corpus
The text corpus used for our models is the En-
glish Gigaword 5th Edition corpus
3
, made avail-
able by the Linguistics Data Consortium and in-
dexed by Lucene
4
. It is a comprehensive archive
of newswire text data (approximately 26 GB), ac-
quired over several years. It is in this corpus that
we expect to find mental state terms cued by con-
textual information from videos.
4 Neighborhood Models
We developed several individual models based on
the neighborhood paradigm, that is, the hypoth-
esis that relevant mental state labels will appear
?near? text cued by the visual features of a scene.
The models take as input the context extracted
from a video scene, defined simply as a list of ?ac-
tivity and actor-type? tuples (e.g., (chase, police)).
Multiple actor types will result in multiple tuples
for a video. The actors can be either a person, a
policeman, a child, or a (non-human) object. If
the detections describe the actor as both a person
and a child, or a person and a policeman, we auto-
matically remove the person label as it is a Word-
Net (Miller, 1995) hypernym of both child and po-
liceman. For each human actor type, we further
increase our coverage by retrieving the synonym
set (synset) of its most frequent sense (i.e., sense
#1) from WordNet. For example, a chase involv-
ing a policeman would generate the following tu-
ples: (chase, policeman) and (chase, officer).
We call these query tuples because they are used
to query text for sentences that ? if all goes well ?
will contain relevant mental state labels.
Given query tuples, our models use an initial
seed set of 160 mental state adjectives to produce
a single distribution over mental state labels, re-
ferred to as the response distribution, for each
video. The seed set is compiled from popular
mental and emotional state dictionaries, includ-
ing the Profile of Mood States (POMS) (McNair
et al., 1971) and Plutchik?s wheel of emotion. We
2
All videos and annotations are available at:
http://trananh.github.io/vlsa
3
Linguistics Data Consortium catalog no. LDC2011T07
4
Apache Lucene: http://lucene.apache.org
123
Source Example Mental State Labels
POMS
alert, annoyed, energetic, exhausted, helpful,
sad, terrified, unworthy, weary, etc.
Plutchik
angry, disgusted, fearful, joyful/joyous,
sad, surprised, trusting, etc.
Others
agitated, competitive, cynical, disappointed,
excited, giddy, happy, inebriated, violent, etc.
Table 1: The initial seed set contains 160 mental
state labels, compiled from different sources like
the popular Profile of Mood States dictionary and
Plutchik?s wheel of emotion.
also included frequently used labels gathered from
synsets found in WordNet (see Table 1 for exam-
ples). Note that the gold standard annotations pro-
duced by MTurk workers (Sec. 3) was not a source
for this set, nor was it restricted to these terms.
4.1 Back-off Interpolation in Vector Space
Our first model uses the recurrent neural net-
work language model (RNNLM) of Mikolov et
al. (2013) to project both mental state labels and
query tuples into a latent conceptual space. Simi-
larity is then trivially computed as the cosine sim-
ilarity between these vectors. In all of our experi-
ments, we used a RNNLM computed over the Gi-
gaword corpus with 600-dimensional vectors.
For this vector space (vec) model, we separate
the query tuples into different levels of back-off
context. The first level includes the set of activ-
ity types as singleton context tuples, e.g., (chase),
while the second level includes all (activity, actor)
context tuples. Hence, each query tuple will yield
two different context tuples, one for each back-off
level. For each context tuple with multiple terms,
such as (chase, policeman), we find the vector rep-
resentation for the context by aggregating the vec-
tors representing the search terms:
vec(chase, policeman) = vec(chase) +
vec(policeman) .
The vector representation for a singleton con-
text tuple is just the vector of the single search
term. We then calculate the distance of each men-
tal state labelm to the normalized vector represen-
tation of the context tuple by computing the cosine
similarity score between the two vectors:
cos(?
m
) =
vec(m) ? vec(context tuple)
||vec(m)|| ||vec(context tuple)||
.
The hypothesis here is that mental state labels
that are related to the search context will have a
RNNLM vector that is closer to the context tuple
vector, resulting in a high cosine similarity score.
Because the number of latent dimensions is rela-
tively small (when compared to vocabulary size),
cosine similarity scores in this latent space tend to
be close. To further separate these scores, we raise
them to an exponential power:
score(m) = e
cos(?
m
)+1
? 1 .
The processing of each context tuple yields 160
different scores, one for each mental state label.
We normalize these scores to form a single distri-
bution of scores for each context tuple. The distri-
butions are then integrated into a single distribu-
tion representative of the complete activity as fol-
lows: (a) the distributions at each context back-off
level are averaged to generate a single distribution
per level ? for the second level (which includes
activity and actor types), it means distributions for
all (activity, actor) tuples are averaged, whereas
the first level only has a single distribution from
the singleton activity tuple (chase); and (b) distri-
butions for the different levels are linearly interpo-
lated, similar to the back-off strategy of (Collins,
1997). Let e
1
and e
2
represent the weights of some
mental state label m from the average distribution
at the first and second level, respectively. Then the
interpolated distribution score e for m is:
e = ?e
1
+ (1? ?)e
2
.
Compiling the distribution scores for each m
produces the final distribution representing the ac-
tivity modeled. We prune this final distribution by
taking the top ranked items that make up some ?
proportion of the distribution. We delay the dis-
cussion of how ? is tuned to Section 6. The final
pruned distribution is normalized to produce the
response distribution.
4.2 Sentence Co-occurrence with Deleted
Interpolation
Our second model, the sent model, extracts mental
state labels based on the likelihood that they ap-
pear in sentences cued by query tuples. For each
tuple, we estimate the conditional probability that
we will see a mental state label m in a sentence,
where m is from the seed set, given that we al-
ready observed the desired activity and actor type
in the same sentence: P (m|activity, actor). In this
case, we refer to the sentence length as the neigh-
borhood window. Furthermore, all terms must ap-
pear as the correct part-of-speech (POS): m must
124
appear as an adjective or verb, the activity as a
verb, and the actor as a noun. (Mental state adjec-
tives are allowed to appear as verbs because some
are often mis-tagged as verbs; e.g., agitated, deter-
mined, welcoming.) We used Stanford?s CoreNLP
toolkit for tokenization and POS tagging.
5
Note that this probability is similar to a trigram
probability in POS tagging, except the triples need
not form an ordered sequence but must appear in
the same sentence and under the correct POS tag.
Unfortunately, we cannot always compute this tri-
gram probability directly from the corpus because
there might be too few instances of each trigram
to compute a probability reliably. As is common,
we instead estimate it as a linear interpolation of
unigrams, bigrams, and trigrams. We define the
maximum likelihood probabilities
?
P , derived from
relative frequencies f , for the unigrams, bigrams,
and trigrams as follows:
?
P (m) =
f(m)
N
?
P (m|activity) =
f(m, activity)
f(activity)
?
P (m|activity, actor) =
f(m, activity, actor)
f(activity, actor)
for all mental state labels m, activities, and actor
types in our queries. N is the total number of to-
kens in the corpus. The aforementioned POS re-
quirement is enforced: f(m) is the number of oc-
currences of m as an adjective or verb. We define
?
P = 0 if the corresponding numerator and denom-
inator are zero. The desired trigram probability is
then estimated as:
P (m|activity, actor) = ?
1
?
P (m) +
?
2
?
P (m|activity) + ?
3
?
P (m|activity, actor) .
As ?
1
+?
2
+?
3
= 1, P represents a probability
distribution. We use the deleted interpolation algo-
rithm (Brants, 2000) to estimate one set of lambda
values for the model, based on all trigrams.
For each query tuple generated in a video, 160
different trigrams are computed, one for each men-
tal state label in the seed set, resulting in 160 con-
ditional probability scores. We normalize these
scores into a single distribution ? the mental state
distribution for that query tuple. We then combine
5
http://nlp.stanford.edu/software/
corenlp.shtml.
all resulting distributions, one from each query tu-
ple, and take the average to produce a single dis-
tribution over mental state labels for the video. As
before, we prune this distribution by taking the
top-ranked items that cover a large fraction ? of
total probability. The pruned distribution is renor-
malized to yield the final response distribution.
4.3 Event-centric with Deleted Interpolation
The sent model has two limitations. On one hand,
it is too sparse: the single sentence neighborhood
window is too small to reliably estimate the fre-
quencies of trigrams for the probabilities of men-
tal state terms. On the other hand, it may be too
lenient, as it extracts all mental state mentions ap-
pearing in the same sentence with the activity, or
event, under consideration, regardless if they ap-
ply to this event or not. We address these limita-
tions next with an event-centric model (event).
Intuitively, the event model focuses on the men-
tal state labels of event participants. Formally,
these mental state terms are extracted as follows:
1: We identify event participants (or actors). We
do this by analyzing the syntactic dependencies of
sentences containing the target verb (e.g., chase)
to find the subject and object. In most cases, the
nominal subject of the verb chase is the chaser and
the direct object is the person being chased. We
implemented additional patterns to model passive
voice and other exceptions. We used Stanford?s
CoreNLP toolkit for syntactic dependency parsing
and the downstream coreference resolution.
2: Once the phrases that point to actors are iden-
tified, we identify all mentions of these actors in
the entire document by traversing the coreference
chains containing the phrases extracted in the pre-
vious step. The sentences traversed in the chains
define the neighborhood area for this model.
3: Lastly, we identify the mental state terms of
event participants using a second set of syntac-
tic patterns. First, we inspect several copulative
verbs, such as to be and feel, and extract men-
tal state labels from these structures if the corre-
sponding subject is one of the mentions detected
above. Second, we search for mental states along
adjectival modifier relations, where the head is an
actor mention. For all patterns, we make sure to
filter for only mental state complements belong-
ing to the initial seed list. The same POS restric-
tion as in the other models also applies. We incre-
ment the joint frequency f for the n-gram once for
125
each neighborhood that properly contain all search
terms from the n-gram in the correct POS.
The event model addresses both limitations of
the sent model: it avoids the lenient extraction of
mental state labels by focusing on labels associ-
ated with event participants; it addresses sparsity
by considering all mentions of event participants
in a document.
To understand the impact of this model, we
compare it against two additional baselines. The
first baseline investigates the importance of focus-
ing on mental state terms associated with event
participants. This model, called coref, implements
the first two steps of the above algorithm, but in-
stead of extracting only mental state terms associ-
ated with event actors (last step), it considers all
mentions appearing anywhere in the coreference
neighborhood. That is, all unique sentences tra-
versed by the relevant coreference chains are first
pieced together to define a single neighborhood for
a given document; then the relative joint frequen-
cies of n-grams are computed by incrementing f
once for each neighborhood that contains all terms
with correct POS tags.
The second baseline analyzes the importance of
coreference resolution to our problem. This model
is similar to sent, with the modification that it in-
creases the size of the neighborhood window to in-
clude the immediate neighbors of target sentences
that contain activity labels. We call this the win-n
model: The window around a target verb contains
2n + 1 sentences. We build the context neigh-
borhood by concatenating all target sentences and
their windows together for a given document. This
defines a single neighborhood for each document.
This contrasts with the sent model, in which the
neighborhood is defined for each sentence con-
taining the activity label in the document, resulting
in several possible neighborhoods in a document.
The joint frequency f for each n-gram ? where
n > 1 ? is computed similarly with the coref
model: it is incremented once for each neighbor-
hood that contains all the terms from the n-gram
in the correct POS. Frequencies for unigrams are
computed similar to sent.
As before, 160 different trigrams are generated
for each query tuple, one for each mental state la-
bel in the seed set, resulting in 160 conditional
probability scores. We similarly combine these
scores and generate a single pruned distribution as
the response for each of the model above.
G (irate, 0.8), (afraid, 0.2)
R
1
(angry, 0.6), (mad, 0.4)
R
2
(irate, 0.2), (afraid, 0.8)
R
3
(mad, 0.4), (irate, 0.4), (scared, 0.2)
Table 2: We show an example gold standard dis-
tribution G and several candidate response distri-
butions to be matched against G. Here, R
3
best
matches the shape and meaning of G, because
(irate, mad) and (afraid, scared) are close syn-
onyms. R
2
appears to match G semantically, but
matches its shape poorly. R
1
misses one of the
mental state labels, afraid, but contains labels that
are semantically close to the weightiest term in G.
4.4 Ensemble Model
We combined the results from the event and
vec models to produce an ensemble model (ens)
which, for a mental state label m, returns the aver-
age of m?s scores according to the response distri-
butions of the two individual models.
5 Evaluation Measures
LetR denote the response distribution over mental
state labels produced for a single video by one of
the models described in the previous section, and
let G denote the gold standard distribution pro-
duced for the same video by MTurk workers. If
R is similar to G then our models produce simi-
lar mental state terms as the workers. There are
many ways to compare distributions (e.g., KL dis-
tance, chi-square statistics) but these give bad re-
sults when distributions are sparse. More impor-
tantly, for our purposes, the measures that compare
the shapes of distributions do not allow semantic
comparisons at the level of distribution elements.
Suppose R assigns high scores to angry and mad,
only, while G assigns a high score to happy, only.
Clearly, R is wrong. But if insteadG had assigned
a high score to irate, only, then R would be more
right than wrong because, at the level of the indi-
vidual elements, angry and mad are similar to irate
but not similar to happy.
We describe a series of measures, starting with
the familiar F
1
score, and discuss their applicabil-
ity. To illustrate the effectiveness of each measure,
we will use the examples shown in Table 2.
5.1 F
1
Score
The F
1
score measures the similarity between two
sets of elements, R and G. F
1
= 1 when R = G
126
and F
1
= 0 when R and G share no elements. F
1
is the harmonic mean of precision and recall:
precision =
|R ?G|
|R|
, recall =
|R ?G|
|G|
,
(1)
F
1
= 2 ?
precision ? recall
precision+ recall
. (2)
The F
1
score penalizes the responses in Table 3
that include semantically similar labels to those in
G, and fails to reflect the weights of the labels in
G and R.
5.2 Similarity-Aligned F
1
Score
Although the standard F
1
does not immediately fit
our needs, it is a good starting point. We can in-
corporate the semantic similarity of distribution el-
ements by generalizing the formulas for precision
and recall as follows:
precision =
1
|R|
?
r?R
max
g?G
?(r, g) ,
recall =
1
|G|
?
g?G
max
r?R
?(r, g) ,
(3)
where ? ? [0, 1] is a function that yields the simi-
larity between two elements. The standard F
1
has:
?(r, g) =
{
1 , if r = g
0 , otherwise
,
but clearly ? can be defined to take values pro-
portional to the similarity of r and g. We can
choose from a wide range of semantic similarity
and relatedness measures that are based on Word-
Net (Pedersen et al., 2004). The recent RNNLM
of Mikolov opens the door to even more similar-
ity measures based on vector space representations
of words (Mikolov et al., 2013). After experi-
mentations, we settled on one proposed by Hirst
and St-Onge (1998). It represents two lexicalized
concepts as semantically close if their WordNet
synsets are connected by a path that is not too
long and that ?does not change direction too of-
ten? (Hirst and St-Onge, 1998). We chose this
metric because it has a finite range, accommodates
numerous POS pairs, and works well in practice.
Given the generalized precision and recall for-
mulas in Eq 3, our similarity-aligned (SA) F
1
score can be computed in the usual way, as the
harmonic mean of precision and recall (Eq 2).
SA-F
1
is inspired by the Constrained Entity-
Aligned F-Measure (CEAF) metric proposed
F
1
SA-F
1
CWSA-F
1
p r f
1
p r f
1
p r f
1
R
1
0 0 0 1 .5
2
3
1 .8 .89
R
2
1 1 1 1 1 1 .4 .4 .4
R
3
1
3
.5 .4 1 1 1 1 1 1
Table 3: The precision (p), recall (r), and F
1
(f
1
) scores under various evaluation models are
presented for the examples from Table 2. Sup-
pose that ?(irate, angry) = ?(irate,mad) =
?(afraid, scared) = 1, with ? of any two identi-
cal strings being 1, and ? of all other pairs are 0.
by (Luo, 2005) for coreference resolution. CEAF
computes an optimal one-to-one mapping between
subsets of reference and system entities before it
computes recall, precision and F. Similarly, SA-F
1
finds optimal mappings between the labels of the
two sets based on ? (this is what the max terms in
Eq 3 do). Table 3 shows that SA-F
1
correctly re-
wards the use of synonyms. The high scores given
to R
2
, however, indicate that it does not measure
the similarity between distribution shapes.
5.3 Constrained Weighted Similarity-Aligned
F
1
Score
Let R(r) and G(r) be the probabilities of label
r in the R and G distributions, respectively. Let
?
?
S
(`) denote the best similarity score achievable
when comparing elements from set S to ` us-
ing the similarity function ?. That is, ?
?
S
(`) =
max
e?S
?(`, e). We can easily weight ?
?
S
(`) by
the probability of `. For example, we might re-
define precision as
?
r?R
R(r) ??
?
G
(r). However,
this would not account for the probability of r in
the gold standard distribution, G.
An analogy might help here: Suppose we have
an unknown ?mystery bag? of 100 colored pen-
cils that we will try to match with a ?response
bag? of pencils. If we fill our response bag with
100 crimson pencils, while the mystery bag con-
tains only 25 crimson pencils, then our precision
score should get points only for the first 25 pen-
cils, while the remaining 75 in the response bag
should not be rewarded. For recall, the reward
given for each color in the mystery bag is capped
by the number of pencils of that color in the re-
sponse bag. The analogy is complete when we
consider that crimson pencils should perhaps be
partially rewarded when matched by cardinal, rose
or cerise pencils. In other words, a similarity mea-
127
sure should account for an accumulated mass of
synonyms. Let M
S
(`) denote the subset of terms
from S that have the best similarity score to `:
M
S
(`) = {e | ?(`, e) = ?
?
S
(`), ?e ? S} .
We define new forms of precision and recall as:
p =
?
r?R
min
?
?
R(r),
?
e?M
G
(r)
G(e)
?
?
?
?
G
(r) ,
r =
?
g?G
min
?
?
G(g),
?
e?M
R
(g)
R(e)
?
?
?
?
R
(g) .
(4)
The resulting constrained weighted similarity-
aligned (CWSA) F
1
score is the harmonic mean
of these new precision and recall scores. Table 3
shows that CWSA-F
1
yields the most intuitive
evaluation of the response distributions, down-
weighting R
2
in favor of R
3
and R
1
.
6 Experimental Procedure
As described in Section 3, MTurk workers anno-
tated 26 videos by identifying the actor types and
mental state labels for each video. The actor types
become query tuples of the form (activity, actor)
and the mental state labels are compiled into one
probability distribution over labels for each video,
designated G. The query tuples were provided to
our neighborhood models (Sec. 4), which returned
a response distribution over mental state labels for
each video, designated R.
We selected four videos of the 26 to calibrate
the prune parameters ? and the interpolation pa-
rameters ? (Sec. 4). One of these videos contains
children, one has police involvement, and two con-
tain adults. We asked additional MTurk workers to
annotate these videos, yielding an independent set
of annotations to be used solely for calibration.
The experimental question is, how well does G
match R for each video?
7 Results & Discussions
We report the average performance of our mod-
els along with two additional baseline methods in
Table 4. The na??ve baseline method unif simply
binds R to the initial seed set of 160 mental state
labels with uniform probability, while the stronger
freq baseline uses the occurrence frequency dis-
tribution of the labels from the Gigaword corpus
(note that only occurrences tagged as adjectives or
F
1
CWSA-F
1
p r f
1
p r f
1
unif .107 .750 .187 .284 .289 .286
freq .107 .750 .187 .362 .352 .355
sent .194 .293 .227 .366 .376 .368
vec .226 .145 .175 .399 .392 .393
coref .264 .251 .253 .382 .461 .416
event .231 .303 .256 .446 .488 .463
ens .259 .296 .274 .488 .517 .500
Table 4: The average evaluation performance
across 26 different chase videos are shown against
2 different baselines for all proposed models. Bold
font indicates the best score in a given column.
verbs were counted). All average improvements
of the ensemble model over the baseline models
are significant (p < 0.01). Significance tests were
one-tailed and were based on nonparametric boot-
strap resampling with 10, 000 iterations.
Using the classical F
1
measure, the coref model
scored highest on precision, while the ensemble
method did best on F
1
. Not surprisingly, no model
can top the baseline methods on recall as both
baselines use the entire seed set of 160 terms.
Even so, the average recall for the baselines were
only .750, which means that the initial seed set did
not include words that were used by the MTurk an-
notators. As we?ve mentioned, the classical F
1
is
misleading because it does not credit synonyms.
For example, in one movie, one of our models
was rewarded once for matching the label angry
and penalized six times for also reporting irate,
enraged, raging, upset, furious, and mad. Fre-
quently, our models were penalized for using the
terms scared and afraid instead of fearful.
Under the CWSA-F
1
evaluation measure,
which correctly accounts for both synonyms and
label probabilities, our ensemble model performed
best. The average CWSA-F
1
score of the ensem-
ble model improves upon the simple uniform base-
line unif by almost 75%, and over the stronger
freq baseline by over 40%. The ensemble method
also outperforms each individual method in all
measured scores. These improvements were also
found to be significant. This strongly suggests
that the vec and event models are complementary,
and not entirely redundant. Furthermore, Table 4
shows that the event model performs considerably
better than coref. This result emphasizes the im-
portance of focusing on the mental state labels of
event participants rather than considering all men-
tal state terms collocated in the same sentence with
an actor or action verb.
128
Models CWSA-F1 Versus coref p-value
win-0 0.388682 ?0.027512 0.0067
win-1 0.415328 ?0.000866 0.4629
win-2 0.399777 ?0.016417 0.0311
win-3 0.392832 ?0.023362 0.0029
Table 5: The average CWSA-F
1
scores for the
win-n model with different window parameters are
shown in comparison to the coref model. The
coref model outperformed all tested configura-
tions, though the difference is not significant for
n = 1. The p-value based on the average differ-
ences were obtained using one-tailed nonparamet-
ric bootstrap resampling with 10, 000 iterations.
Table 5 explores the effectiveness of corefer-
ence resolution in expanding the neighborhood
area. The coref model outperformed the simple
windowing method under every tested configura-
tion. However, the improvement over windowing
with n = 1 is not significant. This can be ex-
plained by fact that immediately neighboring sen-
tences are more likely to be related. Moreover,
since newswire articles tend to be short, the neigh-
borhoods generated by win-1 tend to be similar to
those generated by coref. In general, coref does
not do worse than a simple windowing method and
has the bonus advantage of providing references to
the actors of interest for downstream processes.
In Table 6, we show the performance results
based on the types of chase scenarios happening in
the videos. The average scores under the uniform
baseline unif for chase videos involving children
and sporting events are lower than for police and
other chases. This suggests that our seed set of
160 mental state labels is biased towards the latter
types of events, and is not as fit to describe chases
involving children.
On average, videos involving police officers
show the biggest improvement in the CWSA-F
1
scores over the unif baseline (+0.2693), whereas
videos involving children received the lowest gain
(+0.1517). We believe this is the effect of the
Gigaword text corpus, which is a comprehensive
archive of newswire text, and thus is heavily bi-
ased towards high-speed and violent chases in-
volving the police. The Gigaword corpus is not
the place to find children happily chasing each
other. Similarly, sports-related chases, which are
also news-worthy, have a higher gain than chil-
dren?s videos on average.
Categories Unif Ensemble Gain
children 0.2082 0.3599 +0.1517
police 0.3313 0.6006 +0.2693
sports 0.2318 0.4126 +0.1808
others 0.3157 0.5457 +0.2300
Table 6: The average CWSA-F
1
scores for the en-
semble model are shown in comparison to the uni-
form baseline method, categorize by video types.
8 Conclusion and Future Work
We introduced the novel task of identifying latent
attributes in video scenes, specifically the men-
tal states of actors in chase scenes. We showed
that these attributes can be identified by using ex-
plicit features of videos to query text corpora, and
from the resulting texts extract attributes that are
latent in the videos. We presented several largely
unsupervised methods for identifying distributions
of actors? mental states in video scenes. We de-
fined a similarity measure, CWSA-F
1
, for com-
paring distributions of mental state labels that ac-
counts for both semantic relatedness of the labels
and their probabilities in the corresponding distri-
butions. We showed that very little information
from videos is needed to produce good results that
significantly outperform baseline methods.
In the future, we plan to add more detection
types. Additional contextual information from
videos (e.g., scene locations) should help improve
performance, especially on tougher videos (e.g.,
videos involving children chases). Moreover, we
believe that the initial seed set of mental state la-
bels can be learned simultaneously with the ex-
traction patterns of the event model using a mutual
bootstrapping method, similar to that of (Riloff
and Jones, 1999).
Currently, our experiments assume one distri-
bution of mental state labels for each video. They
do not distinguish between the mental states of the
chaser and chasee, while in reality these partici-
pants may be in very different states of mind. Our
event model is capable of making this distinction
and we will test its performance on this task in the
future. We also plan to test the effectiveness of our
models with actual computer vision detectors. As
a first approximation, we will simulate the noisy
nature of detectors by degrading the quality of an-
notated data. Using artificial noise on ground-truth
data, we can simulate the performance of real de-
tectors and test the robustness of our models.
129
References
Abdul Rehman Abbasi, Matthew N. Dailey, Nitin V.
Afzulpurkar, and Takeaki Uno. 2009. Student men-
tal state inference from unintentional body gestures
using dynamic Bayesian networks. Journal on Mul-
timodal User Interfaces, 3(1-2):21?31, December.
Tadas Baltrusaitis, Daniel McDuff, Ntombikayise
Banda, Marwa Mahmoud, Rana el Kaliouby, Peter
Robinson, and Rosalind Picard. 2011. Real-time
inference of mental states from facial expressions
and upper body gestures. In Face and Gesture 2011,
pages 909?914. IEEE, March.
Thorsten Brants. 2000. TnT: A statistical part-of-
speech tagger. In Proceedings of the sixth confer-
ence on Applied natural language processing, pages
224?231, Morristown, NJ, USA. Association for
Computational Linguistics.
Ernesto Brau, Jinyan Guan, Kyle Simek, Luca Del
Pero, Colin Reimer Dawson, and Kobus Barnard.
2013. Bayesian 3D Tracking from monocular video.
In The IEEE International Conference on Computer
Vision (ICCV), December.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th annual meeting on Association for Computa-
tional Linguistics -, pages 16?23, Morristown, NJ,
USA. Association for Computational Linguistics.
R. El Kaliouby and P. Robinson. 2004. Real-Time In-
ference of Complex Mental States from Facial Ex-
pressions and Head Gestures. In 2004 Conference
on Computer Vision and Pattern Recognition Work-
shop, pages 154?154. IEEE.
Pedro Felzenszwalb, David McAllester, and Deva Ra-
manan. 2008. A discriminatively trained, multi-
scale, deformable part model. In 2008 IEEE Confer-
ence on Computer Vision and Pattern Recognition,
pages 1?8. IEEE, June.
Ryan Gabbard, Marjorie Freedman, and
RM Weischedel. 2011. Coreference for learn-
ing to extract relations: yes, Virginia, coreference
matters. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies: short papers -
Volume 2, pages 288?293.
Maria Gendron, Debi Roberson, Jacoba Marieta
van der Vyver, and Lisa Feldman Barrett. 2014.
Cultural relativity in perceiving emotion from vo-
calizations. Psychological science, 25(4):911?20,
April.
J Giebel, DM Gavrila, and C Schn?orr. 2004. A
bayesian framework for multi-cue 3d object track-
ing. In Computer Vision-ECCV 2004, pages 241?
252.
Graeme Hirst and D St-Onge. 1998. Lexical chains as
representations of context for the detection and cor-
rection of malapropisms. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical Database
(Language, Speech, and Communication), pages
305?332. The MIT Press.
LJ Li, Hao Su, L Fei-Fei, and EP Xing. 2010. Ob-
ject bank: A high-level image representation for
scene classification & semantic feature sparsifica-
tion. In Advances in Neural Information Processing
Systems.
Zhilei Liu and Shangfei Wang. 2011. Emotion recog-
nition using hidden Markov models from facial tem-
perature sequence. In ACII?11 Proceedings of the
4th international conference on Affective computing
and intelligent interaction - Volume Part II, pages
240?247.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of the confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing - HLT
?05, pages 25?32, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
MC De Marneffe, CD Manning, and Christopher Potts.
2010. ?Was it good? It was provocative.? Learning
the meaning of scalar adjectives. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics, pages 167?176.
Stephen J. McKenna, Sumer Jabri, Zoran Duric, Azriel
Rosenfeld, and Harry Wechsler. 2000. Tracking
Groups of People. Computer Vision and Image Un-
derstanding, 80(1):42?56, October.
D M McNair, M Lorr, and L F Droppleman. 1971.
Profile of Mood States (POMS).
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781, pages 1?12.
George A. Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):39?41, November.
Mitra Mohtarami, Hadi Amiri, Man Lan, and
Chew Lim Tan. 2011. Predicting the uncertainty
of sentiment adjectives in indirect answers. In Pro-
ceedings of the 20th ACM international conference
on Information and knowledge management - CIKM
?11, page 2485, New York, New York, USA. ACM
Press.
CB Ng, YH Tay, and BM Goi. 2012. Recognizing hu-
man gender in computer vision: a survey. PRICAI
2012: Trends in Artificial Intelligence, 7458:335?
346.
S O?Hara and B. A. Draper. 2012. Scalable action
recognition with a subspace forest. In 2012 IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 1210?1217. IEEE, June.
130
Ted Pedersen, S Patwardhan, and J Michelizzi. 2004.
WordNet::Similarity: measuring the relatedness of
concepts. In Proceedings of the Nineteenth Na-
tional Conference on Artificial Intelligence (AAAI-
04), pages 1024?1025, San Jose, CA.
Ronald Poppe. 2010. A survey on vision-based human
action recognition. Image and Vision Computing,
28(6):976?990, June.
Deva Ramanan, David a Forsyth, and Andrew Zisser-
man. 2007. Tracking people by learning their ap-
pearance. IEEE transactions on pattern analysis
and machine intelligence, 29(1):65?81, January.
E Riloff and R Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of the sixteenth national conference
on Artificial intelligence (AAAI-1999), pages 474?
479.
S. Sadanand and J. J. Corso. 2012. Action bank: A
high-level representation of activity in video. In
2012 IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 1234?1241. IEEE, June.
C Schuldt, I Laptev, and B Caputo. 2004. Recognizing
human actions: a local SVM approach. In Proceed-
ings of the 17th International Conference on Pattern
Recognition, 2004. ICPR 2004., pages 32?36 Vol.3.
IEEE.
M. Sokolova and G. Lapalme. 2011. Learning opin-
ions in user-generated web content. Natural Lan-
guage Engineering, 17(04):541?567, March.
Daniel Weinland, Remi Ronfard, and Edmond Boyer.
2011. A survey of vision-based methods for action
representation, segmentation and recognition. Com-
puter Vision and Image Understanding, 115(2):224?
241, February.
Yi Yang and Deva Ramanan. 2011. Articulated pose
estimation with flexible mixtures-of-parts. In CVPR
2011, pages 1385?1392. IEEE, June.
131
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 159?177
Manchester, August 2008
The CoNLL-2008 Shared Task on
Joint Parsing of Syntactic and Semantic Dependencies
Mihai Surdeanu
?,?
Richard Johansson
?
Adam Meyers
?
Llu??s M
`
arquez
??
Joakim Nivre
??,??
?: Barcelona Media Innovation Center, mihai.surdeanu@barcelonamedia.org
?: Yahoo! Research Barcelona, mihais@yahoo-inc.com
?: Lund University, richard@cs.lth.se
?: New York University, meyers@cs.nyu.edu
??: Technical University of Catalonia, lluism@lsi.upc.edu
??: V?axj?o University, joakim.nivre@vxu.se
??: Uppsala University, joakim.nivre@lingfil.uu.se
Abstract
The Conference on Computational Natu-
ral Language Learning is accompanied ev-
ery year by a shared task whose purpose
is to promote natural language processing
applications and evaluate them in a stan-
dard setting. In 2008 the shared task was
dedicated to the joint parsing of syntactic
and semantic dependencies. This shared
task not only unifies the shared tasks of
the previous four years under a unique
dependency-based formalism, but also ex-
tends them significantly: this year?s syn-
tactic dependencies include more informa-
tion such as named-entity boundaries; the
semantic dependencies model roles of both
verbal and nominal predicates. In this pa-
per, we define the shared task and describe
how the data sets were created. Further-
more, we report and analyze the results and
describe the approaches of the participat-
ing systems.
1 Introduction
In 2004 and 2005 the shared tasks of the Confer-
ence on Computational Natural Language Learn-
ing (CoNLL) were dedicated to semantic role la-
beling (SRL), in a monolingual setting (English).
In 2006 and 2007 the shared tasks were devoted to
the parsing of syntactic dependencies, using cor-
pora from up to 13 languages. The CoNLL-2008
shared task
1
proposes a unified dependency-based
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1
http://www.yr-bcn.es/conll2008
formalism, which models both syntactic depen-
dencies and semantic roles. Using this formalism,
this shared task merges both the task of syntactic
dependency parsing and the task of identifying se-
mantic arguments and labeling them with semantic
roles. Conceptually, the 2008 shared task can be
divided into three subtasks: (i) parsing of syntactic
dependencies, (ii) identification and disambigua-
tion of semantic predicates, and (iii) identification
of arguments and assignment of semantic roles for
each predicate. Several objectives were addressed
in this shared task:
? SRL is performed and evaluated using a
dependency-based representation for both
syntactic and semantic dependencies. While
SRL on top of a dependency treebank has
been addressed before (Hacioglu, 2004),
our approach has several novelties: (i) our
constituent-to-dependency conversion strat-
egy transforms all annotated semantic argu-
ments in PropBank and NomBank not just a
subset; (ii) we address propositions centered
around both verbal (PropBank) and nominal
(NomBank) predicates.
? Based on the observation that a richer set
of syntactic dependencies improves seman-
tic processing (Johansson and Nugues, 2007),
the syntactic dependencies modeled are more
complex than the ones used in the previous
CoNLL shared tasks. For example, we now
include apposition links, dependencies de-
rived from named entity (NE) structures, and
better modeling of long-distance grammatical
relations.
? A practical framework is provided for the
joint learning of syntactic and semantic de-
pendencies.
159
Given the complexity of this shared task, we
limited the evaluation to a monolingual, English-
only setting. The evaluation is separated into two
different challenges: a closed challenge, where
systems have to be trained strictly with informa-
tion contained in the given training corpus, and an
open challenge, where systems can be developed
making use of any kind of external tools and re-
sources. The participants could submit results in
either one or both challenges.
This paper is organized as follows. Section 2
defines the task, including the format of the data,
the evaluation metrics, and the two challenges.
Section 3 introduces the corpora used and our
constituent-to-dependency conversion procedure.
Section 4 summarizes the results of the submit-
ted systems. Section 5 discusses the approaches
implemented by participants. Section 6 analyzes
the results using additional non-official evaluation
measures. Section 7 concludes the paper.
2 Task Definition
In this section we provide the definition of the
shared task, starting with the format of the shared
task data, followed by a description of the eval-
uation metrics used and a discussion of the two
shared task challenges, i.e., closed and open.
2.1 Data Format
The data format used in this shared task was highly
influenced by the formats used in the 2004?2007
shared tasks. The data follows these general rules:
? The files contain sentences separated by a
blank line.
? A sentence consists of one or more tokens and
the information for each token is represented
on a separate line.
? A token consists of at least 11 fields. The
fields are separated by one or more whites-
pace characters (spaces or tabs). Whitespace
characters are not allowed within fields.
Table 1 describes the fields stored for each token
in the closed-track data sets. Columns 1?3 and
5?8 are available at both training and test time.
Column 4, which contains gold-standard part-of-
speech (POS) tags, is not given at test time. The
same holds for columns 9 and above, which con-
tain the syntactic and semantic dependency struc-
tures that the systems should predict.
The PPOS and PPOSS fields were automati-
cally predicted using the SVMTool POS tagger
(Gim?enez, 2004). To predict the tags in the train-
ing set, a 5-fold cross-validation procedure was
used. The LEMMA and SPLIT LEMMA fields
were predicted using the built-in lemmatizer in
WordNet (Fellbaum, 1998) based on the most fre-
quent sense for the form and part-of-speech tag.
Since NomBank uses a sub-word anal-
ysis in some hyphenated words (such as
[finger]
ARG
-[pointing]
PRED
), the data for-
mat represents the parts in hyphenated words as
separate tokens (columns 6?8). However, the
format also represents how the parts originally fit
together before splitting (columns 2?5). Padding
characters (? ?) are used in columns 2?5 to
ensure the same number of rows for all columns
corresponding to one sentence. All syntactic and
semantic dependencies are annotated relative to
the split word forms (columns 6?8).
Table 2 shows the columns available to the sys-
tems participating in the open challenge: named-
entity labels as in the CoNLL-2003 Shared Task
(Tjong Kim San and De Meulder, 2003) and
from the BBN Wall Street Journal Entity Corpus,
2
WordNet supersense tags, and the output of an off-
the-shelf dependency parser (Nivre et al, 2007b).
Columns 1?3 were predicted using the tagger of
Ciaramita and Altun (2006). Because the BBN
corpus shares lexical content with the Penn Tree-
bank, we generated the BBN tags using a 2-fold
cross-validation procedure.
2.2 Evaluation Measures
We separate the evaluation measures into two
groups: (i) official measures, which were used for
the ranking of participating systems, and (ii) addi-
tional unofficial measures, which provide further
insight into the performance of the participating
systems.
2.2.1 Official Evaluation Measures
The official evaluation measures consist of three
different scores: (i) syntactic dependencies are
scored using the labeled attachment score (LAS),
(ii) semantic dependencies are evaluated using a
labeled F
1
score, and (iii) the overall task is scored
with a macro average of the two previous scores.
We describe all these scoring measures next.
The LAS score is defined similarly as in the pre-
vious two shared tasks, as the percentage of to-
2
LDC catalog number LDC2005T33.
160
Number Name Description
1 ID Token counter, starting at 1 for each new sentence.
2 FORM Unsplit word form or punctuation symbol.
3 LEMMA Predicted lemma of FORM.
4 GPOS Gold part-of-speech tag from the Treebank (empty at test time).
5 PPOS Predicted POS tag.
6 SPLIT FORM Tokens split at hyphens and slashes.
7 SPLIT LEMMA Predicted lemma of SPLIT FORM.
8 PPOSS Predicted POS tags of the split forms.
9 HEAD Syntactic head of the current token, which is either a value of ID or zero (0).
10 DEPREL Syntactic dependency relation to the HEAD.
11 PRED Rolesets of the semantic predicates in this sentence.
12. . . ARG Columns with argument labels for each semantic predicate following textual order.
Table 1: Column format in the closed-track data. The columns in the lower part of the table are unseen
at test time and are to be predicted by systems.
Number Name Description
1 CONLL2003 Named entity labels using the tag set from the CoNLL-2003 shared task.
2 BBN NE labels using the tag set from the BBN Wall Street Journal Entity Corpus.
3 WNSS WordNet super senses.
4 MALT HEAD Head of the syntactic dependencies generated by MaltParser.
5 MALT DEPREL Label of syntactic dependencies generated by MaltParser.
Table 2: Column format in the open-track data.
kens for which a system has predicted the correct
HEAD and DEPREL columns (see Table 1). Same
as before, our scorer also computes the unlabeled
attachment score (UAS), i.e., the percentage of to-
kens with correct HEAD, and label accuracy, i.e.,
the percentage of tokens with correct DEPREL.
The semantic propositions are evaluated by con-
verting them to semantic dependencies, i.e., we
create a semantic dependency from every predicate
to all its individual arguments. These dependen-
cies are labeled with the labels of the correspond-
ing arguments. Additionally, we create a seman-
tic dependency from each predicate to a virtual
ROOT node. The latter dependencies are labeled
with the predicate senses. This approach guaran-
tees that the semantic dependency structure con-
ceptually forms a single-rooted, connected (but not
necessarily acyclic) graph. More importantly, this
scoring strategy implies that if a system assigns
the incorrect predicate sense, it still receives some
points for the arguments correctly assigned. For
example, for the correct proposition:
verb.01: ARG0, ARG1, ARGM-TMP
the system that generates the following output for
the same argument tokens:
verb.02: ARG0, ARG1, ARGM-LOC
receives a labeled precision score of 2/4 because
two out of four semantic dependencies are incor-
rect: the dependency to ROOT is labeled 02 in-
stead of 01 and the dependency to the ARGM-TMP
is incorrectly labeled ARGM-LOC. Using this strat-
egy we compute precision, recall, and F
1
scores
for both labeled and unlabeled semantic dependen-
cies.
Finally, we combine the syntactic and semantic
measures into one global measure using macro av-
eraging. We compute macro precision and recall
scores by averaging the labeled precision and re-
call for semantic dependencies with the LAS for
syntactic dependencies:
3
LMP = W
sem
? LP
sem
+ (1?W
sem
) ? LAS (1)
LMR = W
sem
? LR
sem
+ (1?W
sem
) ? LAS (2)
where LMP is the labeled macro precision and
LP
sem
is the labeled precision for semantic depen-
dencies. Similarly, LMR is the labeled macro re-
call and LR
sem
is the labeled recall for semantic
dependencies. W
sem
is the weight assigned to the
semantic task.
4
The macro labeled F
1
score, which
was used for the ranking of the participating sys-
tems, is computed as the harmonic mean of LMP
and LMR.
3
We can do this because the LAS for syntactic dependen-
cies is a special case of precision and recall, where the pre-
dicted number of dependencies is equal to the number of gold
dependencies.
4
We assign equal weight to the two tasks, i.e., W
sem
=
0.5.
161
2.2.2 Additional Evaluation Measures
We used several additional evaluation measures
to further analyze the performance of the partici-
pating systems.
The first additional measure used is Exact
Match, which reports the percentage of sentences
that are completely correct, i.e., all the generated
syntactic dependencies are correct and all the se-
mantic propositions are present and correct. While
this score is significantly lower than any of the of-
ficial scores, it will award systems that performed
joint learning or optimization for all subtasks.
In the same spirit but focusing on the seman-
tic subtasks, we report the Perfect Proposition F
1
score, where we score entire semantic frames or
propositions. This measure is similar to the PProps
accuracy score from the 2005 shared task (Carreras
and M`arquez, 2005), with the caveat that this year
this score is implemented as an F
1
measure, be-
cause predicates are not provided in the test data.
Hence, propositions may be over or under gener-
ated at prediction time.
Lastly, we analyze systems based on the ratio
between labeled F
1
score for semantic dependen-
cies and the LAS for syntactic dependencies. In
other words, this measure normalizes the seman-
tic scores relative to the performance of the pars-
ing component. This measure estimates the true
overall performance of the semantic subtasks, in-
dependent of the syntactic parser.
5
For example,
this score addresses the situations where the se-
mantic labeled F
1
score of one system is artificially
low because the corresponding syntactic compo-
nent does not perform well.
2.3 Closed and Open Challenges
Similarly to the CoNLL-2005 shared task, this
shared task evaluation is separated into two chal-
lenges:
Closed Challenge - systems have to be built
strictly with information contained in the given
training corpus, and tuned with the development
section. In addition, the PropBank and NomBank
lexical frames can be used. These restrictions
mean that constituent-based parsers or SRL sys-
tems can not be used in this challenge because the
constituent-based annotations are not provided in
our training set. The aim of this challenge is to
5
A correct evaluation of the stand-alone SRL systems
would require the usage of gold syntactic dependencies, but
these were not provided for the testing corpora.
compare the performance of the participating sys-
tems in a fair environment.
Open Challenge - systems can be developed mak-
ing use of any kind of external tools and resources.
The only condition is that such tools or resources
must not have been developed with the annota-
tions of the test set, both for the input and out-
put annotations of the data. In this challenge,
we are interested in learning methods which make
use of any tools or resources that might improve
the performance. For example, we encourage the
use of semantic information, as provided by NE
recognition or word-sense disambiguation (WSD)
systems (such state-of-the-art annotations are pro-
vided by the organizers, see Table 2). Also, in
this challenge participants are encouraged to use
constituent-based parsers and SRL systems, as
long as these systems were trained only with the
sections of Penn Treebank used in the shared task
training corpus. To encourage the participation of
the groups that are only interested in SRL, the or-
ganizers provide also the output of a state-of-the-
art dependency parser as input in this challenge.
The comparison of different systems in this setting
may not be fair, and thus ranking of systems is not
necessarily important.
3 Data
The corpora used in the shared task evaluation
were generated through a process that merges
several input corpora and converts them from
the constituent-based formalism to dependencies.
This section starts with an introduction of the in-
put corpora used, followed by a description of
the constituent-to-dependency conversion process.
The section concludes with an overview of the
shared task corpora.
3.1 Input Corpora
Input to our merging procedures includes the Penn
Treebank, BBN?s named entity corpus, PropBank
and NomBank. In this section, we will pro-
vide brief descriptions of these annotations in
terms of both form and content. All annotations
are currently being distributed by the Linguistic
Data Consortium, with the exception of NomBank,
which is freely downloadable.
6
6
http://nlp.cs.nyu.edu/meyers/NomBank.
html
162
3.1.1 Penn Treebank 3
The Penn Treebank 3 corpus (Marcus et al,
1993) consists of hand-coded parses of the Wall
Street Journal (test, development and training) and
a small subset of the Brown corpus (W. N. Fran-
cis and H. Ku?cera, 1964) (test only). These hand
parses are notated in-line and sometimes involve
changing the strings of the input data. For ex-
ample, in file wsj 0309, the token fearlast in the
text corresponds to the two tokens fear and last
in the annotated data. In a similar way, cannot
is regularly split to can and not. It is significant
that the other annotations assume the tokeniza-
tion of the Penn Treebank, as this makes it easier
for us to merge the annotation. The Penn Tree-
bank syntactic annotation includes phrases, parts
of speech, empty category representations of vari-
ous filler/gap constructions and other phenomena,
based on a theoretical perspective similar to that
of Government and Binding Theory (Chomsky,
1981).
3.1.2 BBN Pronoun Coreference and Entity
Type Corpus
BBN?s NE annotation of the Wall Street Journal
corpus (Weischedel and Brunstein, 2005) takes the
form of SGML inline markup of text, tokenized
to be completely compatible with the Penn Tree-
bank annotation, e.g., fearlast and cannot are split
in the same ways. Named entity categories in-
clude: Person, Organization, Location, GPE, Fa-
cility, Money, Percent, Time and Date, based on
the definitions of these categories in MUC (Chin-
chor and Robinson, 1998) and ACE
7
tasks. Sub-
categories are included as well. Note however that
from this corpus we only use NE boundaries to
derive NAME dependencies between NE tokens,
e.g., we create a NAME dependency from Mary to
Smith given the NE mention Mary Smith.
3.1.3 Proposition Bank I (PropBank)
The PropBank annotation (Palmer et al, 2005)
classifies the arguments of all the main verbs in the
Penn Treebank corpus, other than be. Arguments
are numbered (ARG0, ARG1, . . .) based on lexical
entries or frame files. Different sets of arguments
are assumed for different rolesets. Dependent con-
stituents that fall into categories independent of
the lexical entries are classified as various types
7
http://projects.ldc.upenn.edu/ace/
of ARGM (TMP, ADV, etc.).
8
Rather than us-
ing PropBank directly, we used the version created
for the CoNLL-2005 shared task (Carreras and
M`arquez, 2005). PropBank?s pointers to subtrees
are converted into the list of leaves of those sub-
trees, minus the empty categories. On occasion,
arguments of verbs end up being two non-adjacent
substrings. For example, the argument of claims in
the following sentence is indicated in bold: This
sentence, Mary claims, is self-referential. The
CoNLL-2005 format handles this by marking both
strings A1 (This sentence and is self-referential),
but adding a C- prefix to the argument tag on the
second argument. Another difference between the
PropBank annotation and the CoNLL-2005 ver-
sion of it is their treatments of filler gap construc-
tions involving empty categories. PropBank an-
notation includes the whole chain of empty cate-
gories, as well as the antecedent of the empty cate-
gory (the filler of the gap). In contrast, the CoNLL-
2005 version only includes the filler of the gap and
if there is no filler, the argument is omitted, e.g.,
no ARG0 (subject) for leave would be included in
I said to leave because the subject of leave is un-
specified.
3.1.4 NomBank
NomBank annotation (Meyers et al, 2004) uses
essentially the same framework as PropBank to an-
notate arguments of nouns. Differences between
PropBank and NomBank stem from differences
between noun and verb argument structure; differ-
ences in treatment of nouns and verbs in the Penn
Treebank; and differences in the sophistication of
previous research about noun and verb argument
structure. Only the subset of nouns that take ar-
guments are annotated in NomBank and only a
subset of the non-argument siblings of nouns are
marked as ARGM. These limitations were nec-
essary to make the NomBank task consistent and
tractable. In addition, long distance dependencies
of nouns, e.g., the relation between Mary and walk
in Mary took dozens of walks is handled as fol-
lows: Mary is marked as the ARG0 of walk and
took + dozens + of is marked as a support chain
in NomBank. In contrast, verbal long distance de-
pendencies can be handled by means of empty cat-
egories in the Penn Treebank, e.g., the relation be-
8
PropBank I is used here. Later versions of PropBank
mark instances of be in addition to other verbs. PropBank?s
use of the terms roleset and ARGM correspond approximately
to sense and adjunct in common usage.
163
tween John and walked in John seemed t to walk.
Support chains are needed because nominal long
distance dependencies are not captured under the
Penn Treebank?s system of empty categories.
3.2 Conversion to Dependencies
3.2.1 Syntactic Dependencies
There exists no large-scale dependency tree-
bank for English, and we thus had to construct a
dependency-annotated corpus automatically from
the Penn Treebank (Marcus et al, 1993). Since
dependency syntax represents grammatical struc-
ture by means of labeled binary head?dependent
relations rather than phrases, the task of the con-
version procedure is to identify and label the
head?dependent pairs. The idea underpinning
constituent-to-dependency conversion algorithms
(Magerman, 1994; Collins, 1999; Yamada and
Matsumoto, 2003) is that head?dependent pairs are
created from constituents by selecting one word in
each phrase as the head and setting all other as its
dependents. The dependency labels are then in-
ferred from the phrase?subphrase or phrase?word
relations.
Our conversion procedure (Johansson and
Nugues, 2007) differs from this basic approach by
exploiting the rich structure of the constituent for-
mat used in Penn Treebank 3:
? Grammatical function labels that often can be
directly used in the dependency framework.
? Long-distance grammatical relations repre-
sented by means of empty categories and sec-
ondary edges, which can be used to create (of-
ten nonprojective) dependency links.
Of the grammatical function tags available in the
Treebank, we removed the HLN, NOM, TPC, and
TTL tags since they represent structural properties
of single phrases rather than binary relations. For
compatibility between the WSJ and Brown cor-
pora, we removed the ETC, UNF, and IMP tags
from Brown and the CLR tag from WSJ.
Algorithms 1 and 2 show the constituent-to-
dependency conversion algorithm and function la-
beling, respectively. The first steps apply structural
transformations to the constituent trees. Next, a
head word is assigned to each constituent. After
this, grammatical functions are inferred, allowing
a dependency tree to be created.
To find head children (used in
assign-heads), a system of rules is used
Algorithm 1: Pseudocode for constituent-to-
dependency conversion.
procedure constituents-to-dependencies(T )
import-glarf(T )
reattach-traces(T )
split-small-clauses(T )
assign-heads(T.root)
assign-functions(T )
return create-dependency-tree(T )
procedure import-glarf(T )
Import a GLARF surface dependency graph G
for each multi-word name N in G
for each token d in N
Set the function tag of d to NAME
for each dependency link h ?
L
d in G
if L ? { APPOSITE, A-POS, N-POS, POST-HON, Q-POS,
RED-RELATIVE, SUFFIX, T-POS, TITLE }
or if h and d are inside a split word
Set the function tag of d to L in T
if h and d are part of a larger constituent
Add an NX constituent to T that brackets h and d
procedure reattach-traces(T )
for each empty category t in T
if t is linked to a constituent C via a secondary edge label L
and L ? {
*
ICH
*
,
*
T
*
,
*
RNR
*
}
disconnect C
disconnect the secondary edge
attach C to the parent of t
procedure split-small-clauses(T )
for each verb phrase C in T
if C has a child S and the phrase label of S is S
and S is not preceded by a ?? or , tag
and S has a subject child s
disconnect s
attach s to C
set the function tag of s to OBJ
set the function tag of S to OPRD
procedure assign-heads(N)
for each child C of N
assign-heads(C)
if is-coordinated(N)
e ? index of first CC or CONJP or , or :
else
e ? index of last child of N
find head child H between 1 and e according to head rules (Table 3)
N.head ? H.head
procedure is-coordinated(N)
if N has the label UCP return True
if N has a CC or CONJP child which is not leftmost return True
if N has a , or : child c, and c is not leftmost or rightmost or
crossed by an apposition link, return True
else return False
procedure create-dependency-tree(T )
D ? {}
for each token t in T
let C be the highest constituent that t is the head of
let P be the parent of C
let L be the function tag of C
D ? D ? P.head ?
L
t
return D
(Table 3). The first column in the table indicates
the phrase type, the second is the search direction,
and the third is a priority list of phrase types to
look for. For instance, to find the head of an S
phrase, we look from right to left for a VP. If
no VP is found, look for anything with a PRD
function tag, and so on.
Moreover, since the grammatical structure in-
164
ADJP ? NNS QP NN $ ADVP JJ VBN VBG ADJP JJR NP JJS DT
FW RBR RBS SBAR RB
ADVP ? RB RBR RBS FW ADVP TO CD JJR JJ IN NP JJS NN
CONJP ? CC RB IN
FRAG ? (NN
*
| NP) W
*
SBAR (PP | IN) (ADJP | JJ) ADVP
RB
INTJ ?
*
LST ? LS :
NAC, NP, NX, WHNP ? (NN
*
| NX) NP-? JJR CD JJ JJS RB QP NP
PP, WHPP ? IN TO VBG VBN RP FW
PRN ? S
*
N
*
W
*
PP|IN ADJP|JJ
*
ADVP|RB
*
PRT ? RP
QP ? $ IN NNS NN JJ RB DT CD NCD QP JJR JJS
RRC ? VP NP ADVP ADJP PP
S ? VP
*
-PRD S SBAR ADJP UCP NP
SBAR ? S SQ SINV SBAR FRAG IN DT
SBARQ ? SQ S SINV SBARQ FRAG
SINV ? VBZ VBD VBP VB MD VP
*
-PRD S SINV ADJP NP
SQ ? VBZ VBD VBP VB MD
*
-PRD VP SQ
UCP ?
*
VP ? VBD VBN MD VBZ VB VBG VBP VP
*
-PRD ADJP NN NNS
NP
WHADJP ? CC WRB JJ ADJP
WHADVP ? CC WRB
X ?
*
Table 3: Head rules.
Algorithm 2: Pseudocode for the function la-
beling procedure.
procedure assign-functions(T )
for each constituent C in T
if C has no function tag from Penn or GLARF
L ? infer-function(C)
Set the function tag of C to L
procedure infer-function(C)
let c be the head of C, P the parent of C, and p the head of P
if C is an object return OBJ
if C is PRN return PRN
if h is punctuation return P
if C is coordinated with P return COORD
if C is PP, ADVP, or SBAR and P is VP return ADV
if C is PRT and P is VP return PRT
if C is VP and P is VP, SQ, or SINV return VC
if C is TO and P is VP return IM
if P is SBAR and p is IN return SUB
if P is VP, S, SBAR, SBARQ, SINV, or SQ and C is RB return ADV
if P is NP, NX, NAC, or WHNP return NMOD
if P is ADJP, ADVP, WHADJP, or WHADVP return AMOD
if P is PP or WHPP return PMOD
else return DEP
side noun phrases (NP) is under-specified in the
Penn Treebank, we imported dependencies in-
side NPs and hyphenated words from a version
of the Penn Treebank mapped into GLARF, the
Grammatical and Logical Argument Representa-
tion Framework (Meyers et al, 2007).
The parts of GLARF?s NP analysis that are most
relevant to this task include: (i) identifying ap-
posites (APPO, e.g., that book depends on gift in
Mary?s gift, a book about cheese; (ii) the iden-
tification of name boundaries taken from BBN?s
NE annotation, e.g., identifying that Smith de-
pends on Mary which depends on appointment
in the Mary Smith appointment; (iii) identifying
TITLE and POSTHON dependencies, e.g., deter-
mining that Ms. and III depend on Mary in Ms.
Mary Smith III. These identifications were car-
ried out by hand-coded rules that have been fine
tuned as part of GLARF, over the past several
years. For example, identifying apposition con-
structions requires identifying that both the head
and the apposite can stand alone ? proper nouns
(John Smith), plural nouns (books), and singular
common nouns with determiners (the book) are
stand-alone cases, whereas singular nouns without
determiners (green book) do not qualify.
We split Treebank tokens at a hyphen (-) or a
forward slash (/) if the segments on either side of
these delimiters are: (a) a word in a dictionary
(COMLEX Syntax or any of the dictionaries avail-
able on the NOMLEX website); (b) part of a mark-
able Named Entity;
9
or (c) a prefix from the list:
co, pre, post, un, anti, ante, ex, extra, fore, non,
over, pro, re, super, sub, tri, bi, uni, ultra. For ex-
ample, York-based was split into 3 segments: (1)
York, (2) - and (3) based.
9
The CoNLL-2008 website contains a Named Entity To-
ken gazetteer to aid in this segmentation.
165
3.2.2 Semantic Dependencies
When encoding the semantic dependencies, it
was necessary to convert the underlying con-
stituent analysis of PropBank and NomBank into
a dependency analysis. Because semantic predi-
cates are already assigned to individual tokens in
both PropBank (the version used for the CoNLL-
2005 shared task) and NomBank, constituent-to-
dependency conversion is thus necessary only for
semantic arguments. Conceptually, this conver-
sion can be handled using similar heuristics as de-
scribed in Section 3.2.1. However, in order to
avoid replicating this effort and to ensure compat-
ibility between syntactic and semantic dependen-
cies, we decided to generate semantic dependen-
cies using only argument boundaries and the syn-
tactic dependencies generated in Section 3.2.1, i.e.,
ignoring syntactic constituents. Given this input,
we identify the head of a semantic argument using
the following heuristic:
The head of a semantic argument is as-
signed to the token inside the argument
boundaries whose head is a token out-
side the argument boundaries.
This heuristic works remarkably well: over 99%
of the PropBank arguments in the training corpus
have a single token whose head is located outside
of the argument boundaries. As a simple example,
consider the following annotated text: [sold]
PRED
[1214 cars]
ARG1
[in the U.S.]
ARGM-LOC
. Us-
ing the above heuristic, the head of the ARG1 ar-
gument is set to cars, because it has an OBJ de-
pendency to sold, and the head of the ARGM-
LOC argument is set to in, because it modifies sold
through a LOC dependency.
While this heuristic processes the vast majority
of arguments, there are several cases that require
special treatment. We discuss these situations in
the remainder of this section.
Arguments with several syntactic heads
For 0.7% of the semantic arguments, the above
heuristic detects several syntactic heads for the
given boundary. For example, in the text [it]
ARG0
[expects]
PRED
[its U.S. sales to remain steady
at about 1200 cars]
ARG1
, the above heuris-
tic assigns two syntactic heads to ARG1: sales,
which modifies expects through an OBJ depen-
dency, and to, which modifies expects through a
PRD dependency. These situations are caused
by the constituent-to-dependency conversion pro-
cess described in Section 3.2.1, which in some
cases interprets syntax differently than the orig-
inal Treebank annotation, e.g., the raising phe-
nomenon for the PRD dependency in the above
example. In such cases, we split the original argu-
ment into a sequence of discontinuous arguments,
e.g., the ARG1 in the above example becomes [its
U.S. sales]
ARG1
[to remain steady at about 1200
cars]
C-ARG1
.
Merging discontinuous arguments
While in the above case we split arguments, there
are situations where we can merge arguments that
were initially discontinuous in PropBank or Nom-
Bank. This typically happens when the Prop-
Bank/NomBank predicate is infixed inside one of
its arguments. For example, in the text [Million-
dollar conferences]
ARG1
were [held]
PRED
[to
chew on subjects such as... ]
C-ARG1
, PropBank
lists multiple constituents as aggregately filling the
ARG1 slot of held. These cases are detected au-
tomatically because the least common ancestor of
the argument pieces is actually one of the argument
segments. In the above example, to chew on sub-
jects such as... depends on Million-dollar confer-
ences because to modifies conferences through a
NMOD dependency. In these situations, we treat
the least common ancestor, e.g., conferences in the
above text, as the true argument. This heuristic al-
lowed us to merge 1665 (or 0.6% of total) argu-
ments that were initially discontinuous in the Prop-
Bank training corpus.
Empty categories
PropBank and NomBank both encode chains of
empty categories. As with the 2005 shared task
(Carreras and M`arquez, 2005), we used the head
of the antecedent of empty categories as arguments
rather than empty categories. Furthermore, empty
category arguments with no antecedents were ig-
nored.
10
For example, given The man wanted t to
make a speech, we assume that the A0 of make and
speech is man, rather than the chain consisting of
the empty category represented as t and man.
Annotation disagreements
NomBank and Penn Treebank annotators some-
times disagree about constituent structure. Nom-
10
Under our approach to filler gap constructions, the filler
is a shared argument (as in Relational Grammar, most Feature
Structure and Dependency Grammar frameworks), in con-
trast with the Penn Treebank?s empty category antecedent ap-
proach (more closely resembling the various Chomskian ap-
proaches).
166
Label Freq. Description
NMOD 324834 Modifier of nominal
P 135260 Punctuation
PMOD 115988 Modifier of preposition
SBJ 89371 Subject
OBJ 66677 Object
ROOT 49178 Root
ADV 47379 General adverbial
NAME 41138 Name-internal link
VC 35250 Verb chain
COORD 31140 Coordination
DEP 29456 Unclassified
TMP 26305 Temporal adverbial or nominal modifier
CONJ 24522 Second conjunct (dependent on conjunction)
LOC 18500 Locative adverbial or nominal modifier
AMOD 17868 Modifier of adjective or adverbial
PRD 16265 Predicative complement
APPO 16163 Apposition
IM 16071 Infinitive verb (dependent on infinitive marker to)
HYPH 14073 Token part of a hyphenated word (dependent on a preceding part of the hyphenated word)
HMOD 13885 Token inside a hyphenated word (dependent on the head of the hyphenated word)
SUB 12995 Subordinated clause (dependent on subordinating conjuction)
OPRD 11707 Predicative complement of raising/control verb
SUFFIX 10548 Possessive suffix (dependent on possessor)
DIR 6145 Adverbial of direction
TITLE 5917 Title (dependent on name)
MNR 4753 Adverbial of manner
POSTHON 4377 Posthonorific modifier of nominal
PRP 4013 Adverbial of purpose or reason
PRT 3235 Particle (dependent on verb)
LGS 3115 Logical subject of a passive verb
EXT 2374 Adverbial of extent
PRN 2176 Parenthetical
EXTR 658 Extraposed element in cleft
DTV 496 Dative complement (to) in dative shift
PUT 271 Complement of the verb put
BNF 44 Benefactor complement (for) in dative shift
VOC 24 Vocative
Table 4: Statistics for atomic syntactic labels.
Bank annotators are in effect assuming that the
constituents provided form a phrase. In this case,
the constituents are adjacent to each other. For ex-
ample, consider the NP the human rights discus-
sion. In this case, the Penn Treebank would treat
each of the four words the, human, rights, discus-
sion as daughters of a single NP node. However,
NomBank would treat human rights as a single
ARG1 of discussion. Since noun noun modifica-
tion constructions are head final, we can easily de-
termine (via GLARF) that rights is the markable
dependent of discussion.
Support chains
Finally, NomBank?s encoding of support chains is
handled as chains of dependencies in the data (al-
though these are not scored). For example, given
Mary took dozens of walks, where Mary is the
ARG0 of walks, the support chain took + dozens +
of is represented as a sequence of dependencies: of
depends on Mary, dozens depends on of and took
depends on dozens. Each of these dependencies is
labeled SU.
3.3 Overview of Corpora
The syntactic dependency types are divided into
atomic types that consist of a single label, and non-
atomic types consisting of more than one label.
There are 38 atomic and 70 non-atomic labels in
the corpus. There are three types of non-atomic
labels: those consisting of a PRD or OPRD con-
catenated with an adverbial label such as LOC or
TMP; gapping labels such as GAP-SBJ; and com-
bined adverbial tags such as LOC-TMP.
Table 4 shows statistics for the atomic syntac-
tic dependencies: label type, the frequency of the
label in the complete corpus, and a description of
the label. Table 5 shows the corresponding statis-
tics for non-atomic dependencies, excluding gap-
ping dependencies. The non-atomic labels are rare,
which made it difficult to learn these relations ef-
167
Label Frequency
LOC-PRD 798
PRD-TMP 51
PRD-PRP 45
LOC-OPRD 31
DIR-PRD 4
MNR-PRD 3
LOC-TMP 2
MNR-TMP 1
LOC-MNR 1
DIR-OPRD 1
Table 5: Statistics for non-atomic syntactic labels
excluding gapping labels.
Label Frequency
GAP-SBJ 116
GAP-OBJ 102
DEP-GAP 83
GAP-TMP 69
GAP-PRD 66
GAP-LGS 44
GAP-LOC 42
DIR-GAP 37
GAP-PMOD 22
GAP-VC 20
EXT-GAP 16
ADV-GAP 15
GAP-NMOD 13
GAP-LOC-PRD 6
DTV-GAP 6
AMOD-GAP 6
GAP-MNR 5
GAP-PRP 4
EXTR-GAP 3
GAP-SUB 1
GAP-PUT 1
GAP-OPRD 1
Table 6: Statistics for non-atomic labels containing
a gapping label.
fectively. Table 6 shows the table for non-atomic
labels containing a gapping label.
A dependency link w
i
? w
j
is said to be pro-
jective if all words occurring between w
i
and w
j
in
the surface word order are dominated by w
i
(where
dominance is the transitive closure of the direct
link relation). Nonprojective links are impossible
to handle for the search procedures in many types
of dependency parsers. It has been previously ob-
served that the majority of dependencies in all lan-
guages are projective, and this is particularly true
for English ? in the complete corpus, only 4118
links (0.4%) are nonprojective. 3312 sentences, or
7.6%, contain at least one nonprojective link.
Table 7 shows statistics for different types of
nonprojective links: nonprojectivity caused by
wh-movement, such as in Where are you going?
or What have you done?; split clauses such as
Type Frequency
wh-movement 1709
Split clause 734
Split noun phrase 590
Other 1085
Table 7: Statistics for nonprojective links.
POS Frequency
NN 68477
NNS 30048
VBD 24106
VB 23650
VBN 19339
VBG 14245
VBZ 10883
VBP 6330
Other 83
Table 8: Statistics for predicates, by POS tags.
Even to make love, he says, you need experience;
split noun phrases such as hold a hearing tomor-
row on the topic; and all other types of nonprojec-
tive links.
Lastly, Tables 8 and 9 summarizes statistics for
semantic predicates and roles. Table 8 shows the
number of non-support predicates with a given
POS tag in the whole corpus (we used GPOS or
PPOSS for predicates inside hyphenated words).
The last line shows the number of predicates with
a POS tag that does not start with NN or VB. This
last table entry is generated by POS tagger mis-
takes when producing the PPOSS tags, or by errors
in our NomBank/PropBank conversion software.
11
Nevertheless, the overall picture given by the table
indicates that predicates are almost perfectly dis-
tributed between nouns and verbs: there are 98525
nominal and 98553 verbal predicates.
Table 9 shows the number of arguments with a
given role label. For brevity we list only labels that
are instantiated at least 10 times in the whole cor-
pus. The total number of arguments labeled with a
role label with frequency lower than 10 is listed
in the last line in the table. The table indicates
that, while the top three most common role labels
are ?core? labels (A1, A0, A2), modifier arguments
(AM-
*
) account for approximately 20% of the total
number of arguments. On the other hand, discon-
tinuous arguments are not common: only 0.7% of
the total number of arguments have a continuation
label (C-
*
).
11
In very few situations, we select incorrect head tokens for
multi-word predicates.
168
Label Frequency
A1 161409
A0 109437
A2 51197
AM-TMP 25913
AM-MNR 13080
AM-LOC 11409
A3 10269
AM-MOD 9986
AM-ADV 9496
AM-DIS 5369
R-A0 4432
AM-NEG 4097
A4 3281
C-A1 3118
R-A1 2565
AM-PNC 2445
AM-EXT 1428
AM-CAU 1346
AM-DIR 1318
R-AM-TMP 797
R-A2 307
R-AM-LOC 246
R-AM-MNR 155
A5 91
AM-PRD 78
C-A0 70
C-A2 65
R-AM-CAU 50
C-A3 37
R-A3 29
C-AM-MNR 24
C-AM-ADV 20
AM-REC 16
AA 14
R-AM-PNC 12
C-AM-EXT 11
C-AM-TMP 11
C-A4 11
Frequency < 10 70
Table 9: Statistics for semantic roles.
4 Submissions and Results
Nineteen groups submitted test runs in the closed
challenge and five groups participated in the open
challenge. Three of the latter groups participated
only in the open challenge, and two of these sub-
mitted results only for the semantic subtask. These
results are summarized in Tables 10 and 11.
Table 10 summarizes the official results ? i.e.,
results at evaluation deadline ? for the closed chal-
lenge. Note that several teams corrected bugs
and/or improved their systems and they submit-
ted post-evaluation scores (accounted in the shared
task website). The table indicates that most of the
top results cluster together: three systems had a
labeled macro F
1
score on the WSJ+Brown cor-
pus around 82 points (che, ciaramita, and zhao);
five systems scored around 79 labeled macro F
1
points (yuret, samuelsson, zhang, henderson, and
watanabe). Remarkably, the top-scoring system
(johansson) is in a class of its own, with scores
2?3 points higher than the next system. This is
most likely caused by the fact that Johansson and
Nugues (2008) implemented a thorough system
that addressed all facets of the task with state-of-
the-art methods: second-order parsing model, ar-
gument identification/classification models sepa-
rately tuned for PropBank and NomBank, rerank-
ing inference for the SRL task, and, finally, joint
optimization of the complete task using meta-
learning (more details in Section 5).
Table 11 lists the official results in the open chal-
lenge. The results in this challenge are lower than
in the closed challenge, but this was somewhat
to be expected considering that there were fewer
participants in this challenge and none of the top
five groups in the closed challenge submitted re-
sults in the open challenge. Only one of the sys-
tems that participated in both challenges (zhang)
improved the results submitted in the closed chal-
lenge. Zhang et al (2008) achieved this by ex-
tracting features for their semantic subtask mod-
els both from the parser used in the closed chal-
lenge and a secondary parser that was trained on
a different corpus. The improvements measured
were relatively small for the in-domain WSJ cor-
pus (0.2 labeled macro F
1
points) but larger for the
out-of-domain Brown corpus (approximately 1 la-
beled macro F
1
point).
Tables 10 and 11 indicate that in both chal-
lenges the results on the out-of-domain corpus
(Brown) are much lower than the results measured
in-domain (WSJ). The difference is around 7?8
LAS points for the syntactic subtask and 12?14 la-
beled F
1
points for semantic dependencies. Over-
all, this yields a drop of approximately 10 labeled
macro F
1
points for most systems. This perfor-
mance decrease on out-of-domain corpora is con-
sistent with the results reported in CoNLL-2005
on SRL (using the same Brown corpus). These
results indicate that domain adaptation is a prob-
lem that is far from being solved for both syntactic
and semantic analysis of text. Furthermore, as the
scores on the syntactic and semantic subtasks in-
dicate, domain adaptation becomes even harder as
the task to be solved gets more complex.
We describe the participating systems in the next
section. Then, in Section 6, we revert to result
analysis using different evaluation measures and
different views of the data.
169
Labeled Macro F
1
Labeled Attachment Score Labeled F
1
(complete task) (syntactic dependencies) (semantic dependencies)
WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown
johansson 84.86 (1) 85.95 75.95 89.32 (1) 90.13 82.81 80.37 (1) 81.75 69.06
che 82.66 (2) 83.78 73.57 86.75 (5) 87.51 80.73 78.52 (2) 80.00 66.37
ciaramita 82.06 (3) 83.25 72.46 86.60 (11) 87.47 79.67 77.50 (3) 79.00 65.24
zhao 81.44 (4) 82.62 71.78 86.66 (8) 87.52 79.83 76.16 (4) 77.67 63.69
yuret 79.84 (5) 80.97 70.55 86.62 (10) 87.39 80.46 73.06 (5) 74.54 60.62
samuelsson 79.79 (6) 80.92 70.49 86.63 (9) 87.36 80.77 72.94 (6) 74.47 60.18
zhang 79.32 (7) 80.41 70.48 87.32 (2) 88.14 80.80 71.31 (7) 72.67 60.16
henderson 79.11 (8) 80.19 70.34 86.91 (4) 87.78 80.01 70.97 (8) 72.26 60.38
watanabe 79.10 (9) 80.30 69.29 87.18 (3) 88.06 80.17 70.84 (9) 72.37 58.21
morante 78.43 (10) 79.52 69.55 86.07 (12) 86.88 79.58 70.51 (10) 71.88 59.23
li 78.35 (11) 79.38 70.01 86.69 (6) 87.42 80.8 69.95 (11) 71.27 59.17
baldridge 77.49 (12) 78.57 68.53 86.67 (7) 87.42 80.64 67.92 (14) 69.35 55.95
chen 77.00 (13) 77.95 69.23 84.47 (16) 85.20 78.58 69.45 (12) 70.62 59.81
lee 76.90 (14) 77.96 68.34 84.82 (15) 85.69 77.83 68.71 (13) 69.95 58.63
sun 76.28 (15) 77.10 69.58 85.75 (13) 86.37 80.75 66.61 (15) 67.62 58.26
choi 71.23 (16) 72.22 63.44 77.56 (17) 78.58 69.46 64.78 (16) 65.72 57.4
trandabat 63.45 (17) 64.21 57.41 85.21 (14) 85.96 79.24 40.63 (17) 41.36 34.75
lluis 63.29 (18) 63.74 59.65 71.95 (18) 72.30 69.14 54.52 (18) 55.09 49.95
neumann 19.93 (19) 20.13 18.14 16.25 (19) 16.22 16.47 22.36 (19) 22.86 17.94
Table 10: Official results in the closed challenge (post-evaluation scores are available on the shared
task website). Teams are denoted by the last name of the first author of the corresponding paper in
the proceedings or the last name of the person who registered the team if no paper was submitted.
Italics indicate that there is no corresponding paper in the proceedings. Results are sorted in descending
order of the labeled macro F
1
score on the WSJ+Brown corpus. The number in parentheses next to the
WSJ+Brown scores indicates the system rank in the corresponding task.
Labeled Macro F
1
Labeled Attachment Score Labeled F
1
(complete task) (syntactic dependencies) (semantic dependencies)
WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown
vickrey ? ? ? ? ? ? 76.17 (1) 77.38 66.23
riedel ? ? ? ? ? ? 74.59 (2) 75.72 65.38
zhang 79.61 (1) 80.61 71.45 87.32 (1) 88.14 80.80 71.89 (3) 73.08 62.11
li 77.84 (2) 78.87 69.51 86.69 (2) 87.42 80.80 68.99 (4) 70.32 58.22
wang 76.19 (3) 78.39 59.89 84.56 (3) 85.50 77.06 67.12 (5) 70.41 42.67
Table 11: Official results in the open challenge (post-evaluation scores are available on the shared task
website). Teams are denoted by the last name of the first author of the corresponding paper in the
proceedings or the last name of the person who registered the team if no paper was submitted. Italics
indicate that there is no corresponding paper in the proceedings. Results are sorted in descending order of
the labeled F
1
score for semantic dependencies on the WSJ+Brown corpus. The number in parentheses
next to the WSJ+Brown scores indicates the system rank in the corresponding task.
5 Approaches
Table 5 summarizes the properties of the sys-
tems that participated in the closed the open chal-
lenges. The second column of the table high-
lights the overall architectures. We used + to in-
dicate that the components are sequentially con-
nected. The lack of a + sign indicates that the cor-
responding tasks are performed jointly. For exam-
ple, Riedel and Meza-Ruiz (2008) perform pred-
icate and argument identification and classifica-
tion jointly, whereas Ciaramita et al (2008) im-
plemented a pipeline architecture of three compo-
nents. We use the || to indicate that several differ-
ent architectures that span multiple subtasks were
deployed in parallel.
This summary of system architectures indicates
that it is common that systems combine sev-
eral components in the semantic or syntactic sub-
tasks ? e.g., nine systems jointly performed pred-
icate/argument identification and classification ?
but only four systems combined components be-
tween the syntactic and semantic subtasks: Hen-
derson et al (2008), who implemented a generative
history-based model (Incremental Sigmoid Belief
Networks with vectors of latent variables) where
syntactic and semantic structures are separately
170
generated but using a synchronized derivation (se-
quence of actions); Samuelsson et al (2008),
who, within an ensemble-based architecture, im-
plemented a joint syntactic-semantic model using
MaltParser with labels enriched with semantic in-
formation; Llu??s and M`arquez, who used a modi-
fied version of the Eisner algorithm to jointly pre-
dict syntactic and semantic dependencies; and fi-
nally, Sun et al (2008), who integrated depen-
dency label classification and argument identifi-
cation using a maximum-entropy Markov model.
Additionally, Johansson and Nugues (2008), who
had the highest ranked system in the closed chal-
lenge, integrate syntactic and semantic analysis in
a final reranking step, which maximizes the joint
syntactic-semantic score in the top k solutions. In
the same spirit, Chen et al (2008) search in the
top k solutions for the one that maximizes a global
measure, in this case the joint probability of the
complete problem. These joint learning strategies
are summarized in the Joint Learning/Opt. col-
umn in the table. The system of Riedel and Meza-
Ruiz (2008) deserves a special mention: even
though Riedel and Meza-Ruiz did not implement
a syntactic parser, they are the only group that per-
formed the complete SRL subtask ? i.e., predicate
identification and classification, argument identifi-
cation and classification ? jointly, simultaneously
for all the predicates in a sentence. They imple-
mented a joint SRL model using Markov Logic
Networks and they selected the overall best solu-
tion using inference based on the cutting-plane al-
gorithm.
Although some of the systems that implemented
joint approaches obtained good results, the top
five systems in the closed challenge are essen-
tially systems with pipeline architectures. Further-
more, Johansson and Nugues (2008) and Riedel
and Meza-Ruiz (2008) showed that joint learn-
ing/optimization improves the overall results, but
the improvement is not large. These initial ef-
forts indicate at least that the joint modeling of this
problem is not a trivial task.
The D Arch. and D Inference columns summa-
rize the parsing architectures and the correspond-
ing inference strategies. Similar to last year?s
shared task (Nivre et al, 2007), the vast majority of
parsing models fall in two classes: transition-based
(?trans? in the table) or graph-based (?graph?)
models. By and large, transition-based models use
a greedy inference strategy, whereas graph-based
models used different Maximum Spanning Tree
(MST) algorithms: Carreras (2007) ? MST
C
, Eis-
ner (2000) ? MST
E
, or Chu-Liu/Edmonds (Mc-
Donald et al, 2005; Chu and Liu, 1965; Edmonds,
1967) ? MST
CL/E
. More interestingly, most of
the best systems used some strategy to mitigate
parsing errors. In the top three systems in the
closed challenge, two (che and ciaramita) used
parser combination through voting and/or stacking
of different models (see the D Comb. column).
Samuelsson et al (2008) perform a MST infer-
ence with the bag of all dependencies output by
the individual MALT parser variants. Johansson
and Nugues (2008) use a single parsing model, but
this model is extended with second-order features.
The PA Arch. and PA Inference columns sum-
marize the architectures and inference strategies
used for the identification and classification of
predicates and arguments. The columns indicate
that most systems modeled the SRL problem as a
token-by-token classification problem (?class? in
the table) with a corresponding greedy inference
strategy. Some systems (e.g., yuret, samuelsson,
henderson, lluis) incorporate SRL within parsing,
in which case we report the corresponding parsing
architecture and inference approach. Vickrey and
Koller (2008) simplify the sentences to be labeled
using a set of hand-crafted rules before deploying
a classification model on top of a constituent-based
representation. Unlike in the case of parsing, few
systems (yuret, samuelssson, and morante) com-
bine several PA models and the combination is lim-
ited to simple voting strategies (see the PA Comb.
column).
Finally, the ML Methods column lists the Ma-
chine Learning (ML) methods used. The column
indicates that maximum entropy (ME) was the
most popular method (12 distinct systems relied
on it). Support Vector Machines (SVM) (eight sys-
tems) and the Perceptron algorithm (three systems)
were also popular ML methods.
6 Analysis
Section 4 summarized the results in the closed
and open challenges using the official evaluation
measures. In this section, we analyze the sub-
mitted runs using different evaluation measures,
e.g., Exact Match or Perfect Proposition F
1
scores,
and different views of the data, e.g., only non-
projective dependencies or NomBank versus Prop-
Bank frames.
171
O
v
e
r
a
l
l
D
D
D
P
A
P
A
P
A
J
o
i
n
t
M
L
c
l
o
s
e
d
A
r
c
h
.
A
r
c
h
.
C
o
m
b
.
I
n
f
e
r
e
n
c
e
A
r
c
h
.
C
o
m
b
.
I
n
f
e
r
e
n
c
e
L
e
a
r
n
i
n
g
/
O
p
t
.
M
e
t
h
o
d
s
j
o
h
a
n
s
s
o
n
D
+
P
I
+
P
C
+
A
I
+
A
C
g
r
a
p
h
n
o
M
S
T
C
c
l
a
s
s
n
o
r
e
r
a
n
k
r
e
r
a
n
k
P
e
r
c
e
p
t
r
o
n
,
M
E
c
h
e
D
+
P
I
+
P
C
+
A
I
C
g
r
a
p
h
s
t
a
c
k
i
n
g
M
S
T
C
L
/
E
c
l
a
s
s
n
o
I
L
P
n
o
M
E
c
i
a
r
a
m
i
t
a
D
+
P
I
C
+
A
I
C
t
r
a
n
s
v
o
t
i
n
g
,
g
r
e
e
d
y
c
l
a
s
s
n
o
r
e
r
a
n
k
n
o
S
V
M
,
M
E
,
s
t
a
c
k
i
n
g
P
e
r
c
e
p
t
r
o
n
z
h
a
o
D
+
A
I
C
+
P
I
+
P
C
t
r
a
n
s
n
o
g
r
e
e
d
y
c
l
a
s
s
n
o
g
r
e
e
d
y
n
o
M
E
y
u
r
e
t
D
+
(
P
I
C
+
A
I
+
A
C
|
|
g
r
a
p
h
n
o
M
S
T
E
c
l
a
s
s
,
v
o
t
i
n
g
g
r
e
e
d
y
n
o
M
L
E
,
P
I
C
+
A
I
C
)
g
e
n
e
r
a
t
i
v
e
M
B
L
s
a
m
u
e
l
s
s
o
n
D
+
P
I
+
t
r
a
n
s
M
S
T
C
L
/
E
g
r
e
e
d
y
c
l
a
s
s
,
v
o
t
i
n
g
g
r
e
e
d
y
u
n
i
fi
e
d
S
V
M
(
A
I
+
A
C
|
|
D
A
I
C
)
+
P
C
b
l
e
n
d
i
n
g
t
r
a
n
s
l
a
b
e
l
s
z
h
a
n
g
D
+
P
I
+
A
I
+
A
C
+
P
C
g
r
a
p
h
,
m
e
t
a
-
M
S
T
C
L
/
E
,
c
l
a
s
s
n
o
g
r
e
e
d
y
n
o
S
V
M
,
M
E
t
r
a
n
s
l
e
a
r
n
i
n
g
g
r
e
e
d
y
h
e
n
d
e
r
s
o
n
D
P
A
I
C
+
D
g
e
n
e
r
a
t
i
v
e
,
n
o
b
e
a
m
t
r
a
n
s
n
o
b
e
a
m
s
y
n
c
h
r
o
n
i
z
e
d
I
S
B
N
t
r
a
n
s
s
e
a
r
c
h
s
e
a
r
c
h
d
e
r
i
v
a
t
i
o
n
w
a
t
a
n
a
b
e
D
I
+
D
C
+
P
I
+
P
C
+
A
I
+
A
C
r
e
l
a
t
i
v
e
p
r
e
f
e
r
e
n
c
e
n
o
g
r
e
e
d
y
t
o
u
r
n
a
m
e
n
t
c
l
a
s
s
n
o
n
o
n
o
S
V
M
,
m
o
d
e
l
m
o
d
e
l
,
V
i
t
e
r
b
i
C
R
F
,
M
B
L
m
o
r
a
n
t
e
D
+
P
I
+
A
I
+
A
C
t
r
a
n
s
n
o
g
r
e
e
d
y
c
l
a
s
s
v
o
t
i
n
g
g
r
e
e
d
y
n
o
S
V
M
,
M
B
L
l
i
D
+
P
I
C
+
A
I
C
g
r
a
p
h
n
o
M
S
T
C
L
/
E
c
l
a
s
s
v
o
t
i
n
g
g
r
e
e
d
y
n
o
M
E
c
h
e
n
D
+
P
I
+
P
C
+
A
I
C
t
r
a
n
s
n
o
p
r
o
b
c
l
a
s
s
n
o
p
r
o
b
g
l
o
b
a
l
p
r
o
b
a
b
i
l
i
t
y
M
E
o
p
t
i
m
i
z
a
t
i
o
n
l
e
e
D
+
P
I
+
A
I
C
+
P
C
t
r
a
n
s
n
o
g
r
e
e
d
y
c
l
a
s
s
n
o
p
r
o
b
n
o
S
V
M
,
M
E
s
u
n
D
I
+
P
I
+
D
C
A
I
+
A
C
g
r
a
p
h
n
o
M
S
T
E
,
g
r
a
p
h
n
o
V
i
t
e
r
b
i
,
M
E
M
M
,
M
E
V
i
t
e
r
b
i
I
L
P
V
i
t
e
r
b
i
l
l
u
i
s
D
+
P
I
+
D
A
I
C
+
P
C
g
r
a
p
h
n
o
M
S
T
E
g
r
a
p
h
n
o
M
S
T
E
M
S
T
E
P
e
r
c
e
p
t
r
o
n
,
S
V
M
n
e
u
m
a
n
n
D
+
P
I
+
P
C
+
A
I
+
A
C
t
r
a
n
s
n
o
g
r
e
e
d
y
c
l
a
s
s
n
o
n
o
n
o
M
E
o
p
e
n
v
i
c
k
r
e
y
A
I
+
A
C
+
P
I
+
P
C
?
?
?
s
e
n
t
e
n
c
e
n
o
g
r
e
e
d
y
?
M
E
s
i
m
p
l
i
fi
c
a
t
i
o
n
,
c
l
a
s
s
r
i
e
d
e
l
P
A
I
C
?
?
?
M
a
r
k
o
v
n
o
C
u
t
t
i
n
g
?
M
I
R
A
L
o
g
i
c
P
l
a
n
e
N
e
t
w
o
r
k
w
a
n
g
P
I
+
A
I
C
t
r
a
n
s
,
n
o
g
r
e
e
d
y
,
c
l
a
s
s
n
o
p
r
o
b
n
o
S
V
M
,
M
E
g
r
a
p
h
M
S
T
C
L
/
E
M
I
R
A
T
a
b
l
e
1
2
:
S
u
m
m
a
r
y
o
f
s
y
s
t
e
m
a
r
c
h
i
t
e
c
t
u
r
e
s
t
h
a
t
p
a
r
t
i
c
i
p
a
t
e
d
i
n
t
h
e
c
l
o
s
e
d
a
n
d
o
p
e
n
c
h
a
l
l
e
n
g
e
s
.
T
h
e
c
l
o
s
e
d
-
c
h
a
l
l
e
n
g
e
s
y
s
t
e
m
s
a
r
e
s
o
r
t
e
d
b
y
m
a
c
r
o
l
a
b
e
l
e
d
F
1
s
c
o
r
e
o
n
t
h
e
W
S
J
+
B
r
o
w
n
c
o
r
p
u
s
.
B
e
c
a
u
s
e
s
o
m
e
o
p
e
n
-
c
h
a
l
l
e
n
g
e
s
y
s
t
e
m
s
d
i
d
n
o
t
i
m
p
l
e
m
e
n
t
s
y
n
t
a
c
t
i
c
p
a
r
s
i
n
g
,
t
h
e
s
e
s
y
s
t
e
m
s
a
r
e
s
o
r
t
e
d
b
y
l
a
b
e
l
e
d
F
1
s
c
o
r
e
o
f
t
h
e
s
e
m
a
n
t
i
c
d
e
p
e
n
d
e
n
c
i
e
s
o
n
t
h
e
W
S
J
+
B
r
o
w
n
c
o
r
p
u
s
.
O
n
l
y
t
h
e
s
y
s
t
e
m
s
t
h
a
t
h
a
v
e
a
c
o
r
r
e
s
p
o
n
d
i
n
g
p
a
p
e
r
i
n
t
h
e
p
r
o
c
e
e
d
i
n
g
s
a
r
e
i
n
c
l
u
d
e
d
.
S
y
s
t
e
m
s
t
h
a
t
p
a
r
t
i
c
i
p
a
t
e
d
i
n
b
o
t
h
c
h
a
l
l
e
n
g
e
s
a
r
e
l
i
s
t
e
d
o
n
l
y
i
n
t
h
e
c
l
o
s
e
d
c
h
a
l
l
e
n
g
e
.
A
c
r
o
n
y
m
s
u
s
e
d
:
D
-
s
y
n
t
a
c
t
i
c
d
e
p
e
n
d
e
n
c
i
e
s
,
P
-
p
r
e
d
i
c
a
t
e
,
A
-
a
r
g
u
m
e
n
t
,
I
-
i
d
e
n
t
i
fi
c
a
t
i
o
n
,
C
-
c
l
a
s
s
i
fi
c
a
t
i
o
n
.
O
v
e
r
a
l
l
a
r
c
h
.
s
t
a
n
d
s
f
o
r
t
h
e
c
o
m
p
l
e
t
e
s
y
s
t
e
m
a
r
c
h
i
t
e
c
t
u
r
e
;
D
A
r
c
h
.
s
t
a
n
d
s
f
o
r
t
h
e
a
r
c
h
i
t
e
c
t
u
r
e
o
f
t
h
e
s
y
n
t
a
c
t
i
c
p
a
r
s
e
r
;
D
C
o
m
b
.
i
n
d
i
c
a
t
e
s
i
f
t
h
e
fi
n
a
l
p
a
r
s
e
r
o
u
t
p
u
t
w
a
s
g
e
n
e
r
a
t
e
d
u
s
i
n
g
p
a
r
s
e
r
c
o
m
b
i
n
a
t
i
o
n
;
D
I
n
f
e
r
e
n
c
e
s
t
a
n
d
s
f
o
r
t
h
e
t
y
p
e
o
f
i
n
f
e
r
e
n
c
e
u
s
e
d
f
o
r
s
y
n
t
a
c
t
i
c
p
a
r
s
i
n
g
;
P
A
A
r
c
h
.
s
t
a
n
d
s
t
h
e
t
y
p
e
o
f
a
r
c
h
i
t
e
c
t
u
r
e
u
s
e
d
f
o
r
P
A
I
C
;
P
A
C
o
m
b
.
i
n
d
i
c
a
t
e
s
i
f
t
h
e
P
A
o
u
t
p
u
t
w
a
s
g
e
n
e
r
a
t
e
d
t
h
r
o
u
g
h
s
y
s
t
e
m
c
o
m
b
i
n
a
t
i
o
n
;
P
A
I
n
f
e
r
e
n
c
e
s
t
a
n
d
s
f
o
r
t
h
e
t
h
e
t
y
p
e
o
f
i
n
f
e
r
e
n
c
e
u
s
e
d
f
o
r
P
A
I
C
;
J
o
i
n
t
L
e
a
r
n
i
n
g
/
O
p
t
.
i
n
d
i
c
a
t
e
s
i
f
s
o
m
e
f
o
r
m
o
f
j
o
i
n
t
l
e
a
r
n
i
n
g
o
r
o
p
t
i
m
i
z
a
t
i
o
n
w
a
s
i
m
p
l
e
m
e
n
t
e
d
f
o
r
t
h
e
s
y
n
t
a
c
t
i
c
+
s
e
m
a
n
t
i
c
g
l
o
b
a
l
t
a
s
k
;
M
L
m
e
t
h
o
d
s
l
i
s
t
s
t
h
e
M
L
m
e
t
h
o
d
s
u
s
e
d
t
h
r
o
u
g
h
o
u
t
t
h
e
c
o
m
p
l
e
t
e
s
y
s
t
e
m
.
172
Exact Match Perfect Proposition F
1
(complete task) (semantic dependencies)
closed WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown
johansson 12.46 (1) 12.46 12.68 54.12 (1) 56.12 36.90
che 10.37 (2) 10.21 11.50 48.05 (2) 50.15 30.90
ciaramita 9.27 (3) 9.04 10.80 46.05 (3) 48.05 28.61
zhao 9.20 (4) 9.00 10.56 43.19 (4) 45.23 26.14
henderson 8.11 (5) 7.75 10.33 39.24 (5) 40.64 27.51
watanabe 7.79 (6) 7.54 9.39 36.44 (6) 38.09 22.72
yuret 7.65 (7) 7.33 9.62 34.61 (9) 36.13 21.78
zhang 7.40 (8) 7.46 7.28 34.96 (8) 36.25 24.22
li 7.12 (9) 6.71 9.62 32.08 (10) 33.45 20.62
samuelsson 6.94 (10) 6.62 8.92 35.20 (7) 36.96 20.22
chen 6.83 (11) 6.46 9.15 31.02 (12) 32.08 22.14
lee 6.69 (12) 6.29 9.15 31.40 (11) 32.52 22.18
morante 6.44 (13) 6.04 8.92 30.41 (14) 31.97 17.49
sun 5.38 (14) 4.96 7.98 30.43 (13) 31.51 21.40
baldridge 5.24 (15) 4.92 7.28 25.35 (15) 26.57 15.26
choi 3.33 (16) 3.50 2.58 24.77 (16) 25.71 17.37
trandabat 3.26 (17) 3.08 4.46 6.59 (18) 6.81 4.76
lluis 2.55 (18) 1.96 6.10 16.07 (17) 16.46 13.00
neumann 0.11 (19) 0.12 0.23 0.30 (19) 0.31 0.20
open
vickrey ? ? ? 44.94 (1) 46.68 30.28
riedel ? ? ? 42.77 (2) 44.18 31.15
zhang 8.14 (1) 8.04 8.92 35.46 (3) 36.74 24.84
li 6.90 (2) 6.46 9.62 29.91 (4) 31.30 18.41
wang 5.17 (3) 5.12 5.63 18.63 (5) 20.31 7.09
Table 13: Exact Match and Perfect Proposition F
1
scores for runs submitted in the closed and open
challenges. The closed-challenge systems are sorted in descending order of Exact Match scores on
the WSJ+Brown corpus. Open-challenge submissions are sorted in descending order of the Perfect
Proposition F
1
score. The number in parentheses next to the WSJ+Brown scores indicates the system
rank according to the corresponding scoring measure.
6.1 Exact Match and Perfect Propositions
Table 13 lists the Exact Match and Perfect Propo-
sition F
1
scores for test runs submitted in both
challenges. Both these scores measure the capac-
ity of a system to correctly parse structures with
granularity much larger than a simple dependency,
i.e., entire sentences for Exact Match and complete
propositions for Perfect Proposition F
1
(see Sec-
tion 2.2.2 for a formal definition of these evalua-
tion measures). The table indicates that these val-
ues are much smaller than the scores previously
reported, e.g., labeled macro F
1
. This is to be
expected: the probability of an incorrectly parsed
unit (sentence or proposition) is much larger given
its granularity. However, the main purpose of this
analysis is to investigate if systems that focused
on joint learning or optimization performed bet-
ter than others with respect to these global mea-
sures. This indeed seems to be the case for at
least two systems. The system of Johansson and
Nugues (2008), which jointly optimizes the la-
beled F
1
score (for semantic dependencies) and
then the labeled macro F
1
score (for the complete
task), increases its distance from the next ranked
system: its Perfect Proposition F
1
score is over
6 points higher than the score of the second sys-
tem in Table 13. The system of Henderson et
al. (2008), which was designed for joint learning
of the complete task, improves its rank from eighth
to fifth compared to the official results (Table 10).
6.2 Nonprojectivity
Table 14 shows the unlabeled F1 scores for pre-
diction of nonprojective syntactic dependencies.
Since nonprojectivity is quite rare, many teams
chose to ignore this issue. The table shows only
those systems that submitted well-formed depen-
dency trees, and whose output contained at least
one nonprojective link. The small number of non-
projective links in the training set makes it hard to
learn to predict such links, and this is also reflected
in the figures. In general, the figures for nonpro-
jective wh-movements and split clauses are higher,
and they are also the most common types. Also,
they are detectable by fairly simple patterns, such
as the presence of a wh-word or a pair of commas.
173
System All wh-mov. SpCl SpNP
choi 25.43 49.49 45.47 8.72
lee 46.26 50.30 64.84 20.69
nugues 46.15 58.96 59.26 11.32
samuelsson 24.47 38.15 0 9.83
titov 42.32 50.56 48.71 0
zhang 13.39 5.71 12.33 7.3
Table 14: Unlabeled F1-measures for nonprojec-
tive links. Results are given for all links, wh-
movements, split clauses, and split noun phrases.
6.3 Normalized SRL Performance
Table 6.3 lists the scores for the semantic sub-
task measured as the ratio of the labeled F
1
score
and LAS. As previously mentioned, this score es-
timates the performance of the SRL component
independent of the performance of the syntactic
parser. This analysis is not a substitute for the
actual experiment where the SRL components are
evaluated using correct syntactic information but,
nevertheless, it indicates several interesting facts.
First, the ranking of the top three systems in Ta-
ble 10 changes: the system of Che et al (2008)
is now ranked first, and the system of Johansson
and Nugues (2008) is second. This shows that Che
et al have a relatively stronger SRL component,
whereas Johansson and Nugues developed a bet-
ter parser. Second, several other systems improved
their ranking compared to Table 10: e.g., chen
from position thirteenth to ninth and choi from six-
teenth to eighth. This indicates that these systems
were penalized in the official ranking mainly due
to the relative poor performance of their parsers.
Note that this experiment is relevant only for
systems that implemented pipeline architectures,
where the semantic components are in fact sep-
arated from the syntactic ones; this excludes the
systems that blended syntax with SRL: henderson,
sun, and lluis. Furthermore, systems that had sig-
nificantly lower scores in syntax will receive an un-
reasonable boost in ranking according to this mea-
sure. Fortunately, there was only one such outlier
in this evaluation (neumann), shown in gray in the
table.
6.4 PropBank versus NomBank
Table 16 lists the labeled F
1
scores for semantic
dependencies for two different views of the test-
ing data sets: for propositions centered around ver-
bal predicates, i.e., from PropBank, and for propo-
sitions centered around nominal predicates, i.e.,
from NomBank.
Labeled F
1
/ LAS
closed WSJ+Brown WSJ Brown
neumann 137.60 (1) 140.94 108.93
che 90.51 (2) 91.42 82.21
johansson 89.98 (3) 90.70 83.40
ciaramita 89.49 (4) 90.32 81.89
zhao 87.88 (5) 88.75 79.78
yuret 84.35 (6) 85.30 75.34
samuelsson 84.20 (7) 85.24 74.51
choi 83.52 (8) 83.63 82.64
chen 82.22 (9) 82.89 76.11
morante 81.92 (10) 82.73 74.43
zhang 81.67 (11) 82.45 74.46
henderson 81.66 (12) 82.32 75.47
watanabe 81.26 (13) 82.18 72.61
lee 81.01 (14) 81.63 75.33
li 80.69 (15) 81.53 73.23
baldridge 78.37 (16) 79.33 69.38
sun 77.68 (17) 78.29 72.15
lluis 75.77 (18) 76.20 72.24
trandabat 47.68 (19) 48.12 43.85
open
zhang 82.33 82.91 76.87
li 79.58 80.44 72.05
wang 79.38 82.35 55.37
Table 15: Ratio of the labeled F
1
score for seman-
tic dependencies and LAS for syntactic dependen-
cies. Systems are sorted in descending order of this
ratio score on the WSJ+Brown corpus. We only
show systems that participated in both the syntac-
tic and semantic subtasks.
The table indicates that, generally, systems per-
formed much worse on nominal predicates than
on verbal predicates. This is to be expected con-
sidering that there is significant body of previ-
ous work that analyzes the SRL problem on Prop-
Bank, but minimal work for NomBank. On aver-
age, the difference between the labeled F
1
scores
for verbal predicates and nominal predicates on the
WSJ+Brown corpus is 7.84 points. Furthermore,
the average difference between labeled F
1
scores
on the Brown corpus alone is 12.36 points. This in-
dicates that the problem of SRL for nominal predi-
cates is more sensitive to domain changes than the
equivalent problem for verbal predicates. Our con-
jecture is that, because there is very little syntac-
tic structure between nominal predicates and their
arguments, SRL models for nominal predicates se-
lect mainly lexical features, which are more brittle
than syntactic or other non-lexicalized features.
Remarkably, there is one system (baldridge)
which performed better on the WSJ+Brown for
nominal predicates than verbal predicates. Un-
fortunately, this group did not submit a system-
description paper so it is not clear what was their
approach.
174
Labeled F
1
Labeled F
1
(verbal predicates) (nominal predicates)
closed WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown
johansson 84.45 (1) 86.37 71.87 74.32 (2) 75.42 60.13
che 80.46 (2) 82.17 69.33 75.18 (1) 76.64 56.87
ciaramita 80.15 (3) 82.09 67.62 73.17 (4) 74.42 57.69
zhao 77.67 (4) 79.40 66.38 73.28 (3) 74.69 54.81
samuelsson 76.17 (5) 78.03 64.00 68.13 (7) 69.58 49.24
yuret 75.91 (6) 77.88 63.02 68.81 (5) 69.98 53.58
zhang 74.82 (7) 76.62 63.15 65.61 (11) 66.82 50.18
li 74.36 (8) 76.14 62.92 62.61 (14) 63.76 47.09
henderson 73.80 (9) 75.40 63.36 66.26 (10) 67.44 50.73
watanabe 73.06 (10) 75.02 60.34 67.15 (8) 68.37 50.92
sun 72.97 (11) 74.45 63.50 58.68 (15) 59.73 45.75
morante 72.81 (12) 74.36 62.72 66.50 (9) 67.92 47.97
lee 72.34 (13) 74.15 60.49 62.83 (13) 63.66 52.18
chen 72.02 (14) 73.49 62.46 65.02 (12) 66.14 50.48
choi 70.00 (15) 71.28 61.71 56.16 (16) 57.19 44.05
baldridge 67.02 (16) 68.64 56.50 68.57 (6) 69.78 52.96
lluis 62.42 (17) 63.49 55.49 42.15 (17) 42.81 34.22
trandabat 42.88 (18) 43.79 37.06 37.14 (18) 37.89 27.50
neumann 22.87 (19) 23.53 18.24 21.7 (19) 22.04 17.14
open
vickrey 78.41 (1) 79.75 69.57 71.86 (1) 73.29 53.25
riedel 77.13 (2) 78.72 66.75 70.25 (2) 71.03 60.17
zhang 75.00 (3) 76.62 64.44 66.76 (3) 67.79 53.76
li 73.74 (4) 75.57 62.05 61.24 (5) 62.38 46.36
wang 67.50 (5) 70.34 49.72 66.53 (4) 69.83 28.96
Table 16: Labeled F
1
scores for frames centered around verbal and nominal predicates. The number in
parentheses next to the WSJ+Brown scores indicates the system rank in the corresponding data set.
Systems can mitigate the inherent differences
between verbal and nominal predicates with dif-
ferent models for the two sub-problems. This was
indeed the approach taken by two out of the top
three systems (johansson and che). Johansson and
Nugues (2008) developed different models for ver-
bal and nominal predicates and implemented sep-
arate feature selection processes for each model.
Che et al (2008) followed the same method but
they also implemented separate domain constraints
for inference for the two models.
7 Conclusion
The previous four CoNLL shared tasks popular-
ized and, without a doubt, boosted research in se-
mantic role labeling and dependency parsing. This
year?s shared task introduces a new task that es-
sentially unifies the problems addressed in the past
four years under a unique, dependency-based for-
malism. This novel task is attractive both from
a research perspective and an application-oriented
perspective:
? We believe that the proposed dependency-
based representation is a better fit for many
applications (e.g., Information Retrieval, In-
formation Extraction) where it is often suffi-
cient to identify the dependency between the
predicate and the head of the argument con-
stituent rather than extracting the complete ar-
gument constituent.
? It was shown that the extraction of syntac-
tic and semantic dependencies can be per-
formed with state-of-the-art performance in
linear time (Ciaramita et al, 2008). This can
give a significant boost to the adoption of this
technology in real-world applications.
? We hope that this shared task will motivate
several important research directions. For ex-
ample, is the dependency-based representa-
tion better for SRL than the constituent-based
formalism? Does joint learning improve syn-
tactic and semantic analysis?
? Surface (string related patterns, syntax, etc.)
linguistic features can often be detected with
greater reliability than deep (semantic) fea-
tures. In contrast, deep features can cover
more ground because they regularize across
differences in surface strings. Machine learn-
ing systems can be more effective by using
evidence from both deep and surface features
jointly (Zhao, 2005).
175
Even though this shared task was more complex
than the previous shared tasks, 22 different teams
submitted results in at least one of the challenges.
Building on this success, we hope to expand this
effort in the future with evaluations on multiple
languages and on larger out-of-domain corpora.
Acknowledgments
We want to thank the following people who helped
us with the generation of the data sets: Jes?us
Gim?enez, for generating the predicted POS tags
with his SVMTool POS tagger, and Massimiliano
Ciaramita, for generating columns 1, 2 and 3 in the
open-challenge corpus with his semantic tagger.
We also thank the following people who helped
us with the organization of the shared task: Paola
Merlo and James Henderson for the idea and the
implementation of the Exact Match measure, Se-
bastian Riedel for his dependency visualization
software,
12
Hai Zhao, for the the idea of the F
1
ratio score, and Carlos Castillo, for help with the
shared task website. Last but not least, we thank
the organizers of the previous four shared tasks:
Sabine Buchholz, Xavier Carreras, Ryan McDon-
ald, Amit Dubey, Johan Hall, Yuval Krymolowski,
Sandra K?ubler, Erwin Marsi, Jens Nilsson, Sebas-
tian Riedel, and Deniz Yuret. This shared task
would not have been possible without their previ-
ous effort.
Mihai Surdeanu is a research fellow in the
Ram?on y Cajal program of the Spanish Ministry of
Science and Technology. Richard Johansson was
funded by the Swedish National Graduate School
of Language Technology (GSLT). Adam Meyers?
participation was supported by the National Sci-
ence Foundation, award CNS-0551615 (Towards
a Comprehensive Linguistic Annotation of Lan-
guage) and IIS-0534700 (Collaborative Research:
Structure Alignment-based Machine Translation).
Llu??s M`arquez?s participation was supported by
the Spanish Ministry of Education and Science,
through research projects Trangram (TIN2004-
07925-C03-02) and OpenMT (TIN2006-15307-
C03-02).
References
X. Carreras. 2007. Experiments with a Higher-Order
Projective Dependency Parser. In Proc. of CoNLL-
2007 Shared Task.
12
http://code.google.com/p/whatswrong/
X. Carreras and L. M`arquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling.
In Proc. of CoNLL-2005.
X. Carreras and L. M`arquez. 2004. Introduction to the
CoNLL-2004 Shared Task: Semantic Role Labeling.
In Proc. of CoNLL-2004.
W. Che, Z. Li, Y. Hu, Y. Li, B. Qin, T. Liu and S.
Li. 2008. A Cascaded Syntactic and Semantic De-
pendency Parsing System. In Proc. of CoNLL-2008
Shared Task.
E. Chen, L. Shi and D. Hu. 2008. Probabilistic Model
for Syntactic and Semantic Dependency Parsing. In
Proc. of CoNLL-2008 Shared Task.
Chinchor, N. and P. Robinson. 1998. MUC-7
Named Entity Task Definition. In Proc. of Seventh
Message Understanding Conference (MUC-7).
http://www.itl.nist.gov/iaui/894.02/related projects/
/muc/proceedings/muc 7 toc.html.
Chomsky, Noam. 1981. Lectures on Government and
Binding. Foris Publications, Dordrecht.
Y.J. Chu and T.H. Liu. 1965. On the Shortest Ar-
borescence of a Directed Graph. In Science Sinica,
14:1396-1400.
M. Ciaramita, G. Attardi, F. Dell?Orletta, and M. Sur-
deanu. 2008. DeSRL: A Linear-Time Semantic
Role Labeling System. In Proc. of CoNLL-2008
Shared Task.
M. Ciaramita and Y. Altun. 2006. Broad Coverage
Sense Disambiguation and Information Extraction
with a Supersense Sequence Tagger. In Proc. of
EMNLP.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
J. Edmonds. 1967. Optimum Branchings. In Jour-
nal of Research of the National Bureau of Standards,
71B:233?240.
J. Eisner. 2000. Bilexical Grammars and Their Cubic-
Time Parsing Algorithms. New Developments in
Parsing Algorithms, Kluwer Academic Publishers.
W. N. Francis and H. Ku?cera. 1964. Brown Corpus.
Manual of Information to accompany A Standard
Corpus of Present-Day Edited American English, for
use with Digital Computers. Revised 1971, Revised
and Amplified 1979.
C. Fellbaum, editor. 1998. WordNet: An electronic
lexical database. MIT Press.
J. Gim?enez and L. M`arquez. 2004. SVMTool: A gen-
eral POS tagger generator based on Support Vector
Machines. In Proc. of LREC.
K. Hacioglu. 2004. Semantic Role Labeling Using De-
pendency Trees. In Proc. of COLING-2004.
176
J. Henderson, P. Merlo, G. Musillo and I. Titov. 2008.
A Latent Variable Model of Synchronous Parsing for
Syntactic and Semantic Dependencies. In Proc. of
CoNLL-2008 Shared Task.
R. Johansson and P. Nugues. 2008. Dependency-
based Syntactic?Semantic Analysis with PropBank
and NomBank. In Proc. of CoNLL-2008 Shared
Task.
R. Johansson and P. Nugues. 2007. Extended
Constituent-to-Dependency Conversion for English.
In Proc. of NODALIDA.
X. Llu??s and L. M`arquez. 2008. A Joint Model for
Parsing Syntactic and Semantic Dependencies. In
Proc. of CoNLL-2008 Shared Task.
D. Magerman. 1994. Natural Language Parsing as Sta-
tistical Pattern Recognition. Ph.D. thesis, Stanford
University.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a Large Annotated Corpus of En-
glish: the Penn Treebank. Computational Linguis-
tics, 19.
R. McDonald, F. Pereira, K. Ribarov and J. Hajic.
2005. Non-Projective Dependency Parsing using
Spanning Tree Algorithms In Proc. of HLT-EMNLP.
A. Meyers, R. Grishman, M. Kosaka, and S. Zhao.
2001. Covering Treebanks with GLARF. In Proc.
of the ACL/EACL 2001 Workshop on Sharing Tools
and Resources for Research and Education.
Meyers, A., R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The NomBank Project: An Interim Report. In
NAACL/HLT 2004 Workshop Frontiers in Corpus
Annotation, Boston.
J. Nivre, J. Hall, J. Nilsson and G. Eryigit. 2006. La-
beled Pseudo-Projective Dependency Parsing with
Support Vector Machines. In Proc. of CoNLL-X
Shared Task.
J. Nivre, J. Hall, S. K?ubler, R. McDonald, J. Nilsson,
S. Riedel, D. Yuret. 2007. The CoNLL 2007 Shared
Task on Dependency Parsing. In Proc. of CoNLL-
2007.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryi?git, S.
K?ubler, S. Marinov, and E. Marsi. 2007b. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(2):95?135.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Seman-
tic Roles. Computational Linguistics, 31(1).
S. Riedel and I. Meza-Ruiz. 2008. Collective Seman-
tic Role Labelling with Markov Logic. In Proc. of
CoNLL-2008 Shared Task.
Y. Samuelsson, O. T?ackstr?om, S. Velupillai, J. Eklund,
M. Fishel and M. Saers. 2008. Mixing and Blending
Syntactic and Semantic Dependencies. In Proc. of
CoNLL-2008 Shared Task.
W. Sun, H. Li and Z. Sui. 2008. The Integration of De-
pendency Relation Classification and Semantic Role
Labeling Using Bilayer Maximum Entropy Markov
Models. In Proc. of CoNLL-2008 Shared Task.
E. F. Tjong Kim San and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 Shared Task: Language-
Independent Named Entity Recognition. In Proc. of
CoNLL-2003.
D. Vickrey and D. Koller. 2008. Applying Sentence
Simplification to the CoNLL-2008 Shared Task. In
Proc. of CoNLL-2008 Shared Task.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Technical report,
Linguistic Data Consortium.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines.
In Proc. of IWPT.
Y. Zhang, R. Wang and H. Uszkoreit. 2008. Hybrid
Learning of Dependency Structures from Heteroge-
neous Linguistic Resources. In Proc. of CoNLL-
2008 Shared Task.
Zhao, S. 2005. Information Extraction from Multiple
Syntactic Sources. Ph.D. thesis, NYU.
177
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 258?262
Manchester, August 2008
DeSRL: A Linear-Time Semantic Role Labeling System
Massimiliano Ciaramita
??
massi@yahoo-inc.com
Felice Dell?Orletta
?
dellorle@di.unipi.it
?: Yahoo! Research Barcelona, Ocata 1, 08003, Barcelona, Catalunya, Spain
?: Dipartimento di Informatica, Universit`a di Pisa, L. B. Pontecorvo 3, I-56127, Pisa, Italy
?: Barcelona Media Innovation Center, Ocata 1, 08003, Barcelona, Catalunya, Spain
Giuseppe Attardi
?
attardi@di.unipi.it
Mihai Surdeanu
?,?
mihai.surdeanu@barcelonamedia.org
Abstract
This paper describes the DeSRL sys-
tem, a joined effort of Yahoo! Research
Barcelona and Universit`a di Pisa for the
CoNLL-2008 Shared Task (Surdeanu et
al., 2008). The system is characterized by
an efficient pipeline of linear complexity
components, each carrying out a different
sub-task. Classifier errors and ambigui-
ties are addressed with several strategies:
revision models, voting, and reranking.
The system participated in the closed chal-
lenge ranking third in the complete prob-
lem evaluation with the following scores:
82.06 labeled macro F1 for the overall task,
86.6 labeled attachment for syntactic de-
pendencies, and 77.5 labeled F1 for se-
mantic dependencies.
1 System description
DeSRL is implemented as a sequence of compo-
nents of linear complexity relative to the sentence
length. We decompose the problem into three sub-
tasks: parsing, predicate identification and clas-
sification (PIC), and argument identification and
classification (AIC). We address each of these sub-
tasks with separate components without backward
feedback between sub-tasks. However, the use of
multiple parsers at the beginning of the process,
and re-ranking at the end, contribute beneficial
stochastic aspects to the system. Figure 1 summa-
rizes the system architecture. We detail the parsing
?
All authors contributed equally to this work.
?
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
sub-task in Section 2 and the semantic sub-tasks
(PIC and AIC) in Section 3.
2 Parsing
In the parsing sub-task we use a combination strat-
egy on top of three individual parsing models,
two developed in-house ?DeSR
left?to?right
and
DeSR
revision
right?to?left
? and a third using an off-the-
shelf parser, Malt 1.0.0
1
.
2.1 DeSR
left?to?right
This model is a version of DeSR (Attardi, 2006),
a deterministic classifier-based Shift/Reduce
parser. The parser processes input tokens advanc-
ing on the input from left to right with Shift ac-
tions and accumulates processed tokens on a stack
with Reduce actions. The parser has been adapted
for this year?s shared task and extended with addi-
tional classifiers, e.g., Multi Layer Perceptron and
multiple SVMs.
2
The parser uses the following features:
1. SPLIT LEMMA: from tokens ?1, 0, 1, prev(0),
leftChild(0), rightChild(0)
2. PPOSS: from ?2, ?1, 0, 1, 2, 3, prev(0), next(?1),
leftChild(?1), leftChild(0), rightChild(?1),
rightChild(0)
3. DEPREL: from leftChild(?1), leftChild(0),
rightChild(?1)
4. HDIST: from ?1, 0
In the above list negative numbers refer to tokens
on the stack, positive numbers to tokens in the in-
put queue. We use the following path operators:
leftChild(x) refers to the leftmost child of token
x, rightChild(x) to the rightmost child of token
x, prev(x) and next(x) respectively to the token
preceding or following x in the sentence.
1
http://w3.msi.vxu.se/
?
nivre/research/
MaltParser.html
2
This parser is available for download at: http://
sourceforge.net/projects/desr/.
258
Voting PIC AIC Reranking
Argument
Frame
DeSR left?to?right
Malt
DeSR right?to?leftrevision OutputInput
Figure 1: DeSRL system architecture.
The first three types of features are directly ex-
tracted from the attributes of tokens present in the
training corpus. The fourth feature represents the
distance of the token to the head of the noun phrase
to which it belongs, or ?O? if it does not belong to
a noun phrase. This distance is computed with a
simple heuristic, based on a pattern of POS tags.
Attardi and Dell?Orletta (2008) have shown that
this feature improves the accuracy of a shift/reduce
dependency parser by providing approximate in-
formation about NP chunks in the sentence. In fact
no token besides the head of a noun phrase can
have a head referring to a token outside the noun
phrase. Hence the parser can learn to avoid creat-
ing such links. The addition of this feature yields
an increase of 0.80% in Labeled Accuracy on the
development set.
2.2 Revision Parser: DeSR
revision
right?to?left
Our second individual parsing model implements
an alternative to the method of revising parse trees
of Attardi and Ciaramita (2007) (see also (Hall &
Novak, 2005)). The original approach consisted in
training a classifier to revise the errors of a base-
line parser. The approach assumed that only lo-
cal revisions to the parse tree would be needed,
since the dependency parser mostly gets individual
phrases correctly. The experiments showed that in-
deed most of the corrections can be expressed by
a small set of (about 20) complex movement rules.
Furthermore, there was evidence that one could get
higher improvements from the tree revision classi-
fier if this was trained on the output of a lower ac-
curacy parser. The reason for this is that the num-
ber of errors is higher and this provides a larger
amount of training data.
For the CoNLL 2008 shared task, we refined this
idea, but instead of using an independent classi-
fier for the revision, we use the parser itself. The
second parser is trained on the original corpus ex-
tended with dependency information predicted by
a lower accuracy parser. To obtain the base parser
we use DeSR trained on half the training corpus
using a Maximum Entropy (ME) classifier. The
ME classifier is considerably faster to train but has
a lower accuracy: this model achieved an LAS of
76.49% on the development set. Using the out-
put of the ME-based parser we extend the original
corpus with four additional columns: the lemma
of the predicted head (PHLEMMA), the PPOSS of
the predicted head (PHPPOSS), the dependency of
the predicted head (PHDEPREL), and the indica-
tion of whether a token appears before or after its
predicted head. A second parser is trained on this
corpus, scanning sentences from right to left and
using the following additional features:
1. PHPPOSS: from ?1, 0
2. PHLEMMA: from ?1, 0
3. PHDEPREL: from ?1, 0
4. PHHDIST: from 0
Performing parsing in reverse order helps reduce
several of the errors that a deterministic parser
makes when dependency links span a long distance
in the input sequence. Experiments on the CoNLL
2007 corpora (Dell?Orletta, 2008) have shown that
this indeed occurs, especially for distances in the
range from 6 to 23. In particular, the most signifi-
cant improvements are for dependencies with label
COORD (+ 6%) and P (+ 8%).
The revision parser achieves an LAS of 85.81%
on the development set. Note that the extra fea-
tures from the forward parser are indeed use-
ful, since a simple backward parser only achieves
82.56% LAS on the development set.
2.3 Parser Combination
The final step consists in combining the out-
puts of the three individual models a simple
voting scheme: for each token we use major-
ity voting to select its head and dependency la-
bel. In case of ties, we chose the dependency
predicted by our overall best individual model
(DeSR
revision
right?to?left
).
3
Note that typical approaches to parser
combination combine the outputs of inde-
pendent parsers, while in our case one base
model (DeSR
revision
right?to?left
) is trained with
3
We tried several voting strategies but none performed bet-
ter.
259
information predicted by another individual
model(DeSR
left?to?right
). To the best of our
knowledge, combining individual parsing models
that are inter-dependent is novel.
3 Semantic Role Labeling
We implement the Semantic Role Labeling (SRL)
problem using three components: PIC, AIC, and
reranking of predicted argument frames.
3.1 Predicate Identification and Classification
The PIC component carries out the identification
of predicates, as well as their partial disambigua-
tion, and it is implemented as a multiclass average
Perceptron classifier (Crammer & Singer, 2003).
For each token i we extract the following features
(?, ? stands for token combination):
1. SPLIT LEMMA: from ?i?1, i?, i?1, i, i+1, ?i, i+1?
2. SPLIT FORM: from i? 2, i? 1, i, i+ 1.i+ 2
3. PPOSS: from ?i?2, i?1?, ?i?1, i?, i?1, i, i+1, ?i, i+
1?, ?i+ 1, i+ 2?
4. WORD SHAPE: e.g., ?Xx*? for ?Brazil?, from ?i?2, i?
1, i?, ?i? 1, i?, i? 1, i, i+1, ?i, i+1?, ?i, i+1, i+2?
5. Number of children of node i
6. For each children j of i: split lemma
j
, pposs
j
,
deprel
i,j
, ?split lemma
i
, split lemma
j
?, ?pposs
i
,
pposs
j
?
7. Difference of positions: j ? i, for each child j of i.
The PIC component uses one single classifier map-
ping tokens to one of 8 classes corresponding to
the rolesets suffixes 1 to 6, the 6 most frequent
types, plus a class grouping all other rolesets, and
a class for non predicates; i.e., Y = {0, 1, 2, .., 7}.
Each token classified as y
7
is mapped by default to
the first sense y
1
. This approach is capable of dis-
tinguishing between different predicates based on
features 1 and 2, but it can also exploit information
that is shared between predicates due to similar
frame structures. The latter property is intuitively
useful especially for low-frequency predicates.
The classifier has an accuracy in the multiclass
problem, considering also the mistakes due to the
non-predicted classes, of 96.2%, and an F-score of
92.7% with respect to the binary predicate iden-
tification problem. To extract features from trees
(5-7) we use our parser?s output on training, devel-
opment and evaluation data.
3.2 Argument Identification and
Classification
Algorithm 1 describes our AIC framework. The al-
gorithm receives as input a sentence S where pred-
icates have been identified and classified using the
Algorithm 1: AIC
input : sentence S; inference strategy I; model w
foreach predicate p in S do
set frame F
in
= {}
foreach token i in S do
if validCandidate(i) then
?
y = arg max
y?Y
score(?(p, i),w, y)
if
?
y 6= nil then
add argument (i,
?
y) to F
in
F
out
= inference(F
in
, I)
output: set of all frames F
out
PIC component, an inference strategy I is used
to guarantee that the generated best frames satisfy
the domain constraints, plus an AIC classification
model w. We learn w using a multiclass Percep-
tron, using as output label setY all argument labels
that appear more than 10 times in training plus a nil
label assigned to all other tokens.
During both training and evaluation we se-
lect only the candidate tokens that pass the
validCandidate filter. This function requires that
the length of the dependency path between pred-
icate and candidate argument be less than 6, the
length of the dependency path between argument
and the first common ancestor be less than 3, and
the length of the dependency path between the
predicate and the first common ancestor be less
than 5. This heuristic covers over 98% of the ar-
guments in training.
In the worst case, Algorithm 1 has quadratic
complexity in the sentence size. But, on average,
the algorithm has linear time complexity because
the number of predicates per sentence is small (av-
eraging less than five for sentences of 25 words).
The function ? generates the feature vector for
a given predicate-argument tuple. ? extracts the
following features from a given tuple of a predicate
p and argument a:
1. token(a)
4
, token(modifier of a) if a is the
head of a prepositional phrase, and token(p).
2. Patterns of PPOSS tags and DEPREL labels
for: (a) the predicate children, (b) the children
of the predicate ancestor across VC and IM
dependencies, and (c) the siblings of the same
ancestor. In all paths we mark the position of
p, a and any of their ancestors.
3. The dependency path between p and a. We
add three versions of this feature: just the
4
token extracts the split lemma, split form, and PPOSS
tag of a given token.
260
path, and the path prefixed with p and a?s
PPOSS tags or split lemmas.
4. Length of the dependency path.
5. Distance in tokens between p and a.
6. Position of a relative to p: before or after.
We implemented two inference strategies:
greedy and reranking. The greedy strategy sorts
all arguments in a frame F
in
in descending order
of their scores and iteratively adds each argument
to the output frame F
out
only if it respects the do-
main constraints with the other arguments already
selected. The only domain constraint we use is that
core arguments cannot repeat.
3.3 Reranking of Argument Frames
The reranking inference strategy adapts the ap-
proach of Toutanova et al (2005) to the depen-
dency representation with notable changes in can-
didate selection, feature set, and learning model.
For candidate selection we modify Algorithm 1:
instead of storing only y? for each argument in F
in
we store the top k best labels. Then, from the ar-
guments in F
in
, we generate the top k frames with
the highest score, where the score of a frame is the
product of all its argument probabilities, computed
as the softmax function on the output of the Per-
ceptron. In this set of candidate frames we mark
the frame with the highest F
1
score as the positive
example and all others as negative examples.
From each frame we extract these features:
1. Position of the frame in the set ordered by
frame scores. Hence, smaller positions in-
dicate candidate frames that the local model
considered better (Marquez et al, 2007).
2. The complete sequence of arguments and
predicate for this frame (Toutanova, 2005).
We add four variants of this feature: just the
sequence and sequence expanded with: (a)
predicate voice, (b) predicate split lemma,
and (c) combination of voice and split lemma.
3. The complete sequence of arguments and
predicate for this frame combined with their
PPOSS tags. Same as above, we add four
variants of this feature.
4. Overlap with the PropBank or NomBank
frame for the same predicate lemma and
sense. We add the precision, recall, and F
1
score of the overlap as features (Marquez et
al., 2007).
5. For each frame argument, we add the features
from the local AIC model prefixed with the
WSJ + Brown WSJ Brown
Labeled macro F
1
82.69 83.83 73.51
LAS 87.37 88.21 80.60
Labeled F
1
78.00 79.43 66.41
Table 1: DeSRL results in the closed challenge,
for the overall task, syntactic dependencies, and
semantic dependencies.
Devel WSJ Brown
DeSR
left?to?right
85.61 86.54 79.74
DeSR
revision
right?to?left
85.81 86.19 78.91
MaltParser 84.10 85.50 77.06
Voting 87.37 88.21 80.60
Table 2: LAS of individual and combined parsers.
corresponding argument label in the current
frame (Toutanova, 2005).
The reranking classifier is implemented as multi-
layer perceptron with one hidden layer of 5 units,
trained to solve a regression problem with a least
square criterion function. Previously we experi-
mented, unsuccessfully, with a multiclass Percep-
tron and a ranking Perceptron. The limited number
of hidden units guarantees a small computational
overhead with respect to a linear model.
4 Results and Analysis
Table 1 shows the overall results of our system
in the closed challenge. Note that these scores
are higher than those of our submitted run mainly
due to improved parsing models (discussed be-
low) whose training ended after the deadline. The
score of the submitted system is the third best
for the complete task. The system throughput in
our best configuration is 28 words/second, or 30
words/second without reranking. In exploratory
experiments on feature selection for the re-ranking
model we found that several features classes do
not contribute anything and could be filtered out
speeding up significantly this last SRL step. Note
however that currently over 90% of the runtime is
occupied by the syntactic parsers? SVM classifiers.
We estimate that we can increase throughput one
order of magnitude simply by switching to a faster,
multiclass classifier in parsing.
4.1 Analysis of Parsing
Table 2 lists the labeled attachment scores (LAS)
achieved by each parser and by their combination
on the development set, the WSJ and Brown test
sets. The results are improved with respect to the
official run, by using a revision parser trained on
the output of the lower accuracy ME parser, as
261
Labeled F
1
Unlabeled F
1
Syntax PIC Inference Devel WSJ Brown Devel WSJ Brown
gold gold greedy 88.95 90.21 84.95 93.71 94.34 93.29
predicted gold greedy 85.96 86.70 78.68 90.60 90.98 88.02
predicted predicted greedy 79.88 79.27 66.41 86.07 85.33 80.14
predicted predicted reranking 80.13 79.43 66.41 86.33 85.62 80.41
Table 3: Scores of the SRL component under various configurations.
Devel WSJ Brown
Unlabeled F
1
92.69 90.88 86.96
Labeled F
1
(PIC) 87.29 84.87 71.99
Labeled F
1
(Sense 1) 79.62 78.94 70.11
Table 4: Scores of the PIC component.
mentioned earlier. These results show that vot-
ing helps significantly (+1.56% over the best single
parser) even though inter-dependent models were
used. However, our simple voting scheme does
not guarantee that a well-formed tree is generated,
leaving room for further improvements; e.g., as
in (Sagae & Lavie, 2006).
4.2 Analysis of SRL
Table 3 shows the labeled and unlabeled F
1
scores
of our SRL component as we move from gold to
predicted information for syntax and PIC. For the
shared task setting ?predicted syntax and predicted
PIC? we show results for the two inference strate-
gies implemented: greedy and reranking. The first
line in the table indicates that the performance of
the SRL component when using gold syntax and
gold PIC is good: the labeled F
1
is 90 points for the
in-domain corpus and approximately 85 points for
the out-of-domain corpus. Argument classification
suffers the most on out-of-domain input: there is
a difference of 5 points between the labeled scores
on WSJ and Brown, even though the correspond-
ing unlabeled scores are comparable.
The second line in the table replicates the setup
of the 2005 CoNLL shared task: predicted syntax
but gold PIC. This yields a moderate drop of 3 la-
beled F
1
points on in-domain data and a larger drop
of 6 points for out-of-domain data.
We see larger drops when switching to predicted
PIC (line 3): 5-6 labeled F
1
points in domain and
12 points out of domain. This drop is caused by the
PIC component, e.g., if a predicate is missed the
whole frame is lost. Table 4 lists the scores of our
PIC component, which we compare with a base-
line system that assigns sense 1 to all identified
predicates. The table indicates that, even though
our disambiguation component improves signifi-
cantly over the baseline, it performs poorly, espe-
cially on out-of-domain data. Same as SRL, the
classification sub-task suffers the most out of do-
main (there is a difference of 15 points between
unlabeled and labeled F
1
scores on Brown).
Finally, the reranking inference strategy yields
only modest improvements (last line in Table 3).
We attribute these results to the fact that, unlike
Toutanova et al (2005), we use only one tree to
generate frame candidates, hence the variation in
the candidate frames is small. Considering that the
processing overhead of reranking is already large
(it quadruples the runtime of our AIC component),
we do not consider reranking a practical extension
to a SRL system when processing speed is a dom-
inant requirement.
References
G. Attardi. 2006. Experiments with a Multilan-
guage Non-Projective Dependency Parser. In Proc.
of CoNNL-X 2006.
G. Attardi and M. Ciaramita. 2007. Tree Revi-
sion Learning for Dependency Parsing. In Proc. of
NAACL/HLTC 2007.
G. Attardi, F. Dell?Orletta. 2008. Chunking and De-
pendency Parsing. In Proc. of Workshop on Partial
Parsing.
K. Crammer and Y. Singer. 2003. Ultraconservative
Online Algorithms for Multiclass Problems. Journal
of Machine Learning Research 3: pp.951-991.
F. Dell?Orletta. 2008. Improving the Accuracy of De-
pendency Parsing. PhD Thesis. Dipartimento di In-
formatica, Universit`a di Pisa, forthcoming.
K. Hall and V. Novak. 2005. Corrective Modeling
for Non-Projective Dependency Parsing. In Proc. of
IWPT.
L. Marquez, L. Padro, M. Surdeanu, and L. Villarejo.
2007. UPC: Experiments with Joint Learning within
SemEval Task 9. In Proc. of SemEval 2007.
K. Sagae and A. Lavie. 2006. Parser Combination by
reparsing. In Proc. of HLT/NAACL.
M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez
and J. Nivre. 2008. The CoNLL-2008 Shared Task
on Joint Parsing of Syntactic and Semantic Depen-
dencies. In Proc. of CoNLL-2008.
K. Toutanova, A. Haghighi, and C. Manning. 2005.
Joint Learning Improves Semantic Role Labeling. In
Proc. of ACL.
262
Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 2?10,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
Customizing an Information Extraction System to a New Domain
Mihai Surdeanu, David McClosky, Mason R. Smith, Andrey Gusev,
and Christopher D. Manning
Department of Computer Science
Stanford University
Stanford, CA 94305
{mihais,mcclosky,mrsmith,manning}@stanford.edu
agusev@cs.stanford.edu
Abstract
We introduce several ideas that improve the
performance of supervised information ex-
traction systems with a pipeline architecture,
when they are customized for new domains.
We show that: (a) a combination of a se-
quence tagger with a rule-based approach for
entity mention extraction yields better perfor-
mance for both entity and relation mention
extraction; (b) improving the identification of
syntactic heads of entity mentions helps rela-
tion extraction; and (c) a deterministic infer-
ence engine captures some of the joint domain
structure, even when introduced as a post-
processing step to a pipeline system. All in all,
our contributions yield a 20% relative increase
in F1 score in a domain significantly differ-
ent from the domains used during the devel-
opment of our information extraction system.
1 Introduction
Information extraction (IE) systems generally con-
sist of multiple interdependent components, e.g., en-
tity mentions predicted by an entity mention detec-
tion (EMD) model connected by relations via a re-
lation mention detection (RMD) component (Yao et
al., 2010; Roth and Yih, 2007; Surdeanu and Cia-
ramita, 2007). Figure 1 shows a sentence from a
sports domain where both entity and relation men-
tions are annotated. When training data exists, the
best performance in IE is generally obtained by su-
pervised machine learning approaches. In this sce-
nario, the typical approach for domain customiza-
tion is apparently straightforward: simply retrain
on data from the new domain (and potentially tune
model parameters). In this paper we argue that, even
when considerable training data is available, this is
not sufficient to maximize performance. We apply
several simple ideas that yield a significant perfor-
mance boost, and can be implemented with minimal
effort. In particular:
? We show that a combination of a conditional
random field model (Lafferty et al, 2001) with
a rule-based approach that is recall oriented
yields better performance for EMD and for
the downstream RMD component. The rule-
based approach includes gazetteers, which have
been shown to be important by Mikheev et al
(1999), among others.
? We improve the unification of the predicted se-
mantic annotations with the syntactic analy-
sis of the corresponding text, i.e., finding the
syntactic head of a given semantic constituent.
Since many features in an IE system depend on
syntactic analysis, this leads to more consistent
features and better extraction models.
? We add a simple inference engine that gener-
ates additional relation mentions based solely
on the relation mentions extracted by the RMD
model. This engine mitigates some of the limi-
tations of a text-based RMD model, which can-
not extract relations not explicitly stated in text.
We investigate these ideas using an IE system that
performs recognition of entity mentions followed by
extraction of binary relations between these men-
tions. We used as target a sports domain that is sig-
nificantly different from the corpora previously used
with this IE system. The target domain is also sig-
nificantly different from the dataset used to train the
2
Rookie	 ?Mike	 ?Anderson	 ?scored	 ?two	 ?second-??half	 ?touchdowns,	 ?	 ?
leading	 ?the	 ?Broncos	 ?to	 ?their	 ?sixth	 ?straight	 ?victory,	 ?31	 ?-??	 ?24	 ?	 ?
over	 ?the	 ?Sea?le	 ?Seahawks	 ?on	 ?Sunday.	 ?
ScoreType-??2	 ?
FinalScore	 ? FinalScore	 ?NFLGame	 ?NFLTeam	 ?
NFLTeam	 ? Date	 ?
teamInGame	 ?
gameWinner	 ?
touchdownPar?alCount	 ?
teamScoringAll	 ?
teamInGame	 ?
gameDate	 ?
gameLoser	 ?
teamScoringAll	 ?
teamFinalScore	 ?teamFinalScore	 ?
Figure 1: Sample sentence from the NFL domain. The domain contains entity mentions (underlined with entity types
in bold) and binary relations between entity mentions (indicated by arrows; relation types are italicized).
supporting natural language processing tools (e.g.,
syntactic parser). Our investigation shows that, de-
spite their simplicity, all our proposals help, yielding
a 20% relative improvement in RMD F1 score.
The paper is organized as follows: Section 2 sur-
veys related work. Section 3 describes the IE system
used. We cover the target domain that serves as use
case in this paper in Section 4. Section 5 introduces
our ideas and evaluates their impact in the target do-
main. Finally, Section 6 concludes the paper.
2 Related Work
Other recent works have analyzed the robustness of
information extraction systems. For example, Flo-
rian et al (2010) observed that EMD systems per-
form badly on noisy inputs, e.g., automatic speech
transcripts, and propose system combination (sim-
ilar to our first proposal) to increase robustness in
such scenarios. Ratinov and Roth (2009) also in-
vestigate design challenges for named entity recog-
nition, and showed that other design choices, such
as the representation of output labels and using fea-
tures built on external knowledge, are more impor-
tant than the learning model itself. These works are
conceptually similar to our paper, but we propose
several additional directions to improve robustness,
and we investigate their impact in a complete IE sys-
tem instead of just EMD.
Several of our lessons are drawn from the BioCre-
ative challenge1 and the BioNLP shared task (Kim
1http://biocreative.sourceforge.net/
et al, 2009). These tasks have shown the impor-
tance of high quality syntactic annotations and using
heuristic fixes to correct systematic errors (Schuman
and Bergler, 2006; Poon and Vanderwende, 2010,
among others). Systems in the latter task have also
shown the importance of high recall in the earlier
stages of pipeline system.
3 Description of the Generic IE System
We illustrate our proposed ideas using a simple IE
system that implements a pipeline architecture: en-
tity mention extraction followed by relation men-
tion extraction. Note however that the domain cus-
tomization discussion in Section 5 is independent of
the system architecture or classifiers used for EMD
and RMD, and we expect the proposed ideas to ap-
ply to other IE approaches as well.
We performed all pre-processing (tokenization,
part-of-speech (POS) tagging) with the Stanford
CoreNLP toolkit.2 For EMD we used the Stanford
named entity recognizer (Finkel et al, 2005). In all
our experiments we used a generic set of features
(?macro?) and the IO notation3 for entity mention la-
bels (e.g., the labels for the tokens ?over the Seattle
Seahawks on Sunday? (from Figure 1) are encoded
as ?O O NFLTEAM NFLTEAM O DATE?).
2http://nlp.stanford.edu/software/
corenlp.shtml
3The IO notation facilitates faster inference than the IOB
or IOB2 notations with minimal impact on performance, when
there are fewer adjacent mentions with the same type.
3
Argument
Features
? Head words of the two arguments
and their combination
? Entity mention labels of the two
arguments and their combination
Syntactic
Features
? Sequence of dependency labels
in the dependency path linking the
heads of the two arguments
? Lemmas of all words in the de-
pendency path
? Syntactic path in the constituent
parse tree between the largest con-
stituents headed by the same words
as the two arguments (similar
to Gildea and Jurafsky (2002))
Surface
Features
? Concatenation of POS tags be-
tween arguments
? Binary indicators set to true if
there is an entity mention with a
given type between the two argu-
ments
Table 1: Feature set used for RMD.
The RMD model was built from scratch as a
multi-class classifier that extracts binary relations
between entity mentions in the same sentence. Dur-
ing training, known relation mentions become pos-
itive examples for the corresponding label and all
other possible combinations between entity men-
tions in the same sentence become negative exam-
ples. We used a multiclass logistic regression classi-
fier with L2 regularization. Our feature set is taken
from (Yao et al, 2010; Mintz et al, 2009; Roth and
Yih, 2007; Surdeanu and Ciaramita, 2007) and mod-
els the relation arguments, the surface distance be-
tween the relation arguments, and the syntactic path
between the two arguments, using both constituency
and dependency representations. For syntactic in-
formation, we used the Stanford parser (Klein and
Manning, 2003) and the Stanford dependency repre-
sentation (de Marneffe et al, 2006).
For RMD, we implemented an additive feature se-
lection algorithm similar to the one in (Surdeanu
et al, 2008), which iteratively adds the feature
with the highest improvement in F1 score to the
current feature set, until no improvement is seen.
The algorithm was configured to select features
that yielded the best combined performance on the
dataset from Roth and Yih (2007) and the training
partition of ACE 2007.4 We used ten-fold cross val-
4LDC catalog numbers LDC2006E54 and LDC2007E11
Documents Words Entity Relation
Mentions Mentions
110 70,119 2,188 1,629
Table 2: Summary statistics of the NFL corpus, after our
conversion to binary relations.
idation on both datasets. We decided to use a stan-
dard F1 score to evaluate RMD performance rather
than the more complex ACE score because we be-
lieve that the former is more interpretable. We used
gold entity mentions for the feature selection pro-
cess. Table 1 summarizes the final set of features
selected.
Despite its simplicity, our approach achieves
comparable performance with other state-of-the-art
results reported on these datasets (Roth and Yih,
2007; Surdeanu and Ciaramita, 2007). For exam-
ple, Surdeanu and Ciaramita report a RMD F1 score
of 59.4 for ACE relation types (i.e., ignoring sub-
types) when gold entity mentions are used. Under
the same conditions, our RMD model obtains a F1
score of 59.2.
4 Description of the Target Domain
In this paper we report results on the ?Machine
Reading NFL Scoring? corpus.5 This corpus was
developed by LDC for the DARPA Machine Read-
ing project. The corpus contains 110 newswire arti-
cles on National Football League (NFL) games. The
annotations cover game information, such as partici-
pating teams, winners and losers, partial (e.g., a sin-
gle touchdown or three field goals) and final scores.
Most of the annotated relations in the original corpus
are binary (e.g. GAMEDATE(NFLGAME, DATE))
but some are n-ary relations or include other at-
tributes in addition of the relation type. We reduce
these to annotations compatible with our RMD ap-
proach as follows:
? We concatenate the cardinality of each scoring
event (i.e. how many scoring events are be-
ing talked about) to the corresponding SCORE-
TYPE entity label. Thus SCORETYPE-2 in-
dicates that there were two of a given type
of scoring event (touchdown, field goal, etc.).
This operation is necessary because the cardi-
nality of scoring events is originally annotated
as an additional attribute of the SCORETYPE
5LDC catalog number LDC2009E112
4
Entity Mentions Correct Predicted Actual P R F1
Date 141 190 174 74.2 81.0 77.5
FinalScore 299 328 347 91.2 86.2 88.6
NFLGame 71 109 147 65.1 48.3 55.5
NFLPlayoffGame 8 25 38 32.0 21.1 25.4
NFLTeam 651 836 818 77.9 79.6 78.7
ScoreType-1 329 479 525 68.7 62.7 65.5
ScoreType-2 49 68 79 72.1 62.0 66.7
ScoreType-3 17 26 36 65.4 47.2 54.8
ScoreType-4 6 11 14 54.5 42.9 48.0
Total 1571 2076 2188 75.7 71.8 73.7
Relation Mentions Correct Predicted Actual P R F1
fieldGoalPartialCount 33 41 101 80.5 32.7 46.5
gameDate 32 36 115 88.9 27.8 42.4
gameLoser 22 44 124 50.0 17.7 26.2
gameWinner 6 15 123 40.0 4.9 8.7
teamFinalScore 95 101 232 94.1 40.9 57.1
teamInGame 49 105 257 46.7 19.1 27.1
teamScoringAll 202 232 321 87.1 62.9 73.1
touchDownPartialCount 156 191 322 81.7 48.4 60.8
Total 595 766 1629 77.7 36.5 49.7
Table 3: Baseline results: stock system without any domain customization. Correct/Predicted/Actual indicate the num-
ber of mentions (entities or relations) that are correctly predicted/predicted/gold. P/R/F1 indicate precision/recall/F1
scores for the corresponding label.
entity and our EMD approach does not model
mention attributes.
? We split all n-ary relations into several new
binary relations. For example, the original
TEAMFINALSCORE(NFLTEAM, NFLGAME,
FINALSCORE) relation is split into three binary
relations: TEAMSCORINGALL(NFLTEAM,
FINALSCORE), TEAMINGAME(NFLGAME,
NFLTEAM), and TEAMFINALSCORE(NFL-
GAME, FINALSCORE).
Figure 1 shows an example annotated sentence af-
ter the above conversion and Table 2 lists the corpus
summary statistics for the new binary relations.
The purpose behind this corpus is to encourage
the development of systems that answer structured
queries that go beyond the functionality of informa-
tion retrieval engines, e.g.:
?For each NFL game, identify the win-
ning and losing teams and each team?s fi-
nal score in the game.?
?For each team losing to the Green Bay
Packers, tell us the losing team and the
number of points they scored.?6
6These queries would be written in a formal language but
5 Domain Customization
Table 3 lists the results of the generic IE system de-
scribed in Section 3 on the NFL domain. Through-
out this paper we will report results using ten-fold
cross-validation on all 110 documents in the cor-
pus.7 We consider an entity mention as correct if
both its boundaries and label match exactly the gold
mention. We consider a relation mention correct if
both its arguments and label match the gold relation
mention. For RMD, we report results using the ac-
tual mentions predicted by our EMD model (instead
of using gold entity mentions for RMD). For clar-
ity, we do not show in the tables some labels that are
highly uncommon in the data (e.g., SCORETYPE-5
appears only four times in the entire corpus); but the
?Total? results include all entity and relation men-
tions.
Table 3 shows that the stock IE system obtains an
are presented here in English for clarity.
7Generally, we do not condone reporting results using cross-
validation because it may be a recipe for over-fitting on the
corresponding corpus. However, all our domain customization
ideas were developed using outside world and domain knowl-
edge and were not tuned on this data, so we believe that there is
minimal over-fitting in this case.
5
Entity Mentions P R F1
Date 74.2 81.0 77.5
FinalScore 91.3 87.3 89.2
NFLGame 61.2 48.3 54.0
NFLPlayoffGame 33.3 21.1 25.8
NFLTeam 77.9 81.3 79.5
ScoreType-1 68.8 62.3 65.4
ScoreType-2 72.1 62.0 66.7
ScoreType-3 65.4 47.2 54.8
ScoreType-4 54.5 42.9 48.0
Total 75.6 72.5 74.0
Relation Mentions P R F1
fieldGoalPartialCount 78.0 31.7 45.1
gameDate 91.4 27.8 42.7
gameLoser 50.0 18.5 27.1
gameWinner 40.0 4.9 8.7
teamFinalScore 94.1 40.9 57.1
teamInGame 45.9 19.5 27.3
teamScoringAll 87.0 64.8 74.3
touchDownPartialCount 82.4 49.4 61.7
Total 77.6 37.1 50.2
Table 4: Performance after gazetteer-based features were
added to the EMD model.
EMD F1 score of 73.7 and a RMD F1 score of 49.7.
These are respectable results, in line with state-of-
the-art results in other domains.8 However, there
are some obvious areas for improvement. For exam-
ple, the score for a few relations (e.g., GAMELOSER
and GAMEWINNER) is quite low. This is caused by
the fact that these relations are often not explicitly
stated in text but rather implied (e.g., based on team
scores). Furthermore, the low recall of entity types
that are crucial for all relations (e.g., NFLTEAM and
NFLGAME) negatively impacts the overall recall of
RMD.
5.1 Combining a Rule-based Model with
Conditional Random Fields for EMD
A straightforward way to improve EMD perfor-
mance is to construct domain-specific gazetteers and
include gazetteer-based features in the model. We
constructed a NFL-specific gazetteer as follows: (a)
we included all 32 NFL team names; (b) we built a
lexicon for NFLGame nouns and verbs that included
game types (e.g., ?semi-final?, ?quarter-final?) and
8As a comparison, the best RMD system in ACE 2007 ob-
tained an ACE score of less than 35%, even though the ACE
score gives credit for approximate matches of entity mention
boundaries (Surdeanu and Ciaramita, 2007).
Entity Mentions P R F1
Date 74.2 81.0 77.5
FinalScore 91.3 87.3 89.2
NFLGame 61.2 48.3 54.0
NFLPlayoffGame 33.3 21.1 25.8
NFLTeam 71.4 96.9 82.3
ScoreType-1 68.8 62.3 65.4
ScoreType-2 72.1 62.0 66.7
ScoreType-3 65.4 47.2 54.8
ScoreType-4 54.5 42.9 48.0
Total 72.8 78.4 75.5
Relation Mentions P R F1
fieldGoalPartialCount 81.2 38.6 52.3
gameDate 93.9 27.0 41.9
gameLoser 51.1 19.4 28.1
gameWinner 38.9 5.7 9.9
teamFinalScore 94.1 40.9 57.1
teamInGame 47.4 24.5 32.3
teamScoringAll 87.0 68.8 76.9
touchDownPartialCount 81.6 56.5 66.8
Total 77.2 40.6 53.2
Table 5: Performance after gazetteer-based features were
added to the EMD model, and NFLTeam entity mentions
were extracted using the rule-based model rather than
classification.
typical game descriptors. The game descriptors
were manually bootstrapped from three seed words
(?victory?, ?loss?, ?game?) using Dekang Lin?s
dependency-based thesaurus.9 This process added
other relevant game descriptors such as ?triumph?,
?defeat?, etc. All in all, our gazetteer includes 32
team names and 50 game descriptors. The gazetteer
was built in less than four person hours.
We added features to our EMD model to indi-
cate if a sequence of words matches a gazetteer en-
try, allowing approximate matches (e.g., ?Cowboys?
matches ?Dallas Cowboys?). Table 4 lists the results
after this change. The improvements are modest: 0.3
for both EMD and RMD, caused by a 0.8 improve-
ment for NFLTEAM. The score for NFLGAME suf-
fers a loss of 1.5 F1 points, probably caused by the
fact that our NFLGAME gazetteer is incomplete.
These results are somewhat disappointing: even
though our gazetteer contains an exhaustive list of
NFL team names, the EMD recall for NFLTEAM
is still relatively low. This happens because city
9http://webdocs.cs.ualberta.ca/?lindek/
Downloads/sim.tgz
6
names that are not references to team names are rela-
tively common in this corpus, and the CRF model fa-
vors the generic city name interpretation. However,
since the goal is to answer structured queries over
the extracted relations, we would prefer a model
that favors recall for EMD, to avoid losing candi-
dates for RMD. While this can be achieved in dif-
ferent ways (Minkov et al, 2006), in this paper we
implement a very simple approach: we recognize
NFLTEAM mentions with a rule-based system that
extracts all token sequences that begin, end, or are
equal to a known team name. For example, ?Green
Bay? and ?Packers? are marked as team mentions,
but not ?Bay?. Note that this approach is prone to in-
troducing false positives, e.g., ?Green? in the above
example. For all other entity types we use the CRF
model with gazetteer-based features. Table 5 lists
the results for this model combination. The table
shows that the RMD performance is improved by 3
F1 points. The F1 score for NFLTEAM mentions is
also improved by 3 points, due to a significant in-
crease in recall (from 81% to 97%).
Of course, this simple idea works only for en-
tity types with low ambiguity. In fact, it does not
improve results if we apply it to NFLGAME or
SCORETYPE-*. However, low ambiguity entities
are common in many domains (e.g., medical). In
such domains, our approach offers a straightforward
way to address potential recall errors of a machine
learned model.
5.2 Improving Head Identification for Entity
Mentions
Table 1 indicates that most RMD features (e.g., lex-
ical information on arguments, dependency paths
between arguments) depend on the syntactic heads
of entity mentions. This observation applies to
other natural language processing (NLP) tasks as
well, e.g., semantic role labeling or coreference res-
olution (Gildea and Jurafsky, 2002; Haghighi and
Klein, 2009). It is thus crucial that syntactic heads
of mentions be correctly identified. Originally we
employed a common heuristic: we first try to find a
constituent with the exact same span as the given en-
tity mention in the parse tree of the entire sentence,
and extract its head. If no such constituent exists,
we parse only the text corresponding to the mention
and return the head of the generated tree (Haghighi
Entity Mentions P R F1
Date 69.5 75.9 72.5
FinalScore 90.9 88.8 89.8
NFLGame 60.5 51.0 55.4
NFLPlayoffGame 37.0 26.3 30.8
NFLTeam 72.4 98.3 83.4
ScoreType-1 69.7 62.1 65.7
ScoreType-2 76.9 63.3 69.4
ScoreType-3 64.3 50.0 56.3
ScoreType-4 72.7 57.1 64.0
Total 73.2 79.2 76.1
Relation Mentions P R F1
fieldGoalPartialCount 81.2 55.4 65.9
gameDate 93.9 27.0 41.9
gameLoser 51.2 17.7 26.3
gameWinner 50.0 8.9 15.2
teamFinalScore 96.5 47.4 63.6
teamInGame 48.3 33.5 39.5
teamScoringAll 86.7 72.9 79.2
touchDownPartialCount 89.1 61.2 72.6
Total 78.5 45.9 57.9
Table 6: Performance with the improved syntactic head
identification rules.
and Klein, 2009). Here we argue that the last step of
this heuristic is flawed: since most parsers are heav-
ily context dependent, they are likely to not parse
correctly arbitrarily short text fragments. For exam-
ple, the Stanford parser generates the incorrect parse
tree:
The syntactic head is ?5? for the mention ?a 5-yard
scoring pass? instead of ?pass.?10 This problem is
exacerbated out of domain, where the parse tree of
the entire sentence is likely to be incorrect, which
will often trigger the parsing of the isolated men-
tion text. For example, in the NFL domain, more
than 25% of entity mentions cannot be matched to
a constituent in the parse tree of the corresponding
sentence.
10We tokenize around dashes in this domain because scores
are often dash separated. However, this mention is incorrectly
parsed even when ?5-yard? is a single token.
7
teamFinalScore(G, S) :- teamInGame(T, G), teamScoringAll(T, S).
teamFinalScore(G, S) :- gameWinner(T, G), teamScoringAll(T, S).
teamFinalScore(G, S) :- gameLoser(T, G), teamScoringAll(T, S).
teamInGame(G, T) :- teamScoringAll(T, S), teamFinalScore(G, S).
gameWinner(G, T1) :- teamInGame(G, T1), teamInGame(G, T2),
teamFinalScore(G, S1), teamFinalScore(G, S2),
teamScoringAll(T1, S1), teamScoringAll(T2, S2),
greaterThan(S1, S2).
gameLoser(G, T1) :- teamInGame(G, T1), teamInGame(G, T2),
teamFinalScore(G, S1), teamFinalScore(G, S2),
teamScoringAll(T1, S1), teamScoringAll(T2, S2),
lessThan(S1, S2).
Table 7: Deterministic inference rules for the NFL domain as first-order Horn clauses. G, T, and S indicate game,
team, and score variables.
In this work, we propose several simple heuristics
that improve the parsing of isolated mention texts:
? We append ?It was ? to the beginning of the text
to be parsed. Since entity mentions are noun
phrases (NP), the new text is guaranteed to be
a coherent sentence. A similar heuristic was
used by Moldovan and Rus for the parsing of
WordNet glosses (2001).
? Because dashes are uncommon in the Penn
Treebank, we remove them from the text before
parsing.
? We guide the Stanford parser such that the final
tree contains a constituent with the same span
as the mention text.11
After implementing these heuristics, the Stanford
parser correctly parses the mention in the above ex-
ample as a NP headed by ?pass?. Table 6 lists
the overall extraction scores after deploying these
heuristics. The table shows that the RMD F1 score
is a considerable 4.7 points higher than before this
change (Table 5).
5.3 Deterministic Inference for RMD
Figure 1 underlines the fact that relations in the NFL
domain are highly inter-dependent. This is a com-
mon occurrence in many extraction tasks and do-
mains (Poon and Vanderwende, 2010; Carlson et
al., 2010). The typical way to address these situa-
tions is to jointly model these relations, e.g., using
Markov logic networks (MLN) (Poon and Vander-
wende, 2010). However, this implies a complete
redesign of the corresponding IE system, which
would essentially ignore all the effort behind exist-
ing pipeline systems.
11This is supported by the parser API.
Relation Mentions P R F1
fieldGoalPartialCount 81.2 55.4 65.9
gameDate 93.9 27.0 41.9
gameLoser 45.9 27.4 34.3
gameWinner 45.6 25.2 32.5
teamFinalScore 96.5 47.4 63.6
teamInGame 48.1 44.7 46.4
teamScoringAll 86.7 72.9 79.2
touchDownPartialCount 89.1 61.2 72.6
Total 74.2 49.6 59.5
Table 8: Performance after adding deterministic infer-
ence. The EMD scores are not affected by this change,
so they are not listed here.
In this work, we propose a simple method that
captures some of the joint domain structure indepen-
dently of the IE architecture and the EMD and RMD
models. We add a deterministic inference compo-
nent that generates new relation mentions based on
the data already extracted by the pipeline model. Ta-
ble 7 lists the rules of this inference component that
were developed for the NFL domain. These rules
are domain-dependent, but they are quite simple: the
first four rules implement transitive-closure rules for
relation mentions centered around the same NFL-
GAME mention; the last two add domain knowledge
that is not captured by the text extractors, e.g., the
game winner is the team with the higher score. Ta-
ble 8, which lists the RMD scores after inference, in-
dicates that the inference component is responsible
for an increase of approximately 2 F1 points, caused
by a recall boost of approximately 4%.
Table 9 lists the results of a post-hoc experiment,
where we removed several relation types from the
RMD classifier (the ones predicted with poor perfor-
mance) and let the deterministic inference compo-
nent generate them instead. This experiment shows
8
Without Inference With Inference
P R F1 P R F1
Skip gameWinner, gameLoser 78.6 45.6 57.7 75.1 48.4 58.8
Skip teamInGame 77.0 43.6 55.7 71.7 49.4 58.5
Skip teamInGame, teamFinalScore 74.5 37.1 49.6 70.9 47.6 56.9
Skip nothing 78.5 45.9 57.9 74.2 49.6 59.5
Table 9: Analysis of different combination strategies between the RMD classifier and inference: the RMD model skips
the relation types listed in the first column; the inference component generates all relation types. The other columns
show relation mention scores under the various configurations.
EMD RMD
F1 F1
Baseline 73.7 49.7
+ gazetteer features 74.0 50.2
+ rule-based model for NFLTeam 75.5 53.2
+ improved head identification 76.1 57.9
+ inference 76.1 59.5
Table 10: Summary of domain customization results.
that inference helps in all configurations, and, most
importantly, it is robust: even though the RMD score
without inference decreases by up to 8 F1 points
as relations are removed, the score after inference
varies by less than 3 F1 points (from 56.9 to 59.5
F1). This proves that deterministic inference is ca-
pable of generating relation mentions that are either
missed or cannot be modeled by the RMD classifier.
Finally, Table 10 summarizes the experiments
presented in this paper. It is clear that, despite their
simplicity, all our proposed ideas help. All in all,
our contributions yielded an improvement of 9.8 F1
points (approximately 20% relative) over the stock
IE system without these changes. Our best IE sys-
tem was used in a blind evaluation within the Ma-
chine Reading project. In this evaluation, systems
were required to answer 50 queries similar to the
examples in Section 4 and were evaluated on the
correctness of the individual facts extracted. Note
that this evaluation is more complex than the exper-
iments reported until now, because the correspond-
ing IE system requires additional components, e.g.,
the normalization of all DATE mentions and event
coreference (i.e., are two different game mentions
referring to the same real-world game?). For this
evaluation, we used an internal script for date nor-
malization and we did not implement event corefer-
ence. This system was evaluated at 46.7 F1 (53.7
precision and 41.2 recall), a performance that was
approximately 80% of the F1 score obtained by hu-
man annotators. This further highlights that strong
IE performance can be obtained with simple models.
6 Conclusions
This paper introduces a series of simple ideas that
improve the performance of IE systems when they
are customized to new domains. We evaluated our
contributions on a sports domain (NFL game sum-
maries) that is significantly different from the do-
mains used to develop our IE system or the language
processors used by our system.
Our analysis revealed several interesting and non-
obvious facts. First, we showed that accurate identi-
fication of syntactic heads of entity mentions, which
has received little attention in IE literature, is cru-
cial for good performance. Second, we showed that
a deterministic inference component captures some
of the joint domain structure, even when the under-
lying system follows a pipeline architecture. Lastly,
we introduced a simple way to tune precision and
recall by combining our entity mention extractor
with a rule-based system. Overall, our contributions
yielded a 20% improvement in the F1 score for rela-
tion mention extraction.
We believe that our contributions are model inde-
pendent and some, e.g., the better head identifica-
tion, even task independent. Some of our ideas re-
quire domain knowledge, but they are all very sim-
ple to implement. We thus expect them to impact
other problems as well, e.g., coreference resolution,
semantic role labeling.
Acknowledgments
We thank the reviewers for their detailed comments.
This material is based upon work supported by the Air
Force Research Laboratory (AFRL) under prime contract
no. FA8750-09-C-0181. Any opinions, findings, and
conclusion or recommendations expressed in this mate-
rial are those of the authors and do not necessarily reflect
the view of the Air Force Research Laboratory (AFRL).
9
References
Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-
tevam R. Hruschka Jr., and Tom M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of the Third ACM Interna-
tional Conference on Web Search and Data Mining
(WSDM).
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL 2005).
Radu Florian, John Pitrelli, Salim Roukos, and Imed Zi-
touni. 2010. Improving mention detection robustness
to noisy input. In Proc. of Empirical Methods in Nat-
ural Language Processing (EMNLP).
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3).
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proc. of Empirical Methods in Natural Language
Processing (EMNLP).
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of the Workshop on BioNLP: Shared Task,
pages 1?9. Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Meeting of the Association for Computational Linguis-
tics.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
Proc. of the International Conference on Machine
Learning (ICML).
Andrei Mikheev, Marc Moens, and Claire Grover. 1999.
Named entity recognition without gazetteers. In
EACL, pages 1?8.
Einat Minkov, Richard C. Wang, Anthony Tomasic, and
William W. Cohen. 2006. Ner systems that suit user?s
preferences: Adjusting the recall-precision trade-off
for entity extraction. In Proc. of HLT/NAACL.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Dis-
tant supervision for relation extraction without labeled
data. In Proc. of the Conference of the Association for
Computational Linguistics (ACL-IJCNLP).
Dan I. Moldovan and Vasile Rus. 2001. Logic form
transformation of wordnet and its applicability to ques-
tion answering. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics.
Hoifung Poon and Lucy Vanderwende. 2010. Joint in-
ference for knowledge extraction from biomedical lit-
erature. In Proceedings of the North American Chap-
ter of the Association for Computational Linguistics -
Human Language Technologies Conference (NAACL-
HLT).
Lev Ratinov and Dan Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Proc.
of the Annual Conference on Computational Natural
Language Learning (CoNLL).
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Introduction to Statistical Relational
Learning. MIT Press.
Jonathan Schuman and Sabine Bergler. 2006. Postnom-
inal prepositional phrase attachment in proteomics. In
Proceedings of the HLT-NAACL BioNLP Workshop on
Linking Natural Language and Biology, pages 82?89.
Association for Computational Linguistics, June.
Mihai Surdeanu and Massimiliano Ciaramita. 2007. Ro-
bust information extraction with perceptrons. In Pro-
ceedings of the NIST 2007 Automatic Content Extrac-
tion Workshop (ACE07).
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2008. Learning to rank answers on large
online qa collections. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics (ACL 2008).
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proc. of Empirical Methods
in Natural Language Processing (EMNLP).
10
Proceedings of BioNLP Shared Task 2011 Workshop, pages 41?45,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Event Extraction as Dependency Parsing for BioNLP 2011
David McClosky, Mihai Surdeanu, and Christopher D. Manning
Department of Computer Science
Stanford University
Stanford, CA 94305
{mcclosky,mihais,manning}@stanford.edu
Abstract
We describe the Stanford entry to the BioNLP
2011 shared task on biomolecular event ex-
traction (Kim et al, 2011a). Our framework is
based on the observation that event structures
bear a close relation to dependency graphs.
We show that if biomolecular events are cast
as these pseudosyntactic structures, standard
parsing tools (maximum-spanning tree parsers
and parse rerankers) can be applied to per-
form event extraction with minimum domain-
specific tuning. The vast majority of our
domain-specific knowledge comes from the
conversion to and from dependency graphs.
Our system performed competitively, obtain-
ing 3rd place in the Infectious Diseases track
(50.6% f-score), 5th place in Epigenetics and
Post-translational Modifications (31.2%), and
7th place in Genia (50.0%). Additionally, this
system was part of the combined system in
Riedel et al (2011) to produce the highest
scoring system in three out of the four event
extraction tasks.
1 Introduction
The distinguishing aspect of our approach is that by
casting event extraction as a dependency parsing, we
take advantage of standard parsing tools and tech-
niques rather than creating special purpose frame-
works. In this paper, we show that with minimal
domain-specific tuning, we are able to achieve com-
petitive performance across the three event extrac-
tion domains in the BioNLP 2011 shared task.
At the heart of our system1 is an off-the-shelf
1nlp.stanford.edu/software/eventparser.shtml
dependency parser, MSTParser2 (McDonald et al,
2005; McDonald and Pereira, 2006), extended with
event extraction-specific features and bookended by
conversions to and from dependency trees. While
features in MSTParser must be edge-factored and
thus fairly local (e.g., only able to examine a portion
of each event at once), decoding is performed glob-
ally allowing the parser to consider trade-offs. Fur-
thermore, as MSTParser can use n-best decoders,
we are able to leverage a reranker to capture global
features to improve accuracy.
In ?2, we provide a brief overview of our frame-
work. We describe specific improvements for the
BioNLP 2011 shared task in ?3. In ?4, we present
detailed results of our system. Finally, in ?5 we give
some directions for future work.
2 Event Parsing
Our system includes three components: (1) anchor
detection to identify and label event anchors, (2)
event parsing to form candidate event structures by
linking entities and event anchors, and (3) event
reranking to select the best candidate event structure.
As the full details on our approach are described in
McClosky et al (2011), we will only provide an out-
line of our methods here along with additional im-
plementation notes.
Before running our system, we perform basic
preprocessing on the corpora. Sentences need
to be segmented, tokenized, and parsed syntacti-
cally. We use custom versions of these (except
for Infectious Diseases where we use those from
Stenetorp et al (2011)). To ease event parsing, our
2http://sourceforge.net/projects/mstparser/
41
tokenizations are designed to split off suffixes which
are often event anchors. For example, we split the
token RelA-induced into the two tokens RelA and in-
duced3 since RelA is a protein and induced an event
anchor. If this was a single token, our event parser
would be unable to link them since it cannot pre-
dict self-loops in the dependency graph. For syntac-
tic parsing, we use the self-trained biomedical pars-
ing model from McClosky (2010) with the Charniak
and Johnson (2005) reranking parser. We use its ac-
tual constituency tree, the dependency graph created
by applying head percolation rules, and the Stanford
Dependencies (de Marneffe and Manning, 2008) ex-
tracted from the tree (collapsed and uncollapsed).
Anchor detection uses techniques inspired from
named entity recognition to label each token with
an event type or none. The features for this stage
are primarily drawn from Bjo?rne et al (2009). We
reduce multiword event anchors to their syntactic
head.4 We classify each token independently using a
logistic regression classifier with L2 regularization.
By adjusting a threshold parameter, we can adjust
the balance between precision and recall. We choose
to heavily favor recall (i.e., overgenerate event an-
chors) as the event parser can drop extraneous an-
chors by not attaching any arguments to them.
The event anchors from anchor detection and
the included entities (.t1 files) form a ?reduced?
sentence, which becomes the input to event pars-
ing. Thus, the only words in the reduced sentence
are tokens believed to directly take part in events.
Note, though, that we use the original ?full? sen-
tence (including the various representations of its
syntactic parse) for feature generation. For full de-
tails on this process, see McClosky et al (2011).
As stated before, this stage consists of MSTParser
with additional event parsing features. There are
four decoding options for MSTParser, depending
on (a) whether features are first- or second-order
and (b) whether graphs produced are projective or
non-projective. The projective decoders have com-
plete n-best implementations whereas their non-
projective counterparts are approximate. Neverthe-
3The dash is removed since a lone dash would further con-
fuse the syntactic parser.
4This does not affect performance if the approximate scorer
is used, but it does impact scores if exact matching of anchor
boundaries is imposed.
less, these four decoders constitute slightly different
views of the same data and can be combined inside
the reranking framework. After decoding, we con-
vert parses back to event structures. Details on this
critical step are given in McClosky et al (2011).
Event reranking, the final stage of our system, re-
ceives an n-best list of event structures from each
decoder in the event parsing step. The reranker
can use any global features of an event structure to
rescore it and outputs the highest scoring structure.
This is based on parse reranking (Ratnaparkhi, 1999;
Collins, 2000) but uses features on event structures
instead of syntactic constituency structures. We
used Mark Johnson?s cvlm estimator5 (Charniak
and Johnson, 2005) when learning weights for the
reranking model. Since the reranker can incorporate
the outputs from multiple decoders, we use it as an
ensemble technique as in Johnson and Ural (2010).
3 Extensions for BioNLP 2011
This section outlines the changes between our
BioNLP 2011 shared task submission and the sys-
tem described in McClosky et al (2011). The main
differences are that all dataset-specific portions of
the model have been factored out to handle the ex-
panded Genia (GE) dataset (Kim et al, 2011b) and
the new Epigenetics and Post-translational Modifi-
cations (EPI) and Infectious Diseases (ID) datasets
(Ohta et al, 2011; Pyysalo et al, 2011, respec-
tively). Other changes are relatively minor but doc-
umented here as implementation notes.
Several improvements were made to anchor de-
tection, improving its accuracy on all three do-
mains. The first is the use of distributional sim-
ilarity features. Using a large corpus of abstracts
from PubMed (30,963,886 word tokens of 335,811
word types), we cluster words by their syntactic con-
texts and morphological contents (Clark, 2003). We
used the Ney-Essen clustering model with morphol-
ogy to produce 45 clusters. Using these clusters, we
extended the feature set for anchor detection from
McClosky et al (2011) as follows: for each lexical-
ized feature we create an equivalent feature where
the corresponding word is replaced by its cluster ID.
This yielded consistent improvements of at least 1
percentage point in both anchor detection and event
5http://github.com/BLLIP/bllip-parser
42
extraction in the development partition of the GE
dataset.
Additionally, we improved the head percolation
rules for selecting the head of each multiword event
anchor. The new rules prohibit determiners and
prepositions from being heads, instead preferring
verbs, then nouns, then adjectives. There is also
a small stop list to prohibit the selection of certain
verbs (?has?, ?have?, ?is?, ?be?, and ?was?).
In event parsing, we used the morpha lemma-
tizer (Minnen et al, 2001) to stem words instead
of simply lowercasing them. This generally led to
a small but significant improvement in event extrac-
tion across the three domains. Additionally, we do
not use the feature selection mechanism described
in McClosky et al (2011) due to time restrictions.
It requires running all parsers twice which is espe-
cially cumbersome when operating in a round-robin
frame (as is required to train the reranker).
Also, note that our systems were only trained to
do Task 1 (or ?core?) roles for each dataset. This was
due to time restrictions and not system limitations.
3.1 Adapting to the Epigenetics track
For the EPI dataset, we adjusted our postprocessing
rules to handle the CATALYSIS event type. Similar
to REGULATION events in GE, CATALYSIS events do
not accept multiple CAUSE arguments. We handle
this by replicating such CATALYSIS events and as-
signing each new event a different CAUSE argument.
To adapt the ontology features in the parser (Mc-
Closky et al, 2011, ?3.3), we created a supertype for
all non-CATALYSIS events since they behave simi-
larly in many respects.
There are several possible areas for improvement
in handling this dataset. First, our internal imple-
mentation of the evaluation criteria differed from
the online scorer, sometimes by up to 6% f-score.
As a result, the reranker optimized a noisy version
of the evaluation criteria and potentially could have
performed better. It is unclear why our evaluator
scored EPI structures differently (it replicated the
scores for GE) but it is worthy of investigation. Sec-
ond, due to time constraints, we did not transfer the
parser or reranker consistency features (e.g., non-
REGULATION events should not take events as argu-
ments) or the type ontology in the reranker to the EPI
dataset. As a result, our results describe our system
with incomplete domain-specific knowledge.
3.2 Adapting to the Infectious Diseases track
Looking only at event types and their arguments, ID
is similar to GE. As a result, much of our domain-
specific processing code for this dataset is based on
code for GE. The key difference is that the GE post-
processing code removes event anchors with zero ar-
guments. Since ID allows PROCESS events to have
zero or one anchors, we added this as an exception.
Additionally, the ID dataset includes many nested
entities, e.g., two-component system entities contain
two other entities within their span. In almost all of
these cases, only the outermost entity takes part in
an event. To simplify processing, we removed all
nested entities. Any events attaching to a nested en-
tity were reattached to its outermost entity.
Given the similarities with GE, we explored sim-
ple domain adaptation by including the gold data
from GE along with our ID training data. To en-
sure that the GE data did not overwhelm the ID data,
we tried adding multiple copies of the ID data (see
Table 1 and the next section).
As in EPI, we adjusted the type ontology in the
parser for this dataset. This included ?core enti-
ties? (as defined by the task) and a ?PROTEIN-or-
REGULON-OPERON? type (the type of arguments for
GENE EXPRESSION and TRANSCRIPTION events).
Also as in EPI, the reranker did not use the updated
type ontology.
4 Results
For ID, we present experiments on merging GE with
ID data (Table 1). Since GE is much larger than
ID, we experimented with replicating the ID training
partition. Our best performance came from train-
ing on three copies of the ID data and the training
and development sections of GE. However, as the ta-
ble shows, performance is stable for more than two
copies of the ID data. Note that for this shared task
we simply merged the two domains. We did not
implement any domain adaptation techniques (e.g.,
labeling features based on the domain they come
from (Daume? III, 2007)).
Table 2 shows the performance of the various
parser decoders and their corresponding rerankers.
The last line in each domain block lists the score of
the reranker that uses candidates produced by all de-
43
coders. This reranking model always outperforms
the best individual parser. Furthermore, the rerank-
ing models on top of individual decoders help in all
but one situation (ID ? 2N decoder). To our knowl-
edge, our approach is the first to show that reranking
with features generated from global event structure
helps event extraction. Note that due to approximate
2N decoding in MSTParser, this decoder does not
produce true n-best candidates and generally out-
puts only a handful of unique parses. Because of
this, the corresponding rerankers suffer from insuf-
ficient training data and hurt performance in ID.
Finally, in Table 3, we give our results and rank-
ing on the official test sets. Our results are 6 f
points lower than the best submission in GE and EPI
and 5 points lower in ID. Considering that the we
used generic parsing tools with minimal customiza-
tion (e.g., our parsing models cannot extract directed
acyclic graph structures, which are common in this
data), we believe these results are respectable.
5 Conclusion
Our participation in the BioNLP shared task proves
that standard parsing tools (i.e., maximum-spanning
tree parsers, parse rerankers) can be successfully
used for event extraction. We achieved this by con-
verting the original event structures to a pseudo-
syntactic representation, where event arguments ap-
pear as modifiers to event anchors. Our analysis in-
dicates that reranking always helps, which proves
that there is merit in modeling non-local information
in biomolecular events. To our knowledge, our ap-
proach is the first to use parsing models for biomed-
ical event extraction.
During the shared task, we adapted our system
previously developed for the 2009 version of the
Genia dataset. This process required minimal ef-
fort: we did not add any new features to the pars-
ing model; we added only two domain-specific post-
processing steps (i.e., we allowed events without ar-
guments in ID and we replicated CATALYSIS events
with multiple CAUSE arguments in EPI). Our sys-
tem?s robust performance in all domains proves that
our approach is portable.
A desired side effect of our effort is that we
can easily incorporate any improvements to parsing
models (e.g., parsing of directed acyclic graphs, dual
decomposition, etc.) in our event extractor.
Model Prec Rec f-score
ID 59.3 38.0 46.3
(ID?1) + GE 52.0 40.2 45.3
(ID?2) + GE 52.4 41.7 46.4
(ID?3) + GE 54.8 45.0 49.4
(ID?4) + GE 55.2 43.8 48.9
(ID?5) + GE 55.1 44.7 49.4
Table 1: Impact of merging several copies of ID
training with GE training and development. Scores
on ID development data (2N parser only).
Decoder(s) Parser Reranker
1P 49.0 49.4
2P 49.5 50.5
1N 49.9 50.2
2N 46.5 47.9
All ? 50.7 ?
(a) Genia results (task 1)
Decoder(s) Parser Reranker
1P 62.3 63.3
2P 62.2 63.3
1N 62.9 64.6 ?
2N 60.8 63.8
All ? 64.1
(b) Epigenetics results (core task)
Decoder(s) Parser Reranker
1P 46.0 48.5
2P 47.8 49.8
1N 48.5 49.4
2N 49.4 48.8
All ? 50.2 ?
(c) Infectious Diseases results (core task)
Table 2: Results on development sections in
BioNLP f-scores. ??? indicates the submission
model for each domain.
Domain (task) Prec Rec f-score Ranking
GE (task 1) 61.1 42.4 50.0 7th
EPI (core) 70.2 56.9 62.8 5th
ID (core) 55.9 46.3 50.6 3rd
Table 3: BioNLP f-scores on the final test set.
44
Acknowledgments
We would like to thank the BioNLP shared task or-
ganizers for an enjoyable and interesting task and
their quick responses to questions. We would also
like to thank Sebastian Riedel for many interesting
discussions. We gratefully acknowledge the sup-
port of the Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0181.
References
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings of the BioNLP 2009Work-
shop Companion Volume for Shared Task, pages 10?
18, Boulder, Colorado, June. Association for Compu-
tational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In ACL. The Association for Computer
Linguistics.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the tenth Annual Meeting of
the European Association for Computational Linguis-
tics (EACL), pages 59?66.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Machine Learning: Pro-
ceedings of the Seventeenth International Conference
(ICML 2000), pages 175?182, Stanford, California.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In Conference of the Association for Computa-
tional Linguistics (ACL), Prague, Czech Republic.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed hierarchies representation.
In Proceedings of the COLING Workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mark Johnson and Ahmet Engin Ural. 2010. Rerank-
ing the berkeley and brown parsers. In Proceedings of
the HLT: North American Chapter of the ACL (HLT-
NAACL), pages 665?668. Association for Computa-
tional Linguistics, June.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview
of BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
David McClosky, Mihai Surdeanu, and Chris Manning.
2011. Event extraction as dependency parsing. In
Proceedings of the Association for Computational Lin-
guistics: Human Language Technologies 2011 Confer-
ence (ACL-HLT?11), Main Conference, Portland, Ore-
gon, June.
David McClosky. 2010. Any Domain Parsing: Auto-
matic Domain Adaptation for Parsing. Ph.D. thesis,
Computer Science Department, Brown University.
Ryan T. McDonald and Fernando C. N. Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In Proceedings of EACL. The Association
for Computer Linguistics.
Ryan T. McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT/EMNLP. The Association for Computational
Linguistics.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(03):207?223.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1-3):151?175.
Sebastian Riedel, David McClosky, Mihai Surdeanu, An-
drew McCallum, and Christopher D. Manning. 2011.
Model Combination for Event Extraction in BioNLP
2011. In BioNLP 2011 Shared Task.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources. In
Proceedings of the BioNLP 2011 Workshop Compan-
ion Volume for Shared Task, Portland, Oregon, June.
Association for Computational Linguistics.
45
Proceedings of BioNLP Shared Task 2011 Workshop, pages 51?55,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Model Combination for Event Extraction in BioNLP 2011
Sebastian Riedela, David McCloskyb, Mihai Surdeanub,
Andrew McCalluma, and Christopher D. Manningb
a Department of Computer Science, University of Massachusetts at Amherst
b Department of Computer Science, Stanford University
{riedel,mccallum}@cs.umass.edu
{mcclosky,mihais,manning}@stanford.edu
Abstract
We describe the FAUST entry to the BioNLP
2011 shared task on biomolecular event ex-
traction. The FAUST system explores sev-
eral stacking models for combination using
as base models the UMass dual decomposi-
tion (Riedel and McCallum, 2011) and Stan-
ford event parsing (McClosky et al, 2011b)
approaches. We show that using stacking is
a straightforward way to improving perfor-
mance for event extraction and find that it is
most effective when using a small set of stack-
ing features and the base models use slightly
different representations of the input data. The
FAUST system obtained 1st place in three out
of four tasks: 1st place in Genia Task 1 (56.0%
f-score) and Task 2 (53.9%), 2nd place in the
Epigenetics and Post-translational Modifica-
tions track (35.0%), and 1st place in the In-
fectious Diseases track (55.6%).
1 Introduction
To date, most approaches to the BioNLP event ex-
traction task (Kim et al, 2011a) use a single model
to produce their output. However, model combina-
tion techniques such as voting, stacking, and rerank-
ing have been shown to consistently produce higher
performing systems by taking advantage of multi-
ple views of the same data. The Netflix Prize (Ben-
nett et al, 2007) is a prime example of this. System
combination essentially allows systems to regular-
ize each other, smoothing over the artifacts of each
(c.f. Nivre and McDonald (2008), Surdeanu and
Manning (2010)). To our knowledge, the only previ-
ous example of model combination for the BioNLP
shared task was performed by Kim et al (2009). Us-
ing a weighted voting scheme to combine the out-
puts from the top six systems, they obtained a 4%
absolute f-score improvement over the best individ-
ual system.
This paper shows that using a straightforward
model combination strategy on two competitive
systems produces a new system with substantially
higher accuracy. This is achieved with the frame-
work of stacking: a stacking model uses the output
of a stacked model as additional features.
While we initially considered voting and rerank-
ing model combination strategies, it seemed that
given the performance gap between the UMass and
Stanford systems that the best option was to in-
clude the predictions from the Stanford system into
the UMass system (e.g., as in Nivre and McDon-
ald (2008)). This has the advantage that one model
(Umass) determines how to integrate the outputs of
the other model (Stanford) into its own structure,
whereas in reranking, for example, the combined
model is required to output a complete structure pro-
duced by only one of the input models.
2 Approach
In the following we briefly present both the stacking
and the stacked model and some possible ways of
integrating the stacked information.
2.1 Stacking Model
As our stacking model, we employ the UMass ex-
tractor (Riedel and McCallum, 2011). It is based on
a discriminatively trained model that jointly predicts
trigger labels, event arguments and protein pairs in
51
binding. We will briefly describe this model but first
introduce three types of binary variables that will
represent events in a given sentence. Variables ei,t
are active if and only if the token at position i has
the label t. Variables ai,j,r are active if and only if
there is an event with trigger i that has an argument
with role r grounded at token j. In the case of an
entity mention this means that the mention?s head is
j. In the case of an event j is the position of its trig-
ger. Finally, variables bp,q indicate whether or not
two entity mentions at p and q appear as arguments
in the same binding event.
Two parts form our model: a scoring function, and
a set of constraints. The scoring function over the
trigger variables e, argument variables a and binding
pair variables b is
s (e,a,b) def=
?
ei,t=1
sT (i, t) +
?
ai,j,r=1
sR (i, j, r)+
?
bp,q=1
sB (p, q)
with local scoring functions sT (i, t)
def=
?wT, fT (i, t)?, sR (i, j, r)
def= ?wR, fR (i, j, r)?
and sB (p, q)
def= ?wB, fB (p, q)?.
Our model scores all parts of the structure in iso-
lation. It is a joint model due to the nature of the
constraints we enforce: First, we require that each
active event trigger must have at least one Theme ar-
gument; second, only regulation events (or Catalysis
events for the EPI track) are allowed to have Cause
arguments; third, any trigger that is itself an argu-
ment of another event has to be labelled active, too;
finally, if we decide that two entities p and q are part
of the same binding (as indicated by bp,q = 1), there
needs to be a binding event at some trigger i that
has p and q as arguments. We will denote the set of
structures (e,a,b) that satisfy these constraints as
Y .
Stacking with this model is simple: we only
need to augment the local feature functions fT (i, t),
fR (i, j, r) and fB (p, q) to include predictions from
the systems to be stacked. For example, for every
system S to be stacked and every pair of event types
(t?, tS) we add the features
fS,t? ,tS (i, t) =
{
1 hS (i) = tS ? t? = t
0 otherwise
to fT (i, t). Here hS (i) is the event label given to to-
ken i according to S. These features allow different
weights to be given to each possible combination of
type t? that we want to assign, and type tS that S
predicts.
Inference in this model amounts to maximizing
s (e,a,b) over Y . Our approach to solving this
problem is dual decomposition (Komodakis et al,
2007; Rush et al, 2010). We divide the problem into
three subproblems: (1) finding the best trigger label
and set of outgoing edges for each candidate trigger;
(2) finding the best trigger label and set of incoming
edges for each candidate trigger; (3) finding the best
pairs of entities to appear in the same binding. Due
to space limitations we refer the reader to Riedel and
McCallum (2011) for further details.
2.2 Stacked Model
For the stacked model, we use a system based on an
event parsing framework (McClosky et al, 2011a)
referred to as the Stanford model in this paper. This
model converts event structures to dependency trees
which are parsed using MSTParser (McDonald et
al., 2005).1 Once parsed, the resulting dependency
tree is converted back to event structures. Using the
Stanford model as the stacked model is helpful since
it captures tree structure which is not the focus in
the UMass model. Of course, this is also a limita-
tion since actual BioNLP event graphs are DAGs,
but the model does well considering these restric-
tions. Additionally, this constraint encourages the
Stanford model to provide different (and thus more
useful for stacking) results.
Of particular interest to this paper are the four
possible decoders in MSTParser. These four de-
coders come from combinations of feature order
(first or second) and whether the resulting depen-
dency tree is required to be projective.2 Each de-
coder presents a slightly different view of the data
and thus has different model combination proper-
ties. Projectivity constraints are not captured in the
UMass model so these decoders incorporate novel
information.
To produce stacking output from the Stanford sys-
tem, we need its predictions on the training, devel-
1http://sourceforge.net/projects/mstparser/
2For brevity, the second-order non-projective decoder is ab-
breviated as 2N, first-order projective as 1P, etc.
52
UMass FAUST+All
R P F1 R P F1
GE T1 48.5 64.1 55.2 49.4 64.8 56.0
GE T2 43.9 60.9 51.0 46.7 63.8 53.9
EPI (F) 28.1 41.6 33.5 28.9 44.5 35.0
EPI (C) 57.0 73.3 64.2 59.9 80.3 68.6
ID (F) 46.9 62.0 53.4 48.0 66.0 55.6
ID (C) 49.5 62.1 55.1 50.6 66.1 57.3
Table 1: Results on test sets of all tasks we submitted to.
T1 and T2 stand for task 1 and 2, respectively. C stands
for CORE metric, F for FULL metric.
opment and test sets. For predictions on test and de-
velopment sets we used models learned from the the
complete training set. Predictions over training data
were produced using crossvalidation. This helps to
avoid a scenario where the stacking model learns to
rely on high accuracy at training time that cannot be
matched at test time.
Note that, unlike Stanford?s individual submission
in this shared task, the stacked models in this paper
do not include the Stanford reranker. This is because
it would have required making a reranker model for
each crossvalidation fold.
We made 19 crossvalidation training folds for Ge-
nia (GE) (Kim et al, 2011b), 12 for Epigenetics
(EPI), and 17 for Infectious Diseases (ID) (Kim et
al., 2011b; Ohta et al, 2011; Pyysalo et al, 2011,
respectively). Note that while ID is the smallest and
would seem like it would have the fewest folds, we
combined the training data of ID with the training
and development data from GE. To produce predic-
tions over the test data, we combined the training
folds with 6 development folds for GE, 4 for EPI,
and 1 for ID.
3 Experiments
Table 1 gives an overview of our results on the test
sets for all four tasks we submitted to. Note that
for the EPI and ID tasks we show the CORE metric
next to the official FULL metric. The former is suit-
able for our purposes because it does not measure
performance for negations, speculations and cellular
locations?all of these we did not attempt to predict.
We compare the UMass standalone system to the
FAUST+All system which stacks the Stanford 1N,
1P, 2N and 2P predictions. For all four tasks we
System SVT BIND REG TOTAL
UMass 74.7 47.7 42.8 54.8
Stanford 1N 71.4 38.6 32.8 47.8
Stanford 1P 70.8 35.9 31.1 46.5
Stanford 2N 69.1 35.0 27.8 44.3
Stanford 2P 72.0 36.2 32.2 47.4
FAUST+All 76.9 43.5 44.0 55.9
FAUST+1N 76.4 45.1 43.8 55.6
FAUST+1P 75.8 43.1 44.6 55.7
FAUST+2N 74.9 42.8 43.8 54.9
FAUST+2P 75.7 46.0 44.1 55.7
FAUST+All 76.4 41.2 43.1 54.9
(triggers)
FAUST+All 76.1 41.7 43.6 55.1
(arguments)
Table 2: BioNLP f-scores on the development section of
the Genia track (task 1) for several event categories.
observe substantial improvements due to stacking.
The increase is particular striking for the EPI track,
where stacking improves f-score by more than 4.0
points on the CORE metric.
To analyze the impact of stacking further, Ta-
ble 2 shows a breakdown of our results on the Ge-
nia development set. Presented are f-scores for sim-
ple events (SVT), binding events (BIND), regulation
events (REG) and the set of all event types (TOTAL).
We compare the UMass standalone system, various
Stanford-standalone models and stacked versions of
these (FAUST+X).
Remarkably, while there is a 7 point gap between
the best individual Stanford system and the stand-
alone UMass systems, integrating the Stanford pre-
diction still leads to an f-score improvement of 1.
This can be seen when comparing the UMass, Stan-
ford 1N and FAUST+All results, where the latter
stacks 1N, 1P, 2N and 2P. We also note that stack-
ing the projective 1P and 2P systems helps almost
as much as stacking all Stanford systems. Notably,
both 1P and 2P do not do as well in isolation when
compared to the 1N system. When stacked, how-
ever, they do slightly better. This suggests that pro-
jectivity is a missing aspect in the UMass standalone
system.
The FAUST+All (triggers) and FAUST+All (ar-
guments) lines represent experiments to determine
whether it is useful to incorporate only portions of
53
the stacking information from the Stanford system.
Given the small gains over the original UMass sys-
tem, it is clear that stacking information is only use-
ful when attached to triggers and arguments. Our
theory is that most of our gains come from when the
UMass and Stanford systems disagree on triggers
and the Stanford system provides not only its trig-
gers but also their attached arguments to the UMass
system. This is supported by a pilot experiment
where we trained the Stanford model to use the
UMass triggers and saw no benefit from stacking
(even when both triggers and arguments were used).
Table 3 shows our results on the development set
of the ID task, this time in terms of recall, precision
and f-score. Here the gap between Stanford-only
results, and the UMass results, is much smaller. This
seems to lead to more substantial improvements for
stacking: FAUST+All obtains a f-score 2.2 points
larger than the standalone UMass system. Also note
that, similarly to the previous table, the projective
systems do worse on their own, but are more useful
when stacked.
Another possible approach to stacking conjoins
all the original features of the stacking model with
the predicted features of the stacked model. The
hope is that this allows the learner to give differ-
ent weights to the stacked predictions in different
contexts. However, incorporating Stanford predic-
tions by conjoining them with all features of the
UMass standalone system (FAUST+2P-Conj in Ta-
ble 3) does not help here.
We note that for our results on the ID task we
augment the training data with events from the GE
training set. Merging both training sets is reasonable
since there is a significant overlap between both in
terms of events as well as lexical and syntactic pat-
terns to express these. When building our training
set we add each training document from GE once,
and each ID training document twice?this lead to
substantially better results than including ID data
only once.
4 Discussion
Generally stacking has led to substantial improve-
ments across the board. There are, however, some
exceptions. One is binding events for the GE task.
Here the UMass model still outperforms the best
System Rec Prec F1
UMass 46.2 51.1 48.5
Stanford 1N 43.1 49.1 45.9
Stanford 1P 40.8 46.7 43.5
Stanford 2N 41.6 53.9 46.9
Stanford 2P 42.8 48.1 45.3
FAUST+All 47.6 54.3 50.7
FAUST+1N 45.8 51.6 48.5
FAUST+1P 47.6 52.8 50.0
FAUST+2N 45.4 52.4 48.6
FAUST+2P 49.1 52.6 50.7
FAUST+2P-Conj 48.0 53.2 50.4
Table 3: Results on the development set for the ID track.
stacked system (see Table 2). Likewise, for full pa-
pers in the Genia test set, the UMass model still does
slightly better with 53.1 f-score compared to 52.7
f-score. This suggests that a more informed com-
bination of our systems (e.g., metaclassifiers) could
lead to better performance.
5 Conclusion
We have presented the FAUST entry to the BioNLP
2011 shared task on biomolecular event extraction.
It is based on stacking, a simple approach for model
combination. By using the predictions of the Stan-
ford entry as features of the UMass model, we sub-
stantially improved upon both systems in isolation.
This helped us to rank 1st in three of the four tasks
we submitted results to. Remarkably, in some cases
we observed improvements despite a 7.0 f-score
margin between the models we combined.
In the future we would like to investigate alter-
native means for model combination such as rerank-
ing, union, intersection, and other voting techniques.
We also plan to use dual decomposition to encourage
models to agree. In particular, we will seek to incor-
porate an MST component into the dual decomposi-
tion algorithm used by the UMass system.
Acknowledgments
We thank the BioNLP shared task organizers for setting this
up and their quick responses to questions. This work was sup-
ported in part by the Center for Intelligent Information Re-
trieval. We gratefully acknowledge the support of the Defense
Advanced Research Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181.
54
References
James Bennett, Stan Lanning, and Netflix. 2007. The
netflix prize. In KDD Cup and Workshop in conjunc-
tion with KDD.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of the Workshop on BioNLP: Shared Task,
pages 1?9. Association for Computational Linguistics.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview
of BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Nikos Komodakis, Nikos Paragios, and Georgios Tziri-
tas. 2007. MRF optimization via dual decomposition:
Message-passing revisited. In ICCV.
David McClosky, Mihai Surdeanu, and Chris Manning.
2011a. Event extraction as dependency parsing. In
Proceedings of the Association for Computational Lin-
guistics: Human Language Technologies 2011 Confer-
ence (ACL-HLT?11), Main Conference, Portland, Ore-
gon, June.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011b. Event extraction as dependency
parsing in BioNLP 2011. In BioNLP 2011 Shared
Task.
Ryan T. McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT/EMNLP. The Association for Computational
Linguistics.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT, pages 950?958,
Columbus, Ohio, June. Association for Computational
Linguistics.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sebastian Riedel and Andrew McCallum. 2011. Ro-
bust biomedical event extraction with dual decomposi-
tion and minimal domain adaptation. In BioNLP 2011
Shared Task.
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proc. EMNLP.
Mihai Surdeanu and Christopher D. Manning. 2010. En-
semble models for dependency parsing: Cheap and
good? In Proceedings of the North American Chapter
of the Association for Computational Linguistics Con-
ference (NAACL-2010), Los Angeles, CA, June.
55
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 28?34,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Stanford?s Multi-Pass Sieve Coreference Resolution System at the
CoNLL-2011 Shared Task
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers,
Mihai Surdeanu, Dan Jurafsky
Stanford NLP Group
Stanford University, Stanford, CA 94305
{heeyoung,peirsman,angelx,natec,mihais,jurafsky}@stanford.edu
Abstract
This paper details the coreference resolution
system submitted by Stanford at the CoNLL-
2011 shared task. Our system is a collection
of deterministic coreference resolution mod-
els that incorporate lexical, syntactic, seman-
tic, and discourse information. All these mod-
els use global document-level information by
sharing mention attributes, such as gender and
number, across mentions in the same cluster.
We participated in both the open and closed
tracks and submitted results using both pre-
dicted and gold mentions. Our system was
ranked first in both tracks, with a score of 57.8
in the closed track and 58.3 in the open track.
1 Introduction
This paper describes the coreference resolution sys-
tem used by Stanford at the CoNLL-2011 shared
task (Pradhan et al, 2011). Our system extends
the multi-pass sieve system of Raghunathan et
al. (2010), which applies tiers of deterministic coref-
erence models one at a time from highest to lowest
precision. Each tier builds on the entity clusters con-
structed by previous models in the sieve, guarantee-
ing that stronger features are given precedence over
weaker ones. Furthermore, this model propagates
global information by sharing attributes (e.g., gender
and number) across mentions in the same cluster.
We made three considerable extensions to the
Raghunathan et al (2010) model. First, we added
five additional sieves, the majority of which address
the semantic similarity between mentions, e.g., us-
ing WordNet distance, and shallow discourse under-
standing, e.g., linking speakers to compatible pro-
nouns. Second, we incorporated a mention detection
sieve at the beginning of the processing flow. This
sieve filters our syntactic constituents unlikely to be
mentions using a simple set of rules on top of the
syntactic analysis of text. And lastly, we added a
post-processing step, which guarantees that the out-
put of our system is compatible with the shared task
and OntoNotes specifications (Hovy et al, 2006;
Pradhan et al, 2007).
Using this system, we participated in both the
closed1 and open2 tracks, using both predicted and
gold mentions. Using predicted mentions, our sys-
tem had an overall score of 57.8 in the closed track
and 58.3 in the open track. These were the top scores
in both tracks. Using gold mentions, our system
scored 60.7 in the closed track in 61.4 in the open
track.
We describe the architecture of our entire system
in Section 2. In Section 3 we show the results of sev-
eral experiments, which compare the impact of the
various features in our system, and analyze the per-
formance drop as we switch from gold mentions and
annotations (named entity mentions and parse trees)
to predicted information. We also report in this sec-
tion our official results in the testing partition.
1Only the provided data can be used, i.e., WordNet and gen-
der gazetteer.
2Any external knowledge source can be used. We used
additional animacy, gender, demonym, and country and states
gazetteers.
28
2 System Architecture
Our system consists of three main stages: mention
detection, followed by coreference resolution, and
finally, post-processing. In the first stage, mentions
are extracted and relevant information about men-
tions, e.g., gender and number, is prepared for the
next step. The second stage implements the ac-
tual coreference resolution of the identified men-
tions. Sieves in this stage are sorted from highest
to lowest precision. For example, the first sieve (i.e.,
highest precision) requires an exact string match be-
tween a mention and its antecedent, whereas the
last one (i.e., lowest precision) implements pronom-
inal coreference resolution. Post-processing is per-
formed to adjust our output to the task specific con-
straints, e.g., removing singletons.
It is important to note that the first system stage,
i.e., the mention detection sieve, favors recall heav-
ily, whereas the second stage, which includes the ac-
tual coreference resolution sieves, is precision ori-
ented. Our results show that this design lead to
state-of-the-art performance despite the simplicity
of the individual components. This strategy has
been successfully used before for information ex-
traction, e.g., in the BioNLP 2009 event extraction
shared task (Kim et al, 2009), several of the top sys-
tems had a first high-recall component to identify
event anchors, followed by high-precision classi-
fiers, which identified event arguments and removed
unlikely event candidates (Bjo?rne et al, 2009). In
the coreference resolution space, several works have
shown that applying a list of rules from highest to
lowest precision is beneficial for coreference reso-
lution (Baldwin, 1997; Raghunathan el al., 2010).
However, we believe we are the first to show that this
high-recall/high-precision strategy yields competi-
tive results for the complete task of coreference res-
olution, i.e., including mention detection and both
nominal and pronominal coreference.
2.1 Mention Detection Sieve
In our particular setup, the recall of the mention de-
tection component is more important than its preci-
sion, because any missed mentions are guaranteed
to affect the final score, but spurious mentions may
not impact the overall score if they are left as sin-
gletons, which are discarded by our post-processing
step. Therefore, our mention detection algorithm fo-
cuses on attaining high recall rather than high preci-
sion. We achieve our goal based on the list of sieves
sorted by recall (from highest to lowest). Each sieve
uses syntactic parse trees, identified named entity
mentions, and a few manually written patterns based
on heuristics and OntoNotes specifications (Hovy et
al., 2006; Pradhan et al, 2007). In the first and
highest recall sieve, we mark all noun phrase (NP),
possessive pronoun, and named entity mentions in
each sentence as candidate mentions. In the follow-
ing sieves, we remove from this set al mentions that
match any of the exclusion rules below:
1. We remove a mention if a larger mention with
the same head word exists, e.g., we remove The
five insurance companies in The five insurance
companies approved to be established this time.
2. We discard numeric entities such as percents,
money, cardinals, and quantities, e.g., 9%,
$10, 000, Tens of thousands, 100 miles.
3. We remove mentions with partitive or quanti-
fier expressions, e.g., a total of 177 projects.
4. We remove pleonastic it pronouns, detected us-
ing a set of known expressions, e.g., It is possi-
ble that.
5. We discard adjectival forms of nations, e.g.,
American.
6. We remove stop words in a predetermined list
of 8 words, e.g., there, ltd., hmm.
Note that the above rules extract both mentions in
appositive and copulative relations, e.g., [[Yongkang
Zhou] , the general manager] or [Mr. Savoca] had
been [a consultant. . . ]. These relations are not an-
notated in the OntoNotes corpus, e.g., in the text
[[Yongkang Zhou] , the general manager], only the
larger mention is annotated. However, appositive
and copulative relations provide useful (and highly
precise) information to our coreference sieves. For
this reason, we keep these mentions as candidates,
and remove them later during post-processing.
2.2 Mention Processing
Once mentions are extracted, we sort them by sen-
tence number, and left-to-right breadth-first traversal
29
order in syntactic trees in the same sentence (Hobbs,
1977). We select for resolution only the first men-
tions in each cluster,3 for two reasons: (a) the first
mention tends to be better defined (Fox, 1993),
which provides a richer environment for feature ex-
traction; and (b) it has fewer antecedent candidates,
which means fewer opportunities to make a mis-
take. For example, given the following ordered list
of mentions, {m11, m
2
2, m
2
3, m
3
4, m
1
5, m
2
6}, where
the subscript indicates textual order and the super-
script indicates cluster id, our model will attempt
to resolve only m22 and m
3
4. Furthermore, we dis-
card first mentions that start with indefinite pronouns
(e.g., some, other) or indefinite articles (e.g., a, an)
if they have no antecedents that have the exact same
string extents.
For each selected mention mi, all previous men-
tions mi?1, . . . , m1 become antecedent candidates.
All sieves traverse the candidate list until they find
a coreferent antecedent according to their criteria
or reach the end of the list. Crucially, when com-
paring two mentions, our approach uses informa-
tion from the entire clusters that contain these men-
tions instead of using just information local to the
corresponding mentions. Specifically, mentions in
a cluster share their attributes (e.g., number, gen-
der, animacy) between them so coreference decision
are better informed. For example, if a cluster con-
tains two mentions: a group of students, which is
singular, and five students, which is plural,
the number attribute of the entire cluster becomes
singular or plural, which allows it to match
other mentions that are both singular and plural.
Please see (Raghunathan et al, 2010) for more de-
tails.
2.3 Coreference Resolution Sieves
2.3.1 Core System
The core of our coreference resolution system is
an incremental extension of the system described in
Raghunathan et al (2010). Our core model includes
two new sieves that address nominal mentions and
are inserted based on their precision in a held-out
corpus (see Table 1 for the complete list of sieves
deployed in our system). Since these two sieves use
3We initialize the clusters as singletons and grow them pro-
gressively in each sieve.
Ordered sieves
1. Mention Detection Sieve
2. Discourse Processing Sieve
3. Exact String Match Sieve
4. Relaxed String Match Sieve
5. Precise Constructs Sieve (e.g., appositives)
6-8. Strict Head Matching Sieves A-C
9. Proper Head Word Match Sieve
10. Alias Sieve
11. Relaxed Head Matching Sieve
12. Lexical Chain Sieve
13. Pronouns Sieve
Table 1: The sieves in our system; sieves new to this pa-
per are in bold.
simple lexical constraints without semantic informa-
tion, we consider them part of the baseline model.
Relaxed String Match: This sieve considers two
nominal mentions as coreferent if the strings ob-
tained by dropping the text following their head
words are identical, e.g., [Clinton] and [Clinton,
whose term ends in January].
Proper Head Word Match: This sieve marks two
mentions headed by proper nouns as coreferent if
they have the same head word and satisfy the fol-
lowing constraints:
Not i-within-i - same as Raghunathan et al (2010).
No location mismatches - the modifiers of two men-
tions cannot contain different location named entities,
other proper nouns, or spatial modifiers. For example,
[Lebanon] and [southern Lebanon] are not coreferent.
No numeric mismatches - the second mention cannot
have a number that does not appear in the antecedent, e.g.,
[people] and [around 200 people] are not coreferent.
In addition to the above, a few more rules are
added to get better performance for predicted men-
tions.
Pronoun distance - sentence distance between a pronoun
and its antecedent cannot be larger than 3.
Bare plurals - bare plurals are generic and cannot have a
coreferent antecedent.
2.3.2 Semantic-Similarity Sieves
We first extend the above system with two
new sieves that exploit semantics from WordNet,
Wikipedia infoboxes, and Freebase records, drawing
on previous coreference work using these databases
(Ng & Cardie, 2002; Daume? & Marcu, 2005;
Ponzetto & Strube, 2006; Ng, 2007; Yang & Su,
30
2007; Bengston & Roth, 2008; Huang et al, 2009;
inter alia). Since the input to a sieve is a collection of
mention clusters built by the previous (more precise)
sieves, we need to link mention clusters (rather than
individual mentions) to records in these three knowl-
edge bases. The following steps generate a query for
these resources from a mention cluster.
First, we select the most representative mention
in a cluster by preferring mentions headed by proper
nouns to mentions headed by common nouns, and
nominal mentions to pronominal ones. In case of
ties, we select the longer string. For example, the
mention selected from the cluster {President George
W. Bush, president, he} is President George W.
Bush. Second, if this mention returns nothing from
the knowledge bases, we implement the following
query relaxation algorithm: (a) remove the text fol-
lowing the mention head word; (b) select the lowest
noun phrase (NP) in the parse tree that includes the
mention head word; (c) use the longest proper noun
(NNP*) sequence that ends with the head word; (d)
select the head word. For example, the query pres-
ident Bill Clinton, whose term ends in January is
successively changed to president Bill Clinton, then
Bill Clinton, and finally Clinton. If multiple records
are returned, we keep the top two for Wikipedia and
Freebase, and all synsets for WordNet.
Alias Sieve
This sieve addresses name aliases, which are de-
tected as follows. Two mentions headed by proper
nouns are marked as aliases (and stored in the same
entity cluster) if they appear in the same Wikipedia
infobox or Freebase record in either the ?name? or
?alias? field, or they appear in the same synset in
WordNet. As an example, this sieve correctly de-
tects America Online and AOL as aliases. We also
tested the utility of Wikipedia categories, but found
little gain over morpho-syntactic features.
Lexical Chain Sieve
This sieve marks two nominal mentions as coref-
erent if they are linked by a WordNet lexical chain
that traverses hypernymy or synonymy relations. We
use all synsets for each mention, but restrict it to
mentions that are at most three sentences apart, and
lexical chains of length at most four. This sieve cor-
rectly links Britain with country, and plane with air-
craft.
To increase the precision of the above two sieves,
we use additional constraints before two mentions
can match: attribute agreement (number, gender, an-
imacy, named entity labels), no i-within-i, no loca-
tion or numeric mismatches (as in Section 2.3.1),
and we do not use the abstract entity synset in Word-
Net, except in chains that include ?organization?.
2.3.3 Discourse Processing Sieve
This sieve matches speakers to compatible pro-
nouns, using shallow discourse understanding to
handle quotations and conversation transcripts. Al-
though more complex discourse constraints have
been proposed, it has been difficult to show improve-
ments (Tetreault & Allen, 2003; 2004).
We begin by identifying speakers within text. In
non-conversational text, we use a simple heuristic
that searches for the subjects of reporting verbs (e.g.,
say) in the same sentence or neighboring sentences
to a quotation. In conversational text, speaker infor-
mation is provided in the dataset.
The extracted speakers then allow us to imple-
ment the following sieve heuristics:
? ?I?s4 assigned to the same speaker are coreferent.
? ?you?s with the same speaker are coreferent.
? The speaker and ?I?s in her text are coreferent.
For example, I, my, and she in the following sen-
tence are coreferent: ?[I] voted for [Nader] because
[he] was most aligned with [my] values,? [she] said.
In addition to the above sieve, we impose speaker
constraints on decisions made by subsequent sieves:
? The speaker and a mention which is not ?I? in the
speaker?s utterance cannot be coreferent.
? Two ?I?s (or two ?you?s, or two ?we?s) assigned to
different speakers cannot be coreferent.
? Two different person pronouns by the same speaker
cannot be coreferent.
? Nominal mentions cannot be coreferent with ?I?,
?you?, or ?we? in the same turn or quotation.
? In conversations, ?you? can corefer only with the
previous speaker.
For example, [my] and [he] are not coreferent in the
above example (third constraint).
4We define ?I? as ?I?, ?my?, ?me?, or ?mine?, ?we? as first
person plural pronouns, and ?you? as second person pronouns.
31
Annotations Coref R P F1
Gold Before 92.8 37.7 53.6
Gold After 75.1 70.1 72.6
Not gold Before 87.9 35.6 50.7
Not gold After 71.7 68.4 70.0
Table 2: Performance of the mention detection compo-
nent, before and after coreference resolution, with both
gold and actual linguistic annotations.
2.4 Post Processing
To guarantee that the output of our system matches
the shared task requirements and the OntoNotes
annotation specification, we implement two post-
processing steps:
? We discard singleton clusters.
? We discard the mention that appears later in
text in appositive and copulative relations. For
example, in the text [[Yongkang Zhou] , the
general manager] or [Mr. Savoca] had been
[a consultant. . . ], the mentions Yongkang Zhou
and a consultant. . . are removed in this stage.
3 Results and Discussion
Table 2 shows the performance of our mention de-
tection algorithm. We show results before and after
coreference resolution and post-processing (when
singleton mentions are removed). We also list re-
sults with gold and predicted linguistic annotations
(i.e., syntactic parses and named entity recognition).
The table shows that the recall of our approach is
92.8% (if gold annotations are used) or 87.9% (with
predicted annotations). In both cases, precision is
low because our algorithm generates many spurious
mentions due to its local nature. However, as the ta-
ble indicates, many of these mentions are removed
during post-processing, because they are assigned
to singleton clusters during coreference resolution.
The two main causes for our recall errors are lack
of recognition of event mentions (e.g., verbal men-
tions such as growing) and parsing errors. Parsing
errors often introduce incorrect mention boundaries,
which yield both recall and precision errors. For
example, our system generates the predicted men-
tion, the working meeting of the ?863 Program? to-
day, for the gold mention the working meeting of the
?863 Program?. Due to this boundary mismatch,
all mentions found to be coreferent with this pre-
dicted mention are counted as precision errors, and
all mentions in the same coreference cluster with the
gold mention are counted as recall errors.
Table 3 lists the results of our end-to-end system
on the development partition. ?External Resources?,
which were used only in the open track, includes: (a)
a hand-built list of genders of first names that we cre-
ated, incorporating frequent names from census lists
and other sources, (b) an animacy list (Ji and Lin,
2009), (c) a country and state gazetteer, and (d) a de-
monym list. ?Discourse? stands for the sieve intro-
duced in Section 2.3.3. ?Semantics? stands for the
sieves presented in Section 2.3.2. The table shows
that the discourse sieve yields an improvement of
almost 2 points to the overall score (row 1 versus
3), and external resources contribute 0.5 points. On
the other hand, the semantic sieves do not help (row
3 versus 4). The latter result contradicts our initial
experiments, where we measured a minor improve-
ment when these sieves were enabled and gold men-
tions were used. Our hypothesis is that, when pre-
dicted mentions are used, the semantic sieves are
more likely to link spurious mentions to existing
clusters, thus introducing precision errors. This sug-
gests that a different tuning of the sieve parameters
is required for the predicted mention scenario. For
this reason, we did not use the semantic sieves for
our submission. Hence, rows 2 and 3 in the table
show the performance of our official submission in
the development set, in the closed and open tracks
respectively.
The last three rows in Table 3 give insight on the
impact of gold information. This analysis indicates
that using gold linguistic annotation yields an im-
provement of only 2 points. This implies that the
quality of current linguistic processors is sufficient
for the task of coreference resolution. On the other
hand, using gold mentions raises the overall score by
15 points. This clearly indicates that pipeline archi-
tectures where mentions are identified first are inad-
equate for this task, and that coreference resolution
might benefit from the joint modeling of mentions
and coreference chains.
Finally, Table 4 lists our results on the held-out
testing partition. Note that in this dataset, the gold
mentions included singletons and generic mentions
32
Components MUC B3 CEAFE BLANC
ER D S GA GM R P F1 R P F1 R P F1 R P F1 avg F1?
58.8 56.5 57.6 68.0 68.7 68.4 44.8 47.1 45.9 68.8 73.5 70.9 57.3?
59.1 57.5 58.3 69.2 71.0 70.1 46.5 48.1 47.3 72.2 78.1 74.8 58.6? ?
60.1 59.5 59.8 69.5 71.9 70.7 46.5 47.1 46.8 73.8 78.6 76.0 59.1? ? ?
60.3 58.5 59.4 69.9 71.1 70.5 45.6 47.3 46.4 73.9 78.2 75.8 58.8? ? ?
63.8 61.5 62.7 71.4 72.3 71.9 47.1 49.5 48.3 75.6 79.6 77.5 61.0? ? ?
73.6 90.0 81.0 69.8 89.2 78.3 79.4 52.5 63.2 79.1 89.2 83.2 74.2? ? ? ?
74.0 90.1 81.3 70.2 89.3 78.6 79.7 53.1 63.7 79.5 89.6 83.6 74.5
Table 3: Comparison between various configurations of our system. ER, D, S stand for External Resources, Discourse,
and Semantics sieves. GA and GM stand for Gold Annotations, and Gold Mentions. The top part of the table shows
results using only predicted annotations and mentions, whereas the bottom part shows results of experiments with gold
information. Avg F1 is the arithmetic mean of MUC, B3, and CEAFE. We used the development partition for these
experiments.
MUC B3 CEAFE BLANC
Track Gold Mention Boundaries R P F1 R P F1 R P F1 R P F1 avg F1
Close Not Gold 61.8 57.5 59.6 68.4 68.2 68.3 43.4 47.8 45.5 70.6 76.2 73.0 57.8
Open Not Gold 62.8 59.3 61.0 68.9 69.0 68.9 43.3 46.8 45.0 71.9 76.6 74.0 58.3
Close Gold 65.9 62.1 63.9 69.5 70.6 70.0 46.3 50.5 48.3 72.0 78.6 74.8 60.7
Open Gold 66.9 63.9 65.4 70.1 71.5 70.8 46.3 49.6 47.9 73.4 79.0 75.8 61.4
Table 4: Results on the official test set.
as well, whereas in development (lines 6 and 7 in Ta-
ble 3), gold mentions included only mentions part of
an actual coreference chain. This explains the large
difference between, say, line 6 in Table 3 and line 4
in Table 4.
Our scores are comparable to previously reported
state-of-the-art results for coreference resolution
with predicted mentions. For example, Haghighi
and Klein (2010) compare four state-of-the-art sys-
tems on three different corpora and report B3 scores
between 63 and 77 points. While the corpora used
in (Haghighi and Klein, 2010) are different from the
one in this shared task, our result of 68 B3 suggests
that our system?s performance is competitive. In this
task, our submissions in both the open and the closed
track obtained the highest scores.
4 Conclusion
In this work we showed how a competitive end-to-
end coreference resolution system can be built using
only deterministic models (or sieves). Our approach
starts with a high-recall mention detection compo-
nent, which identifies mentions using only syntactic
information and named entity boundaries, followed
by a battery of high-precision deterministic corefer-
ence sieves, applied one at a time from highest to
lowest precision. These models incorporate lexical,
syntactic, semantic, and discourse information, and
have access to document-level information (i.e., we
share mention attributes across clusters as they are
built). For this shared task, we extended our ex-
isting system with new sieves that model shallow
discourse (i.e., speaker identification) and seman-
tics (lexical chains and alias detection). Our results
demonstrate that, despite their simplicity, determin-
istic models for coreference resolution obtain com-
petitive results, e.g., we obtained the highest scores
in both the closed and open tracks (57.8 and 58.3
respectively). The code used for this shared task is
publicly released.5
Acknowledgments
We thank the shared task organizers for their effort.
This material is based upon work supported by
the Air Force Research Laboratory (AFRL) under
prime contract no. FA8750-09-C-0181. Any opin-
ions, findings, and conclusion or recommendations
expressed in this material are those of the authors
and do not necessarily reflect the view of the Air
Force Research Laboratory (AFRL).
5See http://nlp.stanford.edu/software/
dcoref.shtml for the standalone coreference resolution
system and http://nlp.stanford.edu/software/
corenlp.shtml for Stanford?s suite of natural language
processing tools, which includes this coreference resolution
system.
33
References
B. Baldwin. 1997. CogNIAC: high precision corefer-
ence with limited knowledge and linguistic resources.
In Proceedings of a Workshop on Operational Factors
in Practical, Robust Anaphora Resolution for Unre-
stricted Texts.
E. Bengston & D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting Complex Biological Events with Rich Graph-
Based Feature Sets. Proceedings of the Workshop on
BioNLP: Shared Task.
H. Daume? III and D. Marcu. 2005. A large-scale ex-
ploration of effective global features for a joint entity
detection and tracking model. In EMNLP-HLT.
B. A. Fox 1993. Discourse structure and anaphora:
written and conversational English. Cambridge Uni-
versity Press.
A. Haghighi and D. Klein. 2010. Coreference resolution
in a modular, entity-centered model. In Proc. of HLT-
NAACL.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R.
Weischedel 2006. OntoNotes: The 90% Solution. In
HLT/NAACL.
Z. Huang, G. Zeng, W. Xu, and A. Celikyilmaz 2009.
Accurate semantic class classifier for coreference res-
olution. In EMNLP.
J.R. Hobbs. 1977. Resolving pronoun references. Lin-
gua.
H. Ji and D. Lin. 2009. Gender and animacy knowl-
edge discovery from web-scale n-grams for unsuper-
vised person mention detection. In PACLIC.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of the BioNLP?09 Shared Task on Event Extrac-
tion. Proceedings of the NAACL-HLT 2009 Work-
shop on Natural Language Processing in Biomedicine
(BioNLP?09).
V. Ng 2007. Semantic Class Induction and Coreference
Resolution. In ACL.
V. Ng and C. Cardie. 2002. Improving Machine Learn-
ing Approaches to Coreference Resolution. in ACL
2002
S. Ponzetto and M. Strube. 2006. Exploiting semantic
role labeling, Wordnet and Wikipedia for coreference
resolution. Proceedings of NAACL.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted Coreference: Indentifying Entities and Events
in OntoNotes. In Proceedings of the IEEE Interna-
tional Conference on Semantic Computing (ICSC).
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNotes. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning (CoNLL 2011).
K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers,
M. Surdeanu, D. Jurafsky, and C. Manning 2010.
A Multi-Pass Sieve for Coreference Resolution. In
EMNLP.
J. Tetreault and J. Allen. 2003. An Empirical Evalua-
tion of Pronoun Resolution and Clausal Structure. In
Proceedings of the 2003 International Symposium on
Reference Resolution.
J. Tetreault and J. Allen. 2004. Dialogue Structure and
Pronoun Resolution. In DAARC.
X. Yang and J. Su. 2007. Coreference Resolution Us-
ing Semantic Relatedness Information from Automat-
ically Discovered Patterns. In ACL.
34
