Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 181?185,
Dublin, Ireland, August 23-24, 2014.
CMUQ-Hybrid: Sentiment Classification
By Feature Engineering and Parameter Tuning
Kamla Al-Mannai
1
, Hanan Alshikhabobakr
2
,
Sabih Bin Wasi
2
, Rukhsar Neyaz
2
, Houda Bouamor
2
, Behrang Mohit
2
Texas A&M University in Qatar
1
, Carnegie Mellon University in Qatar
2
almannaika@hotmail.com
1
{halshikh, sabih, rukhsar, hbouamor, behrang}@cmu.edu
Abstract
This paper describes the system we sub-
mitted to the SemEval-2014 shared task
on sentiment analysis in Twitter. Our sys-
tem is a hybrid combination of two system
developed for a course project at CMU-
Qatar. We use an SVM classifier and cou-
ple a set of features from one system with
feature and parameter optimization frame-
work from the second system. Most of the
tuning and feature selection efforts were
originally aimed at task-A of the shared
task. We achieve an F-score of 84.4% for
task-A and 62.71% for task-B and the sys-
tems are ranked 3rd and 29th respectively.
1 Introduction
With the proliferation of Web2.0, people increas-
ingly express and share their opinion through so-
cial media. For instance, microblogging websites
such as Twitter
1
are becoming a very popular com-
munication tool. An analysis of this platform re-
veals a large amount of community messages ex-
pressing their opinions and sentiments on differ-
ent topics and aspects of life. This makes Twit-
ter a valuable source of subjective and opinionated
text that could be used in several NLP research
works on sentiment analysis. Many approaches
for detecting subjectivity and determining polarity
of opinions in Twitter have been proposed (Pang
and Lee, 2008; Davidov et al., 2010; Pak and
Paroubek, 2010; Tang et al., 2014). For instance,
the Twitter sentiment analysis shared task (Nakov
et al., 2013) is an interesting testbed to develop
and evaluate sentiment analysis systems on social
media text. Participants are asked to implement
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1
http://twitter.com
a system capable of determining whether a given
tweet expresses positive, negative or neutral sen-
timent. In this paper, we describe the CMUQ-
Hybrid system we developed to participate in the
two subtasks of SemEval 2014 Task 9 (Rosenthal
et al., 2014). Our system uses an SVM classifier
with a rich set of features and a parameter opti-
mization framework.
2 Data Preprocessing
Working with tweets presents several challenges
for NLP, different from those encountered when
dealing with more traditional texts, such as
newswire data. Tweet messages usually contain
different kinds of orthographic and typographical
errors such as the use of special and decorative
characters, letter duplication used generally for
emphasis, word duplication, creative spelling and
punctuation, URLs, #hashtags as well as the use
of slangs and special abbreviations. Hence, before
building our classifier, we start with a preprocess-
ing step on the data, in order to normalize it. All
letters are converted to lower case and all words
are reduced to their root form using the WordNet
Lemmatizer in NLTK
2
(Bird et al., 2009). We kept
only some punctuation marks: periods, commas,
semi-colons, and question and exclamation marks.
The excluded characters were identified to be per-
formance boosters using the best-first branch and
bound technique described in Section 3.
3 Feature Extraction
Out of a wide variety of features, we selected the
most effective features using the best-first branch
and bound method (Neapolitan, 2014), a search
tree technique for solving optimization problems.
We used this technique to determine which punc-
tuation marks to keep in the preprocessing step and
2
http://www.nltk.org/api/nltk.stem.
html
181
in selecting features as well. In the feature selec-
tion step, the root node is represented by a bag of
words feature, referred as textual tokens.
At each level of the tree, we consider a set of
different features, and iteratively we carry out the
following steps: we process the current feature by
generating its successors, which are all the other
features. Then, we rank features according to the
f-score and we only process the best feature and
prune the rest. We pass all the current pruned fea-
tures as successors to the next level of the tree. The
process iterates until all partial solutions in the tree
are processed or terminated. The selected features
are the following:
Sentiment lexicons : we used the Bing Liu Lex-
icon (Hu and Liu, 2004), the MPQA Subjectivity
Lexicon (Wilson et al., 2005), and NRC Hashtag
Sentiment Lexicon (Mohammad et al., 2013). We
count the number of words in each class, result-
ing in three features: (a) positive words count, (b)
negative words count and (c) neutral words count.
Negative presence: presence of negative words
in a term/tweet using a list of negative words. The
list used is built from the Bing Liu Lexicon (Hu
and Liu, 2004).
Textual tokens: the target term/tweet is seg-
mented into tokens based on space. Token identity
features are created and assigned the value of 1.
Overall polarity score: we determine the polar-
ity scores of words in a target term/tweet using the
Sentiment140 Lexicon (Mohammad et al., 2013)
and the SentiWordNet lexicon (Baccianella et al.,
2010). The overall score is computed by adding
up all word scores.
Level of association: indicates whether the
overall polarity score of a term is greater than 0.2
or not. The threshold value was optimized on the
development set.
Sentiment frequency: indicates the most fre-
quent word sentiment in the tweet. We determine
the sentiment of words using an automatically
generated lexicon. The lexicon comprises 3,247
words and their sentiments. Words were obtained
from the provided training set for task-A and sen-
timents were generated using our expression-level
classifier.
We used slightly different features for Task-A
and Task-B. The features extracted for each task
are summarized in Table 1.
Feature Task A Task B
Positive words count X
Negative words count X
Neutral words count X
Negative presence X X
Textual tokens X X
Overall polarity score X X
Level of association X
Sentiment frequency X
Table 1: Feature summary for each task.
4 Modeling Kernel Functions
Initially we experimented with both logistic
regression and the Support Vector Machine
(SVM) (Fan et al., 2008), using the Stochastic
Gradient Descent (SGD) algorithm for parame-
ter optimization. In our development experiments,
SVM outperformed and became our single classi-
fier. We used the LIBSVM package (Chang and
Lin, 2011) to train and test our classifier.
An SVM kernel function and associated param-
eters were optimized for best F-score on the de-
velopment set. In order to avoid the model over-
fitting the data, we select the optimal parameter
value only if there are smooth gaps between the
near neighbors of the corresponded F-score. Oth-
erwise, the search will continue to the second op-
timal value.
In machine learning, the difference between the
number of training samples, m, and the number
of features, n, is crucial in the selection process
of SVM kernel functions. The Gaussian kernel is
suggested when m is slightly larger than n. Other-
wise, the linear kernel is recommended. In Task-
B, the n : m ratio was 1 : 3 indicating a large
difference between the two numbers. Whereas in
Task-A, a ratio of 5 : 2 indicated a small differ-
ence between the two numbers. We selected the
theoretical types, after conducting an experimen-
tal verification to identify the best kernel function
according to the f-score.
We used a radical basis function kernel for the
expression-level task and the value of its gamma
parameter was adjusted to 0.319. Whereas, we
used a linear function kernel for the message-level
task and the value of its cost parameter was ad-
justed to 0.053.
182
5 Experiments and Results
In this section, we describe the data and the sev-
eral experiments we conducted for both tasks. We
train and evaluate our classifier with the training,
development and testing datasets provided for the
SemEval 2014 shared task. A short summary of
the data distribution is shown in Table 2.
Dataset Postive Negative Neutral
Task-A:
Train (9,451) 62% 33% 5%
Dev (1,135) 57% 38% 5%
Test (10,681) 60% 35% 5%
Task-B:
Train (9,684) 38% 15% 47%
Dev (1,654) 35% 21% 44%
Test (5,754) 45% 15% 40%
Table 2: Datasets distribution percentage per class.
Our test dataset is composed of five different
sets: The test dataset is composed of five dif-
ferent sets: Twitter2013 a set of tweets collected
for the SemEval2013 test set, Twitter2014, tweets
collected for this years version, LiveJournal2014
consisting of formal tweets, SMS2013, a collection
of sms messages, TwitterSarcasm, a collection of
sarcastic tweets.
5.1 Task-A
For this task, we train our classifier on 10,586
terms (9,451 terms in the training set and 1,135
in the development set), tune it on 4,435 terms,
and evaluate it using 10,681 terms. The average
F-score of the positive and negative classes for
each dataset is given in the first part of Table 3.
The best F-score value of 88.94 is achieved on the
Twitter2013.
We conducted an ablation study illustrated in
the second part of Table 3 shows that all the se-
lected features contribute well in our system per-
formance. Other than the textual tokens feature,
which refers to a bag of preprocessed tokens, the
study highlights the role of the term polarity score
feature: ?4.20 in the F-score, when this feature is
not considered on the TwitterSarcasm dataset.
Another study conducted is a feature correlation
analysis, in which we grouped features with sim-
ilar intuitions. Namely the two features negative
presence and negative words count are grouped
as ?negative features?, and the features positive
words count and negative words count are grouped
as ?words count?. We show in Table 4 the effect
on f-score after removing each group from the fea-
tures set. Also we show the f-score after remov-
ing each individual feature within the group. This
helps us see whether features within a group are
redundant or not. For the Twitter2014 dataset, we
notice that excluding one of the features in any of
the two groups leads to a significant drop, in com-
parison to the total drop by its group. The uncor-
related contributions of features within the same
group indicate that features are not redundant to
each other and that they are indeed capturing dif-
ferent information. However, in the case of the
TwitterSarcasm dataset, we observe that the neg-
ative presence feature is not only not contributing
to the system performance but also adding noise
to the feature space, specifically, to the negative
words count feature.
5.2 Task-B
For this task, we trained our classifier on 11,338
tweets (9,684 terms in the training set and 1,654
in the development set), tuned it on 3,813 tweets,
and evaluated it using 8,987 tweets. Results for
different feature configurations are reported in Ta-
ble 5.
It is important to note that if we exclude the tex-
tual tokens feature, all datasets benefit the most
from the polarity score feature. It is interesting to
note that the bag of words, referred to as textual
tokens, is not helping in one of the datasets, the
TwitterSarcasm set. For all datasets, performance
could be improved by removing different features.
In Table 5, we observe that the Negative pres-
ence feature decreases the F-score on the Twitter-
Sarcasm dataset. This could be explained by the
fact that negative words do not usually appear in
a negative implication in sarcastic messages. For
example, this tweet: Such a fun Saturday catch-
ing up on hw. which has a negative sentiment, is
classified positive because of the absence of neg-
ative words. Table 5 shows that the textual tokens
feature increases the classifier?s performance up to
+21.07 for some datasets. However, using a large
number of features in comparison to the number
of training samples could increase data sparseness
and lower the classifier?s performance.
We conducted a post-competition experiment to
examine the relationship between the number of
features and the number of training samples. We
183
Twitter2014 TwitterSarcasm LiveJournal2014 Twitter2013 SMS2013
F-score 84.40 76.99 84.21 88.94 87.98
Negative presence -0.45 0.00 -0.45 -0.23 +0.30
Positive words count -0.52 -1.37 -0.11 -0.02 +0.38
Negative words count -0.50 -2.20 -0.61 -0.47 -1.66
Polarity score -1.83 -4.20 -0.23 -2.14 -3.00
Level of association -0.18 0.00 -0.18 -0.07 +0.57
Textual tokens -8.74 -2.40 -3.02 -4.37 -6.06
Table 3: Task-A feature ablation study. F-scores calculated on each set along with the effect when
removing one feature at a time.
Twitter2014 TwitterSarcasm LiveJournal2014 Twitter2013 SMS2013
F-score 84.40 76.99 84.21 88.94 87.98
Negative features -1.53 -0.84 -3.05 -1.88 -0.67
Negative presence -0.45 0.00 -0.45 -0.23 +0.3
Negative words count -0.50 -2.20 -0.61 -0.47 -1.66
Words count -1.07 -2.2 -0.79 -0.62 -2.01
Positive words count -0.52 -1.37 -0.11 -0.02 +0.38
Negative words count -0.50 -2.20 -0.61 -0.47 -1.66
Table 4: Task-A features correlation analysis. We grouped features with similar intuitions and we calcu-
lated F-scores on each set along with the effect when removing one feature at a time.
fixed the size of our training dataset. Then, we
compared the performance of our classifier using
only the bag of tokens feature, in two different
sizes. In the first experiment, we included all to-
kens collected from all tweets. In the second, we
only considered the top 20 ranked tokens from
each tweet. Tokens were ranked according to the
difference between their highest level of associa-
tion into one of the sentiments and the sum of the
rest. The level of associations for tokens were de-
termined using the Sentiment140 and SentiWord-
Net lexicons. The threshold number of tokens was
identified empirically for best performance. We
found that the classifier?s performance has been
improved by 2 f-score points when the size of to-
kens bag is smaller. The experiment indicates that
the contribution of the bag of words feature can be
increased by reducing the size of vocabulary list.
6 Error Analysis
Our efforts are mostly tuned towards task-A,
hence our inspection and analysis is focused on
task-A. The error rate calculated per sentiment
class: positive, negative and neutral are 6.8%,
14.9% and 93.8%, respectively. The highest error
rate in the neutral class, 93.8%, is mainly due to
the few neutral examples in the training data (only
5% of the data). Hence the system could not learn
from such a small set of neutral class examples.
In the case of negative class error rate, 14.9%,
most of which were classified as positive. An ex-
ample of such classification: I knew it was too
good to be true OTL. Since our system highly re-
lies on lexicon, hence looking at lexicon assigned
polarity to the phrase too good to be true which is
positive, happens because the positive words good
and true has dominating positive polarity.
Lastly for the positive error rate, which is rel-
atively lower, 6%, most of which were classified
negative instead of positive. An example of such
classification: Looks like we?re getting the heavi-
est snowfall in five years tomorrow. Awesome. I?ll
never get tired of winter. Although the phrase car-
ries a positive sentiment, the individual negative
words of the phrase never and tired again domi-
nates over the phrase.
7 Conclusion
We described our systems for Twitter Sentiment
Analysis shared task. We participated in both
tasks, but were mostly focused on task-A. Our hy-
brid system was assembled by integrating a rich
set of lexical features into a framework of fea-
ture selection and parameter tuning, The polarity
184
Twitter2014 TwitterSarcasm LiveJournal2014 Twitter2013 SMS2013
F-score 62.71 40.95 65.14 63.22 61.75
Negative presence -1.65 +1.26 -3.37 -3.66 -0.95
Neutral words count +0.05 0.00 -0.72 -0.57 -0.54
Polarity score -4.03 -6.92 -3.82 -3.83 -4.84
Sentiment frequency +0.10 0.00 +0.18 -0.12 -0.05
textual tokens -17.91 +6.5 -21.07 -19.97 -15.8
Table 5: Task B feature ablation study. F-scores calculated on each set along with the effect when
removing one feature at a time.
score feature was the most important feature for
our model in both tasks. The F-score results were
consistent across all datasets, except the Twitter-
Sarcasm dataset. It indicates that feature selection
and parameter tuning steps were effective in gen-
eralizing the model to unseen data.
Acknowledgment
We would like to thank Kemal Oflazer and also the
shared task organizers for their support throughout
this work.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of the Seventh conference
on International Language Resources and Evalua-
tion (LREC?10), pages 2200?2204, Valletta, Malta.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media, Inc.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A Library for Support Vector Machines.
ACM Transactions on Intelligent Systems and Tech-
nology, 2:27:1?27:27.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, pages 241?249, Uppsala, Sweden.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Minqing Hu and Bing Liu. 2004. Mining and Sum-
marizing Customer Reviews. In Proceedings of the
Tenth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 168?
177, Seattle, WA, USA.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the State-of-
the-Art in Sentiment Analysis of Tweets. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 321?327, Atlanta, Geor-
gia, USA.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment Analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312?
320, Atlanta, Georgia, USA.
Richard E. Neapolitan, 2014. Foundations of Algo-
rithms, pages 257?262. Jones & Bartlett Learning.
Alexander Pak and Patrick Paroubek. 2010. Twitter
Based System: Using Twitter for Disambiguating
Sentiment Ambiguous Adjectives. In Proceedings
of the 5th International Workshop on Semantic Eval-
uation, pages 436?439, Uppsala, Sweden.
Bo Pang and Lillian Lee, 2008. Opinion Mining and
Sentiment Analysis, volume 2, pages 1?135. Now
Publishers Inc.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Proceedings of the
Eighth International Workshop on Semantic Evalu-
ation (SemEval?14), Dublin, Ireland.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning Sentiment-
Specific Word Embedding for Twitter Sentiment
Classification. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 1555?1565, Baltimore, Maryland.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
level Sentiment Analysis. In Proceedings of the
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 347?354, Vancouver, B.C.,
Canada.
185
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 207?216,
October 25, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Unsupervised Word Segmentation Improves
Dialectal Arabic to English Machine Translation
Kamla Al-Mannai
1
, Hassan Sajjad
1
, Alaa Khader
2
, Fahad Al Obaidli
1
,
Preslav Nakov
1
, Stephan Vogel
1
Qatar Computing Research Institute
1
, Carnegie Mellon University in Qatar
2
{kamlmannai,hsajjad,faalobaidli,pnakov,svogel}@qf.org.qa
1
, akhader@cmu.edu
2
Abstract
We demonstrate the feasibility of using
unsupervised morphological segmentation
for dialects of Arabic, which are poor in
linguistics resources. Our experiments us-
ing a Qatari Arabic to English machine
translation system show that unsupervised
segmentation helps to improve the transla-
tion quality as compared to using no seg-
mentation or to using ATB segmentation,
which was especially designed for Mod-
ern Standard Arabic (MSA). We use MSA
and other dialects to improve Qatari Ara-
bic to English machine translation, and we
show that a uniform segmentation scheme
across them yields an improvement of 1.5
BLEU points over using no segmentation.
1 Introduction
The Arabic language has many varieties, where
the Modern Standard Arabic (MSA) coexists with
various dialects. Dialects differ from MSA and
from each other lexically, phonologically, mor-
phologically and syntactically. MSA has stan-
dard orthography and is used in formal contexts
(e.g., publications, newspaper articles, etc.), while
the dialects are usually limited to daily verbal in-
teractions. However, with the recent rise of social
media, it has become increasingly common to use
dialects in written communication as well, which
has constituted the research in dialectal Arabic
(DA) as a separate field within the broader field
of natural language processing (NLP).
As DA NLP is still in its infancy, there is lack
of basic computational resources and tools, which
are needed in order to apply standard NLP ap-
proaches to the dialects of Arabic. For instance,
statistical approaches need a lot of training data,
which makes it very hard, if not impossible, to
apply them to resource-poor languages; this is
especially true for statistical machine translation
(SMT) of Arabic dialects.
The Arabic language and its dialects are highly
inflectional, and a word can appear in many more
inflected forms compared to English. Consider the
Arabic words

IJ
.
?? ,I
.
??K


,I
.
??

K, and
	
??J
.
??K


: they
all belong to one root word I
.
?? ?playing? /lEb/.
Each morphological variation is derived from a
root word with different affixes addressing differ-
ent functions. This causes data sparseness, and
covering all possible word forms of a root word
may not be always possible. Considering the dif-
ferent variants of Arabic, the problem is exacer-
abated as dialects could use different choices of af-
fixes for the same function. For example, the MSA
word
	
??J
.
??K


/yalEabuwn/, meaning ?they are play-
ing?, could be found as
	
??J
.
??K


/ylEbuwn/ in Gulf,
as @?J
.
??K


?? /Eam yilEabuA/ in Levantine, and as
@?J
.
??J


K
.
/biylEabwA/ in Egyptian Arabic.
One possible solution is to use a morphological
segmenter that segments words into simpler units
such as stems and affixes, which might be covered
in the training set (Zollmann et al., 2006; Tsai et
al., 2010). When applied to dialects, this may re-
duce the lexical gap between dialects and MSA by
matching the common stems. Unfortunately, there
are no standard morphological segmentation tools
for dialects. Due to the difference in morphology,
tools designed for MSA do not work well for di-
alects. Developing rule-based segmenters for each
dialect might appear to be the ideal solution, but,
as the orthography of dialects is not standardized,
crafting linguistic rules for them is very hard.
In this paper, we focus on training an unsuper-
vised model for word segmentation, which we ap-
ply to SMT for a given Arabic dialect. We train a
pre-existing unsupervised segmentation model on
the Arabic side of the training bi-text (and on some
other monolingual data), and then we optimize its
parameters based on the resulting SMT quality.
Similarly, a multi-dialectal word segmenter could
be developed by training on multi-dialectal data.
207
In particular, we develop a Qatari Arabic to En-
glish (QA-EN) SMT system, which we train on a
small pre-existing bi-text. As part of the devel-
opment of the unsupervised segmentation model,
we also collected some additional monolingual
data for Qatari Arabic. Qatari Arabic is a subdi-
alect of the more general Gulf dialect, among with
Saudi, Kuwaiti, Emirati, Bahraini, and Omani; we
collected additional monologual data for each of
these subdialects, and we release this data to the
research community.
We train an unsupervised segmentation tool,
Morphessor, and its MAP model (Creutz and La-
gus, 2007), using different variations of the col-
lected Qatari data. We optimize the single hy-
perparameter of the MAP model by maximizing
the translation quality of the QA-EN SMT sys-
tem in terms of BLEU. Our experimental results
demonstrate that the resulting unsupervised seg-
menter yields improvements in translation quality
when compared to (i) using no segmentation and
(ii) using an MSA-based ATB segmenter.
We further develop a multi-dialectal word seg-
mentation model, which we train on the Arabic
side of the multi-dialectal training data, which
consists of Qatari Arabic, Egyptian Arabic (EGY),
Levantine Arabic (LEV) and MSA to English,
i.e., a scaled combination of all the available par-
allel data. We train a QA-EN SMT system using
the segmented multi-dialectal data, and we show
an absolute gain of 1.5 BLEU points compared to
a baseline that uses no segmentation.
The rest of the paper is organized as follows:
First, we provide an overview of related work on
Dialectal Arabic NLP (Section 2). Next, we dis-
cuss and we illustrate the linguistic differences be-
tween different Arabic dialects in comparison with
and with a focus on Qatari Arabic (Section 3).
Then, we provide statistics about the corpora we
collected and used in our experiments, followed by
an illustration of the orthographic normalization
schemes we applied (Section 4). We next provide
a high-level description of our approach, which
uses morphological segmentation to combine re-
sources for other Arabic dialects in a QA-EN SMT
system effectively (Section 4.3). We also explain
our experimental setup and we present the results
(Section 5). We then discuss translating in the
reverse direction, i.e., into Qatari Arabic (Section
6). Finally, we point to possible directions for fu-
ture work and we conclude the paper (Section 7).
2 Related Work
NLP for DA is still in its early stages of develop-
ment and many challenges need to be overcomed
such as the lack of suitable tools and resources.
Collecting resources for dialectal Arabic:
Several researchers have directed efforts to de-
velop DA computational resources (Maamouri et
al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and
Callison-Burch, 2011; Salama et al., 2014). Zbib
et al. (2012) built two dialectal Arabic-English
parallel corpora for Egyptian and Levantine Ara-
bic using crowdsourcing. Bouamor et al. (2014)
presented a multi-dialectal Arabic parallel corpus,
which covers five Arabic dialects besides MSA
and English. Mubarak and Darwish (2014) col-
lected a multi-dialectal corpus using Twitter. Un-
like previous work, we focus on Gulf subdialects,
particularly Qatari Arabic. The monolingual data
that we collected is a high-quality dialectal re-
source and originates from dialect-specific sources
such as novels and forums.
Adapting SMT resources for other Arabic di-
alects: Many researchers have explored the po-
tential of using MSA as a pivot language for im-
proving SMT of Arabic dialects (Bakr et al., 2008;
Sawaf, 2010; Salloum and Habash, 2011; Sajjad et
al., 2013a; Jeblee et al., 2014). This often involves
DA-MSA conversion schemes as an alternative in
the absence of DA-MSA parallel resources. In
contrast, limited work has been done on lever-
aging available resources for other dialects. Re-
cently, Zbib et al. (2012) have shown that using
a small amount of dialectal data could yield great
improvements for SMT. Here, we investigate the
potential of improving the resource adaptability of
Arabic dialects. Our work is different as we use
an unsupervised segmenter that helps in improv-
ing the lexical overlap between dialects and MSA.
Building morphological segmenters for the
Arabic dialects: Researchers have already fo-
cused efforts on crafting and extending existing
MSA tools to DA by mainly using a set of rules
(Habash et al., 2012). Habash and Rambow
(2006) presented MAGEAD, a knowledge-based
morphological analyzer and generator for Egyp-
tian and Levantine Arabic. Chiang et al. (2006)
developed a Levantine morphological analyzer on
top of an existing MSA analyzer using an explicit
knowledge base.
208
Riesa and Yarowsky (2006) trained a supervised
trie-based model using a small lexicon of dialec-
tal affixes. In our work, we eliminate the need
for linguistic knowledge by training an unsuper-
vised model using available resources. The unsu-
pervised mode of learning allowed us to develop a
multi-dialectal morphological segmenter.
3 Arabic Dialects
In this section, we highlight some of the linguis-
tic differences between Arabic dialects and MSA,
with a focus on the Qatari dialect.
3.1 Phonological Variations
The Gulf dialect often preserves the phonological
representation of MSA, which is not the case with
many other Arabic dialects. For example, in Egyp-
tian (EGY) and in some Levantine (LEV) dialects,
the MSA consonants

H /v/,

? /q/, and
	
X /*/ are
realized as

H /t/, glottal stop /?/, and
	
? /Z/, re-
spectively. While, their MSA pronunciations are
preserved in Gulf Arabic.
In Gulf Arabic, there are some phonological dif-
ferences between countries such as Kuwait (KW),
Saudi Arabia (SA), Bahrain (BH), Qatar (QA),
United Arab Emirates (AE), and Oman (OM).
Here, we focus our discussion on Qatari Arabic,
and we compare it to MSA and other dialects.
The QA dialect borrows two Persian characters
namely h

/J/ and

? /V/. For instance, the MSA
letter h
.
/j/ is converted to /J/ in QA, e.g., ?A?

Jk
.
@

?meeting? is pronounced as /<jtimAE/ in MSA
and /<JtimAE/ in QA. The Persian character h

/J/
is also used in place of ? /k/ in some MSA words
when they are used in QA. For example, ??
?
?
?fish?
/samak/ is pronounced i

?
?
?
/smaJ/ in QA, while
the EGY and the LEV dialects maintain the MSA
pronunciation. The Persian

? /V/ is used to map
the sound of the English letter ?v? in borrowed for-
eign words, e.g., ?K


YJ


	
? ?video? is pronounced as
?K


YJ



? /Viydyw/ as opposed to /fiydywu/; the form
in which it is written in MSA.
The MSA consonant
	
? /D/ is not used in the
QA dialect. It is substituted by
	
? /Z/ in Qatari. For
example, the MSA pronunciation /HaD/ of
	
?k
?to encourage? is transformed to
	
?k /HaZ/ in QA,
but it is maintained in EGY.
Meanwhile, the MSA consonant
	
? /Z/ is re-
alized as /D/ in EGY. For example, the MSA
pronunciation /HaZ/ of
	
?k ?luck? is maintained
in QA and transformed to /HaD/ in EGY. This
change is consistent in all words within each di-
alect. However, such phonological variations be-
tween dialects have the potential to add ambiguity
to dialectal Arabic.
The MSA consonant h
.
/j/ can be used to distin-
guish between different dialects, particularly Gulf
subdialects. h
.
/j/ is pronounced as ?


/y/ in KW,
BH, QA, AE,

? /q/ in OM, much like in EGY,
and h
.
/j/ in SA, much like in LEV. For exam-
ple, the MSA word Yj
.
?? ?mosque? /masjid/ is
pronounced as /masjid/ in MSA, SA, LEV, Y

???
/masqid/ in OM, EGY, YJ


?? /masyid/ in KW,
BH, QA, AE, while the MSA pronunciation is
preserved in SA. This change does not apply to
names. However, we should note that it is not con-
sistent in QA, e.g., the MSA pronunciation of h
.
/j/
in ?J
.
k
.
?mountain? /jabal/ and h
.
QK
.
?tower? /burj/ is
preserved in QA.
3.2 Morphological Variations
In Arabic, a root can produce surface wordforms
by means of inflectional and derivational morpho-
logical processes (Habash, 2010).
An inflectional word form is a variant of a root
word with the same meaning but expressing a dif-
ferent function, e.g., gender, number, case. It is
usually formed by adding a prefix, a suffix, or a
circumfix to a stem word. Note that Arabic di-
alects can make different lexical choices for affix-
ations compared to MSA. For example, the MSA
future prefix ? /s/ is replaced by H
.
/b/ in QA
and by ? /h/ in EGY and LEV. Thus, the MSA
word ??

AJ


? ?he will eat? /say>kul/ becomes ?? AJ


K
.
/biyAkil/ in QA and ?? AJ


? /hayAkul/ in EGY and
LEV.
A derivational word form is formed by applying
a pattern to a root word, e.g., ?player? is derived
from ?play? using the pattern noun + ?er?. An
example of an Arabic derivational form is ??
	
?

K
?do? /tafaE?al/. The root is ??
	
? /faEal/ and it uses
the imperative pattern

H+??
	
?. In EGY, @ /A/ is
added as a prefix; so, it becomes ??
	
?

K@ /AitfaE
?
il/.
209
Meanwhile, the original form is preserved in QA.
Changing the structure of a pattern in a dialect
will result in producing a new dialect-specific or-
thography for every word that is represented by the
structure. For example, the MSA word ??
?

K
?learn?
/taEal?am/ becomes ??
?

K
@ /AitEalim/ in EGY, while
the MSA form is preserved in QA.
3.3 Lexical Variations
Lexical variations are among the most obvious
differences between Arabic dialects. For exam-
ple, the MSA word @
	
XA? ?what? /mA*A/ would be
found as ?

? /$uw/ in LEV, ?K


@

/<yh/ in EGY, and
?
	
J

? /$nuw/ in GLF. We can find lexical variations
in subdialects as well. For example, the MSA
negation word
	
?? /lan/, ?not?, is expressed as I
.
?
/mab/ in QA, as ?? /muw/ in KW, and as I
.
?? /ma-
hab/ in SA.
3.4 Orthographic Variations
Due to the lack of orthographic standardization of
dialectal Arabic, some MSA words can be found
in dialectal text with both MSA and phonologi-
cal spellings. For example, the MSA word

???
g
.
?gathering? /jamEap/ can be also spelled as ???
?

/yamEah/, which is a phonetic variation in QA.
Some dialectal words also vary in spelling due
to variation in their pronunciation, e.g.,
	
??

?

@
/A$uwf/, a QA word meaning ?I see?, can be also
spelled as
	
??k
.
@ /Ajuwf/.
In dialectal Arabic, different orthographic
forms are also possible for entire phrases. For
instance, words followed or preceded by pro-
nouns are commonly reduced to a single word,
e.g., A??

I?

? /glt lahA/ ?I told her? is written as
A??

J?

?. Also, commonly used religious phrases can
be found written as a single unit, e.g., ?<? @ Z A

? A?
/mA $A? A
?
lah/ ?God has willed it? as ?<?A

??.
4 Methodology
In the section, we present some statistics about the
Arabic dialectal data that we have collected. We
processed it to remove orthographic inconsisten-
cies. Then, we used a pre-existing unsupervised
morphological segmenter, Morfessor, in order to
segment the text.
Corpus QCA AVIA
QA
AVIA
O
Sents 14.7 0.9 2
Tokens 115 6.7 15
Table 1: Statistics about the collected parallel cor-
pora (in thousands). AVIA
O
shows the statistics
about the AVIA corpus excluding Qatari data.
4.1 Data Collection
We did an extensive search for available monolin-
gual and bilingual resources for the Gulf dialect,
with a focus on Qatari Arabic. Tables 1 and 2
present some statistics about the corpora we col-
lected. More detailed description follows below.
Bilingual corpora:
? The QCA speech corpus, comprises 14.7k
sentences that are phonetically transcribed from
TV broadcasts in Qatari Arabic and translated to
English; see (Elmahdy et al., 2014) for more de-
tail. The corpus was designed for speech recog-
nition and we faced several normalization-related
issues that we had to resolve before it could be
used for machine translation and language mod-
eling. One example is the usage of five Per-
sian characters to represent some sounds in Ara-
bic words. Moreover, the English side had some
grammatical and spelling errors. We normalized
the Arabic side and corrected the English side of
the corpus as described in Section 4.2. The cor-
pus can be found at http://sprosig.isle.
illinois.edu/corpora/1.
? The AVIA corpus
1
is designed as a refer-
ence source of dialectal Arabic. It consists of 3k
sentences in four Gulf subdialects: Emirati (AE),
Kuwaiti (KW), Qatari (QA), and Hejazi (SA).
2
The data consists of dialectal sentences that con-
tain words commonly used in daily conversation.
Monolingual corpora: We further collected
monolingual corpora consisting of a total of 2.7M
tokens for various Gulf subdialects. The Qatari
part of the data consists of 470K tokens. Most of
the corpus is a collection of novels, belonging to
the romance genre.
3
For the Qatari dialect, we also
collected Qatari forum data.
4
1
http://terpconnect.umd.edu/
?
nlynn/
AVIA/Level3/
2
The website also contains small parallel corpora for
MSA, EGY and LEV to English, but here we focus on Gulf
subdialects only.
3
http://forum.te3p.com/264311-52.html
4
www.qatarshares.com/vb/index.php
210
Corpus Novel Forum
AE BH KW OM QA SA QA
Tokens 573 244 178 372 412 614 69
Types 43 22 27 27 43 71 15
Table 2: Statistics about the collected monolingual
corpora (in thousands of words).
To the best of our knowledge, this is the first
collection of monolingual corpora for Gulf Ara-
bic subdialects. It can be helpful for, e.g., lan-
guage modeling when translating into Arabic, for
learning the similarities and differences between
Gulf subdialects, etc. Table 2 shows some statis-
tics about the data after punctuation tokenization.
4.2 Orthographic Normalization
The inconsistency in the orthographic spelling of
the same word can increase data sparseness. Thus,
we normalize the Arabic text in the collected re-
sources by applying the reduced orthographic nor-
malization scheme, e.g., Tah Marbota is reduced to
Hah. We also normalize extended lines between
letters, e.g., Q?? ?sugar? /sukar/ is changed
to Q??, and we reduce character elongations to
be just two characters long. In order to main-
tain consistency among different resources, we re-
move supplementary diacritics, e.g.,

Y


?

? ?knots?
/Euqad/ is normalized to Y

??, and we map Per-
sian letters to their phonological correspondences
in Qatari Arabic
5
, i.e., ? /G/ to

? /g/,

? /V/ to
	
?
/f/, H

/P/ to H
.
/b/, and

P and h

/J/ to h
.
/j/.
For the English texts, the orthographic varia-
tions were already normalized. However, the En-
glish side of the QCA corpus had some spelling
and grammatical errors, which we corrected man-
ually. On the grammatical side, we only corrected
a subset of the data, which we used for tuning and
testing our SMT system (see Section 5).
4.3 Morphological Decomposition
There is no general Arabic morphological seg-
menter that works for all variations of Arabic. The
most commonly used segmenters for Arabic were
designed for MSA (Habash et al., 2009; Green and
DeNero, 2012). Due to the lexical and morpholog-
ical differences between dialects and MSA, these
MSA-based morphological tools do not work well
for dialects.
5
This issue relates to the QCA corpus.
In this work, we used an unsupervised morpho-
logical segmenter, Morfessor-categories MAP
6
,
an unsupervised model with a single hyper-
parameter (Creutz and Lagus, 2007). We chose
Morfessor because of its superior performance on
Arabic compared to other unsupervised models
(Siivola et al., 2007; Poon et al., 2009).
The model has a single hyperparameter, the per-
plexity threshold parameter B, which controls the
granularity of segmentation. The recommended
value ranges from 1 to 400 where 1 means max-
imum fine-grained segmentation, and 400 restricts
it to the least segmented output. We set the thresh-
old empirically to 70, as shown in Section 5.1.
5 Experimental Setup
We performed an extrinsic evaluation of the varia-
tions in segmentation by building a Qatari Arabic
to English machine translation system on each of
them. We also tested Morfessor on other available
dialects and on MSA, and we will show below how
a uniform segmentation can help to better adapt re-
sources for dialects and MSA for SMT. This sec-
tion describes our experimental setup.
Datasets: We divided the QCA corpus into 1k
sentences each for development and testing, and
we used the remaining 12k for training.
We adapted parallel corpora for Egyptian, Lev-
antine and MSA to English to be used for Qatari
Arabic to English SMT. For MSA, we used par-
allel corpora of TED talks (Cettolo et al., 2012)
and the AMARA corpus (Abdelali et al., 2014),
which consists of educational videos. Since the
QCA corpus is in the speech domain, we believe
that an MSA corpus of spoken domain would be
more helpful than a text domain such as News. For
Egyptian and Levantine, we used the parallel cor-
pus provided by Zbib et al. (2012). There is no
Gulf?English parallel data available in the litera-
ture. The data that we found was a very small col-
lection of subdialects of Gulf Arabic; we did not
use it for MT experiments. However, we used the
Qatari part of the AVIA corpus to train Morfessor.
Machine translation system settings: We used
a phrase-based statistical machine translation
model as implemented in the Moses toolkit
(Koehn et al., 2007) for machine translation.
6
This is an extension of the basic Morfessor method and
is based on a Maximum a Posteriori model.
211
We built separate directed word alignments
for source-to-target and target-to-source using
IBM model 4 (Brown et al., 1993), and we
symmetrized them using the grow-diag-final-and
heuristics (Koehn et al., 2003). We then extracted
phrase pairs with a maximum length of seven, and
we scored them using maximum likelihood esti-
mation with Kneser-Ney smoothing (Kneser and
Ney, 1995). We also built a lexicalized reordering
model, msd-bidirectional-fe. We built a 5-gram
language model on the English side of QCA-train
using KenLM (Heafield, 2011). Finally, we built a
log-linear model using the above features.
We tuned the model weights by optimizing
BLEU (Papineni et al., 2002) on the tuning set, us-
ing PRO (Hopkins and May, 2011) with sentence-
level BLEU+1 optimization (Nakov et al., 2012).
In testing, we used minimum Bayes risk decoding
(Kumar and Byrne, 2004), cube pruning, and the
operation sequence model (Durrani et al., 2011).
Baseline: Our baseline Qatari Arabic to English
MT system is trained on the QCA bitext without
any segmentation of Qatari Arabic. For the exper-
iments described in this paper, we used the English
side of the QCA corpus for language modeling.
5.1 Experimental Results
In this section, we first present our work on using
Morfessor for segmenting Qatari Arabic. We tried
different values of its parameter, and we trained it
using corpora of different sizes to find balanced
settings that improve SMT quality as compared
with no segmentation and with segmentation us-
ing the Stanford ATB segmenter. We further ap-
plied our selected settings to segment MSA, EGY
and LEV and used them for Qatari Arabic to En-
glish machine translation. Our results show that a
uniform segmentation scheme across different di-
alects improves machine translation.
Morfessor training variations: We trained
Morfessor using three corpora: (i) QCA,
(ii) AVIA
QA
plus Qatari Novels, and (iii) a com-
bination thereof. Table 3 shows the results for
our SMT system when trained on the QCA par-
allel corpus, which was segmented using different
training models of Morfessor with B = 40. The
result for segmented Qatari Arabic is always bet-
ter than the baseline, irrespective of the training
model used for segmentation. We can see that the
Morfessor model trained on a large monolingual
corpus, i.e., on (ii) or (iii), yields better results.
Morfessor BLEU OOV%
Baseline 12.2 16.6
QCA 12.5 0.6
AVIA
QA
, Novels 13.5 0.8
QCA, AVIA
QA
, Novels 13.4 0.7
Table 3: Study of the effect of varying the train-
ing datasets for Morfessor on the Qatari to English
SMT. ?Baseline? shows the output of the MT sys-
tem with no segmentation.
B 10 40 70 100 130
BLEU 13.3 13.5 13.8 12.9 12.6
OOV 0.3 0.8 1.4 2.8 2.8
After merging
BLEU 12.5 13.4 13.7 12.8 12.3
OOV 1.5 1.9 3.9 6.5 9.8
Table 4: The effect of varying the perplexity
threshold parameter B of Morfessor on SMT qual-
ity. ?After merging? are the results using the post-
processed Qatari segmented data.
The high reduction in OOV in Table 3 is be-
cause of the fine-grained segmentation. We tried
different values for the perplexity parameter B
in order to find a good balance between better
BLEU scores and linguistically correct segmen-
tations. The first part of Table 4 shows the ef-
fect of different values of B on the quality of the
machine translation system trained on AVIA
QA
,
Qatari Novels. We achieved the best SMT score at
B = 70.
We further analyzed the output of Morfessor
at B = 70 and we noticed that it tends to gener-
ate very small segments of length two and three
characters long. The segmentation produces more
than one stem in a word and does not generate le-
gal word units. For example, the word

??A
	
J??@?
?and the industry? /wAlSinAEp/ is segmented as
PRE/? + PRE/?@ + STM/? + PRE/
	
? + PRE/ @
+ STM/? + SUF/

?. We apply a post-processing
step that merges all stems in a word and affixes
between them to one stem. So, a word can have
only one stem. For example, the word

??A
	
J??@?
would be segmented as PRE/?@? + STM/?A
	
J? +
SUF/

?. This yielded linguistically correct segmen-
tations in many cases. The second part of Table
4 shows the effect of the post-processing on the
BLEU score. We can see that it remains almost
the same with an increase in OOV rate.
212
For rest of the experiments in this paper, we
used a value of 70 for the perplexity threshold
parameter plus the post-processing on segmenta-
tion. We trained Morfessor on the concatenation
of QCA, AVIA
Q
A and Novels.
7
Using other Arabic variations: In this section,
we present experiments using MSA, EGY and
LEV to English bitexts combined with the QCA
bitext for Qatari Arabic to English machine trans-
lation. We explored three segmentation options for
the Arabic side of the data: (i) no segmentation,
(ii) ATB segmentation, and (iii) unsupervised seg-
mentation using Morfessor.
The QCA corpus is of much smaller size com-
pared to other Arabic variants, say MSA. It is pos-
sible that in the training of the machine transla-
tion models, the large corpus dominates the QCA
corpus. In order to avoid that, we balanced the
two corpora by replicating the smaller corpus X
number of times in order to make it approximately
equal to the large corpus (Nakov and Ng, 2009).
8
The complete procedure is described below.
In a nutshell, for building a machine transla-
tion system using the MSA plus Qatari corpus, we
first balanced the Qatari corpus to make it approx-
imately equal to MSA and concatenated them. For
training Morfessor, the Qatari Arabic data con-
sisted of QCA, Novels and AVIA
QA
, while for
SMT, it consisted of QCA only. In both cases,
we balanced it to be approximately equal to MSA.
We then trained Morfessor on the balanced (QCA,
Novels, AVIA
QA
) plus MSA data and we seg-
mented the Arabic side of the balanced QCA plus
MSA training data for machine translation. We
built a machine translation system on the seg-
mented data. We segmented the testing and tuning
data sets similarly. We used the same balancing
when we combined EGY-EN and LEV-EN with
the Qatari Arabic ? English data.
We also tried training multiple unsupervised
models, but this yielded lower SMT quality com-
pared to using a single model trained on multi-
dialects. Using different models could result
in having different segmentation schemes, which
might not help in reducing the vocabulary mis-
match between different variants of Arabic.
7
We did not see a big difference in training Morfessor
with and without the QCA corpus, and we decided to use
the complete data for training.
8
Due to the spoken nature of the QCA corpus, it contains
shorter sentences. Thus, we balanced the corpora based on
the number of tokens rather than on the number of sentences.
Train NONE ATB Morfessor
QCA 12.2 12.9 13.7
?QCA,MSA 12.7 13.3 14.6
?QCA,EGY 13.0 13.5 14.5
?QCA,LEV 13.8 13.7 15.2
Table 5: BLEU scores for Qatari Arabic to English
SMT using three different segmentation settings.
?QCA means the modified QCA corpus with num-
ber of tokens approximately equal to MSA, EGY
and LEV in the respective experiments.
Table 5 shows the results. There are two things
to point here. First, the SMT systems that used
the unsupervised morphological segmenter, Mor-
fessor, outperformed the systems that used no seg-
mentation and those using the ATB segmentation.
The Morfessor-based systems showed consistent
improvements compared to the ATB-based sys-
tems over the no-segmentation systems. This val-
idates our point that unsupervised morphological
segmentation generalizes well for a variety of di-
alects and these SMT results complement that.
The second observation is that adding a bitext for
other dialects and MSA improves machine trans-
lation quality for Qatari?English SMT.
6 Translation into Qatari Arabic
Our monolingual corpora of Gulf subdialects
could be also helpful when translating English into
Qatari Arabic. We conducted a few basic experi-
ments in this direction but without segmentation.
We trained an English to Qatari Arabic SMT
system on the QCA bitext, using the same settings
as described in Section 5. We then normalized the
output of the translation system using the QCRI-
Normalizer (Sajjad et al., 2013b).
9
As a language
model, we used the Arabic side of the QCA cor-
pus, novels and forum data, standalone and to-
gether. Table 6 presents the results of the effect of
varying the language model on the quality of the
SMT system. The best system shows an improve-
ment of 0.22 BLEU points absolute compared to
the baseline system that only uses the Arabic side
of the QCA corpus for LM training.
The SMT system achieved the largest gain when
adding QA forum data to the QCA data. SA and
AE monolingual data also showed good improve-
ments. This might be due to their relatively large
sizes; we need further investigation.
9
http://alt.qcri.org/tools/
213
LM BLEU
QCA 2.78
QCA+QA-Novels 2.64
QCA+QA-Novels+BH-Novels 2.86
QCA+QA-Novels+KW-Novels 2.78
QCA+QA-Novels+AE-Novels 2.92
QCA+QA-Novels+SA-Novels 2.96
QCA+ALL-Novels 2.80
QCA+QA-Novels+QForum 3.00
Table 6: Results for English to Qatari SMT for
varying language models. In all cases, the transla-
tion model is trained on the QCA bitext only.
Note the quite low BLEU scores, especially
compared to the reverse translation direction. One
reason is the morphologically rich nature of Qatari
Arabic, which makes translating into it a hard
problem. The small amount of training data fur-
ther adds to it. We expect to see larger gains com-
pared to Qatari Arabic to English machine transla-
tion when segmentation is used.
7 Conclusion and Future Work
We have demonstrated the feasibility of using
an unsupervised morphological segmenter to in-
crease the resource adaptability of Arabic variants.
We evaluated the segmentation on a Qatari dialect
by building a Qatari Arabic to English machine
translation system. We further adapted MSA,
EGY and LEV in the simplest machine translation
settings and we showed a consistent improvement
of 1.5 BLEU points when compared to the respec-
tive baseline system that uses no segmentation.
In the future, we would like to explore the
impact of segmentation on both the translation
model and the language model when translating
into Qatari Arabic. This involves greater chal-
lenges, as a desegmenter is required for the trans-
lation output with every segmentation scheme.
References
Ahmed Abdelali, Francisco Guzman, Hassan Sajjad,
and Stephan Vogel. 2014. The AMARA corpus:
Building parallel language resources for the educa-
tional domain. In Proceedings of the 9th Interna-
tional Conference on Language Resources and Eval-
uation, Reykjavik, Iceland, May.
Rania Al-Sabbagh and Roxana Girju. 2010. Mining
the web for the induction of a dialectical Arabic lexi-
con. In Proceedings of the 7th International Confer-
ence on Language Resources and Evaluation, Val-
letta, Malta, May.
Hitham Abo Bakr, Khaled Shaalan, and Ibrahim
Ziedan. 2008. A hybrid approach for convert-
ing written Egyptian colloquial dialect into dia-
critized Arabic. In Proceedings of the 6th Inter-
national Conference on Informatics and Systems,
Cairo, Egypt, March.
Houda Bouamor, Nizar Habash, and Kemal Oflazer.
2014. A multidialectal parallel corpus of Arabic. In
Proceedings of the 9th edition of the Language Re-
sources and Evaluation Conference, Reykjavik, Ice-
land, May.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and R. L. Mercer. 1993. The mathe-
matics of statistical machine translation: parameter
estimation. Computational Linguistics, 19(2), June.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit
3
: Web inventory of transcribed
and translated talks. In Proceedings of the 16
th
Con-
ference of the European Association for Machine
Translation, Trento, Italy, May.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic
dialects. In Proceedings of the 11th Conference of
the European Chapter of the Association for Com-
putational Linguistics, Trento, Italy, April.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing, 4(1), January.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, Port-
land, OR, June.
Mohamed Elmahdy, Mark Hasegawa-Johnson, and
Eiman Mustafawi. 2014. Development of a TV
broadcasts speech recognition system for Qatari
Arabic. In Proceedings of the 9th edition of the Lan-
guage Resources and Evaluation Conference, Reyk-
javik, Iceland, May.
Spence Green and John DeNero. 2012. A class-based
agreement model for generating accurately inflected
translations. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics, Jeju Island, Korea, July.
Nizar Habash and Owen Rambow. 2006. MAGEAD:
a morphological analyzer and generator for the Ara-
bic dialects. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, Sydney, Australia, July.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
Mada+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, pos
214
tagging, stemming and lemmatization. In Proceed-
ings of the 2nd International Conference on Ara-
bic Language Resources and Tools (MEDAR), Cairo,
Egypt, April.
Nizar Habash, Ramy Eskander, and Abdelati Hawwari.
2012. A morphological analyzer for Egyptian Ara-
bic. In Proceedings of the 12th Meeting of the Spe-
cial Interest Group on Computational Morphology
and Phonology, Montreal, Canada, June.
Nizar Y Habash. 2010. Introduction to Arabic natural
language processing. Synthesis Lectures on Human
Language Technologies, 3(1), August.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the 6th
Workshop on Statistical Machine Translation, Edin-
burgh, UK, July.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
Scotland, UK, July.
Serena Jeblee, Weston Feely, Houda Bouamor, Alon
Lavie, Nizar Habash, and Kemal Oflazer. 2014.
Domain and Dialect Adaptation for Machine Trans-
lation into Egyptian Arabic. In Proceedings of
the Arabic Natural Language Processing Workshop,
Doha, Qatar, October.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for ngram langauge modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, Detroit,
Michigan, May.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology and North
American Association for Computational Linguis-
tics Conference, Edmonton, Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics, Demonstra-
tion Program, Prague, Czech Republic, June.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics,
Boston, MA, May.
Mohamed Maamouri, Ann Bies, Tim Buckwalter,
Mona Diab, Nizar Habash, Owen Rambow, and
Dalila Tabessi. 2006. Developing and using a pi-
lot dialectal Arabic treebank. In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation, Genova, Italy, May.
Hamdy Mubarak and Kareem Darwish. 2014. Using
Twitter to collect a multi-dialectal corpus of Arabic.
In Proceedings of the Arabic Natural Language Pro-
cessing Workshop, Doha, Qatar, October.
Preslav Nakov and Hwee Tou Ng. 2009. Improved
statistical machine translation for resource-poor lan-
guages using related resource-rich languages. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, Suntec,
Singapore, August.
Preslav Nakov, Francisco Guzman, and Stephan Vo-
gel. 2012. Optimizing for sentence-level BLEU+1
yields short translations. In Proceedings of the
24th International Conference on Computational
Linguistics, Mumbai, India, December.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th annual meeting on association for com-
putational linguistics, Philadelphia, PA, July.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics, Denver, CO,
June.
Jason Riesa and David Yarowsky. 2006. Minimally
supervised morphological segmentation with appli-
cations to machine translation. In Proceedings of
the 7th Conference of the Association for Machine
Translation in the Americas, MA, USA, August.
Hassan Sajjad, Kareem Darwish, and Yonatan Be-
linkov. 2013a. Translating dialectal Arabic to En-
glish. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics,
Sofia, Bulgaria, August.
Hassan Sajjad, Francisco Guzman, Preslav Nakov,
Ahmed Abdelali, Kenton Murray, Fahad Al Obaidli,
and Stephan Vogel. 2013b. QCRI at IWSLT 2013:
Experiments in Arabic-English and English-Arabic
Spoken Language Translation. In Proceedings of the
10th International Workshop on Spoken Language
Translation, Hiedelberg, Germany, December.
Ahmed Salama, Houda Bouamor, Behrang Mohit, and
Kemal Oflazer. 2014. YouDACC: the youtube di-
alectal Arabic commentary corpus. In Proceedings
of the 9th edition of the Language Resources and
Evaluation Conference, Reykjavik, Iceland, May.
Wael Salloum and Nizar Habash. 2011. Dialectal
to standard Arabic paraphrasing to improve Arabic-
English statistical machine translation. In Proceed-
ings of the First Workshop on Algorithms and Re-
sources for Modelling of Dialects and Language Va-
rieties, Edinburgh, Scotland, July.
215
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the 9th Con-
ference of the Association for Machine Translation
in the Americas, Denver, CO, October.
Vesa Siivola, Mathias Creutz, and Mikko Kurimo.
2007. Morfessor and VariKN machine learning
tools for speech and language technology. In
Proceedings of the 8th International Conference
on Speech Communication and Technology (Inter-
speech), Antwerpen, Belgium, August.
Ming-Feng Tsai, Preslav Nakov, and Hwee Tou Ng.
2010. Morphological analysis for resource-poor
machine translation. Technical report, Kent Ridge,
Singapore, December.
Omar F Zaidan and Chris Callison-Burch. 2011. The
Arabic online commentary dataset: an annotated
dataset of informal Arabic with high dialectal con-
tent. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies: short papers-Volume
2, Portland, OR, June.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-
Burch. 2012. Machine translation of Arabic di-
alects. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Montreal, Canada, June.
Andreas Zollmann, Ashish Venugopal, and Stephan
Vogel. 2006. Bridging the inflection morphol-
ogy gap for Arabic statistical machine translation.
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Companion Volume:
Short Papers, New York, NY, June.
216
