Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1145?1152,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Multilingual Document Clustering: an Heuristic Approach Based on
Cognate Named Entities
Soto Montalvo
GAVAB Group
URJC
soto.montalvo@urjc.es
Raquel Mart??nez
NLP&IR Group
UNED
raquel@lsi.uned.es
Arantza Casillas
Dpt. EE
UPV-EHU
arantza.casillas@ehu.es
V??ctor Fresno
GAVAB Group
URJC
victor.fresno@urjc.es
Abstract
This paper presents an approach for Mul-
tilingual Document Clustering in compa-
rable corpora. The algorithm is of heuris-
tic nature and it uses as unique evidence
for clustering the identification of cognate
named entities between both sides of the
comparable corpora. One of the main ad-
vantages of this approach is that it does
not depend on bilingual or multilingual re-
sources. However, it depends on the pos-
sibility of identifying cognate named enti-
ties between the languages used in the cor-
pus. An additional advantage of the ap-
proach is that it does not need any infor-
mation about the right number of clusters;
the algorithm calculates it. We have tested
this approach with a comparable corpus
of news written in English and Spanish.
In addition, we have compared the results
with a system which translates selected
document features. The obtained results
are encouraging.
1 Introduction
Multilingual Document Clustering (MDC) in-
volves dividing a set of n documents, written in
different languages, into a specified number k of
clusters, so the documents that are similar to other
documents are in the same cluster. Meanwhile
a multilingual cluster is composed of documents
written in different languages, a monolingual clus-
ter is composed of documents written in one lan-
guage.
MDC has many applications. The increasing
amount of documents written in different lan-
guages that are available electronically, leads to
develop applications to manage that amount of
information for filtering, retrieving and grouping
multilingual documents. MDC tools can make
easier tasks such as Cross-Lingual Information
Retrieval, the training of parameters in statistics
based machine translation, or the alignment of par-
allel and non parallel corpora, among others.
MDC systems have developed different solu-
tions to group related documents. The strate-
gies employed can be classified in two main
groups: the ones which use translation technolo-
gies, and the ones that transform the document into
a language-independent representation.
One of the crucial issues regarding the methods
based on document or features translation is the
correctness of the proper translation. Bilingual re-
sources usually suggest more than one sense for
a source word and it is not a trivial task to select
the appropriate one. Although word-sense disam-
biguation methods can be applied, these are not
free of errors. On the other hand, methods based
on language-independent representation also have
limitations. For instance, those based on thesaurus
depend on the thesaurus scope. Numbers or dates
identification can be appropriate for some types
of clustering and documents; however, for other
types of documents or clustering it could not be so
relevant and even it could be a source of noise.
In this work we dealt with MDC and we pro-
posed an approach based only on cognate Named
Entities (NE) identification. We have tested this
approach with a comparable corpus of news writ-
ten in English and Spanish, obtaining encouraging
results. One of the main advantages of this ap-
proach is that it does not depend on multilingual
resources such as dictionaries, machine translation
systems, thesaurus or gazetteers. In addition, no
information about the right number of clusters has
1145
to be provided to the algorithm. It only depends on
the possibility of identifying cognate named enti-
ties between the languages involved in the corpus.
It could be particularly appropriate for news cor-
pus, where named entities play an important role.
In order to compare the results of our approach
with other based on features translation, we also
dealt with this one, as baseline approach. The sys-
tem uses EuroWordNet (Vossen, 1998) to trans-
late the features. We tried different features cate-
gories and combinations of them in order to deter-
mine which ones lead to improve MDC results in
this approach.
In the following section we relate previous work
in the field. In Section 3 we present our approach
for MDC. Section 4 describes the system we com-
pare our approach with, as well as the experiments
and the results. Finally, Section 5 summarizes the
conclusions and the future work.
2 Related Work
MDC is normally applied with parallel (Silva et.
al., 2004) or comparable corpus (Chen and Lin,
2000), (Rauber et. al., 2001), (Lawrence, 2003),
(Steinberger et. al., 2002), (Mathieu et. al, 2004),
(Pouliquen et. al., 2004). In the case of the com-
parable corpora, the documents usually are news
articles.
Considering the approaches based on transla-
tion technology, two different strategies are em-
ployed: (1) translate the whole document to an an-
chor language, and (2) translate some features of
the document to an anchor language.
With regard to the first approach, some authors
use machine translation systems, whereas others
translate the document word by word consulting
a bilingual dictionary. In (Lawrence, 2003), the
author presents several experiments for clustering
a Russian-English multilingual corpus; several of
these experiments are based on using a machine
translation system. Columbia?s Newsblaster sys-
tem (Kirk et al, 2004) clusters news into events,
it categorizes events into broad topic and summa-
rizes multiple articles on each event. In the clus-
tering process non-English documents are trans-
lated using simple dictionary lookup techniques
for translating Japanese and Russian documents,
and the Systran translation system for the other
languages used in the system.
When the solution involves translating only
some features, first it is necessary to select these
features (usually entities, verbs, nouns) and then
translate them with a bilingual dictionary or/and
consulting a parallel corpus.
In (Mathieu et. al, 2004) before the cluster-
ing process, the authors perform a linguistic anal-
ysis which extracts lemmas and recognizes named
entities (location, organization, person, time ex-
pression, numeric expression, product or event);
then, the documents are represented by a set of
terms (keywords or named entity types). In addi-
tion, they use document frequency to select rele-
vant features among the extracted terms. Finally,
the solution uses bilingual dictionaries to translate
the selected features. In (Rauber et. al., 2001)
the authors present a methodology in which docu-
ments are parsed to extract features: all the words
which appear in n documents except the stop-
words. Then, standard machine translation tech-
niques are used to create a monolingual corpus.
After the translation process the documents are au-
tomatically organized into separate clusters using
an un-supervised neural network.
Some approaches first carry out an independent
clustering in each language, that is a monolingual
clustering, and then they find relations among the
obtained clusters generating the multilingual clus-
ters. Others solutions start with a multilingual
clustering to look for relations between the doc-
uments of all the involved languages. This is the
case of (Chen and Lin, 2000), where the authors
propose an architecture of multilingual news sum-
marizer which includes monolingual and multilin-
gual clustering; the multilingual clustering takes
input from the monolingual clusters. The authors
select different type of features depending on the
clustering: for the monolingual clustering they use
only named entities, for the multilingual clustering
they extract verbs besides named entities.
The strategies that use language-independent
representation try to normalize or standardize the
document contents in a language-neutral way; for
example: (1) by mapping text contents to an inde-
pendent knowledge representation, or (2) by rec-
ognizing language independent text features inside
the documents. Both approaches can be employed
isolated or combined.
The first approach involves the use of exist-
ing multilingual linguistic resources, such as the-
saurus, to create a text representation consisting of
a set of thesaurus items. Normally, in a multilin-
gual thesaurus, elements in different languages are
1146
related via language-independent items. So, two
documents written in different languages can be
considered similar if they have similar representa-
tion according to the thesaurus. In some cases, it
is necessary to use the thesaurus in combination
with a machine learning method for mapping cor-
rectly documents onto thesaurus. In (Steinberger
et. al., 2002) the authors present an approach to
calculate the semantic similarity by representing
the document contents in a language independent
way, using the descriptor terms of the multilingual
thesaurus Eurovoc.
The second approach, recognition of language
independent text features, involves the recognition
of elements such as: dates, numbers, and named
entities. In others works, for instance (Silva
et. al., 2004), the authors present a method
based on Relevant Expressions (RE). The RE are
multilingual lexical units of any length automat-
ically extracted from the documents using the
LiPXtractor extractor, a language independent
statistics-based tool. The RE are used as base
features to obtain a reduced set of new features
for the multilingual clustering, but the clusters
obtained are monolingual.
Others works combine recognition of indepen-
dent text features (numbers, dates, names, cog-
nates) with mapping text contents to a thesaurus.
In (Pouliquen et. al., 2004) the cross-lingual
news cluster similarity is based on a linear com-
bination of three types of input: (a) cognates, (b)
automatically detected references of geographical
place names, and (c) the results of a mapping
process onto a multilingual classification system
which maps documents onto the multilingual the-
saurus Eurovoc. In (Steinberger et. al., 2004) it
is proposed to extract language-independent text
features using gazetteers and regular expressions
besides thesaurus and classification systems.
None of the revised works use as unique evi-
dence for multilingual clustering the identification
of cognate named entities between both sides of
the comparable corpora.
3 MDC by Cognate NE Identification
We propose an approach for MDC based only
on cognate NE identification. The NEs cate-
gories that we take into account are: PERSON,
ORGANIZATION, LOCATION, and MISCEL-
LANY. Other numerical categories such as DATE,
TIME or NUMBER are not considered because
we think they are less relevant regarding the con-
tent of the document. In addition, they can lead to
group documents with few content in common.
The process has two main phases: (1) cognate
NE identification and (2) clustering. Both phases
are described in detail in the following sections.
3.1 Cognate NE identification
This phase consists of three steps:
1. Detection and classification of the NEs in
each side of the corpus.
2. Identification of cognates between the NEs of
both sides of the comparable corpus.
3. To work out a statistic of the number of docu-
ments that share cognates of the different NE
categories.
Regarding the first step, it is carried out in each
side of the corpus separately. In our case we used
a corpus with morphosyntactical annotations and
the NEs identified and classified with the FreeLing
tool (Carreras et al, 2004).
In order to identify the cognates between NEs 4
steps are carried out:
? Obtaining two list of NEs, one for each lan-
guage.
? Identification of entity mentions in each lan-
guage. For instance, ?Ernesto Zedillo?,
?Zedillo?, ?Sr. Zedillo? will be considered
as the same entity after this step since they
refer to the same person. This step is only
applied to entities of PERSON category. The
identification of NE mentions, as well as cog-
nate NE, is based on the use of the Leven-
shtein edit-distance function (LD). This mea-
sure is obtained by finding the cheapest way
to transform one string into another. Trans-
formations are the one-step operations of in-
sertion, deletion and substitution. The result
is an integer value that is normalized by the
length of the longest string. In addition, con-
straints regarding the number of words that
the NEs are made up, as well as the order of
the words are applied.
? Identification of cognates between the NEs
of both sides of the comparable corpus. It
is also based on the LD. In addition, also
1147
constraints regarding the number and the or-
der of the words are applied. First, we tried
cognate identification only between NEs of
the same category (PERSON with PERSON,
. . . ) or between any category and MISCEL-
LANY (PERSON with MISCELLANY, . . . ).
Next, with the rest of NEs that have not been
considered as cognate, a next step is applied
without the constraint of being to the same
category or MISCELLANY. As result of this
step a list of corresponding bilingual cog-
nates is obtained.
? The same procedure carried out for obtaining
bilingual cognates is used to obtain two more
lists of cognates, one per language, between
the NEs of the same language.
Finally, a statistic of the number of documents
that share cognates of the different NE categories
is worked out. This information can be used by the
algorithm (or the user) to select the NE category
used as constraint in the clustering steps 1(a) and
2(b).
3.2 Clustering
The algorithm for clustering multilingual docu-
ments based on cognate NEs is of heuristic nature.
It consists of 3 main phases: (1) first clusters cre-
ation, (2) addition of remaining documents to ex-
isting clusters, and (3) final cluster adjustment.
1. First clusters creation. This phase consists of
2 steps.
(a) First, documents in different languages
that have more cognates in common
than a threshold are grouped into the
same cluster. In addition, at least one of
the cognates has to be of a specific cate-
gory (PERSON, LOCATION or ORGA-
NIZATION), and the number of men-
tions has to be similar; a threshold de-
termines the similarity degree. After
this step some documents are assigned
to clusters while the others are free (with
no cluster assigned).
(b) Next, it is tried to assign each free docu-
ment to an existing cluster. This is pos-
sible if there is a document in the cluster
that has more cognates in common with
the free document than a threshold, with
no constraints regarding the NE cate-
gory. If it is not possible, a new clus-
ter is created. This step can also have as
result free documents.
At this point the number of clusters created is
fixed for the next phase.
2. Addition of the rest of the documents to ex-
isting clusters. This phase is carried out in 2
steps.
(a) A document is added to a cluster that
contains a document which has more
cognates in common than a threshold.
(b) Until now, the cognate NEs have been
compared between both sides of the cor-
pus, that is a bilingual comparison. In
this step, the NEs of a language are com-
pared with those of the same language.
This can be described like a monolin-
gual comparison step. The aim is to
group similar documents of the same
language if the bilingual comparison
steps have not been successful. As in
the other cases, a document is added to
a cluster with at least a document of the
same language which has more cognates
in common than a threshold. In addi-
tion, at least one of the cognates have to
be of a specific category (PERSON, LO-
CATION or ORGANIZATION).
3. Final cluster adjustment. Finally, if there are
still free documents, each one is assigned to
the cluster with more cognates in common,
without constraints or threshold. Nonethe-
less, if free documents are left because they
do not have any cognates in common with
those assigned to the existing clusters, new
clusters can be created.
Most of the thresholds can be customized in or-
der to permit and make the experiments easier. In
addition, the parameters customization allows the
adaptation to different type of corpus or content.
For example, in steps 1(a) and 2(b) we enforce at
least on match in a specific NE category. This pa-
rameter can be customized in order to guide the
grouping towards some type of NE. In Section 4.5
the exact values we used are described.
Our approach is an heuristic method that fol-
lowing an agglomerative approach and in an it-
erative way, decides the number of clusters and
1148
locates each document in a cluster; everything is
based in cognate NEs identification. The final
number of clusters depends on the threshold val-
ues.
4 Evaluation
We wanted not only determine whether our ap-
proach was successful for MDC or not, but we also
wanted to compare its results with other approach
based on feature translation. That is why we try
MDC by selecting and translating the features of
the documents.
In this Section, first the MCD by feature transla-
tion is described; next, the corpus, the experiments
and the results are presented.
4.1 MDC by Feature Translation
In this approach we emphasize the feature selec-
tion based on NEs identification and the grammat-
ical category of the words. The selection of fea-
tures we applied is based on previous work (Casil-
las et. al, 2004), in which several document rep-
resentations are tested in order to study which of
them lead to better monolingual clustering results.
We used this MDC approach as baseline method.
The approach we implemented consists of the
following steps:
1. Selection of features (NE, noun, verb, adjec-
tive, ...) and its context (the whole document
or the first paragraph). Normally, the journal-
ist style includes the heart of the news in the
first paragraph; taking this into account we
have experimented with the whole document
and only with the first paragraph.
2. Translation of the features by using Eu-
roWordNet 1.0. We translate English into
Spanish. When more than one sense for a
single word is provided, we disambiguate by
selecting one sense if it appears in the Span-
ish corpus. Since we work with a comparable
corpus, we expect that the correct translation
of a word appears in it.
3. In order to generate the document represen-
tation we use the TF-IDF function to weight
the features.
4. Use of an clustering algorithm. Particu-
larly, we used a partitioning algorithm of the
CLUTO (Karypis, 2002) library for cluster-
ing.
4.2 Corpus
A Comparable Corpus is a collection of simi-
lar texts in different languages or in different va-
rieties of a language. In this work we com-
piled a collection of news written in Spanish and
English belonging to the same period of time.
The news are categorized and come from the
news agency EFE compiled by HERMES project
(http://nlp.uned.es/hermes/index.html). That col-
lection can be considered like a comparable cor-
pus. We have used three subset of that collection.
The first subset, call S1, consists on 65 news, 32
in Spanish and 33 in English; we used it in order
to train the threshold values. The second one, S2,
is composed of 79 Spanish news and 70 English
news, that is 149 news. The third subset, S3, con-
tains 179 news: 93 in Spanish and 86 in English.
In order to test the MDC results we carried out a
manual clustering with each subset. Three persons
read every document and grouped them consider-
ing the content of each one. They judged inde-
pendently and only the identical resultant clusters
were selected. The human clustering solution is
composed of 12 clusters for subset S1, 26 clus-
ters for subset S2, and 33 clusters for S3. All the
clusters are multilingual in the three subsets.
In the experimentation process of our approach
the first subset, S1, was used to train the parame-
ters and threshold values; with the second one and
the third one the best parameters values were ap-
plied.
4.3 Evaluation metric
The quality of the experimentation results are de-
termined by means of an external evaluation mea-
sure, the F-measure (van Rijsbergen, 1974). This
measure compares the human solution with the
system one. The F-measure combines the preci-
sion and recall measures:
F (i, j) = 2?Recall(i, j)? Precision(i, j)(Precision(i, j) +Recall(i, j)) ,
(1)
where Recall(i, j) = nijni , Precision(i, j) =
nij
nj ,
nij is the number of members of cluster human so-
lution i in cluster j, nj is the number of members
of cluster j and ni is the number of members of
cluster human solution i. For all the clusters:
F =
?
i
ni
n max{F (i)} (2)
The closer to 1 the F-measure value the better.
1149
4.4 Experiments and Results with MDC by
Feature Translation
After trying with features of different grammatical
categories and combinations of them, Table 1 and
Table 2 only show the best results of the experi-
ments.
The first column of both tables indicates the
features used in clustering: NOM (nouns), VER
(verbs), ADJ (adjectives), ALL (all the lemmas),
NE (named entities), and 1rst PAR (those of the
first paragraph of the previous categories). The
second column is the F-measure, and the third one
indicates the number of multilingual clusters ob-
tained. Note that the number of total clusters of
each subset is provided to the clustering algorithm.
As can be seen in the tables, the results depend on
the features selected.
4.5 Experiments and Results with MDC by
Cognate NE
The threshold for the LD in order to determine
whether two NEs are cognate or not is 0.2, except
for entities of ORGANIZATION and LOCATION
categories which is 0.3 when they have more than
one word.
Regarding the thresholds of the clustering phase
(Section 3.2), after training the thresholds with the
collection S1 of 65 news articles we have con-
cluded:
? The first step in the clustering phase, 1(a),
performs a good first grouping with thresh-
old relatively high; in this case 6 or 7. That
is, documents in different languages that have
more cognates in common than 6 or 7 are
grouped into the same cluster. In addition,
at least one of the cognates have to be of an
specific category, and the difference between
the number of mentions have to be equal or
less than 2. Of course, these threshold are ap-
plied after checking that there are documents
that meet the requirements. If they do not,
thresholds are reduced. This first step creates
multilingual clusters with high cohesiveness.
? Steps 1(b) and 2(a) lead to good results with
small threshold values: 1 or 2. They are de-
signed to give priority to the addition of doc-
uments to existing clusters. In fact, only step
1(b) can create new clusters.
? Step 2(b) tries to group similar documents of
the same language when the bilingual com-
parison steps could not be able to deal with
them. This step leads to good results with a
threshold value similar to 1(a) step, and with
the same NE category.
On the other hand, regarding the NE category
enforce on match in steps 1(a) and 2(b), we tried
with the two NE categories of cognates shared by
the most number of documents. Particularly, with
S2 and S3 corpus the NE categories of the cog-
nates shared by the most number of documents
was LOCATION followed by PERSON. We ex-
perimented with both categories.
Table 3 and Table 4 show the results of the ap-
plication of the cognate NE approach to subsets
S2 and S3 respectively. The first column of both
tables indicates the thresholds for each step of the
algorithm. Second and third columns show the re-
sults by selecting PERSON category as NE cat-
egory to be shared by at least a cognate in steps
1(a) and 2(b); whereas fourth and fifth columns are
calculated with LOCATION NE category. The re-
sults are quite similar but slightly better with LO-
CATION category, that is the cognate NE category
shared by the most number of documents. Al-
though none of the results got the exact number of
clusters, it is remarkable that the resulting values
are close to the right ones. In fact, no information
about the right number of cluster is provided to the
algorithm.
If we compare the performance of the two ap-
proaches (Table 3 with Table 1 and Table 4 with
Table 2) our approach obtains better results. With
the subset S3 the results of the F-measure of both
approaches are more similar than with the subset
S2, but the F-measure values of our approach are
still slightly better.
To sum up, our approach obtains slightly bet-
ter results that the one based on feature translation
with the same corpora. In addition, the number of
multilingual clusters is closer to the reference so-
lution. We think that it is remarkable that our ap-
proach reaches results that can be comparable with
those obtained by means of features translation.
We will have to test the algorithm with different
corpora (with some monolingual clusters, differ-
ent languages) in order to confirm its performance.
5 Conclusions and Future Work
We have presented a novel approach for Multilin-
gual Document Clustering based only on cognate
1150
Selected Features F-measure Multilin. Clus./Total
NOM, VER 0.8533 21/26
NOM, ADJ 0.8405 21/26
ALL 0.8209 21/26
NE 0.8117 19/26
NOM, VER, ADJ 0.7984 20/26
NOM, VER, ADJ, 1rst PAR 0.7570 21/26
NOM, ADJ, 1rst PAR 0.7515 22/26
ALL, 1rst PAR 0.7473 19/26
NOM, VER, 1rst PAR 0.7371 20/26
Table 1: MDC results with the feature translation approach and subset S2
Selected Features F-measure Multilin. Clus. /Total
NOM, ADJ 0.8291 26/33
ALL 0.8126 27/33
NOM, VER 0.8028 26/33
NE 0.8015 23/33
NOM, VER, ADJ 0.7917 25/33
NOM, ADJ, 1rst PAR 0.7520 28/33
NOM, VER, ADJ, 1rst PAR 0.7484 26/33
ALL, 1rst PAR 0.7288 26/33
NOM, VER, 1rst PAR 0.7200 24/33
Table 2: MDC results with the feature translation approach and subset S3
Thresholds 1(a), 2(b) match on PERSON 1(a), 2(b) match on LOCATION
Steps Results Clusters Results Clusters
1(a) 1(b) 2(a) 2(b) F-measure Multil./Calc./Total F-measure Multil./Calc./Total
6 2 1 5 0.9097 24/24/26 0.9097 24/24/26
6 2 1 6 0.8961 24/24/26 0.8961 24/24/26
6 2 1 7 0.8955 24/24/26 0.8955 24/24/26
6 2 2 5 0.8861 24/24/26 0.8913 24/24/26
7 2 1 5 0.8859 24/24/26 0.8913 24/24/26
6 2 2 4 0.8785 24/24/26 0.8899 24/24/26
6 2 2 6 0.8773 24/24/26 0.8833 24/24/26
6 2 2 7 0.8773 24/24/26 0.8708 24/24/26
Table 3: MDC results with the cognate NE approach and S2 subset
Thresholds 1(a), 2(b) match on PERSON 1(a), 2(b) match on LOCATION
Steps Results Clusters Results Clusters
1(a) 1(b) 2(a) 2(b) F-measure Multil./Calc./Total F-measure Multil./Calc./Total
7 2 1 5 0.8587 30/30/33 0.8621 30/30/33
6 2 1 5 0.8552 30/30/33 0.8552 30/30/33
6 2 1 6 0.8482 30/30/33 0.8483 30/30/33
6 2 1 7 0.8471 30/30/33 0.8470 30/30/33
6 2 2 5 0.8354 30/30/33 0.8393 30/30/33
6 2 2 6 0.8353 30/30/33 0.8474 30/30/33
6 2 2 4 0.8323 30/30/33 0.8474 30/30/33
6 2 2 7 0.8213 30/30/33 0.8134 30/30/33
Table 4: MDC results with the cognate NE approach and S3 subset
1151
named entities identification. One of the main ad-
vantages of this approach is that it does not depend
on multilingual resources such as dictionaries, ma-
chine translation systems, thesaurus or gazetteers.
The only requirement to fulfill is that the lan-
guages involved in the corpus have to permit the
possibility of identifying cognate named entities.
Another advantage of the approach is that it does
not need any information about the right number
of clusters. In fact, the algorithm calculates it by
using the threshold values of the algorithm.
We have tested this approach with a comparable
corpus of news written in English and Spanish, ob-
taining encouraging results. We think that this ap-
proach could be particularly appropriate for news
articles corpus, where named entities play an im-
portant role. Even more, when there is no previous
evidence of the right number of clusters. In addi-
tion, we have compared our approach with other
based on feature translation, resulting that our ap-
proach presents a slightly better performance.
Future work will include the compilation of
more corpora, the incorporation of machine learn-
ing techniques in order to obtain the thresholds
more appropriate for different type of corpus. In
addition, we will study if changing the order of
the bilingual and monolingual comparison steps
the performance varies significantly for different
type of corpus.
Acknowledgements
We wish to thank the anonymous reviewers for
their helpful and instructive comments. This work
has been partially supported by MCyT TIN2005-
08943-C02-02.
References
Benoit Mathieu, Romanic Besancon and Christian
Fluhr. 2004. ?Multilingual document clusters dis-
covery?. RIAO?2004, p. 1-10.
Arantza Casillas, M. Teresa Gonza?lez de Lena and
Raquel Mart??nez. 2004. ?Sampling and Feature
Selection in a Genetic Algorithm for Document
Clustering?. Computational Linguistics and Intel-
ligent Text Processing, CICLing?04. Lecture Notes
in Computer Science, Springer-Verlag, p. 601-612.
Hsin-Hsi Chen and Chuan-Jie Lin. 2000. ?A Multilin-
gual News Summarizer?. Proceedings of 18th Inter-
national Conference on Computational Linguistics,
p. 159-165.
Xavier Carreras, I. Chao, Lluis Padro? and M.
Padro? 2004 ?An Open-Source Suite of Lan-
guage Analyzers?. Proceedings of the 4th In-
ternational Conference on Language Resources
and Evaluation (LREC?04). Lisbon, Portugal.
http://garraf.epsevg.upc.es/freeling/.
Karypis G. 2002. ? CLUTO: A Clustering Toolkit?.
Technical Report: 02-017. University of Minnesota,
Department of Computer Science, Minneapolis, MN
55455.
David Kirk Evans, Judith L. Klavans and Kathleen
McKeown. 2004. ?Columbian Newsblaster: Multi-
lingual News Summarization on the Web?. Proceed-
ings of the Human Language Technology Confer-
ence and the North American Chapter of the Asso-
ciation for Computational Linguistics Annual Meet-
ing, HLT-NAACL?2004.
Lawrence J. Leftin. 2003. ?Newsblaster Russian-
English Clustering Performance Analysis?.
Columbia computer science Technical Reports.
Bruno Pouliquen, Ralf Steinberger, Camelia Ignat,
Emilia Ksper and Irina Temikova. 2004. ?Multi-
lingual and cross-lingual news topic tracking?. Pro-
ceedings of the 20th International Conference on
computational Linguistics, p. 23-27.
Andreas Rauber, Michael Dittenbach and Dieter Merkl.
2001. ?Towards Automatic Content-Based Organi-
zation of Multilingual Digital Libraries: An English,
French, and German View of the Russian Infor-
mation Agency Novosti News?. Third All-Russian
Conference Digital Libraries: Advanced Methods
and Technologies, Digital Collections Petrozavodsk,
RCDI?2001.
van Rijsbergen, C.J. 1974. ?Foundations of evalua-
tion?. Journal of Documentation, 30 (1974), p. 365-
373.
Joaquin Silva, J. Mexia, Carlos Coelho and Gabriel
Lopes. 2004. ?A Statistical Approach for Multi-
lingual Document Clustering and Topic Extraction
form Clusters?. Pliska Studia Mathematica Bulgar-
ica, v.16,p. 207-228.
Ralf Steinberger, Bruno Pouliquen, and Johan Scheer.
2002. ?Cross-Lingual Document Similarity Cal-
culation Using the Multilingual Thesaurus EU-
ROVOC?. Computational Linguistics and Intelli-
gent Text Processing, CICling?02. Lecture Notes in
Computer Science, Springer-Verlag, p. 415-424.
Ralf Steinberger, Bruno Pouliquen, and Camelia Ignat.
2004. ?Exploiting multilingual nomenclatures and
language-independent text features as an interlingua
for cross-lingual text analysis applications?. Slove-
nian Language Technology Conference. Information
Society, SLTC 2004.
Vossen, P. 1998. ?Introduction to EuroWordNet?.
Computers and the Humanities Special Issue on Eu-
roWordNet.
1152
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 301?310, Dublin, Ireland, August 23-29 2014.
A Data Driven Approach for Person Name Disambiguation in Web Search
Results
Agust??n D. Delgado
1
, Raquel Mart??nez
1
, V??ctor Fresno
1
, Soto Montalvo
2
1
Universidad Nacional de Educaci?on a Distancia (UNED), Madrid, Spain
2
Universidad Rey Juan Carlos (URJC), M?ostoles, Spain
1
{agustin.delgado,raquel,vfresno}@lsi.uned.es,
2
soto.montalvo@urjc.es
Abstract
This paper presents an unsupervised approach for the task of clustering the results of a search
engine when the query is a person name shared by different individuals. We propose an algo-
rithm that calculates the number of clusters and establishes the groups of web pages according
to the different individuals without the need to any training data or predefined thresholds, as
the successful state of the art systems do. In addition, most of those systems do not deal with
social media web pages and their performance could fail in a real scenario. In this paper we
also propose a heuristic method for the treatment of social networking profiles. Our approach is
compared with four gold standard collections for this task obtaining really competitive results,
comparable to those obtained by some approaches with supervision.
1 Introduction
Resolving the ambiguity of person names in web search results is a challenging problem becoming an
area of interest for Natural Language Processing (NLP) and Information Retrieval (IR) communities.
This task can be defined informally as follows: given a query of a person name in addition to the results
of a search engine for that query, the goal is to cluster the resultant web pages according to the different
individuals they refer to. Thus, the challenge of this task is estimating the number of different individuals
and grouping the pages of the same individual in the same cluster. The difficulty of this task resides in
the fact that a single person name can be shared by many people: according to the U.S. Census Bureau,
90000 different names are shared by 100 million people (Artiles et al., 2007). This problem has had an
impact in the Internet and that is why several vertical search engines specialized in web people search
have appeared in the last years, e.g. spokeo.com or 123people.com. This task should not be mixed
up with entity linking (EL), which goal is to link name mentions of entities in a document collection to
entities in a reference knowledge base (typically Wikipedia), or to detect new entities.
The main difficulties of clustering web pages referring to the same individual come from their possible
heterogeneous nature. For example, some pages may be professional sites, while others may be blogs
containing personal information. In addition, the popularity of social networking services makes the
search engine usually returns several social profiles belonging to different individuals sharing the same
name, as much from the same social networking service as from different services. These social pages
often introduce noisy information and make the state of the art algorithms break down (Berendsen et
al., 2012). Due to these problems, the users have to refine the queries with additional terms. This task
gets harder when the person name is shared by a celebrity or a historical figure, because the results
of the search engines are dominated by that individual, making the search of information about other
individuals more difficult.
WePS
1
(Web People Search) evaluation campaigns proposed this task in a web searching scenario
providing several corpora for evaluating the results of their participants, particularly WePS-1, WePS-2
and WePS-3 campaigns. This framework allows our approach to be compared with the state of the art
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
http://nlp.uned.es/weps/
301
systems. We also evaluate our system with ECIR2012 corpus
2
, a data set that includes social networking
profiles, providing a more real scenario for this task.
The most successful state of the art systems have addressed this problem with some kind of supervi-
sion. This work proposes a data-driven method for this task with the aim of eliminating the elements
of human involvement in the process as much as possible. The main contribution of this work is a new
unsupervised approach for resolving person name ambiguity of web search results based on the use of
capitalized n-grams. In our approach the decision if two web pages have to be grouped only depends on
the information of both pages. In addition, we also propose a heuristic method for the treatment of social
media profile web pages in this context.
The paper is organized as follows: in Section 2 we discuss related work; Section 3 details the way
we represent the web pages, the algorithm and the heuristic for social pages; in Section 4 we describe
the collections used for evaluating our method and we show our results making a comparison with other
systems; the paper ends with some conclusions and future work in Section 5.
2 Related Work
Several approaches have been proposed for clustering search results for a person name query. The main
differences among all of them are the features they use to represent the web pages and the clustering
algorithm. However, the most successful of them have in common that they use some kind of supervision:
learning thresholds and/or fixing manually the value of some parameters according to training data.
Regarding the way of representing a web page, the most popular features used by the most success-
ful state of the art approaches are Name Entities (NE) and Bag of Words (BoW) weighted by TF-IDF
function. In addition to such features, the systems usually use other kind of information. Top systems
from WePS-1 and WePS-2 campaigns, CU COMSEM (Chen and Martin, 2007) and PolyUHK (Chen et
al., 2009), distinguish several kind of tokens according to different schemes (URL tokens, title tokens,
. . . ) and build a feature vector for each sort of tokens, using also information based on the noun phrases
appearing in the documents. PolyUHK also adds pattern techniques, attribute extraction and detection
when a web page is written in a formal way. A more recent system, HAC Topic (Liu et al., 2011), also
uses BoW of local and global terms weighted by TF-IDF. It adds a topic capturing method to create a Hit
List of shared high weighted tokens for each cluster obtaining better results than WePS-1 participants.
On the other hand, the WePS-3 best system, YHBJ (Chong and Shi, 2010), uses information extracted
manually from Wikipedia adding to BoW and NE weighted by TF-IDF.
Regarding the clustering algorithms, looking at WePS campaigns results, the top ranked systems have
in common the use of the Hierarchical Agglomerative Clustering algorithm (HAC) described in (Man-
ning et al., 2008). Different versions of this algorithm were used by (Chen and Martin, 2007; Chen et
al., 2009; Elmacioglu et al., 2007; Liu et al., 2011; Balog et al., 2009; Chong and Shi, 2010).
(Berendsen et al., 2012) presented another gold standard for this task, ECIR2012, composed by Dutch
person names and social media profile web pages. The system of the authors, UvA, distinguishes the web
pages between social ones and non social ones, clusters each group separately and then combines both
clustering solutions. They represent each web page as a BoW vector weighted by TF-IDF, and use cosine
similarity for comparing web pages. They use HAC algorithm for clustering non social web pages, while
use a ?one in one? policy for the social ones. Finally, they mix both groups by means of an algorithm
which penalizes clusters with social webs or simply taking the union of both clustering solutions. They
perform a partial parameter sweep on the WePS-2 data set to fix the clustering thresholds, while explore
combinations of other system parameters.
The only system that does not use training data, DAEDALUS (Lana-Serrano et al., 2010), which uses
k-Medoids, got poor results in WePS-3 campaign. In short, the successful state of the art systems need
some kind of supervised learning using training data or fixing parameters manually. In this paper we
explore and propose an approach to address this problem by means of data-driven techniques without the
use of any kind of supervision.
2
http://ilps.science.uva.nl/resources/ecir2012rdwps
302
3 Proposed Approach
We distinguish two main phases in this clustering task: web page representation (Sections 3.1 and 3.2)
and web page grouping (Sections 3.3 and 3.4). In addition, we propose an heuristic to deal with social
profiles web pages (Section 3.5).
3.1 Feature Selection
The aim of this phase is to extract relevant information that could identify an individual. We assume the
main following hypotheses:
(i) Capitalized n-grams co-occurrence could be a reliable way for deciding when two web pages
refer the same individual. Capitalized n-grams usually are Named Entities (organizations and company
names, locations or other person names related with the individual) or information not detected by some
NE recognizers as for example, the title of books, films, TV shows, and so on. In a previous study
with WePS-1 training corpus using the Stanford NER
3
to annotate NE, we detected that only 55.78%
of the capitalized tokens were annotated as NE or components of a NE by the NER tool. So the use
of capitalized tokens allows increase the number of features compared to the use of only NE. We also
compared the n-gram representation with capitalized tokens and with NE. We found that 30.97% of the
3-grams of capitalized tokens were also NE 3-grams, and 25.64% of the 4-grams of capitalized tokens
were also NE 4-grams. So even in the case of n-grams the use of capitalized tokens increases the number
of features compared to the use of only NE. Table 1 shows the differences in performance when using
n-grams representation with NE or with capitalized tokens.
(ii) If two web pages share capitalized n-grams, the higher is the value of n, the more probable the two
web pages refer to the same individual. In this case we define ?long enough n-grams? as those compose
by at least 3 capitalized tokens.
Thus, a web page W is initially represented as the sequence of tokens starting in uppercase, in the
order as they appear in the web page. In each step of the algorithm, a web pageW will be represented by
its long enough n-grams, taking different values for n, as we describe in Section 3.4. Notice that some
web pages could not be represented with this proposal because all their content was written in lowercase.
In the case of the collections that we describe in Section 4.1, 0.63% of the web pages are not represented
for this reason.
3.2 Weighting Functions
We test the well known TF and TF-IDF functions, and z-score (Andrade and Medina, 1998). The z-score
of a n-gram a in a web page W
i
is defined as follows: z-score(a,W
i
) =
TF (a,W
i
)??
?
, where TF (a,W
i
)
is the frequency of the n-gram a in W
i
; ? is the mean frequency of the background set; and ? is the
standard deviation of the background set. In this context the background set is the set of web pages that
share the person name. This score gives an idea of the distance of the frequency of an n-gram in a web
page from the general distribution of this n-gram in the background set.
3.3 Similarity Functions
To determine the similarity between two web pages we try the cosine distance, a widely measure
used in clustering, and the weighted Jaccard coefficient between two bags of n-grams defined as
W.Jaccard(W
n
i
,W
n
j
) =
?
k
min(m(t
n
k
i
,i),m(t
n
k
j
,j))
?
k
max(m(t
n
k
i
,i),m(t
n
k
j
,j))
, where the meaning of m(t
n
k
i
, i) is explained in Sec-
tion 3.4. Since weighted Jaccard coefficient needs non-negative entries and we want the cosine similarity
of two documents to range from 0 to 1, we translate the values of the z-score so that they are always
non-negative.
3.4 Algorithm
The algorithm UPND (Unsupervised Person Name Disambiguator) can be seen in Algorithm 1. The
description of this first algorithm does not take into account social profile web pages.
3
http://nlp.stanford.edu/software/CRF-NER.shtml
303
UPND algorithm receives as input a set of web documents with a mention to the same person name,
let beW = {W
1
,W
2
, . . . ,W
N
}, and starts assigning a cluster C
i
for each document W
i
. UPND also
receives as input a pair of positive integer values r
1
and r
2
, such that r
2
? r
1
, specifying the range of
values of n in the n-grams extracted from each web document. In each step of the algorithm we assign to
each web page W
i
a bag of n-grams W
n
i
= {(t
n
1
,m(t
n
1
, i)), (t
n
2
,m(t
n
2
, i)), . . . , (t
n
k
i
,m(t
n
k
i
, i))}, where
each t
n
r
is a n-gram extracted from W
i
and m(t
n
r
, i) is the corresponding weight of the n-gram t
n
r
in
the web page W
i
, being r ? {1, 2, . . . , k
i
}. In Algorithm 1 the function setNGrams(n,W) in line 6
calculates for each web page in the set W its bag of n-grams representation. Sim(W
n
i
,W
n
j
) in line 9
refers to the similarity between web pages W
i
and W
j
.
To decide when two web pages refer the same individual we propose a threshold ?. For each pair
of web pages represented as bag of n-grams, let be W
n
i
and W
n
j
, we compute the threshold as fol-
lows: ?(W
n
i
,W
n
j
) =
min(m,k)?shared(W
n
i
,W
n
j
)
max(m,k)
, where m and k are the number of n-grams of W
i
and W
j
respectively, and shared(W
n
i
,W
n
j
) is the number of n-grams shared by those web pages i.e.
shared(W
n
i
,W
n
j
) = |W
n
i
?W
n
j
|. Notice that shared(W
n
i
,W
n
j
) is superiorly limited by min(m, k).
This threshold holds two desirable properties: (i) The more n-grams are shared by W
i
and W
j
, the
lower ?(W
n
i
,W
n
j
) is, so the clustering condition of the algorithm is less strict. (ii) It avoids the penal-
ization due to big differences between the size of the web pages.
Thus, we decide that two web pages W
i
and W
j
refer to the same person if Sim(W
n
i
,W
n
j
) ?
?(W
n
i
,W
n
j
), so C
i
= C
i
? C
j
(lines 9, 10 and 11).
We assume that we can get accurate and reliable information for disambiguating with n-grams of at
least size 3. Thus, we propose to iterate this process for 3-grams and 4-grams, i.e. UPND( W, 3, 4).
We consider that selecting a value of n grater than 4 could lead to find few n-grams, so that many web
pages could be under-represented. On the other hand, previous experiments using also bigrams showed
that they are not suitable for this approach. This algorithm is polynomial and has a computational cost
in O(N
2
), where N is the number of web pages.
Algorithm 1 UPND(W, r
1
, r
2
)
Require: Set of web pages that shared a person name W= {W
1
,W
2
, ...,W
N
}, r
1
, r
2
? 1 such that
r
2
? r
1
Ensure: Set of clusters C = {C
1
, C
2
, ..., C
l
}
1: for n = 1 to N do
2: C
i
= {W
i
}
3: end for
4: C = {C
1
, C
2
, ..., C
N
}.
5: for n = r
1
to r
2
do
6: setNGrams(n,W).
7: for i = 1 to N do
8: for j = i+ 1 to N do
9: if Sim(W
n
i
,W
n
j
) ? ?(W
n
i
,W
n
j
) then
10: C
i
= C
i
? C
j
11: C = C \{C
j
}
12: end if
13: end for
14: end for
15: end for
16: return C
3.5 Social Media Treatment
Social networking services have increased their popularity and number of users in the last years. This
fact affects this task mainly in two ways. On one hand, as a result of the success of this kind of platforms,
304
a lot of web pages contain terms related to them (e.g. the name of these platforms: Twitter, Facebook,
LinkedIn, etc.). On the other hand, for a person name query in a search engine, it usually returns several
profiles of such person name that are as much in the same as in different social networking services.
These profiles usually are from different people sharing the same name, so they should be in different
clusters. Most of the methods of the state of the art do not take into account this fact, usually taking as
features tokens from the URL or the title of each web page, which includes the name of these platforms.
This practice could lead to add noise to the representation of the web pages.
(Berendsen et al., 2012) proposed the ?one in one? baseline to deal with social platform web pages,
which creates a singleton cluster for each social web page. However, its main disadvantage is that it does
not consider that a same individual could have accounts in several social platforms. A search engine
could also return web pages from a social platform which are not profiles, as for example, a group page
of Facebook where a person expounds an opinion, in addition to the profile of the same individual in that
social platform. In these cases the ?one in one? baseline also fails.
We propose a heuristic method that takes into account the limitations of the ?one in one? heuristic,
letting group social web pages from different platforms and also cluster social web pages from the same
social platform. The algorithm that implements our heuristic is SUPND (Social UPND). This algorithm
applies UPND with the following restriction: two web pages assigned to the same social networking
service cannot be compared. This policy is taken because when a search engine returns several links from
the same social platform, they usually refer to different individuals. However, this does not necessarily
imply that two web pages belonging to the same social site cannot belong to the same cluster, because
they would be compared to other webs pages separately, possibly ending up in the same cluster in a
transitive way. For example, giving two web pages from Facebook, let be FB
1
and FB
2
, and a non-
social web page W , then FB
1
and FB
2
would not be compared, however each FB
i
would be compared
with W . If SUPND decides to cluster each FB
i
with W , then finally both web pages, from the same
platform, would be in the same cluster. To identify the social web pages we obtain a list of social media
platforms from Wikipedia
4
, so when looking at the URL of a web page, we can detect if it corresponds
to any of those social media platforms. If it is the case, we assign to that web page its social media site.
The computational cost of SUPND is the same of UPND.
4 Experiments
In this section we present the corpora of web pages used, the preprocessing of each web page, the
experiments carried out and the obtained results.
4.1 Web People Search Collections
WePS is a competitive evaluation campaign that proposes several tasks including resolution of disam-
biguation on the Web data. In particular, WePS-1, WePS-2 and WePS-3 campaigns provide an evaluation
framework consisting in several annotated data sets composed of English person names.
In these experiments we use WePS-1 (Artiles et al., 2007) test corpus composed by 30 English person
names and the top 100 search results from Yahoo! search engine; WePS-2 (Artiles et al., 2009a) contain-
ing 30 person names and the top 150 search results from Yahoo! search engine; and WePS-3 (Artiles et
al., 2010) containing 300 person names and the top 200 search results from Yahoo! All WePS corpora
have few social profile web pages, so the impact of this kind of pages in the results of the algorithms is
insignificant. We also use the ECIR2012 corpus, which is composed by 33 Dutch person names selected
from query logs of a people search engine. For each person name the web pages set is built retrieving
several profiles from social media platforms as Facebook, Twitter or LinkedIn, and results returned by
Google, Bing and Yahoo! search engines. This data set gives a more real scenario for this task than the
WePS ones, because it includes social network profiles of several person sharing the same name.
4
en.wikipedia.org/wiki/Category:Social networking services
305
4.2 Corpus Preprocessing
Given a person name and a set of web pages, we first discard web pages that do not mention such name
using several patterns that take into account the usual structure of person names.
For each not discarded web page, we delete the name and the surname because they appear in all the
remaining documents and are the object of the ambiguity. We also delete stop words.
4.3 Results and Discussion
We present our results for all the corpora comparing them with the state of the art systems. The figures
in the tables are macro-averaged, i.e., they are calculated for each person name and then averaged over
all test cases. For WePS data sets we get the same results for UPND and SUPND algorithms, because
these collections include few social networking profiles. The metrics used in this section are the BCubed
metrics defined in (Bagga and Baldwin, 1998): BCubed precision (BP ), BCubed recall (BR) and their
harmonic mean F
0.5
(BP/BR). (Artiles, 2009) showed that these metrics are accurate for clustering
tasks, particularly for person name disambiguation in the Web. We use the Wilcoxon test (Wilcoxon,
1945) to detect statistical significance in the differences of the results considering a confidence level
of 95%. In order to compare our algorithm with the WePS better results using the Wilcoxon test, the
samples consist in the pairs of values F
?=0.5
(BP/BR) of each system for each person name.
First, Table 1 shows the results of UPND using n-grams of capitalized tokens and n-grams of NE
with WePS-1 training corpus. Experiments include the three weighting functions and the two similarity
functions. The results of using n-grams of NE rank below those obtained with n-grams of capitalized
tokens in all cases. The Wilcoxon test comparing the results of both representations shows that there are
significant differences between them, except TF and TF-IDF with cosine. So we can conclude that in our
approach using n-grams of capitalized tokens outperforms the use of n-grams of NE, what confirms our
hypothesis.
TF z-score TF-IDF
Representation W. Jaccard Cosine W. Jaccard Cosine W. Jaccard Cosine
Capitalized n-gram 0.82 0.69 0.83 0.78 0.81 0.63
NE (Stanford NER) 0.77 0.6 0.77 0.72 0.76 0.6
Table 1: F
0.5
(BP/BR) results of UPND algorithm comparing capitalized n-gram and NE n-gram
representations with WePS-1 training corpus.
In Table 2 we show the results of UPND for all WePS test data sets with the three weighting functions
and the two similarity measures.
WePS-1 WePS-2 WePS-3
BP BR F
0.5
(BP/BR) BP BR F
0.5
(BP/BR) BP BR F
0.5
(BP/BR)
W. Jaccard
TF 0.73 0.77 0.74 0.82 0.82 0.81 0.46 0.70 0.50
z-score 0.70 0.78 0.72 0.80 0.84 0.81 0.44 0.72 0.50
TF-IDF 0.73 0.77 0.73 0.82 0.82 0.81 0.46 0.70 0.50
Cosine
TF 0.92 0.61 0.72 0.95 0.61 0.73 0.75 0.45 0.51
z-score 0.85 0.69 0.76 0.91 0.73 0.81 0.62 0.56 0.53
TF-IDF 0.94 0.57 0.7 0.96 0.52 0.65 0.79 0.40 0.49
Table 2: Results of UPND algorithm for WePS test data sets.
The combination of z-score with cosine gets the best balance between the values of BP and BR,
reaching the highest results of F
?=0.5
for the three WePS corpora. The combination of TF-IDF with
cosine gets the best BP results, but BR results are the lowest. On the other hand, the combination of
z-score and Jaccard gets the best BR results, but the BP results are the lowest.
Regarding the significance of the differences between the best results, the improvement between z-
score with cosine and z-score with Jaccard is significant in WePS-1 and WePS-3, but not in WePS-2.
The improvement between z-score with cosine and Jaccard with TF is significant only in WePS-3.
306
Thus, we select the combination of z-score as weight function and cosine as similarity function as the
most suitable combination for our algorithm. Therefore we use it in the following experiments.
Table 3 shows the results of UPND with WePS-1 test, WePS-2 and WePS-3 corpora in addition to
the top ranking systems of the campaigns, and also the results obtained by HAC Topic system in the
case of WePS-1. We include the results obtained by three unsupervised baselines called ALL IN ONE,
ONE IN ONE and Fast AP. ALL IN ONE provides a clustering solution where all the documents are
assigned to a single cluster, ONE IN ONE returns a clustering solution where every document is assigned
to a different cluster, and Fast AP applies a fast version of Affinity Propagation described in (Fujiwara et
al., 2011) using the function TF-IDF to weight the tokens of each web page, and the cosine distance to
compute the similarity.
System BP BR F
0.5
(BP/BR)
WePS-1
(+) HAC Topic 0.79 0.85 0.81 ?
(-) UPND 0.85 0.69 0.76 ?
(+)(*) CU COMSEM 0.61 0.83 0.70 ?
(+)(*) PSNUS 0.68 0.73 0.70 ?
(+)(*) IRST-BP 0.68 0.71 0.69 ?
(+)(*) UVA 0.79 0.50 0.61 ?
(+)(*) SHEF 0.54 0.74 0.62 ?
(-) ONE IN ONE 1.00 0.43 0.57 ?
(-) Fast AP 0.69 0.55 0.56 ?
(-) ALL IN ONE 0.18 0.98 0.25 ?
WePS-2
(+) ORACLE 1 0.89 0.83 0.85 ?
(+) ORACLE 2 0.91 0.81 0.85 ?
(+)(*) PolyUHK 0.87 0.79 0.82
(+)(*) ITC-UT 1 0.93 0.73 0.81
(-) UPND 0.91 0.73 0.81 ?
(+)(*) UVA 1 0.85 0.80 0.81
(+)(*) XMEDIA 3 0.82 0.66 0.72 ?
(+)(*) UCI 2 0.66 0.84 0.71 ?
(-) ALL IN ONE 0.43 1.00 0.53 ?
(-) Fast AP 0.80 0.33 0.41 ?
(-) ONE IN ONE 1.00 0.24 0.34 ?
WePS-3
(+)(*) YHBJ 2 0.61 0.60 0.55
(-) UPND 0.62 0.56 0.53 ?
(+)(*) AXIS 2 0.69 0.46 0.50 ?
(+)(*) TALP 5 0.40 0.66 0.44 ?
(+)(*) RGAI AE 1 0.38 0.61 0.40 ?
(+)(*) WOLVES 1 0.31 0.80 0.40 ?
(-)(*) DAEDALUS 3 0.29 0.84 0.39 ?
(-) Fast AP 0.73 0.30 0.38 ?
(-) ONE IN ONE 1.00 0.23 0.35 ?
(-) ALL IN ONE 0.22 1.00 0.32 ?
Table 3: Results of UPND and the top state of the art systems with WePS corpora: (+) means system
with supervision; (-) without supervision and (*) campaign participant. Significant differences between
UPND and other systems are denoted by (?); (?) means that in this case the statistical significance is
not evaluated.
Our method UPND outperforms WePS-1 participants and all the unsupervised baselines described
before. HAC Topic also outperforms the WePS-1 top participant systems and our algorithm. This system
uses several parameters obtained by training with the WePS-2 data set: token weight according to the
kind of token (terms from URL, title, snippets, . . . ) and thresholds used in the clustering process. Note
that WePS-1 participants used the training corpus provided to the campaign, the WePS-1 training data,
so in this case the best performance of HAC Topic could be not only because of the different approach,
but also because of the different training data set.
Our algorithm obtains significative better results than the WePS-1 top participant results, and
HAC Topic obtains significative better results than it according to the Wilcoxon test. UPND obtains
significative better results than IRST-BP system (the third in the WePS-1 ranking), also based on the
co-ocurrence of n-grams.
Regarding WePS-2 we add in Table 3 two oracle systems provided by the organizers. These systems
use BoW of tokens (ORACLE 1) or bigrams (ORACLE 2) weighted by TF-IDF, deleting previously
stop words, and later apply HAC with single linkage with the best thresholds for each person name. We
do not include the results of the HAC Topic system since it uses this data set for training their algorithm.
The significance test shows that the top WePS-2 systems PolyUHK, UVA 1 and ITC-UT 1 obtain
307
similar results than UPND, however they use some kind of supervision. The results of all these systems
are the closest to the oracle systems provided by the organizers, which know the best thresholds for each
person name.
In the case of WePS-3, the organizers did not take into account the whole clustering solution provided
by the systems like in previous editions, but only checks the accuracy of the clusters corresponding
to two selected individuals per person name. In this case, the first two systems YHBJ 2 and UPND
do not have significant difference in their results. Notice that YHBJ 2 system makes use of concepts
extracted manually from Wikipedia. Note that UPND also obtains significative better results than
DAEDALUS 3, the only one participant that does not use training data.
Regarding the experiments with the ECIR2012 corpus, which contains social profiles, Table 4 shows
the results of the two versions of our algorithm and the results of the system of the University of Ams-
terdam (UvA). As far as we know, no other systems have been tested with this gold standard. SUPND
obtains significative better results than UPND due to its special treatment for social web pages. The
UvA system outperforms our algorithm SUPND and this improvement is significative. Note that the
heuristic for social pages in SUPND outperforms UPND using the ?one in one? heuristic.
System BP BR F
0.5
(BP/BR)
(+) UvA (best perf.) 0.90 0.80 0.83 ?
(-) SUPND 0.95 0.68 0.78 ?
(-) UPND (one in one) 0.98 0.62 0.74 ?
(-) UPND 0.74 0.74 0.72 ?
Table 4: Results of SUPND and UPND algorithms for ECIR2012 corpus: (+) means system with
supervision and (-) without supervision. Significant differences between SUPND and other systems
are denoted by (?); (?) means that in this case the statistical significance is not evaluated.
After all these experiments, we can conclude that our approach gets the best results of all the com-
pletely unsupervised approaches. Moreover, the precision scores for all collections are very high and
confirm that our approach is accurate to get relevant information for characterizing an individual. We
also obtain competitive recall results, what lead to a competitive system that carries out person name
disambiguation in web search results with minimum human supervision.
5 Conclusions and Future Work
We present a new approach for person name disambiguation of web search results. Our method does
not need training data to calculate thresholds to determine the number of different individuals sharing
the same name, or whether two web pages refer to the same individual or not. Although supervised
approaches have been successful in many NLP and IR tasks, they require enough and representative
training data to guaranty the results will be consistent for different data collections, which requires a
huge human effort.
The two algorithms proposed provide a clustering solution for this task by means of data-driven meth-
ods that do not need learning from data. Our approach is not very expensive in computational cost,
obtaining very competitive results in several data sets compared with the best state of the art systems.
Our proposal is based on getting reliable information for disambiguating, particularly long n-grams
composed by uppercase tokens. According to our results, this hypothesis has shown successful, getting
high precision values and acceptable recall scores. Anyway, we would like to improve recall results
without losing of precision, filter out noisy capitalized n-grams, and build an alternative representation
for web pages containing all their tokens in lowercase.
We have observed that this task gets harder when we have to deal with social media profiles. A system
thought for being used in a real scenario has to take into account this kind of web pages, since they are
usually returned by search engines when a user introduces a person name as a query. Most state of the
art systems do not deal with this problem. We have proposed in this paper a new heuristic method for
processing social platforms profiles for this clustering task.
308
Person name disambiguation has been mainly addressed in a monolingual scenario, e.g. WePS cor-
pora are English data sets and Dutch the ECIR2012 collection. We would like to address this task in
a multilingual scenario. Although search engines return their results taking into account the country of
the user, with some queries we can get results written in several languages. This scenario has not been
considered by the state of the art systems so far.
Acknowledgements
This work has been part-funded by the Spanish Ministry of Science and Innovation (MED-RECORD
Project, TIN2013-46616-C2-2-R) and by UNED Project (2012V/PUNED/0004).
References
Miguel A. Andrade, and Alfonso Valencia. 1998. Automatic extraction of keywords from scientific text: applica-
tion to the knowledge domain of protein families. Bioinformatics, 14:600-607, 1998.
Javier Artiles, Julio Gonzalo and Satoshi Sekine. 2007. The SemEval-2007 WePS Evaluation: Establishing a
Benchmark for the Web People Search Task. In Proceedings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), pages 64?69, Prague, Czech Republic, June 2007. Association for Computational
Linguistics.
Javier Artiles. 2009. Web People Search. PhD Thesis, UNED University.
Javier Artiles, Julio Gonzalo and Satoshi Sekine. 2009b. Weps 2 Evaluation Campaign: Overview of the Web
People Search Clustering Task. In 2nd Web People Search Evaluation Workshop (WePS 2009), 18th WWW
Conference, 2009.
Javier Artiles, Andrew Borthwick, Julio Gonzalo, Satoshi Sekine and Enrique Amig?o . 2010. WePS-3 Evaluation
Campaign: Overview of the Web People Search Clustering and Attribute Extraction Tasks. In Third Web People
Search Evaluation Forum (WePS-3), CLEF 2010.
Amit Bagga and Breck Baldwin. 1998. Entity-Based Cross-Document Coreferencing Using the Vector Space
Model. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th
International Conference on Computational Linguistics - Volume 1, ACL ?98, pages 79?85, Stroudsburg, PA,
USA, 1998. Association for Computational Linguistics.
Krisztian Balog, Jiyin He, Katja Hofmann, Valentin Jijkoun, Christof Monz, Manos Tsagkias, Wouter Weerkamp,
and Maarten de Rijke. 2009. The University of Amsterdam at WePS-2. In 2nd Web People Search Evaluation
Workshop (WePS 2009), 18th WWW Conference, 2009.
Richard Berendsen, Bogomil Kovachev, Evangelia-Paraskevi Nastou, Maarten de Rijke, and Wouter Weerkamp.
2012. Result Disambiguation in Web People Search. In Proceedings of the 34th European conference on
Advances in Information Retrieval, ECIR?12, pages 146?157, Berlin, Heidelberg, 2012. Springer-Verlag.
Ying Chen and James Martin. 2007. CU-COMSEM: Exploring Rich Features for Unsupervised Web Personal
Named Disambiguation. In Proceedings of SemEval 2007, Assocciation for Computational Linguistics, pages
125?128, 2007.
Ying Chen, Sophia Yat Mei Lee and Chu-Ren Huang. 2009. PolyUHK: A Robust Information Extraction System
for Web Personal Names. In 2nd Web People Search Evaluation Workshop (WePS 2009), 18th WWW Confer-
ence, 2009.
Ergin Elmacioglu, Yee Fan Tan, Su Yan, Min-Yen Kan, and Dongwon Lee. 2007. PSNUS: Web People Name
Disambiguation by Simple Clustering with Rich Features. In Proceedings of the 4th International Workshop on
Semantic Evaluations, SemEval ?07, pages 268?271, Stroudsburg, PA, USA, 2007. Association for Computa-
tional Linguistics.
Yasuhiro Fujiwara, Go Irie and Tomoe Kitahara. 2011. Fast Algorithm for Affinity Propagation. In Proceedings of
the Twenty-Second International Joint Conference on Artificial Intelligence(IJCAI)- Volume Three, 2238?2243,
Barcelona, Catalonia, Spain.
Sara Lana-Serrano , Julio Villena-Rom?an , Jos?e Carlos Gonz?alez-Crist?obal. 2010. Daedalus at WebPS-3 2010:
k-Medoids Clustering using a Cost Function Minimization. In Third Web People Search Evaluation Forum
(WePS-3), CLEF 2010.
309
Zhengzhong Liu, Qin Lu, and Jian Xu. 2011. High Performance Clustering for Web Person Name Disambiguation
using Topic Capturing. In International Workshop on Entity-Oriented Search (EOS), 2011.
Chong Long and Lei Shi. 2010. Web Person Name Disambiguation by Relevance Weighting of Extended Feature
Sets. In Third Web People Search Evaluation Forum (WePS-3), CLEF 2010.
Gideon S. Mann. 2006. Multi-Document Statistical Fact Extraction and Fusion. PhD thesis, Johns Hopkins
University, Baltimore, MD, USA, 2006. AAI3213760.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch?utze. 2008. Introduction to Information Retrieval.
Cambridge University Press, New York, NY, USA, 2008.
Octavian Popescu and Bernardo Magnini. 2007. IRST-BP: Web People Search Using Name Entities In In
Proceedings of SemEval 2007, Assocciation for Computational Linguistics, pages 195?198, 2007.
Frank Wilcoxon. 1945. Individual Comparisons by Ranking Methods, volume 1 (6). Biometrics Bulletin, Decem-
ber 1945.
310
