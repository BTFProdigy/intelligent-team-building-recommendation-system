Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1533?1541,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Phrase Dependency Parsing for Opinion Mining
Yuanbin Wu, Qi Zhang, Xuanjing Huang, Lide Wu
Fudan University
School of Computer Science
{ybwu,qi zhang,xjhuang,ldwu}@fudan.edu.cn
Abstract
In this paper, we present a novel approach
for mining opinions from product reviews,
where it converts opinion mining task to
identify product features, expressions of
opinions and relations between them. By
taking advantage of the observation that a
lot of product features are phrases, a con-
cept of phrase dependency parsing is in-
troduced, which extends traditional depen-
dency parsing to phrase level. This con-
cept is then implemented for extracting re-
lations between product features and ex-
pressions of opinions. Experimental eval-
uations show that the mining task can ben-
efit from phrase dependency parsing.
1 Introduction
As millions of users contribute rich information
to the Internet everyday, an enormous number of
product reviews are freely written in blog pages,
Web forums and other consumer-generated medi-
ums (CGMs). This vast richness of content be-
comes increasingly important information source
for collecting and tracking customer opinions. Re-
trieving this information and analyzing this con-
tent are impossible tasks if they were to be manu-
ally done. However, advances in machine learning
and natural language processing present us with
a unique opportunity to automate the decoding of
consumers? opinions from online reviews.
Previous works on mining opinions can be di-
vided into two directions: sentiment classification
and sentiment related information extraction. The
former is a task of identifying positive and neg-
ative sentiments from a text which can be a pas-
sage, a sentence, a phrase and even a word (So-
masundaran et al, 2008; Pang et al, 2002; Dave
et al, 2003; Kim and Hovy, 2004; Takamura et
al., 2005). The latter focuses on extracting the el-
ements composing a sentiment text. The elements
include source of opinions who expresses an opin-
ion (Choi et al, 2005); target of opinions which
is a receptor of an opinion (Popescu and Etzioni,
2005); opinion expression which delivers an opin-
ion (Wilson et al, 2005b). Some researchers refer
this information extraction task as opinion extrac-
tion or opinion mining. Comparing with the for-
mer one, opinion mining usually produces richer
information.
In this paper, we define an opinion unit as a
triple consisting of a product feature, an expres-
sion of opinion, and an emotional attitude(positive
or negative). We use this definition as the basis for
our opinion mining task. Since a product review
may refer more than one product feature and ex-
press different opinions on each of them, the rela-
tion extraction is an important subtask of opinion
mining. Consider the following sentences:
1. I highly [recommend]
(1)
the Canon SD500
(1)
to
anybody looking for a compact camera that can take
[good]
(2)
pictures
(2)
.
2. This camera takes [amazing]
(3)
image qualities
(3)
and its size
(4)
[cannot be beat]
(4)
.
The phrases underlined are the product features,
marked with square brackets are opinion expres-
sions. Product features and opinion expressions
with identical superscript compose a relation. For
the first sentence, an opinion relation exists be-
tween ?the Canon SD500? and ?recommend?, but
not between ?picture? and ?recommend?. The ex-
ample shows that more than one relation may ap-
pear in a sentence, and the correct relations are not
simple Cartesian product of opinion expressions
and product features.
Simple inspection of the data reveals that prod-
uct features usually contain more than one word,
such as ?LCD screen?, ?image color?, ?Canon
PowerShot SD500?, and so on. An incomplete
product feature will confuse the successive anal-
ysis. For example, in passage ?Image color is dis-
1533
appointed?, the negative sentiment becomes ob-
scure if only ?image? or ?color? is picked out.
Since a product feature could not be represented
by a single word, dependency parsing might not be
the best approach here unfortunately, which pro-
vides dependency relations only between words.
Previous works on relation extraction usually use
the head word to represent the whole phrase and
extract features from the word level dependency
tree. This solution is problematic because the in-
formation provided by the phrase itself can not be
used by this kind of methods. And, experimental
results show that relation extraction task can ben-
efit from dependencies within a phrase.
To solve this issue, we introduce the concept
of phrase dependency parsing and propose an ap-
proach to construct it. Phrase dependency pars-
ing segments an input sentence into ?phrases? and
links segments with directed arcs. The parsing
focuses on the ?phrases? and the relations be-
tween them, rather than on the single words inside
each phrase. Because phrase dependency parsing
naturally divides the dependencies into local and
global, a novel tree kernel method has also been
proposed.
The remaining parts of this paper are organized
as follows: In Section 2 we discuss our phrase de-
pendency parsing and our approach. In Section 3,
experiments are given to show the improvements.
In Section 4, we present related work and Section
5 concludes the paper.
2 The Approach
Fig. 1 gives the architecture overview for our ap-
proach, which performs the opinion mining task
in three main steps: (1) constructing phrase de-
pendency tree from results of chunking and de-
pendency parsing; (2) extracting candidate prod-
uct features and candidate opinion expressions; (3)
extracting relations between product features and
opinion expressions.
2.1 Phrase Dependency Parsing
2.1.1 Overview of Dependency Grammar
Dependency grammar is a kind of syntactic the-
ories presented by Lucien Tesni`ere(1959). In de-
pendency grammar, structure is determined by the
relation between a head and its dependents. In
general, the dependent is a modifier or comple-
ment; the head plays a more important role in de-
termining the behaviors of the pair. Therefore, cri-
Phrase Dependency Parsing  
Review Crawler 
Review Database
 Chunking DependencyParsing
  
CandidateProduct FeaturesIdentification
CandidateOpinion ExpressionsExtraction
Relation ExtractionOpinionDatabase
Phrase Dependency Tree
Figure 1: The architecture of our approach.
teria of how to establish dependency relations and
how to distinguish the head and dependent in such
relations is central problem for dependency gram-
mar. Fig. 2(a) shows the dependency represen-
tation of an example sentence. The root of the
sentence is ?enjoyed?. There are seven pairs of
dependency relationships, depicted by seven arcs
from heads to dependents.
2.1.2 Phrase Dependency Parsing
Currently, the mainstream of dependency parsing
is conducted on lexical elements: relations are
built between single words. A major informa-
tion loss of this word level dependency tree com-
pared with constituent tree is that it doesn?t ex-
plicitly provide local structures and syntactic cat-
egories (i.e. NP, VP labels) of phrases (Xia and
Palmer, 2001). On the other hand, dependency
tree provides connections between distant words,
which are useful in extracting long distance rela-
tions. Therefore, compromising between the two,
we extend the dependency tree node with phrases.
That implies a noun phrase ?Cannon SD500 Pow-
erShot? can be a dependent that modifies a verb
phrase head ?really enjoy using? with relation type
?dobj?. The feasibility behind is that a phrase is a
syntactic unit regardless of the length or syntac-
tic category (Santorini and Kroch, 2007), and it is
acceptable to substitute a single word by a phrase
with same syntactic category in a sentence.
Formally, we define the dependency parsing
with phrase nodes as phrase dependency parsing.
A dependency relationship which is an asymmet-
ric binary relationship holds between two phrases.
One is called head, which is the central phrase in
the relation. The other phrase is called dependent,
which modifies the head. A label representing the
1534
enjoyed
We nsubj reallyadvmod using
partmod
SD500
thedet Canon PowerShotnn nn
dobj
enjoyed
nsubj really using
partmod
We 
VP
NP SD500
the
det
Canon PowerShot
nn nn
NP
advmod
dobj
(a)
(c)(b)
  NP SEGMENT:      [We] VP SEGMENT:      [really]      [enjoyed ]      [using] NP SEGMENT:      [the]      [Canon]      [PowerShot]      [SD500]
Figure 2: Example of Phrase Dependency Parsing.
relation type is assigned to each dependency rela-
tionship, such as subj (subject), obj (object), and
so on. Fig.2(c) shows an example of phrase de-
pendency parsing result.
By comparing the phrase dependency tree and
the word level dependency tree in Fig.2, the for-
mer delivers a more succinct tree structure. Local
words in same phrase are compacted into a sin-
gle node. These words provide local syntactic and
semantic effects which enrich the phrase they be-
long to. But they should have limited influences on
the global tree topology, especially in applications
which emphasis the whole tree structures, such as
tree kernels. Pruning away local dependency re-
lations by additional phrase structure information,
phrase dependency parsing accelerates following
processing of opinion relation extraction .
To construct phrase dependency tree, we pro-
pose a method which combines results from an
existing shallow parser and a lexical dependency
parser. A phrase dependency tree is defined as
T = (V ,E ), where V is the set of phrases,
E is the dependency relations among the phrases
in V representing by direct edges. To reserve
the word level dependencies inside a phrase, we
define a nested structure for a phrase T
i
in V :
T
i
= (V
i
, E
i
). V
i
= {v
1
, v
2
, ? ? ? , v
m
} is the inter-
nal words, E
i
is the internal dependency relations.
We conduct the phrase dependency parsing in
this way: traverses word level dependency tree
in preorder (visits root node first, then traverses
the children recursively). When visits a node R,
searches in its children and finds the node set D
which are in the same phrase with R according
Algorithm 1 Pseudo-Code for constructing the
phrase dependency tree
INPUT:
T
?
= (V
?
, E
?
) a word level dependency tree
P = phrases
OUTPUT:
phrase dependency tree T = (V , E ) where
V = {T
1
(V
1
, E
1
), T
2
(V
2
, E
2
), ? ? ? , T
n
(V
n
, E
n
)}
Initialize:
V ? {({v
?
}, {})|v
?
? V
?
}
E ? {(T
i
, T
j
)|(v
?
i
, v
?
j
) ? E
?
, v
?
i
? V
i
, v
?
j
? V
j
}
R = (V
r
, E
r
) root of T
PhraseDPTree(R, P )
1: Find p
i
? P where word[R] ? p
i
2: for each S = (V
s
, E
s
), (R,S) ? E do
3: if word[S] ? p
i
then
4: V
r
? V
r
? v
s
; v
s
? V
s
5: E
r
? E
r
? (v
r
, root[S]); v
r
? V
r
6: V ? V ? S
7: E ? E + (R, l); ?(S, l) ? E
8: E ? E ? (R,S)
9: end if
10: end for
11: for each (R,S) ? E do
12: PhraseDPTree(S,P )
13: end for
14: return (V , E )
to the shallow parsing result. Compacts D and R
into a single node. Then traverses all the remain-
ing children in the same way. The algorithm is
shown in Alg. 1.
The output of the algorithm is still a tree, for we
only cut edges which are compacted into a phrase,
the connectivity is keeped. Note that there will be
inevitable disagrees between shallow parser and
lexical dependency parser, the algorithm implies
that we simply follow the result of the latter one:
the phrases from shallow parser will not appear in
the final result if they cannot be found in the pro-
cedure.
Consider the following example:
?We really enjoyed using the Canon PowerShot SD500.?
Fig.2 shows the procedure of phrase depen-
dency parsing. Fig.2(a) is the result of the lex-
ical dependency parser. Shallow parsers result
is shown in Fig.2(b). Chunk phrases ?NP(We)?,
?VP(really enjoyed using)? and ?NP(the Canon
PowerShot SD500)? are nodes in the output phrase
dependency tree. When visiting node ?enjoyed? in
Fig.2(a), the shallow parser tells that ?really? and
?using? which are children of ?enjoy? are in the
same phrase with their parent, then the three nodes
are packed. The final phrase dependency parsing
tree is shown in the Fig. 2(c).
1535
2.2 Candidate Product Features and Opinion
Expressions Extraction
In this work, we define that product features
are products, product parts, properties of prod-
ucts, properties of parts, company names and re-
lated objects. For example,in consumer elec-
tronic domain, ?Canon PowerShot?, ?image qual-
ity?,?camera?, ?laptop? are all product features.
From analyzing the labeled corpus, we observe
that more than 98% of product features are in a
single phrase, which is either noun phrase (NP) or
verb phrase (VP). Based on it, all NPs and VPs
are selected as candidate product features. While
prepositional phrases (PPs) and adjectival phrases
(ADJPs) are excluded. Although it can cover
nearly all the true product features, the precision
is relatively low. The large amount of noise can-
didates may confuse the relation extraction clas-
sifier. To shrink the size of candidate set, we in-
troduce language model by an intuition that the
more likely a phrase to be a product feature, the
more closely it related to the product review. In
practice, for a certain domain of product reviews,
a language model is build on easily acquired unla-
beled data. Each candidate NP or VP chunk in the
output of shallow parser is scored by the model,
and cut off if its score is less than a threshold.
Opinion expressions are spans of text that ex-
press a comment or attitude of the opinion holder,
which are usually evaluative or subjective phrases.
We also analyze the labeled corpus for opinion ex-
pressions and observe that many opinion expres-
sions are used in multiple domains, which is iden-
tical with the conclusion presented by Kobayashi
et al (2007). They collected 5,550 opinion ex-
pressions from various sources . The coverage of
the dictionary is high in multiple domains. Moti-
vated by those observations, we use a dictionary
which contains 8221 opinion expressions to select
candidates (Wilson et al, 2005b). An assump-
tion we use to filter candidate opinion expressions
is that opinion expressions tend to appear closely
with product features, which is also used to extract
product features by Hu and Liu (2004). In our ex-
periments, the tree distance between product fea-
ture and opinion expression in a relation should be
less than 5 in the phrase dependency parsing tree.
2.3 Relation Extraction
This section describes our method on extracting
relations between opinion expressions and product
features using phrase dependency tree. Manually
built patterns were used in previous works which
have an obvious drawback that those patterns can
hardly cover all possible situations. By taking ad-
vantage of the kernel methods which can search a
feature space much larger than that could be repre-
sented by a feature extraction-based approach, we
define a new tree kernel over phrase dependency
trees and incorporate this kernel within an SVM to
extract relations between opinion expressions and
product features.
The potential relation set consists of the all
combinations between candidate product features
and candidate opinion expressions in a sentence.
Given a phrase dependency parsing tree, we
choose the subtree rooted at the lowest common
parent(LCP) of opinion expression and product
feature to represent the relation.
Dependency tree kernels has been proposed by
(Culotta and Sorensen, 2004). Their kernel is de-
fined on lexical dependency tree by the convolu-
tion of similarities between all possible subtrees.
However, if the convolution containing too many
irrelevant subtrees, over-fitting may occur and de-
creases the performance of the classifier. In phrase
dependency tree, local words in a same phrase are
compacted, therefore it provides a way to treat ?lo-
cal dependencies? and ?global dependencies? dif-
ferently (Fig. 3). As a consequence, these two
kinds of dependencies will not disturb each other
in measuring similarity. Later experiments prove
the validity of this statement.
B
A C
D
E
B
A
C
D E
Phrase Local dependencies
Global dependencies
Figure 3: Example of ?local dependencies? and
?global dependencies?.
We generalize the definition by (Culotta and
Sorensen, 2004) to fit the phrase dependency tree.
Use the symbols in Section 2.1.2, T
i
and T
j
are
two trees with root R
i
and R
j
, K(T
i
,T
j
) is the
kernel function for them. Firstly, each tree node
T
k
? T
i
is augmented with a set of features F ,
and an instance of F for T
k
is F
k
= {f
k
}. A
match function m(T
i
, T
j
) is defined on comparing
a subset of nodes? features M ? F . And in the
same way, a similarity function s(T
i
, T
j
) are de-
1536
fined on S ? F
m(T
i
, T
j
) =
{
1 if f
i
m
= f
j
m
?f
m
? M
0 otherwise
(1)
and
s(T
i
, T
j
) =
?
f
s
?S
C(f
i
s
, f
j
s
) (2)
where
C(f
i
s
, f
j
s
) =
{
1 if f
i
s
= f
j
s
0 otherwise
(3)
For the given phrase dependency parsing trees,
the kernel function K(T
i
,T
j
) is defined as fol-
low:
K(T
i
,T
j
) =
?
?
?
?
?
0 if m(R
i
, R
j
) = 0
s(R
i
, R
j
) +K
in
(R
i
, R
j
)
+K
c
(R
i
.C, R
j
.C) otherwise
(4)
where K
in
(R
i
, R
j
) is a kernel function over
R
i
= (V
i
r
, E
i
r
) and R
j
= (V
j
r
, E
j
r
)?s internal
phrase structures,
K
in
(R
i
, R
j
) = K(R
i
, R
j
) (5)
K
c
is the kernel function over R
i
and R
j
?s chil-
dren. Denote a is a continuous subsequence of in-
dices a, a+1, ? ? ? a+ l(a) for R
i
?s children where
l(a) is its length, a
s
is the s-th element in a. And
likewise b for R
j
.
K
c
(R
i
.C, R
j
.C) =
?
a,b,l(a)=l(b)
?
l(a)
K(R
i
.[a], R
j
.[b])
?
?
s=1..l(a)
m(R
i
.[a
s
], R
j
.[b
s
])
(6)
where the constant 0 < ? < 1 normalizes the ef-
fects of children subsequences? length.
Compared with the definitions in (Culotta and
Sorensen, 2004), we add term K
in
to handle the
internal nodes of a pharse, and make this exten-
sion still satisfy the kernel function requirements
(composition of kernels is still a kernel (Joachims
et al, 2001)). The consideration is that the local
words should have limited effects on whole tree
structures. So the kernel is defined on external
children (K
c
) and internal nodes (K
in
) separately,
Table 1: Statistics for the annotated corpus
Category # Products # Sentences
Cell Phone 2 1100
Diaper 1 375
Digital Camera 4 1470
DVD Player 1 740
MP3 Player 3 3258
as the result, the local words are not involved in
subsequences of external children for K
c
. After
the kernel computing through training instances,
support vector machine (SVM) is used for classi-
fication.
3 Experiments and Results
In this section, we describe the annotated corpus
and experiment configurations including baseline
methods and our results on in-domain and cross-
domain.
3.1 Corpus
We conducted experiments with labeled corpus
which are selected from Hu and Liu (2004), Jin-
dal and Liu (2008) have built. Their documents
are collected from Amazon.com and CNet.com,
where products have a large number of reviews.
They also manually labeled product features and
polarity orientations. Our corpus is selected
from them, which contains customer reviews of
11 products belong to 5 categories(Diaper, Cell
Phone, Digital Camera, DVD Player, and MP3
Player). Table 1 gives the detail statistics.
Since we need to evaluate not only the prod-
uct features but also the opinion expressions and
relations between them, we asked two annotators
to annotate them independently. The annotators
started from identifying product features. Then for
each product feature, they annotated the opinion
expression which has relation with it. Finally, one
annotator A
1
extracted 3595 relations, while the
other annotator A
2
extracted 3745 relations, and
3217 cases of them matched. In order to measure
the annotation quality, we use the following metric
to measure the inter-annotator agreement, which is
also used by Wiebe et al (2005).
agr(a||b) =
|A matches B|
|A|
1537
Table 2: Results for extracting product features
and opinion expressions
P R F
Product Feature 42.8% 85.5% 57.0%
Opinion Expression 52.5% 75.2% 61.8%
Table 3: Features used in SVM-1: o denotes an
opinion expression and t a product feature
1) Positions of o/t in sentence(start, end, other);
2) The distance between o and t (1, 2, 3, 4, other);
3) Whether o and t have direct dependency relation;
4) Whether o precedes t;
5) POS-Tags of o/t.
where agr(a||b) represents the inter-annotator
agreement between annotator a and b, A and B
are the sets of anchors annotated by annotators a
and b. agr(A
1
||A
2
) was 85.9% and agr(A
2
||A
1
)
was 89.5%. It indicates that the reliability of our
annotated corpus is satisfactory.
3.2 Preprocessing Results
Results of extracting product features and opin-
ion expressions are shown in Table 2. We use
precision, recall and F-measure to evaluate perfor-
mances. The candidate product features are ex-
tracted by the method described in Section 2.2,
whose result is in the first row. 6760 of 24414
candidate product features remained after the fil-
tering, which means we cut 72% of irrelevant can-
didates with a cost of 14.5%(1-85.5%) loss in true
answers. Similar to the product feature extraction,
the precision of extracting opinion expression is
relatively low, while the recall is 75.2%. Since
both product features and opinion expressions ex-
tractions are preprocessing steps, recall is more
important.
3.3 Relation Extraction Experiments
3.3.1 Experiments Settings
In order to compare with state-of-the-art results,
we also evaluated the following methods.
1. Adjacent method extracts relations between a
product feature and its nearest opinion expression,
which is also used in (Hu and Liu, 2004).
2. SVM-1. To compare with tree kernel based
Table 4: Features used in SVM-PTree
Features for match function
1) The syntactic category of the tree node
(e.g. NP, VP, PP, ADJP).
2) Whether it is an opinion expression node
3) Whether it is a product future node.
Features for similarity function
1) The syntactic category of the tree node
(e.g. NP, VP, PP, ADJP).
2) POS-Tag of the head word of node?s internal
phrases.
3) The type of phrase dependency edge linking
to node?s parent.
4) Feature 2) for the node?s parent
5) Feature 3) for the node?s parent
approaches, we evaluated an SVM
1
result with a
set of manually selected features(Table 3), which
are also used in (Kobayashi et al, 2007).
3. SVM-2 is designed to compare the effective-
ness of cross-domain performances. The features
used are simple bag of words and POS-Tags be-
tween opinion expressions and product features.
4. SVM-WTree uses head words of opinion ex-
pressions and product features in the word-level
dependency tree, as the previous works in infor-
mation extraction. Then conducts tree kernel pro-
posed by Culotta and Sorensen (2004).
5. SVM-PTree denotes the results of our tree-
kernel based SVM, which is described in the Sec-
tion 2.3. Stanford parser (Klein and Manning,
2002) and Sundance (Riloff and Phillips, 2004)
are used as lexical dependency parser and shallow
parser. The features in match function and simi-
larity function are shown in Table 4.
6. OERight is the result of SVM-PTree with
correct opinion expressions.
7. PFRight is the result of SVM-PTree with
correct product features.
Table 5 shows the performances of different
relation extraction methods with in-domain data.
For each domain, we conducted 5-fold cross val-
idation. Table 6 shows the performances of the
extraction methods on cross-domain data. We use
the digital camera and cell phone domain as train-
ing set. The other domains are used as testing set.
1
libsvm 2.88 is used in our experiments
1538
Table 5: Results of different methods
Cell Phone MP3 Player Digital Camera DVD Player Diaper
Methods P R F P R F P R F P R F P R F
Adjacent 40.3% 60.5% 48.4% 26.5% 59.3% 36.7% 32.7% 59.1% 42.1% 31.8% 68.4% 43.4% 23.4% 78.8% 36.1%
SVM-1 69.5% 42.3% 52.6% 60.7% 30.6% 40.7% 61.4% 32.4% 42.4% 56.0% 27.6% 37.0% 29.3% 14.1% 19.0%
SVM-2 60.7% 19.7% 29.7% 63.6% 23.8% 34.6% 66.9% 23.3% 34.6% 66.7% 13.2% 22.0% 79.2% 22.4% 34.9%
SVM-WTree 52.6% 52.7% 52.6% 46.4% 43.8% 45.1% 49.1% 46.0% 47.5% 35.9% 32.0% 33.8% 36.6% 31.7% 34.0%
SVM-PTree 55.6% 57.2% 56.4% 51.7% 50.7% 51.2% 54.0% 49.9% 51.9% 37.1% 35.4% 36.2% 37.3% 30.5% 33.6%
OERight 66.7% 69.5% 68.1% 65.6% 65.9% 65.7% 64.3% 61.0% 62.6% 59.9% 63.9% 61.8% 55.8% 58.5% 57.1%
PFRight 62.8% 62.1% 62.4% 61.3% 56.8% 59.0% 59.7% 56.2% 57.9% 46.9% 46.6% 46.7% 58.5% 51.3% 53.4%
Table 6: Results for total performance with cross domain training data
Diaper DVD Player MP3 Player
Methods P R F P R F P R F
Adjacent 23.4% 78.8% 36.1% 31.8% 68.4% 43.4% 26.5% 59.3% 36.7%
SVM-1 22.4% 30.6% 25.9% 52.8% 30.9% 39.0% 55.9% 36.8% 44.4%
SVM-2 71.9% 15.1% 25.0% 51.2% 13.2% 21.0% 63.1% 22.0% 32.6%
SVM-WTree 38.7% 52.4% 44.5% 30.7% 59.2% 40.4% 38.1% 47.2% 42.2%
SVM-PTree 37.3% 53.7% 44.0% 59.2% 48.3% 46.3% 43.0% 48.9% 45.8%
3.3.2 Results Discussion
Table 5 presents different methods? results in five
domains. We observe that the three learning based
methods(SVM-1, SVM-WTree, SVM-PTree) per-
form better than the Adjacent baseline in the first
three domains. However, in other domains, di-
rectly adjacent method is better than the learning
based methods. The main difference between the
first three domains and the last two domains is the
size of data(Table 1). It implies that the simple Ad-
jacent method is also competent when the training
set is small.
A further inspection into the result of first 3
domains, we can also conclude that: 1) Tree
kernels(SVM-WTree and SVM-PTree) are better
than Adjacent, SVM-1 and SVM-2 in all domains.
It proofs that the dependency tree is important
in the opinion relation extraction. The reason
for that is a connection between an opinion and
its target can be discovered with various syntac-
tic structures. 2) The kernel defined on phrase
dependency tree (SVM-PTree) outperforms ker-
nel defined on word level dependency tree(SVM-
WTree) by 4.8% in average. We believe the main
reason is that phrase dependency tree provides a
more succinct tree structure, and the separative
treatment of local dependencies and global depen-
dencies in kernel computation can indeed improve
the performance of relation extraction.
To analysis the results of preprocessing steps?
influences on the following relation extraction,
we provide 2 additional experiments which the
product features and opinion expressions are all
correctly extracted respectively: OERight and
PFRight. These two results show that given an
exactly extraction of opinion expression and prod-
uct feature, the results of opinion relation extrac-
tion will be much better. Further, opinion expres-
sions are more influential which naturally means
the opinion expressions are crucial in opinion re-
lation extraction.
For evaluations on cross domain, the Adjacent
method doesn?t need training data, its results are
the same as the in-domain experiments. Note
in Table 3 and Table 4, we don?t use domain
related features in SVM-1, SVM-WTree, SVM-
PTree, but SVM-2?s features are domain depen-
dent. Since the cross-domain training set is larger
than the original one in Diaper and DVD domain,
the models are trained more sufficiently. The fi-
nal results on cross-domain are even better than
in-domain experiments on SVM-1, SVM-WTree,
and SVM-PTree with percentage of 4.6%, 8.6%,
10.3% in average. And the cross-domain train-
ing set is smaller than in-domain in MP3, but
it also achieve competitive performance with the
1539
in-domain. On the other hand, SVM-2?s result
decreased compared with the in-domain experi-
ments because the test domain changed. At the
same time, SVM-PTree outperforms other meth-
ods which is similar in in-domain experiments.
4 Related Work
Opinion mining has recently received consider-
able attention. Amount of works have been
done on sentimental classification in different lev-
els (Zhang et al, 2009; Somasundaran et al, 2008;
Pang et al, 2002; Dave et al, 2003; Kim and
Hovy, 2004; Takamura et al, 2005). While we
focus on extracting product features, opinion ex-
pressions and mining relations in this paper.
Kobayashi et al (2007) presented their work on
extracting opinion units including: opinion holder,
subject, aspect and evaluation. Subject and aspect
belong to product features, while evaluation is the
opinion expression in our work. They converted
the task to two kinds of relation extraction tasks
and proposed a machine learning-based method
which combines contextual clues and statistical
clues. Their experimental results showed that the
model using contextual clues improved the perfor-
mance. However since the contextual information
in a domain is specific, the model got by their ap-
proach can not easily converted to other domains.
Choi et al (2006) used an integer linear pro-
gramming approach to jointly extract entities and
relations in the context of opinion oriented infor-
mation extraction. They identified expressions of
opinions, sources of opinions and the linking re-
lation that exists between them. The sources of
opinions denote to the person or entity that holds
the opinion.
Another area related to our work is opinion
expressions identification (Wilson et al, 2005a;
Breck et al, 2007). They worked on identify-
ing the words and phrases that express opinions
in text. According to Wiebe et al (2005), there are
two types of opinion expressions, direct subjective
expressions and expressive subjective elements.
5 Conclusions
In this paper, we described our work on min-
ing opinions from unstructured documents. We
focused on extracting relations between product
features and opinion expressions. The novelties
of our work included: 1) we defined the phrase
dependency parsing and proposed an approach
to construct the phrase dependency trees; 2) we
proposed a new tree kernel function to model
the phrase dependency trees. Experimental re-
sults show that our approach improved the perfor-
mances of the mining task.
6 Acknowledgement
This work was (partially) funded by Chinese
NSF 60673038, Doctoral Fund of Ministry of
Education of China 200802460066, and Shang-
hai Science and Technology Development Funds
08511500302. The authors would like to thank the
reviewers for their useful comments.
References
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In Pro-
ceedings of IJCAI-2007.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction pat-
terns. In Proceedings of HLT/EMNLP.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings EMNLP.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In In Proceed-
ings of ACL 2004.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In
Proceedings of WWW 2003.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the ACM
SIGKDD 2004.
Nitin Jindal and Bing Liu. 2008. Opinion spam and
analysis. In Proceedings of WSDM ?08.
Thorsten Joachims, Nello Cristianini, and John Shawe-
Taylor. 2001. Composite kernels for hypertext cate-
gorisation. In Proceedings of ICML ?01.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of Coling
2004. COLING.
Dan Klein and Christopher D. Manning. 2002. Fast
exact inference with a factored model for natural
language parsing. In In Advances in Neural Infor-
mation Processing Systems.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of
relations in opinion mining. In Proceedings of
EMNLP-CoNLL 2007.
1540
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proc. of EMNLP
2002.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of HLT/EMNLP.
E. Riloff and W. Phillips. 2004. An introduction to
the sundance and autoslog systems. In University of
Utah School of Computing Technical Report UUCS-
04-015.
Beatrice Santorini and Anthony Kroch. 2007.
The syntax of natural language: An on-
line introduction using the Trees program.
http://www.ling.upenn.edu/ beatrice/syntax-
textbook.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpreta-
tion. In Proceedings of COLING 2008.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In Proceedings of ACL?05.
L. Tesni`ere. 1959. El?ements de syntaxe structurale.
Editions Klincksieck.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2/3).
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005a. Opinionfinder: A system for subjectiv-
ity analysis. In Demonstration Description in Con-
ference on Empirical Methods in Natural Language
Processing.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of HLT-
EMNLP.
Fei Xia and Martha Palmer. 2001. Converting depen-
dency structures to phrase structures. In HLT ?01:
Proceedings of the first international conference on
Human language technology research.
Qi Zhang, Yuanbin Wu, Tao Li, Mitsunori Ogihara,
Joseph Johnson, and Xuanjing Huang. 2009. Min-
ing product reviews based on shallow dependency
parsing. In Proceedings of SIGIR 2009.
1541
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 604?611, Vancouver, October 2005. c?2005 Association for Computational Linguistics
  
The Use of Metadata, Web-derived Answer Patterns and Passage 
Context to Improve Reading Comprehension Performance
Yongping Du 
Media Computing and Web 
Intelligence Laboratory 
Fudan University 
Shanghai, China 
ypdu@fudan.edu.cn 
 
 
Helen Meng 
Human-Computer 
Communication Laboratory 
The Chinese University of 
Hong Kong 
 HongKong. SAR. China 
hmmeng@se.cuhk.edu.hk 
 
Xuanjing Huang 
Media Computing and Web 
Intelligence Laboratory 
Fudan University 
Shanghai, China 
xjhuang@fudan.edu.cn 
 
 
Lide Wu 
Media Computing and Web 
Intelligence Laboratory 
Fudan University 
Shanghai, China 
ldwu@fudan.edu.cn 
Abstract 
A reading comprehension (RC) system 
attempts to understand a document and returns 
an answer sentence when posed with a 
question.  RC resembles the ad hoc question 
answering (QA) task that aims to extract an 
answer from a collection of documents when 
posed with a question.  However, since RC 
focuses only on a single document, the system 
needs to draw upon external knowledge 
sources to achieve deep analysis of passage 
sentences for answer sentence extraction.  
This paper proposes an approach towards RC 
that attempts to utilize external knowledge to 
improve performance beyond the baseline set 
by the bag-of-words (BOW) approach.  Our 
approach emphasizes matching of metadata 
(i.e. verbs, named entities and base noun 
phrases) in passage context utilization and 
answer sentence extraction. We have also 
devised an automatic acquisition process for 
Web-derived answer patterns (AP) which 
utilizes question-answer pairs from TREC QA, 
the Google search engine and the Web.  This 
approach gave improved RC performances for 
both the Remedia and ChungHwa corpora, 
attaining HumSent accuracies of 42% and 
69% respectively.  In particular, performance 
analysis based on Remedia shows that relative 
performances of 20.7% is due to metadata 
matching and a further 10.9% is due to the 
application of Web-derived answer patterns. 
1. Introduction 
A reading comprehension (RC) system attempts to 
understand a document and returns an answer 
sentence when posed with a question.  The RC 
task was first proposed by the MITRE 
Corporation which developed the Deep Read 
reading comprehension system (Hirschman et al, 
1999).  Deep Read was evaluated on the Remedia 
Corpus that contains a set of stories, each with an 
average of 20 sentences and five questions (of 
types who, where, when, what and why). The 
MITRE group also defined the HumSent scoring 
metric, i.e. the percentage of test questions for 
which the system has chosen a correct sentence as 
the answer.  HumSent answers were compiled by a 
human annotator, who examined the stories and 
chose the sentence(s) that best answered the 
questions.  It was judged that for 11% of the 
Remedia test questions, there is no single sentence 
in the story that is judged to be an appropriate 
answer sentence.  Hence the upper bound for RC 
on Remedia should by 89% HumSent accuracy.  
(Hirschman et al 1999) reported a HumSent 
accuracy of 36.6% on the Remedia test set.  
Subsequently, (Ng et al, 2000) used a machine 
learning approach of decision tree and achieved 
the accuracy of 39.3%.   Then (Riloff and Thelen, 
2000) and (Charniak et al, 2000) reported 
improvements to 39.7% and 41%, respectively.  
They made use of handcrafted heuristics such as 
the WHEN rule: 
if contain(S, TIME), then Score(S)+=4 
i.e. WHEN questions reward candidate answer 
sentences with four extra points if they contain a 
name entity TIME.  
RC resembles the ad hoc question answering 
(QA) task in TREC.1  The QA task finds answers 
to a set of questions from a collection of 
documents, while RC focuses on a single 
                                                                 
1 http://www.nist.gov. 
604
  
document.  (Light et al 1998) conducted a 
detailed compared between the two tasks.  They 
found that the answers of most questions in the 
TREC QA task appear more than once within the 
document collection.  However, over 80% of the 
questions in the Remedia corpus correspond to 
answer sentences that have a single occurrence 
only.  Therefore an RC system often has only one 
shot at finding the answer. The system is in dire 
need of extensive knowledge sources to help with 
deep text analysis in order to find the correct 
answer sentence.   
Recently, many QA systems have exploited 
the Web as a gigantic data repository in order to 
help question answering (Clarke et al, 2001; 
Kwok et al, 2001; Radev et al, 2002).  Our 
current work attempts to incorporate a similar idea 
in exploiting Web-derived knowledge to aid RC.  
In particular, we have devised an automatic 
acquisition process for Web-derived answer 
patterns. Additionally we propose to emphasize 
the importance of metadata matching in our 
approach to RC.  By metadata, we are referring to 
automatically labeled verbs, named entities as well 
as base noun phrases in the passage.  It is 
important to achieve a metadata match between 
the question and a candidate answer sentence 
before the candidate is selected as the final answer.  
The candidate answer sentence may be one with a 
high degree of word overlap with the posed 
question, or it may come from other sentences in 
the neighboring context. We apply these different 
techniques step by step and obtain better results 
than have ever previously been reported. 
Especially, we give experiment analysis for 
understanding the results. 
    In the rest of this paper, we will first describe 
three main aspects of our approach towards RC ? 
(i) metadata matching, (ii)automatic acquisition of 
Web-derived answer patterns and (iii) the use of 
passage context.  This will be followed by a 
description of our experiments, analysis of results 
and conclusions. 
2. Metadata Matching 
A popular approach in reading comprehension is 
to represent the information content of each 
question or passage sentence as a bag of words 
(BOW).  This approach incorporates stopword 
removal and stemming.  Thereafter, two words are 
considered a match if they share the same 
morphological root.  Given a question, the BOW 
approach selects the passage sentence with the 
maximum number of matching words as the 
answer.  However, the BOW approach does not 
capture the fact that the informativeness of a word 
about a passage sentence varies from one word to 
another.  For example, it has been pointed out by 
(Charniak et al 2000) that the verb seems to be 
especially important for recognizing that a passage 
sentence is related to a specific question.  In view 
of this, we propose a representation for questions 
and answer sentences that emphasizes three types 
of metadata:  
(i) Main Verbs (MVerb), identified by the link 
parser (Sleator and Temperley 1993);  
(ii) Named Entities (NE), including names of 
locations (LCN), persons (PRN) and organizations 
(ORG), identified by a home-grown named entity 
identification tool; and  
(iii) Base Noun Phrases (BNP), identified by a 
home-grown base noun phrase parser respectively. 
We attempt to quantify the relative importance 
of such metadata through corpus statistics 
obtained only from the training set of the Remedia 
corpus, which has 55 stories. The Remedia test set, 
which contains 60 stories, is set aside for 
evaluation. On average, each training story has 20 
sentences and five questions. There are 274 
questions in all in the entire training set.  Each 
question corresponds to a marked answer sentence 
within the story text.  We analyzed all the 
questions and divided them into three question 
sets (Q_SETS) based on the occurrences of 
MVerb, NE and BNP identified with the tools 
mentioned above.  The following are illustrative 
examples of the Q_SETS as well as their sizes: 
Q_SETMverb  
(Count:169) 
Who helped the Pilgrims? 
Q_SETNE    
 (Count:62) 
When was the first merry-go-
round built in the United States? 
Q_SETBNP   
(Count:232) 
Where are the northern lights? 
Table 1.  Examples and sizes of question sets (Q_SETS) 
with different metadata ? main  verb (MVerb), named 
entity (NE) and base noun phrase (BNP). 
   It may also occur that a question belongs to 
multiple Q_SETS.  For example:  
605
  
Q_SETMVerb 
 
When was the first merry-go-round built 
in the United States? 
Q_SETNE 
 
When was the first merry-go-round built 
in the United States? 
Q_SETBNP 
 
When was the first merry-go-round built 
in the United States? 
Table 2.  An example sentence that belongs to multiple 
Q_SETS. 
As mentioned earlier, each question 
corresponds to an answer sentence, which is 
annotated in the story text by MITRE.  Hence we 
can follow the Q_SETS to divide the answer 
sentences into three answer sets (A_SETS).  
Examples of A_SETS that correspond to Table 1 
include: 
A_SETMVerb 
 
An Indian named Squanto came 
to help them. 
A_SETNE 
 
The first merry-go-round in the 
United States was built in 1799.
A_SETBNP 
 
Then these specks reach the air 
high above the earth. 
Table 3.  Examples of the answer sets (A_SETS) 
corresponding to the different metadata categories, 
namely, main verb (MVerb), named entity (NE) and 
base noun phrase) (BNP). 
    In order to quantify the relative importance of 
matching the three kinds of metadata between 
Q_SET and A_SET for reading comprehension, 
we compute the following relative weights based 
on corpus statistics: 
|_|
||
Metadata
Metadata
Metadata SETA
SWeight =  ?..Eqn (1) 
where SMetadata is the set of answer sentences in 
|A_SETMetadata| that contain the metadata of its 
corresponding question.  For example, referring to 
Tables 2 and 3, the question in Q_SETNE ?When 
was the first merry-go-round built in the United 
Sates?? contains the named entity (underlined) 
which is also found in the associated answer 
sentence from A_SETNE, ?The first merry-go-
round in the United States was built in 1799.?  
Hence this answer sentence belongs to the set SNE.   
Contrarily, the question in Q_SETBNP ?Where are 
the northern lights?? contains the base noun 
phrase (underlined) but it is not found in the 
associated answer sentence from A_SETBNP, 
?Then these specks reach the air high above the 
earth.?  Hence this answer sentence does not 
belong to the set SBNP.  Based on the three sets, we 
obtain the metadata weights: 
WeightMVerb=0.64, WeightNE=0.38, WeightBNP=0.21 
To illustrate how these metadata weights are 
utilized in the RC task, consider again the 
question, ?Who helped the Pilgrims?? together 
with three candidate answers that are ?equally 
good? with a single word match when the BOW 
approach is applied.  We further search for 
matching metadata among these candidate 
answers and use the metadata weights for scoring.   
Question Who helped the Pilgrims? 
MVerb identified: ?help? 
BNP identified: ?the Pilgrams? 
Candidate 
Sentence 1 
 
An Indian named Squanto came to help. 
Matched MVerb (underlined) 
Score= WeightMVerb=0.64 
Candidate 
Sentence 2 
 
By fall, the Pilgrims had enough food for 
the winter. 
Matched BNP (underlined) 
Score= WeightBNP=0.21 
Candidate 
Sentence 3 
 
Then the Pilgrims and the Indians ate and 
played games. 
Matched BNP (underlined) 
Score= WeightBNP=0.21 
Table 4.  The use of metadata matching to extend the 
bag-of-words approach in reading comprehension.  
3. Web-derived Answer Patterns 
In addition to using metadata for RC, the proposed 
approach also leverages knowledge sources that 
are external to the core RC resources ? primarily 
the Web and other available corpora.  This section 
describes our approach that attempts to 
automatically derive answer patterns from the 
Web as well as score useful answer patterns to aid 
RC.  We utilize the open domain question-answer 
pairs (2393 in all) from the Question Answering 
track of TREC (TREC8-TREC12) as a basis for 
automatic answer pattern acquisition.   
3.1 Deriving Question Patterns 
We define a set of question tags (Q_TAGS) that 
extend the metadata above in order to represent 
question patterns.  The tags include one for main 
verbs (Q_MVerb), three for named entities 
(Q_LCN, Q_PRN and Q_ORG) and one for base 
noun phrases (Q_BNP). We are also careful to 
ensure that noun phrases tagged as named entities 
are not further tagged as base noun phrases. 
606
  
    A question pattern is expressed in terms of 
Q_TAGS.  A question pattern can be used to 
represent multiple questions in the TREC QA 
resource.  An example is shown in Table 5.  
Tagging the TREC QA resource provides us with 
a set of question patterns {QPi} and for each 
pattern, up to mi example questions. 
Question Pattern (QPi): 
When do Q_PRN Q_MVerb Q_BNP? 
Represented questions: 
Q1: When did Alexander Graham Bell invent the 
telephone? 
Q2: When did Maytag make Magic Chef 
refrigerators? 
Q3: When did Amumdsen reach the South Pole? 
(mi example questions in all) 
Table 5.  A question pattern and some example 
questions that it represents. 
3.2  Deriving Answer Patterns 
For each question pattern, we aim to derive 
answer patterns for it automatically from the Web. 
The set of answer patterns capture possible ways 
of embedding a specific answer in an answer 
sentence.  We will describe the algorithm for 
deriving answer patterns as following and 
illustrate with the following question answer pair 
from TREC QA:  
Q: When did Alexander Graham Bell invent the 
telephone? 
A: 1876 
1. Formulate the Web Query 
The question is tagged and the Web query is 
formulated as ?Q_TAG?+ ?ANSWER?, i.e. 
Question: ?When did Alexander Graham Bell 
invent the telephone?? 
QP:            When do Q_PRN Q_MVerb Q_BNP ? 
where Q_PRN= ?Alexander Graham Bell?, 
Q_MVerb= ?invent?, and  Q_BNP=  ?the 
telephone? 
hence Web query:  ?Alexander Graham Bell?+ 
?invent? + ?the telephone? + ?1876? 
2. Web Search and Snippet Selection 
The Web query is submitted to the search 
engine Google using the GoogleAPI and the top 
100 snippets are downloaded.  From each 
snippet, we select up to ten contiguous words to 
the left as well as to the right of the ?ANSWER? 
for answer pattern extraction.  The selected 
words must be continuous and do not cross the 
snippet boundary that Google denotes with ???. 
3. Answer Pattern Selection 
We label the terms in each selected snippet with 
the Q_TAGs from the question as well as the 
answer tag <A>.  The shortest string containing 
all these tags (underlined below) is extracted as 
the answer pattern (AP).  For example:  
Snippet 1: 1876, Alexander Graham Bell 
invented the telephone in the United States? 
AP 1:   <A>, Q_PRN Q_MVerb Q_BNP. 
(N.B.  The answer tag <A> denotes ?1876? in this 
example). 
Snippet 2: ?which has been invented by 
Alexander Graham Bell in 1876? 
AP 2:    Q_MVerb by Q_PRN in <A>. 
    As may be seen in above, the acquisition 
algorithm for Web-derived answer questions calls 
for specific answers, such as a factoid in a word or 
phrase.  Hence the question-answer pairs from 
TREC QA are suitable for use.  On the other hand, 
Remedia is less suitable here because it contains 
labelled answer sentences instead of factoids.  
Inclusion of whole answer sentences in Web 
query formulation generally does not return the 
answer pattern that we seek in this work. 
3.3 Scoring the Acquired Answer Patterns 
The answer pattern acquisition algorithm returns 
multiple answer patterns for every question-
answer pair submitted to the Web.   In this 
subsection we present an algorithm for deriving 
scores for these answer patterns.  The 
methodology is motivated by the concept of 
confidence level, similar to that used in data 
mining.  The algorithm is as follows: 
1. Formulate the Web Query 
For each question pattern QPi (see Table 5) 
obtained previously, randomly select an example 
question among the mi options that belongs to this 
pattern.  The question is tagged and the Web 
query is formulated in terms of the Q_TAGs only.  
(Please note that the corresponding answer is 
excluded from Web query formulation here, 
which differs from the answer pattern acquisition 
algorithm).  E.g., 
Question: ?When did Alexander Graham Bell 
invent the telephone? 
Q_TAGs: Q_PRN Q_MVerb Q_BNP 
Web query:  ?Alexander Graham Bell?+ 
?invent? + ?the telephone? 
2.   Web Search and Snippet Selection 
The Web query is submitted to the search engine 
607
  
Google and the top 100 snippets are downloaded. 
3.   Scoring each Answer Pattern APij relating to 
QPi 
Based on the question, its pattern QPi, the answer 
and the retrieved snippets, totally the following 
counts for each answer pattern APij relating to 
QPi . 
cij ? # snippets matching APij and for which the 
tag <A> matches the correct answer. 
nij ? #  snippets matching APij and for which the 
tag <A> matches any term 
Compute the ratio rij= cij / nij..........Eqn(2) 
Repeat steps 1-3 above for another example 
question randomly selected from the pool of mi 
example under QPi.  We arbitrarily set the 
maximum number of iterations to be ki = ??
???
?
im3
2  
in order to achieve decent coverage of the 
available examples.  The confidence for APij.is 
computed as          
k
r
APConfidence
k
i
ij
ij
?
== 1)( ??Eqn(3) 
Equation (3) tries to assign high confidence 
values to answer patterns APij that choose the 
correct answers, while other answer patterns are 
assigned low confidence values.  E.g.: 
<A>, Q_PRN Q_MVerb Q_BNP     (Confidence=0.8) 
Q_MVerb by Q_PRN in <A>.         (Confidence=0.76) 
3.4 Answer Pattern Matching in RC 
The Web-derived answer patterns are used in the 
RC task.  Based on the question and its QP, we 
select the related AP to match among the answer 
sentence candidates.  The candidate that matches 
the highest-scoring AP will be selected.  We find 
that this technique is very effective for RC as it 
can discriminate among candidate answer 
sentences that are rated ?equally good? by the 
BOW or metadata matching approaches, e.g.: 
Q:   When is the Chinese New Year? 
QP: When is the Q_BNP? 
where Q_BNP=Chinese New Year 
Related AP:  Q_BNP is <A> (Confidence=0.82) 
Candidate answer sentences 1: you must wait a few more 
weeks for the Chinese New Year. 
Candidate answer sentences 2: Chinese New Year is most 
often between January 20 and February 20. 
Both candidate answer sentences have the same 
number of matching terms ? ?Chinese?, ?New? 
and ?Year? and the same metadata, i.e. 
Q_BNP=Chinese New Year. The term ?is? is 
excluded by stopword removal. However the 
Web-derived answer pattern is able to select the 
second candidate as the correct answer sentence. 
Hence our system gives high priority to the 
Web-derived AP ? if a candidate answer sentence 
can match an answer pattern with confidence > 
0.6, the candidate is taken as the final answer.  No 
further knowledge constraints will be enforced. 
4. Context Assistance 
During RC, the initial application of the BOW 
approach focuses the system?s attention on a small 
set of answer sentence candidates.  However, it 
may occur the true answer sentence is not 
contained in this set.  As was observed by (Riloff 
and Thelen, 2000) and (Charniak et al, 2000), the 
correct answer sentence often precedes/follows the 
sentence with the highest number of matching 
words.  Hence both the preceding and following 
context sentences are searched in their work to 
find the answer sentence especially for why 
questions. 
Our proposed approach references this idea in 
leveraging contextual knowledge for RC.  
Incorporation of contextual knowledge is very 
effective when used in conjunction with named 
entity (NE) identification.  For instance, who 
questions should be answered with words tagged 
with Q_PRN (for persons).  If the candidate 
sentence with the highest number of matching 
words does not contain the appropriate NE, it will 
not be selected as the answer sentence.  Instead, 
our system searches among the two preceding and 
two following context sentences for the 
appropriate NE.  Table 6 offers an illustration. 
Data analysis Remedia training set shows that the 
context window size selected is appropriate for 
when, who and where questions.   
Football Catches On Fast 
(LATROBE, PA., September 4, 1895) - The new 
game of football is catching on fast, and each month new 
teams are being formed. 
Last night was the first time that a football player was 
paid.  The man's name is John Brallier, and he was paid 
$10 to take the place of someone who was hurt.? 
Question: Who was the first football player to be paid? 
Sentence with maximum # matching words: Last night 
was the first time that a football player was paid. 
Correct answer sentence: The man's name is John 
Brallier, and he was paid $10 to take the place of 
someone who was hurt. 
Table 6.  An example illustrating the use of contextual 
knowledge in RC. 
608
  
As for why questions, a candidate answer 
sentence is selected from the context window if its 
first word is one of ?this?, ?that?, ?these?, 
?those?, ?so? or ?because?.  We did not utilize 
contextual constraints for what questions. 
5. Experiments 
RC experiments are run on the Remedia corpus as 
well as the ChungHwa corpus.  The Remedia 
training set has 55 stories, each with about five 
questions.  The Remedia test set has 60 stories and 
5 questions per story.  The ChungHwa corpus is 
derived from the book, ?English Reading 
Comprehension in 100 days,? published by 
Chung Hwa Book Co., (H.K.) Ltd.  The 
ChungHwa training set includes 100 English 
stories and each has four questions on average.  
The ChungHwa testing set includes 50 stories and 
their questions.  We use HumSent as the prime 
evaluation metric for reading comprehension.   
The three kinds of knowledge sources are used 
incrementally in our experimental setup and 
results are labeled as follows: 
Result Technique 
Result_1 BOW 
Result_2 BOW+MD 
Result_3 BOW+MD+AP 
Result_4 BOW+MD+AP+Context 
Table 7.  Experimental setup in RC evaluations.  
Abbrievations are: bag-of-words (BOW), metadata 
(MD), Web-derived answer patterns (AP), contextual 
knowledge (Context). 
5.1 Results on Remedia 
Table 8 shows the RC results for various question 
types in the Remedia test set.  
 When Who What Where Why 
Result_1 32.0% 30.0% 31.8% 29.6% 18.6%
Result_2 40.0% 28.0% 39.0% 38.0% 20.0%
Result_3 52.6% 42.8% 40.6% 38.4% 21.0%
Result_4 55.0% 48.0% 40.6% 36.4% 27.6%
 Table 8.  HumSent accuracies for the Remedia test set. 
We observe that the HumSent accuracies vary 
substantially across different interrogatives. The 
system performs best for when questions and 
worst for why questions. The use of Web-derived 
answer patterns brought improvements to all the 
different interrogatives.  The other knowledge 
sources, namely, meta data and context, bring 
improvements for some question types but 
degraded others.  
Figure 1 shows the overall RC results of our 
system.  The relative incremental gains due to the 
use of metadata, Web-derived answer patterns and 
context are 20.7%, 10.9% and 8.2% respectively.  
We also ran pairwise t-tests to test the statistical 
significance of these improvements and results are 
shown in Table 9.  The improvements due to 
metadata matching and Web-derived answer 
patterns are statistically significant (p<0.05) but 
the improvement due to context is not. 
29%
35%
38.80% 42%
0%
5%
10%
15%
20%
25%
30%
35%
40%
45%
Result_1 Result_2 Result_3 Result_4
H
um
Se
nt
 P
re
ci
sio
n
 
Figure 1.  HumSent accuracies for Remedia. 
Pairwise 
Comparison 
Result_1 & 
Result_2 
Result_2 & 
Result_3 
Result_3 & 
Result_4 
t-test Results t(4)=2.207, 
p=0.046 
t(4)=2.168, 
p=0.048 
t(4)=1.5, 
p=0.104 
Table 9.  Tests of statistical significance in the 
incremental improvements over BOW among the use 
of metadata, Web-derived answer patterns and context.   
We also compared our results across various 
interrogatives with those previously reported in 
(Riloff and Thelen, 2000).  Their system is based 
on handcrafted rules with deterministic algorithms.  
The comparison (see Table 10) shows that our 
approach which is based on data-driven patterns 
and statistics can achieve comparable performance. 
Question Type Riloff &Thelen 2000 Result_4 
When 55% 55.0% 
Who 41% 48.0% 
What 28% 40.6% 
Where 47% 36.4% 
Why 28% 27.6% 
Overall 40% 42.0% 
Table 10.  Comparison of HumSent results with a 
heuristic based RC system (Riloff & Thelen 00).  
5.2 Results on ChungHwa 
Experimental results for the ChungHwa corpus are 
presented in Figure 2.  The HumSent accuracies 
obtained are generally higher than those with 
609
  
Remedia.  We observe similar trends as before, i.e. 
our approach in the use of metadata, Web-derived 
answer patterns and context bring incremental 
gains to RC performance.  However, the actual 
gain levels are much reduced. 
65%
66%
68%
69%
63%
64%
65%
66%
67%
68%
69%
70%
Result_1 Result_2 Result_3 Result_4
H
um
Se
nt
 P
re
ci
sio
n
 
Figure 2.  HumSent accuracies for ChungHwa. 
5.3. Analyses of Results 
In order to understand the underlying reason for 
reduced performance gains as we migrated from 
Remedia to Chunghwa, we analyzed the question 
lengths as well as the degree of word match 
between questions and answers among the two 
corpora.  Figure 3 shows that the average length 
of questions in Chunghwa are longer than 
Remedia.  Longer questions contain more 
information which is beneficial to the BOW 
approach in finding the correct answer. 
32.6
7.5
32.5
60
13.3
54.1
0
10
20
30
40
50
60
70
?4 5,6,7 ?8
Question Length
Pe
rc
en
t o
f Q
ue
st
io
ns
 (%
)
Remedia ChungHwa
 
Figure 3.  Distribution of question lengths among the 
Remedia and ChungHwa corpora. 
The degree of word match between questions 
and answers among the two corpora is depicted in 
Figure 4.  We observe that ChungHwa has a larger 
proportion of questions that have a match- size (i.e. 
number of matching words between a question 
and its answer) larger than 2.  This presents an 
advantage for the BOW approach in RC.  It is also 
observed that approximately 10% of the Remedia 
questions have no correct answers (i.e. match-
size=-1) and about 25% have no matching words 
with the correct answer sentence.  This explains 
the overall discrepancies in HumSent accuracies 
between Remedia and ChungHwa. 
0
5
10
15
20
25
30
35
-1 0 1 2 3 4 5 ?6
Match Size
Pe
rc
en
t o
f Q
ue
st
io
ns
 (%
)
Remedia ChungHwa
 
Figure 4.  Distribution of match-sizes (i.e. the number 
of matching words between questions and their 
answers) in the two corpora. 
While our approach has leveraged a variety of 
knowledge sources in RC, we still observe that 
our system is unable to correctly answer 58% of 
the questions in Remedia. An example of such 
elusive questions is:  
Question: When do the French celebrate their 
freedom? 
Answer Sentence: To the French, July 14 has the 
same meaning as July 4th does to the United 
States.  
6. Conclusions 
A reading comprehension (RC) system aims to 
understand a single document (i.e. story or passage) 
in order to be able to automatically answer questions 
about it.   The task presents an information retrieval 
paradigm that differs significantly from that found in 
Web search engines.  RC resembles the question 
answering (QA) task in TREC which returns an 
answer for a given question from a collection of 
documents.  However, while a QA system can 
utilize the knowledge and information in a collection 
of documents, RC systems focuses only on a single 
document only.  Consequently there is a dire need to 
draw upon a variety of knowledge sources to aid 
deep analysis of the document for answer generation.  
This paper presents our initial effort in designing an 
approach for RC that leverages a variety of 
knowledge sources beyond the context of the 
passage, in an attempt to improve RC performance 
beyond the baseline set by the bag-of-words (BOW) 
approach.  The knowledge sources include the use of 
metadata (i.e. verbs, named entities and base noun 
phrases).  Metadata matching is applied in our 
approach in answer sentence extraction as well as 
use of contextual sentences.  We also devised an 
610
  
automatic acquisition algorithm for Web-derived 
answer patterns.  The acquisition process utilizes 
question-answer pairs from TREC QA, the Google 
search engine and the Web.  These answer patterns 
capture important structures for answer sentence 
extraction in RC.  The use of metadata matching and 
Web-derived answer patterns improved reading 
comprehension performances for the both Remedia 
and ChungHwa corpora. We obtain improvements 
over previously reported results for Remedia, with 
an overall HumSet accuracy of 42%.  In particular, a 
relative gain of 20.7% is due to metadata matching 
and a further 10.9% is due to application of Web-
derived answer patterns. 
Acknowledgement 
This work is partially supported by the Direct 
Grant from The Chinese University of Hong Kong 
(CUHK) and conducted while the first author was 
visiting CUHK. This work is supported by Natural 
Science Foundation of China under Grant 
No.60435020. 
References 
Charles L.A. Clarke, Gordon V. Cormack, Thomas R. 
Lynam. 2001. Exploiting Redundancy in Question 
Answering. In Proceedings of the 24th ACM 
Conference on Research and Development in 
Information Retrieval (SIGIR-2001, New Orleans, 
LA). ACM Press. New York, 358?365. 
Cody C. T. Kwok, Oren Etzioni, Daniel S. Weld. 2001. 
Scaling Question Answering to the Web. In 
Proceedings of the 10th World Wide Web 
Conference (WWW?2001). 150-161. 
Daniel Sleator and Davy Temperley. 1993. Parsing 
English with a Link Grammar. Third International 
Workshop on Parsing Technologies. 
Deepak Ravichandran and Eduard Hovy. 2002. 
Learning Surface Text Patterns for a Question 
Answering System. In Proceedings of the 40th 
Annual Meeting of the Association for 
Computational Linguistics (ACL-2002). 41-47.  
Dell Zhang, Wee Sun Lee. 2002. Web Based Pattern 
Mining and Matching Approach to Question 
Answering. In Proceedings of the TREC-11 
Conference. 2002. NIST, Gaithersburg, MD, 505- 
512. 
Dragomir Radev, Weiguo Fan, Hong Qi, Harris Wu, 
Amardeep Grewal. 2002. Probabilistic Question 
Answering on the Web. In Proceedings of the 11th 
World Wide Web Conference (WWW?2002). 
Ellen Riloff and Michael Thelen. 2000. A Rule-based 
Question Answering System for Reading 
Comprehension Test. ANLP/NAACL-2000 
Workshop on Reading Comprehension Tests as 
Evaluation for Computer-Based Language 
Understanding Systems. 
Eugene Charniak, Yasemin Altun, Rodrigo de Salvo 
Braz, Benjamin Garrett, Margaret Kosmala, Tomer 
Moscovich, Lixin Pang, Changhee Pyo, Ye Sun, 
Wei Wy, Zhongfa Yang, Shawn Zeller, and Lisa 
Zorn. 2000. Reading Comprehension Programs in a 
Statistical-Language-Processing Class. ANLP-
NAACL 2000 Workshop: Reading Comprehension 
Tests as Evaluation for Computer-Based Language 
Understanding Systems. 
Hwee Tou Ng, Leong Hwee Teo, Jennifer Lai Pheng 
Kwan. 2000. A Machine Learning Approach to 
Answering Questions for Reading Comprehension 
Tests.  Proceedings of the 2000 Joint SIGDAT 
Conference on Empirical Methods in Natural 
Language Processing and Very Large Corpora 2000. 
Lynette Hirschman, Marc Light, Eric Breck, and John 
Burger. 1999. Deep Read: A Reading 
Comprehension System. Proceedings of the 37th 
Annual Meeting of the Association for 
Computational Linguistics. 
Marc Light, Gideon S. Mann, Ellen Riloff and Eric 
Break. 1998. Analyses for Elucidating Current 
Question Answering Technology.  Natural 
Language Engineering. Vol. 7, No. 4.  
Martin M. Soubbotin, Sergei M. Soubbotin. 2002. Use 
of Patterns for Detection of Likely Answer Strings: 
A Systematic Approach. In Proceedings of the 
TREC-11 Conference. 2002. NIST, Gaithersburg, 
MD, 134-143. 
611
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 498 ? 506, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Answering Definition Questions Using  
Web Knowledge Bases 
Zhushuo Zhang, Yaqian Zhou, Xuanjing Huang, and Lide Wu 
Department of Computer Science and Engineering, Fudan University, Shanghai, China, 200433 
{zs_zhang, zhouyaqian, xjhuang, ldwu}@fudan.edu.cn 
Abstract. This paper presents a definition question answering approach, which 
is capable of mining textual definitions from large collections of documents. In 
order to automatically identify definition sentences from a large collection of 
documents, we utilize the existing definitions in the Web knowledge bases in-
stead of hand-crafted rules or annotated corpus. Effective methods are adopted to 
make full use of Web knowledge bases, and they promise high quality response 
to definition questions. We applied our system in the TREC 2004 definition 
question-answering task and achieved an encouraging performance with the F-
measure score of 0.404, which was ranked second among all the submitted runs. 
1   Introduction 
When people want to learn an unknown concept from a large collection of documents, 
the most commonly used tools are the search engines. They submit a query to a search 
engine system, and the search engine returns a number of pages related to the query 
terms. Usually, the pages returned are ranked mainly based on keywords matching 
rather than their relevance to the query terms. The users have to read a lot of returned 
pages to organize the information they wanted by themselves. This procedure is time-
consuming, and the information acquired is not concentrative. The research of Ques-
tion Answering (QA) intends to resolve this problem by answering user?s questions 
with exact answers.  
Questions like ?Who is Colin Powell?? or ?What is mold?? are definition questions 
[3]. Their relatively frequent occurrences in logs of Web search engines [2] indicate 
that they are an important type of question. The Text REtrieval Conference (TREC) 
provides an entire evaluation for definition question answering from TREC2003. A 
typical definition QA system extracts definition nuggets that contain the most descrip-
tive information about the question target (the concept for which information is being 
sought is called the target term, or simply, the target) from multiple documents.  
Until recently, definition questions remained a largely unexplored area of question 
answering. Standard factoid question answering technology, designed to extract single 
answers, cannot be directly applied to this task. The solution to this interesting re-
search challenge will involve the techniques in related fields such as information 
extraction, multi-document summarization, and answer fusion. 
In order to extract definitional nuggets/sentences, most systems use various pattern 
matching approaches. Kouylekov et al [10] relied on a set of hand-crafted rules to 
 Answering Definition Questions Using Web Knowledge Bases 499 
find definitional sentences. Sasha et al [12] proposed to combine data-driven statisti-
cal method and machine learned rules to generate definitions. Cui et al [7] used soft 
patterns, which were generated by unsupervised learning. Such methods require hu-
man labor to construct patterns or to annotate corpus more or less. 
Prager et al [8] try to solve this problem through existing technology. They de-
compose a definition question into a series of factoid questions. The answers to the 
factoid questions are merged to form the answer to the original question. However, 
the performance of their system on the TREC definition QA task is unsatisfactory. 
They need a more proper framework to determine how to generate these follow-up 
questions [8]. 
Some systems [1] [7] [9] statistically rank the candidate answers based on the ex-
ternal knowledge. They all adopt a centroid-based ranking method. For each question, 
they form one centroid (i.e., vector of words and frequencies) of the information in 
the external knowledge, and then calculate the similarity between the candidate an-
swer and this centroid. The ones that have large similarity are extracted as the answers 
to this question.  
Among the abundant information on the Web, Web knowledge bases (KBs) are 
one kind of most useful resource to acquire information. Dictionary definitions often 
supply knowledge that can be exploited directly. The information from them can 
model the interests of a typical user more reliably than other information. So we go 
further in identifying and selecting definition sentences from document collection 
using Web knowledge bases.  
Our work differs from the above in that we make use of the Web knowledge bases 
in a novel and effective way. Instead of using centroid-based ranking, we try to find 
out more effective methods in ranking the candidate sentences. We consider the rela-
tionship and the difference between the definitions from different knowledge sources. 
In our first algorithm, we calculate the similarity scores between the candidate sen-
tence and the definitions from different knowledge bases respectively, and merge 
these scores to generate the weight of this candidate sentence. In another algorithm, 
we first summarize the definitions from different KBs in order to eliminate the redun-
dant information, and then use this summary to rank the candidate sentences. We have 
applied our approaches to the TREC 2004 definition question-answering task. The 
results reveal that these procedures can make better use of the knowledge in the Web 
KBs, and the extracted sentences contain the most descriptive information about the 
question target. 
The remainder of the paper is organized as follows. In Section 2, we describe the 
system architecture. Then in Section 3 we give the details of our definition extraction 
methods. The evaluation of our system and the concluding remarks are given in Sec-
tion 4 and Section 5 respectively. 
2   System Architecture 
We adopt a general architecture for definition QA. The system consists of five mod-
ules: question processing, document processing, Web knowledge acquisition, defini-
tion extraction, and an optional module corpus information acquisition. The process 
of answering a definition question is briefly described as follows. 
500 Z. Zhang et al 
Firstly, a definition question is input, and the question processing module identifies 
the question target from this question. The so called target or target term is a term for 
which information is being sought (e.g., the target of the question ?What is Hale Bopp 
comet?? is ?Hale Bopp comet?.) The target term is the input for document processing 
module and knowledge acquisition module.  
Secondly, the document processing module generates the candidate sentence set 
according to this target term. This module has three steps, document retrieval, rele-
vant sentence extraction and redundancy removal. In the first step, the documents that 
relevant to the target are retrieved from the corpus. In the second step, the sentences 
that relevant to the target are extracted from these documents. We first cut the docu-
ments into sentences, and delete the irrelevant sentences by a few heuristic rules. In 
the third step, the redundant sentences are deleted by calculating the percentage of 
shared content words between sentences. After these three steps, we get the candidate 
sentence set.  
 
Definition 
Extraction 
Question 
processing 
Web Knowledge 
Acquisition 
Answer 
Question 
Target 
Candidate sentence set 
Definitions from Web KB 
World Wide Web
Question Corpus 
Corpus Information 
Acquisition 
Document 
processing 
 
Fig. 1.  System architecture 
Thirdly, the Web knowledge acquisition module acquires the definitions of the tar-
get term from the Web knowledge base. If we can find definitions from these sources 
(the Web KBs we used will be described in Section 3.1), we use them to rank the 
candidate sentences set.  
At last, the definition extraction module extracts the definition from the candidate 
sentence set based on the knowledge which is got from the Web knowledge base. 
In very few situations, no definitions can be found from the Web KBs, and the 
module named ?corpus information acquisition? is adopted to form the centroid of the 
 Answering Definition Questions Using Web Knowledge Bases 501 
candidate sentence set. We rank candidate sentences based on this centroid. The sen-
tences that have high similarity with this centroid are extracted as the answers to the 
question. The assumption is that words co-occurring frequently with the target in the 
corpus are more important ones for answering the question.  
The system architecture is illustrated in Fig.1. 
In this paper, we focus on how to make use of the Web KBs in extracting definition 
sentences, so we will describe the detail of the definition extraction module below. 
3   Definition Extraction Based on Web Knowledge Bases 
3.1   Web Knowledge Base 
There are lots of specific websites on the Web, such as online biography dictionaries 
or online cyclopaedias. We can get biography of a person, the profile of an organiza-
tion or the definition of a generic term from them. We call this kind of website Web 
knowledge base (KB). The definitions from them often supply knowledge that can be 
exploited directly. So we answer definition questions by utilizing the existing defini-
tions in the Web knowledge bases. The results of our system reveal that the Web 
knowledge bases are quite helpful to answering definition questions. 
Usually, different knowledge bases may pay attention to different kind of concept, 
and they may have different kind of entries. For example, the biography dictionary 
(www.s9.com) is a dictionary that covers widely on biography of people, and other 
KBs may pay attention to other kinds of concept. We choose several authoritative 
KBs that cover different kinds of concept to achieve our goal.  
The Web knowledge bases we used are the Encyclopedia(www.encyclopedia.com), 
the Wikipedia(www.wikipedia.com), the Merriam-Webster dictionary (www.mw. 
com), the WordNetglossaries (www.cogsci.princeton.edu/cgi-bin/webwn) and a biog-
raphies dictionary (www.s9.com). 
0
0. 2
0. 4
0. 6
0. 8
1
TREC2003 TREC2004
pe
rc
en
t
ency
wi ki
mw
wn
s9
al l
 
Fig. 2. The Web KBs? coverage of TREC data 
These Web KBs can cover most of the target terms, and the definitions in them are 
exact and concise.This can be confirmed from the experiment on TREC?s data set. 
Fig.2 gives our experiment results on the TREC 2003 and TREC 2004?s definition 
question sets, which have 50 and 65 target terms respectively. The ?ency?, ?wiki?, 
502 Z. Zhang et al 
?mw?, ?wn? and ?s9? stand for the five online KBs we have used. Each column repre-
sents the percent of the target terms that can be found in the corresponding online 
knowledge base. The column marked ?all? represents the percent of the target terms 
that can be found in at least one of these five online knowledge bases. 
It is easy to see that a high coverage can be got by using these Web knowledge 
bases. In the Section 3.2 and Section 3.3 we will show how to use these KBs, and in 
Section 4 we can see that it boosts the performance of the system significantly. 
3.2   Definition Extraction Based on GDS 
As mentioned above, we may get most of the submitted target terms? definitions by 
utilizing multiple Web KBs. One target may find its definitions in more than one 
knowledge base. Are all of them useful? The experimental data tells that, the different 
definitions belonging to one target differ from each other in some degree. They are 
short or long, concise or detailed. 
Considering the above factor, we try to utilize all of the definitions from different 
Web KBs to accomplish our task. For one target term, the definitions from all Web 
knowledge bases compose its ?general definition set?, which is abbreviated to GDS. 
Each element of this set is a definition from one Web knowledge base, so the number 
of the elements in this set is the same as the number of the Web KBs we used. When 
we cannot find its entry in a certain Web KB, its corresponding element will be an 
empty string.  
For each target, its candidate sentence set is expressed as SA = {A1, A2,?, Am}, 
where Ak (k=1..m) is a candidate sentence in the set and m is the total number of the 
candidate sentences.  
GDS is expressed as SGD = {D1, D2 ,..., Dn}, where Dk (k=1..n) is the definition of 
the target from the kth knowledge base, and n is the number of the knowledge bases. 
Dk may be an empty string when the target has no definition in the knowledge base k. 
In this algorithm, we rank the candidate sentences set SA = {A1, A2,?, Am} using 
SGD. 
Let Sij be the similarity of Ai and Dj. The similarity is the tf.idf score, where the 
candidate sentence Ai and the definition Dj are all treated as a bag of words. The tf.idf 
function we used is described in [5]. 
For each candidate sentence Ai in the set SA, we calculate its score based on the 
GDS as follows: 
ij
n
j
ji Swscore ?
=
=
1
 (?
=
=
n
j
jw
1
1 ) . (1) 
The weights jw are fixed based on experiment, considering the authoritativeness of 
the knowledge base from which Dj comes. The sentences of set SA are ranked based 
on this score, and the top ones are chosen as the definition of the target term. 
3.3   Definition Extraction Based on EDS 
As we have seen, for a target term, different definitions in its ?general definition set? 
may overlap in some degree. We intent to modify this set by merging its elements into 
 Answering Definition Questions Using Web Knowledge Bases 503 
one concise definition. We extract the essential information from the ?general defini-
tion set? to form the ?essential definition set?, which is abbreviated to EDS.  
EDS is expressed as SED = {d1, d2 ,?, dl}, where each element dk (k=1..l) is an es-
sential definition sentence about the target, and l is the number of the essential defini-
tion sentences. We hope that each element can tell one important aspect of the target 
term, and the whole ?essential definition set? may contain as much information as 
GDS but no redundant information. 
We try to use an automatic text summary technique [11] to get EDS. This tech-
nique is based on sentence?s weight and similarity between sentences. Firstly, calcu-
late the weights of all sentences and similarities between any two sentences, and then 
extract sentence based on these weights. After one sentence has been extracted, calcu-
late the new weights of the remained sentences based on their similarities. Iterate the 
above procedure until the extracted sentences reach the required length. More detail 
of this technique can be found in [11]. In this section we will try to use the ?essential 
definition set? to extract definitions from the candidate sentence set. 
1. Initially set the result set A={}, and i=1. 
2. For the element di in the set SED:  
First get the similarity between di and Aj (j=1..m), which is ex-
pressed as Sij.  
Then let },...,,max{ 21 imiiik SSSS = . If Sik>minsim , then add Ak 
to the set A and delete Ak from the set SA . 
3. If lengthAL
m
k
k max_)(
1
>??
=
 or i equals to l, the algorithm ends; 
otherwise, i = i +1, go to step2. 
Fig. 3.   Definition extraction using EDS 
The algorithm was showed in Fig.3. The candidate sentence set is also expressed as 
SA = {A1, A2,?, Am}, where Ak (k=1..m) is a candidate sentence in the set and m is 
the total number of the candidate sentences. The similarity Sij is calculated as the 
same as in Section 3.2. ( )kAL  represents the length of string Ak in character and m? is 
the number of elements in set A. The parameters max_length and minsim were em-
pirically set based on TREC?s definition question set. The last result is set A, where 
A={A1, A2,?, Am?}. 
4   Evaluation 
In order to get comparable evaluation, we apply our approach to TREC2004 defini-
tion QA task. We can see that our approach is an effective one compared with peer 
systems in this competitive evaluation. 
In this section we present the evaluation criterion and system performance on 
TREC task, and discuss the effectiveness of our approach. 
504 Z. Zhang et al 
4.1   Evaluation Criterion  
The TREC evaluation criterion [3] is summarized here for the purpose of discussing 
the evaluation results.  
For an individual definition question, there is a list of essential nuggets and accept-
able nuggets provided by TREC. These given nuggets are used to score the definition 
generated by the system. 
An individual definition question will be scored using nugget recall (R) and an ap-
proximation to nugget precision (P) based on length. In particular, 
R = # essential nuggets returned in response/# essential nuggets 
P is defined as: if    length < allowance,  P = 1 
else    P=1-[(length-allowance)/length] 
where    allowance = 100*(# essential+acceptable nuggets returned) 
length = total # non-white-space characters in answer strings 
The F measure is:  
RP?
PR?
+
)1+(
=F 2
2
 . (2) 
where ? value is fixed three in TREC 2004, and we also use three to get comparable 
result. 
The score of a system is the arithmetic mean of F-measure scores of all the defini-
tion questions output by the system. 
4.2   Effectiveness of Web Knowledge Bases 
To compare the effectiveness of the Web knowledge bases, we experimented on the 
TREC 2004 definition question set. The result can be seen in Table1. 
Table 1 shows the F-measure scores of our two algorithms and the baseline 
method. It also shows the median of the scores of all participating systems in TREC 
2004. The baseline method is: for an input question, form the candidate sentence set 
by using the approach described in Section 2. Then put the sentence of this set into the 
answer set one by one until all the sentences in the candidate sentence set are consid-
ered or the answer length is greater than a pre-fixed length (we set the length 3000 
characters in our experiment). 
We can see that our two algorithms all outperform the median and the baseline 
method which does not use Web knowledge bases. In conclusion, the Web knowledge 
bases are effective resources to definition question answering. 
Table 1. The F- measure score of the baseline method, the median system in TREC2004, and 
our two methods on TREC 2004 data set 
 Baseline 
method 
Median Ranking 
using  GDS 
Ranking 
using EDS 
F-measure 
score (?=3) 
0.231 0.184 0.404 0.367 
 Answering Definition Questions Using Web Knowledge Bases 505 
4.3   Definition Extraction Based on GDS vs. Based on EDS 
As we have mentioned, we have tried two algorithms in the definition extraction mod-
ule, which are based on GDS and EDS respectively. The performance of these algo-
rithms is shown in Table 2. 
Table 2. Performance of our three runs on the three types of quesitions and on the whole 64 
questions of TREC 2004 
 Num Q Run A Run B Run C 
all 64 0.404 0.389 0.367 
PERSON 23 0.366 0.372 0.404 
ORG 25 0.413 0.389 0.326 
THING 16 0.446 0.415 0.379 
We have submitted three runs in TREC2004, which were generated by using dif-
ferent algorithm in the definition extraction module. Run A and run B were generated 
by using GDS with slightly different weights in formula (1), and run C was generated 
by using EDS. All the 64 questions are divided into three classes based on the entity 
types of the targets, which are person, organization and other thing. Table 2 shows the 
three runs? F-measure scores on these three types and their overall score on the whole 
64 questions. 
Two algorithms? F-measure scores are all among the best of total 63 runs. Run C?s 
score on the ?PERSON?, 0.404 is the highest of our three runs on this type. Run A does 
better on the types named ?ORG? and ?THING?. We can say that these two algorithms 
contribute to different kinds of target terms. Dividing definition questions into different 
subclass and processing them with different methods could be a proper direction. 
Considering the score on all the 64 questions, the former algorithm is slightly 
higher than the latter one. However, the result of the latter one is also encouraging. 
Since the ?essential definition set? contain the important information and less redun-
dancy, it has the potential to get the answers, which are not only concise but also have 
wide coverage about the target. We believe it is an appropriate way to extract the high 
quality definitions. A preliminary analysis shows that the major problem is how to 
improve the quality and the coverage of the essential definition set. We believe that 
the performance could be boosted through improving this technique. 
In conclusion, we can say that our methods can make better use of the external 
knowledge in answering definition question. 
5   Conclusions 
This paper proposes a definition QA approach, which makes use of Web knowledge 
bases and several complementary technology components. The experiments reveal that 
the Web knowledge bases are effective resources to definition question answering, and 
the presented method gives an appropriate framework for answering this kind of ques-
tion. Our approach has achieved an encouraging performance with the F-measure score 
of 0.404, which is ranked second among all the submitted runs in TREC2004. 
506 Z. Zhang et al 
Since definitional patterns can not only filter out those statistically highly-ranked 
sentences that are not definitional, but also bring those definition sentences that are 
written in certain styles for definitions but are not statistically significant into the 
answer set. [6] In the future work, we will employ some pattern matching methods to 
reinforce our existing method. 
Acknowledgements 
This research was partly supported by NSF (Beijing, China) under contract of 
60435020, and Key Project of Science and Technology Committee of Shanghai under 
contract of 035115028. 
References 
1. Abdessamad Echihabi, Ulf Hermjakob, Eduard Hovy: Multiple-Engine Question Answer-
ing in TextMap. In Proceedings of the Twelfth Text REtreival Conference. NIST, Gath-
ersburg, MD (2003) 772?781 
2. Ellen M. Voorhees: Overview of the TREC 2001 question answering track. In Proceedings 
of the Tenth Text REtreival Conference. NIST, Gathersburg, MD (2001) 42?51 
3. Ellen M. Voorhees: Overview of the TREC 2003 Question Answering Track. In Proceed-
ings of the Twelfth Text REtreival Conference. NIST, Gathersburg, MD (2003) 54?68 
4. Ellen M. Voorhees: Evaluating answers to definition questions. In Proceedings of the 2003 
Human Language Technology Conference of the North American Chapter of the Associa-
tion for Computational Linguistics (2003) Volume 2: 109?111 
5. G.Salton, C. Buckley: Term weighting approaches in automatic text retrieval. Information 
Processing and Management (1988) 24(5): 513?523 
6. Hang Cui, Min-Yen Kan, Tat-Seng Chua, Jing Xiao: A comparative Study o Sentence Re-
trieval for Definitional Question Answering. In Proceedings of the 27th Annual Interna-
tional ACM SIGIR Conference (2004) 
7. Hang Cui, Keya Li, Renxu Sun, Tat-Seng Chua, Min-Yen kan: National University of 
Singapore the TREC-13 Question Answering Main Task. In Proceedings of the Thirteenth 
Text REtreival Conference. NIST, Gathersburg, MD (2004) 
8. J. M. Prager, Jennifer Chu-Carroll, Krzysztof Czuba, Christopher Welty, Abraham It-
tycheiach, Ruchi Mahindru: IBM?s PIQUANT in TREC2003. In Proceedings of the 
Twelfth Text REtreival Conference. NIST, Gathersburg, MD (2003) 283?292 
9. Jinxi Xu, Ana Licuanan, Ralph Weischedel: TREC2003 QA at BBN: Answering defini-
tional Questions. In Proceedings of the Twelfth Text REtreival Conference. NIST, Gath-
ersburg, MD (2003) 98~106 
10. Milen Kouylekov, Bernardo Magnini, Matteo Negri, Hristo Tanev: ITC-irst at TREC-
2003: the DIOGENE QA system. In Proceedings of the Twelfth Text REtreival Confer-
ence. NIST, Gathersburg, MD (2003) 349?357 
11. Qi Zhang, Xuanjing Huang, Lide Wu: A New Method for Calculating Similarity between 
Sentences and Application on Automatic Text Summarization. In Proceedings of the first 
National Conference on Information Retrieval and Content Security (2004) 
12. Sasha Blair-Goldensohn, Kathleen R. McKeown, Andrew Hazen Schlaikjer: A hybrid ap-
proach for QA track definitional questions. In Proceedings of the Twelfth Text REtreival 
Conference. NIST, Gathersburg, MD (2003) 185?192 
Transformation Based Chinese Entity Detection and Tracking 
Yaqian Zhou? 
Dept. Computer Science and Enginering 
Fudan Univ. 
Shanghai 200433, China 
ZhouYaqian@fudane.edu.cn 
Changning Huang 
Microsoft Research, Asia 
Beijing 100080, China 
cnhuang@msrchina.research.
microsoft.com 
Jianfeng Gao 
Microsoft Research, Asia 
Beijing 100080, China 
jfgao@microsoft.com 
Lide Wu 
Dept. of CSE., Fudan Univ. 
Shanghai 200433, China 
ldwu@fudane.edu.cn 
                                                          
? This work is done while the first author is visiting Microsoft Research Asia. 
 
 
Abstract 
This paper proposes a unified 
Transformation Based Learning (TBL, 
Brill, 1995) framework for Chinese 
Entity Detection and Tracking (EDT). 
It consists of two sub models: a 
mention detection model and an entity 
tracking/coreference model. The first 
sub-model is used to adapt existing 
Chinese word segmentation and Named 
Entity (NE) recognition results to a 
specific EDT standard to find all the 
mentions. The second sub-model is 
used to find the coreference relation 
between the mentions. In addition, a 
feedback technique is proposed to 
further improve the performance of the 
system. We evaluated our methods on 
the Automatic Content Extraction 
(ACE, NIST, 2003) Chinese EDT 
corpus. Results show that it 
outperforms the baseline, and achieves 
comparable performance with the state-
of-the-art methods. 
1 Introduction 
The task of Entity Detection and Tracking (EDT) 
is suggested by the Automatic Content Extrac-
tion (ACE) project (NIST, 2003). The goal is to 
detect all entities in a given text and track all 
mentions that refer to the same entity. The task 
is a fundamental to many Natural Language 
Processing (NLP) applications, such as informa-
tion retrieval and extraction, text classification, 
summarization, question answering, and ma-
chine translation.  
EDT is an extension of the task of 
coreference resolution in that in EDT we not 
only resolve the coreference between mentions 
but also detect the entities. Each of those entities 
may have one or more mentions. In the ACE 
project, there are five types of entities defined in 
EDT: person (PER), geography political Entity 
(GPE), organization (ORG), location (LOC), 
and facility (FAC). Many traditional coreference 
techniques can be extended to EDT for entity 
tracking. 
Early work on pronoun anaphora resolution 
usually uses rule-based methods (e.g. Hobbs 
1976; Ge et al, 1998; Mitkov, 1998), which try 
to mine the cues of the relation between the pro-
nouns and its antecedents. Recent research 
(Soon et al, 2001; Yang et al, 2003; Ng and 
Cardie, 2002; Ittycherah et al, 2003; Luo et al, 
2004) focuses on the use of statistical machine 
learning methods and tries to resolve references 
among all kinds of noun phases, including name, 
nominal and pronoun phrase. One common ap-
proach applied by them is to first train a binary 
statistical model to measure how likely a pair of 
232
mentions corefer; and then followed by a greedy 
procedure to group the mentions into entities. 
Mention detection is to find all the named en-
tity, noun or noun phrase, pronoun or pronoun 
phrase. Therefore, it needs Named Entity Rec-
ognition, but not only. Though the detection of 
entity mentions is an essential problem for 
EDT/coreference, there has been relatively less 
previous research. Ng and Cardie (2002) shows 
that improving the recall of noun phrase identi-
fication can improve the performance of a 
coreference system. Florian et al (2004) formu-
late the mention detection problem as a charac-
ter-based classification problem. They assign for 
each character in the text a label, indicating 
whether it is the start of a specific mention, in-
side a specific mention, or outside of any men-
tion. 
In this paper, we propose a unified EDT 
model based on the Transformation Based 
Learning (TBL, Brill, 1995) framework for Chi-
nese. The model consists of two sub models: a 
mention detection model and a coreference 
model. The first sub-model is used to adapt ex-
isting Chinese word segmentation and Named 
Entity (NE) recognition system to a specific 
EDT standard. TBL is a widely used machine 
learning method, but it is the first time it is ap-
plied to coreference resolution. In addition, a 
feedback technique is proposed to further im-
prove the performance of the system. 
The rest of the paper is organized as follows. 
In section 2, we propose the unified TBL Chi-
nese EDT model framework. We describe the 
four key techniques of our Chinese EDT, the 
word segmentation adaptation model, the men-
tion detection model, the coreference model and 
the feedback technique in section 3, 4, 5 and 6 
accordingly. The experimental results on the 
ACE Chinese EDT corpus are shown in section 
7. 
2 The Unified System Framework 
Our Chinese EDT system consists of two com-
ponents, mention detection module and corefer-
ence module besides a feedback technique 
between them as illustrated in Figure 1. 
MSRSeg (Gao et al, 2003; Gao et al), Mi-
crosoft Research Asia?s Chinese word segmen-
tation system that is integrated with named 
entity recognition, is used to segment Chinese 
words. However MSRSeg can?t well match the 
standard of ACE EDT evaluation for either 
types or boundaries. The difference of the stan-
dard of named entity between MSRSeg and 
ACE cause more than half of the errors for 
NAME mention detection. In order to overcome 
these problems, we integrate a segmentation 
adapter to mention detection model. 
The EDT system is a unified system that uses 
the TBL scheme. The idea of TBL is to learn a 
list of ordered rules while progressively improve 
upon the current state of the training set. An ini-
tial assignment is made based on simple statis-
tics, and then rules are greedily learned to 
correct the mistakes, until no more improvement 
can be made. There are three main problems in 
the TBL framework: An initial state assignment, 
a set of allowable templates for rules, and an 
objective function for learning. 
Figure 1. Entity detection and tracking system 
flow. 
3 Word Segmentation Adaptation 
The method of applying TBL to adapt the Chi-
nese word segmentation standard has been de-
scribed in Gao et al (2004). Our approach is 
slightly different for not have a correctly seg-
mented corpus according to ACE standard. 
From the un-segmented ACE EDT corpus, 
we can only obtain mention boundary informa-
tion. So the adapting objective is to detect the 
mention boundary instead of all words in text, 
correctly. In the corpus, very few mentions? 
boundaries are crossing1. 
The initial state of the segmentation adapta-
tion model is the output of MSRSeg. And we 
                                                          
1 The mentions? extents are frequently crossing, while 
heads not. 
MSRSeg&POS
Tagging 
Mention 
Detection 
Model 
Coreference 
Model 
Raw 
Document
Mentions Entities
Seg/POS/NE 
Document 
233
define two actions in the model, inserting and 
removing a boundary. The prefix or suffix of 
current word is used to define the boundary of 
inserting. Both inserting and removing action 
consider the combination of POS tag and word 
string of current, left and right words.  
When inserting a boundary, the right part of 
the word keeps the old POS tag, and the left part 
introduces a special POS tag ?new?. When re-
moving a boundary, the new formed word intro-
duces a special POS tag ?new?. The following 
two examples illustrate the strategy. 
????? /nt/court of Russia ? ???
/new/Russia ??/nt/court 
?/nr/Bo ?/nr/Pu ???/new/Bopu 
 
4 Mention Detection 
Since the word segmentation adaptation model 
has corrected the boundaries of mentions, our 
mention detection model bases on word and 
only tagging the entity mention types. The 
model detects the mentions by tagging sixteen 
tags (including the combination of five entity 
types and three mention types and ?OTHER? tag) 
for all the words outputted by segmentation ad-
aptation model. The templates, as illustrated in 
table 1, only refer to local features, such as POS 
tag and word string of left, right, and current 
words; the suffix, and single character feature of 
current word. 
Table 1. Templates for mention detection. 
MT1: P0 MT9: R4,P0 
MT2: W0 MT10: R3,P0 
MT3: P0,W0 MT11: R2,P0 
MT4: P_1,W0 MT12: R1,P0 
MT5: P_1,P0 MT13: S0,P0 
MT6: W0,P1 MT14: T_1,W0 
MT7: P0,P1 MT15: T_1,P0 
MT8: W0,W1 MT16: P0,T1 
Table 2. Examples of transformation rules of 
mention detection. 
MR1: MT13 0 ns GPE 
MR2: MT13 0 nr PER 
MR3: MT13 0 nt ORG 
MR4: MT16 n PER NPER 
MR5: MT16 new ORG GPE 
In table 1, ?MT1? et alrepresent the id of the 
templates; ?R1?, ?R2?, ?R3? and ?R4? represent 
the suffix of current word and the number of 
character is 1, 2, 3 and 4 accordingly; other suf-
fix ?_1?, ?0?, ?1? means the left, current and 
right words? feature; ?W? represent the string of 
word; ?P? represent POS tag; ?T? represent 
mention tag; ?S? represent the binary-value sin-
gle character feature. 
Five best transformation rules are illustrated 
in Table 2. For example, MR3 means ?if current 
word?s POS tag is nt, then it is a ORG?. Follow-
ing example well describe the process of apply-
ing these rules. 
???/new/Russia ??/nt/court  
????/new/Russia [??/nt/court]ORG (MR3)
?[ ? ? ? /new/Russia]GPE [ ? ?
/nt/court]ORG 
(MR5)
5 Entity Tracking 
In our entity tracking/coreference model, the 
initial state is let each mention in a document 
form an entity, as shown in Figure 2 (a). And the 
objective function directs the learning process to 
insert or remove chains between mentions (Fig-
ure 2 b and c) to approach the goal state (Figure 
2 f). 
A list of rules is learned in greedy fashion, 
according to the objective function. When no 
rule that improves the current state of the train-
ing set beyond a pre-set threshold can be found, 
the training phrase ends. The objective function 
in our system is driven by the correctness of the 
binary classification for pair-wise mention pairs. 
The TBL entity tracking model has more 
widely clustering/searching space as compare 
with previous strategies (Soon et al 2001; Ng 
and Cardie, 2002; Luo et al, 2004). For example, 
the state shown in Figure 2 (d) is not reachable 
for them. Because they assume one mention 
should refer to its most confidential mentions or 
entities that before it, while A and B are obvi-
ously not in same entity, as we can see in Figure 
2 (d). Thus C can refer to either A or B, but not 
both. While in TBL model, this state is allowed. 
In order to keep our system robust, the trans-
formation templates refer to only six types of 
simple features, as described below.  
All these features do not need any high level 
tools (i.e. syntactic parser) and little external 
knowledge base. In fact, only a country name 
abbreviation list (171 entrances) and a Chinese 
234
province alias list (34 entrances) are used to de-
tect ?alias? relation for String Match feature. 
String Match feature (STRM): Its possible 
values are exact, alias, abbr, left, right, other. If 
two mentions are exact string matched, then re-
turn exact; else if one mention is an alias of the 
other, then return alias; else if one mention is 
the abbreviation of the other, then return abbr; 
else if one mention is the left substring of the 
other, then return left; else if one mention is the 
right substring of the other, then return right; 
else return other. 
Figure 2. The procedure of TBL entity track-
ing/coreference model 
Edit Distance feature I (ED1): Its possible 
values are true or false. If the edit distance of the 
two mentions are less than or equal to 1, then 
return true, else return false. 
Token Distance feature I (TD1): Its possi-
ble values are true or false. If the edit distance of 
the two mentions are less than or equal to 1(i.e., 
there are not more than one token between the 
two mentions), then return true, else return false. 
Mention Type (MT): Its possible values are 
NAME, NOMINAL, or PRONOUN.. 
Entity Type (ET): Its possible values are 
PER, GPE, ORG, LOC, or FAC. 
Mention String (M): Its possible values are 
the actual mention string. 
These six features can be divided into two 
categories: mention pair features (the first three) 
and single mention features (the other three). 
And the single mention features are suffixed 
with ?L? or ?R? to differentiate for left or right 
mentions (i.e. ETL represent the left mention?s 
entity type). 
Based on the six kinds of basic features, four 
simple transformation templates are used in our 
system, as listed in table 3. 
Table 3. Templates for coreference model. 
CT1: MTL,MTR,STRM 
CT2: MTL,MTR,ETL,ETR,ED1 
CT3: MTL,MTR,ETL,ETR,TD1 
CT4: MTL,MTR,ML,MR 
Table 4. Examples of transformation rules of 
coreference model. 
CR1:CT1 NAME NAME EXACT LINK 
CR2:CT2 NOMINAL NAME PER PER 1 LINK 
CR3:CT1 NAME NAME ALIAS LINK 
CR4:CT1 PRONOUN PRONOUN EXACT LINK 
Though trained on different data set will 
learn different rules, the four rules listed in table 
4 is the best rules that always been learned. For 
example, the first rule means that ?If two NAME 
mentions are exact string matched, then insert a 
chain between them?. The following example 
illustrates the process. 
[??/US]GPE??[???/Russia]GPE?
???????[?? /US]GPE[??
/businessman]NPER[??/Bopu]PER 
? [ ? ? /US]GPE-1 ? ? [ ? ? ?
/Russia]GPE ????????[??
/US]GPE-1[?? /businessman]NPER[??
/Bopu]PER 
(CR1)
? [ ? ? /US]GPE-1 ? ? [ ? ? ?
/Russia]GPE ????????[??
/US]GPE-1[?? /businessman]NPER-2[??
/Bopu]PER-2 
(CR2)
6 Feedback 
There are three reasons push us apply feedback 
technique in the EDT system. The first is to de-
termine whether a signal character is an abbre-
viation is discourse depended. For example, 
Chinese character ??? can represents both a 
country name ?China? and a common preposi-
tion ?in?. If it can links to ??? /China? by 
coreference model, it is likely to represent 
A    B 
 
C    D 
 
E 
A    B 
 
C    D 
 
E 
A    B
 
C    D
 
E 
A    B 
 
C    D 
 
E 
A    B 
 
C    D 
 
E 
A    B 
 
C    D 
 
E 
(e) 
(a)                     (b)                          (c) 
(d) 
(f) 
235
?China?. The second is the definition of men-
tions is hard to hold, especially the nominal 
mentions. An isolated mention is more likely not 
to be a mention. The third is to pick up lost men-
tion according to its multi-appearance in the dis-
course. In fact, [Ji and Crishman, 2004] has used 
five hubristic rules based on coreference results 
to improve the name recognition result. While in 
this section we will present an automatic method. 
The feedback technique is employed by us-
ing entity features in mention detection model. 
In our model, the transformation templates refer 
to the number of mentions in the entity, the sin-
gle character feature, the entity type feature, the 
mention type feature and mention string, as 
listed follows. 
SDD: Its possible values are the combination 
of the mention type and entity type of the men-
tion string in discourse: PER, GPE, ORG, LOC, 
FAC, NPER (NOMINAL PER), NGPE, NORG, 
NLOC, NFAC, PPER (PROUNOUN PER), 
PGPE, PORG, PLOC, and PFAC. 
SC2, SC3, SC4: Their possible values are 
true or false. If the word string appear not less 
than 2 (3, 4) times in the discourse then return 
true, else return false. 
PDD: presents the combination of the men-
tion type and entity type of the mention in dis-
course. Its possible values are same with ?SDD?. 
PC2: Its possible values are true or false. If 
the mention belong to an entity has not less than 
2 mentions then return true, else return false. 
S0: Its possible values are true or false. If the 
mention is a single character word then return 
true, else return false. 
W0: string of the mention. 
Table 5. Templates for feedback. 
FT1: SDD,SC2 FT4: PDD,PC2,S0 
FT2: SDD,SC3 FT5: PDD,PC2,S0 
FT3: SDD,SC4 FT6: PDD,PC2,W0 
Table 6. Examples of transformation rules of 
feedback. 
FR1: FT1 PER T PER FR4: FT4 NORG F O 
FR2: FT5 GPE F 1 O FR5: FT3 PGPE F O 
FR3: FT4 NFAC F O  
The first rule means that ?if a word in the 
document appears as person name more than 
two times, then it is a person name?. This rule 
can pick up lost person names. The second rule 
means that ?if a GPE mention is isolated and it 
is a single character word, then it is not a men-
tion?. This rule can throw away isolated abbre-
viation of GPE, as illustrated in the following 
example. 
?[??/Bopu]PER-3 ?????[???
/Russia]GPE-2[?? /court]ORG-4[? /by]GPE-6 
????? 20??? ? 
??[??/Bopu]PER-3 ?????[??
?/Russia]GPE-2[??/court]ORG-4?/by ?
???? 20??? ? 
(FR2)
7 Experiments 
Our experiments are conducted on Chinese EDT 
corpus for ACE project from LDC. This corpus 
is the training data for ACE evaluation 2003. 
The corpus has two types, paper news (nwire) 
and broadcast news (bnews). the statistics of the 
corpus is shown in Table 7. 
Table 7. Statistics of the ACE corpus. 
 nwire bnews 
Document 99 122 
Character 55,000 45,000 
Entity 2517 2050 
Mention 5423 4506 
Because the test data for ACE evaluation is 
not public, we randomly and equally divide the 
corpus into 3 subsets: set0, set1, set2. Each con-
sists of about 73 documents and 33K Chinese 
Characters 2 . Cross experiments are conducted 
on these data sets. ACE-value is used to evaluate 
the EDT system; and precision (P), recall (R) 
and F (F=2*P*R/(P+R)) to evaluate the mention 
detection result. 
In the experiments, we first use one data set 
train the mention detection system; then use an-
other set train the coreference model based on 
the output of the mention detection; finally use 
the other set test. In practice, we can retrain the 
mention detection model use the two train set to 
get higher performance. 
Table 8. EDT and mention detection results. 
 EDT Mention Detection 
Method ACE-
value 
R P F 
Tag 55.7?1.6 62.3?1.0 85.0?1.4 71.9?0.6
SegTag 61.6?3.6 70.9?4.5 81.9?1.0 75.9?2.6
SegTag+F 63.3?2.0 68.0?4.8 83.8?1.2 75.0?3.1
                                                          
2 Two of the documents (CTS20001110.1300.0506, and 
XIN20001102.2000.0207) in the corpus are not use for 
serious annotation error. 
236
In Table 8, ?SegTag? represent the mention 
detection system integrated with segmentation 
adaptation, ?Tag? represent the mention detec-
tion system without segmentation adaptation. 
?+F? means with feedback. 
The ACE-value of our Chinese EDT system 
is better than 58.8% of Florian et al (2004). In 
fact, the two systems are not comparable for not 
basing on the same training and test data. How-
ever both corpora are under the same standard 
from ACE project, and our training data (about 
66K) is smaller than Florian et al (2004) (about 
80K). Therefore, it is an encouraging result. 
Segmentation adapting and feedback can im-
prove 7.5% of ACE-value for the whole system. 
As we can see from Table 8, using TBL method 
to adapt standard or correct errors can improve 
the mention detection performance especially 
recall, and word segmentation adapting is essen-
tial for mention detection. Feedback can im-
prove the precision of mention detection with 
loss of recall. The two techniques can signifi-
cantly improve the EDT performance, since the 
p-value of the T-test for the performance of 
?SegTag? to ?Tag? is 96.7%, while for ?Seg-
Tag+F? to ?Tag? is 98.9%. The recall of men-
tion detection is dropped after feedback because 
of the great effect of rule FR2, 3, 4 and 5 as il-
lustrated in table 6. 
8 Conclusion 
In this paper, we integrate the mention detection 
model and entity tracking/coreference model 
into a unified TBL framework. Experimental 
results show segmentation adapting and feed-
back can significantly improve the performance 
of EDT system. And even with very limited 
knowledge and shallow NLP tools, our method 
can reach comparable performance with related 
work. 
References 
Eric Brill. 1995. Transformation-based error-driven 
learning and natural language processing: a case 
study in Part-of-Speech tagging. In: Computa-
tional Lingusitics, 21(4). 
R Florian, H Hassan, A Ittycheriah, H Jing, N Kamb-
hatla, X Luo, N Nicolov, and S Roukos. 2004. A 
statistical model for multilingual entity detection 
and tracking. In Proc. of HLT/NAACL-04, pages 
1-8, Boston Massachusetts, USA. 
Jianfeng Gao, Mu Li and Changning Huang. 2003. 
Improved souce-channel model for Chinese word 
segmentation. In Proc. of ACL2003. 
Jianfeng Gao, Andi Wu, Mu Li, Changning Huang, 
Hongqiao Li, Xinsong and Xia, Haowei Qin. 2004. 
Adaptive Chinese word segmentation. In Proc. of 
ACL2004. 
Niyu Ge, John Hale, and Eugene Charniak. 1998. A 
statistical approach to anaphora resolution. In 
Proc. of the Sixth Workshop on Very Large Cor-
pora. 
Sanda M. Harabagiu, Razvan C. Bunescu, and Steven 
J. Maiorano. 2001. Text and knowledge mining 
for coreference resolution. In Proc. of NAACL. 
J. Hobbs. 1976. Pronoun resolution. Technical report, 
Dept. of Computer Science, CUNY, Technical 
Report TR76-1. 
A Ittycheriah, L Lita, N Kambhatla, N Nicolov, S 
Roukos, and M Stys. 2003. Identifying and track-
ing entity mentions in maximum entropy frame-
work. In HLT-NAACL 2003. 
Heng Ji and Ralph Grishman. 2004. Applying 
Coreference to Improve Name Recognition. In 
ACL04 Reference Resolution and its Application Work-
shop. 
Xiaoqiang Luo, A. Ittycheriah, H. Jing, N. Kamb-
hatla, S. Roukos.2004. A Mention-Synchronous 
Coreference Resolution Aogorithm Based on the 
Bell Tree. In Proc. of ACL2004. 
R. Mitkov. 1998. Robust pronoun resolution with 
limited knowledge. In Proc. of the 17th Interna-
tional Conference on Computational Linguistics, 
pages 869-875. 
MUC. 1996. Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6). Morgan Kauf-
mann, San Mateo, CA. 
NIST. 2003. The ACE evaluation plan. 
www.nist.gov/speech/tests/ace/index.htm. 
Wee Meng Soon, Hwee Tou Ng, and Chung Yong 
Lim. 2001. A machine learning approach to 
coreference resolution of noun phrases. Computa-
tional Linguistics, 27(4):521-544. 
M. Vilain, J. Burger, J. Aberdeen, D. Connolly and L. 
Hirschman. 1995. A Model-Theoretic coreference 
scoring scheme. In Proc. of MUC-6, page45-52. 
Morgan Kaufmann. 
Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew 
Lim Tan. 2003. Coreference resolution using 
competition learning approach. In Proc. of 
ACL2003. 
237
A Fast Algorithm for Feature Selection in Conditional
Maximum Entropy Modeling
Yaqian Zhou
Computer Science Department
Fudan University
Shanghai 200433, P.R. China
archzhou@yahoo.com
Fuliang Weng
Research and Technology Center
Robert Bosch Corp.
Palo Alto, CA 94304, USA
Fuliang.weng@rtc.bosch.com
Lide Wu
Computer Science Department
Fudan University
Shanghai 200433, P.R. China
ldwu@fudan.edu.cn
Hauke Schmidt
Research and Technology Center
Robert Bosch Corp.
Palo Alto, CA 94304, USA
hauke.schmidt@rtc.bosch.com
Abstract
This paper describes a fast algorithm that se-
lects features for conditional maximum en-
tropy modeling. Berger et al (1996) presents
an incremental feature selection (IFS) algo-
rithm, which computes the approximate gains
for all candidate features at each selection
stage, and is very time-consuming for any
problems with large feature spaces. In this
new algorithm, instead, we only compute the
approximate gains for the top-ranked features
based on the models obtained from previous
stages. Experiments on WSJ data in Penn
Treebank are conducted to show that the new
algorithm greatly speeds up the feature selec-
tion process while maintaining the same qual-
ity of selected features. One variant of this
new algorithm with look-ahead functionality
is also tested to further confirm the good
quality of the selected features. The new algo-
rithm is easy to implement, and given a fea-
ture space of size F, it only uses O(F) more
space than the original IFS algorithm.
1 Introduction
Maximum Entropy (ME) modeling has received
a lot of attention in language modeling and natural
language processing for the past few years (e.g.,
Rosenfeld, 1994; Berger et al1996; Ratnaparkhi,
1998; Koeling, 2000). One of the main advantages
using ME modeling is the ability to incorporate
various features in the same framework with a
sound mathematical foundation. There are two
main tasks in ME modeling: the feature selection
process that chooses from a feature space a subset
of good features to be included in the model; and
the parameter estimation process that estimates the
weighting factors for each selected feature in the
exponential model. This paper is primarily con-
cerned with the feature selection process in ME
modeling.
While the majority of the work in ME modeling
has been focusing on parameter estimation, less
effort has been made in feature selection. This is
partly because feature selection may not be neces-
sary for certain tasks when parameter estimate al-
gorithms are fast. However, when a feature space
is large and complex, it is clearly advantageous to
perform feature selection, which not only speeds
up the probability computation and requires
smaller memory size during its application, but
also shortens the cycle of model selection during
the training.
Feature selection is a very difficult optimization
task when the feature space under investigation is
large. This is because we essentially try to find a
best subset from a collection of all the possible
feature subsets, which has a size of 2
|
W
|
, where |W|
is the size of the feature space.
In the past, most researchers resorted to a sim-
ple count cutoff technique for selecting features
(Rosenfeld, 1994; Ratnaparkhi, 1998; Reynar and
Ratnaparkhi, 1997; Koeling, 2000), where only the
features that occur in a corpus more than a pre-
defined cutoff threshold get selected. Chen and
Rosenfeld (1999) experimented on a feature selec-
tion technique that uses a c
2
 test to see whether a
feature should be included in the ME model, where
the c
2
 test is computed using the count from a prior
distribution and the count from the real training
data. It is a simple and probably effective tech-
nique for language modeling tasks. Since ME
models are optimized using their likelihood or
likelihood gains as the criterion, it is important to
establish the relationship between c
2
 test score and
the likelihood gain, which, however, is absent.
Berger et al (1996) presented an incremental fea-
ture selection (IFS) algorithm where only one fea-
ture is added at each selection and the estimated
parameter values are kept for the features selected
in the previous stages. While this greedy search
assumption is reasonable, the speed of the IFS al-
gorithm is still an issue for complex tasks. For
better understanding its performance, we re-
implemented the algorithm. Given a task of
600,000 training instances, it takes nearly four
days to select 1000 features from a feature space
with a little more than 190,000 features. Berger
and Printz (1998) proposed an f-orthogonal condi-
tion for selecting k features at the same time with-
out affecting much the quality of the selected
features. While this technique is applicable for
certain feature sets, such as word link features re-
ported in their paper, the f-orthogonal condition
usually does not hold if part-of-speech tags are
dominantly present in a feature subset. Past work,
including Ratnaparkhi (1998) and Zhou et al
(2003), has shown that the IFS algorithm utilizes
much fewer features than the count cutoff method,
while maintaining the similar precision and recall
on tasks, such as prepositional phrase attachment,
text categorization and base NP chunking. This
leads us to further explore the possible improve-
ment on the IFS algorithm.
In section 2, we briefly review the IFS algo-
rithm. Then, a fast feature selection algorithm is
described in section 3. Section 4 presents a number
of experiments, which show a massive speed-up
and quality feature selection of the new algorithm.
Finally, we conclude our discussion in section 5.
2  The Incremental Feature Selection Al-
gorithm
For better understanding of our new algorithm, we
start with briefly reviewing the IFS feature selec-
tion algorithm. Suppose the conditional ME model
takes the following form:
? 
p(y | x) =
1
Z (x )
exp( l
j
f
j
(x, y))
j
?
where f
j 
are the features, l
j 
are their corre-
sponding weights, and Z(x) is the normalization
factor.
The algorithm makes the approximation that the
addition of a feature f in an exponential model af-
fects only its associated weight a, leaving un-
changed the l-values associated with the other
features. Here we only present a sketch of the algo-
rithm in Figure 1. Please refer to the original paper
for the details.
In the algorithm, we use I for the number of
training instances, Y for the number of output
classes, and F for the number of candidate features
or the size of the candidate feature set.
0. Initialize: S = ?, sum[1..I, 1..Y] = 1,
z[1..I] = Y
1. Gain computation:
MaxGain = 0
for f in feature space F do
)(maxarg
? aa a fSG ?=
)(max
? aa fSGg ?=
if MaxGain < 
g
?  then
   MaxGain = 
g
?
   f
*
 = f
  a*=a?
2. Feature selection:
S = S ? { f
*
 }
3. if termination condition is met, then stop
4. Model adjustment:
for instance i such that there is y
and f
*
(x
i
, y) = 1 do
z[i] -=sum[i, y]
sum[i, y] ?= exp(a*)
z[i] += sum[i, y]
5. go to  step 1.
Figure 1: A Variant of the IFS Algorithm.
One difference here from the original IFS algo-
rithm is that we adopt a technique in (Goodman,
2002) for optimizing the parameters in the condi-
tional ME training. Specifically, we use array z to
store the normalizing factors, and array sum for all
the un-normalized conditional probabilities sum[i,
y]. Thus, one only needs to modify those sum[i, y]
that satisfy f
*
(x
i
, y)=1, and to make changes to their
corresponding normalizing factors z[i]. In contrast
to what is shown in Berger et al1996?s paper, here
is how the different values in this variant of the IFS
algorithm are computed.
Let us denote
?
=
j
jj
yxfxysum )),(exp()|( l
?
=
y
xysumxZ )|()(
Then, the model can be represented by sum(y|x)
and Z(x) as follows:
)(/)|()|( xZxysumxyp =
where sum(y|x
i
) and Z(x
i
) correspond to sum[i,y]
and z[i] in Figure 1, respectively.
Assume the selected feature set is S, and f is
currently being considered. The goal of each se-
lection stage is to select the feature f that maxi-
mizes the gain of the log likelihood, where the a
and gain of f are derived through following steps:
Let the log likelihood of the model be
?
?
-=
-?
yx
yx
xZxysumyxp
xypyxppL
,
,
))(/)|(log(),(
~
))|(log(),(
~
)(
and the empirical expectation of feature f be
? 
E
? 
p
( f ) =
? 
p (x,y) f (x, y)
x,y
?
With the approximation assumption in Berger
et al(1996)?s paper, the un-normalized component
and the normalization factor of the model have the
following recursive forms:
  )|()|(
aa
exysumxysum
SfS
?=
?
)|(                           
)|()()(
xysum
xysumxZxZ
fS
SSfS aa
?
?
+
-=
The approximate gain of the log likelihood is
computed by
 
? 
G
S? f
(a) ? L(p
S? f
a
) - L(p
S
)
             = -
? 
p (x)(logZ
S? f ,a (x)
x
?
/Z
S
(x))
                      + aE
? 
p
( f )                    (1)
The maximum approximate gain and its corre-
sponding a are represented as:
  )(max),(~ aa fSGfSL ?=D
  )(maxarg),(~ aa a fSGfS ?=
3 A Fast Feature Selection Algorithm
The inefficiency of the IFS algorithm is due to the
following reasons. The algorithm considers all the
candidate features before selecting one from them,
and it has to re-compute the gains for every feature
at each selection stage. In addition, to compute a
parameter using Newton?s method is not always
efficient. Therefore, the total computation for the
whole selection processing can be very expensive.
Let g(j, k) represent the gain due to the addition
of feature f
j
 to the active model at stage k. In our
experiments, it is found even if D (i.e., the addi-
tional number of stages after stage k) is large, for
most j, g(j, k+D) - g(j, k) is a negative number or at
most a very small positive number. This leads us to
use the g(j, k) to approximate the upper bound of
g(j, k+D).
The intuition behind our new algorithm is that
when a new feature is added to a model, the gains
for the other features before the addition and after
the addition do not change much. When there are
changes, their actual amounts will mostly be within
a narrow range across different features from top
ranked ones to the bottom ranked ones. Therefore,
we only compute and compare the gains for the
features from the top-ranked downward until we
reach the one with the gain, based on the new
model, that is bigger than the gains of the remain-
ing features. With a few exceptions, the gains of
the majority of the remaining features were com-
puted based on the previous models.
As in the IFS algorithm, we assume that the ad-
dition of a feature f only affects its weighting fac-
tor a. Because a uniform distribution is assumed as
the prior in the initial stage, we may derive a
closed-form formula for a(j, 0) and g(j, 0) as fol-
lows.
Let
? 
Ed ( f ) = ? p (x)max
y
{ f (x, y)}
x
?
? 
R
e
( f ) = E
? 
p
( f ) / Ed ( f )
Yp /1
0
=
Then
 )log()0,(
)(1
1)(
0
0
fR
p
p
fR
e
e
j
-
-
?=a
? 
g( j,0) = L(p
?? f
a( i,0)
) - L(p
?
)
          = Ed ( f )[Re ( f )log Re ( f )
p
0
                     + (1- R
e
( f ))log
1-R
e
( f )
1- p
0
] 
where ? denotes an empty set, p? is the uni-
form distribution. The other steps for computing
the gains and selecting the features are given in
Figure 2 as a pseudo code. Because we only com-
pute gains for a small number of top-ranked fea-
tures, we call this feature selection algorithm as
Selective Gain Computation (SGC) Algorithm.
In the algorithm, we use array g to keep the
sorted gains and their corresponding feature indi-
ces. In practice, we use a binary search tree to
maintain the order of the array.
The key difference between the IFS algorithm
and the SGC algorithm is that we do not evaluate
all the features for the active model at every stage
(one stage corresponds to the selection of a single
feature). Initially, the feature candidates are or-
dered based on their gains computed on the uni-
form distribution. The feature with the largest gain
gets selected, and it forms the model for the next
stage. In the next stage, the gain of the top feature
in the ordered list is computed based on the model
just formed in the previous stage. This gain is
compared with the gains of the rest features in the
list. If this newly computed gain is still the largest,
this feature is added to form the model at stage 3.
If the gain is not the largest, it is inserted in the
ordered list so that the order is maintained. In this
case, the gain of the next top-ranked feature in the
ordered list is re-computed using the model at the
current stage, i.e., stage 2.
This process continues until the gain of the top-
ranked feature computed under the current model
is still the largest gain in the ordered list. Then, the
model for the next stage is created with the addi-
tion of this newly selected feature. The whole fea-
ture selection process stops either when the
number of the selected features reaches a pre-
defined value in the input, or when the gains be-
come too small to be useful to the model.
0. Initialize: S = ?, sum[1..I, 1..Y] = 1,
z[1..I] = Y, g[1..F] = {g(1,0),?,g(F,0)}
1. Gain computation:
MaxGain = 0
Loop
    
]},...,1[{maxarg
in  
Fgf
Ff
j
=
    if g[j] ? MaxGain then go to step 2
    else
 
)(maxarg
? aa a fSG ?=
 
)(max
? aa fSGg ?=
       g[j]= 
g
?
       if MaxGain < 
g
?  then
          MaxGain = 
g
?
          f
*
 = f
j
         a*=a?
2. Feature selection:
S = S ? { f
*
 }
3. if termination condition is met, then stop
4. Model adjustment:
for instance i such that there is y
and f
*
(x
i
, y) = 1 do
z[i] -=sum[i, y]
sum[i, y] ?= exp(a*)
z[i] += sum[i, y]
5. go to  step 1.
Figure 2: Selective Gain Computation Algo-
rithm for Feature Selection
In addition to this basic version of the SGC al-
gorithm, at each stage, we may also re-compute
additional gains based on the current model for a
pre-defined number of features listed right after
feature f
*
 (obtained in step 2) in the ordered list.
This is to make sure that the selected feature f
*
 is
indeed the feature with the highest gain within the
pre-defined look-ahead distance. We call this vari-
ant the look-ahead version of the SGC algorithm.
4 Experiments
A number of experiments have been conducted to
verify the rationale behind the algorithm. In par-
ticular, we would like to have a good understand-
ing of the quality of the selected features using the
SGC algorithm, as well as the amount of speed-
ups, in comparison with the IFS algorithm.
The first sets of experiments use a dataset {(x,
y)}, derived from the Penn Treebank, where x is a
10 dimension vector including word, POS tag and
grammatical relation tag information from two ad-
jacent regions, and y is the grammatical relation
tag between the two regions. Examples of the
grammatical relation tags are subject and object
with either the right region or the left region as the
head. The total number of different grammatical
tags, i.e., the size of the output space, is 86. There
are a little more than 600,000 training instances
generated from section 02-22 of WSJ in Penn
Treebank, and the test corpus is generated from
section 23.
In our experiments, the feature space is parti-
tioned into sub-spaces, called feature templates,
where only certain dimensions are included. Con-
sidering all the possible combinations in the 10-
dimensional space would lead to 2
10
 feature tem-
plates. To perform a feasible and fair comparison,
we use linguistic knowledge to filter out implausi-
ble subspaces so that only 24 feature templates are
actually used. With this amount of feature tem-
plates, we get more than 1,900,000 candidate fea-
tures from the training data. To speed up the
experiments, which is necessary for the IFS algo-
rithm, we use a cutoff of 5 to reduce the feature
space down to 191,098 features. On average, each
candidate feature covers about 485 instances,
which accounts for 0.083% over the whole training
instance set and is computed through:
???
=
jj yx
j
yxfac 1/),(
,
The first experiment is to compare the speed of
the IFS algorithm with that of SGC algorithm.
Theoretically speaking, the IFS algorithm com-
putes the gains for all the features at every stage.
This means that it requires O(NF) time to select a
feature subset of size N from a candidate feature
set of size F. On the other hand, the SGC algorithm
considers much fewer features, only 24.1 features
on average at each stage, when selecting a feature
from the large feature space in this experiment.
Figure 3 shows the average number of features
computed at the selected points for the SGC algo-
rithm, SGC with 500 look-ahead, as well as the
IFS algorithm. The averaged number of features is
taken over an interval from the initial stage to the
current feature selection point, which is to smooth
out the fluctuation of the numbers of features each
selection stage considers. The second algorithm
looks at an additional fixed number of features,
500 in this experiment, beyond the ones considered
by the basic SGC algorithm. The last algorithm has
a linear decreasing number of features to select,
because the selected features will not be consid-
ered again. In Figure 3, the IFS algorithm stops
after 1000 features are selected. This is because it
takes too long for this algorithm to complete the
entire selection process. The same thing happens in
Figure 4, which is to be explained below.
0
1
2
3
4
5
6
200 400 600 800 1000 2000 4000 6000 8000 10000
Number of Selected Features
A
v
e
r
a
g
e
 
C
o
n
s
i
d
e
r
e
d
 
F
e
a
t
u
r
e
 
N
u
m
b
e
r
Berger SGC-0 SGC-500
log
10
(Y)
Figure 3: The log number of features considered in
SGC algorithm, in comparison with the IFS algo-
rithm.
To see the actual amount of time taken by the
SGC algorithms and the IFS algorithm with the
currently available computing power, we use a
Linux workstation with 1.6Ghz dual Xeon CPUs
and 1 GB memory to run the two experiments si-
multaneously. As it can be expected, excluding the
beginning common part of the code from the two
algorithms, the speedup from using the SGC algo-
rithm is many orders of magnitude, from more than
100 times to thousands, depending on the number
of features selected. The results are shown in Fig-
ure 4.
-2
-1
0
1
2
3
200 400 600 800 1000 2000 4000 6000 8000 10000
Number of Selected Features
A
v
e
r
a
g
e
 
T
i
m
e
f
o
r
 
e
a
c
h
 
s
e
l
e
c
t
i
o
n
 
s
t
e
p
(
s
e
c
o
n
d
)
Berger SGC-0
log
10
(Y)
Figure 4: The log time used by SGC algorithm, in
comparison with the IFS algorithm.
To verify the quality of the selected features
using our SGC algorithm, we conduct four experi-
ments: one uses all the features to build a condi-
tional ME model, the second uses the IFS
algorithm to select 1,000 features, the third uses
our SGC algorithm, the fourth uses the SGC algo-
rithm with 500 look-ahead, and the fifth takes the
top n most frequent features in the training data.
The precisions are computed on section 23 of the
WSJ data set in Penn Treebank. The results are
listed in Figure 5. Three factors can be learned
from this figure. First, the three IFS and SGC algo-
rithms perform similarly. Second, 3000 seems to
be a dividing line: when the models include fewer
than 3000 selected features, the IFS and SGC algo-
rithms do not perform as well as the model with all
the features; when the models include more than
3000 selected features, their performance signifi-
cantly surpass the model with all the features. The
inferior performance of the model with all the fea-
tures at the right side of the chart is likely due to
the data over-fitting problem. Third, the simple
count cutoff algorithm significantly under-
performs the other feature selection algorithms
when feature subsets with no more than 10,000
features are considered.
To further confirm the findings regarding preci-
sion, we conducted another experiment with Base
NP recognition as the task. The experiment uses
section 15-18 of WSJ as the training data, and sec-
tion 20 as the test data. When we select 1,160 fea-
tures from a simple feature space using our SGC
algorithm, we obtain a precision/recall of
92.75%/93.25%. The best reported ME work on
this task includes Koeling (2000) that has the pre-
cision/recall of 92.84%/93.18% with a cutoff of 5,
and Zhou et al (2003) has reached the perform-
ance of 93.04%/93.31% with cutoff of 7 and
reached a performance of 92.46%/92.74% with
615 features using the IFS algorithm. While the
results are not directly comparable due to different
feature spaces used in the above experiments, our
result is competitive to these best numbers. This
shows that our new algorithm is both very effective
in selecting high quality features and very efficient
in performing the task.
5 Comparison and Conclusion
Feature selection has been an important topic in
both ME modeling and linear regression. In the
past, most researchers resorted to count cutoff
technique in selecting features for ME modeling
(Rosenfeld, 1994; Ratnaparkhi, 1998; Reynar and
Ratnaparkhi, 1997; Koeling, 2000). A more refined
algorithm, the incremental feature selection algo-
rithm by Berger et al(1996), allows one feature
being added at each selection and at the same time
keeps estimated parameter values for the features
selected in the previous stages. As discussed in
(Ratnaparkhi, 1998), the count cutoff technique
works very fast and is easy to implement, but has
the drawback of containing a large number of re-
70
72
74
76
78
80
82
84
86
88
90
92
94
200 400 600 800 1000 2000 4000 6000 8000 10000
Number of Selected Features
P
r
e
c
i
s
i
o
n
 
(
%
)
All (191098) IFS SGC-0
SGC-500 Count Cutoff
Figure 5: Precision results from models using the whole
feature set and the feature subsets through the IFS algo-
rithm, the SGC algorithm, the SGC algorithm with 500
look-ahead, and the count cutoff algorithm.
dundant features. In contrast, the IFS removes the
redundancy in the selected feature set, but the
speed of the algorithm has been a big issue for
complex tasks. Having realized the drawback of
the IFS algorithm, Berger and Printz (1998) pro-
posed an f-orthogonal condition for selecting k
features at the same time without affecting much
the quality of the selected features. While this
technique is applicable for certain feature sets,
such as link features between words, the f -
orthogonal condition usually does not hold if part-
of-speech tags are dominantly present in a feature
subset.
Chen and Rosenfeld (1999) experimented on a
feature selection technique that uses a c
2
 test to see
whether a feature should be included in the ME
model, where the c
2
 test is computed using the
counts from a prior distribution and the counts
from the real training data. It is a simple and
probably effective technique for language model-
ing tasks. Since ME models are optimized using
their likelihood or likelihood gains as the criterion,
it is important to establish the relationship between
c
2
 test score and the likelihood gain, which, how-
ever, is absent.
There is a large amount of literature on feature
selection in linear regression, where least mean
squared errors measure has been the primary opti-
mization criterion. Two issues need to be ad-
dressed in order to effectively use these techniques.
One is the scalability issue since most statistical
literature on feature selection only concerns with
dozens or hundreds of features, while our tasks
usually deal with feature sets with a million of
features. The other is the relationship between
mean squared errors and likelihood, similar to the
concern expressed in the previous paragraph.
These are important issues and require further in-
vestigation.
In summary, this paper presents our new im-
provement to the incremental feature selection al-
gorithm. The new algorithm runs hundreds to
thousands times faster than the original incre-
mental feature selection algorithm. In addition, the
new algorithm selects the features of a similar
quality as the original Berger et alalgorithm,
which has also shown to be better than the simple
cutoff method in some cases.
Acknowledgement
This work is done while the first author is visiting
the Center for Study of Language and Information
(CSLI) at Stanford University and the Research
and Technology Center of Robert Bosch Corpora-
tion. This project is sponsored by the Research and
Technology Center of Robert Bosch Corporation.
We are grateful to the kind support from Prof.
Stanley Peters of CSLI. We also thank the com-
ments from the three anonymous reviewers which
improve the quality of the paper.
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A Maximum Entropy Approach
to Natural Language Processing. Computational Lin-
guistic, 22 (1): 39-71.
Adam L. Berger and Harry Printz. 1998. A Comparison
of Criteria for Maximum Entropy / Minimum Diver-
gence Feature Selection. Proceedings of the 3
rd
 con-
ference on Empirical Methods in Natural Language
Processing. Granda, Spain.
Stanley Chen and Ronald Rosenfeld. 1999. Efficient
Sampling and Feature Selection in Whole Sentence
maximum Entropy Language Models. Proceedings of
ICASSP-1999, Phoenix, Arizona.
Joshua Goodman. 2002. Sequential Conditional Gener-
alized Iterative Scaling. Association for Computa-
tional Linguistics, Philadelphia, Pennsylvania.
Rob Koeling. 2000. Chunking with Maximum Entropy
Models. In: Proceedings of CoNLL-2000 and LLL-
2000, Lisbon, Portugal, 139-141.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution. Ph.D.
thesis, University of Pennsylvania.
Ronald Rosenfeld. 1994. Adaptive Statistical Language
Modeling: A Maximum Entropy Approach. Ph.D.
thesis, Carnegie Mellon University, April.
J. Reynar and A. Ratnaparkhi. 1997. A Maximum En-
tropy Approach to Identifying Sentence Boundaries.
In: Proceedings of the Fifth Conference on Applied
Natural Language Processing, Washington D.C., 16-
19.
Zhou Ya-qian, Guo Yi-kun, Huang Xuan-jing, and Wu
Li-de. 2003. Chinese and English BaseNP Recog-
nized by Maximum Entropy. Journal of Computer
Research and Development. 40(3):440-446, Beijin
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 904?912,
Beijing, August 2010
2D Trie for Fast Parsing
Xian Qian, Qi Zhang, Xuanjing Huang, Lide Wu
Institute of Media Computing
School of Computer Science, Fudan University
{xianqian, qz, xjhuang, ldwu}@fudan.edu.cn
Abstract
In practical applications, decoding speed
is very important. Modern structured
learning technique adopts template based
method to extract millions of features.
Complicated templates bring about abun-
dant features which lead to higher accu-
racy but more feature extraction time. We
propose Two Dimensional Trie (2D Trie),
a novel efficient feature indexing structure
which takes advantage of relationship be-
tween templates: feature strings generated
by a template are prefixes of the features
from its extended templates. We apply
our technique to Maximum Spanning Tree
dependency parsing. Experimental results
on Chinese Tree Bank corpus show that
our 2D Trie is about 5 times faster than
traditional Trie structure, making parsing
speed 4.3 times faster.
1 Introduction
In practical applications, decoding speed is very
important. Modern structured learning technique
adopts template based method to generate mil-
lions of features. Such as shallow parsing (Sha
and Pereira, 2003), named entity recognition
(Kazama and Torisawa, ), dependency parsing
(McDonald et al, 2005), etc.
The problem arises when the number of tem-
plates increases, more features generated, mak-
ing the extraction step time consuming. Espe-
cially for maximum spanning tree (MST) depen-
dency parsing, since feature extraction requires
quadratic time even using a first order model. Ac-
cording to Bohnet?s report (Bohnet, 2009), a fast
FeatureGenerationTemplate:p .word+p .pos0 0 Feature:lucky/ADJ
Index:3228~3233
FeatureRetrievalParse Tree
Build lattice, inference etc.
Figure 1: Flow chart of dependency parsing.
p0.word, p0.pos denotes the word and POS tag
of parent node respectively. Indexes correspond
to the features conjoined with dependency types,
e.g., lucky/ADJ/OBJ, lucky/ADJ/NMOD, etc.
feature extraction beside of a fast parsing algo-
rithm is important for the parsing and training
speed. He takes 3 measures for a 40X speedup,
despite the same inference algorithm. One impor-
tant measure is to store the feature vectors in file
to skip feature extraction, otherwise it will be the
bottleneck.
Now we quickly review the feature extraction
stage of structured learning. Typically, it consists
of 2 steps. First, features represented by strings
are generated using templates. Then a feature in-
dexing structure searches feature indexes to get
corresponding feature weights. Figure 1 shows
the flow chart of MST parsing, where p0.word,
p0.pos denote the word and POS tag of parent
node respectively.
We conduct a simple experiment to investi-
gate decoding time of MSTParser, a state-of-the-
art java implementation of dependency parsing 1.
Chinese Tree Bank 6 (CTB6) corpus (Palmer and
1http://sourceforge.net/projects/mstparser
904
Step Feature Index Other Total
Generation Retrieval
Time 300.27 61.66 59.48 421.41
Table 1: Time spent of each step (seconds) of
MSTParser on CTB6 standard test data (2660 sen-
tences). Details of the hardware and corpus are
described in section 5
Xue, 2009) with standard train/development/test
split is used for evaluation. Experimental results
are shown in Table 1. The observation is that time
spent of inference is trivial compared with feature
extraction. Thus, speeding up feature extraction is
critical especially when large template set is used
for high accuracy.
General indexing structure such as Hash and
Trie does not consider the relationships between
templates, therefore they could not speed up fea-
ture generation, and are not completely efficient
for searching feature indexes. For example, fea-
ture string s1 generated by template ?p0.word?
is prefix of feature s2 from template ?p0.word +
c0.word? (word pair of parent and child), hence
index of s1 could be used for searching s2. Fur-
ther more, if s1 is not in the feature set, then s2
must be absent, its generation can be skipped.
We propose Two Dimensional Trie (2D Trie),
a novel efficient feature indexing structure which
takes advantage of relationship between tem-
plates. We apply our technique to Maximum
Spanning Tree dependency parsing. Experimental
results on CTB6 corpus show that our 2D Trie is
about 5 times faster than traditional Trie structure,
making parsing speed 4.3 times faster.
The paper is structured as follows: in section 2,
we describe template tree which represents rela-
tionship between templates; in section 3, we de-
scribe our new 2D Trie structure; in section 4, we
analyze the complexity of the proposed method
and general string indexing structures for parsing;
experimental results are shown in section 5; we
conclude the work in section 6.
2 Template tree
2.1 Formulation of template
A template is a set of template units which are
manually designed: T = {t1, . . . , tm}. For con-
Unit Meaning
p?i/pi the ith node left/right to parent node
c?i/ci the ith node left/right to child node
r?i/ri the ith node left/right to root node
n.word word of node n
n.pos POS tag of node n
n.length word length of node n
|l conjoin current feature with linear distance
between child node and parent node
|d conjoin current feature with direction of de-
pendency (left/right)
Table 2: Template units appearing in this paper
venience, we use another formulation: T = t1 +
. . .+tm. All template units appearing in this paper
are described in Table 2, most of them are widely
used. For example, ?T = p0.word + c0.word|l ?
denotes the word pair of parent and child nodes,
conjoined with their distance.
2.2 Template tree
In the rest of the paper, for simplicity, let si be a
feature string generated by template Ti.
We define the relationship between templates:
T1 is the ancestor of T2 if and only T1 ? T2, and
T2 is called the descendant of T1. Recall that,
feature string s1 is prefix of feature s2. Suppose
T3 ? T1 ? T2, obviously, the most efficient way
to look up indexes of s1, s2, s3 is to search s3 first,
then use its index id3 to search s1, and finally use
id1 to search s2. Hence the relationship between
T2 and T3 can be neglected.
Therefore we define direct ancestor of T1: T2
is a direct ancestor of T1 if T2 ? T1, and there is
no template T ? such that T2 ? T ? ? T1. Corre-
spondingly, T1 is called the direct descendant of
T2.
Template graph G = (V,E) is a directed graph
that represents the relationship between templates,
where V = {T1, . . . , Tn} is the template set, E =
{e1, . . . , eN} is the edge set. Edge from Ti to Tj
exists, if and only if Ti is the direct ancestor of
Tj . For templates having no ancestor, we add an
empty template as their common direct ancestor,
which is also the root of the graph.
The left part of Figure 2 shows a template
graph for templates T1 =p0.word, T2 =p0.pos ,
T3 =p0.word + p0.pos. In this example, T3 has 2
direct ancestors, but in fact s3 has only one prefix
905
p .word0
p .word +p pos0 0.
root
p .word0
root
p .pos0
p .pos0 p .pos0
Figure 2: Left graph shows template graph for
T1 =p0.word, T2 =p0.pos , T3 =p0.word +
p0.pos. Right graph shows the corresponding tem-
plate tree, where each vertex saves the subset of
template units that do not belong to its father
which depends on the order of template units in
generation step. If s3 = s1 + s2, then its prefix is
s1, otherwise its prefix is s2. In this paper, we sim-
ply use the breadth-first tree of the graph for dis-
ambiguation, which is called template tree. The
only direct ancestor T1 of T2 in the tree is called
father of T2, and T2 is a child of T1. The right
part of Figure 2 shows the corresponding template
tree, where each vertex saves the subset of tem-
plate units that do not belong to its father.
2.3 Virtual vertex
Consider the template tree in the left part of Figure
3, red vertex and blue vertex are partially over-
lapped, their intersection is p0.word, if string s
from template T =p0.word is absent in feature set,
then both nodes can be neglected. For efficiently
pruning candidate templates, each vertex in tem-
plate tree is restricted to have exactly one template
unit (except root). Another important reason for
such restriction will be given in the next section.
To this end, virtual vertexes are created for
multi-unit vertexes. For efficient pruning, the new
virtual vertex should extract the most common
template unit. A natural goal is to minimize the
creation number. Here we use a simple greedy
strategy, for the vertexes sharing a common fa-
ther, the most frequent common unit is extracted
as new vertex. Virtual vertexes are iteratively cre-
ated in this way until all vertexes have one unit.
The final template tree is shown in the right part of
Figure 3, newly created virtual vertexes are shown
in dashed circle.
root
p .word+p .word+p .word-1 01
p .word+p pos0 0. c .word+c pos0 0.
root
p .word0
p .pos0 p .word-1
p .word1
c .word0
c .pos0
Figure 3: Templates that are partially overlapped:
Tred ? Tblue =p0.word, virtual vertexes shown in
dashed circle are created to extract the common
unit
root
p .word0
p .pos0
parse tag
VV NN... ... ... ...
.........
Level 0
Level 1
Level 2 VV ...
Figure 4: 2D Trie for single template, alphabets at
level 1 and level 2 are the word set, POS tag set
respectively
3 2D Trie
3.1 Single template case
Trie stores strings over a fixed alphabet, in our
case, feature strings are stored over several alpha-
bets, such as word list, POS tag list, etc. which are
extracted from training corpus.
To illustrate 2D Trie clearly, we first consider a
simple case, where only one template used. The
template tree degenerates to a sequence, we could
use a Trie like structure for feature indexing, the
only difference from traditional Trie is that nodes
at different levels could have different alphabets.
One example is shown in Figure 4. There are 3
feature strings from template ?p0.word + p0.pos?:
{parse/VV, tag/VV, tag/VV}. Alphabets at level
1 and level 2 are the word set, POS tag set re-
spectively, which are determined by correspond-
ing template vertexes.
As mentioned before, each vertex in template
tree has exactly one template unit, therefore, at
each level, we look up an index of a word or POS
906
HehadbeenasalesandmarketingexecutivewithChryslerfor20years
PRPVBDVBNDTNNSCCNNNNINNNPINCDNNS
2648273111210411506406313374192360231560220300566778
21272804130112120613060214
Figure 5: Look up indexes of words and POS tags
beforehand.
tag in sentence, not their combinations. Hence the
number of alphabets is limited, and all the indexes
could be searched beforehand for reuse, as shown
in Figure 5, the token table is converted to a in-
dex table. For example, when generating features
at position i of a sentence, template ?r0.word +
r1.word? requires index of i+1th word in the sen-
tence, which could be reused for generation at po-
sition i+ 1.
3.2 General case
Generally, for vertex in template tree with K chil-
dren, children of corresponding Trie node are ar-
ranged in a matrix of K rows and L columns, L
is the size of corresponding alphabet. If the vertex
is not virtual, i.e., it generates features, one more
row is added at the bottom to store feature indexes.
Figure 6 shows the 2D Trie for a general template
tree.
3.3 Feature extraction
When extracting features for a pair of nodes in a
sentence, template tree and 2D Trie are visited in
breath first traversal order. Each time, an alpha-
bet and a token index j from index table are se-
lected according to current vertex. For example,
POS tag set and the index of the POS tag of par-
ent node are selected as alphabet and token index
respectively for vertex ?p0.pos?. Then children in
the jth column of the Trie node are visited, valid
children and corresponding template vertexes are
saved for further retrieval or generate feature in-
dexes if the child is at the bottom and current Trie
node is not virtual. Two queues are maintained to
been......
... ......
...
VBNp .word+p .pos?been/VBN0 0...
... ......
p .word?been0... ...
root
root
p .word0p .pos0 c .word0
had ......
...
p .word?had0 ...
VBDp .word+p .pos?had/VBD0 0...
... ......
Hep .word+w .wordhad/He0 0?...
...
nmod vmodobj sub
Featureindex array-1 -13327 2510
nmod vmodobj sub-1 7821-1 -1............ ......
...... beenp .word+w .word?had/been0 0 ...
...
invalid
Figure 6: 2D trie for a general template tree.
Dashed boxes are keys of columns, which are not
stored in the structure
save the valid children and Trie nodes. Details of
feature extraction algorithm are described in Al-
gorithm 1.
3.4 Implementation
When feature set is very large, space complexity
of 2D Trie is expensive. Therefore, we use Double
Array Trie structure (Aoe, 1989) for implementa-
tion. Since children of 2D Trie node are arranged
in a matrix, not an array, so each element of the
base array has a list of bases, not one base in stan-
dard structure. For children that store features,
corresponding bases are feature indexes. One ex-
ample is shown in Figure 7. The root node has
3 bases that point to three rows of the child ma-
trix of vertex ?p0.word? respectively. Number of
bases in each element need not to be stored, since
it can be obtained from template vertex in extrac-
tion procedure.
Building algorithm is similarly to Double Array
Trie, when inserting a Trie node, each row of the
child matrix is independently insert into base and
check arrays using brute force strategy. The inser-
907
been
... been
...
...
...
had
...
had had...
... ...
...
...
been ... had had... ... ... ...... ... been hadroot base1 base2
base3
root base 2base
1
base3
...
VBD... ...
VBN...
...
...VBD
VBN
...
base1
base1 base1
base1
-1 -1... ... 3327 2510 ... ......... ......... -1 7821-1 -1... ...
-1 -1... ... 3327 2510 ... ......... ......... -1 7821-1 -1... ...
Basearray
Feature index array
Feature index array
Figure 7: Build base array for 2D Trie in Figure 6. String in the box represents the key of the child.
Blank boxes are the invalid children. The root node has 3 bases that point to three rows of the child
matrix of vertex ?p0.word? respectively
Algorithm 1 Feature extraction using 2D Trie
Input: 2D Trie that stores features, template
tree, template graph, a table storing token in-
dexes, parent and child positions
Output: Feature index set S of dependency
from parent to child.
Create template vertex queue Q1 and Trie
node queue Q2. Push roots of template tree
and Trie into Q1, Q2 respectively. S = ?
while Q1 is not empty, do
Pop a template vertex T from Q1 and a Trie
node N from Q2. Get token index j from
index table according to T .
for i = 1 to child number of T
if child of N at row i column j is valid,
push it into Q2 and push the ith child
of T into Q1.
else
remove decedents of ith child of T
from template tree
end if
end for
if T is not virtual and the last child of N in
column j is valid
Enumerate dependency types, add
valid feature indexes to S
end if
end while
Return S.
tion repeats recursively until all features stored.
4 Complexity analysis
Let
? |T | = number of templates
? |t| = number of template units
? |V | = number of vertexes in template tree,
i.e, |t|+ number of virtual vertexes
? |F | = number of features
? l = length of sentence
? |f | = average length of feature strings
The procedure of 2D Trie for feature extraction
consists of 2 steps: tokens in string table are
mapped to their indexes, then Algorithm 1 is car-
ried out for all node pairs of sentence. In the first
step, we use double array Trie for efficient map-
ping. In fact, time spent is trivial compared with
step 2 even by binary search. The main time spent
of Algorithm 1 is the traversal of the whole tem-
plate tree, in the worst case, no vertexes removed,
so the time complexity of a sentence is l2|V |,
which is proportional to |V |. In other words, mini-
mizing the number of virtual vertexes is important
for efficiency.
For other indexing structures, feature genera-
tion is a primary step of retrieval. For each node
908
Structure Generation Retrieval
2D Trie l2|V |
Hash / Trie l2|t| l2|f ||T |
Binary Search l2|t| l2|T | log |F |
Table 3: Time complexity of different indexing
structures.
pair of sentence, |t| template units are processed,
including concatenations of tokens and split sym-
bols (split tokens in feature strings), boundary
check ( e.g, p?1.word is out of boundary for be-
ginning node of sentence). Thus the generation
requires l2|t| processes. Notice that, time spent of
each process varies on the length of tokens.
For feature string s with length |s|, if perfect
hashing technique is adopted for index retrieval, it
takes |s| calculations to get hash value and a string
comparison to check the string at the calculated
position. So the time complexity is proportional to
|s|, which is the same as Trie. Hence the total time
for a sentence is l2|f ||T |. If binary search is used
instead, log |F | string comparisons are required,
complexity for a sentence is l2|T | log |F |.
Time complexity of these structures is summa-
rized in Table 3.
5 Experiments
5.1 Experimental settings
We use Chinese Tree Bank 6.0 corpus for evalua-
tion. The constituency structures are converted to
dependency trees by Penn2Malt 2 toolkit and the
standard training/development/test split is used.
257 sentences that failed in the conversion were
removed, yielding 23316 sentences for training,
2060 sentences for development and 2660 sen-
tences for testing respectively.
Since all the dependency trees are projective,
a first order projective MST parser is naturally
adopted. Online Passive Aggressive algorithm
(Crammer et al, 2006) is used for fast training, 2
parameters, i.e, iteration number and C, are tuned
on development data. The quality of the parser is
measured by the labeled attachment score (LAS),
i.e., the percentage of tokens with correct head and
dependency type.
2http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
Group IDs #Temp. #Vert. #Feat. LAS
1 1-2 72 91 3.23M 79.55%
2 1-3 128 155 10.4M 81.38%
3 1-4 240 275 25.0M 81.97%
4 1-5 332 367 34.8M 82.44%
Table 5: Parsing accuracy and number of tem-
plates, vertexes in template tree, features in decod-
ing stage (zero weighted features are excluded) of
each group.
We compare the proposed structure with Trie
and binary search. We do not compare with per-
fect hashing, because it has the same complex-
ity as Trie, and is often used for large data base
retrieval, since it requires only one IO opera-
tion. For easy comparison, all feature indexing
structures and the parser are implemented with
C++. All experiments are carried out on a 64bit
linux platform (CPU: Intel(R) Xeon(R) E5405,
2.00GHz, Memory: 16G Bytes). For each tem-
plate set, we run the parser five times on test data
and the averaged parsing time is reported.
5.2 Parsing speed comparison
To investigate the scalability of our method, rich
templates are designed to generate large feature
sets, as shown in Table 4. All templates are orga-
nized into 4 groups. Each row of Table 5 shows
the details of a group, including parsing accu-
racy and number of templates, vertexes in tem-
plate tree, and features in decoding stage (zero
weighted features are excluded).
There is a rough trend that parsing accuracy
increases as more templates used. Though such
trend is not completely correct, the clear conclu-
sion is that, abundant templates are necessary for
accurate parsing.
Though algorithm described in section 2.3 for
minimizing the number of virtual vertexes is
heuristic, empirical results are satisfactory, num-
ber of newly created vertexes is only 10% as orig-
inal templates. The reason is that complex tem-
plates are often extended from simple ones, their
differences are often one or two template units.
Results of parsing time comparison are shown
in Table 6. We can see that though time com-
plexity of dynamic programming is cubic, pars-
ing time of all systems is consistently dominated
909
ID Templates
1 pi.word pi.pos pi.word+pi.pos
ci.word ci.pos ci.word+ci.pos (|i| ? 2)
pi.length pi.length+pi.pos
ci.length ci.length+ci.pos (|i| ? 1)
p0.length+c0.length|ld p0.length+c0.length+c0.pos|ld p0.length+p0.pos+c0.length|ld
p0.length+p0.pos+c0.pos|ld p0.pos+c0.length+c0.pos|ld p0.length+p0.pos+c0.length+c0.pos|ld
pi.length+pj .length+ck.length+cm.length|ld (|i|+ |j|+ |k|+ |m| ? 2)r0.word r?1.word+r0.word r0.word+r1.word
r0.pos r?1.pos+r0.pos r0.pos+r1.pos
2 pi.pos+cj .pos|d pi.word+cj .word|d pi.pos+cj .word+cj .pos|d
pi.word+pi.pos+cj .pos|d pi.word+pi.pos+cj .word|d pi.word+cj .word+cj .pos|d
pi.word+pi.pos+cj .word+cj .pos|d (|i|+ |j| = 0)
Conjoin templates in the row above with |l
3 Similar with 2 |i|+ |j| = 1
4 Similar with 2 |i|+ |j| = 2
5 pi.word + pj .word + ck.word|d pi.word + cj .word + ck.word|d
pi.pos + pj .pos + ck.pos|d pi.pos + cj .pos + ck.pos|d (|i|+ |j|+ |k| ? 2)
Conjoin templates in the row above with |l
pi.word + pj .word + pk.word + cm.word|d pi.word + pj .word + ck.word + cm.word|d
pi.word + cj .word + ck.word + cm.word|d
pi.pos + pj .pos + pk.pos + cm.pos|d pi.pos + pj .pos + ck.pos + cm.pos|d
pi.pos + cj .pos + ck.pos + cm.pos|d (|i|+ |j|+ |k|+ |m| ? 2)
Conjoin templates in the row above with |l
Table 4: Templates used in Chinese dependency parsing.
by feature extraction. When efficient indexing
structure adopted, i.e, Trie or Hash, time index re-
trieval is greatly reduced, about 4-5 times faster
than binary search. However, general structures
search features independently, their results could
not guide feature generation. Hence, feature gen-
eration is still time consuming. The reason is that
processing each template unit includes a series of
steps, much slower than one integer comparison
in Trie search.
On the other hand, 2D Trie greatly reduces the
number of feature generations by pruning the tem-
plate graph. In fact, no string concatenation oc-
curs when using 2D Trie, since all tokens are con-
verted to indexes beforehand. The improvement
is significant, 2D Trie is about 5 times faster than
Trie on the largest feature set, yielding 13.4 sen-
tences per second parsing speed, about 4.3 times
faster.
Space requirement of 2D Trie is about 2.1 times
as binary search, and 1.7 times as Trie. One possi-
ble reason is that column number of 2D Trie (e.g.
size of words) is much larger than standard double
array Trie, which has only 256 children, i.e, range
of a byte. Therefore, inserting a 2D Trie node is
more strict, yielding sparser double arrays.
5.3 Comparison against state-of-the-art
Recent works on dependency parsing speedup
mainly focus on inference, such as expected
linear time non-projective dependency parsing
(Nivre, 2009), integer linear programming (ILP)
for higher order non-projective parsing (Martins
et al, 2009). They achieve 0.632 seconds per sen-
tence over several languages. On the other hand,
Goldberg and Elhadad proposed splitSVM (Gold-
berg and Elhadad, 2008) for fast low-degree poly-
nomial kernel classifiers, and applied it to transi-
tion based parsing (Nivre, 2003). They achieve
53 sentences per second parsing speed on En-
glish corpus, which is faster than our results, since
transition based parsing is linear time, while for
graph based method, complexity of feature ex-
traction is quadratic. Xavier Llu??s et al (Llu??s
et al, 2009) achieve 8.07 seconds per sentence
speed on CoNLL09 (Hajic? et al, 2009) Chinese
Tree Bank test data with a second order graphic
model. Bernd Bohnet (Bohnet, 2009) also uses
second order model, and achieves 610 minutes on
CoNLL09 English data (2399 sentences, 15.3 sec-
ond per sentence). Although direct comparison
of parsing time is difficult due to the differences
in data, models, hardware and implementations,
910
Group Structure Total Generation Retrieval Other Memory sent/sec
Trie 87.39 63.67 10.33 13.39 402M 30.44
1 Binary Search 127.84 62.68 51.52 13.64 340M 20.81
2D Trie 39.74 26.29 13.45 700M 66.94
Trie 264.21 205.19 39.74 19.28 1.3G 10.07
2 Binary Search 430.23 212.50 198.72 19.01 1.2G 6.18
2D Trie 72.81 53.95 18.86 2.5G 36.53
Trie 620.29 486.40 105.96 27.93 3.2G 4.29
3 Binary Search 982.41 484.62 469.44 28.35 2.9G 2.71
2D Trie 146.83 119.56 27.27 5.9G 18.12
Trie 854.04 677.32 139.70 37.02 4.9G 3.11
4 Binary Search 1328.49 680.36 609.70 38.43 4.1G 2.00
2D Trie 198.31 160.38 37.93 8.6G 13.41
Table 6: Parsing time of 2660 sentences (seconds) on a 64bit linux platform (CPU: Intel(R) Xeon(R)
E5405, 2.00GHz, Memory: 16G Bytes). Title ?Generation? and ?Retrieval? are short for feature gen-
eration and feature index retrieval steps respectively.
System sec/sent
(Martins et al, 2009) 0.63
(Goldberg and Elhadad, 2008) 0.019
(Llu??s et al, 2009) 8.07
(Bohnet, 2009) 15.3
(Galley and Manning, 2009) 15.6
ours group1 0.015
ours group2 0.027
ours group3 0.055
ours group4 0.075
Table 7: Comparison against state of the art, di-
rect comparison of parsing time is difficult due to
the differences in data, models, hardware and im-
plementations.
these results demonstrate that our structure can
actually result in a very fast implementation of a
parser. Moreover, our work is orthogonal to oth-
ers, and could be used for other learning tasks.
6 Conclusion
We proposed 2D Trie, a novel feature indexing
structure for fast template based feature extrac-
tion. The key insight is that feature strings gener-
ated by a template are prefixes of the features from
its extended templates, hence indexes of searched
features can be reused for further extraction. We
applied 2D Trie to dependency parsing task, ex-
perimental results on CTB corpus demonstrate the
advantages of our technique, about 5 times faster
than traditional Trie structure, yielding parsing
speed 4.3 times faster, while using only 1.7 times
as much memory.
7 Acknowledgments
The author wishes to thank the anonymous
reviewers for their helpful comments. This
work was partially funded by 973 Program
(2010CB327906), The National High Technol-
ogy Research and Development Program of China
(2009AA01A346), Shanghai Leading Academic
Discipline Project (B114), Doctoral Fund of Min-
istry of Education of China (200802460066), and
Shanghai Science and Technology Development
Funds (08511500302).
References
Aoe, Jun?ichi. 1989. An efficient digital
search algorithm by using a double-array struc-
ture. IEEE Transactions on software andengineer-
ing, 15(9):1066?1077.
Bohnet, Bernd. 2009. Efficient parsing of syntactic
and semantic dependency structures. In Proceed-
ings of the Thirteenth Conference on Computational
Natural Language Learning (CoNLL 2009): Shared
Task, pages 67?72, Boulder, Colorado, June. Asso-
ciation for Computational Linguistics.
Crammer, Koby, Joseph Keshet, Shai Shalev-Shwartz,
and Yoram Singer. 2006. Online passive-aggressive
algorithms. In JMLR 2006.
911
Galley, Michel and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine
translation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 773?781,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Goldberg, Yoav and Michael Elhadad. 2008. splitsvm:
Fast, space-efficient, non-heuristic, polynomial ker-
nel computation for nlp applications. In Proceed-
ings of ACL-08: HLT, Short Papers, pages 237?240,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Hajic?, Jan, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the
Thirteenth Conference on Computational Natural
Language Learning (CoNLL 2009): Shared Task,
pages 1?18, Boulder, Colorado, June. Association
for Computational Linguistics.
Kazama, Jun?ichi and Kentaro Torisawa. A new per-
ceptron algorithm for sequence labeling with non-
local features. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 315?324.
Llu??s, Xavier, Stefan Bott, and Llu??s Ma`rquez. 2009.
A second-order joint eisner model for syntactic and
semantic dependency parsing. In Proceedings of the
Thirteenth Conference on Computational Natural
Language Learning (CoNLL 2009): Shared Task,
pages 79?84, Boulder, Colorado, June. Association
for Computational Linguistics.
Martins, Andre, Noah Smith, and Eric Xing. 2009.
Concise integer linear programming formulations
for dependency parsing. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 342?
350, Suntec, Singapore, August. Association for
Computational Linguistics.
McDonald, Ryan, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics, pages 91?97. Association for Computa-
tional Linguistics.
Nivre, Joakim. 2003. An efficient algorithm for
projective dependency parsing. In Proceedings of
the 11th International Conference on Parsing Tech-
niques, pages 149?160.
Nivre, Joakim. 2009. Non-projective dependency
parsing in expected linear time. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
pages 351?359, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.
Palmer, Martha and Nianwen Xue. 2009. Adding se-
mantic roles to the Chinese Treebank. Natural Lan-
guage Engineering, 15(1):143?172.
Sha, Fei and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings
of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 134?141,
May.
912
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 187?195,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Joint Training and Decoding Using Virtual Nodes for Cascaded
Segmentation and Tagging Tasks
Xian Qian, Qi Zhang, Yaqian Zhou, Xuanjing Huang, Lide Wu
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, P.R.China
{qianxian, qz, zhouyaqian, xjhuang, ldwu}@fudan.edu.cn
Abstract
Many sequence labeling tasks in NLP require
solving a cascade of segmentation and tag-
ging subtasks, such as Chinese POS tagging,
named entity recognition, and so on. Tradi-
tional pipeline approaches usually suffer from
error propagation. Joint training/decoding in
the cross-product state space could cause too
many parameters and high inference complex-
ity. In this paper, we present a novel method
which integrates graph structures of two sub-
tasks into one using virtual nodes, and per-
forms joint training and decoding in the fac-
torized state space. Experimental evaluations
on CoNLL 2000 shallow parsing data set and
Fourth SIGHAN Bakeoff CTB POS tagging
data set demonstrate the superiority of our
method over cross-product, pipeline and can-
didate reranking approaches.
1 Introduction
There is a typical class of sequence labeling tasks
in many natural language processing (NLP) applica-
tions, which require solving a cascade of segmenta-
tion and tagging subtasks. For example, many Asian
languages such as Japanese and Chinese which
do not contain explicitly marked word boundaries,
word segmentation is the preliminary step for solv-
ing part-of-speech (POS) tagging problem. Sen-
tences are firstly segmented into words, then each
word is assigned with a part-of-speech tag. Both
syntactic parsing and dependency parsing usually
start with a textual input that is tokenized, and POS
tagged.
The most commonly approach solves cascaded
subtasks in a pipeline, which is very simple to im-
plement and allows for a modular approach. While,
the key disadvantage of such method is that er-
rors propagate between stages, significantly affect-
ing the quality of the final results. To cope with this
problem, Shi and Wang (2007) proposed a rerank-
ing framework in which N-best segment candidates
generated in the first stage are passed to the tag-
ging model, and the final output is the one with the
highest overall segmentation and tagging probabil-
ity score. The main drawback of this method is that
the interaction between tagging and segmentation is
restricted by the number of candidate segmentation
outputs. Razvan C. Bunescu (2008) presented an
improved pipeline model in which upstream subtask
outputs are regarded as hidden variables, together
with their probabilities are used as probabilistic fea-
tures in the downstream subtasks. One shortcom-
ing of this method is that calculation of marginal
probabilities of features may be inefficient and some
approximations are required for fast computation.
Another disadvantage of these two methods is that
they employ separate training and the segmentation
model could not take advantages of tagging infor-
mation in the training procedure.
On the other hand, joint learning and decoding
using cross-product of segmentation states and tag-
ging states does not suffer from error propagation
problem and achieves higher accuracy on both sub-
tasks (Ng and Low, 2004). However, two problems
arises due to the large state space, one is that the
amount of parameters increases rapidly, which is apt
to overfit on the training corpus, the other is that
the inference by dynamic programming could be in-
efficient. Sutton (2004) proposed Dynamic Con-
ditional Random Fields (DCRFs) to perform joint
training/decoding of subtasks using much fewer pa-
rameters than the cross-product approach. How-
187
ever, DCRFs do not guarantee non-violation of hard-
constraints that nodes within the same segment get
a single consistent tagging label. Another draw-
back of DCRFs is that exact inference is generally
time consuming, some approximations are required
to make it tractable.
Recently, perceptron based learning framework
has been well studied for incorporating node level
and segment level features together (Kazama and
Torisawa, 2007; Zhang and Clark, 2008). The main
shortcoming is that exact inference is intractable
for those dynamically generated segment level fea-
tures, so candidate based searching algorithm is
used for approximation. On the other hand, Jiang
(2008) proposed a cascaded linear model which has
a two layer structure, the inside-layer model uses
node level features to generate candidates with their
weights as inputs of the outside layer model which
captures non-local features. As pipeline models, er-
ror propagation problem exists for such method.
In this paper, we present a novel graph structure
that exploits joint training and decoding in the fac-
torized state space. Our method does not suffer
from error propagation, and guards against viola-
tions of those hard-constraints imposed by segmen-
tation subtask. The motivation is to integrate two
Markov chains for segmentation and tagging sub-
tasks into a single chain, which contains two types of
nodes, then standard dynamic programming based
exact inference is employed on the hybrid struc-
ture. Experiments are conducted on two different
tasks, CoNLL 2000 shallow parsing and SIGHAN
2008 Chinese word segmentation and POS tagging.
Evaluation results of shallow parsing task show
the superiority of our proposed method over tradi-
tional joint training/decoding approach using cross-
product state space, and achieves the best reported
results when no additional resources at hand. For
Chinese word segmentation and POS tagging task, a
strong baseline pipeline model is built, experimental
results show that the proposed method yields a more
substantial improvement over the baseline than can-
didate reranking approach.
The rest of this paper is organized as follows: In
Section 2, we describe our novel graph structure. In
Section 3, we analyze complexity of our proposed
method. Experimental results are shown in Section
4. We conclude the work in Section 5.
2 Multi-chain integration using Virtual
Nodes
2.1 Conditional Random Fields
We begin with a brief review of the Conditional Ran-
dom Fields(CRFs). Let x = x1x2 . . . xl denote the
observed sequence, where xi is the ith node in the
sequence, l is sequence length, y = y1y2 . . . yl is a
label sequence over x that we wish to predict. CRFs
(Lafferty et al, 2001) are undirected graphic mod-
els that use Markov network distribution to learn the
conditional probability. For sequence labeling task,
linear chain CRFs are very popular, in which a first
order Markov assumption is made on the labels:
p(y|x) = 1Z(x)
?
i
?(x,y, i)
,where
?(x,y, i) = exp
(
wT f(x, yi?1, yi, i)
)
Z(x) =
?
y
?
i
?(x,y, i)
f(x, yi?1, yi, i) =
[f1(x, yi?1, yi, i), . . .,fm(x, yi?1, yi, i)]T , each ele-
ment fj(x, yi?1, yi, i) is a real valued feature func-
tion, here we simplify the notation of state feature
by writing fj(x, yi, i) = fj(x, yi?1, yi, i), m is the
cardinality of feature set {fj}. w = [w1, . . . , wm]T
is a weight vector to be learned from the training
set. Z(x) is the normalization factor over all label
sequences for x.
In the traditional joint training/decoding approach
for cascaded segmentation and tagging task, each
label yi has the form si-ti, which consists of seg-
mentation label si and tagging label ti. Let s =
s1s2 . . . sl be the segmentation label sequence over
x. There are several commonly used label sets such
as BI, BIO, IOE, BIES, etc. To facilitate our dis-
cussion, in later sections we will use BIES label set,
where B,I,E represents Beginning, Inside and End of
a multi-node segment respectively, S denotes a sin-
gle node segment. Let t = t1t2 . . . tl be the tagging
label sequence over x. For example, in named entity
recognition task, ti ? {PER, LOC, ORG, MISC,
O} represents an entity type (person name, loca-
tion name, organization name, miscellaneous entity
188
x2
s?t
2
2
x1
s?t
1
1
S-P S-O
x3
s?t
3
3
S-O
x4
s?t
4
4
B-P
x5
s?t
5
5
E-P
Hendrix ?s girlfriend Kathy Etchingham
Figure 1: Graphical representation of linear chain CRFs
for traditional joint learning/decoding
name and other). Graphical representation of lin-
ear chain CRFs is shown in Figure 1, where tagging
label ?P? is the simplification of ?PER?. For nodes
that are labeled as other, we define si =S, ti =O.
2.2 Hybrid structure for cascaded labeling
tasks
Different from traditional joint approach, our
method integrates two linear markov chains for seg-
mentation and tagging subtasks into one that con-
tains two types of nodes. Specifically, we first
regard segmentation and tagging as two indepen-
dent sequence labeling tasks, corresponding chain
structures are built, as shown in the top and mid-
dle sub-figures of Figure 2. Then a chain of twice
length of the observed sequence is built, where
nodes x1, . . . , xl on the even positions are original
observed nodes, while nodes v1, . . . , vl on the odd
positions are virtual nodes that have no content in-
formation. For original nodes xi, the state space is
the tagging label set, while for virtual nodes, their
states are segmentation labels. The label sequence
of the hybrid chain is y = y1 . . . y2l = s1t1 . . . sltl,
where combination of consecutive labels siti repre-
sents the full label for node xi.
Then we let si be connected with si?1 and si+1
, so that first order Markov assumption is made
on segmentation states. Similarly, ti is connected
with ti?1 and ti+1. Then neighboring tagging and
segmentation states are connected as shown in the
bottom sub-figure of Figure 2. Non-violation of
hard-constraints that nodes within the same seg-
ment get a single consistent tagging label is guar-
anteed by introducing second order transition fea-
tures f(ti?1, si, ti, i) that are true if ti?1 6= ti and
si ? {I,E}. For example, fj(ti?1, si, ti, i) is de-
fined as true if ti?1 =PER, si =I and ti =LOC.
In other words, it is true, if a segment is partially
tagging as PER, and partially tagged as LOC. Since
such features are always false in the training corpus,
their corresponding weights will be very low so that
inconsistent label assignments impossibly appear in
decoding procedure. The hybrid graph structure can
be regarded as a special case of second order Markov
chain.
Hendrix ?s girlfriend Kathy Etchingham
x1 x2 x3 x4 x5
s1 s2 s3 s4 s5
S S S B E
x1 x2 x3 x4 x5
t1 t2 t3 t4 t5
P O O P P
x1 x2 x3 x4 x5
t1 t2 t3 t4 t5
P O O P P
s2s1 s3 s4 s5
S S S B E
v1 v2 v3 v4 v5
Integrate
Figure 2: Multi-chain integration using Virtual Nodes
2.3 Factorized features
Compared with traditional joint model that exploits
cross-product state space, our hybrid structure uses
factorized states, hence could handle more flexible
features. Any state feature g(x, yi, i) defined in
the cross-product state space can be replaced by a
first order transition feature in the factorized space:
f(x, si, ti, i). As for the transition features, we
use f(si?1, ti?1, si, i) and f(ti?1, si, ti, i) instead
of g(yi?1, yi, i) in the conventional joint model.
Features in cross-product state space require that
segmentation label and tagging label take on partic-
ular values simultaneously, however, sometimes we
189
want to specify requirement on only segmentation or
tagging label. For example, ?Smith? may be an end
of a person name, ?Speaker: John Smith?; or a sin-
gle word person name ?Professor Smith will . . . ?. In
such case, our observation is that ?Smith? is likely a
(part of) person name, we do not care about its seg-
mentation label. So we could define state feature
f(x, ti, i) = true, if xi is ?Smith? with tagging la-
bel ti=PER.
Further more, we could define features like
f(x, ti?1, ti, i), f(x, si?1, si, i), f(x, ti?1, si, i),
etc. The hybrid structure facilitates us to use
varieties of features. In the remainder of the
paper, we use notations f(x, ti?1, si, ti, i) and
f(x, si?1, ti?1, si, i) for simplicity.
2.4 Hybrid CRFs
A hybrid CRFs is a conditional distribution that fac-
torizes according to the hybrid graphical model, and
is defined as:
p(s, t|x) = 1Z(x)
?
i
?(x, s, t, i)
?
i
?(x, s, t, i)
Where
?(x, s, t, i) = exp
(
wT1 f(x, si?1, ti?1, si)
)
?(x, s, t, i) = exp
(
wT2 f(x, ti?1, si, ti)
)
Z(x) =
?
s,t
(?
i
?(x, s, t, i)
?
i
?(x, s, t, i)
)
Where w1, w2 are weight vectors.
Luckily, unlike DCRFs, in which graph structure
can be very complex, and the cross-product state
space can be very large, in our cascaded labeling
task, the segmentation label set is often small, so
far as we known, the most complicated segmenta-
tion label set has only 6 labels (Huang and Zhao,
2007). So exact dynamic programming based algo-
rithms can be efficiently performed.
In the training stage, we use second order forward
backward algorithm to compute the marginal proba-
bilities p(x, si?1, ti?1, si) and p(x, ti?1, si, ti), and
the normalization factor Z(x). In decoding stage,
we use second order Viterbi algorithm to find the
best label sequence. The Viterbi decoding can be
Table 1: Time Complexity
Method Training Decoding
Pipeline (|S|2cs + |T |2ct)L (|S|2 + |T |2)U
Cross-Product (|S||T |)2cL (|S||T |)2U
Reranking (|S|2cs + |T |2ct)L (|S|2 + |T |2)NU
Hybrid (|S| + |T |)|S||T |cL (|S| + |T |)|S||T |U
used to label a new sequence, and marginal compu-
tation is used for parameter estimation.
3 Complexity Analysis
The time complexity of the hybrid CRFs train-
ing and decoding procedures is higher than that of
pipeline methods, but lower than traditional cross-
product methods. Let
? |S| = size of the segmentation label set.
? |T | = size of the tagging label set.
? L = total number of nodes in the training data
set.
? U = total number of nodes in the testing data
set.
? c = number of joint training iterations.
? cs = number of segmentation training itera-
tions.
? ct = number of tagging training iterations.
? N = number of candidates in candidate rerank-
ing approach.
Time requirements for pipeline, cross-product, can-
didate reranking and hybrid CRFs are summarized
in Table 1. For Hybrid CRFs, original node xi has
features {fj(ti?1, si, ti)}, accessing all label subse-
quences ti?1siti takes |S||T |2 time, while virtual
node vi has features {fj(si?1, ti?1, si)}, accessing
all label subsequences si?1ti?1si takes |S|2|T | time,
so the final complexity is (|S|+ |T |)|S||T |cL.
In real applications, |S| is small, |T | could be
very large, we assume that |T | >> |S|, so for
each iteration, hybrid CRFs is about |S| times slower
than pipeline and |S| times faster than cross-product
190
Table 2: Feature templates for shallow parsing task
Cross Product CRFs Hybrid CRFs
wi?2yi, wi?1yi, wiyi wi?1si, wisi, wi+1si
wi+1yi, wi+2yi wi?2ti, wi?1ti, witi, wi+1ti, wi+2ti
wi?1wiyi, wiwi+1yi wi?1wisi, wiwi+1si
wi?1witi, wiwi+1ti
pi?2yi, pi?1yi, piyi pi?1si, pisi, pi+1si
pi+1yi, pi+2yi pi?2ti, pi?1ti, pi+1ti, pi+2ti
pi?2pi?1yi, pi?1piyi, pipi+1yi,
pi+1pi+2yi
pi?2pi?1si, pi?1pisi, pipi+1si, pi+1pi+2si
pi?3pi?2ti, pi?2pi?1ti, pi?1piti, pipi+1ti,
pi+1pi+2ti, pi+2pi+3ti, pi?1pi+1ti
pi?2pi?1piyi, pi?1pipi+1yi,
pipi+1pi+2yi
pi?2pi?1pisi, pi?1pipi+1si, pipi+1pi+2si
wipiti
wisi?1si
wi?1ti?1ti, witi?1ti, pi?1ti?1ti, piti?1ti
yi?1yi si?1ti?1si, ti?1siti
method. When decoding, candidate reranking ap-
proach requires more time if candidate number N >
|S|.
Though the space complexity could not be com-
pared directly among some of these methods, hybrid
CRFs require less parameters than cross-product
CRFs due to the factorized state space. This is sim-
ilar with factorized CRFs (FCRFs) (Sutton et al,
2004).
4 Experiments
4.1 Shallow Parsing
Our first experiment is the shallow parsing task. We
use corpus from CoNLL 2000 shared task, which
contains 8936 sentences for training and 2012 sen-
tences for testing. There are 11 tagging labels: noun
phrase(NP), verb phrase(VP) , . . . and other (O), the
segmentation state space we used is BIES label set,
since we find that it yields a little improvement over
BIO set.
We use the standard evaluation metrics, which are
precision P (percentage of output phrases that ex-
actly match the reference phrases), recall R (percent-
age of reference phrases returned by our system),
and their harmonic mean, the F1 score F1 = 2PRP+R
(which we call F score in what follows).
We compare our approach with traditional cross-
product method. To find good feature templates,
development data are required. Since CoNLL2000
does not provide development data set, we divide
the training data into 10 folds, of which 9 folds for
training and 1 fold for developing. After selecting
feature templates by cross validation, we extract fea-
tures and learn their weights on the whole training
data set. Feature templates are summarized in Table
2, where wi denotes the ith word, pi denotes the ith
POS tag.
Notice that in the second row, feature templates
of the hybrid CRFs does not contain wi?2si, wi+2si,
since we find that these two templates degrade per-
formance in cross validation. However, wi?2ti,
wi+2ti are useful, which implies that the proper con-
text window size for segmentation is smaller than
tagging. Similarly, for hybrid CRFs, the window
size of POS bigram features for segmentation is 5
(from pi?2 to pi+2, see the eighth row in the sec-
ond column); while for tagging, the size is 7 (from
pi?3 to pi+3, see the ninth row in the second col-
umn). However for cross-product method, their win-
dow sizes must be consistent.
For traditional cross-product CRFs and our hybrid
CRFs, we use fixed gaussian prior ? = 1.0 for both
methods, we find that this parameter does not signifi-
191
Table 3: Results for shallow parsing task, Hybrid CRFs
significantly outperform Cross-Product CRFs (McNe-
mar?s test; p < 0.01)
Method Cross-Product
CRFs
Hybrid
CRFs
Training Time 11.6 hours 6.3 hours
Feature Num-
ber
13 million 10 mil-
lion
Iterations 118 141
F1 93.88 94.31
cantly affect the results when it varies between 1 and
10. LBFGS(Nocedal and Wright, 1999) method is
employed for numerical optimization. Experimen-
tal results are shown in Table 3. Our proposed CRFs
achieve a performance gain of 0.43 points in F-score
over cross-product CRFs that use state space while
require less training time.
For comparison, we also listed the results of pre-
vious top systems, as shown in Table 4. Our pro-
posed method outperforms other systems when no
additional resources at hand. Though recently semi-
supervised learning that incorporates large mounts
of unlabeled data has been shown great improve-
ment over traditional supervised methods, such as
the last row in Table 4, supervised learning is funda-
mental. We believe that combination of our method
and semi-supervised learning will achieve further
improvement.
4.2 Chinese word segmentation and POS
tagging
Our second experiment is the Chinese word seg-
mentation and POS tagging task. To facilitate com-
parison, we focus only on the closed test, which
means that the system is trained only with a des-
ignated training corpus, any extra knowledge is not
allowed, including Chinese and Arabic numbers, let-
ters and so on. We use the Chinese Treebank (CTB)
POS corpus from the Fourth International SIGHAN
Bakeoff data sets (Jin and Chen, 2008). The train-
ing data consist of 23444 sentences, 642246 Chinese
words, 1.05M Chinese characters and testing data
consist of 2079 sentences, 59955 Chinese words,
0.1M Chinese characters.
We compare our hybrid CRFs with pipeline and
candidate reranking methods (Shi and Wang, 2007)
Table 4: Comparison with other systems on shallow pars-
ing task
Method F1 Additional Re-
sources
Cross-Product CRFs 93.88
Hybrid CRFs 94.31
SVM combination 93.91
(Kudo and Mat-
sumoto, 2001)
Voted Perceptrons 93.74 none
(Carreras and Mar-
quez, 2003)
ETL (Milidiu et al,
2008)
92.79
(Wu et al, 2006) 94.21 Extended features
such as token fea-
tures, affixes
HySOL 94.36 17M words unla-
beled
(Suzuki et al, 2007) data
ASO-semi 94.39 15M words unla-
beled
(Ando and Zhang,
2005)
data
(Zhang et al, 2002) 94.17 full parser output
(Suzuki and Isozaki,
2008)
95.15 1G words unla-
beled data
using the same evaluation metrics as shallow pars-
ing. We do not compare with cross-product CRFs
due to large amounts of parameters.
For pipeline method, we built our word segmenter
based on the work of Huang and Zhao (2007),
which uses 6 label representation, 7 feature tem-
plates (listed in Table 5, where ci denotes the ith
Chinese character in the sentence) and CRFs for pa-
rameter learning. We compare our segmentor with
other top systems using SIGHAN CTB corpus and
evaluation metrics. Comparison results are shown
in Table 6, our segmenter achieved 95.12 F-score,
which is ranked 4th of 26 official runs. Except for
the first system which uses extra unlabeled data, dif-
ferences between rest systems are not significant.
Our POS tagging system is based on linear chain
CRFs. Since SIGHAN dose not provide develop-
ment data, we use the 10 fold cross validation de-
scribed in the previous experiment to turning feature
templates and Gaussian prior. Feature templates are
listed in Table 5, where wi denotes the ith word in
192
Table 5: Feature templates for Chinese word segmentation and POS tagging task
Segmentation feature templates
(1.1) ci?2si, ci?1si, cisi, ci+1si, ci+2si
(1.2) ci?1cisi, cici+1si, ci?1ci+1si
(1.3) si?1si
POS tagging feature templates
(2.1) wi?2ti, wi?1ti, witi, wi+1ti, wi+2ti
(2.2) wi?2wi?1ti, wi?1witi, wiwi+1ti, wi+1wi+2ti, wi?1wi+1ti
(2.3) c1(wi)ti, c2(wi)ti, c3(wi)ti, c?2(wi)ti, c?1(wi)ti
(2.4) c1(wi)c2(wi)ti, c?2(wi)c?1(wi)ti
(2.5) l(wi)ti
(2.6) ti?1ti
Joint segmentation and POS tagging feature templates
(3.1) ci?2si, ci?1si, cisi, ci+1si, ci+2si
(3.2) ci?1cisi, cici+1si, ci?1ci+1si
(3.3) ci?3ti, ci?2ti, ci?1ti, citi, ci+1ti, ci+2ti, ci+3ti
(3.4) ci?3ci?2ti, ci?2ci?1ti, ci?1citi, cici+1ti ci+1ci+2ti, ci+2ci+3ti, ci?2citi, cici+2ti
(3.5) cisiti
(3.6) citi?1ti
(3.7) si?1ti?1si, ti?1siti
Table 6: Word segmentation results on Fourth SIGHAN
Bakeoff CTB corpus
Rank F1 Description
1/26 95.89? official best, using extra un-
labeled data (Zhao and Kit,
2008)
2/26 95.33 official second
3/26 95.17 official third
4/26 95.12 segmentor in pipeline sys-
tem
Table 7: POS results on Fourth SIGHAN Bakeoff CTB
corpus
Rank Accuracy Description
1/7 94.29 POS tagger in pipeline sys-
tem
2/7 94.28 official best
3/7 94.01 official second
4/7 93.24 official third
the sentence, cj(wi), j > 0 denotes the jth Chinese
character of word wi, cj(wi), j < 0 denotes the jth
last Chinese character, l(wi) denotes the word length
of wi. We compare our POS tagger with other top
systems on Bakeoff CTB POS corpus where sen-
tences are perfectly segmented into words, our POS
tagger achieved 94.29 accuracy, which is the best of
7 official runs. Comparison results are shown in Ta-
ble 7.
For reranking method, we varied candidate num-
bers n among n ? {10, 20, 50, 100}. For hybrid
CRFs, we use the same segmentation label set as
the segmentor in pipeline. Feature templates are
listed in Table 5. Experimental results are shown
in Figure 3. The gain of hybrid CRFs over the
baseline pipeline model is 0.48 points in F-score,
about 3 times higher than 100-best reranking ap-
proach which achieves 0.13 points improvement.
Though larger candidate number can achieve higher
performance, such improvement becomes trivial for
n > 20.
Table 8 shows the comparison between our work
and other relevant work. Notice that, such com-
parison is indirect due to different data sets and re-
193
0 20 40 60 80 10090.3
90.4
90.5
90.6
90.7
90.8
90.9
candidate number
F s
cor
e
 
 
candidate reranking
Hybrid CRFs
Figure 3: Results for Chinese word segmentation and
POS tagging task, Hybrid CRFs significantly outperform
100-Best Reranking (McNemar?s test; p < 0.01)
Table 8: Comparison of word segmentation and POS tag-
ging, such comparison is indirect due to different data
sets and resources.
Model F1
Pipeline (ours) 90.40
100-Best Reranking (ours) 90.53
Hybrid CRFs (ours) 90.88
Pipeline (Shi and Wang, 2007) 91.67
20-Best Reranking (Shi and Wang,
2007)
91.86
Pipeline (Zhang and Clark, 2008) 90.33
Joint Perceptron (Zhang and Clark,
2008)
91.34
Perceptron Only (Jiang et al, 2008) 92.5
Cascaded Linear (Jiang et al, 2008) 93.4
sources. One common conclusion is that joint mod-
els generally outperform pipeline models.
5 Conclusion
We introduced a framework to integrate graph struc-
tures for segmentation and tagging subtasks into one
using virtual nodes, and performs joint training and
decoding in the factorized state space. Our approach
does not suffer from error propagation, and guards
against violations of those hard-constraints imposed
by segmentation subtask. Experiments on shal-
low parsing and Chinese word segmentation tasks
demonstrate our technique.
6 Acknowledgements
The author wishes to thank the anonymous review-
ers for their helpful comments. This work was
partially funded by 973 Program (2010CB327906),
The National High Technology Research and De-
velopment Program of China (2009AA01A346),
Shanghai Leading Academic Discipline Project
(B114), Doctoral Fund of Ministry of Education of
China (200802460066), National Natural Science
Funds for Distinguished Young Scholar of China
(61003092), and Shanghai Science and Technology
Development Funds (08511500302).
References
R. Ando and T. Zhang. 2005. A high-performance semi-
supervised learning method for text chunking. In Pro-
ceedings of ACL, pages 1?9.
Razvan C. Bunescu. 2008. Learning with probabilistic
features for improved pipeline models. In Proceedings
of EMNLP, Waikiki, Honolulu, Hawaii.
X Carreras and L Marquez. 2003. Phrase recognition by
filtering and ranking with perceptrons. In Proceedings
of RANLP.
Changning Huang and Hai Zhao. 2007. Chinese word
segmentation: A decade review. Journal of Chinese
Information Processing, 21:8?19.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu.
2008. A cascaded linear model for joint chinese word
segmentation and part-of-speech tagging. In Proceed-
ings of ACL, Columbus, Ohio, USA.
Guangjin Jin and Xiao Chen. 2008. The fourth interna-
tional chinese language processing bakeoff: Chinese
word segmentation, named entity recognition and chi-
nese pos tagging. In Proceedings of Sixth SIGHAN
Workshop on Chinese Language Processing, India.
Junichi Kazama and Kentaro Torisawa. 2007. A new
perceptron algorithm for sequence labeling with non-
local features. In Proceedings of EMNLP, pages 315?
324, Prague, June.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML.
Ruy L. Milidiu, Cicero Nogueira dos Santos, and Julio C.
Duarte. 2008. Phrase chunking using entropy guided
transformation learning. In Proceedings of ACL, pages
647?655.
194
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
ofspeech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Proceedings of EMNLP.
J. Nocedal and S. J. Wright. 1999. Numerical Optimiza-
tion. Springer.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer crfs
based joint decoding method for cascaded segmenta-
tion and labeling tasks. In Proceedings of IJCAI, pages
1707?1712, Hyderabad, India.
C. Sutton, K. Rohanimanesh, and A. McCallum. 2004.
Dynamic conditional random fields: Factorized prob-
abilistic models for labeling and segmenting sequence
data. In Proceedings of ICML.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-word
scale unlabeled data. In Proceedings of ACL, pages
665?673.
Jun Suzuki, Akinori Fujino, and Hideki Isozaki. 2007.
Semi-supervised structured output learning based on
a hybrid generative and discriminative approach. In
Proceedings of EMNLP, Prague.
Yu-Chieh Wu, Chia-Hui Chang, and Yue-Shi Lee. 2006.
A general and multi-lingual phrase chunking model
based on masking method. In Proceedings of Intel-
ligent Text Processing and Computational Linguistics,
pages 144?155.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and pos tagging using a single perceptron.
In Proceedings of ACL, Columbus, Ohio, USA.
T. Zhang, F. Damerau, and D. Johnson. 2002. Text
chunking based on a generalization of winnow. ma-
chine learning research. Machine Learning Research,
2:615?637.
Hai Zhao and Chunyu Kit. 2008. Unsupervised segmen-
tation helps supervised learning of character tagging
forword segmentation and named entity recognition.
In Proceedings of Sixth SIGHAN Workshop on Chinese
Language Processing, pages 106?111.
195
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1332?1341,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Structural Opinion Mining for Graph-based Sentiment Representation
Yuanbin Wu, Qi Zhang, Xuanjing Huang, Lide Wu
Fudan University
School of Computer Science
{ybwu,qz,xjhuang,ldwu}@fudan.edu.cn
Abstract
Based on analysis of on-line review corpus
we observe that most sentences have compli-
cated opinion structures and they cannot be
well represented by existing methods, such as
frame-based and feature-based ones. In this
work, we propose a novel graph-based rep-
resentation for sentence level sentiment. An
integer linear programming-based structural
learning method is then introduced to produce
the graph representations of input sentences.
Experimental evaluations on a manually la-
beled Chinese corpus demonstrate the effec-
tiveness of the proposed approach.
1 Introduction
Sentiment analysis has received much attention in
recent years. A number of automatic methods have
been proposed to identify and extract opinions, emo-
tions, and sentiments from text. Previous researches
on sentiment analysis tackled the problem on vari-
ous levels of granularity including document, sen-
tence, phrase and word (Pang et al, 2002; Riloff et
al., 2003; Dave et al, 2003; Takamura et al, 2005;
Kim and Hovy, 2006; Somasundaran et al, 2008;
Dasgupta and Ng, 2009; Hassan and Radev, 2010).
They mainly focused on two directions: sentiment
classification which detects the overall polarity of a
text; sentiment related information extraction which
tries to answer the questions like ?who expresses
what opinion on which target?.
Most of the current studies on the second direc-
tion assume that an opinion can be structured as a
frame which is composed of a fixed number of slots.
Typical slots include opinion holder, opinion expres-
sion, and evaluation target. Under this representa-
tion, they defined the task as a slots filling prob-
lem for each of the opinions. Named entity recog-
nition and relation extraction techniques are usually
applied in this task (Hu and Liu, 2004; Kobayashi
et al, 2007; Wu et al, 2009).
However, through data analysis, we observe that
60.5% of sentences in our corpus do not follow the
assumption used by them. A lot of important infor-
mation about an opinion may be lost using those rep-
resentation methods. Consider the following exam-
ples, which are extracted from real online reviews:
Example 1: The interior is a bit noisy on the free-
way1.
Example 2: Takes good pictures during the day-
time. Very poor picture quality at night2.
Based on the definition of opinion unit proposed
by Hu and Liu (2004), from the first example, the
information we can get is the author?s negative opin-
ion about ?interior? using an opinion expression
?noisy?. However, the important restriction ?on the
freeway?, which narrows the scope of the opinion,
is ignored. In fact, the tuple (?noisy?,?on the free-
way?) cannot correctly express the original opinion:
it is negative but under certain condition. The sec-
ond example is similar. If the conditions ?during the
daytime? and ?at night? are dropped, the extracted
elements cannot correctly represent user?s opinions.
Example 3: The camera is actually quite good for
outdoors because of the software.
Besides that, an opinion expression may induce
other opinions which are not expressed directly. In
example 3, the opinion expression is ?good? whose
1http://reviews.carreview.com/blog/2010-ford-focus-
review-the-compact-car-that-can/
2http://www.dooyoo.co.uk/digital-camera/sony-cyber-shot-
dsc-s500/1151680/
1332
target is ?camera?. But the ?software? which trig-
gers the opinion expression ?good? is also endowed
with a positive opinion. In practice, this induced
opinion on ?software? is actually more informative
than its direct counterpart. Mining those opinions
may help to form a complete sentiment analysis re-
sult.
Example 4: The image quality is in the middle of
its class, but it can still be a reasonable choice for
students.
Furthermore, the relations among individual opin-
ions also provide additional information which is
lost when they are considered separately. Example
4 is such a case that the whole positive comment of
camera is expressed by a transition from a negative
opinion to a positive one.
In order to address those issues, this paper de-
scribes a novel sentiment representation and analysis
method. Our main contributions are as follows:
1. We investigate the use of graphs for repre-
senting sentence level sentiment. The ver-
tices are evaluation target, opinion expression,
modifiers of opinion. The Edges represent
relations among them. The semantic rela-
tions among individual opinions are also in-
cluded. Through the graph, various informa-
tion on opinion expressions which is ignored
by current representation methods can be well
handled. And the proposed representation is
language-independent.
2. We propose a supervised structural learning
method which takes a sentence as input and the
proposed sentiment representation for it as out-
put. The inference algorithm is based on in-
teger linear programming which helps to con-
cisely and uniformly handle various properties
of our sentiment representation. By setting ap-
propriate prior substructure constraints of the
graph, the whole algorithm achieves reasonable
performances.
The remaining part of this paper is organized as
follows: In Section 2 we discuss the proposed rep-
resentation method. Section 3 describes the com-
putational model used to construct it. Experimental
results in test collections and analysis are shown in
Section 4. In Section 5, we present the related work
and Section 6 concludes the paper.
2 Graph-based Sentiment Representation
In this work, we propose using directed graph to
represent sentiments. In the graph, vertices are
text spans in the sentences which are opinion ex-
pressions, evaluation targets, conditional clauses etc.
Two types of edges are included in the graph: (1)
relations among opinion expressions and their mod-
ifiers; (2) relations among opinion expressions. The
edges of the first type exist within individual opin-
ions. The second type of the edges captures the re-
lations among individual opinions. The following
sections detail the definition.
2.1 Individual Opinion Representation
Let r be an opinion expression in a sentence, the rep-
resentation unit for r is a set of relations {(r, dk)}.
For each relation (r, dk), dk is a modifier which is a
span of text specifying the change of r?s meaning.
The relations between modifier and opinion ex-
pression can be the type of any kind. In this work,
we mainly consider two basic types:
? opinion restriction. (r, dk) is called an opin-
ion restriction if dk narrows r?s scope, adds a
condition, or places limitations on r?s original
meaning.
? opinion expansion. (r, dk) is an opinion expan-
sion if r?s scope expands to dk, r induces an-
other opinion on dk, or the opinion on dk is im-
plicitly expressed by r.
Mining the opinion restrictions can help to get ac-
curate meaning of an opinion, and the opinion ex-
pansions are useful to cover more indirect opinions.
As with previous sentiment representations, we ac-
tually consider the third type of modifier which dk is
the evaluation target of r.
Figure 1 shows a concrete example. In this ex-
ample, there are three opinion expressions: ?good?,
?sharp?, ?slightly soft?. The modifiers of ?good?
are ?indoors? and ?Focus accuracy?, where relation
(?good?,?indoors?) is an opinion restriction because
?indoors? is the condition under which ?Focus ac-
curacy? is good. On the other hand, the relation
1333
(?sharp?, ?little 3x optical zooms?) is an opinion ex-
pansion because the ?sharp? opinion on ?shot? im-
plies a positive opinion on ?little 3x optical zooms?.
It is worth to remark that: 1) a modifier dk can re-
late to more than one opinion expression. For exam-
ple, multiple opinion expressions may share a same
condition; 2) dk itself can employ a set of relations,
although the case appears occasionally. The follow-
ing is an example:
Example 5: The camera wisely get rid of many
redundant buttons.
In the example, ?redundant buttons? is the eval-
uation target of opinion expression ?wisely get rid
of?, but itself is a relation between ?redundant?
and ?buttons?. Such nested semantic structure is
described by a path: ?wisely get rid of? target?????
[?redundant? target??????buttons?]nested target.
2.2 Relations between Individual Opinion
Representation
Assume ?ri? are opinion expressions ordered by
their positions in sentence, and each of them has
been represented by relations {(ri, dik)} individu-
ally (the nested relations for dik have also been de-
termined). Then we define two relations on adja-
cent pair ri, ri+1: coordination when the polarities
of ri and ri+1 are consistent, and transition when
they are opposite. Those relations among ri form a
set B called opinion thread. In Figure 1, the opin-
ion thread is: {(?good?, ?sharp?), (?sharp?, ?slightly
soft?)}.
The whole sentiment representation for a sentence
can be organized by a direct graphG = (V,E). Ver-
tex set V includes all opinion expressions and mod-
ifiers. Edge set E collects both relations of each
individual opinion and relations in opinion thread.
The edges are labeled with relation types in label set
L={?restriction?, ?expansion?, ?target?, ?coordina-
tion?, ?transition?} 3.
Compared with previous works, the advantages of
using G as sentiment representation are: 1) for in-
dividual opinions, the modifiers will collect more
information than using opinion expression alone.
3We don?t define any ?label? on vertices: if two span of text
satisfy a relation in L, they are chosen to be vertices and an
edge with proper label will appear inE. In other words, vertices
are identified by checking whether there exist relations among
them.
Focus accuracy was good indoors, and although the 
little 3x optical zooms produced sharp shots, the 
edges were slightly soft on the Canon. 
Focus 
accuracy 
edges 
slightly soft shots 
sharp 
little 3x optical 
zooms 
indoors 
good 
Expansion 
Target 
Coordinate 
Transition 
Target 
Target 
Restriction 
r
1 
r
2 
r
3 
d
11 
d
12 
d
21 
d
22 
d
31 
Figure 1: Sentiment representation for an example sen-
tence
Thus G is a relatively complete and accurate rep-
resentation; 2) the opinion thread can help to catch
global sentiment information, for example the gen-
eral polarity of a sentence, which is dropped when
the opinions are separately represented.
3 System Description
To produce the representation graph G for a sen-
tence, we need to extract candidate vertices and
build the relations among them to get a graph struc-
ture. For the first task, the experimental results in
Section 4 demonstrate that the standard sequential
labeling method with simple features can achieve
reasonable performance. In this section, we focus
on the second task, and assume the vertices in the
graph have already been correctly collected in the
following formulation of algorithm.
3.1 Preliminaries
In order to construct graph G, we use a structural
learning method. The framework is from the first or-
der discriminative dependency parsing model (Mc-
donald and Pereira, 2005). A sentence is denoted by
s; x are text spans which will be vertices of graph;
xi is the ith vertex in x ordered by their positions in
s. For a set of vertices x, y is the graph of its sen-
timent representation, and e = (xi, xj) ? y is the
direct edge from xi to xj in y. In addition, x0 is a
1334
virtual root node without inedge. G = {(xn,yn)}Nn
is training set.
Following the edge based factorization, the score
of a graph is the sum of its edges? scores,
score(x,y) =
?
(xi,xj)?y
score(xi, xj)
=
?
(xi,xj)?y
?T f(xi, xj), (1)
f(xi, xj) is a high dimensional feature vector of the
edge (xi, xj). The components of f are either 0 or 1.
For example the k-th component could be
fk(xi, xj) =
?
?
?
1 if xi.POS = JJ and xj .POS = NN
and label of (xi, xj)is restriction
0 otherwise
.
Then the score of an edge is the linear combination
of f ?s components, and the coefficients are in vector
?.
Algorithm 1 shows the parameter learning pro-
cess. It aims to get parameter ? which will assign
the correct graph y with the highest score among all
possible graphs of x (denoted by Y).
Algorithm 1 Online structural learning
Training Set:G = {(xn, yn)}Nn
1: ?0 = 0, r = 0, T =maximum iteration
2: for t = 0 to T do
3: for n = 0 to N do
4: y? = argmaxy?Y score(xn, y) B Inference
5: if y? 6= yn then
6: update ?t to ?t+1 B PA
7: r = r + ?t+1
8: end if
9: end for
10: end for
11: return ? = r/(N ? T )
3.2 Inference
Like other structural learning tasks, the ?argmax?
operation in the algorithm (also called inference)
y? = argmax
y?Y
score(x,y)
= argmax
y?Y
?
(xi,xj)?y
?T f(xi, xj) (2)
is hard because all possible values of y form a huge
search space. In our case, Y is all possible directed
acyclic graphs of the given vertex set, which num-
ber is exponential. Directly solving the problem of
finding maximum weighted acyclic graph is equiva-
lent to finding maximum feedback arc set, which is a
NP-hard problem (Karp, 1972). We will use integer
linear programming (ILP) as the framework for this
inference problem.
3.2.1 Graph Properties
We first show some properties of graph G either
from the definition of relations or corpus statistics.
Property 1. The graph is connected and without
directed cycle. From individual opinion represen-
tation, each subgraph of G which takes an opinion
expression as root is connected and acyclic. Thus
the connectedness is guaranteed for opinion expres-
sions are connected in opinion thread; the acyclic is
guaranteed by the fact that if a modifier is shared by
different opinion expressions, the inedges from them
always keep (directed) acyclic.
Property 2. Each vertex can have one outedge
labeled with coordination or transition at most. The
opinion thread B is a directed path in graph.
Property 3. The graph is sparse. The average
in-degree of a vertex is 1.03 in our corpus, thus the
graph is almost a rooted tree. In other words, the
cases that a modifier connects to more than one opin-
ion expression rarely occur comparing with those
vertices which have a single parent. An explaination
for this sparseness is that opinions in online reviews
always concentrate in local context and have local
semantic connections.
3.2.2 ILP Formulation
Based on the property 3, we divide the inference
algorithm into two steps: i) constructing G?s span-
ning tree (arborescence) with property 1 and 2; ii)
finding additional non-tree edges as a post process-
ing task. The first step is close to the works on ILP
formulations of dependency parsing (Riedel and
Clarke, 2006; Martins et al, 2009). In the second
step, we use a heuristic method which greedily adds
non-tree edges. A similar approximation method
is also used in (Mcdonald and Pereira, 2006) for
acyclic dependency graphs.
Step 1. Find MST. Following the multicommodity
1335
flow formulation of maximum spanning tree (MST)
problem in (Magnanti and Wolsey, 1994), the ILP
for MST is:
max.
?
i,j
yij ? score(xi, xj) (3)
s.t.
?
i,j
yij = |V | ? 1 (4)
?
i
fuij ?
?
k
fujk = ?uj ,1 ? u, j ? |V | (5)
?
k
fu0k = 1, 1 ? u ? |V | (6)
fuij ? yij , 1 ? u, j ? |V |,
0 ? i ? |V | (7)
fuij ? 0, 1 ? u, j ? |V |,
0 ? i ? |V | (8)
yij ? { 0, 1}, 0 ? i, j ? |V |. (9)
In this formulation, yij is an edge indicator vari-
able that (xi, xj) is a spanning tree edge when yij =
1, (xi, xj) is a non-tree edge when yij = 0. Then
output y is represented by the set {yij , 0 ? i, j ?
|V |} 4. Eq(4) ensures that there will be exactly
|V | ? 1 edges are chosen. Thus if the edges cor-
responding to those non zero yij is a connected sub-
graph, y is a well-formed spanning tree. Objective
function just says the optimal solution of yij have
the maximum weight.
The connectedness is guaranteed if for every ver-
tex, there is exactly one path from root to it. It is for-
mulated by using |V | ? 1 flows {fu, 1 ? u ? |V |}.
fu starts from virtual root x0 towards vertex xu.
Each flow fu = {fuij , 0 ? i, j ? |V |}. fuij indi-
cates whether flow fu is through edge (xi, xj). so
it should be 0 if edge (xi, xj) does not exist (by
(7)). The Kronecker?s delta ?uj in (5) guarantees fu
is only assumed by vertex xu, so fu is a well-formed
path from root to xu. (6) ensures there is only one
flow (path) from root to xu. Thus the subgraph is
connected. The following are our constraints:
c1: Constraint on edges in opinion thread (10)-
(11).
From the definition of opinion thread, we impose
a constraint on every vertex?s outedges in opinion
thread, which are labeled with ?coordination? or
4For simplicity, we overload symbol y from the graph of the
sentiment represetation to the MST of it.
?transition?. Let Iob be a characteristic function on
edges: Iob((j, k)) = 1 when edge (xj , xk) is labeled
with ?coordination? or ?transition?, otherwise 0. We
denote q variables for vertices:
qj =
?
k
yjk ? Iob((j, k)), 0 ? j ? |V |. (10)
Then following linear inequalities bound the number
of outedges in opinion thread (? 1) on each vertex:
qj ? 1, 0 ? j ? |V |. (11)
c2: Constraint on target edge (12).
We also bound the number of evaluation targets
for a vertex in a similar way. Let It be characteris-
tic function on edges identifing whether it is labeled
with ?target?,
?
k
yjk ? It((j, k)) ? Ct, 0 ? j ? |V |. (12)
The parameter Ct can be adjusted according to the
style of document. In online reviews, authors tend
to use simple and short comments on individual tar-
gets, so Ct could be set small.
c3: Constraint on opinion thread (13)-(18).
From graph property 2, the opinion thread should
be a directed path. It implies the number of con-
nected components whose edges are ?coordination?
or ?transition? should be less than 1. Two set of ad-
ditional variables are needed: {cj , 0 ? j ? |V |} and
{hj , 0 ? j ? |V |}, where
cj =
{
1 if an opinion thread starts at xj
0 otherwise ,
and
hj =
?
i
yij ? Iob((i, j)). (13)
Then cj = ?hj ? qj , which can be linearized by
cj? qj ? hj , (14)
cj? 1 ? hj , (15)
cj? qj , (16)
cj? 0. (17)
If the sum of cj is no more than 1, the opinion thread
of graph is a directed path.
?
j
cj ? 1. (18)
1336
1 
2 
3 
4 
5 
6 
7 
(a) 
(b) 
1 
2 
3 
4 
5 
6 7 
(c) 
1 
2 3 
4 
5 
6 
7 
Figure 2: The effects of c1 and c3. Assume solid lines
are edges labeled with ?coordination? and ?transition?,
dot lines are edges labeled with other types. (a) is an
arbitrary tree. (b) is a tree with c1 constraints. (c) is a
tree with c1 and c3. It shows c1 are not sufficient for
graph property 2: the edges in opinion thread may not be
connected.
Figure 2 illustrates the effects of c1 and c3.
Equations (10)-(18), together with basic multi-
commodity flow model build up the inference algo-
rithm. The entire ILP formulation involves O(|V |3)
variables and O(|V |2) constraints. Generally, ILP
falls into NPC, but as an important result, in the mul-
ticommodity flow formulation of maximum span-
ning tree problem, the integer constraints (9) on yij
can be dropped. So the problem reduces to a linear
programming which is polynomial solvable (Mag-
nanti and Wolsey, 1994). Unfortunately, with our
additional constraints the LP relaxation is not valid.
Step 2. Adding non-tree edges. We examine the
case that a modifier attaches to different opinion ex-
pressions. That often occurs as the result of the
sharing of modifiers among adjacent opinion expres-
sions. We add those edges in the following heuristic
way: If a vertex ri in opinion thread does not have
any modifier, we search the modifiers of its adjacent
vertices ri+1, ri?1 in the opinion thread, and add
edge (ri, d?) where
d? = argmax
d?S
score(ri, d),
and S are the modifiers of ri?1 and ri+1.
3.3 Training
We use online passive aggressive algorithm (PA)
with Hamming cost of two graphs in training (Cram-
mer et al, 2006).
Unigram Feature Template
xi.text w0.text w1.text
w0.POS w1.POS
wk?1.text wk.text
Inside wk?1.POS wk.POS
Features xi.hasDigital
xi.isSingleWord
xi.hasSentimentWord
xi.hasParallelPhrase
w?1.text w?2.text
w?1.POS w?2.POS
wk+1.text wk+2.text
Outside wk+1.POS wk+2.POS
Features c?1.text c?2.text
c?1.POS c?2.POS
cl+1.text cl+2.text
cl+1.POS cl+2.POS
Other Features
distance between parent and child
dependency parsing relations
Table 1: Feature set
3.4 Feature Construction
For each vertex xi in graph, we use 2 sets of fea-
tures: inside features which are extracted inside the
text span of xi; outside features which are outside
the text span of xi. A vertex xi is described both in
word sequence (w0, w1, ? ? ? , wk) and character se-
quence (c0, c1, ? ? ? , cl), for the sentences are in Chi-
nese.
? ? ? , w?1, w0, w1, w2, ? ? ? , wk?1, wk? ?? ?
xi
, wk+1 ? ? ?
? ? ? , c?1, c0, c1, c2, ? ? ? , cl?1, cl? ?? ?
xi
, cl+1 ? ? ?
For an edge (xi, xj), the high dimensional feature
vector f(xi, xj) is generated by using unigram fea-
tures in Table 1 on xi and xj respectively. The dis-
tance between parent and child in sentence is also
attached in features. In order to involve syntactic
information, whether there is certain type of depen-
dency relation between xi and xj is also used as a
feature.
1337
4 Experiments
4.1 Corpus
We constructed a Chinese online review corpus from
Pcpop.com, Zol.com.cn, and It168.com, which have
a large number of reviews about digital camera. The
corpus contains 138 documents and 1735 sentences.
Since some sentences do not contain any opinion,
1390 subjective sentences were finally chosen and
manually labeled.
Two annotators labeled the corpus independently.
The annotators started from locating opinion expres-
sions, and for each of them, they annotated other
modifiers related to it. In order to keep the relia-
bility of annotations, another annotator was asked
to check the corpus and determine the conflicts. Fi-
nally, we extracted 6103 elements, which are con-
nected by 6284 relations.
Relation Number
Target 2479
Coordinate 1173
Transition 154
Restriction 693
Expansion 386
Table 2: Statistics of relation types
Table 2 shows the number of various relation
types appearing in the labeled corpus. We observe
60.5% of sentences and 32.1% of opinion expres-
sions contain other modifiers besides ?target?. Thus
only mining the relations between opinion expres-
sions and evaluation target is actually at risk of inac-
curate and incomplete results.
4.2 Experiments Configurations
In all the experiments below, we take 90% of the cor-
pus as training set, 10% as test set and run 10 folder
cross validation. In feature construction, we use
an external Chinese sentiment lexicon which con-
tains 4566 positive opinion words and 4370 nega-
tive opinion words. For Chinese word segment, we
use ctbparser 5. Stanford parser (Klein and Man-
ning, 2003) is used for dependency parsing. In the
settings of PA, the maximum iteration number is
5http://code.google.com/p/ctbparser/
set to 2, which is chosen by maximizing the test-
ing performances, aggressiveness parameter C is set
to 0.00001. For parameters in inference algorithm,
Ct = 2, the solver of ILP is lpsolve6.
We evaluate the system from the following as-
pects: 1) whether the structural information helps
to mining opinion relations. 2) How the proposed
inference algorithm performs with different con-
straints. 3) How the various features affect the sys-
tem. Except for the last one, the feature set used for
different experiments are the same (?In+Out+Dep?
in Table 5). The criteria for evaluation are simi-
lar to the unlabeled attachment score in parser eval-
uations, but due to the equation |E| = |V | ? 1
is not valid if G is not a tree, we evaluate pre-
cision P = #true edges in result graph#edges in result graph , recall
R = #true edges in result graph#edges in true graph , and F-score
F = 2P ?RP+R .
4.3 Results
1. The effects of structural information. An alter-
native method to extract relations is directly using
a classifier to judge whether there is a relation be-
tween any two elements. Those kinds of methods
were used in previous opinion mining works (Wu
et al, 2009; Kobayashi et al, 2007). To show the
entire structural information is important for min-
ing relations, we use SVM for binary classification
on candidate pairs. The data point representing a
pair (xi, xj) is the same as the high dimensional fea-
ture vectors f(xi, xj). The setting of our algorithm
?MST+c1+c2+c3? is the basic MSTwith all the con-
straints. The results are shown in the Table 3.
P R F
SVM 64.9 24.0 35.0
MST+c1+c2+c3-m 61.5 74.0 67.2
MST+c1+c2+c3 73.1 71.0 72.1
Table 3: Binary classifier and structural learning
From the results, the performance of SVM (espe-
cially recall) is relatively poor. A possible reason
is that the huge imbalance of positive and negative
training samples (only ?(n) positive pairs among
all n2 pairs). And the absence of global structural
6http://sourceforge.net/projects/lpsolve/
1338
knowledge makes binary classifier unable to use
the information provided by classification results of
other pairs.
In order to examine whether the complicated sen-
timent representation would disturb the classifier in
finding relations between opinion expressions and
its target, we evaluate the system by discarding the
modifiers of opinion restriction and expansion from
the corpus. The result is shown in the second row of
Table 3. We observe that ?MST+c1+c2+c3? is still
better which means at least on overall performance
the additional modifiers do not harm.
2. The effect of constraints on inference algo-
rithm. In the inference algorithm, we utilized the
properties of graph G and adapted the basic multi-
commodity flow ILP to our specific task. To evaluate
how the constraints affect the system, we decompose
the algorithm and combine them in different ways.
P R F
MST 69.3 67.3 68.3
MST+c1 70.0 68.0 69.0
MST+c2 69.8 67.8 68.8
MST+c1+c2 70.6 68.6 69.6
MST+c1+c3 72.4 70.4 71.4
MST+c1+c2+c3 73.1 71.0 72.1
MST+c1+c2+c3+g 72.5 72.3 72.4
Table 4: Results on inference methods. ?MST? is the ba-
sic multicommodity flow formulation of maximum span-
ning tree; c1, c2, c3 are groups of constraint from Section
3.2.2; ?g? is our heuristic method for additional non span-
ning tree edges.
From Table 4, we observe that with any additional
constraints the inference algorithm outperforms the
basic maximum spanning tree method. It implies al-
though we did not use high order model (e.g. involv-
ing grandparent and sibling features), prior struc-
tural constraints can also help to get a better out-
put graph. By comparing with different constraint
combinations, the constraints on opinion thread (c1,
c3) are more effective than constraints on evaluation
targets (c2). It is because opinion expressions are
more important in the entire sentiment representa-
tion. The main structure of a graph is clear once the
relations between opinion expressions are correctly
determined.
3. The effects of various features. We evaluate the
performances of different feature configurations in
Table 5. From the results, the outside feature set is
more effective than inside feature set, even if it does
not use any external resource. A possible reason is
that the content of a vertex can be very complicated
(a vertex even can be a clause), but the features sur-
rounding the vertex are relatively simple and easy
to identify (for example, a single preposition can
identify a complex condition). The dependency fea-
ture has limited effect, due to that lots of online re-
view sentences are ungrammatical and parsing re-
sults are unreliable. And the complexity of vertices
also messes the dependency feature.
P R F
In-s 66.3 66.3 66.3
In 66.7 66.4 66.6
Out 67.8 67.4 67.6
In+Out 72.0 70.5 71.0
In+Out+Dep 72.5 72.3 72.4
Table 5: Results with different features. ?In? repre-
sents the result of inside feature set; ?In-s? is ?In? with-
out the external opinion lexicon feature; ?Out? uses the
outside feature set; ?In+Out? uses both ?In? and ?Out?,
?In+Out+Dep? adds the dependency feature. The infer-
ence algorithm is ?MST+c1+c2+c3+g? in Table 4.
We analyze the errors in test results. A main
source of errors is the confusion of classifier be-
tween ?target? relations and ?coordination?, ?tran-
sition? relations. The reason may be that for a mod-
ification on opinion expression (r, dk), we allow
dk recursively has its own modifiers (Example 5).
Thus an opinion expression can be a modifier which
brings difficulties to classifier.
4. Extraction of vertices. Finally we conduct an
experiment on vertex extraction using standard se-
quential labeling method. The tag set is simply {B,
I, O} which are signs of begin, inside, outside of a
vertex. The underlying model is conditional random
field 7. Feature templates involved are in Table 6.
We only use basic features in the experiment. 10
folder cross validation results are in table 7. We sus-
pect that the performances (especially recall) could
be improved if some external resources(i.e. ontol-
ogy, domain related lexicon, etc.) are involved.
7We use CRF++ toolkit, http://crfpp.sourceforge.net/
1339
Unigram Template
ci.char character
ci.isDigit digit
ci.isAlpha english letter
ci.isPunc punctuation
ci.inDict in a sentiment word
ci.BWord start of a word
ci.EWord end of a word
Table 6: Features for vertex extraction. The sequential
labeling is conducted on character level (ci). The senti-
ment lexicon used in ci.inDict is the same as Table1. We
also use bigram feature templates on ci.char, ci.isAlpha,
ci.inDict with respect to ci?1 and ci+1.
P R F
E+Unigram 56.8 45.1 50.3
E+Unigram+Bigram 57.3 47.9 52.1
O+Unigram 71.9 57.2 63.7
O+Unigram+Bigram 72.3 60.2 65.6
Table 7: Results on vertices extraction with 10 folder
cross validation. We use two criterion: 1) the vertex is
correct if it is exactly same as ground truth(?E?), 2) the
vertex is correct if it overlaps with ground truth(?O?).
5 Related Work
Opinion mining has recently received considerable
attentions. Large amount of work has been done on
sentimental classification in different levels and sen-
timent related information extraction. Researches on
different types of sentences such as comparative sen-
tences (Jindal and Liu, 2006) and conditional sen-
tences (Narayanan et al, 2009) have also been pro-
posed.
Kobayashi et al (2007) presented their work on
extracting opinion units including: opinion holder,
subject, aspect and evaluation. They used slots
to represent evaluations, converted the task to two
kinds of relation extraction tasks and proposed a ma-
chine learning-based method which used both con-
textual and statistical clues.
Jindal and Liu (2006) studied the problem of iden-
tifying comparative sentences. They analyzed dif-
ferent types of comparative sentences and proposed
learning approaches to identify them.
Sentiment analysis of conditional sentences were
studied by Narayanan et al (2009). They aimed
to determine whether opinions expressed on dif-
ferent topics in a conditional sentence are posi-
tive, negative or neutral. They analyzed the con-
ditional sentences in both linguistic and computi-
tional perspectives and used learning method to do
it. They followed the feature-based sentiment anal-
ysis model (Hu and Liu, 2004), which also use flat
frames to represent evaluations.
Integer linear programming was used in many
NLP tasks (Denis and Baldridge, 2007), for its
power in both expressing and approximating various
inference problems, especially in parsing (Riedel
and Clarke, 2006; Martins et al, 2009). Martins
etc. (2009) also applied ILP with flow formulation
for maximum spanning tree, besides, they also han-
dled dependency parse trees involving high order
features(sibling, grandparent), and with projective
constraint.
6 Conclusions
This paper introduces a representation method for
opinions in online reviews. Inspections on corpus
show that the information ignored in previous sen-
timent representation can cause incorrect or incom-
plete mining results. We consider opinion restric-
tion, opinion expansions, relations between opin-
ion expressions, and represent them with a directed
graph. Structural learning method is used to produce
the graph for a sentence. An inference algorithm is
proposed based on the properties of the graph. Ex-
perimental evaluations with a manually labeled cor-
pus are given to show the importance of structural
information and effectiveness of proposed inference
algorithm.
7 Acknowledgement
The author wishes to thank the anonymous review-
ers for their helpful comments. This work was
partially funded by 973 Program (2010CB327906),
National Natural Science Foundation of China
(61003092, 61073069),863 Program of China
(2009AA01A346), Shanghai Science and Tech-
nology Development Funds(10dz1500104), Doc-
toral Fund of Ministry of Education of China
(200802460066), Shanghai Leading Academic Dis-
cipline Project (B114), and Key Projects in
the National Science & Technology Pillar Pro-
1340
gram(2009BAH40B04).
References
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Sajib Dasgupta and Vincent Ng. 2009. Mine the easy,
classify the hard: A semi-supervised approach to auto-
matic sentiment classification. In Proceedings of ACL-
IJCNLP.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In Pro-
ceedings of WWW.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Proceedings of NAACL-
HLT.
Ahmed Hassan and Dragomir R. Radev. 2010. Identify-
ing text polarity using random walks. In Proceedings
of ACL, pages 395?403, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of SIGKDD.
Nitin Jindal and Bing Liu. 2006. Identifying comparative
sentences in text documents. In Proceedings of SIGIR.
R. Karp. 1972. Reducibility among combinatorial prob-
lems. In R. Miller and J. Thatcher, editors, Complex-
ity of Computer Computations, pages 85?103. Plenum
Press.
Soo-Min Kim and Eduard Hovy. 2006. Automatic iden-
tification of pro and con reasons in online reviews. In
Proceedings of the COLING-ACL.
Dan Klein and Christopher D.Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In In Advances in Neural Information Pro-
cessing Systems 15 (NIPS, pages 3?10. MIT Press.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of rela-
tions in opinion mining. In Proceedings of EMNLP-
CoNLL.
Thomas L. Magnanti and Laurence A. Wolsey. 1994.
Optimal trees.
Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of ACL-IJCNLP.
R. Mcdonald and F. Pereira. 2005. Identifying gene
and protein mentions in text using conditional random
fields. BMC Bioinformatics.
Ryan Mcdonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proc. of EACL, pages 81?88.
Ramanathan Narayanan, Bing Liu, and Alok Choudhary.
2009. Sentiment analysis of conditional sentences. In
Proceedings of EMNLP.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proc. of EMNLP 2002.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In Proceedings of EMNLP.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the seventh confer-
ence on Natural language learning at HLT-NAACL.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpreta-
tion. In Proceedings of COLING.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In Proceedings of ACL.
Yuanbin Wu, Qi Zhang, Xuangjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion mining.
In Proceedings of EMNLP.
1341
