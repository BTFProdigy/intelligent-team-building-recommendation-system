Proceedings of NAACL HLT 2009: Short Papers, pages 273?276,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Automatic Chinese Abbreviation Generation Using Conditional Random
Field
Dong Yang, Yi-cheng Pan, and Sadaoki Furui
Department of Computer Science
Tokyo Institute of Technology
Tokyo 152-8552 Japan
{raymond,thomas,furui}@furui.cs.titech.ac.jp
Abstract
This paper presents a new method for au-
tomatically generating abbreviations for Chi-
nese organization names. Abbreviations are
commonly used in spoken Chinese, especially
for organization names. The generation of
Chinese abbreviation is much more complex
than English abbreviations, most of which are
acronyms and truncations. The abbreviation
generation process is formulated as a character
tagging problem and the conditional random
field (CRF) is used as the tagging model. A
carefully selected group of features is used in
the CRF model. After generating a list of ab-
breviation candidates using the CRF, a length
model is incorporated to re-rank the candi-
dates. Finally the full-name and abbreviation
co-occurrence information from a web search
engine is utilized to further improve the per-
formance. We achieved top-10 coverage of
88.3% by the proposed method.
1 Introduction
Long named entities are frequently abbreviated in
oral Chinese language for efficiency and simplic-
ity. Therefore, abbreviation modeling is an impor-
tant building component for many systems that ac-
cept spoken input, such as directory assistance and
voice search systems.
While English abbreviations are usually formed
as acronyms, Chinese abbreviations are much more
complex, as shown in Figure 1. Most of the Chi-
nese abbreviations are formed by selecting several
characters from full-names, which are not necessar-
ily the first character of each word. Usually the orig-
inal character order in the full-name is preserved in
???? ?? T s i n g h u a U n i v e r s i t y
??????? ?? C h i n a c e n t r a l t e l e v i s i o n
F u l l ? n a m e  a b b r e v i a t i o n  E n g l i s h  e x p l a n a t i o n
?????? ?? ???? P e k i n g  U n i v e r s i t y  N o . 3  h o s p i t a l
Figure 1: Chinese abbreviation examples
the abbreviation. However, re-ordering of charac-
ters as shown in the third example in Figure 1 where
characters ?n? and ??? are swapped in the abbre-
viation, also happens.
There has been a considerable amount of research
on extracting full-name and abbreviation pairs in
the same document for obtaining abbreviations (Li
and Yarowsky, 2008; Sun et al, 2006; Fu et al,
2006). However, generation of abbreviations given
a full-name is still a non-trivial problem. Chang
and Lai (Chang and Lai, 2004) have proposed using
a hidden Markov model to generate abbreviations
from full-names. However, their method assumes
that there is no word-to-null mapping, which means
that every word in the full-name has to contribute at
least one character to the abbreviation. This assump-
tion does not hold for organizations? names which
have many word skips in the abbreviation genera-
tion.
The CRF was first introduced to natural language
processing (NLP) by (Lafferty et al, 2001) and has
been widely used in word segmentation, part-of-
speech (POS) tagging, and some other NLP tasks.
In this paper, we convert the Chinese abbreviation
generation process to a CRF tagging problem. The
key problem here is how to find a group of discrim-
273
inant and robust features. After using the CRF, we
get a list of abbreviation candidates with associate
probability scores. We also use the prior condi-
tional probability of the length of the abbreviations
given the length of the full-names to complement the
CRF probability scores. Such global information is
hard to include in the CRF model. In addition, we
apply the full-name and abbreviation candidate co-
occurrence statistics obtained on the web to increase
the correctness of the abbreviation candidates.
2 Chinese Abbreviation Introduction
Chinese abbreviations are generated by three meth-
ods (Lee, 2005): reduction, elimination, and gener-
alization.
Both in the reduction and elimination methods,
characters are selected from the full-name, and the
order of the characters is sometime changed. Note
that this paper does not cover the case when the or-
der is changed. The elimination means that one or
more words in the full-name are ignored completely,
while the reduction requires that at least one char-
acter is selected from each word. All the three ex-
amples in Figure 1 are produced by the elimination,
where at least one word is skipped.
Generalization, which is used to abbreviate a list
of similar terms, is usually composed of the number
of terms and a shared character across the terms. A
example is ?n? (three forces) for ?????
?? (land force, sea force, air force). This is the
most difficult scenario for the abbreviations and is
not considered in this paper.
3 CRF Model for Abbreviation Modeling
3.1 CRF model
A CRF is an undirected graphical model and assigns
the following probability to a label sequence L =
l1l2 . . . lT , given an input sequence C = c1c2 . . . cT ,
P (L|C) = 1Z(C)exp(
T?
t=1
?
k
?kfk(lt, lt?1, C, t))
(1)
Here, fk is the feature function for the k-th fea-
ture, ?k is the parameter which controls the weight
of the k-th feature in the model, and Z(C) is the nor-
malization term that makes the summation of the
probability of all label sequences to 1. CRF training
is usually performed through the typical L-BFGS al-
gorithm (Wallach, 2002) and decoding is performed
by Viterbi algorithm (Viterbi, 1967). In this paper,
we use an open source toolkit ?crf++?.
3.2 Abbreviation modeling as a tagging
problem
In order to use the CRF method in abbreviation gen-
eration, the abbreviation generation problem was
converted to a tagging problem. The character is
used as a tagging unit and each character in a full-
name is tagged by a binary variable with the values
of either Y or N: Y stands for a character used in the
abbreviation and N means not. An example is given
in Figure 2.
??????? ??
?/ N ?/ N ?/ N ?/ Y ?/ N ?/ Y ?/ N
Figure 2: Abbreviation in the CRF tagging format
3.3 Feature selection for the CRF
In the CRF method, feature function describes
a co-occurrence relation, and it is defined as
fk(lt, lt?1, C, t) (Eq. 1). fk is usually a binary func-
tion, and takes the value 1 when both observation ct
and transition lt?1 ? lt are observed. In our ab-
breviation generation model, we use the following
features:
1. Current character The character itself is the
most important feature for abbreviation as it will be
either retained or discarded. For example, ??? (bu-
reau) and ??? (institue), indicating a government
department, are very common characters used in ab-
breviations. When they appear in full-names, they
are likely to be kept in abbreviations.
2. Current word In the full name of ??I??
??? (China Agricultural university), the word ??
I? (China) is usually ignored in the abbreviation,
but the word ???? (agriculture) is usually kept.
The length (the number of characters) is also an im-
portant feature of the current word.
3. Position of the current character in the cur-
rent word Previous work (Chang and Lai, 2004)
showed that the first character of a word has high
possibility to form part of the abbreviation and this
is also true for the last character of a three-character
word.
4. Combination of feature 2. and 3. above
Combination of the features 2 and 3 is expected to
improve the performance, since the position infor-
274
mation affects the abbreviation along with the cur-
rent word. For example, ending character in ????
(university) and that in ???? (research institute)
have very different possibilities to be selected for ab-
breviations.
Besides the features above, we have examined
context information (previous word, previous char-
acter, next character, etc.) and other local features
like the length of the word, but these features did
not improve the performance. The reason may be
due to the sparseness of the training data.
4 Improvement by a Length Model and a
Web Search Engine
4.1 Length model
There is a strong correlation between the length of
organizations? full-names and their abbreviations.
We use the length modeling based on discrete prob-
ability of P (M |L), in which the variables M and
L are lengths of abbreviations and full-names, re-
spectively. Since it is difficult to incorporate length
information into the CRF model explicitly, we use
P (M |L) to rescore the output of the CRF.
In order to use the length information, we model
the abbreviation process with two steps:
? 1st step: evaluate the length in abbreviation ac-
cording to the length model P (M |L);
? 2nd step: choose the abbreviation, given the
length and full-name.
We assume the following approximation:
P (A|F ) ? P (M |L) ? P (A|M,F ) (2)
in which variable A is the abbreviation and F is the
full-name; P (M |L) is the length model, and the sec-
ond probability can be calculated according to the
Bayesian rule:
P (A|M,F ) = P (A,M |F )P (M |F )
= P (A,M |F )?
length(A?)=M P (A?,M |F )
(3)
It is obvious that P (A,M |F ) = P (A|F ) (as A
contains the information M implicitly) and P (A|F )
can be obtained from the output of the CRF.
4.2 Web search engine
Co-occurrence of a full-name and an abbreviation
candidate can be a clue of the correctness of the ab-
breviation. We use the ?abbreviation candidate?+
?full-name? as queries and input them to the most
popular Chinese search engine (www.baidu.com),
and then we use the number of hits as the metric
to perform re-ranking. The hits is theoretically re-
lated to the number of pages which contain both the
full-name and abbreviation. The bigger the value of
hits, the higher probability that the abbreviation is
correct.
We then simply multiply the previous probability
score, obtained from Eq. 2, by the number of hits
and re-rank the top-30 candidates accordingly.
There are some other ways to use information re-
trieval methods (Mandala et al, 2000). Our method
has an advantage that the access load to the web
search engine is relatively small.
5 Experiment
5.1 Data introduction
The corpus we use in this paper comes from two
sources: one is the book ?modern Chinese abbre-
viation dictionary? (Yuan and Ruan, 2002) and the
other is the wikipedia. Altogether we collected 1945
pairs of organization full-names and their abbrevia-
tions.
The data is randomly divided into two parts, a
training set with 1298 pairs and a test set with 647
pairs. Table 1 shows the length mapping statistics
of the training set. It can be seen that the average
length of full-names is about 7.29. We know that for
a full-name with length N, the number of abbrevia-
tion candidates is about 2N ? 2?N (exclude length
of 0, 1, and N) and we can conclude that the average
number of candidates for organization names in this
corpus is more than 100.
5.2 Results
The abbreviation method described is part of a
project to develop a voice-based search application.
For our name abbreviation system we plan to add 10
abbreviation candidates for each organization name
into the vocabulary of our voice search application,
hence here we consider top-10 coverage.
275
length of length of abbreviation
full-name 2 3 4 5 >5 sum
4 107 1 0 0 0 108
5 89 140 0 0 0 229
6 96 45 46 0 0 187
7 60 189 49 16 0 314
8 48 29 60 3 6 146
9 10 47 35 12 2 106
10 18 11 29 8 6 73
others 21 43 38 17 14 133
average length of the full-name 7.27
average length of the abbreviation 3.01
Table 1: Length statistics on the training set
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 72?75,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Combining a Two-step Conditional Random Field Model and a Joint
Source Channel Model for Machine Transliteration
Dong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku Oonishi
Masanobu Nakamura and Sadaoki Furui
Department of Computer Science
Tokyo Institute of Techonology
{raymond,dixonp,thomas,oonishi,masa,furui}@furui.cs.titech.ac.jp
Abstract
This paper describes our system for
?NEWS 2009 Machine Transliteration
Shared Task? (NEWS 2009). We only par-
ticipated in the standard run, which is a
direct orthographical mapping (DOP) be-
tween two languages without using any
intermediate phonemic mapping. We
propose a new two-step conditional ran-
dom field (CRF) model for DOP machine
transliteration, in which the first CRF seg-
ments a source word into chunks and the
second CRF maps the chunks to a word
in the target language. The two-step CRF
model obtains a slightly lower top-1 ac-
curacy when compared to a state-of-the-
art n-gram joint source-channel model.
The combination of the CRF model with
the joint source-channel leads to improve-
ments in all the tasks. The official re-
sult of our system in the NEWS 2009
shared task confirms the effectiveness of
our system; where we achieved 0.627 top-
1 accuracy for Japanese transliterated to
Japanese Kanji(JJ), 0.713 for English-to-
Chinese(E2C) and 0.510 for English-to-
Japanese Katakana(E2J) .
1 Introduction
With the increasing demand for machine transla-
tion, the out-of-vocabulary (OOV) problem caused
by named entities is becoming more serious.
The translation of named entities from an alpha-
betic language (like English, French and Spanish)
to a non-alphabetic language (like Chinese and
Japanese) is usually performed through transliter-
ation, which tries to preserve the pronunciation in
the source language.
For example, in Japanese, foreign words im-
ported from other languages are usually written
H a r r i n g t o n ? ? ? ? ? English-to-Japanese
T i m o t h y ??? English-to-Chinese
Source Name       Target Name          Note
ti mo   xi                     Chinese Romanized writing
ha  ri n   to  n Japanese Romanized writing
Figure 1: Transliteration examples
in a special syllabary called Katakana; in Chi-
nese, foreign words accepted to Chinese are al-
ways written by Chinese characters; examples are
given in Figure 1.
An intuitive transliteration method is to first
convert a source word into phonemes, then find the
corresponding phonemes in the target language,
and finally convert to the target language?s writ-
ing system (Knight and Graehl, 1998; Oh et al,
2006). One major limitation of this method is that
the named entities are usually OOVs with diverse
origins and this makes the grapheme-to-phoneme
conversion very difficult.
DOP is gaining more attention in the transliter-
ation research community which is also the stan-
dard evaluation of NEWS 2009.
The source channel and joint source-channel
models (Li et al, 2004) have been proposed for
DOP, which try to model P (T |S) and P (T, S) re-
spectively, where T and S denotes the words in
the target and source languages. (Ekbal et al,
2006) modified the joint source-channel model to
incorporate different context information into the
model for the Indian languages. Here we propose
a two-step CRF model for transliteration, and the
idea is to make use of the discriminative ability of
CRF. For example, in E2C transliteration, the first
step is to segment an English name into alphabet
chunks and after this step the number of Chinese
characters is decided. The second step is to per-
form a context-dependent mapping from each En-
glish chunk into one Chinese character. Figure 1
shows that this method is applicable to many other
72
transliteration tasks including E2C and E2J.
Our CRF method and the n-gram joint source-
channel model use different information in pre-
dicting the corresponding Chinese characters and
therefore in combination better results are ex-
pected. We interpolate the two models linearly
and use this as our final system for NEWS 2009.
The rest of the paper is organized as follows: Sec-
tion 2 introduces our system in detail including the
alignment and decoding modules, Section 3 ex-
plains our experiments and finally Section 4 de-
scribes conclusions and future work.
2 System Description
Our system starts from a joint source channel
alignment to train the CRF segmenter. The CRF
is used to re-segment and align the training data,
and from this alignment we create a Weighted Fi-
nite State Transducer (WFST) based n-gram joint
source-channel decoder and a CRF E2C converter.
The following subsections explain the structure of
our system shown in Figure 2.
N-gram joint source-channel Alignment
CRF segmenter
N-gram WFST decoder CRF E2C converter
Each pair in the training corpus
New Alignment
N-gram WFST decoder
CRF E2C converter
Linear combination
Each source name in the test corpus
CRF segmenter
Tr
ai
ni
ng
Te
st
in
g
Output
Figure 2: System structure
2.1 Theoretical background
2.1.1 Joint source channel model
The source channel model represents the condi-
tional probability of target names given a source
name P (T |S). The joint source channel model
calculates how the source words and target names
are generated simultaneously (Li et al, 2004):
P (S, T ) = P (s1, s2, ..., sk, t1, t2, ..., tk)
= P (< s, t >1, < s, t >2, ..., < s, t >k)
=
K?
k=1
P (< s, t >k | < s, t >k?11 ) (1)
where, S = (s1, s2, ..., sk) and T =
(t1, t2, ..., tk).
2.1.2 CRF
A CRF (Lafferty et al, 2001) is an undirected
graphical model which assigns a probability to a
label sequence L = l1l2 . . . lT , given an input se-
quence C = c1c2 . . . cT ,
P (L|C) = 1
Z(C)
exp(
T?
t=1
?
k
?kfk(lt, lt?1, C, t))
(2)
For the kth feature, fk denotes the feature function
and ?k is the parameter which controls the weight-
ing. Z(C) is a normalization term that ensure the
distribution sums to one. CRF training is usually
performed through the L-BFGS algorithm (Wal-
lach, 2002) and decoding is performed by Viterbi
algorithm (Viterbi, 1967). In this paper, we use an
open source toolkit ?crf++?1.
2.2 N-gram joint source-channel alignment
To calculate the probability in Equation 1, the
training corpus needs to be aligned first. We use
the Expectation-Maximization(EM) algorithm to
optimize the alignment A between the source S
and target T pairs, that is:
A? = arg max
A
P (S, T,A) (3)
The procedure is summarized as follows:
1. Initialize a random alignment
2. E-step: update n-gram probability
3. M-step: apply the n-gram model to realign
each entry in corpus
4. Go to step 2 until the alignment converges
2.3 CRF alignment & segmentation
The performance of EM algorithm is often af-
fected by the initialization. Fortunately, we can
correct mis-alignments by using the discriminative
ability of the CRF. The alignment problem is con-
verted into a tagging problem that doesn?t require
the use of the target words at all. Figure 3 is an
example of a segmentation and alignment, where
the labels B and N indicate whether the character
is in the starting position of the chunk or not.
In the CRF method the feature function de-
scribes a co-occurrence relation, and it is formally
1crfpp.sourceforge.net
73
T i m o t h y ???
T/B i/N m/B o/N t/B h/N y/N
Ti/? mo/? thy/?
Figure 3: An example of the CRF segmenter for-
mat and E2C converter
defined as fk(lt, lt?1, C, t) (Eq. 2). fk is usually a
binary function, and takes the value 1 when both
observation ct and transition lt?1 ? lt are ob-
served. In our segmentation tool, we use the fol-
lowing features
? 1. Unigram features: C?2, C?1, C0, C1, C2
? 2. Bigram features:C?1C0, C0C1
Here, C0 is the current character, C?1 and C1 de-
note the previous and next characters and C?2 and
C2 are the characters two positions to the left and
right of C0.
In the alignment process, we use the CRF seg-
menter to split each English word into chunks.
Sometimes a problem occurs in which the num-
ber of chunks in the segmented output will not be
equal to the number of Chinese characters. In such
cases our solution is to choose from the n-best list
the top scoring segmentation which contains the
correct number of chunks.
In the testing process, we use the segmenter in
the similar way, but only take top-1 output seg-
mented English chunks for use in the following
CRF E2C conversion.
2.4 CRF E2C converter
Similar to the CRF segmenter, the CRF E2C con-
verter has the format shown in Figure 3. For this
CRF, we use the following features:
? 1. Unigram features: C?1, C0, C1
? 2. Bigram features:C?1C0, C0C1
where C represents the English chunks and the
subscript notation is the same as the CRF seg-
menter.
2.5 N-gram WFST decoder for joint source
channel model
Our decoding approach makes use of WFSTs to
represent the models and simplify the develop-
ment by utilizing standard operations such as com-
position and shortest path algorithms.
After the alignments are generated, the first
step is to build a corpus to train the translit-
eration WFST. Each aligned word is converted
to a sequence of transliteration alignment pairs
?s, t?1 , ?s, t?2 , ... ?s, t?k, where each s can be a
chunk of one or more characters and t is assumed
to be a single character. Each of the pairs is
treated as a word and the entire set of alignments is
used to train an n-gram language model. In these
evaluations we used the MITLM toolkit (Hsu and
Glass, 2008) to build a trigram model with modi-
fied Kneser-Ney smoothing.
We then use the procedure described in (Caseiro
et al, 2002) and convert the n-gram to a weighted
acceptor representation where each input label be-
longs to the set of transliteration alignment pairs.
Next the pairs labels are broken down into the in-
put and output parts and the acceptor is converted
to a transducer M . To allow transliteration from a
sequence of individual characters, a second WFST
T is constructed. T has a single state and for each
s a path is added to allow a mapping from the
string of individual characters.
To perform the actual transliteration, the input
word is converted to an acceptor I which has one
arc for each of the characters in the word. I is
then combined with T and M according to O =
I ?T ?M where ? denotes the composition opera-
tor. The n?best paths are extracted from O by pro-
jecting the output, removing the epsilon labels and
applying the n-shortest paths algorithm with de-
terminization from the OpenFst Toolkit(Allauzen
et al, 2007).
2.6 Linear combination
We notice that there is a significant difference be-
tween the correct answers of the n-gram WFST
and CRF decoders. The reason may be due to
the different information utilized in the two de-
coding methods. Since their performance levels
are similar, the overall performance is expected
to be improved by the combination. From the
CRF we compute the probability PCRF (T |S) and
from the list of scores output from the n-gram de-
coder we calculate the conditional probability of
Pn?gram(T |S). These are used in our combina-
tion method according to:
P (T |S) = ?PCRF (T |S)+(1??)Pn?gram(T |S)
(4)
where ? denotes the interpolation weight (0.3 in
this paper).
74
3 Experiments
We use the training and development sets of
NEWS 2009 data in our experiments as detailed
in Table 12. There are several measure metrics in
the shared task and due to limited space in this pa-
per we provide the results for top-1 accuracy.
Task Training data size Test data size
E2C 31961 2896
E2J 23808 1509
Table 1: Corpus introduction
n-gram+CRF
Task Alignment interpolation
WFST CRF
E2C 70.3 67.3 71.5
E2J 44.9 44.8 46.7
Table 2: Top-1 accuracies(%)
The results are listed in Table 2. For E2C
task the top-1 accuracy of the joint source-channel
model is 70.3% and 67.3% for the two-step CRF
model. After combining the two results together
the top-1 accuracy increases to 71.5% correspond-
ing to a 1.2% absolute improvement over the state-
of-the-art joint source-channel model. Similarly,
we get 1.8% absolute improvement for E2J task.
4 Conclusions and future work
In this paper we have presented our new hybrid
method for machine transliteration which com-
bines a new two-step CRF model with a state-of-
the-art joint source-channel model. In compari-
son to the joint source-channel model the combi-
nation approach achieved 1.2% and 1.8% absolute
improvements for E2C and E2J task respectively.
In the first step of the CRF method we only
use the top-1 segmentation, which may propagate
transliteration errors to the following step. In fu-
ture work we would like to optimize the 2-step
CRF jointly. Currently, we are also investigating
minimum classification error (MCE) discriminant
training as a method to further improve the joint
source channel model.
2For the JJ task the submitted results
are only based on the joint source
channel model. Unfortunately, we were
unable to submit a combination result
because the training time for the CRF
was too long.
Acknowledgments
The corpora used in this paper are from ?NEWS
2009 Machine Transliteration Shared Task? (Li et
al., 2004; CJK, website)
References
Kevin Knight and Jonathan Graehl. 1998. Machine
Transliteration, 1998 Association for Computa-
tional Linguistics.
Li Haizhou, Zhang Min and Su Jian. 2004. A joint
source-channel model for machine transliteration,
2004 Proceedings of the 42nd Annual Meeting on
Association for Computational Linguistics.
Asif Ekbal, Sudip Kumar Naskar and Sivaji Bandy-
opadhyay. 2006. A modified joint source-channel
model for transliteration, Proceedings of the COL-
ING/ACL, pages 191-198.
Jong-Hoon Oh, Key-Sun Choi and Hitoshi Isahara.
2006. A comparison of different machine transliter-
ation models , Journal of Artificial Intelligence Re-
search, 27, pages 119-151.
John Lafferty, Andrew McCallum, and Fernando
Pereira 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data., Proceedings of International Confer-
ence on Machine Learning, 2001, pages 282-289.
Hanna Wallach 2002. Efficient Training of Condi-
tional Random Fields. M. Thesis, University of Ed-
inburgh, 2002.
Andrew J. Viterbi 1967. Error Bounds for Convolu-
tional Codes and an Asymptotically Optimum De-
coding Algorithm. IEEE Transactions on Informa-
tion Theory, Volume IT-13, 1967,pages 260-269.
Bo-June Hsu and James Glass 2008. Iterative Lan-
guage Model Estimation: Efficient Data Structure
& Algorithms. Proceedings Interspeech, pages 841-
844.
Diamantino Caseiro, Isabel Trancosoo, Luis Oliveira
and Ceu Viana 2002. Grapheme-to-phone using
finite state transducers. Proceedings 2002 IEEE
Workshop on Speech Synthesis.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut and Mehryar Mohri 2002. OpenFst: A
General and Efficient Weighted Finite-State Trans-
ducer Library. Proceedings of the Ninth Interna-
tional Conference on Implementation and Applica-
tion of Automata, (CIAA 2007), pages 11-23.
http://www.cjk.org
75
Proceedings of the ACL 2010 Conference Short Papers, pages 275?280,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Jointly optimizing a two-step conditional random field model for machine
transliteration and its fast decoding algorithm
Dong Yang, Paul Dixon and Sadaoki Furui
Department of Computer Science
Tokyo Institute of Technology
Tokyo 152-8552 Japan
{raymond,dixonp,furui}@furui.cs.titech.ac.jp
Abstract
This paper presents a joint optimization
method of a two-step conditional random
field (CRF) model for machine transliter-
ation and a fast decoding algorithm for
the proposed method. Our method lies in
the category of direct orthographical map-
ping (DOM) between two languages with-
out using any intermediate phonemic map-
ping. In the two-step CRF model, the first
CRF segments an input word into chunks
and the second one converts each chunk
into one unit in the target language. In this
paper, we propose a method to jointly op-
timize the two-step CRFs and also a fast
algorithm to realize it. Our experiments
show that the proposed method outper-
forms the well-known joint source channel
model (JSCM) and our proposed fast al-
gorithm decreases the decoding time sig-
nificantly. Furthermore, combination of
the proposed method and the JSCM gives
further improvement, which outperforms
state-of-the-art results in terms of top-1 ac-
curacy.
1 Introduction
There are more than 6000 languages in the world
and 10 languages of them have more than 100 mil-
lion native speakers. With the information revolu-
tion and globalization, systems that support mul-
tiple language processing and spoken language
translation become urgent demands. The transla-
tion of named entities from alphabetic to syllabary
language is usually performed through translitera-
tion, which tries to preserve the pronunciation in
the original language.
For example, in Chinese, foreign words are
written with Chinese characters; in Japanese, for-
eign words are usually written with special char-
G o o g l e ?? ? ? English-to-Japanese
G o o g l e ?? English-to-Chinese
Source Name       Target Name          Note
gu ge Chinese Romanized writing         
guu gu ru Japanese Romanized writing
Figure 1: Transliteration examples
acters called Katakana; examples are given in Fig-
ure 1.
An intuitive transliteration method (Knight and
Graehl, 1998; Oh et al, 2006) is to firstly convert
a source word into phonemes, then find the corre-
sponding phonemes in the target language, and fi-
nally convert them to the target language?s written
system. There are two reasons why this method
does not work well: first, the named entities have
diverse origins and this makes the grapheme-to-
phoneme conversion very difficult; second, the
transliteration is usually not only determined by
the pronunciation, but also affected by how they
are written in the original language.
Direct orthographical mapping (DOM), which
performs the transliteration between two lan-
guages directly without using any intermediate
phonemic mapping, is recently gaining more at-
tention in the transliteration research community,
and it is also the ?Standard Run? of the ?NEWS
2009 Machine Transliteration Shared Task? (Li et
al., 2009). In this paper, we try to make our system
satisfy the standard evaluation condition, which
requires that the system uses the provided parallel
corpus (without pronunciation) only, and cannot
use any other bilingual or monolingual resources.
The source channel and joint source channel
models (JSCMs) (Li et al, 2004) have been pro-
posed for DOM, which try to model P (T |S) and
P (T, S) respectively, where T and S denote the
words in the target and source languages. Ekbal
et al (2006) modified the JSCM to incorporate
different context information into the model for
275
Indian languages. In the ?NEWS 2009 Machine
Transliteration Shared Task?, a new two-step CRF
model for transliteration task has been proposed
(Yang et al, 2009), in which the first step is to
segment a word in the source language into char-
acter chunks and the second step is to perform a
context-dependent mapping from each chunk into
one written unit in the target language.
In this paper, we propose to jointly optimize a
two-step CRF model. We also propose a fast de-
coding algorithm to speed up the joint search. The
rest of this paper is organized as follows: Sec-
tion 2 explains the two-step CRF method, fol-
lowed by Section 3 which describes our joint opti-
mization method and its fast decoding algorithm;
Section 4 introduces a rapid implementation of a
JSCM system in the weighted finite state trans-
ducer (WFST) framework; and the last section
reports the experimental results and conclusions.
Although our method is language independent, we
use an English-to-Chinese transliteration task in
all the explanations and experiments.
2 Two-step CRF method
2.1 CRF introduction
A chain-CRF (Lafferty et al, 2001) is an undi-
rected graphical model which assigns a probability
to a label sequence L = l1l2 . . . lT , given an input
sequence C = c1c2 . . . cT . CRF training is usually
performed through the L-BFGS algorithm (Wal-
lach, 2002) and decoding is performed by the
Viterbi algorithm. We formalize machine translit-
eration as a CRF tagging problem, as shown in
Figure 2.
T i m o t h y ???
T/B i/N m/B o/N t/B h/N y/N
Ti/? mo/? thy/?
Figure 2: An pictorial description of a CRF seg-
menter and a CRF converter
2.2 CRF segmenter
In the CRF, a feature function describes a co-
occurrence relation, and it is usually a binary func-
tion, taking the value 1 when both an observa-
tion and a label transition are observed. Yang et
al. (2009) used the following features in the seg-
mentation tool:
? Single unit features: C?2, C?1, C0, C1, C2
? Combination features: C?1C0, C0C1
Here, C0 is the current character, C?1 and C1 de-
note the previous and next characters, and C?2 and
C2 are the characters located two positions to the
left and right of C0.
One limitation of their work is that only top-1
segmentation is output to the following CRF con-
verter.
2.3 CRF converter
Similar to the CRF segmenter, the CRF converter
has the format shown in Figure 2.
For this CRF, Yang et al (2009) used the fol-
lowing features:
? Single unit features: CK?1, CK0, CK1
? Combination features: CK?1CK0,
CK0CK1
where CK represents the source language chunk,
and the subscript notation is the same as the CRF
segmenter.
3 Joint optimization and its fast decoding
algorithm
3.1 Joint optimization
We denote a word in the source language by S, a
segmentation of S by A, and a word in the target
langauge by T . Our goal is to find the best word T?
in the target language which maximizes the prob-
ability P (T |S).
Yang et al (2009) used only the best segmen-
tation in the first CRF and the best output in the
second CRF, which is equivalent to
A? = arg max
A
P (A|S)
T? = arg max
T
P (T |S, A?), (1)
where P (A|S) and P (T |S,A) represent two
CRFs respectively. This method considers the seg-
mentation and the conversion as two independent
steps. A major limitation is that, if the segmenta-
tion from the first step is wrong, the error propa-
gates to the second step, and the error is very dif-
ficult to recover.
In this paper, we propose a new method to
jointly optimize the two-step CRF, which can be
276
written as:
T? = arg max
T
P (T |S)
= arg max
T
?
A
P (T,A|S)
= arg max
T
?
A
P (A|S)P (T |S,A)
(2)
The joint optimization considers all the segmen-
tation possibilities and sums the probability over
all the alternative segmentations which generate
the same output. It considers the segmentation and
conversion in a unified framework and is robust to
segmentation errors.
3.2 N-best approximation
In the process of finding the best output using
Equation 2, a dynamic programming algorithm for
joint decoding of the segmentation and conversion
is possible, but the implementation becomes very
complicated. Another direction is to divide the de-
coding into two steps of segmentation and conver-
sion, which is this paper?s method. However, exact
inference by listing all possible candidates explic-
itly and summing over all possible segmentations
is intractable, because of the exponential computa-
tion complexity with the source word?s increasing
length.
In the segmentation step, the number of possible
segmentations is 2N , where N is the length of the
source word and 2 is the size of the tagging set. In
the conversion step, the number of possible candi-
dates is MN ? , where N ? is the number of chunks
from the 1st step and M is the size of the tagging
set. M is usually large, e.g., about 400 in Chinese
and 50 in Japanese, and it is impossible to list all
the candidates.
Our analysis shows that beyond the 10th candi-
date, almost all the probabilities of the candidates
in both steps drop below 0.01. Therefore we de-
cided to generate top-10 results for both steps to
approximate the Equation 2.
3.3 Fast decoding algorithm
As introduced in the previous subsection, in the
whole decoding process we have to perform n-best
CRF decoding in the segmentation step and 10 n-
best CRF decoding in the second CRF. Is it really
necessary to perform the second CRF for all the
segmentations? The answer is ?No? for candidates
with low probabilities. Here we propose a no-loss
fast decoding algorithm for deciding when to stop
performing the second CRF decoding.
Suppose we have a list of segmentation candi-
dates which are generated by the 1st CRF, ranked
by probabilities P (A|S) in descending order A :
A1, A2, ..., AN and we are performing the 2nd
CRF decoding starting from A1. Up to Ak,
we get a list of candidates T : T1, T2, ..., TL,
ranked by probabilities in descending order. If
we can guarantee that, even performing the 2nd
CRF decoding for all the remaining segmentations
Ak+1, Ak+2, ..., AN , the top 1 candidate does not
change, then we can stop decoding.
We can show that the following formula is the
stop condition:
Pk(T1|S) ? Pk(T2|S) > 1 ?
k
?
j=1
P (Aj |S). (3)
The meaning of this formula is that the prob-
ability of all the remaining candidates is smaller
than the probability difference between the best
and the second best candidates; on the other hand,
even if all the remaining probabilities are added to
the second best candidate, it still cannot overturn
the top candidate. The mathematical proof is pro-
vided in Appendix A.
The stop condition here has no approximation
nor pre-defined assumption, and it is a no-loss fast
decoding algorithm.
4 Rapid development of a JSCM system
The JSCM represents how the source words and
target names are generated simultaneously (Li et
al., 2004):
P (S, T ) = P (s1, s2, ..., sk, t1, t2, ..., tk)
= P (< s, t >1, < s, t >2, ..., < s, t >k)
=
K
?
k=1
P (< s, t >k | < s, t >k?11 ) (4)
where S = (s1, s2, ..., sk) is a word in the source
langauge and T = (t1, t2, ..., tk) is a word in the
target language.
The training parallel data without alignment is
first aligned by a Viterbi version EM algorithm (Li
et al, 2004).
The decoding problem in JSCM can be written
as:
T? = arg max
T
P (S, T ). (5)
277
After the alignments are generated, we use the
MITLM toolkit (Hsu and Glass, 2008) to build a
trigram model with modified Kneser-Ney smooth-
ing. We then convert the n-gram to a WFST
M (Sproat et al, 2000; Caseiro et al, 2002). To al-
low transliteration from a sequence of characters,
a second WFST T is constructed. The input word
is converted to an acceptor I , and it is then com-
bined with T and M according to O = I ? T ?M
where ? denotes the composition operator. The
n?best paths are extracted by projecting the out-
put, removing the epsilon labels and applying the
n-shortest paths algorithm with determinization in
the OpenFst Toolkit (Allauzen et al, 2007).
5 Experiments
We use several metrics from (Li et al, 2009) to
measure the performance of our system.
1. Top-1 ACC: word accuracy of the top-1 can-
didate
2. Mean F-score: fuzziness in the top-1 candi-
date, how close the top-1 candidate is to the refer-
ence
3. MRR: mean reciprocal rank, 1/MRR tells ap-
proximately the average rank of the correct result
5.1 Comparison with the baseline and JSCM
We use the training, development and test sets of
NEWS 2009 data for English-to-Chinese in our
experiments as detailed in Table 1. This is a paral-
lel corpus without alignment.
Training data Development data Test data
31961 2896 2896
Table 1: Corpus size (number of word pairs)
We compare the proposed decoding method
with the baseline which uses only the best candi-
dates in both CRF steps, and also with the well
known JSCM. As we can see in Table 2, the pro-
posed method improves the baseline top-1 ACC
from 0.670 to 0.708, and it works as well as, or
even better than the well known JSCM in all the
three measurements.
Our experiments also show that the decoding
time can be reduced significantly via using our fast
decoding algorithm. As we have explained, with-
out fast decoding, we need 11 CRF n-best decod-
ing for each word; the number can be reduced to
3.53 (1 ?the first CRF?+2.53 ?the second CRF?)
via the fast decoding algorithm.
We should notice that the decoding time is sig-
nificantly shorter than the training time. While
testing takes minutes on a normal PC, the train-
ing of the CRF converter takes up to 13 hours on
an 8-core (8*3G Hz) server.
Measure Top-1 Mean MRR
ACC F-score
Baseline 0.670 0.869 0.750
Joint optimization 0.708 0.885 0.789
JSCM 0.706 0.882 0.789
Table 2: Comparison of the proposed decoding
method with the previous method and the JSCM
5.2 Further improvement
We tried to combine the two-step CRF model and
the JSCM. From the two-step CRF model we get
the conditional probability PCRF (T |S) and from
the JSCM we get the joint probability P (S, T ).
The conditional probability of PJSCM(T |S) can
be calculuated as follows:
PJSCM (T |S) =
P (T, S)
P (S) =
P (T, S)
?
T P (T, S)
. (6)
They are used in our combination method as:
P (T |S) = ?PCRF (T |S) + (1 ? ?)PJSCM (T |S)
(7)
where ? denotes the interpolation weight (? is set
by development data in this paper).
As we can see in Table 3, the linear combination
of two sytems further improves the top-1 ACC to
0.720, and it has outperformed the best reported
?Standard Run? (Li et al, 2009) result 0.717. (The
reported best ?Standard Run? result 0.731 used
target language phoneme information, which re-
quires a monolingual dictionary; as a result it is
not a standard run.)
Measure Top-1 Mean MRR
ACC F-score
Baseline+JSCM 0.713 0.883 0.794
Joint optimization
+ JSCM 0.720 0.888 0.797
state-of-the-art 0.717 0.890 0.785
(Li et al, 2009)
Table 3: Model combination results
6 Conclusions and future work
In this paper we have presented our new joint
optimization method for a two-step CRF model
and its fast decoding algorithm. The proposed
278
method improved the system significantly and out-
performed the JSCM. Combining the proposed
method with JSCM, the performance was further
improved.
In future work we are planning to combine our
system with multilingual systems. Also we want
to make use of acoustic information in machine
transliteration. We are currently investigating dis-
criminative training as a method to further im-
prove the JSCM. Another issue of our two-step
CRF method is that the training complexity in-
creases quadratically according to the size of the
label set, and how to reduce the training time needs
more research.
Appendix A. Proof of Equation 3
The CRF segmentation provides a list of segmen-
tations: A : A1, A2, ..., AN , with conditional
probabilities P (A1|S), P (A2|S), ..., P (AN |S).
N
?
j=1
P (Aj |S) = 1.
The CRF conversion, given a segmenta-
tion Ai, provides a list of transliteration out-
put T1, T2, ..., TM , with conditional probabilities
P (T1|S,Ai), P (T2|S,Ai), ..., P (TM |S,Ai).
In our fast decoding algorithm, we start per-
forming the CRF conversion from A1, then A2,
and then A3, etc. Up to Ak, we get a list of can-
didates T : T1, T2, ..., TL, ranked by probabili-
ties Pk(T |S) in descending order. The probability
Pk(Tl|S)(l = 1, 2, ..., L) is accumulated probabil-
ity of P (Tl|S) over A1, A2, ..., Ak , calculated by:
Pk(Tl|S) =
k
?
j=1
P (Aj |S)P (Tl|S,Aj)
If we continue performing the CRF conversion
to cover all N (N ? k) segmentations, eventually
we will get:
P (Tl|S) =
N
?
j=1
P (Aj |S)P (Tl|S,Aj)
?
k
?
j=1
P (Aj |S)P (Tl|S,Aj)
= Pk(Tl|S) (8)
If Equation 3 holds, then for ?i 6= 1,
Pk(T1|S) > Pk(T2|S) + (1 ?
k
?
j=1
P (Aj |S))
? Pk(Ti|S) + (1 ?
k
?
j=1
P (Aj |S))
= Pk(Ti|S) +
N
?
j=k+1
P (Aj |S)
? Pk(Ti|S)
+
N
?
j=k+1
P (Aj |S)P (Ti|S,Aj)
= P (Ti|S) (9)
Therefore, P (T1|S) > P (Ti|S)(i 6= 1), and T1
maximizes the probability P (T |S).
279
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut and Mehryar Mohri 2007. OpenFst: A
General and Efficient Weighted Finite-State Trans-
ducer Library. Proceedings of the Ninth Interna-
tional Conference on Implementation and Applica-
tion of Automata, (CIAA), pages 11-23.
Diamantino Caseiro, Isabel Trancosoo, Luis Oliveira
and Ceu Viana 2002. Grapheme-to-phone using fi-
nite state transducers. Proceedings IEEE Workshop
on Speech Synthesis.
Asif Ekbal, Sudip Kumar Naskar and Sivaji Bandy-
opadhyay. 2006. A modified joint source-channel
model for transliteration, Proceedings of the COL-
ING/ACL, pages 191-198.
Bo-June Hsu and James Glass 2008. Iterative Lan-
guage Model Estimation: Efficient Data Structure
& Algorithms. Proceedings Interspeech, pages 841-
844.
Kevin Knight and Jonathan Graehl. 1998. Machine
Transliteration, Association for Computational Lin-
guistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data., Proceedings of International Confer-
ence on Machine Learning, pages 282-289.
Haizhou Li, Min Zhang and Jian Su. 2004. A joint
source-channel model for machine transliteration,
Proceedings of the 42nd Annual Meeting on Asso-
ciation for Computational Linguistics.
Haizhou Li, A. Kumaran, Vladimir Pervouchine and
Min Zhang 2009. Report of NEWS 2009 Ma-
chine Transliteration Shared Task, Proceedings of
the 2009 Named Entities Workshop: Shared Task on
Transliteration (NEWS 2009), pages 1-18
Jong-Hoon Oh, Key-Sun Choi and Hitoshi Isahara.
2006. A comparison of different machine transliter-
ation models , Journal of Artificial Intelligence Re-
search, 27, pages 119-151.
Richard Sproat 2000. Corpus-Based Methods and
Hand-Built Methods. Proceedings of International
Conference on Spoken Language Processing, pages
426-428.
Andrew J. Viterbi 1967. Error Bounds for Convolu-
tional Codes and an Asymptotically Optimum De-
coding Algorithm. IEEE Transactions on Informa-
tion Theory, Volume IT-13, pages 260-269.
Hanna Wallach 2002. Efficient Training of Condi-
tional Random Fields. M. Thesis, University of Ed-
inburgh.
Dong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku Oon-
ishi, Masanobu Nakamura and Sadaoki Furui 2009.
Combining a Two-step Conditional Random Field
Model and a Joint Source Channel Model for Ma-
chine Transliteration, Proceedings of the 2009
Named Entities Workshop: Shared Task on Translit-
eration (NEWS 2009), pages 72-75
280
