Proceedings of the ACL Student Research Workshop, pages 23?30,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Detecting Metaphor by Contextual Analogy
Eirini Florou
Dept of Linguistics, Faculty of Philosophy
University of Athens, Greece
eirini.florou@gmail.com
Abstract
As one of the most challenging issues in
NLP, metaphor identification and its in-
terpretation have seen many models and
methods proposed. This paper presents a
study on metaphor identification based on
the semantic similarity between literal and
non literal meanings of words that can ap-
pear at the same context.
1 Introduction
A metaphor is a literary figure of speech that de-
scribes a subject by asserting that it is, on some
point of comparison, the same as another other-
wise unrelated object. Metaphor is a type of anal-
ogy and is closely related to other rhetorical fig-
ures of speech that achieve their effects via asso-
ciation, comparison or resemblance including al-
legory, hyperbole, and simile. Rhetorical theo-
rists and other scholars of language have discussed
numerous dimensions of metaphors, though these
nomenclatures are by no means universal nor nec-
essarily mutually exclusive.
A very challenging task in linguistics is the
metaphor identification and the its interpreta-
tion. Metaphor identification procedure (MIP)
is a method for identifying metaphorically used
words in discourse. It can be used to recognize
metaphors in spoken and written language. The
procedure aims to determine the relationship of
a particular lexical unit in the discourse and rec-
ognize its use in a particular context as possibly
metaphorical. Since many words can be consid-
ered metaphorical in different contexts, MIP re-
quires a clear distinction between words that con-
vey metaphorical meaning and those that do not,
despite the fact that language generally differs in
the degrees of metaphoricity.
In this paper we propose a method for identi-
fying metaphorical usage in verbs. Our method
is looking for semantic analogies in the context
of a verb by comparing it against prior known in-
stances of literal and non-literal usage of the same
verb in different contexts. After discussing the
metaphor identication literature (Section 2), we
proceed to present our research proposal (Section
3) and to present and discuss our first experiments
based on WordNet similarity measures (Section
4). Experiment results help us to draw conclu-
sions and insights about analogical reasoning and
memory-based learning for this task and to outline
promising research paths (Section 5).
2 Background
According to Lakoff and Johnson (1980),
metaphor is a productive phenomenon that oper-
ates at the level of mental processes. Metaphor
is thus not merely a property of language, but
rather a property of thought. This view was sub-
sequently acquired and extended by a multitude
of approaches (Grady, 1997; Narayanan, 1997;
Fauconnier and Tuner, 2002; Feldman, 2006;
Pinker, 2007) and the term conceptual metaphor
was adopted to describe it.
In cognitive linguistics, conceptual metaphor, or
cognitive metaphor, refers to the understanding of
an idea, or conceptual domain, in terms of another,
for example, understanding quantity in terms of
directionality as in, for example, ?prices are ris-
ing?. A conceptual metaphor uses an idea and
links it to another in order to better understand
something. It is generaly accepted that the concep-
tual metaphor of viewing communication as a con-
duit is a large theory explained with a metaphor.
These metaphors are prevalent in communication
and everyone actually perceives and acts in accor-
dance with the metaphors.
2.1 Metaphor Identification
Automatic processing of metaphor can be clearly
divided into two subtasks: metaphor identifica-
23
tion (distinguishing between literal and metaphor-
ical language in text) and metaphor interpreta-
tion (identifying the intended literal meaning of a
metaphorical expression). Both of them have been
repeatedly attempted in NLP.
The most influential account of metaphor iden-
tification is that of Wilks (1978). According to
Wilks, metaphors represent a violation of selec-
tional restrictions in a given context. Consider an
example such as My car drinks gasoline; the verb
drink normally takes an animate subject and a liq-
uid object.
This approach was automated by Fass (1991)
in his MET* system. However, Fass himself in-
dicated a problem with the method: it detects
any kind of non-literalness or anomaly in lan-
guage (metaphors, metonymies and others), i.e.,
it overgenerates with respect to metaphor. The
techniques MET* uses to differentiate between
those are mainly based on hand-coded knowledge,
which implies a number of limitations. First, lit-
eralness is distinguished from non-literalness us-
ing selectional preference violation as an indica-
tor. In the case that non-literalness is detected, the
respective phrase is tested for being a metonymic
relation using hand-coded patterns. If the system
fails to recognize metonymy, it proceeds to search
the knowledge base for a relevant analogy in or-
der to discriminate metaphorical relations from
anomalous ones.
Berber Sardinha (2002) describes a collocation-
based method for spotting metaphors in corpora.
His procedure is based on the notion that two
words sharing collocations in a corpus may have
been used metaphorically. The first step was to
pick out a reasonable number of words that had
an initial likelihood of being part of metaphori-
cal expressions. First, words with marked fre-
quency (in relation to a large general corpus of
Portuguese) were selected. Then, their colloca-
tions were scored for closeness in meaning using
a program called ?distance? (Padwardhan et al,
2003), under the assumption that words involved
in metaphorical expressions tend to be denota-
tionally unrelated. This program accesses Word-
Net in order to set the scores for each word pair.
The scores had to be adapted in order for them
to be useful for metaphor analysis. Finally, those
words that had an acceptable semantic distance
score were evaluated for their metaphoric poten-
tial. The results indicated that the procedure did
pick up some major metaphors in the corpus, but
it also captured metonyms.
Another approach to finding metaphor in cor-
pora is CorMet, presented by Mason (2004). It
works by searching corpora of different domains
for verbs that are used in similar patterns. When
the system spots different verbs with similar se-
lectional preferences (i.e., with similar words in
subject, object and complement positions), it con-
siders them potential metaphors.
CorMet requires specific domain corpora and a
list of verbs for each domain. The specific do-
main corpora are compiled by searching the web
for domain-specific words. These words are se-
lected by the author, based on his previous knowl-
edge of subject areas and are stemmed. The most
typical verbs for each specific corpus are identified
through frequency markedness, by comparing the
frequencies of word stems in the domain corpus
with those of the BNC. The resulting words have a
frequency that is statistically higher in the domain
corpus than in the reference corpus. These stems
are then classified according to part of speech by
consulting WordNet.
Alternative approaches search for metaphors
of a specific domain defined a priori in a spe-
cific type of discourse. The method by Gedi-
gian et al (2006) discriminates between literal and
metaphorical use. They trained a maximum en-
tropy classifier for this purpose. They obtained
their data by extracting the lexical items whose
frames are related to MOTION and CURE from
FrameNet (Fillmore et al, 2003). Then, they
searched the PropBank Wall Street Journal corpus
(Kingsbury and Palmer, 2002) for sentences con-
taining such lexical items and annotated them with
respect to metaphoricity.
Birke and Sarkar (2006) present a sentence clus-
tering approach for non-literal language recog-
nition implemented in the TroFi system (Trope
Finder). This idea originates from a similarity-
based word sense disambiguation method devel-
oped by Karov and Edelman (1998). The method
employs a set of seed sentences, where the senses
are annotated, computes similarity between the
sentence containing the word to be disambiguated
and all of the seed sentences and selects the sense
corresponding to the annotation in the most simi-
lar seed sentences. Birke and Sarkar (2006) adapt
this algorithm to perform a two-way classification:
literal vs. non-literal, and they do not clearly de-
24
fine the kinds of tropes they aim to discover. They
attain a performance of 53.8% in terms of f-score.
Both Birke and Sarkar (2006) and Gedigian
et al (2006) focus only on metaphors expressed
by a verb. As opposed to that the approach of Kr-
ishnakumaran and Zhu (2007) deals with verbs,
nouns and adjectives as parts of speech. They
use hyponymy relation in WordNet and word bi-
gram counts to predict metaphors at the sentence
level. Given an IS-A metaphor (e.g. The world is
a stage) they verify if the two nouns involved are
in hyponymy relation in WordNet, and if this is
not the case then this sentence is tagged as con-
taining a metaphor. Along with this they con-
sider expressions containing a verb or an adjec-
tive used metaphorically. Hereby they calculate
bigram probabilities of verb-noun and adjective-
noun pairs (including the hyponyms/hypernyms
of the noun in question). If the combination
is not observed in the data with sufficient fre-
quency, the system tags the sentence containing it
as metaphorical. This idea is a modification of the
selectional preference view of Wilks (1978).
Berber Sardinha (2010) presents a computer
program for identifying metaphor candidates,
which is intended as a tool that can help re-
searchers find words that are more likely to be
metaphor vehicles in a corpus. As such, it may be
used as a device for signalling those words that the
researcher might want to focus on first, because
these have a higher probability of being metaphors
in their corpus, or conversely, it may indicate those
words that are worth looking at because of their
apparent low probability of being metaphors. The
program is restricted to finding one component of
linguistic metaphors and has been trained on busi-
ness texts in Portuguese, and so it is restricted to
that kind of text.
Shutova et al (2012) present an approach to
automatic metaphor identification in unrestricted
text. Starting from a small seed set of manually
annotated metaphorical expressions, the system is
capable of harvesting a large number of metaphors
of similar syntactic structure from a corpus. Their
method captures metaphoricity by means of verb
and noun clustering. Their system starts from
a seed set of metaphorical expressions exempli-
fying a range of source-target domain mappings;
performs unsupervised noun clustering in order
to harvest various target concepts associated with
the same source domain; by means of unsuper-
vised verb clustering creates a source domain verb
lexicon; searches the BNC for metaphorical ex-
pressions describing the target domain concepts
using the verbs from the source domain lexicon.
According to Shutova et al (2012), abstract con-
cepts that are associated with the same source do-
main are often related to each other on an intu-
itive and rather structural level, but their mean-
ings, however, are not necessarily synonymous or
even semantically close. The consensus is that
the lexical items exposing similar behavior in a
large body of text most likely have the same mean-
ing. They tested their system starting with a col-
lection of metaphorical expressions representing
verb-subject and verb-object constructions, where
the verb is used metaphorically. They evaluated
the precision of metaphor identification with the
help of human judges. Shutova?s system employ-
ing unsupervised methods for metaphor identifica-
tion operates with precision of 0.79.
For verb and noun clustering, they used the sub-
categorization frame acquisition system by Preiss
et al (2007) and spectral clustering for both verbs
and nouns. They acquired selectional preference
distributions for Verb-Subject and Verb-Object re-
lations from the BNC parsed by RASP; adopted
Resnik?s selectional preference measure; and ap-
plied to a number of tasks in NLP including word
sense disambiguation (Resnik, 1997).
3 Detecting the metaphor use of a word
by contextual analogy
The first task for metaphor processing is its
identification in a text. We have seen above
how previous approaches either utilize hand-coded
knowledge (Fass, 1991), (Krishnakumaran and
Zhu, 2007) or reduce the task to searching for
metaphors of a specific domain defined a priori in
a specific type of discourse (Gedigian et al, 2006).
By contrast, our research proposal is a method
that relies on distributional similarity; the assump-
tion is that the lexical items showing similar be-
haviour in a large body of text most likely have
related meanings. Noun clustering, specifically,
is central to our approach. It is traditionally as-
sumed that noun clusters produced using distribu-
tional clustering contain concepts that are similar
to each other.
25
3.1 Word Sense Disambiguation and
Metaphor
One of the major developments in metaphor re-
search in the last several years has been the fo-
cus on identifying and explicating metaphoric lan-
guage in real discourse. Most research in Word
Sense Disambiguation has concentrated on using
contextual features, typically neighboring words,
to help infer the correct sense of a target word. In
contrast, we are going to discover the predominant
sense of a word from raw text because the first
sense heuristic is so powerful and because man-
ually sense-tagged data is not always available.
In word sense disambiguation, the first or pre-
dominant sense heuristic is used when informa-
tion from the context is not sufficient to make a
more informed choice. We will need to use parsed
data to find distributionally similar words (near-
est neighbors) to the target word which will reflect
the different senses of the word and have associ-
ated distributional similarity scores which could
be used for ranking the senses according to preva-
lence.
The predominant sense for a target word is de-
termined from a prevalence ranking of the possible
senses for that word. The senses will come from
a predefined inventory which might be a dictio-
nary or WordNet-like resource. The ranking will
be derived using a distributional thesaurus auto-
matically produced from a large corpus, and a se-
mantic similarity measure will be defined over the
sense inventory. The distributional thesaurus will
contain a set of words that will be ?nearest neigh-
bors? Lin (1998) to the target word with respect
to similarity of the way in which they will be dis-
tributed. The thesaurus will assign a distributional
similarity score to each neighbor word, indicating
its closeness to the target word.
We assume that the number and distributional
similarity scores of neighbors pertaining to a given
sense of a target word will reflect the prevalence of
that sense in the corpus from which the thesaurus
was derived. This is because the more prevalent
senses of the word will appear more frequently
and in more contexts than other, less prevalent
senses. The neighbors of the target word relate
to its senses, but are themselves word forms rather
than senses. The senses of the target word have
to be predefined in a sense inventory and we will
need to use a semantic similarity score which will
be defined over the sense inventory to relate the
neighbors to the various senses of the target word.
The measure for ranking the senses will use the
sum total of the distributional similarity scores of
the k nearest neighbors. This total will be divided
between the senses of the target word by appor-
tioning the distributional similarity of each neigh-
bor to the senses. The contribution of each neigh-
bor will be measured in terms of its distributional
similarity score so that ?nearer? neighbors count
for more. The distributional similarity score of
each neighbor will be divided between the vari-
ous senses rather than attributing the neighbor to
only one sense. This is done because neighbors
can relate to more than one sense due to relation-
ships such as systematic polysemy. To sum up, we
will rank the senses of the target word by appor-
tioning the distributional similarity scores of the
top k neighbors between the senses. Each distri-
butional similarity score (dss) will be weighted by
a normalized semantic similarity score (sss) be-
tween the sense and the neighbor.
We chose to use the distributional similarity
score described by Lin (1998) because it is an un-
parameterized measure which uses pointwise mu-
tual information to weight features and which has
been shown Weeds et al (2004) to be highly com-
petitive in making predictions of semantic similar-
ity. This measure is based on Lin?s information-
theoretic similarity theorem (Lin, 1997) : The sim-
ilarity between A and B is measured by the ratio
between the amount of information needed to state
the commonality of A and B and the information
needed to fully describe what A and B are.
3.2 Similarity-based metaphorical usage
estimation
After the noun clustering and finding the predomi-
nant sense of an ambiguous word, as the local con-
text of this word can give important clues to which
of its senses was intended, the metaphor identifi-
cation system will start from a small set of seed
metaphors (the seed metaphors are a model ex-
tracted from metaphor-annotated and dependency-
parsed sentences), to point out if a word is used lit-
eraly or non literaly at the certain context. For the
purposes of this work as context should be consid-
ered the verb of the seed metaphors. We are going
to take as seed metaphors the examples of Lakoff?s
Master Metaphor List (Lakoff et al, 1991).
Then, as we will have already find the k nearest
neighbors for each noun and we will have created
26
clusters for nouns which can appear at the same
context, we will be able to calculate their seman-
tic similarity. We then will use the WordNet sim-
ilarity package Padwardhan et al (2003) in order
to measure the semantic similarity between each
member of the cluster and the noun of the anno-
tated metaphor. The WordNet similarity package
supports a range of WordNet similarity scores. We
will experiment using a lot of these in order to
find those which perform the best. Each time, we
want to estimate if the similarity between the tar-
get noun and the seed metaphor will be higher than
the similarity between the target noun and another
literal word which could appear at the certain con-
text. Calculating the target word?s semantic sim-
ilarity with the seed words (literal or non literal)
we will be able to find out if the certain word has
a literal or metaphorical meaning at the concrete
context.
By this way, starting from an already known
metaphor, we will be able to identify other non lit-
eral uses of words which may appear at the same
context, estimating the similarity measure of the
target word between the seed metaphor and an-
other literal meaning of a word at the same con-
text. If the semantic similarity?s rate of the target
word (for instance the word ?assistance? at the con-
text of the verb ?give?) and the annotated metaphor
(like ?quidance? at the certaincontext) is higher
that the rate of the target word and the seed word
with the literal meaning (for example the word
?apple? at the same context) , then we will be able
to assume that the tartget word is used metaphori-
cally, at the concrete context.
4 First Experiments and Results
In order to evaluate our method we search for com-
mon English verbs which can take either literal
or non literal predicates. As the most common
verbs (be, have and do) can function as verbs and
auxiliary verbs, we didn?t use them for our ex-
periments. As a consequence, we chose common
function verbs which can take a direct object as
predicate. More specifically, at our experiments
we concentrated on literal and non literal predi-
cates of the verbs: break, catch, cut, draw, drop,
find, get, hate, hear, hold, keep, kill, leave, listen,
lose, love, make, pay, put, save, see, take, want.
We used the VU Amsterdam Metaphor Corpus1
1Please see http://www.metaphorlab.
vu.nl/en/research/funded_research/
in order to extract data for our experiments. We
used shallow heuristics to match verbs and direct
objects, with manually checking and correcting
the result. We have also used the British National
Corpus (BNC), in order to take more samples,
mostly literal. In the case of he BNC, we were
able to extract the direct object from the depency
parses, but had manually controlled metaphorical
vs. literal usage. In all, we collected 124 instances
of literal usage and 275 instances of non-literal us-
age involving 311 unique nouns.
With this body of literal and non-literal con-
texts, we tried every possible combination of one
literal and one non-literal object for each verb as
seed, and tested with the remaining words. The
mean results are collected in Table 1, where we see
how the LCS-based measures by Resnik (1997)
and Wu and Palmer (1994) performed the best.
One observation is that the differences between
the different measures although significant, they
are not as dramatic as to effect reversals in the
decision. This is apparent in the simple voting
results (right-most column in Table 1) where all
measures yield identical results. Only when dif-
ferences in the similarities accumulate before the
comparison between literal and non-literal context
is made (three left-most columns in Table 1), does
the choice of similarity measure make a differ-
ence.
Another observation pertains to relaxing the de-
pendency on WordNet so that method can be based
on similarities defined over more widely available
lexical resources. In this respect, the low F-score
by the adapted Lesk measure is not very encourag-
ing, as variations of the Lesk measure could be de-
fined over the glosses in digital dictionaries with-
out explicit WordNet-style relations. Combined
with the high valuation of methods using the LCS,
this leads us to conclude that the relative taxo-
nomic position is a very important factor.
Finally, and happily counter to our prior in-
tuition, we would like to note the robustness of
the method to the number of different senses test
words have: plotting the F-score against the num-
ber of senses did not result in consistently de-
teriorating results as the senses multiply (Fig-
ure 1).2 If this had happened, we would have con-
VU-Amsterdam-Metaphor-Corpus
2Although some of the nouns in our collection have as
many as 33 senses, we have only plotted the data for up to 15
senses; the data is too sparse to be reasonably usuable beyond
that point.
27
Table 1: F?=1 scores for all combinations of seven different similarity measures and five ways of deriving
a single judgement on literal usage by testing all senses of a word against all senses of the seed words.
Measure Maximum Average Sum Simple Voting
Mean Std dev Mean Std dev Mean Std dev Mean Std dev
Adapted Lesk 63.87 6.96 63.39 9.41 64.77 6.47 68.64 10.69
Jiang et al (1997) 70.92 9.19 64.31 8.41 65.14 6.45 68.64 10.69
Lin (1998) 71.35 10.70 70.39 10.02 70.07 9.47 68.64 10.69
Path length 67.63 9.83 72.60 8.83 65.33 6.91 68.64 10.69
Resnik (1993) 66.14 9.13 72.92 9.08 70.54 8.24 68.64 10.69
Wu and Palmer (1994) 70.84 9.38 72.97 9.05 66.02 6.82 68.64 10.69Resnik graph
Page 1
1 2 3 4 5 6 7 8 9 10 11 12 13 14
0
10
20
30
40
50
60
70
80
90
100
WUP graph
Page 1
1 2 3 4 5 6 7 8 9 10 11 12 13 14
0
10
20
30
40
50
60
70
80
90
100
Resnik (1997) Wu and Palmer (1994)
Figure 1: Plot of precision (dotted line, circles), recall (dotted line, triangles), and F?=1 score (solid
line) versus the number of different senses for a word. Also includes the frequency of each sense count
(dashed line, squares). For both measures, final judgement is made on average similarity of all senses.
fronted a Catch-22 situation where disambiguation
is needed in order to carry out metaphora iden-
tification, a disambiguation task itself. The way
things stand, our method can be successfully ap-
plied to shallow NLP tasks or as a pre-processing
and optimization step for WSD and parsing.
5 Conclusions
In this paper, we presented a mildly supervised
method for identifying metaphorical verb usage by
taking the local context into account. This proce-
dure is different from the majority of the previous
works in that it does not rely on any metaphor-
specic hand-coded knowledge, but rather on pre-
vious observed unambiguous usages of the verb.
The method can operates on open domain texts
and the memory needed for the seeds can be rela-
tively easily collected by mining unannotated cor-
pora. Furthermore, our method differs as com-
pares the meaning of nouns which appear at the
same context without associating them with con-
cepts and then comparing the concepts. We se-
lected this procedure as words of the same abstract
concept maybe not appear at the same context
while words from different concepts could appear
at the same context, especially when the certain
context is metaphorical. Although the system has
been tested only on verb-direct object metaphors,
the described identi- cation method should be im-
mediately applicable to a wider range of word
classes, which is one of the future research direc-
tions we will pursue. Another promising research
direction relates to our observation regarding the
importance of measuring similarities by consider-
ing the relative taxonomic position of the two con-
cepts; more specifically, we will experiment with
clustering methods over unannotated corpora as a
way of producing the taxonomy over which we
will dene some Resnik-esque similarity measure.
28
References
Tony Berber Sardinha. 2002. Metaphor in early
applied linguistics writing: A corpus-based
analysis of lexis in dissertations. In I Confer-
ence on Metaphor in Language and Thought.
Tony Berber Sardinha. 2010. Creating and using
the Corpus do Portugues and the frequency dic-
tionary of portuguese. Working with Portuguese
Corpora.
Julia Birke and Anoop Sarkar. 2006. A clustering
approach for the nearly unsupervised recogni-
tion of nonliteral language. In Proceedings of
EACL-06, pages 329?336. Trento, Italy.
Dan Fass. 1991. met*: a method for discrimi-
nating metonymy and metaphor by computer.
Computational Linguistics, 17(1).
Gilles Fauconnier and Mark Tuner. 2002. The Way
We Think: Conceptual Blending and the Mind?s
Hidden Complexities. Basic Books.
Jerome Feldman. 2006. From Molecule to
Metaphor: A Neutral Theory of Language. The
MIT Press.
Charles Fillmore, Christopher Johnson and
Miriam Petruck. 2003. Background to
framenet. International Journal of Lexicogra-
phy, 16(3):235?250.
Matt Gedigian, John Bryant, Srini Narayanan, and
Branimir Ciric. 2006. Catching metaphors. In
Proceedings of the 3rd Workshop on Scalable
Natural Language Understanding, pages 41?
48. New York.
Joseph Edward Grady. 1997. Foundations of
meaning: primary metaphors and primary
scenes. University Microfilms International.
Yael Karov and Shimon Edelman. 1998.
Similarity-based word sense disambigua-
tion. Computational Linguistics, 24(1):41?59.
Paul Kingsbury and Martha Palmer. 2002. From
treebank to propbank. In Proceedings of LREC-
2002, pages 1989?1993. Gran Canaria, Canary
Islands, Spain.
Saisuresh Krishnakumaran and Xiaojin Zhu.
2007. Hunting elusive metaphors using lex-
ical resources. In Proceedings of the Work-
shop on Computational Approaches to Fig-
urative Language, April, 2007, Rochester,
New York, pages 13?20. Association for
Computational Linguistics, Rochester, New
York. URL http://www.aclweb.org/
anthology/W/W07/W07-0103.
George Lakoff, Jane Espenson, and Alan
Schwartz. 1991. The master metaphor list.
University of California at Berkeley.
George Lakoff and Mark Johnson. 1980.
Metaphors We Live By. University of Chicago
Press.
Dekang Lin. 1997. Using syntactic dependency
as local context to resolve word sense ambigu-
ity. In Proceedings of ACL-97, pages 64?71.
Madrid, Spain.
Dekang Lin. 1998. An information-theoretic def-
inition of similarity. In Proceedings of the 15th
International Conference on Machine Learn-
ing (ICML-98). Madison, WI, USA, July 1998.,
page 296304.
Zachary Mason. 2004. Cormet: a computational,
corpus-based conventional metaphor extraction
system. Computational Linguistics, 30(1):23?
44.
Srini Narayanan. 1997. Knowledge-based Ac-
tion Representations for Metaphor and Aspect
(KARMA). University of Californial.
Siddharth Padwardhan, Satanjeev Banerjee, and
Ted Pedersen. 2003. Using measures of seman-
tic relatedness for word sense disambiguation.
In Proceedings of the 4th International Confer-
ence on Intelligent Text Processing and Compu-
tational Linguistics (CICLing-03), Mexico City,
pages 241?257.
Stephen Pinker. 2007. The Stuff of Thought: Lan-
guage as a Window into Human Nature. Viking
Adult.
Judita Preiss, Ted Briscoe, and Anna Korhonen.
2007. A system for large-scale acquisition of
verbal, nominal and adjectival subcategoriza-
tion frames from corpora. In Proceedings of
ACL-07, volume 45, page 912.
Philip Resnik. 1997. Selectional preference and
sense disambiguation. In ACL SIGLEX Work-
shop on Tagging Text with Lexical Semantics.
Washington, D.C.
Ekaterina Shutova, Simone Teufel and Anna Ko-
rhonen. 2012. Statistical metaphor processing.
Computational Linguistics, 39(2).
Julie Weeds, David Weir, and Diana McCarthy.
2004. Characterising measures of lexical dis-
29
tributional similarity. In Proceedings of Col-
ing 2004, pages 1015?1021. COLING, Geneva,
Switzerland.
Yorick Wilks. 1978. Making preferences more ac-
tive. Artificial Intelligence, 11(3).
Zhibiao Wu and Martha Palmer. 1994. Verb se-
mantics and lexical selection. In Proceedings of
the 32nd Annual Meeting of the ACL (ACL-04),
pages 133?138.
30
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 49?54,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Argument extraction for supporting public policy formulation
Eirini Florou
Dept of Linguistics, Faculty of Philosophy
University of Athens, Greece
eirini.florou@gmail.coml
Stasinos Konstantopoulos
Institute of Informatics and
Telecommunications, NCSR ?Demokritos?
konstant@iit.demokritos.gr
Antonis Kukurikos Pythagoras Karampiperis
Institute of Informatics and Telecommunications, NCSR ?Demokritos?, Athens, Greece
{kukurik,pythk}@iit.demokritos.gr
Abstract
In this paper we describe an application
of language technology to policy formula-
tion, where it can support policy makers
assess the acceptance of a yet-unpublished
policy before the policy enters public con-
sultation. One of the key concepts is that
instead of relying on thematic similarity,
we extract arguments expressed in support
or opposition of positions that are general
statements that are, themselves, consistent
with the policy or not. The focus of this
paper in this overall pipeline, is identify-
ing arguments in text: we present and em-
pirically evaluate the hypothesis that ver-
bal tense and mood are good indicators of
arguments that have not been explored in
the relevant literature.
1 Introduction
The large-scale acquisition, thematic classifica-
tion, and sentiment analysis of Web content has
been extensively applied to brand monitoring, dig-
ital reputation management, product development,
and a variety of similar applications. More re-
cently, it has also seen application in public policy
validation, where the ?brand? to be monitored is
a publicized and widely commented government
policy.
All these methods typically rely on the seman-
tic similarity between a given text or set of terms
and Web content; often using domain-specific on-
tological and terminological resources in order to
measure this similarity. This approach, however
requires that all parties involved discourse on the
same topic; that is to say, that we are seeking the
collective opinion of the Web on a topic that has
been publicized enough to attract the attention of
the Web.
In this paper we present a slightly different ap-
proach, where we are looking for arguments ex-
pressed in support or opposition of opinions with
little semantic similarity to our topic of interest.
As a rough example, consider how drafting envi-
ronmental policy can benefit from access to statis-
tics about how people felt about industrial growth
at the expense of environmental concerns when
other policy in completely different domains was
on public consultation: many of the arguments
about the relative merits of industrial growth and
environmental concerns can retain their structure
and be thematically transferred to the new domain,
helping draft a policy that best addresses people?s
concerns.
Of paramount importance for implementing
such an approach is the linguistic tools for identi-
fying arguments. In this paper, we first motivate
the inclusion of argument extraction inside the
larger policy formulation and validation cycle and
present the position of an argument extraction tool
inside a computational system for supporting this
cycle (Section 2). We then proceed to a present the
argument extraction literature (Section 3) and our
hypothesis that verbal morpho-syntactic features
are good discriminators of arguments (Section 4).
We close the paper by presenting and discussing
empirical results (Section 5) and concluding (Sec-
tion 6).
2 Policy formulation and validation
Our work is carried out in the context of a project
that develops computational tools for the early
phases of policy making, before policy drafts have
been made available for public consultation.1
At that stage, the policy?s impact on public
opinion cannot be estimated by similarity-based
searching for relevant Web content, since the pol-
icy text has not been announced yet ? or even
fully authored for that matter. One of the core
1Full details about the project have been suppressed to
preserve anonymity, but will be included in the camera-ready.
49
ideas of the project is that in order to assist the
policy formulation process, a tool needs to esti-
mate the acceptance of a yet unpublished docu-
ment based on Web content that is not themati-
cally similar, but is rather supporting or opposing a
more general position or maxim that also supports
or opposes the policy under formulation.
To make this more concrete, consider a new pol-
icy for increasing the penetration of wind power
production, setting specific conditions and prior-
ities. The project is developing an authoring en-
vironment where specific policy statements are
linked to more general statements, such as:
(1) Greenhouse gas emissions should not be a
concern at all.
(2) It is desired to reduce greenhouse gas
emissions, but this should be balanced
against other concerns.
(3) It is desired to reduce greenhouse gas
emissions at all costs.
We have, thus, created a formulation of ?relevant
content? that includes Examples 1 and 2 below.
These are taken from different domains, are com-
menting policies and laws that are already formu-
lated and made public, and can be used to infer
the level of support for the new wind power policy
although no textual similarity exists.
(4) In case hard packaging is made compulsory
by law, producers will be forced to consume
more energy, leading to more greenhouse
gas emissions.
(5) Tidal power production does not emit
greenhouse gases, but other environmental
problems are associated with its widespread
deployment.
Leaving aside the ontological conceptualization
that achieves this matching, which is reported else-
where, we will now discuss the language process-
ing pipeline that retrieves and classifies relevant
Web content.
Content is acquired via focused crawling, us-
ing search engine APIs to retrieve public Web
pages and social network APIs to retrieve con-
tent from social content sharing platforms. Con-
tent is searched and filtered (in case of feed-like
APIs) based on fairly permissive semantic similar-
ity measures, emphasising a high retrieval rate at
the expense of precision. As a second step, clean
text is extracted from the raw Web content using
the Boilerpipe library (Kohlschu?tter et al, 2010)
in order to remove HTML tags, active compo-
nents (e.g. JavaScript snippets), and content that
is irrelevant to the main content (menus, ad sec-
tions, links to other web pages), and also to replace
HTML entities with their textual equivalent, e.g.,
replacing ?&amp;? with the character ?&?.
The resulting text is tokenized and sentence-
splitted and each sentence classified as relevant or
not using standard information retrieval methods
to assess the semantic similarity of each sentence
to the general policy statements. This is based on
both general-purpose resources2 and the domain
ontology for the particular policy. Consecutive
sentences that are classified as positive are joined
into a segment.
The main objective of the work described here
is the classification of these segments as being
representative of a stance that would also support
or oppose the policy being formulated, given the
premise of the general statements (1)?(3). Our ap-
proach is to apply the following criteria:
? That they are semantically similar to the gen-
eral statements associated with the policy.
? That they are arguments, rather than state-
ments of fact or other types of prose.
? That their polarity towards the general state-
ments is expressed.
In order to be able to assess segments, we thus
need a linguistic pipeline that can calculate seman-
tic similarity, identify arguments, and extract their
structure (premises/consequences) and polarity (in
support or opposition).
The focus of the work described here is iden-
tifying arguments, although we also outline how
the features we are proposing can also be used in
order to classify chunks of text as premises or con-
sequences.
3 Related Work
The first approaches of argument extraction were
concentrated on building wide-coverage argument
2WordNets are publicly available for both English and
Greek, that is the language of the experiments reported here.
Simpler semantic taxonomies can also be used; the accuracy
of the semantic similarity measured here does not have a ma-
jor bearing on the argument extraction experiments that are
the main contribution of this paper.
50
structure lexicons, originally manually Fitzpatrick
and Sager (1980, 1981) and later from elec-
tronic versions of conventional dictionaries, since
such dictionaries contain morpho-syntactic fea-
tures Briscoe et al (1987). More recently, the fo-
cus shifted to automatically extracting these lex-
ical resources from corpora Brent (1993) and to
hybrid approaches using dictionaries and corpora.
Works using syntactic features to extract top-
ics and holders of opinions are numerous (Bethard
et al, 2005). Semantic role analysis has also
proven useful: Kim and Hovy (2006) used a
FrameNet-based semantic role labeler to deter-
mine holder and topic of opinions. Similarly, Choi
and Cardie (2006) successfully used a PropBank-
based semantic role labeler for opinion holder ex-
traction.
Somasundaran et al (2008; 2010) argued that
semantic role techniques are useful but not com-
pletely sufficient for holder and topic identifica-
tion, and that other linguistic phenomena must be
studied as well. In particular, they studied dis-
course structure and found specific cue phrases
that are strong features for use in argument extrac-
tion. Discourse markers that are strongly associ-
ated with pragmatic functions can be used to pre-
dict the class of content, therefore useful features
include the presence of a known marker such as
?actually?, ?because?, ?but?.
Tseronis (2011) describes three main ap-
proaches to describing argument markers: Geneva
School, Argument within Language Theory and
the Pragma-dialectical Approach. According the
Geneva School, there are three main types of
markers/connective, organisation markers, illocu-
tionary function markers (the relations between
acts) and interactive function markers. Argument
within Language Theory is a study of individual
words and phrases. The words identified are argu-
ment connectors: these describe an argumentative
function of a text span and change the potential
of it either realising or de-realising the span. The
Pragma-dialectical Approach looks at the context
beyond words and expressions that directly refer to
the argument. It attempts to identify words and ex-
pressions that refer to any moves in the argument
process. Similarly to Marcu and Echihabi (2002),
the approach is to create a model of an ideal argu-
ment and annotate relevant units.
4 Approach and Experimental Setup
As seen above, shallow techniques are typically
based on connectives and other discourse mark-
ers in order to define shallow argument patterns.
What has not been investigated is whether shallow
morpho-syntactic features, such as the tense and
mood of the verbal constructs in a passage, can
also indicate argumentative discourse.
Our hypothesis is that future and conditional
tenses and moods often indicate conjectures and
hypotheses which are commonly used in argumen-
tation techniques such as illustration, justification,
rebuttal where the effects of a position counter to
the speaker?s argument are analysed. Naturally,
such features cannot be the sole basis of argument
identification, so we need to experiment regarding
their interaction with discourse markers.
To make this more concrete, consider the exam-
ples in Section 2: although both are perfectly valid
arguments that can help us infer the acceptance or
rejection of a policy, in the first one future tense
is used to speculate about the effects of a policy;
in the second example there is no explicit marker
that the effects of large-scale tidal power produc-
tion are also a conjecture.
Another difficulty is that conditional and fu-
ture verbal groups are constructed using auxil-
iary verbs and (in some languages) other auxil-
iary pointers. Consider, for example, the follow-
ing PoS-tagged and chunked Greek translation of
Example 4:
(6) [oi
[the-NomPl
paragogoi]np
producers-NounNomPl]
[tha
[Pointer-Fut
ipochreothoun]vp
force-Perf-3PP]
[na
[Pointer-Subj
katanalosoun]vp
consume-Inf]
?producers will be forced to consume?
In order to be able to correctly assign simple fu-
ture, information from the future pointer ?tha?
needs to be combined with the perfective feature
of finite verb form. Conditionals, future perfect,
past perfect, and similar tenses or moods like sub-
juncive also involve the tense of the auxiliary verb,
besides the future pointer and the main verb.
We have carried out our experiments in Greek
language texts, for which we have developed a
JAPE grammar3 that extract the tense and mood of
3JAPE is finite state transducer over GATE annota-
51
Table 1: Categories of morpho-syntactic features extracted from text segments.
Label Description Features
DM Absolute number of occurrences of discourse markers 5 numerical features
from a given category
Rel Relative frequency of each of the 6 tenses and each of the 6 moods 12 numerical features
RCm Relative frequency of each tense/mood 9 numerical features
combination (only for those that actually appear).
Bin Appearance of each of the 6 tenses and each of the 6 moods 12 binary features
Dom Most frequent tense, mood, and tense/mood combination 3 string features
TOTAL 41 features
each verb chunk. The grammar uses patterns that
combine the features of pointers and auxiliary and
main verbs, without enforcing any restrictions on
what words (e.g., adverbs) might be interjected in
the chunk. That is to say, the chunker is responsi-
ble for identifying verb groups and our grammar is
restricted to propagating and combining the right
features from each of the chunk?s constituents to
the chunk?s own feature structure.
PoS-tagging and chunking annotations have
been previously assigned by the ILSP suite of
Greek NLP tools (Papageorgiou et al, 2000;
Prokopidis et al, 2011), as provided by the rele-
vant ILSP Web Services4 to get PoS tagged and
chunked texts in the GATE XML format.
At a second layer of processing, we create one
data instance for each segment (as defined in Sec-
tion 2 above) and for each such segment we ex-
tract features relating to verbal tense/mood and to
the appearance of discourse markers. The former
are different ways to aggregate the various tenses
and moods found in the whole segment, by mea-
suring relative frequencies, recording the appear-
ance of a tense or mood even once, and naming
the predominant (most frequent) tense and mood;
tense and mood are seen both individually and as
tense/mood combinations.
Furthermore, we have defined five absolute fre-
quency features which record the matching against
the several patterns and keywords provided for the
following five categories of arguments:
? justification, matching patterns such as ?be-
cause?, ?the reason being?, ?due to?, etc.
tions. Please see http://gate.ac.uk/sale/tao/
splitch8.html The JAPE grammar we have developed
will be made available on-line; location cannot be yet dis-
closed in order to preserve anonymity.
4Currently at http://ilp.ilsp.gr
? explanation, matching patterns such as ?in
other words?, ?for instance?, quotesfor this
reason(s), etc.
? deduction, ?as a consequence?, ?in accor-
dance with the above?, ?proving that?, etc.
? rebuttal, ?despite?, ?however?, etc.
? conditionals, ?supposing that?, ?in case that?,
etc.
All features extracted by this process are given
on Table 1.
5 Results and Discussion
We have used the method described in Section 2
in order to obtain 677 text segments, with a size
ranging between 10 and 100 words, with an av-
erage of 60 words. Of these, 332 were manually
annotated to not be arguments; the remaining 345
positive examples were obtained by oversampling
the 69 segments in our corpus that we have manu-
ally annotated to be arguments.5
We have then applied the feature extraction de-
scribed in Section 4 in order to set up a classifi-
cation task for J48, the Weka6 implementation of
the C4.5 decision tree learning algorithm (Quin-
lan, 1992). We have applied a moderate confi-
dence factor of 0.25, noting that experimenting
with the confidence factor did not yield any sub-
stantially different results.
In order to better understand the feature space,
we have run a series of experiments, with quan-
titative results summarized in Table 2. The first
5The data and relevant scripts for carrying out these
experiments are available at http://users.iit.
demokritos.gr/?konstant/dload/arguments.
tgz
6Please see http://www.cs.waikato.ac.nz/
ml/weka
52
Table 2: Precision and recall for retrieving arguments using different feature mixtures. Please cf. Table 1
for an explanation of the feature labels. The results shown are the 10-fold cross-validation mean.
Morpho-syntactic With Discourse Markers Without Discourse Markers
features used Prec. Rec. F?=1 Prec. Rec. F?=1
All 75.8% 71.9% 73.8% 75.5% 70.4% 72.9%
no Dom 79.8% 73.3% 76.4% 74.0% 71.9% 72.9%
no Rel 74.5% 72.8% 73.8% 73.1% 69.3% 71.1%
no RCm 76.3% 71.0% 73.6% 76.8% 70.1% 73.3%
no Bin 70.0% 70.4% 70.2% 66.7% 69.6% 68.1%
Rel 73.4% 75.9% 74.6% 70.3% 72.2% 71.2%
Dom 57.1% 98.8% 72.4% 54.9% 94.2% 69.4%
RCm 69.3% 66.7% 67.9% 71.9% 62.9% 67.1%
Bin 71.7% 49.9% 58.8% 70.1% 44.9% 54.8%
None 67.9% 20.9% 31.9% ?
observation is that both morpho-syntactic features
and discourse markers are needed, because if ei-
ther category is omitted results deteriorate. How-
ever, not all morpho-syntactic features are needed:
note how omitting the Dom, Rel, or RCm cate-
gories yields identical or improved results. On the
other hand, the binary presence feature category
Bin is significant (cf. 5th row). We cannot, how-
ever, claim that only the Bin category is sufficient,
and, in fact, if one category has to be chosen that
would have to be that of relative frequency fea-
tures (cf. rows 6-9).
6 Conclusion
We describe here an application of language tech-
nology to policy formulation, and, in particular, to
using Web content to assess the acceptance of a
yet-unpublished policy before public consultation.
The core of the idea is that classifying Web con-
tent as similar to the policy or not does apply, be-
cause the policy document has not been made pub-
lic yet; but that we should rather extract arguments
from Web content and assess whether these argue
in favour or against general concepts that are (or
are not) consistent with the policy being formu-
lated.
As a first step to this end, our paper focuses
on the identification of arguments in Greek lan-
guage content using shallow features. Based on
our observation that verb tense appears to be a sig-
nificant feature that is not exploited by the rele-
vant literature, we have carried out an empirical
evaluation of this hypothesis. We have, in partic-
ular, demonstrated that the relative frequency of
each verb tense/mood and the binary appearance
of each verb tense/mood inside a text segment are
as discriminative of argumentative text as the (typ-
ically used) discourse markers; and that classifica-
tion is improved by combining discourse marker
features with our verbal tense/mood features. For
doing this, we developed a regular grammar that
combines the PoS tags of the members of a verb
chunk in order to assign tense and mood to the
chunk. In this manner, our approach depends on
PoS taggin and chunking only.
In subsequent steps of our investigation, we are
planning to refine our approach to extracting ar-
gument structure: it would be interesting to test
if argument premises tend to correlate with cer-
tain tenses or moods, distinguishing them from
from conclusions. Further experiments can also
examine if the simultaneous appearance of con-
crete tenses at the same sentence is an indicator
of an argument. Finally, we plan to examine the
predicates of an argument, and especially if the
head word of each sentence (be it verb or deverbal
noun) and its seat at the boundaries of the sentence
may contribute to extract an argument or not, espe-
cially for impersonal, modal, and auxiliary verbs.
Acknowledgements
The research leading to these results has re-
ceived funding from the European Union?s Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement no 288513. For more de-
tails, please see the NOMAD project?s website,
http://www.nomad-project.eu
53
References
Steven Bethard, Hong Yu, Ashley Thornton,
Vasileios Hatzivassiloglou, and Dan Jurafsky.
2005. Extracting opinion propositions and opin-
ion holders using syntactic and lexical cues. In
James G. Shanahan, Yan Qu, and Janyce Wiebe,
editors, Computing Attitude and Affect in Text:
Theory and Applications. Springer.
Michael Brent. 1993. From grammar to lexi-
con: unsupervised learning of lexical syntax.
In Computational Linguistics - Special issue on
using large corpora: II, Volume 19 Issue 2,
pages 243?262.
Ted Briscoe, Claire Grover, Bran Boguraev, and
John. Carroll. 1987. The derivation of a
grammatically-indexed lexicon from the long-
man dictionary of contemporary english. In
Proceeding of ACL ?87.
Yejin Choi and Claire Cardie. 2006. Joint extrac-
tion of entities and relations for opinion recog-
nition. In Proceedings of EMNLP 2006.
Eileen Fitzpatrick and Naomi Sager. 1980. The
Lexical subclasses of the LSP English Gram-
mar. Linguistic String Project, New York Uni-
versity.
Eileen Fitzpatrick and Naomi Sager. 1981. The
lexical subclasses of the lsp english grammar.
In N. Sager (ed), Natural Language Information
Processing, Addison- Wesley, Reading, Ma.
Soo-Min Kim and Eduard Hovy. 2006. Extract-
ing opinions, opinion holders, and topics ex-
pressedin online news media text. In Proceed-
ings of ACL/COLING Workshop on Sentiment
and Subjectivity in Text.
Christian Kohlschu?tter, Peter Fankhauser, and
Wolfgang Nejdl. 2010. Boilerplate detection us-
ing shallow text features. In Proc. 3rd ACM
International Conference on Web Search and
Data Mining (WSDM 2010) New York, USA.
Daniel Marcu and Abdessamad Echihabi. 2002.
An unsupervised approach to recognizing dis-
course relations. In Proc. ACL ?02.
Haris Papageorgiou, Prokopis Prokopidis, Voula
Giouli, and Stelios Piperidis. 2000. A unified
POS tagging architecture and its application to
Greek. In Proceedings of the 2nd Language
Resources and Evaluation Conference (LREC
2000), Athens, Greece, pages 1455?1462.
Prokopis Prokopidis, Byron Georgantopoulos, and
Haris Papageorgiou. 2011. A suite of NLP tools
for Greek. In Proceedings of the 10th Interna-
tional Conference of Greek Linguistics (ICGL
2011), Komotini, Greece.
J. Ross Quinlan. 1992. C4.5: Programs for Ma-
chine Learning. Morgan Kaufmann, San Mateo,
CA, USA.
Swapna Somasundaran and Janyce Wiebe. 2010.
Recognizing stances in ideological on-line de-
bates. In Proc. NAACL HLT Workshop on Com-
putational Approaches to Analysis and Genera-
tion of Emotion in Text (CAAGET 2010)
Swapna Somasundaran, Janyce Wiebe, and Joseph
Ruppenhofer. 2008. Discourse level opinion in-
terpretation. In Proceedings of the 22nd Inter-
national Conference on Computational Linguis-
tics - Volume 1, COLING ?08, Stroudsburg, PA,
USA. Association for Computational Linguis-
tics.
Assimakis Tseronis. 2011. From connectives to
argumentative markers: A quest for markers
of argumentative moves and of related aspects
of argumentative discourse. Argumentation,
25(4):427?444.
54
