Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 104?107,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
English?Hindi Transliteration Using Context-Informed PB-SMT:    
the DCU System for NEWS 2009 
Rejwanul Haque, Sandipan Dandapat, Ankit Kumar Srivastava,  
Sudip Kumar Naskar and Andy Way 
CNGL, School of Computing 
Dublin City University, Dublin 9, Ireland 
{rhaque,sdandapat,snaskar,asrivastava,away}@computing.dcu.ie 
 
Abstract 
This paper presents English?Hindi translit-
eration in the NEWS 2009 Machine Translit-
eration Shared Task adding source context 
modeling into state-of-the-art log-linear 
phrase-based statistical machine translation 
(PB-SMT). Source context features enable us 
to exploit source similarity in addition to tar-
get similarity, as modelled by the language 
model. We use a memory-based classification 
framework that enables efficient estimation of 
these features while avoiding data sparseness 
problems.We carried out experiments both at 
character and transliteration unit (TU) level. 
Position-dependent source context features 
produce significant improvements in terms of 
all evaluation metrics. 
1 Introduction 
Machine Transliteration is of key importance in 
many cross-lingual natural language processing 
applications, such as information retrieval, ques-
tion answering and machine translation (MT). 
There are numerous ways of performing auto-
matic transliteration, such as noisy channel mod-
els (Knight and Graehl, 1998), joint source chan-
nel models (Li et al, 2004), decision-tree models 
(Kang and Choi, 2000) and statistical MT models 
(Matthews, 2007). 
For the shared task, we built our machine 
transliteration system based on phrase-based sta-
tistical MT (PB-SMT) (Koehn et al, 2003) using 
Moses (Koehn et al, 2007).  We adapt PB-SMT 
models for transliteration by translating charac-
ters rather than words as in character-level trans-
lation systems (Lepage & Denoual, 2006). How-
ever, we go a step further from the basic PB-
SMT model by using source-language context 
features (Stroppa et al, 2007). We also create 
translation models by constraining the character-
level segmentations, i.e. treating a consonant-
vowel cluster as one transliteration unit.  
The remainder of the paper is organized as fol-
lows. In section 2 we give a brief overview of 
PB-SMT. Section 3 describes how context-
informed features are incorporated into state-of-
art log-linear PB-SMT. Section 4 includes the 
results obtained, together with some analysis. 
Section 5 concludes the paper. 
2 Log-Linear PB-SMT  
Translation is modelled in PB-SMT as a decision 
process, in which the translation Ie1 = e1 . . .  eI of 
a source sentence Jf1 = f1 . . . fJ is chosen to 
maximize (1): 
)1()().|(maxarg)|(maxarg 111
,
11
, 11
IIJ
eI
JI
eI
ePefPfeP
II
?  
where )|( 11
IJ efP  and )( 1
IeP  denote respec-
tively the translation model and the target lan-
guage model (Brown et al, 1993). In log-linear 
phrase-based SMT, the posterior probability 
)|( 11
JI feP  is directly modelled as a (log-linear) 
combination of features (Och and Ney, 2002), 
that usually comprise M translational features, 
and the language model, as in (2): 
?
?
?
m
m
KIJ
mm
JI sefhfeP
1
11111 ),,()|(log ?   
                             )(log 1
I
LM eP??                  (2) 
where k
K sss ...11 ?  denotes a segmentation of the 
source and target sentences respectively into the 
sequences of phrases )?,...,?( 1 kee  and )
?,...,?( 1 kff  
such that (we set i0 = 0) (3): 
,1 Kk ???  sk = (ik ; bk, jk), 
          
kk iik
eee ...? 11 ??? , 
                      
kk jbk
fff ...? ?                              (3) 
The translational features involved depend 
only on a pair of source/target phrases and do not 
take into account any context of these phrases. 
This means that each feature mh   in (2) can be 
rewritten as in (4): 
104
?
?
?
K
k
kkkm
KIJ
m sefhsefh
1
111 ),?,?(?),,(           (4) 
where mh? is a feature that applies to a single 
phrase-pair. Thus (2) can be rewritten as: 
? ??
? ??
?
K
k
K
k
kkkkkkm
m
m
m sefhsefh
1 11
),?,?(?),?,?(??        (5) 
where, m
m
m
mhh ??
1
?
?
? ? . In this context, the transla-
tion process amounts to: (i) choosing a segmen-
tation of the source sentence, (ii) translating each 
source phrase. 
3 Source Context Features in Log-
Linear PB-SMT 
The context of a source phrase kf?  is defined as 
the sequence before and after a focus phrase kf?  
=
kk ji
ff ... . Source context features (Stroppa et 
al., 2007) include the direct left and right context 
words (in our case, character/TU instead of word) 
of length l (resp. lii kk ff ?? ...1  and ljj kk ff ?? ...1 ) of 
a given focus phrase kf? = kk ji ff ... . A window of 
size 2l+1 features including the focus phrase is 
formed. Thus lexical contextual information (CI) 
can be described as in (6): 
CI = }...,...{ 11 ljjili kkkk ffff ????                    (6) 
As in (Haque et al, 2009), we considered a 
context window of ?1 and ?2 (i.e. l=1, 2) for our 
experiments. 
One natural way of expressing a context-
informed feature is as the conditional probability 
of the target phrase given the source phrase and 
its context information, as in (7): 
mh? ( kf? ,CI( kf? ), ke? , sk) = log P( ke? | kf? , CI( kf? ))  (7) 
3.1 Memory-Based Classification 
As (Stroppa et al, 2007) point out, directly esti-
mating P( ke? | kf? , CI( kf? )) using relative fre-
quencies is problematic. Indeed, Zens and Ney 
(2004) showed that the estimation of P( ke? | kf? ) 
using relative frequencies results in the overesti-
mation of the probabilities of long phrases, so 
smoothing factors in the form of lexical-based 
features are often used to counteract this bias 
(Foster et al, 2006). In the case of context-
informed features, since the context is also taken 
into account, this estimation problem can only 
become worse. To avoid such problems, in this 
work we use three memory-based classifiers: 
IGTree, IB1 and TRIBL 1  (Daelemans et al, 
2005). When predicting a target phrase given a 
source phrase and its context, the source phrase 
is intuitively the feature with the highest predic-
tion power; in all our experiments, it is the fea-
ture with the highest gain ratio (GR).  
In order to build the set of examples required 
to train the classifier, we modify the standard 
phrase-extraction method of (Koehn et al, 2003) 
to extract the context of the source phrases at the 
same time as the phrases themselves. Importantly, 
therefore, the context extraction comes at no ex-
tra cost.  
We refer interested readers to (Stroppa et al, 
2007) and (Haque et al, 2009) as well as the ref-
erences therein for more details of how Memory-
Based Learning (MBL) is used for classification 
of source examples for use in the log-linear MT 
framework. 
3.2 Implementation Issues 
We split named entities (NE) into characters. We 
break NEs into transliteration units (TU), which 
bear close resemblance to syllables. We split 
English NEs into TUs having C*V* pattern and 
Hindi NEs are divided into TUs having Ch+M 
pattern (M: Hindi Matra / vowel modifier, Ch: 
Characters other than Matras). We carry out ex-
periments on both character-level (C-L) and TU-
level (TU-L) data. We use a 5-gram language 
model for all our experiments. The Moses PB-
SMT system serves as our baseline system. 
The distribution of target phrases given a 
source phrase and its contextual information is 
normalised to estimate P( ke? | kf? ,CI( kf? )). There-
fore our expected feature is derived as in (8): 
mblh? = log P( ke? | kf? ,CI( kf? ))                         (8) 
As for the standard phrase-based approach, 
their weights are optimized using Minimum Er-
ror Rate Training (MERT) of (Och, 2003) for 
each of the experiments. 
As (Stroppa et al, 2007) point out, PB-SMT 
decoders such as Pharaoh (Koehn, 2004) or 
Moses (Koehn, 2007) rely on a static phrase-
table represented as a list of aligned phrases ac-
companied with several features. Since these fea-
                                               
1 An implementation of IGTree, IB1 and TRIBL is available 
in the TiMBL software package (http://ilk.uvt.nl/timbl). 
 
105
tures do not express the context in which those 
phrases occur, no context information is kept in 
the phrase-table, and there is no way to recover 
this information from the phrase-table. 
In order to take into account the context-
informed features for use with such decoders, the 
devset and testset that need to be translated are 
pre-processed. Each token appearing in the test-
set and devset is assigned a unique id. First we 
prepare the phrase table using the training data. 
Then we generate all possible phrases from the 
devset and testset. These devset and testset 
phrases are then searched for in the phrase table, 
and if found, then the phrase along with its con-
textual information is given to MBL for classifi-
cation. MBL produces class distributions accord-
ing to the maximum-match of the features con-
tained in the source phrase. We derive new 
scores from this class distribution and merge 
them with the initial information contained in the 
phrase table to take into account our feature 
functions ( mblh? ) in the log-linear model (2). 
In this way we create a dynamic phrase table 
containing both the standard and the context-
informed features. The new phrase table contains 
the source phrase (represented by the sequence 
of ids of the words composing the phrase), target 
phrase and the new score. 
Similarly, replacing all the words by their ids 
in the development set, we perform MERT using 
our new phrase table to optimize the feature 
weights. We translate the test set (words repre-
sented by ids) using our new phrase table. 
4 Results and Analysis 
We used 10,000 NEs from the NEWS 2009 Eng-
lish?Hindi training data (Kumaran and Kellner, 
2007) for the standard submission, and the addi-
tional English?Hindi parallel person names data 
(105,905 distinct name pairs) of the Election 
Commission of India2 for the non-standard sub-
missions. In addition to the baseline Moses sys-
tem, we carried out three different set of experi-
ments on IGTree, IB1 and TRIBL. Each of these 
experiments was carried out on both the standard 
data and the combined larger data, both at char-
acter level and the TU level, and considering 
?1/?2 tokens as context. For each experiment, 
we produce the 10-best distinct hypotheses. The 
results are shown in Table 1. 
We observed that many of the (unseen) TUs in 
the testset remain untranslated in TU-L systems 
                                               
2 http://www.eci.gov.in/DevForum/Fullname.asp 
due to the problems of data sparseness. When-
ever a TU-L system fails to translate a TU, we 
fallback on the corresponding C-L system to 
translate the TU as a post-processing step. 
The accuracy of the TU-L baseline system 
(0.391) is much higher compared to the C-L 
baseline system (0.290) on standard dataset. Fur-
thermore, contextual modelling of the source 
language gives an accuracy of 0.416 and 0.399 
for TU-L system and C-L system respectively. 
Similar trends are observed in case of larger 
dataset. However, the highest accuracy (0.445) 
has been achieved with the TU-L system using 
the larger dataset. 
5 Conclusion 
In this work, we employed source context model-
ing into the state-of-the-art log-linear PB-SMT 
for the English?Hindi transliteration task. We 
have shown that taking source context into ac-
count substantially improve the system perform-
ance (an improvement of 43.44% and 26.42% 
respectively for standard and larger datasets). 
IGTree performs best for TU-L systems while 
TRIBL seems to perform better for C-L systems 
on both standard and non-standard datasets. 
Acknowledgements 
We would like to thank Antal van den Bosch for 
his input on the use of memory based classifiers. 
We are grateful to SFI (http://www.sfi.ie) for 
generously sponsoring this research under grant 
07/CE/I1142. 
References  
Adimugan Kumaran and Tobias Kellner. A generic 
framework for machine transliteration. Proc. of the 
30th SIGIR, 2007. 
Byung-Ju Kang and Key-Sun Choi. Automatic trans-
literation and back-transliteration by decision tree 
learning. 2000. Proc. of LREC-2000, Athens, 
Greece, pp. 1135-1141. 
David Matthews. 2007. Machine Transliteration of 
Proper Names. Master's Thesis, University of Ed-
inburgh, Edinburgh, United Kingdom. 
Franz Och and Hermann Ney. 2002. Discriminative 
training and maximum entropy models for statisti-
cal machine translation. Proc. of ACL 2002, Phila-
delphia, PA, pp. 295?302. 
George Foster, Roland Kuhn, and Howard Johnson. 
2006. Phrasetable smoothing for statistical machine 
translation.  Proc. of EMNLP-2006, Sydney, Aus-
tralia, pp. 53-61. 
106
Table1: Experimental Results (S/B ? Standard / Big data, S*? TM on Standard data, but LM on Big data, 
C/TU ? Character / TU level, SD? Standard submission, NSD? Non-standard submission). Better results with 
bold faces have not been submitted in the NEWS 2009 Machine Transliteration Shared Task. 
Haizhou Li, Zhang Min and Su Jian. 2004. A joint 
source-channel model for machine translitera-
tion. Proc. of ACL 2004, Barcelona, Spain, 
pp.159-166. 
Kevin Knight and Jonathan Graehl. 1998. Machine 
Transliteration. Computational Linguistics, 
24(4):559-612. 
Nicolas Stroppa, Antal van den Bosch and Andy 
Way. 2007. Exploiting Source Similarity for 
SMT using Context-Informed Features. Proc. of  
TMI-2007, Sk?vde, Sweden, pp. 231-240. 
Peter F. Brown, S. A. D. Pietra, V. J. D. Pietra and 
R. L. Mercer. 1993. The mathematics of statisti-
cal machine translation: parameter estimation. 
Computational Linguistics 19 (2), pp. 263-311. 
Philipp Koehn, F. J. Och, and D. Marcu. 2003. Sta-
tistical phrase-based translation. Proc. of HLT-
NAACL 2003, Edmonton, Canada, pp. 48-54. 
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine trans-
lation models. Machine translation: from real 
users to research: Proc. of AMTA 2004, Berlin: 
Springer Verlag, 2004, pp. 115-124. 
Philipp Koehn, H. Hoang, A. Birch, C. Callison-
Burch, M. Federico, N. Bertoldi, B. Cowan, W. 
Shen, C. Moran, R. Zens, C. Dyer,  O. Bojar, A. 
Constantin and E. Herbst. 2007. Moses: open 
source toolkit for statistical machine translation. 
Proc. of ACL, Prague, Czech Republic, pp. 177-
180. 
Rejwanul Haque, Sudip Kumar Naskar, Yanjun Ma 
and Andy Way. 2009. Using Supertags as Source 
Language Context in SMT. Proc. of EAMT-09, 
Barcelona, Spain, pp. 234-241. 
Richard Zens and Hermann Ney. 2004. Improve-
ments in phrase-based statistical machine trans-
lation. Proc. of HLT/NAACL 2004, Boston, MA, 
pp. 257?264. 
Walter Daelemans & Antal van den Bosch. 2005. 
Memory-based language processing. Cambridge, 
UK, Cambridge University Press. 
Yves Lepage and Etienne Denoual. 2006. Objective 
evaluation of the analogy-based machine transla-
tion system ALEPH. Proc. of the 12th Annual 
Meeting of the Association of NLP, pp. 873-876. 
 S/B C/TU Context ACC M-F-Sc MRR MAP_ref MAP_10 MAP_sys 
C 0 .290 .814 .393 .286 .131 .131  
S TU 0 .391 .850 .483 .384 .160 .160 
C 0 .352 .830 .463 .346 .156 .156 
 
Baseline 
Moses  
B TU 0 .407 .853 .500 .402 .165 .165 
?1 .391 .858 .501 .384 .166 .166  
C ?2 .386 .860 .479 .379 .155 .155 
?1 .406 .858 .466 .398 .178 .178 
 
S 
 
TU ?2 .359 .838 .402 .349 .165 .165 
?1 .431 .865 .534 .423 .177 .177  
C ?2 (NSD1) .420 .867 .519 .413 .170 .170 
?1 .437 .863 .507 .429 .191 .191 
 
 
 
 
IB1  
B 
 
TU ?2 .427 .862 .487 .418 .194 .194 
?1 .372 .849 .482 .366 .160 .160  
C ?2 .371 .847 .476 .364 .156 .156 
?1 .412 .859 .486 .404 .164 .164 
 
S 
 
TU ?2 .416 .860 .493 .409 .166 .166 
?1 .413 .855 .518 .406 .173 .173  
C ?2 (NSD2) .407 .856 .507 .399 .168 .168 
?1 .445 .864 .527 .440 .176 .176 
 
 
 
 
IGTree  
B 
 
TU ?2 .427 .861 .516 .422 .173 .173 
?1 .382 .854 .493 .375 .164 .164  
C ?2 (SD) .399 .863 .488 .392 .157 .157 
?1 .408 .858 .474 .400 .181 .181 
 
S 
 
TU ?2 .395 .857 .453 .385 .182 .182 
?1 .439 .866 .543 .430 .179 .179  
C ?2 (NSD3) .421 .864 .519 .415 .171 .171 
?1 .444 .863 .512 .436 .193 .193 
 
 
 
 
TRIBL  
B 
 
TU ?2 .439 .865 .497 .430 .197 .197 
 S* C ?2 (NSD4) .419 .868 .464 .419 .338 .338 
107
Named Entity Recognition in Bengali: A Conditional Random Field
Approach
Asif Ekbal
Department of CSE
Jadavpur University
Kolkata-700032, India
asif.ekbal@gmail.com
Rejwanul Haque
Department of CSE
Jadavpur University
Kolkata-700032, India
rejwanul@gmail.com
Sivaji Bandyopadhyay
Department of CSE
Jadavpur University
Kolkata-700032, India
sivaji cse ju@yahoo.com
Abstract
This paper reports about the development of
a Named Entity Recognition (NER) system
for Bengali using the statistical Conditional
Random Fields (CRFs). The system makes
use of the different contextual information
of the words along with the variety of fea-
tures that are helpful in predicting the var-
ious named entity (NE) classes. A portion
of the partially NE tagged Bengali news cor-
pus, developed from the archive of a lead-
ing Bengali newspaper available in the web,
has been used to develop the system. The
training set consists of 150K words and has
been manually annotated with a NE tagset
of seventeen tags. Experimental results of
the 10-fold cross validation test show the ef-
fectiveness of the proposed CRF based NER
system with an overall average Recall, Pre-
cision and F-Score values of 93.8%, 87.8%
and 90.7%, respectively.
1 Introduction
Named Entity Recognition (NER) is an impor-
tant tool in almost all Natural Language Process-
ing (NLP) application areas. Proper identifica-
tion and classification of named entities (NEs) are
very crucial and pose a very big challenge to the
NLP researchers. The level of ambiguity in NER
makes it difficult to attain human performance.
NER has applications in several domains includ-
ing information extraction, information retrieval,
question-answering, automatic summarization, ma-
chine translation etc.
The current trend in NER is to use the machine-
learning approach, which is more attractive in that
it is trainable and adoptable and the maintenance
of a machine-learning system is much cheaper than
that of a rule-based one. The representative ma-
chine-learning approaches used in NER are Hid-
den Markov Model (HMM) (BBN?s IdentiFinder
in (Bikel et al, 1999)), Maximum Entropy (New
York University?s MENE in (Borthwick, 1999)) and
Conditional Random Fields (CRFs) (Lafferty et al,
2001; McCallum and Li, 2003).
There is no concept of capitalization in Indian
languages (ILs) like English and this fact makes
the NER task more difficult and challenging in ILs.
There has been very little work in the area of NER
in ILs. In Indian languages particularly in Ben-
gali, the work in NER can be found in (Ekbal and
Bandyopadhyay, 2007a; Ekbal and Bandyopadhyay,
2007b) with pattern directed shallow parsing ap-
proach and in (Ekbal et al, 2007c) with HMM.
Other than Bengali, a CRF based NER system can
be found in (Li and McCallum, 2004) for Hindi.
2 Conditional Random Fields
Conditional Random Fields (CRFs) (Lafferty et al,
2001) are used to calculate the conditional proba-
bility of values on designated output nodes given
values on other designated input nodes. The con-
ditional probability of a state sequence S =<
s1, s2, . . . , sT > given an observation sequence
O =< o1, o2, . . . , oT > is calculated as:
P?(s|o) =
1
Z0
exp(
T
?
t=1
?
k
?k ? fk(st?1, st, o, t)),
589
where, fk(st?1, st, o, t) is a feature function whose
weight ?k, is to be learned via training. The val-
ues of the feature functions may range between
??, . . .+?, but typically they are binary. To make
all conditional probabilities sum up to 1, we must
calculate the normalization factor,
Z0 =
?
s
exp(
T
?
t=1
?
k
?k ? fk(st?1, st, o, t)),
which as in HMMs, can be obtained efficiently by
dynamic programming.
To train a CRF, the objective function to be maxi-
mized is the penalized log-likelihood of the state se-
quences given the observation sequences:
L? =
N
?
i=1
log(P?(s(i)|o(i))) ?
?
k
?2k
2?2 ,
where {< o(i), s(i) >} is the labeled training data.
The second sum corresponds to a zero-mean, ?2
-variance Gaussian prior over parameters, which
facilitates optimization by making the likelihood
surface strictly convex. Here, we set parameters
? to maximize the penalized log-likelihood using
Limited-memory BFGS (Sha and Pereira, 2003), a
quasi-Newton method that is significantly more ef-
ficient, and which results in only minor changes in
accuracy due to changes in ?.
When applying CRFs to the NER problem, an ob-
servation sequence is a token of a sentence or docu-
ment of text and the state sequence is its correspond-
ing label sequence. While CRFs generally can use
real-valued functions, in our experiments maximum
of the features are binary valued. A feature function
fk(st?1, st, o, t) has a value of 0 for most cases and
is only set to be 1, when st?1, st are certain states
and the observation has certain properties. We have
used the C++ based OpenNLP CRF++ package 1.
3 Named Entity Recognition in Bengali
Bengali is one of the widely used languages all over
the world. It is the seventh popular language in the
world, second in India and the national language of
Bangladesh. A partially NE tagged Bengali news
corpus (Ekbal and Bandyopadhyay, 2007d), devel-
oped from the archive of a widely read Bengali news
1http://crfpp.sourceforge.net
paper available in the web, has been used in this
work to identify and classify NEs. The corpus con-
tains around 34 million word forms in ISCII (Indian
Script Code for Information Interchange) and UTF-
8 format. The location, reporter, agency and differ-
ent date tags (date, ed, bd, day) in the partially NE
tagged corpus help to identify some of the location,
person, organization and miscellaneous names, re-
spectively, that appear in some fixed places of the
newspaper. These tags cannot detect the NEs within
the actual news body. The date information obtained
from the news corpus provides example of miscella-
neous names. A portion of this partially NE tagged
corpus has been manually annotated with the seven-
teen tags as described in Table 1.
NE tag Meaning Example
PER Single-word sachin/ PER
person name
LOC Single-word jadavpur/LOC
location name
ORG Single-word infosys/ ORG
organization name
MISC Single-word 100%/ MISC
miscellaneous name
B-PER Beginning, Internal sachin/B-PER
I-PER or End of a multiword ramesh/I-PER
E-PER person name tendulkar/E-PER
B-LOC Beginning, Internal or mahatma/B-LOC
I-LOC End of a multiword gandhi/I-LOC
E-LOC location name road/E-LOC
B-ORG Beginning, Internal or bhaba/B-ORG
I-ORG End of a multiword atomic/I-ORG
E-ORG organization name research/I-ORG
center/E-ORG
B-MISC Beginning, Internal or 10e/B-MISC
I-MISC End of a multiword magh/ I-MISC
E-MISC miscellaneous name 1402/E-MISC
NNE Words that are not NEs neta/NNE
Table 1: Named Entity Tagset
3.1 Named Entity Tagset
A CRF based NER system has been developed
in this work to identify NEs in Bengali and clas-
sify them into the predefined four major categories,
namely, ?Person name?, ?Location name?, ?Organi-
zation name? and ?Miscellaneous name?. In order to
590
properly denote the boundaries of NEs and to apply
CRF in NER task, sixteen NE and one non-NE tags
have been defined as shown in Table 1. In the out-
put, sixteen NE tags are replaced appropriately with
the four major NE tags by some simple heuristics.
3.2 Named Entity Features
Feature selection plays a crucial role in CRF frame-
work. Experiments were carried out to find out the
most suitable features for NER in Bengali. The
main features for the NER task have been iden-
tified based on the different possible combination
of available word and tag context. The features
also include prefix and suffix for all words. The
term prefix/suffix is a sequence of first/last few
characters of a word, which may not be a lin-
guistically meaningful prefix/suffix. The use of
prefix/suffix information works well for highly in-
flected languages like the Indian languages. In
addition, various gazetteer lists have been devel-
oped for use in the NER task. We have consid-
ered different combination from the following set
for inspecting the best feature set for NER task:
F={wi?m, . . . , wi?1, wi, wi+1, . . . wi+n, |prefix| ?
n, |suffix| ? n, previous NE tag, POS tags, First
word, Digit information, Gazetteer lists}.
Following are the details of the set of features that
were applied to the NER task:
? Context word feature: Previous and next words of
a particular word might be used as a feature.
? Word suffix: Word suffix information is helpful
to identify NEs. This feature can be used in two
different ways. The first and the nai?ve one is, a
fixed length word suffix of the current and/or the sur-
rounding word(s) might be treated as feature. The
second and the more helpful approach is to modify
the feature as binary valued. Variable length suf-
fixes of a word can be matched with predefined lists
of useful suffixes for different classes of NEs. The
different suffixes that may be particularly helpful in
detecting person (e.g., -babu, -da, -di etc.) and lo-
cation names (e.g., -land, -pur, -lia etc.) have been
considered also. Here, both types of suffixes have
been used.
? Word prefix: Prefix information of a word is also
helpful. A fixed length prefix of the current and/or
the surrounding word(s) might be treated as features.
? Part of Speech (POS) Information: The POS of
the current and/or the surrounding word(s) can be
used as features. Multiple POS information of the
words can be a feature but it has not been used in the
present work. The alternative and the better way is
to use a coarse-grained POS tagger.
Here, we have used a CRF-based POS tagger,
which was originally developed with the help of 26
different POS tags2, defined for Indian languages.
For NER, we have considered a coarse-grained POS
tagger that has only the following POS tags:
NNC (Compound common noun), NN (Com-
mon noun), NNPC (Compound proper noun), NNP
(Proper noun), PREP (Postpositions), QFNUM
(Number quantifier) and Other (Other than the
above).
The POS tagger is further modified with two
POS tags (Nominal and Other) for incorporating
the nominal POS information. Now, a binary val-
ued feature ?nominalPOS? is defined as: If the cur-
rent/previous/next word is ?Nominal? then the ?nom-
inalPOS? feature of the corresponding word is set to
1; otherwise, it is set to 0. This ?nominalPOS? fea-
ture has been used additionally with the 7-tag POS
feature. Sometimes, postpositions play an important
role in NER as postpositions occur very frequently
after a NE. A binary valued feature ?nominalPREP?
is defined as: If the current word is nominal and the
next word is PREP then the feature ?nomianlPREP?
of the current word is set to 1, otherwise set to 0.
? Named Entity Information: The NE tag of the pre-
vious word is also considered as the feature. This is
the only dynamic feature in the experiment.
? First word: If the current token is the first word of
a sentence, then the feature ?FirstWord? is set to 1.
Otherwise, it is set to 0.
? Digit features: Several binary digit features
have been considered depending upon the presence
and/or the number of digits in a token (e.g., Con-
tainsDigit [token contains digits], FourDigit [token
consists of four digits], TwoDigit [token consists
of two digits]), combination of digits and punctu-
ation symbols (e.g., ContainsDigitAndComma [to-
ken consists of digits and comma], ConatainsDigi-
tAndPeriod [token consists of digits and periods]),
combination of digits and symbols (e.g., Contains-
DigitAndSlash [token consists of digit and slash],
2http://shiva.iiit.ac.in/SPSAL2007/iiit tagset guidelines.pdf
591
ContainsDigitAndHyphen [token consists of digits
and hyphen], ContainsDigitAndPercentage [token
consists of digits and percentages]). These binary
valued features are helpful in recognizing miscella-
neous NEs such as time expressions, monetary ex-
pressions, date expressions, percentages, numerical
numbers etc.
? Gazetteer Lists: Various gazetteer lists have been
developed from the partially NE tagged Bengali
news corpus (Ekbal and Bandyopadhyay, 2007d).
These lists have been used as the binary valued fea-
tures of the CRF. If the current token is in a particu-
lar list then the corresponding feature is set to 1 for
the current and/or the surrounding word(s); other-
wise, set to 0. The following is the list of gazetteers:
(i) Organization suffix word (94 entries): This list
contains the words that are helpful in identifying or-
ganization names (e.g., kong, limited etc). The fea-
ture ?OrganizationSuffix? is set to 1 for the current
and the previous words.
(ii) Person prefix word (245 entries): This is useful
for detecting person names (e.g., sriman, sree, sri-
mati etc.). The feature ?PersonPrefix? is set to 1 for
the current and the next two words.
(iii) Middle name (1,491 entries): These words gen-
erally appear inside the person names (e.g., chandra,
nath etc.). The feature ?MiddleName? is set to 1 for
the current, previous and the next words.
(iv) Surname (5,288 entries): These words usually
appear at the end of person names as their parts. The
feature ?SurName? is set to 1 for the current word.
(v) Common location word (547 entries): This list
contains the words that are part of location names
and appear at the end (e.g., sarani, road, lane etc.).
The feature ?CommonLocation? is set to 1 for the
current word.
(vi) Action verb (221 entries): A set of action verbs
like balen, ballen, ballo, shunllo, haslo etc. often
determines the presence of person names. The fea-
ture ?ActionVerb? is set to 1 for the previous word.
(vii) Frequent word (31,000 entries): A list of most
frequently occurring words in the Bengali news cor-
pus has been prepared using a part of the corpus.
The feature ?RareWord? is set to 1 for those words
that are not in this list.
(viii) Function words (743 entries): A list of func-
tion words has been prepared manually. The feature
?NonFunctionWord? is set to 1 for those words that
are not in this list.
(ix) Designation words (947 entries): A list of com-
mon designation words has been prepared. This
helps to identify the position of the NEs, partic-
ularly person names (e.g., neta, sangsad, kheloar
etc.). The feature ?DesignationWord? is set to 1 for
the next word.
(x) Person name (72, 206 entries): This list contains
the first name of person names. The feature ?Person-
Name? is set to 1 for the current word.
(xi) Location name (7,870 entries): This list contains
the location names and the feature ?LocationName?
is set to 1 for the current word.
(xii) Organization name (2,225 entries): This list
contains the organization names and the feature ?Or-
ganizationName? is set to 1 for the current word.
(xiii) Month name (24 entries): This contains the
name of all the twelve different months of both En-
glish and Bengali calendars. The feature ?Month-
Name? is set to 1 for the current word.
(xiv) Weekdays (14 entries): It contains the name of
seven weekdays in Bengali and English both. The
feature ?WeekDay? is set to 1 for the current word.
4 Experimental Results
A partially NE tagged Bengali news corpus (Ekbal
and Bandyopadhyay, 2007d) has been used to cre-
ate the training set for the NER experiment. Out of
34 million wordforms, a set of 150K wordforms has
been manually annotated with the 17 tags as shown
in Table 1 with the help of Sanchay Editor 3, a text
editor for Indian languages. Around 20K NE tagged
corpus has been selected as the development set and
the rest 130K wordforms has been used as the train-
ing set of the CRF based NER system.
We define the baseline model as the one where
the NE tag probabilities depend only on the cur-
rent word: P (t1, t2, . . . , tn|w1, w2, . . . , wn) =
?
i=1,...,n P (ti, wi).
In this model, each word in the test data will be
assigned the NE tag which occurred most frequently
for that word in the training data. The unknown
word is assigned the NE tag with the help of vari-
ous gazetteers and NE suffix lists.
Ninety-five different experiments were conducted
taking the different combinations from the set ?F? to
3Sourceforge.net/project/nlp-sanchay
592
Feature (word, tag) FS
(in %)
pw, cw, nw, FirstWord 71.31
pw2, pw, cw, nw, nw2, FirstWord 72.23
pw3, pw2, pw, cw, nw, nw2, nw3, 71.12
FirstWord
pw2, pw, cw, nw, nw2, FirstWord, pt 74.91
pw2, pw, cw, nw, nw2, FirstWord, pt, 77.61
|pre| ? 4, |suf| ? 4
pw2, pw, cw, nw, nw2, FirstWord, pt, 79.70
|suf| ? 3, |pre| ? 3
pw2, pw, cw, nw, nw2, FirstWord, pt, 81.50
|suf| ? 3, |pre| ? 3, Digit features
pw2, pw, cw, nw, nw2, FirstWord, pt, 83.60
|suf| ? 3, |pre| ? 3, Digit features, pp,
cp, np
pw2, pw, cw, nw, nw2, FirstWord, pt, 82.20
|suf| ? 3, |pre| ? 3, Digit features,
pp2, pp, cp, np, np2
pw2, pw, cw, nw, nw2, FirstWord, pt, 83.10
|suf| ? 3, |pre| ? 3, Digit features, pp, cp
pw2, pw, cw, nw, nw2, FirstWord, pt, 83.70
|suf| ? 3, |pre| ? 3, Digit features, cp, np
pw2, pw, cw, nw, nw2, FirstWord, pt, 89.30
|suf| ? 3,|pre| ? 3, Digit features, pp,
cp, np, nominalPOS, nominalPREP,
Gazetteer lists
Table 2: Results on Development Set
identify the best suited set of features for the NER
task. From our empirical analysis, we found that the
following combination gives the best result with 744
iterations:
F=[wi?2, wi?1, wi, wi+1, wi+2, |prefix| ? 3,
|sufix| ? 3, NE information of the previous word,
POS information of the window three, nominalPOS
of the current word, nominalPREP, FirstWord, Digit
features, Gazetteer lists].
The meanings of the notations, used in experi-
mental results, are defined as below:
cw, pw, nw: Current, previous and next word; pwi,
nwi: Previous and the next ith word from the current
word; pre, suf: Prefix and suffix of the current word;
pt: NE tag of the previous word; cp, pp, np: POS tag
of the current, previous and the next word; ppi, npi:
POS tag of the previous and the next ith word.
Evaluation results of the system for the develop-
ment set in terms of overall F-Score (FS) are pre-
sented in Table 2. It is observed from Table 2 that
word window [?2,+2] gives the best result with
?FirstWord? feature only and the further increase of
the window size reduces the overall F-Score value.
Results of Table 2 (3rd and 5th rows) show that
the inclusion of NE information of the previous
word increases the overall F-Score by 2.68%. It is
also indicative from the evaluation results that the
performance of the system can be improved by in-
cluding the prefix and suffix features. Results (6th
and 7th rows) also show the fact that prefix and suf-
fix of length upto three of the current word is more
effective. In another experiment, it has been also ob-
served that the surrounding word suffixes and/or pre-
fixes do not increase the F-Score value. The overall
F-Score value is further improved by 1.8% (7th and
8th rows) with the inclusion of various digit features.
Results (8th and 9th rows) show that POS in-
formation of the words improves the overall F-score
by 2.1%. In the above experiment, the POS tag-
ger was developed with 26 POS tags. Experimen-
tal results (9th, 10th, 11th and 12th rows) suggest
that the POS tags of the previous, current and the
next words, i.e., POS information of the window
[?1,+1] is more effective than POS information of
the window [?2,+2], [?1, 0] or [0,+1]. In another
experiment, we also observed that the POS informa-
tion of the current word alone is less effective than
the window [?1,+1]. The modified POS tagger that
is developed with 7 POS tags increases the overall F-
Score to 85.2%, while other set of features are kept
unchanged. So, it can be decided that smaller POS
tagset is more effective than the larger POS tagset
in NER. We have observed from two separate ex-
periments that the overall F-Score values can further
be improved by 0.4% and 0.2%, respectively, with
the ?nominalPOS? and ?nominalPREP? features. Fi-
nally, an overall F-Score value of 89.3% is obtained
by including the gazetteer lists.
The best set of features is identified by training
the system with 130K wordforms and testing with
the development set of 20K wordforms. Now, the
development set is included as part of the train-
ing set and resultant training set is thus consists of
150K wordforms. The training set has 20,455 per-
son names, 11,668 location names, 963 organization
593
names and 11,554 miscellaneous names. We have
performed 10-fold cross validation test on this train-
ing set. The Recall, Precision and F-Score values
for the 10 different experiments in the 10-fold cross
validation test are presented in Table 3. The over-
all average Recall, Precision and F-Score values are
93.8%, 87.8% and 90.7%, respectively.
The other existing Bengali NER systems along
with the baseline model are also trained and tested
under the same experimental setup. The baseline
model has demonstrated the overall F-Score value of
56.3%. The overall F-Score value of the CRF based
NER system is 90.7%, which is an improvement of
more than 6% over the HMM based system, best re-
ported Bengali NER system (Ekbal et al, 2007c).
The reason behind the rise in overall F-Score value
might be its better capability than HMM to capture
the morphologically rich and overlapping features of
Bengali language. The system has been evaluated
also for the four individual NE classes and it has
shown the average F-Score values of 91.2%, 89.7%,
87.1% and 99.2%, respectively, for person, location,
organization and miscellaneous names.
5 Conclusion
In this paper, we have developed a NER system us-
ing CRF with the help of a partially NE tagged Ben-
gali news corpus, developed from the archive of a
leading Bengali newspaper available in the web. Ex-
perimental results with the 10-fold cross validation
test have shown reasonably good Recall, Precision
and F-Score values. It has been shown that the con-
textual window [-2, +2], prefix and suffix of length
upto three, first word of the sentence, POS informa-
tion of the window [-1, +1], current word, NE infor-
mation of the previous word, different digit features
and the various gazetteer lists are the best-suited fea-
tures for the Bengali NER.
Analyzing the performance using other methods
like MaxEnt and Support Vector Machines (SVMs)
will be other interesting experiments.
References
Daniel M. Bikel, Richard L. Schwartz, and Ralph M.
Weischedel. 1999. An Algorithm that Learns What?s
in a Name. Machine Learning, 34(1-3):211?231.
A. Borthwick. 1999. Maximum Entropy Approach to
Test set no. Recall Precision FS (%)
1 92.4 87.3 89.78
2 92.3 87.4 89.78
3 91.4 86.6 88.94
4 95.2 87.7 91.29
5 91.6 86.7 89.08
6 92.2 87.1 89.58
7 94.5 87.9 91.08
8 93.8 89.3 91.49
9 96.9 88.4 92.45
10 97.7 89.6 93.47
Average 93.8 87.8 90.7
Table 3: Results for the 10-fold Cross Validation
Test
Named Entity Recognition. Ph.D. thesis, New York
University.
A. Ekbal and S. Bandyopadhyay. 2007a. Lexical Pattern
Learning from Corpus Data for Named Entity Recog-
nition. In Proceedings of ICON, pages 123?128, India.
A. Ekbal and S. Bandyopadhyay. 2007b. Pattern Based
Bootstrapping Method for Named Entity Recognition.
In Proceedings of ICAPR, pages 349?355, India.
A. Ekbal and S. Bandyopadhyay. 2007d. A Web-
based Bengali News Corpus for Named Entity Recog-
nition. Language Resources and Evaluation Journal
(accepted).
A. Ekbal, S.K. Naskar, and S. Bandyopadhyay. 2007c.
Named Entity Recognition and Transliteration in Ben-
gali. Named Entities: Recognition, Classification
and Use, Special Issue of Lingvisticae Investigationes
Journal, 30(1):95?114.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional Random Fields: Proba-
bilistic Models for Segmenting and Labeling Sequence
Data. In ICML, pages 282?289.
Wei Li and Andrew McCallum. 2004. Rapid Develop-
ment of Hindi Named Entity Recognition using Con-
ditional Random Fields and Feature Induction. ACM
TALIP, 2(3):290?294.
A. McCallum and W. Li. 2003. Early results for Named
Entity Recognition with Conditional Random Fields,
Feature Induction and Web-enhanced Lexicons. In
Proceedings of CoNLL, pages 188?191, Canada.
Fei Sha and Fernando Pereira. 2003. Shallow Parsing
with Conditional Random Fields. In Proceedings of
NAACL ?03, pages 134?141, Canada.
594
Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 33?40,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
 
Language Independent Named Entity Recognition in  Indian Languages  
Asif Ekbal, Rejwanul Haque, Amitava Das, Venkateswarlu Poka 
and Sivaji Bandyopadhyay 
 Department of Computer Science and Engineering 
Jadavpur University 
Kolkata-700032, India 
asif.ekbal@gmail.com, rejwanul@gmail.com, 
amit_santu_kuntal@yahoo.com, venkat.ju@gmail.com and 
sivaji_cse_ju@yahoo.com 
            
Abstract 
This paper reports about the development 
of a Named Entity Recognition (NER) sys-
tem for South and South East Asian lan-
guages, particularly for Bengali, Hindi, Te-
lugu, Oriya and Urdu as part of the 
IJCNLP-08 NER Shared Task
1
. We have 
used the statistical Conditional Random 
Fields (CRFs). The system makes use of 
the different contextual information of the 
words along with the variety of features 
that are helpful in predicting the various 
named entity (NE) classes. The system uses 
both the language independent as well as 
language dependent features. The language 
independent features are applicable for all 
the languages. The language dependent 
features have been used for Bengali and 
Hindi only. One of the difficult tasks of 
IJCNLP-08 NER Shared task was to iden-
tify the nested named entities (NEs) though 
only the type of the maximal NEs were 
given. To identify nested NEs, we have 
used rules that are applicable for all the 
five languages. In addition to these rules, 
gazetteer lists have been used for Bengali 
and Hindi. The system has been trained 
with Bengali (122,467 tokens), Hindi 
(502,974 tokens), Telugu (64,026 tokens), 
Oriya (93,173 tokens) and Urdu (35,447 
tokens) data. The system has been tested 
with the 30,505 tokens of Bengali, 38,708 
tokens of Hindi, 6,356 tokens of Telugu, 
                                                
1
http://ltrc.iiit.ac.in/ner-ssea-08  
24,640 tokens of Oriya and 3,782 tokens of 
Urdu. Evaluation results have demonstrated 
the highest maximal F-measure of 53.36%, 
nested F-measure of 53.46% and lexical F-
measure of 59.39% for Bengali.   
1 Introduction 
Named Entity Recognition (NER) is an impor-
tant tool in almost all Natural Language Proc-
essing (NLP) application areas. Proper identifi-
cation and classification of named entities are 
very crucial and pose a very big challenge to 
the NLP researchers. The level of ambiguity in 
named entity recognition (NER) makes it diffi-
cult to attain human performance.     
NER has drawn more and more attention 
from the named entity (NE) tasks (Chinchor 
95; Chinchor 98) in Message Understanding 
Conferences (MUCs) [MUC6; MUC7]. The 
problem of correct identification of named enti-
ties is specifically addressed and benchmarked 
by the developers of Information Extraction 
System, such as the GATE system (Cunning-
ham, 2001). NER also finds application in 
question-answering systems (Maldovan et al, 
2002) and machine translation (Babych and 
Hartley, 2003).  
The current trend in NER is to use the ma-
chine-learning approach, which is more attrac-
tive in that it is trainable and adoptable and the 
maintenance of a machine-learning system is 
much cheaper than that of a rule-based one. 
The representative machine-learning ap-
proaches used in NER are HMM (BBN?s Iden-
tiFinder in (Bikel, 1999)), Maximum Entropy 
33
(New York University?s MENE in (Borthwick, 
1999)), Decision Tree (New York University?s 
system in (Sekine 1998), SRA?s system in 
(Bennet, 1997) and Conditional Random Fields 
(CRFs) (Lafferty et al, 2001; McCallum and 
Li, 2003).       
There is no concept of capitalization in Indian 
languages (ILs) like English and this fact makes 
the NER task more difficult and challenging in 
ILs. There has been very little work in the area of 
NER in Indian languages. In Indian languages par-
ticularly in Bengali, the work in NER can be found 
in (Ekbal and Bandyopadhyay, 2007a) and  (Ekbal 
and Bandyopadhyay, 2007b). These two systems 
are based on the pattern directed shallow parsing 
approach. An HMM-based NER in Bengali can be 
found in (Ekbal et al, 2007c). Other than Bengali, 
the work on NER can be found in (Li and 
McCallum, 2004) for Hindi. This system is based 
on CRF.  
In this paper, we have reported a named entity 
recognition system for the south and south east 
Asian languages, particularly for Bengali, Hindi, 
Telugu, Oriya and Urdu. Bengali is the seventh 
popular language in the world, second in India and 
the national language of Bangladesh. Hindi is the 
third popular language in the world and the na-
tional language of India. Telugu is one of the popu-
lar languages and predominantly spoken in the 
southern part of India. Oriya and Urdu are the 
other two popular languages of India and widely 
used in the eastern and the northern part, respec-
tively. The statistical Conditional Random Field 
(CRF) model has been used to develop the system, 
as it is more efficient than HMM to deal with the 
non-independent and diverse overlapping features 
of the highly inflective Indian languages. We have 
used a fine-grained named entity tagset
2
, defined as 
part of the IJCNLP-08 NER Shared Task for 
SSEA. The system makes use of the different con-
textual information of the words along with the 
variety of orthographic word level features that are 
helpful in predicting the various named entity 
classes. In this work, we have considered language 
independent features as well as the language de-
pendent features. Language independent features 
include the contextual words, prefix and suffix in-
formation of all the words in the training corpus, 
several digit features depending upon the presence 
                                                
 
2
http://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=3  
and/or the number of digits in a token and the fre-
quency features of the words. The system consid-
ers linguistic features particularly for Bengali and 
Hindi. Linguistic features of Bengali include the 
set of known suffixes that may appear with named 
entities, clue words that help in predicating the lo-
cation and organization names, words that help to 
recognize measurement expressions, designation 
words that help in identifying person names, the 
various gazetteer lists like the first names, middle 
names, last names, location names and organiza-
tion names. As part of linguistic features for Hindi, 
the system uses only the lists of first names, middle 
names and last names along with the list of words 
that helps to recognize measurements. No linguis-
tic features have been considered for Telugu, Oriya 
and Urdu. It has been observed from the evaluation 
results that the use of linguistic features improves 
the performance of the system. A number of ex-
periments have been carried out to find out the 
best-suited set of features for named entity recog-
nition in Bengali, Hindi, Telugu, Oriya and Urdu.  
2 Conditional Random Fields 
Conditional Random Fields (CRFs) (Lafferty et al, 
2001) are undirected graphical models, a special 
case of which corresponds to conditionally trained 
probabilistic finite state automata. Being 
conditionally trained, these CRFs can easily 
incorporate a large number of arbitrary, non-
independent features while still having efficient 
procedures for non-greedy finite-state inference 
and training. CRFs have shown success in various 
sequence modeling tasks including noun phrase 
segmentation (Sha and Pereira, 2003) and table 
extraction (Pinto et al, 2003).     
CRFs are used to calculate the conditional 
probability of values on designated output nodes 
given values on other designated input nodes. The 
conditional probability of a state sequence 
1, 2, , TS s s s given an observation 
sequence 1 2,, ....., )TO o o o  is calculated as: 
1 ,
1
1
( | ) exp( ( , , )),
T
k k t t
o
t k
P s o f s s o t
Z
where
1 ,( , , )k t tf s s o t is a feature function whose weight 
k is to be learned via training. The values of the 
feature functions may range between ..... , 
but typically they are binary. To make all 
34
conditional probabilities sum up to 1, we must 
calculate the normalization 
factor, 0 1 ,
1
exp( ( , , ))
T
s k k t t
t k
Z f s s o t , 
which, as in HMMs, can be obtained efficiently by 
dynamic programming. 
To train a CRF, the objective function to be 
maximized is the penalized log-likelihood of the 
state sequences given observation sequences: 
2
( ) ( )
2
1
log( ( | ))
2
N
i i
k
i k
L P s o , 
where, {
( ) ( )
,
i i
o s } is the labeled training 
data. The second sum corresponds to a zero-mean,  
2
-variance Gaussaian prior over parameters, 
which facilitates optimization by making the like-
lihood surface strictly convex. Here, we set pa-
rameters 
 
to maximize the penalized log-
likelihood using Limited-memory BFGS (Sha and 
Pereira, 2003), a quasi-Newton method that is sig-
nificantly more efficient, and which results in only 
minor changes in accuracy due to changes in .  
When applying CRFs to the named entity 
recognition problem, an obsevation sequence is a 
token of a sentence or document of text and the 
state sequence is its corresponding label sequence. 
While CRFs generally can use real-valued 
functions, in our experiments maximum of the 
features are binary. A feature function 
1 ,( , , )k t tf s s o t has a value of 0 for most cases and 
is only set to be 1, when 1,t ts s are certain states 
and the observation has certain properties. We 
have used the C
++ 
based OpenNLP CRF++ pack-
age
3
, a simple, customizable, and open source im-
plementation of Conditional Random Fields 
(CRFs) for segmenting /labeling sequential data. 
3 Named Entity Recognition in Indian 
Languages 
Named Entity Recognition in Indian languages 
(ILs) is difficult and challenging as capitalization 
is not a clue in ILs. The training data were pro-
vided for five different Indian languages, namely 
Bengali, Hindi, Telugu, Oriya and Urdu in Shakti 
Standard Format
4
. The training data in all the lan-
                                                
3
http://crfpp.sourceforge.net  
4
http://shiva.iiit.ac.in/SPSAL 2007/ssf.html  
guages were annotated with the twelve NE tags, as 
defined for the IJCNLP-08 NER shared task taget
5
. 
Only the maximal named entities and not the inter-
nal structures of the entities were annotated in the 
training data. For example, mahatma gandhi road 
was annotated as location and assigned the tag 
?NEL? even if mahatma and gandhi  are named 
entity title person (NETP) and person name (NEP) 
respectively, according to the IJCNLP-08 shared 
task tagset. These internal structures of the entities 
were to be identified during testing. So, mahatma 
gandhi road will be tagged as mahatma /NETP 
gandhi/NEP road/NEL. The structure of the tagged 
element using the SSF form will be as follows:  
1 (( NP <ne=NEL>  
1.1 (( NP <ne=NEP> 
1.1.1 (( NP  <ne=NETP> 
1.1.1.1 mahatma 
)) 
1.1.2 gandhi 
)) 
1.2 road 
)) 
3.1 Training Data Preparation for CRF  
Training data for all the languages required some 
preprocessing in order to use in the Conditional 
Random Field framework. The training data is 
searched for the multiword NEs. Each component 
of the multiword NE is searched in the training set 
to find whether it occurs as a single-word NE. The 
constituent components are then replaced by their 
NE tags (NE type of the single-word NE). For ex-
ample, mahatma gandhi road/NEL will be tagged 
as mahatma/NETP gandhi/NEP road/NEL if the 
internal components are found to appear with these 
NE tags in the training set. Each component of a 
multiword NE is also checked whether the compo-
nent is made up of digits only. If a component is 
made up digits only, then it is assigned the tag 
?NEN?. Various gazetteers for Bengali and Hindi 
have been also used in order to identify the internal 
structure of the NEs properly.  The list of gazet-
teers, which have been used in preparing the train-
ing data, is shown in Table 1. 
The individual components (not occurring as a 
single-word NE in the training data) of a multi-
word NE are searched in the gazetteer lists and 
                                                
5
http://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=3  
35
assigned the appropriate NE tags. Other than NEs 
are marked with the NNE tags. The procedure is 
given below:  
Gazetteer list Number of entries 
First person name in Ben-
gali 
27,842 
Last person name in Ben-
gali 
5,288 
Middle name in Bengali 1,491 
Person name designation 
in Bengali 
947 
Location name in Bengali 7,870 
First person name in Hindi 1,62,881 
Last person name in Hindi 3,573 
Middle name in Hindi 450 
Cardinals in Bengali, 
Hindi and Telugu 
100 
Ordinals in Bengali, Hindi 
and Telugu 
65 
Month names in Bengali, 
Hindi and Telugu 
24 
Weekdays in Bengali, 
Hindi and Telugu 
14 
Words that denote meas-
urement in Bengali, Hindi 
and Telugu 
52 
Table 1. Gazetteer lists used during training data 
preparation  
 Step 1: Search the multiword NE in the training 
data 
Step 2: Extract each component from the mult-
word NE. 
Step 3: Check whether the constituent individual 
component (except the last one) appears in the 
training data as a single-word NE. 
Step 4: If the constituent NE appears in the training 
data as a single-word NE then 
Step 4.1: Assign the NE tag, extracted from the 
single-word NE, to the component of the multi-
word NE. 
else 
Step 4.2: Search the component in the gazetteer 
lists and assign the appropriate NE tag. 
Step 4.2.1: If the component is not found to appear 
in the gazetteer list then assign the NE tag of the 
maximal NE to the individual component.  
For example, if mahatma gandhi road is tagged 
as NEL, i.e., mahatma gandhi road/NEL then each 
component except the last one (road ) of this mult-
word NE is searched in the training set to look for 
it?s appearance (Step 3). Gazetteer lists are 
searched in case the component is not found in the 
training set (Step 4.2). If the components are found 
either in the training set or in the gazetteer list, 
then mahatma gandhi road/NEL will be tagged as: 
mahatma/NETP gandhi/NEP road/NEL. 
3.2 Named Entity Features 
Feature selection plays a crucial role in CRF 
framework. Experiments were carried out to find 
out most suitable features for NE tagging task. The 
main features for the NER task have been identi-
fied based on the different possible combination of 
available word and tag context. The features also 
include prefix and suffix for all words. The term 
prefix/suffix is a sequence of first/last few charac-
ters of a word, which may not be a linguistically 
meaningful prefix/suffix. The use of prefix/suffix 
information works well for highly inflected lan-
guages like the Indian languages. In addition, vari-
ous gazetteer lists have been developed to use in 
the NER task particularly for Bengali and Hindi. 
We have considered different combination from 
the following set for inspecting the best feature set 
for the NER task: 
 F={
1 1
,..., , , ,...,
i m i i i i n
w w w w w , |prefix| n, 
|suffix| n, previous NE tag, POS tags, First word, 
Digit information, Gazetteer lists} 
     Following is the details of the set of features 
that were applied to the NER task: 
 
Context word feature: Previous and next words 
of a particular word might be used as a feature. We 
have considered the word window of size five, i.e., 
previous and next two words from the current word 
for all the languages.  
Word suffix: Word suffix information is helpful 
to identify NEs. A fixed length word suffix of the 
current and surrounding words might be treated as 
feature. In this work, suffixes of length up to three 
the current word have been considered for all the 
languages. More helpful approach is to modify the 
feature as binary feature. Variable length suffixes 
of a word can be matched with predefined lists of 
useful suffixes for different classes of NEs. For 
Bengali, we have considered the different suffixes 
that may be particularly helpful in detecting person 
(e.g., -babu, -da, -di etc.). 
36
Word prefix: Prefix information of a word is also 
helpful. A fixed length prefix of the current and the 
surrounding words might be treated as features. 
Here, the prefixes of length up to three have been 
considered for all the language. 
Rare word: The lists of most frequently occurring 
words in the training sets have been calculated for 
all the five languages. The words that occur more 
than 10 times are considered as the frequently oc-
curring words in Bengali and Hindi. For Telugu, 
Oriya and Urdu, the cutoff frequency was chosen 
to be 5. Now, a binary feature ?RareWord? is de-
fined as: If current word is found to appear in the 
frequent word list then it is set to 1; otherwise, set 
to 0.   
First word: If the current token is the first word of 
a sentence, then this feature is set to 1. Otherwise, 
it is set to 0. 
Contains digit: For a token, if it contains digit(s) 
then the feature ?ContainsDigit? is set to 1. This 
feature is helpful for identifying the numbers.  
Made up of four digits: For a token if all the char-
acters are digits and having 4 digits then the fea-
ture ?FourDigit? is set to 1. This is helpful in iden-
tifying the time (e.g., 2007sal) and numerical (e.g., 
2007) expressions. 
Made up of two digits: For a token if all the char-
acters are digits and having 2 digits then the fea-
ture ?TwoDigit? is set to 1. This is helpful for iden-
tifying the time expressions (e.g., 12 ta, 8 am, 9 pm) 
in general. 
Contains digits and comma: For a token, if it con-
tains digits and commas then the feature ?Con-
tainsDigitsAndComma? is set to 1. This feature is 
helpful in identifying named entity measurement 
expressions (e.g., 120,45,330 taka) and numerical 
numbers (e.g., 120,45,330) 
Contains digits and slash: If the token contains 
digits and slash then the feature ?ContainsDigi-
tAndslash? is set to 1. This helps in identifying 
time expressions (e.g., 15/8/2007). 
Contains digits and hyphen: If the token contains 
digits and hyphen then the feature ?ContainsDigit-
sAndHyphen? is set to 1. This is helpful for the 
identification of time expressions (e.g., 15-8-2007). 
Contains digits and period: If the token contains 
digits and periods then the feature ?ContainsDigit-
sAndPeriod? is set to 1. This helps to recognize 
numerical quantities (e.g., 120453.35) and meas-
urements (e.g., 120453.35 taka). 
Contains digits and percentage: If the token con-
tains digits and percentage symbol then the feature 
?ContainsDigitsAndPercentage? is set to 1. This 
helps to recognize measurements (e.g., 120%). 
Named Entity Information: The NE tag of the 
previous word is also considered as the feature, i.e., 
the combination of the current and the previous 
output token has been considered. This is the only 
dynamic feature in the experiment. 
Gazetteer Lists: Various gazetteer lists have been 
created from a tagged Bengali news corpus (Ekbal 
and Bandyopadhyay, 2007d) for Bengali. The first, 
last and middle names of person for Hindi have 
been created from the election commission data
6
. 
The person name collections had to be processed in 
order to use it in the CRF framework. The simplest 
approach of using these gazetteers is to compare 
the current word with the lists and make decisions. 
But this approach is not good, as it can?t resolve 
ambiguity. So, it is better to use these lists as the 
features of the CRF. If the current token is in a par-
ticular list, then the corresponding feature is set to 
1 for the current/previous/next token; otherwise, 
set to 0. The list of gazetteers is shown in Table 2. 
3.3 Nested Named Entity Identification 
One of the important tasks of the IJCNLP-NER 
shared task was to identify the internal named enti-
ties within the maximal NEs. In the training data, 
only the type of the maximal NEs were given. In 
order to identify the internal NEs during testing, 
we have defined some rules. After testing the un-
annotated test data with the CRF based NER sys-
tem, it is searched to find the sequence of NE tags. 
The last NE tag in the sequence is assigned as the 
NE tag of the maximal NE. The NE tags of the 
constituent NEs may either be changed or may not 
be changed. The NE tags are changed with the help 
of rules and various gazetteer lists. We identified 
NEM (Named entity measurement), NETI (Named 
entity time expressions), NEO (Named entity or-
ganization names), NEP (Named entity person 
names) and NEL (Named entity locations) to be 
the potential NE tags, where nesting could occur. 
A NEM expression may contain NEN, an NETI 
may contain NEN, an NEO may contain NEP/ 
NEL, an NEL may contain NEP/NETP/NED and 
an NEP may contain NEL expressions. The nested 
                                                
 
6 
http://www.eci.gov.in/DevForum/Fullname.asp 
37
NEN tags could be identified by simply checking 
whether it contains digits only and checking the 
lists of cardinal and ordinal numbers.  
  Gazetteer  Number 
of entries
 
 Feature Descrip-
tions 
Designation 
words in Bengali 
947 ?Designation? set to 
1, otherwise 0  
Organization 
names in Bengali 
2, 225 ?Organization? set 
to 1, otherwise 0. 
Organization 
suffixes in Ben-
gali 
94 ?OrgSuffix? set to 
1, otherwise 0 
Person prefix for 
Bengali 
245 ?PersonPrefix? set 
to 1, otherwise set 
to 0 
First person 
names in Bengali
27,842 ?FirstName? set to 
1, otherwise 0 
Middle names in 
Bengali 
1,491 ?MiddleName? set 
to 1, otherwise 0 
Surnames in 
Bengali 
5,288 ?SurName? set to 1, 
otherwise 0 
Common loca-
tion word in 
Bengali 
75 ?CommonLocation? 
set 1, otherwise 0 
Action verb in 
Bengali 
215 ?ActionVerb? set to 
1, otherwise 0 
First person 
names in Hindi 
1,62,881 ?FirstName? set to 
1, otherwise 0 
Middle person 
names in Hindi 
450 ?MiddleName? set 
to 1, otherwise 0 
Last person 
names in Hindi 
3,573 ?SurName? set to 1, 
otherwise 0 
Location names 
in Bengali 
7,870 ?LocationName? 
set to 1, otherwise 
0 
Week days in 
Bengali, Hindi 
and Telugu 
14 ?WeekDay? set to 
1, otherwise 0 
Month names in 
Bengali, Hindi 
and Telugu 
24 ?MonthName? set 
to 1, otherwise 0 
Measurements in 
Bengali, Hindi 
and Telugu 
52 ?Measurement? set 
to 1, otherwise 0. 
 Table 2. Named entity gazetteer list   
The procedure for identifying the nested NEs are 
shown below: 
Step1: Test the unannotated test set. 
Step 2: Look for the sequence of NE tags. 
Step 3: All the words in the sequence will belong     
to a maximal NE. 
Step 4: Assign the last NE tag in the sequence to 
the maximal NE. 
Step 5: The test set is searched to look whether 
each component word appears with a NE tag.  
Step 6: Assign the particular NE tag to the compo-
nent if it appears in the test set with that NE tag. 
Otherwise, search the gazetteer lists as shown in 
Tables 1-2 to assign the tag. 
4 Evaluation 
The evaluation measures used for all the five lan-
guages are precision, recall and F-measure. These 
measures are calculated in three different ways: 
(i). Maximal matches: The largest possibles 
named entities are matched with the reference data. 
(ii). Nested matches: The largest possible as 
well as nested named entities are matched. 
(iii). Maximal lexical item matches: The lexical 
items inside the largest possible named entities are 
matched. 
(iv). Nested lexical item matches: The lexical 
items inside the largest possible as well as nested 
named entities are matched.  
5 Experimental Results 
The CRF based NER system has been trained and 
tested with five different Indian languages namely, 
Bengali, Hindi, Telugu, Oriya and Urdu data.  The 
training and test sets statistics are presented in Ta-
ble 3. Results of evaluation as explained in the 
previous section are shown in Table 4. The F-
measures for the nested lexical match are also 
shown individually for each named entity tag sepa-
rately in Table 5. 
Experimental results of Table 4 show that the 
CRF based NER system performs best for Bengali 
with maximal F-measure of 55.36%, nested F-
measure of 61.46% and lexical F-measure 59.39%. 
The system has demonstrated the F-measures of 
35.37%, 36.75% and 33.12%, respectively for 
maximal, nested and lexical matches. The system 
has shown promising precision values for Hindi. 
But due to the low recall values, the F-measures 
get reduced. The large difference between the re-
call and precision values in the evaluation results 
of Hindi indicates that the system is not able to 
retrieve a significant number of NEs from the test 
38
data. In comparison to Hindi, the precision values 
are low and the recall values are high for Bengali. 
It can be decided from the evaluation results that 
system retrieves more NEs in Bengali than Hindi 
but involves more errors. The lack of features in 
Oriya, Telugu and Urdu might be the reason be-
hind their poor performance.   
Language
 
Number of 
tokens in the 
training set 
Number of to-
kens in the test 
set 
Bengali 122,467 30,505 
Hindi 502,974 38,708 
Telugu 64,026 6,356 
Oriya 93,173 24,640 
Urdu 35,447 3,782 
Table 3: Training and Test Sets Statistics  
Tag Bengali Hindi Oriya Telugu Urdu 
NEP 85.68 21.43 43.76 1.9 7.69 
NED 35.9 38.70 NF NF NF 
NEO 52.53 NF 5.60 NF 22.02 
NEA 26.92 30.77 NF NF NF 
NEB NF NF NF NF NF 
NETP 61.44 NF 12.55 NF NF 
NETO 45.98 NF NF NF NF 
NEL 80.00 22.70 31.49 0.73 50.14 
NETI 53.43 49.60 27.08 7.64 49.28 
NEN 30.12 85.40 9.19 9.16 NF 
NEM 79.08 36.64 7.56 NF 79.27 
NETE 18.06 1.64 NF 5.74 NF 
Table 4. Evaluation for Specific NE Tags (F-
Measures for nested lexical match) [NF: Nothing 
found]  
Experimental results of Table 5 show the F-
measures for the nested lexical item matches for 
individual NE tags. For Bengali, the system has 
shown reasonably high F-measures for NEP, NEL 
and NEM tags and medium F-measures for NETP, 
NETI, NEO and NETO tags. The overall F-
measures in Bengali might have reduced due to 
relatively poor F-measures for NETE, NEN, NEA 
and NED tags. For Hindi, the highest F-measure 
obtained is 85.4% for NEN tag followed by NETI, 
NED, NEM, NEA, NEL and NEP tags. In some 
cases, the system has shown better F-measures for 
Hindi than Bengali also. The system has performed 
better for NEN, NED and NEA tags in Hindi than 
all other languages. 
6 Conclusion  
We have developed a named entity recognition 
system using Conditional Random Fields for the 
five different Indian languages, namely Bengali, 
Hindi, Telugu, Oriya and Urdu. We have consid-
ered the contextual window of size five, prefix and 
suffix of length upto three of the current word, NE 
information of the previous word, different digit 
features and the frequently occurring word lists. 
The system also uses linguistic features extracted 
from the various gazetteer lists for Bengali and 
Hindi. Evaluation results show that the system per-
forms best for Bengali. The performance of the 
system for Bengali can further be improved by in-
cluding the part of speech (POS) information of the 
current and/or the surrounding word(s). The per-
formance of the system for other languages can be 
improved with the use of different linguistic fea-
tures as like Bengali.  
The system did not perform as expected due to 
the problems faced during evaluation regarding the 
tokenization. We have tested the system for Ben-
gali with 10-fold cross validation and obtained im-
pressive results.  
References 
Babych, Bogdan, A. Hartley. Improving machine trans-
lation quality with automatic named entity recogni-
tion. In Proceedings of EAMT/EACL 2003 Workshop 
on MT and other language technology tools, 1-8, 
Hungary. 
Bennet, Scott W.; C. Aone; C. Lovell. 1997. Learning to 
Tag Multilingual Texts Through Observation. In 
Proceedings of EMNLP, 109-116,  Rhode Island.  
Bikel, Daniel M., R. Schwartz, Ralph M. Weischedel. 
1999. An Algorithm that Learns What?s in Name. 
Machine Learning (Special Issue on NLP), 1-20. 
Bothwick, Andrew. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. Thesis, 
New York University. 
Chinchor, Nancy. 1995. MUC-6 Named Entity Task 
Definition (Version 2.1). MUC-6, Columbia, Mary-
land.   
39
     
Table 5. Evaluation of the Five Languages  
Chinchor, Nancy. 1998. MUC-7 Named Entity Task 
Definition (Version 3.5). MUC-7. Fairfax, Vir-
ginia. 
Cunningham, H. 2001. GATE: A general architecture 
for text engineering. Comput.  Humanit. (36), 223-
254. 
Ekbal, Asif, and S. Bandyopadhyay. 2007a. Pattern 
Based Bootstrapping Method for Named Entity 
Recognition. In Proceedings of 6
th 
International 
Conference on Advances in Pattern Recognition, 
Kolkata,    India, 349-355. 
Ekbal, Asif, and S. Bandyopadhyay. 2007b. Lexical 
Pattern Learning from Corpus Data for Named En-
tity Recognition. In Proceedings of the 5
th 
Interna-
tional Conference on Natural Language Process-
ing, Hyderabad, India, 123-128. 
Ekbal, Asif, Naskar, Sudip and S. Bandyopadhyay.   
2007c. Named Entity Recognition and Translitera-
tion in Bengali. Named Entities: Recognition, 
Classification and Use, Special Issue of Lingvisti-
cae Investigationes Journal, 30:1 (2007), 95-114. 
Ekbal, Asif, and S. Bandyopadhyay. 2007d. A Web-
based Bengali News Corpus for Named Entity 
Recognition. Language Resources and Evaluation 
Journal (Accepted) 
Lafferty, J., McCallum, A., and Pereira, F. 2001. 
Conditional random fields: Probabilistic models 
for segmenting and labeling sequence data. In 
Proc. of 18
th
 International Conference on Machine 
learning. 
Li, Wei and Andrew McCallum. 2003. Rapid Devel-
opment of Hindi Named Entity Recognition Using 
Conditional Random Fields and Feature Induc-
tions, ACM TALIP, 2(3), (2003), 290-294. 
McCallum, A.; W. Li. 2003. Early Results for Named 
Entity Recognition with Conditional Random 
Fields, Feature Induction and Web-Enhanced 
Lexicons. In Proceedings CoNLL-03, Edmanton, 
Canada. 
Moldovan, Dan I., Sanda M. Harabagiu, Roxana 
Girju, P. Morarescu, V. F. Lacatusu, A. Novischi, 
A. Badulescu, O. Bolohan. 2002. LCC Tools for 
Question Answering. In Proceedings of the TREC, 
Maryland, 1-10. 
Pinto, D., McCallum, A., Wei, X., and Croft, W. B. 
2003. Table extraction using conditional random 
fields. In Proceedings of SIGIR 03 Conference, 
Toronto, Canada. 
Sekine, Satoshi. 1998. Description of the Japanese 
NE System Used for MET-2, MUC-7, Fairfax, 
Virginia. 
Sha, F. and Pereira, F. 2003. Shallow parsing with 
conditional random fields. In Proceedings of Hu-
man Language Technology, NAACL. 
Measure
Precision Recall F-measure 
Language P
m 
P
n 
P
l 
R
m 
R
n 
R
l 
F
m 
F
n 
F
l 
Bengali 51.63 47.74 52.90 59.60 61.46 67.71 55.36 61.46 59.39
Hindi 71.05 76.08 80.59 23.54 24.23 20.84 35.37 36.75 33.12
Oriya 27.12 27.18 50.40 12.88 10.53 20.07 17.47 15.18 28.71
Telugu 1.70 2.70 8.10 0.538 0.539 3.34 0.827 0.902 4.749
Urdu 49.16 48.93 54.45 21.95 20.15 26.36 30.35 28.55 35.52
M: Maximal,  n: Nested, l: Lexical 
40
Bengali, Hindi and Telugu to English Ad-hoc Bilingual task  
 
Sivaji Bandyopadhyay, Tapabrata Mondal, Sudip Kumar Naskar, 
Asif Ekbal, Rejwanul Haque, Srinivasa Rao Godavarthy 
 
Abstract 
 
This paper presents the experiments carried out at Jadavpur University as 
part of participation in the CLEF 2007 ad-hoc bilingual task. This is our first 
participation in the CLEF evaluation task and we have considered Bengali, 
Hindi and Telugu as query languages for the retrieval from English 
document collection. We have discussed our Bengali, Hindi and Telugu to 
English CLIR system as part of the ad-hoc bilingual task, English IR system 
for the ad-hoc monolingual task and the associated experiments at CLEF. 
Query construction was manual for Telugu-English ad-hoc bilingual task, 
while it was automatic for all other tasks. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 143?148,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
MATREX: The DCU MT System for WMT 2010
Sergio Penkale, Rejwanul Haque, Sandipan Dandapat, Pratyush Banerjee, Ankit K. Srivastava,
Jinhua Du, Pavel Pecina, Sudip Kumar Naskar, Mikel L. Forcada, Andy Way
CNGL, School of Computing
Dublin City University, Dublin 9, Ireland
{ spenkale, rhaque, sdandapat, pbanerjee, asrivastava, jdu, ppecina, snaskar, mforcada, away }@computing.dcu.ie
Abstract
This paper describes the DCU machine
translation system in the evaluation cam-
paign of the Joint Fifth Workshop on Sta-
tistical Machine Translation and Metrics
in ACL-2010. We describe the modular
design of our multi-engine machine trans-
lation (MT) system with particular focus
on the components used in this partici-
pation. We participated in the English?
Spanish and English?Czech translation
tasks, in which we employed our multi-
engine architecture to translate. We also
participated in the system combination
task which was carried out by the MBR
decoder and confusion network decoder.
1 Introduction
In this paper, we present the DCU multi-engine
MT system MATREX (Machine Translation using
Examples). This system exploits example-based
MT, statistical MT (SMT), and system combina-
tion techniques.
We participated in the English?Spanish (en?
es) and English?Czech (en?cs) translation
tasks. For these two tasks, we employ several
individual MT systems: 1) Baseline: phrase-
based SMT (Koehn et al, 2007); 2) EBMT:
Monolingually chunking both source and target
sides of the dataset using a marker-based chunker
(Gough and Way, 2004); 3) Factored translation
model (Koehn and Hoang, 2007); 4) Source-side
context-informed (SSCI) systems (Stroppa et al,
2007); 5) the moses-chart (a Moses imple-
mentation of the hierarchical phrase-based (HPB)
approach of Chiang (2007)) and 6) Apertium (For-
cada et al, 2009) rule-based machine translation
(RBMT). Finally, we use a word-level combina-
tion framework (Rosti et al, 2007) to combine the
multiple translation hypotheses and employ a new
rescoring model to generate the final translation.
For the system combination task, we first use
the minimum Bayes-risk (MBR) (Kumar and
Byrne, 2004) decoder to select the best hypoth-
esis as the alignment reference for the confusion
network (CN) (Mangu et al, 2000). We then build
the CN using the TER metric (Snover et al, 2006),
and finally search for the best translation.
The remainder of this paper is organised as fol-
lows: Section 2 details the various components of
our system, in particular the multi-engine strate-
gies used for the shared task. In Section 3, we
outline the complete system setup for the shared
task and provide evaluation results on the test set.
Section 4 concludes the paper.
2 The MATREX System
2.1 System Architecture
The MATREX system is a combination-based
multi-engine architecture, which exploits as-
pects of both the EBMT and SMT paradigms.
The architecture includes various individual sys-
tems: phrase-based, example-based, hierarchical
phrase-based and tree-based MT.
The combination structure uses the MBR and
CN decoders, and is based on a word-level com-
bination strategy (Du et al, 2009). In the final
stage, we use a new rescoring module to process
the N -best list generated by the combination mod-
ule. Figure 1 illustrates the architecture.
2.2 Example-Based Machine Translation
The EBMT system uses a language-specific, re-
duced set of closed-class marker morphemes or
lexemes (Gough and Way, 2004) to define a way
to segment sentences into chunks, which are then
aligned using an edit-distance-style algorithm, in
which edit costs depend on word-to-word transla-143
Figure 1: System Framework.
tion probabilities and the amount of word-to-word
cognates (Stroppa and Way, 2006).
Once these phrase pairs were obtained they
were merged with the phrase pairs extracted by
the baseline system adding word alignment infor-
mation.
2.3 Apertium RBMT
Apertium1 is a free/open-source platform for
RBMT. The current version of the en?es system
in Apertium was used for the system combination
task (section 2.7), and its morphological analysers
and part-of-speech taggers were used to build a
factored Moses model.
2.4 Factored Translation Model
We also used a factored model for the en?es
translation task. Factored models (Koehn and
Hoang, 2007) facilitate the translation by break-
ing it down into several factors which are further
combined using a log-linear model (Och and Ney,
2002).
We used three factors in our factored translation
model, which are used in two different decoding
paths: a surface form (SF) to SF translation factor,
a lemma to lemma translation factor, and a part-of-
speech (PoS) to PoS translation factor.
Finally, we used two decoding paths based on
1http://www.apertium.org
the above three translation factors: an SF to SF
decoding path and a path which maps lemma to
lemma, PoS to PoS, and an SF generated using
the TL lemma and PoS. The lemmas and PoS for
en and es were obtained using Apertium (sec-
tion 2.3).
2.5 Source-Side Context-informed PB-SMT
One natural way to express a context-informed
feature (h?MBL) is to view it as the conditional
probability of the target phrases (e?k) given the
source phrase (f?k) and its source-side context in-
formation (CI):
h?MBL = logP (e?k|f?k,CI(f?k)) (1)
We use a memory-based machine learning
(MBL) classifier (TRIBL:2 Daelemans and
van den Bosch (2005)) that is able to estimate
P (e?k|f?k,CI(f?k)) by similarity-based reasoning
over memorized nearest-neighbour examples of
source?target phrase translations. In equation (1),
SSCI may include any feature (lexical, syntactic,
etc.), which can provide useful information to
disambiguate a given source phrase. In addition
to using local words and PoS-tags as features,
as in (Stroppa et al, 2007), we incorporate
grammatical dependency relations (Haque et al,
2009a) and supertags (Haque et al, 2009b) as
syntactic source context features in the log-linear
PB-SMT model.
In addition to the above feature, we derived a
simple binary feature h?best, defined in (2):
h?best =
{
1 if e?k maximizes P (e?k|f?k,CI(f?k))
0 otherwise
(2)
We performed experiments by integrating these
two features, h?MBL and h?best, directly into the
log-linear framework of Moses.
2.6 Hierarchical PB-SMT model
For the en?cs translation task, we built
a weighted synchronous context-free grammar
model (Chiang, 2007) of translation that uses
the bilingual phrase pairs of PB-SMT as a start-
ing point to learn hierarchical rules. We used
the open-source Tree-Based translation system
moses-chart3 to perform this experiment.
2An implementation of TRIBL is freely available as part
of the TiMBL software package, which can be downloaded
from http://ilk.uvt.nl/timbl
3http://www.statmt.org/moses/?n=Moses.SyntaxTutorial144
2.7 System Combination
For multiple system combination, we used an
MBR-CN framework (Du et al, 2009, 2010) as
shown in Figure 1. Due to the varying word or-
der in the MT hypotheses, it is essential to define
the backbone which determines the general word
order of the CN. Instead of using a single system
output as the skeleton, we employ an MBR de-
coder to select the best single system output Er
from the merged N -best list by minimizing the
BLEU (Papineni et al, 2002) loss, as in (3):
r = argmin
i
Ns?
j=1
(1? BLEU(Ej , Ei)) (3)
where Ns indicates the number of translations in
the merged N -best list, and {Ei}Nsi=1 are the trans-
lations themselves. In our task, we only merge the
1-best output of each individual system.
The CN is built by aligning other hypotheses
against the backbone, based on the TER metric.
Null words are allowed in the alignment. Ei-
ther votes or different confidence measures are as-
signed to each word in the network. Each arc in
the CN represents an alternative word at that po-
sition in the sentence and the number of votes for
each word is counted when constructing the net-
work. The features we used are as follows:
? word posterior probability (Fiscus, 1997);
? 3, 4-gram target language model;
? word length penalty;
? Null word length penalty;
We use MERT (Och, 2003) to tune the weights
of the CN.
2.8 Rescoring
Rescoring is a very important part in post-
processing which can select a better hypothesis
from the N -best list. We augmented our previ-
ous rescoring model (Du et al, 2009) with more
large-scale data. The features we used include:
? Direct and inverse IBM model;
? 3, 4-gram target language model;
? 3, 4, 5-gram PoS language model (Schmid,
1994; Ratnaparkhi, 1996);
? Sentence length posterior probability (Zens
and Ney, 2006);
? N -gram posterior probabilities within the N -
Best list (Zens and Ney, 2006);
? Minimum Bayes Risk probability;
? Length ratio between source and target sen-
tence;
The weights are optimized via MERT.
3 Experimental Setup
This section describes our experimental setup for
the en?cs and en?es translation tasks.
3.1 Data
Bilingual data: In the experiments we used data
sets provided by the workshop organizers. For the
en?cs translation table extraction we employed
both parallel corpora (News-Commentary10 and
CzEng 0.9), and for the en?es experiments, we
used the Europarl(Koehn, 2005), News Commen-
tary and United Nations parallel data. We used a
maximum sentence length of 80 for en?es and
40 for en?cs. Detailed statistics are shown in Ta-
ble 1.
Corpus Langs. Sent. Source
tokens
Target
tokens
Europarl en?es 1.6M 43M 45M
News-comm en?es 97k 2.4M 2.7M
UN en?es 5.9M 160M 190M
News-Comm en?cs 85k 1.8M 1.6M
CzEng en?cs 7.8M 80M 69M
Table 1: Statistics of en?cs and en?es parallel data.
Monolingual data: For language modeling pur-
poses, in addition to the target parts of the bilin-
gual data, we used the monolingual News corpus
for cs; and the Gigaword corpus for es. For both
languages, we used the SRILM toolkit (Stolcke,
2002) to train a 5-gram language model using all
monolingual data provided. However, for en?es
we used the IRSTLM toolkit (Federico and Cet-
tolo, 2007) to train a 5-gram language model using
the es Gigaword corpus. Both language models
use modified Kneser-Ney smoothing (Chen and
Goodman, 1996). Statistics for the monolingual
corpora are given in Table 2.
Corpus Language Sentences Tokens
E/N/NC/UN es 9,6M 290M
Gigaword es 40M 1,2G
News cs 13M 210M
Table 2: Statistics of Monolingual Data. E/N/NC/UN
refers to Europarl/News/News Commentary/United Nations
corpora.
For all the systems except Apertium, we first
lowercase and tokenize all the monolingual and
bilingual data using the tools provided by the
WMT10 organizers. After translation, system
combination output is detokenised and true-cased.145
3.2 English?Czech (en?cs) Experiments
The CzEng corpus (Bojar and Z?abokrtsky?, 2009)
is a collection of parallel texts from sources of dif-
ferent quality and as such it contains some noise.
As the first step, we discarded those sentence pairs
having more than 10% of non-Latin characters.
The CzEng corpus is quite large (8M sen-
tence pairs). Although we were able to build
a vanilla SMT system on all parallel data avail-
able (News-Commentary + CzEng), we also at-
tempted to build additional systems using News-
Commentary data (which we considered in-
domain) and various in-domain subsets of CzEng
hoping to achieve better results on domain-
specific data.
For our first system, we selected 128,218 sen-
tence pairs from CzEng labeled as news. For the
other two systems, we selected subsets of 2M and
4M sentence pairs identified as most similar to
the development sets (as a sample of in-domain
data) based on cosine similarity of their represen-
tation in a TF-IDF weighted vector space model
(cf. Byrne et al (2003)). We also applied the
pseudo-relevavance-feedback technique for query
expansion (Manning et al, 2008) to select another
subset with 2M sentence pairs.
We used the output of 15 systems for sys-
tem combination for the en?cs translation task.
Among these, 5 systems were built using Moses
and varying the size of the training data (DCU-
All, DCU-Ex2M, DCU-4M, DCU-2M and DCU-
News); 9 context-informed PB-SMT systems
(DCU-SSCI-*) using (combinations of) various
context features (word, PoS, supertags and depen-
dency relations) trained only on the News Com-
mentary data (marked with ? in Table 4); and one
system using the moses-chart decoder, also
trained on the news commentary data.
3.3 English?Spanish (en?es) Experiments
Three baseline systems using Moses were built,
where we varied the amount of training data used:
? epn: This system uses all of the Europarl and
News-Commentary parallel data.
? UN-half: This system uses the data suplied
to ?epn?, plus an additional 2.1M sentences
pairs randomly selected from the United Na-
tions corpus.
? all: This system uses all of the available par-
allel data.
For en?es we also obtained output from the
factored model (trained only on the news com-
mentary corpus) and the Apertium RBMT sys-
tem. We also derived phrase alignments using the
MaTrEx EBMT system (Stroppa and Way, 2006),
and added those phrase translations in the Moses
phrase table. The systems marked with ? use a
language model built using the Spanish Gigaword
corpus, in addition to the one built using the pro-
vided monolingual data. These 6 sets of system
outputs are then used for system combination.
3.4 Experimental Results
The evaluation results for en?es and en?cs ex-
periments are shown in Table 3 and Table 4 re-
spectively. The output of the systems marked ?
were submitted in the shared tasks.
System BLEU NIST METEOR TER
DCU-half ?? 29.77% 7.68 59.86% 59.55%
DCU-all ?? 29.63% 7.66 59.82% 59.74%
DCU-epn ?? 29.45% 7.66 59.71% 59.64%
DCU-ebmt ?? 29.38% 7.62 59.59% 60.11%
DCU-factor 22.58% 6.56 54.94% 67.65%
DCU-apertium 19.22% 6.37 49.68% 67.68%
DCU-system-
combination ? 30.42% 7.78 60.56% 58.71%
Table 3: en?es experimental results.
System BLEU NIST METEOR TER
DCU-All 10.91% 4.60 39.18% 81.76%
DCU-Ex2M 10.63% 4.56 39.12% 81.96%
DCU-4M 10.61% 4.56 39.26% 82.04%
DCU-2M 10.48% 4.58 39.35% 81.56%
DCU-Chart 9.34% 4.25 37.04% 83.87%
DCU-News 8.64% 4.16 36.27% 84.96%
DCU-SSCI-ccg? 8.26% 4.02 34.76% 85.58%
DCU-SSCI-
supertag-pair? 8.11% 3.95 34.93% 86.63%
DCU-SSCI-
ccg-ltag? 8.09% 3.96 34.90% 86.62%
DCU-SSCI-PR? 8.06% 4.00 34.89% 85.99%
DCU-SSCI-base? 8.05% 3.97 34.61% 86.02%
DCU-SSCI-PRIR? 8.03% 3.99 34.81% 85.98%
DCU-SSCI-ltag? 8.00% 3.95 34.57% 86.41%
DCU-SSCI-PoS? 7.91% 3.94 34.57% 86.51%
DCU-SSCI-word? 7.57% 3.88 34.16% 87.14%
DCU-system-
combination ? 13.22% 4.98 40.39% 78.59%
Table 4: en?cs experimental results.
4 Conclusion
This paper presents the Dublin City University
MT system in WMT2010 shared task campaign.
This was DCU?s first attempt to translate from en
to es and cs in any shared task. We developed a
multi-engine framework which combined the out-
puts of several individual MT systems and gener-
ated a new N -best list after CN decoding. Then by146
using some global features, the rescoring model
generated the final translation output. The experi-
mental results demonstrated that the combination
module and rescoring module are effective in our
framework for both language pairs, and produce
statistically significant improvements as measured
by bootstrap resampling methods (Koehn, 2004)
on BLEU over the single best system.
Acknowledgements: This work is supported
by Science Foundation Ireland (Grant No.
07/CE/I1142) and by PANACEA, a 7th Frame-
work Research Programme of the European
Union, contract number 7FP-ITC-248064. M.L.
Forcada?s sabbatical stay at Dublin City Univer-
sity is supported by Science Foundation Ireland
through ETS Walton Award 07/W.1/I1802 and by
the Universitat d?Alacant (Spain).
References
Bojar, O. and Z?abokrtsky?, Z. (2009). CzEng0.9:
Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics,
92:63?83.
Byrne, W., Khudanpur, S., Kim, W., Kumar, S.,
Pecina, P., Virga, P., Xu, P., and Yarowsky, D.
(2003). The Johns Hopkins University 2003
Chinese?English machine translation system.
In Proceedings of MT Summit IX, pages 447?
450, New Orleans, LA.
Chen, S. F. and Goodman, J. (1996). An Empir-
ical Study of Smoothing Techniques for Lan-
guage Modeling. In Proc. 34th Ann. Meeting of
the Association for Computational Linguistics,
pages 310?318, San Francisco, CA.
Chiang, D. (2007). Hierarchical phrase-
based translation. Computational Linguistics,
33(2):201?228.
Daelemans, W. and van den Bosch, A. (2005).
Memory-Based Language Processing (Studies
in Natural Language Processing). Cambridge
University Press, New York, NY.
Du, J., He, Y., Penkale, S., and Way, A. (2009).
MaTrEx: The DCU MT System for WMT2009.
In Proc. 3rd Workshop on Statistical Machine
Translation, EACL 2009, pages 95?99, Athens,
Greece.
Du, J., Pecina, P., and Way, A. (2010). An
Augmented Three-Pass System Combination
Framework: DCU Combination System for
WMT 2010. In Proc. ACL 2010 Joint Workshop
in Statistical Machine Translation and Metrics
Matr, Uppsala, Greece.
Federico, M. and Cettolo, M. (2007). Efficient
Handling of N-gram Language Models for Sta-
tistical Machine Translation. In Proceedings
of the Second Workshop on Statistical Machine
Translation, pages 88?95, Prague, Czech Re-
public.
Fiscus, J. G. (1997). A post-processing sys-
tem to yield reduced word error rates: Recog-
nizer output voting error reduction (ROVER).
In Proceedings 1997 IEEE Workshop on Auto-
matic Speech Recognition and Understanding
(ASRU), pages 347?352, Santa Barbara, CA.
Forcada, M. L., Tyers, F. M., and Ram??rez-
Sa?nchez, G. (2009). The free/open-source ma-
chine translation platform Apertium: Five years
on. In Proceedings of the First International
Workshop on Free/Open-Source Rule-Based
Machine Translation FreeRBMT?09, pages 3?
10.
Gough, N. and Way, A. (2004). Robust Large-
Scale EBMT with Marker-Based Segmenta-
tion. In Proceedings of the 10th International
Conference on Theoretical and Methodological
Issues in Machine Translation (TMI-04), pages
95?104, Baltimore, MD.
Haque, R., Naskar, S. K., Bosch, A. v. d., and
Way, A. (2009a). Dependency relations as
source context in phrase-based smt. In Proc.
23rd Pacific Asia Conference on Language, In-
formation and Computation, pages 170?179,
Hong Kong, China.
Haque, R., Naskar, S. K., Ma, Y., and Way, A.
(2009b). Using supertags as source language
context in SMT. In EAMT-2009: Proceed-
ings of the 13th Annual Conference of the Eu-
ropean Association for Machine Translation,
pages 234?241, Barcelona, Spain.
Koehn, P. (2004). Statistical significance tests for
machine translation evaluation. In Proceedings
of EMNLP, volume 4, pages 388?395.
Koehn, P. (2005). Europarl: A Parallel Corpus
for Statistical Machine Translation. In Machine
Translation Summit X, pages 79?86, Phuket,
Thailand.
Koehn, P. and Hoang, H. (2007). Factored Trans-
lation Models. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural147
Language Learning (EMNLP-CoNLL), pages
868?876, Prague, Czech Republic.
Koehn, P., Hoang, H., Birch, A., Callison-Burch,
C., Federico, M., Bertoldi, N., Cowan, B.,
Shen, W., Moran, C., Zens, R., Dyer, C., Bo-
jar, O., Constantin, A., and Herbst, E. (2007).
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In Annual Meeting of the As-
sociation for Computational Linguistics (ACL),
demonstration session, pages 177?180, Prague,
Czech Republic.
Kumar, S. and Byrne, W. (2004). Minimum
Bayes-Risk Decoding for Statistical Machine
Translation. In Proceedings of the Joint Meet-
ing of the Human Language Technology Con-
ference and the North American Chapter of
the Association for Computational Linguistics
(HLT-NAACL 2004), pages 169?176, Boston,
MA.
Mangu, L., Brill, E., and Stolcke, A. (2000). Find-
ing consensus in speech recognition: Word er-
ror minimization and other applications of con-
fusion networks. Computer Speech and Lan-
guage, 14(4):373?400.
Manning, C. D., Raghavan, P., and Schu?tze, H.
(2008). Introduction to Information Retrieval.
Cambridge University Press.
Och, F. (2003). Minimum error rate training
in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL),
pages 160?167, Sapporo, Japan.
Och, F. and Ney, H. (2002). Discriminative train-
ing and maximum entropy models for statistical
machine translation. In Proceedings of ACL,
volume 2, pages 295?302.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL-02), pages
311?318, Philadelphia, PA.
Ratnaparkhi, A. (1996). A Maximum Entropy
Model for Part-Of-Speech Tagging. In Pro-
ceedings of the Empirical Methods in Natural
Language Processing Conference (EMNLP),
pages 133?142, Philadelphia, PA.
Rosti, A.-V. I., Xiang, B., Matsoukas, S.,
Schwartz, R., Ayan, N. F., and Dorr, B. J.
(2007). Combining outputs from multiple ma-
chine translation systems. In Proceedings of the
Joint Meeting of the Human Language Technol-
ogy Conference and the North American Chap-
ter of the Association for Computational Lin-
guistics (HLT-NAACL 2007), pages 228?235,
Rochester, NY.
Schmid, H. (1994). Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings
of International Conference on New Methods
in Language Processing, pages 44?49, Manch-
ester, UK.
Snover, M., Dorr, B., Schwartz, R., Micciula, L.,
and Makhoul, J. (2006). A study of transla-
tion edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Amer-
icas (AMTA 2006), pages 223?231, Cambridge,
MA.
Stolcke, A. (2002). SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of
the International Conference Spoken Language
Processing, pages 901?904, Denver, CO.
Stroppa, N., van den Bosch, A., and Way, A.
(2007). Exploiting Source Similarity for SMT
using Context-Informed Features. In Proceed-
ings of the 11th International Conference on
Theoretical and Methodological Issues in Ma-
chine Translation (TMI-07), pages 231?240,
Sko?vde, Sweden.
Stroppa, N. and Way, A. (2006). MaTrEx: the
DCU machine translation system for IWSLT
2006. In Proceedings of the International Work-
shop on Spoken Language Translation, pages
31?36, Kyoto, Japan.
Zens, R. and Ney, H. (2006). N-gram Poste-
rior Probabilities for Statistical Machine Trans-
lation. In Proceedings of the Joint Meeting of
the Human Language Technology Conference
and the North American Chapter of the As-
sociation for Computational Linguistics (HLT-
NAACL 2006), pages 72?77, New York, NY.
148
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 215?220,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
DCU-Lingo24 Participation in WMT 2014 Hindi-English Translation task
Xiaofeng Wu, Rejwanul Haque*, Tsuyoshi Okita
Piyush Arora, Andy Way, Qun Liu
CNGL, Centre for Global Intelligent Content
School of Computing, Dublin City University
Dublin 9, Ireland
{xf.wu,tokita,parora,away,qliu}@computing.dcu.ie
*Lingo24, Edinburgh, UK
rejwanul.haque@lingo24.com
Abstract
This paper describes the DCU-Lingo24
submission to WMT 2014 for the Hindi-
English translation task. We exploit
miscellaneous methods in our system,
including: Context-Informed PB-SMT,
OOV Word Conversion (OWC), Multi-
Alignment Combination (MAC), Oper-
ation Sequence Model (OSM), Stem-
ming Align and Normal Phrase Extraction
(SANPE), and Language Model Interpola-
tion (LMI). We also describe various pre-
processing steps we tried for Hindi in this
task.
1 Introduction
This paper describes the DCU-Lingo24 submis-
sion to WMT 2014 for the Hindi-English transla-
tion task.
All our experiments on WMT 2014 are built
upon the Moses phrase-based model (PB-SMT)
(Koehn et al., 2007) and tuned with MERT
(Och, 2003). Starting from this baseline system,
we exploit various methods including Context-
Informed PB-SMT (CIPBSMT), zero-shot learn-
ing (Palatucci et al., 2009) using neural network-
based language modelling (Bengio et al., 2000;
Mikolov et al., 2013) for OOV word conversion,
various lexical reordering models (Axelrod et al.,
2005; Galley and Manning, 2008), various Mul-
tiple Alignment Combination (MAC) (Tu et al.,
2012), Operation Sequence Model (OSM) (Dur-
rani et al., 2011) and Language Model Interpola-
tion(LMI).
In the next section, the preprocessing steps are
explained. In Section 3 a detailed explanation of
the technique we exploit is provided. Then in Sec-
tion 4, we provide our experimental results and re-
sultant discussion.
2 Pre-processing Steps
We use all the training data provided for Hindi?
English translation. Following Bojar et al. (2010),
we apply a number of normalisation methods on
the Hindi corpus. The HindEnCorp parallel cor-
pus compiles several sources of parallel data. We
observe that the source-side (Hindi) of the TIDES
data source contains font-related noise, i.e. many
Hindi sentences are a mixture of two different en-
codings: UTF-8
1
and WX
2
notations. We pre-
pared a WX-to-UTF-8 font conversion script for
Hindi which converts all WX encoded characters
into UTF-8, thus removing all WX encoding ap-
pearing in the TIDES data.
We also observe that a portion of the English
training corpus contained the following bracket-
like sequences of characters: -LRB-, -LSB-, -
LCB-, -RRB-, -RSB-, and -RCB-.
3
For consis-
tency, those character sequences in the training
data were replaced by the corresponding brackets.
For English ? both monolingual and the target
side of the bilingual data ? we perform tokeniza-
tion, normalization of punctuation, and truecasing.
For parallel training data, we filter sentences pairs
containing more than 80 tokens on either side and
1
http://en.wikipedia.org/wiki/UTF-8
2
http://en.wikipedia.org/wiki/WX_notation
3
The acronyms stand for (Left|Right)
(Round|Square|Curly) Bracket.
215
sentence pairs with length difference larger than 3
times.
3 Techniques Deployed
3.1 Combination of Various Lexical
Reordering Model (LRM)
Clearly, Hindi and English have quite different
word orders, so we adopt three lexical reordering
models to address this problem. They are word-
based LRM and phrase-based LRM, which mainly
focus on local reordering phenomena, and hierar-
chical phrase-based LRM, which mainly focuses
on longer distance reordering (Galley and Man-
ning, 2008).
3.2 Operation Sequence Model
The Operation Sequence Model (OSM) of Dur-
rani et al. (2011) defines four translation opera-
tions: Generate(X,Y), Continue Source Concept,
Generate Source Only (X) and Generate Identical,
as well as three reordering operations: Insert Gap,
Jump Back(W) and Jump Forward.
The probability of an operation sequence O =
(o
1
o
2
? ? ? o
J
) is calculated as in (1):
p(O) =
J
?
j=1
p(o
j
|o
j?n+1
? ? ? o
j?1
) (1)
where n indicates the number of previous opera-
tions used.
We employ a 9-order OSM in our framework.
3.3 Language Model Interpolation (LMI)
We build a large language model by including data
from the English Gigaword fifth edition, the En-
glish side of the UN corpus, the English side of the
10
9
French?English corpus and the English side of
the Hindi?English parallel data provided by the or-
ganisers. We interpolate language models trained
using each dataset, with the monolingual data pro-
vided split into three parts (news 2007-2013, Eu-
roparl (?) and news commentary) and the weights
tuned to minimize perplexity on the target side of
the devset.
The language models in our systems are trained
with SRILM (Stolcke, 2002). We train a 5-gram
model with Kneser-Ney discounting (Chen and
Goodman, 1996).
3.4 Context-informed PB-SMT
Haque et al. (2011) express a context-dependent
phrase translation as a multi-class classification
problem, where a source phrase with given addi-
tional context information is classified into a dis-
tribution over possible target phrases. The size of
this distribution needs to be limited, and would
ideally omit irrelevant target phrase translations
that the standard PB-SMT (Koehn et al., 2003) ap-
proach would normally include. Following Haque
et al. (2011), we derive a context-informed feature
?
h
mbl
that is expressed as the conditional probabil-
ity of the target phrase e?
k
given the source phrase
?
f
k
and its context information (CI), as in (2):
?
h
mbl
= log P(e?
k
|
?
f
k
,CI(
?
f
k
)) (2)
Here, CI may include any feature that can pro-
vide useful information to disambiguate the given
source phrase. In our experiment, we use CCG su-
pertag (Steedman, 2000) as a contextual features.
CCG supertag expresses the specific syntactic be-
haviour of a word in terms of the arguments it
takes, and more generally the syntactic environ-
ment in which it appears.
We consider the CCG supertags of the context
words, as well as of the focus phrase itself. In our
model, the supertag of a multi-word focus phrase
is the concatenation of the supertags of the words
composing that phrase. We generate a window
of size 2l + 1 features (we set l:=2), including
the concatenated complex supertag of the focus
phrase. Accordingly, the supertag-based contex-
tual information (CI
st
) is described as in (3):
CI
st
(
?
f
k
) = {st(f
i
k
?l
), ..., st(f
i
k
?1
), st(
?
f
k
),
st(f
j
k
+1
), ..., st(f
j
k
+l
)}
(3)
For the Hindi-to-English translation task, we use
part-of-speech (PoS) tags
4
of the source phrase
and the neighbouring words as the contextual fea-
ture, owing to the fact that supertaggers are readily
available only for English.
We use a memory-based machine learning
(MBL) classifier (TRIBL: (Daelemans, 2005))
5
that is able to estimate P(e?
k
|
?
f
k
,CI(
?
f
k
)) by
similarity-based reasoning over memorized
nearest-neighbour examples of source?target
phrase translations. Thus, we derive the feature
?
h
mbl
defined in Equation (2). In addition to
?
h
mbl
,
4
In order to obtain PoS tags of Hindi words,
we used the LTRC shallow parser for Hindi from
http://ltrc.iiit.ac.in/analyzer/hindi/shallow-parser-hin-
4.0.fc8.tar.gz.
5
An implementation of TRIBL is freely available as part
of the TiMBL software package, which can be downloaded
from http://ilk.uvt.nl/timbl.
216
we derive a simple two-valued feature
?
h
best
,
defined in Equation (4):
?
h
best
=
{
1 if e?
k
maximizes P(e?
k
|
?
f
k
,CI(
?
f
k
))
u 0 otherwise
(4)
where
?
h
best
is set to 1 when e?
k
is one of the tar-
get phrases with highest probability according to
P(e?
k
|
?
f
k
,CI(
?
f
k
)) for each source phrase
?
f
k
; oth-
erwise
?
h
best
is set to 0.000001. We performed ex-
periments by integrating these two features
?
h
mbl
and
?
h
best
directly into the log-linear model of
Moses. Their weights are optimized using mini-
mum error-rate training (MERT)(Och, 2003) on a
held-out development set for each of the experi-
ments.
3.5 Morphological Segmentation
Haque et al. (2012) applied a morphological suffix
separation process in a Bengali-to-English trans-
lation task and showed that suffix separation sig-
nificantly reduces data sparseness in the Bengali
corpus. They also showed an SMT model trained
on the suffix-stripped training data significantly
outperforms the state-of-the-art PB-SMT baseline.
Like Bengali, Hindi is a morphologically very rich
and highly inflected Indian language. As done
previously for Bengali-to-English (Haque et al.,
2012), we employ a suffix-stripping method for
lemmatizing inflected Hindi words in the WMT
Hindi-to-English translation task. Following Das-
gupta and Ng (2006), we developed an unsu-
pervised morphological segmentation method for
Hindi. We also used a Hindi lightweight stem-
mer (Ramanathan and Rao, 2003) in order to pre-
pare a training corpus with only Hindi stems. We
prepared Hindi-to-English SMT systems on the
both types of training data (i.e. suffix-stripped and
stemmed).
6
3.6 Multi-Alignment Combination (MAC)
Word alignment is a critical component of MT
systems. Various methods for word alignment
have been proposed, and different models can pro-
duce signicantly different outputs. For example,
Tu et al. (2012) demonstrates that the alignment
agreement between the two best-known alignment
tools, namely Giza++(Och and Ney, 2003) and
6
Suffixes were separated and completely removed from
the training data.
the Berkeley aligner
7
(Liang et al., 2006), is be-
low 70%. Taking into consideration the small size
of the the corpus, in order to extract more ef-
fective phrase tables, we concatenate three align-
ments: Giza++ with grow-diag-final-and, Giza++
with intersection, and that derived from the Berke-
ley aligner.
3.7 Stemming Alignment and Normal Phrase
Extraction (SANPE)
The rich morphology of Hindi will cause word
alignment sparsity, which results in poor align-
ment quality. Furthermore, word stemming on
the Hindi side usually results in too many English
words being aligned to one stemmed Hindi word,
i.e. we encounter the problem of phrase over-
extraction. Therefore, we conduct word alignment
with the stemmed version of Hindi, and then at
the phrase extraction step, we replace the stemmed
form with the original Hindi form.
3.8 OOV Word Conversion Method
Our algorithm for OOV word conversion uses the
recently developed zero-shot learning (Palatucci
et al., 2009) using neural network language mod-
elling (Bengio et al., 2000; Mikolov et al., 2013).
The same technique is used in (Okita et al., 2014).
This method requires neither parallel nor compa-
rable corpora, but rather two monolingual corpora.
In our context, we prepare two monolingual cor-
pora on both sides, which are neither parallel nor
comparable, and a small amount of already known
correspondences between words on the source and
target sides (henceforth, we refer to this as the
?dictionary?). Then, we train both sides with the
neural network language model, and use a contin-
uous space representation to project words to each
other on the basis of a small amount of correspon-
dences in the dictionary. The following algorithm
shows the steps involved:
1. Prepare the monolingual source and target
sentences.
2. Prepare the dictionary which consists of U
entries of source and target sentences com-
prising non-stop-words.
3. Train the neural network language model on
the source side and obtain the real vectors of
X dimensions for each word.
7
http://code.google.com/p/berkeleyaligner/
217
4. Train the neural network language model on
the target side and obtain the real vectors of
X dimensions for each word.
5. Using the real vectors obtained in the above
steps, obtain the linear mapping between the
dictionary items in two continuous spaces us-
ing canonical component analysis (CCA).
In our experiments we use U the same as the en-
tries of Wiki corpus, which is provided among
WMT14 corpora, and X as 50. The resulted pro-
jection by this algorithm can be used as the OOV
word conversion which projects from the source
language which among OOV words into the tar-
get language. The overall algorithm which uses
the projection which we build in the above step is
shown in the following.
1. Collect unknown words in the translation out-
puts.
2. Do Hindi named-entity recognition (NER) to
detect noun phrases.
3. If they are noun phrases, do the projection
from each unknown word in the source side
into the target words (We use the projection
prepared in the above steps). If they are not
noun phrases, run the transliteration to con-
vert each of them.
We perform Hindi NER by training CRF++ (Kudo
et al., 2004) using the Hindi named entity corpus,
and use the Hindi shallow parser (Begum et al.,
2008) for preprocessing of the inputs.
4 Results and Discussion
4.1 Data
We conduct our experiments on the standard
datasets released in the WMT14 shared translation
task. We use HindEnCorp
8
(Bojar et al., 2014)
parallel corpus for MT system building. We also
used the CommonCrawl Hindi monolingual cor-
pus (Bojar et al., 2014) in order to build an addi-
tional language model for Hindi.
For the Hindi-to-English direction, we also em-
ployed monolingual English data used in the other
translation tasks for building the English language
model.
8
http://ufallab.ms.mff.cuni.cz/ bojar/hindencorp/
4.2 Moses Baseline
We employ a standard Moses PB-SMT model as
our baseline. The Hindi side is preprocessed but
unstemmed. We use Giza++ to perform word
alignment, the phrase table is extracted via the
grow-diag-final-and heuristic and the max-phrase-
length is set to 7.
4.3 Automatic Evaluation
Experiments BLEU
Moses Baseline 8.7
Context-Based 9.4
Context-Based + CommonCrawl LM 11.4
Table 1: BLEU scores of the English-to-Hindi MT
Systems on the WMT test set.
Experiments BLEU
Moses Baseline 10.1
Context-Based 10.1
Suffix-Stripped 10.0
OWC 11.2
OSM 10.3
Three LRMs 10.5
MAC 10.7
SANPE 10.6
LMI 10.9
LMI+SANPE+MAC+ThreeLRMs+OSM 11.7
Table 2: BLEU scores of the Hindi-to-English MT
Systems on the WMT test set.
We prepared a number of MT systems for both
English-to-Hindi and Hindi-to-English, and sub-
mitted their runs in the WMT 2014 Evaluation
Matrix. The BLEU scores of the different English-
to-Hindi MT systems (Moses Baseline, Context-
Based (CCG) MT system, and Context-Based
(CCG) MT system with an additional LM built
on the CommonCrawl Hindi monolingual corpus
(Bojar et al., 2014)) on the WMT 2014 English-
to-Hindi test set are reported in Table 1. As can
be seen from Table 1, Context-Based (CCG) MT
system produces 0.7 BLEU points improvement
(8.04% relative) over the Moses Baseline. When
we add an additional large LM built on the Com-
monCrawl data to the Context-Based (CCG) MT
system, we achieved a 2 BLEU-point improve-
ment (21.3% relative) (cf. last row in Table 1) over
218
the Context-Based (CCG) MT system.
9
The BLEU scores of the different Hindi-to-
English MT systems on the WMT 2014 Hindi-
to-English test set are reported in Table 2. The
first row of Table 2 shows the BLEU score for
the Baseline MT system. We note that the per-
formance of the Context-Based (PoS) MT system
obtains identical performance to the Moses base-
line (10.1 BLEU points) on the WMT 2014 Hindi-
to-English test set.
We employed a source language (Hindi) nor-
malisation technique, namely suffix separation,
but unfortunately this did not bring about any
improvement for the Hindi-to-English translation
task. The improvement gained by individually
employing OSM, three lexical reordering mod-
els, Multi-alignment Combination, Stem-align and
normal Phrase Extraction and Language Model In-
terpolation can be seen in Table 2. Our best sys-
tem is achieved by combining OSM, Three LMR,
MAC, SANPE and LMI, which results in a 1.6
BLEU point improvement over the Baseline.
5 Acknowledgments
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of
the CNGL Centre for Global Intelligent Content
(www.cngl.ie) at Dublin City University.
References
Amittai Axelrod, Ra Birch Mayne, Chris Callison-
burch, Miles Osborne, and David Talbot. 2005. Ed-
inburgh system description for the 2005 iwslt speech
translation evaluation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation
(IWSLT).
Rafiya Begum, Samar Husain, Arun Dhwaj,
Dipti Misra Sharma, Lakshmi Bai, and Rajeev
Sangal. 2008. Dependency annotation scheme for
indian languages. In Proceedings of The Third In-
ternational Joint Conference on Natural Language
Processing (IJCNLP).
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2000. A neural probabilistic language model. In
Proceedings of Neural Information Systems.
Ond Bojar, Pavel Stranak, and Daniel Zeman. 2010.
Data issues in english-to-hindi machine translation.
In LREC.
9
Please note that this is an unconstrained submission.
Ondrej Bojar, V. Diatka, Rychly P., Pavel Stranak,
A. Tamchyna, and Daniel Zeman. 2014. Hindi-
english and hindi-only corpus for machine transla-
tion. In LREC.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics,
ACL ?96, pages 310?318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Walter Daelemans. 2005. Memory-based language
processing. Cambridge University Press.
Sajib Dasgupta and Vincent Ng. 2006. Unsupervised
morphological parsing of bengali. Language Re-
sources and Evaluation, 40(3-4):311?330.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ?11, pages 1045?1054, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848?856, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Rejwanul Haque, Sudip Kumar Naskar, Antal van den
Bosch, and Andy Way. 2011. Integrating source-
language context into phrase-based statistical ma-
chine translation. Machine translation, 25(3):239?
285.
Rejwanul Haque, Sergio Penkale, Jie Jiang, and Andy
Way. 2012. Source-side suffix stripping for bengali-
to-english smt. In Asian Language Processing
(IALP), 2012 International Conference on, pages
193?196. IEEE.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
219
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Appliying conditional random fields to
japanese morphological analysis. In Proceedings of
EMNLP.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 104?
111. Association for Computational Linguistics.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013.
Exploiting similarities among languages for ma-
chine translation. ArXiv.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Tsuyoshi Okita, Ali Hosseinzadeh Vahid, Andy Way,
and Qun Liu. 2014. Dcu terminology translation
system for medical query subtask at wmt14.
Mark Palatucci, Dean Pomerleau, Geoffrey Hinton,
and Tom Mitchell. 2009. Zero-shot learning with
semantic output codes. In Neural Information Pro-
cessing Systems (NIPS), December.
Ananthakrishnan Ramanathan and Durgesh D Rao.
2003. A lightweight stemmer for hindi. In the Pro-
ceedings of EACL.
Mark Steedman. 2000. The syntactic process, vol-
ume 35. MIT Press.
Andreas Stolcke. 2002. Srilm ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference Spoken Language Processing,
pages 901?904, Denver, CO.
Zhaopeng Tu, Yang Liu, Yifan He, Josef van Genabith,
Qun Liu, and Shouxun Lin. 2012. Combining mul-
tiple alignments to improve machine translation. In
COLING (Posters), pages 1249?1260.
220
Proceedings of the 4th International Workshop on Computational Terminology, pages 42?51,
Dublin, Ireland, August 23 2014.
Bilingual Termbank Creation via Log-Likelihood Comparison and
Phrase-Based Statistical Machine Translation
Rejwanul Haque, Sergio Penkale, Andy Way
?
Lingo24, Edinburgh, UK
{rejwanul.haque, sergio.penkale}@lingo24.com
?
CNGL, Centre for Global Intelligent Content
School of Computing, Dublin City University
Dublin 9, Ireland
away@computing.dcu.ie
Abstract
Bilingual termbanks are important for many natural language processing (NLP) applications, es-
pecially in translation workflows in industrial settings. In this paper, we apply a log-likelihood
comparison method to extract monolingual terminology from the source and target sides of a
parallel corpus. Then, using a Phrase-Based Statistical Machine Translation model, we create a
bilingual terminology with the extracted monolingual term lists. We manually evaluate our novel
terminology extraction model on English-to-Spanish and English-to-Hindi data sets, and observe
excellent performance for all domains. Furthermore, we report the performance of our monolin-
gual terminology extraction model comparing with a number of the state-of-the-art terminology
extraction models on the English-to-Hindi datasets.
1 Introduction
Terminology plays an important role in various NLP tasks including Machine Translation (MT) and
Information Retrieval. It is also exploited in human translation workflows, where it plays a key role
in ensuring translation consistency and reducing ambiguity across large translation projects involving
multiple files and translators over a long period of time. The creation of monolingual and bilingual
terminological resources using human experts are, however, expensive and time-consuming tasks. In
contrast, automatic terminology extraction is much faster and less expensive, but cannot be guaranteed
to be error-free. Accordingly, in real NLP applications, a manual inspection is required to amend or
discard anomalous items from an automatically extracted terminology list.
The automatic terminology extraction task starts with selecting candidate terms from the input domain
corpus, usually in two different ways: (i) linguistic processors are used to identify noun phrases that are
regarded as candidate terms (Kupiec, 1993; Frantzi et al., 2000), and (ii) non-linguistic n-gram word
sequences are regarded as candidate terms (Deane, 2005).
Various statistical measures have been used to rank candidate terms, such as C-Value (Ananiadou et
al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood comparison (Rayson and Garside, 2000), and
TF-IDF (Basili et al., 2001). In this paper, we present our bilingual terminology extraction model, which
is composed of two consecutive and independent processes:
1. A log-likelihood comparison method is employed to rank candidate terms (n-gram word sequences)
independently from the source and target sides of a parallel corpus,
2. The extracted source terms are aligned to one or more extracted target terms using a Phrase-Based
Statistical Machine Translation (PB-SMT) model (Koehn et al., 2003).
We then evaluate our novel bilingual terminology extraction model on various domain corpora consid-
ering English-to-Spanish and low-resourced and less-explored English-to-Hindi language-pairs and see
excellent performance for all data sets.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
42
The remainder of the paper is organized as follows. In Section 2, we discuss related work. In Section
3, we describe our two-stage terminology extraction model. Section 4 presents the results and analyses
of our experiments, while Section 5 concludes, and provides avenues for further work.
2 Related Work
Several algorithms have been proposed to extract terminology from a domain-specific corpus, which can
be divided into three broad categories: linguistic, statistical and hybrid. Statistical or hybrid approaches
dominate this field, with some of the leading work including the use of frequency-based filtering (Daille
et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood and mutual information (Rayson and Gar-
side, 2000; Pantel and Lin, 2001), TF-IDF (Basili et al., 2001; Kim et al., 2009), weirdness algorithm
(Ahmad et al., 1999), Glossex (Kozakov et al., 2004) and Termex (Sclano and Velardi, 2007).
In this work, we focus on extracting bilingual terminology from a parallel corpus. He et al. (2006)
demonstrate that using log-likelihood for term discovery performs better than TF-IDF. Accordingly, sim-
ilarly to Rayson and Garside (2000) and Gelbukh et al. (2010), we extract terms independently from both
sides of a parallel corpus using log-likelihood comparisons with a generic reference corpus. Some of the
most influential research on bilingual terminology extraction includes Kupiec (1993), Gaussier (1998),
Ha et al. (2008) and Lefever et al. (2009). Lefever et al. (2009) proposed a sub-sentential alignment-
based terminology extraction module that links linguistically motivated phrases in parallel texts. Unlike
our approach, theirs relies on linguistic analysis tools such as PoS taggers or lemmatizers, which might
be unavailable for under-resourced languages (e.g., Hindi). Gaussier (1998) and Ha et al. (2008) applied
statistical approaches to acquire parallel term-pairs directly from a sentence-aligned corpus, with the lat-
ter focusing on improving monolingual term extraction, rather than on obtaining a bilingual term list. In
contrast, we build a PB-SMT model (Koehn et al., 2003) from the input parallel corpus, which we use
to align a source term to one or more target terms. While Rayson and Garside (2000) and Gelbukh et al.
(2010) only allowed the extraction of single-word terms, we focus on extraction of up to 3-gram terms.
3 Methodology
In this section, we describe our two-stage bilingual terminology extraction model. In the first stage, we
extract monolingual terms independently from either side of a sentence-aligned domain-specific parallel
corpus. In the second stage, the extracted source terms are aligned to one or more extracted target terms
using a PB-SMT model.
3.1 Monolingual Terminology Extraction
The monolingual term extraction task involves the identification of terms from a list of candidate terms
formed from all n-gram word sequences from the monolingual domain corpus (i.e. in our case, each side
of the domain parallel corpus, cf. Section 4.1). On both source and target sides, we used lists of language-
specific stop-words and punctuation marks in order to filter out anomalous items from the candidate
termlists. In order to rank the candidate terms in those lists, we used a log-likelihood comparison method
that compares the frequencies of each candidate term in both the domain corpus and the large general
corpus used as a reference.
1
The log-likelihood (LL) value of a candidate term (C
n
) is calculated using equation (1) from Gelbukh
et al. (2010).
LL = 2 ? ((F
d
? log(F
d
/E
d
)) + (F
g
? log(F
g
/E
g
))) (1)
where F
d
and F
g
are the frequencies of C
n
in the domain corpus and the generic reference corpus,
respectively. E
d
and E
g
are the expected frequencies of C
n
, which are calculated using (2) and (3).
E
d
= N
n
d
? (F
d
+ F
g
)/(N
n
d
+ N
n
g
) (2)
E
g
= N
n
g
? (F
d
+ F
g
)/(N
n
d
+ N
n
g
) (3)
1
Before the term-extraction process begins, we apply a number of preprocessing methods including tokenisation to the input
domain corpus and the generic reference corpus.
43
where N
n
d
and N
n
g
are the numbers of n-grams in the domain corpus and reference corpus, respectively.
Thus, each candidate term is associated with a weight (LL value) which is used to sort the candidate
terms: those candidates with the highest weights have the most significant differences in frequency in the
two corpora. However, we are interested in those candidate terms that are likely to be terms in the domain
corpus. Gelbukh et al. (2010) used the condition in (4) in order to filter out those candidate terms whose
relative frequencies are bigger in the domain corpus than in the reference corpus, and we do likewise.
F
d
/N
n
d
> F
g
/N
n
g
(4)
In contrast with Gelbukh et al. (2010), we extract multi-word terms up to 3-grams, whereas they focused
solely on extracting single word terms.
3.2 Creating a Bilingual Termbank
We obtained source and target termlists from the bilingual domain corpus using the approach described
in Section 3.1. We use a PB-SMT model (Koehn et al., 2003) to create a bilingual termbank from the
extracted source and target termlists.
This section provides a mathematical derivation of the PB-SMT model to show how we scored can-
didate term-pairs using the PB-SMT model. We built a source-to-target PB-SMT model from the bilin-
gual domain corpus using the Moses toolkit (Koehn et al., 2007). In PB-SMT, the posterior probability
P(e
I
1
|f
J
1
) is directly modelled as a (log-linear) combination of features (Och and Ney, 2002), that usually
comprise M translational features, and the language model, as in (5):
log P(e
I
1
|f
J
1
) =
M
?
m=1
?
m
h
m
(f
J
1
, e
I
1
, s
K
1
) + ?
LM
log P(e
I
1
) (5)
where e
I
1
= e
1
, ..., e
I
is the probable candidate translation for the given input sentence f
J
1
= f
1
, ..., f
J
and s
K
1
= s
1
, ..., s
k
denotes a segmentation of the source and target sentences respectively into the se-
quences of phrases (
?
f
1
, ...,
?
f
k
) and (e?
1
, ..., e?
k
) such that (we set i
0
:= 0):
?k ? [1,K] s
k
:= (i
k
; b
k
, j
k
), (b
k
corresponds to starting index of f
k
)
e?
k
:= e?
i
k?1
+1
, ..., e?
i
k
,
?
f
k
:=
?
f
b
k
, ...,
?
f
j
k
Each feature h
m
in (5) can be rewritten as in (6):
h
m
(f
J
1
, e
I
1
, s
K
1
) =
K
?
k=1
?
h
m
(
?
f
k
, e?
k
, s
k
) (6)
Therefore, the translational features in (5) can be rewritten as in (7):
M
?
m=1
?
m
h
m
(f
J
1
, e
I
1
, s
K
1
) =
M
?
m=1
?
m
K
?
k=1
?
h
m
(
?
f
k
, e?
k
, s
k
) (7)
In equation (7),
?
h
m
is a feature defined on phrase-pairs (
?
f
k
, e?
k
), and ?
m
is the feature weight of
?
h
m
.
These weights (?
m
) are optimized using minimum error-rate training (MERT) (Och, 2003) on a held-out
500 sentence-pair development set for each of the experiments.
We create a list of probable source?target term-pairs by taking each source and target term from the
source and target termlists, respectively, provided that those source?target term-pairs are present in the
PB-SMT phrase-table. We calculate a weight (w) for each source?target term-pair (essentially, a phrase-
pair, i.e. (e?
k
,
?
f
k
)) using (8):
2
w(e?
k
,
?
f
k
) =
M
?
m=1
?
m
?
h
m
(
?
f
k
, e?
k
) (8)
2
Equation (8) is derived from the right-hand side of equation (7) for a single source?target phrase-pair.
44
In order to calculate w, we used the four standard PB-SMT translational features (
?
h
m
), namely forward
phrase translation log-probability (log P(e?
k
|
?
f
k
)), its inverse (log P(
?
f
k
|e?
k
)), the lexical log-probability
(log P
lex
(e?
k
|
?
f
k
)), and its inverse (log P
lex
(
?
f
k
|e?
k
)). We considered a higher threshold value for weights
and considered those term-pairs whose weights exceeded this threshold. For each source term, we con-
sidered a maximum of the four highest-weighted target terms.
Domain Parallel Corpus
Domain Sentences Words (English)
English-to-Spanish
Banking, Finance and Economics 50,112 548,594
Engineering 91,896 1,165,384
IT 33,148 367,046
Tourism and Travel 50,042 723,088
Science 79,858 1,910,482
Arts and Culture 9,124 100,620
English-to-Hindi
EILMT 7,096 173,770
EMILLE 9,907 159,024
Launchpad 67,663 380,546
KDE4 84,089 324,289
Reference Corpus
Language Sentences Words
English 4,000,000 82,048,154
Spanish 4,132,386 128,005,190
Hindi 10,000,000 182,066,982
Table 1: Corpus Statistics.
4 Experiments and Discussion
4.1 Data Used
We conducted experiments on several data domains for two different language-pairs, English-to-Spanish
and English-to-Hindi. For English-to-Spanish, we worked with client-provided data taken from six dif-
ferent domains in the form of translation memories. For English-to-Hindi, we used three parallel corpora
from three different sources (EILMT, EMILLE and Launchpad) taken from HindEnCorp
3
(Bojar et al.,
2014) released for the WMT14 shared translation task,
4
and a parallel corpus of KDE4 localization files
5
(Tiedemann, 2009). The EMILLE corpus contains leaflets from the UK Government and various local
authorities. The domain of the EILMT
6
corpus is tourism.
We used data from a collection of translated documents from the United Nations (MultiUN)
7
(Tiede-
mann, 2009) and the European Parliament (Koehn et al., 2005) as the monolingual English and Spanish
reference corpora. We used the HindEnCorp monolingual corpus (Bojar et al., 2014) as the monolingual
Hindi reference corpus. The statistics of the data used in our experiments are shown in Table 1.
4.2 Runtime Performance
Our terminology extraction model is composed of two main processes: (i) Moses training and tuning
(restricting the number of iterations of MERT to a maximum of 6), and (ii) terminology extraction. In
Table 2, we report the actual runtimes of these two processes on the six domain corpora. As Table
3
http://ufallab.ms.mff.cuni.cz/ bojar/hindencorp/
4
http://www.statmt.org/wmt14/
5
http://opus.lingfil.uu.se/KDE4.php
6
English-to-Indian Language Machine Translation (EILMT) is a Ministry of IT, Govt. of India sponsored project.
7
http://opus.lingfil.uu.se/MultiUN.php
45
2 demonstrates, both MT system-building (training and tuning combined) and terminology extraction
processes are very short on each corpus. Given the crucial influence of bilingual terminology on quality
in translation workflows, we believe that the creation of such assets from scratch in less than 30 minutes
may prove to be a significant breakthrough for translators.
MT System Terminology
Building Extraction
English-to-Spanish
Banking, Finance and Economics 05:49 04:23
Engineering 06:47 04:33
IT 04:10 04:31
Tourism and Travel 05:34 04:24
Science 15:26 04:52
Arts and Culture 03:20 04:16
English-to-Hindi
EILMT 12:41 15:47
EMILLE 05:41 17.18
Launchpad 04:37 24.11
KDE4 04:05 16:50
Table 2: Runtimes (minutes:seconds) for MT system-building and bilingual terminology extraction on
the different domain data sets.
4.3 Human Evaluation
Of course, it is one thing to rapidly create translation assets such as bilingual termbanks, and another en-
tirely to ensure the quality of such resources. Accordingly, we evaluated the performance of our bilingual
terminology extraction model on each English-to-Spanish and English-to-Hindi domain corpus reported
in Table 1, with the evaluation goals being twofold: (i) measuring the accuracy of the monolingual ter-
minology extraction process, and (ii) measuring the accuracy of our novel bilingual terminology creation
model.
As mentioned in Section 3.2, a source term may be aligned with up to four target terms. For evaluation
purposes, we considered the top-100 source terms based on the LL values (cf. (1)) and their target coun-
terparts (i.e. one to four target terms). The quality of the extracted terms was judged by native Spanish
and Hindi speakers, both with excellent English skills, and the evaluation results are reported in Table
3. Note that we were not able to measure recall of the term extraction model on the domain corpora due
to the unavailability of a reference terminology set. The evaluator counted the number of valid terms in
the source term list for the domain in question, and the percentage of valid terms with respect to the total
number of terms (i.e. 100) is reported in the second column in Table 3. We refer to this as VST (Valid
Source Terms). For each valid source term there are one to four target terms that are ranked according to
the weights in (8). In theory, therefore, the top-ranked target term is the most suitable target translation of
the aligned source term. The evaluator counted the number of instances where the top-ranked target term
was a suitable target translation of the source term; the percentage with respect to the number of valid
source terms is shown in the third column in Table 3, and denoted as VTT (Valid Target Terms). The
evaluator also reported the number of cases where any of the four target terms was a suitable translation
of the source term; the percentage with respect to the number of valid source terms is given in the fourth
column in Table 3. Furthermore, the evaluator counted the number of instances where any of the four
target terms with minor editing can be regarded as suitable target translation; the percentage with respect
to the number of valid source terms is reported in the last column of Table 3. In Table 4, we show three
English?Spanish term-pairs extracted by our automatic term extractor where the target terms (Spanish)
are slightly incorrect. In all these examples the edit distance between the correct term and the one pro-
posed by our automatic extraction method is quite low, meaning that just a few keystrokes can transform
46
the candidate term into the correct one. In these cases editing the candidate term is much cheaper (in
terms of time) than creating the translations from scratch.
VST VTT1 VTT4 VTTME4
(%) (%) (%) (%)
English-to-Spanish
Banking, Finance and Economics 76 92.1 93.4 94.7
Engineering 84 90.5 91.7 94.1
IT 89 90.0 97.8 97.8
Tourism and Travel 72 86.1 93.1 93.1
Science 94 93.6 93.6 93.6
Arts and Culture 89 91.9 96.5 96.5
English-to-Hindi
EILMT 91 81.3 83.5 96.7
EMILLE 79 62.1 83.5 98.7
Launchpad 88 95.4 98.8 98.8
KDE4 79 88.6 89.8 94.9
Table 3: Manual evaluation results obtained on the top-100 term pairs. VST: Valid Source Terms, VTT1:
Valid Target Terms (1-best), VTT4: Valid Target Terms (4-best), VTTME4: Valid Target Terms with
Minor Editing (4-best).
Source Terms Target Terms Target Terms Edit
(using Bilingual Term Extractor) corrected with Minor Editing Distance
Shutter Obturaci?on Obturador 5
comment: wrong choice of inflection is likely caused by the term being most frequently used as
?shutter speed?
Lenses Objetivos EF Objetivos 3
comment: The qualifier ?EF? should not be present in the target, as it is not in the source
Leave Cancel Cancelaci?on Vacaciones Cancelaci?on de Vacaciones 3
comment: The preposition ?de? is missing in the target term
Table 4: Slightly wrong target terms corrected with minor editing.
In Table 3, we see that the accuracy of the monolingual term extraction model varies from 72% to 94%
for both English-to-Spanish and English-to-Hindi. For English-to-Spanish, the accuracy of our bilingual
terminology creation model ranges from 86.1% to 93.6%, 91.7% to 97.8% and 93.1% to 97.8% when
the 1-best, 4-best and 4-best with slightly edited target terms are considered, respectively. For English-
to-Hindi, the accuracy of our bilingual terminology creation model ranges from 62.1% to 95.4%, 83.5%
to 98.8% and 94.9% to 98.8% when the 1-best, 4-best and 4-best with slightly edited target terms are
considered, respectively.
We are greatly encouraged by these results, as they demonstrate that our novel bilingual termbank
creation method is robust in the face of the somewhat noisy monolingual term-extraction results; as a
consequence, if better methods for suggesting monolingual term candidates are proposed, we expect the
performance of our bilingual term-creation model to improve accordingly.
We calculated the distributions of unigram, bigram and trigram in the valid source terms (cf. Table 3)
and reported in Table 5. We also calculated the percentages of their distributions in the valid source terms
averaged over all 10 data sets. As can be seen from Table 3, the percentage of the average distribution of
the trigram terms is quite low (i.e. 2.5%). This result justifies our decision for extraction of up to 3-gram
terms.
47
Unigram Bigram Trigram
English-to-Spanish
Banking, Finance and Economics 55 20 1
Engineering 64 18 2
IT 75 12 2
Tourism and Travel 49 18 5
Science 91 3 0
Arts and Culture 76 10 3
English-to-Hindi
EILMT 73 17 1
EMILLE 35 37 7
Launchpad 85 3 0
KDE4 74 5 0
Average 80.4% 17.0 % 2.5%
Table 5: Distributions of unigram, bigram and trigram in the valid source term pairs (cf. second column
in Table 3).
4.4 Comparison: Monolingual Terminology Extraction
In this section we report the performance of our monolingual terminology extraction model (cf. Section
3.1) comparing with the performance of several state-of-the-art terminology extraction algorithms capa-
ble of recognising multiword terms. In order to extract monolingual multiword terms we used the JATE
toolkit
8
(Zhang et al., 2008). This toolkit first extracts candidate terms from a corpus using linguistic
tools and then applies term extraction algorithms to recognise terms specific to the domain corpus. The
JATE toolkit is currently available only for the English language. For evaluation purposes, we considered
the source-side of the English-to-Hindi domain corpora.
Algorithm Reference EILMT EMILLE Launchpad KDE4
LLC (Bilingual) cf. VST in Table 3 91 79 88 79
LLC 77 53 80 71
STF 46 04 54 44
ACTF 42 15 62 48
TF-IDF 50 36 45 17
Glossex Kozakov et al. (2004) 76 43 76 71
JK Justeson & Katz (1995) 42 13 58 42
NC-Value Frantzi et al. (2000) 46 34 52 25
RIDF Church & Gale (1995) 27 16 23 21
TermEx Sclano et al. (2007) 42 08 46 41
C-Value Ananiadou (1994) 49 44 62 40
Weirdness Ahmed et al. (1999) 77 57 82 63
Table 6: Monolingual evaluation results. LLC: Log-Likelihood Comparison, STF: Simple Term Fre-
quency, ACTF: Average Corpus Term Frequency, JK: Justeson Katz
For comparison, we considered the top-100 source terms based on the log-likelihood values (cf. (1)).
The automatic term extraction algorithms in JATE assign weights (domain representativeness) to the
candidate terms giving an indication of the likelihood of being a good domain-specific term. The quality
of the extracted terms (top-100 highest weighted) was judged by an evaluator with excellent English
skills, and the evaluation results are reported in Table 6. The evaluator counted the number of valid terms
8
https://code.google.com/p/jatetoolkit/
48
in the highest weighted 100 terms that were extracted using different state-of-the-art term extraction
algorithms.
The third row of Table 6 represents the percentage of the valid source terms extracted by our log-
likelihood comparison (LLC) based monolingual term extraction algorithm. The next three rows rep-
resent three basic monolingual term extraction algorithms (STF: simple term frequency, ACTF: aver-
age corpus term frequency and TF-IDF) available in the JATE toolkit. The last seven rows represent
seven state-of-the-art terminology extraction algorithms. As can be seen from Table 6, LLC is the best-
performing algorithm with the Weirdness (Ahmad et al., 1999) and the Glossex (Kozakov et al., 2004)
algorithms on the EILMT and the KDE4 corpora, respectively. The LLC is also the second-best per-
forming algorithm on the EMILLE and the Lauchpad corpora.
We see in Table 6 that the percentage of valid source terms is quite low on the EMILLE corpus.
This might be caused by it containing information leaflets in a variety of domains (consumer, education,
housing, health, legal, social), which might bring down the percentage of valid source terms on this
corpus.
Note that the percentage of valid source terms (VST) reported in Table 3 is calculated taking the
top-100 source terms from the bilingual term-pair list that were extracted using the method described in
Section 3.2. For comparison purposes we again report this percentage (VST in Table 3) in the second row
in Table 6. Our bilingual term extraction method discards any anomalous pairs from the initial candidate
term-pair list (cf. Section 3.2). This essentially removes some of the source entries that are not pertinent
to the domain. As a result, the percentage of the valid source terms extracted applying our bilingual
terminology extraction method (Table 3) is higher than the percentage of the valid source terms extracted
applying our monolingual terminology extraction algorithm (LLC) (Table 6). We clearly see from Tables
3 and 6 that this bilingual approach to term extraction not only achieves remarkable performance on the
bilingual task, but that when used in a monolingual context it outperforms most state-of-the-art extraction
algorithms, and is comparable with the best ones. We should also note that JATE?s implementation of
these algorithms (including Weirdness) uses language-dependent modules such as a lemmatizer, unlike
our implementation of LLC which is language-independent.
5 Conclusions and Future Work
In this paper we presented a bilingual multi-word terminology extraction model based on two inde-
pendent consecutive processes. Firstly, we employed a log-likelihood comparison method to extract
source and target terms independently from both sides of a parallel domain corpus. Secondly, we used
a PB-SMT model to align source terms to one or more target terms. The manual evaluation results
on ten different domain corpora of two syntactically divergent language-pairs showed the accuracy of
our bilingual terminology extraction model to be very high, especially in the light of the rather noisier
monolingual candidate terms presented to it. Given the reported high levels of performance ? minimum
levels of 93.1% and 94.9% in the 4-best set-up across all six domains for English-to-Spanish and all four
domains for English-to-Hindi, respectively ? we are convinced that the extracted bilingual multiword
termbanks are useful ?as is?, and with a small amount of post-processing from domain experts would be
completely error-free.
The proposed bilingual terminology extraction model has been tested on a highly investigated
language-pair, English-to-Spanish, and a less-explored and low-resourced English-to-Indic language-
pair, English-to-Hindi. Interestingly, the performance of the bilingual terminology extraction model
is excellent for the both language-pairs. We also tested several state-of-the-art monolingual terminol-
ogy extraction algorithms including our own (log-likelihood comparison) on the source-side of the four
English-to-Hindi domain data sets. According to the manual evaluation results, our monolingual multi-
word term extraction model proves to be the best-performing algorithm on two domain data sets and the
second best-performing algorithm on the remaining two domain data sets. Our monolingual multiword
terminology extraction method is clearly comparable to the state-of-the-art monolingual terminology
extraction algorithms.
In this work, we considered all n-gram word sequences from the domain corpus as candidate terms.
49
In future work, we would like to incorporate the candidate phrasal term identification model of Deane
(2005), which would omit irrelevant multiword units, and help us extend our evaluation beyond the top-
100 terms. We also plan to demonstrate the impact of the created termbanks on translator productivity in
a number of workflows ? different language pairs, domains, and levels of post-editing ? in an industrial
setting.
Acknowledgements
This work was partially supported by the Science Foundation Ireland (Grant 07/CE/I1142) as part of
CNGL at Dublin City University, and by Grant 610879 for the Falcon project funded by the European
Commission.
References
S. Ananiadou. 1994. A methodology for automatic term recognition. In COLING: 15th International Conference
on Computational Linguistics, pages 1034?1038.
K. Ahmad, L. Gillam and L. Tostevin. 1999. University of Surrey Participation in TREC8: Weirdness Indexing for
Logical Document Extrapolation and Retrieval (WILDER). In the Eighth Text REtrieval Conference (TREC-8).
National Institute of Standards and Technology, Gaithersburg, MD., pp.717?724.
R. Basili, A. Moschitti, M. Pazienza and F. Zanzotto. 2001. A contrastive approach to term extraction. In Pro-
ceedings of the 4th Conference on Terminology and Artificial Intelligence (TIA 2001). Nancy, France, 10pp.
K. Church and W. Gale. 1995. Inverse Document Frequency (IDF): A Measure of Deviation from Poisson. In
Proceedings of the 3rd Workshop on Very Large Corpora, pages 121?130. Cambridge, MA.
B. Daille, E. Gaussier and J-M. Lang?e. 1994. Towards automatic extraction of monolingual and bilingual termi-
nology. In COLING 94, The 15th International Conference on Computational Linguistics, Proceedings. Kyoto,
Japan, pp.515?521.
P. Deane. 2007. A nonparametric method for extraction of candidate phrasal terms. In ACL-05: 43rd Annual
Meeting of the Association for Computational Linguistics. Ann Arbor, Michigan, USA, pp.605?613.
K. Frantzi, S. Ananiadou and H. Mima. 2000. Automatic Recognition of Multi-word Terms: the C-value/NC-value
Method. International Journal of Digital Libraries. 3(2): 115?130.
E. Gaussier. 1998. Flow Network Models for Word Alignment and Terminology Extraction from Bilingual Cor-
pora. In COLING-ACL ?98, 36th Annual Meeting of the Association for Computational Linguistics and 17th
International Conference on Computational Linguistics, Proceedings of the Conference, Volume II. Montreal,
Quebec, Canada, pp.444?450.
A. Gelbukh, G. Sidorov, E. Lavin-Villa and L. Chanona-Hernandez. 2010. Automatic Term Extraction Using
Log-Likelihood Based Comparison with General Reference Corpus. In 15th International Conference on Ap-
plications of Natural Language to Information Systems, NLDB 2010, Proceedings. LNCS vol. 6177. Berlin:
Springer. pp.248?255.
L. Ha, G. Fernandez, R. Mitkov and G. Corpas. 2008. Mutual bilingual terminology extraction. In LREC 2008:
6th Language Resources and Evaluation Conference. Marrakech, Morocco, pp.1818?1824.
T. He, T., X. Zhang and Y. Xinghuo. 2006. An Approach to Automatically Constructing Domain Ontology. In
Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation, PACLIC 2006.
Wuhan, China, pp.150?157.
J. S. Justeson, and S. M. Katz. 1995. Technical terminology: some linguistic properties and an algorithm for
identification in text. Natural language engineering, 1(1) 9?27.
S. Kim, T. Baldwin and M-Y. Kan. 2009. An Unsupervised Approach to Domain-Specific Term Extraction. In
Proceedings of the Australasian Language Technology Association Workshop 2009. Sydney, Australia, pp.94?
98.
P. Koehn. 2005. Europarl: a parallel corpus for statistical machine translation. In MT Summit X: The Tenth
Machine Translation Summit. Phuket, Thailand, pp.79?86.
50
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin and E. Herbst. 2007. Moses: Open Source Toolkit for Statistical
Machine Translation. In ACL 2007, Proceedings of the Interactive Poster and Demonstration Sessions. Prague,
Czech Republic, pp.177?180.
P. Koehn, F. Och and H. Ney. 2003. Statistical Phrase-Based Translation. In HLT-NAACL 2003: conference
combining Human Language Technology conference series and the North American Chapter of the Association
for Computational Linguistics conference series. Edmonton, Canada, pp. 48?54.
L. Kozakov, Y. Park, T. H. Fin, Y. Drissi, Y. N. Doganata, and T. Cofino. 2004. Glossary extraction and knowledge
in large organisations via semantic web technologies. In Proceedings of the 6th International Semantic Web
Conference and the 2nd Asian Semantic Web Conference.
J. Kupiec. 1993. An algorithm for finding noun phrase correspondences in bilingual corpora. In 31st Annual
Meeting of the Association for Computational Linguistics, Proceedings of the Conference. Columbus, Ohio,
USA, pp.17?22.
E. Lefever, L. Macken and V. Hoste. 2009. Language-Independent Bilingual Terminology Extraction from a
Multilingual Parallel Corpus. In EACL ?09 Proceedings of the 12th Conference of the European Chapter of the
Association for Computational Linguistics. Athens, Greece, pp.496?504.
F. Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In 41st Annual Meeting of the
Association for Computational Linguistics, Proceedings of the Conference. Sapporo, Japan, pp.160?167.
F. Och and H. Ney. 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Transla-
tion. In 40th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference.
Philadelphia, PA, USA, pp.295?302.
O. Bojar, V. Diatka, P. Rychl?y, P. Stra?n?ak, A. Tamchyna, and D. Zeman. 2014. Hindi-English and Hindi-only
Corpus for Machine Translation. In Proceedings of the Ninth International Language Resources and Evaluation
Conference (LREC?14). Reykjavik, Iceland.
P. Pantel and D. Lin. 2001. A Statistical Corpus-Based Term Extractor. In E. Stroulia and S. Matwin (eds.)
Advances in Artificial Intelligence, 14th Biennial Conference of the Canadian Society for Computational Studies
of Intelligence, AI 2001, Ottawa, Canada, Proceedings. LNCS vol. 2056. Berlin: Springer, pp.36?46.
P. Rayson and R. Garside. 2000. Comparing corpora using frequency profiling. In Proceedings of the Workshop
on Comparing Corpora, held in conjunction with the 38th Annual Meeting of the Association for Computational
Linguistics (ACL 2000). Hong Kong, pp.1?6.
F. Sclano and P. Velardi. 2007. TermExtractor: a web application to learn the shared terminology of emergent web
communities. In Proceedings of the 3rd International Conference on Interoperability for Enterprise Software
and Applications (I-ESA 2007). Funchal, Madeira Island, Portugal, pp.287?290.
J. Tiedemann. 2009. News from OPUS - A Collection of Multilingual Parallel Corpora with Tools and Interfaces.
In N. Nicolov and K. Bontcheva and G. Angelova and R. Mitkov (eds.) Recent Advances in Natural Language
Processing (vol V), pages 237?248, John Benjamins, Amsterdam/Philadelphia.
Z. Zhang, J. Iria, C. Brewster and F. Ciravegna. 2008. A Comparative Evaluation of Term Recognition Algorithms.
In Proceedings of The sixth international conference on Language Resources and Evaluation, (LREC 2008),
pages 2108?2113, Marrakech, Morocco.
51
