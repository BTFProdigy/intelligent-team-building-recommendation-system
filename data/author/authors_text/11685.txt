Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 101?102,
Suntec, Singapore, 6 August 2009.
c?2009 ACL and AFNLP
UDel: Generating Referring Expressions
Guided by Psycholinguistic Findings
Charles Greenbacker and Kathleen McCoy
Dept. of Computer and Information Sciences
University of Delaware
Newark, Delaware, USA
[charlieg|mccoy]@cis.udel.edu
Abstract
We present an approach to generating refer-
ring expressions in context utilizing feature se-
lection informed by psycholinguistic research.
Features suggested by studies on pronoun in-
terpretation were used to train a classifier sys-
tem which determined the most appropriate
selection from a list of possible references.
This application demonstrates one way to help
bridge the gap between computational and
empirical means of reference generation.
1 Introduction
This paper provides a system report on our submis-
sion for the GREC-MSR (Main Subject References)
Task, one of the two shared task competitions for
Generation Challenges 2009. The objective is to se-
lect the most appropriate reference to the main sub-
ject entity from a given list of alternatives. The cor-
pus consists of introductory sections from approxi-
mately 2,000 Wikipedia articles in which references
to the main subject have been annotated (Belz and
Varges, 2007). The training set contains articles
from the categories of cities, countries, mountains,
people, and rivers. The overall purpose is to develop
guidelines for natural language generation systems
to determine what forms of referential expressions
are most appropriate in a particular context.
2 Method
The first step of our approach was to perform a lit-
erature survey of psycholinguistic research related
to the production of referring expressions by human
beings. Our intuition was that findings in this field
could be used to develop a useful set of features
with which to train a classifier system to perform the
GREC-MSR task. Several common factors govern-
ing the interpretation of pronouns were identified by
multiple authors (Arnold, 1998; Gordon and Hen-
drick, 1998). These included Subjecthood, Paral-
lelism, Recency, and Ambiguity. Following (McCoy
and Strube, 1999), we selected Recency as our start-
ing point and tracked the intervals between refer-
ences measured in sentences. Referring expressions
which were separated from the most recent reference
by more than two sentences were marked as long-
distance references. To cover the Subjecthood and
Parallelism factors, we extracted the syntactic cate-
gory of the current and three most recent references
directly from the GREC data. This information also
helped us determine if the entity was the subject of
the sentence at hand, as well as the two previous
sentences. Additionally, we tracked whether the en-
tity was in subject position of the sentence where
the previous reference appeared. Finally, we made
a simple attempt at recognizing potential interfering
antecedents (Siddharthan and Copestake, 2004) oc-
curring in the current sentence and the text since that
last reference.
Observing the performance of prototyping sys-
tems led us to include boolean features indicat-
ing whether the reference immediately followed the
words ?and,? ?but,? or ?then,? or if it appeared be-
tween a comma and the word ?and.? We also found
that non-annotated instances of the entity?s name,
which actually serve as references to the name itself
rather than to the entity, factor into Recency. Fig-
ure 1 provides an example of such a ?non-referential
instance.? We added a feature to measure distance
to these items, similar to the distance between refer-
ences. Sentence and reference counters rounded out
101
the full set of features.
The municipality was abolished in 1928, and the
name ?Mexico City? can now refer to two things.
Figure 1: Example of non-referential instance. In this
sentence, ?Mexico City? is not a reference to the main en-
tity (Mexico City), but rather to the name ?Mexico City.?
3 System Description
A series of C5.0 decision trees (RuleQuest Research
Pty Ltd, 2008) were trained to determine the most
appropriate reference type for each instance in the
training set. Each tree used a slightly different sub-
set of features. It was determined that one decision
tree in particular performed the best on mountain
and person articles, and another tree on the remain-
ing categories. Both of these trees were incorporated
into the submitted system.
Our system first performed some preprocessing
for sentence segmentation and identified any non-
referential instances as described in Section 2. Next,
it marshalled all of the relevant data for the feature
set. These data points were used to represent the
context of the referring expression and were sent to
the decision trees to determine the most appropriate
reference type. Once the type had been selected, the
list of alternative referring expressions were scanned
using a few simple rules. For the first instance of a
name in an article, the longest non-emphatic name
was chosen. For subsequent instances, the shortest
non-emphatic name was selected. For the other 3
types, the first matching option in the list was used,
backing off to a pronoun or name if the preferred
type was not available.
4 Results
The performance of our system, as tested on the de-
velopment set and scored by the GREC evaluation
software, is offered in Table 1.
5 Conclusions
We?ve shown that psycholinguistic research can be
helpful in determining feature selection for gener-
ating referring expressions. We suspect the perfor-
mance of our system could be improved by employ-
Table 1: Scores from GREC evaluation software.
Component Score Value
total pairs 656
reg08 type matches 461
reg08 type accuracy 0.702743902439024
reg08 type precision 0.702743902439024
reg08 type recall 0.702743902439024
string matches 417
string accuracy 0.635670731707317
mean edit distance 0.955792682926829
mean normalised edit distance 0.338262195121951
BLEU 1 score 0.6245
BLEU 2 score 0.6103
BLEU 3 score 0.6218
BLEU 4 score 0.6048
ing more sophisticated means of sentence segmen-
tation and named entity recognition for identifying
interfering antecedents.
References
Jennifer E. Arnold. 1998. Reference Form and Discourse
Patterns. Doctoral dissertation, Department of Lin-
guistics, Stanford University, June.
Anja Belz and Sabastian Varges. 2007. Generation of re-
peated references to discourse entities. In Proceedings
of the 11th European Workshop on NLG, pages 9?16,
Schloss Dagstuhl, Germany.
Peter C. Gordon and Randall Hendrick. 1998. The rep-
resentation and processing of coreference in discourse.
Cognitive Science, 22(4):389?424.
Kathleen F. McCoy and Michael Strube. 1999. Gener-
ating anaphoric expressions: Pronoun or definite de-
scription. In Proceedings of Workshop on The Rela-
tion of Discourse/Dialogue Structure and Reference,
Held in Conjunction with the 38th Annual Meeting,
pages 63 ? 71, College Park, Maryland. Association
for Computational Linguistics.
RuleQuest Research Pty Ltd. 2008. Data mining
tools See5 and C5.0. http://www.rulequest.com/see5-
info.html.
Advaith Siddharthan and Ann Copestake. 2004. Gener-
ating referring expressions in open domains. In Pro-
ceedings of the 42th Meeting of the Association for
Computational Linguistics Annual Conference, pages
408?415, Barcelona, Spain.
102
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 105?106,
Suntec, Singapore, 6 August 2009.
c?2009 ACL and AFNLP
UDel: Extending Reference Generation to Multiple Entities
Charles Greenbacker and Kathleen McCoy
Dept. of Computer and Information Sciences
University of Delaware
Newark, Delaware, USA
[charlieg|mccoy]@cis.udel.edu
Abstract
We report on an attempt to extend a reference
generation system, originally designed only
for main subjects, to generate references for
multiple entities in a single document. This
endeavor yielded three separate systems: one
utilizing the original classifier, another with a
retrained classifier, and a third taking advan-
tage of new data to improve the identification
of interfering antecedents. Each subsequent
system improved upon the results of the pre-
vious iteration.
1 Introduction
This paper provides a system report on our submis-
sion for the GREC-NEG (Named Entity Generation)
Task, one of the two shared task competitions for
Generation Challenges 2009. The objective is to se-
lect the most appropriate reference to named entities
from a given list of alternatives. The corpus consists
of introductory sections from approximately 1,000
Wikipedia articles in which single and plural refer-
ences to all people mentioned in the text have been
annotated (Belz and Varges, 2007). The training set
contains articles from the categories of Chefs, Com-
posers, and Inventors. GREC-NEG differs from the
other challenge task, GREC-MSR (Main Subject
References), in that systems must now account for
multiple entities rather than a single main subject,
and the corpus includes only articles about persons
rather than a variety of topics.
2 System Description
Our GREC-NEG systems build upon our work for
the GREC-MSR task. Our original approach was
to consult findings in psycholinguistic research for
guidance regarding appropriate feature selection for
the production of referring expressions. We relied
upon several common factors recognized by multi-
ple authors (Arnold, 1998; Gordon and Hendrick,
1998), including Subjecthood, Parallelism, Recency,
and Ambiguity. We followed (McCoy and Strube,
1999) who stressed the importance of Recency in
reference generation. Finally, we made a prelimi-
nary attempt at identifying potential interfering an-
tecedents that could affect the Ambiguity of pro-
nouns (Siddharthan and Copestake, 2004).
As an initial attempt (UDel-NEG-1), we simply
extended our GREC-MSR submission. By adapt-
ing our system to account for multiple entities and
the slightly different data format, we were able to
use the existing classifier to generate references for
GREC-NEG. We suspected that accuracy could be
improved by retraining the classifier, so our next sys-
tem (UDel-NEG-2) added entity and mention num-
bers as features to train on. Presumably, this could
help distinguish between the main subject and sec-
ondary entities, as well as plural references. As
all named entities are tagged in the GREC-NEG
corpus, we leveraged this information to improve
our recognition of other antecedents interfering with
pronoun usage in a third new system (UDel-NEG-
3). As in our GREC-MSR submission, all three of
our GREC-NEG systems trained C5.0 decision trees
(RuleQuest Research Pty Ltd, 2008) on our set of
features informed by psycholinguistic research.
3 Results
System performance, as tested on the development
set and scored by the GREC evaluation software,
105
is offered in Tables 1, 2, and 3. Type accuracy
for UDel-NEG-1 remained close to our GREC-MSR
submission, and error rate was reduced by over 20%
for UDel-NEG-2 and UDel-NEG-3. However, string
accuracy was very low across all three systems, as
compared to GREC-MSR results.
Table 1: GREC scores for UDel-NEG-1 (unmodified).
Component Score Value
total pairs 907
reg08 type matches 628
reg08 type accuracy 0.69239250275634
reg08 type precision 0.688699360341151
reg08 type recall 0.688699360341151
string matches 286
string accuracy 0.315325248070562
mean edit distance 1.55126791620728
mean normalised edit dist. 0.657521668367265
BLEU 1 score 0.4609
BLEU 2 score 0.5779
BLEU 3 score 0.6331
BLEU 4 score 0.6678
Table 2: GREC scores for UDel-NEG-2 (retrained).
Component Score Value
total pairs 907
reg08 type matches 692
reg08 type accuracy 0.762954796030871
reg08 type precision 0.749466950959488
reg08 type recall 0.749466950959488
string matches 293
string accuracy 0.323042998897464
mean edit distance 1.4773980154355
mean normalised edit dist. 0.64564100951858
BLEU 1 score 0.4747
BLEU 2 score 0.6085
BLEU 3 score 0.6631
BLEU 4 score 0.6917
4 Conclusions
The original classifier performed well when ex-
tended to multiple entities, and showed marked im-
provement when retrained to take advantage of new
Table 3: GREC scores for UDel-NEG-3 (interference).
Component Score Value
total pairs 907
reg08 type matches 694
reg08 type accuracy 0.7651598676957
reg08 type precision 0.752665245202559
reg08 type recall 0.752665245202559
string matches 302
string accuracy 0.332965821389195
mean edit distance 1.46306504961411
mean normalised edit dist. 0.636499985162561
BLEU 1 score 0.4821
BLEU 2 score 0.6113
BLEU 3 score 0.6614
BLEU 4 score 0.6874
data. All three systems yielded poor scores for string
accuracy as compared to GREC-MSR results, sug-
gesting an area for improvement.
References
Jennifer E. Arnold. 1998. Reference Form and Discourse
Patterns. Doctoral dissertation, Department of Lin-
guistics, Stanford University, June.
Anja Belz and Sabastian Varges. 2007. Generation of re-
peated references to discourse entities. In Proceedings
of the 11th European Workshop on NLG, pages 9?16,
Schloss Dagstuhl, Germany.
Peter C. Gordon and Randall Hendrick. 1998. The rep-
resentation and processing of coreference in discourse.
Cognitive Science, 22(4):389?424.
Kathleen F. McCoy and Michael Strube. 1999. Gener-
ating anaphoric expressions: Pronoun or definite de-
scription. In Proceedings of Workshop on The Rela-
tion of Discourse/Dialogue Structure and Reference,
Held in Conjunction with the 38th Annual Meeting,
pages 63 ? 71, College Park, Maryland. Association
for Computational Linguistics.
RuleQuest Research Pty Ltd. 2008. Data mining
tools See5 and C5.0. http://www.rulequest.com/see5-
info.html.
Advaith Siddharthan and Ann Copestake. 2004. Gener-
ating referring expressions in open domains. In Pro-
ceedings of the 42th Meeting of the Association for
Computational Linguistics Annual Conference, pages
408?415, Barcelona, Spain.
106
Proceedings of the ACL-HLT 2011 Student Session, pages 75?80,
Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational Linguistics
Towards a Framework for
Abstractive Summarization of Multimodal Documents
Charles F. Greenbacker
Dept. of Computer & Information Sciences
University of Delaware
Newark, Delaware, USA
charlieg@cis.udel.edu
Abstract
We propose a framework for generating an ab-
stractive summary from a semantic model of a
multimodal document. We discuss the type of
model required, the means by which it can be
constructed, how the content of the model is
rated and selected, and the method of realizing
novel sentences for the summary. To this end,
we introduce a metric called information den-
sity used for gauging the importance of con-
tent obtained from text and graphical sources.
1 Introduction
The automatic summarization of text is a promi-
nent task in the field of natural language processing
(NLP). While significant achievements have been
made using statistical analysis and sentence extrac-
tion, ?true abstractive summarization remains a re-
searcher?s dream? (Radev et al, 2002). Although
existing systems produce high-quality summaries of
relatively simple articles, there are limitations as to
the types of documents these systems can handle.
One such limitation is the summarization of mul-
timodal documents: no existing system is able to in-
corporate the non-text portions of a document (e.g.,
information graphics, images) into the overall sum-
mary. Carberry et al (2006) showed that the con-
tent of information graphics is often not repeated
in the article?s text, meaning important information
may be overlooked if the graphical content is not in-
cluded in the summary. Systems that perform statis-
tical analysis of text and extract sentences from the
original article to assemble a summary cannot access
the information contained in non-text components,
let alne seamlessly combine that information with
the extracted text. The problem is that information
from the text and graphical components can only be
integrated at the conceptual level, necessitating a se-
mantic understanding of the underlying concepts.
Our proposed framework enables the genera-
tion of abstractive summaries from unified semantic
models, regardless of the original format of the in-
formation sources. We contend that this framework
is more akin to the human process of conceptual in-
tegration and regeneration in writing an abstract, as
compared to the traditional NLP techniques of rat-
ing and extracting sentences to form a summary.
Furthermore, this approach enables us to generate
summary sentences about the information collected
from graphical formats, for which there are no sen-
tences available for extraction, and helps avoid the
issues of coherence and ambiguity that tend to affect
extraction-based summaries (Nenkova, 2006).
2 Related Work
Summarization is generally seen as a two-phase pro-
cess: identifying the important elements of the doc-
ument, and then using those elements to construct
a summary. Most work in this area has focused on
extractive summarization, assembling the summary
from sentences representing the information in a
document (Kupiec et al, 1995). Statistical methods
are often employed to find key words and phrases
(Witbrock and Mittal, 1999). Discourse structure
(Marcu, 1997) also helps indicate the most impor-
tant sentences. Various machine learning techniques
have been applied (Aone et al, 1999; Lin, 1999), as
well as approaches combining surface, content, rel-
75
evance and event features (Wong et al, 2008).
However, a few efforts have been directed to-
wards abstractive summaries, including the modifi-
cation (i.e., editing and rewriting) of extracted sen-
tences (Jing and McKeown, 1999) and the genera-
tion of novel sentences based on a deeper under-
standing of the concepts being described. Lexical
chains, which capture relationships between related
terms in a document, have shown promise as an in-
termediate representation for producing summaries
(Barzilay and Elhadad, 1997). Our work shares sim-
ilarities with the knowledge-based text condensation
model of Reimer and Hahn (1988), as well as with
Rau et al (1989), who developed an information ex-
traction approach for conceptual information sum-
marization. While we also build a conceptual model,
we believe our method of construction will produce
a richer representation. Moreover, Reimer and Hahn
did not actually produce a natural language sum-
mary, but rather a condensed text graph.
Efforts towards the summarization of multimodal
documents have included na??ve approaches relying
on image captions and direct references to the im-
age in the text (Bhatia et al, 2009), while content-
based image analysis and NLP techniques are being
combined for multimodal document indexing and
retrieval in the medical domain (Ne?ve?ol et al, 2009).
3 Method
Our method consists of the following steps: building
the semantic model, rating the informational con-
tent, and generating a summary. We construct the
semantic model in a knowledge representation based
on typed, structured objects organized under a foun-
dational ontology (McDonald, 2000). To analyze the
text, we use Sparser,1 a linguistically-sound, phrase
structure-based chart parser with an extensive and
extendible semantic grammar (McDonald, 1992).
For the purposes of this proposal, we assume a rela-
tively complete semantic grammar exists for the do-
main of documents to be summarized. In the proto-
type implementation (currently in progress), we are
manually extending an existing grammar on an as-
needed basis, with plans for large-scale learning of
new rules and ontology definitions as future work.
Projects like the Never-Ending Language Learner
1https://github.com/charlieg/Sparser
(Carlson et al, 2010) may enable us to induce these
resources automatically.
Although our framework is general enough to
cover any image type, as well as other modalities
(e.g., audio, video), since image understanding re-
search has not yet developed tools capable of ex-
tracting semantic content from every possible im-
age, we must restrict our focus to a limited class of
images for the prototype implementation. Informa-
tion graphics, such as bar charts and line graphs, are
commonly found in popular media (e.g., magazines,
newspapers) accompanying article text. To integrate
this graphical content, we use the SIGHT system
(Demir et al, 2010b) which identifies the intended
message of a bar chart or line graph along with other
salient propositions conveyed by the graphic. Ex-
tending the prototype to incorporate other modalities
would not entail a significant change to the frame-
work. However, it would require adding a module
capable of mapping the particular modality to its un-
derlying message-level semantic content.
The next sections provide detail regarding the
steps of our method, which will be illustrated on
a short article from the May 29, 2006 edition of
Businessweek magazine entitled, ?Will Medtronic?s
Pulse Quicken??2 This particular article was chosen
due to good coverage in the existing Sparser gram-
mar for the business news domain, and because it ap-
pears in the corpus of multimodal documents made
available by the SIGHT project.
3.1 Semantic Modeling
Figure 1 shows a high-level (low-detail) overview
of the type of semantic model we can build using
Sparser and SIGHT. This particular example mod-
els the article text (including title) and line graph
from the Medtronic article. Each box represents
an individual concept recognized in the document.
Lines connecting boxes correspond to relationships
between concepts. In the interest of space, the in-
dividual attributes of the model entries have been
omitted from this diagram, but are available in Fig-
ure 2, which zooms into a fragment of the model
showing the concepts that are eventually rated most
salient (Section 3.2) and selected for inclusion in
2Available at http://www.businessweek.com/
magazine/content/06_22/b3986120.htm.
76
Company1
StockPriceChange1
Idiom1
BeatForecast1
EarningsForecast1
EarningsReport1
Group2
Prediction2
MakeAnnouncement1
AmountPerShare1
AmountPerShare2
WhQuestion1
Group3
Comparison3
RevenuePct1
Market2
RevenuePct1 Company3
Comparison1
GrowthSlowed1Market1
MissForecast1SalesForecast1
Comparison2 CounterArgument1
MarketFluctuations1 Protected1
EarningsForecast2
AmountPerShare3
EarningsForecast3
AmountPerShare4
SalesForecast2
SalesForecast3
StockOwnership1Company4
EmployedAt1
EarningsGrowth1
Prediction4
Prediction3
Person2
GainMarketShare1
StockRating2
HistoricLow1
Group1
Prediction1
Person1
EmployedAt1
Company2
StockRating1
TargetStockPrice1
AmountPerShare2
StockPriceChange3
StockPriceChange2
LineGraph1
Volatile1
ChangeTrend1
AmountPerShare5
AmountPerShare6
AmountPerShare7
Figure 1: High-level overview of semantic model for Medtronic article.
the summary (Section 3.3). The top portion of each
box in Figure 2 indicates the name of the conceptual
category (with a number to distinguish between in-
stances), the middle portion shows various attributes
of the concept with their values, and the bottom por-
tion contains some of the original phrasings from
the text that were used to express these concepts
(formally stored as a synchronous TAG) (McDon-
ald and Greenbacker, 2010)). Attribute values in an-
gle brackets (<>) are references to other concepts,
hash symbols (#) refer to a concept or category that
has not been instantiated in the current model, and
each expression is preceded by a sentence tag (e.g.,
?P1S4? stands for ?paragraph 1, sentence 4?).
P1S1: "medical device
    giant Medtronic"
P1S5: "Medtronic"
Name: "Medtronic"
Stock: "MDT"
Industry: (#pacemakers,
    #defibrillators,
    #medical devices)
Company1
P1S4: "Investment firm
    Harris Nesbitt's
    Joanne Wuensch"
P1S7: "Wuensch"
FirstName: "Joanne"
LastName: "Wuensch"
Person1
P1S4: "a 12-month
    target of 62"
Person: <Person 1>
Company: <Company 1>
Price: $62.00
Horizon: #12_months
TargetStockPrice1
Figure 2: Detail of Figure 1 showing concepts rated most
important and selected for inclusion in the summary.
As illustrated in this example, concepts conveyed
by the graphics in the document can also be included
in the semantic model. The overall intended mes-
sage (ChangeTrend1) and additional propositions
(Volatile1, StockPriceChange3, etc.) that SIGHT
extracts from the line graph and deems important
are added to the model produced by Sparser by sim-
ply inserting new concepts, filling slots for existing
concepts, and creating new connections. This way,
information gathered from both text and graphical
sources can be integrated at the conceptual level re-
gardless of the format of the source.
3.2 Rating Content
Once document analysis is complete and the seman-
tic model has been built, we must determine which
concepts conveyed by the document and captured
in the model are most salient. Intuitively, the con-
cepts containing the most information and having
the most connections to other important concepts in
the model are those we?d like to convey in the sum-
mary. We propose the use of an information den-
sity metric (ID) which rates a concept?s importance
based on a number of factors:3
? Completeness of attributes: the concept?s
filled-in slots (f ) vs. its total slots (s) [?satura-
tion level?], and the importance of the concepts
(ci) filling these slots [a recursive value]:
f
s ? log(s) ?
?f
i=1 ID(ci)
3The first three factors are similar to the dominant slot
fillers, connectivity patterns, and frequency criteria described
by Reimer and Hahn (1988).
77
? Number of connections/relationships (n) with
other concepts (cj), and the importance of these
connected concepts [a recursive value]:
?n
j=1 ID(cj)
? Number of expressions (e) realizing the con-
cept in the current document
? Prominence based on document and rhetorical
structure (WD & WR), and salience assessed
by the graph understanding system (WG)
Saturation refers to the level of completeness with
which the knowledge base entry for a given concept
is ?filled-out? by information obtained from the doc-
ument. As information is collected about a concept,
the corresponding slots in its concept model entry
are assigned values. The more slots that are filled,
the more we know about a given instance of a con-
cept. When all slots are filled, the model entry for
that concept is ?complete,? at least as far as the on-
tological definition of the concept category is con-
cerned. As saturation level is sensitive to the amount
of detail in the ontology definition, this factor must
be normalized by the number of attribute slots in its
definition, thus log(s) above.
In Figure 3 we can see an example of relative
saturation level by comparing the attribute slots for
Company2 with that of Company1 in Figure 2.
Since the ?Stock? slot is filled for Medtronic and
remains empty for Harris Nesbitt, we say that the
concept for Company1 is more saturated (i.e., more
complete) than that of Company2.
P1S4: "Investment firm
    Harris Nesbitt"
Name: "Harris Nesbitt"
Stock:
Industry: (#investments)
Company2
Figure 3: Detail of Figure 1 showing example concept
with unfilled attribute slot.
Document and rhetorical structure (WD and WR)
take into account the location of a concept within
a document (e.g., mentioned in the title) and the
use of devices highlighting particular concepts (e.g.,
juxtaposition) in computing the overall ID score.
For the intended message and informational proposi-
tions conveyed by the graphics, the weights assigned
by SIGHT are incorporated into ID as WG.
After computing the ID of each concept, we will
apply Demir?s (2010a) graph-based ranking algo-
rithm to select items for the summary. This algo-
rithm is based on PageRank (Page et al, 1999), but
with several changes. Beyond centrality assessment
based on relationships between concepts, it also in-
corporates apriori importance nodes that enable us
to capture concept completeness, number of expres-
sions, and document and rhetorical structure. More
importantly from a generation perspective, Demir?s
algorithm iteratively selects concepts one at a time,
re-ranking the remaining items by increasing the
weight of related concepts and discounting redun-
dant ones. Thus, we favor concepts that ought to be
conveyed together while avoiding redundancy.
3.3 Generating a Summary
After we determine which concepts are most im-
portant as scored by ID, the next step is to de-
cide what to say about them and express these el-
ements as sentences. Following the generation tech-
nique of McDonald and Greenbacker (2010), the ex-
pressions observed by the parser and stored in the
model are used as the ?raw material? for express-
ing the concepts and relationships. The two most
important concepts as rated in the semantic model
built from the Medtronic article would be Company1
(?Medtronic?) and Person1 (?Joanne Wuensch,? a
stock analyst). To generate a single summary sen-
tence for this document, we should try to find some
way of expressing these concepts together using the
available phrasings. Since there is no direct link
between these two concepts in the model (see Fig-
ure 1), none of the collected phrasings can express
both concepts at the same time. Instead, we need to
find a third concept that provides a semantic link be-
tween Company1 and Person1. If multiple options
are available, deciding which linking concept to use
becomes a microplanning problem, with the choice
depending on linguistic constraints and the relative
importance of the applicable linking concepts.
In this example, a reasonable selection would be
TargetStockPrice1 (see Figure 1). Combining orig-
inal phrasings from all three concepts (via substi-
tution and adjunction operations on the underlying
TAG trees), along with a ?built-in? realization inher-
ited by the TargetStockPrice category (a subtype of
Expectation ? not shown in the figure), produces a
78
construction resulting in this final surface form:
Wuensch expects a 12-month target of 62
for medical device giant Medtronic.
Thus, we generate novel sentences, albeit with some
?recycled? expressions, to form an abstractive sum-
mary of the original document.
Studies have shown that nearly 80% of human-
written summary sentences are produced by a cut-
and-paste technique of reusing original sentences
and editing them together in novel ways (Jing and
McKeown, 1999). By reusing selected short phrases
(?cutting?) coupled together with generalized con-
structions (?pasting?), we can generate abstracts
similar to human-written summaries.
The set of available expressions is augmented
with numerous built-in schemas for realizing com-
mon relationships such as ?is-a? and ?has-a,? as
well as realizations inherited from other concep-
tual categories in the hierarchy. If the knowledge
base persists between documents, storing the ob-
served expressions and making them available for
later use when realizing concepts in the same cat-
egory, the variety of utterances we can generate is
increased. With a sufficiently rich set of expres-
sions, the reliance on straightforward ?recycling? is
reduced while the amount of paraphrasing and trans-
formation is increased, resulting in greater novelty
of production. By using ongoing parser observations
to support the generation process, the more the sys-
tem ?reads,? the better it ?writes.?
4 Evaluation
As an intermediate evaluation, we will rate the con-
cepts stored in a model built only from text and use
this rating to select sentences containing these con-
cepts from the original document. These sentences
will be compared to another set chosen by traditional
extraction methods. Human judges will be asked
to determine which set of sentences best captures
the most important concepts in the document. This
?checkpoint? will allow us to assess how well our
system identifies the most salient concepts in a text.
The summaries ultimately generated as final out-
put by our prototype system will be evaluated
against summaries written by human authors, as
well as summaries created by extraction-based sys-
tems and a baseline of selecting the first few sen-
tences. For each comparison, participants will be
asked to indicate a preference for one summary
over another. We propose to use preference-strength
judgment experiments testing multiple dimensions
of preference (e.g., accuracy, clarity, completeness).
Compared to traditional rating scales, this alterna-
tive paradigm has been shown to result in better
evaluator self-consistency and high inter-evaluator
agreement (Belz and Kow, 2010). This allows a
larger proportion of observed variations to be ac-
counted for by the characteristics of systems under-
going evaluation, and can result in a greater number
of significant differences being discovered.
Automatic evaluation, though desirable, is likely
unfeasible. As human-written summaries have only
about 60% agreement (Radev et al, 2002), there is
no ?gold standard? to compare our output against.
5 Discussion
The work proposed herein aims to advance the state-
of-the-art in automatic summarization by offering a
means of generating abstractive summaries from a
semantic model built from the original article. By
incorporating concepts obtained from non-text com-
ponents (e.g., information graphics) into the seman-
tic model, we can produce unified summaries of
multimodal documents, resulting in an abstract cov-
ering the entire document, rather than one that ig-
nores potentially important graphical content.
Acknowledgments
This work was funded in part by the National Insti-
tute on Disability and Rehabilitation Research (grant
#H133G080047). The author also wishes to thank
Kathleen McCoy, Sandra Carberry, and David Mc-
Donald for their collaborative support.
References
Chinatsu Aone, Mary E. Okurowski, James Gorlinsky,
and Bjornar Larsen. 1999. A Trainable Summarizer
with Knowledge Acquired from Robust NLP Tech-
niques. In Inderjeet Mani and Mark T. Maybury, edi-
tors, Advances in Automated Text Summarization. MIT
Press.
Regina Barzilay and Michael Elhadad. 1997. Using lex-
ical chains for text summarization. In In Proceedings
79
of the ACL Workshop on Intelligent Scalable Text Sum-
marization, pages 10?17, Madrid, July. ACL.
Anja Belz and Eric Kow. 2010. Comparing rating
scales and preference judgements in language evalu-
ation. In Proceedings of the 6th International Natural
Language Generation Conference, INLG 2010, pages
7?16, Trim, Ireland, July. ACL.
Sumit Bhatia, Shibamouli Lahiri, and Prasenjit Mitra.
2009. Generating synopses for document-element
search. In Proceeding of the 18th ACM Conference
on Information and Knowledge Management, CIKM
?09, pages 2003?2006, Hong Kong, November. ACM.
Sandra Carberry, Stephanie Elzer, and Seniz Demir.
2006. Information graphics: an untapped resource for
digital libraries. In Proceedings of the 29th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR ?06,
pages 581?588, Seattle, August. ACM.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the 24th
Conference on Artificial Intelligence (AAAI 2010),
pages 1306?1313, Atlanta, July. AAAI.
Seniz Demir, Sandra Carberry, and Kathleen F. Mc-
Coy. 2010a. A discourse-aware graph-based content-
selection framework. In Proceedings of the 6th In-
ternational Natural Language Generation Conference,
INLG 2010, pages 17?26, Trim, Ireland, July. ACL.
Seniz Demir, David Oliver, Edward Schwartz, Stephanie
Elzer, Sandra Carberry, and Kathleen F. McCoy.
2010b. Interactive SIGHT into information graphics.
In Proceedings of the 2010 International Cross Dis-
ciplinary Conference on Web Accessibility, W4A ?10,
pages 16:1?16:10, Raleigh, NC, April. ACM.
Hongyan Jing and Kathleen R. McKeown. 1999. The
decomposition of human-written summary sentences.
In Proceedings of the 22nd Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, SIGIR ?99, pages 129?136,
Berkeley, August. ACM.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings
of the 18th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, SIGIR ?95, pages 68?73, Seattle, July. ACM.
Chin-Yew Lin. 1999. Training a selection function for
extraction. In Proceedings of the 8th International
Conference on Information and Knowledge Manage-
ment, CIKM ?99, pages 55?62, Kansas City, Novem-
ber. ACM.
Daniel C. Marcu. 1997. The Rhetorical Parsing, Summa-
rization, and Generation of Natural Language Texts.
Ph.D. thesis, University of Toronto, December.
David D. McDonald and Charles F. Greenbacker. 2010.
?If you?ve heard it, you can say it? - towards an ac-
count of expressibility. In Proceedings of the 6th In-
ternational Natural Language Generation Conference,
INLG 2010, pages 185?190, Trim, Ireland, July. ACL.
David D. McDonald. 1992. An efficient chart-based
algorithm for partial-parsing of unrestricted texts. In
Proceedings of the 3rd Conference on Applied Natural
Language Processing, pages 193?200, Trento, March.
ACL.
David D. McDonald. 2000. Issues in the repre-
sentation of real texts: the design of KRISP. In
Lucja M. Iwan?ska and Stuart C. Shapiro, editors, Nat-
ural Language Processing and Knowledge Represen-
tation, pages 77?110. MIT Press, Cambridge, MA.
Ani Nenkova. 2006. Understanding the process of multi-
document summarization: content selection, rewrite
and evaluation. Ph.D. thesis, Columbia University,
January.
Aure?lie Ne?ve?ol, Thomas M. Deserno, Ste?fan J. Darmoni,
Mark Oliver Gu?ld, and Alan R. Aronson. 2009. Nat-
ural language processing versus content-based image
analysis for medical document retrieval. Journal of the
American Society for Information Science and Tech-
nology, 60(1):123?134.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web. Technical Report 1999-
66, Stanford InfoLab, November. Previous number:
SIDL-WP-1999-0120.
Dragomir R. Radev, Eduard Hovy, and Kathleen McKe-
own. 2002. Introduction to the special issue on sum-
marization. Computational Linguistics, 28(4):399?
408.
Lisa F. Rau, Paul S. Jacobs, and Uri Zernik. 1989. In-
formation extraction and text summarization using lin-
guistic knowledge acquisition. Information Process-
ing & Management, 25(4):419 ? 428.
Ulrich Reimer and Udo Hahn. 1988. Text condensation
as knowledge base abstraction. In Proceedings of the
4th Conference on Artificial Intelligence Applications,
CAIA ?88, pages 338?344, San Diego, March. IEEE.
Michael J. Witbrock and Vibhu O. Mittal. 1999. Ultra-
summarization: a statistical approach to generating
highly condensed non-extractive summaries. In Pro-
ceedings of the 22nd Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, SIGIR ?99, pages 315?316, Berkeley,
August. ACM.
Kam-Fai Wong, Mingli Wu, and Wenjie Li. 2008.
Extractive summarization using supervised and semi-
supervised learning. In Proceedings of the 22nd Int?l
Conference on Computational Linguistics, COLING
?08, pages 985?992, Manchester, August. ACL.
80
  
?If you?ve heard it, you can say it? - Towards an Account of Expressibility 
 David D. McDonald Raytheon BBN Technologies Cambridge, MA USA dmcdonald@bbn.com 
Charles F. Greenbacker  University of Delaware Newark, DE, USA charlieg@cis.udel.edu    Abstract 
We have begun a project to automatically cre-ate the lexico-syntactic resources for a mi-croplanner as a side-effect of running a do-main-specific language understanding system. The resources are parameterized synchronous TAG Derivation Trees. Since the KB is as-sembled from the information in the texts that these resources are abstracted from, it will de-compose along those same lines when used for generation. As all possible ways of expressing each concept are pre-organized into general patterns known to be linguistically-valid (they were observed in natural text), we obtain an architectural account for expressibility. 1. Expressibility People speak grammatically. They may stutter, restart, or make the occasional speech error, but all in all they are faithful to the grammar of the language dialects they use. One of the ways that a language generation system can account for this is through the use of grammar that defines all of the possible lexico-syntactic elements from which a text can be constructed and defines all their rules of composition, such as lexicalized Tree Adjoining Grammar (TAG). Without the ability to even formulate an ungrammatical text, such a generator provides an account for human grammaticality based on its architecture rather than its programmer.  We propose a similar kind of accounting for the problem of expressibility: one based on architecture rather than accident. Expressibility, as defined by Meteer (1992), is an issue for microplanners as they decide on which lexical and syntactic resources to employ. Not all of the options they might want to use are available in the language ? they are not expressible. Consider the examples in Figure 1, adapted from Meteer 1992 pg. 50.  
Expression Construction (?decide?) ?quick decision? <result> + <quick> ?decide quickly? <action> + <quick> ?important decision? <result> + <important> * ?decide importantly? <action> + <important> Figure 1: Constraints on expressibility: To say that there was a decision and it was important, you are forced to use the noun form because there is no adverbial form for important as there is for quick  In this short paper, we discuss our approach to expressibility. We describe in detail our novel method centered on how to use parser observa-tions to guide generator decisions, and we pro-vide a snapshot of the current status of our system implementation. 2. Related Work Natural language generation (NLG) systems must have some way of making sure that the messages they build are actually expressible. Template-based generators avoid problems with expressibility largely by anticipating all of the wording that will be needed and packaging it in chunks that are guaranteed to compose correctly. Becker (2006), for example, does this via fully lexicalized TAG trees.   Among more general-purpose generators, one approach to expressibility is to look ahead into the lexicon, avoiding constructions that are lexically incompatible. Look-ahead is expensive, however, and is only practical at small abstrac-tion distances such as Shaw?s re-writing sentence planner (1998). Meteer?s own approach to expressibility started by interposing another level of represen-tation between the microplanner and the surface realizer, an ?abstract syntactic representation? in the sense of RAGS (Cahill et al 1999), that employed functional relationships (head, argu-ment, matrix, adjunct) over semantically typed, 
  
lexicalized constituents. This blocks *decide importantly because ?important? only has a realization as a property and her composition rules prohibit using a property to modify an action (?decide?). Shifting the perspective from the action to its result allows the composition to go through. We are in sympathy with this approach ? a microplanner needs its own representational level to serve as a scratch pad (if using a revi-sion-based approach) or just as a scaffold to hold intermediate results. However, Meteer?s seman-tic and lexical constraints do require operating with fine-grain details. We believe that we can work with larger chunks that have already been vetted for expressibility because we?ve observed someone use them, either in writing or speech.  3. Method Our approach is similar to that of Zhong  & Stent (2005) in that we use the analysis of a corpus as the basis for creating the resources for the reali-zation component. Several differences stand out. For one, we are working in specific domains rather than generic corpora like the WSJ. This enables the biggest difference: our analysis is performed by a completely accurate,1 domain-specific NLU system (?parser?)2 based on a semantic grammar (McDonald 1993). It is read-ing for the benefit of a knowledge base, adding specific facts within instances of a highly struc-tured, predefined prototypes. Such instances are used as the starting point for the generation process. On the KB side, our present focus happens to be on hurricanes and the process they go through as they evolve. We have developed a semantic grammar for this domain, and it lets us analyze texts like these:3 (1) ?Later that day it made landfall near the Haitian town of Jacmel.?                                                            1 Parse accuracy and correct word sense interpretation is only possible if the semantic domain under analysis is restricted by topic and sublanguage.  2 Most systems referred to as ?parsers? stop at a structural description. Ours stops at the level of a disambiguated conceptual model and is more integrated than most. 3 #1 and 2 are from the Wikipedia article on Hurricane Gustav. #3 is from a New York Times article. 
(2) ?? and remained at that intensity until landfall on the morning of September 1 near Cocodrie, Louisiana.? (3) ?By landfall on Monday morning ?? Such texts tell us how people talk about hurri-canes, specifically here about landfall events. They tell us what combinations of entities are reasonable to include within single clauses (intensity, time, location), and they tell us which particular realizations of the landfall concept have been used in which larger linguistic con-texts. They also indicate what information can be left out under the discourse conditions defined by the larger texts they appear in.4 As different texts are read, we accumulate dif-ferent realization forms for the same content. In example #1, landfall is expressed via the idiom make landfall, the time is given in an initial adverbial, and the location as a trailing adjunct. In #2, the landfall stands by itself as the head of a time-adverbial and the time and location are adjuncts off of it. This set of alternative phras-ings provides the raw material for the microplan-ner to work with ? a natural set of paraphrases. 3.1 Derivation Trees as templates As shown in Figure 3, to create resources for the microplanner, we start with the semantic analysis that the parser anchors to its referent when it instantiates the appropriate event type within the prototypical model of what hurricanes do, here a ?landfall event?, noting the specific time and location. Following Bateman (e.g. 2007) and Meteer (1992), we work with typed, structured objects organized under a foundational ontol-ogy.5 Figure 2 shows the current definition of the landfall class in a local notation for OWL Full. (Class HurricaneLandfall   (restrict hurricane - Hurricane)   (restrict intensity ? Saffir-Simpson)   (restrict location ? PhysEndurant)   (restrict time ? Date&Time)) Figure 2. The Landfall class                                                             4 For example, in #1 and #3 the precise date had been given already in earlier sentences. 5 An extension of Dolce (Gangemi et al 2002). 
  
Figure 3. Overview The semantic analysis recursively maps con-stituents? referents to properties of a class in-stance. Accompanying it is a syntactic analysis in the form of a TAG Derivation Tree6 (DT) where each of its nodes (initial trees, insertions or adjunctions) points both to its lexical anchor and its specific correspondence in the domain model. To create a reusable resource, we abstract away from the lexicalization in these DT/model-anchored pairs, and replace it with the corre-sponding model classes as determined by the restrictions on the properties. For example, the day of the week in #3, lexically given as Monday morning and then dereferenced to an object with the meaning ?9/1/2008 before noon? is replaced in the resource with that object?s type. The result is a set of templates associated with the combination of types that corresponds to the participants in its source text ? the more com-posed the type, the more insertions / adjunctions in the template derivation tree.   3.2 Synchronous TAGS This combination of derived trees and model-levels classes and properties where the nodes of the two structures are linked is a synchronous TAG (ST). As observed by Shieber and Schabes (1991) who introduced this notion, ?[STs] make the fine-grained correspondences between ex-pressions of natural language and their meanings explicit by ? node linking?.                                                             6 The primary analysis is phrase structure in a chart, but since every rule in the grammar corresponds to either a lexicalized insertion or adjunction, the pattern of rule application is read out as a TAG derivation tree. 
pp("by")
   insert: prep-comp("landfall")
   adjoin: pp ("on")
              insert: prep-comp("Monday")
(Individual HurricaneLandfall new-instance
 (hurricane #<>)
 (intensity #<>)
 (location #<>)
 (time #<DayOfWeek Monday>))
 
 
 
  Figure 4. Synchronous TAG In particular, they observe that STs solve an otherwise arbitrary problem of ?where does one start? when faced with a bag of content to be realized as a text. Our STs identify natural ?slices? of the content ? those parts that have already been observed to have been realized together in a naturally occurring text. Because we have the luxury to be creating the knowledge base of our hurricane model by the accretion of relationships among individually small chunks of information (a triple store), we can take synchronous TAGS a step further and allow them to dictate the permitted ways that information can be delimited within the KB for purposes of generation following the ideas in (Stone 2002). If we can surmount the issues described be-low, this stricture ? that one can only select for generation units of content of the types that have been observed to be used together (the model side of the STs) ? is a clean architectural expla-nation of how it is that the generator?s messages are always expressible.  4. State of Development We are at an early stage in our work. Everything we have described is implemented, but only on a 
  
?thin slice? to establish that our ideas were credi-ble. There are many issues to work out as we ?bulk up? the system and begin to actually inte-grate it in a in ?tactical? microplanner and begin to actually do the style of macro-planning (de-termining the relevant portions of the domain model to use as content given the intent and affect) that our use of synchronous TAGS should allow. The most salient issues are how broadly we should generalize when we substitute domain types for lexicalizations in the templates, and what contextual information must be kept with the templates.  The type generalizations need to be broad enough to encompass as many substitutions as possible, while being strict enough to ensure that when the template is applied to those objects the realizations available to them permit them to be expressed in that linguistic context.7 The examples all have specific contexts in the sentences and recent discourse. Two of them (#2, #3) are using the landfall event as a time phrase. Can we move them and still retain the natural-ness of the original (e.g. from sentence initial to sentence final), or does this sort of information need to be encoded? Another issue is how to evaluate a system like this. Given the accuracy of the analysis, recreat-ing the source text is trivial, so comparison to the source of the resources as a gold standard is meaningless. Some alternative must be found. While we work out these issues, we are ex-tending the NLU domain model and grammar to cover more cases and thence create more syn-chronized TAG templates. We then manually identify alternative domain content to app hly to them to in order to explore the space of realiza-tions and identify unforeseen interactions. Our short-term goals are to vastly increase the grammar coverage for our motivating examples and to hand over all microplanning decisions to the system itself. Long-term goals include broad-ening the coverage further still, to as open a domain as is feasible, as well as testing different macroplanners and applications with which to drive the entire process. Among several possi-bilities are automatic merged-and-modified summarization and a query-based discourse system.                                                            7 In our example, substituting different days and times is obvious (by landfall on the afternoon of August 22), but as we move away from that precise set of types (general-time-of-day + date) we see that what had been lexically fixed in the derivation tree (by landfall on) has to shift: ? at 2:00 on August 22.  
5. Discussion Because the phrasal patterns observed in the corpus act as templates guiding the generation process, and as the underlying NLU system and generator (McDonald 1993, Meteer et al 1987) are mature and grounded in linguistic principles, our system combines template-based and theory-based approaches.  Van Deemter et al (2005) outlined three crite-ria for judging template-driven applications against "standard" (non-template) NLG systems. (1) Maintainability is addressed by the fact that our templates aren't hand-made. To extend the set of available realization forms we expose the NLU system to more text. The subject domain has to be one that has already been modeled, but we are operating from the premise that a NLG component would only bother to speak about things that the system as a whole understands. (2) Output quality and variability are determined by the corpus; using corpora containing high quality and varied constructions will enable similar output from the generator. (3) Most crucially, our parser and generator components are linguistically well-founded. Composition into our ?templates? is smoothly accommodated (extra modifiers, shifts in tense or aspect, appli-cation of transformations over the DT to form questions, relative clauses, dropped constituents under conjunction). The fully-articulated syntac-tic structure can be automatically annotated to facilitate prosody or to take information structure markup on the DT. The closest system to ours may be Marciniak & Strube (2005) who also use an annotated corpus as a knowledge source for generation, getting their annotations via ?a simple rule-based system tuned to the given types of text?. As far as we can tell, they are more concerned with discourse while we focus on the integration with the underlying knowledge base and how that KB is extended over time. Like them, we believe that one of the most promising aspects of this work going forward is that the use of a parser provides us with ?self-labeling data? to draw on for statistical analysis. Such training material would reduce the effort required to adapt a generator to a new domain, while simultaneously improving its output.  Acknowledgments This work was supported in part by the BBN POIROT project: DARPA IPTO contract FA865 0-06-C-7606. 
  
References  John Bateman, Thora Tenbrink, and Scott Farrar. 2007. The Role of Conceptual and Linguistic On-tologies in Interpreting Spatial Discourse. Dis-course Processes, 44(3):175?213. Tilman Becker. 2006. Natural Language Generation with Fully Specified Templates. In W. Wahlster (Ed.), SmartKom: Foundations of Multimodal Dia-log Systems, 401?410. Springer, Berlin Heidelberg. Lynne Cahill, Christy Doran, Roger Evans, Chris Mellish, Daniel Paiva, Mike Reape, Donia Scott, & Neil Tipper. 1999. Towards a Reference Architec-ture for Natural Language Generation Systems, The RAGS project. ITRI technical report number ITRI-99-14, University of Brighton, March. Aldo Gangemi, Nicola Guarino, Claudio Masolo, Alessandro Oltramari, & Luc Schneider. 2002. Sweetening Ontologies with DOLCE. In Proceed-ings of the 13th International Conference on Knowledge Acquisition, Modeling and Manage-ment (EKAW), pages 166?181, Sig?enza, Spain, October 1?4. Tomasz Marciniak & Michael Strube. 2005. Using an Annotated Corpus As a Knowledge Source For Language Generation. In Proceedings of the Cor-pus Linguistics 2005 Workshop on Using Corpora for Natural Language Generation (UCNLG), pages 19?24, Birmingham, UK, July 14. David McDonald. 2003. The Interplay of Syntactic and Semantic Node Labels in Partial Parsing, in the proceedings of the Third International Workshop on Parsing Technologies, August 10-13, 1993 Til-burg, The Netherlands, pp. 171-186; revised ver-sion in Bunt and Tomita (eds), Recent Advances in Parsing Technology, Kluwer Academic Publishers, pgs. 295-323. Marie W. Meteer. 1992. Expressibility and the Prob-lem of Efficient Text Planning. Pinter, London.  Marie Meteer, David McDonald, Scott Anderson, David Forster, Linda Gay, Alison Huettner & Penelope Sibun. 1987. Mumble-86: Design and Implementation, TR #87-87 Dept. Computer & Information Science, UMass., September 1987, 174 pgs. James Shaw. 1998. Clause Aggregation Using Lin-guistic Knowledge. In Proceedings of the 9th In-ternational Workshop on Natural Language Gen-eration, pages 138?147, Niagara-on-the-Lake, On-tario, August 5?7.  Stuart Shieber & Yves Schabes. 1991. Generation and synchronous tree-adjoining grammar. Computa-tional Intelligence, 7(4):220?228. Matthew Stone. 2003. Specifying Generation of Referring Expressions by Example. In Proceedings 
of the AAAI Spring Symposium on Natural Lan-guage Generation in Spoken and Written Dialogue, pages 133?140, Stanford, March. Kees van Deemter, Emiel Krahmer, & Mari?t Theune. 2005. Real versus Template-Based Natural Lan-guage Generation: A False Opposition? Computa-tional Linguistics, 31(1):15?24. Huvava Zhong & Amada Stent. 2005. Building Surface Realizers Automatically From Corpora. In Proceedings of the Corpus Linguistics 2005 Work-shop on Using Corpora for Natural Language Generation (UCNLG), pages 49?54, Birmingham, UK, July 14. 
UDel: Refining a Method of Named Entity Generation
Charles F. Greenbacker, Nicole L. Sparks, Kathleen F. McCoy, and Che-Yu Kuo
Department of Computer and Information Sciences
University of Delaware
Newark, Delaware, USA
[charlieg|sparks|mccoy|kuo]@cis.udel.edu
Abstract
This report describes the methods and re-
sults of a system developed for the GREC
Named Entity Challenge 2010. We de-
tail the refinements made to our 2009 sub-
mission and present the output of the self-
evaluation on the development data set.
1 Introduction
The GREC Named Entity Challenge 2010 (NEG)
is an NLG shared task whereby submitted systems
must select a referring expression from a list of
options for each mention of each person in a text.
The corpus is a collection of 2,000 introductory
sections from Wikipedia articles about individual
people in which all mentions of person entities
have been annotated. An in-depth description of
the task, along with the evaluation results from the
previous year, is provided by Belz et al (2009).
Our 2009 submission (Greenbacker and Mc-
Coy, 2009a) was an extension of the system we
developed for the GREC Main Subject Refer-
ence Generation Challenge (MSR) (Greenbacker
and McCoy, 2009b). Although our system per-
formed reasonably-well in predicting REG08-
Type in the NEG task, our string accuracy scores
were disappointingly-low, especially when com-
pared to the other competing systems and our own
performance in the MSR task. As suggested by the
evaluators (Belz et al, 2009), this was due in large
part to our reliance on the list of REs being in a
particular order, which had changed for the NEG
task.
2 Method
The first improvement we made to our existing
methods related to the manner by which we se-
lected the specific RE to employ. In 2009, we
trained a series of decision trees to predict REG08-
Type based on our psycholinguistically-inspired
feature set (described in (Greenbacker and Mc-
Coy, 2009c)), and then simply chose the first op-
tion in the list of REs matching the predicted type.
For 2010, we incorporated the case of each RE
into our target attribute so that the decision tree
classifier would predict both the type and case for
the given reference. Then, we applied a series
of rules governing the length of initial and sub-
sequent REs involving a person?s name (following
Nenkova and McKeown (2003)), as well as ?back-
offs? if the predicted type or case were not avail-
able.
Another improvement we made involved our
method of determining whether the use of a pro-
noun would introduce ambiguity in a given con-
text. Previously, we searched for references to
other people entities since the most recent mention
of the entity at hand, and if any were found, we
assumed these would cause the use of a pronoun
to be ambiguous. However, this failed to account
for the fact that personal pronouns in English are
gender-specific (ie. the mention of a male individ-
ual would not make the use of ?she? ambiguous).
So, we refined this by determining the gender of
each named entity (by seeing which personal pro-
nouns were associated with it in the list of REs),
and only noting ambiguity when the current entity
and candidate interfering antecedent were of the
same gender.
Other small changes from 2009 include an ex-
panded abbreviation set in the sentence segmenter,
separate decision trees for the main subject and
other entities, and fixing how we handled embed-
ded REF elements with unspecified mention IDs.
3 Results
Scores for REG08-Type precision & recall, string
accuracy, and string-edit distance are presented in
Figure 1. These were computed on the entire de-
velopment set, as well as the three subsets, us-
ing the geval.pl self-evaluation tool provided in the
NEG participants? pack.
While we were able to achieve an improvement
of nearly 50% over our 2009 scores in string ac-
curacy, we saw less than a 1% gain in overall
REG08-Type performance.
Metric Score
Type Precision/Recall 0.757995735607676
String Accuracy 0.650496141124587
Mean Edit Distance 0.875413450937156
Normalized Distance 0.319266300067796
(a) Scores on the entire development set.
Metric Score
Type Precision/Recall 0.735294117647059
String Accuracy 0.623287671232877
Mean Edit Distance 0.839041095890411
Normalized Distance 0.345490867579909
(b) Scores on the ?Chefs? subset.
Metric Score
Type Precision/Recall 0.790769230769231
String Accuracy 0.683544303797468
Mean Edit Distance 0.882911392405063
Normalized Distance 0.279837251356239
(c) Scores on the ?Composers? subset.
Metric Score
Type Precision/Recall 0.745928338762215
String Accuracy 0.642140468227425
Mean Edit Distance 0.903010033444816
Normalized Distance 0.335326519731057
(d) Scores on the ?Inventors? subset.
Figure 1: Scores on the development set obtained
via the geval.pl self-evaluation tool. REG08-Type
precision and recall were equal in all four sets.
4 Conclusions
The fact that our string accuracy scores improved
over our 2009 submission far more than REG08-
Type prediction is hardly surprising. Our efforts
during this iteration of the NEG task were primar-
ily focused on enhancing our methods of choosing
the best RE once the reference type was selected.
We remain several points below the best-
performing team from 2009 (ICSI-Berkeley), pos-
sibly due to the inclusion of additional items in
their feature set, or the use of Conditional Ran-
dom Fields as their learning technique (Favre and
Bohnet, 2009).
5 Future Work
Moving forward, we hope to expand our feature
set by including the morphology of words immedi-
ately surrounding the reference, as well as a more
extensive reference history, as suggested by (Favre
and Bohnet, 2009). We suspect that these features
may play a significant role in determining the type
of referenced used, the prediction of which acts as
a ?bottleneck? in generating exact REs.
We would also like to compare the efficacy of
several different machine learning techiques as ap-
plied to our feature set and the NEG task.
References
Anja Belz, Eric Kow, and Jette Viethen. 2009. The
GREC named entity generation challenge 2009:
Overview and evaluation results. In Proceedings
of the 2009 Workshop on Language Generation and
Summarisation (UCNLG+Sum 2009), pages 88?98,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Benoit Favre and Bernd Bohnet. 2009. ICSI-
CRF: The generation of references to the main
subject and named entities using conditional ran-
dom fields. In Proceedings of the 2009 Workshop
on Language Generation and Summarisation (UC-
NLG+Sum 2009), pages 99?100, Suntec, Singapore,
August. Association for Computational Linguistics.
Charles Greenbacker and Kathleen McCoy. 2009a.
UDel: Extending reference generation to multiple
entities. In Proceedings of the 2009 Workshop
on Language Generation and Summarisation (UC-
NLG+Sum 2009), pages 105?106, Suntec, Singa-
pore, August. Association for Computational Lin-
guistics.
Charles Greenbacker and Kathleen McCoy. 2009b.
UDel: Generating referring expressions guided by
psycholinguistc findings. In Proceedings of the
2009 Workshop on Language Generation and Sum-
marisation (UCNLG+Sum 2009), pages 101?102,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Charles F. Greenbacker and Kathleen F. McCoy.
2009c. Feature selection for reference generation as
informed by psycholinguistic research. In Proceed-
ings of the CogSci 2009 Workshop on Production of
Referring Expressions (PRE-Cogsci 2009), Amster-
dam, July.
Ani Nenkova and Kathleen McKeown. 2003. Improv-
ing the coherence of multi-document summaries:
a corpus study for modeling the syntactic realiza-
tion of entities. Technical Report CUCS-001-03,
Columbia University, Computer Science Depart-
ment.
UDel: Named Entity Recognition and Reference Regeneration
from Surface Text
Nicole L. Sparks, Charles F. Greenbacker, Kathleen F. McCoy, and Che-Yu Kuo
Department of Computer and Information Sciences
University of Delaware
Newark, Delaware, USA
[sparks|charlieg|mccoy|kuo]@cis.udel.edu
Abstract
This report describes the methods and re-
sults of a system developed for the GREC
Named Entity Recognition and GREC
Named Entity Regeneration Challenges
2010. We explain our process of automat-
ically annotating surface text, as well as
how we use this output to select improved
referring expressions for named entities.
1 Introduction
Generation of References in Context (GREC) is a
set of shared task challenges in NLG involving a
corpus of introductory sentences from Wikipedia
articles. The Named Entity Recognition (GREC-
NER) task requires participants to recognize all
mentions of people in a document and indicate
which mentions corefer. In the Named Entity Re-
generation (GREC-Full) task, submitted systems
attempt to improve the clarity and fluency of a
text by generating improved referring expressions
(REs) for all references to people. Participants are
encouraged to use the output from GREC-NER as
input for the GREC-Full task. To provide ample
opportunities for improvement, a certain portion
of REs in the corpus have been replaced by more-
specified named references. Ideally, the GREC-
Full output will be more fluent and have greater
referential clarity than the GREC-NER input.
2 Method
The first step in our process to complete the
GREC-NER task is to prepare the corpus for in-
put into the parser by stripping all XML tags and
segmenting the text into sentences. This is accom-
plished with a simple script based on common ab-
breviations and sentence-final punctuation.
Next, the files are run through the Stanford
Parser (The Stanford Natural Language Process-
ing Group, 2010), providing a typed dependency
representation of the input text from which we ex-
tract the syntactic functions (SYNFUNC) of, and
relationships between, words in the sentence.
The unmarked segmented text is also used
as input for the Stanford Named Entity Recog-
nizer (The Stanford Natural Language Processing
Group, 2009). We eliminate named entity tags for
locations and organizations, leaving only person
entities behind. We find the pronouns and com-
mon nouns (e.g. ?grandmother?) referring to per-
son entities that the NER tool does not tag. We
also identify the REG08-Type and case for each
RE. Entities found by the NER tool are marked
as names, and the additional REs we identified
are marked as either pronouns or common nouns.
Case values are determined by analyzing the as-
signed type and any type dependency representa-
tion (provided by the parser) involving the entity.
At this stage we also note the gender of each pro-
noun and common noun, the plurality of each ref-
erence, and begin to deal with embedded entities.
The next step identifies which tagged mentions
corefer. We implemented a coreference resolu-
tion tool using a shallow rule-based approach in-
spired by Lappin and Leass (1994) and Bontcheva
et al (2002). Each mention is compared to all
previously-seen entities on the basis of case, gen-
der, SYNFUNC, plurality, and type. Each en-
tity is then evaluated in order of appearance and
compared to all previous entities starting with the
most recent and working back to the first in the
text. We apply rules to each of these pairs based
on the REG08-Type attribute of the current en-
tity. Names and common nouns are analyzed us-
ing string and word token matching. We collected
extensive, cross-cultural lists of male and female
first names to help identify the gender of named
entities, which we use together with SYNFUNC
values for pronoun resolution. Separate rules gov-
ern gender-neutral pronouns such as ?who.? By
the end of this stage, we have all of the resources
MUC-6 CEAF B-CUBED
Corpus F prec. recall F prec. recall F prec. recall
Entire Set 71.984 69.657 74.471 68.893 68.893 68.893 72.882 74.309 71.509
Chefs 71.094 65.942 77.119 65.722 65.722 65.722 71.245 69.352 73.244
Composers 68.866 66.800 71.064 68.672 68.672 68.672 71.929 73.490 70.433
Inventors 76.170 77.155 75.210 72.650 72.650 72.650 75.443 80.721 70.812
Table 1: Self-evaluation scores for GREC-NER.
necessary to complete the GREC-NER task.
As a post-processing step, we remove all extra
(non-GREC) tags used in previous steps, re-order
the remaining attributes in the proper sequence,
add the list of REs (ALT-REFEX), and write the
final output following the GREC format. At this
point, the GREC-NER task is concluded and its
output is used as input for the GREC-Full task.
To improve the fluency and clarity of the text
by regenerating the referring expressions, we rely
on the system we developed for the GREC Named
Entity Challenge 2010 (NEG), a refined version
of our 2009 submission (Greenbacker and Mc-
Coy, 2009a). This system trains decision trees
on a psycholinguistically-inspired feature set (de-
scribed by Greenbacker and McCoy (2009b)) ex-
tracted from a training corpus. It predicts the most
appropriate reference type and case for the given
context, and selects the best match from the list of
available REs. For the GREC-Full task, however,
instead of using the files annotated by the GREC
organizers as input, we use the files we annotated
automatically in the GREC-NER task. By keep-
ing the GREC-NER output in the GREC format,
our NEG system was able to successfully run un-
modified and generate our output for GREC-Full.
3 Results
Scores calculated by the GREC self-evaluation
tools are provided in Table 1 for GREC-NER and
in Table 2 for GREC-Full.
Corpus NIST BLEU-4
Entire Set 8.1500 0.7953
Chefs 7.5937 0.7895
Composers 7.5381 0.8026
Inventors 7.5722 0.7936
Table 2: Self-evaluation scores for GREC-Full.
4 Conclusions
Until we compare our results with others teams or
an oracle, it is difficult to gauge our performance.
However, at this first iteration of these tasks, we?re
pleased just to have end-to-end RE regeneration
working to completion with meaningful output.
5 Future Work
Future improvements to our coreference resolu-
tion approach involve analyzing adjacent text, uti-
lizing more of the parser output, and applying ma-
chine learning to our GREC-NER methods.
References
Kalina Bontcheva, Marin Dimitrov, Diana Maynard,
Valentin Tablan, and Hamish Cunningham. 2002.
Shallow Methods for Named Entity Coreference
Resolution. In Cha??nes de re?fe?rences et re?solveurs
d?anaphores, workshop TALN 2002, Nancy, France.
Charles Greenbacker and Kathleen McCoy. 2009a.
UDel: Extending reference generation to multiple
entities. In Proceedings of the 2009 Workshop
on Language Generation and Summarisation (UC-
NLG+Sum 2009), pages 105?106, Suntec, Singa-
pore, August. Association for Computational Lin-
guistics.
Charles F. Greenbacker and Kathleen F. McCoy.
2009b. Feature selection for reference generation as
informed by psycholinguistic research. In Proceed-
ings of the CogSci 2009 Workshop on Production of
Referring Expressions (PRE-Cogsci 2009), Amster-
dam, July.
Shalom Lappin and Herbert J. Leass. 1994. An Algo-
rithm for Pronominal Anaphora Resolution. Com-
putational Linguistics, 20(4):535?561.
The Stanford Natural Language Processing Group.
2009. Stanford Named Entity Recognizer.
http://nlp.stanford.edu/software/
CRF-NER.shtml.
The Stanford Natural Language Processing Group.
2010. The Stanford Parser: A statistical parser.
http://nlp.stanford.edu/software/
lex-parser.shtml.
Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 41?48,
Portland, Oregon, June 23, 2011. c?2011 Association for Computational Linguistics
Abstractive Summarization of Line Graphs from Popular Media
Charles F. Greenbacker Peng Wu
Sandra Carberry Kathleen F. McCoy Stephanie Elzer*
Department of Computer and Information Sciences
University of Delaware, Newark, Delaware, USA
[charlieg|pwu|carberry|mccoy]@cis.udel.edu
*Department of Computer Science
Millersville University, Millersville, Pennsylvania, USA
elzer@cs.millersville.edu
Abstract
Information graphics (bar charts, line graphs,
etc.) in popular media generally have a dis-
course goal that contributes to achieving the
communicative intent of a multimodal docu-
ment. This paper presents our work on ab-
stractive summarization of line graphs. Our
methodology involves hypothesizing the in-
tended message of a line graph and using it
as the core of a summary of the graphic. This
core is then augmented with salient proposi-
tions that elaborate on the intended message.
1 Introduction
Summarization research has focused primarily on
summarizing textual documents, and until recently,
other kinds of communicative vehicles have been
largely ignored. As noted by Clark (1996), language
is more than just words ? it is any signal that is
intended to convey a message. Information graph-
ics (non-pictorial graphics such as bar charts, line
graphs, etc.) in popular media such as Newsweek,
Businessweek, or newspapers, generally have a com-
municative goal or intended message. For exam-
ple, the graphic in Figure 1 is intended to convey
a changing trend in sea levels ? relatively flat from
1900 to 1930 and then rising from 1930 to 2003.
Thus, using Clark?s view of language, information
graphics are a means of communication.
Research has shown that the content of informa-
tion graphics in popular media is usually not re-
peated in the text of the accompanying article (Car-
berry et al, 2006). The captions of such graphics
are also often uninformative or convey little of the
graphic?s high-level message (Elzer et al, 2005).
This contrasts with scientific documents in which
graphics are often used to visualize data, with ex-
plicit references to the graphic being used to explain
their content (e.g., ?As shown in Fig. A...?). Infor-
mation graphics in popular media contribute to the
overall communicative goal of a multimodal docu-
ment and should not be ignored.
Our work is concerned with the summarization
of information graphics from popular media. Such
summaries have several major applications: 1) they
can be integrated with the summary of a multimodal
document?s text, thereby producing a richer sum-
mary of the overall document?s content; 2) they can
be stored in a digital library along with the graphic
itself and used to retrieve appropriate graphics in re-
sponse to user queries; and 3) for individuals with
sight impairments, they can be used along with a
screen reader to convey not only the text of a docu-
ment, but also the content of the document?s graph-
ics. In this paper we present our work on summariz-
ing line graphs. This builds on our previous efforts
into summarizing bar charts (Demir et al, 2008;
Elzer et al, 2011); however, line graphs have dif-
ferent messages and communicative signals than bar
charts and their continuous nature requires different
processing. In addition, a very different set of visual
features must be taken into account in deciding the
importance of including a proposition in a summary.
2 Methodology
Most summarization research has focused on ex-
tractive techniques by which segments of text are
extracted and put together to form the summary.
41
?
102468 1900?1
0?20
?50?60
?70?80
?90
?03
?30?40
2000
10
8.9
1.979 inches over
 the past ce
ntury. Ann
ual differen
ce from Se
attle?s
In the seatt
le area, for
 example, t
he Pacific 
Ocean has 
risen nearly
 
they are ris
ing about 0
.04?0.09 o
f an inch ea
ch year.
Sea levels 
fluctuate ar
ound the gl
obe, but oc
eanographe
rs believe
Ocean leve
ls rising
1899 sea le
vel, in inch
es:
Figure 1: From ?Worry flows from Arctic ice to tropical
waters? in USA Today, May 31, 2006.
However, the Holy Grail of summarization work is
abstractive summarization in which the document?s
content is understood and the important concepts are
integrated into a coherent summary. For informa-
tion graphics, extractive summarization might mean
treating the text in the graphic (e.g., the caption) as if
it were document text. One could imagine perhaps
expanding this view to include selecting particular
data points or segments and constructing sentences
that convey them. Abstractive summarization, on
the other hand, requires that the high-level content
of the graphic be identified and conveyed in the sum-
mary. The goal of our work is abstractive summa-
rization. The main issues are identifying the knowl-
edge conveyed by a graphic, selecting the concepts
that should be conveyed in a summary, and integrat-
ing them into coherent natural language sentences.
As noted in the Introduction, information graphics
in popular media generally have a high-level mes-
sage that they are intended to convey. This mes-
sage constitutes the primary communicative or dis-
course goal (Grosz and Sidner, 1986) of the graphic
and captures its main contribution to the overall dis-
course goal of the entire document. However, the
graphic also includes salient features that are impor-
tant components of the graphic?s content. For exam-
ple, the graphic in Figure 1 is very jagged with sharp
fluctuations, indicating that short-term changes have
been inconsistent. Since the graphic?s intended mes-
sage represents its primary discourse goal, we con-
tend that this message should form the core or fo-
cus of the graphic?s summary. The salient features
should be used to augment the summary of the graph
and elaborate on its intended message. Thus, our
methodology consists of the following steps: 1) hy-
pothesize the graphic?s primary discourse or com-
municative goal (i.e., its intended message), 2) iden-
tify additional propositions that are salient in the
graphic, and 3) construct a natural language sum-
mary that integrates the intended message and the
additional salient propositions into a coherent text.
Section 3 presents our methodology for hypothe-
sizing a line graph?s intended message or discourse
goal. It starts with an XML representation of the
graphic that specifies the x-y coordinates of the sam-
pled pixels along the data series in the line graph, the
axes with tick marks and labels, the caption, etc.;
constructing the XML representation is the respon-
sibility of a Visual Extraction Module similar to the
one for bar charts described by Chester and Elzer
(2005). Section 4 presents our work on identifying
the additional propositions that elaborate on the in-
tended message and should be included in the sum-
mary. Section 5 discusses future work on realizing
the propositions in a natural language summary, and
Section 6 reviews related work in multimodal and
abstractive summarization.
3 Identifying a Line Graph?s Message
Research has shown that human subjects have a
strong tendency to use line graphs to portray trend
relationships, as well as a strong tendency to de-
scribe line graphs in terms of trends (Zacks and
Tversky, 1999). We analyzed a corpus of sim-
ple line graphs collected from various popular me-
dia including USA Today, Businessweek, and The
(Wilmington) News Journal, and identified a set of
10 high-level message categories that capture the
kinds of messages that are conveyed by a simple
line graph. Table 1 defines four of them. The com-
plete list can be found in (Wu et al, 2010b). Each
of these messages requires recognizing the visual
trend(s) in the depicted data. We use a support vec-
tor machine (SVM) to first segment the line graph
into a sequence of visually-distinguishable trends;
this sequence is then input into a Bayesian net-
work that reasons with evidence from the graphic
42
Intention Category Description
RT: Rising-trend There is a rising trend from <param1> to <param2>.
CT: Change-trend There is a <direction2> trend from <param2> to <param3> that is signifi-
cantly different from the <direction1> trend from <param1> to <param2>.
CTR:
Change-trend-return
There is a <direction1> trend from <param3> to <param4> that is different
from the <direction2> trend between <param2> and <param3> and reflects
a return to the kind of <direction1> trend from <param1> to <param2>.
BJ: Big-jump There was a very significant sudden jump in value between <param1> and
<param2> which may or may not be sustained.
Table 1: Four categories of High Level Messages for Line Graphs
in order to recognize the graphic?s intended mes-
sage. The next two subsections outline these
steps. (Our corpus of line graphs can be found at
www.cis.udel.edu/?carberry/Graphs/viewallgraphs.php)
3.1 Segmenting a Line Graph
A line graph can consist of many short, jagged
line segments, although a viewer of the graphic ab-
stracts from it a sequence of visually-distinguishable
trends. For example, the line graph in Figure 1 con-
sists of two trends: a relatively stable trend from
1900 to 1930 and a longer, increasing trend from
1930 to 2003. Our Graph Segmentation Module
(GSM) takes a top-down approach (Keogh et al,
2001) to generalize the line graph into sequences of
rising, falling, and stable segments, where a segment
is a series of connected data points. The GSM starts
with the entire line graph as a single segment and
uses a learned model to recursively decide whether
each segment should be split into two subsegments;
if the decision is to split, the division is made at the
point being the greatest distance from a straight line
between the two end points of the original segment.
This process is repeated on each subsegment until
no further splits are identified. The GSM returns a
sequence of straight lines representing a linear re-
gression of the points in each subsegment, where
each straight line is presumed to capture a visually-
distinguishable trend in the original graphic.
We used Sequential Minimal Optimization (Platt,
1999) in training an SVM to make segment split-
ting decisions. We chose to use an SVM because it
works well with high-dimensional data and a rela-
tively small training set, and lessens the chance of
overfitting by using the maximum margin separat-
ing hyperplane which minimizes the worst-case gen-
eralization errors (Tan et al, 2005). 18 attributes,
falling into two categories, were used in building
the data model (Wu et al, 2010a). The first cat-
egory captures statistical tests computed from the
sampled data points in the XML representation of
the graphic; these tests estimate how different the
segment is from a linear regression (i.e., a straight
line). The second category of attributes captures
global features of the graphic. For example, one
such attribute relates the segment size to the size of
the entire graphic, based on the hypothesis that seg-
ments comprising more of the total graph may be
stronger candidates for splitting than segments that
comprise only a small portion of the graph.
Our Graph Segmentation Module was trained
on a set of 649 instances that required a split/no-
split decision. Using leave-one-out cross validation,
in which one instance is used for testing and the
other 648 instances are used for training, our model
achieved an overall accuracy rate of 88.29%.
3.2 A Bayesian Recognition System
Once the line graph has been converted into
a sequence of visually-distinguishable trends, a
Bayesian network is built that captures the possible
intended messages for the graphic and the evidence
for or against each message. We adopted a Bayesian
network because it weighs different pieces of evi-
dence and assigns a probability to each candidate
intended message. The next subsections briefly out-
line the Bayesian network and its evaluation; details
can be found in (Wu et al, 2010b).
Structure of the Bayesian Network Figure 2
shows a portion of the Bayesian network constructed
for Figure 1. The top-level node in our Bayesian net-
work represents all of the high-level message cat-
43
Intended Message
... ...
... ...
...
CT?Suggestion?1
CT IntentionRT Intention
EvidenceOtherPointsAnnotated
Have SuggestionEvidence
Portion of GraphicEvidence EndpointsAnnotatedEvidence EvidenceSplittingPointsAnnotated
Adjective in CaptionEvidence
Verb in CaptionEvidence
Figure 2: A portion of the Bayesian network
egories. Each of these possible non-parameterized
message categories is repeated as a child of the
top-level node; this is purely for ease of repre-
sentation. Up to this point, the Bayesian net-
work is a static structure with conditional proba-
bility tables capturing the a priori probability of
each category of intended message. When given
a line graph to analyze, an extension of this net-
work is built dynamically according to the partic-
ulars of the graph itself. Candidate (concrete) in-
tended messages, having actual instantiated param-
eters, appear beneath the high-level message cat-
egory nodes. These candidates are introduced by
a Suggestion Generation Module; it dynamically
constructs all possible intended messages with con-
crete parameters using the visually-distinguishable
trends (rising, falling, or stable) identified by the
Graph Segmentation Module. For example, for each
visually-distinguishable trend, a Rising, Falling, or
Stable trend message is suggested; similary, for each
sequence of two visually-distinguishable trends, a
Change-trend message is suggested. For the graphic
in Figure 1, six candidate messages will be gener-
ated, including RT(1930, 2003), CT(1900, stable,
1930, rise, 2003) and BJ(1930, 2003) (see Table 1).
Entering Evidence into the Bayesian Network
Just as listeners use evidence to identify the intended
meaning of a speaker?s utterance, so also must a
viewer use evidence to recognize a graphic?s in-
tended message. The evidence for or against each
of the candidate intended messages must be entered
into the Bayesian network. We identified three kinds
of evidence that are used in line graphs: attention-
getting devices explicitly added by the graphic de-
signer (e.g., the annotation of a point with its value),
aspects of a graphic that are perceptually-salient
(e.g., the slope of a segment), and clues that sug-
gest the general message category (e.g., a verb [or
noun derived from a verb such as rebound] in the
caption which might indicate a Change-trend mes-
sage). The first two kinds of evidence are attached
to the Bayesian network as children of each candi-
date message node, such as the child nodes of ?CT-
Suggestion-1? in Figure 2. The third kind of evi-
dence is attached to the top level node as child nodes
named ?Verb in Caption Evidence? and ?Adjective
in Caption Evidence? in Figure 2.
Bayesian Network Inference We evaluated the
performance of our system for recognizing a line
graph?s intended message on a corpus of 215 line
graphs using leave-one-out cross validation in which
one graph is held out as a test graph and the con-
ditional probability tables for the Bayesian network
are computed from the other 214 graphs. Our sys-
tem recognized the correct intended message with
the correct parameters for 157 line graphs, resulting
in a 73.36% overall accuracy rate.
4 Identifying Elaborative Propositions
Once the intended message has been determined,
the next step is to identify additional important
informational propositions1 conveyed by the line
graph which should be included in the summary.
To accomplish this, we collected data to determine
what kinds of propositions in what situations were
deemed most important by human subjects, and de-
veloped rules designed to make similar assessments
based on the graphic?s intended message and visual
features present in the graphic.
4.1 Collecting Data from Human Subjects
Participants in our study were given 23 different line
graphs. With each graph, the subjects were provided
1We define a ?proposition? as a logical representation de-
scribing a relationship between one or more concepts, while a
?sentence? is a surface form realizing one or more propositions.
44
Figure 3: From ?This Cable Outfit Is Getting Tuned In?
in Businessweek magazine, Oct 4, 1999.
with an initial sentence describing the overall in-
tended message of the graphic. The subjects were
asked to add additional sentences so that the com-
pleted summary captured the most important infor-
mation conveyed by the graphic. The graphs were
presented to the subjects in different orders, and the
subjects completed as many graphs as they wanted
during the one hour study session. The set covered
the eight most prevalent of our intended message
categories and a variety of visual features. Roughly
half of the graphs were real-world examples from
the corpus used to train the Bayesian network in
Section 3.2, (e.g., Figure 3), with the others created
specifically to fill a gap in the coverage of intended
messages and visual features.
We collected a total of 998 summaries written by
69 human subjects for the 23 different line graphs.
The number of summaries we received per graph
ranged from 37 to 50. Most of the summaries were
between one and four sentences long, in addition to
the initial sentence (capturing the graphic?s intended
message) that was provided for each graph. A rep-
resentative sample summary collected for the line
graph shown in Figure 3 is as follows, with the initial
sentence provided to the study participants in italics:
This line graph shows a big jump in Blon-
der Tongue Laboratories stock price in
August ?99. The graph has many peaks
and valleys between March 26th 1999 to
August ?99 but maintains an average stock
price of around 6 dollars. However, in Au-
gust ?99 the stock price jumps sharply to
around 10 dollars before dropping quickly
to around 9 dollars by September 21st.
4.2 Extracting & Weighting Propositions
The data collected during the study was analyzed by
a human annotator who manually coded the propo-
sitions that appeared in each individual summary in
order to determine, for each graphic, which proposi-
tions were used and how often. For example, the set
of propositions coded in the sample summary from
Section 4.1 were:
? volatile(26Mar99, Aug99)
? average val(26Mar99, Aug99, $6)
? jump 1(Aug99, $10)
? steep(jump 1)
? decrease 1(Aug99, $10, 21Sep99, $9)
? steep(decrease 1)
From this information, we formulated a set of
rules governing the use of each proposition accord-
ing to the intended message category and various
visual features. Our intuition was that by finding
and exploiting a correlation between the intended
message category and/or certain visual features and
the propositions appearing most often in the human-
written summaries, our system could use these in-
dicators to determine which propositions are most
salient in new graphs. Our rules assign a weight
to each proposition in the situation captured by the
rule; these weights are based on the relative fre-
quency of the proposition being used in summaries
reflecting similar situations in our corpus study. The
rules are organized into three types:
1. Message Category-only (M):
IF M = m THEN select P with weight w1
2. Visual Feature-only (V):
IF V = v THEN select P with weight w2
3. Message Category + Visual Feature:
IF M = m and V = v
THEN select P with weight w2
We constructed type 1 (Message Category-only)
rules when a plurality of human-written summaries
45
in our corpus for all line graphs belonging to a
given message category contain the proposition. A
weight was assigned according to the frequency with
which the proposition was included. This weighting,
shown in Equation 1, is based on the proportion of
summaries for each line graph in the corpus having
intended message m and containing proposition P.
w1 =
n?
i=1
Pi
Si
(1)
In this equation, n is the number of line graphs in
this intended message category, Si is the total num-
ber of summaries for a particular line graph with this
intended message category, and Pi is the number of
these summaries that contain the proposition.
Intuitively, a proposition appearing in all sum-
maries for all graphs in a given message category
will have a weight of 1.0, while a proposition which
never appears will have a weight of zero. How-
ever, a proposition appearing in all summaries for
half of the graphs in a category, and rarely for the
other half of the graphs in that category, will have a
much lower weight than one which appears in half
of the summaries for all the graphs in that category,
even though the overall frequencies could be equal
for both. In this case, the message category is an
insufficient signal, and it is likely that the former
proposition is more highly correlated to some par-
ticular visual feature than to the message category.
Weights for type 2 and type 3 rules (Visual
Feature-only and Message Category + Visual Fea-
ture) are slightly more complicated in that they in-
volve a measure of degree for the associated visual
feature rather than simply its presence. The defini-
tion of this measure varies depending on the nature
of the visual feature (e.g., steepness of a trend line,
volatility), but all such measures range from zero to
one. Additionally, since the impact of a visual fea-
ture is a matter of degree, the weighting cannot rely
on a simple proportion of summaries containing the
proposition as in type 1 rules. Instead, it is neces-
sary to find the covariance between the magnitude of
the visual feature (|v|) and how frequently the corre-
sponding proposition is used (PS ) in the corpus sum-
maries for the n graphs having this visual feature, as
shown in Equation 2.
Cov(|v|,
P
S
) =
[(?n
i=1 |vi|
n
?n
i=1
Pi
Si
n
)
?
?n
i=1 |vi|
Pi
Si
n
] (2)
Then for a particular graphic whose magnitude for
this feature is |v|, we compute the weight w2 for the
proposition P as shown in Equation 3.
w2 = |v| ? Cov(|v|,
P
S
) (3)
This way, the stronger a certain visual feature is in a
given line graph, the higher the weight for the asso-
ciated proposition.
Type 3 rules (Message Category + Visual Fea-
ture) differ only from type 2 rules in that they are
restricted to a particular intended message category,
rather than any line graph having the visual feature
in question. For example, a proposition compar-
ing the slope of two trends may be appropriate for
a graph in the Change-trend message category, but
does not make sense for a line graph with only a sin-
gle trend (e.g., Rising-trend).
Once all propositions have been extracted and
ranked, these weights are passed along to a graph-
based content selection framework (Demir et al,
2010) that iteratively selects for inclusion in the ini-
tial summary those propositions which provide the
best coverage of the highest-ranked information.
4.3 Sample Rule Application
Figures 1 and 4 consist of two different line graphs
with the same intended message category: Change-
trend. Figure 1 shows a stable trend in annual sea
level difference from 1900 to 1930, followed by a
rising trend through 2003, while Figure 4 shows a
rising trend in Durango sales from 1997 to 1999,
followed by a falling trend through 2006. Proposi-
tions associated with type 1 rules will have the same
weights for both graphs, but propositions related to
visual features may have different weights. For ex-
ample, the graph in Figure 1 is far more volatile than
the graph in Figure 4. Thus, the type 2 rule associ-
ated with volatility will have a very high weight for
the graph in Figure 1 and will almost certainly be in-
cluded in the initial summary of that line graph (e.g.,
46
20062005200420032002
19971998
1999
20012000
200,000 150,000199
9: 189,840
70,6062006:
50,000100,000Declining Du
rango sales
0
Figure 4: From ?Chrysler: Plant had $800 million im-
pact? in The (Wilmington) News Journal, Feb 15, 2007.
?The values vary a lot...?, ?The trend is unstable...?),
possibly displacing a type 1 proposition that would
still appear in the summary for the graph in Figure 4.
5 Future Work
Once the propositions that should be included in the
summary have been selected, they must be coher-
ently organized and realized as natural language sen-
tences. We anticipate using the FUF/SURGE sur-
face realizer (Elhadad and Robin, 1996); our col-
lected corpus of line graph summaries provides a
large set of real-world expressions to draw from
when crafting the surface realization forms our sys-
tem will produce for the final-output summaries.
Our summarization methodology must also be eval-
uated. In particular, we must evaluate the rules for
identifying the additional informational propositions
that are used to elaborate the overall intended mes-
sage, and the quality of the summaries both in terms
of content and coherence.
6 Related Work
Image summarization has focused on constructing a
smaller image that contains the important content of
a larger image (Shi et al, 2009), selecting a set of
representative images that summarize a collection
of images (Baratis et al, 2008), or constructing a
new diagram that summarizes one or more diagrams
(Futrelle, 1999). However, all of these efforts pro-
duce an image as the end product, not a textual sum-
mary of the content of the image(s).
Ferres et al (2007) developed a system for con-
veying graphs to blind users, but it generates the
same basic information for each instance of a graph
type (e.g., line graphs) regardless of the individual
graph?s specific characteristics. Efforts toward sum-
marizing multimodal documents containing graph-
ics have included na??ve approaches relying on cap-
tions and direct references to the image in the text
(Bhatia et al, 2009), while content-based image
analysis and NLP techniques are being combined for
multimodal document indexing and retrieval in the
medical domain (Ne?ve?ol et al, 2009).
Jing and McKeown (1999) approached abstrac-
tive summarization as a text-to-text generation task,
modifying sentences from the original document via
editing and rewriting. There have been some at-
tempts to do abstractive summarization from seman-
tic models, but most of it has focused on text docu-
ments (Rau et al, 1989; Reimer and Hahn, 1988),
though Alexandersson (2003) used abstraction and
semantic modeling for speech-to-speech translation
and multilingual summary generation.
7 Discussion
Information graphics play an important communica-
tive role in popular media and cannot be ignored.
We have presented our methodology for construct-
ing a summary of a line graph. Our method is ab-
stractive, in that we identify the important high-level
knowledge conveyed by a graphic and capture it in
propositions to be realized in novel, coherent natu-
ral language sentences. The resulting summary can
be integrated with a summary of the document?s text
to produce a rich summary of the entire multimodal
document. In addition, the graphic?s summary can
be used along with a screen reader to provide sight-
impaired users with full access to the knowledge
conveyed by multimodal documents.
Acknowledgments
This work was supported in part by the National In-
stitute on Disability and Rehabilitation Research un-
der Grant No. H133G080047.
References
Jan Alexandersson. 2003. Hybrid Discourse Modeling
and Summarization for a Speech-to-Speech Transla-
tion System. Ph.D. thesis, Saarland University.
Evdoxios Baratis, Euripides Petrakis, and Evangelos Mil-
ios. 2008. Automatic web site summarization by im-
age content: A case study with logo and trademark
47
images. IEEE Transactions on Knowledge and Data
Engineering, 20(9):1195?1204.
Sumit Bhatia, Shibamouli Lahiri, and Prasenjit Mitra.
2009. Generating synopses for document-element
search. In Proceeding of the 18th ACM Conference
on Information and Knowledge Management, CIKM
?09, pages 2003?2006, Hong Kong, November. ACM.
Sandra Carberry, Stephanie Elzer, and Seniz Demir.
2006. Information graphics: an untapped resource for
digital libraries. In Proc. of the 29th Annual Int?l ACM
SIGIR Conf. on Research & Development in Informa-
tion Retrieval, SIGIR ?06, pages 581?588, Seattle, Au-
gust. ACM.
Daniel Chester and Stephanie Elzer. 2005. Getting com-
puters to see information graphics so users do not have
to. In Proceedings of the 15th International Sympo-
sium on Methodologies for Intelligent Systems (LNAI
3488), ISMIS 2005, pages 660?668, Saratoga Springs,
NY, June. Springer-Verlag.
Herbert Clark. 1996. Using Language. Cambridge Uni-
versity Press.
Seniz Demir, Sandra Carberry, and Kathleen F. McCoy.
2008. Generating textual summaries of bar charts.
In Proceedings of the 5th International Natural Lan-
guage Generation Conference, INLG 2008, pages 7?
15, Salt Fork, Ohio, June. ACL.
Seniz Demir, Sandra Carberry, and Kathleen F. Mc-
Coy. 2010. A discourse-aware graph-based content-
selection framework. In Proceedings of the 6th In-
ternational Natural Language Generation Conference,
INLG 2010, pages 17?26, Trim, Ireland, July. ACL.
Michael Elhadad and Jacques Robin. 1996. An overview
of SURGE: a re-usable comprehensive syntactic re-
alization component. In Proceedings of the 8th In-
ternational Natural Language Generation Workshop
(Posters & Demos), Sussex, UK, June. ACL.
Stephanie Elzer, Sandra Carberry, Daniel Chester, Seniz
Demir, Nancy Green, Ingrid Zukerman, and Keith
Trnka. 2005. Exploring and exploiting the limited
utility of captions in recognizing intention in infor-
mation graphics. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 223?230, Ann Arbor, June. ACL.
Stephanie Elzer, Sandra Carberry, and Ingrid Zukerman.
2011. The automated understanding of simple bar
charts. Artificial Intelligence, 175:526?555, February.
Leo Ferres, Petro Verkhogliad, Gitte Lindgaard, Louis
Boucher, Antoine Chretien, and Martin Lachance.
2007. Improving accessibility to statistical graphs: the
iGraph-Lite system. In Proc. of the 9th Int?l ACM
SIGACCESS Conf. on Computers & Accessibility, AS-
SETS ?07, pages 67?74, Tempe, October. ACM.
Robert P. Futrelle. 1999. Summarization of diagrams in
documents. In I. Mani and M. Maybury, editors, Ad-
vances in Automatic Text Summarization. MIT Press.
Barbara Grosz and Candace Sidner. 1986. Attention,
Intentions, and the Structure of Discourse. Computa-
tional Linguistics, 12(3):175?204.
Hongyan Jing and Kathleen R. McKeown. 1999. The
decomposition of human-written summary sentences.
In Proc. of the 22nd Annual Int?l ACM SIGIR Conf.
on Research & Development in Information Retrieval,
SIGIR ?99, pages 129?136, Berkeley, August. ACM.
Eamonn J. Keogh, Selina Chu, David Hart, and
Michael J. Pazzani. 2001. An online algorithm
for segmenting time series. In Proceedings of the
2001 IEEE International Conference on Data Mining,
ICDM ?01, pages 289?296, Washington, DC. IEEE.
Aure?lie Ne?ve?ol, Thomas M. Deserno, Ste?fan J. Darmoni,
Mark Oliver Gu?ld, and Alan R. Aronson. 2009. Nat-
ural language processing versus content-based image
analysis for medical document retrieval. Journal of the
American Society for Information Science and Tech-
nology, 60(1):123?134.
John C. Platt. 1999. Fast training of support vector
machines using sequential minimal optimization. In
B. Scho?lkopf, C. J. C. Burges, and A. J. Smola, editors,
Advances in kernel methods: support vector learning,
pages 185?208. MIT Press, Cambridge, MA, USA.
Lisa F. Rau, Paul S. Jacobs, and Uri Zernik. 1989. In-
formation extraction and text summarization using lin-
guistic knowledge acquisition. Information Process-
ing & Management, 25(4):419 ? 428.
Ulrich Reimer and Udo Hahn. 1988. Text condensation
as knowledge base abstraction. In Proceedings of the
4th Conference on Artificial Intelligence Applications,
CAIA ?88, pages 338?344, San Diego, March. IEEE.
Liang Shi, Jinqiao Wang, Lei Xu, Hanqing Lu, and
Changsheng Xu. 2009. Context saliency based im-
age summarization. In Proceedings of the 2009 IEEE
international conference on Multimedia and Expo,
ICME ?09, pages 270?273, New York. IEEE.
Pang-Ning Tan, Michael Steinbach, and Vipin Kumar.
2005. Introduction to Data Mining. Addison Wesley.
Peng Wu, Sandra Carberry, and Stephanie Elzer. 2010a.
Segmenting line graphs into trends. In Proceedings of
the 2010 International Conference on Artificial Intel-
ligence, ICAI ?10, pages 697?703, Las Vegas, July.
Peng Wu, Sandra Carberry, Stephanie Elzer, and Daniel
Chester. 2010b. Recognizing the intended message
of line graphs. In Proc. of the 6th Int?l Conf. on Dia-
grammatic Representation & Inference, Diagrams ?10,
pages 220?234, Portland. Springer-Verlag.
Jeff Zacks and Barbara Tversky. 1999. Bars and lines:
A study of graphic communication. Memory & Cog-
nition, 27:1073?1079.
48
Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 52?62,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Improving the Accessibility of Line Graphs in Multimodal Documents
Charles F. Greenbacker Peng Wu Sandra Carberry Kathleen F. McCoy
Stephanie Elzer* David D. McDonald? Daniel Chester Seniz Demir?
Dept. of Computer & Information Sciences, University of Delaware, USA
[charlieg|pwu|carberry|mccoy|chester]@cis.udel.edu
*Dept. of Computer Science, Millersville University, USA elzer@cs.millersville.edu
?SIFT LLC., Boston, Massachusetts, USA dmcdonald@sift.info
?TU?BI?TAK BI?LGEM, Gebze, Kocaeli, Turkey senizd@uekae.tubitak.gov.tr
Abstract
This paper describes our work on improv-
ing access to the content of multimodal docu-
ments containing line graphs in popular media
for people with visual impairments. We pro-
vide an overview of our implemented system,
including our method for recognizing and con-
veying the intended message of a line graph.
The textual description of the graphic gener-
ated by our system is presented at the most rel-
evant point in the document. We also describe
ongoing work into obtaining additional propo-
sitions that elaborate on the intended message,
and examine the potential benefits of analyz-
ing the text and graphical content together in
order to extend our system to produce sum-
maries of entire multimodal documents.
1 Introduction
Individuals with visual impairments have difficulty
accessing the information contained in multimodal
documents. Although screen-reading software can
render the text of the document as speech, the graph-
ical content is largely inaccessible. Here we con-
sider information graphics (e.g., bar charts, line
graphs) often found in popular media sources such
as Time magazine, Businessweek, and USA Today.
These graphics are typically intended to convey a
message that is an important part of the overall story,
yet this message is generally not repeated in the ar-
ticle text (Carberry et al, 2006). People who are
unable to see and assimilate the graphical material
will be left with only partial information.
While some work has addressed the accessibility
of scientific graphics through alternative means like
touch or sound (see Section 7), such graphs are de-
signed for an audience of experts trained to use them
for data visualization. In contrast, graphs in popular
media are constructed to make a point which should
be obvious without complicated scientific reasoning.
We are thus interested in generating a textual pre-
sentation of the content of graphs in popular media.
Other research has focused on textual descriptions
(e.g., Ferres et al (2007)); however in that work the
same information is included in the textual summary
for each instance of a graph type (i.e., all summaries
of line graphs contain the same sorts of informa-
tion), and the summary does not attempt to present
the overall intended message of the graph.
SIGHT (Demir et al, 2008; Elzer et al, 2011) is
a natural language system whose overall goal is pro-
viding blind users with interactive access to multi-
modal documents from electronically-available pop-
ular media sources. To date, the SIGHT project
has concentrated on simple bar charts. Its user in-
terface is implemented as a browser helper object
within Internet Explorer that works with the JAWS
screen reader. When the system detects a bar chart
in a document being read by the user, it prompts the
user to use keystrokes to request a brief summary of
the graphic capturing its primary contribution to the
overall communicative goal of the document. The
summary text can either be read to the user with
JAWS or read by the user with a screen magnifier
tool. The interface also enables the user to request
further information about the graphic, if desired.
However, SIGHT is limited to bar charts only.
In this work, we follow the methodology put forth
by SIGHT, but investigate producing a summary of
52
?
102468 1900?10
?20
?50?60
?70?80
?90?
03
?30?40
2000
10
8.9
1.979 inches over t
he past centu
ry. Annual d
ifference fro
m Seattle?s
In the seattl
e area, for e
xample, the
 Pacific Oce
an has risen
 nearly 
they are risi
ng about 0.0
4?0.09 of an
 inch each y
ear.
Sea levels fl
uctuate arou
nd the globe
, but oceano
graphers bel
ieve
Ocean level
s rising
1899 sea lev
el, in inches
:
Figure 1: From ?Worry flows from Arctic ice to tropical
waters? in USA Today, May 31, 2006.
line graphs. Line graphs have different discourse
goals and communicative signals than bar charts,1
and thus require significantly different processing.
In addition, our work addresses the issue of coher-
ent placement of a graphic?s summary when reading
the text to the user and considers the summarization
of entire documents ? not just their graphics.
2 Message Recognition for Line Graphs
This section provides an overview of our imple-
mented method for identifying the intended message
of a line graph. In processing a line graph, a vi-
sual extraction module first analyzes the image file
and produces an XML representation which fully
specifies the graphic (including the beginning and
ending points of each segment, any annotations on
points, axis labels, the caption, etc.). To identify
the intended message of a line graph consisting of
many short, jagged segments, we must generalize
it into a sequence of visually-distinguishable trends.
This is performed by a graph segmentation module
which uses a support vector machine and a variety
of attributes (including statistical tests) to produce a
model that transforms the graphic into a sequence of
straight lines representing visually-distinguishable
trends. For example, the line graph in Figure 1 is
divided into a stable trend from 1900 to 1930 and a
rising trend from 1930 to 2003. Similarly, the line
graph in Figure 2 is divided into a rising trend from
1Bar charts present data as discrete bars and are often used
to compare entities, while line graphs contain continuous data
series and are designed to portray longer trend relationships.
20062005200420032002
19971998
1999
20012000
200,000 150,0001999
: 189,840
70,6062006:
50,000100,000Declining Dur
ango sales
0
Figure 2: From ?Chrysler: Plant had $800 million im-
pact? in The (Wilmington) News Journal, Feb 15, 2007.
1997 to 1999 and a falling trend from 1999 to 2006.
In analyzing a corpus of around 100 line graphs
collected from several popular media sources, we
identified 10 intended message categories (includ-
ing rising-trend, change-trend, change-trend-return,
and big-jump, etc.), that seem to capture the kinds
of high-level messages conveyed by line graphs. A
suggestion generation module uses the sequence of
trends identified in the line graph to construct all
of its possible candidate messages in these message
categories. For example, if a graph contains three
trends, several candidate messages are constructed,
including two change-trend messages (one for each
adjacent pair of trends), a change-trend-return mes-
sage if the first and third trends are of the same type
(rising, falling, or stable), as well as a rising, falling,
or stable trend message for each individual trend.
Next, various communicative signals are ex-
tracted from the graphic, including visual features
(such as a point annotated with its value) that draw
attention to a particular part of the line graph, and
linguistic clues (such as the presence of certain
words in the caption) that suggest a particular in-
tended message category. Figure 2 contains several
such signals, including two annotated points and the
word declining in its caption. Next, a Bayesian net-
work is built to estimate the probability of the can-
didate messages; the extracted communicative sig-
nals serve as evidence for or against each candidate
message. For Figure 2, our system produces change-
trend(1997, rise, 1999, fall, 2006) as the logical rep-
resentation of the most probable intended message.
Since the dependent axis is often not explicitly la-
beled, a series of heuristics are used to identify an
appropriate referent, which we term the measure-
ment axis descriptor. In Figure 2, the measurement
axis descriptor is identified as durango sales. The
53
intended message and measurement axis descriptor
are then passed to a realization component which
uses FUF/SURGE (Elhadad and Robin, 1996) to
generate the following initial description:
This graphic conveys a changing trend in
durango sales, rising from 1997 to 1999
and then falling to 2006.
3 Identifying a Relevant Paragraph
In presenting a multimodal document to a user via a
screen reader, if the author does not specify a read-
ing order in the accessibility preferences, it is not
entirely clear where the description of the graph-
ical content should be given. The text of scien-
tific articles normally makes explicit references to
any graphs contained in the document; in this case,
it makes sense to insert the graphical description
alongside the first such reference. However, popular
media articles rarely contain explicit references to
graphics. We hypothesize that describing the graphi-
cal content together with the most relevant portion of
the article text will result in a more coherent presen-
tation. Results of an experiment described in Sec-
tion 3.3 suggest the paragraph which is geograph-
ically closest to the graphic is very often not rele-
vant. Thus, our task becomes identifying the portion
of the text that is most relevant to the graph.
We have developed a method for identifying the
most relevant paragraph by measuring the similarity
between the graphic?s textual components and the
content of each individual paragraph in the docu-
ment. An information graphic?s textual components
may consist of a title, caption, and any additional
descriptions it contains (e.g., the five lines of text in
Figure 1 beneath the caption Ocean levels rising).
An initial method (P-KL) based on KL divergence
measures the similarity between a paragraph and the
graphic?s textual component; a second method (P-
KLA) is an extension of the first that incorporates
an augmented version of the textual component.
3.1 Method P-KL: KL Divergence
Kullback-Leibler (KL) divergence (Kullback, 1968)
is widely used to measure the similarity between two
language models. It can be expressed as:
DKL(p||q) =
?
i?V
p(i)log
p(i)
q(i)
where i is the index of a word in vocabulary V , and
p and q are two distributions of words. Liu et al
(Liu and Croft, 2002) applied KL divergence to text
passages in order to improve the accuracy of docu-
ment retrieval. For our task, p is a smoothed word
distribution built from the line graph?s textual com-
ponent, and q is another smoothed word distribution
built from a paragraph in the article text. Smoothing
addresses the problem of zero occurrences of a word
in the distributions. We rank the paragraphs by their
KL divergence scores from lowest to highest, since
lower scores indicate a higher similarity.
3.2 Method P-KLA: Using Augmented Text
In analyzing paragraphs relevant to the graphics, we
realized that they included words that were germane
to describing information graphics in general, but
not related to the domains of individual graphs. This
led us to build a set of ?expansion words? that tend to
appear in paragraphs relevant to information graph-
ics. If we could identify domain-independent terms
that were correlated with information graphics in
general, these expansion words could then be added
to the textual component of a graphic when measur-
ing its similarity to a paragraph in the article text.
We constructed the expansion word set using an
iterative process. The first step is to use P-KL to
identify m pseudo-relevant paragraphs in the cor-
responding document for each graphic in the train-
ing set (the current implementation uses m = 3).
This is similar to pseudo-relevance feedback used in
IR (Zhai, 2008), except only a single query is used
in the IR application, whereas we consider many
pairs of graphics and documents to obtain an ex-
pansion set applicable to any subsequent informa-
tion graphic. Given n graphics in the training set,
we identify (up to) m ? n relevant paragraphs.
The second step is to extract a set of words re-
lated to information graphics from these m ?n para-
graphs. We assume the collection of pseudo-relevant
paragraphs was generated by two models, one pro-
ducing words relevant to the information graphics
and another producing words relevant to the topics
of the individual documents. Let Wg represent the
word frequency vector yielding words relevant to
the graphics, Wa represent the word frequency vec-
tor yielding words relevant to the document topics,
and Wp represent the word frequency vector of the
54
pseudo-relevant paragraphs. We compute Wp from
the pseudo-relevant paragraphs themselves, and we
estimate Wa using the word frequencies from the
article text in the documents. Finally, we compute
Wg by filtering-out the components ofWa fromWp.
This process is related to the work by Widdows
(2003) on orthogonal negation of vector spaces.
The task can be formulated as follows:
1. Wp = ?Wa + ?Wg where ? > 0 and ? > 0,
which means the word frequency vector for
the pseudo-relevant paragraphs is a linear com-
bination of the background (topic) word fre-
quency vector and the graphic word vector.
2. < Wa,Wg >= 0 which means the background
word vector is orthogonal to the graph descrip-
tion word vector, under the assumption that the
graph description word vector is independent of
the background word vector and that these two
share minimal information.
3. Wg is assumed to be a unit vector, since we are
only interested in the relative rank of the word
frequencies, not their actual values.
Solving the above equations, we obtain:
? =
< Wp,Wa >
< Wa,Wa >
Wg = normalized
(
Wp ?
< Wp,Wa >
< Wa,Wa >
?Wa
)
After computing Wg, we use WordNet to filter-
out words having a predominant sense other than
verb or adjective, under the assumption that nouns
will be mainly relevant to the domains or topics
of the graphs (and are thus ?noise?) whereas we
want a general set of words (e.g., ?increasing?)
that are typically used when describing the data in
any graph. As a rough estimate of whether a word
is predominantly a verb or adjective, we determine
whether there are more verb and adjective senses of
the word in WordNet than there are noun senses.
Next, we rank the words in the filteredWg accord-
ing to frequency and select the k most frequent as
our expansion word list (we used k = 25 in our ex-
periments). The two steps (identifyingm?n pseudo-
relevant paragraphs and then extracting a word list of
size k to expand the graphics? textual components)
are applied iteratively until convergence occurs or
minimal changes are observed between iterations.
In addition, parameters of the intended message
that represent points on the x-axis capture domain-
specific content of the graphic?s communicative
goal. For example, the intended message of the line
graph in Figure 1 conveys a changing trend from
1900 to 2003 with the change occurring in 1930. To
help identify relevant paragraphs mentioning these
years, we also add these parameters of the intended
message to the augmented word list.
The result of this process is the final expansion
word list used in method P-KLA. Because the tex-
tual component may be even shorter than the expan-
sion word list, we do not add a word from the expan-
sion word list to the textual component unless the
paragraph being compared also contains this word.
3.3 Results of P-KL and P-KLA
334 training graphs with their accompanying articles
were used to build the expansion word set. A sepa-
rate set of 66 test graphs and articles was analyzed
by two human annotators who identified the para-
graphs in each document that were most relevant to
its associated information graphic, ranking them in
terms of relevance. On average, annotator 1 selected
2.00 paragraphs and annotator 2 selected 1.71 para-
graphs. The annotators agreed on the top ranked
paragraph for only 63.6% of the graphs. Consid-
ering the agreement by chance, we can calculate the
kappa statistic as 0.594. This fact shows that the
most relevant paragraph is not necessarily obvious
and multiple plausible options may exist.
We applied both P-KL and P-KLA to the test set,
with each method producing a list of the paragraphs
ranked by relevance. Since our goal is to provide
the summary of the graphic at a suitable point in the
article text, two evaluation criteria are appropriate:
1. TOP: the method?s success rate in selecting
the most relevant paragraph, measured as how
often it chooses the paragraph ranked highest
by either of the annotators
2. COVERED: the method?s success rate in se-
lecting a relevant paragraph, measured as how
often it chooses one of the relevant paragraphs
identified by the annotators
Table 1 provides the success rates of both of our
methods for the TOP and COVERED criteria, along
with a simple baseline that selected the paragraph
55
geographically-closest to the graphic. These results
show that both methods outperform the baseline,
and that P-KLA further improves on P-KL. P-KLA
selects the best paragraph in 60.6% of test cases,
and selects a relevant paragraph in 71.2% of the
cases. For both TOP and COVERED, P-KLA nearly
doubles the baseline success rate. The improve-
ment of P-KLA over P-KL suggests that our expan-
sion set successfully adds salient words to the tex-
tual component. A one-sided Z-test for proportion
based on binomial distribution is shown in Table 1
and indicates that the improvements of P-KL over
the baseline and P-KLA over P-KL are statistically-
significant at the 0.05 level across both criteria. The
Z-test is calculated as:
p? p0
?
p0(1?p0)
n
where p0 is the lower result and p is the improved
result. The null hypothesis is H0 : p = p0 and the
alternative hypothesis is H1 : p > p0.
3.4 Using relevant paragraph identification to
improve the accessibility of line graphs
Our system improves on SIGHT by using method
P-KLA to identify the paragraph that is most rele-
vant to an information graphic. When this paragraph
is encountered, the user is asked whether he or she
would like to access the content of the graphic. For
example, our system identifies the following para-
graph as most relevant to Figure 2:
Doing so likely would require the com-
pany to bring in a new model. Sales of
the Durango and other gas-guzzling SUVs
have slumped in recent years as prices at
the pump spiked.
In contrast, the geographically-closest paragraph has
little relevance to the graphic:
?We have three years to prove to them
we need to stay open,? said Sam Latham,
president of the AFL-CIO in Delaware,
who retired from Chrysler after 39 years.
4 Identifying Additional Propositions
After the intended message has been identified, the
system next looks to identify elaborative informa-
tional propositions that are salient in the graphic.
These additional propositions expand on the initial
description of the graph by filling-in details about
the knowledge being conveyed (e.g., noteworthy
points, properties of trends, visual features) in order
to round-out a summary of the graphic.
We collected a corpus of 965 human-written sum-
maries for 23 different line graphs to discover which
propositions were deemed most salient under varied
conditions.2 Subjects received an initial description
of the graph?s intended message, and were asked to
write additional sentences capturing the most impor-
tant information conveyed by the graph. The propo-
sitions appearing in each summary were manually
coded by an annotator to determine which were most
prevalent. From this data, we developed rules to
identify important propositions in new graphs. The
rules assign weights to propositions indicating their
importance, and the weights can be compared to de-
cide which propositions to include in a summary.
Three types of rules were built. Type-1 (message
category-only) rules were created when a plurality
of summaries for all graphs having a given intended
message contained the same proposition (e.g., pro-
vide the final value for all rising-trend and falling-
trend graphs). Weights for type-1 rules were based
on the frequency with which the proposition ap-
peared in summaries for graphs in this category.
Type-2 (visual feature-only) rules were built when
there was a correlation between a visual feature and
the use of a proposition describing that feature, re-
gardless of the graph?s message category (e.g., men-
tion whether the graph is highly volatile). Type-2
rule weights are a function of the covariance be-
tween the magnitude of the visual feature (e.g., de-
gree of volatility) and the proportion of summaries
mentioning this proposition for each graph.
For propositions associated with visual features
linked to a particular message category (e.g., de-
scribe the trend immediately following a big-jump
or big-fall when it terminates prior to the end of the
graph), we constructed Type-3 (message category
+ visual feature) rules. Type-3 weights were cal-
culated just like Type-2 weights, except the graphs
were limited to the given category.
As an example of identifying additional proposi-
2This corpus is described in greater detail by Greenbacker et
al. (2011) and is available at www.cis.udel.edu/~mccoy/corpora
56
closest P-KL significance level over closest P-KLA significance level over P-KL
TOP 0.272 0.469 (z = 3.5966, p < 0.01) 0.606 (z = 2.2303, p < 0.025)
COVERED 0.378 0.606 (z = 3.8200, p < 0.01) 0.712 (z = 1.7624, p < 0.05)
Table 1: Success rates for baseline method (?closest?), P-KL, and P-KLA using the TOP and COVERED criteria.
tions, consider Figures 1 and 2. Both line graphs
belong to the same intended message category:
change-trend. However, the graph in Figure 1 is far
more volatile than Figure 2, and thus it is likely that
we would want to mention this proposition (i.e., ?the
graph shows a high degree of volatility...?) in a sum-
mary of Figure 1. By finding the covariance between
the visual feature (i.e., volatility) and the frequency
with which a corresponding proposition was anno-
tated in the corpus summaries, a Type-2 rule assigns
a weight to this proposition based on the magnitude
of the visual feature. Thus, the volatility proposi-
tion will be weighted strongly for Figure 1, and will
likely be selected to appear in the initial summary,
while the weight for Figure 2 will be very low.
5 Integrating Text and Graphics
Until now, our system has only produced summaries
for the graphical content of multimodal documents.
However, a user might prefer a summary of the en-
tire document. Possible use cases include examining
this summary to decide whether to invest the time re-
quired to read a lengthy article with a screen reader,
or simply addressing the common problem of having
too much material to review in too little time (i.e.,
information overload). We are developing a system
extension that will allow users to request summaries
of arbitrary length that cover both the text and graph-
ical content of a multimodal document.
Graphics in popular media convey a message that
is generally not repeated in the article text. For ex-
ample, the March 3, 2003 issue of Newsweek con-
tained an article entitled, ?The Black Gender Gap,?
which described the professional achievements of
black women. It included a line graph (Figure 3)
showing that the historical gap in income equality
between white women and black women had been
closed, yet this important message appears nowhere
in the article text. Other work in multimodal doc-
ument summarization has relied on image captions
and direct references to the graphic in the text (Bha-
tia et al, 2009); however, these textual elements do
Figure 3: From ?The Black Gender Gap? in Newsweek,
Mar 3, 2003.
not necessarily capture the message conveyed by in-
formation graphics in popular media. Thus, the user
may miss out on an essential component of the over-
all communicative goal of the document if the sum-
mary covers only material presented in the text.
One approach to producing a summary of the en-
tire multimodal document might be to ?concatenate?
a traditional extraction-based summary of the text
(Kupiec et al, 1995; Witbrock and Mittal, 1999)
with the description generated for the graphics by
our existing system. The summary of the graphi-
cal content could be simply inserted wherever it is
deemed most relevant in the text summary. How-
ever, such an approach would overlook the relation-
ships and interactions between the text and graphical
content. The information graphics may make certain
concepts mentioned in the text more salient, and vice
versa. Unless we consider the contributions of both
the text and graphics together during the content se-
lection phase, the most important information might
not appear in the summary of the document.
Instead, we must produce a summary that inte-
grates the content conveyed by the text and graphics.
We contend that this integration must occur at the se-
mantic level if it is to take into account the influence
of the graphic?s content on the salience of concepts
in the text and vice versa. Our tack is to first build
a single semantic model of the concepts expressed
in both the article text and information graphics, and
then use this model as the basis for generating an
abstractive summary of the multimodal document.
57
Drawing from a model of the semantic content of the
document, we select as many or as few concepts as
we wish, at any level of detail, to produce summaries
of arbitrary length. This will permit the user to re-
quest a quick overview in order to decide whether to
read the original document, or a more comprehen-
sive synopsis to obtain the most important content
without having to read the entire article.
5.1 Semantic Modeling of Multimodal
Documents
Content gathered from the article text by a seman-
tic parser and from the information graphics by
our graph understanding system is combined into
a single semantic model based on typed, struc-
tured objects organized under a foundational ontol-
ogy (McDonald, 2000a). For the semantic pars-
ing of text, we use Sparser (McDonald, 1992), a
bottom-up, phrase-structure-based chart parser, op-
timized for semantic grammars and partial parsing.3
Using a built-in model of core English grammar
plus domain-specific grammars, Sparser extracts in-
formation from the text and produces categorized
objects as a semantic representation (McDonald,
2000b). The intended message and salient additional
propositions identified by our system for the infor-
mation graphics are decomposed and added to the
model constructed by Sparser.4
Model entries contain slots for attributes in the
concept category?s ontology definition (fillable by
other concepts or symbols), the original phrasings
mentioning this concept in the text (represented as
parameterized synchronous TAG derivation trees),
and markers recording document structure (i.e.,
where in the text [including title, headings, etc.] or
graphic the concept appeared). Figure 4 shows some
of the information contained in a small portion of
the semantic model built for an article entitled ?Will
Medtronic?s Pulse Quicken?? from the May 29,
2006 edition of Businessweek magazine5, which in-
cluded a line graph. Nodes correspond to concepts
3https://github.com/charlieg/Sparser
4Although the framework is general enough to accommo-
date any modality (e.g., images, video) given suitable seman-
tic analysis tools, our prototype implementation focuses on bar
charts and line graphs analyzed by SIGHT.
5http://www.businessweek.com/magazine/
content/06_22/b3986120.htm
and edges denote relationships between concepts;
dashed lines indicate links to concepts not shown in
this figure. Nodes are labelled with the name of the
conceptual category they instantiate, and a number
to distinguish between individuals. The middle of
each box displays the attributes of the concept, while
the bottom portion shows some of the original text
phrasings. Angle brackets (<>) note references to
other concepts, and hash marks (#) indicate a sym-
bol that has not been instantiated as a concept.
P1S1: "medical device
    giant Medtronic"
P1S5: "Medtronic"
Name: "Medtronic"
Stock: "MDT"
Industry: (#pacemakers,
    #defibrillators,
    #medical devices)
Company1
P1S4: "Joanne
    Wuensch"
P1S7: "Wuensch"
FirstName: "Joanne"
LastName: "Wuensch"
Person1
P1S4: "a 12-month
    target of 62"
Person: <Person 1>
Company: <Company 1>
Price: $62.00
Horizon: #12_months
TargetStockPrice1
Figure 4: Detail of model for Businessweek article.
5.2 Rating Content in Semantic Models
The model is then rated to determine which items are
most salient. The concepts conveying the most in-
formation and having the most connections to other
important concepts in the model are the ones that
should be chosen for the summary. The importance
of each concept is rated according to a measure of
information density (ID) involving several factors:6
Saturation Level Completeness of attributes in
model entry: a concept?s filled-in slots (f ) vs. its
total slots (s), and the importance of the concepts
(ci) filling those slots:
f
s ? log(s) ?
?f
i=1 ID(ci)
Connectedness Number of connections (n) with
other concepts (cj), and the importance of these con-
nected concepts:
?n
j=1 ID(cj)
Frequency Number of observed phrasings (e) re-
alizing the concept in text of the current document
Prominence in Text Prominence based on docu-
ment structure (WD) and rhetorical devices (WR)
Graph Salience Salience assessed by the graph
understanding system (WG) ? only applies to con-
cepts appearing in the graphics
6The first three factors are similar to the dominant slot
fillers, connectivity patterns, and frequency criteria described
by Reimer and Hahn (1988).
58
Saturation corresponds to the completeness of the
concept in the model. The more attribute slots that
are filled, the more we know about a particular con-
cept instance. However, this measure is highly sen-
sitive to the degree of detail provided in the seman-
tic grammar and ontology class definition (whether
created by hand or automatically). A concept having
two slots, both of which are filled-out, is not neces-
sarily more important than a concept with only 12
of its 15 slots filled. The more important a concept
category is in a given domain, the more detailed its
ontology class definition will likely be. Thus, we
can assume that a concept definition having a dozen
or more slots is, broadly speaking, more important
in the domain than a less well-defined concept hav-
ing only one or two slots. This insight is the basis of
a normalization factor (log(s)) used in ID.
Saturation differs somewhat from repetition in
that it attempts to measure the amount of informa-
tion associated with a concept, rather than simply
the number of times a concept is mentioned in the
text. For example, a news article about a proposed
law might mention ?Washington? several times, but
the fact that the debate took place in Washington,
D.C. is unlikely to be an important part of the article.
However, the key provisions of the bill, which may
individually be mentioned only once, are likely more
important as a greater amount of detail is provided
concerning them. Simple repetition is not necessar-
ily indicative of the importance of a concept, but if a
large amount of information is provided for a given
concept, it is safe to assume the concept is important
in the context of that document.
Document structure (WD) is another important
clue in determining which elements of a text are
important enough to include in a summary (Marcu,
1997). If a concept is featured prominently in the
title, or appears in the first or final paragraphs, it is
likely more important than a concept buried in the
middle of the document. Importance is also affected
by certain rhetorical devices (WR) which serve to
highlight particular concepts. Being used in an id-
iom, or compared to another concept by means of
juxtaposition suggests that a given concept may hold
special significance. Finally, the weights assigned
by our graph understanding system for the additional
propositions identified in the graphics are incorpo-
rated into the ID of the concepts involved as WG.
5.3 Selecting Content for a Summary
To select concepts for inclusion in the summary,
the model will then be passed to a discourse-aware
graph-based content selection framework (Demir et
al., 2010), which selects concepts one at a time
and iteratively re-weights the remaining items so
as to include related concepts and avoid redun-
dancy. This algorithm incorporates PageRank (Page
et al, 1999), but with several modifications. In ad-
dition to centrality assessment based on relation-
ships between concepts, it includes apriori impor-
tance nodes enabling us to incorporate concept com-
pleteness, number of expressions, document struc-
ture, and rhetorical devices. More importantly from
a summary generation perspective, the algorithm it-
eratively picks concepts one at a time, and re-ranks
the remaining entries by increasing the weight of re-
lated items and discounting redundant ones. This
allows us to select concepts that complement each
other while simultaneously avoiding redundancy.
6 Generating an Abstractive Summary of
a Multimodal Document
Figure 4 shows the two most important concepts
(Company1 & Person1) selected from the Medtronic
article in Section 5.1. Following McDonald and
Greenbacker (2010), we use the phrasings observed
by the parser as the ?raw material? for expressing
these selected concepts. Reusing the original phras-
ings reduces the reliance on built-in or ?canned?
constructions, and allows the summary to reflect the
style of the original text. The derivation trees stored
in the model to realize a particular concept may use
different syntactic constituents (e.g., noun phrases,
verb phrases). Multiple trees are often available for
each concept, and we must select particular trees that
fit together to form a complete sentence.
The semantic model also contains concepts rep-
resenting propositions extracted from the graphics,
as well as relationships connecting these graphical
concepts with those derived from the text, and there
are no existing phrasings in the original document
that can be reused to convey this graphical content.
However, the set of proposition types that can be ex-
tracted from the graphics is finite. To ensure that we
have realizations for every concept in our model, we
create TAG derivation trees for each type of graphi-
59
cal proposition. As long as realizations are supplied
for every proposition that can be decomposed in the
model, our system will never be stuck with a concept
without the means to express it.
The set of expressions is augmented by many
built-in realizations for common semantic relation-
ships (e.g., ?is-a,? ?has-a?), as well as expressions
inherited from other conceptual categories in the hi-
erarchy. If the observed expressions are retained as
the system analyzes multiple documents over time,
making these realizations available for later use by
concepts in the same category, the variety of utter-
ances we can generate is increased greatly.
By using synchronous TAG trees, we know that
the syntactic realizations of two semantically-related
concepts will fit together syntactically (via substitu-
tion or adjunction). However, the concepts selected
for the summary of the Medtronic article (Com-
pany1 & Person1), are not directly connected in the
model. To produce a single summary sentence for
these two concepts, we must find a way of express-
ing them together with the available phrasings. This
can be accomplished by using an intermediary con-
cept that connects both of the selected items in the
semantic model, in order to ?bridge the gap? be-
tween them. In this example, a reasonable option
would be TargetStockPrice1, one of the many con-
cepts linking Company1 and Person1. Combining
original phrasings from all three concepts (via sub-
stitution and adjunction operations on the underly-
ing TAG trees), along with a ?built-in? realization
inherited by the TargetStockPrice category (a sub-
type of Expectation), yields this surface form:
Wuensch expects a 12-month target of 62
for medical device giant Medtronic.
7 Related Work
Research into providing alternative access to graph-
ics has taken both verbal and non-verbal approaches.
Kurze (1995) presented a verbal description of the
properties (e.g., diagram style, number of data sets,
range and labels of axes) of business graphics. Fer-
res et al (2007) produced short descriptions of the
information in graphs using template-driven genera-
tion based on the graph type. The SIGHT project
(Demir et al, 2008; Elzer et al, 2011) generated
summaries of the high-level message content con-
veyed by simple bar charts. Other modalities, like
sound (Meijer, 1992; Alty and Rigas, 1998; Choi
and Walker, 2010) and touch (Ina, 1996; Krufka et
al., 2007), have been used to impart graphics via a
substitute medium. Yu et al (2002) and Abu Doush
et al (2010) combined haptic and aural feedback,
enabling users to navigate and explore a chart.
8 Discussion
This paper presented our system for providing ac-
cess to the full content of multimodal documents
with line graphs in popular media. Such graph-
ics generally have a high-level communicative goal
which should constitute the core of a graphic?s sum-
mary. Rather than providing this summary at the
point where the graphic is first encountered, our sys-
tem identifies the most relevant paragraph in the
article and relays the graphic?s summary at this
point, thus increasing the presentation?s coherence.
System extensions currently in development will
provide a more integrative and accessible way for
visually-impaired readers to experience multimodal
documents. By producing abstractive summaries of
the entire document, we reduce the amount of time
and effort required to assimiliate the information
conveyed by such documents in popular media.
Several tasks remain as future work. The intended
message descriptions generated by our system need
to be evaluated by both sighted and non-sighted hu-
man subjects for clarity and accuracy. We intend
to test our hypothesis that graphics ought to be de-
scribed alongside the most relevant part of the text
by performing an experiment designed to determine
the presentation order preferred by people who are
blind. The rules developed to identify elaborative
propositions also must be validated by a corpus or
user study. Finally, once the system is fully imple-
mented, the abstractive summaries generated for en-
tire multimodal documents will need to be evaluated
by both sighted and sight-impaired judges.
Acknowledgments
This work was supported in part by the by the Na-
tional Institute on Disability and Rehabilitation Re-
search under grant H133G080047 and by the Na-
tional Science Foundation under grant IIS-0534948.
60
References
Iyad Abu Doush, Enrico Pontelli, Tran Cao Son, Dominic
Simon, and Ou Ma. 2010. Multimodal presenta-
tion of two-dimensional charts: An investigation using
Open Office XML and Microsoft Excel. ACM Trans-
actions on Accessible Computing (TACCESS), 3:8:1?
8:50, November.
James L. Alty and Dimitrios I. Rigas. 1998. Communi-
cating graphical information to blind users using mu-
sic: the role of context. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems,
CHI ?98, pages 574?581, Los Angeles, April. ACM.
Sumit Bhatia, Shibamouli Lahiri, and Prasenjit Mitra.
2009. Generating synopses for document-element
search. In Proceeding of the 18th ACM Conference
on Information and Knowledge Management, CIKM
?09, pages 2003?2006, Hong Kong, November. ACM.
Sandra Carberry, Stephanie Elzer, and Seniz Demir.
2006. Information graphics: an untapped resource for
digital libraries. In Proceedings of the 29th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR ?06,
pages 581?588, Seattle, August. ACM.
Stephen H. Choi and Bruce N. Walker. 2010. Digitizer
auditory graph: making graphs accessible to the visu-
ally impaired. In Proceedings of the 28th International
Conference on Human Factors in Computing Systems,
CHI ?10, pages 3445?3450, Atlanta, April. ACM.
Seniz Demir, Sandra Carberry, and Kathleen F. McCoy.
2008. Generating textual summaries of bar charts.
In Proceedings of the 5th International Natural Lan-
guage Generation Conference, INLG 2008, pages 7?
15, Salt Fork, Ohio, June. ACL.
Seniz Demir, Sandra Carberry, and Kathleen F. Mc-
Coy. 2010. A discourse-aware graph-based content-
selection framework. In Proceedings of the 6th In-
ternational Natural Language Generation Conference,
INLG 2010, pages 17?26, Trim, Ireland, July. ACL.
Michael Elhadad and Jacques Robin. 1996. An overview
of SURGE: a re-usable comprehensive syntactic re-
alization component. In Proceedings of the 8th In-
ternational Natural Language Generation Workshop
(Posters and Demonstrations), Sussex, UK, June.
ACL.
Stephanie Elzer, Sandra Carberry, and Ingrid Zukerman.
2011. The automated understanding of simple bar
charts. Artificial Intelligence, 175:526?555, February.
Leo Ferres, Petro Verkhogliad, Gitte Lindgaard, Louis
Boucher, Antoine Chretien, and Martin Lachance.
2007. Improving accessibility to statistical graphs: the
iGraph-Lite system. In Proceedings of the 9th Inter-
national ACM SIGACCESS Conference on Computers
and Accessibility, ASSETS ?07, pages 67?74, Tempe,
October. ACM.
Charles F. Greenbacker, Sandra Carberry, and Kathleen F.
McCoy. 2011. A corpus of human-written summaries
of line graphs. In Proceedings of the EMNLP 2011
Workshop on Language Generation and Evaluation,
UCNLG+Eval, Edinburgh, July. ACL. (to appear).
Satoshi Ina. 1996. Computer graphics for the blind. SIG-
CAPH Newsletter on Computers and the Physically
Handicapped, pages 16?23, June. Issue 55.
Stephen E. Krufka, Kenneth E. Barner, and Tuncer Can
Aysal. 2007. Visual to tactile conversion of vector
graphics. IEEE Transactions on Neural Systems and
Rehabilitation Engineering, 15(2):310?321, June.
Solomon Kullback. 1968. Information Theory and
Statistics. Dover, revised 2nd edition.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings
of the 18th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, SIGIR ?95, pages 68?73, Seattle, July. ACM.
Martin Kurze. 1995. Giving blind people access
to graphics (example: Business graphics). In Pro-
ceedings of the Software-Ergonomie ?95 Workshop
on Nicht-visuelle graphische Benutzungsoberfla?chen
(Non-visual Graphical User Interfaces), Darmstadt,
Germany, February.
Xiaoyong Liu and W. Bruce Croft. 2002. Passage re-
trieval based on language models. In Proceedings of
the eleventh international conference on Information
and knowledge management, CIKM ?02, pages 375?
382.
Daniel C. Marcu. 1997. The Rhetorical Parsing, Summa-
rization, and Generation of Natural Language Texts.
Ph.D. thesis, University of Toronto, December.
David D. McDonald and Charles F. Greenbacker. 2010.
?If you?ve heard it, you can say it? - towards an ac-
count of expressibility. In Proceedings of the 6th In-
ternational Natural Language Generation Conference,
INLG 2010, pages 185?190, Trim, Ireland, July. ACL.
David D. McDonald. 1992. An efficient chart-based
algorithm for partial-parsing of unrestricted texts. In
Proceedings of the 3rd Conference on Applied Natural
Language Processing, pages 193?200, Trento, March.
ACL.
David D. McDonald. 2000a. Issues in the repre-
sentation of real texts: the design of KRISP. In
Lucja M. Iwan?ska and Stuart C. Shapiro, editors, Nat-
ural Language Processing and Knowledge Represen-
tation, pages 77?110. MIT Press, Cambridge, MA.
David D. McDonald. 2000b. Partially saturated refer-
ents as a source of complexity in semantic interpreta-
tion. In Proceedings of the NAACL-ANLP 2000 Work-
shop on Syntactic and Semantic Complexity in Natural
61
Language Processing Systems, pages 51?58, Seattle,
April. ACL.
Peter B.L. Meijer. 1992. An experimental system for
auditory image representations. IEEE Transactions on
Biomedical Engineering, 39(2):112?121, February.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web. Technical Report 1999-
66, Stanford InfoLab, November. Previous number:
SIDL-WP-1999-0120.
Ulrich Reimer and Udo Hahn. 1988. Text condensation
as knowledge base abstraction. In Proceedings of the
4th Conference on Artificial Intelligence Applications,
CAIA ?88, pages 338?344, San Diego, March. IEEE.
Dominic Widdows. 2003. Orthogonal negation in vector
spaces for modelling word-meanings and document
retrieval. In Proceedings of the 41st Annual Meeting
on Association for Computational Linguistics - Volume
1, ACL ?03, pages 136?143, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Michael J. Witbrock and Vibhu O. Mittal. 1999. Ultra-
summarization: a statistical approach to generating
highly condensed non-extractive summaries. In Pro-
ceedings of the 22nd Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, SIGIR ?99, pages 315?316, Berkeley,
August. ACM.
Wai Yu, Douglas Reid, and Stephen Brewster. 2002.
Web-based multimodal graphs for visually impaired
people. In Proceedings of the 1st Cambridge Work-
shop on Universal Access and Assistive Technology,
CWUAAT ?02, pages 97?108, Cambridge, March.
Chengxiang Zhai. 2008. Statistical Language Models
for Information Retrieval. Morgan and Claypool Pub-
lishers, December.
62
Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 23?27,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
A Corpus of Human-written Summaries of Line Graphs
Charles F. Greenbacker, Sandra Carberry, and Kathleen F. McCoy
Department of Computer and Information Sciences
University of Delaware, Newark, Delaware, USA
[charlieg|carberry|mccoy]@cis.udel.edu
Abstract
We describe a corpus of human-written En-
glish language summaries of line graphs. This
corpus is intended to help develop a system
to automatically generate summaries captur-
ing the most salient information conveyed by
line graphs in popular media, as well as to
evaluate the output of such a system.
1 Motivation
We are developing a system designed to automati-
cally generate summaries of the high-level knowl-
edge conveyed by line graphs found in multimodal
documents from popular media sources (e.g., mag-
azines, newspapers). Intended applications include
making these graphics more accessible for people
with visual impairments and indexing their infor-
mational content for digital libraries. Information
graphics like line graphs are generally included in
a multimodal document in order to make a point
supporting the overall communicative intent of the
document. Our goal is to produce summaries that
convey the knowledge gleaned by humans when in-
formally viewing the graphic, focusing on the ?take-
away? message rather than the raw data points.1
Studies have shown (Carberry et al, 2006) that
the captions of information graphics in popular me-
dia often do not repeat the message conveyed by the
graphic itself; such captions are thus not appropriate
for use as a summary. Furthermore, while scientific
graphs are designed for experts trained in their use
1Users generally prefer conceptual image descriptions over
perceptual descriptions (Jo?rgensen, 1998; Hollink et al, 2004).
for data visualization, information graphics in pop-
ular media are meant to be understood by all read-
ers, including those with only a primary school ed-
ucation. Accordingly, summaries for these graphics
should be tailored for the same general audience.
Research into information graphics by Wu et al
(2010) has identified a limited number of intended
message categories conveyed by line graphs in pop-
ular media. Their efforts included the creation of a
corpus2 of line graphs marked with the overall in-
tended message identified by human annotators.
However, we hypothesize that an effective sum-
mary should present the graph?s intended message
plus additional informational propositions that elab-
orate on this message. McCoy et al (2001) observed
that the intended message was consistently included
in line graph summaries written by human subjects.
Furthermore, participants in that study augmented
the intended message with descriptions of salient vi-
sual features of the graphic (e.g., steepness of a trend
line, volatility of data values). As part of the pro-
cess of building a system to identify which visual
features are salient and to describe them using nat-
ural language expressions, we collected a corpus of
human-written summaries of line graphs.
2 Building the Corpus
We selected 23 different line graphs for use in build-
ing our corpus. This set covered the eight most-
common intended message categories from the Wu
corpus; only Point Correlation and Stable Trend
were omitted. Table 1 shows the distribution of
2www.cis.udel.edu/~carberry/Graphs/viewallgraphs.php
23
Message Category No. (graphs)
Big Fall (BF) 4 (20?23)
Big Jump (BJ) 2 (18, 19)
Changing Trend (CT) 4 (8?11)
Change Trend Return (CTR) 2 (12, 13)
Contrast Trend with
Last Segment (CTLS)
2 (14, 15)
Contrast Segment with
Changing Trend (CSCT)
2 (16, 17)
Rising Trend (RT) 4 (1?4)
Falling Trend (FT) 3 (5?7)
Total 23 (1?23)
Table 1: Distribution of overall intended message cate-
gories in the set of line graphs used to build the corpus.
graphs across message categories.3 Ten of the line
graphs were real world examples in popular media
taken from the Wu corpus (e.g., Figure 1). Another
ten graphs were adapted from items in the Wu cor-
pus ? modified in order to isolate visual features so
that their individual effects could be analyzed (e.g.,
Figure 2). The remaining three line graphs were cre-
ated specifically to fill a gap in the coverage of in-
tended messages and visual features for which no
good example was available (e.g., Figure 3). Our
goal was to include as many different combinations
of message category and visual features as possible
(e.g., for graphs containing a dramatic change in val-
ues because of a big jump or fall, we included ex-
amples which sustained the change as well as others
that did not sustain the change).
69 subjects participated in our study. All were
native English speakers, 18 years of age or older,
without major sight impairments, and enrolled in an
introductory computer science course at a university
in the US. They received a small amount of extra
credit in their course for participating in this study.
Each participant was given the full set of 23 line
graphs in differing orders. With each graph, the sub-
jects were presented with an initial summary sen-
tence describing the overall intended message of the
graphic, as identified by a human annotator. The
captions for Figures 1, 2, and 3 each contain the cor-
responding initial summary sentence that was pro-
vided to the participants. Participants were tasked
with writing additional sentences so that the com-
3Category descriptions can be found in (Wu et al, 2010).
Figure 1: From ?This Cable Outfit Is Getting Tuned In?
in Businessweek magazine, Oct 4, 1999. (Initial sentence:
?This line graph shows a big jump in Blonder Tongue
Laboratories stock price in August ?99.?)
pleted summary of each line graph captured the most
important information conveyed by the graphic, fin-
ishing as many or as few of the 23 graphs as they
wished during a single one-hour session.
Participants were told that we were developing a
system to convey an initial summary of an informa-
tion graphic from popular media (as opposed to text-
books or scientific articles) to blind users via speech.
We indicated that the summaries they write should
be brief (though we did not specify any length re-
quirements), but ought to include all essential infor-
mation provided by the graphic. Subjects were only
given the graphics and did not receive the original ar-
ticle text (if any existed) that accompanied the real-
world graphs. Finally, the participants were told that
a person able to see the graphics should not think
that the summaries they wrote were misleading.
3 Corpus Characteristics
A total of 965 summaries were collected, ranging
from 37 to 49 summaries for each individual line
graph. Table 2 offers some descriptive statistics for
the corpus as a whole, while Table 3 lists the ten
most commonly-occurring content words.
Sample summary 1 (18-4.txt) was written for Fig-
ure 1, summary 2 (7-40.txt) for Figure 2, and sum-
maries 3 (9-2.txt) and 4 (9-5.txt) both for Figure 3:
24
!Figure 2: Adapted from original in ?Dell goes with a few
AMD chips,? USA Today, Oct 19, 2006. (Initial sentence:
?This line graph shows a falling trend in Dell stock from
May ?05 to May ?06.?)
From March 26, 1999 the graph rises and de-
clines up until August 1999 where it rises at
about a 90-degree angle then declines again.
(1)
The graph peaked in July ?05 but then sharply
decreased after that. It had several sharp in-
clines and declines and ended with a shaper
decline from March ?06 to May ?06.
(2)
February has a much larger amount of jackets
sold than the other months shown. From dec-
ember to january, there was a slight drop in
the amount of jackets sold and then a large
spike from january to february.
(3)
The values in November and May are pretty
close, with both being around 37 or 38
jackets. At its peak (February), around 47
jackets were sold.
(4)
4 Potential Usage
To our knowledge, this is the first and only publicly-
available corpus of line graph summaries. It has sev-
eral possible applications in both natural language
generation and evaluation tasks. By finding and ex-
amining patterns in the summaries, we can discover
which propositions are found to be most salient for
certain kinds of graphs. We are currently analyzing
the collected corpus for this very purpose ? to iden-
tify relationships between visual features, intended
messages, and the relative importance of includ-
ing corresponding propositions in a summary (e.g.,
volatility is more salient in Figure 2 than Figure 3).
!Figure 3: Sample line graph created for this study. (Ini-
tial sentence: ?This line graph shows a rising trend in
Boscov?s jacket sales from November to February fol-
lowed by a falling trend through May.?)
Metric Value
total characters 213,261
total words (w) 45,217
total sentences 2,184
characters per word 4.72
words per sentence 20.70
sentences per summary 2.26
unique words (u) 1,831
lexical diversity (w/u) 24.70
hapax legomena 699
pct. of unique words 38.18%
pct. of total words 1.55%
Table 2: Various descriptive statistics for the corpus.
Not only does this corpus offer insight into what
humans perceive to be the most important informa-
tion conveyed by line graphs, it provides a large set
of real-world expressions from which to draw when
crafting the surface realization forms for summaries
of line graphs. From a generation perspective, this
collection of summaries offers copious examples of
the expressions human use to describe characteris-
tics of information graphics. The corpus could also
be used to determine the proper structural character-
istics of a line graph summary (e.g., when multiple
information is included, how propositions are aggre-
gated into sentences, which details come first).
The evaluation of graph understanding systems
will also benefit from the use of this corpus. It will
enable comparisons between system and human-
25
Word Count Word Count
graph 715 stock 287
price 349 increase 280
august 305 may 279
dollars 300 decrease 192
around 299 trend 183
Table 3: The ten most frequently occurring words in the
corpus (omitting stopwords and punctuation).
generated descriptions at the propositional (content)
level, as well as judgments involving clarity and co-
herence. The set of summaries for each graph may
be used as a ?gold standard? against which to com-
pare automatically-generated summaries in prefer-
ence judgment experiments involving human judges.
We are currently developing rules for identifying
the most salient information conveyed by a given
line graph based on an analysis of this corpus, and
will also use the expressions in the collected sum-
maries as examples for surface realization during the
summary generation process. Additionally, we are
planning to use the corpus during part of the evalu-
ation phase of our project, by asking human judges
to compare these human-written summaries against
our system?s output across multiple dimensions of
preference. It may also be useful to perform some
additional human subjects experiments to determine
which summaries in the corpus are found to be most
helpful and understandable.
5 Related Work
Prior to this study, we performed an initial investi-
gation based on a questionnaire similar to the one
used by Demir (2010) for bar charts. A group of
human subjects was asked to review several line
graphs and indicate how important it would be to
include various propositions in an initial summary
of each graphic. Although this method was effec-
tive with bar charts, it proved to be far too cumber-
some to work with line graphs. Bar charts are some-
what simpler, propositionally-speaking, as there are
fewer informational propositions that can be ex-
tracted from data represented as discrete bars rather
than as a continuous data series in a line graph.
It required far more effort for subjects to evaluate
the relative importance of each individual proposi-
tion than to simply provide (in the form of a writ-
ten summary) the set of propositions they consid-
ered to be most important. In the end, the summary-
based approach allowed for a more direct exami-
nation of salience judgments without subjects be-
ing constrained or influenced by the questions and
structure of the questionnaire-based approach, with
the added bonus of producing a reusable corpus of
human-written summaries of line graphs.
McCoy et al (2001) performed a study in which
participants were asked to write brief summaries for
a series of line graphs. While they did not release
a corpus for distribution, their analysis did suggest
that a graph?s visual features could be used to help
select salient propositions to include in a summary.
Although several corpora exist for general im-
age descriptions, we are unaware of any other cor-
pora of human-written summaries for information
graphics. Jo?rgensen (1998) collected unconstrained
descriptions of pictorial images, while Hollink et
al. (2004) analyzed descriptions of mental images
formed by subjects to illustrate a given text pas-
sage. Aker and Gaizauskas (2010) built a corpus of
human-generated captions for location-related im-
ages. Large collections of general image captions
have been assembled for information retrieval tasks
(Smeaton and Quigley, 1996; Tribble, 2010). Roy
(2002) evaluated automatically-generated descrip-
tions of visual scenes against human-generated de-
scriptions. The developers of the iGraph-Lite system
(Ferres et al, 2007) released a corpus of descrip-
tions for over 500 graphs collected from Statistics
Canada, but these descriptions were generated auto-
matically by their system and not written by human
authors. Additionally, the descriptions contained in
their corpus focus on the quantitative data presented
in the graphics rather than the high-level message,
and tend to vary only slightly between graphs.4
Since using corpus texts as a ?gold standard? in
generation and evaluation can be tricky (Reiter and
Sripada, 2002), we tried to mitigate some of the
common problems, including giving participants as
much time as they wanted for each summary to
avoid ?hurried writing.? However, as we intend to
use this corpus to understand which propositions hu-
mans find salient for line graphs, as well as generat-
4The iGraph-Lite system provides the same information for
each instance of a graph type (i.e., all summaries of line graphs
contain the same sorts of information).
26
ing and evaluating new summaries, a larger collec-
tion of examples written by many authors for several
different graphics was more desirable than a smaller
corpus of higher-quality texts from fewer authors.
6 Availability
The corpus is freely available for download5 without
restrictions under an open source license.
The structure of the corpus is as follows. The
?summaries? directory consists of a series of subdi-
rectories numbered 1-23 containing the summaries
for all 23 line graphs, with each summary stored in
a separate file (encoded as ASCII text). The files
are named according to the graph they are associ-
ated with and their position in that graph?s collec-
tion (e.g., 8-10.txt is the 10th summary for the 8th
line graph, and is located in the directory named 8).
The root of the distribution package contains a
directory of original image files for the line graphs
(named ?line graphs?), the initial sentences describ-
ing each graph?s intended message (which was pro-
vided to the participants) in sentences.txt, and a
README file describing the corpus layout.
The corpus is easily loaded with NLTK (Loper
and Bird, 2002) using these Python commands:
from nltk.corpus import PlaintextCorpusReader
LGSroot = './LGSummaryCorpus/summaries'
corpus = PlaintextCorpusReader(LGSroot, '.*')
Acknowledgments
This work was supported in part by the National In-
stitute on Disability and Rehabilitation Research un-
der Grant No. H133G080047.
References
Ahmet Aker and Robert Gaizauskas. 2010. Model sum-
maries for location-related images. In Proceedings
of the Seventh Conference on International Language
Resources and Evaluation, LREC ?10, pages 3119?
3124, Malta, May. ELRA.
Sandra Carberry, Stephanie Elzer, and Seniz Demir.
2006. Information graphics: an untapped resource for
digital libraries. In Proceedings of the 29th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR ?06,
pages 581?588, Seattle, August. ACM.
5www.cis.udel.edu/~mccoy/corpora
Seniz Demir. 2010. SIGHT for Visually Impaired Users:
Summarizing Information Graphics Textually. Ph.D.
thesis, University of Delaware, February.
Leo Ferres, Petro Verkhogliad, Gitte Lindgaard, Louis
Boucher, Antoine Chretien, and Martin Lachance.
2007. Improving accessibility to statistical graphs: the
iGraph-Lite system. In Proceedings of the 9th Inter-
national ACM SIGACCESS Conference on Computers
and Accessibility, ASSETS ?07, pages 67?74, Tempe,
October. ACM.
L. Hollink, A. Th. Schreiber, B. J. Wielinga, and M. Wor-
ring. 2004. Classification of user image descriptions.
International Journal of Human-Computer Studies,
61(5):601?626, November.
Corinne Jo?rgensen. 1998. Attributes of images in de-
scribing tasks. Information Processing and Manage-
ment, 34:161?174, March?May.
Edward Loper and Steven Bird. 2002. NLTK: The nat-
ural language toolkit. In Proceedings of the ACL-02
Workshop on Effective Tools and Methodologies for
Teaching Natural Language Processing and Compu-
tational Linguistics, pages 63?70, Philadelphia, July.
ACL.
Kathleen F. McCoy, M. Sandra Carberry, Tom Roper,
and Nancy Green. 2001. Towards generating textual
summaries of graphs. In Proceedings of the 1st Inter-
national Conference on Universal Access in Human-
Computer Interaction, UAHCI 2001, pages 695?699,
New Orleans, August. Lawrence Erlbaum.
Ehud Reiter and Somayajulu Sripada. 2002. Should cor-
pora texts be gold standards for NLG? In Proceed-
ings of the Second International Conference on Natu-
ral Language Generation, INLG 2002, pages 97?104,
Harriman, New York, July. ACL.
Deb K. Roy. 2002. Learning visually grounded words
and syntax for a scene description task. Computer
Speech & Language, 16(3?4):353?385, July?October.
Alan F. Smeaton and Ian Quigley. 1996. Experiments on
using semantic distances between words in image cap-
tion retrieval. In Proceedings of the 19th Annual Inter-
national ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, SIGIR ?96, pages
174?180, Zurich, August. ACM.
Alicia Tribble. 2010. Textual Inference for Retrieving
Labeled Object Descriptions. Ph.D. thesis, Carnegie
Mellon University, April.
Peng Wu, Sandra Carberry, Stephanie Elzer, and Daniel
Chester. 2010. Recognizing the intended message
of line graphs. In Proceedings of the Sixth Interna-
tional Conference on the Theory and Application of
Diagrams, Diagrams 2010, pages 220?234, Portland,
Oregon, August. Springer-Verlag.
27
