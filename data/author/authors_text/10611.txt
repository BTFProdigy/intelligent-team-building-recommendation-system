Proceedings of the Third Workshop on Statistical Machine Translation, pages 159?162,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Using syntactic coupling features for discriminating phrase-based
translations (WMT-08 Shared Translation Task)
Vassilina Nikoulina and Marc Dymetman
Xerox Research Centre Europe
Grenoble, France
{nikoulina,dymetman}@xrce.xerox.com
Abstract
Our participation in the shared translation task
at WMT-08 focusses on news translation from
English to French. Our main goal is to con-
trast a baseline version of the phrase-based
MATRAX system, with a version that incor-
porates syntactic ?coupling? features in order
to discriminate translations produced by the
baseline system. We report results comparing
different feature combinations.
1 Introduction
Our goal is to try to improve the fluency and ad-
equacy of a baseline phrase-based SMT system by
using a variety of ?syntactic coupling features?, ex-
tracted from parses for the source and target strings.
These features are used for reranking the n-best can-
didates of the baseline system.
The phrase-based SMT system MATRAX, devel-
oped at XRCE, is used as the baseline in the experi-
ments. MATRAX is based on a fairly standard log-
linear model, but one original aspect of the system
is the use of non-contiguous bi-phrases such as ne
... plus / not ... anymore, where words in the source
and target phrases may be separated by gaps, to be
filled at translation time by lexical material provided
by some other such pairs (Simard et al, 2005).
For parsing, we use the Xerox Incremental Parser
XIP (A??t-Mokhtar et al, 2002), which is a robust
dependency parser developed at the Xerox Research
Centre Europe. XIP is fast (around 2000 words per
second for English) and is well adapted to a situ-
ation, like the one we have here, were we need to
parse on the order of a few hundred target candi-
dates on the fly. Also of interest to us is the fact that
XIP produces labelled dependencies, a feature that
we use in some of our experiments.
1.1 Decoding and Training
We resort to a standard reranking approach in which
we produce an n-best list of MATRAX candidate
translations (with n = 100 in our experiments), and
then rerank this list with a linear combination of our
parse-dependent features. In order to train the fea-
ture weights, we use an averaged structured percep-
tron approach (Roark et al, 2004), where we try to
learn weights such that the first candidate to emerge
is equal to the ?oracle? candidate, that is, the candi-
date that is closest to the reference in terms of NIST
score.
1.2 Coupling Features
Our general approach to computing coupling fea-
tures between the dependency structure of the source
and that of a candidate translation produced by MA-
TRAX is the following: we start by aligning the
words between the source and the candidate trans-
lation, we parse both sides, and we count (possi-
bly according to a weighting scheme) the number of
configurations (?rectangles?) that are of the follow-
ing type: ((s1, s12, s2), (t1, t12, t2)), where s12 is an
edge between s1 and s2, t12 is an edge between t1
and t2, s1 is aligned with t1 and s2 is aligned with
t2. We implemented several variants of this basic
scheme.
We start by describing different ?generic? cou-
pling functions derived from the basic scheme, as-
159
suming that word alignments have been already de-
termined, then we describe the option of taking into
account specific dependency labels when counting
rectangles, and finally we describe two options for
computing the word alignments.
1.2.1 Generic features
The first measure of coupling is based on sim-
ple, non-weighted, word alignments. Here we sim-
ply consider that a word of the source and a word
of the target are aligned or not aligned, without any
intermediary degree, and consider that a rectangle
exists on the quadruple of words s1, s2, t1, t2 iff si
is aligned to ti, s1 and s2 have a dependency link
between them (in whatever direction) and similarly
for t1 and t2. The first feature that we introduce,
Coupling-Count, is simply the count of all such rect-
angles between the source and the target.
We note that the value of this feature tends to be
correlated with the size of the source and target de-
pendency trees. We therefore introduce some nor-
malized variants of the feature:
? Coupling-Recall. We compute the number of
source edges for which there exists a projec-
tion in the target. More formally, the number of
edges between two words s1, s2 such that there
exist two words t1, t2 with si aligned to ti and
such that t1, t2 have an edge between them. We
then divide this number by the total number of
edges in the source.
? Coupling-Precision. We do the same thing this
time starting from the target.
? Coupling-F-measure. This is defined as the
harmonic mean of the two previous features.
1.2.2 Label-specific features
The features previously defined do not take into
account the labels associated with edges in the de-
pendency trees. However, while rectangles of the
form ((s1, subj, s2), (t1, subj, t2)) may be rather sys-
tematic between such languages as English and
French, other rectangles may be much less so, due
on the one hand to actual linguistic divergences be-
tween the two languages, but also, as importantly
in practice, to different representational conventions
used by different grammar developers for the two
languages.1
In order to control this problem, we introduce a
collection of Label-Specific-Coupling features, each
for a specific pair of source label and target label.
The values of a label-specific feature are the num-
ber of occurrences for this specific label pair. We
use only label pairs that have been observed to be
aligned in the training corpus (that is, that partici-
pate in observed rectangles). In one version of that
approach, we use all such pairs found in the corpus,
in another version only the pairs above a certain fre-
quency threshold in the corpus.
1.2.3 Alignment
In order to compute the features described above,
a prerequisite is to be able to determine a word align-
ment between the source and a candidate translation.
Our first approach is to use GIZA++ (correspond-
ing roughly to IBM Model 4) to create these align-
ments, by producing for a given source and a given
candidate translation n-best alignment lists in both
directions and applying standard techniques of sym-
metrization to produce a bidirectional alignment.
Another way to find word alignments is to use the
information provided by the baseline system. Since
MATRAX is a phrase-based system, it has access to
the bi-phrases (aligned by definition) that are used in
order to generate a candidate translation. However
note that when we use a bi-phrase based alignment,
there will be differences from the word alignment
that we discussed before, and we need to adapt our
coupling functions.
1.2.4 Related approaches
There is a growing body of work on the use of
syntax for improving the quality of SMT systems.
Our approach is closest to the line taken in (Och
et al, 2003), where syntactic features are also used
for discriminating between candidates produced by
a phrase-based system, but here we introduce and
compare results for a wider variety of coupling fea-
tures, taking into account different combinations in-
volving normalization of the counts, symmetrized
features between the source and target, labelled de-
1Although the XIP formalism is shared between grammar
developers of French and English, the grammars do sometimes
follow different conventions.
160
pendencies, and also consider several ways for com-
puting the word alignment on the basis of which
edge couplings are determined.
2 Experiments
2.1 Description
Our participation concerns the English to French
News translation task. To train our baseline system
we used the News Commentary corpus, namely the
training (? 1M words) and development (1057 sen-
tences) sets proposed for the shared translation task.
The same development set was used for the MERT
training procedure of the baseline system, as well
as for learning the parameters of the reranking pro-
cedure. Note that the test data on which we report
our experimental results here is the one proposed as
development test set for the News translation task
(1064 sentences, nc-devtest2007).
Using MATRAX as the baseline system we gen-
erate 100-best lists of candidate translations for all
source sentences of the test set, we rerank these can-
didates using our features, and we output the top
candidate. We present our results in Table 1, distin-
guished according to the actual combination of fea-
tures used in each experiment.
? The Baseline entry in the table corresponds to
MATRAX results on the test set, without the
use of any of the coupling features.
? We distinguish two sub-tables, according to
whether Giza-based alignments or phrase-
based alignments were used.
? The Generic keyword corresponds to the cou-
pling features introduced in section 1.2.1, based
on rectangle counts, independent of the labels
of the edges.
? The Matrax keyword corresponds to using
MATRAX ?internal? features as reranking fea-
tures, along with the coupling features. These
MATRAX features are pretty standard phrase-
based features, apart from some features deal-
ing explicitly with gapped phrases, and are de-
scribed in detail in (Simard et al, 2005).
? The Labels and Frequent Labels keywords cor-
responds to using label-specific features. In
the first case (Labels) we extracted all of the
aligned label pairs (label pair associated with
a coupling rectangle) found in a training set,
while in the second case (Frequent Labels), we
only kept the most frequently observed among
these label pairs.
? When several keywords appear on a line, we
used the union of the corresponding features,
and in the last line of the table, we show a
combination involving at the same time some
features computed on the basis of Giza-based
alignments and of phrase-based alignments.
? Along with the NIST and BLEU scores of each
combination, we also conducted an informal
manual assessment of the quality of the re-
sults relative to the MATRAX baseline. We
took a random sample of 100 source sentences
from the test set and for each sentence, assessed
whether the first candidate produced by rerank-
ing was better, worse, or indistinguishable in
terms of quality relative to the baseline trans-
lation. We report the number of improvements
(+) and deteriorations (-) among these 100 sam-
ples as well as their difference.2
3 Discussion
While the overall results in terms of Bleu and Nist
do not show major improvements relative to the
baseline, there are several interesting observations
to make. First of all, if we focus on feature com-
binations in which MATRAX features are included
(shown in italics in the table), we see that there is a
general tendency for the results, both in terms of au-
tomatic and human evaluations, to be better than for
the same combination without the MATRAX fea-
tures; the explanation seems to be that if we do
not use the MATRAX features during reranking, but
consider the 100 candidates in the n-best list to be
equally valuable from the viewpoint of MATRAX
features, we lose essential information that cannot
2All the results reported here correspond to our own evalu-
ations, prior to the WMT evaluations. Given time constraints,
we focussed more on contrasting the baseline with the baseline
+ coupling features, than in tuning the baseline itself for the
task at hand. After the submission deadline, we were able to
improve the baseline for this task.
161
NIST BLEU - + Diff
Baseline 6.4093 0.2034 0 0 0
Giza-based alignments
Generic 6.3383 0.2043 15 17 2
Generic, Matrax 6.3782 0.2083 4 18 14
Labels 6.3483 0.1963 12 18 6
Labels, Generic 6.3514 0.2010 3 18 15
Labels, Generic, Matrax 6.4016 0.2075 3 20 17
Frequent Labels 6.3815 0.2054 7 11 4
Frequent Labels, Generic 6.3826 0.2044 6 18 12
Frequent Labels, Generic, Matrax 6.4177 0.2100 2 16 14
Phrase-based alignments
Generic 6.2869 0.1964 12 14 2
Generic, Matrax 6.3972 0.2031 4 11 7
Labels 6.3677 0.1995 16 15 -1
Labels, Generic 6.3567 0.1977 8 15 7
Labels, Generic, Matrax 6.4269 0.2049 4 17 13
Frequent Labels 6.3701 0.1998 3 15 12
Frequent Labels, Generic 6.3846 0.2013 7 16 9
Frequent Labels, Generic, Matrax 6.4160 0.2049 4 16 12
Giza Generic, Phrase Generic, Giza Labels, Matrax 6.4351 0.2060 7 22 15
Table 1: Reranking results.
be recovered simply by appeal to the syntactic cou-
pling features.
If we now concentrate on the lines which do in-
clude MATRAX features and compare their results
with the baseline, we see a trend for these results to
be better than the baseline, both in terms of auto-
matic measures as (more strongly) in terms of hu-
man evaluation. Taken individually, perhaps the im-
provements are not very clear, but collectively, a
trend does seem to appear in favor of syntactic cou-
pling features generally, although we have not con-
ducted formal statistical tests to validate this impres-
sion. A more detailed comparison between individ-
ual lines, inside the class of combinations that in-
clude MATRAX features, appears however difficult
to make on the basis of the reported experiments.
References
Salah A??t-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2002. Robustness beyond shallowness: incre-
mental deep parsing. Natural Language Engineering,
8(3):121?144.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2003. Syn-
tax for Statistical Machine Translation: Final report of
John Hopkins 2003 Summer Workshop. Technical re-
port, John Hopkins University.
B. Roark, M. Saraclar, M. Collins, and M. Johnson.
2004. Discriminative language modeling with condi-
tional random fields and the perceptron algorithm. In
Proceedings of the 42nd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?04), July.
Michel Simard, Nicola Cancedda, Bruno Cavestro,
Marc Dymetman, ?Eric Gaussier, Cyril Goutte,
Kenji Yamada, Philippe Langlais, and Arne Mauser.
2005. Translating with non-contiguous phrases. In
HLT/EMNLP.
162
Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 55?60,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Experiments in discriminating phrase-based translations on the basis of
syntactic coupling features
Vassilina Nikoulina and Marc Dymetman
Xerox Research Centre Europe
Grenoble, France
{nikoulina,dymetman}@xrce.xerox.com
Abstract
We describe experiments on discriminating
English to French phrase-based translations
through the use of syntactic ?coupling? fea-
tures. Using a robust rule-based dependency
parser, we parse both the English source and
the French translation candidates from the n-
best list returned by our phrase-based system;
we compute for each candidate a number of
coupling features, that is, values that depend
on the amount of alignment between edges in
the source and target structures, and discrim-
inatively train the weights of these coupling
features. We compare different feature combi-
nations. Although the improvements in terms
of automatic measures such as Bleu and Nist
are inconclusive, an initial human assessment
of the results appears to show certain qualita-
tive improvements.
1 Introduction
1.1 Motivation
When we use the phrase-based SMT system MA-
TRAX (Simard et al, 2005) to translate the sen-
tence Our declaration of rights is the first of this
millenium from English to French, the result re-
turned by the system is the erroneous translation
Notre de?claration des droits de la premie`re est de
ce mille?naire, while somewhere down the n-best list
of lesser-scored candidates we find a correct transla-
tion: Notre de?claration des droits est la premie`re de
ce mille?naire.
On closer inspection, the difference of scores be-
tween the two candidates is the following. In the
second (correct) case, the phrase of rights was trans-
lated into the phrase des droits, while in the first (in-
correct) case, the phrase of was translated into the
phrase de and the phrase rights into the phrase des
droits. However, while the two bi-phrases of/de and
rights/des droits independently make perfect sense,
the sequence de des droits in French is not possi-
ble, a situation which is easily detected by a stan-
dard ngram language model; the language model has
then a tendency to try to place the (in fact superflu-
ous) de at a further place in the target (just before
la premie`re), where it is more acceptable to it. The
overall consequence is a translation that while for-
mally possible from the viewpoint of a simple lan-
guage model, is not an adequate representation of
the meaning of the source.
Now suppose that we parse both the source and
the two candidates with a dependency parser. If we
compare the parses of the source and of the correct
translation, we find a close (in the current exam-
ple, very close) isomorphism between dependency
edges connecting pairs of aligned words (s1, s2) and
(t1, t2), where si is aligned to ti: the presence of
an edge between s1 and s2 often implies that of an
edge between t1 and t2. This is less the case if we
compare the parses of the source and of the incor-
rect translation; in this case, the word premie`re is
now linked to droits, while the word first was linked
to millenium.
While this is of course just one example, it does
help to motivate the approach we have taken: we
compute different measures of association strength
between edges in the source and target dependency
trees, and use these measures as features for rerank-
55
ing the n-best candidates of a baseline phrase-based
system. The hope is that by doing so, we will in-
crease the adequacy of translations, and possibly to
some extent, their fluency (at least their ?seman-
tic? fluency, which is influenced by their adequacy,
as opposed to their ?grammatical? fluency, which
would be better addressed by target-specific syntac-
tic features than by coupling syntactic features).
1.2 Related Work
There is a growing body of work on the use of syntax
for improving statistical machine translation, from
approaches such as (Chiang, 2007) that use ?formal
syntax?, that is syntactic structures for the source
and target that are discovered on the basis of a bilin-
gual corpus, but without resort to an externally mo-
tivated parser, to approaches such as (Yamada and
Knight, 2001) and (Marcu et al, 2006) that use an
external parser on the target only, or such as (Quirk
et al, 2005) on the source only, or such as (Cowan et
al., 2006) that use external parsers both on the source
and on the target.
Our approach is in this last category, but is distin-
guished from all the cited approaches by the fact that
it does not try to build a target structure (or string)
directly, but rather by using a baseline phrase-based
system as a generator of candidates, and then select-
ing between these candidates through a discrimina-
tive procedure. Some other researchers have taken a
similar line, for example (Hasan et al, 2006), which
only uses a parser on the target, and attempts to im-
prove the fluency of the translation produced, and es-
pecially (Och et al, 2003) that reports experiments
using a large number of syntactic features. In one
of the experiments briefly reported, a dependency
parser is used both for the source and for the tar-
get and a few features are introduced for counting
the number of edges that project from the source
to the target. This experiment, which as far as we
know was not followed up by deeper investigations,
is very similar to what we do. However we intro-
duce and compare results for a wider variety of cou-
pling features, taking into account different combi-
nations involving normalization of the counts, sym-
metrized features between the source and target, la-
belled dependencies, and also consider several ways
for computing the word alignment on the basis of
which edge couplings are determined.
2 The approach
2.1 Background
Matrax. The phrase-based SMT system Matrax
(Simard et al, 2005), developed at Xerox, was
used in the experiments. Matrax is based on a
fairly standard log-linear model, but one original as-
pect of the system is the use of non-contiguous bi-
phrases. Most existing phrase-based models depend
on phrases that are sequences of contiguous words
on either the source or the target side (e.g. pren-
dre feu / catch fire). By contrast, Matrax considers
pairs of non-contiguous phrases, such as ne ... plus /
not ... anymore, where words in the source and tar-
get phrases may be separated by gaps, to be filled
at translation time by lexical material provided by
some other such pairs. One motivation behind this
approach is that, basically, the fact that the source
expression ne ... plus is a good predictor of not
... anymore does not depend on the lexical material
appearing inside the source expression, an insight
which is generally unexploitable by models based
on contiguous phrases.1
XIP. For parsing, we used the Xerox Incremen-
tal Parser XIP (A??t-Mokhtar et al, 2002), which is
a robust dependency parser developed at the Xerox
Research Centre Europe. XIP is fast (around 2000
words per second for English) and is well adapted to
a situation, like the one we have here, were we need
to parse on the order of a few hundred target candi-
dates on the fly. Also of interest to us is the fact that
XIP produces labelled dependencies, a feature that
we use in some of our experiments.
2.2 Decoding and Training
Coupling features such as the ones we use require
access to the parses of candidate translations, and
these parses, at least for a parser such as XIP (and
for many similar parsers), can only be obtained once
the complete candidate translation is known. This is
why it is difficult to introduce them internally in the
Matrax stack-based decoder, which would require to
provide partial parses for prefixes of the target can-
didates and also associated heuristics to estimate the
syntactic structure of completions of these prefixes.
1The Hiero system (Chiang, 2007) is a well-known in-
stance of a structure-oriented system that also has a notion of
gapped phrases, but contrary to Hiero, Matrax is based on non-
hierarchical phrases.
56
Instead, we resort to a standard reranking approach
in which we produce an n-best list of Matrax candi-
date translations (with n = 100 in our experiments),
and then rerank this list with a linear combination
of our parse-dependent features. In order to train
the feature weights, we use an averaged structured
perceptron approach a` la Collins, where we try to
learn weights such that the first candidate to emerge
is equal to the ?oracle? candidate, that is, the candi-
date that is closest to the reference in terms of NIST
score.
2.3 Coupling Features
Our general approach to computing coupling fea-
tures between the dependency structure of the source
and that of a candidate translation produced by Ma-
trax is the following: we start by aligning the words
between the source and the candidate translation, we
parse both sides, and we count (possibly according
to a weighting scheme) the number of configura-
tions (?rectangles?) that are of the following type:
((s1, s12, s2), (t1, t12, t2)), where s12 is an edge be-
tween s1 and s2, t12 is an edge between t1 and t2,
s1 is aligned with t1 and s2 is aligned with t2. We
implemented several variants of this basic scheme.
We start by describing different ?generic? cou-
pling functions derived from the basic scheme, as-
suming that word alignments have been already de-
termined, then we describe the option of taking into
account specific dependency labels when counting
rectangles, and finally we describe two options for
computing the word alignments.
2.3.1 Generic features
The first measure of coupling is based on sim-
ple, non-weighted, word alignments. Here we sim-
ply consider that a word of the source and a word
of the target are aligned or not aligned, without any
intermediary degree, and consider that a rectangle
exists on the quadruple of words s1, s2, t1, t2 iff si
is aligned to ti, s1 and s2 have a dependency link
between them (in whatever direction) and similarly
for t1 and t2. The first feature that we introduce,
Coupling-Count, is simply the count of all such rect-
angles between the source and the target.
We note that the value of this feature tends to be
correlated with the size of the source and target de-
pendency trees. We therefore introduce some nor-
malized variants of the feature:
? Coupling-Recall. We compute the number of
source edges for which there exists a projec-
tion in the target. More formally, the number of
edges between two words s1, s2 such that there
exist two words t1, t2 with si aligned to ti and
such that t1, t2 have an edge between them. We
then divide this number by the total number of
edges in the source.
? Coupling-Precision. We do the same thing this
time starting from the target.
? Coupling-F-measure. In the case of perfectly
isomorphic dependency trees (a situation that
of course rarely occurs because of the linguis-
tic divergences between languages), we would
have precision and recall both equal to 1. In or-
der to measure divergence from this ideal case,
we introduce a feature that we call Coupling-
F-measure, which is defined as the harmonic
mean of the two previous features.
One deficiency of the previous measures is that
they rely a lot on ?hard? word alignments, but do not
take into account the probability of aligning a source
and a target word. We introduce another feature
Coupling-Lex that exploits lexical translation prob-
abilities: each rectangle found between the source
and target trees is weighted according to the prod-
uct of the translation probabilities associated with
(s1, t1) and (s2, t2).
2.3.2 Label-specific features
The features previously defined do not take into
account the labels associated with edges in the de-
pendency trees. However, while rectangles of the
form ((s1, subj, s2), (t1, subj, t2)) may be rather sys-
tematic between such languages as English and
French, other rectangles may be much less so, due
on the one hand to actual linguistic divergences be-
tween the two languages, but also, as importantly
in practice, to different representational conventions
used by different grammar developers for the two
languages.2
In order to control this problem, we introduce a
collection of Label-Specific-Coupling features, each
for a specific pair of source label and target label.
2Although the XIP formalism is shared between grammar
developers of French and English, the grammars do sometimes
follow slightly different conventions.
57
The values of a label-specific feature are the num-
ber of occurrences for this specific label pair. We
use only label pairs that have been observed to be
aligned in the training corpus (that is, that partici-
pate in observed rectangles). In one version of that
approach, we use all such pairs found in the corpus,
in another version only the pairs above a certain fre-
quency threshold in the corpus.
2.3.3 Giza-based alignment
In order to compute the features described above,
a prerequisite is to be able to determine a word align-
ment between the source and a candidate transla-
tion. Our first approach is to use GIZA++ to create
these alignments, by producing for a given source
and a given candidate translation n-best alignment
lists in both directions and applying standard tech-
niques of symmetrization to produce a bidirectional
alignment.
2.3.4 Phrase-based alignment
Another way to find word alignments is to use the
information provided by our baseline system. Since
Matrax is a phrase-based system, it has access to
the bi-phrases (aligned by definition) that are used
in order to generate a candidate translation. How-
ever note that if we use the bi-phrases directly we
are not able to establish the alignments on a word
level (since Matrax does not provide any informa-
tion about word alignments inside the bi-phrases),
but only on a phrase level, and we need to adapt the
coupling features accordingly.
To overcome this problem, we will transform the
dependencies between words into dependencies be-
tween phrases. Thus, two phrases c1, c2 will have a
dependency edge between them if there exists a de-
pendency edge between a word w1 ? c1 and a word
w2 ? c2. Once this transformation is done both
for the source and the target, we get dependency
graphs having phrases as nodes. We also know the
alignments between these phrases, implicit in the bi-
phrases used by Matrax. So, we can consider the
phrases as super-words, and introduce coupling fea-
tures of the same type as before, but operating on a
higher level (super-words) this time.
3 Experiments
3.1 Description
For all our experiments we use the training, develop-
ment and test sets provided for the English-French
News Commentary corpus in WMT-08. The num-
ber of sentences in these sets are respectively 55039,
1057 and 1064, and the average sentence length is 21
words (English) and 24.5 words (French).
We take Matrax as the baseline system. With this
system we generate 100-best lists of candidate trans-
lations for all source sentences of the test set, we
rerank these candidates using our features, and we
output the top candidate. We present our results in
Table 1, distinguished according to the actual com-
bination of features used in each experiment.
? The Baseline entry in the table corresponds to
Matrax results on the test set, without the use
of any of the coupling features.
? We distinguish two sub-tables, according to
whether Giza-based alignments or phrase-
based alignments were used.
? The Generic keyword corresponds to the cou-
pling features introduced in section 2.3.1, based
on rectangle counts, independent of the labels
of the edges.
? The Matrax keyword corresponds to using Ma-
trax ?internal? features as reranking features,
along with the coupling features. These Ma-
trax features are pretty standard phrase-based
features, apart from some features dealing ex-
plicitly with gapped phrases, and are described
in detail in (Simard et al, 2005).
? The Labels and Frequent Labels keywords cor-
responds to using label-specific features. In
the first case (Labels) we extracted all of the
aligned label pairs (label pair associated with a
coupling rectangle) found in a training set of
1000 source sentences along with their 100-
best Matrax translations (this set was chosen
to be different from the development set in or-
der to avoid overfitting effects when rerank-
ing on the development set); we then obtained
2053 features of this kind. In the second case
58
NIST BLEU - + Diff
Baseline 6.4093 0.2034 0 0 0
Giza-based alignments
Generic 6.3383 0.2043 15 17 2
Generic, Matrax 6.3782 0.2083 4 18 14
Labels 6.3483 0.1963 12 18 6
Labels, Generic 6.3514 0.2010 3 18 15
Labels, Generic, Matrax 6.4016 0.2075 3 20 17
Frequent Labels 6.3815 0.2054 7 11 4
Frequent Labels, Generic 6.3826 0.2044 6 18 12
Frequent Labels, Generic, Matrax 6.4177 0.2100 2 16 14
Phrase-based alignments
Generic 6.2869 0.1964 12 14 2
Generic, Matrax 6.3972 0.2031 4 11 7
Labels 6.3677 0.1995 16 15 -1
Labels, Generic 6.3567 0.1977 8 15 7
Labels, Generic, Matrax 6.4269 0.2049 4 17 13
Frequent Labels 6.3701 0.1998 3 15 12
Frequent Labels, Generic 6.3846 0.2013 7 16 9
Frequent Labels, Generic, Matrax 6.4160 0.2049 4 16 12
Giza Generic, Phrase Generic, Giza Labels, Matrax 6.4351 0.2060 7 22 15
Table 1: Reranking results.
(Frequent Labels), we only kept the most fre-
quently observed among these label pairs, re-
taining only 137 such features.
? When several keywords appear on a line, we
used the union of the corresponding features,
and in the last line of the table, we show a
combination involving at the same time some
features computed on the basis of Giza-based
alignments and of phrase-based alignments.
? Along with the NIST and BLEU scores of each
combination3, we also conducted an informal
manual assessment of the quality of the results
relative to the Matrax baseline. We took a ran-
dom sample of 100 source sentences from the
test set and for each sentence, assessed whether
the first candidate produced by reranking was
better, worse, or indistinguishable in terms of
quality relative to the baseline translation. We
report the number of improvements (+) and de-
teriorations (-) among these 100 samples as
well as their difference.
3These scores were computed on the basis of only one ref-
erence.
3.2 Discussion of the results
While the overall results in terms of Bleu and Nist
do not show major improvements relative to the
baseline, there are several interesting observations to
make. First of all, if we focus on feature combina-
tions in which Matrax features are included (shown
in italics in the table), we see that there is a gen-
eral tendency for the results, both in terms of auto-
matic and human evaluations, to be better than for
the same combination without the Matrax features;
the explanation seems to be that if we do not use the
Matrax features during reranking, but consider the
100 candidates in the n-best list to be equally valu-
able from the viewpoint of Matrax features, we lose
essential information that cannot be recovered sim-
ply by appeal to the syntactic coupling features.4
If we now concentrate on the lines which do in-
clude Matrax features and compare their results with
the baseline, we see a trend for these results to be
better than the baseline, both in terms of automatic
measures as (more strongly) in terms of human eval-
4This is not very surprising and probably on the basis of this
observation it would be useful in further experiments to intro-
duce as an additional feature the log-linear score given by the
Matrax baseline.
59
uation. Taken individually, perhaps the improve-
ments are not very clear, but collectively, a trend
does seem to appear in favor of syntactic coupling
features generally, although we have not conducted
formal statistical tests to validate this impression. A
more detailed comparison between individual lines,
inside the class of combinations that include Matrax
features, appears however difficult to make on the
basis of the current experiments.
4 Conclusion and Perspectives
Although there is some consensus that the future
of statistical machine translation lies in the use of
structural information, it is generally admitted that
it is currently difficult to significantly improve over
phrase-base systems in this way, at least in terms of
automatic evaluation measures. Our results do not
contradict that impression, although they are more
encouraging in terms of preliminary human asses-
ments than in terms of the automatic measures.
The reranking approach to using syntactic fea-
tures on top of a phrase-based system is attractive
because on the one hand it is easier to implement
than a full new syntax-aware decoder, and on the
other hand it guarantees at least as good perfor-
mance as the baseline phrase-based system, if some
precautions are taken. On the other hand, its main
limitations concern the size of the n-best list of can-
didates that is realistic in terms of decoding time.5
At least two approaches seem promising in order to
alleviate this problem: (1) find a way to capitalize
on the factorization of translation candidates in the
internal lattice used by the phrase-based decoder, in
order to produce factorized parses that would permit
comparison between more candidates than can be
seen through a final n-best list; (2) allow the reranker
to perform local transformations of the n-best candi-
dates, in the spirit of (Langlais et al, 2007), in order
to be able to explore a larger space of promising can-
didates than is provided by the static list.
Another interesting direction would be to learn
the feature weights by reranking towards another
type of oracle than the one we used, which is de-
fined as the closest candidate in the list in terms of
NIST score relative to the reference; instead it might
5It should be noted however that we could increase this size
from 100 to 1000 without incurring too much penalty, given the
speed of the XIP parser we use.
be worthwhile to use as an oracle the candidate in
the list which receives the best human assessment
in terms of fluency and adequacy, giving a better
chance to the syntactic features to show their worth;
but this would probably also require that these sys-
tems be mostly evaluated in terms of human assess-
ment, a trend which is more and more noticeable in
the SMT community.
References
Salah A??t-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2002. Robustness beyond shallowness: incre-
mental deep parsing. Natural Language Engineering,
8(3):121?144.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Brooke Cowan, Ivona Kucerova, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proceedings EMNLP.
Sas?a Hasan, Oliver Bender, and Hermann Ney. 2006.
Reranking translation hypotheses using structural
properties. In Proceedings of the EACL Workshop on
Learning Structured Information in Natural Language
Applications.
Philippe Langlais, Alexandre Patry, and Fabrizio Gotti.
2007. A greedy decoder for phrase-based statistical
machine translation. In Proceedings of the 11th Inter-
national Conference on Theoretical and Methodolog-
ical Issues in Machine Translation, pages 104?113,
Skvde, Sweden, Sept.
D. Marcu, W. Wang, A. Echihabi, and K. Knight.
2006. SPMT: Statistical Machine Translation with
Syntactified Target Language Phrases. In Proceedings
EMNLP, pages 44?52.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2003. Syn-
tax for statistical machine translation: Final report of
john hopkins 2003 summer workshop. Technical re-
port, John Hopkins University.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In ACL05.
Michel Simard, Nicola Cancedda, Bruno Cavestro,
Marc Dymetman, E?ric Gaussier, Cyril Goutte,
Kenji Yamada, Philippe Langlais, and Arne Mauser.
2005. Translating with non-contiguous phrases. In
HLT/EMNLP.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings ACL,
pages 531?538.
60
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 109?119,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Adaptation of Statistical Machine Translation Model for Cross-Lingual
Information Retrieval in a Service Context
Vassilina Nikoulina
Xerox Research Center Europe
vassilina.nikoulina@xrce.xerox.com
Bogomil Kovachev
Informatics Institute
University of Amsterdam
B.K.Kovachev@uva.nl
Nikolaos Lagos
Xerox Research Center Europe
nikolaos.lagos@xrce.xerox.com
Christof Monz
Informatics Institute
University of Amsterdam
C.Monz@uva.nl
Abstract
This work proposes to adapt an existing
general SMT model for the task of translat-
ing queries that are subsequently going to
be used to retrieve information from a tar-
get language collection. In the scenario that
we focus on access to the document collec-
tion itself is not available and changes to
the IR model are not possible. We propose
two ways to achieve the adaptation effect
and both of them are aimed at tuning pa-
rameter weights on a set of parallel queries.
The first approach is via a standard tuning
procedure optimizing for BLEU score and
the second one is via a reranking approach
optimizing for MAP score. We also extend
the second approach by using syntax-based
features. Our experiments show improve-
ments of 1-2.5 in terms of MAP score over
the retrieval with the non-adapted transla-
tion. We show that these improvements are
due both to the integration of the adapta-
tion and syntax-features for the query trans-
lation task.
1 Introduction
Cross Lingual Information Retrieval (CLIR) is an
important feature for any digital content provider
in today?s multilingual environment. However,
many of the content providers are not willing to
change existing well-established document index-
ing and search tools, nor to provide access to
their document collection by a third-party exter-
nal service. The work presented in this paper as-
sumes such a context of use, where a query trans-
lation service allows translating queries posed to
the search engine of a content provider into sev-
eral target languages, without requiring changes
to the undelying IR system used and without ac-
cessing, at translation time, the content provider?s
document set. Keeping in mind these constraints,
we present two approaches on query translation
optimisation.
One of the important observations done dur-
ing the CLEF 2009 campaign (Ferro and Peters,
2009) related to CLIR was that the usage of Sta-
tistical Machine Translation (SMT) systems (eg.
Google Translate) for query translation led to
important improvements in the cross-lingual re-
trieval performance (the best CLIR performance
increased from ?55% of the monolingual baseline
in 2008 to more than 90% in 2009 for French
and German target languages). However, general-
purpose SMT systems are not necessarily adapted
for query translation. That is because SMT sys-
tems trained on a corpus of standard parallel
phrases take into account the phrase structure im-
plicitly. The structure of queries is very differ-
ent from the standard phrase structure: queries are
very short and the word order might be different
than the typical full phrase one. This problem can
be seen as a problem of genre adaptation for SMT,
where the genre is ?query?.
To our knowledge, no suitable corpora of par-
allel queries is available to train an adapted SMT
system. Small corpora of parallel queries1 how-
ever can be obtained (eg. CLEF tracks) or man-
ually created. We suggest to use such corpora
in order to adapt the SMT model parameters for
query translation. In our approach the parameters
of the SMT models are optimized on the basis of
the parallel queries set. This is achieved either di-
rectly in the SMT system using the MERT (Mini-
mum Error Rate Training) algorithm and optimiz-
1Insufficient for a full SMT system training (?500 entries)
109
ing according to the BLEU2(Papineni et al 2001)
score, or via reranking the Nbest translation can-
didates generated by a baseline system based on
new parameters (and possibly new features) that
aim to optimize a retrieval metric.
It is important to note that both of the pro-
posed approaches allow keeping the MT system
independent of the document collection and in-
dexing, and thus suitable for a query translation
service. These two approaches can also be com-
bined by using the model produced with the first
approach as a baseline that produces the Nbest list
of translations that is then given to the reranking
approach.
The remainder of this paper is organized as fol-
lows. We first present related work addressing the
problem of query translation. We then describe
two approaches towards adapting an SMT system
to the query-genre: tuning the SMT system on a
parallel set of queries (Section 3.1) and adapting
machine translation via the reranking framework
(Section 3.2). We then present our experimental
settings and results (Section 4) and conclude in
section 5.
2 Related work
We may distinguish two main groups of ap-
proaches to CLIR: document translation and
query translation. We concentrate on the second
group which is more relevant to our settings. The
standard query translation methods use different
translation resources such as bilingual dictionar-
ies, parallel corpora and/or machine translation.
The aspect of disambiguation is important for the
first two techniques.
Different methods were proposed to deal with
disambiguation issues, often relying on the docu-
ment collection or embedding the translation step
directly into the retrieval model (Hiemstra and
Jong, 1999; Berger et al 1999; Kraaij et al
2003). Other methods rely on external resources
like query logs (Gao et al 2010), Wikipedia (Ja-
didinejad and Mahmoudi, 2009) or the web (Nie
and Chen, 2002; Hu et al 2008). (Gao et al
2006) proposes syntax-based translation models
to deal with the disambiguation issues (NP-based,
dependency-based). The candidate translations
proposed by these models are then reranked with
the model learned to minimize the translation er-
2Standard MT evaluation metric
ror on the training data.
To our knowledge, existing work that use MT-
based techniques for query translation use an out-
of-the-box MT system, without adapting it for
query translation in particular (Jones et al 1999;
Wu et al 2008) (although some query expan-
sion techniques might be applied to the produced
translation afterwards (Wu and He, 2010)).
There is a number of works done for do-
main adaptation in Statistical Machine Transla-
tion. However, we want to distinguish between
genre and domain adaptation in this work. Gen-
erally, genre can be seen as a sub-problem of do-
main. Thus, we consider genre to be the general
style of the text e.g. conversation, news, blog,
query (responsible mostly for the text structure)
while the domain reflects more what the text is
about ? eg. social science, healthcare, history, so
domain adaptation involves lexical disambigua-
tion and extra lexical coverage problems. To our
knowledge, there is not much work addressing ex-
plicitly the problem of genre adaptation for SMT.
Some work done on domain adaptation could be
applied to genre adaptation, such as incorporating
available in-domain corpora in the SMT model:
either monolingual (Bertoldi and Federico, 2009;
Wu et al 2008; Zhao et al 2004; Koehn and
Schroeder, 2007), or small parallel data used for
tuning the SMT parameters (Zheng et al 2010;
Pecina et al 2011).
3 Our approach
This work is based on the hypothesis that the
general-purpose SMT system needs to be adapted
for query translation. Although in (Ferro and
Peters, 2009) it has been mentioned that using
Google translate (general-purpose MT) for query
translation allowed to CLEF participants to obtain
the best CLIR performance, there is still 10% gap
between monolingual and cross-lingual IR. We
believe that, as in (Clinchant and Renders, 2007),
more adapted query translation, possibly further
combined with query expansion techniques, can
lead to improved retrieval.
The problem of the SMT adaptation for query-
genre translation has different quality aspects.
On the one hand, we want our model to pro-
duce a ?good? translation (well-formed and trans-
mitting the information contained in the source
query) of an input query. On the other hand, we
want to obtain good retrieval performance using
110
the proposed translation. These two aspects are
not necessarily correlated: a bag-of-word transla-
tion can lead to good retrieval performance, even
though it won?t be syntactically well-formed; at
the same time a well-formed translation can lead
to worse retrieval if the wrong lexical choice is
done. Moreover, often the retrieval demands some
linguistic preprocessing (eg. lemmatisation, PoS
tagging) which in interaction with badly-formed
translations might bring some noise.
A couple of works studied the correlation be-
tween the standard MT evaluation metrics and
the retrieval precision. Thus, (Fujii et al 2009)
showed a good correlation of the BLEU scores
with the MAP scores for Cross-Lingual Patent
Retrieval. However, the topics in patent search
(long and well structured) are very different from
standard queries. (Kettunen, 2009) also found a
pretty high correlation ( 0.8 ? 0.9) between stan-
dard MT evaluation metrics (METEOR(Banerjee
and Lavie, 2005), BLEU, NIST(Doddington,
2002)) and retrieval precision for long queries.
However, the same work shows that the correla-
tion decreases ( 0.6? 0.7) for short queries.
In this paper we propose two approaches to
SMT adaptation for queries. The first one op-
timizes BLEU, while the second one optimizes
Mean Average Precision (MAP), a standard met-
ric in information retrieval. We?ll address the is-
sue of the correlation between BLEU and MAP in
Section 4.
Both of the proposed approaches rely on the
phrase-based SMT (PBMT) model (Koehn et al
2003) implemented in the Open Source SMT
toolkit MOSES (Koehn et al 2007).
3.1 Tuning for genre adaptation
First, we propose to adapt the PBMT model by
tuning the model?s weights on a parallel set of
queries. This approach addresses the first as-
pect of the problem, which is producing a ?good?
translation. The PBMT model combines differ-
ent types of features via a log-linear model. The
standard features include (Koehn, 2010, Chapter
5): language model, word penalty, distortion, dif-
ferent translation models, etc. The weights of
these features are learned during the tuning step
with the MERT (Och, 2003) algorithm. Roughly
the MERT algorithm tunes feature weights one by
one and optimizes them according to the BLEU
score obtained.
Our hypothesis is that the impact of different
features should be different depending on whether
we translate a full sentence, or a query-genre en-
try. Thus, one would expect that in the case
of query-genre the language model or the distor-
tion features should get less importance than in
the case of the full-sentence translation. MERT
tuning on a genre-adapted parallel corpus should
leverage this information from the data, adapting
the SMT model to the query-genre. We would
also like to note that the tuning approach (pro-
posed for domain adaptation by (Zheng et al
2010)) seems to be more appropriate for genre
adaptation than for domain adaptation where the
problem of lexical ambiguity is encoded in the
translation model and re-weighting the main fea-
tures might not be sufficient.
We use the MERT implementation provided
with the Moses toolkit with default settings. Our
assumption is that this procedure although not ex-
plicitly aimed at improving retrieval performance
will nevertheless lead to ?better? query transla-
tions when compared to the baseline. The results
of this apporach allow us also to observe whether
and to what extent changes in BLEU scores are
correlated to changes in MAP scores.
3.2 Reranking framework for query
translation
The second approach addresses the retrieval qual-
ity problem. An SMT system is usually trained to
optimize the quality of the translation (eg. BLEU
score for SMT), which is not necessarily corre-
lated with the retrieval quality (especially for the
short queries). Thus, for example, the word or-
der which is crucial for translation quality (and is
taken into account by most MT evaluation met-
rics) is often ignored by IR models. Our second
approach follows (Nie, 2010, pp.106) argument
that ?the translation problem is an integral part
of the whole CLIR problem, and unified CLIR
models integrating translation should be defined?.
We propose integrating the IR metric (MAP) into
the translation model optimisation step via the
reranking framework.
Previous attempts to apply the reranking ap-
proach to SMT did not show significant improve-
ments in terms of MT evaluation metrics (Och
et al 2003; Nikoulina and Dymetman, 2008).
One of the reasons being the poor diversity of the
Nbest list of the translations. However, we be-
111
lieve that this approach has more potential in the
context of query translation.
First of all the average query length is ?5 words,
which means that the Nbest list of the translations
is more diverse than in the case of general phrase
translation (average length 25-30 words).
Moreover, the retrieval precision is more natu-
rally integrated into the reranking framework than
standard MT evaluation metrics such as BLEU.
The main reason is that the notion of Average Re-
trieval Precision is well defined for a single query
translation, while BLEU is defined on the corpus
level and correlates poorly with human quality
judgements for the individual translations (Specia
et al 2009; Callison-Burch et al 2009).
Finally, the reranking framework allows a lot
of flexibility. Thus, it allows enriching the base-
line translation model with new complex features
which might be difficult to introduce into the
translation model directly.
Other works applied the reranking framework
to different NLP tasks such as Named Entities
Extraction (Collins, 2001), parsing (Collins and
Roark, 2004), and language modelling (Roark et
al., 2004). Most of these works used the reranking
framework to combine generative and discrimina-
tive methods when both approaches aim at solv-
ing the same problem: the generative model pro-
duces a set of hypotheses, and the best hypoth-
esis is chosen afterwards via the discriminative
reranking model, which allows to enrich the base-
line model with the new complex and heteroge-
neous features. We suggest using the reranking
framework to combine two different tasks: Ma-
chine Translation and Cross-lingual Information
Retrieval. In this context the reranking framework
doesn?t only allow enriching the baseline transla-
tion model but also performing training using a
more appropriate evaluation metric.
3.2.1 Reranking training
Generally, the reranking framework can be re-
sumed in the following steps :
1. The baseline (generic-purpose) MT system
generates a list of candidate translations
GEN(q) for each query q;
2. A vector of features F (t) is assigned to each
translation t ? GEN(q);
3. The best translation t? is chosen as the one
maximizing the translation score, which is
defined as a weighted linear combination of
features: t?(?) = argmaxt?GEN(q) ??F (t)
As shown above the best translation is selected ac-
cording to features? weights ?. In order to learn
the weights ? maximizing the retrieval perfor-
mance, an appropriate annotated training set has
to be created. We use the CLEF tracks to create
the training set. The retrieval scores annotations
are based on the document relevance annotations
performed by human annotators during the CLEF
campaign.
The annotated training set is created out of
queries {q1, ..., qK} with an Nbest list of trans-
lations GEN(qi) of each query qi, i ? {1..K} as
follows:
? A list of N (we take N = 1000) translations
(GEN(qi)) is produced by the baseline MT
model for each query qi, i = 1..K.
? Each translation t ? GEN(qi) is used
to perform a retrieval from a target docu-
ment collection, and an Average Precision
score (AP (t)) is computed for each t ?
GEN(qi) by comparing its retrieval to the
relevance annotations done during the CLEF
campaign.
The weights ? are learned with the objective of
maximizing MAP for all the queries of the train-
ing set, and, therefore, are optimized for retrieval
quality.
The weights optimization is done with
the Margin Infused Relaxed Algorithm
(MIRA)(Crammer and Singer, 2003), which
was applied to SMT by (Watanabe et al 2007;
Chiang et al 2008). MIRA is an online learning
algorithm where each weights update is done to
keep the new weights as close as possible to the
old weights (first term), and score oracle trans-
lation (the translation giving the best retrieval
score : t?i = argmaxtAP (t)) higher than each
non-oracle translation (tij) by a margin at least as
wide as the loss lij (second term):
? = min??
1
2??
?
? ??2 +
C
?K
i=1 maxj=1..N
(
lij ? ?
?
? (F (t?i )? F (tij)
)
The loss lij is defined as the difference in the re-
trieval average precision between the oracle and
non-oracle translations: lij = AP (t?i )?AP (tij).
C is the regularization parameter which is chosen
via 5-fold cross-validation.
112
3.2.2 Features
One of the advantages of the reranking frame-
work is that new complex features can be easily
integrated. We suggest to enrich the reranking
model with different syntax-based features, such
as:
? features relying on dependency structures:
called therein coupling features (proposed by
(Nikoulina and Dymetman, 2008));
? features relying on Part of Speech Tagging:
called therein PoS mapping features.
By integrating the syntax-based features we
have a double goal: showing the potential of
the reranking framework with more complex fea-
tures, and examining whether the integration of
syntactic information could be useful for query
translation.
Coupling features. The goal of the coupling
features is to measure the similarity between
source and target dependency structures. The ini-
tial hypothesis is that a better translation should
have a dependency structure closer to the one of
the source query.
In this work we experiment with two dif-
ferent coupling variants proposed in (Nikoulina
and Dymetman, 2008), namely, Lexicalised and
Label-specific coupling features.
The generic coupling features are based on
the notion of ?rectangles? that are of the follow-
ing type : ((s1, ds12, s2), (t1, dt12, t2)), where
ds12 is an edge between source words s1 and s2,
dt12 is an edge between target words t1 and t2,
s1 is aligned with t1 and s2 is aligned with t2.
Lexicalised features take into account the qual-
ity of lexical alignment, by weighting each rect-
angle (s1, s2, t1, t2) by a probability of align-
ing s1 to t1 and s2 to t2 (eg. p(s1|t1)p(s2|t2) or
p(t1|s1)p(t2|s2)).
The Label-Specific features take into account
the nature of the aligned dependencies. Thus, the
rectangles of the form ((s1, subj, s2), (t1, subj,
t2)) will get more weight than a rectangle ((s1,
subj, s2), (t1, nmod, t2)). The importance of
each ?rectangle? is learned on the parallel anno-
tated corpus by introducing a collection of Label-
Specific coupling features, each for a specific pair
of source label and target label.
PoS mapping features. The goal of the PoS
mapping features is to control the correspondence
of Part Of Speech Tags between an input query
and its translation. As the coupling features, the
PoS mapping features rely on the word align-
ments between the source sentence and its trans-
lation3. A vector of sparse features is introduced
where each component corresponds to a pair of
PoS tags aligned in the training data. We intro-
duce a generic PoS map variant, which counts a
number of occurrences of a specific pair of PoS
tags, and lexical PoS map variant, which weights
down these pairs by a lexical alignment score
(p(s|t) or p(t|s)).
4 Experiments
4.1 Experimental basis
4.1.1 Data
To simulate parallel query data we used trans-
lation equivalent CLEF topics. The data set used
for the first approach consists of the CLEF topic
data from the following years and tasks: AdHoc-
main track from 2000 to 2008; CLEF AdHoc-
TEL track 2008; Domain Specific tracks from
2000 to 2008; CLEF robust tracks 2007 and 2008;
GeoCLEf tracks 2005-2007. To avoid the issue of
overlapping topics we removed duplicates. The
created parallel queries set contained 500 ? 700
parallel entries (depending on the language pair,
Table 1) and was used for Moses parameters tun-
ing.
In order to create the training set for the rerank-
ing approach, we need to have access to the rele-
vance judgements. We didn?t have access to all
relevance judgements of the previously desribed
tracks. Thus we used only a subset of the previ-
ously extracted parallel set, which includes CLEF
2000-2008 topics from the AdHoc-main, AdHoc-
TEL and GeoCLEF tracks.
The number of queries obtained altogether is
shown in (Table 1).
4.1.2 Baseline
We tested our approaches on the CLEF AdHoc-
TEL 2009 task (50 topics). This task dealt
with monolingual and cross-lingual search in a
library catalog. The monolingual retrieval is
3This alignment can be either produced by a toolkit like
GIZA++(Och and Ney, 2003) or obtained directly by a sys-
tem that produced the Nbest list of the translations (Moses).
113
Language pair Number of queries
Total queries
En - Fr, Fr - En 470
En - De, De - En 714
Annotated queries
En - Fr, Fr - En 400
En - De, De - En 350
Table 1: Top: total number of parallel queries gathered
from all the CLEF tasks (size of the tuning set). Bot-
tom: number of queries extracted from the tasks for
which the human relevance judgements were availble
(size of the reranking training set).
performed with the lemur4 toolkit (Ogilvie and
Callan, 2001). The preprocessing includes lem-
matisation (with the Xerox Incremental Parser-
XIP (A??t-Mokhtar et al 2002)) and filtering out
the function words (based on XIP PoS tagging).
Table 2 shows the performance of the monolin-
gual retrieval model for each collection. The
monolingual retrieval results are comparable to
the CLEF AdHoc-TEL 2009 participants (Ferro
and Peters, 2009). Let us note here that it is not
the case for our CLIR results since we didn?t ex-
ploit the fact that each of the collections could ac-
tually contain the entries in a language other than
the official language of the collection.
The cross-lingual retrieval is performed as fol-
lows :
? the input query (eg. in English) is first trans-
lated into the language of the collection (eg.
German);
? this translation is used to search the target
collection (eg. Austrian National Library for
German ) .
The baseline translation is produced with
Moses trained on Europarl. Table 2 reports the
baseline performance both in terms of MT evalu-
ation metrics (BLEU) and Information Retrieval
evaluation metric MAP (Mean Average Preci-
sion).
The 1best MAP score corresponds to the case
when the single translation is proposed for the
retrieval by the query translation model. 5best
MAP score corresponds to the case when the 5
top translations proposed by the translation ser-
vice are concatenated and used for the retrieval.
4http://www.lemurproject.org/
The 5best retrieval can be seen as a sort of query
expansion, without accessing the document col-
lection or any external resources.
Given that the query length is shorter than for a
standard sentence, the 4-gramm BLEU (used for
standart MT evaluation) might not be able to cap-
ture the difference between the translations (eg.
English-German 4-gramm BLEU is equal to 0 for
our task). For that reason we report both 3- and
4-gramm BLEU scores.
Note, that the French-English baseline retrieval
quality is much better than the German-English.
This is probably due to the fact that our German-
English translation system doesn?t use any de-
coumpounding, which results into many non-
translated words.
4.2 Results
We performed the query-genre adaptation ex-
periments for English-French, French-English,
German-English and English-German language
pairs.
Ideally, we would have liked to combine the
two approaches we proposed: use the query-
genre-tuned model to produce the Nbest list
which is then reranked to optimize the MAP
score. However, it was not possible in our exper-
imental settings due to the small amount of train-
ing data available. We thus simply compare these
two approaches to a baseline approach and com-
ment on their respective performance.
4.2.1 Query-genre tuning approach
For the CLEF-tuning experiments we used the
same translation model and language model as for
the baseline (Europarl-based). The weights were
then tuned on the CLEF topics described in sec-
tion 4.1.1. We then tested the system obtained on
50 parallel queries from the CLEF AdHoc-TEL
2009 task.
Table 3 describes the results of the evalua-
tion. We observe consistent 1-best MAP improve-
ments, but unstable BLEU (3-gramm) (improve-
ments for English-German, and degradation for
other language pairs), although one would have
expected BLEU to be improved in this experi-
mental setting given that BLEU was the objective
function for MERT. These results, on one side,
confirm the remark of (Kettunen, 2009) that there
is a correlation (although low) between BLEU
and MAP scores. The unstable BLEU scores
114
MAP
MAP MAP BLEU BLEU
1-best 5-best 4-gramm 3-gramm
Monolingual IR Bilingual IR
English 0.3159
French-English 0.1828 0.2186 0.1199 0.1568
German-English 0.0941 0.0942 0.2351 0.2923
French 0.2386 English-French 0.1504 0.1543 0.2863 0.3423
German 0.2162 English-German 0.1009 0.1157 0.0000 0.1218
Table 2: Baseline MAP scores for monolingual and bilingual CLEF AdHoc TEL 2009 task.
MAP MAP BLEU BLEU
1-best 5-best 4-gramm 3-gramm
Fr-En 0.1954 0.2229 0.1062 0.1489
De-En 0.1018 0.1078 0.2240 0.2486
En-Fr 0.1611 0.1516 0.2072 0.2908
En-De 0.1062 0.1132 0.0000 0.1924
Table 3: BLEU and MAP performance on CLEF AdHoc TEL 2009 task for the genre-tuned model.
might also be explained by the small size of the
test set (compared to a standard test set of 1000
full-sentences).
Secondly, we looked at the weights of the fea-
tures both in the baseline model (Europarl-tuned)
and in the adapted model (CLEF-tuned), shown in
Table 4. We are unsure how suitable the sizes of
the CLEF tuning sets are, especially for the pairs
involving English and French. Nevertheless we
do observe and comment on some patterns.
For the pairs involving English and German
the distortion weight is much higher when tuning
with CLEF data compared to tuning with Europarl
data. The picture is reversed when looking at the
two pairs involving English and French. This is
to be expected if we interpret a high distortion
weight as follows: ?it is not encouraged to place
source words that are near to each other far away
from each other in the translation?. Indeed, the lo-
cal reorderings are much more frequent between
English and French (e.g. white house = maison
blanche), while the long-distance reorderings are
more typcal between English and German.
The word penalty is consistenly higher over all
pairs when tuning with CLEF data compared to
tuning with Europarl data. We could see an ex-
planation for this pattern in the smaller size of
the CLEF sentences if we interpret higher word
penalty as a preference for shorter translations.
This can be explained both with the smaller aver-
age size of the queries and with the specific query
structure: mostly content words and fewer func-
tion words when compared to the full sentence.
The language model weight is consistently
though not drastically smaller when tuning with
CLEF data. We suppose that this is due to the
fact that a Europarl-base language model is not
the best choice for translating query data.
4.2.2 Reranking approach
The reranking experiments include different
features combinations. First, we experiment with
the Moses features only in order to make this ap-
proach comparable with the first one. Secondly,
we compare different syntax-based features com-
binations, as described in section 3.2.2. Thus, we
compare the following reranking models (defined
by the feature set): moses, lex (lexical coupling
+ moses features), lab (label-specific coupling +
moses features), posmaplex (lexical PoS mapping
+ moses features ), lab-lex (label-specific cou-
pling + lexical coupling + moses features), lab-
lex-posmap (label-specific coupling + lexical cou-
pling features + generic PoS mapping). To reduce
the size of feature-functions vectors we take only
the 20 most frequent features in the training data
for Label-specific coupling and PoS mapping fea-
tures. The computation of the syntax features is
based on the rule-based XIP parser, where some
heuristics specific to query processing have been
integrated into English and French (but not Ger-
man) grammars (Brun et al 2012).
The results of these experiments are illustrated
115
Lng pair Tune set DW LM ?(f |e) lex(f |e) ?(e|f) lex(e|f) PP WP
Fr-En
Europarl 0.0801 0.1397 0.0431 0.0625 0.1463 0.0638 -0.0670 -0.3975
CLEF 0.0015 0.0795 -0.0046 0.0348 0.1977 0.0208 -0.2904 0.3707
De-En
Europarl 0.0588 0.1341 0.0380 0.0181 0.1382 0.0398 -0.0904 -0.4822
CLEF 0.3568 0.1151 0.1168 0.0549 0.0932 0.0805 0.0391 -0.1434
En-Fr
Europarl 0.0789 0.1373 0.0002 0.0766 0.1798 0.0293 -0.0978 -0.4002
CLEF 0.0322 0.1251 0.0350 0.1023 0.0534 0.0365 -0.3182 -0.2972
En-De
Europarl 0.0584 0.1396 0.0092 0.0821 0.1823 0.0437 -0.1613 -0.3233
CLEF 0.3451 0.1001 0.0248 0.0872 0.2629 0.0153 -0.0431 0.1214
Table 4: Feature weights for the query-genre tuned model. Abbreviations: DW - distortion weight, LM - language
model weight, PP - phrase penalty, WP - word penalty, ?-phrase translation probability, lex-lexical weighting.
Query Example MAP bleu1
Src1 Weibliche Ma?rtyrer
Ref Female Martyrs
T1 female martyrs 0.07 1
T2 Women martyr 0.4 0
Src 2 Genmanipulation am
Menschen
Ref Human Gene Manipula-
tion
T1 On the genetic manipula-
tion of people
0.044 0.167
T2 genetic manipulation of
the human being
0.069 0.286
Src 3 Arbeitsrecht in der Eu-
ropa?ischen Union
Ref European Union Labour
Laws
T1 Labour law in the Euro-
pean Union
0.015 0.5
T2 labour legislation in the
European Union
0.036 0.5
Table 5: Some examples of queries translations (T1:
baseline, T2: after reranking with lab-lex), MAP and
1-gramm BLEU scores for German-English.
in Figure 1. To keep the figure more readable,
we report only on 3-gramm BLEU scores. When
computing the 5best MAP score, the order in the
Nbest list is defined by a corresponding reranking
model. Each reranking model is illustrated by a
single horizontal red bar. We compare the rerank-
ing results to the baseline model (vertical line) and
also to the results of the first approach (yellow bar
labelled MERT:moses) on the same figure.
First, we remark that the adapted models
(query-genre tuning and reranking) outperform
the baseline in terms of MAP (1best and 5 best)
for French-English and German-English transla-
tions for most of the models. The only exception
is posmaplex model (based on PoS tagging) for
German which can be explained by the fact that
the German grammar used for query processing
was not adapted for queries as opposed to English
and French grammars. However, we do not ob-
serve the same tendency for BLEU score, where
only a few of the adapted models outperform the
baseline, which confirms the hypothesis of the
low correlation between BLEU and MAP scores
in these settings. Table 5 gives some examples of
the queries translations before (T1) and after (T2)
reranking. These examples also illustrate differ-
ent types of disagreement between MAP and 1-
gramm BLEU5 score.
The results for English-German and English-
French look more confusing. This can be partly
due to the more rich morphology of the target lan-
guages which may create more noise in the syn-
tax structure. Reranking however improves over
the 1-best MAP baseline for English-German, and
5-best MAP is also improved excluding the mod-
els involving PoS tagging for German (posmap,
posmaplex, lab-lex-posmap). The results for
English-French are more difficult to interpret. To
find out the reason of such a behavior, we looked
at the translations. We observed the following to-
kenization problem for French: the apostrophe is
systematically separated, e.g. ?d ? aujourd ? hui?.
This leads to both noisy pre-retrieval preprocess-
ing (eg. d is tagged as a NOUN) and noisy syntax-
based feature values, which might explain the un-
stable results.
Finally, we can see that the syntax-based fea-
tures can be beneficial for the final retrieval qual-
ity: the models with syntax features can outper-
form the model basd on the moses features only.
The syntax-based features leading to the most sta-
5The higher order BLEU scores are equal to 0 for most
of the individual translations.
116
Figure 1: Reranking results. The vertical line corresponds to the baseline scores. The lowest bar (MERT:moses,
in yellow): the results of the tuning approach, other bars(in red): the results of the reranking approach.
ble results seem to be lab-lex (combination of lex-
ical and label-specific coupling): it leads to the
best gains over 1-best and 5-best MAP for all lan-
guage pairs excluding English-French. This is a
surprising result given the fact that the underlying
IR model doesn?t take syntax into account in any
way. In our opinion, this is probably due to the
interaction between the pre-retrieval preprocess-
ing (lemmatisation, PoS tagging) done with the
linguistic tools which might produce noisy results
when applied to the SMT outputs. The rerank-
ing with syntax-based features allows to choose
a better-formed query for which the PoS tagging
and lemmatisation tools produce less noise which
leads to a better retrieval.
5 Conclusion
In this work we proposed two methods for query-
genre adaptation of an SMT model: the first
method addressing the translation quality aspect
and the second one the retrieval precision aspect.
We have shown that CLIR performance in terms
of MAP is improved between 1-2.5 points. We
believe that the combination of these two meth-
ods would be the most beneficial setting, although
we were not able to prove this experimentally
(due to the lack of training data). None of these
methods require access to the document collec-
tion at test time, and can be used in the context
of a query translation service. The combination
of our adapted SMT model with other state-of-the
art CLIR techniques (eg. query expansion with
PRF) will be explored in future work.
Acknowledgements
This research was supported by the European
Union?s ICT Policy Support Programme as part of
the Competitiveness and Innovation Framework
Programme, CIP ICT-PSP under grant agreement
nr 250430 (Project GALATEAS).
References
Salah A??t-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2002. Robustness beyond shallowness: in-
117
cremental deep parsing. Natural Language Engi-
neering, 8:121?144, June.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
an automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Adam Berger, John Lafferty, and John La Erty. 1999.
The weaver system for document retrieval. In In
Proceedings of the Eighth Text REtrieval Confer-
ence (TREC-8, pages 163?174.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In Proceedings of
the Fourth Workshop on Statistical Machine Trans-
lation, pages 182?189. Association for Computa-
tional Linguistics.
Caroline Brun, Vassilina Nikoulina, and Nikolaos La-
gos. 2012. Linguistically-adapted structural query
annotation for digital libraries in the social sciences.
In Proceedings of the 6th EACL Workshop on Lan-
guage Technology for Cultural Heritage, Social Sci-
ences, and Humanities, Avignon, France, April.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
David Chiang, Yuval Marton, and Philip Resnik.
2008. Online large-margin training of syntactic and
structural translation features. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 224?233. Association
for Computational Linguistics.
Ste?phane Clinchant and Jean-Michel Renders. 2007.
Query translation through dictionary adaptation. In
CLEF?07, pages 182?187.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In ACL ?04:
Proceedings of the 42nd Annual Meeting on Asso-
ciation for Computational Linguistics.
Michael Collins. 2001. Ranking algorithms for
named-entity extraction: boosting and the voted
perceptron. In ACL?02: Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, pages 489?496, Philadelphia, Pennsyl-
vania. Association for Computational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
George Doddington. 2002. Automatic evaluation
of Machine Translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Language
Technology Research, pages 138?145, San Diego,
California. Morgan Kaufmann Publishers Inc.
Nicola Ferro and Carol Peters. 2009. CLEF 2009
ad hoc track overview: TEL and persian tasks.
In Working Notes for the CLEF 2009 Workshop,
Corfu, Greece.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2009. Evaluating effects of ma-
chine translation accuracy on cross-lingual patent
retrieval. In Proceedings of the 32nd international
ACM SIGIR conference on Research and develop-
ment in information retrieval, SIGIR ?09, pages
674?675.
Jianfeng Gao, Jian-Yun Nie, and Ming Zhou. 2006.
Statistical query translation models for cross-
language information retrieval. 5:323?359, Decem-
ber.
Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Kam-
Fai Wong, and Hsiao-Wuen Hon. 2010. Exploit-
ing query logs for cross-lingual query suggestions.
ACM Trans. Inf. Syst., 28(2).
Djoerd Hiemstra and Franciska de Jong. 1999. Dis-
ambiguation strategies for cross-language informa-
tion retrieval. In Proceedings of the Third European
Conference on Research and Advanced Technology
for Digital Libraries, pages 274?293.
Rong Hu, Weizhu Chen, Peng Bai, Yansheng Lu,
Zheng Chen, and Qiang Yang. 2008. Web query
translation via web log mining. In Proceedings of
the 31st annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ?08, pages 749?750. ACM.
Amir Hossein Jadidinejad and Fariborz Mahmoudi.
2009. Cross-language information retrieval us-
ing meta-language index construction and structural
queries. In Proceedings of the 10th cross-language
evaluation forum conference on Multilingual in-
formation access evaluation: text retrieval experi-
ments, CLEF?09, pages 70?77, Berlin, Heidelberg.
Springer-Verlag.
Gareth Jones, Sakai Tetsuya, Nigel Collier, Akira Ku-
mano, and Kazuo Sumita. 1999. Exploring the
use of machine translation resources for english-
japanese cross-language information retrieval. In In
Proceedings of MT Summit VII Workshop on Ma-
chine Translation for Cross Language Information
Retrieval, pages 181?188.
Kimmo Kettunen. 2009. Choosing the best mt pro-
grams for clir purposes ? can mt metrics be help-
ful? In Proceedings of the 31th European Confer-
ence on IR Research on Advances in Information
Retrieval, ECIR ?09, pages 706?712, Berlin, Hei-
delberg. Springer-Verlag.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT
118
?07, pages 224?227. Association for Computational
Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54, Morristown, NJ, USA.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondr?ej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In ACL ?07: Proceedings of the 45th
Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, pages 177?180. As-
sociation for Computational Linguistics.
Philip Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Wessel Kraaij, Jian-Yun Nie, and Michel Simard.
2003. Embedding web-based statistical trans-
lation models in cross-language information re-
trieval. Computational Linguistiques, 29:381?419,
September.
Jian-yun Nie and Jiang Chen. 2002. Exploiting the
web as parallel corpora for cross-language informa-
tion retrieval. Web Intelligence, pages 218?239.
Jian-Yun Nie. 2010. Cross-Language Information Re-
trieval. Morgan & Claypool Publishers.
Vassilina Nikoulina and Marc Dymetman. 2008. Ex-
periments in discriminating phrase-based transla-
tions on the basis of syntactic coupling features. In
Proceedings of the ACL-08: HLT Second Workshop
on Syntax and Structure in Statistical Translation
(SSST-2), pages 55?60. Association for Computa-
tional Linguistics, June.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2003.
Syntax for Statistical Machine Translation: Final
report of John Hopkins 2003 Summer Workshop.
Technical report, John Hopkins University.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160?
167, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Paul Ogilvie and James P. Callan. 2001. Experiments
using the lemur toolkit. In TREC.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation.
Pavel Pecina, Antonio Toral, Andy Way, Vassilis Pa-
pavassiliou, Prokopis Prokopidis, and Maria Gi-
agkou. 2011. Towards using web-crawled data for
domain adaptation in statistical machine translation.
In Proceedings of the 15th Annual Conference of
the European Associtation for Machine Translation,
pages 297?304, Leuven, Belgium. European Asso-
ciation for Machine Translation.
Brian Roark, Murat Saraclar, Michael Collins, and
Mark Johnson. 2004. Discriminative language
modeling with conditional random fields and the
perceptron algorithm. In Proceedings of the 42nd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?04), July.
Lucia Specia, Marco Turchi, Nicola Cancedda, Marc
Dymetman, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In Proceedings of the 13th Annual Confer-
ence of the EAMT, page 28?35, Barcelona, Spain.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In Proceed-
ings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 764?773, Prague, Czech Republic.
Association for Computational Linguistics.
Dan Wu and Daqing He. 2010. A study of query
translation using google machine translation sys-
tem. Computational Intelligence and Software En-
gineering (CiSE).
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proceedings of the 22nd International
Conference on Computational Linguistics (Col-
ing2008), pages 993?100.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Pro-
ceedings of the 20th international conference on
Computational Linguistics, COLING ?04. Associ-
ation for Computational Linguistics.
Zhongguang Zheng, Zhongjun He, Yao Meng, and
Hao Yu. 2010. Domain adaptation for statisti-
cal machine translation in development corpus se-
lection. In Universal Communication Symposium
(IUCS), 2010 4th International, pages 2?7. IEEE.
119
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 49?52,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
A Lightweight Terminology Verification Service for External Machine
Translation Engines
Alessio Bosca
?
, Vassilina Nikoulina
?
, Marc Dymetman
?
?
CELI, Turin, Italy
?
Xerox Research Centre Europe, Grenoble, France
?
alessio.bosca@celi.it,
?
{first.last}@xrce.xerox.com
Abstract
We propose a demonstration of a domain-
specific terminology checking service
which works on top of any generic black-
box MT, and only requires access to a
bilingual terminology resource in the do-
main. In cases where an incorrect trans-
lation of a source term was proposed by
the generic MT service, our service locates
the wrong translation of the term in the tar-
get and suggests a terminologically correct
translation for this term.
1 Introduction
Today there exist generic MT services for a large
number of language pairs, which allow relatively
easily to make your domain-specific portal mul-
tilingual, and allow access to its documents for a
broad international public. However, applying a
generic MT service to domain-specific texts often
leads to wrong results, especially relative to the
translation of domain-specific terminology. Table
1 illustrates an example of a terminology inconsis-
tent translation provided by a generic MT system.
English Source: Farmers tend to implement a
broad non-focused weed-control strategy, on
the basis of broad spectrum products and mix-
tures of different products.
Bing
1
: Los agricultores tienden a aplicar una
estrategia amplia para control de malezas no
centrado, sobre la base de productos de am-
plio espectro y las mezclas de diferentes pro-
ductos.
Table 1: Example of the translation produced by
a generic MT model for a domain-specific docu-
ment. Source term : weed-control, official Span-
ish term translation: control de malas hierbas.
The importance of domain-specific terminology
for Machine Translation has been mentioned in
several previous works (eg. (Carl and Langlais,
2002; Skadins et al., 2013)). However, most of
these works handle the case where the terminology
is tightly integrated into the translation process.
This requires both a good expertise in SMT and
a large amount of both in-domain and generic par-
allel texts, which is often difficult, especially for
low-resourced languages like Turkish or Estonian.
Here, we are targeting the situation where the con-
tent provider is not willing to train a dedicated
translation system, for some reason such as lack of
technical skills or lack of necessary resources (par-
allel data or computational resources), but has at
his disposal a multilingual in-domain terminology
which could be helpful for improving the generic
translation provided by an external translation ser-
vice. We propose a demonstration of a multilin-
gual terminology verification/correction service,
which detects the wrongly translated terms and
suggests a better translation of these terms. This
service can be seen as an aid for machine transla-
tion post-editing focused on in-domain terminol-
ogy and as a tool for supporting the workflow of
practicing translators.
2 Related Work
There has recently been a growing interest for ter-
minology integration into MT models. Direct in-
tegration of terminology into the SMT model has
been considered, either by extending SMT train-
ing data (Carl and Langlais, 2002), or via adding
an additional term indicator feature (Pinnis and
Skadins, 2012; Skadins et al., 2013) into the trans-
lation model. However none of the above is possi-
ble when we deal with an external black-box MT
service.
(Itagaki and Aikawa, 2008) propose a post-
processing step for an MT engine, where a
wrongly translated term is replaced with a user-
provided term translation. The authors claim that
translating the term directly often gives a different
49
translation from the one obtained when translating
the term in context: for English-Japanese the out-
of-context term translation matches exactly the in
context term translation in 62% of cases only. In
order to address this problem the authors propose
15 simple context templates that induce the same
term translation as the one obtained in the initial
sentence context. Such templates include ?This
is TERM? or ?TERM is a ...?. The main prob-
lem with this approach is that these templates are
both language-pair and MT engine/model specific.
Thus a certain human expertise is required to de-
velop such templates when moving to a new lan-
guage pair or underlying MT engine.
Our approach is close to the (Itagaki and
Aikawa, 2008) approach, but instead of devel-
oping specific templates we propose a generic
method for wrong terminology translation detec-
tion. We do not aim at producing the final trans-
lation by directly replacing the wrongly translated
term ? which can be tricky?, but rather perform
the term correction in an interactive manner, where
the user is proposed a better term translation and
may choose to use it if the suggestion is correct.
3 Terminology-checking service
We assume that the provider of the terminology-
checking service has a bilingual domain-specific
terminology D at his disposal, which he wishes
to use to improve the translation produced by a
generic MT service MT . Our method verifies
whether the terminology was translated correctly
by the MT service (terminology verification), and
if not, locates the wrong translation of the term and
suggests a better translation for it.
3.1 Terminology checking
The basic terminology verification procedure ap-
plied to the source sentence s and to its translation
MT (s) by the generic service is done through the
following steps:
1. For each term T = (T
s
, T
t
) in D check
whether its source part T
s
is present in the
source sentence s.
2. If s contains T
s
, check whether the target
part of the term T
t
is present in the transla-
tion MT (s). If yes, and the number of oc-
currences of T
s
in s is equal to that of T
t
in MT (s) : the term translation is consis-
tent with terminological base. Otherwise, we
attempt to locate the wrong term translation
and suggest a better translation to the user.
Both steps require a sub-string matching algo-
rithm which is able to deal with term detection
problems such as morphological variants or dif-
ferent term variants. We describe the approach we
take for efficient sub-string matching in more de-
tail in section 3.3.
3.2 Terminology correction
Once we have detected that there is a source term
T
s
which has been incorrectly translated we would
like to suggest a better translation for this term.
This requires not only knowing a correct transla-
tion T
t
of the source term T
s
, but also its position
in the target sentence. To do that, we need to iden-
tify what was the incorrect translation proposed by
the MT engine for the term and to locate it in the
translation MT (s).
This can be seen as a sub-problem of the word-
alignment problem, which is usually solved us-
ing bilingual dictionaries or by learning statistical
alignment models out of bilingual corpora. How-
ever, in practice, these resources are not easily
available, especially for low-resourced language
pairs. In order to be able to locate the wrong term
translation in the target sentence without resort-
ing to such resources, our approach is to rely in-
stead on the same external MT engine that was
used for translating the whole source sentence in
the first place, an approach also taken in (Itagaki
and Aikawa, 2008).
To overcome the problem mentioned by (Ita-
gaki and Aikawa, 2008) of non-matching out-of-
context terms translations we propose to com-
bine out-of context term translation (MT (T
s
)) and
context-extended term translation, as follows:
? Translate the term T
s
extended with
its left and/or right n-gram context:
s
i?n
s
i?n+1
...T
s
...s
j+n?1
s
j+n
, where
T
s
= s
i
...s
j
;
? Find a fuzzy match in MT (s) for the
translation of the context-extended term
MT (s
i?n
...T
s
...s
j+n
) using the same sub-
string matching algorithm as in the terminol-
ogy verification step.
Various combinations of out-of-context term
translation (MT (T
s
)) and n-extended term trans-
lation (MT (s
i?n
...T
s
...s
j+n
)) are possible.
50
The term location is performed in a sequential
way: if the wrong term translation was not located
after the first step (out-of-context translation), at-
tempt the following step, extending size of the
context (n) until the term is located.
3.3 Implementation
The implementation of the terminology-checking
service that we demonstrate exploits Bing Trans-
lator
2
as SMT service, refers to the Agricul-
ture domain and supports two terminology re-
sources: the multilingual ontology from the Or-
ganic.Edunet portal
3
and Agrovoc, a multilingual
theasurus from FAO
4
. The presented prototype en-
ables terminology checking for all the language
pairs involving English, French, German, Italian,
Portoguese, Spanish and Turkish.
The component for matching the textual input
(i.e. either the source or the translation from the
SMT service) with elements from domain termi-
nologies is based on the open source search engine
Lucene
5
and exploits its built-in textual search ca-
pabilities and indexing facilities. We created a
search index for each of the supported languages,
containing the textual representations of the ter-
minology elements in that language along with
their URI (unique for each terminology element).
The terms expressions are indexed in their origi-
nal form as well as in their lemmatized and POS
tagged ones; for Turkish, resources for morpho-
logical analysis were not available therefore stem-
ming has been used instead of lemmatization.
In order to find the terminological entries within
a textual input in a given language a two-steps pro-
cedure is applied:
? In a first step, the text is used as a query over
the search index (in that language) in order
to find a list of all the terminology elements
containing a textual fragment present in the
query.
? In a second step, in order to retain only the
domain terms with a complete match (no par-
tial matches) and locate them in the text, a
new search index is built in memory, con-
taining a single document, namely the orig-
inal textual input (lemmatized or stemmed
according to the resources available for that
2
http://www.bing.com/translator
3
http://organic-edunet.eu/
4
http://aims.fao.org/standards/agrovoc/about
5
https://lucene.apache.org
specific language). Then the candidate ter-
minology elements found in the first step are
used as queries over the in-memory index and
the ?highlighter? component of the search en-
gine is exploited to locate them in the text
(when found). A longest match criterion is
used when the terminology elements found
refer to overlapping spans of text.
Following this procedure a list with terminology
elements (along with their URIs and the position
within the text) is generated for both the source
text and its translation. A matching strategy based
on the URI allows to pair domain terms from the
two collections. For domain terms in the source
text without a corresponding terminology element
in the translated text, the ?wrong? translation is
located in the text according to the approach de-
scribed in 3.2. The domain term is retranslated
with the same SMT (with context extension, if
needed) in order to obtain the ?wrong? translation
and the translated string is located within the trans-
lation text with the same approach used in the sec-
ond step of the procedure used for locating termi-
nological entries (with an in-memory search index
over the full text and the fragment used as query).
The service outputs two lists: one containing
the pairs of terminology elements found both in
the source and in the translation and another one
with the terminology elements without a ?correct?
translation (according to the domain terminology
used) and for each of those an alternative transla-
tion from the domain terminology is proposed. In
our demonstration a web interface allows users to
access and test the service.
4 Proof of concept evaluation
In order to evaluate the quality of locating the
wrong term translation, we applied the terminol-
ogy verification service to an SMT model trained
with Moses (Hoang et al., 2007) on the Europarl
(Koehn, 2005) corpus. This SMT model was used
for translating a test set in the Agricultural domain
from Spanish into English. In these settings we
have access to the internal sub-phrase alignment
provided by Moses, thus we know the exact loca-
tion of the wrong term translation, which allows
us to evaluate how good our locating technique is.
The test set consists of 100 abstracts in Spanish
from a bibliographical database of scientific publi-
cations in the Agriculture domain. These abstracts
were translated into English with our translation
51
model, and we then applied terminology verifi-
cation and terminology correction procedures to
these translations.
When applying terminology verification we de-
tected in total 171 terms in Spanish, 71 of them
being correctly translated into English (consistent
with terminology), and 100 being wrongly trans-
lated (not consistent with terminology).
We then attempted to locate these wrongly
translated terms in the system translation MT (s).
Matching the out-of-context term translation
with initial translation allowed to find a match for
82 wrongly translated terms (out of 100); Match-
ing 1 left/right word extended term translation
(MT (w
i?1
T
s
w
j+1
)) allowed to find a match for
16 more terms (out of 18 left).
Using the internal word alignments provided by
Moses, we also evaluated how precisely the bor-
ders of the wrongly translated term were recovered
by our term location procedure. This precision is
measured as follows:
? The target tokens identified by our procedure
(as described in 3) are: g
T
= t
1
, . . . , t
j
;
? We then identify the reference target tokens
corresponding to the translation of the term
T
s
using the Moses word alignment : r
T
=
{r
t
1
, . . . , r
t
k
}.
We define term location precision p as p =
|t
j
?r
T
?g
T
|
|g
T
|
. The precision of term location with
out-of-context term translation is of 0.92; the pre-
cision of term location with context-extended term
translation is 0.91.
Overall, our approach allows to match 98% of
the wrongly translated terms, with an overall lo-
cation precision of 0.91. Although these numbers
may vary for other language pairs and other MT
systems, this performance is encouraging.
5 Conclusion
We propose a demonstration of a terminology ver-
ification system that can be used as an aid for post-
editing machine translations explicitly focused on
bilingual terminology consistency. This system re-
lies on an external black-box generic MT engine
extended with available domain-specific terminol-
ogy. The location of the wrong term translation is
located via re-translation of the original term with
the same MT engine. We show that we partially
overcome the situation where the out-of-context
translation of the term differs from the original
translation of this term (in the full sentence) by
extending the term context with surrounding n-
grams. The terminology verification method is
both MT engine and language independent, does
not require any access to the internals of the MT
engine used, and is easily portable.
Acknowledgments
This work has been partially supported by
the Organic.Lingua project (http://www.organic-
lingua.eu/), funded by the European Commission
under the ICT Policy Support Programme.
References
Michael Carl and Philippe Langlais. 2002. An intel-
ligent terminology database as a pre-processor for
statistical machine translation. In COLING-02: Sec-
ond International Workshop on Computational Ter-
minology, pages 1?7.
Hieu Hoang, Alexandra Birch, Chris Callison-burch,
Richard Zens, Rwth Aachen, Alexandra Constantin,
Marcello Federico, Nicola Bertoldi, Chris Dyer,
Brooke Cowan, Wade Shen, Christine Moran, and
Ondej Bojar. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL 2007 Demo
and Poster Sessions, pages 177?180.
Masaki Itagaki and Takako Aikawa. 2008. Post-mt
term swapper: Supplementing a statistical machine
translation system with a user dictionary. In LREC.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In MT Summit X,
pages 79?86, Phuket Thailand.
Marcis Pinnis and Raivis Skadins. 2012. Mt adapta-
tion for under-resourced domains - what works and
what not. In Baltic HLT, volume 247, pages 176?
184.
Raivis Skadins, Marcis Pinnis, Tatiana Gornostay, and
Andrejs Vasiljevs. 2013. Application of online ter-
minology services in statistical machine translation.
In MT Summit XIV, pages 281?286.
52
Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 55?64,
Avignon, France, 24 April 2012. c?2012 Association for Computational Linguistics
Linguistically-Adapted Structural Query Annotation for Digital Li-
braries in the Social Sciences 
 Carol ine Brun Vassilina Nikoulina Nikolaos Lagos 
Xerox Research Centre Europe 
6, chemin de Maupertuis 
38240, Meylan France 
{firstname.lastname}@xrce.xerox.com 
 
Abstrac t 
Query processing is an essential part of a  
range of applications in the social sciences 
and cultural heritage domain. However, out-
of-the-box natural language processing tools 
originally developed for full phrase analysis 
are inappropriate for query analysis. In this  
paper, we propose an approach to solving 
this problem by adapting a complete and in-
tegrated chain of NLP tools, to make it  suit-
able for queries analysis. Using as a case 
study the automatic translation of queries 
posed to the Europeana library, we demon-
strate that adapted linguistic processing can 
lead to improvements in translation quality. 
1 Introduction 
Query processing tools are essential components 
of digital libraries and content aggregators. Their 
operation varies from simple stop word removal 
and stemming to advanced parsing, that treats 
queries as a collection of phrases rather than sin-
gle terms (Mothe and Tanguy, 2007). They are 
used in a range of applications, from information 
retrieval (via search engines that provide access 
to the digital collections) to query analysis.  
Current query processing solutions tend to 
use out-of-the-box Natural Language Processing 
(NLP) tools that were originally developed for 
full phrase analysis, being inappropriate for 
query analysis. 
Correct query annotation and interpretation is 
even more important in the cultural heritage or 
social sciences domain, as a lot of the content 
can be in multimedia form and only metadata 
(most of the times in the form of tags) is exploit-
able by traditional text-oriented information re-
trieval and analysis techniques. 
Furthermore, as recent studies of user query-
ing behavior mention, queries in these domains 
are not only very short but are also quite specific 
in terms of content: they refer to artist names, 
titles, dates, and objects (Koolen and Kamps, 
2010; Ireson and Oomen, 2007). Take the exam-
ple of a query like ?coupe apollon? (?bowl apol-
lon?). While in standard analysis ?coupe? would 
be identified as a verb (?couper?, i.e. ?to cut?), in 
the context of a query it should be actually 
tagged as a noun, which refers to an object. Such 
a difference may lead to different preprocessing 
and worse retrieval. 
In this paper, we propose an approach to solv-
ing this problem by adapting a complete and in-
tegrated chain of NLP tools, based on the Xerox 
Incremental Parser (XIP), to make it suitable for 
queries? analysis. The adaptation includes recapi-
talization, adapted Part of Speech (PoS) tagging, 
adapted chunking and Named Entities (NE) rec-
ognition. We claim that several heuristics espe-
cially important for queries? analysis, such as 
favoring nominal interpretations, result in im-
proved linguistic structures, which can have an 
impact in a wide range of further applications 
(e.g. information retrieval, query translation, in-
formation extraction, query reformulation etc.). 
2 Prior art 
The problem of adapted query processing, often 
referred to as structural query annotation, in-
cludes capitalization, NEs detection, PoS tagging 
and query segmentation. Most of the existing 
works treat each of these steps independently and 
address only one of the above issues. 
Many works address the problem of query 
segmentation. According to Tan and Peng 
(2008), query segmentation is a problem which is 
close to the chunking problem, but the chunking 
problem is directly linked to the PoS tagging re-
sults, which are often noisy for the queries. Thus, 
most of the works on query segmentation are 
based on the statistical interaction between a pair 
of query words to identify the border between the 
segments in the query (Jones et al, 2006; Guo et 
55
al., 2008). Tan and Peng (2008) propose a gen-
erative language model enriched with Wikipedia 
to identify ?concepts? rather than simply ?fre-
quency-based? patterns. The segmentation pro-
posed by Bergsma and Wang (2007) is closer to 
the notion of NP chunking. They propose a ma-
chine-learned query segmentation system trained 
on manually annotated set of 500 AOL queries. 
However, in this work PoS tagging is used as one 
of the features in query segmentation and is done 
with a generic PoS tagger, non adapted for que-
ries. 
PoS tagging is an important part of query 
processing and used in many information ana-
lytics tasks (query reformulation, query segmen-
tation, etc.). However very few works address 
query-oriented PoS tagging. Allan and Raghavan 
(2002) consider that PoS tagging might be am-
biguous for short queries and propose to interact 
with the user for disambiguation. Barr et al 
(2008) produce a set of manually annotated que-
ries, and then train a Brill tagger on this set in 
order to create an adapted PoS tagger for search 
queries. 
A notable work is the one by Bendersky et al 
(2010), which addresses the capitalization, PoS 
tagging and query segmentation in the same pa-
per. However, this approach proposes for each of 
the above steps a probabilistic model that relies 
on the document corpus rather on the query it-
self. Such an approach is not applicable for most 
digital content providers who would reluctantly 
give access to their document collection. More-
over, the query expansion, which is the central 
idea of the described approach, is not possible for 
most of digital libraries that are organized in a 
database. Secondly, Bendersky et al (2010) pro-
poses adapting each processing step independ-
ently. Although this is not mentioned in the 
paper, these three steps can be applied in a se-
quence, where PoS tagging can profit from the 
recapitalization, and chunking from the PoS tag-
ging step. However, once the recapitalization is 
done, it can not be changed in the following 
steps. This work doesn?t address the adaptation 
of the NE recognition component, as we do, and 
which might change the final chunking and PoS 
tagging in certain cases. 
In our approach, part of the recapitalization is 
done during the PoS tagging, in interaction with 
the NE recognition, which allows us to consider 
these two steps as interleaved. Moreover, the 
linguistic processing we propose is generic: cor-
pus-independent (at least most of its parts except 
for NE recognition) and doesn?t require access to 
the document collection. 
3 Data 
This work is based on search logs from Euro-
peana 1
4 Motivation 
. These are real users? queries, where 
Named Entities are often lowercased and the 
structures are very different from normal phrase 
structure. Thus, this data is well adapted to dem-
onstrate the impact of adapted linguistic process-
ing. 
We show the importance of the adapted linguistic 
query processing using as example the task of 
query translation, a real need for today?s digital 
content providers operating in a multilingual en-
vironment. We took a sample of Europeana que-
ries and translated them with different MT 
systems: in-house (purely statistical) or available 
online (rule-based). Some examples of problem-
atic translations are shown in the Table 1. 
 
 Input query Automatic 
Translation 
Human 
translation 
 French-English 
1 
 
journal pano-
rama paris  
newspaper 
panorama bets 
newspaper 
panorama 
paris   
2 saint jean de 
luz 
saint jean of 
luz 
saint jean 
de luz 
3 vie et mort 
de l?image 
life and died of 
the image 
life and 
death of 
image 
4 langue et 
r?alit? 
and the reality 
of language 
language 
and reality 
 English-French 
5 maps europe trace l?Europe cartes de 
l?Europe 
6 17th century 
saw 
Du 17?me 
si?cle a vu 
scie du 
17?me 
si?cle 
7 chopin 
george sand 
george sable 
chopin soit 
chopin 
george 
sand 
Table 1: Examples of the problematic query 
translations 
 
                                                                 
1 A portal that acts as an interface to millions of digitized 
records, allowing users to explore Europe?s cultural heri-
tage. For more information please visit 
http://www.europeana.eu/portal/ 
56
Although in general, the errors done by statis-
tical and rule-based models are pretty different, 
there are some common errors done in the case 
of the query translation. Both models, being de-
signed for full-sentence translation, find the 
query structure very unnatural and tend to repro-
duce the full sentence in the output (ex. 1, 3, 4, 5, 
6). The errors may come either from a wrong 
PoS tagging (for rule-based systems), or from the 
wrong word order (statistical-based systems), or 
from the choice of the wrong translation (both 
types of systems). 
One might think that the word order problem 
is not crucial for queries, because most of the IR 
models use the bag of words models, which ig-
nore the order of words. However, it might mat-
ter in some cases: for example, if and/or are 
interpreted as a logical operator, it is important to 
place them correctly in the sentence (examples3, 
4).  
Errors also may happen when translating NEs 
(ex.1, 2, 7). The case information, which is often 
missing in the real-life queries, helps to deal with 
the NEs translation. 
The examples mentioned above illustrate that 
adapted query processing is important for a task 
such as query translation, both in the case of 
rule-based and empirical models. Although the 
empirical models can be adapted if an appropri-
ately sized corpus exists, such a corpus is not 
always available.  
Thus we propose adapting the linguistic proc-
essing prior to query translation (which is further 
integrated in the SMT model). We demonstrate 
the feasibility and impact of our approach based 
on the difference in translation quality but the 
adaptations can be useful in a number of other 
tasks involving query processing (e.g. question 
answering, query logs analysis, etc.). 
5 Linguistic Processing Adaptation 
As said before, queries have specific linguistic 
properties that make their analysis difficult for 
standard NLP tools. This section describes the 
approach we have designed to improve query 
chunking. Following a study of the corpus of 
query logs, we rely on the specific linguistic 
properties of the queries to adapt different steps 
of linguistic analysis, from preprocessing to 
chunking.  
These adaptations consist in the following 
very general processes, for both English and 
French: 
Recapitalization: we recapitalize, in a preproc-
essing step, some uncapitalized words in queries 
that can be proper nouns when they start with a 
capital letter. 
Part of Speech disambiguation
? the part of speech tagging favors nominal 
interpretation (whereas standard part of 
speech taggers are designed to find a verb in 
the input, as PoS tagging generally applies 
on complete sentences); 
:  
? the recapitalization information transmitted 
from the previous step is used to change the 
PoS interpretation in some contexts. 
Chunking
? considering that a full NE is a chunk, which 
is not the case in standard text processing, 
where a NE can perfectly be just a part of a 
chunk; 
:  the chunking is improved by: 
? grouping coordinated NEs of the same type; 
? performing PP and AP attachment with the 
closest antecedent that is morphologically 
compatible 
These processes are very general and may ap-
ply to queries in different application domains, 
with maybe some domain-dependent adaptations 
(for example, NEs may change across domains). 
These adaptations have been implemented 
within the XIP engine, for the French and Eng-
lish grammars. The XIP framework allows inte-
grating the adaptations of different steps of query 
processing into a unified framework, where the 
changes from one step can influence the result of 
the next step: the information performed at a 
given step is transmitted to the next step by XIP 
through linguistic features. 
5.1 Preprocessing 
Queries are often written with misspelling errors, 
in particular for accents and capital letters of 
NEs. See the following query examples extracted 
from our query log corpus: 
 
lafont Robert (French query) 
henry de forge et jean maucl?re 
(French query) 
muse prado madrid (French query) 
carpaccio queen cornaro (English 
query) 
man ray (English query) 
 
This might be quite a problem for linguistic 
treatments, like PoS tagging and of course NE 
57
recognition, which often use capital letter infor-
mation as a triggering feature.  
Recapitalizing these words at the preprocess-
ing step of a linguistic analysis, i.e. during the 
morphological analysis, is technically relatively 
easy, however it would be an important generator 
of spurious ambiguities in the context of full sen-
tence parsing (standard context of linguistic pars-
ing). Indeed, considering that all lower case 
words that can be proper nouns with a capital 
letter should also have capitalized interpretation, 
such as price, jean, read, us, bush, lay, etc., in 
English or pierre, m?decin, ? in French) would 
be problematic for a PoS tagger as well as for a 
NE recognizer. That?s why it is not performed in 
a standard analysis context, considering also that 
misspelling errors are not frequent in ?standard? 
texts. In the case of queries however, they are 
frequent, and since queries are far shorter in av-
erage than full sentences the tagging can be 
adapted to this context (see next section), we can 
afford to perform recapitalization using the fol-
lowing methodology, combining lexical informa-
tion and contextual rules: 
1. The preprocessing lexicon integrates all 
words starting with a lower case letter 
which can be first name (henry, jean, 
isaac ?), family and celebrity name 
(chirac, picasso...) and  place names 
(paris, saint p?tersbourg, ?) when capi-
talized. 
2. When an unknown word starting with a 
lower case letter is preceded by a first 
name and eventually by a particle (de, 
van, von ?), it is analyzed as a last 
name, in order to be able to trigger stan-
dard NE recognition. This is one exam-
ple of interleaving of the processes: here 
part-of-speech interpretation is condi-
tioned by the recapitalization steps which 
transmits information about recapitaliza-
tion (via features within XIP) that trig-
gers query-specific pos disambiguation 
rules. 
The recapitalization (1) has been imple-
mented within the preprocessing components of 
XIP within finite state transducers (see (Kart-
tunen, 2000)). The second point (2) is done di-
rectly within XIP in the part-of-speech tagging 
process, with a contextual rule.  For example, the 
analysis of the input query ?jean maucl?re? gets 
the following structure and dependency output 
with the standard French grammar. 
 
Query: jean maucl?re 
NMOD(jean, maucl?re) 
0>GROUP[NP[jean] AP[maucl?re]] 
 
Because jean is a common noun and maucl?re 
is an unknown word which has been guessed as 
an adjective by the lexical guesser. 
It gets the following analysis with the pre-
processing adaptations described above: 
 
NMOD(jean,maucl?re) 
PERSON_HUM(jean maucl?re) 
FIRSTNAME(jean,jean maucl?re) 
LASTNAME(maucl?re,jean maucl?re) 
0>GROUP[NP[NOUN[jean maucl?re]]] 
 
Because jean has been recognized as a first 
name and consequently the unknown word after 
has been inferred has a proper noun (last name) 
by the pos tagging contextual rule; the recapitali-
zation process and part-of-speech interpretation 
are therefore interleaved. 
5.2 Part of speech disambiguation 
In the context of query analysis, part-of-speech 
tagging has to be adapted also, since standard 
part-of-speech disambiguation strategies aim 
generally at disambiguating in the context of full 
sentences. But queries are very different from 
full sentences: they are mostly nominal with 
sometimes infinitive, past participial, or gerun-
dive insertions, e.g.: 
 
statuettes hommes jouant avec un 
chien (French query) 
coupe apollon (French query) 
architecture musique (French 
query) 
statue haut relief grecque du 5 
siecle (French query)  
david playing harp fpr saul (Eng-
lish query) 
stained glass angel (English 
query) 
 
Standard techniques for part-of-speech tag-
ging include rule based methods and statistical 
methods, mainly based on hidden Markov mod-
els (see for example (Chanod and Tapanainen, 
1995)). In this case, it would be possible to re-
compute the probabilities on a corpus of queries 
manually annotated. However, the correction of 
part-of-speech tags in the context of queries is 
easy to develop with a small set of rules. We fo-
cus on English and French, and in queries, the 
main problems come from the ambiguity be-
58
tween noun and verbs, which has to be solved 
differently than in the context of a standard sen-
tence.   
The approach we adopt to correct the tagging 
with the main following contextual rules: 
? If there is a noun/verb ambiguity: 
? If the ambiguity is on the first word of 
the query (e.g. ?coupe apollon?, ?oil
? If the ambiguity is on the second word of 
the query, prefer the noun interpretation 
if the query starts with an adjective or a 
noun (e.g. in ?young 
 
flask?), select the noun interpretation; 
people
? Select noun interpretation if there is no 
person agreement with one of the previ-
ous nouns (e.g. ?les fr?res 
 social com-
petences?, select the noun interpretation 
for people, instead of verb) 
bissons
? For a verb which is neither at the past 
participle form nor the infinitive form, 
select the noun interpretation if it is not 
followed by a determiner (e.g. ?tremble-
ment 
?, 
fr?res belongs to the 3rd person but bis-
sons to the 1st one of the verb ?bisser?) 
terre
? Choose the noun interpretation if the 
word is followed by a conjunction and a 
noun or preceded by a noun and a con-
junction (e.g. in ?gauguin 
 lisbonne?, terre is disambigu-
ated as a noun?)) 
moon and 
earth?, choose the noun interpretation 
for moon, instead of verb2
? In case of ambiguity between adjective and 
past participle verb, select the adjective in-
terpretation if the word is followed by a 
noun (e.g.  ?stained glass angel?, stained is 
disambiguated as an adjective instead of a 
past participle verb) 
). 
5.3 Chunking 
The goal of chunking is to assign a partial 
structure to a sentence and focuses on easy to 
parse pieces in order to avoid ambiguity and re-
cursion. In the very specific context of query 
analysis, and once again since queries have spe-
cific linguistic properties (they are not sentences 
but mostly nominal sequences), chunking can be 
improved along several heuristics. We propose 
here some adaptations to improve query chunk-
                                                                 
2To moon about 
ing to deal with AP and PP attachment, and co-
ordination, using also NE information to guide 
the chunking strategy. 
AP and PP attachment 
In standard cases of chunking, AP and PP at-
tachment is not considered, because of attach-
ment ambiguity problems that cannot be solved 
at this stage of linguistic analysis.  
Considering the shortness of queries and the 
fact that they are mostly nominal, some of these 
attachments can be solved however in this con-
text. 
For the adjectival attachment in French, we 
attach the post modifier adjectival phrases to the 
first previous noun with which there is agreement 
in number and gender. For example, the chunk-
ing structure for the query ?Biblioth?que eu-
ropeenne numerique? is: 
 
NP[ [Biblioth?que AP[europeenne] 
AP[numerique] ] 
 
while it is  
 
NP[Biblioth?que] AP[europeenne] 
AP[numerique] 
 
with our standard French grammar.  
For PP attachment, we simply consider that 
the PP attaches systematically to the previous 
noun. For example, the chunking structure for 
?The history of the University of Oxford? is: 
 
NP[the history PP[of the University 
PP[of Oxford] ] ]  
 
instead of: 
 
NP[The history] PP[of the Univer-
sity] PP[of  Oxford ] 
Coordination 
Some cases of coordination, usually very com-
plex, can be solved in the query context, in par-
ticular when NEs are involved. For both English 
and French, we attach coordinates when they 
belong to the same entity type (person conj per-
son, date conj date, place conj place, etc.), for 
example, ?vase achilles et priam? :  
 
NP[vase] NP[Achille et Priam]  
 
instead of: 
 
NP[vase] NP[Achille] et NP[Priam] 
59
 
We also attach coordinates when the second 
is introduced by a reflexive pronoun, such as in: 
?[Le laboureur et ses enfants] La Fontaine? and 
attach coordinates within a PP when they are in-
troduced by the preposition ?entre? in French 
and ?between? in English. 
Use of NE information to guide the chunking 
strategy 
We also use information about NEs present in 
the queries to guide the query chunking strategy. 
In standard analysis, NEs are generally part of 
larger chunking units. In queries, however, be-
cause of their strong semantic, they can be iso-
lated as separate chunking units. We have 
adapted our chunking strategy using this infor-
mation: when the parser detects a NE (including 
a date), it chunks it as a separate NP. The follow-
ing examples show the chunking results for this 
adapted strategy versus the analysis of standard 
grammar: 
 
? ?Anglo Saxon 11th century? (English) 
Adapted chunking:  
NP[Anglo Saxon] NP[ 11th century] 
 
Standard chunking:  
NP[Anglo Saxon 11th century ] 
 
? ?Alexandre le Grand Persepolis?  (French) 
Adapted chunking:  
NP[Alexandre le Grand] NP[Perspolis] 
 
Standard chunking: 
NP[Alexandre le Grand Perspolis] 
 
The whole process is illustrated in Figure 1.  
 
When applying the full chain on an example 
query like ?gauguin moon and earth?, we have 
the following steps and result: 
Preprocessing: gauguin is recognized as Gauguin 
(proper noun of celebrity); 
Part of speech tagging:  moon is disambiguated 
as a noun instead of a verb); 
Chunking:
So we get the following structure:  
 moon and earth are grouped together 
in a coordination chunk, gauguin is a NE 
chunked separately.  
 
NP[Gauguin] NP[moon and earth] 
 
and gauguin is recognized as a person name, 
instead of  
 
SC 3 [NP[gauguin] FV 4
 
Input query 
Query 
Preprocessing 
Query POS 
Disambiguation 
Query  
Chunking 
Adapted lexical 
Resources combined 
with contextual rules 
Improved query structure 
Adapted strategy for 
POS tagging 
(Contextual rules) 
Adapted Chunking 
strategy: contextual 
rules + named entities  
[moon]] and 
NP[earth],  
 
gauguin remaining unknown,  with the standard 
English grammar. 
 
Fig 1: Linguistic processing adaptation for que-
ries 
 
5.4 Examples of query structures 
The following table shows the differences of 
query structures obtained with the standard lin-
guistic processing and with the adapted linguistic 
processing.  
 
1. Albert Camus la peste 
Standard LP: NP {albert}  AP {camus}  
NP {la peste}   
Adapted LP: NP {albert camus}  NP {la 
peste}   
2. dieux ou h?ros grec 
Standard LP: NP {dieux}  COORD {ou}  
NP {h?ros}  AP {grec}   
                                                                 
3 SC: chunk tag for sentential clause 
4 FV: finite verb chunk 
60
Adapted LP: NP {dieux}  COORD {ou}  
NP {h?ros grec}   
3. pierre berg? 
Standard LP: NP {pierre}  VERB {berg?}   
Adapted LP: NP {pierre berg?} 
Table 2:  Some examples of query structure produced 
by standard and adapted linguistic processing. 
 
The evaluation of this customization is done 
indirectly through query translation, and is de-
scribed in the next section 
6 Experiments 
6.1 Experimental settings 
In our experiments we tried to enrich our base-
line SMT system with an adapted linguistic proc-
essing in order to improve the query translation. 
These experiments have double goal. First, to 
show that the adapted linguistic processing al-
lows to improve query translation compared to a 
standard linguistic processing, and second, to 
show that enriching an SMT model with a lin-
guistic processing (adapted) is helpful for the 
translation.  
We use an open source toolkit Moses (trained 
on Europarl) as a baseline model for query trans-
lations. Based on the examples from the section 
5, we choose to integrate the chunking and NE 
information in the translation. We integrate this 
knowledge in the following way: 
? Chunking: We check whether the query 
matches one of the following patterns: ?NP1 
and NP2?, ?NP1 or NP2?, ?NP1 NP2?, 
?NP1, NP2?, etc. If it is the case, the NPs are 
translated independently.  Thus, we make 
sure that the output query will preserve the 
logical structure, if ?and/or? are treated as 
logical operators. Also, translating NPs inde-
pendently might result at different (hopefully 
better) lexical choices.  
? Named entities: We introduce XML tags for 
person names where we propose a possible 
translation. During the translation process the 
proposed translation competes with the pos-
sible translations from a bi-phrase library. 
The translation maximizing internal transla-
tion score is chosen. In these experiments we 
propose not to translate an NE at all, how-
ever in more general case we could imagine 
having an adapted NE dictionary.  
6.2 Evaluation 
We have translated the totality of available 
Europeana French logs to English (8870 distinct 
queries), with the following translation models:  
? Moses trained on Europarl (Baseline 
MT) 
? Baseline MT model enriched with lin-
guistic processing (as defined in 6.1) 
based on basic grammar (Baseline MT + 
basic grammar) 
? Baseline MT enriched with linguistic 
processing based on adapted grammar 
(Baseline MT + adapted grammar) 
Our approach brings two new aspects com-
pared to simple SMT system. First, an SMT sys-
tem is enriched with linguistic processing as 
opposed to system without linguistic processing 
(baseline system), second: usage of an adapted 
linguistic processing as opposed to standard lin-
guistic processing. Thus, we evaluate: 
1. The impact of linguistic processing on 
the final query translations; 
2. The impact of grammar adaptation 
(adapted linguistic processing) in the 
context of query translation. 
First, we measure the overall impact of each 
of the two aspects mentioned above. Table 3 re-
flects the general impact of linguistic enrichment 
and grammar adaptation on query structure and 
translation. 
First, we note that the linguistic processing as 
defined in 6.1 won?t be applied to all queries. 
Thus, we count an amount of queries out of our 
test set to which this processing can actually be 
applied. This corresponds to the first line of the 
Table 3 (26% of all queries). 
Second, we compare the queries translation 
with and without linguistic processing. This is 
shown in the second line of the Table 3: the 
amount of queries for which the linguistic proc-
essing lead to different translation (25% of que-
ries for which the linguistic processing was 
applied). 
The second part of the table shows the differ-
ence between the standard linguistic processing 
and an adapted linguistic processing. First, we 
check how many queries get different structure 
after grammar adaptation (Section 5) (~42%) and 
second, we check how many of these queries 
61
actually get different translation (~16% queries 
with new structure obtained after adaptation get 
different translations).  
These numbers show that the linguistic 
knowledge that we integrated into the SMT 
framework may impact a limited portion of que-
ries? translations. However, we believe that this 
is due, to some extent, to the way the linguistic 
knowledge was integrated in SMT, which ex-
plores only a small portion of the actual linguis-
tic information that is available. We carried out 
these experiments as a proof of concept for the 
adapted linguistic processing, but we believe that 
a deeper integration of the linguistic knowledge 
into the SMT framework will lead to more sig-
nificant results. For example, integrating such an 
adapted linguistic processing in a rule-based MT 
system will be straightforward and beneficial, 
since the linguistic information is explored di-
rectly by a translation model (e.g. in the example 
6 in Table 1 tagging "saw" as a noun will defi-
nitely lead to a better translation). 
Next, we define 2 evaluation tasks, where the 
goal of each task is to compare 2 translation 
models. We compare:  
1. Baseline MT versus linguistically en-
riched translation model (Baseline MT+adapted 
adapted linguistic processing). This task evalu-
ates the impact of linguistic enrichment in the 
query translation task with SMT.  
2. Translation model using standard lin-
guistic processing versus translation model using 
adapted linguistic processing. This task evalu-
ates the impact of the adapted linguistic process-
ing in the query translation task.  
For each evaluation task we have randomly 
selected a sample of 200 translations (excluding 
previously the identical translations for the 2 
models compared) and we perform a pairwise 
evaluation for each evaluation task. Thus, for the 
first evaluation task, a baseline translation (per-
formed by standard Moses without linguistic 
processing) is compared to the translation done 
by Moses + adapted linguistic processing. In the 
second evaluation task, the translation performed 
by Moses + standard linguistic processing is 
compared to the translation performed by Moses 
+ adapted linguistic processing. 
The evaluation has been performed by 3 
evaluators. However, no overlapping evaluations 
have been performed to calculate intra-evaluators 
agreement. We could observe, however, the simi-
lar tendency for improvement in each on the 
evaluated sample (similar to the one shown in the 
Table 2).  
We evaluate the overall translation perform-
ance, independently of the task in which the 
translations are going to be used afterwards (text  
Table 3: Impact of linguistic processing and 
grammar adaptation for query translation 
 
understanding, text analytics, cross-lingual in-
formation retrieval etc.) 
The difference between slight improvements 
and important improvements as in the examples 
below has been done during the evaluation. 
 
src1: max weber 
t1:max mr weber 
t2:max weber (slight improvement)
   
src2: albert camus la peste 
t1:albert camus fever 
t2:albert camus the plague (impor-
tant improvement) 
 
Thus, each pair of translations (t1, t2) re-
ceives a score from the scale [-2, 2] which can 
be: 
? 2, if t2 is much better than t1,  
? 1, if t2 is better than t1,  
? 0, if t2 is equivalent to t1,  
? -1, if t1 is better than t2,  
? -2, if t1 is much better than t2,  
Linguistic enrichment 
Nb of  queries to which the adapted 
linguistic processing was applied 
before translation.  
2311 
(26% of 
8870) 
Nb of translations which differ 
between baseline Moses and Moses 
with adapted linguistic processing.  
582  
(25% of 
2311) 
Grammar adaptation 
Nb of queries which get different 
structures between standard linguistic 
processing and adapted linguistic 
processing. 
3756  
(42% of 
8870) 
Nb of translations which differ 
between Moses+standard linguistic 
processing and Moses+adapted 
linguistic processing 
638   
(16 %  of 
3756) 
62
Table 4 presents the results of translation 
evaluation. 
 
Note, that a part of slight decreases can be 
corrected by introducing an adapted named enti-
ties dictionary to the translation system. For ex-
ample, for the source query ?romeo et juliette?, 
keeping NEs untranslated results at the following 
translation: ?romeo and juliette?, which is con-
sidered as a slight decrease in comparison to a 
baseline translation: ?romeo and juliet?. Creating 
an adapted NEs dictionary, either by crawling 
Wikipedia, or other parallel resources, might be 
helpful for such cases. 
Often, the cases of significantly better transla-
tions could potentially lead to the better retrieval. 
For example, a better lexical choice (don juan 
moliere vs. donation juan moliere, the plague vs. 
fever) often judged as significant improvement 
may lead to a better retrieval. 
Based on this observation one may hope that 
the adapted linguistic processing can indeed be 
useful in the query translation task in CLIR con-
text, but also in general query analysis context. 
7 Conclusion 
Queries posed to digital library search engines in 
the cultural heritage and social sciences domain 
tend to be very short, referring mostly to artist 
names, objects, titles, and dates. As we have il-
lustrated with several examples, taken from the 
logs of the Europeana portal, standard NLP 
analysis is not well adapted to treat that domain. 
In this work we have proposed adapting a com-
plete chain of linguistic processing tools for 
query processing, instead of using out-of-the-box 
tools designed to analyze full sentences. 
Focusing on the cultural heritage domain, we 
translated queries from the Europeana portal us-
ing a state-of-the-art machine translation system 
and evaluated translation quality before and after 
applying the adaptations. The impact of the lin-
guistic adaptations is quite significant, as in 42% 
of the queries the resulting structure changes. 
Subsequently, 16% of the query translations are 
also different. The positive impact of the adapted 
linguistic processing on the translation quality is 
evident, as for 99 queries the translation (out of 
200 sample evaluated) is improved when com-
pared to having no linguistic processing. We ob-
serve also that 78 queries are better translated 
after adapting the linguistic processing compo-
nents. 
Our results show that customizing the linguis-
tic processing of queries can lead to important  
 
improvements in translation (and eventually to 
multilingual information retrieval and data min-
ing). A lot of the differences are related to the 
ability of properly identifying and treating do-
main-specific named entities. We plan to further 
research this aspect in future works. 
 
Acknowledge ments  
This research was supported by the European 
Union?s ICT Policy Support Programme as part 
of the Competitiveness and Innovation Frame-
work Programme, CIP ICT-PSP under grant 
agreement nr 250430 (Project GALATEAS). 
References 
Bin Tan and Fuchun Peng. 2008. Unsupervised query 
segmentation using generative language models 
and wikipedia. In Proceedings of the 17th interna-
tional conference on World Wide Web (WWW 
'08). ACM, New York, NY, USA, 347-356. 
Cory Barr, Rosie Jones, Moira Regelson. 2008. The 
Linguistic Structure of EnglishWeb-Search Que-
ries, Proceedings of ENMLP'08, pp 1021?1030, 
Octobre 2008, Honolulu. 
James Allan and Hema Raghavan. 2002. Using part-
of-speech patterns to reduce query ambiguity. In  
Proceedings of the 25th annual international ACM 
SIGIR conference on Research and development in  
informat ion retrieval (SIGIR '02). ACM, New 
York, NY, USA, 307-314. 
Jeann-Pierre Chanod, Pasi Tapanainen. 1995.  Tag-
ging French - comparing a statistical and a con-
straint-based method. Proc. From Texts To Tags: 
 Impor
tant 
++ 
 Total 
nb+ 
Impo
rtant 
- - 
Total  
nb - 
Overall 
impact   
Moses<
Moses+
adapted  
35 87 4 19 99 
Moses+
basic<
Moses+
adapted  
28 66 2 12 80 
Table 4: Translation evaluation. Total nb+ (-): total 
number of improvements (decreases), not distinguish-
ing whether it is slight or important; important ++ (--): 
the number of important improvements (decreases). 
Overall impact = (Total nb+) + (Importan++ ) ? (Total 
nb-) ? (Important --) 
63
Issues In Multilingual Language Analysis, EACL 
SIGDAT workshop. Dublin, 1995. 
Jiafeng Guo, Gu Xu, Hang Li, Xueqi Cheng. 2008. A 
Unified and Discriminative Model for Query Re-
finement. Proc. SIGIR?08, July 20?24, 2008, Sin-
gapore. 
Josiane Mothe and Ludovic Tanguy. 2007. Linguistic 
Analysis of Users' Queries: towards an adaptive In-
formation Retrieval System. International Confer-
ence on Signal-Image Technology & Internet?
Based Systems, Shangai, China, 2007. 
http://halshs.archives-ouvertes.fr/halshs-
00287776/fr/ [Last accessed March 3, 2011]  
Lauri Karttunen. 2000. Applications of Finite-State 
Transducers in Natural Language Processing. Pro-
ceedings of CIAA-2000. Lecture Notes in Com-
puter Science. Springer Verlag. 
Marijn Koolen and Jaap Kamps. 2010. Searching cul-
tural heritage data: does structure help expert  
searchers?. In Adaptivity, Personalizat ion and Fu-
sion of Heterogeneous Information (RIAO '10). Le 
centre des hautes etudes internationals 
d?informat ique documentaire, Paris, France, 152-
155. 
Michael Bendersky, W. Bruce Croft and David A. 
Smith. 2010. Structural Annotation of Search Que-
ries Using Pseudo-Relevance Feedback. Proceed-
ings of CIKM'10, October 26-29, 2010, Toronto, 
Ontario, Canada 
Neil Ireson and Johan Oomen. 2007. Capturing e-
Culture: Metadata in MultiMatch., J. In Proc. 
DELOS-MultiMatch workshop, February 2007, 
Tirrenia, Italy. 
Rosie Jones, Ben jamin Rey, Omid Madani, and Wiley 
Greiner. 2006. Generating query substitutions. In 
Proceedings of the 15th international conference on 
World Wide Web (WWW '06). ACM, New York, 
NY, USA, 387-396. 
Shane Bergsma and Qin Iris Wang. 2007. Learning 
Noun Phrase Query Segmentation, Proceedings of 
the 2007 Jo int Conference on Empirical Methods 
in Natural Language Processing and Computational 
Natural Language Learning, pp. 819?826, Prague, 
June 2007. 
64
