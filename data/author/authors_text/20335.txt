Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 517?521,
Dublin, Ireland, August 23-24, 2014.
SAP-RI: A Constrained and Supervised Approach for Aspect-Based
Sentiment Analysis
Nishtha Malhotra
1,2?
, Akriti Vij
1,2,?
, Naveen Nandan
1
and Daniel Dahlmeier
1
1
Research & Innovation, SAP Asia, Singapore
2
Nanyang Technological University, Singapore
{nishtha.malhotra,akriti.vij,naveen.nandan,d.dahlmeier}@sap.com
Abstract
We describe the submission of the SAP
Research & Innovation team to the Se-
mEval 2014 Task 4: Aspect-Based Senti-
ment Analysis (ABSA). Our system fol-
lows a constrained and supervised ap-
proach for aspect term extraction, catego-
rization and sentiment classification of on-
line reviews and the details are included in
this paper.
1 Introduction
The increasing popularity of the internet as a
source of information, and e-commerce as a way
of life, has led to a major surge in the number of
reviews that can be found online, for a wide range
of products and services. Consequently, more and
more consumers have taken to consulting these on-
line reviews as part of their pre-purchase research
before deciding on availing services from a local
business or investing in a product from a particu-
lar brand. This calls for innovative techniques for
the sentiment analysis of online reviews so as to
generate accurate and relevant recommendations.
Sentiment analysis has been extensively studied
and applied in different domains. Predicting the
sentiment polarity (positive, negative, neutral) of
user opinions by mining user reviews (Hu and Liu,
2004; Liu, 2012; Pang and Lee, 2008; Liu, 2010)
has been of high commercial and research interest.
In these studies, sentiment analysis is often con-
ducted at one of the three levels: document level,
sentence level or attribute level.
Through the SemEval 2014 Task 4 on Aspect
Based Sentiment Analysis (Pontiki et al., 2014),
we explore sentiment analysis at the aspect level.
?
The work was done during an internship at SAP.
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
The task consists of four subtasks: in subtask 1 as-
pect term extraction, participants need to identify
the aspect terms present in a sentence and return
a list containing all distinct aspect terms, in sub-
task 2 aspect term polarity, participants were to
determine the polarity of each aspect term in a sen-
tence, in subtask 3 aspect category detection, par-
ticipants had to identify the aspect categories dis-
cussed in a given sentence, and in subtask 4 aspect
category polarity, participants were to determine
the polarity of each aspect category. The polarity
classification subtasks consider sentiment analysis
to be a three-way classification problem between
positive, negative and neutral sentiment. On the
other hand, the aspect category detection subtask
is a multi-label classification problem where one
sentence can be labelled with more than one as-
pect category.
In this paper, we describe the submission of the
SAP-RI team to the SemEval 2014 Task 4. We
make use of supervised techniques to extract the
aspects of interest (Jakob and Gurevych, 2010),
categorize them (Lu et al., 2011) and predict the
sentiment of customer online reviews on Laptops
and Restaurants. We developed a constrained sys-
tem for aspect-based sentiment analysis of these
online reviews. The system is constrained in the
sense that we only use the training data that was
provided by the challenge organizers and no other
external data sources. Our system performed rea-
sonably well, especially with a F
1
score of 75.61%
for the aspect category polarity subtask, 79.04%
F
1
score on the aspect category detection task and
66.61% F
1
score on the aspect term extraction
task.
2 Subtask 1: Aspect Term Extraction
Given a review with annotated entities in the train-
ing set, the task was to extract the aspect terms for
reviews in the test set. For this subtask, training,
development and testing were conducted for both
517
the laptop and the restaurant domain.
2.1 Features
Each review was represented as a feature vector
made up of the following features:
? Word N-grams: all unigrams, bigrams and
trigrams from the review text
? Casing: presence or absence of capital case/
title case words
? POS tags: POS tags of a word and its neigh-
bours
? Parse dependencies and relations: parse
dependency relations of the aspects, i.e.,
presence/absence of adjectives and adverbs in
the dependency parse tree
? Punctuation Marks: presence/absence of
punctuation marks, such as ?, !
2.2 Method
We approach the task by casting it as a sequence
tagging task where each token in a candidate sen-
tence is labelled as either Beginning, Inside or
Outside (BIO). We then employ conditional ran-
dom fields (CRF), which is a discriminative, prob-
abilistic model for sequence data with state-of-the-
art performance (Lafferty et al., 2001). A linear-
chain CRF tries to estimate the conditional prob-
ability of a label sequence y given the observed
features x, where each label y
t
is conditioned on
the previous label y
t?1
. In our case, we use BIO
CoNLL-style tags (Sang and De Meulder, 2003).
During development, we split the training data
in the ratio of 60:20:20 as training, development
(dev) and testing (dev-test). We train the CRF
model on the training set of the data, perform
feature selection based on the dev set, and test
the resulting model on the dev-test. In all ex-
periments, we use the CRF++
1
implementation
of conditional random fields with the parameter
c=4.0. This value was chosen based on manual
observation. We perform a feature ablation study
and the results are reported in Table 1. Features
listed in section 2.1 were those that were retained
for the final run.
1
code.google.com/p/crfpp/
3 Subtask 2: Aspect Term Polarity
Estimation
For this subtask, the training, development and
testing was done using reviews on laptops and
restaurants. Given the aspect terms in a sentence,
the task was to predict their sentiment polarities.
3.1 Features
For each review, we used the following features:
? Word N-grams: all lowercased unigrams,
bigrams and trigrams from the review text
? Polarity of neighbouring adjectives: ex-
tracted word sentiment from SentiWordNet
lexicon (Baccianella et al., 2010)
? Neighbouring POS tags: the POS tags of up
to neighbouring 3 words
? Parse dependencies and relations: parse
dependency relations of the aspects, i.e.,
presence/absence of adjectives and adverbs in
the dependency parse tree
3.2 Method
For each aspect term of a sentence, the afore-
mentioned features were extracted. For exam-
ple, for the term Sushi in the sentence Sushi
was delicious., the following feature vector is
constructed, {aspect: ?sushi?, advmod:?null?,
amod:?delicious?, uni sushi: 1, uni was: 1,
uni delicious, uni the: 0, .. }.
We then treat the aspect sentiment polarity es-
timation as a multi-class classification task where
each instance would be labelled as either positive,
negative or neutral. For the classification task, we
experimented with Naive Bayes and Support Vec-
tor Machines (SVM) ? both linear and RBF ker-
nels ? and it was observed that linear SVM per-
formed best. Hence, we use linear SVM for the
classification task. Table 2 summarizes the results
obtained from our experiments for various feature
combinations. The classifiers used are implemen-
tations from scikit-learn
2
, which is also used for
the remaining tasks.
4 Subtask3: Aspect Category Detection
Given a review with annotated entities or aspect
terms, the task was to predict the aspect categories.
2
scikit-learn.org/stable/
518
Features Precision Recall F1-Score
N-grams, POS tags 0.7655 0.4283 0.5496
N-grams, Parse relations, POS tags 0.8192 0.6641 0.7336
N-Grams, Parse relations, POS tags, casing 0.8101 0.6641 0.7299
N-grams, Parse relations, POS tags, ! 0.8116 0.6641 0.7305
N-grams, Parse relations, POS tags,!, ? 0.8123 0.6672 0.7326
Table 1: Training-phase experimental results for Subtask 1 on Restaurant reviews.
Features Laptops Restaurants
Neighbouring words, 2,3 POS grams, bigrams, trigrams, Sentiment,1,2 ngram lower 0.4196 0.5997
Parse Relations, 2,3 POS grams, bigrams, trigrams, Sentiment, 1,2 ngram lower 0.5869 0.6375
Parse Relations, Neighbouring words, bigram, trigrams, Sentiment, 1,2 ngram lower 0.5848 0.6380
Parse Relations, 2,3 POS grams, Neighbouring words, Sentiment, 1,2 ngram lower 0.5890 0.6240
Parse Relations, 2,3 POS grams , Neighbouring words, bigram, trigrams, 1,2 ngram lower 0.5626 0.6239
Parse Relations, 2,3 POS grams , Neighbouring words, bigram, trigrams, Sentiment 0.5922 0.6409
Table 2: Training-phase experimental results (Accuracy) for Subtask 2.
As one sentence in a review could belong to mul-
tiple aspect categories, we model the task as a
multi-label classification problem, i.e., given an
instance, predict all labels that the instance fits to.
4.1 Features
We experimented with different features, for ex-
ample unigrams, dependency tree relations, bi-
grams, POS tags and sentiment of the words (Sen-
tiWordNet), but using just the unigrams alone hap-
pened to yield the best result. The feature vector
was merely a bag-of-words vector indicating the
presence or absence of a word in an instance.
4.2 Method
The training instances were divided into 5 sets
based on the aspect categories and thereby, we
treated the multi-label classification task as 5 dif-
ferent binary classification tasks. Hence, we used
an ensemble of binary classifiers for the multi-
label classification. An SVM model was trained
using one classifier per class to distinguish it from
all other classes. For the binary classification
tasks, directly estimating a linear separating func-
tion (such as linear SVM) gave better results, as
shown in Table 3. Finally, the results of the 5 bi-
nary classifiers were combined to label the test in-
stance.
The category Miscellaneous was observed to
have the lowest accuracy, probably due to the fact
that miscellaneous captures all those aspects terms
that do not have a clearly defined category.
5 Subtask4 Aspect Category Polarity
Detection
For each review with pre-labelled aspect cate-
gories, the task was to produce a model which
predicts the sentiment polarity of each aspect cat-
egory.
5.1 Features
The training data contains reviews with the po-
larity for the corresponding aspect category. The
models performed best on using just unigram and
bigram features.
5.2 Method
The training instances were split into 5 sets based
on the aspect categories. We make use of the sen-
timent polarity classifier, as described in section
3.2, thereby, training one sentiment polarity classi-
fier for each aspect category. Table 4 indicates the
performance of different classifiers for this task,
using features as discussed in section 5.1.
6 Results
Table 5 gives an overview of the performance of
our system in this year?s task based on the offi-
cial scores from the organizers. We see that our
system performs relatively well for subtasks 1, 3
and 4, while for subtask 2 the F
1
scores are be-
hind the best system by about 12%. As observed,
a sentence could have more than one aspect and
each of these aspects could have different polar-
ities expressed. Including features that preserve
the context of the aspect could probably improve
the performance in the subtask 2. In most cases,
a simple set of features was enough to result in a
519
Restaurants Category Naive Bayes AdaBoost LinearSVC
Food 0.7130 0.8000 0.8470
Service 0.6064 0.9137 0.8997
Miscellaneous 0.6710 0.7490 0.7890
Ambience 0.6770 0.9063 0.8940
Price 0.7608 0.8548 0.9590
Table 3: Training-phase experimental results (F
1
score) for Subtask 3.
Restaurants Category Naive Bayes AdaBoost LinearSVC
Food 0.7136 0.6711 0.7417
Service 0.6733 0.5244 0.6688
Miscellaneous 0.4756 0.3170 0.4756
Ambience 0.6574 0.7232 0.6885
Price 0.7477 0.7752 0.6651
Table 4: Training-phase experimental results (F
1
score) for Subtask 4.
high F
1
score, for example, in subtask 3 a bag-of-
words feature set proved to yield a relatively high
F
1
score. In general, for the classification tasks,
we observe that the linear SVM performs best.
Subtask Dataset Best score Our score Rank
1 Laptops 74.55 66.61 8/27
1 Restaurants 84.01 77.88 12/29
2 Laptops 70.48 58.56 18/32
2 Restaurants 80.95 69.92 22/36
3 Restaurants 88.57 79.04 7/21
4 Restaurants 82.92 75.61 5/25
Table 5: Results (F
1
score and ranking) for the
Semeval-2014 test set.
7 Conclusion
In this paper, we have described the submission of
the SAP-RI team to the SemEval 2014 Task 4. We
model the classification tasks using linear SVM
and the term extraction task using CRF in order
to develop an aspect-based sentiment analysis sys-
tem that performs reasonably well.
Acknowledgement
The research is partially funded by the Economic
Development Board and the National Research
Foundation of Singapore.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh Conference on
International Language Resources and Evaluation
(LREC?10), volume 10, pages 2200?2204.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 168?177.
Niklas Jakob and Iryna Gurevych. 2010. Extracting
opinion targets in a single-and cross-domain setting
with conditional random fields. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1035?1045.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282?289.
Bing Liu. 2010. Sentiment analysis and subjectiv-
ity. In Handbook of Natural Language Processing,
pages 627?666. Chapman & Hall, 2 edition.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Bin Lu, Myle Ott, Claire Cardie, and Benjamin Tsou.
2011. Multi-aspect sentiment analysis with topic
models. In Proceedings of Sentiment Elicitation
from Natural Text for Information Retrieval and Ex-
traction, pages 81?88.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Haris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. SemEval-2014 Task 4:
Aspect based sentiment analysis. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval 2014).
Erik Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
520
Proceedings of the Seventh Conference on Natural
Language Learning at HLT-NAACL, pages 142?147.
521
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 522?526,
Dublin, Ireland, August 23-24, 2014.
SAP-RI: Twitter Sentiment Analysis in Two Days
Akriti Vij
1,2?
, Nishtha Malhotra
1,2?
, Naveen Nandan
1
and Daniel Dahlmeier
1
1
SAP Research & Innovation, Singapore
2
Nanyang Technological University, Singapore
{akriti.vij,nishtha.malhotra,naveen.nandan,d.dahlmeier}@sap.com
Abstract
We describe the submission of the SAP
Research & Innovation team to the Se-
mEval 2014 Task 9: Sentiment Analy-
sis in Twitter. We challenged ourselves
to develop a competitive sentiment anal-
ysis system within a very limited time
frame. Our submission was developed
in less than two days and achieved an
F
1
score of 77.26% for contextual polar-
ity disambiguation and 55.47% for mes-
sage polarity classification, which shows
that rapid prototyping of sentiment anal-
ysis systems with reasonable accuracy is
possible.
1 Introduction
Microblogging platforms and social networks
have become increasingly popular for expressing
opinions on a wide range of topics, hence mak-
ing them valuable and ever-growing logs of pub-
lic sentiment. This has motivated the development
of automatic natural language processing (NLP)
methods to analyse the sentiment expressed in
these short, informal messages (Liu, 2012; Pang
and Lee, 2008).
In particular, the study of sentiment and opin-
ions in messages from the Twitter microblogging
platform has attracted a lot of interest (Jansen et
al., 2009; Pak and Paroubek, 2010; Barbosa and
Feng, 2010; O?Connor et al., 2010; Bifet et al.,
2011). However, comparative evaluations of senti-
ment analysis of Twitter messages have previously
been hindered by the lack of a large benchmark
data set. The goal of the SemEval 2013 task 2:
Sentiment Analysis in Twitter (Nakov et al., 2013)
?
The work was done during an internship at SAP.
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
and this year?s continuation in the SemEval 2014
task 9: Sentiment Analysis in Twitter (Rosenthal
et al., 2014) is to close this gap by hosting a shared
task competition which provided a large corpus of
Twitter messages which are annotated with sen-
timent polarity labels. The task consists of two
subtasks: in subtask A contextual polarity disam-
biguation, participants need to predict the polarity
of a given word or phrase in the context of a tweet
message, in subtask B message polarity classifica-
tion, participants need to predict the dominating
sentiment of the complete message. Both tasks
consider sentiment analysis to be a three-way clas-
sification problem between positive, negative, and
neutral sentiment.
In this paper, we describe the submission of the
SAP-RI team to the SemEval 2014 task 9. We
challenged ourselves to develop a competitive sen-
timent analysis system within a very limited time
frame. The complete system was implemented
within only two days. Our system is based on
supervised classification with support vector ma-
chines with lexical and dictionary-based features.
Our system achieved an F
1
score of 77.26% for
contextual polarity disambiguation and 55.47%
for message polarity classification. Although our
scores are about 10-20% behind the top-scoring
systems, we show that it is possible to develop
sentiment analysis systems via rapid prototyping
with reasonable accuracy in a very short amount
of time.
2 Methods
Our system is based on supervised classification
with support vector machines and a variety of lex-
ical and dictionary-based features. From the be-
ginning, we decided to restrict ourselves to super-
vised classification and to focus on the constrained
system setting. Experiments with more data or
semi-supervised learning would have required ad-
ditional time and the results of last year?s task
522
did not show any convincing improvements using
from additional unconstrained data (Nakov et al.,
2013). We cast sentiment analysis as a multi-class
classification problem with three classes: positive,
negative, and neutral. For the features, we tried to
re-implement most of the features from the NRC-
Canada system (Mohammad et al., 2013) which
was the best performing system in last year?s task.
We describe the features in the following sections.
2.1 Task A : Features
For the contextual polarity disambiguation task,
we extract features from the target phrase itself
and from a surrounding word window of four
words before and after the target phrase. To handle
negation, we append the suffix -neg to all words
in a negated context. A negated context includes
any word in the target phrase or context that is fol-
lowing a negation word
1
up to the next following
punctuation symbol.
? Word N-grams: all lowercased unigrams
and bigrams from the target phrase and the
context. We extract the lowercased full string
of the target phrase as an additional feature.
? Character N-grams: lowercased character
bigram and trigram prefixes and suffixes from
all words in the target phrase and the context.
? Elongations: binary feature that indicates the
presence of one or more words in the target
phrase or context that have a letter repeated
for 3 for more times e.g., coool.
? Emoticons: two binary features that indicate
the presence of positive or negative emoti-
cons in the target phrase or the context, re-
spectively. Two additional binary features
indicate the presence of positive or negative
emoticons at the end of the target phrase or
context
2
.
? Punctuation: three count features for the
number of tokens that consist only of excla-
mation marks, only of questions marks, or
a mix of exclamation marks and questions
marks, in the target phrase and context, re-
ceptively.
1
http://sentiment.christopherpotts.
net/lingstruc.html
2
positive emoticons: :-), :), :B, :-B, :3, =), <3, :D, :-D,
=D, :?), :d, ;), :}, :], :P, :-P, :-p, :p. negative emoticons: :-(,
:/, :{, :[, -.-, - -, :O, :o, :
?
(, :x, :X, v.v, ;(
? Casing: two binary features that indicate the
presence of at least one all upper-case word
and at least one title-cased word in the target
phrase or context, respectively.
? Stop words: a binary feature that indicates if
all the words in the target phrase or context
are stop words. If so, an additional feature
indicates the number of stop words: 1, 2, 3,
or more stop words.
? Length: the number of tokens in the target
phrase and the context, plus a binary feature
that indicates the presence of any word with
more than three characters.
? Position: three binary features that indicate
whether a target phrase is at the beginning, in
the middle, or at the end of the tweet.
? Hashtags: all hashtags in the target phrase
or the context. To handle hashtags which are
formed by concatenating words, e.g., #ihate-
mondays, we additionally split hashtags us-
ing a simple dictionary-based approach and
add each token of the segmented hashtag as
an additional features.
? Twitter user: binary feature that indicates
whether the context or the target phrase con-
tain a mention of a Twitter user.
? URL: binary feature that indicates whether
the context or the target phrase contains a
URL.
? Brown cluster: the word cluster index for
each word in the context. Cluster indexes are
obtained from the Brown word clusters of the
ARK Twitter tagger (Owoputi et al., 2013).
? Sentiment lexicons: we add the follow-
ing sentiment dictionary features for the tar-
get phrase and the context for four differ-
ent sentiment lexicons (NRC sentiment lex-
icon, NRC Hashtag lexicon (Mohammad et
al., 2013), MPQA sentiment lexicon (Wilson
et al., 2005), and Bing Liu lexicon (Hu and
Liu, 2004)):
? the count of words with positive senti-
ment score.
? the sum of the sentiment scores for all
words.
523
? the maximum non-negative sentiment
score for any word.
? the sentiment score of the last word with
positive sentiment score.
We extract these features for both the target
phrase and the context. For words that are
marked as negated, the sign of the sentiment
scores flipped. The MPQA lexicons requires
part of speech information. We use the ARK
Twitter part-of-speech tagger (Owoputi et al.,
2013) to tag the input with part of speech
tags.
2.2 Task B : Features
For the message polarity task, we extract features
from the entire tweet message. The features are
similar to the features for phrase polarity disam-
biguation. As before we handle negation by ap-
pending the suffix -neg to all words that appear in
a negated context.
? Word N-grams: all lowercased N-grams for
N=1, . . . , 4 from the message. We also in-
clude ?skipgrams? for each N-gram by re-
placing each token in the N-gram with a as-
terisk place holder, e.g., the cat ? * cat,
the *.
? Character N-grams: lowercased charac-
ter level N-grams for N=3, . . . , 5 for all the
words in the message. Character N-grams do
not cross word boundaries.
? Elongations: count of words in the message
which have a letter repeated for 3 for more
times.
? Emoticons: similar to the contextual polarity
disambiguation task: two binary features for
presence of positive or negative emoticons in
the message and two binary features indicate
the presence of positive or negative emoti-
cons at the end of the message.
? Punctuation: similar to the contextual polar-
ity disambiguation task: three count features
for the number of tokens that consist only of
exclamation marks, only of questions marks,
or a mix of exclamation marks and questions
marks.
? Hashtags: all hashtags in the message. We
do not split concatonated hashtags.
# Tokens # Tweets
Subtask A
Training (SemEval 2014 train) 160,992 7,884
Development (SemEval 2013 test) 76,409 3,710
Subtask B
Training (SemEval 2014 train) 139,128 7,112
Development (SemEval 2013 test) 47,052 2,405
Table 1: Overview of the data sets.
? Casing: the count of all upper-case words in
the message.
? Brown cluster: similar to the contextual po-
larity disambiguation task: the cluster index
for each word in the message.
3 Experiment and Results
In this section, we report experimental result for
our method. We used the scikit-learn Python ma-
chine learning library (Pedregosa et al., 2011) to
implement the feature extraction pipeline and the
support vector machine classifier. We use a linear
kernel for the support vector machine and fixed the
SVM hyper-parameter C to 1.0. We found that
scikit-learn allowed us to implement the system
faster and resulted in much more compact code
than other machine learning tools we have worked
with in the past.
We used the official training set provided for the
SemEval 2014 task to train our system and evalu-
ated on the test set of the SemEval 2013 task which
served as development data for this year?s task
3
.
Tweets in the training data that were not available
any more through the Twitter API were removed
from the training set. An overview of the data sets
is shown in Table 1. For the evaluation, we com-
pute precision, recall and F
1
measure for the pos-
itive, negative, and neutral sentiment tweets. Fol-
lowing the official evaluation metric, the overall
precision, recall, and F
1
measure of the system is
the average of the precision, recall, and F
1
mea-
sures for positive and negative sentiment, respec-
tively.
Here, we report a feature ablation study: we
omitted each individual feature category from the
complete feature set to determine its influence on
the overall performance. Table 2 summarizes the
results for subtask A and B. Surprisingly many of
the features do not result in a reduction of the F
1
score when removed, or even increase the score,
3
We also did some experiments with a 60:40 training/test
split of the SemEval 2014 training data which showed com-
parable results
524
Features Positive Negative Neutral Overall
Subtask A P R F
1
P R F
1
P R F
1
P R F
1
All features 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.13 0.17 0.82 0.83 0.82
w/o Word N-grams 0.84 0.82 0.83 0.71 0.74 0.72 0.14 0.16 0.15 0.77 0.78 0.78
w/o character N-grams 0.85 0.89 0.87 0.80 0.78 0.79 0.27 0.12 0.17 0.82 0.83 0.83
w/o elongation 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.13 0.17 0.81 0.82 0.81
w/o emoticons 0.85 0.87 0.86 0.78 0.78 0.78 0.24 0.14 0.18 0.82 0.83 0.82
w/o punctuation 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.13 0.17 0.81 0.83 0.82
w/o casing 0.86 0.87 0.87 0.78 0.78 0.78 0.23 0.13 0.17 0.82 0.83 0.82
w/o stop words 0.86 0.87 0.86 0.78 0.79 0.78 0.24 0.15 0.18 0.82 0.83 0.82
w/o length 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.14 0.17 0.82 0.83 0.82
w/o position 0.86 0.87 0.86 0.77 0.78 0.78 0.24 0.13 0.17 0.81 0.83 0.82
w/o hashtags 0.86 0.87 0.87 0.78 0.78 0.78 0.24 0.14 0.18 0.82 0.83 0.82
w/o twitter user 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.13 0.17 0.82 0.83 0.82
w/o URL 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.13 0.17 0.81 0.82 0.81
w/o Brown cluster 0.86 0.88 0.87 0.78 0.80 0.79 0.25 0.13 0.17 0.82 0.84 0.83
w/o Sentiment lexicon 0.81 0.84 0.82 0.70 0.68 0.69 0.16 0.09 0.11 0.75 0.76 0.76
Subtask B
All features 0.81 0.54 0.65 0.66 0.34 0.44 0.59 0.89 0.71 0.74 0.44 0.54
w/o word N-grams 0.73 0.59 0.65 0.52 0.46 0.49 0.61 0.75 0.67 0.62 0.52 0.57
w/o character N-grams 0.80 0.49 0.61 0.65 0.23 0.34 0.56 0.90 0.69 0.72 0.36 0.48
w/o elongation 0.81 0.54 0.65 0.66 0.34 0.44 0.59 0.89 0.71 0.74 0.44 0.55
w/o emoticons 0.82 0.54 0.65 0.66 0.33 0.44 0.59 0.89 0.72 0.74 0.44 0.55
w/o punctuation 0.81 0.54 0.65 0.66 0.34 0.45 0.59 0.89 0.71 0.74 0.44 0.55
w/o casing 0.81 0.54 0.65 0.66 0.33 0.44 0.59 0.89 0.71 0.74 0.44 0.55
w/o hashtags 0.82 0.54 0.65 0.65 0.33 0.44 0.59 0.89 0.71 0.74 0.44 0.54
w/o Brown cluster 0.81 0.54 0.65 0.65 0.33 0.44 0.59 0.89 0.71 0.73 0.43 0.54
Table 2: Experimental Results for feature ablation study. Each row shows the precision, recall, and F
1
score for the positive, negative, and neutral class and the overall precision, recall, and F
1
score after
removing the particular feature from the features set.
although not significantly. The most effective fea-
tures are word N-grams and the sentiment lexi-
cons. It is interesting that the performance for the
neutral class is very low for subtask A and high
for subtask B. We can also see that for subtask B,
our system clearly has a problem with recall for
the positive and negative sentiment.
For the performance of our system in the Se-
mEval 2014 shared task, we report the official
overall F
1
scores of our system as released by the
organizers on the official test set in Table 3. The
scores were reported separately for different test
sets: the SemEval 2013 Twitter test set, a new Se-
mEval 2014 Twitter test set, a new test set from
LiveJournal blogs, the SMS test set from the NUS
SMS corpus (Chen and Kan, 2012), and a new
test set of sarcastic tweets. We also include the F
1
score of the best participating system for each test
set and the rank of our system among all partic-
ipating systems. The results of our system were
fairly robust across different domains, with the
exception of messages containing sarcasm which
shows understanding sarcasm requires a deeper
and more subtle understanding of the text that is
not captured well in a simple linear model.
Dataset Best score Our score Rank
Subtask A
LiveJournal 2014 85.61 77.68 18 / 27
SMS 2013 89.31 80.26 13 / 27
Twitter 2013 90.14 80.32 17 / 27
Twitter 2014 86.63 77.26 15 / 27
Twitter 2014 Sarcasm 82.75 70.64 14 / 27
Subtask B
LiveJournal 2014 74.84 57.86 33 / 42
SMS 2013 70.28 49.00 34 / 42
Twitter 2013 72.12 50.18 37 / 42
Twitter 2014 70.96 55.47 32 / 42
Twitter 2014 Sarcasm 58.16 48.64 15 / 42
Table 3: Official results for Semeval 2014 test set.
Reported scores are overall F
1
scores.
4 Conclusion
In this paper, we have described the submission of
the SAP-RI team to the SemEval 2014 task 9. We
showed that is possible to develop sentiment anal-
ysis systems via rapid prototyping with reasonable
accuracy within a couple of days.
Acknowledgement
The research is partially funded by the Economic
Development Board and the National Research
Foundation of Singapore.
525
References
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on Twitter from biased and noisy
data. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, pages 36?44.
Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer,
and Ricard Gavalda. 2011. Detecting sentiment
change in Twitter streaming data. Journal of Ma-
chine LearningResearch - Proceedings Track, 17:5?
11.
Tao Chen and Min-Yen Kan. 2012. Creating a live,
public short message service corpus: the NUS SMS
corpus. Language Resources and Evaluation, pages
1?37.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 168?177.
Bernhard J. Jansen, Mimi Zhang, Kate Sobel, and Ab-
dur Chowdury. 2009. Twitter power: Tweets as
electronic word of mouth. J. Am. Soc Inf. Sci. Tech-
nol., 60(11):2169?2188.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-
the-art in sentiment analysis of Tweets. In Proceed-
ings of the 7th International Workshop on Semantic
Evaluation, pages 321?327.
Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara
Rosenthal, Veselin Stoyanov, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis
in Twitter. In Proceedings of the 7th International
Workshop on Semantic Evaluation, pages 312?320.
Brendan O?Connor, Routledge Bryan R. Balasubra-
manyan, Ramnath, and Noah A. Smith. 2010. From
Tweets to polls: Linking text sentiment to public
opinion time series. In Proceedings of the Fourth In-
ternational Conference on Weblogs and Social Me-
dia (ICWSM ?10), pages 122?129.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), pages 380?390.
Alexander Pak and Patrick Paroubek. 2010. Twitter
based system: Using Twitter to disambiguating sen-
timent ambiguous adjectives. In Proceedings of the
8th International Workshop on Semantic Evaluation,
pages 436?439.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Oliver
Grisel, Mathieu Blondel, Peter. Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and
?
Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Preslav Nakov and
Torsten Zesch, editors, Proceedings of the 8th In-
ternational Workshop on Semantic Evaluation, Se-
mEval ?14, Dublin, Ireland.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technology and Empirical Methods in
Natural Language Processing (HLT-EMNLP), pages
307?314.
526
