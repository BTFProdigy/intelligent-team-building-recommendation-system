Chinese Segmentation and New Word Detection
using Conditional Random Fields
Fuchun Peng, Fangfang Feng, Andrew McCallum
Computer Science Department, University of Massachusetts Amherst
140 Governors Drive, Amherst, MA, U.S.A. 01003
{fuchun, feng, mccallum}@cs.umass.edu
Abstract
Chinese word segmentation is a difficult, im-
portant and widely-studied sequence modeling
problem. This paper demonstrates the abil-
ity of linear-chain conditional random fields
(CRFs) to perform robust and accurate Chi-
nese word segmentation by providing a prin-
cipled framework that easily supports the in-
tegration of domain knowledge in the form of
multiple lexicons of characters and words. We
also present a probabilistic new word detection
method, which further improves performance.
Our system is evaluated on four datasets used
in a recent comprehensive Chinese word seg-
mentation competition. State-of-the-art perfor-
mance is obtained.
1 Introduction
Unlike English and other western languages, many
Asian languages such as Chinese, Japanese, and
Thai, do not delimit words by white-space. Word
segmentation is therefore a key precursor for lan-
guage processing tasks in these languages. For Chi-
nese, there has been significant research on find-
ing word boundaries in unsegmented sequences
(see (Sproat and Shih, 2002) for a review). Un-
fortunately, building a Chinese word segmentation
system is complicated by the fact that there is no
standard definition of word boundaries in Chinese.
Approaches to Chinese segmentation fall roughly
into two categories: heuristic dictionary-based
methods and statistical machine learning methods.
In dictionary-based methods, a predefined dictio-
nary is used along with hand-generated rules for
segmenting input sequence (Wu, 1999). However
these approaches have been limited by the impos-
sibility of creating a lexicon that includes all pos-
sible Chinese words and by the lack of robust sta-
tistical inference in the rules. Machine learning ap-
proaches are more desirable and have been success-
ful in both unsupervised learning (Peng and Schuur-
mans, 2001) and supervised learning (Teahan et al,
2000).
Many current approaches suffer from either lack
of exact inference over sequences or difficulty in in-
corporating domain knowledge effectively into seg-
mentation. Domain knowledge is either not used,
used in a limited way, or used in a complicated way
spread across different components. For example,
the N-gram generative language modeling based ap-
proach of Teahan et al(2000) does not use domain
knowledge. Gao et al(2003) uses class-based lan-
guage for word segmentation where some word cat-
egory information can be incorporated. Zhang et
al (2003) use a hierarchical hidden Markov Model
to incorporate lexical knowledge. A recent advance
in this area is Xue (2003), in which the author uses
a sliding-window maximum entropy classifier to tag
Chinese characters into one of four position tags,
and then covert these tags into a segmentation using
rules. Maximum entropy models give tremendous
flexibility to incorporate arbitrary features. How-
ever, a traditional maximum entropy tagger, as used
in Xue (2003), labels characters without considering
dependencies among the predicted segmentation la-
bels that is inherent in the state transitions of finite-
state sequence models.
Linear-chain conditional random fields (CRFs)
(Lafferty et al, 2001) are models that address
both issues above. Unlike heuristic methods, they
are principled probabilistic finite state models on
which exact inference over sequences can be ef-
ficiently performed. Unlike generative N-gram or
hidden Markov models, they have the ability to
straightforwardly combine rich domain knowledge,
for example in this paper, in the form of multiple
readily-available lexicons. Furthermore, they are
discriminatively-trained, and are often more accu-
rate than generative models, even with the same fea-
tures. In their most general form, CRFs are arbitrary
undirected graphical models trained to maximize
the conditional probability of the desired outputs
given the corresponding inputs. In the linear-chain
special case we use here, they can be roughly un-
derstood as discriminatively-trained hidden Markov
models with next-state transition functions repre-
sented by exponential models (as in maximum en-
tropy classifiers), and with great flexibility to view
the observation sequence in terms of arbitrary, over-
lapping features, with long-range dependencies, and
at multiple levels of granularity. These beneficial
properties suggests that CRFs are a promising ap-
proach for Chinese word segmentation.
New word detection is one of the most impor-
tant problems in Chinese information processing.
Many machine learning approaches have been pro-
posed (Chen and Bai, 1998; Wu and Jiang, 2000;
Nie et al, 1995). New word detection is normally
considered as a separate process from segmentation.
However, integrating them would benefit both seg-
mentation and new word detection. CRFs provide a
convenient framework for doing this. They can pro-
duce not only a segmentation, but also confidence
in local segmentation decisions, which can be used
to find new, unfamiliar character sequences sur-
rounded by high-confidence segmentations. Thus,
our new word detection is not a stand-alone process,
but an integral part of segmentation. Newly detected
words are re-incorporated into our word lexicon,
and used to improve segmentation. Improved seg-
mentation can then be further used to improve new
word detection.
Comparing Chinese word segmentation accuracy
across systems can be difficult because many re-
search papers use different data sets and different
ground-rules. Some published results claim 98% or
99% segmentation precision and recall, but these ei-
ther count only the words that occur in the lexicon,
or use unrealistically simple data, lexicons that have
extremely small (or artificially non-existant) out-
of-vocabulary rates, short sentences or many num-
bers. A recent Chinese word segmentation competi-
tion (Sproat and Emerson, 2003) has made compar-
isons easier. The competition provided four datasets
with significantly different segmentation guidelines,
and consistent train-test splits. The performance of
participating system varies significantly across dif-
ferent datasets. Our system achieves top perfor-
mance in two of the runs, and a state-of-the-art per-
formance on average. This indicates that CRFs are a
viable model for robust Chinese word segmentation.
2 Conditional Random Fields
Conditional random fields (CRFs) are undirected
graphical models trained to maximize a conditional
probability (Lafferty et al, 2001). A common
special-case graph structure is a linear chain, which
corresponds to a finite state machine, and is suitable
for sequence labeling. A linear-chain CRF with pa-
rameters ? = {?1, ...} defines a conditional proba-
bility for a state (label) sequence y = y1...yT (for
example, labels indicating where words start or have
their interior) given an input sequence x = x1...xT
(for example, the characters of a Chinese sentence)
to be
P?(y|x) = 1Zx exp
( T?
t=1
?
k
?kfk(yt?1, yt,x, t)
)
,
(1)
where Zx is the per-input normalization that makes
the probability of all state sequences sum to one;
fk(yt?1, yt,x, t) is a feature function which is of-
ten binary-valued, but can be real-valued, and ?k is
a learned weight associated with feature fk. The
feature functions can measure any aspect of a state
transition, yt?1 ? yt, and the entire observation se-
quence, x, centered at the current time step, t. For
example, one feature function might have value 1
when yt?1 is the state START, yt is the state NOT-
START, and xt is a word appearing in a lexicon of
people?s first names. Large positive values for ?k
indicate a preference for such an event; large nega-
tive values make the event unlikely.
The most probable label sequence for an input x,
y? = argmaxy P?(y|x),
can be efficiently determined using the Viterbi al-
gorithm (Rabiner, 1990). An N -best list of label-
ing sequences can also be obtained using modi-
fied Viterbi algorithm and A* search (Schwartz and
Chow, 1990).
The parameters can be estimated by maximum
likelihood?maximizing the conditional probability
of a set of label sequences, each given their cor-
responding input sequences. The log-likelihood of
training set {(xi, yi) : i = 1, ...M} is written
L? =
?
i
logP?(yi|xi)
=
?
i
( T?
t=1
?
k
?kfk(yt?1, yt,x, t)? logZxi
)
.
Traditional maximum entropy learning algorithms,
such as GIS and IIS (della Pietra et al, 1995), can
be used to train CRFs. However, our implemen-
tation uses a quasi-Newton gradient-climber BFGS
for optimization, which has been shown to converge
much faster (Malouf, 2002; Sha and Pereira, 2003).
The gradient of the likelihood is ?P?(y|x)/??k =
?
i,t
fk(yt?1, y(i)t ,x(i), t)
?
?
i,y,t
P?(y|x(i))fk(yt?1, yt,x(i), t)
CRFs share many of the advantageous properties
of standard maximum entropy classifiers, including
their convex likelihood function, which guarantees
that the learning procedure converges to the global
maximum.
2.1 Regularization in CRFs
To avoid over-fitting, log-likelihood is usually pe-
nalized by some prior distribution over the parame-
ters. A commonly used prior is a zero-mean Gaus-
sian. With a Gaussian prior, log-likelihood is penal-
ized as follows.
L? =
?
i
logP?(yi|xi)?
?
k
?2k
2?2k
(2)
where ?2k is the variance for feature dimension k.
The variance can be feature dependent. However
for simplicity, constant variance is often used for
all features. We experiment an alternate version of
Gaussian prior in which the variance is feature de-
pendent. We bin features by frequency in the train-
ing set, and let the features in the same bin share
the same variance. The discounted value is set to be
?k
dck/Me??2 where ck is the count of features, M is
the bin size set by held out validation, and dae is the
ceiling function. See Peng and McCallum (2004)
for more details and further experiments.
2.2 State transition features
Varying state-transition structures with different
Markov order can be specified by different CRF
feature functions, as determined by the number of
output labels y examined together in a feature func-
tion. We define four different state transition feature
functions corresponding to different Markov orders.
Higher-order features capture more long-range de-
pendencies, but also cause more data sparseness
problems and require more memory for training.
The best Markov order for a particular application
can be selected by held-out cross-validation.
1. First-order: Here the inputs are examined in
the context of the current state only. The
feature functions are represented as f(yt,x).
There are no separate parameters for state tran-
sitions.
2. First-order+transitions: Here we add parame-
ters corresponding to state transitions. The fea-
ture functions used are f(yt,x), f(yt?1, yt).
3. Second-order: Here inputs are examined in the
context of the current and previous states. Fea-
ture function are represented as f(yt?1, yt,x).
4. Third-order: Here inputs are examined in
the context of the current, and two previous
states. Feature function are represented as
f(yt?2, yt?1, yt,x).
3 CRFs for Word Segmentation
We cast the segmentation problem as one of se-
quence tagging: Chinese characters that begin a new
word are given the START tag, and characters in
the middle and at the end of words are given the
NONSTART tag. The task of segmenting new, un-
segmented test data becomes a matter of assigning
a sequence of tags (labels) to the input sequence of
Chinese characters.
Conditional random fields are configured as a
linear-chain (finite state machine) for this purpose,
and tagging is performed using the Viterbi algo-
rithm to efficiently find the most likely label se-
quence for a given character sequence.
3.1 Lexicon features as domain knowledge
One advantage of CRFs (as well as traditional max-
imum entropy models) is its flexibility in using ar-
bitrary features of the input. To explore this advan-
tage, as well as the importance of domain knowl-
edge, we use many open features from external re-
sources. To specifically evaluate the importance of
domain knowledge beyond the training data, we di-
vide our features into two categories: closed fea-
tures and open features, (i.e., features allowed in the
competition?s ?closed test? and ?open test? respec-
tively). The open features include a large word list
(containing single and multiple-character words), a
character list, and additional topic or part-of-speech
character lexicons obtained from various sources.
The closed features are obtained from training data
alone, by intersecting the character list obtained
from training data with corresponding open lexi-
cons.
Many lexicons of Chinese words and characters
are available from the Internet and other sources.
Besides the word list and character list, our lexicons
include 24 lists of Chinese words and characters ob-
tained from several Internet sites1 cleaned and aug-
mented by a local native Chinese speaker indepen-
dently of the competition data. The list of lexicons
used in our experiments is shown in Figure 1.
3.2 Feature conjunctions
Since CRFs are log-linear models, feature conjunc-
tions are required to form complex, non-linear de-
cision boundaries in the original feature space. We
1http://www.mandarintools.com,
ftp://xcin.linux.org.tw/pub/xcin/libtabe,
http://www.geocities.com/hao510/wordlist
noun (e.g.,?,?) verb (e.g.,?)
adjective (e.g.,?,?) adverb (e.g.,!,?)
auxiliary (e.g.,,?) preposition (e.g.,?)
number (e.g.,,) negative (e.g.,X,:)
determiner (e.g.,?,?,Y) function (e.g. ?,?)
letter (English character) punctuation (e.g., # $)
last name (e.g.,K) foreign name (e.g.,?)
maybe last-name (e.g.,?,[) plural character (e.g.,?,?)
pronoun (e.g.,fi,?,?) unit character (e.g.,G,?)
country name (e.g.,?,?) Chinese place name (e.g.,?)
organization name title suffix (e.g.,?,?)
title prefix (e.g.,,?) date (e.g.,#,?,?)
Figure 1: Lexicons used in our experiments
C?2: second previous character in lexicon
C?1: previous character in lexicon
C1: next character in lexicon
C2: second next character in lexicon
C0C1: current and next character in lexicon
C?1C0: current and previous character in lexicon
C?2C?1: previous two characters in lexicon
C?1C0C1: previous, current, and next character in the lexicon
Figure 2: Feature conjunctions used in experiments
use feature conjunctions in both the open and closed
tests, as listed Figure 2.
4 Probabilistic New Word Identification
Since no vocabulary list could ever be complete,
new word (unknown word) identification is an im-
portant issue in Chinese segmentation. Unknown
words cause segmentation errors in that these out-
of-vocabulary words in input text are often in-
correctly segmented into single-character or other
overly-short words (Chen and Bai, 1998). Tradi-
tionally, new word detection has been considered as
a standalone process. We consider here new word
detection as an integral part of segmentation, aiming
to improve both segmentation and new word detec-
tion: detected new words are added to the word list
lexicon in order to improve segmentation; improved
segmentation can potentially further improve new
word detection. We measure the performance of
new word detection by its improvements on seg-
mentation.
Given a word segmentation proposed by the CRF,
we can compute a confidence in each segment. We
detect as new words those that are not in the existing
word list, yet are either highly confident segments,
or low confident segments that are surrounded by
high confident words. A confidence threshold of 0.9
is determined by cross-validation.
Segment confidence is estimated using con-
strained forward-backward (Culotta and McCal-
lum, 2004). The standard forward-backward algo-
rithm (Rabiner, 1990) calculates Zx, the total like-
lihood of all label sequences y given a sequence x.
Constrained forward-backward algorithm calculates
Z ?x, total likelihood of all paths passing through
a constrained segment (in our case, a sequence of
characters starting with a START tag followed by a
few NONSTART tags before the next START tag).
The confidence in this segment is then Z
?
x
Zx , a real
number between 0 and 1.
In order to increase recall of new words, we con-
sider not only the most likely (Viterbi) segmen-
tation, but the segmentations in the top N most
likely segmentations (an N -best list), and detect
new words according to the above criteria in all N
segmentations.
Many errors can be corrected by new word de-
tection. For example, person name ????? hap-
pens four times. In the first pass of segmentation,
two of them are segmented correctly and the other
two are mistakenly segmented as ?? ? ?? (they
are segmented differently because Viterbi algorithm
decodes based on context.). However, ?????
is identified as a new word and added to the word
list lexicon. In the second pass of segmentation, the
other two mistakes are corrected.
5 Experiments and Analysis
To make a comprehensive evaluation, we use all
four of the datasets from a recent Chinese word seg-
mentation bake-off competition (Sproat and Emer-
son, 2003). These datasets represent four different
segmentation standards. A summary of the datasets
is shown in Table 1. The standard bake-off scoring
program is used to calculate precision, recall, F1,
and OOV word recall.
5.1 Experimental design
Since CTB and PK are provided in the GB encod-
ing while AS and HK use the Big5 encoding, we
convert AS and HK datasets to GB in order to make
cross-training-and-testing possible. Note that this
conversion could potentially worsen performance
slightly due to a few conversion errors.
We use cross-validation to choose Markov-order
and perform feature selection. Thus, each training
set is randomly split?80% used for training and the
remaining 20% for validation?and based on vali-
dation set performance, choices are made for model
structure, prior, and which word lexicons to include.
The choices of prior and model structure shown in
Table 2 are used for our final testing.
We conduct closed and open tests on all four
datasets. The closed tests use only material from the
training data for the particular corpus being tested.
Open tests allows using other material, such as lex-
icons from Internet. In open tests, we use lexi-
cons obtained from various resources as described
Corpus Abbrev. Encoding #Train words #Test Words OOV rate (%)
UPenn Chinese Treebank CTB GB 250K 40K 18.1
Beijing University PK GB 1.1M 17K 6.9
Hong Kong City U HK Big 5 240K 35K 7.1
Academia Sinica AS Big 5 5.8M 12K 2.2
Table 1: Datasets statistics
bin-Size M Markov order
CTB 10 first-order + transitions
PK 15 first-order + transitions
HK 1 first-order
AS 15 first-order + transitions
Table 2: Optimal prior and Markov order setting
in Section 3.1. In addition, we conduct cross-dataset
tests, in which we train on one dataset and test on
other datasets.
5.2 Overall results
Final results of CRF based segmentation with new
word detection are summarized in Table 3. The up-
per part of the table contains the closed test results,
and the lower part contains the open test results.
Each entry is the performance of the given metric
(precision, recall, F1, and Roov) on the test set.
Closed
Precision Recall F1 Roov
CTB 0.828 0.870 0.849 0.550
PK 0.935 0.947 0.941 0.660
HK 0.917 0.940 0.928 0.531
AS 0.950 0.962 0.956 0.292
Open
Precision Recall F1 Roov
CTB 0.889 0.898 0.894 0.619
PK 0.941 0.952 0.946 0.676
HK 0.944 0.948 0.946 0.629
AS 0.953 0.961 0.957 0.403
Table 3: Overall results of CRF segmentation on
closed and open tests
To compare our results against other systems,
we summarize the competition results reported
in (Sproat and Emerson, 2003) in Table 4. XXc and
XXo indicate the closed and open runs on dataset
XX respectively. Entries contain the F1 perfor-
mance of each participating site on different runs,
with the best performance in bold. Our results are
in the last row. Column SITE-AVG is the average
F1 performance over the datasets on which a site re-
ported results. Column OUR-AVG is the average F1
performance of our system over the same datasets.
Comparing performance across systems is diffi-
cult since none of those systems reported results
on all eight datasets (open and closed runs on 4
datasets). Nevertheless, several observations could
be made from Table 4. First, no single system
achieved best results in all tests. Only one site (S01)
achieved two best runs (CTBc and PKc) with an av-
erage of 91.8% over 6 runs. S01 is one of the best
segmentation systems in mainland China (Zhang et
al., 2003). We also achieve two best runs (ASo and
HKc), with a comparable average of 91.9% over the
same 6 runs, and a 92.7% average over all the 8 runs.
Second, performance varies significantly across dif-
ferent datasets, indicating that the four datasets have
different characteristics and use very different seg-
mentation guidelines. We also notice that the worst
results were obtained on CTB dataset for all sys-
tems. This is due to significant inconsistent segmen-
tation in training and testing (Sproat and Emerson,
2003). We verify this by another test. We randomly
split the training data into 80% training and 20%
testing, and run the experiments for 3 times, result-
ing in a testing F1 of 97.13%. Third, consider a
comparison of our results with site S12, who use
a sliding-window maximum entropy model (Xue,
2003). They participated in two datasets, with an
average of 93.8%. Our average over the same two
runs is 94.2%. This gives some empirical evidence
of the advantages of linear-chain CRFs over sliding-
window maximum entropy models, however, this
comparison still requires further investigation since
there are many factors that could affect the perfor-
mance such as different features used in both sys-
tems.
To further study the robustness of our approach
to segmentation, we perform cross-testing?that is,
training on one dataset and testing on other datasets.
Table 5 summarizes these results, in which the rows
are the training datasets and the columns are the
testing datasets. Not surprisingly, cross testing re-
sults are worse than the results using the same
ASc ASo CTBc CTBo HKc HKo PKc PKo SITE-AVG OUR-AVG
S01 93.8 88.1 88.1 90.1 95.1 95.3 91.8 91.9
S02 87.4 91.2 89.3 87.2
S03 87.2 82.9 88.6 92.5 87.8 93.6
S04 93.9 93.7 93.8 94.4
S05 94.2 73.2 89.4 85.6 91.5
S06 94.5 82.9 92.4 92.4 90.6 91.9
S07 94.0 94.0 94.6
S08 90.4 95.6 93.6 93.8 93.4 94.0
S09 96.1 94.6 95.4 94.9
S10 83.1 90.1 94.7 95.9 91.0 90.8
S11 90.4 88.4 87.9 88.6 88.8 93.6
S12 95.9 91.6 93.8 94.2
95.6 95.7 84.9 89.4 92.8 94.6 94.1 94.6 92.7
Table 4: Comparisons against other systems: the first column contains the 12 sites participating in bake-off
competition; the second to the ninth columns contain their results on the 8 runs, where a bold entry is the
winner of that run; column SITE-AVG contains the average performance of the site over the runs in which it
participated, where a bold entry indicates that this site performs better than our system; column OUR-AVG
is the average of our system over the same runs, where a bolded entry indicates our system performs better
than the other site; the last row is the performance of our system over all the runs and the overall average.
source as training due to different segmentation
policies, with an exception on CTB where mod-
els trained on other datasets perform better than the
model trained on CTB itself. This is due to the data
problem mentioned above. Overall, CRFs perform
robustly well across all datasets.
From both Table 3 and 5, we see, as expected,
improvement from closed tests to open tests, indi-
cating the significant contribution of domain knowl-
edge lexicons.
Closed
CTB PK HK AS
CTB 0.822 0.810 0.815
PK 0.816 0.824 0.830
HK 0.790 0.807 0.825
AS 0.890 0.844 0.864
Open
CTB PK HK AS
CTB 0.863 0.870 0.894
PK 0.852 0.862 0.871
HK 0.861 0.871 0.889
AS 0.898 0.867 0.871
Table 5: Crossing test of CRF segmentation
5.3 Effects of new word detection
Table 6 shows the effect of new word detection
on the closed tests. An interesting observation is
CTB PK HK AS
w/o NWD 0.792 0.934 0.916 0.956
NWD 0.849 0.941 0.928 0.946
Table 6: New word detection effects: w/o NWD is
the results without new word detection and NWD is
the results with new word detection.
that the improvement is monotonically related to the
OOV rate (OOV rates are listed in Table 1). This
is desirable because new word detection is most
needed in situations that have high OOV rate. At
low OOV rate, noisy new word detection can result
in worse performance, as seen in the AS dataset.
5.4 Error analysis and discussion
Several typical errors are observed in error anal-
ysis. One typical error is caused by inconsistent
segmentation labeling in the test set. This is most
notorious in CTB dataset. The second most typi-
cal error is in new, out-of-vocabulary words, espe-
cially proper names. Although our new word detec-
tion fixes many of these problems, it is not effective
enough to recognize proper names well. One solu-
tion to this problem could use a named entity ex-
tractor to recognize proper names; this was found to
be very helpful in Wu (2003).
One of the most attractive advantages of CRFs
(and maximum entropy models in general) is its the
flexibility to easily incorporate arbitrary features,
here in the form domain-knowledge-providing lex-
icons. However, obtaining these lexicons is not a
trivial matter. The quality of lexicons can affect
the performance of CRFs significantly. In addition,
compared to simple models like n-gram language
models (Teahan et al, 2000), another shortcoming
of CRF-based segmenters is that it requires signifi-
cantly longer training time. However, training is a
one-time process, and testing time is still linear in
the length of the input.
6 Conclusions
The contribution of this paper is three-fold. First,
we apply CRFs to Chinese word segmentation and
find that they achieve state-of-the art performance.
Second, we propose a probabilistic new word de-
tection method that is integrated in segmentation,
and show it to improve segmentation performance.
Third, as far as we are aware, this is the first work
to comprehensively evaluate on the four benchmark
datasets, making a solid baseline for future research
on Chinese word segmentation.
Acknowledgments
This work was supported in part by the Center for In-
telligent Information Retrieval, in part by The Cen-
tral Intelligence Agency, the National Security Agency
and National Science Foundation under NSF grant #IIS-
0326249, and in part by SPAWARSYSCEN-SD grant
number N66001-02-1-8903.
References
K.J. Chen and M.H. Bai. 1998. Unknown Word Detec-
tion for Chinese by a Corpus-based Learning Method.
Computational Linguistics and Chinese Language
Processing, 3(1):27?44, Feburary.
A. Culotta and A. McCallum. 2004. Confidence Esti-
mation for Information Extraction. In Proceedings of
Human Language Technology Conference and North
American Chapter of the Association for Computa-
tional Linguistics(HLT-NAACL).
S. della Pietra, V. della Pietra, and J. Lafferty. 1995. In-
ducing Features Of Random Fields. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
19(4).
J. Gao, M. Li, and C. Huang. 2003. Improved Source-
Channel Models for Chinese Word Segmentation. In
Proceedings of the 41th Annual Meeting of Associa-
tion of Computaional Linguistics (ACL), Japan.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceedings
of the 18th International Conf. on Machine Learning,
pages 282?289.
R. Malouf. 2002. A Comparison of Algorithms for Max-
imum Entropy Parameter Estimation. In Sixth Work-
shop on Computational Language Learning (CoNLL).
J. Nie, M. Hannan, and W. Jin. 1995. Unknown Word
Detection and Segmentation of Chinese using Statis-
tical and Heuristic Knowledge. Communications of
the Chinese and Oriental Languages Information Pro-
cessing Society, 5:47?57.
F. Peng and A. McCallum. 2004. Accurate Informa-
tion Extraction from Research Papers using Condi-
tional Random Fields. In Proceedings of Human
Language Technology Conference and North Amer-
ican Chapter of the Association for Computational
Linguistics(HLT-NAACL), pages 329?336.
F. Peng and D. Schuurmans. 2001. Self-Supervised Chi-
nese Word Segmentation. In F. Hoffmann et al, ed-
itor, Proceedings of the 4th International Symposium
of Intelligent Data Analysis, pages 238?247. Springer-
Verlag Berlin Heidelberg.
L. Rabiner. 1990. A Tutorial on Hidden Markov Mod-
els and Selected Applications in Speech Recognition.
In Alex Weibel and Kay-Fu Lee, editors, Readings in
Speech Recognition, pages 267?296.
R. Schwartz and Y. Chow. 1990. The N-best Algorithm:
An Efficient and Exact Procedure for Finding the N
most Likely Sentence Hypotheses. In Proceedings of
IEEE International Conference on Acoustics, Speech,
and Signal Processing (ICASSP).
F. Sha and F. Pereira. 2003. Shallow Parsing with Con-
ditional Random Fields. In Proceedings of Human
Language Technology Conference and North Amer-
ican Chapter of the Association for Computational
Linguistics(HLT-NAACL).
R. Sproat and T. Emerson. 2003. First International Chi-
nese Word Segmentation Bakeoff. In Proceedings of
the Second SIGHAN Workshop on Chinese Language
Processing.
R. Sproat and C. Shih. 2002. Corpus-based Methods
in Chinese Morphology and Phonology. In Proceed-
ings of the 19th International Conference on Compu-
tational Linguistics (COLING).
W. J. Teahan, Y. Wen, R. McNab, and I. H. Wit-
ten. 2000. A Compression-based Algorithm for Chi-
nese Word Segmentation. Computational Linguistics,
26(3):375?393.
A. Wu and Z. Jiang. 2000. Statistically-Enhanced New
Word Identification in a Rule-Based Chinese System.
In Proceedings of the Second Chinese Language Pro-
cessing Workshop, pages 46?51, Hong Kong, China.
Z. Wu. 1999. LDC Chinese Segmenter.
http://www.ldc.upenn.edu/ Projects/ Chinese/ seg-
menter/ mansegment.perl.
A. Wu. 2003. Chinese Word Segmentation in MSR-
NLP. In Proceedings of the Second SIGHAN Work-
shop on Chinese Language Processing, Japan.
N. Xue. 2003. Chinese Word Segmentation as Charac-
ter Tagging. International Journal of Computational
Linguistics and Chinese Language Processing, 8(1).
H. Zhang, Q. Liu, X. Cheng, H. Zhang, and H. Yu.
2003. Chinese Lexical Analysis Using Hierarchical
Hidden Markov Model. In Proceedings of the Second
SIGHAN Workshop, pages 63?70, Japan.
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 81?90,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Active Learning by Labeling Features
Gregory Druck
Dept. of Computer Science
University of Massachusetts
Amherst, MA 01003
gdruck@cs.umass.edu
Burr Settles
Dept. of Biostatistics &
Medical Informatics
Dept. of Computer Sciences
University of Wisconsin
Madison, WI 53706
bsettles@cs.wisc.edu
Andrew McCallum
Dept. of Computer Science
University of Massachusetts
Amherst, MA 01003
mccallum@cs.umass.edu
Abstract
Methods that learn from prior informa-
tion about input features such as general-
ized expectation (GE) have been used to
train accurate models with very little ef-
fort. In this paper, we propose an ac-
tive learning approach in which the ma-
chine solicits ?labels? on features rather
than instances. In both simulated and real
user experiments on two sequence label-
ing tasks we show that our active learning
method outperforms passive learning with
features as well as traditional active learn-
ing with instances. Preliminary experi-
ments suggest that novel interfaces which
intelligently solicit labels on multiple fea-
tures facilitate more efficient annotation.
1 Introduction
The application of machine learning to new prob-
lems is slowed by the need for labeled training
data. When output variables are structured, an-
notation can be particularly difficult and time-
consuming. For example, when training a condi-
tional random field (Lafferty et al, 2001) to ex-
tract fields such as rent, contact, features, and utilities
from apartment classifieds, labeling 22 instances
(2,540 tokens) provides only 66.1% accuracy.
1
Recent work has used unlabeled data and lim-
ited prior information about input features to boot-
strap accurate structured output models. For ex-
ample, both Haghighi and Klein (2006) and Mann
and McCallum (2008) have demonstrated results
better than 66.1% on the apartments task de-
scribed above using only a list of 33 highly dis-
criminative features and the labels they indicate.
However, these methods have only been applied
in scenarios in which the user supplies such prior
knowledge before learning begins.
1
Averaged over 10 randomly selected sets of 22 instances.
In traditional active learning (Settles, 2009), the
machine queries the user for only the labels of in-
stances that would be most helpful to the machine.
This paper proposes an active learning approach in
which the user provides ?labels? for input features,
rather than instances. A labeled input feature de-
notes that a particular input feature, for example
the word call, is highly indicative of a particular
label, such as contact. Table 1 provides an excerpt
of a feature active learning session.
In this paper, we advocate using generalized
expectation (GE) criteria (Mann and McCallum,
2008) for learning with labeled features. We pro-
vide an alternate treatment of the GE objective
function used by Mann and McCallum (2008) and
a novel speedup to the gradient computation. We
then provide a pool-based feature active learning
algorithm that includes an option to skip queries,
for cases in which a feature has no clear label.
We propose and evaluate feature query selection
algorithms that aim to reduce model uncertainty,
and compare to several baselines. We evaluate
our method using both real and simulated user ex-
periments on two sequence labeling tasks. Com-
pared to previous approaches (Raghavan and Al-
lan, 2007), our method can be used for both classi-
fication and structured tasks, and the feature query
selection methods we propose perform better.
We use experiments with simulated labelers on
real data to extensively compare feature query se-
lection algorithms and evaluate on multiple ran-
dom splits. To make these simulations more re-
alistic, the effort required to perform different la-
beling actions is estimated from additional exper-
iments with real users. The results show that ac-
tive learning with features outperforms both pas-
sive learning with features and traditional active
learning with instances.
In the user experiments, each annotator actively
labels instances, actively labels features one at a
time, and actively labels batches of features orga-
81
accuracy 46.5? 60.5
feature label
PHONE* contact
call contact
deposit rent
month rent
pets restrict.
lease rent
appointment contact
parking features
EMAIL* contact
information contact
accuracy 60.5? 67.1
feature label
water utilities
close neighbor.
garbage utilities
included utilities
features
shopping neighbor.
bart neighbor.
downtown neighbor.
TIME* contact
bath size
Table 1: Two iterations of feature active learning.
Each table shows the features labeled, and the re-
sulting change in accuracy. Note that the word in-
cluded was labeled as both utilities and features, and
that ? denotes a regular expression feature.
nized using a ?grid? interface. The results support
the findings of the simulated experiments and pro-
vide evidence that the ?grid? interface can facili-
tate more efficient annotation.
2 Conditional Random Fields
In this section we describe the underlying proba-
bilistic model for all methods in this paper. We
focus on sequence labeling, though the described
methods could be applied to other structured out-
put or classification tasks. We model the proba-
bility of the label sequence y ? Y
n
conditioned
on the input sequence x ? X
n
, p(y|x; ?) using
first-order linear-chain conditional random fields
(CRFs) (Lafferty et al, 2001). This probability is
p(y|x; ?) =
1
Z
x
exp
(
?
i
?
j
?
j
f
j
(y
i
, y
i+1
,x, i)
)
,
where Z
x
is the partition function and feature
functions f
j
consider the entire input sequence
and at most two consecutive output variables.
The most probable output sequence and transition
marginal distributions can be computed using vari-
ants of Viterbi and forward-backward.
Provided a training data distribution p?, we es-
timate CRF parameters by maximizing the condi-
tional log likelihood of the training data.
L(?) = E
p?(x,y)
[log p(y|x; ?)]
We use numerical optimization to maximize L(?),
which requires the gradient of L(?) with respect
to the parameters. It can be shown that the par-
tial derivative with respect to parameter j is equal
to the difference between the empirical expecta-
tion of F
j
and the model expectation of F
j
, where
F
j
(y,x) =
?
i
f
j
(y
i
, y
i+1
,x, i).
?
??
j
L(?) = E
p?(x,y)
[F
j
(y,x)]
? E
p?(x)
[E
p(y|x;?)
[F
j
(y,x)]].
We also include a zero-mean variance ?
2
= 10
Gaussian prior on parameters in all experiments.
2
2.1 Learning with missing labels
The training set may contain partially labeled se-
quences. Let z denote missing labels. We esti-
mate parameters with this data by maximizing the
marginal log-likelihood of the observed labels.
L
MML
(?) = E
p?(x,y)
[log
?
z
p(y, z|x; ?)]
We refer to this training method as maximum
marginal likelihood (MML); it has also been ex-
plored by Quattoni et al (2007).
The gradient of L
MML
(?) can also be written
as the difference of two expectations. The first is
an expectation over the empirical distribution of x
and y, and the model distribution of z. The second
is a double expectation over the empirical distribu-
tion of x and the model distribution of y and z.
?
??
j
L
MML
(?) = E
p?(x,y)
[E
p(z|y,x;?)
[F
j
(y, z,x)]]
? E
p?(x)
[E
p(y,z|x;?)
[F
j
(y, z,x)]].
We train models using L
MML
(?) with expected
gradient (Salakhutdinov et al, 2003).
To additionally leverage unlabeled data, we
compare with entropy regularization (ER). ER
adds a term to the objective function that en-
courages confident predictions on unlabeled data.
Training of linear-chain CRFs with ER is de-
scribed by Jiao et al (2006).
3 Generalized Expectation Criteria
In this section, we give a brief overview of gen-
eralized expectation criteria (GE) (Mann and Mc-
Callum, 2008; Druck et al, 2008) and explain how
we can use GE to learn CRF parameters with esti-
mates of feature expectations and unlabeled data.
GE criteria are terms in a parameter estimation
objective function that express preferences on the
2
10 is a default value that works well in many settings.
82
value of a model expectation of some function.
Given a score function S, an empirical distribution
p?(x), a model distribution p(y|x; ?), and a con-
straint function G
k
(x,y), the value of a GE crite-
rion is G(?) = S(E
p?(x)
[E
p(y|x;?)
[G
k
(x,y)]]).
GE provides a flexible framework for parameter
estimation because each of these elements can take
an arbitrary form. The most important difference
between GE and other parameter estimation meth-
ods is that it does not require a one-to-one cor-
respondence between constraint functions G
k
and
model feature functions. We leverage this flexi-
bility to estimate parameters of feature-rich CRFs
with a very small set of expectation constraints.
Constraint functions G
k
can be normalized so
that the sum of the expectations of a set of func-
tions is 1. In this case, S may measure the di-
vergence between the expectation of the constraint
function and a target expectation
?
G
k
.
G(?) =
?
G
k
log(E[G
k
(x,y)]), (1)
where E[G
k
(x,y)] = E
p?(x)
[E
p(y|x;?)
[G
k
(x,y)]].
It can be shown that the partial derivative of
G(?) with respect to parameter j is proportional to
the predicted covariance between the model fea-
ture function F
j
and the constraint function G
k
.
3
?
??
j
G(?) =
?
G
k
E[G
k
(x,y)]
? (2)
(
E
p?(x)
[
E
p(y|x;?)
[F
j
(x,y)G
k
(x,y)]
? E
p(y|x;?)
[F
j
(x,y)]E
p(y|x;?)
[G
k
(x,y)]
]
)
The partial derivative shows that GE learns pa-
rameter values for model feature functions based
on their predicted covariance with the constraint
functions. GE can thus be interpreted as a boot-
strapping method that uses the limited training sig-
nal to learn about parameters for related model
feature functions.
3.1 Learning with feature-label distributions
Mann and McCallum (2008) apply GE to a linear-
chain, first-order CRF. In this section we provide
an alternate treatment that arrives at the same ob-
jective function from the general form described
in the previous section.
Often, feature functions in a first-order linear-
chain CRF f are binary, and are the conjunction
3
If we use squared error for S, the partial derivative is the
covariance multiplied by 2(
?
G
k
? E[G
k
(x,y)]).
of an observational test q(x, i) and a label pair test
1
{y
i
=y
?
,y
i+1
=y
??
}
.
4
f(y
i
, y
i+1
,x, i) = 1
{y
i
=y
?
,y
i+1
=y
??
}
q(x, i)
The constraint functions G
k
we use here decom-
pose and operate similarly, except that they only
include a test for a single label. Single label con-
straints are easier for users to estimate and make
GE training more efficient. Label transition struc-
ture can be learned automatically from single la-
bel constraints through the covariance-based pa-
rameter update of Equation 2. For convenience,
we can write G
yk
to denote the constraint func-
tion that combines observation test k with a test
for label y. We also add a normalization constant
C
k
= E
p?(x)
[
?
i
q
k
(x, i)],
G
yk
(x,y) =
?
i
1
C
k
1
{y
i
=y}
q
k
(x, i)
Under this construction the expectation of G
yk
is
the predicted conditional probability that the label
at some arbitrary position i is y when the observa-
tional test at i succeeds, p?(y
i
=y|q
k
(x, i)=1; ?).
If we have a set of constraint functions {G
yk
:
y ? Y}, and we use the score function in Equa-
tion 1, then the GE objective function specifies the
minimization of the KL divergence between the
model and target distributions over labels condi-
tioned on the success of the observational test. In
general the objective function will consist of many
such KL divergence penalties.
Computing the first term of the covariance in
Equation 2 requires a marginal distribution over
three labels, two of which will be consecutive, but
the other of which could appear anywhere in the
sequence. We can compute this marginal using
the algorithm of Mann and McCallum (2008). As
previously described, this algorithm is O(n|Y|
3
)
for a sequence of length n. However, we make
the following novel observation: we do not need
to compute the extra lattices for feature label pairs
with
?
G
yk
= 0, since this makes Equation 2 equal
to zero. In Mann and McCallum (2008), probabil-
ities were smoothed so that ?
y
?
G
yk
> 0. If we
assume that only a small number of labels m have
non-zero probability, then the time complexity of
the gradient computation is O(nm|Y|
2
). In this
paper typically 1 ?m? 4, while |Y| is 11 or 13.
4
We this notation for an indicator function that returns 1
if the condition in braces is satisfied, and 0 otherwise.
83
In experiments in this paper, using this optimiza-
tion does not significantly affect final accuracy.
We use numerical optimization to estimate
model parameters. In general GE objective func-
tions are not convex. Consequently, we initial-
ize 0th-order CRF parameters using a sliding win-
dow logistic regression model trained with GE.
We also include a Gaussian prior on parameters
with ?
2
= 10 in the objective function.
3.2 Learning with labeled features
The training procedure described above requires
a set of observational tests or input features with
target distributions over labels. Estimating a dis-
tribution could be a difficult task for an annotator.
Consequently, we abstract away from specifying
a distribution by allowing the user to assign labels
to features (c.f. Haghighi and Klein (2006) , Druck
et al (2008)). For example, we say that the word
feature call has label contact. A label for a feature
simply indicates that the feature is a good indicator
of the label. Note that features can have multiple
labels, as does included in the active learning ses-
sion shown in Table 1. We convert an input feature
with a set of labels L into a distribution by assign-
ing probability 1/|L| for each l ? L and probabil-
ity 0 for each l /? L. By assigning 0 probability to
labels l /? L, we can use the speed-up described in
the previous section.
3.3 Related Work
Other proposed learning methods use labeled fea-
tures to label unlabeled data. The resulting
partially-labeled corpus can be used to train a CRF
by maximizing MML. Similarly, prototype-driven
learning (PDL) (Haghighi and Klein, 2006) opti-
mizes the joint marginal likelihood of data labeled
with prototype input features for each label. Ad-
ditional features that indicate similarity to the pro-
totypes help the model to generalize. In a previ-
ous comparison between GE and PDL (Mann and
McCallum, 2008), GE outperformed PDL without
the extra similarity features, whose construction
may be problem-specific. GE also performed bet-
ter when supplied accurate label distributions.
Additionally, both MML and PDL do not natu-
rally generalize to learning with features that have
multiple labels or distributions over labels, as in
these scenarios labeling the unlabeled data is not
straightforward. In this paper, we attempt to ad-
dress this problem using a simple heuristic: when
there are multiple choices for a token?s label, sam-
ple a label. In Section 5 we use this heuristic with
MML, but in general obtain poor results.
Raghavan and Allan (2007) also propose sev-
eral methods for learning with labeled features,
but in a previous comparison GE gave better re-
sults (Druck et al, 2008). Additionally, the gen-
eralization of these methods to structured output
spaces is not straightforward. Chang et al (2007)
present an algorithm for learning with constraints,
but this method requires users to set weights by
hand. We plan to explore the use of the recently
developed related methods of Bellare et al (2009),
Grac?a et al (2008), and Liang et al (2009) in fu-
ture work. Druck et al (2008) provide a survey
of other related methods for learning with labeled
input features.
4 Active Learning by Labeling Features
Feature active learning, presented in Algorithm 1,
is a pool-based active learning algorithm (Lewis
and Gale, 1994) (with a pool of features rather
than instances). The novel components of the
algorithm are an option to skip a query and the
notion that skipping and labeling have different
costs. The option to skip is important when us-
ing feature queries because a user may not know
how to label some features. In each iteration the
model is retrained using the train procedure, which
takes as input a set of labeled features C and un-
labeled data distribution p?. For the reasons de-
scribed in Section 3.3, we advocate using GE for
the train procedure. Then, while the iteration cost
c is less than the maximum cost c
max
, the feature
query q that maximizes the query selection met-
ric ? is selected. The accept function determines
whether the labeler will label q. If q is labeled, it
is added to the set of labeled features C, and the
label cost c
label
is added to c. Otherwise, the skip
cost c
skip
is added to c. This process continues for
N iterations.
4.1 Feature query selection methods
In this section we propose feature query selection
methods ?. Queries with a higher scores are con-
sidered better candidates. Note again that by fea-
tures we mean observational tests q
k
(x, i). It is
also important to note these are not feature selec-
tion methods since we are determining the features
for which supervisory feedback will be most help-
ful to the model, rather than determining which
features will be part of the model.
84
Algorithm 1 Feature Active Learning
Input: empirical distribution p?, initial feature constraints
C, label cost c
label
, skip cost c
skip
, max cost per iteration
c
max
, max iterations N
Output: model parameters ?
for i = 1 to N do
? = train(p?, C)
c = 0
while c < c
max
do
q = argmax
q
k
?(q
k
)
if accept(q) then
C = C ? label(q)
c = c+ c
label
else
c = c+ c
skip
end if
end while
end for
? = train(p?, C)
We propose to select queries that provide the
largest reduction in model uncertainty. We notate
possible responses to a query q
k
as g?. The Ex-
pected Information Gain (EIG) of a query is the
expectation of the reduction in model uncertainty
over all possible responses. Mathematically, IG is
?
EIG
(q
k
) = E
p(g?|q
k
;?)
[E
p?(x)
[H(p(y|x; ?)?
H(p(y|x; ?
g?
)]],
where ?
g?
are the new model parameters if the re-
sponse to q
k
is g?. Unfortunately, this method is
computationally intractable. Re-estimating ?
g?
will
typically involve retraining the model, and do-
ing this for each possible query-response pair is
prohibitively expensive for structured output mod-
els. Computing the expectation over possible re-
sponses is also difficult, as in this paper users may
provide a set of labels for a query, and more gen-
erally g? could be a distribution over labels.
Instead, we propose a tractable strategy for re-
ducing model uncertainty, motivated by traditional
uncertainty sampling (Lewis and Gale, 1994). We
assume that when a user responds to a query, the
reduction in uncertainty will be equal to the To-
tal Uncertainty (TU), the sum of the marginal en-
tropies at the positions where the feature occurs.
?
TU
(q
k
) =
?
i
?
j
q
k
(x
i
, j)H(p(y
j
|x
i
; ?))
Total uncertainty, however, is highly biased to-
wards selecting frequent features. A mean un-
certainty variant, normalized by the feature?s
count, would tend to choose very infrequent fea-
tures. Consequently we propose a tradeoff be-
tween the two extremes, called weighted uncer-
tainty (WU), that scales the mean uncertainty by
the log count of the feature in the corpus.
?
WU
(q
k
) = log(C
k
)
?
TU
(q
k
)
C
k
.
Finally, we also suggest an uncertainty-based met-
ric called diverse uncertainty (DU) that encour-
ages diversity among queries by multiplying TU
by the mean dissimilarity between the feature and
previously labeled features. For sequence labeling
tasks, we can measure the relatedness of features
using distributional similarity.
5
?
DU
(q
k
) = ?
TU
(q
k
)
1
|C|
?
j?C
1?sim(q
k
, q
j
)
We contrast the notion of uncertainty described
above with another type of uncertainty: the en-
tropy of the predicted label distribution for the fea-
ture, or expectation uncertainty (EU). As above
we also multiply by the log feature count.
?
EU
(q
k
) = log(C
k
)H(p?(y
i
= y|q
k
(x, i)=1; ?))
EU is flawed because it will have a large value for
non-discriminative features.
The methods described above require the model
to be retrained between iterations. To verify that
this is necessary, we compare against query selec-
tion methods that only consider the previously la-
beled features. First, we consider a feature query
selection method called coverage (cov) that aims
to select features that are dissimilar from existing
labeled features, increasing the labeled features?
?coverage? of the feature space. In order to com-
pensate for choosing very infrequent features, we
multiply by the log count of the feature.
?
cov
(q
k
) = log(C
k
)
1
|C|
?
j?C
1? sim(q
k
, q
j
)
Motivated by the feature query selection method
of Tandem Learning (Raghavan and Allan, 2007)
(see Section 4.2 for further discussion), we con-
sider a feature selection metric similarity (sim)
that is the maximum similarity to a labeled fea-
ture, weighted by the log count of the feature.
?
sim
(q
k
) = log(C
k
)max
j?C
sim(q
k
, q
j
)
5
sim(q
k
, q
j
) returns the cosine similarity between context
vectors of words occurring in a window of ?3.
85
Features similar to those already labeled are likely
to be discriminative, and therefore likely to be la-
beled (rather than skipped). However, insufficient
diversity may also result in an inaccurate model,
suggesting that coverage should select more use-
ful queries than similarity.
Finally, we compare with several passive base-
lines. Random (rand) assigns scores to features
randomly. Frequency (freq) scores input features
using their frequency in the training data.
?
freq
(q
k
) =
?
i
?
j
q
k
(x
i
, j)
Top LDA (LDA) selects the top words from 50
topics learned from the unlabeled data using la-
tent Dirichlet alocation (LDA) (Blei et al, 2003).
More specifically, the words w generated by each
topic t are ranked using the conditional probability
p(w|t). The word feature is assigned its maximum
rank across all topics.
?
LDA
(q
k
) = max
t
rank
LDA
(q
k
, t)
This method will select useful features if the top-
ics discovered are relevant to the task. A similar
heuristic was used by Druck et al (2008).
4.2 Related Work
Tandem Learning (Raghavan and Allan, 2007) is
an algorithm that combines feature and instance
active learning for classification. The algorithm it-
eratively queries the user first for instance labels,
then for feature labels. Feature queries are selected
according to their co-occurrence with important
model features and previously labeled features. As
noted in Section 3.3, GE is preferable to the meth-
ods Tandem Learning uses to learn with labeled
features. We address the mixing of feature and in-
stance queries in Section 4.3.
In order to better understand differences in fea-
ture query selection methodology, we proposed a
feature query selection method motivated
6
by the
method used in Tandem Learning in Section 4.1.
However, this method performs poorly in the ex-
periments in Section 5.
Liang et al (2009) simultaneously developed
a method for learning with and actively selecting
6
The query selection method of Raghavan and Allan
(2007) requires a stack that is modified between queries
within each iteration. Here query scores are only updated
after each iteration of labeling.
measurements, or target expectations with associ-
ated noise. The measurement selection method
proposed by Liang et al (2009) is based on
Bayesian experimental design and is similar to
the expected information gain method described
above. Consequently this method is likely to be
intractable for real applications. Note that Liang
et al (2009) only use this method in synthetic ex-
periments, and instead use a method similar to to-
tal uncertainty for experiments in part-of-speech
tagging. Unlike the experiments presented in this
paper, Liang et al (2009) conduct only simulated
active learning experiments and do not consider
skipping queries.
Sindhwani (Sindhwani et al, 2009) simultane-
ously developed an active learning method that
queries for both instance and feature labels that
are then used in a graph-based learning algorithm.
They find that querying certain features outper-
forms querying uncertain features, but this is likely
because their query selection method is similar
to the expectation uncertainty method described
above, and consequently non-discriminative fea-
tures may be queried often (see also the discus-
sion in Section 4.1). It is also not clear how this
graph-based training method would generalize to
structured output spaces.
4.3 Expectation Constraint Active Learning
Throughout this paper, we have focussed on label-
ing input features. However, the proposed meth-
ods generalize to queries for expectation estimates
of arbitrary functions, for example queries for the
label distributions for input features, labels for in-
stances (using a function that is non-zero only for
a particular instance), partial labels for instances,
and class priors. The uncertainty-based query se-
lection methods described in Section 4.1 apply
naturally to these new query types. Importantly
this framework would allow principled mixing of
different query types, instead of alternating be-
tween them as in Tandem Learning (Raghavan and
Allan, 2007). When mixing queries, it will be
important to use different costs for different an-
notation types (Vijayanarasimhan and Grauman,
2008), and estimate the probability of obtaining a
useful response to a query. We plan to pursue these
directions in future work. This idea was also pro-
posed by Liang et al (2009), but no experiments
with mixed active learning were presented.
86
5 Simulated User Experiments
In this section we experiment with an automated
oracle labeler. When presented an instance query,
the oracle simply provides the true labels. When
presented a feature query, the oracle first decides
whether to skip the query. We have found that
users are more likely to label features that are rel-
evant for only a few labels. Therefore, the oracle
labels a feature if the entropy of its per occurrence
label expectation, H(p?(y
i
= y|q
k
(x, i) = 1; ?)) ?
0.7. The oracle then labels the feature using a
heuristic: label the feature with the label whose
expectation is highest, as well as any label whose
expectation is at least half as large.
We estimate the effort of different labeling ac-
tions with preliminary experiments in which we
observe users labeling data for ten minutes. Users
took an average of 4 seconds to label a feature, 2
seconds to skip a feature, and 0.7 seconds to la-
bel a token. We setup experiments such that each
iteration simulates one minute of labeling by set-
ting c
max
= 60, c
skip
= 2 and c
label
= 4. For
instance active learning, we use Algorithm 1 but
without the skip option, and set c
label
= 0.7. We
use N = 10 iterations, so the entire experiment
simulates 10 minutes of annotation time. For ef-
ficiency, we consider the 500 most frequent unla-
beled features in each iteration. To start, ten ran-
domly selected seed labeled features are provided.
We use random (rand) selection, uncertainty
sampling (US) (using sequence entropy, normal-
ized by sequence length) and information den-
sity (ID) (Settles and Craven, 2008) to select in-
stance queries. We use Entropy Regularization
(ER) (Jiao et al, 2006) to leverage unlabeled in-
stances.
7
We weight the ER term by choosing the
best
8
weight in {10
?3
, 10
?2
, 10
?1
, 1, 10} multi-
plied by
#labeled
#unlabeled
for each data set and query se-
lection method. Seed instances are provided such
that the simulated labeling time is equivalent to la-
beling 10 features.
We evaluate on two sequence labeling tasks.
The apartments task involves segmenting 300
apartment classified ads into 11 fields including
features, rent, neighborhood, and contact. We use
the same feature processing as Haghighi and Klein
(2006), with the addition of context features in a
window of ?3. The cora references task is to ex-
tract 13 BibTeX fields such as author and booktitle
7
Results using self-training instead of ER are similar.
8
As measured by test accuracy, giving ER an advantage.
method apartments cora
mean final mean final
ER rand 48.1 53.6 75.9 81.1
ER US 51.7 57.9 76.0 83.2
ER ID 51.4 56.9 75.9 83.1
MML rand 47.7 51.2 58.6 64.6
MML WU 57.6 60.8 61.0 66.2
GE rand 59.0 64.8
?
77.6 83.7
GE freq 66.5
?
71.6
?
68.6 79.8
GE LDA 65.7
?
71.4
?
74.9 85.0
GE cov 68.2
??
72.6
?
73.5 83.3
GE sim 57.8 65.9
?
67.1 79.2
GE EU 66.5
?
71.6
?
68.6 79.8
GE TU 70.1
??
73.6
??
76.9 88.2
??
GE WU 71.6
??
74.6
??
80.3
??
88.1
??
GE DU 70.5
??
74.4
??
78.4
?
87.5
??
Table 2: Mean and final token accuracy results.
A
?
or
?
denotes that a GE method significantly
outperforms all non-GE or passive GE methods,
respectively. Bold entries significantly outperform
all others. Methods in italics are passive.
from 500 research paper references. We use a stan-
dard set of word, regular expressions, and lexicon
features, as well as context features in a window
of ?3. All results are averaged over ten random
80:20 splits of the data.
5.1 Results
Table 2 presents mean (across all iterations) and
final token accuracy results. On the apartments
task, GE methods greatly outperform MML
9
and
ER methods. Each uncertainty-based GE method
also outperforms all passive GE methods. On the
cora task, only GE with weighted uncertainty sig-
nificantly outperforms ER and passive GE meth-
ods in terms of mean accuracy, but all uncertainty-
based GE methods provide higher final accuracy.
This suggests that on the cora task, active GE
methods are performing better in later iterations.
Figure 1, which compares the learning curves of
the best performing methods of each type, shows
this phenomenon. Further analysis reveals that the
uncertainty-based methods are choosing frequent
features that are more likely to be skipped than
those selected randomly in early iterations.
We next compare with the results of related
methods published elsewhere. We cannot make
claims about statistical significance, but the results
9
Only the best MML results are shown.
87
illustrate the competitiveness of our method. The
74.6% final accuracy on apartments is higher than
any result obtained by Haghighi and Klein (2006)
(the highest is 74.1%), higher than the supervised
HMM results reported by Grenager et al (2005)
(74.4%), and matches the results of Mann and Mc-
Callum (2008) with GE with more accurate sam-
pled label distributions and 10 labeled examples.
Chang et al (2007) only obtain better results than
88.2% on cora when using 300 labeled examples
(two hours of estimated annotation time), 5000 ad-
ditional unlabeled examples, and extra test time in-
ference constraints. Note that obtaining these re-
sults required only 10 simulated minutes of anno-
tation time, and that GE methods are provided no
information about the label transition matrix.
6 User Experiments
Another advantage of feature queries is that fea-
ture names are concise enough to be browsed,
rather than considered individually. This allows
the design of improved interfaces that can further
increase the speed of feature active learning. We
built a prototype interface that allows the user to
quickly browse many candidate features. The fea-
tures are split into groups of five features each.
Each group contains features that are related, as
measured by distributional similarity. The features
within each group are sorted according to the ac-
tive learning metric. This interface, displayed in
Figure 3, may be useful because features in the
same group are likely to have the same label.
We conduct three types of experiments. First, a
user labels instances selected by information den-
sity, and models are trained using ER. The in-
stance labeling interface allows the user to label
tokens quickly by extending the current selection
one token at a time and only requiring a single
keystroke to label an entire segment. Second,
the user labels features presented one-at-a-time by
weighted uncertainty, and models are trained us-
ing GE. To aid the user in understanding the func-
tion of the feature quickly, we provide several ex-
amples of the feature occurring in context and the
model?s current predicted label distribution for the
feature. Finally, the user labels features organized
using the grid interface described in the previous
paragraph. Weighted uncertainty is used to sort
feature queries within each group, and GE is used
to train models. Each iteration of labeling lasts
two minutes, and there are five iterations. Retrain-
ing with ER between iterations takes an average
of 5 minutes on cora and 3 minutes on apart-
ments. With GE, the retraining times are on av-
erage 6 minutes on cora and 4 minutes on apart-
ments. Consequently, even when viewed with to-
tal time, rather than annotation time, feature active
learning is beneficial. While waiting for models to
retrain, users can perform other tasks.
Figure 2 displays the results. User 1 labeled
apartments data, while Users 2 and 3 labeled cora
data. User 1 was able to obtain much better results
with feature labeling than with instance labeling,
but performed slightly worse with the grid inter-
face than with the serial interface. User 1 com-
mented that they found the label definitions for
apartments to be imprecise, so the other experi-
ments were conducted on the cora data. User 2
obtained better results with feature labeling than
instance labeling, and obtained higher mean ac-
curacy with the grid interface. User 3 was much
better at labeling features than instances, and per-
formed especially well using the grid interface.
7 Conclusion
We proposed an active learning approach in which
features, rather than instances, are labeled. We
presented an algorithm for active learning with
features and several feature query selection meth-
ods that approximate the expected reduction in
model uncertainty of a feature query. In simu-
lated experiments, active learning with features
outperformed passive learning with features, and
uncertainty-based feature query selection outper-
formed other baseline methods. In both simulated
and real user experiments, active learning with
features outperformed passive and active learning
with instances. Finally, we proposed a new label-
ing interface that leverages the conciseness of fea-
ture queries. User experiments suggested that this
grid interface can improve labeling efficiency.
Acknowledgments
We thank Kedar Bellare for helpful discussions and Gau-
rav Chandalia for providing code. This work was supported
in part by the Center for Intelligent Information Retrieval
and the Central Intelligence Agency, the National Security
Agency and National Science Foundation under NSF grant
#IIS-0326249. The second author was supported by a grant
from National Human Genome Research Institute. Any opin-
ions, findings and conclusions or recommendations are the
authors? and do not necessarily reflect those of the sponsor.
88
2 4 6 8 1035
4045
5055
6065
7075
80
simulated annotation time (minutes)
token
 accu
racy
apartments
 
 
ER + uncertaintyMML + weighted uncertaintyGE + frequencyGE + weighted uncertainty 2 4 6 8 1045
5055
6065
7075
8085
90
simulated annotation time (minutes)
token
 accu
racy
cora
 
 
ER + uncertaintyMML + weighted uncertaintyGE + randomGE + weighted uncertainty
Figure 1: Token accuracy vs. time for best performing ER, MML, passive GE, and active GE methods.
2 4 6 8 10510
1520
2530
3540
4550
5560
65
annotation time (minutes)
token 
accura
cy
user 1 ? apartments
 
 
ER + information densityGE + weighted uncertainty (serial)GE + weighted uncertainty (grid) 2 4 6 8 1030
3540
4550
5560
6570
annotation time (minutes)
token 
accura
cy
user 2 ? cora
 
 
ER + information densityGE + weighted uncertainty (serial)GE + weighted uncertainty (grid) 2 4 6 8 103540
4550
5560
6570
7580
85
annotation time (minutes)
token 
accura
cy
user 3 ? cora
 
 
ER + information densityGE + weighted uncertainty (serial)GE + weighted uncertainty (grid)
Figure 2: User experiments with instance labeling and feature labeling with the serial and grid interfaces.
Figure 3: Grid feature labeling interface. Boxes on the left contain groups of features that appear in
similar contexts. Features in the same group often receive the same label. On the right, the model?s
current expectation and occurrences of the selected feature in context are displayed.
89
References
Kedar Bellare, Gregory Druck, and Andrew McCal-
lum. 2009. Alternating projections for learning with
expectation constraints. In UAI.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet alocation.
Journal of Machine Learning Research, 3:2003.
Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007.
Guiding semi-supervision with constraint-driven
learning. In ACL, pages 280?287.
Gregory Druck, Gideon Mann, and Andrew McCal-
lum. 2008. Learning from labeled features using
generalized expectation criteria. In SIGIR.
Joao Grac?a, Kuzman Ganchev, and Ben Taskar. 2008.
Expectation maximization and posterior constraints.
In J.C. Platt, D. Koller, Y. Singer, and S. Roweis,
editors, Advances in Neural Information Processing
Systems 20. MIT Press.
Trond Grenager, Dan Klein, and Christopher D. Man-
ning. 2005. Unsupervised learning of field segmen-
tation models for information extraction. In ACL.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In HTL-NAACL.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved
sequence segmentation and labeling. In ACL, pages
209?216.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.
David D. Lewis and William A. Gale. 1994. A sequen-
tial algorithm for training text classifiers. In SIGIR,
pages 3?12, New York, NY, USA. Springer-Verlag
New York, Inc.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning from measurements in exponential fami-
lies. In ICML.
Gideon Mann and Andrew McCallum. 2008. General-
ized expectation criteria for semi-supervised learn-
ing of conditional random fields. In ACL.
A. Quattoni, S. Wang, L.-P Morency, M. Collins, and
T. Darrell. 2007. Hidden conditional random fields.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 29:1848?1852, October.
Hema Raghavan and James Allan. 2007. An interac-
tive algorithm for asking and incorporating feature
feedback into support vector machines. In SIGIR,
pages 79?86.
Ruslan Salakhutdinov, Sam Roweis, and Zoubin
Ghahramani. 2003. Optimization with em and
expectation-conjugate-gradient. In ICML, pages
672?679.
Burr Settles and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In EMNLP.
Burr Settles. 2009. Active learning literature survey.
Technical Report 1648, University of Wisconsin -
Madison.
Vikas Sindhwani, Prem Melville, and Richard D.
Lawrence. 2009. Uncertainty sampling and trans-
ductive experimental design for active dual supervi-
sion. In ICML.
Sudheendra Vijayanarasimhan and Kristen Grauman.
2008. Multi-level active prediction of useful image
annotations for recognition. In NIPS.
90
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 131?140,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Generalized Expectation Criteria for
Bootstrapping Extractors using Record-Text Alignment
Kedar Bellare
Dept. of Computer Science
University of Massachusetts
Amherst, MA 01003
kedarb@cs.umass.edu
Andrew McCallum
Dept. of Computer Science
University of Massachusetts
Amherst, MA 01003
mccallum@cs.umass.edu
Abstract
Traditionally, machine learning ap-
proaches for information extraction
require human annotated data that can be
costly and time-consuming to produce.
However, in many cases, there already
exists a database (DB) with schema
related to the desired output, and records
related to the expected input text. We
present a conditional random field (CRF)
that aligns tokens of a given DB record
and its realization in text. The CRF model
is trained using only the available DB and
unlabeled text with generalized expecta-
tion criteria. An annotation of the text
induced from inferred alignments is used
to train an information extractor. We eval-
uate our method on a citation extraction
task in which alignments between DBLP
database records and citation texts are
used to train an extractor. Experimental
results demonstrate an error reduction
of 35% over a previous state-of-the-art
method that uses heuristic alignments.
1 Introduction
A substantial portion of information on the Web
consists of unstructured and semi-structured text.
Information extraction (IE) systems segment and
label such text to populate a structured database
that can then be queried and mined efficiently.
In this paper, we mainly deal with information
extraction from text fragments that closely resem-
ble structured records. Examples of such texts
include citation strings in research papers, con-
tact addresses on person homepages and apart-
ment listings in classified ads. Pattern match-
ing and rule-based approaches for IE (Brin, 1998;
Agichtein and Gravano, 2000; Etzioni et al, 2005)
that only use specific patterns, and delimiter and
font-based cues for segmentation are prone to fail-
ure on such data because these cues are gen-
erally not broadly reliable. Statistical machine
learning methods such as hidden Markov models
(HMMs) (Rabiner, 1989; Seymore et al, 1999;
Freitag and McCallum, 1999) and conditional ran-
dom fields (CRFs) (Lafferty et al, 2001; Peng
and McCallum, 2004; Sarawagi and Cohen, 2005)
have become popular approaches to address the
text extraction problem. However, these methods
require labeled training data, such as annotated
text, which is often scarce and expensive to pro-
duce.
In many cases, however, there already exists a
database with schema related to the desired out-
put, and records that are imperfectly rendered in
the available unlabeled text. This database can
serve as a source of significant supervised guid-
ance to machine learning methods. Previous work
on using databases to train information extrac-
tors has taken one of three simpler approaches.
In the first, a separate language model is trained
on each column of the database and these mod-
els are then used to segment and label a given
text sequence (Agichtein and Ganti, 2004; Can-
isius and Sporleder, 2007). However, this ap-
proach does not model context, errors or differ-
ent formats of fields in text, and requires large
number of database entries to learn an accurate
language model. The second approach (Sarawagi
and Cohen, 2004; Michelson and Knoblock, 2005;
Mansuri and Sarawagi, 2006) uses database or
dictionary lookups in combination with similarity
measures to add features to the text sequence. Al-
though these features are very informative, learn-
ing algorithms still require annotated data to make
use of them. The final approach heuristically
labels texts using matching records and learns
extractors from these annotations (Ramakrishnan
and Mukherjee, 2004; Bellare and McCallum,
2007; Michelson and Knoblock, 2008). Heuris-
131
tic labeling decisions, however, are made indepen-
dently without regard for the Markov dependen-
cies among labels in text and are sensitive to subtle
changes in text.
Here we propose a method that automatically
induces a labeling of an input text sequence us-
ing a word alignment with a matching database
record. This induced labeling is then used to train
a text extractor. Our approach has several advan-
tages over previous methods. First, we are able
to model field ordering and context around fields
by learning an extractor from annotations of the
text itself. Second, a probabilistic model for word
alignment can exploit dependencies among align-
ments, and is also robust to errors, formatting dif-
ferences, and missing fields in text and the record.
Our word alignment model is a conditional ran-
dom field (CRF) (Lafferty et al, 2001) that gen-
erates alignments between tokens of a text se-
quence and a matching database record. The
structure of the graphical model resembles IBM
Model 1 (Brown et al, 1993) in which each tar-
get (record) word is assigned one or more source
(text) words. The alignment is generated con-
ditioned on both the record and text sequence,
and therefore supports large sets of rich and non-
independent features of the sequence pairs. Our
model is trained without the need for labeled word
alignments by using generalized expectation (GE)
criteria (Mann and McCallum, 2008) that penal-
ize the divergence of specific model expectations
from target expectations. Model parameters are
estimated by minimizing this divergence. To limit
over-fitting we include a L
2
-regularization term in
the objective. The model expectations in GE cri-
teria are taken with respect to a set of alignment
latent variables that are either specific to each se-
quence pair (local) or summarizing the entire data
set (global). This set is constructed by including
all alignment variables a that satisfy a certain bi-
nary feature (e.g., f(a,x
1
,y
1
,x
2
) = 1, for la-
beled record (x
1
,y
1
), and text sequence x
2
). One
example global criterion is that ?an alignment ex-
ists between two orthographically similar
1
words
95% of the time.? Here the criterion has a target
expectation of 95% and is defined over alignments
{a = ?i, j? | x
1
[i] ? x
2
[j],?x
1
,x
2
}. Another cri-
terion for extraction can be ?the word ?EMNLP?
is always aligned with the record label booktitle?.
1
Two words are orthographically similar if they have low
edit distance.
This criterion has a target of 100% and defined
for {a = ?i, j? | y
1
[i] = booktitle ? x
2
[j] =
?EMNLP?,?y
1
,x
2
}. One-to-one correspondence
between words in the sequence pair can be speci-
fied as collection of local expectation constraints.
Since we directly encode prior knowledge of how
alignments behave in our criteria, we obtain suffi-
ciently accurate alignments with little supervision.
We apply our method to the task of citation
extraction. The input to our training algorithm
is a set of matching DBLP
2
-record/citation-text
pairs and global GE criteria
3
of the following two
types: (1) alignment criteria that consider fea-
tures of mapping between record and text words,
and, (2) extraction criteria that consider features
of the schema label assigned to a text word. In
our experiments, the parallel record-text pairs are
collected manually but this process can be auto-
mated using systems that match text sequences
to records in the DB (Michelson and Knoblock,
2005; Michelson and Knoblock, 2008). Such sys-
tems achieve very high accuracy close to 90% F1
on semi-structured domains similar to ours.
4
Our
trained alignment model can be used to directly
align new record-text pairs to create a labeling of
the texts. Empirical results demonstrate a 20.6%
error reduction in token labeling accuracy com-
pared to a strong baseline method that employs a
set of high-precision alignments. Furthermore, we
provide a 63.8% error reduction compared to IBM
Model 4 (Brown et al, 1993). Alignments learned
by our model are used to train a linear-chain CRF
extractor. We obtain an error reduction of 35.1%
over a previous state-of-the-art extraction method
that uses heuristically generated alignments.
2 Record-Text Alignment
Here we provide a brief description of the record-
text alignment task. For the sake of clarity and
space, we describe our approach on a fictional
restaurant address data set. The input to our sys-
tem is a database (DB) consisting of records (pos-
sibly containing errors) and corresponding texts
that are realizations of these DB records. An ex-
ample of a matching record-text pair is shown in
2
http://www.informatik.uni-trier.de/?ley/db/
3
Expectation criteria used in our experiments are listed at
http://www.cs.umass.edu/?kedarb/dbie expts.txt.
4
To obtain more accurate record-text pairs, matching
methods can be tuned for high precision at the expense
of recall. Alternatively, humans can cheaply provide
match/mismatch labels on automatically matched pairs.
132
Record
name address city state phone
restaurant katsu n. hillhurst avenue los angeles 665-1891
Text
katzu, 1972 hillhurst ave., los feliz, california
Table 1: An example of a matching record-text pair for restaurant addresses.
Table 1. This example displays the differences
between the record and text: (1) spelling errors:
katsu ? katzu, (2) word insertions (restaurant),
deletions (1972), substitutions (angeles ? feliz),
(3) abbreviations (avenue ? ave.), (4) missing
fields in text (phone=665-1891), and (5) extra
fields in text (state=california). These discrep-
ancies plus the unknown ordering of fields within
text can be addressed through word alignment.
restaurant [name]       
katsu [name]       
*null* [name]       
n. [address]       
hillhurst [address]       
avenue [address]       
*null* [address]       
los [city]       
angeles [city]       
*null* [city]       
*null* [state]       
665-1891 [phone]       
*null* [phone]       
k
a
t
z
u
,
1
9
7
2
h
i
l
l
h
u
r
s
t
a
v
e
.
,
l
o
s
f
e
l
i
z
,
c
a
l
i
f
o
r
n
i
a
Table 2: Example of a word alignment.  repre-
sents aligned tokens. Vertical text at the bottom
are the text tokens. Horizontal text on the left are
tokens from the DB record with labels shown in
braces.
An example word alignment between the record
and text is shown in Table 2. Tokenization of
record/text string is based on whitespace charac-
ters. We add a special *null* token at the field
boundaries for each label in the schema to model
word insertions. The record sequence is obtained
by concatenating individual fields according to the
DB schema order. As in statistical word align-
ment, we assume the DB record to be our source
and the text to be our target. The induced labeling
of the text is given by (name, address, address,
address, city, city, state) which can be used to
train an information extractor. In the next section,
we present our approach to address this task.
3 Approach
We first define notation that will be used
throughout this section. Let (x
1
,y
1
) be a
database record with token sequence x
1
=
?x
1
[1], x
1
[2], . . . , x
1
[m]? and label sequence y
1
=
?y
1
[1], y
1
[2], . . . , y
1
[m]? with y
1
[?] ? Y where
Y is the database schema. Let x
2
=
?x
2
[1], x
2
[2], . . . , x
2
[n]? be the text sequence. Let
a = ?a
1
, a
2
, . . . , a
n
? be an alignment sequence
of same length as the target text sequence. The
alignment a
i
= j assigns the DB token-label pair
(x
1
[j], y
1
[j]) to the text token x
2
[i].
3.1 Conditional Random Field for Alignment
Our conditional random field (CRF) for alignment
has a graphical model structure that resembles that
of IBM Model 1 (Brown et al, 1993). The CRF
is an undirected graphical model that defines a
probability distribution over alignment sequences
a conditioned on the inputs (x
1
,y
1
,x
2
) as:
p
?
(a|x
1
,y
1
,x
2
) =
exp(
P
n
t=1
?
>
~
f(a
t
,x
1
,y
1
,x
2
,t))
Z
?
(x
1
,y
1
,x
2
)
, (1)
where
~
f(a
t
,x
1
,y
1
,x
2
, t) are feature functions
defined over the alignments and inputs, ? are
the model parameters and Z
?
(x
1
,y
1
,x
2
) =
?
a
?
exp(
?
n
t=1
?
>
~
f(a
?
t
,x
1
,y
1
,x
2
, t)) is the par-
tition function.
The feature vector
~
f(a
t
,x
1
,y
1
,x
2
, t) is the
concatenation of two types of feature functions:
(1) alignment features f
align
(a
t
,x
1
,x
2
, t) defined
on source-target tokens, and, (2) extraction fea-
tures f
extr
(a
t
,y
1
,x
2
, t) defined on source labels
and target text. To obtain the probability of an
alignment in a particular position t we marginal-
ize out the alignments over the rest of the positions
{1, . . . , n}\{t},
p
?
(a
t
|x
1
,y
1
,x
2
) =
?
{a[1...n]}\{a
t
}
p
?
(a|x
1
,y
1
,x
2
)
133
=exp(?
>
~
f(a
t
,x
1
,y
1
,x
2
, t))
exp(
?
a
?
?
>
~
f(a
?
,x
1
,y
1
,x
2
, t))
(2)
Furthermore, the marginal over label y
t
assigned
to the text token x
2
[t] at time step t during align-
ment is given by
p
?
(y
t
|x
2
) =
?
{a
t
|y
1
[a
t
]=y
t
}
p
?
(a
t
|x
1
,y
1
,x
2
),
(3)
where {a
t
| y
1
[a
t
] = y
t
} is the set of alignments
that result in a labeling y
t
for token x
2
[t]. Hence-
forth, we abbreviate p
?
(a
t
|x
1
,y
1
,x
2
) to p
?
(a
t
).
The gradient of p
?
(a
t
) with respect to parameters
? is given by
?p
?
(a
t
)
??
= p
?
(a
t
)
[
~
f(a
t
,x
1
,y
1
,x
2
, t)
?E
p
?
(a)
(
~
f(a,x
1
,y
1
,x
2
, t)
)]
,(4)
where the expectation term in the above equation
sums over all alignments a at position t. We use
the Baum-Welch and Viterbi algorithms to com-
pute marginal probabilities and best alignment se-
quences respectively.
3.2 Expectation Criteria and Parameter
Estimation
LetD = ?(x
(1)
1
,y
(1)
1
,x
(1)
2
), . . . , (x
(K)
1
,y
(K)
1
,x
(K)
2
)?
be a data set of K record-text pairs gathered man-
ually or automatically through matching (Michel-
son and Knoblock, 2005; Michelson and
Knoblock, 2008). A global expectation criterion
is defined on the set of alignment latent variables
A
f
= {a|f(a,x
(i)
1
,y
(i)
1
,x
(i)
2
) = 1,?i = 1 . . .K}
on the entire data set that satisfy a given bi-
nary feature f(a,x
1
,y
1
,x
2
). Similarly a local
expectation criterion is defined only for a
specific instance (x
(i)
1
,y
(i)
1
,x
(i)
2
) with the set
A
f
= {a|f(a,x
(i)
1
,y
(i)
1
,x
(i)
2
) = 1}. For a feature
function f , a target expectation p, and, a weight
w, our criterion minimizes the squared divergence
?(f, p, w,?) = w
(
E
p
?
(A
f
)
|A
f
|
? p
)
2
, (5)
where E
p
?
(A
f
) =
?
a?A
f
p
?
(a) is the sum of
marginal probabilities given by Equation (2) and
|A
f
| is the size of the variable set. The weight
w influences the importance of satisfying a given
expectation criterion. Equation (5) is an instance
of generalized expectation criteria (Mann and Mc-
Callum, 2008) that penalizes the divergence of
a specific model expectation from a given target
value. The gradient of the divergence with respect
to ? is given by,
??(f, p, w,?)
??
= 2w
(
E
p
?
(A
f
)
|A
f
|
? p
)
?
?
?
1
|A
f
|
?
a?A
f
?p
?
(a)
??
? p
?
?
, (6)
where the gradient
?p
?
(a)
??
is given by Eq. (4).
Given expectation criteria C = ?F,P,W? with
a set of binary feature functions F = ?f
1
, . . . , f
l
?,
target expectations P = ?p
1
, . . . , p
l
? and weights
W = ?w
1
, . . . , w
l
?, we maximize the objective
O(?;D, C) = max
?
?
l
?
i=1
?(f
i
, p
i
, w
i
,?)?
||?||
2
2
,
(7)
where ||?||
2
/2 is the regularization term added to
limit over-fitting. Hence the gradient of the objec-
tive is
?O(?;D, C)
??
= ?
l
?
i=1
??(f
i
, p
i
, w
i
,?)
??
??.
We maximize our objective (Equation 7) using the
L-BFGS algorithm. It is sometimes necessary to
restart maximization after resetting the Hessian
calculation in L-BFGS due to non-convexity of
our objective.
5
Also, non-convexity may lead to
a local instead of a global maximum. Our experi-
ments show that local maxima do not adversely af-
fect performance since our accuracy is within 4%
of a model trained with gold-standard labels.
3.3 Linear-chain CRF for Extraction
The alignment CRF (AlignCRF) model described
in Section 3.1 is able to predict labels for a text
sequence given a matching DB record. However,
without corresponding records for texts the model
does not perform well as an extractor because it
has learned to rely on the DB record and alignment
features (Sutton et al, 2006). Hence, we train
a separate linear-chain CRF on the alignment-
induced labels for evaluation as an extractor.
The extraction CRF (ExtrCRF) employs a
fully-connected state machine with a unique state
5
Our L-BFGS optimization procedure checks whether the
approximate Hessian computed from cached gradient vectors
is positive semi-definite. The optimization is restarted if this
check fails.
134
per label y ? Y in the database schema. The CRF
induces a conditional probability distribution over
label sequences y = ?y
1
, . . . , y
n
? and input text
sequences x = ?x
1
, . . . , x
n
? as
p
?
(y|x) =
exp
(
?
n
t=1
?
>
~g(y
t?1
, y
t
,x, t)
)
Z
?
(x)
.
(8)
In comparison to our earlier zero-order AlignCRF
model, our ExtrCRF is a first-order model. All
the feature functions in this model g(y
t?1
, y
t
,x, t)
are a conjunction of the label pair (y
t?1
, y
t
) and
input observational features. Z
?
(x) in the equa-
tion above is the partition function. Inference in
the model is performed using the Viterbi algo-
rithm.
Given expectation criteria C and data set
D = ?(x
(1)
1
,y
(1)
1
,x
(1)
2
), . . . , (x
(K)
1
,y
(K)
1
,x
(K)
2
)?,
we first estimate the parameters ? of AlignCRF
model as described in Section 3.2. Next, for all
text sequences x
(i)
2
, i = 1 . . .K we compute the
marginal probabilities of the labels p
?
(y
t
|x
(i)
2
),?t
using Equation (3). To estimate parameters ? we
minimize the KL-divergence between p
?
(y|x) =
?
n
t=1
p
?
(y
t
|x) and p
?
(y|x) for all sequences x,
KL(p
?
?p
?
) =
?
y
p
?
(y|x) log(
p
?
(y|x)
p
?
(y|x)
)
= H(p
?
(y|x))
?
?
t,y
t?1
,y
t
E
p
?
(y
t?1
,y
t
)
[?
>
~g(y
t?1
, y
t
,x, t)]
+ log(Z
?
(x)). (9)
The gradient of the above equation is given by
?KL
??
=
?
t,y
t?1
,y
t
E
p
?
(y
t?1
,y
t
|x)
[~g(y
t?1
, y
t
,x, t)]
?E
p
?
(y
t?1
,y
t
|x)
[~g(y
t?1
, y
t
,x, t)]. (10)
Both the expectations can be computed using the
Baum-Welch algorithm. The parameters ? are es-
timated for a given data set D and learned param-
eters ? by optimizing the objective
O(?;D,?) = min
?
K
?
i=1
KL(p
?
(y|x
(i)
2
)?p
?
(y|x
(i)
2
)
+???
2
/2.
The objective is minimized using L-BFGS. Since
the objective is convex we are guaranteed to obtain
a global minima.
4 Experiments
In this section, we present details about the appli-
cation of our method to citation extraction task.
Data set. We collected a set of 260 random
records from the DBLP bibliographic database.
The schema of DBLP has the following labels
{author, editor, address, title, booktitle, pages,
year, journal, volume, number, month, url, ee,
cdrom, school, publisher, note, isbn, chapter, se-
ries}. The complexity of our alignment model de-
pends on the number of schema labels and number
of tokens in the DB record. We reduced the num-
ber of schema labels by: (1) mapping the labels
address, booktitle, journal and school to venue, (2)
mapping month and year to date, and (3) dropping
the fields url, ee, cdrom, note, isbn and chapter,
since they never appeared in citation texts. We
also added the other label O for fields in text that
are not represented in the database. Therefore, our
final DB schema is {author, title, date, venue, vol-
ume, number, pages, editor, publisher, series, O}.
For each DBLP record we searched on the web
for matching citation texts using the first author?s
last name and words in the title. Each citation text
found is manually labeled for evaluation purposes.
An example of a matching DBLP record-citation
text pair is shown in Table 3. Our data set
6
con-
tains 522 record-text pairs for 260 DBLP entries.
Features and Constraints. We use a variety of
rich, non-independent features in our models to
optimize system performance. The input features
in our models are of the following two types:
(a) Extraction features in the AlignCRF
model (f(a
t
,y
1
,x
2
, t)) and ExtrCRF model
(g(y
t?1
, y
t
,x, t)) are conjunctions of assigned la-
bels and observational tests on text sequence at
time step t. The following observational tests
are used: (1) regular expressions to detect to-
kens containing all characters (ALLCHAR), all dig-
its (ALLDIGITS) or both digits and characters (AL-
PHADIGITS), (2) number of characters or digits
in the token (NUMCHAR=3, NUMDIGITS=1), (3)
domain-specific patterns for date and pages, (4)
token identity, suffixes, prefixes and character n-
grams, (5) presence of a token in lexicons such as
?last names,? ?publisher names,? ?cities,? (6) lex-
icon features within a window of 10, (7) regular
6
The data set can be found at
http://www.cs.umass.edu/?kedarb/dbie cite data.sgml.
135
DBLP record Citation text
[Chengzhi Li]
author
[Edward W. Knightly]
author
[Coordinated Net-
work Scheduling: A Framework for End-to-End Services.]
title
[69-]
pages
[2000]
date
[ICNP]
venue
[C. Li]
author
[and]
O
[E. Knightly.]
author
[Coordinated network schedul-
ing: A framework for end-to-end services.]
title
[In Proceedings of IEEE
ICNP]
venue
[?00,]
date
[Osaka, Japan,]
venue
[November 2000.]
date
Table 3: Example of matching record-text pair found on the web.
expression features within a window of 10, and (8)
token identity features within a window of 3.
(b) Alignment features in the AlignCRF model
(f(a
t
,x
1
,x
2
, t)) that operate on the aligned
source token x
1
[a
t
] and target token x
2
[t]. Again
the observational tests used for alignment are: (1)
exact token match tests whether the source-target
tokens are string identical, (2) approximate token
match produces a binary feature after binning the
Jaro-Winkler edit distance (Cohen et al, 2003) be-
tween the tokens, (3) substring token match tests
whether one token is a substring of the other,
(4) prefix token match returns true if the pre-
fixes match for lengths {1, 2, 3, 4}, (5) suffix to-
ken match returns true if the prefixes match for
lengths {1, 2, 3, 4}, and (6) exact and approximate
token matches at offsets {?1,?1} and {+1,+1}
around the alignment.
Thus, a conditional model lets us use these ar-
bitrary helpful features that cannot be exploited
tractably in a generative model.
As is common practice (Haghighi and Klein,
2006; Mann and McCallum, 2008), we simulate
user-specified expectation criteria through statis-
tics on manually labeled citation texts. For ex-
traction criteria, we select for each label, the top
N extraction features ordered by mutual informa-
tion (MI) with that label. Also, we aggregate the
alignment features of record tokens whose align-
ment with a target text token results in a correct
label assignment. The top N alignment features
that have maximum MI with this correct label-
ing are selected as alignment criteria. We bin tar-
get expectations of these criteria into 11 bins as
[0.05, 0.1, 0.2, 0.3, . . . , 0.9, 0.95].
7
In our exper-
iments, we set N = 10 and use a fixed weight
w = 10.0 for all expectation criteria (no tuning
of parameters was performed). Table 4 shows a
sample of GE criteria used in our experiments.
8
7
Mann and McCallum (2008) note that GE criteria are ro-
bust to deviation of specified targets from actual expectations.
8
A complete list of expectation criteria is available at
http://www.cs.umass.edu/?kedarb/dbie expts.txt.
Label Feature Prior
alignment PREFIX MATCH4 0.95
author LEXICON LASTNAME 0.6
title WINDOW WORD=Maintenance 0.95
venue WINDOW WORD=Conference 0.95
date YEAR PATTERN 0.95
volume NUMDIGITS=2 0.6
number NUMDIGITS=1 0.6
pages PAGES PATTERN 0.95
editor WORD PREFIX[2]=ed 0.95
publisher WORD=Press 0.95
series WORD=Notes 0.95
O WORD=and 0.7
Table 4: Sample of expectation criteria used by
our model.
Experimental Setup. Our experiments use a 3:1
split of the data for training and testing. We re-
peat the experiment 20 times with different ran-
dom splits of the data. We train the AlignCRF
model using the training data and the automati-
cally created expectation criteria (Section 3.2). We
evaluate our alignment model indirectly in terms
of token labeling accuracy (i.e., percentage of cor-
rectly labeled tokens in test citation data) since we
do not have annotated alignments. The alignment
model is then used to train a ExtrCRF model as
described in Section 3.3. Again, we use token la-
beling accuracy for evaluation. We also measure
F1 performance as the harmonic mean of precision
and recall for each label.
4.1 Alternate approaches
We compare our method against alternate ap-
proaches that either learn alignment or extraction
models from training data.
Alignment approaches. We use GIZA++ (Och
and Ney, 2003) to train generative directed align-
ment models: HMM and IBM Model4 (Brown et
al., 1993) from training record-text pairs. These
models are currently being used in state-of-the-art
machine translation systems. Alignments between
matching DB records and text sequences are then
used for labeling at test time.
136
Extraction approaches. The first alternative
(DB-CRF) trains a linear-chain CRF for extrac-
tion on fields of the database entries only. Each
field of the record is treated as a separate labeled
text sequence. Given an unlabeled text sequence,
it is segmented and labeled using the Viterbi algo-
rithm. This method is an enhanced representative
for (Agichtein and Ganti, 2004) in which a lan-
guage model is trained for each column of the DB.
Another alternative technique constructs par-
tially annotated text data using the matching
records and a labeling function. The labeling func-
tion employs high-precision alignment rules to as-
sign labels to text tokens using labeled record to-
kens. We use exact and approximate token match-
ing rules to create a partially labeled sequence,
skipping tokens that cannot be unambiguously la-
beled. In our experiments, we achieve a pre-
cision of 97% and a recall of 70% using these
rules. Given a partially annotated citation text,
we train a linear-chain CRF by maximizing the
marginal likelihood of the observed labels. This
marginal CRF training method (Bellare and Mc-
Callum, 2007) (M-CRF) was the previous state-
of-the-art on this data set. Additionally, if a match-
ing record is available for a test citation text,
we can partially label tokens and use constrained
Viterbi decoding with labeled positions fixed at
their observed values (M+R-CRF approach).
Our third approach is similar to (Mann and Mc-
Callum, 2008). We create extraction expectation
criteria from labeled text sequences in the training
data and uses these criteria to learn a linear-chain
CRF for extraction (MM08). The performance
achieved by this approach is an upper bound on
methods that: (1) use labeled training records to
create extraction criteria, and, (2) only use extrac-
tion criteria without any alignment criteria.
Finally, we train a supervised linear-chain CRF
(GS-CRF) using the labeled text sequences from
the training set. This represents an upper bound on
the performance that can be achieved on our task.
All the extraction methods have access to the same
features as the ExtrCRF model.
4.2 Results
Table 5 shows the results of various alignment
algorithms applied to the record-text data set.
Alignment methods use the matching record to
perform labeling of a test citation text. The Align-
CRF model outperforms the best generative align-
HMM Model4 AlignCRF
accuracy 78.5% 79.8% 92.7%
author 92.7 94.9 99.0
title 93.3 95.1 97.3
date 69.5 66.3 81.9
venue 73.3 73.1 91.2
volume 50.0 49.2 78.5
number 53.5 66.3 68.0
pages 38.2 44.1 88.2
editor 22.8 21.5 78.1
publisher 29.7 31.0 72.6
series 77.4 77.3 74.6
O 49.6 58.8 85.7
Table 5: Token-labeling accuracy and per-label F1
for different alignment methods. These methods
all use matching DB records at test time. Bold-
faced numbers indicate the best performing model.
HMM, Model4: generative alignment models
from GIZA++, AlignCRF: alignment model from
this paper.
ment model Model4 (IBM Model 4) with an er-
ror reduction of 63.8%. Our conjecture is that
Model4 is getting stuck in sub-optimal local max-
ima during EM training since our training set only
contains hundreds of parallel record-text pairs.
This problem may be alleviated by training on a
large parallel corpus. Additionally, our alignment
model is superior toModel4 since it leverages rich
non-independent features of input sequence pairs.
Table 6 shows the performance of various ex-
traction methods. Except M+R-CRF, all extrac-
tion approaches, do not use any record information
at test time. In comparison to the previous state-
of-the-artM-CRF, theExtrCRFmethod provides
an error reduction of 35.1%. ExtrCRF also pro-
duces an error reduction of 21.7% compared to
M+R-CRF without the use of matching records.
These reductions are significant at level p = 0.005
using the two-tailed t-test. Training only on DB
records is not helpful for extraction as we do not
learn the transition structure
9
and additional con-
text information
10
in text. This explains the low
accuracy of the DB-CRF method. Furthermore,
theMM08 approach (Mann and McCallum, 2008)
achieves low accuracy since it does not use any
9
In general, the editor field follows the title field while the
author field precedes it.
10
The token ?Vol.? generally precedes the volume field in
text. Similarly, tokens ?pp? and ?pages? occur before the
pages field.
137
DB-CRF M-CRF M+R-CRF
?
MM08 ExtrCRF GS-CRF
accuracy 70.4% 88.9% 90.8% 73.5% 92.8% 96.5%
author 72.4 93.7 94.1 85.4 98.5 99.0
title 79.4 96.7 98.4 83.1 94.6 98.1
date 60.1 74.5 76.2 57.8 81.7 93.5
venue 67.3 89.4 91.5 73.2 89.8 95.9
volume 20.3 69.4 74.2 27.7 78.9 90.5
number 30.1 72.8 80.8 47.8 75.1 91.4
pages 41.4 80.9 84.5 49.6 92.1 94.1
editor 7.1 71.1 79.3 75.3 73.3 93.7
publisher 62.1 67.5 77.2 40.2 58.5 82.2
series 65.2 74.9 76.3 65.9 73.8 85.8
O 54.1 7.0 8.3 57.7 91.9 94.5
Table 6: Token-labeling accuracy and per-label F1 for different extraction methods. Except M+R-CRF
?
,
all other approaches do not use any records at test time. Bold-faced numbers indicate the best performing
model. DB-CRF: CRF trained on DB fields. M+R-CRF, M-CRF: CRFs trained from heuristic align-
ments. ExtrCRF: Extraction model presented in this paper. GS-CRF: CRF trained on human annotated
citation texts.
alignment criteria during training. Hence, align-
ment information is crucial for obtaining high ac-
curacy.
Note that we do not observe a decrease in per-
formance of ExtrCRF over AlignCRF although
we are not using the test records during decoding.
This is because: (1) a first-order model in Extr-
CRF improves performance compared to a zero-
order model in AlignCRF and (2) the use of noisy
DB records in the test set for alignment often in-
creases extraction error.
Both our models have a high F1 value for the
other label O because we provide our algorithm
with constraints for the label O. In contrast, since
there is no realization of the O field in the DB
records, both M-CRF and M+R-CRF methods
fail to label such tokens correctly. Our alignment
model trained using expectation criteria achieves
a performance of 92.7% close to gold-standard
training (GS-CRF) (96.5%). Furthermore, Ex-
trCRF obtains an accuracy of 92.8% similar to
AlignCRF without access to DB records due to
better modeling of transition structure and context.
5 Related Work
Recent research in information extraction (IE) has
focused on reducing the labeling effort needed
to train supervised IE systems. For instance,
Grenager et al (2005) perform unsupervised
HMM learning for field segmentation, and bias
the model to prefer self-transitions and transi-
tions on boundary tokens. Unfortunately, such
unsupervised IE approaches do not attain perfor-
mance close to state-of-the-art supervised meth-
ods. Semi-supervised approaches that learn a
model with only a few constraints specifying
prior knowledge have generated much interest.
Haghighi and Klein (2006) assign each label in
the model certain prototypical features and train a
Markov random field for sequence tagging from
these labeled features. In contrast, our method
uses GE criteria (Mann and McCallum, 2008) ?
allowing soft-labeling of features with target ex-
pectation values ? to train conditional models with
complex and non-independent input features. Ad-
ditionally, in comparison to previous methods, an
information extractor trained from our record-text
alignments achieves accuracy of 93% making it
useful for real-world applications. Chang et al
(2007) use beam search for decoding unlabeled
text with soft and hard constraints, and train a
model with top-K decoded label sequences. How-
ever, this model requires large number of labeled
examples (e.g., 300 annotated citations) to boot-
strap itself. Active learning is another popular ap-
proach for reducing annotation effort. Settles and
Craven (2008) provide a comparison of various ac-
tive learning strategies for sequence labeling tasks.
We have shown, however, that in domains where a
database can provide significant supervision, one
can bootstrap accurate extractors with very little
human effort.
138
Another area of research, related to the task
described in our paper, is learning extractors
from database records. These records are also
known as field books and reference sets in liter-
ature (Canisius and Sporleder, 2007; Michelson
and Knoblock, 2008). Both Agichtein and Ganti
(2004) and Canisius and Sporleder (2007) train a
language model for each database column. The
language modeling approach is sensitive to word
re-orderings in text and other variability present
in real-world text (e.g., abbreviation). We allow
for word and field re-orderings through alignments
and model complex transformations through fea-
ture functions. Michelson and Knoblock (2008)
extract information from unstructured texts using a
rule-based approach to align segments of text with
fields in a DB record. Our probabilistic alignment
approach is more robust and uses rich features of
the alignment to obtain high performance.
Recently, Snyder and Barzilay (2007) and Liang
et al (2009) have explored record-text matching in
domains with unstructured texts. Unlike a semi-
structured text sequence obtained by noisily con-
catenating fields from a single record, an unstruc-
tured sequence may contain fields from multiple
records embedded in large amounts of extraneous
text. Hence, the problems of record-text matching
and word alignment are significantly harder in un-
structured domains. Snyder and Barzilay (2007)
achieve a state-of-the-art performance of 80% F1
on matching multiple NFL database records to
sentences in the news summary of a football game.
Their algorithm is trained using supervised ma-
chine learning and learns alignments at the level of
sentences and DB records. In contrast, this paper
presents a semi-supervised learning algorithm for
learning token-level alignments between records
and texts. Liang et al (2009) describe a model that
simultaneously performs record-text matching and
word alignment in unstructured domains. Their
model is trained in an unsupervised fashion using
EM. It may be possible to further improve their
model performance by incorporating prior knowl-
edge in the form of expectation criteria.
Traditionally, generative word alignment mod-
els have been trained on massive parallel cor-
pora (Brown et al, 1993). Recently, discrim-
inative alignment methods trained using anno-
tated alignments on small parallel corpora have
achieved superior performance. Taskar et al
(2005) train a discriminative alignment model
from annotated alignments using a large-margin
method. Labeled alignments are also used by
Blunsom and Cohn (2006) to train a CRF word
alignment model. Our method is trained using a
small number of easily specified expectation cri-
teria thus avoiding tedious and expensive human
labeling of alignments. An alternate method of
learning alignment models is proposed by McCal-
lum et al (2005) in which the training set consists
of sequence pairs classified as match or mismatch.
Alignments are learned to identify the class of a
given sequence pair. However, this method relies
on carefully selected negative examples to pro-
duce high-accuracy alignments. Our method pro-
duces good alignments as we directly encode prior
knowledge about alignments.
6 Conclusion and Future Work
Information extraction is an important first step in
data mining applications. Earlier approaches for
learning reliable extractors have relied on manu-
ally annotated text corpora. This paper presents a
novel approach for training extractors using align-
ments between texts and existing database records.
Our approach achieves performance close to su-
pervised training with very little supervision.
In the future, we plan to surpass supervised ac-
curacy by applying our method to millions of par-
allel record-text pairs collected automatically us-
ing matching. We also want to explore the addi-
tion of Markov dependencies into our alignment
model and other constraints such as monotonicity
and one-to-one correspondence.
Acknowledgments
This work was supported in part by the Center
for Intelligent Information Retrieval and in part by
The Central Intelligence Agency, the National Se-
curity Agency and National Science Foundation
under NSF grant #IIS-0326249. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are the authors? and do not
necessarily reflect those of the sponsor.
References
Eugene Agichtein and Venkatesh Ganti. 2004. Mining
reference tables for automatic text segmentation. In
KDD.
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In ICDL.
139
Kedar Bellare and Andrew McCallum. 2007. Learn-
ing extractors from unlabeled text using relevant
databases. In IIWeb workshop at AAAI 2007.
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In
ACL.
Sergey Brin. 1998. Extracting patterns and relations
from the world wide web. In EDBT Workshop,
pages 172?183.
Peter Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert Mercer. 1993. The mathematics
of statistical machine translation: parameter estima-
tion. Computational Linguistics, 19:263?311.
Sander Canisius and Caroline Sporleder. 2007. Boot-
strapping information extraction from field books.
In EMNLP-CoNLL.
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. In
ACL, pages 280?287.
William Cohen, Pradeep Ravikumar, and Stephen Fien-
berg. 2003. A comparison of string distance metrics
for name-matching tasks. In IJCAI.
O. Etzioni, M. Cafarella, D. Downey, A.-M. Popescu,
T. Shaked, S. Soderland, D. S. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from
the Web: An experimental study. Artificial Intelli-
gence, 165.
D. Freitag and A. McCallum. 1999. Information ex-
traction with HMM and shrinkage. In AAAI.
T. Grenager, D. Klein, and C. D. Manning. 2005. Un-
supervised learning of field segmentation models for
information extraction. In ACL.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In HLT-NAACL.
John Lafferty, Andrew McCallum, and Fernando C N
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML, page 282.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Association for Computational Linguistics (ACL).
Gideon S. Mann and Andrew McCallum. 2008.
Generalized expectation criteria for semi-supervised
learning of conditional random fields. In Proceed-
ings of ACL?08, pages 870?878.
I. R. Mansuri and S. Sarawagi. 2006. Integrating un-
structured data into relational databases. In ICDE.
Andrew McCallum, Kedar Bellare, and Fernando
Pereira. 2005. A conditional random field for
discriminatively-trained finite-state string edit dis-
tance. In UAI.
MatthewMichelson and Craig A. Knoblock. 2005. Se-
mantic annotation of unstructured and ungrammati-
cal text. In IJCAI, pages 1091?1098.
Matthew Michelson and Craig A. Knoblock. 2008.
Creating relational data from unstructured and un-
grammatical data sources. JAIR, 31:543?590.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29.
Fuchun Peng and A. McCallum. 2004. Accurate infor-
mation extraction from research papers using condi-
tional random fields. In HLT-NAACL.
Lawrence R. Rabiner. 1989. A tutorial on hidden
markov models and selected applications in speech
processing. IEEE, 17:257?286.
Sridhar Ramakrishnan and Sarit Mukherjee. 2004.
Taming the unstructured: Creating structured con-
tent from partially labeled schematic text sequences.
In CoopIS/DOA/ODBASE, volume 2, page 909.
Sunita Sarawagi and William W. Cohen. 2004. Ex-
ploiting dictionaries in named entity extraction:
combining semi-markov extraction processes and
data integration methods. In KDD, page 89.
Sunita Sarawagi and William W. Cohen. 2005. Semi-
Markov conditional random fields for information
extraction. In NIPS.
Burr Settles and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In EMNLP, pages 1070?1079.
K. Seymore, A. McCallum, and R. Rosenfeld. 1999.
Learning hidden markov model structure for infor-
mation extraction. In Proceedings of the AAAI
Workshop on ML for IE.
Benjamin Snyder and Regina Barzilay. 2007.
Database-text alignment via structured multi-label
classification. In IJCAI.
Charles Sutton, Michael Sindelar, and Andrew McCal-
lum. 2006. Reducing weight undertraining in struc-
tured discriminative learning. In HLT-NAACL.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In HLT-EMNLP, pages 73?80.
140
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 880?889,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Polylingual Topic Models
David Mimno Hanna M. Wallach Jason Naradowsky
University of Massachusetts, Amherst
Amherst, MA 01003
{mimno, wallach, narad, dasmith, mccallum}cs.umass.edu
David A. Smith Andrew McCallum
Abstract
Topic models are a useful tool for analyz-
ing large text collections, but have previ-
ously been applied in only monolingual,
or at most bilingual, contexts. Mean-
while, massive collections of interlinked
documents in dozens of languages, such
as Wikipedia, are now widely available,
calling for tools that can characterize con-
tent in many languages. We introduce a
polylingual topic model that discovers top-
ics aligned across multiple languages. We
explore the model?s characteristics using
two large corpora, each with over ten dif-
ferent languages, and demonstrate its use-
fulness in supporting machine translation
and tracking topic trends across languages.
1 Introduction
Statistical topic models have emerged as an in-
creasingly useful analysis tool for large text col-
lections. Topic models have been used for analyz-
ing topic trends in research literature (Mann et al,
2006; Hall et al, 2008), inferring captions for im-
ages (Blei and Jordan, 2003), social network anal-
ysis in email (McCallum et al, 2005), and expand-
ing queries with topically related words in infor-
mation retrieval (Wei and Croft, 2006). Much of
this work, however, has occurred in monolingual
contexts. In an increasingly connected world, the
ability to access documents in many languages has
become both a strategic asset and a personally en-
riching experience. In this paper, we present the
polylingual topic model (PLTM). We demonstrate
its utility and explore its characteristics using two
polylingual corpora: proceedings of the European
parliament (in eleven languages) and a collection
of Wikipedia articles (in twelve languages).
There are many potential applications for
polylingual topic models. Although research liter-
ature is typically written in English, bibliographic
databases often contain substantial quantities of
work in other languages. To perform topic-based
bibliometric analysis on these collections, it is
necessary to have topic models that are aligned
across languages. Such analysis could be sig-
nificant in tracking international research trends,
where language barriers slow the transfer of ideas.
Previous work on bilingual topic modeling
has focused on machine translation applications,
which rely on sentence-aligned parallel transla-
tions. However, the growth of the internet, and
in particular Wikipedia, has made vast corpora
of topically comparable texts?documents that are
topically similar but are not direct translations of
one another?considerably more abundant than
ever before. We argue that topic modeling is
both a useful and appropriate tool for leveraging
correspondences between semantically compara-
ble documents in multiple different languages.
In this paper, we use two polylingual corpora
to answer various critical questions related to
polylingual topic models. We employ a set of di-
rect translations, the EuroParl corpus, to evaluate
whether PLTM can accurately infer topics when
documents genuinely contain the same content.
We also explore how the characteristics of dif-
ferent languages affect topic model performance.
The second corpus, Wikipedia articles in twelve
languages, contains sets of documents that are not
translations of one another, but are very likely to
be about similar concepts. We use this corpus
to explore the ability of the model both to infer
similarities between vocabularies in different lan-
guages, and to detect differences in topic emphasis
between languages. The internet makes it possible
for people all over the world to access documents
from different cultures, but readers will not be flu-
ent in this wide variety of languages. By linking
topics across languages, polylingual topic mod-
els can increase cross-cultural understanding by
providing readers with the ability to characterize
880
the contents of collections in unfamiliar languages
and identify trends in topic prevalence.
2 Related Work
Bilingual topic models for parallel texts with
word-to-word alignments have been studied pre-
viously using the HM-bitam model (Zhao and
Xing, 2007). Tam, Lane and Schultz (Tam et
al., 2007) also show improvements in machine
translation using bilingual topic models. Both
of these translation-focused topic models infer
word-to-word alignments as part of their inference
procedures, which would become exponentially
more complex if additional languages were added.
We take a simpler approach that is more suit-
able for topically similar document tuples (where
documents are not direct translations of one an-
other) in more than two languages. A recent ex-
tended abstract, developed concurrently by Ni et
al. (Ni et al, 2009), discusses a multilingual topic
model similar to the one presented here. How-
ever, they evaluate their model on only two lan-
guages (English and Chinese), and do not use the
model to detect differences between languages.
They also provide little analysis of the differ-
ences between polylingual and single-language
topic models. Outside of the field of topic mod-
eling, Kawaba et al (Kawaba et al, 2008) use
a Wikipedia-based model to perform sentiment
analysis of blog posts. They find, for example,
that English blog posts about the Nintendo Wii of-
ten relate to a hack, which cannot be mentioned in
Japanese posts due to Japanese intellectual prop-
erty law. Similarly, posts about whaling often
use (positive) nationalist language in Japanese and
(negative) environmentalist language in English.
3 Polylingual Topic Model
The polylingual topic model (PLTM) is an exten-
sion of latent Dirichlet alocation (LDA) (Blei et
al., 2003) for modeling polylingual document tu-
ples. Each tuple is a set of documents that are
loosely equivalent to each other, but written in dif-
ferent languages, e.g., corresponding Wikipedia
articles in French, English and German. PLTM as-
sumes that the documents in a tuple share the same
tuple-specific distribution over topics. This is un-
like LDA, in which each document is assumed to
have its own document-specific distribution over
topics. Additionally, PLTM assumes that each
?topic? consists of a set of discrete distributions
D
N
1
T
N
L
...
w
? ?
wz
z
...
?
1
?
L
?
1
?
L
Figure 1: Graphical model for PLTM.
over words?one for each language l = 1, . . . , L.
In other words, rather than using a single set of
topics ? = {?
1
, . . . ,?
T
}, as in LDA, there are L
sets of language-specific topics, ?
1
, . . . ,?
L
, each
of which is drawn from a language-specific sym-
metric Dirichlet with concentration parameter ?
l
.
3.1 Generative Process
A new document tuplew = (w
1
, . . . ,w
L
) is gen-
erated by first drawing a tuple-specific topic dis-
tribution from an asymmetric Dirichlet prior with
concentration parameter ? and base measurem:
? ? Dir (?, ?m). (1)
Then, for each language l, a latent topic assign-
ment is drawn for each token in that language:
z
l
? P (z
l
|?) =
?
n
?
z
l
n
. (2)
Finally, the observed tokens are themselves drawn
using the language-specific topic parameters:
w
l
? P (w
l
| z
l
,?
l
) =
?
n
?
l
w
l
n
|z
l
n
. (3)
The graphical model is shown in figure 1.
3.2 Inference
Given a corpus of training and test document
tuples?W and W
?
, respectively?two possible
inference tasks of interest are: computing the
probability of the test tuples given the training
tuples and inferring latent topic assignments for
test documents. These tasks can either be accom-
plished by averaging over samples of ?
1
, . . . ,?
L
and ?m from P (?
1
, . . . ,?
L
, ?m |W
?
, ?) or by
evaluating a point estimate. We take the lat-
ter approach, and use the MAP estimate for ?m
and the predictive distributions over words for
?
1
, . . . ,?
L
. The probability of held-out docu-
ment tuples W
?
given training tuples W is then
approximated by P (W
?
|?
1
, . . . ,?
L
, ?m).
Topic assignments for a test document tuple
w = (w
1
, . . . ,w
L
) can be inferred using Gibbs
881
sampling. Gibbs sampling involves sequentially
resampling each z
l
n
from its conditional posterior:
P (z
l
n
= t |w, z
\l,n
,?
1
, . . . ,?
L
, ?m)
? ?
l
w
l
n
|t
(N
t
)
\l,n
+ ?m
t
?
t
N
t
? 1 + ?
, (4)
where z
\l,n
is the current set of topic assignments
for all other tokens in the tuple, while (N
t
)
\l,n
is
the number of occurrences of topic t in the tuple,
excluding z
l
n
, the variable being resampled.
4 Results on Parallel Text
Our first set of experiments focuses on document
tuples that are known to consist of direct transla-
tions. In this case, we can be confident that the
topic distribution is genuinely shared across all
languages. Although direct translations in multi-
ple languages are relatively rare (in contrast with
comparable documents), we use direct translations
to explore the characteristics of the model.
4.1 Data Set
The EuroParl corpus consists of parallel texts in
eleven western European languages: Danish, Ger-
man, Greek, English, Spanish, Finnish, French,
Italian, Dutch, Portuguese and Swedish. These
texts consist of roughly a decade of proceedings
of the European parliament. For our purposes we
use alignments at the speech level rather than the
sentence level, as in many translation tasks using
this corpus. We also remove the twenty-five most
frequent word types for efficiency reasons. The
remaining collection consists of over 121 million
words. Details by language are shown in Table 1.
Table 1: Average document length, # documents, and
unique word types per 10,000 tokens in the EuroParl corpus.
Lang. Avg. leng. # docs types/10k
DA 160.153 65245 121.4
DE 178.689 66497 124.5
EL 171.289 46317 124.2
EN 176.450 69522 43.1
ES 170.536 65929 59.5
FI 161.293 60822 336.2
FR 186.742 67430 54.8
IT 187.451 66035 69.5
NL 176.114 66952 80.8
PT 183.410 65718 68.2
SV 154.605 58011 136.1
Models are trained using 1000 iterations of
Gibbs sampling. Each language-specific topic?
word concentration parameter ?
l
is set to 0.01.
centralbank europ?iske ecb s l?n centralbanks 
zentralbank ezb bank europ?ischen investitionsbank darlehen 
??????? ???????? ???????? ??? ????????? ???????? 
bank central ecb banks european monetary 
banco central europeo bce bancos centrales 
keskuspankin ekp n euroopan keskuspankki eip 
banque centrale bce europ?enne banques mon?taire 
banca centrale bce europea banche prestiti 
bank centrale ecb europese banken leningen 
banco central europeu bce bancos empr?stimos 
centralbanken europeiska ecb centralbankens s l?n 
b?rn familie udnyttelse b?rns b?rnene seksuel 
kinder kindern familie ausbeutung familien eltern 
?????? ??????? ?????????? ??????????? ?????? ???????? 
children family child sexual families exploitation 
ni?os familia hijos sexual infantil menores 
lasten lapsia lapset perheen lapsen lapsiin 
enfants famille enfant parents exploitation familles 
bambini famiglia figli minori sessuale sfruttamento 
kinderen kind gezin seksuele ouders familie 
crian?as fam?lia filhos sexual crian?a infantil 
barn barnen familjen sexuellt familj utnyttjande 
m?l n? m?ls?tninger m?let m?ls?tning opn? 
ziel ziele erreichen zielen erreicht zielsetzungen 
??????? ????? ?????? ?????? ?????? ???????? 
objective objectives achieve aim ambitious set 
objetivo objetivos alcanzar conseguir lograr estos 
tavoite tavoitteet tavoitteena tavoitteiden tavoitteita tavoitteen 
objectif objectifs atteindre but cet ambitieux 
obiettivo obiettivi raggiungere degli scopo quello 
doelstellingen doel doelstelling bereiken bereikt doelen 
objectivo objectivos alcan?ar atingir ambicioso conseguir 
m?l m?let uppn? m?len m?ls?ttningar m?ls?ttning 
andre anden side ene andet ?vrige 
anderen andere einen wie andererseits anderer 
????? ???? ???? ????? ?????? ???? 
other one hand others another there 
otros otras otro otra parte dem?s 
muiden toisaalta muita muut muihin muun 
autres autre part c?t? ailleurs m?me 
altri altre altro altra dall parte 
andere anderzijds anderen ander als kant 
outros outras outro lado outra noutros 
andra sidan ? annat ena annan 
DA
DE
EL
EN
ES
FI
FR
IT
NL
PT
SV
 
DA
DE
EL
EN
ES
FI
FR
IT
NL
PT
SV
 
DA
DE
EL
EN
ES
FI
FR
IT
NL
PT
SV
 
DA
DE
EL
EN
ES
FI
FR
IT
NL
PT
SV
 
Figure 2: EuroParl topics (T=400)
The concentration parameter ? for the prior over
document-specific topic distributions is initialized
to 0.01T , while the base measure m is initialized
to the uniform distribution. Hyperparameters ?m
are re-estimated every 10 Gibbs iterations.
4.2 Analysis of Trained Models
Figure 2 shows the most probable words in all lan-
guages for four example topics, from PLTM with
400 topics. The first topic contains words relating
to the European Central Bank. This topic provides
an illustration of the variation in technical ter-
minology captured by PLTM, including the wide
array of acronyms used by different languages.
The second topic, concerning children, demon-
strates the variability of everyday terminology: al-
though the four Romance languages are closely
882
related, they use etymologically unrelated words
for children. (Interestingly, all languages except
Greek and Finnish use closely related words for
?youth? or ?young? in a separate topic.) The third
topic demonstrates differences in inflectional vari-
ation. English and the Romance languages use
only singular and plural versions of ?objective.?
The other Germanic languages include compound
words, while Greek and Finnish are dominated by
inflected variants of the same lexical item. The fi-
nal topic demonstrates that PLTM effectively clus-
ters ?syntactic? words, as well as more semanti-
cally specific nouns, adjectives and verbs.
Although the topics in figure 2 seem highly fo-
cused, it is interesting to ask whether the model
is genuinely learning mixtures of topics or simply
assigning entire document tuples to single topics.
To answer this question, we compute the posterior
probability of each topic in each tuple under the
trained model. If the model assigns all tokens in
a tuple to a single topic, the maximum posterior
topic probability for that tuple will be near to 1.0.
If the model assigns topics uniformly, the maxi-
mum topic probability will be near 1/T . We com-
pute histograms of these maximum topic prob-
abilities for T ? {50, 100, 200, 400, 800}. For
clarity, rather than overlaying five histograms, fig-
ure 3 shows the histograms converted into smooth
curves using a kernel density estimator.
1
Although
there is a small bump around 1.0 (for extremely
short documents, e.g., ?Applause?), values are
generally closer to, but greater than, 1/T .
0.0 0.2 0.4 0.6 0.8 1.0
0
2
4
6
8
10
12
Smoothed histograms of max(P(t))
Maximum topic probability in document
Den
sity 800 topics400 topics200 topics100 topics50 topics
Figure 3: Smoothed histograms of the probability of the
most probable topic in a document tuple.
Although the posterior distribution over topics
for each tuple is not concentrated on one topic,
it is worth checking that this is not simply be-
cause the model is assigning a single topic to the
1
We use the R density function.
tokens in each of the languages. Although the
model does not distinguish between topic assign-
ment variables within a given document tuple (so
it is technically incorrect to speak of different pos-
terior distributions over topics for different docu-
ments in a given tuple), we can nevertheless divide
topic assignment variables between languages and
use them to estimate a Dirichlet-multinomial pos-
terior distribution for each language in each tuple.
For each tuple we can then calculate the Jensen-
Shannon divergence (the average of the KL di-
vergences between each distribution and a mean
distribution) between these distributions. Figure 4
shows the density of these divergences for differ-
ent numbers of topics. As with the previous fig-
ure, there are a small number of documents that
contain only one topic in all languages, and thus
have zero divergence. These tend to be very short,
formulaic parliamentary responses, however. The
vast majority of divergences are relatively low (1.0
indicates no overlap in topics between languages
in a given document tuple) indicating that, for each
tuple, the model is not simply assigning all tokens
in a particular language to a single topic. As the
number of topics increases, greater variability in
topic distributions causes divergence to increase.
0.0 0.1 0.2 0.3 0.4 0.5
0
5
10
15
20
Smoothed histograms of inter?language JS divergence
Jensen?Shannon Divergence
Den
sity
800 topics400 topics200 topics100 topics50 topics
Figure 4: Smoothed histograms of the Jensen-Shannon
divergences between the posterior probability of topics be-
tween languages.
4.3 Language Model Evaluation
A topic model specifies a probability distribution
over documents, or in the case of PLTM, docu-
ment tuples. Given a set of training document tu-
ples, PLTM can be used to obtain posterior esti-
mates of ?
1
, . . . ,?
L
and ?m. The probability of
previously unseen held-out document tuples given
these estimates can then be computed. The higher
the probability of the held-out document tuples,
the better the generalization ability of the model.
883
Analytically calculating the probability of a set
of held-out document tuples given ?
1
, . . . ,?
L
and
?m is intractable, due to the summation over an
exponential number of topic assignments for these
held-out documents. However, recently developed
methods provide efficient, accurate estimates of
this probability. We use the ?left-to-right? method
of (Wallach et al, 2009). We perform five esti-
mation runs for each document and then calculate
standard errors using a bootstrap method.
Table 2 shows the log probability of held-out
data in nats per word for PLTM and LDA, both
trained with 200 topics. There is substantial varia-
tion between languages. Additionally, the predic-
tive ability of PLTM is consistently slightly worse
than that of (monolingual) LDA. It is important to
note, however, that these results do not imply that
LDA should be preferred over PLTM?that choice
depends upon the needs of the modeler. Rather,
these results are intended as a quantitative analy-
sis of the difference between the two models.
Table 2: Held-out log probability in nats/word. (Smaller
magnitude implies better language modeling performance.)
PLTM does slightly worse than monolingual LDA models,
but the variation between languages is much larger.
Lang PLTM sd LDA sd
DA -8.11 0.00067 -8.02 0.00066
DE -8.17 0.00057 -8.08 0.00072
EL -8.44 0.00079 -8.36 0.00087
EN -7.51 0.00064 -7.42 0.00069
ES -7.98 0.00073 -7.87 0.00070
FI -9.25 0.00089 -9.21 0.00065
FR -8.26 0.00072 -8.19 0.00058
IT -8.11 0.00071 -8.02 0.00058
NL -7.84 0.00067 -7.75 0.00099
PT -7.87 0.00085 -7.80 0.00060
SV -8.25 0.00091 -8.16 0.00086
As the number of topics is increased, the word
counts per topic become very sparse in mono-
lingual LDA models, proportional to the size of
the vocabulary. Figure 5 shows the proportion
of all tokens in English and Finnish assigned to
each topic under LDA and PLTM with 800 topics.
More than 350 topics in the Finnish LDA model
have zero tokens assigned to them, and almost all
tokens are assigned to the largest 200 topics. En-
glish has a larger tail, with non-zero counts in all
but 16 topics. In contrast, PLTM assigns a sig-
nificant number of tokens to almost all 800 top-
ics, in very similar proportions in both languages.
PLTM topics therefore have a higher granularity ?
i.e., they are more specific. This result is impor-
tant: informally, we have found that increasing the
granularity of topics correlates strongly with user
perceptions of the utility of a topic model.
0 200 400 600 800
0.00
0.01
0.02
0.03
0.04
Sorted topic rank
Perc
enta
ge o
f tok
ens
Figure 5: Topics sorted by number of words assigned.
Finnish is in black, English is in red; LDA is solid, PLTM is
dashed. LDA in Finnish essentially learns a 200 topic model
when given 800 topics, while PLTM uses all 800 topics.
4.4 Partly Comparable Corpora
An important application for polylingual topic
modeling is to use small numbers of comparable
document tuples to link topics in larger collections
of distinct, non-comparable documents in multiple
languages. For example, a journal might publish
papers in English, French, German and Italian. No
paper is exactly comparable to any other paper, but
they are all roughly topically similar. If we wish
to perform topic-based bibliometric analysis, it is
vital to be able to track the same topics across all
languages. One simple way to achieve this topic
alignment is to add a small set of comparable doc-
ument tuples that provide sufficient ?glue? to bind
the topics together. Continuing with the exam-
ple above, one might extract a set of connected
Wikipedia articles related to the focus of the jour-
nal and then train PLTM on a joint corpus consist-
ing of journal papers and Wikipedia articles.
In order to simulate this scenario we create a
set of variations of the EuroParl corpus by treat-
ing some documents as if they have no paral-
lel/comparable texts ? i.e., we put each of these
documents in a single-document tuple. To do this,
we divide the corpusW into two sets of document
tuples: a ?glue? set G and a ?separate? set S such
that |G| / |W| = p. In other words, the proportion
of tuples in the corpus that are treated as ?glue?
(i.e., placed in G) is p. For every tuple in S, we
assign each document in that tuple to a new single-
document tuple. By doing this, every document in
S has its own distribution over topics, independent
of any other documents. Ideally, the ?glue? doc-
884
uments in G will be sufficient to align the topics
across languages, and will cause comparable doc-
uments in S to have similar distributions over top-
ics even though they are modeled independently.
Table 3: The effect of the proportion p of ?glue? tuples on
mean Jensen-Shannon divergence in estimated topic distribu-
tions for pairs of documents in S that were originally part of
a document tuple. Lower divergence means the topic distri-
butions distributions are more similar to each other.
p Mean JS # of pairs Std. Err.
0.01 0.83755 487670 0.00018
0.05 0.79144 467288 0.00021
0.1 0.70228 443753 0.00026
0.25 0.38480 369608 0.00029
0.5 0.29712 246380 0.00030
Table 4: Topics are meaningful within languages but di-
verge between languages when only 1% of tuples are treated
as ?glue? tuples. With 25% ?glue? tuples, topics are aligned.
lang Topics at p = 0.01
DE ru?land russland russischen tschetschenien sicherheit
EN china rights human country s burma
FR russie tch?etch?enie union avec russe r?egion
IT ho presidente mi perch?e relazione votato
lang Topics at p = 0.25
DE ru?land russland russischen tschetschenien ukraine
EN russia russian chechnya cooperation region belarus
FR russie tch?etch?enie avec russe russes situation
IT russia unione cooperazione cecenia regione russa
We train PLTM with 100 topics on corpora with
p ? {0.01, 0.05, 0.1, 0.25, 0.5}. We use 1000 it-
erations of Gibbs sampling with ? = 0.01. Hy-
perparameters ?m are re-estimated every 10 it-
erations. We calculate the Jensen-Shannon diver-
gence between the topic distributions for each pair
of individual documents in S that were originally
part of the same tuple prior to separation. The
lower the divergence, the more similar the distri-
butions are to each other. From the results in fig-
ure 4, we know that leaving all document tuples
intact should result in a mean JS divergence of
less than 0.1. Table 3 shows mean JS divergences
for each value of p. As expected, JS divergence is
greater than that obtained when all tuples are left
intact. Divergence drops significantly when the
proportion of ?glue? tuples increases from 0.01 to
0.25. Example topics for p = 0.01 and p = 0.25
are shown in table 4. At p = 0.01 (1% ?glue? doc-
uments), German and French both include words
relating to Russia, while the English and Italian
word distributions appear locally consistent but
unrelated to Russia. At p = 0.25, the top words
for all four languages are related to Russia.
These results demonstrate that PLTM is appro-
priate for aligning topics in corpora that have only
a small subset of comparable documents. One area
for future work is to explore whether initializa-
tion techniques or better representations of topic
co-occurrence might result in alignment of topics
with a smaller proportion of comparable texts.
4.5 Machine Translation
Although the PLTM is clearly not a substitute for
a machine translation system?it has no way to
represent syntax or even multi-word phrases?it is
clear from the examples in figure 2 that the sets of
high probability words in different languages for a
given topic are likely to include translations. We
therefore evaluate the ability of the PLTM to gen-
erate bilingual lexica, similar to other work in un-
supervised translation modeling (Haghighi et al,
2008). In the early statistical translation model
work at IBM, these representations were called
?cepts,? short for concepts (Brown et al, 1993).
We evaluate sets of high-probability words in
each topic and multilingual ?synsets? by compar-
ing them to entries in human-constructed bilingual
dictionaries, as done by Haghighi et al (2008).
Unlike previous work (Koehn and Knight, 2002),
we evaluate all words, not just nouns. We col-
lected bilingual lexica mapping English words to
German, Greek, Spanish, French, Italian, Dutch
and Swedish. Each lexicon is a set of pairs con-
sisting of an English word and a translated word,
{w
e
, w
`
}. We do not consider multi-word terms.
We expect that simple analysis of topic assign-
ments for sequential words would yield such col-
locations, but we leave this for future work.
For every topic t we select a small number K
of the most probable words in English (e) and
in each ?translation? language (`): W
te
and W
t`
,
respectively. We then add the Cartesian product
of these sets for every topic to a set of candidate
translations C. We report the number of elements
of C that appear in the reference lexica. Results
for K = 1, that is, considering only the single
most probable word for each language, are shown
in figure 6. Precision at this level is relatively
high, above 50% for Spanish, French and Italian
with T = 400 and 800. Many of the candidate
pairs that were not in the bilingual lexica were
valid translations (e.g. EN ?comitology? and IT
885
?comitalogia?) that simply were not in the lexica.
We also do not count morphological variants: the
model finds EN ?rules? and DE ?vorschriften,? but
the lexicon contains only ?rule? and ?vorschrift.?
Results remain strong as we increase K. With
K = 3, T = 800, 1349 of the 7200 candidate
pairs for Spanish appeared in the lexicon.
l l
l
l
l
200 400 600 800
0
100
200
300
400
500
Translation pairs at K=1
Topics
Corr
ect t
rans
lation
s
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ESFRITDESVEL
Figure 6: Are the single most probable words for a given
topic in different languages translations of each other? The
number of such pairs that appear in bilingual lexica is shown
on the y-axis. For T = 800, the top English and Spanish
words in 448 topics were exact translations of one another.
4.6 Finding Translations
In addition to enhancing lexicons by aligning
topic-specific vocabulary, PLTM may also be use-
ful for adapting machine translation systems to
new domains by finding translations or near trans-
lations in an unstructured corpus. These aligned
document pairs could then be fed into standard
machine translation systems as training data. To
evaluate this scenario, we train PLTM on a set of
document tuples from EuroParl, infer topic distri-
butions for a set of held-out documents, and then
measure our ability to align documents in one lan-
guage with their translations in another language.
It is not necessarily clear that PLTM will be ef-
fective at identifying translations. In finding a low-
dimensional semantic representation, topic mod-
els deliberately smooth over much of the varia-
tion present in language. We are therefore inter-
ested in determining whether the information in
the document-specific topic distributions is suffi-
cient to identify semantically identical documents.
We begin by dividing the data into a training
set of 69,550 document tuples and a test set of
17,435 document tuples. In order to make the task
more difficult, we train a relatively coarse-grained
PLTM with 50 topics on the training set. We then
use this model to infer topic distributions for each
40
50
60
70
80
90
100
Min query doc length
% o
f tra
nsl 
at r
ank
0 50 100 200
Rank 1
Rank 5
Rank 10
Rank 20
Figure 7: Percent of query language documents for which
the target language translation is ranked at or above 1, 5, 10
or 20 by JS divergence, averaged over all language pairs.
of the 11 documents in each of the held-out doc-
ument tuples using a method similar to that used
to calculate held-out probabilities (Wallach et al,
2009). Finally, for each pair of languages (?query?
and ?target?) we calculate the difference between
the topic distribution for each held-out document
in the query language and the topic distribution for
each held-out document in the target language. We
use both Jensen-Shannon divergence and cosine
distance. For each document in the query language
we rank all documents in the target language and
record the rank of the actual translation.
Results averaged over all query/target language
pairs are shown in figure 7 for Jensen-Shannon
divergence. Cosine-based rankings are signifi-
cantly worse. It is important to note that the
length of documents matters. As noted before,
many of the documents in the EuroParl collection
consist of short, formulaic sentences. Restrict-
ing the query/target pairs to only those with query
and target documents that are both longer than 50
words results in significant improvement and re-
duced variance: the average proportion of query
documents for which the true translation is ranked
highest goes from 53.9% to 72.7%. Performance
continues to improve with longer documents, most
likely due to better topic inference. Results vary
by language. Table 5 shows results for all tar-
get languages with English as a query language.
Again, English generally performs better with Ro-
mance languages than Germanic languages.
5 Results on Comparable Texts
Directly parallel translations are rare in many lan-
guages and can be extremely expensive to pro-
duce. However, the growth of the web, and in par-
ticular Wikipedia, has made comparable text cor-
886
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
Figure 8: Squares represent the proportion of tokens in each language assigned to a topic. The left topic, world ski km won,
centers around Nordic counties. The center topic, actor role television actress, is relatively uniform. The right topic, ottoman
empire khan byzantine, is popular in all languages but especially in regions near Istanbul.
Table 5: Percent of English query documents for which the
translation was in the top n ? {1, 5, 10, 20} documents by JS
divergence between topic distributions. To reduce the effect
of short documents we consider only document pairs where
the query and target documents are longer than 100 words.
Lang 1 5 10 20
DA 78.0 90.7 93.8 95.8
DE 76.6 90.0 93.4 95.5
EL 77.1 90.4 93.3 95.2
ES 81.2 92.3 94.8 96.7
FI 76.7 91.0 94.0 96.3
FR 80.1 91.7 94.3 96.2
IT 79.1 91.2 94.1 96.2
NL 76.6 90.1 93.4 95.5
PT 80.8 92.0 94.7 96.5
SV 80.4 92.1 94.9 96.5
pora ? documents that are topically similar but are
not direct translations of one another ? consider-
ably more abundant than true parallel corpora.
In this section, we explore two questions re-
lating to comparable text corpora and polylingual
topic modeling. First, we explore whether com-
parable document tuples support the alignment of
fine-grained topics, as demonstrated earlier using
parallel documents. This property is useful for
building machine translation systems as well as
for human readers who are either learning new
languages or analyzing texts in languages they do
not know. Second, because comparable texts may
not use exactly the same topics, it becomes cru-
cially important to be able to characterize differ-
ences in topic prevalence at the document level (do
different languages have different perspectives on
the same article?) and at the language-wide level
(which topics do particular languages focus on?).
5.1 Data Set
We downloaded XML copies of all Wikipedia ar-
ticles in twelve different languages: Welsh, Ger-
man, Greek, English, Farsi, Finnish, French, He-
brew, Italian, Polish, Russian and Turkish. These
versions of Wikipedia were selected to provide a
diverse range of language families, geographic ar-
eas, and quantities of text. We preprocessed the
data by removing tables, references, images and
info-boxes. We dropped all articles in non-English
languages that did not link to an English article. In
the English version of Wikipedia we dropped all
articles that were not linked to by any other lan-
guage in our set. For efficiency, we truncated each
article to the nearest word after 1000 characters
and dropped the 50 most common word types in
each language. Even with these restrictions, the
size of the corpus is 148.5 million words.
We present results for a PLTM with 400 topics.
1000 Gibbs sampling iterations took roughly four
days on one CPU with current hardware.
5.2 Which Languages Have High Topic
Divergence?
As with EuroParl, we can calculate the Jensen-
Shannon divergence between pairs of documents
within a comparable document tuple. We can then
average over all such document-document diver-
gences for each pair of languages to get an over-
all ?disagreement? score between languages. In-
terestingly, we find that almost all languages in
our corpus, including several pairs that have his-
torically been in conflict, show average JS diver-
gences of between approximately 0.08 and 0.12
for T = 400, consistent with our findings for
EuroParl translations. Subtle differences of sen-
timent may be below the granularity of the model.
887
sadwrn blaned gallair at lloeren mytholeg 
space nasa sojus flug mission 
?????????? sts nasa ???? small 
space mission launch satellite nasa spacecraft 
??????? ??????? ???? ???? ??????? ?????  
sojuz nasa apollo ensimm?inen space lento 
spatiale mission orbite mars satellite spatial 
?????? ? ???? ??? ???? ????  
spaziale missione programma space sojuz stazione 
misja kosmicznej stacji misji space nasa 
??????????? ???? ???????????? ??????? ???????
uzay soyuz ay uzaya salyut sovyetler 
sbaen madrid el la jos? sbaeneg 
de spanischer spanischen spanien madrid la 
???????? ??????? de ??????? ??? ??????? 
de spanish spain la madrid y 
?????? ???? ????????? ???????  de ????  
espanja de espanjan madrid la real 
espagnol espagne madrid espagnole juan y 
???? ??????? ????? ?? ?????? ????  
de spagna spagnolo spagnola madrid el 
de hiszpa?ski hiszpanii la juan y 
?? ?????? ??????? ??????? ????????? de 
ispanya ispanyol madrid la k?ba real 
bardd gerddi iaith beirdd fardd gymraeg 
dichter schriftsteller literatur gedichte gedicht werk 
??????? ?????? ?????? ???? ??????? ???????? 
poet poetry literature literary poems poem 
???? ???? ????? ?????? ??? ????  
runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi 
po?te ?crivain litt?rature po?sie litt?raire ses 
?????? ????? ???? ???? ????? ?????
poeta letteratura poesia opere versi poema 
poeta literatury poezji pisarz in jego 
???? ??? ???????? ?????????? ?????? ????????? 
?air edebiyat ?iir yazar edebiyat? adl? 
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
 
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
 
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
Figure 9: Wikipedia topics (T=400).
Overall, these scores indicate that although indi-
vidual pages may show disagreement, Wikipedia
is on average consistent between languages.
5.3 Are Topics Emphasized Differently
Between Languages?
Although we find that if Wikipedia contains an ar-
ticle on a particular subject in some language, the
article will tend to be topically similar to the arti-
cles about that subject in other languages, we also
find that across the whole collection different lan-
guages emphasize topics to different extents. To
demonstrate the wide variation in topics, we cal-
culated the proportion of tokens in each language
assigned to each topic. Figure 8 represents the es-
timated probabilities of topics given a specific lan-
guage. Competitive cross-country skiing (left) ac-
counts for a significant proportion of the text in
Finnish, but barely exists in Welsh and the lan-
guages in the Southeastern region. Meanwhile,
interest in actors and actresses (center) is consis-
tent across all languages. Finally, historical topics,
such as the Byzantine and Ottoman empires (right)
are strong in all languages, but show geographical
variation: interest centers around the empires.
6 Conclusions
We introduced a polylingual topic model (PLTM)
that discovers topics aligned across multiple lan-
guages. We analyzed the characteristics of PLTM
in comparison to monolingual LDA, and demon-
strated that it is possible to discover aligned top-
ics. We also demonstrated that relatively small
numbers of topically comparable document tu-
ples are sufficient to align topics between lan-
guages in non-comparable corpora. Additionally,
PLTM can support the creation of bilingual lexica
for low resource language pairs, providing candi-
date translations for more computationally intense
alignment processes without the sentence-aligned
translations typically used in such tasks. When
applied to comparable document collections such
as Wikipedia, PLTM supports data-driven analysis
of differences and similarities across all languages
for readers who understand any one language.
7 Acknowledgments
The authors thank Limin Yao, who was involved
in early stages of this project. This work was
supported in part by the Center for Intelligent In-
formation Retrieval, in part by The Central In-
telligence Agency, the National Security Agency
and National Science Foundation under NSF grant
number IIS-0326249, and in part by Army prime
contract number W911NF-07-1-0216 and Uni-
versity of Pennsylvania subaward number 103-
548106, and in part by National Science Founda-
tion under NSF grant #CNS-0619337. Any opin-
ions, findings and conclusions or recommenda-
tions expressed in this material are the authors?
and do not necessarily reflect those of the sponsor.
References
David Blei and Michael Jordan. 2003. Modeling an-
notated data. In SIGIR.
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. JMLR.
Peter F Brown, Stephen A Della Pietra, Vincent J Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. CL, 19(2):263?311.
888
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In ACL, pages 771?779.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using
topic models. In EMNLP.
Mariko Kawaba, Hiroyuki Nakasaki, Takehito Utsuro,
and Tomohiro Fukuhara. 2008. Cross-lingual blog
analysis based on multilingual blog distillation from
multilingual Wikipedia entries. In ICWSM.
Philipp Koehn and Kevin Knight. 2002. Learn-
ing a translation lexicon from monolingual corpora.
In Proceedings of ACL Workshop on Unsupervised
Lexical Acquisition.
Gideon Mann, David Mimno, and Andrew McCal-
lum. 2006. Bibliometric impact measures leverag-
ing topic analysis. In JCDL.
Andrew McCallum, Andr?es Corrada-Emmanuel, and
Xuerui Wang. 2005. Topic and role discovery in
social networks. In IJCAI.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from Wikipedia.
In WWW.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for statistical ma-
chine translation. Machine Translation, 28:187?
207.
Hanna Wallach, Iain Murray, Ruslan Salakhutdinov,
and David Mimno. 2009. Evaluation methods for
topic models. In ICML.
Xing Wei and Bruce Croft. 2006. LDA-based docu-
ment models for ad-hoc retrieval. In SIGIR.
Bing Zhao and Eric P. Xing. 2007. HM-BiTAM: Bilin-
gual topic exploration, word alignment, and transla-
tion. In NIPS.
889
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 748?754, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Composition of Conditional Random Fields for Transfer Learning
Charles Sutton and Andrew McCallum
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
{sutton,mccallum}@cs.umass.edu
Abstract
Many learning tasks have subtasks for which
much training data exists. Therefore, we want
to transfer learning from the old, general-
purpose subtask to a more specific new task,
for which there is often less data. While work
in transfer learning often considers how the
old task should affect learning on the new
task, in this paper we show that it helps to
take into account how the new task affects the
old. Specifically, we perform joint decoding of
separately-trained sequence models, preserv-
ing uncertainty between the tasks and allowing
information from the new task to affect predic-
tions on the old task. On two standard text data
sets, we show that joint decoding outperforms
cascaded decoding.
1 Introduction
Many tasks in natural language processing are solved by
chaining errorful subtasks. Within information extrac-
tion, for example, part-of-speech tagging and shallow
parsing are often performed before the main extraction
task. Commonly these subtasks have their own standard
sets of labeled training data: for example, many large
data sets exist for learning to extract person names from
newswire text; whereas the available training data for new
applications, such as extracting appointment information
from email, tends to be much smaller. Thus, we need to
transfer regularities learned from a well-studied subtask,
such as finding person names in newswire text, to a new,
related task, such as finding names of speakers in email
seminar announcements.
In previous NLP systems, transfer is often accom-
plished by training a model for the subtask, and using its
prediction as a feature for the new task. For example, re-
cent CoNLL shared tasks (Tjong Kim Sang & De Meul-
der, 2003; Carreras & Marquez, 2004), which are stan-
dard data sets for such common NLP tasks as clause iden-
tification and named-entity recognition, include predic-
tions from a part-of-phrase tagger and a shallow parser as
features. But including only the single most likely sub-
task prediction fails to exploit useful dependencies be-
tween the tasks. First, if the subtask prediction is wrong,
the model for the new task may not be able to recover. Of-
ten, errors propagate upward through the chain of tasks,
causing errors in the final output. This problem can be
ameliorated by preserving uncertainty in the subtask pre-
dictions, because even if the best subtask prediction is
wrong, the distribution over predictions can still be some-
what accurate.
Second, information from the main task can inform the
subtask. This is especially important for learning trans-
fer, because the new domain often has different charac-
teristics than the old domain, which is often a standard
benchmark data set. For example, named-entity recog-
nizers are usually trained on newswire text, which is more
structured and grammatical than email, so we expect an
off-the-shelf named-entity recognizer to perform some-
what worse on email. An email task, however, often has
domain-specific features, such as PREVIOUS WORD IS
Speaker:), which were unavailable or uninformative to
the subtask on the old training set, but are very informa-
tive to the subtask in the new domain. While previous
work in transfer learning has considered how the old task
can help the new task, in this paper we show how the new
task can help itself by improving predictions on the old.
In this paper we address the issue of transfer by train-
ing a cascade of models independently on the various
training sets, but at test time combining them into a single
model in which decoding is performed jointly. For the in-
dividual models, we use linear-chain conditional random
fields (CRFs), because the great freedom that they allow
in feature engineering facilitates the learning of richer in-
teractions between the subtasks. We train a linear chain
CRF on each subtask, using the prediction of the previous
subtask as a feature. At test time, we combine the learned
weights from the original CRFs into a single grid-shaped
factorial CRF, which makes predictions for all the tasks
748
at once. Viterbi decoding in this combined model im-
plicitly considers all possible predictions for the subtask
when making decisions in the main task.
We evaluate joint decoding for learning transfer on a
standard email data set and a standard entity recognition
task. On the email data set, we show a significant gain
in performance, including new state-of-the-art results. Of
particular interest for transfer learning, we also show that
using joint decoding, we achieve equivalent results to cas-
caded decoding with 25% less training data.
2 Linear-chain CRFs
Conditional random fields (CRFs) (Lafferty et al, 2001)
are undirected graphical models that are conditionally
trained. In this section, we describe CRFs for the linear-
chain case. Linear-chain CRFs can be roughly under-
stood as conditionally-trained finite state machines. A
linear-chain CRF defines a distribution over state se-
quences s = {s1, s2, . . . , sT } given an input sequence
x = {x1, x2, . . . , xT } by making a first-order Markov
assumption on states. These Markov assumptions imply
that the distribution over sequences factorizes in terms of
pairwise functions ?t(st?1, st,x) as:
p(s|x) =
?
t ?t(st?1, st,x)
Z(x)
, (1)
The partition function Z(x) is defined to ensure that the
distribution is normalized:
Z(x) =
?
s?
?
t
?t(s
?
t?1, s
?
t,x). (2)
The potential functions ?t(st?1, st,x) can be interpreted
as the cost of making a transition from state st?1 to state
st at time t, similar to a transition probability in an HMM.
Computing the partition function Z(x) requires sum-
ming over all of the exponentially many possible state
sequences s?. By exploiting Markov assumptions, how-
ever, Z(x) (as well as the node marginals p(st|x) and the
Viterbi labeling) can be calculated efficiently by variants
of the standard dynamic programming algorithms used
for HMMs.
We assume the potentials factorize according to a set
of features {fk}, which are given and fixed, so that
?(st?1, st,x) = exp
(
?
k
?kfk(st?1, st,x, t)
)
. (3)
The model parameters are a set of real weights? = {?k},
one for each feature.
Feature functions can be arbitrary. For example, one
feature function could be a binary test fk(st?1, st,x, t)
that has value 1 if and only if st?1 has the label SPEAK-
ERNAME, st has the label OTHER, and the word xt be-
gins with a capital letter. The chief practical advantage
of conditional models, in fact, is that we can include ar-
bitrary highly-dependent features without needing to es-
timate their distribution, as would be required to learn a
generative model.
Given fully-labeled training instances {(sj ,xj)}Mj=1,
CRF training is usually performed by maximizing the pe-
nalized log likelihood
`(?) =
?
j
?
t
?
k
?kfk(sj,t?1, sj,t,x, t)
?
?
j
logZ(xj)?
?
k
?2k
2?2
(4)
where the final term is a zero-mean Gaussian prior placed
on parameters to avoid overfitting. Although this maxi-
mization cannot be done in closed form, it can be op-
timized numerically. Particularly effective are gradient-
based methods that use approximate second-order infor-
mation, such as conjugate gradient and limited-memory
BFGS (Byrd et al, 1994). For more information on
current training methods for CRFs, see Sha and Pereira
(2003).
3 Dynamic CRFs
Dynamic conditional random fields (Sutton et al, 2004)
extend linear-chain CRFs in the same way that dynamic
Bayes nets (Dean & Kanazawa, 1989) extend HMMs.
Rather than having a single monolithic state variable,
DCRFs factorize the state at each time step by an undi-
rected model.
Formally, DCRFs are the class of conditionally-trained
undirected models that repeat structure and parameters
over a sequence. If we denote by ?c(yc,t,xt) the repe-
tition of clique c at time step t, then a DCRF defines the
probability of a label sequence s given the input x as:
p(s|x) =
?
t ?c(yc,t,xt)
Z(x)
, (5)
where as before, the clique templates are parameterized
in terms of input features as
?c(yc,t,xt) = exp
{
?
k
?kfk(yc,t,xt)
}
. (6)
Exact inference in DCRFs can be performed by
forward-backward in the cross product state space, if the
cross-product space is not so large as to be infeasible.
Otherwise, approximate methods must be used; in our
experience, loopy belief propagation is often effective
in grid-shaped DCRFs. Even if inference is performed
monolithically, however, a factorized state representation
is still useful because it requires much fewer parame-
ters than a fully-parameterized linear chain in the cross-
product state space.
749
Sutton et al (2004) introduced the factorial CRF
(FCRF), in which the factorized state structure is a grid
(Figure 1). FCRFs were originally applied to jointly
performing interdependent language processing tasks, in
particular part-of-speech tagging and noun-phrase chunk-
ing. The previous work on FCRFs used joint training,
which requires a single training set that is jointly labeled
for all tasks in the cascade. For many tasks such data
is not readily available, for example, labeling syntac-
tic parse trees for every new Web extraction task would
be prohibitively expensive. In this paper, we train the
subtasks separately, which allows us the freedom to use
large, standard data sets for well-studied subtasks such as
named-entity recognition.
4 Alternatives for Learning Transfer
In this section, we enumerate several classes of methods
for learning transfer, based on the amount and type of
interaction they allow between the tasks. The principal
differences between methods are whether the individual
tasks are performed separately in a cascade or jointly;
whether a single prediction from the lower task is used,
or several; and what kind of confidence information is
shared between the subtasks.
The main types of transfer learning methods are:
1. Cascaded training and testing. This is the traditional
approach in NLP, in which the single best prediction
from the old task is used in the new task at training
and test time. In this paper, we show that allowing
richer interactions between the subtasks can benefit
performance.
2. Joint training and testing. In this family of ap-
proaches, a single model is trained to perform all the
subtasks at once. For example, in Caruana?s work
on multitask learning (Caruana, 1997), a neural net-
work is trained to jointly performmultiple classifica-
tion tasks, with hidden nodes that form a shared rep-
resentation among the tasks. Jointly trained meth-
ods allow potentially the richest interaction between
tasks, but can be expensive in both computation time
required for training and in human effort required to
label the joint training data.
Exact inference in a jointly-trained model, such
as forward-backward in an FCRF, implicitly con-
siders all possible subtask predictions with confi-
dence given by the model?s probability of the pre-
diction. However, for computational efficiency, we
can use inference methods such as particle filtering
and sparse message-passing (Pal et al, 2005), which
communicate only a limited number of predictions
between sections of the model.
Main Task
Subtask A
Subtask B
Input
Figure 1: Graphical model for the jointly-decoded CRF.
All of the pairwise cliques also have links to the observed
input, although we omit these edges in the diagram for
clarity.
3. Joint testing with cascaded training. Although a
joint model over all the subtasks can have better per-
formance, it is often much more expensive to train.
One approach for reducing training time is cascaded
training, which provides both computational effi-
ciency and the ability to reuse large, standard train-
ing sets for the subtasks. At test time, though, the
separately-trained models are combined into a sin-
gle model, so that joint decoding can propagate in-
formation between the tasks.
Even with cascaded training, it is possible to pre-
serve some uncertainty in the subtask?s predictions.
Instead of using only a single subtask prediction
for training the main task, the subtask can pass up-
wards a lattice of likely predictions, each of which
is weighted by the model?s confidence. This has the
advantage of making the training procedure more
similar to the joint testing procedure, in which all
possible subtask predictions are considered.
In the next two sections, we describe and evaluate
joint testing with cascaded training for transfer learning
in linear-chain CRFs. At training time, only the best
subtask prediction is used, without any confidence infor-
mation. Even though this is perhaps the simplest joint-
testing/cascaded-training method, we show that it still
leads to a significant gain in accuracy.
5 Composition of CRFs
In this section we briefly describe how we combine
individually-trained linear-chain CRFs using composi-
tion. For a series of N cascaded tasks, we train indi-
vidual CRFs separately on each task, using the prediction
of the previous CRF as a feature. We index the CRFs
by i, so that the state of CRF i at time t is denoted sit.
Thus, the feature functions for CRF i are of the form
f ik(s
i
t?1, s
i
t, s
i?1
t ,x, t)?that is, they depend not only on
the observed input x and the transition (sit?1 ? s
i
t) but
750
wt = w
wt matches [A-Z][a-z]+
wt matches [A-Z][A-Z]+
wt matches [A-Z]
wt matches [A-Z]+
wt matches [A-Z]+[a-z]+[A-Z]+[a-z]
wt appears in list of first names,
last names, honorifics, etc.
wt appears to be part of a time followed by a dash
wt appears to be part of a time preceded by a dash
wt appears to be part of a date
Tt = T
qk(x, t + ?) for all k and ? ? [?4, 4]
Table 1: Input features qk(x, t) for the seminars data. In
the above wt is the word at position t, Tt is the POS tag
at position t, w ranges over all words in the training data,
and T ranges over all Penn Treebank part-of-speech tags.
The ?appears to be? features are based on hand-designed
regular expressions that can span several tokens.
also on the state si?1t of the previous transducer.
We also add all conjunctions of the input features and
the previous transducer?s state, for example, a feature that
is 1 if the current state is SPEAKERNAME, the previ-
ous transducer predicted PERSONNAME, and the previ-
ous word is Host:.
To perform joint decoding at test time, we form the
composition of the individual CRFs, viewed as finite-
state transducers. That is, we define a new linear-chain
CRF whose state space is the cross product of the states
of the individual CRFs, and whose transition costs are the
sum of the transition costs of the individual CRFs.
Formally, let S1, S2, . . . SN be the state sets and
?1,?2, . . .?N the weights of the individual CRFs. Then
the state set of the combined CRF is S = S1?S2? . . .?
SN . We will denote weight k in an individual CRF i by
?ik and a single feature by f
i
k(s
i
t?1, s
i
t, s
i?1
t ,x, t). Then
for s ? S, the combined model is given by:
p(s|x) =
?
t exp
{?N
i=1
?
k ?
i
kf
i
k(s
i
t?1, s
i
t, s
i?1
t ,x, t)
}
Z(x)
.
(7)
The graphical model for the combined model is the fac-
torial CRF in Figure 1.
6 Experiments
6.1 Email Seminar Announcements
We evaluate joint decoding on a collection of 485 e-mail
messages announcing seminars at Carnegie Mellon Uni-
versity, gathered by Freitag (1998). The messages are
annotated with the seminar?s starting time, ending time,
location, and speaker. This data set has been the sub-
ject of much previous work using a wide variety of learn-
ing methods. Despite all this work, however, the best
50 100 150 200 250
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Number of training instances
F1
JointCascaded
Figure 2: Learning curves for the seminars data set on
the speaker field, averaged over 10-fold cross validation.
Joint training performs equivalently to cascaded decoding
with 25% more data.
reported systems have precision and recall on speaker
names of only about 70%?too low to use in a practical
system. This task is so challenging because the messages
are written by many different people, who each have dif-
ferent ways of presenting the announcement information.
Because the task includes finding locations and per-
son names, the output of a named-entity tagger is a use-
ful feature. It is not a perfectly indicative feature, how-
ever, because many other kinds of person names appear in
seminar announcements?for example, names of faculty
hosts, departmental secretaries, and sponsors of lecture
series. For example, the token Host: indicates strongly
both that what follows is a person name, but that person
is not the seminars? speaker.
Even so, named-entity predictions do improve per-
formance on this task. We use the predictions from a
CRF named-entity tagger that we trained on the standard
CoNLL 2003 English data set. The CoNLL 2003 data
set consists of newswire articles from Reuters labeled as
either people, locations, organizations, or miscellaneous
entities. It is much larger than the seminar announce-
ments data set. While the named-entity data contains
203,621 tokens for training, the seminar announcements
data set contains only slightly over 60,000 training to-
kens.
Previous work on the seminars data has used a one-
field-per-document evaluation. That is, for each field, the
CRF selects a single field value from its Viterbi path, and
this extraction is counted as correct if it exactly matches
any of the true field mentions in the document. We com-
pute precision and recall following this convention, and
report their harmonic mean F1. As in the previous work,
751
System stime etime location speaker overall
WHISK (Soderland, 1999) 92.6 86.1 66.6 18.3 65.9
SRV (Freitag, 1998) 98.5 77.9 72.7 56.3 76.4
HMM (Frietag & McCallum, 1999) 98.5 62.1 78.6 76.6 78.9
RAPIER (Califf & Mooney, 1999) 95.9 94.6 73.4 53.1 79.3
SNOW-IE (Roth & Wen-tau Yih, 2001) 99.6 96.3 75.2 73.8 86.2
(LP)2 (Ciravegna, 2001) 99.0 95.5 75.0 77.6 86.8
CRF (no transfer) This paper 99.1 97.3 81.0 73.7 87.8
CRF (cascaded) This paper 99.2 96.0 84.3 74.2 88.4
CRF (joint) This paper 99.1 96.0 85.3 76.3 89.2
Table 2: Comparison of F1 performance on the seminars data. Joint decoding performs significantly better than
cascaded decoding. The overall column is the mean of the other four. (This table was adapted from Peshkin and
Pfeffer (2003).)
we use 10-fold cross validation with a 50/50 training/test
split. We use a spherical Gaussian prior on parameters
with variance ?2 = 0.5.
We evaluate whether joint decoding with cascaded
training performs better than cascaded training and de-
coding. Table 2 compares cascaded and joint decoding
for CRFs with other previous results from the literature.1
The features we use are listed in Table 1. Although previ-
ous work has used very different feature sets, we include
a no-transfer CRF baseline to assess the impact of transfer
from the CoNLL data set. All the CRF runs used exactly
the same features.
On the most challenging fields, location and speaker,
cascaded transfer is more accurate than no transfer at all,
and joint decoding is more accurate than cascaded decod-
ing. In particular, for speaker, we see an error reduction
of 8% by using joint decoding over cascaded. The differ-
ence in F1 between cascaded and joint decoding is statis-
tically significant for speaker (paired t-test; p = 0.017)
but only marginally significant for location (p = 0.067).
Our results are competitive with previous work; for ex-
ample, on location, the CRF is more accurate than any of
the existing systems.
Examining the trained models, we can observe both
errors made by the general-purpose named entity tagger,
and how they can be corrected by considering the sem-
inars labels. In newswire text, long runs of capitalized
words are rare, often indicating the name of an entity. In
email announcements, runs of capitalized words are com-
mon in formatted text blocks like:
Location: Baker Hall
Host: Michael Erdmann
In this type of situation, the named entity tagger often
mistakes Host: for the name of an entity, especially be-
cause the word precedingHost is also capitalized. On one
of the cross-validated testing sets, of 80 occurrences of
1We omit one relevant paper (Peshkin & Pfeffer, 2003) be-
cause its evaluation method differs from all the other previous
work.
wt = w
wt matches [A-Z][a-z]+
wt matches [A-Z][A-Z]+
wt matches [A-Z]
wt matches [A-Z]+
wt matches [A-Z]+[a-z]+[A-Z]+[a-z]
wt is punctuation
wt appears in list of first names, last names, honorifics, etc.
qk(x, t + ?) for all k and ? ? [?2, 2]
Conjunction qk(x, t) and qk?(x, t) for all features k, k
?
Conjunction qk(x, t) and qk?(x, t + 1) for all features k, k
?
Table 3: Input features qk(x, t) for the ACE named-entity
data. In the above wt is the word at position t, and w
ranges over all words in the training data.
the wordHost:, the named-entity tagger labels 52 as some
kind of entity. When joint decoding is used, however,
only 20 occurrences are labeled as entities. Recall that
the joint model uses exactly the same weights as the cas-
caded model; the only difference is that the joint model
takes into account information about the seminar labels
when choosing named-entity labels. This is an example
of how domain-specific information from the main task
can improve performance on a more standard, general-
purpose subtask.
Figure 2 shows the difference in performance between
joint and cascaded decoding as a function of training set
size. Cascaded decoding with the full training set of 242
emails performs equivalently to joint decoding on only
181 training instances, a 25% reduction in the training
set.
In summary, even with a simple cascaded training
method on a well-studied data set, joint decoding per-
forms better for transfer than cascaded decoding.
6.2 Entity Recognition
In this section we give results on joint decoding for trans-
fer between two newswire data sets with similar but over-
lapping label sets. The Automatic Content Extraction
(ACE) data set is another standard entity recognition data
752
Transfer Type
none cascaded joint
Person name 81.0 86.9 87.3
Person nominal 34.9 36.1 42.4
Organization name 53.9 62.6 61.1
Organization nominal 33.7 35.3 40.8
GPE name 78.5 84.0 84.0
GPE nominal 51.2 54.1 59.2
Table 4: Comparison of F1 performance between joint
and cascaded training on the ACE entity recognition task.
GPE means geopolitical entities, such as countries. Joint
decoding helps most on the harder nominal (common
noun) references. These results were obtained using a
small subset of the training set.
set, containing 422 stories from newspaper, newswire,
and broadcast news. Unlike the CoNLL entity recog-
nition data set, in which only proper names of entities
are annotated, the ACE data includes annotation both for
named entities like United States, and also nominal men-
tions of entities like the nation. Thus, although the input
text has similar distribution in the CoNLL NER and ACE
data set, the label distributions are very different.
Current state-of-the-art systems for the ACE task (Flo-
rian et al, 2004) use the predictions of other named-entity
recognizers as features, that is, they use cascaded trans-
fer. In this experiment, we test whether the transfer be-
tween these datasets can be further improved using joint
decoding. We train a CRF entity recognizer on the ACE
dataset, with the output of a named-entity entity recog-
nizer trained on the CoNLL 2003 English data set. The
CoNLL recognizer is the same CRF as was used in the
previous experiment. In these results, we use a subset of
10% of the ACE training data. Table 3 lists the features
we use. Table 4 compares the results on some represen-
tative entity types. Again, cascaded decoding for transfer
is better than no transfer at al, and joint decoding is better
than cascaded decoding. Interestingly, joint decoding has
most impact on the harder nominal references, showing
marked improvement over the cascaded approach.
7 Related Work
Researchers have begun to accumulate experimental ev-
idence that joint training and decoding yields better per-
formance than the cascaded approach. As mentioned ear-
lier, the original work on dynamic CRFs (Sutton et al,
2004) demonstrated improvement due to joint training in
the domains of part-of-speech tagging and noun-phrase
chunking. Also, Carreras and Marquez (Carreras &
Ma`rquez, 2004) have obtained increased performance in
clause finding by training a cascade of perceptrons to
minimize a single global error function. Finally, Miller et
al. (Miller et al, 2000) have combined entity recognition,
parsing, and relation extraction into a jointly-trained sin-
gle statistical parsing model that achieves improved per-
formance on all the subtasks.
Part of the contribution of the current work is to sug-
gest that joint decoding can be effective even when joint
training is not possible because jointly-labeled data is un-
available. For example, Miller et al report that they orig-
inally attempted to annotate newswire articles for all of
parsing, relations, and named entities, but they stopped
because the annotation was simply too expensive. In-
stead they hand-labeled relations only, assigning parse
trees to the training set using a standard statistical parser,
which is potentially less flexible than the cascaded train-
ing, because the model for main task is trained explicitly
to match the noisy subtask predictions, rather than being
free to correct them.
In the speech community, it is common to com-
pose separately trained weighted finite-state transducers
(Mohri et al, 2002) for joint decoding. Our method ex-
tends this work to conditional models. Ordinarily, higher-
level transducers depend only on the output of the previ-
ous transducer: a transducer for the lexicon, for exam-
ple, consumes only phonemes, not the original speech
signal. In text, however, such an approach is not sensi-
ble, because there is simply not enough information in
the named-entity labels, for example, to do extraction if
the original words are discarded. In a conditional model,
weights in higher-level transducers are free to depend on
arbitrary features of the original input without any addi-
tional complexity in the finite-state structure.
Finally, stacked sequential learning (Cohen & Car-
valho, 2005) is another potential method for combining
the results of the subtask transducers. In this general
meta-learning method for sequential classification, first
a base classifier predicts the label at each time step, and
then a higher-level classifier makes the final prediction,
including as features a window of predictions from the
base classifier. For transfer learning, this would corre-
spond to having an independent base model for each sub-
task (e.g., independent CRFs for named-entity and sem-
inars), and then having a higher-level CRF that includes
as a feature the predictions from the base models.
8 Conclusion
In this paper we have shown that joint decoding improves
transfer between interdependent NLP tasks, even when
the old task is named-entity recognition, for which highly
accurate systems exist. The rich features afforded by a
conditional model allow the new task to influence the pre-
753
dictions of the old task, an effect that is only possible with
joint decoding.
It is now common for researchers to publicly release
trained models for standard tasks such as part-of-speech
tagging, named-entity recognition, and parsing. This pa-
per has implications for how such standard tools are pack-
aged. Our results suggest that off-the-shelf NLP tools
will need not only to provide a single-best prediction, but
also to be engineered so that they can easily communicate
distributions over predictions to models for higher-level
tasks.
Acknowledgments
This work was supported in part by the Center for Intelligent In-
formation Retrieval, in part by The Central Intelligence Agency,
the National Security Agency and National Science Foundation
under NSF grants #IIS-0326249 and #IIS-0427594, and in part
by the Defense Advanced Research Projects Agency (DARPA),
through the Department of the Interior, NBC, Acquisition Ser-
vices Division, under contract number NBCHD030010. Any
opinions, findings and conclusions or recommendations ex-
pressed in this material are the author(s) and do not necessarily
reflect those of the sponsor.
References
Byrd, R. H., Nocedal, J., & Schnabel, R. B. (1994). Repre-
sentations of quasi-Newton matrices and their use in limited
memory methods. Math. Program., 63, 129?156.
Califf, M. E., & Mooney, R. J. (1999). Relational learning
of pattern-match rules for information extraction. Proceed-
ings of the Sixteenth National Conference on Artificial Intel-
ligence (AAAI-99) (pp. 328?334).
Carreras, X., & Marquez, L. (2004). Introduction to the
CoNLL-2004 shared task: Semantic role labeling. Proceed-
ings of CoNLL-2004.
Carreras, X., & Ma`rquez, L. (2004). Online learning via global
feedback for phrase recognition. In S. Thrun, L. Saul and
B. Scho?lkopf (Eds.), Advances in neural information pro-
cessing systems 16. Cambridge, MA: MIT Press.
Caruana, R. (1997). Multitask learning. Machine Learning, 28,
41?75.
Ciravegna, F. (2001). Adaptive information extraction from text
by rule induction and generalisation. Proceedings of 17th In-
ternational Joint Conference on Artificial Intelligence (IJCAI
2001).
Cohen, W. W., & Carvalho, V. R. (2005). Stacked sequential
learning. International Joint Conference on Artificial Intelli-
gence (pp. 671?676).
Dean, T., & Kanazawa, K. (1989). A model for reasoning about
persistence and causation. Computational Intelligence, 5(3),
142?150.
Florian, R., Hassan, H., Ittycheriah, A., Jing, H., Kambhatla,
N., Luo, X., Nicolov, N., Roukos, S., & Zhang, T. (2004). A
statistical model for multilingual entity detection and track-
ing. In HLT/NAACL 2004.
Freitag, D. (1998). Machine learning for information extraction
in informal domains. Doctoral dissertation, Carnegie Mellon
University.
Frietag, D., & McCallum, A. (1999). Information extraction
with HMMs and shrinkage. AAAI Workshop on Machine
Learning for Information Extraction.
Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. Proc. 18th International Conf. on Ma-
chine Learning.
Miller, S., Fox, H., Ramshaw, L. A., & Weischedel, R. M.
(2000). A novel use of statistical parsing to extract infor-
mation from text. ANLP 2000 (pp. 226?233).
Mohri, M., Pereira, F., & Riley, M. (2002). Weighted finite-
state transducers in speech recognition. Computer Speech
and Language, 16, 69?88.
Pal, C., Sutton, C., & McCallum, A. (2005). Fast inference
and learning with sparse belief propagation (Technical Re-
port IR-433). Center for Intelligent Information Retrieval,
University of Massachusetts.
Peshkin, L., & Pfeffer, A. (2003). Bayesian information extrac-
tion network. Proceedings of the International Joint Confer-
ence on Artificial Intelligence (IJCAI).
Roth, D., & Wen-tau Yih (2001). Relational learning via propo-
sitional algorithms: An information extraction case study. In-
ternational Joint Conference on Artificial Intelligence (pp.
1257?1263).
Sha, F., & Pereira, F. (2003). Shallow parsing with conditional
random fields. Proceedings of HLT-NAACL 2003.
Soderland, S. (1999). Learning information extraction rules for
semi-structured and free text. Machine Learning, 233?272.
Sutton, C., Rohanimanesh, K., & McCallum, A. (2004). Dy-
namic conditional random fields: Factorized probabilistic
models for labeling and segmenting sequence data. Proceed-
ings of the Twenty-First International Conference on Ma-
chine Learning (ICML).
Tjong Kim Sang, E. F., & De Meulder, F. (2003). Introduc-
tion to the CoNLL-2003 shared task: Language-independent
named entity recognition. Proceedings of CoNLL-2003 (pp.
142?147). Edmonton, Canada.
754
Accurate Information Extraction from Research Papers
using Conditional Random Fields
Fuchun Peng
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
fuchun@cs.umass.edu
Andrew McCallum
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
mccallum@cs.umass.edu
Abstract
With the increasing use of research paper
search engines, such as CiteSeer, for both lit-
erature search and hiring decisions, the accu-
racy of such systems is of paramount impor-
tance. This paper employs Conditional Ran-
dom Fields (CRFs) for the task of extracting
various common fields from the headers and
citation of research papers. The basic the-
ory of CRFs is becoming well-understood, but
best-practices for applying them to real-world
data requires additional exploration. This paper
makes an empirical exploration of several fac-
tors, including variations on Gaussian, expo-
nential and hyperbolic-L1 priors for improved
regularization, and several classes of features
and Markov order. On a standard benchmark
data set, we achieve new state-of-the-art perfor-
mance, reducing error in average F1 by 36%,
and word error rate by 78% in comparison with
the previous best SVM results. Accuracy com-
pares even more favorably against HMMs.
1 Introduction
Research paper search engines, such as CiteSeer
(Lawrence et al, 1999) and Cora (McCallum et al,
2000), give researchers tremendous power and conve-
nience in their research. They are also becoming increas-
ingly used for recruiting and hiring decisions. Thus the
information quality of such systems is of significant im-
portance. This quality critically depends on an informa-
tion extraction component that extracts meta-data, such
as title, author, institution, etc, from paper headers and
references, because these meta-data are further used in
many component applications such as field-based search,
author analysis, and citation analysis.
Previous work in information extraction from research
papers has been based on two major machine learn-
ing techniques. The first is hidden Markov models
(HMM) (Seymore et al, 1999; Takasu, 2003). An
HMM learns a generative model over input sequence
and labeled sequence pairs. While enjoying wide his-
torical success, standard HMM models have difficulty
modeling multiple non-independent features of the ob-
servation sequence. The second technique is based
on discriminatively-trained SVM classifiers (Han et al,
2003). These SVM classifiers can handle many non-
independent features. However, for this sequence label-
ing problem, Han et al (2003) work in a two stages pro-
cess: first classifying each line independently to assign it
label, then adjusting these labels based on an additional
classifier that examines larger windows of labels. Solving
the information extraction problem in two steps looses
the tight interaction between state transitions and obser-
vations.
In this paper, we present results on this research paper
meta-data extraction task using a Conditional Random
Field (Lafferty et al, 2001), and explore several practi-
cal issues in applying CRFs to information extraction in
general. The CRF approach draws together the advan-
tages of both finite state HMM and discriminative SVM
techniques by allowing use of arbitrary, dependent fea-
tures and joint inference over entire sequences.
CRFs have been previously applied to other tasks such
as name entity extraction (McCallum and Li, 2003), table
extraction (Pinto et al, 2003) and shallow parsing (Sha
and Pereira, 2003). The basic theory of CRFs is now
well-understood, but the best-practices for applying them
to new, real-world data is still in an early-exploration
phase. Here we explore two key practical issues: (1) reg-
ularization, with an empirical study of Gaussian (Chen
and Rosenfeld, 2000), exponential (Goodman, 2003), and
hyperbolic-L1 (Pinto et al, 2003) priors; (2) exploration
of various families of features, including text, lexicons,
and layout, as well as proposing a method for the bene-
ficial use of zero-count features without incurring large
memory penalties.
We describe a large collection of experimental results
on two traditional benchmark data sets. Dramatic im-
provements are obtained in comparison with previous
SVM and HMM based results.
2 Conditional Random Fields
Conditional random fields (CRFs) are undirected graph-
ical models trained to maximize a conditional probabil-
ity (Lafferty et al, 2001). A common special-case graph
structure is a linear chain, which corresponds to a finite
state machine, and is suitable for sequence labeling. A
linear-chain CRF with parameters ? = {?, ...} defines
a conditional probability for a state (or label1) sequence
y = y1...yT given an input sequence x = x1...xT to be
P?(y|x) =
1
Zx
exp
( T
?
t=1
?
k
?kfk(yt?1, yt,x, t)
)
,
(1)
where Zx is the normalization constant that makes
the probability of all state sequences sum to one,
fk(yt?1, yt,x, t) is a feature function which is often
binary-valued, but can be real-valued, and ?k is a learned
weight associated with feature fk. The feature functions
can measure any aspect of a state transition, yt?1 ? yt,
and the observation sequence, x, centered at the current
time step, t. For example, one feature function might
have value 1 when yt?1 is the state TITLE, yt is the state
AUTHOR, and xt is a word appearing in a lexicon of peo-
ple?s first names. Large positive values for ?k indicate a
preference for such an event, while large negative values
make the event unlikely.
Given such a model as defined in Equ. (1), the most
probable labeling sequence for an input x,
y? = argmax
y
P?(y|x),
can be efficiently calculated by dynamic programming
using the Viterbi algorithm. Calculating the marginal
probability of states or transitions at each position in
the sequence by a dynamic-programming-based infer-
ence procedure very similar to forward-backward for hid-
den Markov models.
The parameters may be estimated by maximum
likelihood?maximizing the conditional probability of
a set of label sequences, each given their correspond-
ing input sequences. The log-likelihood of training set
1We consider here only finite state models in which there is
a one-to-one correspondence between states and labels; this is
not, however, strictly necessary.
?6 ?5 ?4 ?3 ?2 ?1 0 1 2 3
0
2
4
6
8
10
12
lambda
co
u
n
ts
 o
f l
am
da
 (in
 lo
g s
ca
le)
Figure 1: Empirical distribution of ?
{(xi, yi) : i = 1, ...M} is written
L? =
?
i
logP?(yi|xi)
=
?
i
( T
?
t=1
?
k
?kfk(yt?1, yt,x, t) ? logZxi
)
.
(2)
Maximizing (2) corresponds to satisfying the follow-
ing equality, wherein the the empirical count of each fea-
ture matches its expected count according to the model
P?(y|x).
?
i
fk(yt?1, yt, xi, t) =
?
i
P?(y|x)fk(yt?1, yt, xi, t)
CRFs share many of the advantageous properties of
standard maximum entropy models, including their con-
vex likelihood function, which guarantees that the learn-
ing procedure converges to the global maximum. Tra-
ditional maximum entropy learning algorithms, such as
GIS and IIS (Pietra et al, 1995), can be used to train
CRFs, however, it has been found that a quasi-Newton
gradient-climber, BFGS, converges much faster (Malouf,
2002; Sha and Pereira, 2003). We use BFGS for opti-
mization. In our experiments, we shall focus instead on
two other aspects of CRF deployment, namely regulariza-
tion and selection of different model structure and feature
types.
2.1 Regularization in CRFs
To avoid over-fitting, log-likelihood is often penalized by
some prior distribution over the parameters. Figure 1
shows an empirical distribution of parameters, ?, learned
from an unpenalized likelihood, including only features
with non-zero count in the training set. Three prior dis-
tributions that have shape similar to this empirical dis-
tribution are the Gaussian prior, exponential prior, and
hyperbolic-L1 prior, each shown in Figure 2. In this pa-
per we provide an empirical study of these three priors.
?10 ?8 ?6 ?4 ?2 0 2 4 6 8 10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Gaussian varianec=2
Exponential a=0.5
Hyperbolic
Figure 2: Shapes of prior distributions
2.1.1 Gaussian prior
With a Gaussian prior, log-likelihood (2) is penalized
as follows:
L? =
?
i
logP?(yi|xi) ?
?
k
?2k
2?2k
, (3)
where ?2k is a variance.
Maximizing (3) corresponds to satisfying
?
i
fk(yt?1, yt, xi, t)?
?k
?2k
=
?
i
P?(y|x)fk(yt?1, yt, xi, t)
This adjusted constraint (as well as the adjustments im-
posed by the other two priors) is intuitively understand-
able: rather than matching exact empirical feature fre-
quencies, the model is tuned to match discounted feature
frequencies. Chen and Rosenfeld (2000) discuss this in
the context of other discounting procedures common in
language modeling. We call the term subtracted from the
empirical counts (in this case ?k/?2) a discounted value.
The variance can be feature dependent. However for
simplicity, constant variance is often used for all features.
In this paper, however, we experiment with several alter-
nate versions of Gaussian prior in which the variance is
feature dependent.
Although Gaussian (and other) priors are gradually
overcome by increasing amounts of training data, per-
haps not at the right rate. The three methods below all
provide ways to alter this rate by changing the variance
of the Gaussian prior dependent on feature counts.
1. Threshold Cut: In language modeling, e.g, Good-
Turing smoothing, only low frequency words are
smoothed. Here we apply the same idea and only
smooth those features whose frequencies are lower
than a threshold (7 in our experiments, following
standard practice in language modeling).
2. Divide Count: Here we let the discounted value
for a feature depend on its frequency in the training
set, ck =
?
i
?
t fk(yt?1, yt,x, t). The discounted
value used here is ?kck??2 where ? is a constant over
all features. In this way, we increase the smoothing
on the low frequency features more so than the high
frequency features.
3. Bin-Based: We divide features into classes based
on frequency. We bin features by frequency in the
training set, and let the features in the same bin share
the same variance. The discounted value is set to be
?k
dck/Ne??2 where ck is the count of features, N is
the bin size, and dae is the ceiling function. Alterna-
tively, the variance in each bin may be set indepen-
dently by cross-validation.
2.1.2 Exponential prior
Whereas the Gaussian prior penalizes according to the
square of the weights (an L2 penalizer), the intention here
is to create a smoothly differentiable analogue to penal-
izing the absolute-value of the weights (an L1 penalizer).
L1 penalizers often result in more ?sparse solutions,? in
which many features have weight nearly at zero, and thus
provide a kind of soft feature selection that improves gen-
eralization.
Goodman (2003) proposes an exponential prior,
specifically a Laplacian prior, as an alternative to Gaus-
sian prior. Under this prior,
L? =
?
i
logP?(yi|xi)?
?
k
?k|?k| (4)
where ?k is a parameter in exponential distribution.
Maximizing (4) would satisfy
?
i
fk(yt?1, yt, xi, t)??k =
?
i
P?(y|x)fk(yt?1, yt, xi, t)
This corresponds to the absolute smoothing method in
language modeling. We set the ?k = ?; i.e. all features
share the same constant whose value can be determined
using absolute discounting? = n1n1+2n2 , where n1 and n2
are the number of features occurring once and twice (Ney
et al, 1995).
2.1.3 Hyperbolic-L1 prior
Another L1 penalizer is the hyperbolic-L1 prior, de-
scribed in (Pinto et al, 2003). The hyperbolic distribution
has log-linear tails. Consequently the class of hyperbolic
distribution is an important alternative to the class of nor-
mal distributions and has been used for analyzing data
from various scientific areas such as finance, though less
frequently used in natural language processing.
Under a hyperbolic prior,
L? =
X
i
logP?(yi|xi) ?
X
k
log(e
?k + e??k
2 ) (5)
which corresponds to satisfying
X
i
fk(yt?1, yt, xi, t) ?
e|?k| ? e?|?k|
e|?k| + e?|?k| =
X
i
P?(y|x)fi(yt?1, yt, xi, t)
The hyperbolic prior was also tested with CRFs in Mc-
Callum and Li (2003).
2.2 Exploration of Feature Space
Wise choice of features is always vital the performance
of any machine learning solution. Feature induction (Mc-
Callum, 2003) has been shown to provide significant im-
provements in CRFs performance. In some experiments
described below we use feature induction. The focus in
this section is on three other aspects of the feature space.
2.2.1 State transition features
In CRFs, state transitions are also represented as fea-
tures. The feature function fk(yt?1, yt,x, t) in Equ. (1)
is a general function over states and observations. Differ-
ent state transition features can be defined to form dif-
ferent Markov-order structures. We define four differ-
ent state transitions features corresponding to different
Markov order for different classes of features. Higher
order features model dependencies better, but also create
more data sparse problem and require more memory in
training.
1. First-order: Here the inputs are examined in the con-
text of the current state only. The feature functions
are represented as f(yt,x). There are no separate
parameters or preferences for state transitions at all.
2. First-order+transitions: Here we add parameters
corresponding to state transitions. The feature func-
tions used are f(yt,x), f(yt?1, yt).
3. Second-order: Here inputs are examined in the con-
text of the current and previous states. Feature func-
tion are represented as f(yt?1, yt,x).
4. Third-order: Here inputs are examined in the con-
text of the current, two previous states. Feature func-
tion are represented as f(yt?2, yt?1, yt,x).
2.2.2 Supported features and unsupported features
Before the use of prior distributions over parameters
was common in maximum entropy classifiers, standard
practice was to eliminate all features with zero count
in the training data (the so-called unsupported features).
However, unsupported, zero-count features can be ex-
tremely useful for pushing Viterbi inference away from
certain paths by assigning such features negative weight.
The use of a prior allows the incorporation of unsup-
ported features, however, doing so often greatly increases
the number parameters and thus the memory require-
ments.
Below we experiment with models containing and not
containing unsupported features?both with and without
regularization by priors, and we argue that non-supported
features are useful.
We present here incremental support, a method of in-
troducing some useful unsupported features without ex-
ploding the number of parameters with all unsupported
features. The model is trained for several iterations with
supported features only. Then inference determines the
label sequences assigned high probability by the model.
Incorrect transitions assigned high probability by the
model are used to selectively add to the model those un-
supported features that occur on those transitions, which
may help improve performance by being assigned nega-
tive weight in future training. If desired, several iterations
of this procedure may be performed.
2.2.3 Local features, layout features and lexicon
features
One of the advantages of CRFs and maximum entropy
models in general is that they easily afford the use of arbi-
trary features of the input. One can encode local spelling
features, layout features such as positions of line breaks,
as well as external lexicon features, all in one framework.
We study all these features in our research paper extrac-
tion problem, evaluate their individual contributions, and
give some guidelines for selecting good features.
3 Empirical Study
3.1 Hidden Markov Models
Here we also briefly describe a HMM model we used
in our experiments. We relax the independence assump-
tion made in standard HMM and allow Markov depen-
dencies among observations, e.g., P (ot|st, ot?1). We
can vary Markov orders in state transition and observa-
tion transitions. In our experiments, a model with second
order state transitions and first order observation transi-
tions performs the best. The state transition probabilities
and emission probabilities are estimated using maximum
likelihood estimation with absolute smoothing, which
was found to be effective in previous experiments, includ-
ing Seymore et al (1999).
3.2 Datasets
We experiment with two datasets of research paper con-
tent. One consists of the headers of research papers. The
other consists of pre-segmented citations from the refer-
ence sections of research papers. These data sets have
been used as standard benchmarks in several previous
studies (Seymore et al, 1999; McCallum et al, 2000;
Han et al, 2003).
3.2.1 Paper header dataset
The header of a research paper is defined to be all of
the words from the beginning of the paper up to either
the first section of the paper, usually the introduction,
or to the end of the first page, whichever occurs first.
It contains 15 fields to be extracted: title, author, affil-
iation, address, note, email, date, abstract, introduction,
phone, keywords, web, degree, publication number, and
page (Seymore et al, 1999). The header dataset contains
935 headers. Following previous research (Seymore et
al., 1999; McCallum et al, 2000; Han et al, 2003), for
each trial we randomly select 500 for training and the re-
maining 435 for testing. We refer this dataset as H.
3.2.2 Paper reference dataset
The reference dataset was created by the Cora
project (McCallum et al, 2000). It contains 500 refer-
ences, we use 350 for training and the rest 150 for test-
ing. References contain 13 fields: author, title, editor,
booktitle, date, journal, volume, tech, institution, pages,
location, publisher, note. We refer this dataset as R.
3.3 Performance Measures
To give a comprehensive evaluation, we measure per-
formance using several different metrics. In addition to
the previously-used word accuracy measure (which over-
emphasizes accuracy of the abstract field), we use per-
field F1 measure (both for individual fields and averaged
over all fields?called a ?macro average? in the informa-
tion retrieval literature), and whole instance accuracy for
measuring overall performance in a way that is sensitive
to even a single error in any part of header or citation.
3.3.1 Measuring field-specific performance
1. Word Accuracy: We define A as the number of true
positive words, B as the number of false negative
words, C as the number of false positive words, D
as the number of true negative words, and A+ B +
C +D is the total number of words. Word accuracy
is calculated to be A+DA+B+C+D
2. F1-measure: Precision, recall and F1 measure are
defined as follows. Precision = AA+C Recall =
A
A+B
F1 = 2?Precision?RecallPrecision+Recall
3.3.2 Measuring overall performance
1. Overall word accuracy: Overall word accuracy
is the percentage of words whose predicted labels
equal their true labels. Word accuracy favors fields
with large number of words, such as the abstract.
2. Averaged F-measure: Averaged F-measure is com-
puted by averaging the F1-measures over all fields.
Average F-measure favors labels with small num-
ber of words, which complements word accuracy.
Thus, we consider both word accuracy and average
F-measure in evaluation.
3. Whole instance accuracy: An instance here is de-
fined to be a single header or reference. Whole
instance accuracy is the percentage of instances in
which every word is correctly labeled.
3.4 Experimental Results
We first report the overall results by comparing CRFs
with HMMs, and with the previously best benchmark re-
sults obtained by SVMs (Han et al, 2003). We then break
down the results to analyze various factors individually.
Table 1 shows the results on dataset H with the best re-
sults in bold; (intro and page fields are not shown, fol-
lowing past practice (Seymore et al, 1999; Han et al,
2003)). The results we obtained with CRFs use second-
order state transition features, layout features, as well as
supported and unsupported features. Feature induction
is used in experiments on dataset R; (it didn?t improve
accuracy on H). The results we obtained with the HMM
model use a second order model for transitions, and a first
order for observations. The results on SVM is obtained
from (Han et al, 2003) by computing F1 measures from
the precision and recall numbers they report.
HMM CRF SVM
Overall acc. 93.1% 98.3% 92.9%
Instance acc. 4.13% 73.3% -
acc. F1 acc. F1 acc. F1
Title 98.2 82.2 99.7 97.1 98.9 96.5
Author 98.7 81.0 99.8 97.5 99.3 97.2
Affiliation 98.3 85.1 99.7 97.0 98.1 93.8
Address 99.1 84.8 99.7 95.8 99.1 94.7
Note 97.8 81.4 98.8 91.2 95.5 81.6
Email 99.9 92.5 99.9 95.3 99.6 91.7
Date 99.8 80.6 99.9 95.0 99.7 90.2
Abstract 97.1 98.0 99.6 99.7 97.5 93.8
Phone 99.8 53.8 99.9 97.9 99.9 92.4
Keyword 98.7 40.6 99.7 88.8 99.2 88.5
Web 99.9 68.6 99.9 94.1 99.9 92.4
Degree 99.5 68.8 99.8 84.9 99.5 70.1
Pubnum 99.8 64.2 99.9 86.6 99.9 89.2
Average F1 75.6 93.9 89.7
Table 1: Extraction results for paper headers on H
Table 2 shows the results on dataset R. SVM results
are not available for these datasets.
3.5 Analysis
3.5.1 Overall performance comparison
From Table (1, 2), one can see that CRF performs
significantly better than HMMs, which again supports
the previous findings (Lafferty et al, 2001; Pinto et al,
HMM CRF
Overall acc. 85.1% 95.37%
instance acc. 10% 77.33%
acc. F1 acc. F1
Author 96.8 92.7 99.9 99.4
Booktitle 94.4 0.85 97.7 93.7
Date 99.7 96.9 99.8 98.9
Editor 98.8 70.8 99.5 87.7
Institution 98.5 72.3 99.7 94.0
Journal 96.6 67.7 99.1 91.3
Location 99.1 81.8 99.3 87.2
Note 99.2 50.9 99.7 80.8
Pages 98.1 72.9 99.9 98.6
Publisher 99.4 79.2 99.4 76.1
Tech 98.8 74.9 99.4 86.7
Title 92.2 87.2 98.9 98.3
Volume 98.6 75.8 99.9 97.8
Average F1 77.6% 91.5%
Table 2: Extraction results for paper references on R
2003). CRFs also perform significantly better than SVM-
based approach, yielding new state of the art performance
on this task. CRFs increase the performance on nearly all
the fields. The overall word accuracy is improved from
92.9% to 98.3%, which corresponds to a 78% error rate
reduction. However, as we can see word accuracy can be
misleading since HMM model even has a higher word ac-
curacy than SVM, although it performs much worse than
SVM in most individual fields except abstract. Interest-
ingly, HMM performs much better on abstract field (98%
versus 93.8% F-measure) which pushes the overall accu-
racy up. A better comparison can be made by compar-
ing the field-based F-measures. Here, in comparison to
the SVM, CRFs improve the F1 measure from 89.7% to
93.9%, an error reduction of 36%.
3.5.2 Effects of regularization
The results of different regularization methods are
summarized in Table (3). Setting Gaussian variance of
features depending on feature count performs better, from
90.5% to 91.2%, an error reduction of 7%, when only
using supported features, and an error reduction of 9%
when using supported and unsupported features. Re-
sults are averaged over 5 random runs, with an aver-
age variance of 0.2%. In our experiments we found the
Gaussian prior to consistently perform better than the
others. Surprisingly, exponential prior hurts the perfor-
mance significantly. It over penalizes the likelihood (sig-
nificantly increasing cost?defined as negative penalized
log-likelihood). We hypothesized that the problem could
be that the choice of constant ? is inappropriate. So we
tried varying ? instead of computing it using absolute
discounting, but found the alternatives to perform worse.
These results suggest that Gaussian prior is a safer prior
support feat. all features
Method F1 F1
Gaussian infinity 90.5 93.3
Gaussian variance = 0.1 81.7 91.8
Gaussian variance = 0.5 87.2 93.0
Gaussian variance = 5 90.1 93.7
Gaussian variance = 10 89.9 93.5
Gaussian cut 7 90.1 93.4
Gaussian divide count 90.9 92.8
Gaussian bin 5 90.9 93.6
Gaussian bin 10 90.2 92.9
Gaussian bin 15 91.2 93.9
Gaussian bin 20 90.4 93.2
Hyperbolic 89.4 92.8
Exponential 80.5 85.6
Table 3: Regularization comparisons: Gaussian infinity is
non-regularized, Gaussian variance = X sets variance to
be X. Gaussian cut 7 refers to the Threshold Cut method,
Gaussian divide count refers to the Divide Count method,
Gaussian bin N refers to the Bin-Based method with bin
size equals N, as described in 2.1.1
to use in practice.
3.5.3 Effects of exploring feature space
State transition features and unsupported features.
We summarize the comparison of different state tran-
sition models using or not using unsupported features in
Table 4. The first column describes the four different state
transition models, the second column contains the overall
word accuracy of these models using only support fea-
tures, and the third column contains the result of using
all features, including unsupported features. Comparing
the rows, one can see that the second-order model per-
forms the best, but not dramatically better than the first-
order+transitions and the third order model. However, the
first-order model performs significantly worse. The dif-
ference does not come from sharing the weights, but from
ignoring the f(yt?1, yt). The first order transition feature
is vital here. We would expect the third order model to
perform better if enough training data were available.
Comparing the second and the third columns, we can
see that using all features including unsupported features,
consistently performs better than ignoring them. Our
preliminary experiments with incremental support have
shown performance in between that of supported-only
and all features, and are still ongoing.
Effects of layout features
To analyze the contribution of different kinds of fea-
tures, we divide the features into three categories: local
features, layout features, and external lexicon resources.
The features we used are summarized in Table 5.
support all
first-order 89.0 90.4
first-order+trans 95.6 -
second-order 96.0 96.5
third-order 95.3 96.1
Table 4: Effects of using unsupported features and state
transitions on H
Feature name Description
Local features
INITCAP Starts with a capitalized letter
ALLCAPS All characters are capitalized
CONTAINSDIGITS Contains at least one digit
ALLDIGITS All characters are digits
PHONEORZIP Phone number or zip code
CONTAINSDOTS Contains at least one dot
CONTAINSDASH Contains at least one -
ACRO Acronym
LONELYINITIAL Initials such as A.
SINGLECHAR One character only
CAPLETTER One capitalized character
PUNC Punctuation
URL Regular expression for URL
EMAIL Regular expression for e-address
WORD Word itself
Layout features
LINE START At the beginning of a line
LINE IN In middle of a line
LINE END At the end of a line
External lexicon features
BIBTEX AUTHOR Match word in author lexicon
BIBTEX DATE Words like Jan. Feb.
NOTES Words like appeared, submitted
AFFILIATION Words like institution, Labs, etc
Table 5: List of features used
The results of using different features are shown in Ta-
ble 6. The layout feature dramatically increases the per-
formance, raising the F1 measure from 88.8% to 93.9%,
whole sentence accuracy from 40.1% to 72.4%. Adding
lexicon features alone improves the performance. How-
ever, when combing lexicon features and layout fea-
tures, the performance is worse than using layout features
alone.
The lexicons were gathered from a large collection of
BibTeX files, and upon examination had difficult to re-
move noise, for example words in the author lexicon that
were also affiliations. In previous work, we have gained
significant benefits by dividing each lexicon into sections
based on point-wise information gain with respect to the
lexicon?s class.
3.5.4 Error analysis
Table 7 is the classification confusion matrix of header
extraction (field page is not shown to save space). Most
Word Acc. F1 Inst. Acc.
local feature 96.5% 88.8% 40.1%
+ lexicon 96.9% 89.9% 53.1%
+ layout feature 98.2% 93.4% 72.4%
+ layout + lexicon 98.0% 93.0% 71.7%
Table 6: Results of using different features on H
errors happen at the boundaries between two fields. Es-
pecially the transition from author to affiliation, from ab-
stract to keyword. The note field is the one most con-
fused with others, and upon inspection is actually labeled
inconsistently in the training data. Other errors could
be fixed with additional feature engineering?for exam-
ple, including additional specialized regular expressions
should make email accuracy nearly perfect. Increasing
the amount of training data would also be expected to
help significantly, as indicated by consistent nearly per-
fect accuracy on the training set.
4 Conclusions and Future Work
This paper investigates the issues of regularization, fea-
ture spaces, and efficient use of unsupported features in
CRFs, with an application to information extraction from
research papers.
For regularization we find that the Gaussian prior with
variance depending on feature frequencies performs bet-
ter than several other alternatives in the literature. Feature
engineering is a key component of any machine learn-
ing solution?especially in conditionally-trained mod-
els with such freedom to choose arbitrary features?and
plays an even more important role than regularization.
We obtain new state-of-the-art performance in extract-
ing standard fields from research papers, with a signifi-
cant error reduction by several metrics. We also suggest
better evaluation metrics to facilitate future research in
this task?especially field-F1, rather than word accuracy.
We have provided an empirical exploration of a few
previously-published priors for conditionally-trained log-
linear models. Fundamental advances in regularization
for CRFs remains a significant open research area.
5 Acknowledgments
This work was supported in part by the Cen-
ter for Intelligent Information Retrieval, in part by
SPAWARSYSCEN-SD grant number N66001-02-1-
8903, in part by the National Science Foundation Co-
operative Agreement number ATM-9732665 through a
subcontract from the University Corporation for Atmo-
spheric Research (UCAR) and in part by The Cen-
tral Intelligence Agency, the National Security Agency
and National Science Foundation under NSF grant #IIS-
0326249. Any opinions, findings and conclusions or rec-
title auth. pubnum date abs. aff. addr. email deg. note ph. intro k.w. web
title 3446 0 6 0 22 0 0 0 9 25 0 0 12 0
author 0 2653 0 0 7 13 5 0 14 41 0 0 12 0
pubnum 0 14 278 2 0 2 7 0 0 39 0 0 0 0
date 0 0 3 336 0 1 3 0 0 18 0 0 0 0
abstract 0 0 0 0 53262 0 0 1 0 0 0 0 0 0
affil. 19 13 0 0 10 3852 27 0 28 34 0 0 0 1
address 0 11 3 0 0 35 2170 1 0 21 0 0 0 0
email 0 0 1 0 12 2 3 461 0 2 2 0 15 0
degree 2 2 0 2 0 2 0 5 465 95 0 0 2 0
note 52 2 9 6 219 52 59 0 5 4520 4 3 21 3
phone 0 0 0 0 0 0 0 1 0 2 215 0 0 0
intro 0 0 0 0 0 0 0 0 0 32 0 625 0 0
keyword 57 0 0 0 18 3 15 0 0 91 0 0 975 0
web 0 0 0 0 2 0 0 0 0 31 0 0 0 294
Table 7: Confusion matrix on H
ommendations expressed in this material are the author(s)
and do not necessarily reflect those of the sponsor.
References
S. Chen and R. Rosenfeld. 2000. A Survey of Smoothing
Techniques for ME Models. IEEE Trans. Speech and
Audio Processing, 8(1), pp. 37?50. January 2000.
J. Goodman. 2003. Exponential Priors for Maximum
Entropy Models. MSR Technical report, 2003.
H. Han, C. Giles, E. Manavoglu, H. Zha, Z. Zhang, and E.
Fox. 2003. Automatic Document Meta-data Extrac-
tion using Support Vector Machines. In Proceedings
of Joint Conference on Digital Libraries 2003.
J. Lafferty, A. McCallum and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceed-
ings of International Conference on Machine Learning
2001.
S. Lawrence, C. L. Giles, and K. Bollacker. 1999. Digital
Libraries and Autonomous Citation Indexing. IEEE
Computer, 32(6): 67-71.
R. Malouf. 2002. A Comparison of Algorithms for Max-
imum Entropy Parameter Estimation. In Proceedings
of the Sixth Conference on Natural Language Learning
(CoNLL)
A. McCallum. 2003. Efficiently Inducing Features
of Conditional Random Fields. In Proceedings of
Conference on Uncertainty in Articifical Intelligence
(UAI).
A. McCallum, K. Nigam, J. Rennie, K. Seymore. 2000.
Automating the Construction of Internet Portals with
Machine Learning. Information Retrieval Journal,
volume 3, pages 127-163. Kluwer. 2000.
A. McCallum and W. Li. 2003. Early Results for Named
Entity Recognition with Conditional Random Fields,
Feature Induction and Web-Enhanced Lexicons. In
Proceedings of Seventh Conference on Natural Lan-
guage Learning (CoNLL).
H. Ney, U. Essen, and R. Kneser 1995. On the Estima-
tion of Small Probabilities by Leaving-One-Out. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 17(12):1202-1212, 1995.
S. Pietra, V. Pietra, J. Lafferty 1995. Inducing Fea-
tures Of Random Fields. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, Vol. 19, No.
4.
D. Pinto, A. McCallum, X. Wei and W. Croft. 2003. Ta-
ble Extraction Using Conditional Random Fields. In
Proceedins of the 26th Annual International ACM SI-
GIR Conference on Research and Development in In-
formation Retrieval (SIGIR?03)
K. Seymore, A. McCallum, R. Rosenfeld. 1999. Learn-
ing Hidden Markov Model Structure for Information
Extraction. In Proceedings of AAAI?99 Workshop on
Machine Learning for Information Extraction.
F. Sha and F. Pereira. 2003. Shallow Parsing with Con-
ditional Random Fields. In Proceedings of Human
Language Technology Conference and North Ameri-
can Chapter of the Association for Computational Lin-
guistics (HLT-NAACL?03)
A. Takasu. 2003. Bibliographic Attribute Extrac-
tion from Erroneous References Based on a Statistical
Model. In Proceedings of Joint Conference on Digital
Libraries 2003.
Confidence Estimation for Information Extraction
Aron Culotta
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
culotta@cs.umass.edu
Andrew McCallum
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
mccallum@cs.umass.edu
Abstract
Information extraction techniques automati-
cally create structured databases from un-
structured data sources, such as the Web or
newswire documents. Despite the successes of
these systems, accuracy will always be imper-
fect. For many reasons, it is highly desirable to
accurately estimate the confidence the system
has in the correctness of each extracted field.
The information extraction system we evalu-
ate is based on a linear-chain conditional ran-
dom field (CRF), a probabilistic model which
has performed well on information extraction
tasks because of its ability to capture arbitrary,
overlapping features of the input in a Markov
model. We implement several techniques to es-
timate the confidence of both extracted fields
and entire multi-field records, obtaining an av-
erage precision of 98% for retrieving correct
fields and 87% for multi-field records.
1 Introduction
Information extraction usually consists of tagging a se-
quence of words (e.g. a Web document) with semantic
labels (e.g. PERSONNAME, PHONENUMBER) and de-
positing these extracted fields into a database. Because
automated information extraction will never be perfectly
accurate, it is helpful to have an effective measure of
the confidence that the proposed database entries are cor-
rect. There are at least three important applications of
accurate confidence estimation. First, accuracy-coverage
trade-offs are a common way to improve data integrity in
databases. Efficiently making these trade-offs requires an
accurate prediction of correctness.
Second, confidence estimates are essential for inter-
active information extraction, in which users may cor-
rect incorrectly extracted fields. These corrections are
then automatically propagated in order to correct other
mistakes in the same record. Directing the user to
the least confident field allows the system to improve
its performance with a minimal amount of user effort.
Kristjannson et al (2004) show that using accurate con-
fidence estimation reduces error rates by 46%.
Third, confidence estimates can improve performance
of data mining algorithms that depend upon databases
created by information extraction systems (McCallum
and Jensen, 2003). Confidence estimates provide data
mining applications with a richer set of ?bottom-up? hy-
potheses, resulting in more accurate inferences. An ex-
ample of this occurs in the task of citation co-reference
resolution. An information extraction system labels each
field of a paper citation (e.g. AUTHOR, TITLE), and then
co-reference resolution merges disparate references to the
same paper. Attaching a confidence value to each field
allows the system to examine alternate labelings for less
confident fields to improve performance.
Sound probabilistic extraction models are most con-
ducive to accurate confidence estimation because of their
intelligent handling of uncertainty information. In this
work we use conditional random fields (Lafferty et al,
2001), a type of undirected graphical model, to automat-
ically label fields of contact records. Here, a record is an
entire block of a person?s contact information, and a field
is one element of that record (e.g. COMPANYNAME). We
implement several techniques to estimate both field con-
fidence and record confidence, obtaining an average pre-
cision of 98% for fields and 87% for records.
2 Conditional Random Fields
Conditional random fields (Lafferty et al, 2001) are undi-
rected graphical models to calculate the conditional prob-
ability of values on designated output nodes given val-
ues on designated input nodes. In the special case in
which the designated output nodes are linked by edges in
a linear chain, CRFs make a first-order Markov indepen-
dence assumption among output nodes, and thus corre-
spond to finite state machines (FSMs). In this case CRFs
can be roughly understood as conditionally-trained hid-
den Markov models, with additional flexibility to effec-
tively take advantage of complex overlapping features.
Let o = ?o1, o2, ...oT ? be some observed input data se-
quence, such as a sequence of words in a document (the
values on T input nodes of the graphical model). Let S be
a set of FSM states, each of which is associated with a la-
bel (such as COMPANYNAME). Let s = ?s1, s2, ...sT ? be
some sequence of states (the values on T output nodes).
CRFs define the conditional probability of a state se-
quence given an input sequence as
p?(s|o) =
1
Zo
exp
( T
?
t=1
?
k
?kfk(st?1, st,o, t)
)
,
(1)
where Zo is a normalization factor over all state se-
quences, fk(st?1, st,o, t) is an arbitrary feature func-
tion over its arguments, and ?k is a learned weight for
each feature function. Zo is efficiently calculated using
dynamic programming. Inference (very much like the
Viterbi algorithm in this case) is also a matter of dynamic
programming. Maximum aposteriori training of these
models is efficiently performed by hill-climbing methods
such as conjugate gradient, or its improved second-order
cousin, limited-memory BFGS.
3 Field Confidence Estimation
The Viterbi algorithm finds the most likely state sequence
matching the observed word sequence. The word that
Viterbi matches with a particular FSM state is extracted
as belonging to the corresponding database field. We can
obtain a numeric score for an entire sequence, and then
turn this into a probability for the entire sequence by nor-
malizing. However, to estimate the confidence of an indi-
vidual field, we desire the probability of a subsequence,
marginalizing out the state selection for all other parts
of the sequence. A specialization of Forward-Backward,
termed Constrained Forward-Backward (CFB), returns
exactly this probability.
Because CRFs are conditional models, Viterbi finds
the most likely state sequence given an observation se-
quence, defined as s? = argmaxs p?(s|o). To avoid an
exponential-time search over all possible settings of s,
Viterbi stores the probability of the most likely path at
time t that accounts for the first t observations and ends
in state si. Following traditional notation, we define this
probability to be ?t(si), where ?0(si) is the probability of
starting in each state si, and the recursive formula is:
?t+1(si) = max
s?
[
?t(s?) exp
(
?
k
?kfk(s?, si,o, t)
)]
(2)
terminating in s? = argmax
s1?si?sN
[?T (si)].
The Forward-Backward algorithm can be viewed as a
generalization of the Viterbi algorithm: instead of choos-
ing the optimal state sequence, Forward-Backward eval-
uates all possible state sequences given the observation
sequence. The ?forward values? ?t+1(si) are recursively
defined similarly as in Eq. 2, except the max is replaced
by a summation. Thus we have
?t+1(si) =
?
s?
[
?t(s?) exp
(
?
k
?kfk(s?, si,o, t)
)]
.
(3)
terminating in Zo =
?
i ?T (si) from Eq. 1.
To estimate the probability that a field is extracted
correctly, we constrain the Forward-Backward algorithm
such that each path conforms to some subpath of con-
straints C = ?sq . . . sr? from time step q to r. Here,
sq ? C can be either a positive constraint (the sequence
must pass through sq) or a negative constraint (the se-
quence must not pass through sq).
In the context of information extraction, C corresponds
to an extracted field. The positive constraints specify the
observation tokens labeled inside the field, and the neg-
ative constraints specify the field boundary. For exam-
ple, if we use states names B-TITLE and I-JOBTITLE to
label tokens that begin and continue a JOBTITLE field,
and the system labels observation sequence ?o2, . . . , o5?
as a JOBTITLE field, then C = ?s2 = B-JOBTITLE,
s3 = . . . = s5 = I-JOBTITLE, s6 6= I-JOBTITLE?.
The calculations of the forward values can be made to
conform to C by the recursion ??q(si) =
{
P
s?
h
??q?1(s?) exp
?
P
k ?kfk(s?, si, o, t)
?i
if si ' sq
0 otherwise
for all sq ? C, where the operator si ' sq means si
conforms to constraint sq . For time steps not constrained
by C, Eq. 3 is used instead.
If ??t+1(si) is the constrained forward value, then
Z ?o =
?
i ??T (si) is the value of the constrained lat-
tice, the set of all paths that conform to C. Our confi-
dence estimate is obtained by normalizing Z ?o using Zo,
i.e. Z ?o ? Zo.
We also implement an alternative method that uses the
state probability distributions for each state in the ex-
tracted field. Let ?t(si) = p(si|o1, . . . , oT ) be the prob-
ability of being in state i at time t given the observation
sequence . We define the confidence measure GAMMA
to be
?v
i=u ?i(si), where u and v are the start and end
indices of the extracted field.
4 Record Confidence Estimation
We can similarly use CFB to estimate the probability that
an entire record is labeled correctly. The procedure is
the same as in the previous section, except that C now
specifies the labels for all fields in the record.
We also implement three alternative record confidence
estimates. FIELDPRODUCT calculates the confidence of
each field in the record using CFB, then multiplies these
values together to obtain the record confidence. FIELD-
MIN instead uses the minimum field confidence as the
record confidence. VITERBIRATIO uses the ratio of the
probabilities of the top two Viterbi paths, capturing how
much more likely s? is than its closest alternative.
5 Reranking with Maximum Entropy
We also trained two conditional maximum entropy clas-
sifiers to classify fields and records as being labeled cor-
rectly or incorrectly. The resulting posterior probabil-
ity of the ?correct? label is used as the confidence mea-
sure. The approach is inspired by results from (Collins,
2000), which show discriminative classifiers can improve
the ranking of parses produced by a generative parser.
After initial experimentation, the most informative in-
puts for the field confidence classifier were field length,
the predicted label of the field, whether or not this field
has been extracted elsewhere in this record, and the CFB
confidence estimate for this field. For the record confi-
dence classifier, we incorporated the following features:
record length, whether or not two fields were tagged with
the same label, and the CFB confidence estimate.
6 Experiments
2187 contact records (27,560 words) were collected from
Web pages and email and 25 classes of data fields were
hand-labeled.1 The features for the CRF consist of the
token text, capitalization features, 24 regular expressions
over the token text (e.g. CONTAINSHYPHEN), and off-
sets of these features within a window of size 5. We also
use 19 lexicons, including ?US Last Names,? ?US First
Names,? and ?State Names.? Feature induction is not
used in these experiments. The CRF is trained on 60% of
the data, and the remaining 40% is split evenly into de-
velopment and testing sets. The development set is used
to train the maximum entropy classifiers, and the testing
set is used to measure the accuracy of the confidence es-
timates. The CRF achieves an overall token accuracy of
87.32 on the testing data, with a field-level performance
of F1 = 84.11, precision = 85.43, and recall = 82.83.
To evaluate confidence estimation, we use three meth-
ods. The first is Pearson?s r, a correlation coefficient
ranging from -1 to 1 that measures the correlation be-
tween a confidence score and whether or not the field
(or record) is correctly labeled. The second is average
precision, used in the Information Retrieval community
1The 25 fields are: FirstName, MiddleName, LastName,
NickName, Suffix, Title, JobTitle, CompanyName, Depart-
ment, AddressLine, City1, City2, State, Country, PostalCode,
HomePhone, Fax, CompanyPhone, DirectCompanyPhone, Mo-
bile, Pager, VoiceMail, URL, Email, InstantMessage
Pearson?s r Avg. Prec
CFB .573 .976
MaxEnt .571 .976
Gamma .418 .912
Random .012 .858
WorstCase ? .672
Table 1: Evaluation of confidence estimates for field confi-
dence. CFB and MAXENT outperform competing methods.
Pearson?s r Avg. Prec
CFB .626 .863
MaxEnt .630 .867
FieldProduct .608 .858
FieldMin .588 .843
ViterbiRatio .313 .842
Random .043 .526
WorstCase ? .304
Table 2: Evaluation of confidence estimates for record confi-
dence. CFB, MAXENT again perform best.
to evaluate ranked lists. It calculates the precision at
each point in the ranked list where a relevant document
is found and then averages these values. Instead of rank-
ing documents by their relevance score, here we rank
fields (and records) by their confidence score, where a
correctly labeled field is analogous to a relevant docu-
ment. WORSTCASE is the average precision obtained
by ranking all incorrect instances above all correct in-
stances. Tables 1 and 2 show that CFB and MAXENT are
statistically similar, and that both outperform competing
methods. Note that WORSTCASE achieves a high aver-
age precision simply because so many fields are correctly
labeled. In all experiments, RANDOM assigns confidence
values chosen uniformly at random between 0 and 1.
The third measure is an accuracy-coverage graph. Bet-
ter confidence estimates push the curve to the upper-right.
Figure 1 shows that CFB and MAXENT dramatically out-
perform GAMMA. Although omitted for space, similar
results are also achieved on a noun-phrase chunking task
(CFB r = .516, GAMMA r = .432) and a named-entity
extraction task (CFB r = .508, GAMMA r = .480).
7 Related Work
While there has been previous work using probabilistic
estimates for token confidence, and heuristic estimates
for field confidence, to the best of our knowledge this pa-
per is the first to use a sound, probabilistic estimate for
confidence of multi-word fields and records in informa-
tion extraction.
Much of the work in confidence estimation
for IE has been in the active learning literature.
Scheffer et al (2001) derive confidence estimates using
hidden Markov models in an information extraction
system. However, they do not estimate the confidence
of entire fields, only singleton tokens. They estimate
 0.84
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 0.98
 1
 0  0.2  0.4  0.6  0.8  1
a
cc
u
ra
cy
coverage
"Optimal"
"CFB"
"MaxEnt"
"Gamma"
"Random"
Figure 1: The precision-recall curve for fields shows that CFB
and MAXENT outperform GAMMA.
the confidence of a token by the difference between
the probabilities of its first and second most likely
labels, whereas CFB considers the full distribution of
all suboptimal paths. Scheffer et al (2001) also explore
an idea similar to CFB to perform Baum-Welch training
with partially labeled data, where the provided labels
are constraints. However, these constraints are again for
singleton tokens only.
Rule-based extraction methods (Thompson et al,
1999) estimate confidence based on a rule?s coverage in
the training data. Other areas where confidence estima-
tion is used include document classification (Bennett et
al., 2002), where classifiers are built using meta-features
of the document; speech recognition (Gunawardana et al,
1998), where the confidence of a recognized word is esti-
mated by considering a list of commonly confused words;
and machine translation (Gandrabur and Foster, 2003),
where neural networks are used to learn the probability of
a correct word translation using text features and knowl-
edge of alternate translations.
8 Conclusion
We have shown that CFB is a mathematically and empir-
ically sound confidence estimator for finite state informa-
tion extraction systems, providing strong correlation with
correctness and obtaining an average precision of 97.6%
for estimating field correctness. Unlike methods margin
maximization methods such as SVMs and M3Ns (Taskar
et al, 2003), CRFs are trained to maximize conditional
probability and are thus more naturally appropriate for
confidence estimation. Interestingly, reranking by MAX-
ENT does not seem to improve performance, despite the
benefit Collins (2000) has shown discriminative rerank-
ing to provide generative parsers. We hypothesize this is
because CRFs are already discriminative (not joint, gen-
erative) models; furthermore, this may suggest that future
discriminative parsing methods will also have the benefits
of discriminative reranking built-in directly.
Acknowledgments
We thank the reviewers for helpful suggestions and refer-
ences. This work was supported in part by the Center for
Intelligent Information Retrieval, by the Advanced Research
and Development Activity under contract number MDA904-
01-C-0984, by The Central Intelligence Agency, the Na-
tional Security Agency and National Science Foundation un-
der NSF grant #IIS-0326249, and by the Defense Advanced
Research Projects Agency, through the Department of the Inte-
rior, NBC, Acquisition Services Division, under contract num-
ber NBCHD030010.
References
Paul N. Bennett, Susan T. Dumais, and Eric Horvitz. 2002.
Probabilistic combination of text classifiers using reliability
indicators: models and results. In Proceedings of the 25th
annual international ACM SIGIR conference on Research
and development in information retrieval, pages 207?214.
ACM Press.
Michael Collins. 2000. Discriminative reranking for natu-
ral language parsing. In Proc. 17th International Conf. on
Machine Learning, pages 175?182. Morgan Kaufmann, San
Francisco, CA.
Simona Gandrabur and George Foster. 2003. Confidence esti-
mation for text prediction. In Proceedings of the Conference
on Natural Language Learning (CoNLL 2003), Edmonton,
Canada.
A. Gunawardana, H. Hon, and L. Jiang. 1998. Word-based
acoustic confidence measures for large-vocabulary speech
recognition. In Proc. ICSLP-98, pages 791?794, Sydney,
Australia.
Trausti Kristjannson, Aron Culotta, Paul Viola, and Andrew
McCallum. 2004. Interactive information extraction with
conditional random fields. To appear in Nineteenth National
Conference on Artificial Intelligence (AAAI 2004).
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. 18th Interna-
tional Conf. on Machine Learning, pages 282?289. Morgan
Kaufmann, San Francisco, CA.
Andrew McCallum and David Jensen. 2003. A note on the
unification of information extraction and data mining using
conditional-probability, relational models. In IJCAI03 Work-
shop on Learning Statistical Models from Relational Data.
Tobias Scheffer, Christian Decomain, and Stefan Wrobel. 2001.
Active hidden markov models for information extraction.
In Advances in Intelligent Data Analysis, 4th International
Conference, IDA 2001.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003. Max-
margin markov networks. In Proceedings of Neural Infor-
mation Processing Systems Conference.
Cynthia A. Thompson, Mary Elaine Califf, and Raymond J.
Mooney. 1999. Active learning for natural language pars-
ing and information extraction. In Proc. 16th International
Conf. on Machine Learning, pages 406?414. Morgan Kauf-
mann, San Francisco, CA.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 89?95,
New York, June 2006. c?2006 Association for Computational Linguistics
Reducing Weight Undertraining
in Structured Discriminative Learning
Charles Sutton, Michael Sindelar, and Andrew McCallum
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA 01003 USA
{casutton,mccallum}@cs.umass.edu, msindela@student.umass.edu
Abstract
Discriminative probabilistic models are very
popular in NLP because of the latitude they
afford in designing features. But training
involves complex trade-offs among weights,
which can be dangerous: a few highly-
indicative features can swamp the contribution
of many individually weaker features, causing
their weights to be undertrained. Such a model
is less robust, for the highly-indicative features
may be noisy or missing in the test data. To
ameliorate this weight undertraining, we intro-
duce several new feature bagging methods, in
which separate models are trained on subsets
of the original features, and combined using a
mixture model or a product of experts. These
methods include the logarithmic opinion pools
used by Smith et al (2005). We evaluate fea-
ture bagging on linear-chain conditional ran-
dom fields for two natural-language tasks. On
both tasks, the feature-bagged CRF performs
better than simply training a single CRF on all
the features.
1 Introduction
Discriminative methods for training probabilistic models
have enjoyed wide popularity in natural language pro-
cessing, such as in part-of-speech tagging (Toutanova et
al., 2003), chunking (Sha and Pereira, 2003), named-
entity recognition (Florian et al, 2003; Chieu and Ng,
2003), and most recently parsing (Taskar et al, 2004).
A discriminative probabilistic model is trained to maxi-
mize the conditional probability p(y|x) of output labels
y given input variables x, as opposed to modeling the
joint probability p(y, x), as in generative models such as
the Naive Bayes classifier and hidden Markov models.
The popularity of discriminative models stems from the
great flexibility they allow in defining features: because
the distribution over input features p(x) is not modeled,
it can contain rich, highly overlapping features without
making the model intractable for training and inference.
In NLP, for example, useful features include word bi-
grams and trigrams, prefixes and suffixes, membership in
domain-specific lexicons, and information from semantic
databases such as WordNet. It is not uncommon to have
hundreds of thousands or even millions of features.
But not all features, even ones that are carefully engi-
neered, improve performance. Adding more features to a
model can hurt its accuracy on unseen testing data. One
well-known reason for this is overfitting: a model with
more features has more capacity to fit chance regulari-
ties in the training data. In this paper, however, we focus
on another, more subtle effect: adding new features can
cause existing ones to be underfit. Training of discrimi-
native models, such as regularized logistic regression, in-
volves complex trade-offs among weights. A few highly-
indicative features can swamp the contribution of many
individually weaker features, even if the weaker features,
taken together, are just as indicative of the output. Such
a model is less robust, for the few strong features may be
noisy or missing in the test data.
This effect was memorably observed by Dean Pomer-
leau (1995) when training neural networks to drive vehi-
cles autonomously. Pomerleau reports one example when
the system was learning to drive on a dirt road:
The network had no problem learning and then
driving autonomously in one direction, but
when driving the other way, the network was
erratic, swerving from one side of the road to
the other. . . . It turned out that the network
was basing most of its predictions on an easily-
identifiable ditch, which was always on the
right in the training set, but was on the left
when the vehicle turned around. (Pomerleau,
1995)
The network had features to detect the sides of the road,
and these features were active at training and test time,
although weakly, because the dirt road was difficult to
89
detect. But the ditch was so highly indicative that the
network did not learn the dependence between the road
edge and the desired steering direction.
A natural way of avoiding undertraining is to train sep-
arate models for groups of competing features?in the
driving example, one model with the ditch features, and
one with the side-of-the-road features?and then average
them into a single model. This is same idea behind log-
arithmic opinion pools, used by Smith, Cohn, and Os-
borne (2005) to reduce overfitting in CRFs. In this pa-
per, we tailor our ensemble to reduce undertraining rather
than overfitting, and we introduce several new combina-
tion methods, based on whether the mixture is taken ad-
ditively or geometrically, and on a per-sequence or per-
transition basis. We call this general class of methods
feature bagging, by analogy to the well-known bagging
algorithm for ensemble learning.
We test these methods on conditional random fields
(CRFs) (Lafferty et al, 2001; Sutton and McCallum,
2006), which are discriminatively-trained undirected
models. On two natural-language tasks, we show that
feature bagging performs significantly better than train-
ing a single CRF with all available features.
2 Conditional Random Fields
Conditional random fields (CRFs) (Lafferty et al, 2001;
Sutton and McCallum, 2006) are undirected graphical
models of a conditional distribution. Let G be an undi-
rected graphical model over random vectors y and x.
As a typical special case, y = {yt} and x = {xt} for
t = 1, . . . , T , so that y is a labeling of an observed se-
quence x. For a given collection C = {{yc, xc}} of
cliques in G, a CRF models the conditional probability
of an assignment to labels y given the observed variables
x as:
p?(y|x) = 1Z(x)
?
c?C
?(yc, xc), (1)
where ? is a potential function and the partition function
Z(x) = ?y
?
c?C ?(yc, xc) is a normalization factorover all possible label assignments.
We assume the potentials factorize according to a set
of features {fk}, which are given and fixed, so that
?(yc, xc) = exp
(?
k
?kfk(yc, xc)
)
(2)
The model parameters are a set of real weights? = {?k},
one weight for each feature.
Many applications have used the linear-chain CRF, in
which a first-order Markov assumption is made on the
hidden variables. In this case, the cliques of the condi-
tional model are the nodes and edges, so that there are
feature functions fk(yt?1, yt, x, t) for each label transi-
tion. (Here we write the feature functions as potentially
? ? ? ?
?
?
0 2 4 6 8 10
0.50
0.55
0.60
0.65
0.70
0.75
Alpha
Acc
urac
y
Strong feature presentStrong feature removed
Figure 1: Effect of a single strong feature drowning out
weaker features in logistic regression on synthetic data.
The x-axis indicates the strength of the strong feature. In
the top line, the strong feature is present at training and
test time. In the bottom line, the strong feature is missing
from the training data at test time.
depending on the entire input sequence.) Feature func-
tions can be arbitrary. For example, a feature function
fk(yt?1, yt, x, t) could be a binary test that has value 1 if
and only if yt?1 has the label ?adjective?, yt has the label
?proper noun?, and xt begins with a capital letter.
Linear-chain CRFs correspond to finite state machines,
and can be roughly understood as conditionally-trained
hidden Markov models (HMMs). This class of CRFs
is also a globally-normalized extension to Maximum En-
tropy Markov Models (McCallum et al, 2000) that avoids
the label bias problem (Lafferty et al, 2001).
Note that the number of state sequences is exponential
in the input sequence length T . In linear-chain CRFs, the
partition function Z(x), the node marginals p(yi|x), and
the Viterbi labeling can be calculated efficiently by vari-
ants of the dynamic programming algorithms for HMMs.
3 Weight Undertraining
In the section, we give a simple demonstration of weight
undertraining. In a discriminative classifier, such as
a neural network or logistic regression, a few strong
features can drown out the effect of many individually
weaker features, even if the weak features are just as
indicative put together. To demonstrate this effect, we
present an illustrative experiment using logistic regres-
sion, because of its strong relation to CRFs. (Linear-
90
chain conditional random fields are the generalization of
logistic regression to sequence data.)
Consider random variables x1 . . . xn, each distributed
as independent standard normal variables. The output
y is a binary variable whose probability depends on all
the xi; specifically, we define its distribution as y ?
Bernoulli(logit(?i xi)). The correct decision boundaryin this synthetic problem is the hyperplane tangent to the
weight vector (1, 1, . . . , 1). Thus, if n is large, each xi
contributes weakly to the output y. Finally, we include
a highly indicative feature xS = ??i xi + N (? =0,?2 = 0.04). This variable alone is sufficient to deter-
mine the distribution of y. The variable ? is a parameter
of the problem that determines how strongly indicative
xS is; specifically, when ? = 0, the variable xS is ran-
dom noise.
We choose this synthetic model by analogy to Pomer-
leau?s observations. The xi correspond to the side of
the road in Pomerleau?s case?the weak features present
at both testing and training?and xS corresponds to the
ditch?the strongly indicative feature that is corrupted at
test time.
We examine how badly the learned classifier is de-
graded when xS feature is present at training time but
missing at test time. For several values of the weight pa-
rameter ?, we train a regularized logistic regression clas-
sifier on 1000 instances with n = 10 weak variables. In
Figure 1, we show how the amount of error caused by
ablating xS at test time varies according to the strength
of xS . Each point in Figure 1 is averaged over 100
randomly-generated data sets. When xS is weakly in-
dicative, it does not affect the predictions of the model at
all, and the classifier?s performance is the same whether
it appears at test time or not. When xS becomes strongly
indicative, however, the classifier learns to depend on it,
and performs much more poorly when xS is ablated, even
though exactly the same information is available in the
weak features.
4 Feature Bagging
In this section, we describe the feature bagging method.
We divide the set of features F = {fk} into a collec-
tion of possibly overlapping subsets F = {F1, . . . FM},
which we call feature bags. We train individual CRFs
on each of the feature bags using standard MAP training,
yielding individual CRFs {p1, . . . pM}.
We average the individual CRFs into a single com-
bined model. This averaging can be performed in several
ways: we can average probabilities of entire sequences,
or of individual transitions; and we can average using the
arithmetic mean, or the geometric mean. This yields four
combination methods:
1. Per-sequence mixture. The distribution over label
sequences y given inputs x is modeled as a mixture
of the individual CRFs. Given nonnegative weights
{?1, . . .?m} that sum to 1, the combined model is
given by
pSM(y|x) =
M?
i=1
?ipi(y|x). (3)
It is easily seen that if the sequence model is de-
fined as in Equation 3, then the pairwise marginals
are mixtures as well:
pSM(yt, yt?1|x) =
M?
i=1
?ipi(yt, yt?1|x). (4)
The probabilities pi(yt, yt?1|x) are pairwise
marginal probabilities in the individual mod-
els, which can be efficiently computed by the
forward-backward algorithm.
We can perform decoding in the mixture model by
maximizing the individual node marginals. That is,
to predict yt we compute
y?t = arg maxyt pSM(yt|x) = arg maxyt
?
i
?ipi(yt|x),
(5)
where pi(yt|x) is computed by first running
forward-backward on each of the individual CRFs.
In the results here, however, we compute the
maximum probability sequence approximately, as
follows. We form a linear-chain distribution
pAPPX(y|x) = ?t pSM(yt|yt?1, x), and compute themost probable sequence according to pAPPX by the
Viterbi algorithm. This is approximate because pSM
is not a linear-chain distribution in general, even
when all the components are. However, the dis-
tribution pAPPX does minimize the KL-divergence
D(pSM?q) over all linear-chain distributions q.
The mixture weights can be selected in a variety of
ways, including equal voting, as in traditional bag-
ging, or EM.
2. Per-sequence product of experts. These are the log-
arithmic opinion pools that have been applied to
CRFs by (Smith et al, 2005). The distribution over
label sequences y given inputs x is modeled as a
product of experts (Hinton, 2000). In a product of
experts, instead of summing the probabilities from
the individual models, we multiply them together.
Essentially we take a geometric mean instead of
an arithmetic mean. Given nonnegative weights
{?1, . . .?m} that sum to 1, the product model is
p(y|x) ?
M?
i=1
(pi(y|x))?i . (6)
91
The combined model can also be viewed as a condi-
tional random field whose features are the log prob-
abilities from the original models:
p(y|x) ? exp
{ M?
i=1
?i log pi(y|x)
}
(7)
By substituting in the CRF definition, it can be seen
that the model in Equation 7 is simply a single CRF
whose parameters are a weighted average of the
original parameters. So feature bagging using the
product method does not increase the family of mod-
els that are considered: standard training of a single
CRF on all available features could potentially pick
the same parameters as the bagged model.
Nevertheless, in Section 5, we show that this feature
bagging method performs better than standard CRF
training.
The previous two combination methods combine the
individual models by averaging probabilities of en-
tire sequences. Alternatively, in a sequence model
we can average probabilities of individual transitions
pi(yt|yt?1, x). Computing these transition proba-
bilities requires performing probabilistic inference in
each of the original CRFs, because pi(yt|yt?1, x) =?
y\yt,yt+1 p(y|yt?1, x).This yields two other combination methods:
3. Per-transition mixture. The transition probabilities
are modeled as
pTM(yt|yt?1, x) =
M?
i=1
?ipi(yt|yt?1, x) (8)
Intuitively, the difference between per-sequence and
per-transition mixtures can be understood genera-
tively. In order to generate a label sequence y given
an input x, the per-sequence model selects a mix-
ture component, and then generates y using only
that component. The per-transition model, on the
other hand, selects a component, generates y1 from
that component, selects another component, gener-
ates y2 from the second component given y1, and so
on.
4. Per-transition product of experts. Finally, we can
combine the transition distributions using a product
model
pSP(yt|yt?1, x) ?
M?
i=1
p(yt|yt?1, x)?i (9)
Each transition distribution is thus?similarly to the
per-sequence case?an exponential-family distribu-
tion whose features are the log transition proba-
bilities from the individual models. Unlike the
per-sequence product, there is no weight-averaging
trick here, because the probabilities p(yt|yt?1, x)
are marginal probabilities.
Considered as a sequence distribution p(y|x),
the per-transition product is a locally-normalized
maximum-entropy Markov model (McCallum et al,
2000). It would not be expected to suffer from label
bias, however, because each of the features take the
future into account; they are marginal probabilities
from CRFs.
Of these four combination methods, Method 2, the per-
sequence product of experts, is originally due to Smith et
al. (2005). The other three combination methods are as
far as we know novel. In the next section, we compare
the four combination methods on several sequence label-
ing tasks. Although for concreteness we describe them
in terms of sequence models, they may be generalized to
arbitrary graphical structures.
5 Results
We evaluate feature bagging on two natural language
tasks, named entity recognition and noun-phrase chunk-
ing. We use the standard CoNLL 2003 English data set,
which is taken from Reuters newswire and consists of
a training set of 14987 sentences, a development set of
3466 sentences, and a testing set of 3684 sentences. The
named-entity labels in this data set corresponding to peo-
ple, locations, organizations and other miscellaneous en-
tities. Our second task is noun-phrase chunking. We
use the standard CoNLL 2000 data set, which consists of
8936 sentences for training and 2012 sentences for test-
ing, taken from Wall Street Journal articles annotated by
the Penn Treebank project. Although the CoNLL 2000
data set is labeled with other chunk types as well, here
we use only the NP chunks.
As is standard, we compute precision and recall for
both tasks based upon the chunks (or named entities for
CoNLL 2003) as
P = # correctly labeled chunks# labeled chunks
R = # correctly labeled chunks# actual chunks
We report the harmonic mean of precision and recall as
F1 = (2PR)/(P + R).
For both tasks, we use per-sequence product-of-experts
feature bagging with two feature bags which we manu-
ally choose based on prior experience with the data set.
For each experiment, we report two baseline CRFs, one
trained on union of the two feature sets, and one trained
only on the features that were present in both bags, such
as lexical identity and regular expressions. In both data
92
sets, we trained the individual CRFs with a Gaussian
prior on parameters with variance ?2 = 10.
For the named entity task, we use two feature bags
based upon character ngrams and lexicons. Both bags
contain a set of baseline features, such as word identity
and regular expressions (Table 4). The ngram CRF in-
cludes binary features for character ngrams of length 2,
3, and 4 and word prefixes and suffixes of length 2, 3,
and 4. The lexicon CRF includes membership features
for a variety of lexicons containing people names, places,
and company names. The combined model has 2,342,543
features. The mixture weight ? is selected using the de-
velopment set.
For the chunking task, the two feature sets are selected
based upon part of speech and lexicons. Again, a set of
baseline features are used, similar to the regular expres-
sions and word identity features used on the named entity
task (Table 4). The first bag also includes part-of-speech
tags generated by the Brill tagger and the conjunctions of
those tags used by Sha and Pereira (2003). The second
bag uses lexicon membership features for lexicons con-
taining names of people, places, and organizations. In ad-
dition, we use part-of-speech lexicons generated from the
entire Treebank, such as a list of all words that appear as
nouns. These lists are also used by the Brill tagger (Brill,
1994). The combined model uses 536,203 features. The
mixture weight ? is selected using 2-fold cross valida-
tion. The chosen model had weight 0.55 on the lexicon
model, and weight 0.45 on the ngram model.
In both data sets, the bagged model performs better
than the single CRF trained with all of the features. For
the named entity task, bagging improves performance
from 85.45% to 86.61%, with a substantial error reduc-
tion of 8.32%. This is lower than the best reported results
for this data set, which is 89.3% (Ando and Zhang, 2005),
using a large amount of unlabeled data. For the chunking
task, bagging improved the performance from 94.34% to
94.77%, with an error reduction of 7.60%. In both data
sets, the improvement is statistically significant (McNe-
mar?s test; p < 0.01).
On the chunking task, the bagged model also outper-
forms the models of Kudo and Matsumoto (2001) and
Sha and Pereira (2003), and equals the currently-best re-
sults of (Ando and Zhang, 2005), who use a large amount
of unlabeled data. Although we use lexicons that were
not included in the previous models, the additional fea-
tures actually do not help the original CRF. Only with
feature bagging do these lexicons improve performance.
Finally, we compare the four bagging methods of Sec-
tion 4: pre-transition mixture, pre-transition product of
experts, and per-sequence mixture. On the named en-
tity data, all four models perform in a statistical tie, with
no statistically significant difference in their performance
(Table 1). As we mentioned in the last section, the de-
Model F1
Per-sequence Product of Experts 86.61
Per-transition Product of Experts 86.58
Per-sequence Mixture 86.46
Per-transition Mixture 86.42
Table 1: Comparison of various bagging methods on the
CoNLL 2003 Named Entity Task.
Model F1
Single CRF(Base Feat.) 81.52
Single CRF(All Feat.) 85.45
Combined CRF 86.61
Table 2: Results for the CoNLL 2003 Named Entity
Task. The bagged CRF performs significantly better than
a single CRF with all available features (McNemar?s test;
p < 0.01).
coding procedure for the per-sequence mixture is approx-
imate. It is possible that a different decoding procedure,
such as maximizing the node marginals, would yield bet-
ter performance.
6 Previous Work
In the machine learning literature, there is much work on
ensemble methods such as stacking, boosting, and bag-
ging. Generally, the ensemble of classifiers is generated
by training on different subsets of data, rather than dif-
ferent features. However, there is some literature within
unstructured classified on combining models trained on
feature subsets. Ho (1995) creates an ensemble of de-
cision trees by randomly choosing a feature subset on
which to grow each tree using standard decision tree
learners. Other work along these lines include that of Bay
(1998) using nearest-neighbor classifiers, and more re-
cently Bryll et al(2003). Also, in Breiman?s work on ran-
dom forests (2001), ensembles of random decision trees
are constructed by choosing a random feature at each
node. This literature mostly has the goal of improving
accuracy by reducing the classifier?s variance, that is, re-
ducing overfitting.
In contrast, O?Sullivan et al (2000) specifically focus
on increasing robustness by training classifiers to use all
of the available features. Their algorithm FeatureBoost
is analogous to AdaBoost, except that the meta-learning
algorithm maintains weights on features instead of on in-
stances. Feature subsets are automatically sampled based
on which features, if corrupted, would most affect the
ensemble?s prediction. They show that FeatureBoost is
more robust than AdaBoost on synthetically corrupted
UCI data sets. Their method does not easily extend to se-
quence models, especially natural-language models with
hundreds of thousands of features.
93
Model F1
Single CRF(Base Feat.) 89.60
Single CRF(All Feat.) 94.34
(Sha and Pereira, 2003) 94.38
(Kudo and Matsumoto, 2001) 94.39
(Ando and Zhang, 2005) 94.70
Combined CRF 94.77
Table 3: Results for the CoNLL 2000 Chunking Task.
The bagged CRF performs significantly better than a sin-
gle CRF (McNemar?s test; p < 0.01), and equals the re-
sults of (Ando and Zhang, 2005), who use a large amount
of unlabeled data.
wt = w
wt begins with a capital letter
wt contains only capital letters
wt is a single capital letter
wt contains some capital letters and some lowercase
wt contains a numeric character
wt contains only numeric characters
wt appears to be a number
wt is a string of at least two periods
wt ends with a period
wt contains a dash
wt appears to be an acronym
wt appears to be an initial
wt is a single letter
wt contains punctuation
wt contains quotation marks
Pt = P
All features for time t + ? for all ? ? [?2, 2]
Table 4: Baseline features used in all bags. In the above
wt is the word at position t, Pt is the POS tag at position
t, w ranges over all words in the training data, and P
ranges over all chunk tags supplied in the training data.
The ?appears to be? features are based on hand-designed
regular expressions.
There is less work on ensembles of sequence models,
as opposed to unstructured classifiers. One example is
Altun, Hofmann, and Johnson (2003), who describe a
boosting algorithm for sequence models, but they boost
instances, not features. In fact, the main advantage of
their technique is increased model sparseness, whereas in
this work we aim to fully use more features to increase
accuracy and robustness.
Most closely related to the present work is that on log-
arithmic opinion pools for CRFs (Smith et al, 2005),
which we have called per-sequence mixture of experts in
this paper. The previous work focuses on reducing over-
fitting, combining a model of many features with several
simpler models. In contrast, here we apply feature bag-
ging to reduce feature undertraining, combining several
models with complementary feature sets. Our current
positive results are probably not due to reduction in over-
fitting, for as we have observed, all the models we test,
including the bagged one, have 99.9% F1 on the train-
ing set. Now, feature undertraining can be viewed as a
type of overfitting, because it arises when a set of fea-
tures is more indicative in the training set than the test-
ing set. Understanding this particular type of overfitting
is useful, because it motivates the choice of feature bags
that we explore in this work. Indeed, one contribution of
the present work is demonstrating how a careful choice
of feature bags can yield state-of-the-art performance.
Concurrently and independently, Smith and Osborne
(2006) present similar experiments on the CoNLL-2003
data set, examining a per-sequence mixture of experts
(that is, a logarithmic opinion pool), in which the lexi-
con features are trained separately. Their work presents
more detailed error analysis than we do here, while we
present results both on other combination methods and
on NP chunking.
7 Conclusion
Discriminatively-trained probabilistic models have had
much success in applications because of their flexibil-
ity in defining features, but sometimes even highly-
indicative features can fail to increase performance. We
have shown that this can be due to feature undertrain-
ing, where highly-indicative features prevent training of
many weaker features. One solution to this is feature bag-
ging: repeatedly selecting feature subsets, training sepa-
rate models on each subset, and averaging the individual
models.
On large, real-world natural-language processing
tasks, feature bagging significantly improves perfor-
mance, even with only two feature subsets. In this work,
we choose the subsets based on our intuition of which
features are complementary for this task, but automati-
cally determining the feature subsets is an interesting area
for future work.
Acknowledgments
We thank Andrew Ng, Hanna Wallach, Jerod Weinman,
and Max Welling for helpful conversations. This work
was supported in part by the Center for Intelligent Infor-
mation Retrieval, in part by the Defense Advanced Re-
search Projects Agency (DARPA), in part by The Cen-
tral Intelligence Agency, the National Security Agency
and National Science Foundation under NSF grant #IIS-
0326249, and in part by The Central Intelligence Agency,
the National Security Agency and National Science
Foundation under NSF grant #IIS-0427594. Any opin-
ions, findings and conclusions or recommendations ex-
pressed in this material are the author(s) and do not nec-
essarily reflect those of the sponsor.
94
References
Yasemin Altun, Thomas Hofmann, and Mark Johnson.
2003. Discriminative learning for label sequences via
boosting. In Advances in Neural Information Process-
ing Systems (NIPS*15).
Rie Ando and Tong Zhang. 2005. A high-performance
semi-supervised learning method for text chunking. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05), pages
1?9, Ann Arbor, Michigan, June. Association for Com-
putational Linguistics.
Stephen D. Bay. 1998. Combining nearest neighbor
classifiers through multiple feature subsets. In ICML
?98: Proceedings of the Fifteenth International Con-
ference on Machine Learning, pages 37?45. Morgan
Kaufmann Publishers Inc.
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5?32, October.
Eric Brill. 1994. Some advances in transformation-based
part of speech tagging. In AAAI ?94: Proceedings
of the twelfth national conference on Artificial intelli-
gence (vol. 1), pages 722?727. American Association
for Artificial Intelligence.
Robert Bryll, Ricardo Gutierrez-Osuna, and Francis
Quek. 2003. Attribute bagging: improving accuracy
of classifier ensembles by using random feature sub-
sets. Pattern Recognition, 36:1291?1302.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach. In
Walter Daelemans and Miles Osborne, editors, Pro-
ceedings of CoNLL-2003, pages 160?163. Edmonton,
Canada.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong
Zhang. 2003. Named entity recognition through clas-
sifier combination. In Proceedings of CoNLL-2003.
G.E. Hinton. 2000. Training products of experts by mini-
mizing contrastive divergence. Technical Report 2000-
004, Gatsby Computational Neuroscience Unit.
T. K. Ho. 1995. Random decision forests. In Proc. of
the 3rd Int?l Conference on Document Analysis and
Recognition, pages 278?282, Montreal, Canada, Au-
gust.
T. Kudo and Y. Matsumoto. 2001. Chunking with sup-
port vector machines. In Proceedings of NAACL-2001.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. Proc. 18th Inter-
national Conf. on Machine Learning.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy Markov models
for information extraction and segmentation. In Proc.
17th International Conf. on Machine Learning, pages
591?598. Morgan Kaufmann, San Francisco, CA.
Joseph O?Sullivan, John Langford, Rich Caruana, and
Avrim Blum. 2000. Featureboost: A meta learning
algorithm that improves model robustness. In Interna-
tional Conference on Machine Learning.
Dean Pomerleau. 1995. Neural network vision for robot
driving. In M. Arbib, editor, The Handbook of Brain
Theory and Neural Networks.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings
of HLT-NAACL 2003. Association for Computational
Linguistics.
Andrew Smith and Miles Osborne. 2006. Using
gazetteers in discriminative information extraction. In
CoNLL-X, Tenth Conference on Computational Natu-
ral Language Learning.
Andrew Smith, Trevor Cohn, and Miles Osborne. 2005.
Logarithmic opinion pools for conditional random
fields. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 18?25, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. In Lise Getoor and Ben Taskar, editors, Intro-
duction to Statistical Relational Learning. MIT Press.
To appear.
Ben Taskar, Dan Klein, Michael Collins, Daphne Koller,
and Chris Manning. 2004. Max-margin parsing. In
Empirical Methods in Natural Language Processing
(EMNLP04).
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In HLT-
NAACL 2003.
95
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 296?303,
New York, June 2006. c?2006 Association for Computational Linguistics
Integrating Probabilistic Extraction Models and Data Mining
to Discover Relations and Patterns in Text
Aron Culotta
University of Massachusetts
Amherst, MA 01003
culotta@cs.umass.edu
Andrew McCallum
University of Massachusetts
Amherst, MA 01003
mccallum@cs.umass.edu
Jonathan Betz
Google, Inc.
New York, NY 10018
jtb@google.com
Abstract
In order for relation extraction systems
to obtain human-level performance, they
must be able to incorporate relational pat-
terns inherent in the data (for example,
that one?s sister is likely one?s mother?s
daughter, or that children are likely to
attend the same college as their par-
ents). Hand-coding such knowledge can
be time-consuming and inadequate. Addi-
tionally, there may exist many interesting,
unknown relational patterns that both im-
prove extraction performance and provide
insight into text. We describe a probabilis-
tic extraction model that provides mutual
benefits to both ?top-down? relational pat-
tern discovery and ?bottom-up? relation
extraction.
1 Introduction
Consider these four sentences:
1. George W. Bush?s father is George H. W. Bush.
2. George H.W. Bush?s sister is Nancy Bush Ellis.
3. Nancy Bush Ellis?s son is John Prescott Ellis.
4. John Prescott Ellis analyzed George W. Bush?s
campaign.
We would like to build an automated system to
extract the set of relations shown in Figure 1.
cousin
Nancy Ellis Bush
sibling
George HW Bush
George W Bush
son
John Prescott Ellis
son
Figure 1: Bush family tree
State of the art extraction algorithms may be able
to detect the son and sibling relations from local lan-
guage clues. However, the cousin relation is only
implied by the text and requires additional knowl-
edge to be extracted. Specifically, the system re-
quires knowledge of familial relation patterns.
One could imagine a system that accepts such
rules as input (e.g. cousin = father?s sister?s son)
and applies them to extract implicit relations. How-
ever, exhaustively enumerating all possible rules can
be tedious and incomplete. More importantly, many
relational patterns unknown a priori may both im-
prove extraction accuracy and uncover informative
trends in the data (e.g. that children often adopt the
religion of their parents). Indeed, the goal of data
mining is to learn such patterns from database reg-
ularities. Since these patterns will not always hold,
we would like to handle them probabilistically.
We propose an integrated supervised machine
learning method that learns both contextual and re-
lational patterns to extract relations. In particular,
we construct a linear-chain conditional random field
(Lafferty et al, 2001; Sutton and McCallum, 2006)
to extract relations from biographical texts while si-
multaneously discovering interesting relational pat-
terns that improve extraction performance.
296
2 Related Work
This work can be viewed as a step toward the in-
tegration of information extraction and data mining
technology, a direction of growing interest. Nahm
and Mooney (2000) present a system that mines as-
sociation rules from a database constructed from au-
tomatically extracted data, then applies these learned
rules to improve data field recall without revisiting
the text. Our work attempts to more tightly inte-
grate the extraction and mining tasks by learning
relational patterns that can be included probabilis-
tically into extraction to improve its accuracy; also,
our work focuses on mining from relational graphs,
rather than single-table databases.
McCallum and Jensen (2003) argue the theoreti-
cal benefits of an integrated probabilistic model for
extraction and mining, but do not construct such a
system. Our work is a step in the direction of their
proposal, using an inference procedure based on a
closed-loop iteration between extraction and rela-
tional pattern discovery.
Most other work in this area mines raw text, rather
than a database automatically populated via extrac-
tion (Hearst, 1999; Craven et al, 1998).
This work can also be viewed as part of a trend
to perform joint inference across multiple language
processing tasks (Miller et al, 2000; Roth and tau
Yih, 2002; Sutton and McCallum, 2004).
Finally, using relational paths between entities is
also examined in (Richards and Mooney, 1992) to
escape local maxima in a first-order learning system.
3 Relation Extraction as Sequence
Labeling
Relation extraction is the task of discovering seman-
tic connections between entities. In text, this usu-
ally amounts to examining pairs of entities in a doc-
ument and determining (from local language cues)
whether a relation exists between them. Common
approaches to this problem include pattern match-
ing (Brin, 1998; Agichtein and Gravano, 2000),
kernel methods (Zelenko et al, 2003; Culotta and
Sorensen, 2004; Bunescu and Mooney, 2006), lo-
gistic regression (Kambhatla, 2004), and augmented
parsing (Miller et al, 2000).
The pairwise classification approach of kernel
methods and logistic regression is commonly a two-
phase method: first the entities in a document are
identified, then a relation type is predicted for each
pair of entities. This approach presents at least
two difficulties: (1) enumerating all pairs of enti-
ties, even when restricted to pairs within a sentence,
results in a low density of positive relation exam-
ples; and (2) errors in the entity recognition phase
can propagate to errors in the relation classification
stage. As an example of the latter difficulty, if a per-
son is mislabeled as a company, then the relation
classifier will be unsuccessful in finding a brother
relation, despite local evidence.
We avoid these difficulties by restricting our in-
vestigation to biographical texts, e.g. encyclopedia
articles. A biographical text mostly discusses one
entity, which we refer to as the principal entity. We
refer to other mentioned entities as secondary enti-
ties. For each secondary entity, our goal is to predict
what relation, if any, it has to the principal entity.
This formulation allows us to treat relation ex-
traction as a sequence labeling task such as named-
entity recognition or part-of-speech tagging, and we
can now apply models that have been successful on
those tasks. By anchoring one argument of relations
to be the principal entity, we alleviate the difficulty
of enumerating all pairs of entities in a document.
By converting to a sequence labeling task, we fold
the entity recognition step into the relation extrac-
tion task. There is no initial pass to label each entity
as a person or company. Instead, an entity?s label is
its relation to the principal entity. Below is an exam-
ple of a labeled article:
George W. Bush
George is the son of George H. W. Bush
? ?? ?
father
and Barbara Bush
? ?? ?
mother
.
Additionally, by using a sequence model we can
capture the dependence between adjacent labels. For
example, in our data it is common to see phrases
such as ?son of the Republican president George H.
W. Bush? for which the labels politicalParty, jobTi-
tle, and father occur consecutively. Sequence mod-
els are specifically designed to handle these kinds
of dependencies. We now discuss the details of our
extraction model.
297
3.1 Conditional Random Fields
We build a model to extract relations using linear-
chain conditional random fields (CRFs) (Lafferty
et al, 2001; Sutton and McCallum, 2006). CRFs
are undirected graphical models (i.e. Markov net-
works) that are discriminatively-trained to maximize
the conditional probability of a set of output vari-
ables y given a set of input variables x. This condi-
tional distribution has the form
p?(y|x) =
1
Zx
?
c?C
?c(yc,xc; ?) (1)
where ? are potential functions parameterized by ?
and Zx =
?
y
?
c?C ?(yc,xc) is a normalization
factor. Assuming ?c factorizes as a log-linear com-
bination of arbitrary features computed over clique
c, then ?c(yc,xc; ?) = exp (
?
k ?kfk(yc,xc)),
where f is a set of arbitrary feature functions over
the input, each of which has an associate model
parameter ?k. Parameters ? = {?k} are a set
of real-valued weights typically estimated from la-
beled training data by maximizing the data likeli-
hood function using gradient ascent.
In these experiments, we make a first-order
Markov assumption on the dependencies among y,
resulting in a linear-chain CRF.
4 Relational Patterns
The modeling flexibility of CRFs permits the fea-
ture functions to be complex, overlapping features of
the input without requiring additional assumptions
on their inter-dependencies. In addition to common
language features (e.g. neighboring words and syn-
tactic information), in this work we explore features
that cull relational patterns from a database of enti-
ties.
As described in the introductory example (Figure
1), context alone is often insufficient to extract re-
lations. Even in simpler examples, it may be the
case that modeling relational patterns can improve
extraction accuracy.
To capture this evidence, we compute features
from a database to indicate relational connections
between entities, similar to the relational path-
finding performed in Richards and Mooney (1992).
Imagine that the four sentence example about the
Bush family is included in a training set, and the en-
cousin
father son
X Y
sibling
Figure 2: A feature template for the cousin relation.
tities are labeled with their correct relations. In this
case, the cousin relation in sentence 4 would also be
labeled. From this data, we can create a relational
database that contains the relations in Figure 1.
Assume sentence 4 comes from a biography about
John Ellis. We calculate a feature for the entity
George W. Bush that indicates the path from John
Ellis to George W. Bush in the database, annotat-
ing each edge in the path with its relation label; i.e.
father-sibling-son. By abstracting away the actual
entity names, we have created a cousin template fea-
ture, as shown in Figure 2.
By adding these relational paths as features to
the model, we can learn interesting relational pat-
terns that may have low precision (e.g. ?people are
likely to be friends with their classmates?) without
hampering extraction performance. This is in con-
trast to the system described in Nahm and Mooney
(2000), in which patterns are induced from a noisy
database and then applied directly to extraction. In
our system, since each learned path has an associ-
ated weight, it is simply another piece of evidence
to help the extractor. Low precision patterns may
have lower weights than high precision patterns, but
they will still influence the extractor.
A nice property of this approach is that examin-
ing highly weighted patterns can provide insight into
regularities of the data.
4.1 Feature Induction
During CRF training, weights are learned for each
relational pattern. Patterns that increase extraction
performance will receive higher weights, while pat-
terns that have little effect on performance will re-
ceive low weights.
We can explore the space of possible conjunctions
of these patterns using feature induction for CRFs,
as described in McCallum (2003). Search through
the large space of possible conjunctions is guided
298
by adding features that are estimated to increase the
likelihood function most.
When feature induction is used with relational
patterns, we can view this as a type of data mining,
in which patterns are created based on their influ-
ence on an extraction model. This is similar to work
by Dehaspe (1997), where inductive logic program-
ming is embedded as a feature induction technique
for a maximum entropy classifier. Our work restricts
induced features to conjunctions of base features,
rather than using first-order clauses. However, the
patterns we learn are based on information extracted
from natural language.
4.2 Iterative Database Construction
The top-down knowledge provided by data min-
ing algorithms has the potential to improve the per-
formance of information extraction systems. Con-
versely, bottom-up knowledge generated by ex-
traction systems can be used to populate a large
database, from which more top-down knowledge
can be discovered. By carefully communicating the
uncertainty between these systems, we hope to iter-
atively expand a knowledge base, while minimizing
fallacious inferences.
In this work, the top-down knowledge consists of
relational patterns describing the database path be-
tween entities in text. The uncertainty of this knowl-
edge is handled by associating a real-valued CRF
weight with each pattern, which increases when the
pattern is predictive of other relations. Thus, the ex-
traction model can adapt to noise in these patterns.
Since we also desire to extract relations between
entities that appear in text but not in the database, we
first populate the database with relations extracted
by a CRF that does not use relational patterns. We
then do further extraction with a CRF that incorpo-
rates the relational patterns found in this automati-
cally generated database. In this manner, we create a
closed-loop system that alternates between bottom-
up extraction and top-down pattern discovery. This
approach can be viewed as a type of alternating opti-
mization, with analogies to formal methods such as
expectation-maximization.
The uncertainty in the bottom-up extraction step
is handled by estimating the confidence of each ex-
traction and pruning the database to remove en-
tries with low confidence. One of the benefits of
a probabilistic extraction model is that confidence
estimates can be straight-forwardly obtained. Cu-
lotta and McCallum (2004) describe the constrained
forward-backward algorithm to efficiently estimate
the conditional probability that a segment of text is
correctly extracted by a CRF.
Using this algorithm, we associate a confidence
value with each relation extracted by the CRF. This
confidence value is then used to limit the noise
introduced by incorrect extractions. This differs
from Nahm and Mooney (2000) and Mooney and
Bunescu (2005), in which standard decision tree rule
learners are applied to the unfiltered output of ex-
traction.
4.3 Extracting Implicit Relations
An implicit relation is one that does not have direct
contextual evidence, for example the cousin relation
in our initial example. Implicit relations generally
require some background knowledge to be detected,
such as relational patterns (e.g. rules about familial
relations). These are the sorts of relations on which
current extraction models perform most poorly.
Notably, these are exactly the sorts of relations
that are likely to have the biggest impact on informa-
tion access. A system that can accurately discover
knowledge that is only implied by the text will dra-
matically increase the amount of information a user
can uncover, effectively providing access to the im-
plications of a corpus.
We argue that integrating top-down and bottom-
up knowledge discovery algorithms discussed in
Section 4.2 can enable this technology. By per-
forming pattern discovery in conjunction with infor-
mation extraction, we can collate facts from multi-
ple sources to infer new relations. This is an ex-
ample of cross-document fusion or cross-document
information extraction, a growing area of research
transforming raw extractions into usable knowledge
bases (Mann and Yarowsky, 2005; Masterson and
Kushmerik, 2003).
5 Experiments
5.1 Data
We sampled 1127 paragraphs from 271 articles from
the online encyclopediaWikipedia1 and labeled a to-
1http://www.wikipedia.org
299
George W. Bush
Dick Cheney
underling
Yale
education
Republican
partyPresident
jobTitle
George H. W. Bush
son
underlingHarken Energy
executive
education party
jobTitle
Prescott Bush
son
education
Bill Clinton
rival
Bob Dole
rival
education
Democrat
party
jobTitle
Hillary Clinton
husband
education
party
Halliburton
executiveeducation
Pres Medal of Freedom
awardparty
Nelson Rockefeller
award
Elizabeth Dole
wife
WWII
participant
awardparty
party
Martin Luther King, Jr.
award
Figure 3: An example of the connectivity of the entities in the data.
birthday birth year death day
death year nationality visited
birth place death place religion
job title member of cousin
friend discovered education
employer associate opus
participant influence award
brother wife supported idea
executive of political party supported person
founder son father
rival underling superior
role inventor husband
grandfather sister brother-in-law
nephew mother daughter
granddaughter grandson great-grandson
grandmother rival organization owner of
uncle descendant ancestor
great-grandfather aunt
Table 1: The set of labeled relations.
tal of 4701 relation instances. In addition to a large
set of person-to-person relations, we also included
links between people and organizations, as well as
biographical facts such as birthday and jobTitle. In
all, there are 53 labels in the training data (Table 1).
We sample articles that result in a high density
of interesting relations by choosing, for example, a
collection of related family members and associates.
Figure 3 shows a small example of the type of con-
nections in the data. We then split the data into train-
ing and testing sets (70-30 split), attempting to sep-
arate the entities into connected components. For
example, all Bush family members were placed in
the training set, while all Kennedy family members
were placed in the testing set. While there are still
occasional paths connecting entities in the training
set to those in the test set, we believe this method-
ology reflects a typical real-world scenario in which
we would like to extend an existing database to a
different, but slightly related, domain.
The structure of the Wikipedia articles somewhat
simplifies the extraction task, since important enti-
ties are hyper-linked within the text. This provides
an automated way to detect entities in the text, al-
though these entities are not classified by type. This
also allows us to easily construct database queries,
since we can reason at the entity level, rather than
the token level. (Although, see Sarawagi and Cohen
(2004) for extensions of CRFs that model the en-
tity length distribution.) The results we report here
are constrained to predict relations only for hyper-
linked entities. Note that despite this property, we
still desire to use a sequence model to capture the
dependencies between adjacent labels.
We use the MALLET CRF implementation (Mc-
Callum, 2002) with the default regularization pa-
rameters.
Based on initial experiments, we restrict relational
path features to length two or three. Paths of length
one will learn trivial paths and can lead to over-
fitting. Paths longer than three can increase compu-
tational costs without adding much new information.
In addition to the relational pattern features de-
scribed in Section 4, the list of local features in-
cludes context words (such as the token identity
within a 6 word window of the target token), lexi-
cons (such as whether a token appears in a list of
cities, people, or companies), regular expressions
(such as whether the token is capitalized or contains
digits or punctuation), part-of-speech (predicted by
a CRF that was trained separately for part of speech
tagging), prefix/suffix (such as whether a word ends
in -ed or begins with ch-), and offset conjunctions
(combinations of adjacent features within a window
of size six).
300
ME CRF0 CRFr CRFr0.9 CRFr0.5 CRFt CRFt0.5
F1 .5489 .5995 .6100 .6008 .6136 .6791 .6363
P .6475 .7019 .6799 .7177 .7095 .7553 .7343
R .4763 .5232 .5531 .5166 .5406 .6169 .5614
Table 2: Results comparing the relative benefits of using relational patterns in extraction.
5.2 Extraction Results
We evaluate performance by calculating the preci-
sion (P) and recall (R) of extracted relations, as well
as the F1 measure, which is the harmonic mean of
precision and recall.
CRF0 is the conditional random field constructed
without relational features. Results for CRF0 are
displayed in the second column of Table 2. ME is
a maximum entropy classifier trained on the same
feature set as CRF0. The difference between these
two models is that CRF0 models the dependence of
relations that appear consecutively in the text. The
superior performance of CRF0 suggests that this de-
pendence is important to capture.
The remaining models incorporate the relational
patterns described in Section 4. We compare three
different confidence thresholds for the construction
of the initial testing database, as described in Sec-
tion 4.2. CRFr uses no threshold, while CRFr0.9
andCRFr0.5 restrict the database to extractions with
confidence greater than 0.9 and 0.5, respectively.
As shown by comparing CRF0 and CRFr in Ta-
ble 2, the relational features constructed from the
database with no confidence threshold provides a
considerable boost in recall (reducing error by 7%),
at the cost of a decrease in precision. Here we see
the effect of making fallacious inferences on a noisy
database.
In column four, we see the opposite effect for
the overly conservative threshold of CRFr0.9. Here,
precision improves slightly over CRF0, and consid-
erably over CRFr (12% error reduction), but this is
accompanied by a drop in recall (8% reduction).
Finally, in column five, a confidence of 0.5 results
in the best F1 measure (a 3.5% error reduction over
CRF0). CRFr0.5 also obtains better recall and preci-
sion than CRF0, reducing recall error by 3.6%, pre-
cision error by 2.5%.
Comparing the performance on different relation
types, we find that the biggest increase from CRF0
to CRFr0.5 is on the memberOf relation, for which
the F1 score improves from 0.4211 to 0.6093. We
conjecture that the reason for this is that the patterns
most useful for thememberOf label contain relations
that are well-detected by the first-pass CRF. Also,
the local language context seems inadequate to prop-
erly extract this relation, given the low performance
of CRF0.
To better gauge how much relational pattern fea-
tures are affected by errors in the database, we run
two additional experiments for which the relational
features are fixed to be correct. That is, imagine that
we construct a database from the true labeling of the
testing data, and create the relational pattern features
from this database. Note that this does not trivialize
the problem, since there are no relational path fea-
tures of length one (e.g., if X is the wife of Y, there
will be no feature indicating this).
We construct two experiments under this scheme,
one where the entire test database is used (CRFt),
and another where only half the relations are in-
cluded in the test database, selected uniformly at
random (CRFt0.5).
Column six shows the improvements enabled by
using the complete testing database. More inter-
estingly, column seven shows that even with only
half the database accurately known, performance
improves considerably over both CRF and CRFr0.5.
A realistic scenario for CRFt0.5 is a semi-automated
system, in which a partially-filled database is used to
bootstrap extraction.
5.3 Mining Results
Comparing the impact of discovered patterns on ex-
traction is a way to objectively measure mining per-
formance. We now give a brief subjective evaluation
of the learned patterns. By examining relational pat-
terns with high weights for a particular label, we can
glean some regularities from our dataset. Examples
of such patterns are in Table 3.
301
Relation Relational Path Feature
mother father ? wife
cousin mother ? husband ? nephew
friend education ? student
education father ? education
boss boss ? son
memberOf grandfather ? memberOf
rival politicalParty ? member ? rival
Table 3: Examples of highly weighted relational pat-
terns.
From the familial relations in our training data, we
are able to discover many equivalences for mothers,
cousins, grandfathers, and husbands. In addition to
these high precision patterns, the system also gener-
ates interesting, low precision patterns. Row 3-7 of
Table 3 can be summarized by the following gener-
alizations: friends tend to be classmates; children of
alumni often attend the same school as their parents;
a boss? child often becomes the boss; grandchildren
are often members of the same organizations as their
grandparents; and rivals of a person from one polit-
ical party are often rivals of other members of the
same political party. While many of these patterns
reflect the high concentration of political entities and
familial relations in our training database, many will
have applicability across domains.
5.4 Implicit Relations
It is difficult to measure system performance on im-
plicit relations, since our labeled data does not dis-
tinguish between explicit and implicit relations. Ad-
ditionally, accurately labeling all implicit relations
is challenging even for a human annotator.
We perform a simple exploratory analysis to de-
termine how relational patterns can help discover
implicit relations. We construct a small set of syn-
thetic sentences for which CRF0 successfully ex-
tracts relations using contextual features. We then
add sentences with slightly more ambiguous lan-
guage and measure whether CRFr can overcome this
ambiguity using relational pattern features.
For example, we create an article about an en-
tity named ?Bob Smith? that includes the sentences
?His brother, Bill Smith, was a biologist? and ?His
companion, Bill Smith, was a biologist.? CRF0 suc-
cessfully returns the brother relation in the first sen-
tence, but not the second. After a fact is added to
the database that says Bob and Bill have a brother in
common named John, CRFr is able to correctly label
the second sentence in spite of the ambiguous word
?companion,? because CRF0 has a highly-weighted
relational pattern feature for brother.
Similar behavior is observed for low precision
patterns like ?associates tend to win the same
awards.? A synthetic article for the entity ?Tom
Jones? contains the sentences ?He was awarded the
Pulitzer Prize in 1998? and ?Tom got the Pulitzer
Prize in 1998.? Because CRF0 is highly-reliant on
the presence of the verb ?awarded? or ?won? to indi-
cate a prize fact, it fails to label the second sentence
correctly. After the database is augmented to include
the fact that Tom?s associate Jill received the Pulitzer
Prize, CRFr labels the second sentence correctly.
However, we also observed that CRFr still re-
quires some contextual clues to extract implicit re-
lations. For example, if the Tom Jones article in-
stead contains the sentence ?The Pulitzer Prize was
awarded to him in 1998,? neither CRF labels the
prize fact correctly, since this passive construction
is rarely seen in the training data.
We conclude from this brief analysis that rela-
tional patterns used by CRFr can help extract im-
plicit relations when (1) the database contains ac-
curate relational information, and (2) the sentence
contains limited contextual clues. Since relational
patterns are treated only as additional features by
CRFr, they are generally not powerful enough to
overcome a complete absence of contextual clues.
From this perspective, relational patterns can be seen
as enhancing the signal from contextual clues. This
differs from deterministically applying learned rules
independent of context, which may boost recall at
the cost of precision.
6 Conclusions and Future Work
We have shown that integrating pattern discovery
with relation extraction can lead to improved per-
formance on each task.
In the future, we wish to explore extending this
methods to larger datasets, where we expect rela-
tional patterns to be even more interesting. Also,
we plan to improve upon iterative database construc-
tion by performing joint inference among distant
302
relations in an article. Inference in these highly-
connected models will likely require approximate
methods. Additionally, we wish to focus on extract-
ing implicit relations, dealing more formally with
the precision-recall trade-off inherent in applying
noisy rules to improve extraction.
7 Acknowledgments
Thanks to the Google internship program, and to Charles Sutton
for providing the CRF POS tagger. This work was supported in
part by the Center for Intelligent Information Retrieval, in part
by U.S. Government contract #NBCH040171 through a sub-
contract with BBNT Solutions LLC, in part by The Central In-
telligence Agency, the National Security Agency and National
Science Foundation under NSF grant #IIS-0326249, and in part
by the Defense Advanced Research Projects Agency (DARPA),
through the Department of the Interior, NBC, Acquisition Ser-
vices Division, under contract number NBCHD030010. Any
opinions, findings and conclusions or recommendations ex-
pressed in this material are the author(s) and do not necessarily
reflect those of the sponsor.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball: Extract-
ing relations from large plain-text collections. In Proceed-
ings of the Fifth ACM International Conference on Digital
Libraries.
Sergey Brin. 1998. Extracting patterns and relations from the
world wide web. In WebDB Workshop at 6th International
Conference on Extending Database Technology.
Razvan Bunescu and Raymond Mooney. 2006. Subsequence
kernels for relation extraction. In Y. Weiss, B. Scho?lkopf,
and J. Platt, editors, Advances in Neural Information Pro-
cessing Systems 18. MIT Press, Cambridge, MA.
Mark Craven, Dan DiPasquo, Dayne Freitag, Andrew K. Mc-
Callum, Tom M. Mitchell, Kamal Nigam, and Sea?n Slattery.
1998. Learning to extract symbolic knowledge from the
World Wide Web. In Proceedings of AAAI-98, 15th Confer-
ence of the American Association for Artificial Intelligence,
pages 509?516, Madison, US. AAAI Press, Menlo Park, US.
Aron Culotta and Andrew McCallum. 2004. Confidence es-
timation for information extraction. In Human Langauge
Technology Conference (HLT 2004), Boston, MA.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree
kernels for relation extraction. In ACL.
L. Dehaspe. 1997. Maximum entropy modeling with clausal
constraints. In Proceedings of the Seventh International
Workshop on Inductive Logic Programming, pages 109?125,
Prague, Czech Republic.
M. Hearst. 1999. Untangling text data mining. In 37th Annual
Meeting of the Association for Computational Linguistics.
Nanda Kambhatla. 2004. Combining lexical, syntactic, and se-
mantic features with maximum entropy models for extract-
ing relations. In ACL.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. 18th Interna-
tional Conf. on Machine Learning, pages 282?289. Morgan
Kaufmann, San Francisco, CA.
Gideon Mann and David Yarowsky. 2005. Multi-field informa-
tion extraction and cross-document fusion. In ACL.
D. Masterson and N. Kushmerik. 2003. Information extraction
from multi-document threads. In ECML-2003: Workshop on
Adaptive Text Extraction and Mining, pages 34?41.
Andrew McCallum and David Jensen. 2003. A note on the
unification of information extraction and data mining us-
ing conditional-probability, relational models. In IJCAI03
Workshop on Learning Statistical Models from Relational
Data.
Andrew McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
Andrew McCallum. 2003. Efficiently inducing features of con-
ditional random fields. In Nineteenth Conference on Uncer-
tainty in Artificial Intelligence (UAI03).
Scott Miller, Heidi Fox, Lance A. Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing to ex-
tract information from text. In ANLP.
Raymond J. Mooney and Razvan Bunescu. 2005. Mining
knowledge from text using information extraction. SigKDD
Explorations on Text Mining and Natural Language Process-
ing.
Un Yong Nahm and Raymond J. Mooney. 2000. A mutually
beneficial integration of data mining and information extrac-
tion. In AAAI/IAAI.
Bradley L. Richards and Raymond J. Mooney. 1992. Learning
relations by pathfinding. In Proceedings of the Tenth Na-
tional Conference on Artificial Intelligence (AAAI-92), pages
50?55, San Jose, CA.
Dan Roth and Wen tau Yih. 2002. Probabilistic reasoning for
entity and relation recognition. In COLING.
Sunita Sarawagi and William W. Cohen. 2004. Semi-markov
conditional random fields for information extraction. In
NIPS 04.
Charles Sutton and Andrew McCallum. 2004. Dynamic condi-
tional random fields: Factorized probabilistic models for la-
beling and segmenting sequence data. In Proceedings of the
Twenty-First International Conference on Machine Learning
(ICML).
Charles Sutton and Andrew McCallum. 2006. An introduction
to conditional random fields for relational learning. In Lise
Getoor and Ben Taskar, editors, Introduction to Statistical
Relational Learning. MIT Press. To appear.
Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella.
2003. Kernel methods for relation extraction. Journal of
Machine Learning Research, 3:1083?1106.
303
Proceedings of NAACL HLT 2007, pages 81?88,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
First-Order Probabilistic Models for Coreference Resolution
Aron Culotta and Michael Wick and Andrew McCallum
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
{culotta,mwick,mccallum}@cs.umass.edu
Abstract
Traditional noun phrase coreference res-
olution systems represent features only
of pairs of noun phrases. In this paper,
we propose a machine learning method
that enables features over sets of noun
phrases, resulting in a first-order proba-
bilistic model for coreference. We out-
line a set of approximations that make this
approach practical, and apply our method
to the ACE coreference dataset, achiev-
ing a 45% error reduction over a com-
parable method that only considers fea-
tures of pairs of noun phrases. This result
demonstrates an example of how a first-
order logic representation can be incorpo-
rated into a probabilistic model and scaled
efficiently.
1 Introduction
Noun phrase coreference resolution is the problem
of clustering noun phrases into anaphoric sets. A
standard machine learning approach is to perform a
set of independent binary classifications of the form
?Is mention a coreferent with mention b??
This approach of decomposing the problem into
pairwise decisions presents at least two related diffi-
culties. First, it is not clear how best to convert the
set of pairwise classifications into a disjoint cluster-
ing of noun phrases. The problem stems from the
transitivity constraints of coreference: If a and b are
coreferent, and b and c are coreferent, then a and c
must be coreferent.
This problem has recently been addressed by a
number of researchers. A simple approach is to per-
form the transitive closure of the pairwise decisions.
However, as shown in recent work (McCallum and
Wellner, 2003; Singla and Domingos, 2005), bet-
ter performance can be obtained by performing rela-
tional inference to directly consider the dependence
among a set of predictions. For example, McCal-
lum and Wellner (2005) apply a graph partitioning
algorithm on a weighted, undirected graph in which
vertices are noun phrases and edges are weighted by
the pairwise score between noun phrases.
A second and less studied difficulty is that the
pairwise decomposition restricts the feature set to
evidence about pairs of noun phrases only. This re-
striction can be detrimental if there exist features of
sets of noun phrases that cannot be captured by a
combination of pairwise features. As a simple exam-
ple, consider prohibiting coreferent sets that consist
only of pronouns. That is, we would like to require
that there be at least one antecedent for a set of pro-
nouns. The pairwise decomposition does not make
it possible to capture this constraint.
In general, we would like to construct arbitrary
features over a cluster of noun phrases using the
full expressivity of first-order logic. Enabling this
sort of flexible representation within a statistical
model has been the subject of a long line of research
on first-order probabilistic models (Gaifman, 1964;
Halpern, 1990; Paskin, 2002; Poole, 2003; Richard-
son and Domingos, 2006).
Conceptually, a first-order probabilistic model
can be described quite compactly. A configura-
tion of the world is represented by a set of predi-
81
He
President Bush 
Laura Bush
She
0.2
0.9
0.7
0.4
0.001
0.6
Figure 1: An example noun coreference graph in
which vertices are noun phrases and edge weights
are proportional to the probability that the two nouns
are coreferent. Partitioning such a graph into disjoint
clusters corresponds to performing coreference res-
olution on the noun phrases.
cates, each of which has an associated real-valued
parameter. The likelihood of each configuration of
the world is proportional to a combination of these
weighted predicates. In practice, however, enu-
merating all possible configurations, or even all the
predicates of one configuration, can result in in-
tractable combinatorial growth (de Salvo Braz et al,
2005; Culotta and McCallum, 2006).
In this paper, we present a practical method to per-
form training and inference in first-order models of
coreference. We empirically validate our approach
on the ACE coreference dataset, showing that the
first-order features can lead to an 45% error reduc-
tion.
2 Pairwise Model
In this section we briefly review the standard pair-
wise coreference model. Given a pair of noun
phrases xij = {xi, xj}, let the binary random vari-
able yij be 1 if xi and xj are coreferent. Let F =
{fk(xij , y)} be a set of features over xij . For exam-
ple, fk(xij , y) may indicate whether xi and xj have
the same gender or number. Each feature fk has an
associated real-valued parameter ?k. The pairwise
model is
p(yij |xij) =
1
Zxij
exp
?
k
?kfk(xij , yij)
where Zxij is a normalizer that sums over the two
settings of yij .
This is a maximum-entropy classifier (i.e. logis-
tic regression) in which p(yij |xij) is the probability
that xi and xj are coreferent. To estimate ? = {?k}
from labeled training data, we perform gradient as-
cent to maximize the log-likelihood of the labeled
data.
Two critical decisions for this method are (1) how
to sample the training data, and (2) how to combine
the pairwise predictions at test time. Systems of-
ten perform better when these decisions complement
each other.
Given a data set in which noun phrases have been
manually clustered, the training data can be cre-
ated by simply enumerating over each pair of noun
phrases xij , where yij is true if xi and xj are in
the same cluster. However, this approach generates
a highly unbalanced training set, with negative ex-
amples outnumbering positive examples. Instead,
Soon et al (2001) propose the following sampling
method: Scan the document from left to right. Com-
pare each noun phrase xi to each preceding noun
phrase xj , scanning from right to left. For each pair
xi, xj , create a training instance ?xij , yij?, where yij
is 1 if xi and xj are coreferent. The scan for xj ter-
minates when a positive example is constructed, or
the beginning of the document is reached. This re-
sults in a training set that has been pruned of distant
noun phrase pairs.
At testing time, we can construct an undirected,
weighted graph in which vertices correspond to
noun phrases and edge weights are proportional to
p(yij |xij). The problem is then to partition the graph
into clusters with high intra-cluster edge weights and
low inter-cluster edge weights. An example of such
a graph is shown in Figure 1.
Any partitioning method is applicable here; how-
ever, perhaps most common for coreference is to
perform greedy clustering guided by the word or-
der of the document to complement the sampling
method described above (Soon et al, 2001). More
precisely, scan the document from left-to-right, as-
signing each noun phrase xi to the same cluster
as the closest preceding noun phrase xj for which
p(yij |xij) > ?, where ? is some classification
threshold (typically 0.5). Note that this method con-
trasts with standard greedy agglomerative cluster-
ing, in which each noun phrase would be assigned
to the most probable cluster according to p(yij |xij).
82
Choosing the closest preceding phrase is common
because nearby phrases are a priori more likely to
be coreferent.
We refer to the training and inference methods de-
scribed in this section as the Pairwise Model.
3 First-Order Logic Model
We propose augmenting the Pairwise Model to
enable classification decisions over sets of noun
phrases.
Given a set of noun phrases xj = {xi}, let the bi-
nary random variable yj be 1 if all the noun phrases
xi ? xj are coreferent. The features fk and weights
?k are defined as before, but now the features can
represent arbitrary attributes over the entire set xj .
This allows us to use the full flexibility of first-order
logic to construct features about sets of nouns. The
First-Order Logic Model is
p(yj |xj) =
1
Zxj
exp
?
k
?kfk(x
j , yj)
where Zxj is a normalizer that sums over the two
settings of yj .
Note that this model gives us the representational
power of recently proposed Markov logic networks
(Richardson and Domingos, 2006); that is, we can
construct arbitrary formulae in first-order logic to
characterize the noun coreference task, and can learn
weights for instantiations of these formulae. How-
ever, naively grounding the corresponding Markov
logic network results in a combinatorial explosion of
variables. Below we outline methods to scale train-
ing and prediction with this representation.
As in the Pairwise Model, we must decide how to
sample training examples and how to combine inde-
pendent classifications at testing time. It is impor-
tant to note that by moving to the First-Order Logic
Model, the number of possible predictions has in-
creased exponentially. In the Pairwise Model, the
number of possible y variables is O(|x|2), where
x is the set of noun phrases. In the First-Order
Logic Model, the number of possible y variables is
O(2|x|): There is a y variable for each possible el-
ement of the powerset of x. Of course, we do not
enumerate this set; rather, we incrementally instan-
tiate y variables as needed during prediction.
A simple method to generate training examples
is to sample positive and negative cluster examples
uniformly at random from the training data. Positive
examples are generated by first sampling a true clus-
ter, then sampling a subset of that cluster. Negative
examples are generated by sampling two positive ex-
amples and merging them into the same cluster.
At testing time, we perform standard greedy ag-
glomerative clustering, where the score for each
merger is proportional to the probability of the
newly formed clustering according to the model.
Clustering terminates when there exists no addi-
tional merge that improves the probability of the
clustering.
We refer to the system described in this section as
First-Order Uniform.
4 Error-driven and Rank-based training
of the First-Order Model
In this section we propose two enhancements to
the training procedure for the First-Order Uniform
model.
First, because each training example consists of
a subset of noun phrases, the number of possible
training examples we can generate is exponential in
the number of noun phrases. We propose an error-
driven sampling method that generates training ex-
amples from errors the model makes on the training
data. The algorithm is as follows: Given initial pa-
rameters ?, perform greedy agglomerative cluster-
ing on training document i until an incorrect cluster
is formed. Update the parameter vector according to
this mistake, then repeat for the next training docu-
ment. This process is repeated for a fixed number of
iterations.
Exactly how to update the parameter vector is ad-
dressed by the second enhancement. We propose
modifying the optimization criterion of training to
perform ranking rather than classification of clus-
ters. Consider a training example cluster with a neg-
ative label, indicating that not all of the noun phrases
it contains are coreferent. A classification training
algorithm will ?penalize? all the features associated
with this cluster, since they correspond to a negative
example. However, because there may exists subsets
of the cluster that are coreferent, features represent-
ing these positive subsets may be unjustly penalized.
To address this problem, we propose constructing
training examples consisting of one negative exam-
83
fc
y
12
x
2
x
1
y
23
x
3
y
13
f
c
f
c
f
t
Figure 2: An example noun coreference factor graph
for the Pairwise Model in which factors fc model the
coreference between two nouns, and ft enforce the
transitivity among related decisions. The number of
y variables increases quadratically in the number of
x variables.
ple and one ?nearby? positive example. In particular,
when agglomerative clustering incorrectly merges
two clusters, we select the resulting cluster as the
negative example, and select as the positive example
a cluster that can be created by merging other exist-
ing clusters.1 We then update the weight vector so
that the positive example is assigned a higher score
than the negative example. This approach allows
the update to only penalize the difference between
the two features of examples, thereby not penaliz-
ing features representing any overlapping coreferent
clusters.
To implement this update, we use MIRA (Mar-
gin Infused Relaxed Algorithm), a relaxed, online
maximum margin training algorithm (Crammer and
Singer, 2003). It updates the parameter vector with
two constraints: (1) the positive example must have
a higher score by a given margin, and (2) the change
to ? should be minimal. This second constraint is
to reduce fluctuations in ?. Let s+(?,xj) be the
unnormalized score for the positive example and
s?(?,xk) be the unnormalized score of the neg-
ative example. Each update solves the following
1Of the possible positive examples, we choose the one with
the highest probability under the current model to guard against
large fluctuations in parameter updates
f
c
y
12
x
2
x
1
y
23
x
3
y
13
f
c
f
c
f
t
y
123
f
c
Figure 3: An example noun coreference factor graph
for the First-Order Model in which factors fc model
the coreference between sets of nouns, and ft en-
force the transitivity among related decisions. Here,
the additional node y123 indicates whether nouns
{x1, x2, x3} are all coreferent. The number of y
variables increases exponentially in the number of
x variables.
quadratic program:
?t+1 = argmin
?
||?t ? ?||2
s.t.
s+(?,xj) ? s?(?,xk) ? 1
In this case, MIRA with a single constraint can be
efficiently solved in one iteration of the Hildreth and
D?Esopo method (Censor and Zenios, 1997). Ad-
ditionally, we average the parameters calculated at
each iteration to improve convergence.
We refer to the system described in this section as
First-Order MIRA.
5 Probabilistic Interpretation
In this section, we describe the Pairwise and First-
Order models in terms of the factor graphs they ap-
proximate.
For the Pairwise Model, a corresponding undi-
rected graphical model can be defined as
P (y|x) =
1
Zx
?
yij?y
fc(yij , xij)
?
yij ,yjk?y
ft(yij , yj,k, yik, xij , xjk, xik)
84
where Zx is the input-dependent normalizer and fac-
tor fc parameterizes the pairwise noun phrase com-
patibility as fc(yij , xij) = exp(
?
k ?kfk(yij , xij)).
Factor ft enforces the transitivity constraints by
ft(?) = ?? if transitivity is not satisfied, 1 oth-
erwise. This is similar to the model presented in
McCallum and Wellner (2005). A factor graph for
the Pairwise Model is presented in Figure 2 for three
noun phrases.
For the First-Order model, an undirected graphi-
cal model can be defined as
P (y|x) =
1
Zx
?
yj?y
fc(yj ,xj)
?
yj?y
ft(yj ,xj)
where Zx is the input-dependent nor-
malizer and factor fc parameterizes the
cluster-wise noun phrase compatibility as
fc(yj ,xj) = exp(
?
k ?kfk(yj , x
j)). Again,
factor ft enforces the transitivity constraints by
ft(?) = ?? if transitivity is not satisfied, 1 other-
wise. Here, transitivity is a bit more complicated,
since it also requires that if yj = 1, then for any
subset xk ? xj , yk = 1. A factor graph for the
First-Order Model is presented in Figure 3 for three
noun phrases.
The methods described in Sections 2, 3 and 4 can
be viewed as estimating the parameters of each fac-
tor fc independently. This approach can therefore
be viewed as a type of piecewise approximation of
exact parameter estimation in these models (Sutton
and McCallum, 2005). Here, each fc is a ?piece?
of the model trained independently. These pieces
are combined at prediction time using clustering al-
gorithms to enforce transitivity. Sutton and McCal-
lum (2005) show that such a piecewise approxima-
tion can be theoretically justified as minimizing an
upper bound of the exact loss function.
6 Experiments
6.1 Data
We apply our approach to the noun coreference ACE
2004 data, containing 443 news documents with
28,135 noun phrases to be coreferenced. 336 doc-
uments are used for training, and the remainder for
testing. All entity types are candidates for corefer-
ence (pronouns, named entities, and nominal enti-
ties). We use the true entity segmentation, and parse
each sentence in the corpus using a phrase-structure
grammar, as is common for this task.
6.2 Features
We follow Soon et al (2001) and Ng and Cardie
(2002) to generate most of our features for the Pair-
wise Model. These include:
? Match features - Check whether gender, num-
ber, head text, or entire phrase matches
? Mention type (pronoun, name, nominal)
? Aliases - Heuristically decide if one noun is the
acronym of the other
? Apposition - Heuristically decide if one noun is
in apposition to the other
? Relative Pronoun - Heuristically decide if one
noun is a relative pronoun referring to the other.
? Wordnet features - Use Wordnet to decide if
one noun is a hypernym, synonym, or antonym
of another, or if they share a hypernym.
? Both speak - True if both contain an adjacent
context word that is a synonym of ?said.? This
is a domain-specific feature that helps for many
newswire articles.
? Modifiers Match - for example, in the phrase
?President Clinton?, ?President? is a modifier
of ?Clinton?. This feature indicates if one noun
is a modifier of the other, or they share a modi-
fier.
? Substring - True if one noun is a substring of
the other (e.g. ?Egypt? and ?Egyptian?).
The First-OrderModel includes the following fea-
tures:
? Enumerate each pair of noun phrases and com-
pute the features listed above. All-X is true if
all pairs share a featureX ,Most-True-X is true
if the majority of pairs share a feature X , and
Most-False-X is true if most of the pairs do not
share feature X .
85
? Use the output of the Pairwise Model for each
pair of nouns. All-True is true if all pairs are
predicted to be coreferent, Most-True is true if
most pairs are predicted to be coreferent, and
Most-False is true if most pairs are predicted
to not be coreferent. Additionally, Max-True
is true if the maximum pairwise score is above
threshold, and Min-True if the minimum pair-
wise score is above threshold.
? Cluster Size indicates the size of the cluster.
? Count how many phrases in the cluster are
of each mention type (name, pronoun, nom-
inal), number (singular/plural) and gender
(male/female). The features All-X and Most-
True-X indicate how frequent each feature is
in the cluster. This feature can capture the soft
constraint such that no cluster consists only of
pronouns.
In addition to the listed features, we also include
conjunctions of size 2, for example ?Genders match
AND numbers match?.
6.3 Evaluation
We use the B3 algorithm to evaluate the predicted
coreferent clusters (Amit and Baldwin, 1998). B3
is common in coreference evaluation and is similar
to the precision and recall of coreferent links, ex-
cept that systems are rewarded for singleton clus-
ters. For each noun phrase xi, let ci be the number
of mentions in xi?s predicted cluster that are in fact
coreferent with xi (including xi itself). Precision for
xi is defined as ci divided by the number of noun
phrases in xi?s cluster. Recall for xi is defined as
the ci divided by the number of mentions in the gold
standard cluster for xi. F1 is the harmonic mean of
recall and precision.
6.4 Results
In addition to Pairwise, First-Order Uniform, and
First-Order MIRA, we also compare against Pair-
wise MIRA, which differs from First-Order MIRA
only by the fact that it is restricted to pairwise fea-
tures.
Table 1 suggests both that first-order features and
error-driven training can greatly improve perfor-
mance. The First-OrderModel outperforms the Pair-
F1 Prec Rec
First-Order MIRA 79.3 86.7 73.2
Pairwise MIRA 72.5 92.0 59.8
First-Order Uniform 69.2 79.0 61.5
Pairwise 62.4 62.5 62.3
Table 1: B3 results for ACE noun phrase corefer-
ence. FIRST-ORDER MIRA is our proposed model
that takes advantage of first-order features of the
data and is trained with error-driven and rank-based
methods. We see that both the first-order features
and the training enhancements improve performance
consistently.
wise Model in F1 measure for both standard train-
ing and error-driven training. We attribute some of
this improvement to the capability of the First-Order
model to capture features of entire clusters that may
indicate some phrases are not coreferent. Also, we
attribute the gains from error-driven training to the
fact that training examples are generated based on
errors made on the training data. (However, we
should note that there are also small differences in
the feature sets used for error-driven and standard
training results.)
Error analysis indicates that often noun xi is cor-
rectly not merged with a cluster xj when xj has a
strong internal coherence. For example, if all 5 men-
tions of France in a document are string identical,
then the system will be extremely cautious of merg-
ing a noun that is not equivalent to France into xj ,
since this will turn off the ?All-String-Match? fea-
ture for cluster xj .
To our knowledge, the best results on this dataset
were obtained by the meta-classification scheme of
Ng (2005). Although our train-test splits may differ
slightly, the best B-Cubed F1 score reported in Ng
(2005) is 69.3%, which is considerably lower than
the 79.3% obtained with our method. Also note that
the Pairwise baseline obtains results similar to those
in Ng and Cardie (2002).
7 Related Work
There has been a recent interest in training methods
that enable the use of first-order features (Paskin,
2002; Daume? III and Marcu, 2005b; Richardson
and Domingos, 2006). Perhaps the most related is
86
?learning as search optimization? (LASO) (Daume?
III and Marcu, 2005b; Daume? III and Marcu,
2005a). Like the current paper, LASO is also an
error-driven training method that integrates predic-
tion and training. However, whereas we explic-
itly use a ranking-based loss function, LASO uses
a binary classification loss function that labels each
candidate structure as correct or incorrect. Thus,
each LASO training example contains all candidate
predictions, whereas our training examples contain
only the highest scoring incorrect prediction and the
highest scoring correct prediction. Our experiments
show the advantages of this ranking-based loss func-
tion. Additionally, we provide an empirical study to
quantify the effects of different example generation
and loss function decisions.
Collins and Roark (2004) present an incremental
perceptron algorithm for parsing that uses ?early up-
date? to update the parameters when an error is en-
countered. Our method uses a similar ?early update?
in that training examples are only generated for the
first mistake made during prediction. However, they
do not investigate rank-based loss functions.
Others have attempted to train global scoring
functions using Gibbs sampling (Finkel et al, 2005),
message propagation, (Bunescu and Mooney, 2004;
Sutton and McCallum, 2004), and integer linear pro-
gramming (Roth and Yih, 2004). The main distinc-
tions of our approach are that it is simple to imple-
ment, not computationally intensive, and adaptable
to arbitrary loss functions.
There have been a number of machine learning
approaches to coreference resolution, traditionally
factored into classification decisions over pairs of
nouns (Soon et al, 2001; Ng and Cardie, 2002).
Nicolae and Nicolae (2006) combine pairwise clas-
sification with graph-cut algorithms. Luo et al
(2004) do enable features between mention-cluster
pairs, but do not perform the error-driven and rank-
ing enhancements proposed in our work. Denis and
Baldridge (2007) use a ranking loss function for pro-
noun coreference; however the examples are still
pairs of pronouns, and the example generation is not
error driven. Ng (2005) learns a meta-classifier to
choose the best prediction from the output of sev-
eral coreference systems. While in theory a meta-
classifier can flexibly represent features, they do not
explore features using the full flexibility of first-
order logic. Also, their method is neither error-
driven nor rank-based.
McCallum and Wellner (2003) use a conditional
random field that factors into a product of pairwise
decisions about pairs of nouns. These pairwise de-
cisions are made collectively using relational infer-
ence; however, as pointed out in Milch et al (2004),
this model has limited representational power since
it does not capture features of entities, only of pairs
of mention. Milch et al (2005) address these issues
by constructing a generative probabilistic model,
where noun clusters are sampled from a generative
process. Our current work has similar representa-
tional flexibility as Milch et al (2005) but is discrim-
inatively trained.
8 Conclusions and Future Work
We have presented learning and inference proce-
dures for coreference models using first-order fea-
tures. By relying on sampling methods at training
time and approximate inference methods at testing
time, this approach can be made scalable. This re-
sults in a coreference model that can capture features
over sets of noun phrases, rather than simply pairs of
noun phrases.
This is an example of a model with extremely
flexible representational power, but for which exact
inference is intractable. The simple approximations
we have described here have enabled this more flex-
ible model to outperform a model that is simplified
for tractability.
A short-term extension would be to consider fea-
tures over entire clusterings, such as the number of
clusters. This could be incorporated in a ranking
scheme, as in Ng (2005).
Future work will extend our approach to a wider
variety of tasks. The model we have described here
is specific to clustering tasks; however a similar for-
mulation could be used to approach a number of lan-
guage processing tasks, such as parsing and relation
extraction. These tasks could benefit from first-order
features, and the present work can guide the approx-
imations required in those domains.
Additionally, we are investigating more sophis-
ticated inference algorithms that will reduce the
greediness of the search procedures described here.
87
Acknowledgments
We thank Robert Hall for helpful contributions. This work
was supported in part by the Defense Advanced Research
Projects Agency (DARPA), through the Department of the
Interior, NBC, Acquisition Services Division, under con-
tract #NBCHD030010, in part by U.S. Government contract
#NBCH040171 through a subcontract with BBNT Solutions
LLC, in part by The Central Intelligence Agency, the National
Security Agency and National Science Foundation under NSF
grant #IIS-0326249, in part by Microsoft Live Labs, and in part
by the Defense Advanced Research Projects Agency (DARPA)
under contract #HR0011-06-C-0023. Any opinions, findings
and conclusions or recommendations expressed in this mate-
rial are the author(s)? and do not necessarily reflect those of the
sponsor.
References
B. Amit and B. Baldwin. 1998. Algorithms for scoring coref-
erence chains. In Proceedings of the Seventh Message Un-
derstanding Conference (MUC7).
Razvan Bunescu and Raymond J. Mooney. 2004. Collective
information extraction with relational markov networks. In
ACL.
Y. Censor and S.A. Zenios. 1997. Parallel optimization : the-
ory, algorithms, and applications. Oxford University Press.
Michael Collins and Brian Roark. 2004. Incremental parsing
with the perceptron algorithm. In ACL.
Koby Crammer and Yoram Singer. 2003. Ultraconservative
online algorithms for multiclass problems. JMLR, 3:951?
991.
Aron Culotta and Andrew McCallum. 2006. Tractable learn-
ing and inference with high-order representations. In ICML
Workshop on Open Problems in Statistical Relational Learn-
ing, Pittsburgh, PA.
Hal Daume? III and Daniel Marcu. 2005a. A large-scale explo-
ration of effective global features for a joint entity detection
and tracking model. In HLT/EMNLP, Vancouver, Canada.
Hal Daume? III and Daniel Marcu. 2005b. Learning as search
optimization: Approximate large margin methods for struc-
tured prediction. In ICML, Bonn, Germany.
Rodrigo de Salvo Braz, Eyal Amir, and Dan Roth. 2005. Lifted
first-order probabilistic inference. In IJCAI, pages 1319?
1325.
Pascal Denis and Jason Baldridge. 2007. A ranking approach
to pronoun resolution. In IJCAI.
Jenny Rose Finkel, Trond Grenager, and Christopher Manning.
2005. Incorporating non-local information into information
extraction systems by gibbs sampling. In ACL, pages 363?
370.
H. Gaifman. 1964. Concerning measures in first order calculi.
Israel J. Math, 2:1?18.
J. Y. Halpern. 1990. An analysis of first-order logics of proba-
bility. Artificial Intelligence, 46:311?350.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kamb-
hatla, and Salim Roukos. 2004. A mention-synchronous
coreference resolution algorithm based on the Bell tree. In
ACL, page 135.
A. McCallum and B. Wellner. 2003. Toward conditional mod-
els of identity uncertainty with application to proper noun
coreference. In IJCAI Workshop on Information Integration
on the Web.
Andrew McCallum and Ben Wellner. 2005. Conditional mod-
els of identity uncertainty with application to noun corefer-
ence. In Lawrence K. Saul, Yair Weiss, and Le?on Bottou,
editors, NIPS17. MIT Press, Cambridge, MA.
Brian Milch, Bhaskara Marthi, and Stuart Russell. 2004.
BLOG: Relational modeling with unknown objects. In
ICML 2004 Workshop on Statistical Relational Learning and
Its Connections to Other Fields.
Brian Milch, Bhaskara Marthi, Stuart Russell, David Sontag,
Daniel L. Ong, and Andrey Kolobov. 2005. BLOG: Proba-
bilistic models with unknown objects. In IJCAI.
Vincent Ng and Claire Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In ACL.
Vincent Ng. 2005. Machine learning for coreference resolu-
tion: From local classification to global ranking. In ACL.
Cristina Nicolae and Gabriel Nicolae. 2006. Bestcut: A graph
algorithm for coreference resolution. In EMNLP, pages
275?283, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Mark A. Paskin. 2002. Maximum entropy probabilistic logic.
Technical Report UCB/CSD-01-1161, University of Califor-
nia, Berkeley.
D. Poole. 2003. First-order probabilistic inference. In IJCAI,
pages 985?991, Acapulco, Mexico. Morgan Kaufman.
Matthew Richardson and Pedro Domingos. 2006. Markov
logic networks. Machine Learning, 62:107?136.
D. Roth and W. Yih. 2004. A linear programming formulation
for global inference in natural language tasks. In The 8th
Conference on Compuational Natural Language Learning,
May.
Parag Singla and Pedro Domingos. 2005. Discriminative train-
ing of markov logic networks. In AAAI, Pittsburgh, PA.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim.
2001. A machine learning approach to coreference resolu-
tion of noun phrases. Comput. Linguist., 27(4):521?544.
Charles Sutton and Andrew McCallum. 2004. Collective seg-
mentation and labeling of distant entities in information ex-
traction. Technical Report TR # 04-49, University of Mas-
sachusetts, July.
Charles Sutton and Andrew McCallum. 2005. Piecewise train-
ing of undirected models. In 21st Conference on Uncertainty
in Artificial Intelligence.
88
Proceedings of NAACL HLT 2007, Companion Volume, pages 109?112,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Efficient Computation of Entropy Gradient for
Semi-Supervised Conditional Random Fields
Gideon S. Mann and Andrew McCallum
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
gideon.mann@gmail.com , mccallum@cs.umass.edu
Abstract
Entropy regularization is a straightforward
and successful method of semi-supervised
learning that augments the traditional con-
ditional likelihood objective function with
an additional term that aims to minimize
the predicted label entropy on unlabeled
data. It has previously been demonstrated
to provide positive results in linear-chain
CRFs, but the published method for cal-
culating the entropy gradient requires sig-
nificantly more computation than super-
vised CRF training. This paper presents
a new derivation and dynamic program
for calculating the entropy gradient that
is significantly more efficient?having the
same asymptotic time complexity as su-
pervised CRF training. We also present
efficient generalizations of this method
for calculating the label entropy of all
sub-sequences, which is useful for active
learning, among other applications.
1 Introduction
Semi-supervised learning is of growing importance
in machine learning and NLP (Zhu, 2005). Condi-
tional random fields (CRFs) (Lafferty et al, 2001)
are an appealing target for semi-supervised learning
because they achieve state-of-the-art performance
across a broad spectrum of sequence labeling tasks,
and yet, like many other machine learning methods,
training them by supervised learning typically re-
quires large annotated data sets.
Entropy regularization (ER) is a method of semi-
supervised learning first proposed for classification
tasks (Grandvalet and Bengio, 2004). In addition to
maximizing conditional likelihood of the available
labels, ER also aims to minimize the entropy of the
predicted label distribution on unlabeled data. By in-
sisting on peaked, confident predictions, ER guides
the decision boundary away from dense regions of
input space. It is simple and compelling?no pre-
clustering, no ?auxiliary functions,? tuning of only
one meta-parameter and it is discriminative.
Jiao et al (2006) apply this method to linear-
chain CRFs and demonstrate encouraging accuracy
improvements on a gene-name-tagging task. How-
ever, the method they present for calculating the
gradient of the entropy takes substantially greater
time than the traditional supervised-only gradient.
Whereas supervised training requires only classic
forward/backward, taking time O(ns2) (sequence
length times the square of the number of labels),
their training method takes O(n2s3)?a factor of
O(ns) more. This greatly reduces the practicality
of using large amounts of unlabeled data, which is
exactly the desired use-case.
This paper presents a new, more efficient entropy
gradient derivation and dynamic program that has
the same asymptotic time complexity as the gradient
for traditional CRF training, O(ns2). In order to de-
scribe this calculation, the paper introduces the con-
cept of subsequence constrained entropy?the en-
tropy of a CRF for an observed data sequence when
part of the label sequence is fixed. These meth-
ods will allow training on larger unannotated data
set sizes than previously possible and support active
109
learning.
2 Semi-Supervised CRF Training
Lafferty et al (2001) present linear-chain CRFs, a
discriminative probabilistic model over observation
sequences x and label sequences Y = ?Y1..Yn?,
where |x| = |Y | = n, and each label Yi has s differ-
ent possible discrete values. For a linear-chain CRF
of Markov order one:
p?(Y |x) =
1
Z(x)
exp
(
?
k
?kFk(x, Y )
)
,
where Fk(x, Y ) =
?
i fk(x, Yi, Yi+1, i),
and the partition function Z(x) =
?
Y exp(
?
k ?kFk(x, Y )). Given training
data D = ?d1..dn?, the model is trained by
maximizing the log-likelihood of the data
L(?;D) =
?
d log p?(Y
(d)|x(d)) by gradient
methods (e.g. Limited Memory BFGS), where the
gradient of the likelihood is:
?
??k
L(?;D) =
?
d
Fk(x
(d), Y (d))
?
?
d
?
Y
p?(Y |x
(d))Fk(x
(d), Y ).
The second term (the expected counts of the features
given the model) can be computed in a tractable
amount of time, since according to the Markov as-
sumption, the feature expectations can be rewritten:
?
Y
p?(Y |x)Fk(x, Y ) =
?
i
?
Yi,Yi+1
p?(Yi, Yi+1|x)fk(x, Yi, Yi+1).
A dynamic program (the forward/backward algo-
rithm) then computes in time O(ns2) all the needed
probabilities p?(Yi, Yi+1), where n is the sequence
length, and s is the number of labels.
For semi-supervised training by entropy regular-
ization, we change the objective function by adding
the negative entropy of the unannotated data U =
?u1..un?. (Here Gaussian prior is also shown.)
L(?;D,U) =
?
n
log p?(Y
(d)|x(d)) ?
?
k
?k
2?2
+ ?
?
u
p?(Y
(u)|x(u)) log p?(Y
(u)|x(u)).
This negative entropy term increases as the decision
boundary is moved into sparsely-populated regions
of input space.
3 An Efficient Form of the Entropy
Gradient
In order to maximize the above objective function,
the gradient for the entropy term must be computed.
Jiao et al (2006) perform this computation by:
?
??
? H(Y |x) = covp?(Y |x)[F (x, Y )]?,
where
covp?(Y |x)[Fj(x, Y ), Fk(x, Y )] =
Ep?(Y |x)[Fj(x, Y ), Fk(x, Y )]
? Ep?(Y |x)[Fj(x, Y )]Ep?(Y |x)[Fk(x, Y )].
While the second term of the covariance is easy
to compute, the first term requires calculation of
quadratic feature expectations. The algorithm they
propose to compute this term is O(n2s3) as it re-
quires an extra nested loop in forward/backward.
However, the above form of the gradient is not
the only possibility. We present here an alternative
derivation of the gradient:
?
??k
?H(Y |x) =
?
??k
X
Y
p?(Y |x) log p?(Y |x)
=
X
Y
?
?
??k
p?(Y |x)
?
log p?(Y |x)
+ p?(Y |x)
?
?
??k
log p?(Y |x)
?
=
X
Y
p?(Y |x) log p?(Y |x)
?
 
Fk(x, Y ) ?
X
Y ?
p?(Y
?|x)Fk(x, Y
?)
!
+
X
Y
p?(Y |x)
 
Fk(x, Y ) ?
X
Y ?
p?(Y
?|x)Fk(x, Y
?)
!
.
Since
?
Y p?(Y |x)
?
Y ? p?(Y
?|X)Fk(x, Y ?) =?
Y ? p?(Y
?|X)Fk(x, Y ?), the second summand can-
cels, leaving:
?
??
?H(Y |x) =
X
Y
p?(Y |x) log p?(Y |x)Fk(x, Y )
?
 
X
Y
p?(Y |x) log p?(Y |x)
! 
X
Y ?
p?(Y
?|x)Fk(x, Y
?)
!
.
Like the gradient obtained by Jiao et al (2006),
there are two terms, and the second is easily com-
putable given the feature expectations obtained by
110
forward/backward and the entropy for the sequence.
However, unlike the previous method, here the first
term can be efficiently calculated as well. First,
the term must be further factored into a form more
amenable to analysis:
?
Y
p?(Y |x) log p?(Y |x)Fk(x, Y )
=
?
Y
p?(Y |x) log p?(Y |x)
?
i
fk(x, Yi, Yi+1, i)
=
?
i
?
Yi,Yi+1
fk(x, Yi, Yi+1, i)
?
Y?(i..i+1)
p?(Y |x) log p?(Y |x).
Here, Y?(i..i+1) = ?Y1..(i?1)Y(i+2)..n?. In order
to efficiently calculate this term, it is sufficient
to calculate
?
Y?(i..i+1)
p?(Y |x) log p?(Y |x) for all
pairs yi, yi+1. The next section presents a dynamic
program which can perform these computations in
O(ns2).
4 Subsequence Constrained Entropy
We define subsequence constrained entropy as
H?(Y?(a..b)|ya..b, x) =
?
Y?(a..b)
p?(Y |x) log p?(Y |x).
The key to the efficient calculation for all subsets
is to note that the entropy can be factored given a
linear-chain CRF of Markov order 1, since Yi+2 is
independent of Yi given Yi+1.
?
Y?(a..b)
p?(Y?(a..b), ya..b|x) log p?(Y?(a..b), ya..b|x)
=
?
Y?(a..b)
p?(ya..b|x)p?(Y?(a..b)|ya..b, x)?
(
log p?(ya..b|x) + log p?(Y?(a..b)|ya..b, x)
)
=p?(ya..b|x) log p?(ya..b|x)
+ p?(ya..b|x)H
?(Y?(a..b)|ya..b, x)
=p?(ya..b|x) log p?(ya..b|x)
+ p?(ya..b|x)H
?(Y1..(a?1)|ya, x)
+ p?(ya..b|x)H
?(Y(b+1)..n|yb, x).
Given the H?(?) and H?(?) lattices, any sequence
entropy can be computed in constant time. Figure 1
H (0|y6)H (Y6|y5)H (0|y1) H (Y1|y2)
y4
y3
? ? ? ?
Figure 1: Partial lattice shown for com-
puting the subsequence constrained entropy:P
Y p(Y?(3..4), y3, y4) log p(Y?(3..4), y3, y4). Once the
complete H? and H? lattices are constructed (in the direction
of the arrows), the entropy for each label sequence can be
computed in linear time.
illustrates an example in which the constrained se-
quence is of size two, but the method applies to
arbitrary-length contiguous label sequences.
Computing the H?(?) and H?(?) lattices is easily
performed using the probabilities obtained by for-
ward/backward. First recall the decomposition for-
mulas for entropy:
H(X,Y ) = H(X) + H(Y |X)
H(Y |X) =
?
x
P (X = x)H(Y |X = x).
Using this decomposition, we can define a dynamic
program over the entropy lattices similar to for-
ward/backward:
H?(Y1..i|yi+1, x)
=H(Yi|yi+1, x) + H(Y1..(i?1)|Yi, yi+1, x)
=
?
yi
p?(yi|yi+1, x) log p?(yi|yi+1, x)
+
?
yi
p?(yi|yi+1, x)H
?(Y1..(i?1)|yi).
The base case for the dynamic program is
H?(?|y1) = p(y1) log p(y1). The backward entropy
is computed in a similar fashion. The conditional
probabilities p?(yi|yi?1, x) in each of these dynamic
programs are available by marginalizing over the
per-transition marginal probabilities obtained from
forward/backward.
The computational complexity of this calcula-
tion for one label sequence requires one run of for-
ward/backward at O(ns2), and equivalent time to
111
calculate the lattices for H? and H? . To calculate
the gradient requires one final iteration over all label
pairs at each position, which is again time O(ns2),
but no greater, as forward/backward and the en-
tropy calculations need only to be done once. The
complete asymptotic computational cost of calcu-
lating the entropy gradient is O(ns2), which is the
same time as supervised training, and a factor of
O(ns) faster than the method proposed by Jiao et
al. (2006).
Wall clock timing experiments show that this
method takes approximately 1.5 times as long as
traditional supervised training?less than the con-
stant factors would suggest.1 In practice, since the
three extra dynamic programs do not require re-
calculation of the dot-product between parameters
and input features (typically the most expensive part
of inference), they are significantly faster than cal-
culating the original forward/backward lattice.
5 Confidence Estimation
In addition to its merits for computing the entropy
gradient, subsequence constrained entropy has other
uses, including confidence estimation. Kim et al
(2006) propose using entropy as a confidence esti-
mator in active learning in CRFs, where examples
with the most uncertainty are selected for presenta-
tion to humans labelers. In practice, they approxi-
mate the entropy of the labels given the N-best la-
bels. Not only could our method quickly and ex-
actly compute the true entropy, but it could also be
used to find the subsequence that has the highest un-
certainty, which could further reduce the additional
human tagging effort.
6 Related Work
Hernando et al (2005) present a dynamic program
for calculating the entropy of a HMM, which has
some loose similarities to the forward pass of the
algorithm proposed in this paper. Notably, our algo-
rithm allows for efficient calculation of entropy for
any label subsequence.
Semi-supervised learning has been used in many
models, predominantly for classification, as opposed
to structured output models like CRFs. Zhu (2005)
1Reporting experimental results with accuracy is unneces-
sary since we duplicate the training method of Jiao et al (2006).
provides a comprehensive survey of popular semi-
supervised learning techniques.
7 Conclusion
This paper presents two algorithmic advances. First,
it introduces an efficient method for calculating
subsequence constrained entropies in linear-chain
CRFs, (useful for active learning). Second, it
demonstrates how these subsequence constrained
entropies can be used to efficiently calculate the
gradient of the CRF entropy in time O(ns2)?
the same asymptotic time complexity as the for-
ward/backward algorithm, and a O(ns) improve-
ment over previous algorithms?enabling the prac-
tical application of CRF entropy regularization to
large unlabeled data sets.
Acknowledgements
This work was supported in part by DoD contract #HM1582-
06-1-2013, in part by The Central Intelligence Agency, the Na-
tional Security Agency and National Science Foundation under
NSF grant #IIS-0427594, and in part by the Defense Advanced
Research Projects Agency (DARPA), through the Department
of the Interior, NBC, Acquisition Services Division, under con-
tract number NBCHD030010. Any opinions, findings and con-
clusions or recommendations expressed in this material belong
to the author(s) and do not necessarily reflect those of the spon-
sor.
References
Y. Grandvalet and Y. Bengio. 2004. Semi-supervised learning
by entropy minimization. In NIPS.
D. Hernando, V. Crespi, and G. Cybenko. 2005. Efficient com-
putation of the hidden markov model entropy for a given
observation sequence. IEEE Trans. on Information Theory,
51:7:2681?2685.
F. Jiao, S. Wang, C.-H. Lee, R. Greiner, and D. Schuur-
mans. 2006. Semi-supervised conditional random fields
for improved sequence segmentation and labeling. In COL-
ING/ACL.
S. Kim, Y. Song, K. Kim, J.-W. Cha, and G. G. Lee. 2006.
Mmr-based active machine learning for bio named entity
recognition. In HLT/NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proceedings of ICML, pages 282?
289.
X. Zhu. 2005. Semi-supervised learning literature survey.
Technical Report 1530, Computer Sciences, University of
Wisconsin-Madison.
112
Proceedings of ACL-08: HLT, pages 870?878,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Generalized Expectation Criteria for Semi-Supervised Learning of
Conditional Random Fields
Gideon S. Mann
Google Inc.
76 Ninth Avenue
New York, NY 10011
Andrew McCallum
Department of Computer Science
University of Massachusetts
140 Governors Drive
Amherst, MA 01003
Abstract
This paper presents a semi-supervised train-
ing method for linear-chain conditional ran-
dom fields that makes use of labeled features
rather than labeled instances. This is accom-
plished by using generalized expectation cri-
teria to express a preference for parameter set-
tings in which the model?s distribution on un-
labeled data matches a target distribution. We
induce target conditional probability distribu-
tions of labels given features from both anno-
tated feature occurrences in context and ad-
hoc feature majority label assignment. The
use of generalized expectation criteria allows
for a dramatic reduction in annotation time
by shifting from traditional instance-labeling
to feature-labeling, and the methods presented
outperform traditional CRF training and other
semi-supervised methods when limited human
effort is available.
1 Introduction
A significant barrier to applying machine learning
to new real world domains is the cost of obtaining
the necessary training data. To address this prob-
lem, work over the past several years has explored
semi-supervised or unsupervised approaches to the
same problems, seeking to improve accuracy with
the addition of lower cost unlabeled data. Tradi-
tional approaches to semi-supervised learning are
applied to cases in which there is a small amount of
fully labeled data and a much larger amount of un-
labeled data, presumably from the same data source.
For example, EM (Nigam et al, 1998), transduc-
tive SVMs (Joachims, 1999), entropy regularization
(Grandvalet and Bengio, 2004), and graph-based
address          :            *number*           oak            avenue          rent             $
ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS RENT RENT
Traditional Full Instance Labeling
ADDRESS
address : *number* oak avenue rent $ ....
CONTACT
.. ( please include the address of this rental )
ADDRESS
... pm . address : *number* marie street sausalito ...
ADDRESS
.. laundry . address : *number* macarthur blvd ....
Feature Labeling
ConditionalDistribution
of LabelsGiven Word=address
ADDRESS
CONTACT
Figure 1: Top: Traditional instance-labeling in which se-
quences of contiguous tokens are annotated as to their
correct label. Bottom: Feature-labeling in which non-
contiguous feature occurrences in context are labeled for
the purpose of deriving a conditional probability distribu-
tion of labels given a particular feature.
methods (Zhu and Ghahramani, 2002; Szummer and
Jaakkola, 2002) have all been applied to a limited
amount of fully labeled data in conjunction with un-
labeled data to improve the accuracy of a classifier.
In this paper, we explore an alternative approach
in which, instead of fully labeled instances, the
learner has access to labeled features. These fea-
tures can often be labeled at a lower-cost to the hu-
man annotator than labeling entire instances, which
may require annotating the multiple sub-parts of a
sequence structure or tree. Features can be labeled
either by specifying the majority label for a partic-
ular feature or by annotating a few occurrences of
a particular feature in context with the correct label
(Figure 1).
To train models using this information we use
870
generalized expectation (GE) criteria. GE criteria
are terms in a training objective function that as-
sign scores to values of a model expectation. In
particular we use a version of GE that prefers pa-
rameter settings in which certain model expectations
are close to target distributions. Previous work has
shown how to apply GE criteria to maximum en-
tropy classifiers. In section 4, we extend GE crite-
ria to semi-supervised learning of linear-chain con-
ditional random fields, using conditional probability
distributions of labels given features.
To empirically evaluate this method we compare
it with several competing methods for CRF train-
ing, including entropy regularization and expected
gradient, showing that GE provides significant im-
provements. We achieve competitive performance
in comparison to alternate model families, in partic-
ular generative models such as MRFs trained with
EM (Haghighi and Klein, 2006) and HMMs trained
with soft constraints (Chang et al, 2007). Finally, in
Section 5.3 we show that feature-labeling can lead to
dramatic reductions in the annotation time that is re-
quired in order to achieve the same level of accuracy
as traditional instance-labeling.
2 Related Work
There has been a significant amount of work on
semi-supervised learning with small amounts of
fully labeled data (see Zhu (2005)). However there
has been comparatively less work on learning from
alternative forms of labeled resources. One exam-
ple is Schapire et al (2002) who present a method
in which features are annotated with their associated
majority labels and this information is used to boot-
strap a parameterized text classification model. Un-
like the model presented in this paper, they require
some labeled data in order to train their model.
This type of input information (features + major-
ity label) is a powerful and flexible model for spec-
ifying alternative inputs to a classifier, and has been
additionally used by Haghighi and Klein (2006). In
that work, ?prototype? features?words with their
associated labels?are used to train a generative
MRF sequence model. Their probability model can
be formally described as:
p?(x,y) =
1
Z(?)
exp
(
?
k
?kFk(x,y)
)
.
Although the partition function must be computed
over all (x,y) tuples, learning via EM in this model
is possible because of approximations made in com-
puting the partition function.
Another way to gather supervision is by means
of prior label distributions. Mann and McCallum
(2007) introduce a special case of GE, label regular-
ization, and demonstrate its effectiveness for train-
ing maximum entropy classifiers. In label regu-
larization, the model prefers parameter settings in
which the model?s predicted label distribution on the
unsupervised data match a target distribution. Note
that supervision here consists of the the full distribu-
tion over labels (i.e. conditioned on the maximum
entropy ?default feature?), instead of simply the ma-
jority label. Druck et al (2007) also use GE with full
distributions for semi-supervised learning of maxi-
mum entropy models, except here the distributions
are on labels conditioned on features. In Section 4
we describe how GE criteria can be applied to CRFs
given conditional probability distributions of labels
given features.
Another recent method that has been proposed for
training sequence models with constraints is Chang
et al (2007). They use constraints for approximate
EM training of an HMM, incorporating the con-
straints by looking only at the top K most-likely
sequences from a joint model of likelihood and the
constraints. This model can be applied to the combi-
nation of labeled and unlabeled instances, but cannot
be applied in situations where only labeled features
are available. Additionally, our model can be easily
combined with other semi-supervised criteria, such
as entropy regularization. Finally, their model is a
generative HMM which cannot handle the rich, non-
independent feature sets that are available to a CRF.
There have been relatively few different ap-
proaches to CRF semi-supervised training. One ap-
proach has been that proposed in both Miller et al
(2004) and Freitag (2004), uses distributional clus-
tering to induce features from a large corpus, and
then uses these features to augment the feature space
of the labeled data. Since this is an orthogonal
method for improving accuracy it can be combined
with many of the other methods discussed above,
and indeed we have obtained positive preliminary
experimental results with GE criteria (not reported
on here).
871
Another method for semi-supervised CRF train-
ing is entropy regularization, initially proposed by
Grandvalet and Bengio (2004) and extended to
linear-chain CRFs by Jiao et al (2006). In this for-
mulation, the traditional label likelihood (on super-
vised data) is augmented with an additional term that
encourages the model to predict low-entropy label
distributions on the unlabeled data:
O(?;D,U) =
?
d
log p?(y
(d)|x(d))? ?H(y|x).
This method can be quite brittle, since the minimal
entropy solution assigns all of the tokens the same
label.1 In general, entropy regularization is fragile,
and accuracy gains can come only with precise set-
tings of ?. High values of ? fall into the minimal
entropy trap, while low values of ? have no effect on
the model (see (Jiao et al, 2006) for an example).
When some instances have partial labelings (i.e.
labels for some of their tokens), it is possible to train
CRFs via expected gradient methods (Salakhutdinov
et al, 2003). Here a reformulation is presented in
which the gradient is computed for a probability dis-
tribution with a marginalized hidden variable, z, and
observed training labels y:
?L(?) =
?
??
?
z
log p(x, y, z; ?)
=
?
z
p(z|y, x)fk(x, y, z)
?
?
z,y?
p(z, y?|x; ?)fk(x, y, z).
In essence, this resembles the standard gradient for
the CRF, except that there is an additional marginal-
ization in the first term over the hidden variable z.
This type of training has been applied by Quattoni
et al (2007) for hidden-state conditional random
fields, and can be equally applied to semi-supervised
conditional random fields. Note, however, that la-
beling variables of a structured instance (e.g. to-
kens) is different than labeling features?being both
more coarse-grained and applying supervision nar-
rowly only to the individual subpart, not to all places
in the data where the feature occurs.
1In the experiments in this paper, we use ? = 0.001, which
we tuned for best performance on the test set, giving an unfair
advantage to our competitor.
Finally, there are some methods that use auxil-
iary tasks for training sequence models, though they
do not train linear-chain CRFs per se. Ando and
Zhang (2005) include a cluster discovery step into
the supervised training. Smith and Eisner (2005)
use neighborhoods of related instances to figure out
what makes found instances ?good?. Although these
methods can often find good solutions, both are quite
sensitive to the selection of auxiliary information,
and making good selections requires significant in-
sight.2
3 Conditional Random Fields
Linear-chain conditional random fields (CRFs) are a
discriminative probabilistic model over sequences x
of feature vectors and label sequences y = ?y1..yn?,
where |x| = |y| = n, and each label yi has s dif-
ferent possible discrete values. This model is anal-
ogous to maximum entropy models for structured
outputs, where expectations can be efficiently calcu-
lated by dynamic programming. For a linear-chain
CRF of Markov order one:
p?(y|x) =
1
Z(x)
exp
(
?
k
?kFk(x,y)
)
,
where Fk(x,y) =
?
i fk(x, yi, yi+1, i),
and the partition function Z(x) =
?
y exp(
?
k ?kFk(x,y)). Given training data
D =
?
(x(1),y(1))..(x(n),y(n))
?
, the model is tra-
ditionally trained by maximizing the log-likelihood
O(?;D) =
?
d log p?(y
(d)|x(d)) by gradient ascent
where the gradient of the likelihood is:
?
??k
O(?;D) =
?
d
Fk(x
(d),y(d))
?
?
d
?
y
p?(y|x
(d))Fk(x
(d),y).
The second term (the expected counts of the features
given the model) can be computed in a tractable
amount of time, since according to the Markov as-
2Often these are more complicated than picking informative
features as proposed in this paper. One example of the kind of
operator used is the transposition operator proposed by Smith
and Eisner (2005).
872
sumption, the feature expectations can be rewritten:
?
y
p?(y|x)Fk(x,y) =
?
i
?
yi,yi+1
p?(yi, yi+1|x)fk(x, yi, yi+1, i).
A dynamic program (the forward/backward algo-
rithm) then computes in time O(ns2) all the needed
probabilities p?(yi, yi+1), where n is the sequence
length, and s is the number of labels.
4 Generalized Expectation Criteria for
Conditional Random Fields
Prior semi-supervised learning methods have aug-
mented a limited amount of fully labeled data with
either unlabeled data or with constraints (e.g. fea-
tures marked with their majority label). GE crite-
ria can use more information than these previous
methods. In particular GE criteria can take advan-
tage of conditional probability distributions of la-
bels given a feature (p(y|fk(x) = 1)). This in-
formation provides richer constraints to the model
while remaining easily interpretable. People have
good intuitions about the relative predictive strength
of different features. For example, it is clear that
the probability of label PERSON given the feature
WORD=JOHN is high, perhaps around 0.95, where
as for WORD=BROWN it would be lower, perhaps
0.4. These distributions need not be not estimated
with great precision?it is far better to have the free-
dom to express shades of gray than to be force into
a binary supervision signal. Another advantage of
using conditional probability distributions as prob-
abilistic constraints is that they can be easily esti-
mated from data. For the feature INITIAL-CAPITAL,
we identify all tokens with the feature, and then
count the labels with which the feature co-occurs.
GE criteria attempt to match these conditional
probability distributions by model expectations on
unlabeled data, encouraging, for example, the model
to predict that the proportion of the label PERSON
given the word ?john? should be .95 over all of the
unlabeled data.
In general, a GE (generalized expectation) crite-
rion (McCallum et al, 2007) expresses a preference
on the value of a model expectation. One kind of
preference may be expressed by a distance function
?, a target expectation f? , data D, a function f , and
a model distribution p?, the GE criterion objective
function term is ?
(
f? , E[f(x)]
)
. For the purposes
of this paper, we set the functions to be conditional
probability distributions and set ?(p, q) = D(p||q),
the KL-divergence between two distributions.3 For
semi-supervised training of CRFs, we augment the
objective function with the regularization term:
O(?;D,U) =
?
d
log p?(y
(d)|x(d))?
?
k ?k
2?2
? ?D(p?||p??),
where p? is given as a target distribution and
p?? = p??(yj |fm(x, j) = 1)
=
1
Um
?
x?Um
?
j?
p?(y
?
j |x),
with the unnormalized potential
q?? = q??(yj |fm(x, j) = 1) =
?
x?Um
?
j?
p?(y
?
j |x),
where fm(x, j) is a feature that depends only on
the observation sequence x, and j? is defined as
{j : fm(x, j) = 1}, and Um is the set of sequences
where fm(x, j) is present for some j.4
Computing the Gradient
To compute the gradient of the GE criteria,
D(p?||p??), first we drop terms that are constant with
respect to the partial derivative, and we derive the
gradient as follows:
?
??k
?
l
p? log q?? =
?
l
p?
q??
?
??k
q??
=
?
l
p?
q??
?
x?U
?
j?
?
??k
p?(yj? = l|x)
=
?
l
p?
q??
?
x?U
?
j?
?
y?j?
?
??k
p?(yj? = l,y?j? |x),
where y?j = ?y1..(j?1)y(j+1)..n?. The last step fol-
lows from the definition of the marginal probability
3We are actively investigating different choices of distance
functions which may have different generalization properties.
4This formulation assumes binary features.
873
P (yj |x). Now that we have a familiar form in which
we are taking the gradient of a particular label se-
quence, we can continue:
=
?
l
p?
q??
?
x?U
?
j?
?
y?j?
p?(yj? = l,y?j? |x)Fk(x,y)
?
?
l
p?
q??
?
x?U
?
j?
?
y?j?
p?(yj? = l,y?j? |x)
?
y?
p?(y
?|x)Fk(x,y)
=
?
l
p?
q??
?
x?U
?
i
?
yi,yi+1
fk(x, yi, yi+1, i)
?
j?
p?(yi, yi+1, yj? = l|x)
?
?
l
p?
q??
?
x?U
?
i
?
yi,yi+1
fk(x, yi, yi+1, i)
p?(yi, yi+1|x)
?
j?
p?(yj? = l|x).
After combining terms and rearranging we arrive at
the final form of the gradient:
=
?
x?U
?
i
?
yi,yi+1
fk(x, yi, yi+1, i)
?
l
p?
q??
?
(
?
j?
p?(yi, yi+1, yj? = l|x)?
p?(yi, yi+1|x)
?
j?
p?(yj? = l|x)
)
.
Here, the second term is easily gathered from for-
ward/backward, but obtaining the first term is some-
what more complicated. Computing this term
naively would require multiple runs of constrained
forward/backward. Here we present a more ef-
ficient method that requires only one run of for-
ward/backward.5 First we decompose the prob-
ability into two parts:
?
j? p?(yi, yi+1, yj? =
l|x) =
?i
j=1 p?(yi, yi+1, yj = l|x)I(j ? j
?) +
?J
j=i+1 p?(yi, yi+1, yj = l|x)I(j ? j
?). Next, we
show how to compute these terms efficiently. Simi-
lar to forward/backward, we build a lattice of inter-
mediate results that then can be used to calculate the
5(Kakade et al, 2002) propose a related method that com-
putes p(y1..i = l1..i|yi+1 = l).
quantity of interest:
i?
j=1
p?(yi, yi+1, yj = l|x)I(j ? j
?)
= p(yi, yi+1|x)?(yi, l)I(i ? j?)
+
i?1?
j=1
p?(yi, yi+1, yj = l|x)I(j ? j
?)
= p(yi, yi+1|x)?(yi, l)I(i ? j?)
+
?
?
?
yi?1
i?1?
j=1
p?(yi?1, yi, yj = l|x)I(j ? j
?)
?
?
p?(yi+1|yi,x).
For efficiency,
?
yi?1
?i?1
j=1 p?(yi?1, yi, yj =
l|x)I(j ? j?) is saved at each stage in the lat-
tice.
?J
j=i+1 p?(yi?1, yi, yj = l|x)I(j ? j
?) can
be computed in the same fashion. To compute the
lattices it takes time O(ns2), and one lattice must be
computed for each label so the total time is O(ns3).
5 Experimental Results
We use the CLASSIFIEDS data provided by Grenager
et al (2005) and compare with results reported
by HK06 (Haghighi and Klein, 2006) and CRR07
(Chang et al, 2007). HK06 introduced a set of 33
features along with their majority labels, these are
the primary set of additional constraints (Table 1).
As HK06 notes, these features are selected using
statistics of the labeled data, and here we used sim-
ilar features here in order to compare with previous
results. Though in practice we have found that fea-
ture selection is often intuitive, recent work has ex-
perimented with automatic feature selection using
LDA (Druck et al, 2008). For some of the exper-
iments we also use two sets of 33 additional fea-
tures that we chose by the same method as HK06,
the first 33 of which are also shown in Table 1. We
use the same tokenization of the dataset as HK06,
and training/test/unsupervised sets of 100 instances
each. This data differs slightly from the tokenization
used by CRR07. In particular it lacks the newline
breaks which might be a useful piece of information.
There are three types of supervised/semi-
supervised data used in the experiments. Labeled
instances are the traditional or conventionally
874
Label HK06: 33 Features 33 Added Features
CONTACT *phone* call *time please appointment more
FEATURES kitchen laundry parking room new large
ROOMMATES roommate respectful drama i bit mean
RESTRICTIONS pets smoking dog no sorry cats
UTILITIES utilities pays electricity water garbage included
AVAILABLE immediately begin cheaper *month* now *ordinal*0
SIZE *number*1*1 br sq *number*0*1 bedroom bath
PHOTOS pictures image link *url*long click photos
RENT *number*15*1 $ month deposit lease rent
NEIGHBORHOOD close near shopping located bart downtown
ADDRESS address carlmont ave san *ordinal*5 #
Table 1: Features and their associated majority label.
Features for each label were chosen by the method de-
scribed in HK06 ? top frequency for that label and not
higher frequency for any other label.
+ SVD features
HK06 53.7% 71.5%
CRF + GE/Heuristic 66.9% 68.3%
Table 2: Accuracy of semi-supervised learning methods
with majority labeled features alone. GE outperforms
HK06 when neither model has access to SVD features.
When SVD features are included, HK06 has an edge in
accuracy.
labeled instances used for estimation in traditional
CRF training. Majority labeled features are fea-
tures annotated with their majority label.6 Labeled
features are features m where the distribution
p(yi|fm(x, i)) has been specified. In Section 5.3 we
estimate these distributions from isolated labeled
tokens.
We evaluate the system in two scenarios: (1) with
feature constraints alone and (2) feature constraints
in conjunction with a minimal amount of labeled in-
stances. There is little prior work that demonstrates
the use of both scenarios; CRR07 can only be ap-
plied when there is some labeled data, while HK06
could be applied in both scenarios though there are
no such published experiments.
5.1 Majority Labeled Features Only
When using majority labeled features alone, it can
be seen in Table 2 that GE is the best performing
method. This is important, as it demonstrates that
GE out of the box can be used effectively, without
tuning and extra modifications.
6While HK06 and CRR07 require only majority labeled fea-
tures, GE criteria use conditional probability distributions of la-
bels given features, and so in order to apply GE we must decide
on a particular distribution for each feature constraint. In sec-
tions 5.1 and 5.2 we use a simple heuristic to derive distribu-
tions from majority label information: we assign .99 probabil-
ity to the majority label of the feature and divide the remaining
probability uniformly among the remainder of the labels.
Labeled Instances
10 25 100
supervised HMM 61.6% 70.0% 76.3%
supervised CRF 64.6% 72.9% 79.4%
CRF+ Entropy Reg. 67.3% 73.7% 79.5%
CRR07 70.9% 74.8% 78.6%
+ inference constraints 74.7% 78.5% 81.7%
CRF+GE/Heuristic 72.6% 76.3% 80.1%
Table 3: Accuracy of semi-supervised learning meth-
ods with constraints and limited amounts of training
data. Even though CRR07 uses more constraints and re-
quires additional development data for estimating mix-
ture weights, GE still outperforms CRR07 when that sys-
tem is run without applying constraints during inference.
When these constraints are applied during test-time infer-
ence, CRR07 has an edge over the CRF trained with GE
criteria.
In their original work, HK06 propose a method
for generating additional features given a set of ?pro-
totype? features (the feature constraints in Table 1),
which they demonstrate to be highly effective. In
their method, they collect contexts around all words
in the corpus, then perform a SVD decomposition.
They take the first 50 singular values for all words,
and then if a word is within a thresholded distance
to a prototype feature, they assign that word a new
feature which indicates close similarity to a proto-
type feature. When SVD features such as these are
made available to the systems, HK06 has a higher
accuracy.7 For the remainder of the experiments we
use the SVD feature enhanced data sets.8
We ran additional experiments with expected gra-
dient methods but found them to be ineffective,
reaching around 50% accuracy on the experiments
with the additional SVD features, around 20% less
than the competing methods.
5.2 Majority Labeled Features and Labeled
Instances
Labeled instances are available, the technique de-
scribed in CRR07 can be used. While CRR07 is
run on the same data set as used by HK06, a direct
comparison is problematic. First, they use additional
constraints beyond those used in this paper and those
7We generated our own set of SVD features, so they might
not match exactly the SVD features described in HK06.
8One further experiment HK06 performs which we do not
duplicate here is post-processing the label assignments to better
handle field boundaries. With this addition they realize another
2.5% improvement.
875
used by HK06 (e.g. each contiguous label sequence
must be at least 3 labels long)?so their results can-
not be directly compared. Second, they require addi-
tional training data to estimate weights for their soft
constraints, and do not measure how much of this
additional data is needed. Third, they use a slightly
different tokenization procedure. Fourth, CRR07
uses different subsets of labeled training instances
than used here. For these reasons, the comparison
between the method presented here and CRR07 can-
not be exact.
The technique described in CRR07 can be applied
in two ways: constraints can be applied during learn-
ing, and they can also be applied during inference.
We present comparisons with both of these systems
in Table 3. CRFs trained with GE criteria consis-
tently outperform CRR07 when no constraints are
applied during inference time, even though CRR07
has additional constraints. When the method in
CRR07 is applied with constraints in inference time,
it is able to outperform CRFs trained with GE. We
tried adding the additional constraints described in
CRR07 during test-time inference in our system, but
found no accuracy improvement. After doing error
inspection, those additional constraints weren?t fre-
quently violated by the GE trained method, which
also suggests that adding them wouldn?t have a sig-
nificant effect during training either. It is possible
that for GE training there are alternative inference-
time constraints that would improve performance,
but we didn?t pursue this line of investigation as
there are benefits to operating within a formal prob-
abilistic model, and eschewing constraints applied
during inference time. Without these constraints,
probabilistic models can be combined easily with
one another in order to arrive at a joint model, and
adding in these constraints at inference time compli-
cates the nature of the combination.
5.3 Labeled Features vs. Labeled Instances
In the previous section, the supervision signal was
the majority label of each feature.9 Given a feature
of interest, a human can gather a set of tokens that
have this feature and label them to discover the cor-
9It is not clear how these features would be tagged with ma-
jority label in a real use case. Tagging data to discover the ma-
jority label could potentially require a large number of tagged
instances before the majority label was definitively identified.
Ac
cur
acy
Tokens
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 10  100  1000  10000  100000
Traditional Instance Labeling33 Labeled Features66 Labeled Features99 Labeled FeaturesCRR07 + inference time constraints
Figure 2: Accuracy of supervised and semi-supervised
learning methods for fixed numbers of labeled tokens.
Training a GE model with only labeled features sig-
nificantly outperforms traditional log-likelihood training
with labeled instances for comparable numbers of labeled
tokens. When training on less than 1500 annotated to-
kens, it also outperforms CRR07 + inference time con-
straints, which uses not only labeled tokens but additional
constraints and development data for estimating mixture
weights.
Labeled Instances
0 10 25 100
HK06 71.5% - - -
GE/Heuristic 68.3% 72.6% 76.3% 80.1%
GE/Sampled 73.0% 74.6% 77.2% 80.5%
Table 4: Accuracy of semi-supervised learning methods
comparing the effects of (1) a heuristic for setting con-
ditional distributions of labels given features and (2) es-
timating this distributions via human annotation. When
GE is given feature distributions are better than the sim-
ple heuristic it is able to realize considerable gains.
relation between the feature and the labels.10 While
the resulting label distribution information could not
be fully utilized by previous methods (HK06 and
CRR07 use only the majority label of the word), it
can, however, be integrated into the GE criteria by
using the distribution from the relative proportions
of labels rather than a the previous heuristic distri-
bution. We present a series of experiments that test
the advantages of this annotation paradigm.
To simulate a human labeler, we randomly sam-
ple (without replacement) tokens with the particu-
lar feature in question, and generate a label using
the human annotations provided in the data. Then
we normalize and smooth the raw counts to obtain a
10In this paper we observe a 10x speed-up by using isolated
labeled tokens instead of a wholly labeled instances?so even
if it takes slightly longer to label isolated tokens, there will still
be a substantial gain.
876
conditional probability distribution over labels given
feature. We experiment with samples of 1, 2,5, 10,
100 tokens per feature, as well as with all available
labeled data. We sample instances for labeling ex-
clusively from the training and development data,
not from the testing data. We train a model using GE
with these estimated conditional probability distri-
butions and compare them with corresponding num-
bers of tokens of traditionally labeled instances.
Training from labeled features significantly out-
performs training from traditional labeled instances
for equivalent numbers of labeled tokens (Figure
2). With 1000 labeled tokens, instance-labeling
achieves accuracy around 65%, while labeling 33
features reaches 72% accuracy.11 To achieve the
same level of performance as traditional instance la-
beling, it can require as much as a factor of ten-fold
fewer annotations of feature occurrences. For exam-
ple, the accuracy achieved after labeling 257 tokens
of 33 features is 71% ? the same accuracy achieved
only after labeling more than 2000 tokens in tradi-
tional instance-labeling.12
Assuming that labeling one token in isolation
takes the same time as labeling one token in a
sequence, these results strongly support a new
paradigm of labeling in which instead of annotat-
ing entire sentences, the human instead selects some
key features of interest and labels tokens that have
this feature. Particularly intriguing is the flexibility
our scenario provides for the selection of ?features
of interest? to be driven by error analysis.
Table 4 compares the heuristic method described
above against sampled conditional probability distri-
butions of labels given features13. Sampled distribu-
tions yield consistent improvements over the heuris-
tic method. The accuracy with no labeled instances
(73.0%) is better than HK06 (71.5%), which demon-
strates that the precisely estimated feature distribu-
tions are helpful for improving accuracy.
Though accuracy begins to level off with distri-
11Labeling 99 features with 1000 tokens reaches nearly 76%.
12Accuracy at one labeled token per feature is much worse
than accuracy with majority label information. This due to the
noise introduced by sampling, as there is the potential for a rel-
atively rare label be sampled and labeled, and thereby train the
system on a non-canonical supervision signal.
13Where the tokens labeled is the total available number in
the data, roughly 2500 tokens.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2  4  6  8  10  12
Proba
bility
Label  0
 0.2
 0.4
 0.6
 0.8
 1
 0  2  4  6  8  10  12
Proba
bility
Label
Figure 3: From left to right: distributions (with standard
error) for the feature WORD=ADDRESS obtained from
sampling, using 1 sample per feature and 10 samples per
feature. Labels 1, 2, 3, and 9 are (respectively) FEA-
TURES, CONTACT, SIZE, and ADDRESS. Instead of more
precisely estimating these distributions, it is more benefi-
cial to label a larger set of features.
butions over the original set of 33 labeled features,
we ran additional experiments with 66 and 99 la-
beled features, whose results are also shown in Fig-
ure 2.14 The graph shows that with an increased
number of labeled features, for the same numbers
of labeled tokens, accuracy can be improved. The
reason behind this is clear?while there is some gain
from increased precision of probability estimates (as
they asymptotically approach their ?true? values as
shown in Figure 3), there is more information to be
gained from rougher estimates of a larger set of fea-
tures. One final point about these additional features
is that their distributions are less peaked than the
original feature set. Where the original feature set
distribution has entropy of 8.8, the first 33 added fea-
tures have an entropy of 22.95. Surprisingly, even
ambiguous feature constraints are able to improve
accuracy.
6 Conclusion
We have presented generalized expectation criteria
for linear-chain conditional random fields, a new
semi-supervised training method that makes use of
labeled features rather than labeled instances. Pre-
vious semi-supervised methods have typically used
ad-hoc feature majority label assignments as con-
straints. Our new method uses conditional proba-
bility distributions of labels given features and can
dramatically reduce annotation time. When these
distributions are estimated by means of annotated
feature occurrences in context, there is as much as
a ten-fold reduction in the annotation time that is re-
quired in order to achieve the same level of accuracy
over traditional instance-labeling.
14Also note that for less than 1500 tokens of labeling, the 99
labeled features outperform CRR07 with inference time con-
straints.
877
References
R. K. Ando and T. Zhang. 2005. A framework for learn-
ing predictive structures from multiple tasks and unla-
beled data. JMLR, 6.
M.-W. Chang, L. Ratinov, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. In
ACL.
G. Druck, G. Mann, and A. McCallum. 2007. Lever-
aging existing resources using generalized expectation
criteria. In NIPS Workshop on Learning Problem De-
sign.
G. Druck, G. S. Mann, and A. McCallum. 2008. Learn-
ing from labeled features using generalized expecta-
tion criteria. In SIGIR.
D. Freitag. 2004. Trained named entity recognition using
distributional clusters. In EMNLP.
Y. Grandvalet and Y. Bengio. 2004. Semi-supervised
learning by entropy minimization. In NIPS.
T. Grenager, D. Klein, and C. Manning. 2005. Unsuper-
vised learning of field segmentation models for infor-
mation extraction. In ACL.
A. Haghighi and D. Klein. 2006. Prototype-driver learn-
ing for sequence models. In NAACL.
F. Jiao, S. Wang, C.-H. Lee, R. Greiner, and D. Schu-
urmans. 2006. Semi-supervised conditional random
fields for improved sequence segmentation and label-
ing. In COLING/ACL.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
ICML.
S. Kakade, Y-W. Teg, and S.Roweis. 2002. An alternate
objective function for markovian fields. In ICML.
G. Mann and A. McCallum. 2007. Simple, robust, scal-
able semi-supervised learning via expectation regular-
ization. In ICML.
A. McCallum, G. S. Mann, and G. Druck. 2007. Gener-
alized expectation criteria. Computer science techni-
cal note, University of Massachusetts, Amherst, MA.
S. Miller, J. Guinness, and A. Zamanian. 2004. Name
tagging with word clusters and discriminative training.
In ACL.
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell.
1998. Learning to classify text from labeled and un-
labeled documents. In AAAI.
A. Quattoni, S. Wang, L-P. Morency, M. Collins, and
T. Darrell. 2007. Hidden-state conditional random
fields. In PAMI.
H. Raghavan, O. Madani, and R. Jones. 2006. Active
learning with feedback on both features and instances.
JMLR.
R. Salakhutdinov, S. Roweis, and Z. Ghahramani. 2003.
Optimization with em and expectation-conjugate-
gradient. In ICML.
R. Schapire, M. Rochery, M. Rahim, and N. Gupta.
2002. Incorporating prior knowledge into boosting.
In ICML.
N. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In ACL.
Martin Szummer and Tommi Jaakkola. 2002. Partially
labeled classification with markov random walks. In
NIPS, volume 14.
X. Zhu and Z. Ghahramani. 2002. Learning from labeled
and unlabeled data with label propagation. Technical
Report CMU-CALD-02-107, CMU.
X. Zhu. 2005. Semi-supervised learning lit-
erature survey. Technical Report 1530, Com-
puter Sciences, University of Wisconsin-Madison.
http://www.cs.wisc.edu/?jerryzhu/pub/ssl survey.pdf.
878
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 360?368,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Semi-supervised Learning of Dependency Parsers
using Generalized Expectation Criteria
Gregory Druck
Dept. of Computer Science
University of Massachusetts
Amherst, MA 01003
gdruck@cs.umass.edu
Gideon Mann
Google, Inc.
76 9th Ave.
New York, NY 10011
gideon.mann@gmail.com
Andrew McCallum
Dept. of Computer Science
University of Massachusetts
Amherst, MA 01003
mccallum@cs.umass.edu
Abstract
In this paper, we propose a novel method
for semi-supervised learning of non-
projective log-linear dependency parsers
using directly expressed linguistic prior
knowledge (e.g. a noun?s parent is often a
verb). Model parameters are estimated us-
ing a generalized expectation (GE) objec-
tive function that penalizes the mismatch
between model predictions and linguistic
expectation constraints. In a comparison
with two prominent ?unsupervised? learn-
ing methods that require indirect biasing
toward the correct syntactic structure, we
show that GE can attain better accuracy
with as few as 20 intuitive constraints. We
also present positive experimental results
on longer sentences in multiple languages.
1 Introduction
Early approaches to parsing assumed a grammar
provided by human experts (Quirk et al, 1985).
Later approaches avoided grammar writing by
learning the grammar from sentences explicitly
annotated with their syntactic structure (Black et
al., 1992). While such supervised approaches have
yielded accurate parsers (Charniak, 2001), the
syntactic annotation of corpora such as the Penn
Treebank is extremely costly, and consequently
there are few treebanks of comparable size.
As a result, there has been recent interest in
unsupervised parsing. However, in order to at-
tain reasonable accuracy, these methods have to
be carefully biased towards the desired syntac-
tic structure. This weak supervision has been
encoded using priors and initializations (Klein
and Manning, 2004; Smith, 2006), specialized
models (Klein and Manning, 2004; Seginer,
2007; Bod, 2006), and implicit negative evi-
dence (Smith, 2006). These indirect methods for
leveraging prior knowledge can be cumbersome
and unintuitive for a non-machine-learning expert.
This paper proposes a method for directly guid-
ing the learning of dependency parsers with nat-
urally encoded linguistic insights. Generalized
expectation (GE) (Mann and McCallum, 2008;
Druck et al, 2008) is a recently proposed frame-
work for incorporating prior knowledge into the
learning of conditional random fields (CRFs) (Laf-
ferty et al, 2001). GE criteria express a preference
on the value of a model expectation. For example,
we know that ?in English, when a determiner is di-
rectly to the left of a noun, the noun is usually the
parent of the determiner?. With GE we may add
a term to the objective function that encourages a
feature-rich CRF to match this expectation on un-
labeled data, and in the process learn about related
features. In this paper we use a non-projective de-
pendency tree CRF (Smith and Smith, 2007).
While a complete exploration of linguistic prior
knowledge for dependency parsing is beyond the
scope of this paper, we provide several promis-
ing demonstrations of the proposed method. On
the English WSJ10 data set, GE training outper-
forms two prominent unsupervised methods using
only 20 constraints either elicited from a human
or provided by an ?oracle? simulating a human.
We also present experiments on longer sentences
in Dutch, Spanish, and Turkish in which we obtain
accuracy comparable to supervised learning with
tens to hundreds of complete parsed sentences.
2 Related Work
This work is closely related to the prototype-
driven grammar induction method of Haghighi
and Klein (2006), which uses prototype phrases
to guide the EM algorithm in learning a PCFG.
Direct comparison with this method is not possi-
ble because we are interested in dependency syn-
tax rather than phrase structure syntax. However,
the approach we advocate has several significant
360
advantages. GE is more general than prototype-
driven learning because GE constraints can be un-
certain. Additionally prototype-driven grammar
induction needs to be used in conjunction with
other unsupervised methods (distributional simi-
larity and CCM (Klein and Manning, 2004)) to
attain reasonable accuracy, and is only evaluated
on length 10 or less sentences with no lexical in-
formation. In contrast, GE uses only the provided
constraints and unparsed sentences, and is used to
train a feature-rich discriminative model.
Conventional semi-supervised learning requires
parsed sentences. Kate and Mooney (2007) and
McClosky et al (2006) both use modified forms
of self-training to bootstrap parsers from limited
labeled data. Wang et al (2008) combine a struc-
tured loss on parsed sentences with a least squares
loss on unlabeled sentences. Koo et al (2008) use
a large unlabeled corpus to estimate cluster fea-
tures which help the parser generalize with fewer
examples. Smith and Eisner (2007) apply entropy
regularization to dependency parsing. The above
methods can be applied to small seed corpora, but
McDonald1 has criticized such methods as work-
ing from an unrealistic premise, as a significant
amount of the effort required to build a treebank
comes in the first 100 sentences (both because of
the time it takes to create an appropriate rubric and
to train annotators).
There are also a number of methods for unsu-
pervised learning of dependency parsers. Klein
and Manning (2004) use a carefully initialized and
structured generative model (DMV) in conjunc-
tion with the EM algorithm to get the first positive
results on unsupervised dependency parsing. As
empirical evidence of the sensitivity of DMV to
initialization, Smith (2006) (pg. 37) uses three dif-
ferent initializations, and only one, the method of
Klein and Manning (2004), gives accuracy higher
than 31% on the WSJ10 corpus (see Section 5).
This initialization encodes the prior knowledge
that long distance attachments are unlikely.
Smith and Eisner (2005) develop contrastive
estimation (CE), in which the model is encour-
aged to move probability mass away from im-
plicit negative examples defined using a care-
fully chosen neighborhood function. For instance,
Smith (2006) (pg. 82) uses eight different neigh-
borhood functions to estimate parameters for the
DMV model. The best performing neighborhood
1R. McDonald, personal communication, 2007
function DEL1ORTRANS1 provides accuracy of
57.6% on WSJ10 (see Section 5). Another neigh-
borhood, DEL1ORTRANS2, provides accuracy of
51.2%. The remaining six neighborhood func-
tions provide accuracy below 50%. This demon-
strates that constructing an appropriate neighbor-
hood function can be delicate and challenging.
Smith and Eisner (2006) propose structural an-
nealing (SA), in which a strong bias for local de-
pendency attachments is enforced early in learn-
ing, and then gradually relaxed. This method is
sensitive to the annealing schedule. Smith (2006)
(pg. 136) use 10 annealing schedules in conjunc-
tion with three initializers. The best performing
combination attains accuracy of 66.7% on WSJ10,
but the worst attains accuracy of 32.5%.
Finally, Seginer (2007) and Bod (2006) ap-
proach unsupervised parsing by constructing
novel syntactic models. The development and tun-
ing of the above methods constitute the encoding
of prior domain knowledge about the desired syn-
tactic structure. In contrast, our framework pro-
vides a straightforward and explicit method for in-
corporating prior knowledge.
Ganchev et al (2009) propose a related method
that uses posterior constrained EM to learn a pro-
jective target language parser using only a source
language parser and word alignments.
3 Generalized Expectation Criteria
Generalized expectation criteria (Mann and Mc-
Callum, 2008; Druck et al, 2008) are terms in
a parameter estimation objective function that ex-
press a preference on the value of a model expec-
tation. Let x represent input variables (i.e. a sen-
tence) and y represent output variables (i.e. a parse
tree). A generalized expectation term G(?) is de-
fined by a constraint function G(y,x) that returns
a non-negative real value given input and output
variables, an empirical distribution p?(x) over in-
put variables (i.e. unlabeled data), a model distri-
bution p?(y|x), and a score function S:
G(?) = S(Ep?(x)[Ep?(y|x)[G(y,x)]]).
In this paper, we use a score function that is the
squared difference of the model expectation of G
and some target expectation G?:
Ssq = ?(G?? Ep?(x)[Ep?(y|x)[G(y,x)]])
2 (1)
We can incorporate prior knowledge into the train-
ing of p?(y|x) by specifying the from of the con-
straint function G and the target expectation G?.
361
Importantly, G does not need to match a particular
feature in the underlying model.
The complete objective function2 includes mul-
tiple GE terms and a prior on parameters3, p(?)
O(?;D) = p(?) +
?
G
G(?)
GE has been applied to logistic regression mod-
els (Mann and McCallum, 2007; Druck et al,
2008) and linear chain CRFs (Mann and McCal-
lum, 2008). In the following sections we apply
GE to non-projective CRF dependency parsing.
3.1 GE in General CRFs
We first consider an arbitrarily structured condi-
tional random field (Lafferty et al, 2001) p?(y|x).
We describe the CRF for non-projective depen-
dency parsing in Section 3.2. The probability of
an output y conditioned on an input x is
p?(y|x) =
1
Zx
exp
(?
j
?jFj(y,x)
)
,
where Fj are feature functions over the cliques
of the graphical model and Z(x) is a normaliz-
ing constant that ensures p?(y|x) sums to 1. We
are interested in the expectation of constraint func-
tion G(x,y) under this model. We abbreviate this
model expectation as:
G? = Ep?(x)[Ep?(y|x)[G(y,x)]]
It can be shown that partial derivative of G(?) us-
ing Ssq4 with respect to model parameter ?j is
?
??j
G(?) = 2(G??G?) (2)
(
Ep?(x)
[
Ep?(y|x) [G(y,x)Fj(y,x)]
?Ep?(y|x) [G(y,x)]Ep?(y|x) [Fj(y,x)]
])
.
Equation 2 has an intuitive interpretation. The first
term (on the first line) is the difference between the
model and target expectations. The second term
2In general, the objective function could also include the
likelihood of available labeled data, but throughout this paper
we assume we have no parsed sentences.
3Throughout this paper we use a Gaussian prior on pa-
rameters with ?2 = 10.
4In previous work, S was the KL-divergence from the tar-
get expectation. The partial derivative of the KL divergence
score function includes the same covariance term as above
but substitutes a different multiplicative term: G?/G?.
(the rest of the equation) is the predicted covari-
ance between the constraint function G and the
model feature function Fj . Therefore, if the con-
straint is not satisfied, GE updates parameters for
features that the model predicts are related to the
constraint function.
If there are constraint functions G for all model
feature functions Fj , and the target expectations
G? are estimated from labeled data, then the glob-
ally optimal parameter setting under the GE objec-
tive function is equivalent to the maximum likeli-
hood solution. However, GE does not require such
a one-to-one correspondence between constraint
functions and model feature functions. This al-
lows bootstrapping of feature-rich models with a
small number of prior expectation constraints.
3.2 Non-Projective Dependency Tree CRFs
We now define a CRF p?(y|x) for unlabeled, non-
projective5 dependency parsing. The tree y is rep-
resented as a vector of the same length as the sen-
tence, where yi is the index of the parent of word
i. The probability of a tree y given sentence x is
p?(y|x) =
1
Zx
exp
( n?
i=1
?
j
?jfj(xi, xyi ,x)
)
,
where fj are edge-factored feature functions that
consider the child input (word, tag, or other fea-
ture), the parent input, and the rest of the sen-
tence. This factorization implies that dependency
decisions are independent conditioned on the in-
put sentence x if y is a tree. ComputingZx and the
edge expectations needed for partial derivatives re-
quires summing over all possible trees for x.
By relating the sum of the scores of all possible
trees to counting the number of spanning trees in a
graph, it can be shown that Zx is the determinant
of the Kirchoff matrixK, which is constructed us-
ing the scores of possible edges. (McDonald and
Satta, 2007; Smith and Smith, 2007). Computing
the determinant takes O(n3) time, where n is the
length of the sentence. To compute the marginal
probability of a particular edge k ? i (i.e. yi=k),
the score of any edge k? ? i such that k? 6= k is
set to 0. The determinant of the resulting modi-
fied Kirchoff matrix Kk?i is then the sum of the
scores of all trees that include the edge k ? i. The
5Note that we could instead define a CRF for projective
dependency parse trees and use a variant of the inside outside
algorithm for inference. We choose non-projective because it
is the more general case.
362
marginal p(yi=k|x; ?) can be computed by divid-
ing this score by Zx (McDonald and Satta, 2007).
Computing all edge expectations with this algo-
rithm takes O(n5) time. Smith and Smith (2007)
describe a more efficient algorithm that can com-
pute all edge expectations in O(n3) time using the
inverse of the Kirchoff matrix K?1.
3.3 GE for Non-Projective Dependency Tree
CRFs
While in general constraint functions G may
consider multiple edges, in this paper we use
edge-factored constraint functions. In this case
Ep?(y|x)[G(y,x)]Ep?(y|x)[Fj(y,x)], the second
term of the covariance in Equation 2, can be
computed using the edge marginal distributions
p?(yi|x). The first term of the covariance
Ep?(y|x)[G(y,x)Fj(y,x)] is more difficult to
compute because it requires the marginal proba-
bility of two edges p?(yi, yi? |x). It is important to
note that the model p? is still edge-factored.
The sum of the scores of all trees that contain
edges k ? i and k? ? i? can be computed by set-
ting the scores of edges j ? i such that j 6= k and
j? ? i? such that j? 6= k? to 0, and computing the
determinant of the resulting modified Kirchoff ma-
trixKk?i,k??i? . There areO(n4) pairs of possible
edges, and the determinant computation takes time
O(n3), so this naive algorithm takes O(n7) time.
An improved algorithm computes, for each pos-
sible edge k ? i, a modified Kirchoff matrix
Kk?i that requires the presence of that edge.
Then, the method of Smith and Smith (2007) can
be used to compute the probability of every pos-
sible edge conditioned on the presence of k ? i,
p?(yi? =k?|yi = k,x), using K
?1
k?i. Multiplying
this probability by p?(yi=k|x) yields the desired
two edge marginal. Because this algorithm pulls
the O(n3) matrix operation out of the inner loop
over edges, the run time is reduced to O(n5).
If it were possible to perform only one O(n3)
matrix operation per sentence, then the gradient
computation would take onlyO(n4) time, the time
required to consider all pairs of edges. Unfortu-
nately, there is no straightforward generalization
of the method of Smith and Smith (2007) to the
two edge marginal problem. Specifically, Laplace
expansion generalizes to second-order matrix mi-
nors, but it is not clear how to compute second-
order cofactors from the inverse Kirchoff matrix
alone (c.f. (Smith and Smith, 2007)).
Consequently, we also propose an approxima-
tion that can be used to speed up GE training at
the expense of a less accurate covariance compu-
tation. We consider different cases of the edges
k ? i, and k? ? i?.
? p?(yi=k, yi?=k?|x)=0 when i=i? and k 6=k?
(different parent for the same word), or when
i=k? and k=i? (cycle), because these pairs of
edges break the tree constraint.
? p?(yi=k, yi? =k?|x)=p?(yi=k|x) when i=
i?, k=k?.
? p?(yi=k, yi? =k?|x)?p?(yi=k|x)p?(yi? =
k?|x) when i 6= i? and i 6= k? or i? 6= k
(different words, do not create a cycle). This
approximation assumes that pairs of edges
that do not fall into one of the above cases
are conditionally independent given x. This
is not true because there are partial trees in
which k ? i and k? ? i? can appear sepa-
rately, but not together (for example if i = k?
and the partial tree contains i? ? k).
Using this approximation, the covariance for one
sentence is approximately equal to
n?
i
Ep?(yi|x)[fj(xi, xyi ,x)g(xi, xyi ,x)]
?
n?
i
Ep?(yi|x)[fj(xi, xyi ,x)]Ep?(yi|x)[g(xi, xyi ,x)]
?
n?
i,k
p?(yi=k|x)p?(yk=i|x)fj(xi, xk,x)g(xk, xi,x).
Intuitively, the first and second terms compute a
covariance over possible parents for a single word,
and the third term accounts for cycles. Computing
the above takes O(n3) time, the time required to
compute single edge marginals. In this paper, we
use the O(n5) exact method, though we find that
the accuracy attained by approximate training is
usually within 5% of the exact method.
If G is not edge-factored, then we need to com-
pute a marginal over three or more edges, making
exact training intractable. An appealing alterna-
tive to a similar approximation to the above would
use loopy belief propagation to efficiently approx-
imate the marginals (Smith and Eisner, 2008).
In this paper g is binary and normalized by its
total count in the corpus. The expectation of g is
then the probability that it indicates a true edge.
363
4 Linguistic Prior Knowledge
Training parsers using GE with the aid of linguists
is an exciting direction for future work. In this pa-
per, we use constraints derived from several basic
types of linguistic knowledge.
One simple form of linguistic knowledge is the
set of possible parent tags for a given child tag.
This type of constraint was used in the devel-
opment of a rule-based dependency parser (De-
busmann et al, 2004). Additional information
can be obtained from small grammar fragments.
Haghighi and Klein (2006) provide a list of proto-
type phrase structure rules that can be augmented
with dependencies and used to define constraints
involving parent and child tags, surrounding or
interposing tags, direction, and distance. Finally
there are well known hypotheses about the direc-
tion and distance of attachments that can be used
to define constraints. Eisner and Smith (2005) use
the fact that short attachments are more common
to improve unsupervised parsing accuracy.
4.1 ?Oracle? constraints
For some experiments that follow we use ?ora-
cle? constraints that are estimated from labeled
data. This involves choosing feature templates
(motivated by the linguistic knowledge described
above) and estimating target expectations. Oracle
methods used in this paper consider three simple
statistics of candidate constraint functions: count
c?(g), edge count c?edge(g), and edge probability
p?(edge|g). Let D be the labeled corpus.
c?(g) =
?
x?D
?
i
?
j
g(xi, xj ,x)
c?edge(g) =
?
(x,y)?D
?
i
g(xi, xyi ,x)
p?(edge|g) =
c?edge(g)
c?(g)
Constraint functions are selected according to
some combination of the above statistics. In
some cases we additionally prune the candidate
set by considering only certain templates. To
compute the target expectation, we simply use
bin(p?(edge|g)), where bin returns the closest
value in the set {0, 0.1, 0.25, 0.5, 0.75, 1}. This
can be viewed as specifying that g is very indica-
tive of edge, somewhat indicative of edge, etc.
5 Experimental Comparison with
Unsupervised Learning
In this section we compare GE training with meth-
ods for unsupervised parsing. We use the WSJ10
corpus (as processed by Smith (2006)), which is
comprised of English sentences of ten words or
fewer (after stripping punctuation) from the WSJ
portion of the Penn Treebank. As in previous work
sentences contain only part-of-speech tags.
We compare GE and supervised training of an
edge-factored CRF with unsupervised learning of
a DMV model (Klein and Manning, 2004) using
EM and contrastive estimation (CE) (Smith and
Eisner, 2005). We also report the accuracy of an
attach-right baseline6. Finally, we report the ac-
curacy of a constraint baseline that assigns a score
to each possible edge that is the sum of the target
expectations for all constraints on that edge. Pos-
sible edges without constraints receive a score of
0. These scores are used as input to the maximum
spanning tree algorithm, which returns the best
tree. Note that this is a strong baseline because it
can handle uncertain constraints, and the tree con-
straint imposed by the MST algorithm helps infor-
mation propagate across edges.
We note that there are considerable differences
between the DMV and CRF models. The DMV
model is more expressive than the CRF because
it can model the arity of a head as well as sib-
ling relationships. Because these features consider
multiple edges, including them in the CRF model
would make exact inference intractable (McDon-
ald and Satta, 2007). However, the CRF may con-
sider the distance between head and child, whereas
DMV does not model distance. The CRF also
models non-projective trees, which when evaluat-
ing on English is likely a disadvantage.
Consequently, we experiment with two sets of
features for the CRF model. The first, restricted
set includes features that consider the head and
child tags of the dependency conjoined with the
direction of the attachment, (parent-POS,child-
POS,direction). With this feature set, the CRF
model is less expressive than DMV. The sec-
ond full set includes standard features for edge-
factored dependency parsers (McDonald et al,
2005), though still unlexicalized. The CRF can-
not consider valency even with the full feature set,
but this is balanced by the ability to use distance.
6The reported accuracies with the DMV model and the
attach-right baseline are taken from (Smith, 2006).
364
feature ex. feature ex.
MD? VB 1.00 NNS? VBD 0.75
POS? NN 0.75 PRP? VBD 0.75
JJ? NNS 0.75 VBD? TO 1.00
NNP? POS 0.75 VBD? VBN 0.75
ROOT?MD 0.75 NNS? VBP 0.75
ROOT? VBD 1.00 PRP? VBP 0.75
ROOT? VBP 0.75 VBP? VBN 0.75
ROOT? VBZ 0.75 PRP? VBZ 0.75
TO? VB 1.00 NN? VBZ 0.75
VBN? IN 0.75 VBZ? VBN 0.75
Table 1: 20 constraints that give 61.3% accuracy
on WSJ10. Tags are grouped according to heads,
and are in the order they appear in the sentence,
with the arrow pointing from head to modifier.
We generate constraints in two ways. First,
we use oracle constraints of the form (parent-
POS,child-POS,direction) such that c?(g) ? 200.
We choose constraints in descending order of
p?(edge|g). The first 20 constraints selected using
this method are displayed in Table 1.
Although the reader can verify that the con-
straints in Table 1 are reasonable, we addition-
ally experiment with human-provided constraints.
We use the prototype phrase-structure constraints
provided by Haghighi and Klein (2006), and
with the aid of head-finding rules, extract 14
(parent-pos,child-pos,direction) constraints.7 We
then estimated target expectations for these con-
straints using our prior knowledge, without look-
ing at the training data. We also created a second
constraint set with an additional six constraints for
tag pairs that were previously underrepresented.
5.1 Results
We present results varying the number of con-
straints in Figures 1 and 2. Figure 1 compares
supervised and GE training of the CRF model, as
well as the feature constraint baseline. First we
note that GE training using the full feature set sub-
stantially outperforms the restricted feature set,
despite the fact that the same set of constraints
is used for both experiments. This result demon-
strates GE?s ability to learn about related but non-
constrained features. GE training also outper-
forms the baseline8.
We compare GE training of the CRF model
7Because the CFG rules in (Haghighi and Klein, 2006)
are ?flattened? and in some cases do not generate appropriate
dependency constraints, we only used a subset.
8The baseline eventually matches the accuracy of the re-
stricted CRF but this is understandable because GE?s ability
to bootstrap is greatly reduced with the restricted feature set.
with unsupervised learning of the DMV model
in Figure 29. Despite the fact that the restricted
CRF is less expressive than DMV, GE training of
this model outperforms EM with 30 constraints
and CE with 50 constraints. GE training of the
full CRF outperforms EM with 10 constraints and
CE with 20 constraints (those displayed in Ta-
ble 1). GE training of the full CRF with the set of
14 constraints from (Haghighi and Klein, 2006),
gives accuracy of 53.8%, which is above the inter-
polated oracle constraints curve (43.5% accuracy
with 10 constraints, 61.3% accuracy with 20 con-
straints). With the 6 additional constraints, we ob-
tain accuracy of 57.7% and match CE.
Recall that CE, EM, and the DMV model in-
corporate prior knowledge indirectly, and that the
reported results are heavily-tuned ideal cases (see
Section 2). In contrast, GE provides a method to
directly encode intuitive linguistic insights.
Finally, note that structural annealing (Smith
and Eisner, 2006) provides 66.7% accuracy on
WSJ10 when choosing the best performing an-
nealing schedule (Smith, 2006). As noted in Sec-
tion 2 other annealing schedules provide accuracy
as low as 32.5%. GE training of the full CRF at-
tains accuracy of 67.0% with 30 constraints.
6 Experimental Comparison with
Supervised Training on Long
Sentences
Unsupervised parsing methods are typically eval-
uated on short sentences, as in Section 5. In this
section we show that GE can be used to train
parsers for longer sentences that provide compa-
rable accuracy to supervised training with tens to
hundreds of parsed sentences.
We use the standard train/test splits of the
Spanish, Dutch, and Turkish data from the 2006
CoNLL Shared Task. We also use standard
edge-factored feature templates (McDonald et al,
2005)10. We experiment with versions of the dat-
9Klein and Manning (2004) report 43.2% accuracy for
DMV with EM on WSJ10. When jointly modeling con-
stituency and dependencies, Klein and Manning (2004) re-
port accuracy of 47.5%. Seginer (2007) and Bod (2006) pro-
pose unsupervised phrase structure parsing methods that give
better unlabeled F-scores than DMV with EM, but they do
not report directed dependency accuracy.
10Typical feature processing uses only supported features,
or those features that occur on at least one true edge in the
training data. Because we assume that the data is unlabeled,
we instead use features on all possible edges. This generates
tens of millions features, so we prune those features that oc-
cur fewer than 10 total times, as in (Smith and Eisner, 2007).
365
10 20 30 40 50 6010
20
30
40
50
60
70
80
90
number of constraints
accu
racy
 
 
constraint baselineCRF restricted supervisedCRF supervisedCRF restricted GECRF GECRF GE human
Figure 1: Comparison of the constraint baseline and
both GE and supervised training of the restricted and
full CRF. Note that supervised training uses 5,301
parsed sentences. GE with human provided con-
straints closely matches the oracle results.
10 20 30 40 50 6010
20
30
40
50
60
70
80
number of constraints
accu
racy
 
 
attach right baselineDMV EMDMV CECRF restricted GECRF GECRF GE human
Figure 2: Comparison of GE training of the re-
stricted and full CRFs with unsupervised learning of
DMV. GE training of the full CRF outperforms CE
with just 20 constraints. GE also matches CE with
20 human provided constraints.
sets in which we remove sentences that are longer
than 20 words and 60 words.
For these experiments, we use an oracle
constraint selection method motivated by the
linguistic prior knowledge described in Section 4.
The first set of constraints specify the most
frequent head tag, attachment direction, and
distance combinations for each child tag. Specif-
ically, we select oracle constraints of the type
(parent-CPOS,child-CPOS,direction,distance)11.
We add constraints for every g such that
c?edge(g) > 100 for max length 60 data sets, and
c?edge(g)>10 times for max length 20 data sets.
In some cases, the possible parent constraints
described above will not be enough to provide
high accuracy, because they do not consider other
tags in the sentence (McDonald et al, 2005).
Consequently, we experiment with adding an
additional 25 sequence constraints (for what are
often called ?between? and ?surrounding? fea-
tures). The oracle feature selection method aims to
choose such constraints that help to reduce uncer-
tainty in the possible parents constraint set. Con-
sequently, we consider sequence features gs with
p?(edge|gs=1) ? 0.75, and whose corresponding
(parent-CPOS,child-CPOS,direction,distance)
constraint g, has edge probability p?(edge|g) ?
0.25. Among these candidates, we sort by
c?(gs=1), and select the top 25.
We compare with the constraint baseline de-
scribed in Section 5. Additionally, we report
11For these experiments we use coarse-grained part-of-
speech tags in constraints.
the number of parsed sentences required for su-
pervised CRF training (averaged over 5 random
splits) to match the accuracy of GE training using
the possible parents + sequence constraint set.
The results are provided in Table 2. We first
observe that GE always beats the baseline, espe-
cially on parent decisions for which there are no
constraints (not reported in Table 2, but for exam-
ple 53.8% vs. 20.5% on Turkish 20). Second, we
note that accuracy is always improved by adding
sequence constraints. Importantly, we observe
that GE gives comparable performance to super-
vised training with tens or hundreds of parsed sen-
tences. These parsed sentences provide a tremen-
dous amount of information to the model, as for
example in 20 Spanish length ? 60 sentences, a
total of 1,630,466 features are observed, 330,856
of them unique. In contrast, the constraint-based
methods are provided at most a few hundred con-
straints. When comparing the human costs of
parsing sentences and specifying constraints, re-
member that parsing sentences requires the devel-
opment of detailed annotation guidelines, which
can be extremely time-consuming (see also the
discussion is Section 2).
Finally, we experiment with iteratively
adding constraints. We sort constraints with
c?(g) > 50 by p?(edge|g), and ensure that 50%
are (parent-CPOS,child-CPOS,direction,distance)
constraints and 50% are sequence constraints.
For lack of space, we only show the results for
Spanish 60. In Figure 3, we see that GE beats
the baseline more soundly than above, and that
366
possible parent constraints + sequence constraints complete trees
baseline GE baseline GE
dutch 20 69.5 70.7 69.8 71.8 80-160
dutch 60 66.5 69.3 66.7 69.8 40-80
spanish 20 70.0 73.2 71.2 75.8 40-80
spanish 60 62.1 66.2 62.7 66.9 20-40
turkish 20 66.3 71.8 67.1 72.9 80-160
turkish 60 62.1 65.5 62.3 66.6 20-40
Table 2: Experiments on Dutch, Spanish, and Turkish with maximum sentence lengths of 20 and 60. Observe that GE
outperforms the baseline, adding sequence constraints improves accuracy, and accuracy with GE training is comparable to
supervised training with tens to hundreds of parsed sentences.
parent tag true predicted
det. 0.005 0.005
adv. 0.018 0.013
conj. 0.012 0.001
pron. 0.011 0.009
verb 0.355 0.405
adj. 0.067 0.075
punc. 0.031 0.013
noun 0.276 0.272
prep. 0.181 0.165
direction true predicted
right 0.621 0.598
left 0.339 0.362
distance true predicted
1 0.495 0.564
2 0.194 0.206
3 0.066 0.050
4 0.042 0.037
5 0.028 0.031
6-10 0.069 0.033
> 10 0.066 0.039
feature (distance) false pos. occ.
verb? punc. (>10) 1183
noun? prep. (1) 1139
adj. ? prep. (1) 855
verb? verb (6-10) 756
verb? verb (>10) 569
noun? punc. (1) 512
verb? punc. (2) 509
prep. ? punc. (1) 476
verb? punc. (4) 427
verb? prep. (1) 422
Table 3: Error analysis for GE training with possible parent + sequence constraints on Spanish 60 data. On the left, the
predicted and true distribution over parent coarse part-of-speech tags. In the middle, the predicted and true distributions over
attachment directions and distances. On the right, common features on false positive edges.
100 200 300 400 500 600 700 8002530
3540
4550
5560
6570
75
number of constraints
accura
cy
Spanish (maximum length 60)
 
 
constraint baselineGE
Figure 3: Comparing GE training of a CRF and constraint
baseline while increasing the number of oracle constraints.
adding constraints continues to increase accuracy.
7 Error Analysis
In this section, we analyze the errors of the model
learned with the possible parent + sequence con-
straints on the Spanish 60 data. In Table 3, we
present four types of analysis. First, we present
the predicted and true distributions over coarse-
grained parent part of speech tags. We can see
that verb is being predicted as a parent tag more
often then it should be, while most other tags are
predicted less often than they should be. Next, we
show the predicted and true distributions over at-
tachment direction and distance. From this we see
that the model is often incorrectly predicting left
attachments, and is predicting too many short at-
tachments. Finally, we show the most common
parent-child tag with direction and distance fea-
tures that occur on false positive edges. From this
table, we see that many errors concern the attach-
ments of punctuation. The second line indicates a
prepositional phrase attachment ambiguity.
This analysis could also be performed by a lin-
guist by looking at predicted trees for selected sen-
tences. Once errors are identified, GE constraints
could be added to address these problems.
8 Conclusions
In this paper, we developed a novel method for
the semi-supervised learning of a non-projective
CRF dependency parser that directly uses linguis-
tic prior knowledge as a training signal. It is our
hope that this method will permit more effective
leveraging of linguistic insight and resources and
enable the construction of parsers in languages and
domains where treebanks are not available.
Acknowledgments
We thank Ryan McDonald, Keith Hall, John Hale, Xiaoyun
Wu, and David Smith for helpful discussions. This work
was completed in part while Gregory Druck was an intern
at Google. This work was supported in part by the Center
for Intelligent Information Retrieval, The Central Intelligence
Agency, the National Security Agency and National Science
Foundation under NSF grant #IIS-0326249, and by the De-
fense Advanced Research Projects Agency (DARPA) under
Contract No. FA8750-07-D-0185/0004. Any opinions, find-
ings and conclusions or recommendations expressed in this
material are the author?s and do not necessarily reflect those
of the sponsor.
367
References
E. Black, J. Lafferty, and S. Roukos. 1992. Development and
evaluation of a broad-coverage probabilistic grammar of
english language computer manuals. In ACL, pages 185?
192.
Rens Bod. 2006. An all-subtrees approach to unsupervised
parsing. In ACL, pages 865?872.
E. Charniak. 2001. Immediate-head parsing for language
models. In ACL.
R. Debusmann, D. Duchier, A. Koller, M. Kuhlmann,
G. Smolka, and S. Thater. 2004. A relational syntax-
semantics interface based on dependency grammar. In
COLING.
G. Druck, G. S. Mann, and A. McCallum. 2008. Learning
from labeled features using generalized expectation crite-
ria. In SIGIR.
J. Eisner and N.A. Smith. 2005. Parsing with soft and hard
constraints on dependency length. In IWPT.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext projec-
tion constraints. In ACL.
A. Haghighi and D. Klein. 2006. Prototype-driven grammar
induction. In COLING.
R. J. Kate and R. J. Mooney. 2007. Semi-supervised learning
for semantic parsing using support vector machines. In
HLT-NAACL (Short Papers).
D. Klein and C. Manning. 2004. Corpus-based induction
of syntactic structure: Models of dependency and con-
stituency. In ACL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and
labeling sequence data. In ICML.
G. Mann and A. McCallum. 2007. Simple, robust, scal-
able semi-supervised learning via expectation regulariza-
tion. In ICML.
G. Mann and A. McCallum. 2008. Generalized expectation
criteria for semi-supervised learning of conditional ran-
dom fields. In ACL.
D. McClosky, E. Charniak, and M. Johnson. 2006. Effective
self-training for parsing. In HLT-NAACL.
Ryan McDonald and Giorgio Satta. 2007. On the complex-
ity of non-projective data-driven dependency parsing. In
Proc. of IWPT, pages 121?132.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In ACL, pages 91?98.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.
A Comprehensive Grammar of the English Language.
Longman.
Yoav Seginer. 2007. Fast unsupervised incremental parsing.
In ACL, pages 384?391, Prague, Czech Republic.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: training log-linear models on unlabeled data. In
ACL, pages 354?362.
Noah A. Smith and Jason Eisner. 2006. Annealing struc-
tural bias in multilingual weighted grammar induction. In
COLING-ACL, pages 569?576.
David A. Smith and Jason Eisner. 2007. Bootstrapping
feature-rich dependency parsers with entropic priors. In
EMNLP-CoNLL, pages 667?677.
David A. Smith and Jason Eisner. 2008. Dependency parsing
by belief propagation. In EMNLP.
David A. Smith and Noah A. Smith. 2007. Probabilistic
models of nonprojective dependency trees. In EMNLP-
CoNLL, pages 132?140.
Noah A. Smith. 2006. Novel Estimation Methods for Un-
supervised Discovery of Latent Structure in Natural Lan-
guage Text. Ph.D. thesis, Johns Hopkins University.
Qin Iris Wang, Dale Schuurmans, and Dekang Lin. 2008.
Semi-supervised convex training for dependency parsing.
In ACL, pages 532?540.
368
Early Results for
Named Entity Recognition with Conditional Random Fields,
Feature Induction and Web-Enhanced Lexicons
Andrew McCallum and Wei Li
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA 01003
{mccallum,weili}@cs.umass.edu
1 Introduction
Models for many natural language tasks benefit from the
flexibility to use overlapping, non-independent features.
For example, the need for labeled data can be drastically
reduced by taking advantage of domain knowledge in
the form of word lists, part-of-speech tags, character n-
grams, and capitalization patterns. While it is difficult to
capture such inter-dependent features with a generative
probabilistic model, conditionally-trained models, such
as conditional maximum entropy models, handle them
well. There has been significant work with such mod-
els for greedy sequence modeling in NLP (Ratnaparkhi,
1996; Borthwick et al, 1998).
Conditional Random Fields (CRFs) (Lafferty et al,
2001) are undirected graphical models, a special case of
which correspond to conditionally-trained finite state ma-
chines. While based on the same exponential form as
maximum entropy models, they have efficient procedures
for complete, non-greedy finite-state inference and train-
ing. CRFs have shown empirical successes recently in
POS tagging (Lafferty et al, 2001), noun phrase segmen-
tation (Sha and Pereira, 2003) and Chinese word segmen-
tation (McCallum and Feng, 2003).
Given these models? great flexibility to include a wide
array of features, an important question that remains is
what features should be used? For example, in some
cases capturing a word tri-gram is important, however,
there is not sufficient memory or computation to include
all word tri-grams. As the number of overlapping atomic
features increases, the difficulty and importance of con-
structing only certain feature combinations grows.
This paper presents a feature induction method for
CRFs. Founded on the principle of constructing only
those feature conjunctions that significantly increase log-
likelihood, the approach builds on that of Della Pietra et
al (1997), but is altered to work with conditional rather
than joint probabilities, and with a mean-field approxi-
mation and other additional modifications that improve
efficiency specifically for a sequence model. In compari-
son with traditional approaches, automated feature induc-
tion offers both improved accuracy and significant reduc-
tion in feature count; it enables the use of richer, higher-
order Markov models, and offers more freedom to liber-
ally guess about which atomic features may be relevant
to a task.
Feature induction methods still require the user to cre-
ate the building-block atomic features. Lexicon member-
ship tests are particularly powerful features in natural lan-
guage tasks. The question is where to get lexicons that are
relevant for the particular task at hand?
This paper describes WebListing, a method that obtains
seeds for the lexicons from the labeled data, then uses the
Web, HTML formatting regularities and a search engine
service to significantly augment those lexicons. For ex-
ample, based on the appearance of Arnold Palmer in the
labeled data, we gather from the Web a large list of other
golf players, including Tiger Woods (a phrase that is dif-
ficult to detect as a name without a good lexicon).
We present results on the CoNLL-2003 named entity
recognition (NER) shared task, consisting of news arti-
cles with tagged entities PERSON, LOCATION, ORGANI-
ZATION and MISC. The data is quite complex; for exam-
ple the English data includes foreign person names (such
as Yayuk Basuki and Innocent Butare), a wide diversity of
locations (including sports venues such as The Oval, and
rare location names such as Nirmal Hriday), many types
of organizations (from company names such as 3M, to
acronyms for political parties such as KDP, to location
names used to refer to sports teams such as Cleveland),
and a wide variety of miscellaneous named entities (from
software such as Java, to nationalities such as Basque, to
sporting competitions such as 1,000 Lakes Rally).
On this, our first attempt at a NER task, with just a few
person-weeks of effort and little work on development-
set error analysis, our method currently obtains overall
English F1 of 84.04% on the test set by using CRFs, fea-
ture induction and Web-augmented lexicons. German F1
using very limited lexicons is 68.11%.
2 Conditional Random Fields
Conditional Random Fields (CRFs) (Lafferty et al, 2001)
are undirected graphical models used to calculate the con-
ditional probability of values on designated output nodes
given values assigned to other designated input nodes.
In the special case in which the output nodes of the
graphical model are linked by edges in a linear chain,
CRFs make a first-order Markov independence assump-
tion, and thus can be understood as conditionally-trained
finite state machines (FSMs). In the remainder of this
section we introduce the likelihood model, inference and
estimation procedures for CRFs.
Let o = ?o1, o2, ...oT ? be some observed input data
sequence, such as a sequence of words in text in a doc-
ument, (the values on n input nodes of the graphical
model). Let S be a set of FSM states, each of which
is associated with a label, l ? L, (such as ORG). Let
s = ?s1, s2, ...sT ? be some sequence of states, (the val-
ues on T output nodes). By the Hammersley-Clifford the-
orem, CRFs define the conditional probability of a state
sequence given an input sequence to be
P?(s|o) =
1
Zo
exp
(
T?
t=1
?
k
?kfk(st?1, st,o, t)
)
,
where Zo is a normalization factor over all state se-
quences, fk(st?1, st,o, t) is an arbitrary feature func-
tion over its arguments, and ?k is a learned weight for
each feature function. A feature function may, for exam-
ple, be defined to have value 0 in most cases, and have
value 1 if and only if st?1 is state #1 (which may have
label OTHER), and st is state #2 (which may have la-
bel LOCATION), and the observation at position t in o
is a word appearing in a list of country names. Higher ?
weights make their corresponding FSM transitions more
likely, so the weight ?k in this example should be pos-
itive. More generally, feature functions can ask pow-
erfully arbitrary questions about the input sequence, in-
cluding queries about previous words, next words, and
conjunctions of all these, and fk(?) can range ??...?.
CRFs define the conditional probability of a label
sequence based on total probability over the state se-
quences, P?(l|o) =
?
s:l(s)=l P?(s|o), where l(s) is
the sequence of labels corresponding to the labels of the
states in sequence s.
Note that the normalization factor, Zo, is the sum
of the ?scores? of all possible state sequences, Zo =
?
s?ST exp
(?T
t=1
?
k ?kfk(st?1, st,o, t)
)
, and that
the number of state sequences is exponential in the in-
put sequence length, T . In arbitrarily-structured CRFs,
calculating the normalization factor in closed form is
intractable, but in linear-chain-structured CRFs, as in
forward-backward for hidden Markov models (HMMs),
the probability that a particular transition was taken be-
tween two CRF states at a particular position in the input
sequence can be calculated efficiently by dynamic pro-
gramming. We define slightly modified forward values,
?t(si), to be the ?unnormalized probability? of arriving
in state si given the observations ?o1, ...ot?. We set ?0(s)
equal to the probability of starting in each state s, and
recurse:
?t+1(s) =
?
s?
?t(s
?) exp
(
?
k
?kfk(s
?, s,o, t)
)
.
The backward procedure and the remaining details of
Baum-Welch are defined similarly. Zo is then
?
s ?T (s).
The Viterbi algorithm for finding the most likely state
sequence given the observation sequence can be corre-
spondingly modified from its HMM form.
2.1 Training CRFs
The weights of a CRF, ?={?, ...}, are set to maximize the
conditional log-likelihood of labeled sequences in some
training set, D = {?o, l?(1), ...?o, l?(j), ...?o, l?(N)}:
L? =
N?
j=1
log
(
P?(l(j)|o(j))
)
?
?
k
?2k
2?2
,
where the second sum is a Gaussian prior over parameters
(with variance ?) that provides smoothing to help cope
with sparsity in the training data.
When the training labels make the state sequence un-
ambiguous (as they often do in practice), the likelihood
function in exponential models such as CRFs is con-
vex, so there are no local maxima, and thus finding the
global optimum is guaranteed. It has recently been shown
that quasi-Newton methods, such as L-BFGS, are signifi-
cantly more efficient than traditional iterative scaling and
even conjugate gradient (Malouf, 2002; Sha and Pereira,
2003). This method approximates the second-derivative
of the likelihood by keeping a running, finite-sized win-
dow of previous first-derivatives.
L-BFGS can simply be treated as a black-box opti-
mization procedure, requiring only that one provide the
first-derivative of the function to be optimized. Assum-
ing that the training labels on instance j make its state
path unambiguous, let s(j) denote that path, and then the
first-derivative of the log-likelihood is
?L
??k
=
?
?
N?
j=1
Ck(s(j),o(j))
?
??
?
?
N?
j=1
?
s
P?(s|o(j))Ck(s,o(j))
?
??
?k
?2
where Ck(s,o) is the ?count? for feature k given s
and o, equal to
?T
t=1 fk(st?1, st,o, t), the sum of
fk(st?1, st,o, t) values for all positions, t, in the se-
quence s. The first two terms correspond to the differ-
ence between the empirical expected value of feature fk
and the model?s expected value: (E?[fk]?E?[fk])N . The
last term is the derivative of the Gaussian prior.
3 Efficient Feature Induction for CRFs
Typically the features, fk, are based on some number of
hand-crafted atomic observational tests (such as word is
capitalized or word is ?said?, or word appears in lexi-
con of country names), and a large collection of features
is formed by making conjunctions of the atomic tests in
certain user-defined patterns; (for example, the conjunc-
tions consisting of all tests at the current sequence po-
sition conjoined with all tests at the position one step
ahead?specifically, for instance, current word is capi-
talized and next word is ?Inc?). There can easily be
over 100,000 atomic tests (mostly based on tests for the
identity of words in the vocabulary), and ten or more
shifted-conjunction patterns?resulting in several million
features (Sha and Pereira, 2003). This large number of
features can be prohibitively expensive in memory and
computation; furthermore many of these features are ir-
relevant, and others that are relevant are excluded.
In response, we wish to use just those time-shifted
conjunctions that will significantly improve performance.
We start with no features, and over several rounds of fea-
ture induction: (1) consider a set of proposed new fea-
tures, (2) select for inclusion those candidate features that
will most increase the log-likelihood of the correct state
path s(j), and (3) train weights for all features. The pro-
posed new features are based on the hand-crafted obser-
vational tests?consisting of singleton tests, and binary
conjunctions of tests with each other and with features
currently in the model. The later allows arbitrary-length
conjunctions to be built. The fact that not all singleton
tests are included in the model gives the designer great
freedom to use a very large variety of observational tests,
and a large window of time shifts.
To consider the effect of adding a new feature, define
the new sequence model with additional feature, g, hav-
ing weight ?, to be
P?+g,?(s|o) =
P?(s|o) exp
(?T
t=1 ? g(st?1, st,o, t)
)
Zo(?, g, ?)
;
Zo(?, g, ?)
def
=
?
s? P?(s
?|o) exp(
?T
t=1 ? g(s
?
t?1, s
?
t,o, t))
in the denominator is simply the additional portion of
normalization required to make the new function sum to
1 over all state sequences.
Following (Della Pietra et al, 1997), we efficiently as-
sess many candidate features in parallel by assuming that
the ? parameters on all included features remain fixed
while estimating the gain, G(g), of a candidate feature, g,
based on the improvement in log-likelihood it provides,
G?(g) = max
?
G?(g, ?) = max
?
L?+g? ? L?.
where L?+g? includes ??2/2?2.
In addition, we make this approach tractable for CRFs
with two further reasonable and mutually-supporting ap-
proximations specific to CRFs. (1) We avoid dynamic
programming for inference in the gain calculation with
a mean-field approximation, removing the dependence
among states. (Thus we transform the gain from a se-
quence problem to a token classification problem. How-
ever, the original posterior distribution over states given
each token, P?(s|o) = ?t(s|o)?t+1(s|o)/Zo, is still
calculated by dynamic programming without approxima-
tion.) Furthermore, we can calculate the gain of aggre-
gate features irrespective of transition source, g(st,o, t),
and expand them after they are selected. (2) In many
sequence problems, the great majority of the tokens are
correctly labeled even in the early stages of training. We
significantly gain efficiency by including in the gain cal-
culation only those tokens that are mislabeled by the cur-
rent model. Let {o(i) : i = 1...M} be those tokens, and
o(i) be the input sequence in which the ith error token
occurs at position t(i). Then algebraic simplification us-
ing these approximations and previous definitions gives
G?(g, ?) =
M?
i=1
log
(
exp
(
? g(st(i),o(i), t(i))
)
Zo(i)(?, g, ?)
)
?
?2
2?2
= M?E?[g] ?
M?
i=1
log(E?[exp(? g)|o(i)] ?
?2
2?2
,
where Zo(i)(?, g, ?) (with non-bold o) is simply?
s P?(s|o(i)) exp(?g(s,o(i), t(i))). The optimal val-
ues of the ??s cannot be solved in closed form, but New-
ton?s method finds them all in about 12 quick iterations.
There are two additional important modeling choices:
(1) Because we expect our models to still require sev-
eral thousands of features, we save time by adding many
of the features with highest gain each round of induction
rather than just one; (including a few redundant features
is not harmful). (2) Because even models with a small se-
lect number of features can still severely overfit, we train
the model with just a few BFGS iterations (not to con-
vergence) before performing the next round of feature in-
duction. Details are in (McCallum, 2003).
4 Web-augmented Lexicons
Some general-purpose lexicons, such a surnames and lo-
cation names, are widely available, however, many nat-
ural language tasks will benefit from more task-specific
lexicons, such as lists of soccer teams, political parties,
NGOs and English counties. Creating new lexicons en-
tirely by hand is tedious and time consuming.
Using a technique we call WebListing, we build lexi-
cons automatically from HTML data on the Web. Previ-
ous work has built lexicons from fixed corpora by deter-
mining linguistic patterns for the context in which rele-
vant words appear (Collins and Singer, 1999; Jones et al,
1999). Rather than mining a small corpus, we gather data
from nearly the entire Web; rather than relying on fragile
linguistic context patterns, we leverage robust formatting
regularities on the Web. WebListing finds co-occurrences
of seed terms that appear in an identical HTML format-
ting pattern, and augments a lexicon with other terms on
the page that share the same formatting. Our current im-
plementation uses GoogleSets, which we understand to
be a simple implementation of this approach based on us-
ing HTML list items as the formatting regularity. We are
currently building a more sophisticated replacement.
5 Results
To perform named entity extraction on the news articles
in the CoNLL-2003 English shared task, several families
of features are used, all time-shifted by -2, -1, 0, 1, 2: (a)
the word itself, (b) 16 character-level regular expressions,
mostly concerning capitalization and digit patterns, such
as A, A+, Aa+, Aa+Aa*, A., D+, where A, a and D indi-
cate the regular expressions [A-Z], [a-z] and [0-9],
(c) 8 lexicons entered by hand, such as honorifics, days
and months, (d) 15 lexicons obtained from specific web
sites, such as countries, publicly-traded companies, sur-
names, stopwords, and universities, (e) 25 lexicons ob-
tained by WebListing (including people names, organi-
zations, NGOs and nationalities), (f) all the above tests
with prefix firstmention from any previous duplicate of
the current word, (if capitalized). A small amount of
hand-filtering was performed on some of the WebList-
ing lexicons. Since GoogleSets? support for non-English
is severely limited, only 5 small lexicons were used for
German; but character bi- and tri-grams were added.
A Java-implemented, first-order CRF was trained for
about 12 hours on a 1GHz Pentium with a Gaussian prior
variance of 0.5, inducing 1000 or fewer features (down
to a gain threshold of 5.0) each round of 10 iterations of
L-BFGS. Candidate conjunctions are limited to the 1000
atomic and existing features with highest gain. Perfor-
mance results for each of the entity classes can be found
in Figure 1. The model achieved an overall F1 of 84.04%
on the English test set using 6423 features. (Using a set
of fixed conjunction patterns instead of feature induction
results in F1 73.34%, with about 1 million features; trial-
and-error tuning the fixed patterns would likely improve
this.) Accuracy gains are expected from experimentation
with the induction parameters and improved WebListing.
Acknowledgments
We thank John Lafferty, Fernando Pereira, Andres Corrada-
Emmanuel, Drew Bagnell and Guy Lebanon, for helpful
input. This work was supported in part by the Center
for Intelligent Information Retrieval, SPAWARSYSCEN-SD
grant numbers N66001-99-1-8912 and N66001-02-1-8903, Ad-
vanced Research and Development Activity under contract
number MDA904-01-C-0984, and DARPA contract F30602-
01-2-0566.
References
A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman. 1998.
Exploiting diverse knowledge sources via maximum entropy
in named entity recognition. In Proceedings of the Sixth
Workshop on Very Large Corpora, Association for Compu-
tational Linguistics.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In Proceedings of the Joint SIG-
DAT Conference on Empirical Methods in Natural Language
Processing and Very Large Corpora.
Stephen Della Pietra, Vincent J. Della Pietra, and John D. Laf-
ferty. 1997. Inducing Features of Random Fields. IEEE
English devel. Precision Recall F?=1
LOC 93.82% 91.78% 92.79
MISC 83.99% 78.52% 81.17
ORG 84.23% 82.03% 83.11
PER 92.64% 93.65% 93.14
Overall 89.84% 88.10% 88.96
English test Precision Recall F?=1
LOC 87.23% 87.65% 87.44
MISC 74.44% 71.37% 72.87
ORG 79.52% 78.33% 78.92
PER 91.05% 89.98% 90.51
Overall 84.52% 83.55% 84.04
German devel. Precision Recall F?=1
LOC 68.55% 68.84% 68.69
MISC 72.66% 45.25% 55.77
ORG 70.64% 54.88% 61.77
PER 82.21% 64.31% 72.17
Overall 73.60% 59.01% 65.50
German test Precision Recall F?=1
LOC 71.92% 69.28% 70.57
MISC 69.59% 42.69% 52.91
ORG 63.85% 48.90% 55.38
PER 90.04% 74.14% 81.32
Overall 75.97% 61.72% 68.11
Table 1: English and German named entity extraction.
Transactions on Pattern Analysis and Machine Intelligence,
19(4):380?393.
Rosie Jones, Andrew McCallum, Kamal Nigam, and Ellen
Riloff. 1999. Bootstrapping for Text Learning Tasks. In
IJCAI-99 Workshop on Text Mining: Foundations, Tech-
niques and Applications.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. ICML.
Robert Malouf. 2002. A comparison of algorithms for max-
imum entropy parameter estimation. In Sixth Workshop on
Computational Language Learning (CoNLL-2002).
Andrew McCallum and Fang-Fang Feng. 2003. Chinese
Word Segmentation with Conditional Random Fields and In-
tegrated Domain Knowledge. In Unpublished Manuscript.
Andrew McCallum. 2003. Efficiently Inducing Features of
Conditional Random Fields. In Nineteenth Conference on
Uncertainty in Artificial Intelligence (UAI03). (Submitted).
Adwait Ratnaparkhi. 1996. A Maximum Entropy Model for
Part-of-Speech Tagging. In Eric Brill and Kenneth Church,
editors, Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 133?142. Asso-
ciation for Computational Linguistics.
Fei Sha and Fernando Pereira. 2003. Shallow Parsing with
Conditional Random Fields. In Proceedings of Human Lan-
guage Technology, NAACL.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 225?228, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Joint Parsing and Semantic Role Labeling
Charles Sutton and Andrew McCallum
Department of Computer Science
University of Massachusetts
Amherst, MA 01003 USA
{casutton,mccallum}@cs.umass.edu
Abstract
A striking feature of human syntactic pro-
cessing is that it is context-dependent, that
is, it seems to take into account seman-
tic information from the discourse con-
text and world knowledge. In this paper,
we attempt to use this insight to bridge
the gap between SRL results from gold
parses and from automatically-generated
parses. To do this, we jointly perform
parsing and semantic role labeling, using
a probabilistic SRL system to rerank the
results of a probabilistic parser. Our cur-
rent results are negative, because a locally-
trained SRL model can return inaccurate
probability estimates.
1 Introduction
Although much effort has gone into developing
statistical parsing models and they have improved
steadily over the years, in many applications that
use parse trees errors made by the parser are a ma-
jor source of errors in the final output. A promising
approach to this problem is to perform both pars-
ing and the higher-level task in a single, joint prob-
abilistic model. This not only allows uncertainty
about the parser output to be carried upward, such
as through an k-best list, but also allows informa-
tion from higher-level processing to improve pars-
ing. For example, Miller et al (2000) showed that
performing parsing and information extraction in a
joint model improves performance on both tasks. In
particular, one suspects that attachment decisions,
which are both notoriously hard and extremely im-
portant for semantic analysis, could benefit greatly
from input from higher-level semantic analysis.
The recent interest in semantic role labeling pro-
vides an opportunity to explore how higher-level se-
mantic information can inform syntactic parsing. In
previous work, it has been shown that SRL systems
that use full parse information perform better than
those that use shallow parse information, but that
machine-generated parses still perform much worse
than human-corrected gold parses.
The goal of this investigation is to narrow the gap
between SRL results from gold parses and from au-
tomatic parses. We aim to do this by jointly perform-
ing parsing and semantic role labeling in a single
probabilistic model. In both parsing and SRL, state-
of-the-art systems are probabilistic; therefore, their
predictions can be combined in a principled way by
multiplying probabilities. In this paper, we rerank
the k-best parse trees from a probabilistic parser us-
ing an SRL system. We compare two reranking ap-
proaches, one that linearly weights the log proba-
bilities, and the other that learns a reranker over
parse trees and SRL frames in the manner of Collins
(2000).
Currently, neither method performs better than
simply selecting the top predicted parse tree. We
discuss some of the reasons for this; one reason be-
ing that the ranking over parse trees induced by the
semantic role labeling score is unreliable, because
the model is trained locally.
2 Base SRL System
Our approach to joint parsing and SRL begins with
a base SRL system, which uses a standard architec-
ture from the literature. Our base SRL system is a
cascade of maximum-entropy classifiers which se-
lect the semantic argument label for each constituent
of a full parse tree. As in other systems, we use
three stages: pruning, identification, and classifica-
tion. First, in pruning, we use a deterministic pre-
processing procedure introduced by Xue and Palmer
(2004) to prune many constituents which are almost
certainly not arguments. Second, in identification,
a binary MaxEnt classifier is used to prune remain-
ing constituents which are predicted to be null with
225
Base features [GJ02]
Path to predicate
Constituent type
Head word
Position
Predicate
Head POS [SHWA03]
All conjunctions of above
Table 1: Features used in base identification classi-
fier.
high probability. Finally, in classification, a multi-
class MaxEnt classifier is used to predict the argu-
ment type of the remaining constituents. This clas-
sifer also has the option to output NULL.
It can happen that the returned semantic argu-
ments overlap, because the local classifiers take no
global constraints into account. This is undesirable,
because no overlaps occur in the gold semantic an-
notations. We resolve overlaps using a simple recur-
sive algorithm. For each parent node that overlaps
with one of its descendents, we check which pre-
dicted probability is greater: that the parent has its
locally-predicted argument label and all its descen-
dants are null, or that the descendants have their op-
timal labeling, and the parent is null. This algorithm
returns the non-overlapping assignment with glob-
ally highest confidence. Overlaps are uncommon,
however; they occurred only 68 times on the 1346
sentences in the development set.
We train the classifiers on PropBank sections 02?
21. If a true semantic argument fails to match
any bracketing in the parse tree, then it is ignored.
Both the identification and classification models are
trained using gold parse trees. All of our features are
standard features for this task that have been used
in previous work, and are listed in Tables 1 and 2.
We use the maximum-entropy implementation in the
Mallet toolkit (McCallum, 2002) with a Gaussian
prior on parameters.
3 Reranking Parse Trees Using SRL
Information
Here we give the general framework for the rerank-
ing methods that we present in the next section. We
write a joint probability model over semantic frames
F and parse trees t given a sentence x as
p(F, t|x) = p(F |t,x)p(t|x), (1)
where p(t|x) is given by a standard probabilistic
parsing model, and p(F |t,x) is given by the base-
line SRL model described previously.
Base features [GJ02]
Head word
Constituent type
Position
Predicate
Voice
Head POS [SHWA03]
From [PWHMJ04]
Parent Head POS
First word / POS
Last word / POS
Sibling constituent type / head word / head POS
Conjunctions [XP03]
Voice & Position
Predicate & Head word
Predicate & Constituent type
Table 2: Features used in baseline labeling classifier.
Parse Trees Used SRL F1
Gold 77.1
1-best 63.9
Reranked by gold parse F1 68.1
Reranked by gold frame F1 74.2
Simple SRL combination (? = 0.5) 56.9
Chosen using trained reranker 63.6
Table 3: Comparison of Overall SRL F1 on devel-
opment set by the type of parse trees used.
In this paper, we choose (F ?, t?) to approximately
maximize the probability p(F, t|x) using a reranking
approach. To do the reranking, we generate a list of
k-best parse trees for a sentence, and for each pre-
dicted tree, we predict the best frame using the base
SRL model. This results in a list {(F i, ti)} of parse
tree / SRL frame pairs, from which the reranker
chooses. Thus, our different reranking methods vary
only in which parse tree is selected; given a parse
tree, the frame is always chosen using the best pre-
diction from the base model.
The k-best list of parses is generated using Dan
Bikel?s (2004) implementation of Michael Collins?
parsing model. The parser is trained on sections 2?
21 of the WSJ Treebank, which does not overlap
with the development or test sets. The k-best list is
generated in Bikel?s implementation by essentially
turning off dynamic programming and doing very
aggressive beam search. We gather a maximum of
500 best parses, but the limit is not usually reached
using feasible beam widths. The mean number of
parses per sentence is 176.
4 Results and Discussion
In this section we present results on several rerank-
ing methods for joint parsing and semantic role la-
226
beling. Table 3 compares F1 on the development set
of our different reranking methods. The first four
rows in Table 3 are baseline systems. We present
baselines using gold trees (row 1 in Table 3) and
predicted trees (row 2). As shown in previous work,
gold trees perform much better than predicted trees.
We also report two cheating baselines to explore
the maximum possible performance of a reranking
system. First, we report SRL performance of ceil-
ing parse trees (row 3), i.e., if the parse tree from the
k-best list is chosen to be closest to the gold tree.
This is the best expected performance of a parse
reranking approach that maximizes parse F1. Sec-
ond, we report SRL performance where the parse
tree is selected to maximize SRL F1, computing
using the gold frame (row 4). There is a signifi-
cant gap both between parse-F1-reranked trees and
SRL-F1-reranked trees, which shows promise for
joint reranking. However, the gap between SRL-
F1-reranked trees and gold parse trees indicates that
reranking of parse lists cannot by itself completely
close the gap in SRL performance between gold and
predicted parse trees.
4.1 Reranking based on score combination
Equation 1 suggests a straightforward method for
reranking: simply pick the parse tree from the k-best
list that maximizes p(F, t|x), in other words, add the
log probabilities from the parser and the base SRL
system. More generally, we consider weighting the
individual probabilities as
s(F, t) = p(F |t,x)1??p(t|x)?. (2)
Such a weighted combination is often used in the
speech community to combine acoustic and lan-
guage models.
This reranking method performs poorly, however.
No choice of ? performs better than ? = 1, i.e.,
choosing the 1-best predicted parse tree. Indeed, the
more weight given to the SRL score, the worse the
combined system performs. The problem is that of-
ten a bad parse tree has many nodes which are obvi-
ously not constituents: thus p(F |t,x) for such a bad
tree is very high, and therefore not reliable. As more
weight is given to the SRL score, the unlabeled re-
call drops, from 55% when ? = 0 to 71% when
? = 1. Most of the decrease in F1 is due to the drop
in unlabeled recall.
4.2 Training a reranker using global features
One potential solution to this problem is to add
features of the entire frame, for example, to vote
against predicted frames that are missing key argu-
ments. But such features depend globally on the en-
tire frame, and cannot be represented by local clas-
sifiers. One way to train these global features is to
learn a linear classifier that selects a parse / frame
pair from the ranked list, in the manner of Collins
(2000). Reranking has previously been applied to
semantic role labeling by Toutanova et al (2005),
from which we use several features. The difference
between this paper and Toutanova et al is that in-
stead of reranking k-best SRL frames of a single
parse tree, we are reranking 1-best SRL frames from
the k-best parse trees.
Because of the the computational expense of
training on k-best parse tree lists for each of 30,000
sentences, we train the reranker only on sections 15?
18 of the Treebank (the same subset used in previ-
ous CoNLL competitions). We train the reranker
using LogLoss, rather than the boosting loss used
by Collins. We also restrict the reranker to consider
only the top 25 parse trees.
This globally-trained reranker uses all of the fea-
tures from the local model, and the following global
features: (a) sequence features, i.e., the linear se-
quence of argument labels in the sentence (e.g.
A0_V_A1), (b) the log probability of the parse tree,
(c) has-arg features, that is, for each argument type
a binary feature indicating whether it appears in the
frame, (d) the conjunction of the predicate and has-
arg feature, and (e) the number of nodes in the tree
classified as each argument type.
The results of this system on the development set
are given in Table 3 (row 6). Although this performs
better than the score combination method, it is still
no better than simply taking the 1-best parse tree.
This may be due to the limited training set we used
in the reranking model. A base SRL model trained
only on sections 15?18 has 61.26 F1, so in com-
parison, reranking provides a modest improvement.
This system is the one that we submitted as our offi-
cial submission. The results on the test sets are given
in Table 4.
5 Summing over parse trees
In this section, we sketch a different approach to
joint SRL and parsing that does not use rerank-
ing at all. Maximizing over parse trees can mean
that poor parse trees can be selected if their se-
mantic labeling has an erroneously high score. But
we are not actually interested in selecting a good
parse tree; all we want is a good semantic frame.
This means that we should select the semantic frame
227
Precision Recall F?=1
Development 64.43% 63.11% 63.76
Test WSJ 68.57% 64.99% 66.73
Test Brown 62.91% 54.85% 58.60
Test WSJ+Brown 67.86% 63.63% 65.68
Test WSJ Precision Recall F?=1
Overall 68.57% 64.99% 66.73
A0 69.47% 74.35% 71.83
A1 66.90% 64.91% 65.89
A2 64.42% 61.17% 62.75
A3 62.14% 50.29% 55.59
A4 72.73% 70.59% 71.64
A5 50.00% 20.00% 28.57
AM-ADV 55.90% 49.60% 52.57
AM-CAU 76.60% 49.32% 60.00
AM-DIR 57.89% 38.82% 46.48
AM-DIS 79.73% 73.75% 76.62
AM-EXT 66.67% 43.75% 52.83
AM-LOC 50.26% 53.17% 51.67
AM-MNR 54.32% 51.16% 52.69
AM-MOD 98.50% 95.46% 96.96
AM-NEG 98.20% 94.78% 96.46
AM-PNC 46.08% 40.87% 43.32
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 72.15% 67.43% 69.71
R-A0 0.00% 0.00% 0.00
R-A1 0.00% 0.00% 0.00
R-A2 0.00% 0.00% 0.00
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-TMP 0.00% 0.00% 0.00
V 99.21% 86.24% 92.27
Table 4: Overall results (top) and detailed results on
the WSJ test (bottom).
that maximizes the posterior probability: p(F |x) =
?
t p(F |t,x)p(t|x). That is, we should be sum-
ming over the parse trees instead of maximizing over
them. The practical advantage of this approach is
that even if one seemingly-good parse tree does not
have a constituent for a semantic argument, many
other parse trees in the k-best list might, and all
are considered when computing F ?. Also, no sin-
gle parse tree need have constituents for all of F ?;
because it sums over all parse trees, it can mix and
match constituents between different trees. The op-
timal frame F ? can be computed by an O(N3) pars-
ing algorithm if appropriate independence assump-
tions are made on p(F |x). This requires designing
an SRL model that is independent of the bracketing
derived from any particular parse tree. Initial experi-
ments performed poorly because the marginal model
p(F |x) was inadequate. Detailed exploration is left
for future work.
6 Conclusion and Related Work
In this paper, we have considered several methods
for reranking parse trees using information from se-
mantic role labeling. So far, we have not been
able to show improvement over selecting the 1-best
parse tree. Gildea and Jurafsky (Gildea and Jurafsky,
2002) also report results on reranking parses using
an SRL system, with negative results. In this paper,
we confirm these results with a MaxEnt-trained SRL
model, and we extend them to show that weighting
the probabilities does not help either.
Our results with Collins-style reranking are too
preliminary to draw definite conclusions, but the po-
tential improvement does not appear to be great. In
future work, we will explore the max-sum approach,
which has promise to avoid the pitfalls of max-max
reranking approaches.
Acknowledgements
This work was supported in part by the Center for Intelligent
Information Retrieval, in part by National Science Foundation
under NSF grants #IIS-0326249 ond #IIS-0427594, and in part
by the Defense Advanced Research Projec ts Agency (DARPA),
through the Department of the Interior, NBC, Acquisition Ser-
vices Division, under contract number NBCHD030010. Any
opinions, findings and conclusions or recommendations ex-
pressed in this material are the author(s) and do not necessarily
reflect those of the sponsor.
References
Daniel M. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics.
Michael Collins. 2000. Discriminative reranking for natu-
ral language parsing. In Proc. 17th International Conf. on
Machine Learning, pages 175?182. Morgan Kaufmann, San
Francisco, CA.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3):245?288.
Andrew Kachites McCallum. 2002. Mallet: A machine learn-
ing for language toolkit. http://mallet.cs.umass.
edu.
Scott Miller, Heidi Fox, Lance A. Ramshaw, and Ralph M.
Weischedel. 2000. A novel use of statistical parsing to ex-
tract information from text. In ANLP 2000, pages 226?233.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul
Aarseth. 2003. Using predicate-argument structures for in-
formation extraction. In ACL-2003.
Kristina Toutanova, Aria Haghighi, and Christopher D. Man-
ning. 2005. Joint learning improves semantic role labeling.
In ACL 2005.
Nianwen Xue and Martha Palmer. 2004. Calibrating features
for semantic role labeling. In Proceedings of 2004 Confer-
ence on Empirical Methods in Natural Language Process-
ing.
228
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 603?611,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning Field Compatibilities
to Extract Database Records from Unstructured Text
Michael Wick, Aron Culotta and Andrew McCallum
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
{mwick, culotta, mccallum}@cs.umass.edu
Abstract
Named-entity recognition systems extract
entities such as people, organizations, and
locations from unstructured text. Rather
than extract these mentions in isolation,
this paper presents a record extraction sys-
tem that assembles mentions into records
(i.e. database tuples). We construct a
probabilistic model of the compatibility
between field values, then employ graph
partitioning algorithms to cluster fields
into cohesive records. We also investigate
compatibility functions over sets of fields,
rather than simply pairs of fields, to ex-
amine how higher representational power
can impact performance. We apply our
techniques to the task of extracting contact
records from faculty and student home-
pages, demonstrating a 53% error reduc-
tion over baseline approaches.
1 Introduction
Information extraction (IE) algorithms populate a
database with facts discovered from unstructured
text. This database is often used by higher-level
tasks such as question answering or knowledge
discovery. The richer the structure of the database,
the more useful it is to higher-level tasks.
A common IE task is named-entity recognition
(NER), the problem of locating mentions of en-
tities in text, such as people, places, and organi-
zations. NER techniques range from regular ex-
pressions to finite-state sequence models (Bikel et
al., 1999; Grishman, 1997; Sutton and McCallum,
2006). NER can be viewed as method of populat-
ing a database with single-tuple records, e.g. PER-
SON=Cecil Conner or ORGANIZATION= IBM.
We can add richer structure to these single-tuple
records by extracting the associations among en-
tities. For example, we can populate multi-field
records such as a contact record [PERSON=Steve
Jobs, JOBTITLE = CEO, COMPANY = Apple,
CITY = Cupertino, STATE = CA]. The relational
information in these types of records presents a
greater opportunity for text analysis.
The task of associating together entities is of-
ten framed as a binary relation extraction task:
Given a pair of entities, label the relation be-
tween them (e.g. Steve Jobs LOCATED-IN Cuper-
tino). Common approaches to relation extraction
include pattern matching (Brin, 1998; Agichtein
and Gravano, 2000) and classification (Zelenko et
al., 2003; Kambhatla, 2004).
However, binary relation extraction alone is not
well-suited for the contact record example above,
which requires associating together many fields
into one record. We refer to this task of piecing
together many fields into a single record as record
extraction.
Consider the task of extracting contact records
from personal homepages. An NER system may
label all mentions of cities, people, organizations,
phone numbers, job titles, etc. on a page, from
both semi-structured an unstructured text. Even
with a highly accurate NER system, it is not obvi-
ous which fields belong to the same record. For
example, a single document could contain five
names, three phone numbers and only one email.
Additionally, the layout of certain fields may be
convoluted or vary across documents.
Intuitively, we would like to learn the compat-
ibility among fields, for example the likelihood
that the organization University of North Dakota
is located in the state North Dakota, or that phone
numbers with area code 212 co-occur with the
603
city New York. Additionally, the system should
take into account page layout information, so that
nearby fields are more likely to be grouped into the
same record.
In this paper, we describe a method to induce a
probabilistic compatibility function between sets
of fields. Embedding this compatibility func-
tion within a graph partitioning method, we de-
scribe how to cluster highly compatible fields into
records.
We evaluate our approach on personal home-
pages that have been manually annotated with
contact record information, and demonstrate a
53% error reduction over baseline methods.
2 Related Work
McDonald et al (2005) present clustering tech-
niques to extract complex relations, i.e. relations
with more than two arguments. Record extraction
can be viewed as an instance of complex relation
extraction. We build upon this work in three ways:
(1) Our system learns the compatibility between
sets of fields, rather than just pairs of field; (2) our
system is not restricted to relations between en-
tities in the same sentence; and (3) our problem
domain has a varying number of fields per record,
as opposed to the fixed schema in McDonald et al
(2005).
Bansal et al (2004) present algorithms for the
related task of correlational clustering: finding an
optimal clustering from a matrix of pairwise com-
patibility scores. The correlational clustering ap-
proach does not handle compatibility scores calcu-
lated over sets of nodes, which we address in this
paper.
McCallum and Wellner (2005) discriminatively
train a model to learn binary coreference deci-
sions, then perform joint inference using graph
partitioning. This is analogous to our work, with
two distinctions. First, instead of binary coref-
erence decisions, our model makes binary com-
patibility decisions, reflecting whether a set of
fields belong together in the same record. Second,
whereas McCallum and Wellner (2005) factor the
coreference decisions into pairs of vertices, our
compatibility decisions are made between sets of
vertices. As we show in our experiments, factoring
decisions into sets of vertices enables more power-
ful features that can improve performance. These
higher-order features have also recently been in-
vestigated in other models of coreference, both
discriminative (Culotta and McCallum, 2006) and
generative (Milch et al, 2005).
Viola and Narasimhan (2005) present a prob-
abilistic grammar to parse contact information
blocks. While this model is capable of learn-
ing long-distance compatibilities (such as City and
State relations), features to enable this are not ex-
plored. Additionally, their work focuses on la-
beling fields in documents that have been pre-
segmented into records. This record segmentation
is precisely what we address in this paper.
Borkar et al (2001) and Kristjannson et al
(2004) also label contact address blocks, but ig-
nore the problem of clustering fields into records.
Also, Culotta et al (2004) automatically extract
contact records from web pages, but use heuristics
to cluster fields into records.
Embley et al (1999) provide heuristics to de-
tect record boundaries in highly structured web
documents, such as classified ads, and Embley
and Xu (2000) improve upon these heuristics for
slightly more ambiguous domains using a vector
space model. Both of these techniques apply to
data for which the records are highly contiguous
and have a distinctive separator between records.
These heuristic approaches are unlikely to be suc-
cessful in the unstructured text domain we address
in this paper.
Most other work on relation extraction focuses
only on binary relations (Zelenko et al, 2003;
Miller et al, 2000; Agichtein and Gravano, 2000;
Culotta and Sorensen, 2004). A serious difficulty
in applying binary relation extractors to the record
extraction task is that rather than enumerating over
all pairs of entities, the system must enumerate
over all subsets of entities, up to subsets of size
k, the maximum number of fields per record. We
address this difficulty by employing two sampling
methods: one that samples uniformly, and another
that samples on a focused subset of the combina-
torial space.
3 From Fields to Records
3.1 Problem Definition
Let a fieldF be a pair ?a, v?, where a is an attribute
(column label) and v is a value, e.g. Fi = ?CITY,
San Francisco?. Let record R be a set of fields,
R = {F1 . . . Fn}. Note that R may contain mul-
tiple fields with the same attribute but different
values (e.g. a person may have multiple job ti-
tles). Assume we are given the output of a named-
604
entity recognizer, which labels tokens in a doc-
ument with their attribute type (e.g. NAME or
CITY). Thus, a document initially contains a set
of fields, {F1 . . . Fm}.
The task is to partition the fields in each anno-
tated document into a set of records {R1 . . . Rk}
such that each record Ri contains exactly the set
of fields pertinent to that record. In this paper, we
assume each field belongs to exactly one record.
3.2 Solution Overview
For each document, we construct a fully-
connected weighted graph G = (V,E), with ver-
tices V and weighted edges E. Each field in the
document is represented by a vertex in V , and the
edges are weighted by the compatibility of adja-
cent fields, i.e. a measure of how likely it is that
Fi and Fj belong to the same record.
Partitioning V into k disjoint clusters uniquely
maps the set of fields to a set of k records. Be-
low, we provide more detail on the two principal
steps in our solution: (1) estimating the compati-
bility function and (2) partitioning V into disjoint
clusters.
3.3 Learning field compatibility
Let F be a candidate cluster of fields forming a
partial record. We construct a compatibility func-
tion C that maps two sets of fields to a real value,
i.e. C : Fi ? Fj ? R. We abbreviate the value
C(Fi,Fj) as Cij . The higher the value of Cij the
more likely it is that Fi and Fj belong to the same
record.
For example, in the contact record domain, Cij
can reflect whether a city and state should co-
occur, or how likely a company is to have a certain
job title.
We represent Cij by a maximum-entropy clas-
sifier over the binary variable Sij , which is true if
and only if field set Fi belongs to the same record
as field set Fj . Thus, we model the conditional
distribution
P?(Sij |Fi,Fj) ? exp
(
?
k
?kfk(Sij ,Fi,Fj)
)
where fk is a binary feature function that com-
putes attributes over the field sets, and ? = {?k}
is the set of real-valued weights that are the pa-
rameters of the maximum-entropy model. We set
Cij = P?(Sij =true|Fi,Fj). This approach can
be viewed as a logistic regression model for field
compatibility.
Examples of feature functions include format-
ting evidence (Fi appears at the top of the docu-
ment, Fj at the bottom), conflicting value infor-
mation (Fi and Fj contain conflicting values for
the state field), or other measures of compatibility
(a city value in Fi is known to exist in a state in
Fj). A feature may involve more than one field,
for example, if a name, title and university occurs
consecutively in some order. We give a more de-
tailed description of the feature functions in Sec-
tion 4.3.
We propose learning the ? weights for each of
these features using supervised machine learning.
Given a set of documents D for which the true
mapping from fields to set of records is known,
we wish to estimate P (Sij |Fi,Fj) for all pairs of
field sets Fi,Fj .
Enumerating all positive and negative pairs of
field sets is computationally infeasible for large
datasets, so we instead propose two sampling
methods to generate training examples. The first
simply samples pairs of field sets uniformly from
the training data. For example, given a document
D containing true records {R1 . . . Rk}, we sam-
ple positive and negative examples of field sets of
varying sizes from {Ri . . . Rj}. The second sam-
pling method first trains the model using the exam-
ples generated by uniform sampling. This model
is then used to cluster the training data. Additional
training examples are created during the clustering
process and are used to retrain the model parame-
ters. This second sampling method is an attempt to
more closely align the characteristics of the train-
ing and testing examples.
Given a sample of labeled training data, we set
the parameters of the maximum-entropy classi-
fier in standard maximum-likelihood fashion, per-
forming gradient ascent on the log-likelihood of
the training data. The resulting weights indi-
cate how important each feature is in determin-
ing whether two sets of fields belong to the same
record.
3.4 Partitioning Fields into Records
One could employ the estimated classifier to con-
vert fields into records as follows: Classify each
pair of fields as positive or negative, and perform
transitive closure to enforce transitivity of deci-
sions. That is, if the classifier determines that A
and B belong to the same record and that B and
C belong to the same record, then by transitivity
605
A and C must belong to the same record. The
drawback of this approach is that the compatibility
between A and C is ignored. In cases where the
classifier determines that A and C are highly in-
compatible, transitive closure can lead to poor pre-
cision. McCallum and Wellner (2005) explore this
issue in depth for the related task of noun corefer-
ence resolution.
With this in mind, we choose to avoid transitive
closure, and instead employ a graph partitioning
method to make record merging decisions jointly.
Given a document D with fields {F1 . . . Fn},
we construct a fully connected graph G = (V,E),
with edge weights determined by the learned com-
patibility functionC. We wish to partition vertices
V into clusters with high intra-cluster compatibil-
ity.
One approach is to simply use greedy agglom-
erative clustering: initialize each vertex to its own
cluster, then iteratively merge clusters with the
highest inter-cluster edge weights. The compati-
bility between two clusters can be measured using
single-link or average-link clustering. The clus-
tering algorithm converges when the inter-cluster
edge weight between any pair of clusters is below
a specified threshold.
We propose a modification to this approach.
Since the compatibility function we have de-
scribed maps two sets of vertices to a real value,
we can use this directly to calculate the compati-
bility between two clusters, rather than performing
average or single link clustering.
We now describe the algorithmmore concretely.
? Input: (1) Graph G = (V,E), where each
vertex vi represents a field Fi. (2) A threshold
value ? .
? Initialization: Place each vertex vi in its own
cluster R?i. (The hat notation indicates that
this cluster represents a possible record.)
? Iterate: Re-calculate the compatibility func-
tion Cij between each pair of clusters. Merge
the two most compatible clusters, R??i , R?
?
j .
? Termination: If there does not exist a pair of
clusters R?i, R?j such that Cij > ? , the algo-
rithm terminates and returns the current set of
clusters.
A natural threshold value is ? = 0.5, since this
is the point at which the binary compatibility clas-
sifier predicts that the fields belong to different
records. In Section 4.4, we examine how perfor-
mance varies with ? .
3.5 Representational power of cluster
compatibility functions
Most previous work on inducing compatibility
functions learns the compatibility between pairs of
vertices, not clusters of vertices. In this section,
we provide intuition to explain why directly mod-
eling the compatibility of clusters of vertices may
be advantageous. We refer to the cluster compat-
ibility function as Cij , and the pairwise (binary)
compatibility function as Bij .
First, we note that Cij is a generalization of
single-link and average-link clustering methods
that use Bij , since the output of these methods
can simply be included as features in Cij . For ex-
ample, given two clusters R?i = {v1, v2, v3} and
R?j = {v4, v5, v6}, average-link clustering calcu-
lates the inter-cluster score between R?i and R?j as
SAL(R?i, R?j) =
1
|R?i||R?j |
?
a?R?i,b?R?j
Bab
SAL(R?i, R?j) can be included as a feature for
the compatibility function Cij , with an associated
weight estimated from training data.
Second, there may exist phenomena of the data
that can only be captured by a classifier that con-
siders ?higher-order? features. Below we describe
two such cases.
In the first example, consider three vertices of
mild compatibility, as in Figure 1(a). (For these
examples, let Bij , Cij ? [0, 1].) Suppose that
these three phone numbers occur nearby in a doc-
ument. Since it is not uncommon for a person to
have two phone numbers with different area codes,
the pairwise compatibility function may score any
pair of nearby phone numbers as relatively com-
patible. However, since it is fairly uncommon for
a person to have three phone numbers with three
different area codes, we would not like all three
numbers to be merged into the same record.
Assume an average-link clustering algorithm.
After merging together the 333 and 444 numbers,
Bij will recompute the new inter-cluster compat-
ibility as 0.51, the average of the inter-cluster
edges. In contrast, the cluster compatibility func-
tion Cij can represent the fact that three numbers
with different area codes are to be merged, and can
penalize their compatibility accordingly. Thus, in
606
333-555-5555
666-555-5555444-555-5555
.6
.49
.53
333-555-5555
666-555-5555
444-555-5555
.6
C = 0.1
  B = 0.51
(a)
Univ of North 
Dakota, 
Pleasantville
Pleasantville
North 
Dakota
.48
.49
.9
Univ of North 
Dakota, 
Pleasantville
Pleasantville
North 
Dakota
C = 0.8
    B = 0.485
.9
(b)
Figure 1: Two motivating examples illustrating why the cluster compatibility measure (C) may have
higher representational power than the pairwise compatibility measure (B). In (a), the pairwise measure
over-estimates the inter-cluster compatibility when there exist higher-order features such as A person
is unlikely to have phone numbers with three different area codes. In (b), the pairwise measure under-
estimates inter-cluster compatibility when weak features like string comparisons can be combined into a
more powerful feature by examining multiple field values.
this example, the pairwise compatibility function
over-estimates the true compatibility.
In the second example (Figure 1(b)), we con-
sider the opposite case. Consider three edges,
two of which have weak compatibility, and one of
which has high compatibility. For example, per-
haps the system has access to a list of city-state
pairs, and can reliably conclude that Pleasantville
is a city in the state North Dakota.
Deciding that Univ of North Dakota, Pleas-
antville belongs in the same record as North
Dakota and Pleasantville is a bit more difficult.
Suppose a feature function measures the string
similarity between the city field Pleasantville and
the company field Univ of North Dakota, Pleas-
antville. Alone, this string similarity might not
be very strong, and so the pairwise compatibil-
ity is low. However, after Pleasantville and North
Dakota are merged together, the cluster compat-
ibility function can compute the string similarity
of the concatenation of the city and state fields,
resulting in a higher compatibility. In this ex-
ample, the pairwise compatibility function under-
estimates the true compatibility.
These two examples show that the cluster com-
patibility score can have more representational
power than the average of pairwise compatibility
scores.
FirstName MiddleName
LastName NickName
Suffix Title
JobTitle CompanyName
Department AddressLine
City1 City2
State Country
PostalCode HomePhone
Fax CompanyPhone
DirectCompanyPhone Mobile
Pager VoiceMail
URL Email
InstantMessage
Table 1: The 25 fields annotated in the contact
record dataset.
4 Experiments
4.1 Data
We hand-labeled a subset of faculty and student
homepages from the WebKB dataset1. Each page
was labeled with the 25 fields listed in Table 1.
In addition, we labeled the records to which each
field belonged. For example, in Figure 2, we la-
beled the contact information for Professor Smith
into a separate record from that of her administra-
tive assistant. There are 252 labeled pages in total,
containing 8996 fields and 16679 word tokens. We
perform ten random samples of 70-30 splits of the
data for all experiments.
4.2 Systems
We evaluate five different record extraction sys-
tems. With the exception of Transitive Closure,
all methods employ the agglomerative clustering
1http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-
20/www/data/
607
Professor Jane Smith
Somesuch University
555-555-5555
Professor Smith is the Director of the Knowledge Lab ...
Mr. John Doe
Administrative Assistant
555-367-7777
Record 1
Record 2
Figure 2: A synthetic example representative of the labeled data. Note that Record 1 contains information
both from an address block and from free text, and that Record 2 must be separated from Record 1 even
though fields from each may be nearby in the text.
algorithm described previously. The difference is
in how the inter-cluster compatibility is calculated.
? Transitive Closure: The method described
in the beginning of Section 3.4, where hard
classification decisions are made, and transi-
tivity is enforced.
? Pairwise Compatibility: In this approach,
the compatibility function only estimates the
compatibility between pairs of fields, not sets
of fields. To compute inter-cluster compat-
ibility, the mean of the edges between the
clusters is calculated.
? McDonald: This method uses the pairwise
compatibility function, but instead of calcu-
lating the mean of inter-cluster edges, it cal-
culates the geometric mean of all pairs of
edges in the potential new cluster. That is,
to calculate the compatibility of records Ri
and Rj , we construct a new record Rij that
contains all fields of Ri and Rj , then calcu-
late the geometric mean of all pairs of fields
in Rij . This is analogous to the method used
in McDonald et al (2005) for relation extrac-
tion.
? Cluster Compatibility (uniform): Inter-
cluster compatibility is calculated directly by
the cluster compatibility function. This is the
method we advocate in Section 3. Training
examples are sampled uniformly as described
in Section 3.3.
? Cluster Compatibility (iterative): Same as
above, but training examples are sampled us-
ing the iterative method described in Section
3.3.
4.3 Features
For the pairwise compatibility classifier, we ex-
ploit various formatting as well as knowledge-
based features. Formatting features include the
number of hard returns between fields, whether
the fields occur on the same line, and whether the
fields occur consecutively. Knowledge-based fea-
tures include a mapping we compiled of cities and
states in the United States and Canada. Addition-
ally, we used compatibility features, such as which
fields are of the same type but have different val-
ues.
In building the cluster compatibility classifier,
we use many of the same features as in the bi-
nary classifier, but cast them as first-order existen-
tial features that are generated if the feature exists
between any pair of fields in the two clusters. Ad-
ditionally, we are able to exploit more powerful
compatibility and knowledge-base features. For
example, we examine if a title, a first name and a
last name occur consecutively (i.e., no other fields
occur in-between them). Also, we examine multi-
ple telephone numbers to ensure that they have the
same area codes. Additionally, we employ count
features that indicate if a certain field occurs more
than a given threshold.
4.4 Results
For these experiments, we compare performance
on the true record for each page. That is, we cal-
culate how often each system returns a complete
and accurate extraction of the contact record per-
taining to the owner of the webpage. We refer to
608
this record as the canonical record and measure
performance in terms of precision, recall and F1
for each field in the canonical record.
Table 2 compares precision, recall and F1 across
the various systems. The cluster compatibility
method with iterative sampling has the highest F1,
demonstrating a 14% error reduction over the next
best method and a 53% error reduction over the
transitive closure baseline.
Transitive closure has the highest recall, but it
comes at the expense of precision, and hence ob-
tains lower F1 scores than more conservative com-
patibility methods. The McDonald method also
has high recall, but drastically improves precision
over the transitivity method by taking into consid-
eration all edge weights.
The pairwise measure yields a slightly higher
F1 score than McDonald mostly due to precision
improvements. Because the McDonald method
calculates the mean of all edge weights rather
than just the inter-cluster edge weights, inter-
cluster weights are often outweighed by intra-
cluster weights. This can cause two densely-
connected clusters to be merged despite low inter-
cluster edge weights.
To further investigate performance differences,
we perform three additional experiments. The first
measures how sensitive the algorithms are to the
threshold value ? . Figure 3 plots the precision-
recall curve obtained by varying ? from 1.0 to 0.1.
As expected, high values of ? result in low recall
but high precision, since the algorithms halt with
a large number of small clusters. The highlighted
points correspond to ? = 0.5. These results indi-
cate that setting ? to 0.5 is near optimal, and that
the cluster compatibility method outperforms the
pairwise across a wide range of values for ? .
In the second experiment, we plot F1 versus
the size of the canonical record. Figure 4 indi-
cates that most of the performance gain occurs
in smaller canonical records (containing between
6 and 12 fields). Small canonical records are
most susceptible to precision errors simply be-
cause there are more extraneous fields that may
be incorrectly assigned to them. These precision
errors are often addressed by the cluster compati-
bility method, as shown in Table 2.
In the final experiment, we plot F1 versus the
total number of fields on the page. Figure 5 indi-
cates that the cluster compatibility method is best
at handling documents with large number of fields.
F1 Precision Recall
Cluster (I) 91.81 (.013) 92.87 (.005) 90.78 (.007)
Cluster (U) 90.02 (.012) 93.56 (.007) 86.74 (.011)
Pairwise 90.51 (.013) 91.07 (.004) 89.95 (.006)
McDonald 88.36 (.012) 83.55 (.004) 93.75 (.005)
Trans Clos 82.37 (.002) 70.75 (.009) 98.56 (.020)
Table 2: Precision, recall, and F1 performance for
the record extraction task. The standard error is
calculated over 10 cross-validation trials.
 
0.55 0.6
 
0.65 0.7
 
0.75 0.8
 
0.85 0.9
 
0.95 1  0
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5
 
0.6
 
0.7
 
0.8
 
0.9
 
1
precision
recall
cluste
r
pairwis
e
Figure 3: Precision-recall curve for cluster, pair-
wise, and mcdonald. The graph is obtained by
varying the stopping threshold ? from 1.0 to 0.1.
The highlighted points correspond to ? = 0.5.
When there are over 80 fields in the document, the
performance of the pairwise method drops dramat-
ically, while cluster compatibility only declines
slightly. We believe the improved precision of the
cluster compatibility method explains this trend as
well.
We also examine documents where cluster com-
patibility outperforms the pairwise methods. Typ-
ically, these documents contain interleaving con-
tact records. Often, it is the case that a single pair
of fields is sufficient to determine whether a clus-
ter should not be merged. For example, the cluster
classifier can directly model the fact that a con-
tact record should not have multiple first or last
names. It can also associate a weight with the fact
that several fields overlap (e.g., the chances that
a cluster has two first names, two last names and
two cities). In contrast, the binary classifier only
examines pairs of fields in isolation and averages
these probabilities with other edges. This averag-
ing can dilute the evidence from a single pair of
fields. Embarrassing errors may result, such as
a contact record with two first names or two last
609
0.740.760.78
0.80.820.84
0.860.880.9
0.920.940.96
6-9 9-12 12+number fields per record
F1
pairwisemcdonaldcluster
Figure 4: Field F1 as the size of the canonical
record increases. This figure suggests that clus-
ter compatibility is most helpful for small records.
0.80.82
0.840.86
0.880.9
0.920.94
0.96
0-20 20-40 40-60 60-80 80+number fields per document
F1
pairwisemcdonaldcluster
Figure 5: Field F1 as the number of fields in
the document increases. This figure suggests that
cluster compatibility is most helpful when the doc-
ument has more than 80 fields.
names. These errors are particularly prevalent in
interleaving contact records since adjacent fields
often belong to the same record.
5 Conclusions and Future Work
We have investigated graph partitioning methods
for discovering database records from fields anno-
tated in text. We have proposed a cluster compat-
ibility function that measures how likely it is that
two sets of fields belong to the same cluster. We
argue that this enhancement to existing techniques
provides more representational power.
We have evaluated these methods on a set of
hand-annotated data and concluded that (1) graph
partitioning techniques are more accurate than per-
forming transitive closure, and (2) cluster compat-
ibility methods can avoid common mistakes made
by pairwise compatibility methods.
As information extraction systems become
more reliable, it will become increasingly impor-
tant to develop accurate ways of associating dis-
parate fields into cohesive records. This will en-
able more complex reasoning over text.
One shortcoming of this approach is that fields
are not allowed to belong to multiple records,
because the partitioning algorithm returns non-
overlapping clusters. Exploring overlapping clus-
tering techniques is an area of future work.
Another avenue of future research is to consider
syntactic information in the compatibility func-
tion. While performance on contact record extrac-
tion is highly influenced by formatting features,
many fields occur within sentences, and syntactic
information (such as dependency trees or phrase-
structure trees) may improve performance.
Overall performance can also be improved by
increasing the sophistication of the partitioning
method. For example, we can examine ?block
moves? to swap multiple fields between clusters
in unison, possibly avoiding local minima of the
greedy method (Kanani et al, 2006). This can be
especially helpful because many mistakes may be
made at the start of clustering, before clusters are
large enough to reflect true records.
Additionally, many personal web pages con-
tain a time-line of information that describe a per-
son?s educational and professional history. Learn-
ing to associate time information with each con-
tact record enables career path modeling, which
presents interesting opportunities for knowledge
discovery techniques, a subject of ongoing work.
Acknowledgments
We thank the anonymous reviewers for helpful
suggestions. This work was supported in part by
the Center for Intelligent Information Retrieval, in
part by U.S. Government contract #NBCH040171
through a subcontract with BBNT Solutions LLC,
in part by The Central Intelligence Agency, the
National Security Agency and National Science
Foundation under NSF grant #IIS-0326249, and in
part by the Defense Advanced Research Projects
Agency (DARPA), through the Department of the
Interior, NBC, Acquisition Services Division, un-
der contract number NBCHD030010. Any opin-
610
ions, findings and conclusions or recommenda-
tions expressed in this material are the author(s)
and do not necessarily reflect those of the sponsor.
References
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In Proceedings of the Fifth ACM Interna-
tional Conference on Digital Libraries.
Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004.
Correlation clustering. Machine Learining, 56:89?
113.
Daniel M. Bikel, Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Machine Learning, 34:211?231.
Vinayak R. Borkar, Kaustubh Deshmukh, and Sunita
Sarawagi. 2001. Automatic segmentation of text
into structured records. In SIGMOD Conference.
Sergey Brin. 1998. Extracting patterns and rela-
tions from the world wide web. In WebDB Work-
shop at 6th International Conference on Extending
Database Technology.
Aron Culotta and Andrew McCallum. 2006. Practical
Markov logic containing first-order quantifiers with
application to identity uncertainty. In HLT Work-
shop on Computationally Hard Problems and Joint
Inference in Speech and Language Processing, June.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In ACL.
Aron Culotta, Ron Bekkerman, and Andrew McCal-
lum. 2004. Extracting social networks and contact
information from email and the web. In First Con-
ference on Email and Anti-Spam (CEAS), Mountain
View, CA.
David W. Embley and Lin Xu. 2000. Record location
and reconfiguration in unstructured multiple-record
web documents. In WebDB, pages 123?128.
David W. Embley, Xiaoyi Jiang, and Yiu-Kai Ng.
1999. Record-boundary discovery in web docu-
ments. In SIGMOD Conference, pages 467?478.
Ralph Grishman. 1997. Information extraction: Tech-
niques and challenges. In SCIE, pages 10?27.
Nanda Kambhatla. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for extracting relations. In ACL.
Pallika Kanani, Andrew McCallum, and Chris Pal.
2006. Improving author coreference by resource-
bounded information gathering from the web. Tech-
nical note.
Trausti Kristjannson, Aron Culotta, Paul Viola, and
Andrew McCallum. 2004. Interactive information
extraction with conditional random fields. Nine-
teenth National Conference on Artificial Intelligence
(AAAI 2004).
Andrew McCallum and Ben Wellner. 2005. Condi-
tional models of identity uncertainty with applica-
tion to noun coreference. In Lawrence K. Saul, Yair
Weiss, and Le?on Bottou, editors, Advances in Neu-
ral Information Processing Systems 17. MIT Press,
Cambridge, MA.
Ryan McDonald, Fernando Pereira, Seth Kulick, Scott
Winters, Yang Jin, and Pete White. 2005. Simple
algorithms for complex relation extraction with ap-
plications to biomedical ie. In 43rd Annual Meeting
of the Association for Computational Linguistics.
Brian Milch, Bhaskara Marthi, and Stuart Russell.
2005. BLOG: Probabilistic models with unknown
objects. In IJCAI.
Scott Miller, Heidi Fox, Lance A. Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing
to extract information from text. In ANLP.
Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press. To appear.
Paul Viola and Mukund Narasimhan. 2005. Learning
to extract information from semi-structured text us-
ing a discriminative context free grammar. In SIGIR
?05: Proceedings of the 28th annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 330?337, New
York, NY, USA. ACM Press.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. Journal of Machine Learning Research,
3:1083?1106.
611
Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 41?48,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
Practical Markov Logic Containing First-Order Quantifiers
with Application to Identity Uncertainty
Aron Culotta and Andrew McCallum
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
{culotta, mccallum}@cs.umass.edu
Abstract
Markov logic is a highly expressive language
recently introduced to specify the connec-
tivity of a Markov network using first-order
logic. While Markov logic is capable of
constructing arbitrary first-order formulae
over the data, the complexity of these for-
mulae is often limited in practice because
of the size and connectivity of the result-
ing network. In this paper, we present ap-
proximate inference and estimation meth-
ods that incrementally instantiate portions
of the network as needed to enable first-
order existential and universal quantifiers
in Markov logic networks. When applied
to the problem of identity uncertainty, this
approach results in a conditional probabilis-
tic model that can reason about objects,
combining the expressivity of recently in-
troduced BLOG models with the predic-
tive power of conditional training. We vali-
date our algorithms on the tasks of citation
matching and author disambiguation.
1 Introduction
Markov logic networks (MLNs) combine the proba-
bilistic semantics of graphical models with the ex-
pressivity of first-order logic to model relational de-
pendencies (Richardson and Domingos, 2004). They
provide a method to instantiate Markov networks
from a set of constants and first-order formulae.
While MLNs have the power to specify Markov
networks with complex, finely-tuned dependencies,
the difficulty of instantiating these networks grows
with the complexity of the formulae. In particular,
expressions with first-order quantifiers can lead to
networks that are large and densely connected, mak-
ing exact probabilistic inference intractable. Because
of this, existing applications of MLNs have not ex-
ploited the full richness of expressions available in
first-order logic.
For example, consider the database of researchers
described in Richardson and Domingos (2004),
where predicates include Professor(person),
Student(person), AdvisedBy(person, per-
son), and Published(author, paper). First-
order formulae include statements such as ?students
are not professors? and ?each student has at most
one advisor.? Consider instead statements such as
?all the students of an advisor publish papers with
similar words in the title? or ?this subset of stu-
dents belong to the same lab.? To instantiate an
MLN with such predicates requires existential and
universal quantifiers, resulting in either a densely
connected network, or a network with prohibitively
many nodes. (In the latter example, it may be nec-
essary to ground the predicate for each element of
the power set of students.)
However, as discussed in Section 2, there may
be cases where these aggregate predicates increase
predictive power. For example, in predicting
the value of HaveSameAdvisor(ai . . . ai+k),
it may be useful to know the values
of aggregate evidence predicates such as
CoauthoredAtLeastTwoPapers(ai . . . ai+k),
which indicates whether there are at least two papers
that some combination of authors from ai . . . ai+k
have co-authored. Additionally, we can construct
predicates such as NumberOfStudents(ai) to
model the number of students a researcher is likely
to advise simultaneously.
These aggregate predicates are examples of uni-
versal and existentially quantified predicates over ob-
served and unobserved values. To enable these sorts
41
of predicates while limiting the complexity of the
ground Markov network, we present an algorithm
that incrementally expands the set of aggregate pred-
icates during the inference procedure. In this paper,
we describe a general algorithm for incremental ex-
pansion of predicates in MLNs, then present an im-
plementation of the algorithm applied to the problem
of identity uncertainty.
2 Related Work
MLNs were designed to subsume various previously
proposed statistical relational models. Probabilistic
relational models (Friedman et al, 1999) combine
descriptive logic with directed graphical models, but
are restricted to acyclic graphs. Relational Markov
networks (Taskar et al, 2002) use SQL queries to
specify the structure of undirected graphical mod-
els. Since first-order logic subsumes SQL, MLNs
can be viewed as more expressive than relational
Markov networks, although existing applications of
MLNs have not fully utilized this increased expres-
sivity. Other approaches combining logic program-
ming and log-linear models include stochastic logic
programs (Cussens, 2003) and MACCENT(Dehaspe,
1997), although MLNs can be shown to represent
both of these.
Viewed as a method to avoid grounding an in-
tractable number of predicates, this paper has similar
motivations to recent work in lifted inference (Poole,
2003; de Salvo Braz et al, 2005), which performs
inference directly at the first-order level to avoid in-
stantiating all predicates. Although our model is not
an instance of lifted inference, it does attempt to re-
duce the number of predicates by instantiating them
incrementally.
Identity uncertainty (also known as record linkage,
deduplication, object identification, and co-reference
resolution) is the problem of determining whether a
set of constants (mentions) refer to the same object
(entity). Successful identity resolution enables vi-
sion systems to track objects, database systems to
deduplicate redundant records, and text processing
systems to resolve disparate mentions of people, or-
ganizations, and locations.
Many probabilistic models of object identification
have been proposed in the past 40 years in databases
(Fellegi and Sunter, 1969; Winkler, 1993) and nat-
ural language processing (McCarthy and Lehnert,
1995; Soon et al, 2001). With the introduction
of statistical relational learning, more sophisticated
models of identity uncertainty have been developed
that consider the dependencies between related con-
solidation decisions.
Most relevant to this work are the recent relational
models of identity uncertainty (Milch et al, 2005;
McCallum and Wellner, 2003; Parag and Domingos,
2004). McCallum and Wellner (2003) present exper-
iments using a conditional random field that factor-
izes into a product of pairwise decisions about men-
tion pairs (Model 3). These pairwise decisions are
made collectively using relational inference; however,
as pointed out in Milch et al (2004), there are short-
comings to this model that stem from the fact that it
does not capture features of objects, only of mention
pairs. For example, aggregate features such as ?a re-
searcher is unlikely to publish in more than 2 differ-
ent fields? or ?a person is unlikely to be referred to by
three different names? cannot be captured by solely
examining pairs of mentions. Additionally, decom-
posing an object into a set of mention pairs results
in ?double-counting? of attributes, which can skew
reasoning about a single object (Milch et al, 2004).
Similar problems apply to the model in Parag and
Domingos (2004).
Milch et al (2005) address these issues by con-
structing a generative probabilistic model over pos-
sible worlds called BLOG, where realizations of ob-
jects are typically sampled from a generative process.
While BLOG model provides attractive semantics for
reasoning about unknown objects, the transition to
generatively trained models sacrifices some of the at-
tractive properties of the discriminative model in Mc-
Callum and Wellner (2003) and Parag and Domin-
gos (2004), such as the ability to easily incorporate
many overlapping features of the observed mentions.
In contrast, generative models are constrained either
to assume the independence of these features or to
explicitly model their interactions.
Object identification can also be seen as an in-
stance of supervised clustering. Daume? III and
Marcu (2004) and Carbonetto et al (2005) present
similar Bayesian supervised clustering algorithms
that use a Dirichlet process to model the number
of clusters. As a generative model, it has similar ad-
vantages and disadvantages as Milch et al (2005),
with the added capability of integrating out the un-
certainty in the true number of objects.
In this paper, we present of identity uncertainty
that incorporates the attractive properties of Mc-
Callum and Wellner (2003) and Milch et al (2005),
resulting in a discriminative model to reason about
objects.
3 Markov logic networks
Let F = {Fi} be a set of first order formulae with
corresponding real-valued weights w = {wi}. Given
a set of constants C = {ci}, define ni(x) to be the
number of true groundings of Fi realized in a setting
42
of the world given by atomic formulae x. A Markov
logic network (MLN) (Richardson and Domingos,
2004) defines a joint probability distribution over
possible worlds x. In this paper, we will work with
discriminative MLNs (Singla and Domingos, 2005),
which define the conditional distribution over a set
of query atoms y given a set of evidence atoms x.
Using the normalizing constant Zx, the conditional
distribution is given by
P (Y = y|X = x) =
1
Zx
exp
?
?
|Fy|?
i=1
wini(x, y)
?
? (1)
where Fy ? F is the set of clauses for which at least
one grounding contains a query atom, and ni(x, y)
is the number of true groundings of the ith clause
containing evidence atom x and query atom y.
3.1 Inference Complexity in Ground
Markov Networks
The set of predicates and constants in Markov logic
define the structure of a Markov network, called a
ground Markov network. In discriminative Markov
logic networks, this resulting network is a conditional
Markov network (also known as a conditional ran-
dom field (Lafferty et al, 2001)).
From Equation 1, the formulae Fy specify the
structure of the corresponding Markov network as
follows: Each grounding of a predicate specified in
Fy has a corresponding node in the Markov network;
and an edge connects two nodes in the network if and
only if their corresponding predicates co-occur in a
grounding of a formula Fy. Thus, the complexity
of the formulae in Fy will determine the complexity
of the resulting Markov network, and therefore the
complexity of inference. When Fy contains complex
first-order quantifiers, the resulting Markov network
may contain a prohibitively large number of nodes.
For example, let the set of constants C be the set of
authors {ai}, papers {pi}, and conferences {ci} from
a research publication database. Predicates may in-
clude AuthorOf(ai, pj), AdvisorOf(ai, aj), and
ProgramCommittee(ai, cj). Each grounding of a
predicate corresponds to a random variable in the
corresponding Markov network.
It is important to notice how query predicates and
evidence predicates differ in their impact on inference
complexity. Grounded evidence predicates result in
observed random variables that can be highly con-
nected without resulting in an increase in inference
complexity. For example, consider the binary evi-
dence predicate HaveSameLastName(ai . . . ai+k).
This aggregate predicate reflects informa-
tion about a subset of (k ? i + 1) constants.
The value of this predicate is dependent on
the values of HaveSameLastName(ai, ai+1),
HaveSameLastName(ai, ai+2), etc. However,
since all of the corresponding variables are observed,
inference does not need to ensure their consistency
or model their interaction.
In contrast, complex query predicates can make
inference more difficult. Consider the query
predicate HaveSameAdvisor(ai . . . ai+k). Here,
the related predicatesHaveSameAdvisor(ai, ai+1),
HaveSameAdvisor(ai, ai+2), etc., all correspond
to unobserved binary random variables that the
model must predict. To ensure their consistency,
the resulting Markov network must contain depen-
dency edges between each of these variables, result-
ing in a densely connected network. Since inference
in general in Markov networks scales exponentially
with the size of the largest clique, inference in the
grounded network quickly becomes intractable.
One solution is to limit the expressivity of the
predicates. In the previous example, we can decom-
pose the predicate HaveSameAdvisor(ai . . . ai+k)
into its (k ? i + 1)2 corresponding pairwise pred-
icates, such as HaveSameAdvisor(ai, ai+1). An-
swering an aggregate query about the advisors of a
group of students can be handled by a conjunction
of these pairwise predicates.
However, as discussed in Sections 1 and 2, we
would like to reason about objects, not just pairs
of mentions, because this enables richer evidence
predicates. For example, the evidence predicates
AtLeastTwoCoauthoredPapers(ai . . . ai+k)
and NumberOfStudents(ai) can be
highly predictive of the query predicate
HaveSameAdvisor(ai . . . ai+k).
Below, we describe a discriminative MLN for iden-
tity uncertainty that is able to reason at the object
level.
3.2 Identity uncertainty
Typically, MLNs make a unique names assumption,
requiring that different constants refer to distinct ob-
jects. In the publications database example, each
author constant ai is a string representation of one
author mention found in the text of a citation. The
unique names assumption assumes that each ai refers
to a distinct author in the real-world. This simplifies
the network structure at the risk of weak or fallacious
predictions (e.g., AdvisorOf(ai, aj) is erroneous if
ai and aj actually refer to the same author). The
identity uncertainty problem is the task of removing
the unique names assumption by determining which
43
constants refer to the same real-world objects.
Richardson and Domingos (2004) address this con-
cern by creating the predicate Equals(ci, cj) be-
tween each pair of constants. While this retains the
coherence of the model, the restriction to pairwise
predicates can be a drawback if there exist informa-
tive features over sets of constants. In particular,
by only capturing features of pairs of constants, this
solution cannot model the compatibility of object at-
tributes, only of constant attributes (Section 2).
Instead, we desire a conditional model that allows
predicates to be defined over a set of constants.
One approach is to introduce constants that repre-
sent objects, and connect them to their mentions by
predicates such as IsMentionOf(ci, cj). In addition
to computational issues, this approach also some-
what problematically requires choosing the number
of objects. (See Richardson and Domingos (2004) for
a brief discussion.)
Instead, we propose instantiating aggregate pred-
icates over sets of constants, such that a setting of
these predicates implicitly determines the number of
objects. This approach allows us to model attributes
over entire objects, rather than only pairs of con-
stants. In the following sections, we describe aggre-
gate predicates in more detail, as well as the approx-
imations necessary to implement them efficiently.
3.3 Aggregate predicates
Aggregate predicates are predicates that take as ar-
guments an arbitrary number of constants. For ex-
ample, the HaveSameAdvisor(ai . . . ai+k) predi-
cate in the previous section is an example of an ag-
gregate predicate over k ? i + 1 constants.
Let IC = {1 . . . N} be the set of indices into the set
of constants C, with power set P(IC). For any subset
d ? P(IC), an aggregate predicate A(d) defines a
property over the subset of constants d.
Note that aggregate predicates can be trans-
lated into first-order formulae. For example,
HaveSameAdvisor(ai . . . ai+k) can be re-written
as ?(ax, ay) ? {ai . . . ai+k} SameAdvisor(ax, ay).
By using aggregate predicates we make explicit the
fact that we are modeling the attributes at the object
level.
We distinguish between aggregate query predi-
cates, which represent unobserved aggregate vari-
ables, and aggregate evidence predicates, which rep-
resent observed aggregate variables. Note that using
aggregate query predicates can complicate inference,
since they represent a collection of fully connected
hidden variables. The main point of this paper is
that although these aggregate query predicates are
specifiable in MLNs, they have not been utilized be-
cause of the resulting inference complexity. We show
that the gains made possible by these predicates of-
ten outweigh the approximations required for infer-
ence.
As discussed in Section 3.1, for each aggregate
query predicates A(d), it is critical that the model
predict consistent values for every related subset of d.
Enforcing this consistency requires introducing de-
pendency edges between aggregate query predicates
that share arguments. In general, this can be a diffi-
cult problem. Here, we focus on the special case for
identity uncertainty where the main query predicate
under consideration is AreEqual(d).
The aggregate query predicate AreEqual(d) is
true if and only if all constants di ? d refer to the
same object. Since each subset of constants corre-
sponds to a candidate object, a (consistent) setting
of all the AreEqual predicates results in a solution
to the object identification problem. The number
of objects is chosen based on the optimal grounding
of each of these aggregate predicates, and therefore
does not require a prior over the number of objects.
That is, once all the AreEqual predicates are set,
they determine a clustering with a fixed number of
objects. The number of objects is not modeled or set
directly, but is implied by the result of MAP infer-
ence. (However, a posterior over the number of ob-
jects could be modeled discriminatively in an MLN
(Richardson and Domingos, 2004).)
This formulation also allows us to compute aggre-
gate evidence predicates over objects to help predict
the values of each AreEqual predicate. For exam-
ple, NumberFirstNames(d) returns the number of
different first names used to refer to the object ref-
erenced by constants d. In this way, we can model
aggregate features of an object, capturing the com-
patibility among its attributes.
For a given C, there are |P(IC)| possible ground-
ings of the AreEqual query predicates. Naively im-
plemented, such an approach would require enumer-
ating all subsets of constants, ultimately resulting in
an unwieldy network.
An equivalent way to state the problem is that
using N -ary predicates results in a Markov network
with one node for each grounding of the predicate.
Since in the general case there is one grounding
for each subset of C, the size of the corresponding
Markov network will be exponential in |C|. See Fig-
ure 1 for an example instantiation of an MLN with
three constants (a, b, c) and one AreEqual predi-
cate.
In this paper, we provide algorithms to per-
form approximate inference and parameter estima-
tion by incrementally instantiating these predicates
44
AreEqual(a,b) AreEqual(a,c) AreEqual(b,c)
AreEqual(a,b,c)
Figure 1: An example of the network instantiated
by an MLN with three constants and the aggregate
predicate AreEqual, instantiated for all possible
subsets with size ? 2.
as needed.
3.4 MAP Inference
Maximum a posteriori (MAP) inference seeks the so-
lution to
y? = argmax
y
P (Y = y|X = x)
where y? is the setting of all the query predicates
Fy (e.g. AreEqual) with the maximal conditional
density.
In large, densely connected Markov networks, a
common approximate inference technique is loopy
belief propagation (i.e. the max-product algorithm
applied to a cyclic graph). However, the use of ag-
gregate predicates makes it intractable even to in-
stantiate the entire network, making max-product
an inappropriate solution.
Instead, we employ an incremental inference tech-
nique that grounds aggregate query predicates in
an agglomerative fashion based on the model?s cur-
rent MAP estimates. This algorithm can be viewed
as a greedy agglomerative search for a local opti-
mum of P (Y |X), and has connections to recent work
on correlational clustering (Bansal et al, 2004) and
graph partitioning for MAP estimation (Boykov et
al., 2001).
First, note that finding the MAP estimate does not
require computing Zx, since we are only interested in
the relative values of each configuration, and Zx is
fixed for a given x. Thus, at iteration t, we compute
an unnormalized score for yt (the current setting of
the query predicates) given the evidence predicates
x as follows:
S(yt, x) = exp
?
?
|F t|?
i=0
wini(x, y
t)
?
?
where F t ? Fy is the set of aggregate predicates
representing a partial solution to the object identifi-
cation task for constants C, specified by yt.
Algorithm 1 Approximate MAP Inference Algo-
rithm
1: Given initial predicates F 0
2: while ScoreIsIncreased do
3: F ?i ? FindMostLikelyPredicate(F
t)
4: F ?i ? true
5: F t ? ExpandPredicates(F ?i , F
t)
6: end while
Algorithm 1 outlines a high-level description of the
approximate MAP inference algorithm. The algo-
rithm first initializes the set of query predicated F 0
such that all AreEqual predicates are restricted
to pairs of constants, i.e. AreEqual(ci, cj) ?(i, j).
This is equivalent to a Markov network containing
one unobserved random variable for each pair of con-
stants, where each variable indicates whether a pair
of constants refer to the same object.
Initially, each AreEqual predicate is assumed
false. In line 3, the procedure FindMostLike-
lyPredicate iterates through each query predicate
in F t, setting each to true in turn and calculating its
impact on the scoring function. The procedure re-
turns the predicate F ?i such that setting F
?
i to True
results in the greatest increase in the scoring function
S(yt, x).
Let (c?i . . . c
?
j ) be the set of constants ?merged?
by setting their AreEqual predicate to true. The
ExpandPredicates procedure creates new predi-
cates AreEqual(c?i . . . c
?
j , ck . . . cl) corresponding to
all the potential predicates created by merging the
constants c?i . . . c
?
j with any of the other previously
merged constants. For example, after the first it-
eration, a pair of constants (c?i , c
?
j ) are merged.
The set of predicates are expanded to include
AreEqual(c?i , c
?
j , ck) ?ck, reflecting all possible ad-
ditional references to the proposed object referenced
by c?i , c
?
j .
This algorithm continues until there is no predi-
cate that can be set to true that increases the score
function.
In this way, the final setting of Fy is a local max-
imum of the score function. As in other search
algorithms, we can employ look-ahead to reduce
the greediness of the search (i.e., consider multiple
merges simultaneously), although we do not include
look-ahead in experiments reported here.
It is important to note that each expansion of the
aggregate query predicates Fy has a corresponding
set of aggregate evidence predicates. These evidence
predicates characterize the compatibility of the at-
tributes of each hypothesized object.
45
3.4.1 Pruning
The space required for the above algorithm scales
?(|C|2), since in the initialization step we must
ground a predicate for each pair of constants. We use
the canopy method of McCallum et al (2000), which
thresholds a ?cheap? similarity metric to prune un-
necessary comparisons. This pruning can be done
at subsequent stages of inference to restrict which
predicates variables will be introduced.
Additionally, we must ensure that predicate set-
tings at time t do not contradict settings at t ? 1
(e.g. if F t(a, b, c) = 1, then F t+1(a, b) = 1). By
greedily setting unobserved nodes to their MAP es-
timates, the inference algorithm ignores inconsistent
settings and removes them from the search space.
3.5 Parameter estimation
Given a fully labeled training set D of constants an-
notated with their referent objects, we would like to
estimate the value of w that maximizes the likelihood
of D. That is, w? = argmaxw Pw(y|x).
When the data are few, we can explicitly instan-
tiate all AreEqual(d) predicates, setting their cor-
responding nodes to the values implied by D. The
likelihood is given by Equation 1, where the normal-
izer is Zx =
?
y? exp
(?|F ?y|
i=1 wini(x, y
?)
)
.
Although this sum over y? to calculate Zx is ex-
ponential in |y|, many inconsistent settings can be
pruned as discussed in Section 3.4.1.
In general, however, instantiating the entire set
of predicates denoted by y and calculating Zx is
intractable. Existing methods for MLN parame-
ter estimation include pseudo-likelihood and voted
perceptron (Richardson and Domingos, 2004; Singla
and Domingos, 2005). We instead follow the recent
success in piecewise training for complex undirected
graphical models (Sutton and McCallum, 2005) by
making the following two approximations. First, we
avoid calculating the global normalizer Zx by calcu-
lating local normalizers, which sum only over the two
values for each aggregate query predicate grounded
in the training data. We therefore maximize the sum
of local probabilities for each query predicate given
the evidence predicates.
This approximation can be viewed as constructing
a log-linear binary classifier to predict whether an
isolated set of constants refer to the same object.
Input features include arbitrary first-order features
over the input constants, and the output is a binary
variable. The parameters of this classifier correspond
to the w weights in the MLN. This simplification
results in a convex optimization problem, which we
solve using gradient descent with L-BFGS, a second-
order optimization method (Liu and Nocedal, 1989).
The second approximation addresses the fact that
all query predicates from the training set cannot be
instantiated. We instead sample a subset FS ? Fy
and maximize the likelihood of this subset. The sam-
pling is not strictly uniform, but is instead obtained
by collecting the predicates created while perform-
ing object identification using a weak method (e.g.
string comparisons). More explicitly, predicates are
sampled from the training data by performing greedy
agglomerative clustering on the training mentions,
using a scoring function that computes the similar-
ity between two nodes by string edit distance. The
goal of this clustering is not to exactly reproduce the
training clusters, but to generate correct and incor-
rect clusters that have similar characteristics (size,
homogeneity) to what will be present in the testing
data.
4 Experiments
We perform experiments on two object identification
tasks: citation matching and author disambiguation.
Citation matching is the task of determining whether
two research paper citation strings refer to the same
paper. We use the Citeseer corpus (Lawrence et al,
1999), containing approximately 1500 citations, 900
of which are unique. The citations are manually la-
beled with cluster identifiers, and the strings are seg-
mented into fields such as author, title, etc. The cita-
tion data is split into four disjoint categories by topic,
and the results presented are obtained by training on
three categories and testing on the fourth.
Using first-order logic, we create a number of ag-
gregate predicates such as AllTitlesMatch, Al-
lAuthorsMatch, AllJournalsMatch, etc., as
well as their existential counterparts, ThereExist-
sTitleMatch, etc. We also include count predi-
cates, which indicate the number of these matches in
a set of constants.
Additionally, we add edit distance predicates,
which calculate approximate matches1 between title
fields, etc., for each pair of citations in a set of cita-
tions. Aggregate features are used for these, such as
?there exists a pair of citations in this cluster which
have titles that are less than 30% similar? and ?the
minimum edit distance between titles in a cluster is
greater than 50%.?
We evaluate using pairwise precision, recall, and
F1, which measure the system?s ability to predict
whether each pair of constants refer to the same ob-
ject or not. Table 1 shows the advantage of our
1We use the Secondstring package, found at
http://secondstring.sourceforge.net
46
Table 1: Precision, recall, and F1 performance for
citation matching task, where Objects is an MLN
using aggregate predicates, and Pairs is an MLN us-
ing only pairwise predicates. Objects outperforms
Pairs on three of the four testing sets.
Objects Pairs
pr re f1 pr re f1
constraint 85.8 79.1 82.3 63.0 98.0 76.7
reinforce 97.0 90.0 93.4 65.6 98.2 78.7
face 93.4 84.8 88.9 74.2 94.7 83.2
reason 97.4 69.3 81.0 76.4 95.5 84.9
Table 2: Performance on the author disambiguation
task. Objects outperforms Pairs on two of the
three testing sets.
Objects Pairs
pr re f1 pr re f1
miller d 73.9 29.3 41.9 44.6 1.0 61.7
li w 39.4 47.9 43.2 22.1 1.0 36.2
smith b 61.2 70.1 65.4 14.5 1.0 25.4
proposed model (Objects) over a model that only
considers pairwise predicates of the same features
(Pairs). Note that Pairs is a strong baseline that
performs collective inference of citation matching de-
cisions, but is restricted to use only IsEqual(ci, cj)
predicates over pairs of citations. Thus, the perfor-
mance difference is due to the ability to model first-
order features of the data.
Author disambiguation is the task of deciding
whether two strings refer to the same author. To in-
crease the task complexity, we collect citations from
the Web containing different authors with matching
last names and first initials. Thus, simply performing
a string match on the author?s name would be insuffi-
cient in many cases. We searched for three common
last name / first initial combinations (Miller, D;
Li, W; Smith, B). From this set, we collected 400
citations referring to 56 unique authors. For these
experiments, we train on two subsets and test on the
third.
We generate aggregate predicates similar to those
used for citation matching. Additionally, we in-
clude features indicating the overlap of tokens from
the titles and indicating whether there exists a pair
of authors in this cluster that have different mid-
dle names. This last feature exemplifies the sort of
reasoning enabled by aggregate predicates: For ex-
ample, consider a pairwise predicate that indicates
whether two authors have the same middle name.
Very often, middle name information is unavailable,
so the name ?Miller, A.? may have high similarity to
both ?Miller, A. B.? and ?Miller, A. C.?. However,
it is unlikely that the same person has two different
middle names, and our model learns a weight for this
feature. Table 2 demonstrates the advantage of this
method.
Overall, Objects achieves F1 scores superior to
Pairs on 5 of the 7 datasets. These results indicate
the potential advantages of using complex first-order
quantifiers in MLNs. The cases in which Pairs out-
performs Objects are likely due to the fact that the
approximate inference used in Objects is greedy.
Increasing the robustness of inference is a topic of
future research.
5 Conclusions and Future Work
We have presented an algorithm that enables practi-
cal inference in MLNs containing first-order existen-
tial and universal quantifiers, and have demonstrated
the advantages of this approach on two real-world
datasets. Future work will investigate efficient ways
to improve the approximations made during infer-
ence, for example by reducing its greediness by revis-
ing the MAP estimates made at previous iterations.
Although the optimal number of objects is cho-
sen implicitly by the inference algorithm, there may
be reasons to explicitly model this number. For ex-
ample, if there exist global features of the data that
suggest there are many objects, then the inference al-
gorithm should be less inclined to merge constants.
Additionally, the data may exhibit ?preferential at-
tachment? such that the probability of a constant
being added to an existing object is proportional to
the number of constants that refer to that object.
Future work will examine the feasibility of adding
aggregate query predicates to represent these values.
More subtly, one may also want to directly model
the size of the object population. For example, given
a database of authors, we may want to estimate not
only how many distinct authors exist in the database,
but also how many distinct authors exist outside of
the database, as discussed in Milch et al (2005).
Discriminatively-trained models cannot easily reason
about objects for which they have no observations;
so a generative/discriminative hybrid model may be
required to properly estimate this value.
Finally, while the inference algorithm we describe
is evaluated only on the object uncertainty task, we
would like to extend it to perform inference over ar-
bitrary query predicates.
47
6 Acknowledgments
We would like to thank the reviewers, and Pallika Kanani
for helpful discussions. This work was supported in
part by the Center for Intelligent Information Retrieval,
in part by U.S. Government contract #NBCH040171
through a subcontract with BBNT Solutions LLC, in
part by The Central Intelligence Agency, the National
Security Agency and National Science Foundation un-
der NSF grant #IIS-0326249, and in part by the Defense
Advanced Research Projects Agency (DARPA), through
the Department of the Interior, NBC, Acquisition Ser-
vices Division, under contract number NBCHD030010.
Any opinions, findings and conclusions or recommenda-
tions expressed in this material are the author(s)? and do
not necessarily reflect those of the sponsor.
References
Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004.
Correlation clustering. Machine Learining, 56:89?113.
Yuri Boykov, Olga Veksler, and Ramin Zabih. 2001. Fast
approximate energy minimization via graph cuts. In
IEEE transactions on Pattern Analysis and Machine
Intelligence (PAMI), 23(11):1222?1239.
Peter Carbonetto, Jacek Kisynski, Nando de Freitas, and
David Poole. 2005. Nonparametric bayesian logic. In
UAI.
J. Cussens. 2003. Individuals, relations and structures
in probabilistic models. In Proceedings of the Fifteenth
Conference on Uncertainty in Artificial Intelligence,
pages 126?133, Acapulco, Mexico.
Hal Daume? III and Daniel Marcu. 2004. Supervised clus-
tering with the dirichlet process. In NIPS?04 Learn-
ing With Structured Outputs Workshop, Whistler,
Canada.
Rodrigo de Salvo Braz, Eyal Amir, and Dan Roth. 2005.
Lifted first-order probabilistic inference. In IJCAI,
pages 1319?1325.
L. Dehaspe. 1997. Maximum entropy modeling with
clausal constraints. In Proceedings of the Seventh
International Workshop on Inductive Logic Program-
ming, pages 109?125, Prague, Czech Republic.
I. P. Fellegi and A. B. Sunter. 1969. A theory for record
linkage. Journal of the American Statistical Associa-
tion, 64:1183?1210.
Nir Friedman, Lise Getoor, Daphne Koller, and Avi Pf-
effer. 1999. Learning probabilistic relational models.
In IJCAI, pages 1300?1309.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289. Morgan Kaufmann, San Francisco, CA.
S. Lawrence, C. L. Giles, and K. Bollaker. 1999. Digi-
tal libraries and autonomous citation indexing. IEEE
Computer, 32:67?71.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory BFGS method for large scale optimization. Math.
Programming, 45(3, (Ser. B)):503?528.
A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In IJCAI Workshop on
Information Integration on the Web.
Andrew K. McCallum, Kamal Nigam, and Lyle Ungar.
2000. Efficient clustering of high-dimensional data sets
with application to reference matching. In Proceed-
ings of the Sixth International Conference On Knowl-
edge Discovery and Data Mining (KDD-2000), Boston,
MA.
Joseph F. McCarthy and Wendy G. Lehnert. 1995. Us-
ing decision trees for coreference resolution. In IJCAI,
pages 1050?1055.
Brian Milch, Bhaskara Marthi, and Stuart Russell. 2004.
Blog: Relational modeling with unknown objects. In
ICML 2004 Workshop on Statistical Relational Learn-
ing and Its Connections to Other Fields.
Brian Milch, Bhaskara Marthi, and Stuart Russell. 2005.
BLOG: Probabilistic models with unknown objects. In
IJCAI.
Parag and Pedro Domingos. 2004. Multi-relational
record linkage. In Proceedings of the KDD-2004 Work-
shop on Multi-Relational Data Mining, pages 31?48,
August.
D. Poole. 2003. First-order probabilistic inference. In
Proceedings of the Eighteenth International Joint Con-
ference on Artificial Intelligence, pages 985?991, Aca-
pulco, Mexico. Morgan Kaufman.
M. Richardson and P. Domingos. 2004. Markov logic
networks. Technical report, University of Washington,
Seattle, WA.
Parag Singla and Pedro Domingos. 2005. Discriminative
training of markov logic networks. In Proceedings of
the Twentieth National Conference of Artificial Intel-
ligence, Pittsburgh, PA.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Comput. Linguist.,
27(4):521?544.
Charles Sutton and Andrew McCallum. 2005. Piecewise
training of undirected models. In Submitted to 21st
Conference on Uncertainty in Artificial Intelligence.
Ben Taskar, Abbeel Pieter, and Daphne Koller. 2002.
Discriminative probabilistic models for relational data.
In Uncertainty in Artificial Intelligence: Proceedings of
the Eighteenth Conference (UAI-2002), pages 485?492,
San Francisco, CA. Morgan Kaufmann Publishers.
William E. Winkler. 1993. Improved decision rules in
the fellegi-sunter model of record linkage. Technical
report, Statistical Research Division, U.S. Census Bu-
reau, Washington, DC.
48
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), page 1,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Joint Inference for Natural Language Processing
Andrew McCallum
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA 01002
mccallum@cs.umass.edu
Abstract of the Invited Talk
In recent decades, researchers in natural language
processing have made great progress on well-
defined subproblems such as part-of-speech tagging,
phrase chunking, syntactic parsing, named-entity
recognition, coreference and semantic-role label-
ing. Better models, features, and learning algorithms
have allowed systems to perform many of these tasks
with 90% accuracy or better. However, success in in-
tegrated, end-to-end natural language understanding
remains elusive.
I contend that the chief reason for this failure
is that errors cascade and accumulate through a
pipeline of naively chained components. For exam-
ple, if we naively use the single most likely output
of a part-of-speech tagger as the input to a syntactic
parser, and those parse trees as the input to a coref-
erence system, and so on, errors in each step will
propagate to later ones: each components 90% ac-
curacy multiplied through six components becomes
only 53%.
Consider, for instance, the sentence ?I know you
like your mother.? If a part-of-speech tagger de-
terministically labels ?like? as a verb, then certain
later syntactic and semantic analysis will be blocked
from alternative interpretations, such as ?I know you
like your mother (does).? The part-of-speech tag-
ger needs more syntactic and semantic information
to make this choice. Consider also the classic exam-
ple ?The boy saw the man with the telescope.? No
single correct syntactic parse of this sentence is pos-
sible in isolation. Correct interpretation requires the
integration of these syntactic decisions with seman-
tics and context.
Humans manage and resolve ambiguity by uni-
fied, simultaneous consideration of morphology,
syntax, semantics, pragmatics and other contextual
information. In statistical modeling such unified
consideration is known as joint inference. The need
for joint inference appears not only in natural lan-
guage processing, but also in information integra-
tion, computer vision, robotics and elsewhere. All of
these applications require integrating evidence from
multiple sources, at multiple levels of abstraction. I
believe that joint inference is one of the most fun-
damentally central issues in all of artificial intelli-
gence.
In this talk I will describe work in probabilistic
models that perform joint inference across multiple
components of an information processing pipeline
in order to avoid the brittle accumulation of errors.
I will survey work in exact inference, variational
inference and Markov-chain Monte Carlo methods.
We will discuss various approaches that have been
applied to natural language processing, and hypoth-
esize about why joint inference has helped in some
cases, and not in others.
I will then focus on our recent work at Univer-
sity of Massachusetts in large-scale conditional ran-
dom fields with complex relational structure. In a
single factor graph we seamlessly integrate multiple
subproblems, using our new probabilistic program-
ming language to compactly express complex, muta-
ble variable-factor structure both in first-order logic
as well as in more expressive Turing-complete im-
perative procedures. We avoid unrolling this graph-
ical model by using Markov-chain Monte Carlo for
inference, and make inference more efficient with
learned proposal distributions. Parameter estimation
is performed by SampleRank, which avoids com-
plete inference as a subroutine by learning simply
to correctly rank successive states of the Markov-
chain.
Joint work with Aron Culotta, Michael Wick,
Rob Hall, Khashayar Rohanimanesh, Karl Schultz,
Sameer Singh, Charles Sutton and David Smith.
1
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1013?1023,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Collective Cross-Document Relation Extraction Without Labelled Data
Limin Yao Sebastian Riedel Andrew McCallum
University of Massachusetts, Amherst
{lmyao,riedel,mccallum}@cs.umass.edu
Abstract
We present a novel approach to relation ex-
traction that integrates information across doc-
uments, performs global inference and re-
quires no labelled text. In particular, we
tackle relation extraction and entity identifi-
cation jointly. We use distant supervision to
train a factor graph model for relation ex-
traction based on an existing knowledge base
(Freebase, derived in parts from Wikipedia).
For inference we run an efficient Gibbs sam-
pler that leads to linear time joint inference.
We evaluate our approach both for an in-
domain (Wikipedia) and a more realistic out-
of-domain (New York Times Corpus) setting.
For the in-domain setting, our joint model
leads to 4% higher precision than an isolated
local approach, but has no advantage over a
pipeline. For the out-of-domain data, we ben-
efit strongly from joint modelling, and observe
improvements in precision of 13% over the
pipeline, and 15% over the isolated baseline.
1 Introduction
Relation Extraction is the task of predicting seman-
tic relations over entities expressed in structured or
semi-structured text. This includes, for example,
the extraction of employer-employee relations men-
tioned in newswire, or protein-protein interactions
expressed in biomedical papers. It also includes the
prediction of entity types such as country, citytown
or person, if we consider entity types as unary rela-
tions.
A particularly attractive approach to relation ex-
traction is based on distant supervision.1 Here in
1Also called self training, or weak supervision.
place of annotated text, only an existing knowl-
edge base (KB) is needed to train a relation extrac-
tor (Mintz et al, 2009; Bunescu and Mooney, 2007;
Riedel et al, 2010). The facts in the KB are heuris-
tically aligned to an unlabelled training corpus, and
the resulting alignment is the basis for learning the
extractor.
Naturally, the predictions of a distantly supervised
relation extractor will be less accurate than those of
a supervised one. While facts of existing knowledge
bases are inexpensive to come by, the heuristic align-
ment to text will often lead to noisy patterns in learn-
ing. When applied to unseen text, these patterns will
produce noisy facts. Indeed, we find that extraction
precision still leaves much room for improvement.
This room is not as large as in previous work (Mintz
et al, 2009) where target text and training KB are
closely related. However, when we use the knowl-
edge base Freebase (Bollacker et al, 2008) and the
New York Times corpus (Sandhaus, 2008), we ob-
serve very low precision. For example, the preci-
sion of the top-ranked 50 nationality relation
instances is only 28%.
On inspection, it turns out that many of the errors
can be easily identified: they amount to violations
of basic compatibility constraints between facts. In
particular, we observe unsatisfied selectional pref-
erences of relations towards particular entity types
as types of their arguments. An example is the fact
that the first argument of nationality is always
a person while the second is a country. A sim-
ple way to address this is a pipeline: first predict
entity types, and then condition on these when pre-
dicting relations. However, this neglects the fact that
relations could as well be used to help entity type
prediction.
1013
While there is some existing work on enforcing
such constraints in a joint fashion (Roth and Yih,
2007; Kate and Mooney, 2010; Riedel et al, 2009),
they are not directly applicable here. The difference
is the amount of facts they take into account at the
same time. They focus on single sentence extrac-
tions, and only consider very few interacting facts.
This allows them to work with exact optimization
techniques such as (Integer) Linear Programs and
still remain efficient.2 However, when working on
a sentence level they fail to exploit the redundancy
present in a corpus. Moreover, the fewer facts they
consider at the same time, the lower the chance that
some of these will be incompatible, and that mod-
elling compatibility will make a difference.
In this work we present a novel approach that
performs relation extraction across documents, en-
forces selectional preferences, and needs no labelled
data. It is based on an undirected graphical model
in which variables correspond to facts, and factors
between them measure compatibility. In order to
scale up, we run an efficient Gibbs-Sampler at in-
ference time, and train our model using SampleR-
ank (Wick et al, 2009). In practice this leads to a
runtime behaviour that is linear in the size of the cor-
pus. For example, 200,000 documents take less than
three hours for training and testing.
For evaluation we consider two scenarios. First
we follow Mintz et al (2009), use Freebase as
source of distant supervision, and employ Wikipedia
as source of unlabelled text?we will call this an
in-domain setting. This scenario is somewhat arti-
ficial in that Freebase itself is partially derived from
Wikipedia, and in practice we cannot expect text and
training knowledge base to be so close. Hence we
also evaluate our approach on the New York Times
corpus (out-of-domain setting).
For in-domain data we make the following find-
ing. When we compare to an isolated baseline that
makes no use of entity types, our joint model im-
proves average precision by 4%. However, it does
not outperform a pipelined system. In the out-of-
domain setting, our collective model substantially
outperforms both other approaches. Compared to
the isolated baseline, we achieve a 15% increase in
2The pyramid algorithm of Kate and Mooney (2010) may
scale well, but it is not clear how to apply their scheme to cross-
document extraction.
precision. With respect to the pipeline approach, the
increase is 13%.
In the following we will first give some back-
ground information on relation extraction with dis-
tant supervision. Then we will present our graphi-
cal model as well as the inference and learning tech-
niques we apply. After discussing related work, we
present our empirical results and conclude.
2 Background
In this section we will introduce the terminology and
concepts we use throughout the paper. We will also
give a brief introduction to relation extraction, in
particular in the context of distant supervision.
2.1 Relations
We seek to extract facts about entities. Example en-
tities would be the company founder BILL GATES,
the company MICROSOFT, and the country USA.
A relation R is a set of tuples c over entities. We
will follow (Mintz et al, 2009) and call the term
R (c1, . . . cn) with c ? R a relation instance.3 It
denotes the membership of the tuple c in the re-
lation R. For example, founded (BILL GATES,
MICROSOFT) is a relation instance denoting that
BILL GATES and MICROSOFT are related in the
founded relation.
In the following we will always consider some set
of candidate tuples C that may or may not be re-
lated. We define Cn ? C to be set of all n-ary tu-
ples in C. Note that while our definition considers
general n-nary relations, in practice we will restrict
us to unary and binary relations C1 and C2.
Following previous work (Mintz et al, 2009; Ze-
lenko et al, 2003; Culotta and Sorensen, 2004) we
make one more simplifying assumption: every can-
didate tuple can be member of at most one relation.
2.2 Entity Types
An entity can be of one or several entity types. For
example, BILL GATES is a person, and a company
founder. Entity types correspond to the special
case of relations with arity one, and will be treated
as such in the following.
3Other commonly used terms are relational facts, ground
facts, ground atoms, and assertions.
1014
We care about entity types for two reasons. First,
they can be important for downstream applications:
if consumers of our extracted facts know the type
of entities, they can find them more easily, visu-
alize them more adequately, and perform opera-
tions specific to these types (write emails to persons,
book a hotel in a city, etc.). Second, they are use-
ful for extracting binary relations due to selectional
preferences?see section 2.6.
2.3 Mentions
In natural language text spans of tokens are used to
refer to entities. We call such spans entity mentions.
Consider, for example, the following sentence snip-
pet:
(1) Political opponents of President Evo Morales
of Bolivia have in recent days stepped up...
Here ?Evo Morales? is an entity mention of pres-
ident EVO MORALES, and ?Bolivia? a mention of
the country BOLIVIA he is the president of.
People often express relations between entities in
natural language texts by mentioning the participat-
ing entities in specific syntactic and lexical patterns.
We will refer to any tuple of mentions of entities
(e1, . . . en) in a sentence as candidate mention tu-
ple. If such a candidate expresses the relation R,
then it is a relation mention of the relation instance
R (e1, . . . , en).
Consider again example 1. Here the pair of en-
tity mentions (?Evo Morales?, ?Bolivia?) is a candi-
date mention tuple. In fact, in this case the candidate
is indeed a relation mention of the relation instance
nationality (EVO MORALES, BOLIVIA).
2.4 Relation Extraction
We define the task of relation extraction as follows.
We are given a corpus of documents and a set of
target relations. Then we are asked to predict all re-
lation instances I so that for each R (c) ? I there
exists at least one relation mention in the given cor-
pus.
The above definition covers a range of existing
approaches by varying over what we define as tar-
get corpus. On one end, we have extractors that
process text on a per sentence basis (Zelenko et al,
2003; Culotta and Sorensen, 2004). On the other
end, we have methods that take relation mentions
from several documents and use these as input fea-
tures (Mintz et al, 2009; Bunescu and Mooney,
2007).
There is a compelling reason for performing re-
lation extraction within a larger scope that consid-
ers mentions across documents: redundancy. Often
facts are mentioned in several sentences and doc-
uments. Some of these mentions may be difficult
to parse, or they use unseen patterns. But the more
mentions we consider, the higher the probability that
one does parse, and fits a pattern we have seen in the
training data.
Note that for relation extraction that considers
more than a single mention we have to solve the
coreference problem in order to determine which
mentions refer to the same entity. In the follow-
ing we will assume that coreference clusters are pro-
vided by a preprocessing step.
2.5 Distant Supervision
In relation extraction we often encounter a lack of
explicitly annotated text, but an abundance of struc-
tured data sources such as company databases or col-
laborative knowledge bases like Freebase. In order
to exploit this, many approaches use simple but ef-
fective heuristics to align existing facts with unla-
belled text. This labelled text can then be used as
training material of a supervised learner.
One heuristic is to assume that each candidate
mention tuple of a training fact is indeed expressing
the corresponding relation (Bunescu and Mooney,
2007). Mintz et al (2009) refer to this as the dis-
tant supervision assumption.
Clearly, this heuristic can fail. Let us again
consider the nationality relation between EVO
MORALES and BOLIVIA. In an 2007 article of the
New York Times we find this relation mention can-
didate:
(2) ...the troubles faced by Evo Morales in
Bolivia...
This sentence does not directly express that EVO
MORALES is a citizen of BOLIVIA, and hence vi-
olates the distant supervision assumption. The prob-
lem with this observation is that at training time
we may learn a relatively large weight for the
feature ?<Entity1> in <Entity2>? associated with
1015
nationality. When testing our model we then
encounter a sentence such as
(3) Arrest Warrant Issued for Richard Gere in
India.
that leads us to extract that RICHARD GERE is a cit-
izen of INDIA.
2.6 Global Consistency of Facts
As discussed above, distant supervision can lead to
noisy extractions. However, such noise can often be
easily identified by testing how compatible the ex-
tracted facts are to each other. In this work we are
concerned with a particular type of compatibility:
selectional preferences.
Relations require, or prefer, their arguments to be
of certain types. For example, the nationality
relation requires the first argument to be a person,
and the second to be a country. On inspection,
we find that these preferences are often not satis-
fied in a baseline distant supervision system akin to
Mintz et al (2009). This often results from patterns
such as ?<Entity1> in <Entity2>? that fire in many
cases where <Entity2> is a location, but not a
country.
3 Model
Our observations in the previous section suggest
that we should (a) explicitly model compatibil-
ity between extracted facts, and (b) integrate ev-
idence from several documents to exploit redun-
dancy. In this work we choose a Conditional Ran-
dom Field (CRF) to achieve this. CRFs are a natural
fit for this task: They allow us to capture correlations
in an explicit fashion, and to incorporate overlapping
input features from multiple documents.
The hidden output variables of our model areY =
(Yc)c?C . That is, we have one variable Yc for each
candidate tuple c ? C . This variable can take as
value any relation in C with the same arity as c. See
example relation variables in figure 1.
The observed input variablesX consists of a fam-
ily of variables Xc =
(
X1c, . . .X
m
c
)
m?M for each
candidate tuple c. Here Xic stores relevant observa-
tions we make for the i-th candidate mention tuple of
c in the corpus. For example, X1BILL GATES,MICROSOFT
in figure 1 would contain, among others, the pattern
?[M2] was founded by [M1]?.
3.1 Factor Templates
Our conditional probability distribution over vari-
ables X and Y is defined using using a set T of
factor templates. Each template Tj ? T defines
a set of factors {(yi,xi)}, a set Kj of feature in-
dices, parameters
{
?jk
}
k?Kj
and feature functions
{
f jk
}
k?Kj
. Together they define the following con-
ditional distribution:
p (y|x) =
1
Zx
?
Tj?T
?
(yi,xi)?Tj
e
P
k?Kj
?jkf
j
k(yi,xi)
(4)
In our case the set T consists of four templates
we will describe below. We construct this graphical
model using FACTORIE (McCallum et al, 2009), a
probabilistic programming language that simplifies
the construction process, as well as inference and
learning.
3.1.1 Bias Template
We use a bias template TBias that prefers certain
relations a priori over others. When the template
is unrolled, it creates one factor per variable Yc for
candidate tuple c ? C. The template also consists of
one weight ?Biasr and feature function f
Bias
r for each
possible relation r. fBiasr fires if the relation associ-
ated with tuple c is r.
3.1.2 Mention Template
In order to extract relations from text, we need
to model the correlation between relation instances
and their mentions in text. For this purpose we de-
fine the template TMen that connects each relation
instance variable Yc with its observed mention vari-
ables Xc. Crucially, this template gathers mentions
from multiple documents, and enables us to exploit
redundancy.
The feature functions of this template are taken
from Mintz et al (2009). This includes features that
inspect the lexical content between entity mentions
in the same sentence, and the syntactic path between
them. One example is
fMen101 (yc,xc)
def
=
?
??
??
1 yc = founded ? ?i with
"M2 was founded by M1" ? xic
0 otherwise
.
1016
founder
Microsoft was 
founded by Bill Gates...
person
company
nationality
country
With Microsoft chairman 
Bill Gates soon relinquishing...
Bill Gates was 
born in the USA  in 1955
1
nationof
Elevation Partners , was 
founded by Roger McNamee ...
Roger McNamee, USA
Yrel
Z1
person
R McNamee
country
USA
worksfor
comp.
Microsoft
1
Z1
1
Z1
Roger McNamee, Microsoft
functionality-factor
ner-relation-factors
relation-mention factors
Ytypel
mention factors
Elevation Partners , was 
founded by Roger McNamee ...
Elevation Partners , was 
founded by Roger McNamee ...
Figure 1: Factor Graph for joint relation mention prediction and relation type identification.
fine the following conditional distribution:
p (y|x) =
1
Zx
?
Tj?T
?
(yi,xi)?Tj
e
PKj
k=1
?
j
k
f
j
k
(yi,xi) (3)
In our case the set T consist of four templates
we will describe below. Note that to construct this
graphical model we use FACTORIE (McCallum et
al., 2009), a probabilistic programming language
that simplifies the construction process, as well as
inference and learning.
3.1.1 Bias Template
We use a bias template TBias that prefers certain
relations a priori over others. When the template is
unrolled, it creates one factor per variable Ycfor can-
didate tuple c and one weight ?Biasr and feature func-
tion fBiasr for each possible relation r. f
Bias
r fires if
the relation associated with tuple c is r.
3.1.2 Mention Template
In order to extract relations from text, we need to
model the correlation between relation instances and
their mentions in text. For this purpose we define
the mention template TMen that connects each rela-
tion instance variable Yc with its observed variables
mention variables XMc .
The feature functions of this template are taken
from (Mintz et al, 2009b) (with minor modifica-
tions). This includes features that inspect the lexical
context between entity mentions in the same sen-
tence, and the syntactic path between these. One
example is
fMen101 (yc,xMc)
def=
?
??
??
1 yc = founder?
m1", director of "m2 ? xMc
0 otherwise
.
It tests whether for any of the mentions of the can-
didate tuple the sequence ", director of " appears be-
tween the mentions of the argument entites.
Crucially, these templates function on a cross-
document level. They gather all mentions of the can-
didate tuple c and extract features from all of these.
3.1.3 Se ectional Preference Templates
To capture the correlations between entity types
and the relations the entities participate in, we in-
troduce the joint template TJoint. It connects a re-
lation instance variable Ye1,...,ea to the entity type
variables Ye1 , . . . , Yen . To measure the compabil-
ity between relation and entity variables, we use
one feature f Jointr,t1...ta (and weight ?
Joint
r,t1...ta
) for each
combination of relation and entity types r, t1 . . . ta.
The feature fires when the variables are in the
state r, t1 . . . ta. After training we would expect
a weight ?Joint
founder,person,company to be larger than
?Joint
founder,person,country.
We also add a template TPair that measures the
compability between Ye1,...,ea and each Yei in iso-
lation. Here we use features fPairi,r,t that fire if ei is
1
nationof
Elevation Partners , was 
founded by Roger McNamee ...
Roger McNamee, USA
Yrel
Z1
person
R McNamee
country
USA
worksfor
comp.
Microsoft
1
Z1
1
Z1
Roger McNamee, Microsoft
functionality-factor
ner-relation-factors
relation-mention factors
Ytypel
mention factors
Elevation Partners , was 
founded by Roger McNamee ...
Elevation Partners , was 
founded by Roger McNamee ...
Figure 1: Factor Graph for joint relation mention prediction and relation type identification.
fine the following conditional distribution:
p (y|x) =
1
Zx
?
Tj?T
?
(yi,xi)?Tj
e
PKj
k=1 ?
j
kf
j
k(yi,xi) (3)
In our case the set T consist of four templates
we will describe below. Note that to construct this
graphical model we use FACTORIE (McCallum et
al., 2009), a probabilistic programming language
that simplifies the construction process, as well as
inference and learning.
3.1.1 Bias Template
We use a bias template TBias that prefers certain
relations a priori over others. When the template is
unrolled, it creates one factor per variable Ycfor can-
didate tuple c and one weight ?Biasr and feature func-
tion fBiasr for each possible relation r. f
Bias
r fires if
the relation associated with tuple c is r.
3.1.2 Mention Template
In order to extract relations from text, we need to
model the correlation between relation instances and
t ir mentions in text. For this purpose we define
the mention template TMen that connects each rela-
tion instance variable Yc with its observed variables
mention variables XMc .
The feature functions of this template are taken
fr m (Mintz et al, 2009b) (with inor modifica-
tions). This includes features that inspect the lexical
context between entity mentions in the same sen-
tence, and the syntactic path between these. One
example is
fMen101 (yc,xMc)
def=
?
??
??
1 yc = founder?
m1", director of "m2 ? xMc
0 otherwise
.
It tests whether for any of the mentions of the can-
didate tuple the sequence ", director of " appears be-
tween the mentions of the argument entites.
Crucially, these templates function on a cross-
document level. They gather all mentions of the can-
didate tuple c and extract features from all of these.
3.1.3 Selectional Preference Templates
To capture the correlations between entity types
and the relations the entities participate in, we in-
troduce th j int tem late TJoint. It connects a re-
la ion instance variable Ye1,...,ea to the entity type
variables Ye1 , . . . , Ye . To measure the compabil-
ity between relation and entity variables, we use
one feature f Jointr,t1...ta (and weight ?
Joint
r,t1...ta
) for each
combination of relation and entity types r, t1 . . . ta.
The feature fires when the variables are in the
state r, t1 . . . ta. After training we would expect
a weight ?Jointfounder,person,company to be larger than
?Jointfounder,person,country.
We also add a template TPair that measures the
compability between Ye1,...,ea and each Yei in iso-
lation. Here we use features fPairi,r,t that fire if ei is
1
nationof
Elevation Partners , was 
founded by Roger McNamee ...
Roger McNamee, USA
Yrel
Z1
person
R McNamee
country
USA
worksfor
comp.
Microsoft
1
Z1
1
Z1
Roger McNamee, Microsoft
functionality-factor
ner-relation-factors
relation-mention factors
Ytypel
mention factors
Elevation Partners , was 
founded by Roger McNamee ...
Elevation Partners , was 
founded by Roger McNamee ...
Figure 1: Factor Graph for joint relation mention prediction and relation type identification.
fine the following conditional distribution:
p (y|x) =
1
Zx
?
Tj?T
?
(yi,xi)?Tj
e
PKj
k=1 ?
j
kf
j
k(yi,xi) (3)
In our case the set T consist of four templates
we will describe below. Note that to construct this
graphical model we use FACTORIE (McCallum et
al., 2009), a probabilistic programming language
that simplifies the construction process, as well as
inference and learning.
3.1.1 Bias Template
We use a bias template TBias that prefers certain
relations a priori over others. When the template is
unrolled, it creates one factor per variable Ycfor can-
didate tuple c and one weight ?Biasr and feature func-
tion fBiasr for each possible relation r. f
Bias
r fires if
the relation associated with tuple c is r.
3.1.2 Mention Template
In order to extract relations from text, we ne d to
model the correlation between relation instances and
their mentions in text. For this purpose we define
the mention template TMen that connects each rela-
tion instance variable Yc with its observed variables
mention variables XMc .
The feature functions of this template are taken
from (Mintz et al, 2009b) (with minor modifica-
tions). This includes features that inspect the lexical
context between entity mentions in the same sen-
tence, and the syntactic path between these. One
example is
fMen101 (yc,xMc)
def=
?
??
??
1 yc = founder?
m1", director of "m2 ? xMc
0 otherwise
.
It tests whether for any of the mentions of the can-
didate tuple the sequence ", director of " appears be-
tween the mentions of the argument entites.
Cruci lly, these templates function on a cross-
document level. Th y gather all mentions of the can-
didate tupl c and extract features from all o these.
3.1.3 Selectional Preference Templates
To capture the correlations between entity types
and the relations he entities participate in, we in-
troduce the joint template TJoint. It connects a re-
lation instance variable Ye1,...,e to the entity type
variables Ye1 , . . . , Yen . To measure the compabil-
ity between relation and entity variables, we use
one feature f Jointr,t1...ta (and weight ?
Joint
r, 1...ta
) for each
combination of relation and entity types r, t1 . . . ta.
The featur fires when the variables are i the
state r, t1 . . . ta. After training we would xpect
a weight ?Jointfounder,person,company to be larger than
?Jointfounder,person,country.
We also add a template TPair that measures the
compability between Ye1,...,ea and each Yei in iso-
lation. Here we use features fPairi,r,t tha fire if ei is
g ( , ) ? DKL ( || )
g ( ) = log
?
1? ?i + ?ie?i
?
? ?ie?i
. . . + w?f? (y , y , y ) + . . .
> 0
= max
y? ,y? ,y?
f?
?
y? , y? , y?
?
< max
y? ,y? ,y?
f?
?
y? , y? , y?
?
?1 (y5,7,;x) = exp (. . . + w f (y;x) + . . .)
? (yi,j ;x) = exp
?
?
k
wkfk (yi,j ;x)
?
p (y;x) =
1
Zx
?1 (y;x) ? . . . ? ?n (y;x)
log E [?i]? ?i
?i (y;x) = exp (?i?i (y;x))
?i = E [?i]
Y
Y
X1
X2
g ( , ) ? DKL ( || )
g ( ) = log
?
1? ?i + ?ie?i
?
? ?ie
?i
. . . + w?f? (y , y , y ) + . . .
> 0
= max
y? ,y? ,y?
f?
?
y? , y? , y?
?
< max
y? ,y? ,y?
f?
?
y? , y? , y?
?
?1 (y5,7,;x) = exp (. . . + w f (y;x) + . . .)
? (yi,j ;x) = exp
?
?
k
wkfk (yi,j ;x)
?
p (y;x) =
1
Zx
?1 (y;x) ? . . . ? ?n (y;x)
log E [?i]? ?i
?i (y;x) = exp (?i?i (y;x))
?i = E [?i]
Y
Y
Y
Y
Y
X1
X2
g ( , ) ? DKL ( || )
g ( ) = log
?
1? ?i + ?ie?i
?
? ?ie
?i
. . . + w?f? (y , y , y ) + . . .
> 0
= max
y? ,y? ,y?
f?
?
y? , y? , y?
?
< max
y? ,y? ,y?
f?
?
y? , y? , y?
?
?1 (y5,7,;x) = exp (. . . + w f (y;x) + . . .)
? (yi,j ;x) = exp
?
?
k
wkfk (yi,j ;x)
?
p (y;x) =
1
Zx
?1 (y;x) ? . . . ? ?n (y;x)
log E [?i]? ?i
?i (y;x) = exp (?i?i (y;x))
?i = E [?i]
Y
Y
Y
Y
Y
X1
X2
g ( , ) ? DKL ( || )
g ( ) = log
?
1? ?i + ?ie?i
?
? ?ie
?i
. . . + w?f? (y , y , y ) + . . .
> 0
= max
y? ,y? ,y?
f?
?
y? , y? , y?
?
< max
y? ,y? ,y?
f?
?
y? , y? , y?
?1 (y5,7,;x) = exp (. . . + w f (y;x) + . . .)
? (yi,j ;x) = exp
?
?
k
wkfk (yi,j ;x)
?
p (y;x) =
1
Zx
?1 (y;x) ? . . . ? ?n (y;x)
log E [?i]? ?i
?i (y;x) = exp (?i?i (y;x))
?i = E [?i]
Y
Y
Y
Y
Y
X1
X2
g ( , ) ? DKL ( || )
g ( ) = log
?
1? ?i + ?ie?i
?
? ?ie
?i
. . . + w?f? (y , y , y ) + . . .
> 0
= max
y? ,y? ,y?
f?
?
y? , y? , y?
?
< max
y? ,y? ,y?
f?
?
y? , y? , y?
?
?1 (y5,7,;x) = exp (. . . + w f (y;x) + . . .)
? (yi,j ;x) = exp
?
?
k
wkfk (yi,j ;x)
?
p (y;x) =
1
Zx
?1 (y;x) ? . . . ? ?n (y;x)
log E [?i]? ?i
?i (y;x) = exp (?i?i )
?i = E [?i]
Y
Y
Y
Y
Y
X1
X2
g ( , ) ? DKL ( || )
g ( ) = log
?
1? ?i + ?ie?i
?
? ?ie
?i
. . . + w?f? (y , y , y ) + . . .
> 0
= max
y? ,y? ,y?
f?
?
y? , y? , y?
?
< max
y? ,y? ,y?
f?
?
y? , y? , y?
?
?1 (y5,7,; exp (. . . + w f (y;x) + . . .)
? (yi,j ;x) = exp
?
?
k
wkfk (yi,j ;x)
?
p (y;x) =
1
Zx
?1 (y;x) ? . . . ? ?n (y;x)
log E [?i]? ?i
?i (y;x) = exp (?i?i (y;x))
?i = E [?i]
Y
Y
Y
Y
Y
X1
X2
g ( , ) ? DKL ( || )
g ( ) = log
?
1? ?i + ?ie?i
?
? ?ie
?i
. . . + w?f? (y , y , y ) + . . .
> 0
= max
y? ,y? ,y?
f?
?
y? , y? , y?
?
< max
y? ,y? ,y?
f?
?
y? , y? , y?
?
?1 (y5,7,;x) = exp (. . . + w f (y;x) + . . .)
? (yi,j ;x) = exp
?
?
k
wkfk (yi,j ;x)
?
p (y;x) =
1
Zx
?1 (y;x) ? . . . ? ?n (y;x)
log E [?i]? ?i
?i (y;x) = exp (?i?i (y;x))
?i = E [?i]
Y
Y
Y
Y
Y
X1
X2
Figure 1: Factor Graph of our model that captures selectional preferences and functionality constraints. For
readability we only label a subsets of equivalent variables and factors. Note that the graph shows an example
assignment to variables.
It tests whether for any mentions of the candidate
tuple the phrase "founded by" appears between the
mentions of the argument entities.
3.1.3 S lectional Preference T mplates
To capture the correlations between entity types
and relations the entities participate in, we introduce
the template TJoint. It connects a relation instance
variable Ye1,...,en to the individual entity type vari-
ables Ye1 , . . . , Yen . To measure the compatibility
between relation and entity variables, we use one
feature f Jointr,t1...ta (and weight ?
Joint
r,t1...ta) for each com-
bination of relation and entity types r, t1 . . . ta.
f Jointr,t1...ta fires when the factor variables are in the
state r, t1 . . . ta. For example, f Jointfounded,person,company
fires if Ye1 is in state person, Ye2 in state company,
and Ye1,e2 in state founded.
We also add a template TPair that measures the
pairwise compatibility between the relation variable
Ye1,...,ea and each entity variable Yei in isolation.
Here we use features fPairi,r,t that fire if ei is the i-th ar-
gument of c, has the entity type t and the candidate
tuple c is labelled as instance of relation r. For ex-
ample, fPair1,founded,person fires if Ye1(argument i = 1)
is in state person, and Ye1,e2 in state founded, re-
gardless of the state of Ye2 .
3.2 Inference
There are two types of inference we have to perform:
sampling from the posterior during training (see sec-
tion 3.3), and finding the most likely configuration
(aka MAP inference). In both settings we employ a
Gibbs sampler (Geman and Geman, 1990) that ran-
domly picks a variable Yc and samples its relation
value conditioned on its Markov Blanket. At test
time we decrease the temperature of our sampler in
order to find an approximation of the MAP solution.
3.3 Training
Most learning methods need to calculate the model
expectations (Lafferty et al, 2001) or the MAP con-
figuration (Collins, 2002) before making an update
to the parameters. This step of inference is usually
the bottleneck for learning, even when performed
approximately.
SampleRank (Wick et al, 2009) is a rank-based
learning framework that alleviates this problem by
performing parameter updates within MCMC infer-
ence. Every pair of consecutive samples in the
MCMC chain is ranked according to the model and
the ground truth, and the parameters are updated
when the rankings disagree. This update can fol-
low different schemes, here we use MIRA (Cram-
mer and Singer, 2003). This allows the learner to
acquire more supervision per instance, and has led
to efficient training for models in which inference
1017
is expensive and generally intractable (Singh et al,
2009).
4 Related Work
Distant Supervision Learning to extract relations
by using distant supervision has raised much interest
in recent years. Our work is inspired by Mintz et al
(2009) who also use Freebase as distant supervision
source. We also heuristically align our knowledge
base to text by making the distant supervision as-
sumption (Bunescu and Mooney, 2007; Mintz et al,
2009). However, in contrast to these previous ap-
proaches, and other related distant supervision meth-
ods (Craven and Kumlien, 1999; Weld et al, 2009;
Hoffmann et al, 2010), we perform relation extrac-
tion collectively with entity type prediction.
Schoenmackers et al (2008) use entailment rules
on assertion extracted by TextRunner to increase re-
call. They also perform cross-document probabilis-
tic inference based on Markov Networks. However,
they do not infer the types of entities and work in an
open IE setting.
Selectional Preferences In the context of super-
vised relation extraction, selectional preferences
have been applied. For example, Roth and Yih
(2007) have used Linear Programming to enforce
consistency between entity types and extracted re-
lations. Kate and Mooney (2010) use a pyramid
parsing scheme to achieve the same. Riedel et al
(2009) use Markov Logic to model interactions be-
tween event-argument relations for biomedical event
extraction. However, their work is (a) supervised,
and (b) performs extraction on a per-sentence basis.
Carlson et al (2010) also use selectional prefer-
ences. However, instead of exploiting them for train-
ing a graphical model using distant supervision, they
use selectional preferences to improve a bootstrap-
ping process. Here in each iteration of bootstrap-
ping, extracted facts that violate compatibility con-
straints will not be used to generate additional pat-
terns in the next iteration.
5 Experiments
We set up experiments to answer the following ques-
tions: (i) Does the explicit modelling of selectional
preferences improve accuracy? (ii) Can we also per-
form joint entity and relation extraction in a pipeline
and achieve similar results? (iii) How does our
cross-document approach scale?
To answer these questions we carry out experi-
ments on two data sets, Wikipedia and New York
Times articles, and use Freebase as distant supervi-
sion source for both.
5.1 Experimental Setup
We follow Mintz et al (2009) and perform two types
of evaluation: held-out and manual. In both cases
we have a training and a test corpus of documents,
and training and test sets of entities. For held-out
evaluation we split the set of entities in Freebase into
training and test sets. For manual evaluation we use
all Freebase entities during training. For testing we
use all entities that appear in the test document cor-
pus.
For both training and testing we then choose the
candidate tuples C that may or may not be relation
instances. To pick the entities C1 we want to predict
entity types for, we choose all entities that are men-
tioned at least once in the train/test corpus. To pick
the entity pairs C2 that we want to predict the rela-
tions of, we choose those that appear at least once
together in a sentence.
The set of candidates C will contain many tuples
which are not related in any Freebase relations. For
efficiency, we filter out a large fraction of these neg-
ative candidates for training. The number of neg-
ative examples we keep is chosen to be about 10
times the number of positive candidates. This num-
ber stems from trading-off the accuracy it leads to
and the increased training time it requires.
For both manual and held-out evaluation we rank
extracted test relation instances in the MAP state of
the network. This state is found by sampling 20 iter-
ations with a low temperature of 0.00001. The rank-
ing is done according to the log linear score that the
assigned relation for a candidate tuple gets from the
factors in its Markov Blanket. For optimal perfor-
mance, the score is normalized by the number of re-
lation mentions.
For manual evaluation we pick the top ranked 50
relation instances for the most frequent relations.
We ask three annotators to inspect the mentions of
these relation instances to decide whether they are
correct. Upon disagreement, we use majority vote.
To summarize precisions across relations, we take
1018
their average, and their average weighted by the pro-
portion of predicted instances for the given relation.
5.1.1 Data preprocessing
We preprocess our textual data as follows:
We first use the Stanford named entity recog-
nizer (Finkel et al, 2005) to find entity mentions in
the corpus. The NER tagger segments each docu-
ment into sentences and classifies each token into
four categories: PERSON, ORGANIZATION, LO-
CATION and NONE. We treat consecutive tokens
which share the same category as single entity men-
tion. Then we associate these mentions with Free-
base entities. This is achieved by performing a
string match between entity mention phrases and the
canonical names of entities as present in Freebase.
For each candidate tuple c with arity 2 and each
of its mention tuples iwe extract a set of featuresXic
similar to those used in (Mintz et al, 2009): lexical,
Part-Of-Speech (POS), named entity and syntactic
features, i.e. features obtained from the dependency
parsing tree of a sentence. We use the openNLP POS
tagger4 to obtain POS tags and employ the Malt-
Parser (Nivre et al, 2004) for dependency parsing.
For candidate tuples with arity 1 (entity types) we
use the following features: the entity?s word form,
the POS sequence, the head of the entity in the de-
pendency parse tree, the Stanford named entity tag,
and the left and right words to the current entity
mention phrase.
5.1.2 Configurations
We apply the following configurations of our fac-
tor graphs. As our baseline, and roughly equivalent
to previous work (Mintz et al, 2009), we pick the
templates TBias and TMen. These describe a fully dis-
connected graph, and we will refer to this configu-
ration as isolated. Next, we add the templates TJoint
and TPair to model selectional preferences, and refer
to this setting as joint.
In addition, we evaluate howwell selectional pref-
erences can be captured with a simple pipeline. For
this pipeline we first train an isolated system for en-
tity type prediction. Then we use the output of the
entity type prediction system as input for the relation
extraction system.
4available at http://opennlp.sourceforge.net/
5.1.3 Entity types and Relation types
Freebase contains many relation types and only
a subset of those relation types occur frequently
in the corpus. Since classes with very few
training instances are generally hard to learn,
we restrict ourselves to the 54 most frequently
mentioned relations. These include, for ex-
ample, nationality, contains, founded
and place_of_birth. Note that we con-
vert two Freebase non-binary temporal relations
to binary relations: employment_tenure and
place_lived. In both cases we simply disregard
the temporal information in the Freebase data.
As our main focus is relation extraction, we re-
strict ourselves to entity types compatible with our
selected relations. To this end we inspect the Free-
base schema information provided for each relation,
and include those entity types that are declared as
arguments of our relations. This leads to 10 entity
types including person, citytown, country,
and company.
Note that a Freebase entity can have several types.
We pick one of these by choosing the most specific
one that is a member of our entity type subset, or
MISC if no such member exists.
5.2 Wikipedia
In our first set of experiments we train and test using
Wikipedia as the text corpus. This is a comparatively
easy scenario because the facts in Freebase are partly
derived from Wikipedia, hence there is an increased
chance of properly aligning training facts and text.
This is similar to the setting of Mintz et al (2009).
5.2.1 Held Out Evaluation
We split 1,300,000 Wikipedia articles into train-
ing and test sets. Table 1 shows the statistics for this
split. The last row provides the number of negative
relation instances (candidates which are not related
according to Freebase) associated with each data set.
Figure 2 shows the precision-recall curves of re-
lation extraction for held-out data of various config-
urations. We notice a slight advantage of the joint
approach in the low recall area. Moreover, the joint
model predicts more relation instances, as can be
seen by its longer line in the graph.
For higher recall, the joint model performs
slightly worse. On closer inspection, we find that
1019
Wikipedia NYT
Train Test Train Test
#Documents 900K 400K 177K 39K
#Entities 213K 137K 56K 27K
#Positive 36K 24K 5K 2K
#Negative 219K 590K 64K 94K
Table 1: The statistics of held-out evaluation on
Wikipedia and New York Times.
0.0 0.1 0.2 0.3 0.4
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Recall
Pre
cisio
n
joint
pipe
isolated
Figure 2: Precision-recall curves for various setups
in Wikipedia held-out setting.
this observation is somewhat misleading. Many of
the predictions of the joint model are not in the
held-out test set derived from Freebase, but never-
theless correct. Hence, to understand if one system
really outperforms another, we need to rely on man-
ual evaluation.
Note that the figure only considers binary
relations?for entity types all configurations per-
form similarly.
5.2.2 Manual Evaluation
As mentioned above, held-out evaluation in this
context suffers from false negatives in Freebase. Ta-
ble 2 therefore shows the results of our manual eval-
uation. They are based on the average, and weighted
average, of the precisions for the relation instances
of the most frequent relations. We notice that here
Isolated Pipeline Joint
Wikipedia 0.82 0.87 0.86
Wikipedia (w) 0.95 0.94 0.95
NYT 0.63 0.65 0.78
NYT (w) 0.78 0.82 0.94
Table 2: Average and weighted (w) average preci-
sion over frequent relations for New York Times and
Wikipedia data, based on manual evaluation.
all systems perform comparably for weighted aver-
age precision. For average precision we see an ad-
vantage for both the pipeline and the joint model
over the isolated system.
One reason for similar weighted average preci-
sions is the fact that all approaches accurately pre-
dict a large number of contains instances. This is
due to very regular and simple patterns inWikipedia.
For example, most articles on towns start with ?A is
a municipality in the district of B in C, D.? For these
sentences, the relative position of two location men-
tions is a very good predictor of contains. When
used as a feature, it leads to high precision for all
models. And since contains instances are most
frequent, and we take the weighted average, results
are generally close to each other.
To summarize: in this in-domain setting, mod-
elling compatibility between entity types and rela-
tions helps to improve average precision, but not
weighted average precision. This holds for both the
joint and the pipeline model. However, we will see
how this changes substantially when moving to an
out-of-domain scenario.
5.3 New York Times
For our second set of experiments we use New
York Times data as training and test corpora. As
we argued before, this is expected to be the more
difficult?and more realistic?scenario.
5.3.1 Held-out Evaluation
We choose all articles of the New York times dur-
ing 2005 and 2006 as training corpus. As test corpus
we use the first 6 months of 2007.
Figure 3 shows precision-recall curves for our var-
ious setups. We see that jointly modelling entity
1020
0.00 0.05 0.10 0.15 0.20
0.2
0.4
0.6
0.8
1.0
Recall
Pre
cisio
n
joint
pipe
isolated
Figure 3: Precision-recall curves for various setups
in New York Times held-out setting.
types and relations helps to improve precision.
Due to the smaller overlap between Freebase and
NYT data, figure 3 also has to be taken with more
caution. The systems may predict correct relation
instances that just do not appear in Freebase. Hence
manual evaluation is even more important.
When evaluating entity precision we find that for
both models it is about 84%. This raises the ques-
tion why the joint entity type and relation extrac-
tion model outperforms the pipeline on relations.
We take a close look at the entities which partici-
pate in relations and find that joint model performs
better on most entity types, for example, country
and citytown. We also look at the relation in-
stances which are predicted by both systems and find
that the joint model does predict correct entity types
when the pipeline mis-predicts. And exactly these
mis-predictions lead the pipeline astray. Consider-
ing binary relation instances where the pipeline fails
but the joint model does not, we observe an entity
precision of 76% for the pipeline and 86% for our
joint approach. The joint model fails to correctly
predict some entity types that the pipeline gets right,
but these tend to appear in contexts where relation
instances are easy to extract without considering en-
Relation Type Iso. Pipe Joint
contains 0.92 0.98 0.96
nationality 0.28 0.64 0.82
plc_lived 0.88 0.70 0.96
plc_of_birth 0.32 0.20 0.25
works_for 0.96 0.98 0.98
plc_of_death 0.24 0.40 0.42
children 1.00 0.92 0.98
founded 0.42 0.34 0.71
Table 3: Precision at 50 for the most frequent rela-
tions on New York Times
tity types.5
5.3.2 Manual Evaluation
Manually evaluated precision for New York
Times data can be seen in table 2. In contrast to the
Wiki setting, here modelling entity types and rela-
tions jointly makes a substantial difference. For av-
erage precision, our joint model improves over the
isolated baseline by 15%, and over the pipeline by
13%. Similar improvements can be observed for
weighted average precision.
Let us look at a break-down of precisions with
respect to different relations shown in table 3. We
see dramatic improvements for nationality and
founded when applying the joint model. Note that
the nationality relation takes a larger part in
the predicted relation instances of the joint model
and hence contributes significantly to the weighted
average precision.
5.4 Scalability
We propose to perform joint inference for large scale
information extraction. An obvious concern in this
scenario is scalability. In practice we find that infer-
ence (and hence learning) in our model scales lin-
early with the number of candidate tuples. This can
be seen in figure 4a. It is to be expected since the
number of candidates equals the number of variables
the sampler has to process in each iteration.
The above observation also means that our ap-
proach scales linearly with corpus size. To illustrate
5Note that our learned preferences are soft, and hence can
be violated in case of wrong entity type predictions.
1021
1e+05 2e+05 3e+05 4e+05 5e+05
200
300
400
500
600
700
800
Number of Candidate Tuples
Time pe
r iteratio
n (seco
nds)
(a) CPU time
0 5000 10000 15000 20000 25000
1e+05
2e+05
3e+05
4e+05
5e+05
6e+05
Number of Documents
Numbe
r of Can
didate T
uples
(b) Candidate tuples
Figure 4: CPU time for one iteration per candidate
tuple, and candidate tuples per document.
this, figure 4b shows how the number of candidates
scales with the number of documents. Again we ob-
serve a linear behavior. Since both are linear, we can
say that our joint approach is linear in the number of
documents.
Total training and test times are moderate, too.
For example, the held-out experiments with 200,000
NYT documents finish within three hours.
6 Conclusion
This paper presents a novel approach to extracting
relational facts from text. Akin to previous work in
relation extraction with distant supervision, we re-
quire no annotated text. However, instead extract-
ing facts in isolation, we model interactions between
facts in order to improve precision. In particular, we
capture selectional preferences of relations. These
preferences are modelled in a cross-document fash-
ion using a large scale factor graph. We show in-
ference and learning can be efficiently performed
in linear time by Gibbs Sampling and SampleRank.
When applied to out-of-domain text, this approach
leads to a 15% increase in precision over an isolated
baseline, and a 13% improvement over a pipelined
system.
A crucial aspect of our approach is its extensibil-
ity. Since it is exclusively framed in terms of an
undirected graphical model, it is conceptually easy
to extend it to other types of compatibilities, such
as functionality constraints. It could also be ex-
tended to tackle coreference resolution. Eventually
we seek to model the complete process of the au-
tomatic construction of KB within this framework,
and capture dependencies between extractions in a
joint and principled fashion. As we have seen here,
in particular when learning is less supervised and
extractions are noisy, capturing such interactions is
paramount.
Acknowledgements
This work was supported in part by the Center for
Intelligent Information Retrieval, in part by The
Central Intelligence Agency, the National Secu-
rity Agency and National Science Foundation un-
der NSF grant #IIS-0326249, and in part by UPenn
NSF medium IIS-0803847. The University of Mas-
sachusetts also gratefully acknowledges the support
of Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0181. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the author(s) and do not
necessarily reflect the view of the DARPA, AFRL,
or the US government.
References
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collabo-
ratively created graph database for structuring human
knowledge. In SIGMOD ?08: Proceedings of the 2008
ACM SIGMOD international conference on Manage-
ment of data, pages 1247?1250, New York, NY, USA.
ACM.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using min-
imal supervision. In Proceedings of the 45rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?07).
Andrew Carlson, Justin Betteridge, Richard Wang, Es-
tevam Hruschka, and Tom Mitchell. 2010. Cou-
pled semi-supervised learning for information extrac-
tion. In Third ACM International Conference on Web
Search and Data Mining (WSDM ?10).
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the Conference on Empirical methods in natural lan-
guage processing (EMNLP ?02), volume 10, pages 1?
8.
1022
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951?991.
M. Craven and J. Kumlien. 1999. Constructing biolog-
ical knowledge-bases by extracting information from
text sources. In Proceedings of the Seventh Interna-
tional Conference on Intelligent Systems for Molecular
Biology, pages 77?86, Germany.
Aron Culotta and Jeffery Sorensen. 2004. Dependency
tree kernels for relation extraction. In 42nd Annual
Meeting of the Association for Computational Linguis-
tics, Barcelona, Spain.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL
?05), pages 363?370, June.
S. Geman and D. Geman. 1990. Stochastic relaxation,
gibbs distributions, and the bayesian restoration of im-
ages. pages 452?472.
Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.
2010. Learning 5000 relational extractors. In ACL.
Rohit J. Kate and Raymond J. Mooney. 2010. Joint en-
tity and relation extraction using card-pyramid pars-
ing. In Proceedings of the 12th Conference on Com-
putational Natural Language Learning (CoNLL? 10).
John D. Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In International Conference on Machine Learn-
ing (ICML).
Andrew McCallum, Karl Schultz, and Sameer Singh.
2009. Factorie: Probabilistic programming via imper-
atively defined factor graphs. In Y. Bengio, D. Schuur-
mans, J. Lafferty, C. K. I. Williams, and A. Culotta, ed-
itors, Advances in Neural Information Processing Sys-
tems 22, pages 1249?1257.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP (ACL ?09),
pages 1003?1011. Association for Computational Lin-
guistics.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of CoNLL, pages
49?56.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A markov logic approach to
bio-molecular event extraction. In Proceedings of the
Natural Language Processing in Biomedicine NAACL
2009 Workshop (BioNLP ?09), pages 41?49.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD ?10).
D. Roth and W. Yih. 2007. Global inference for en-
tity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
Evan Sandhaus, 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia.
Stefan Schoenmackers, Oren Etzioni, and Daniel S.
Weld. 2008. Scaling textual inference to the web.
In EMNLP ?08: Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 79?88, Morristown, NJ, USA. Association for
Computational Linguistics.
Sameer Singh, Karl Schultz, and Andrew McCallum.
2009. Bi-directional joint inference for entity res-
olution and segmentation using imperatively-defined
factor graphs. In European Conference on Machine
Learning and Principles and Practice of Knowledge
Discovery in Databases (ECML PKDD), pages 414?
429.
Daniel S. Weld, Raphael Hoffmann, and Fei Wu. 2009.
Using wikipedia to bootstrap open information extrac-
tion. In ACM SIGMOD Record.
Michael Wick, Khashayar Rohanimanesh, Aron Culotta,
and Andrew McCallum. 2009. Samplerank: Learning
preferences from atomic gradients. In Neural Infor-
mation Processing Systems (NIPS), Workshop on Ad-
vances in Ranking.
Dimitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. JMLR, 3(6):1083 ? 1106.
1023
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1?12,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Fast and Robust Joint Models for Biomedical Event Extraction
Sebastian Riedel Andrew McCallum
Department of Computer Science
University of Massachusetts, Amherst
{riedel,mccallum}@cs.umass.edu
Abstract
Extracting biomedical events from literature
has attracted much recent attention. The best-
performing systems so far have been pipelines
of simple subtask-specific local classifiers. A
natural drawback of such approaches are cas-
cading errors introduced in early stages of the
pipeline. We present three joint models of
increasing complexity designed to overcome
this problem. The first model performs joint
trigger and argument extraction, and lends it-
self to a simple, efficient and exact infer-
ence algorithm. The second model captures
correlations between events, while the third
model ensures consistency between arguments
of the same event. Inference in these models
is kept tractable through dual decomposition.
The first two models outperform the previous
best joint approaches and are very competi-
tive with respect to the current state-of-the-
art. The third model yields the best results re-
ported so far on the BioNLP 2009 shared task,
the BioNLP 2011 Genia task and the BioNLP
2011 Infectious Diseases task.
1 Introduction
Whenever we advance our scientific understanding
of the world, we seek to publish our findings. The
result is a vast and ever-expanding body of natural
language text that is becoming increasingly difficult
to leverage. This is particularly true in the context
of life sciences, where large quantities of biomedi-
cal articles are published on a daily basis. To sup-
port tasks such data mining, search and visualiza-
tion, there is a clear need for structured representa-
tions of the knowledge these articles convey. This is
indicated by a large number of public databases with
content ranging from simple protein-protein interac-
tions to complex pathways. To increase coverage of
such databases, and to keep up with the rate of pub-
lishing, we need to automatically extract structured
representations from biomedical text?a process of-
ten referred to as biomedical text mining.
One major focus of biomedical text mining has
been the extraction of named entities, such genes
or gene products, and of flat binary relations be-
tween such entities, such as protein-protein interac-
tions. However, in recent years there has also been
an increasing interest in the extraction of biomedi-
cal events and their causal relations. This gave rise
to the BioNLP 2009 and 2011 shared tasks which
challenged participants to gather such events from
biomedical text (Kim et al, 2009; Kim et al, 2011).
Notably, these events can be complex and recursive:
they may have several arguments, and some of the
arguments may be events themselves.
Current state-of-the-art event extractors fol-
low the same architectural blueprint and divide
the extraction process into a pipeline of three
stages (Bj?rne et al, 2009; Miwa et al, 2010c). First
they predict a set of candidate event trigger words
(say, tokens 2, 5 and 6 in figure 1), then argument
mentions are attached to these triggers (say, token
4 for trigger 2). The final stage decides how ar-
guments are shared between events?compare how
one event subsumes all arguments of trigger 6 in fig-
ure 1, while two events share the three arguments
of trigger 4 in figure 2. This architecture is prone
to cascading errors: If we miss a trigger in the first
stage, we will never be able to extract the full event
1
... the phosphorylation of TRAF2 inhibits binding to the CD40 cytoplasmic domain ...
E1:Phosphorylation
E2:Regulation
E3:Binding
Theme
Cause
Theme
Theme
Theme
Regulation BindingPhosphorylation
Theme
Cause
Theme
Theme
Theme
Same Binding
1 2 3
4
5 6 7 8 9 10 11
b
4,9
e
2,Phos.
a
6,9,Theme
(a)
(b)
Figure 1: (a) sentence with target event structure to extract; (b) projection to a set of labelled graph over tokens.
it concerns. Current systems attempt to tackle this
problem by passing several candidates to the next
stage. However, this tends to increase the false pos-
itive rate. In fact, Miwa et al (2010c) observe that
30% of their errors stem from this type of ad-hoc
module communication.
Joint models have been proposed to overcome this
problem (Poon and Vanderwende, 2010; Riedel et
al., 2009). However, besides not being as accurate
as their pipelined competitors, mostly because they
do not yet exploit the rich set of features used by
Miwa et al (2010b) and Bj?rne et al (2009), they
also suffer from the complexity of inference. For
example, to remain tractable, the best joint system
so far (Poon and Vanderwende, 2010) works with
a simplified representation of the problem in which
certain features are harder to capture, employs local
search without certificates of optimality, and further-
more requires a 32-core cluster for quick train-test
cycles. Existing joint models also rely on heuristics
when it comes to deciding which arguments share
the same event. Contrast this with the best current
pipeline (Miwa et al, 2010c; Miwa et al, 2010b)
which uses a classifier for this task.
We present a family of event extraction mod-
els that address the aforementioned problems. The
first model jointly predicts triggers and arguments.
Notably, the highest scoring event structure under
this model can be found efficiently in O (mn) time
where m is the number of trigger candidates, and
n the number of argument candidates. This is
only slightly slower than the O (m?n) runtime of a
pipeline, where m? is the number of trigger candi-
dates as filtered by the first stage. We achieve these
guarantees through a novel algorithm that jointly
picks best trigger label and arguments on a per-token
basis. Remarkably, it takes roughly as much time to
train this model on one core as the model of Poon
and Vanderwende (2010) on 32 cores, and leads to
better results.
The second model enforces additional constraints
that ensure consistency between events in hierarchi-
cal regulation structures. While inference in this
model is more complicated, we show how dual de-
composition (Komodakis et al, 2007; Rush et al,
2010) can be used to efficiently find exact solutions
for a large fraction of problems.
Our third model includes the first two, and explic-
itly captures which arguments are part in the same
event?the third stage of existing pipelines. Due to
a complex coupling between this model and the first
two, inference here requires a projected version of
the sub-gradient technique demonstrated by Rush et
al. (2010).
When evaluated on the BioNLP 2009 shared task,
the first two models outperform the previous best
joint approaches and are competitive when com-
pared to current state-of-the-art. With 57.4 F1 on
the test set, the third model yields the best results
reported so far with a 1.1 F1 margin to the results
of Miwa et al (2010b). For the BioNLP 2011 Ge-
nia task 1 and the BioNLP 2011 Infectious Diseases
task, Model 3 yields the second-best and best results
reported so far. The second-best results are achieved
with Model 3 as is (Riedel and McCallum, 2011),
the best results when using Stanford event predic-
tions as input features (Riedel et al, 2011). The
margins between Model 3 and the best runner-ups
range from 1.9 F1 to 2.8 F1.
In the following we will first introduce biomedical
event extraction and our notation. Then we go on to
present our models and their inference routines. We
present related work, show our empirical evaluation,
and conclude.
2
Grb2 can be coimmunoprecipitated with Sos1 and Sos2
Binding Binding
Theme
Theme
Theme
Theme
Theme
Theme
Theme
1
2 3 4 5 6 7 8
Figure 2: Two binding events with identical trigger. The
projection graph does not change even if both events are
merged.
2 Biomedical Event Extraction
By bio-molecular event we mean a change of state
of one or more bio-molecules. Our task is to extract
structured information about such events from nat-
ural language text. More concretely, let us consider
part (a) of figure 1. We see a snippet of text from a
biomedical abstract, and the three events that can be
extracted from it. We will use these to characterize
the types of events we ought to extract, as defined
by the 2009 BioNLP shared task. Note that for the
shared task, protein mentions are given by the task
organizers and hence do not need to be extracted.
The event E1 in the figure refers to a Phosphory-
lation of the TRAF2 protein. It is an instance of a
set of simple events that describe changes to a sin-
gle gene or gene product. Other members of this
set are: Expression, Transcription, Localization, and
Catabolism. Each of these events has to have exactly
one theme, the protein of which a state change is de-
scribed. A labelled edge in figure 1a) shows that
TRAF2 is the theme of E1.
Event E3 is a Binding of TRAF2 and CD40.
Binding events are particular in that they may have
more than one theme, as there can be several bio-
molecules associated in a binding structure. This is
in fact the case for E3.
In the top-center of figure 1a) we see the Regu-
lation event E2. Such events describe regulatory or
causal relations between events. Other instances of
this type of events are: Positive Regulation and Neg-
ative Regulation. Regulations have to have exactly
one theme; this theme can a be protein or, as in our
case, another event. Regulations may also have zero
or one cause arguments that denote events or pro-
teins which trigger the regulation.
In the BioNLP shared task, we are also asked to
find a trigger (or clue) token for each event. This
token grounds the event in text and allows users to
quickly validate extracted events. For example, the
trigger for event E2 is ?inhibit?, as indicated by a
dashed line.
2.1 Event Projection
To formulate the search for event structures of the
form shown in figure 1a) as an optimization prob-
lem, it will be convenient to represent them through
a set of binary variables. We introduce such a rep-
resentation, inspired by previous work (Riedel et al,
2009; Bj?rne et al, 2009) and based on a projection
of events to a graph structure over tokens, as seen
figure 1b).
Consider sentence x and a set of candidate trig-
ger tokens, denoted by Trig (x). We label each can-
didate i with the event type it is a trigger for, or
None if it is not a trigger. This decision is rep-
resented through a set of binary variables ei,t, one
for each possible event type t. In our example we
have e6,Binding = 1. The set of possible event types
will be denoted as T , the regulation event types as
TReg def= {PosReg, NegReg, Reg} and its complement
as T?reg def= T \ TReg.
For each candidate trigger i we consider the argu-
ments of all events that have i as trigger. Each ar-
gument a will either be an event itself, or a protein.
For events we add a labelled edge between i and the
trigger j of a. For proteins we add an edge between
i and the syntactic head j of the protein mention. In
both cases we label the edge i ? j with the role
of the argument a. The edge is represented through
a binary variable ai,j,r, where r ? R is the argu-
ment role and R def= {Theme, Cause, None}. The
role None is active whenever no Theme or Cause
role is present. In our example we get, among oth-
ers, a2,4,Theme = 1.
So far our representation is equivalent to map-
pings in previous work (Riedel et al, 2009; Bj?rne et
al., 2009) and hence shares their main shortcoming:
we cannot differentiate between two (or more) bind-
ing events with the same trigger but different argu-
ments, or one binding event with several arguments.
Consider, for example, the arguments of trigger 6 in
figure 1b) that are all subsumed in a single event. By
contrast, the arguments of trigger 4 shown in figure
2 are split between two events.
Previous work has resolved this ambiguity
3
through ad-hoc rules (Bj?rne et al, 2009) or with
a post-processing classifier (Miwa et al, 2010c).
We propose to augment the graph representation
through edges between pairs of proteins that are
themes in the same binding event. For two protein
tokens p and q we represent this edge through the
binary variable bp,q. Hence, in figure 1b) we have
b4,9 = 1, whereas for figure 2 we get b1,6 = b1,8 = 1
but b6,8 = 0. By explicitly modeling such ?sib-
ling? edges we not only minimize the need for post-
processing. We can also improve attachment deci-
sions akin to second order models in dependency
parsing (McDonald and Pereira, 2006). Note that
while merely introducing such variables is easy, en-
forcing consistency between them and the ei,t and
ai,j,r variables is not. We address this in section
3.3.1.
Reconstruction of events from solutions (e,a,b)
can be done almost exactly as described by Bj?rne
et al (2009). However, while they group binding
arguments according to ad-hoc rules based on de-
pendency paths from trigger to argument, we simply
query the variables bp,q.
To simplify our exposition we introduce addi-
tional notation. We denote the set of protein head
tokens with Prot (x); the set of a possible targets
for outgoing edges from a trigger is Cand(x) def=
Trig (x) ? Prot (x). We will often omit the do-
mains of indices and instead assign them a fixed do-
main in advance: i, l ? Trig (x), j, k ? Cand (x),
p, q ? Prot (x), r ? R and t ? T . Bold face
letters are used to denote composite vectors e, a
and b of variables ei,t, ai,j,r and bp,q. The vector
y is the joint vector of e,a and b. The short-form
ei ? t will mean ?t? : ei,t? ? ?t,t? where ?t,t? is
the Kronecker Delta. Likewise, ai,j ? r means
?r? : ai,j,r? ? ?r,r? .
3 Models
In this section we will present three structured pre-
diction models of increasing complexity and expres-
siveness, as well as their corresponding MAP infer-
ence algorithms. Each model m can be represented
by a mapping from sentence x to a set of legal struc-
tures Ym (x), and a linear scoring function
sm (y;x,w) = ?w, f (y,x)? . (1)
Here f is a feature function on structures y and input
x, and w is a weight vector for these features.
We can use the scoring function sm and the set of
legal structures Ym (x) to predict the event hm (x)
for a given sentence x according to
hm (x) def= arg max
y?Ym(x)
sm (y;x,w) . (2)
For brevity we will from now on omit observations x
and weights w when they are clear from the context.
3.1 Model 1
Model 1 performs a simple version of joint trigger
and argument extraction. It independently scores
trigger labels and argument roles:
s1 (e,a) def=
?
ei,t=1
sT (i, t) +
?
ai,j,r=1
sR (i, j, r) . (3)
Here sT (i, t) = ?wT, fT (i, t)? is a per-trigger scor-
ing function that measures how well the event la-
bel t fits to token i. Likewise, sR (i, j, r) =
?wR, fR (i, j, r)? measures the compatibility of role
r as label for the edge i? j.
The jointness of Model 1 stems from enforcing
consistency between the trigger label of i and its out-
going edges. By consistency we mean that: (a) there
is at least one Theme whenever there is an event at i;
(b) only regulation events are allowed to have Cause
arguments; (c) all arguments of a None trigger must
have the None role. We will denote the set assign-
ments that fulfill these constraints by O and hence
have Y1 def= O.
Enforcing (e,a) ? O guarantees that we never
predict triggers i for which no sensible, high-
scoring, argument j can be found. It also ensures
that when we see an ?obvious? argument edge i r? j
with high score sR (i, j, r) there is pressure to extract
a trigger at i, even if the fact that i is a trigger may
not be as obvious.
3.1.1 Inference
As it turns out, the maximizer of equation 2 can be
found very efficiently in O (mn) time where m =
|Trig (x)| and n = |Cand (x)|. The corresponding
procedure, bestOut(?), is shown in algorithm 1. It
takes as input a vector of trigger and edge penalties
c that are added to the local scores of the sT and
sR functions. For Model 2 and 3 we will use these
4
penalties to enforce agreement with predictions of
other inference subroutines. When using Model 1
by itself we set them to 0. We point out that the
scoring function s1 is multiplied with 12 throughoutthe algorithm. For doing inference in Model 1 and
2 this has no effect, but when we use bestOut(?) for
Model 3 inference, it is required.
The bestOut (c) routine exploits the fact that the
constraints of Model 1 only act on the label for
trigger i and its outgoing edges. In particular, en-
forcing consistency between ei,t and outgoing edges
ai,j,r has no effect on consistency between el,t and
ai?,j?,r? for any other trigger i? 6= i. Moreover,
for a given trigger the constraints only differenti-
ate between three cases: (a) regulation event, (b)
non-regulation event and (c) no event. This means
that we can extract events on a per-trigger basis,
and find the best per-trigger structure by compar-
ing cases (a), (b) and (c). Note that bestOut (c)
uses the shorthand emptyOut (i) to denote the par-
tial assignment ei ? None and ?j : ai,j ? None.
The function sc1 (i,y) def=
?
t ei,t
(
ci,t + 12sT (i, t)
)
+?
j,r ai,j,r
(
ci,j,r + 12sR (i, j, r)
) is a per-trigger
frame score with penalties c.
3.2 Model 2
Model 1 may still predict structures that cannot be
mapped to events. For example, in figure 1b) we
may label token 5 as Regulation, add the edge
5 Cause? 2 but fail to label token 2 as an event. While
consistent with (e,a) ? O, this violates the con-
straint that every active edge must either end at a
protein, or at an active event trigger. This is a re-
quirement on the label of a trigger and the assign-
ment of roles for its incoming edges.
Model 2 enforces the above constraint in addition
to (e,a) ? O, while inheriting the scoring function
fromModel 1. Hence, using I to denote the set of as-
signments with consistent trigger labels and incom-
ing edges, we get Y2 def= Y1 ? I and s2 (y) def= s1 (y).
3.2.1 Inference
Inference in Model 2 amounts to optimizing
s2 (e,a) over O ? I. This is more involved, as we
now have to ensure that when predicting an outgoing
edge from trigger i to trigger l there is a high-scoring
event at l. We follow Rush et al (2010) and solve
this problem in the framework of dual decomposi-
Algorithm 1 Sub-procedures for inference in Model
1, 2 and 3.
best label and outgoing edges for all triggers under penalties c
bestOut (c) :
?i y0 ? emptyOut (i)
y1 ? out (i, c, Treg,R
)
y2 ? out (i, c, T?reg,R \ {Cause}
)
yi ? argmaxy?{y0,y1,y2} sc1(i,y)
return (yi)i
best label and incoming edges for all triggers under penalties c
bestIn (c) :
?l y0 ? emptyIn (l)
y1 ? in (l, c, T ,R \ {None})
yl ? argmaxy?{y0,y1} sc2 (l,y)
return (yl)l
pick best binding pairs p, q and trigger i for each using penalties c
bestBind (c) :
?p, q bp,q ? [sB (p, q) + maxi ci,p,q > 0]
Ip,q ?
{
i|ci,p,q = maxi? ci?,p,q
}
if bp,q = 1 or maxi? ci?,p,q > 0
?i : ti,p,q ? [i ? Ip,q] |Ip,q|?1
else
?i : ti,p,q ? 0
return (b, t)
best label in T and outgoing edge roles in R for i, using penalties c
out (i, c, T,R) :
ei ? argmaxt?T 12sT (i, t) + ci,t
ai,bestTheme(i,c) ? Theme
?j ai,j ? argmaxr?R 12sR (i, j, r) + ci,j,rreturn (ei,ai)
best label in T , incoming edge roles in R
and outgoing protein roles, using costs c
in (l, c, T,R) :
el ? argmaxt?T 12sT (l, t) + cl,t
?i ai,l ? argmaxr?R 12sR (i, l, r) + ci,l,r
?p al,p ? argmaxr?R 12sR (l, p, r) + cl,p,rreturn (ei,ai)
best Theme argument for i
bestTheme (i, c) :
s (j) def= maxj,r 12sR (i, j, r) + ci,j,r
?(j) def= 12sR (i, j, Theme) + ci,j,Theme ? s (j)return argmaxj ?(j)
5
tion. To this end we write our optimization problem
as
maximize
e,a,e?,a?
1
2s2 (e,a) +
1
2s2 (e?, a?)
subject to (e,a) ? O ? (e?, a?) ? I?
e = e? ? a = a?
(M2)
and note that this problem could be solved separately
for e,a and e?, a? if the coupling constraints e = e?
and a = a? were removed.
M2 is an Integer Linear Program, as variables are
binary and both objective and constraints can be rep-
resented through linear constraints.1 Dual decompo-
sition solves a Linear Programming (LP) relaxation
of M2 (that allows fractional values for all binary
variables) through subgradient descent on a particu-
lar dual of M2. This dual can be derived by intro-
ducing Lagrange multipliers for the coupling con-
straints. Its attractiveness stems from the fact that
calculating the subgradient amounts to solving the
decoupled problems in isolation. If, by design, these
decoupled problems can be solved efficiently, we
can often quickly find the optimal solution to an LP
relaxation of our original problem.
Dual decomposition applied to Model 2 is shown
in algorithm 2. It maintains the dual variables ?
that will appear as local penalties in the subprob-
lems to be solved. The algorithm will try to tune
these variables such that at convergence the coupling
constraints will be fulfilled. This is done by first op-
timizing s2 (e,a) over O and s2 (e?, a?) over I. Now,
whenever there is disagreement between two vari-
ables to be coupled, the corresponding dual param-
eter is shifted, increasing the chance that next time
both models will agree. For example, if in the first
iteration we predict e6,Bind = 1 but e?6,Bind = 0, we
set ?6,Bind = ?? where ? is some stepsize (chosen
according to Koo et al (2010)). This will decrease
the coefficient for e6,Bind, and increase the coeffi-
cient for e?6,Bind. Hence, we have a higher chance of
agreement for this variable in the next iteration.
The algorithm repeats the process described
above until all variables agree, or some predefined
numberR of iterations is reached. In the former case
we in fact have the exact solution to the original ILP.
1The ILP representation could be taken from the MLNs of
Riedel et al (2009) and the mapping to ILPs of Riedel (2008).
Algorithm 2 Subgradient descent for Model 2, and
projected subgradient descent for Model 3.
require:
R: max. iteration, ?t: stepsizes
t? 0 [model 2,3] ?? 0 [model 2,3] ?? 0 [model 3]
repeat
model
2 (e,a)? bestOut (?)
2,3 (e?, a?)? bestIn (??)
3 (e,a)? bestOut (cout (?,?))
3 (b, t)? bestBind (cbind (?))
2,3 ?i,t ? ?i,t ? ?t (ei,t ? e?i,t)
2,3 ?i,j,r ? ?i,j,r ? ?t (ai,j,r ? a?i,j,r)
3 ?trigi,p,q ?
[
?trigi,p,q ? ?t (ei,Bind ? ti,p,q)
]
+
3 ?arg1i,j,k ?
[
?arg1i,p,q ? ?t (ai,p,Theme ? ti,p,q)
]
+
3 ?arg2i,p,q ?
[
?arg2i,p,q ? ?t (ai,q,Theme ? ti,p,q)
]
+
2,3 t ? t + 1
until no ?, ? changed or t > R
return (e,a)[model 2] or (e,a,b) [model 3]
In the later case we have no such guarantee, but find
that in practice the solutions are still of high qual-
ity. Notice that we could still assess the quality of
this approximation by measuring the duality gap be-
tween primal score and the final dual score.
Algorithm 2 for Model 2 requires us to opti-
mize s2 (e,a) over O and s2 (e?, a?) over I. The
former, with added penalties, can be done with
bestOut(c). As the constraint set for I again
decomposes on a per-token basis, solving the
latter problem requires a very similar procedure,
and again O (mn) time. Algorithm 1 shows this
procedure under bestIn(c). It chooses, for each
trigger candidate, the best label and incoming
set of arguments together with the best outgoing
edges to proteins. Adding edges to proteins is
not strictly required, but simplifies our exposition.
Algorithm bestIn(c) requires a per-trigger incoming
score: sc2 (l,yl) def=
?
t el,t
(
cl,t + 12sT (l, t)
)
+?
i,r ai,l,r
(
ci,l,r + 12sR (i, l, r)
)
+?
p,r al,p,r
(
cl,p,r + 12sR (l, p, r)
)
. Finally, note
that emptyIn (i) not only assigns None as trigger la-
bel of i and to all incoming edges, but also greedily
picks outgoing protein edges (as done within in(?)).
6
3.3 Model 3
Model 2 does not predict the bp,q variables that rep-
resent protein pairs p, q in bindings. Model 3 fixes
this by (a) adding binding variables bp,q into the ob-
jective, and (b) enforcing that the binding assign-
ment b is consistent with the trigger and argument
assignments e and a. We will also enforce that the
same pair of entities p, q cannot be arguments in
more than one event together.
The scoring function for Model 3 is simply
s3 (e,a,b) def= s2 (e,a,b) +
?
bp,q=1
sB (p, q) . (4)
Here sB (p, q) = ?wB, fB (p, q)? is a per-protein-pair
score based on a feature representation of the lexical
and syntactic relation between both protein heads.
Our strategy will be based on enforcing consis-
tency partly through linear constraints which we du-
alize, and partly within our search algorithm. To
this end we first introduce a set of auxiliary binary
variables ti,p,q . When a ti,p,q is active, we enforce
that there is a binding trigger at i with proteins p
and q as Theme arguments. A set of linear con-
straints can be used for this: ei,Bind ? ti,p,q ? 0,
ai,p,Theme ? ti,p,q ? 0 and ai,q,Theme ? ti,p,q ? 0 for
all suitable i, p and q. We denote the set of assign-
ments (e,a, t) that fulfill these constraints by T.
Consistency between e, a and b can now be en-
forced by making sure that t is consistent with e and
a, and that b is consistent with this t. The latter
means that an active bp,q requires a trigger i to point
to p and q. Or in other words, ti,p,q = 1 for exactly
one trigger i.
With the set of consistent assignments (b, t) re-
ferred to as B, and a slight abuse of notation, this
gives us Y3 def= Y2?T?B. Note that it is (e,a, t) ? T
that will be enforced by dualizing constraints, and
(b, t) ? B that will be enforced within search.
3.3.1 Inference
We note that inference in Model 3 can be per-
formed by solving the following problem:
maximize
e,a,e?,a?,b,t
1
2s1 (e,a) +
1
2s2 (e?, a?) +
?
bp,q=1
sB (p, q)
subject to (e,a) ? O ? (e?, a?) ? I ? (b, t) ? B?
e = e? ? a = a? ? (e,a, t) ? T.
(M3)
Again, without the final row, M3 would be separa-
ble. We exploit this by performing dual decompo-
sition with a dual objective that has multipliers ?
for the coupling constraints and multipliers? for the
constraints which enforce (e,a, t) ? T. The result-
ing subgradient descent method is also shown in al-
gorithm 2. Notably, since the constraints for T are
inequalities, we require a projected version of the
descent algorithm which enforces ? ? 0. This man-
ifests itself when ? is updated using the [?]+ projec-
tion.
We have already described how to find the best
e,a and e?, a? assignments. What changes for Model
3 is the derivation of the penalties for e and a
that now come from both ? and ?. We set
couti,t (?,?)
def= ?i,t + ?t,Bind
?
p,q ?
trig
i,p,q. For j /?
Prot (x) we set couti,j,r (?,?) def= ?i,j,r; otherwise we
use couti,j,r (?,?) def= ?i,j,r +
?
p ?
arg1
i,j,p +
?
q ?
arg2
i,q,j .
For finding a (b, t) ? B that maximizes?
bp,q=1 sB (p, q) we use bestBind (c), as shown inalgorithm 1. It groups together two proteins p, q if
their score plus the penalty of the best possible trig-
ger i exceeds 0. In this case, or if there is at least one
trigger with positive penalty ci,p,q > 0 , we activate
the set of triggers I (p, q) with maximal score.
Note that when several triggers i maximize the
score, we assign them all the same fractional value
|I (p, q)|?1. This enforces the constraint that at most
one binding event can point to both p and q and also
means that we are solving an LP relaxation. We
could enforce integer solutions and pick arbitrary
triggers at a tie, but this would lower the chances
of matching against predictions of other routines.
The penalties for bestBind (c) are derived from
the dual ? by setting cbindi,p,q (?) = ??trigi,p,q ? ?arg1i,p,q ?
?arg2i,,p,q.
3.4 Training
We choose prediction-based passive-aggressive (PA)
online learning (Crammer and Singer, 2003) with
averaging to estimate the weights w for each of our
models. PA is an error-driven learner that shifts
weights towards features of the gold solution, and
away from features of the current guess, whenever
the current model makes a mistake.
PA learning takes into account a user-defined
loss function for which we use a weighted sum
7
of false positives and false negatives: l (y,y?) def=
FP (y,y?) + ?FN (y,y?). We set ? = 3.8 by op-
timizing on the BioNLP 2009 development set.
4 Related Work
Riedel et al (2009) use Integer Linear Programming
and cutting planes (Riedel, 2008) for inference in
a model similar to Model 2. By using dual de-
composition instead, we can exploit tractable sub-
structure and achieve quadratic (Model 2) and cu-
bic (Model 3) runtime guarantees. An advantage of
ILP inference are guaranteed certificates of optimal-
ity. However, in practice we also gain certificates
of optimality for a large fraction of the instances
we process. Poon and Vanderwende (2010) use lo-
cal search and hence provide no such certificates.
Their problem formulation also makes n-gram de-
pendency path features harder to incorporate. Mc-
Closky et al (2011b) cast event extraction as depen-
dency parsing task. Their model assumes that event
structures are trees, an assumption that is frequently
violated in practice. Finally, all previous joint ap-
proaches use heuristics to decide whether binding
arguments are part of the same event, while we cap-
ture these decisions in the joint model.
We follow a long line of research in NLP that ad-
dresses search problems using (Integer) Linear Pro-
grams (Germann et al, 2001; Roth and Yih, 2004;
Riedel and Clarke, 2006). However, instead of us-
ing off-the-shelf solvers, we work in the framework
of dual decomposition. Here we extend the approach
of Rush et al (2010) in that in addition to equality
constraints we dualize more complex coupling con-
straints between models. This requires us to work
with a projected version of subgradient descent.
While tailored towards (biomedical) event extrac-
tion, we believe that our models can also be ef-
fective in a more general Semantic Role Label-
ing (SRL) context. Using variants of Model 1,
we can enforce many of the SRL constraints?such
as ?unique agent? constraints (Punyakanok et al,
2004)?without having to call out to ILP optimiz-
ers. Meza-Ruiz and Riedel (2009) showed that in-
ducing pressure on arguments to be attached to at
least one predicate is helpful; this is a soft incoming
edge constraint. Finally, Model 3 can be used to effi-
ciently capture compatibilities between semantic ar-
guments; such compatibilities have also been shown
to be helpful in SRL (Toutanova et al, 2005).
5 Experiments
We evaluate our models on several tracks of the 2009
and 2011 BioNLP shared tasks, using the official
?Approximate Span Matching/Approximate Recur-
sive Matching? F1 metric for each. We also investi-
gate the runtime behavior of our algorithms.
5.1 Preprocessing
Each document is first processed by the Stanford
CoreNLP2 tokenizer and sentence splitter. Parse
trees come from the Charniak-Johnson parser (Char-
niak and Johnson, 2005) with a self-trained biomed-
ical parsing model (McClosky and Charniak, 2008),
and are converted to dependency structures again us-
ing Stanford CoreNLP. Based on trigger words col-
lected from the training set, a set of candidate trigger
tokens Trig (x) is generated for each sentence x.
5.2 Features
The feature function fT (i, t) extracts a per-trigger
feature vector for trigger i and type t ? T .
It creates one active feature for each element in{
t, t ? TReg
}
? feats (i). Here feats (i) denotes a
collection of representations for the token i: word-
form, lemma, POS tag, syntactic heads, syntactic
children, and membership in two dictionaries taken
from Riedel et al (2009).
For fR (i, j, r) we create active features for each
element of {r} ? feats (i, j). Here feats (i, j) is
a collection of representations of the token pair
(i, j) taken from Miwa et al (2010c) and contains:
labelled and unlabeled n-gram dependency paths;
edge and vertex walk features, argument and trigger
modifiers and heads, words in between.
For fB (p, q) we re-use the token pair representa-
tions from fR. In particular, we create one active
feature for each element in feats (p, q).
5.3 Shared Task 2009
We first evaluate our models on the Bionlp 2009 task
1. The training, development and test sets for this
2http://nlp.stanford.edu/software/
corenlp.shtml
8
SVT BIND REG TOT
McClosky 75.4 48.4 40.4 53.5
Poon 77.5 47.9 44.1 55.5
Bjoerne 77.9 42.2 45.5 55.7
Miwa 78.6 46.9 47.7 57.8
M1 77.2 43.0 45.8 56.2
M2 77.9 42.4 47.6 57.2
M3 78.4 48.0 49.1 58.7
Table 1: F1 scores for the development set of Task 1 of
the BioNLP 2009 shared task.
task consist of 797, 150 and 250 documents, respec-
tively.
Table 1 shows our results for the development set.
We compare our three models (M1, M2 andM3) and
previous state-of-the-art systems: McClosky (Mc-
Closky et al, 2011a), Poon (Poon and Vander-
wende, 2010), Bjoerne (Bj?rne et al, 2009) and
Miwa (Miwa et al, 2010b; Miwa et al, 2010a). Pre-
sented is F1 score for all events (TOT), regulation
events (REG), binding events (BIND) and simple
events (SVT).
Model 1 is outperforming the previous best joint
models of Poon and Vanderwende (2010), as well as
the best entry of the 2009 task (Bj?rne et al, 2009).
This is achieved without careful tuning of thresh-
olds that control flow of information between trigger
and argument extraction. Notably, training Model 1
takes approximately 20 minutes using a single core
implementation. Contrast this with 20 minutes on 32
cores reported by Poon and Vanderwende (2010).
Model 2 focuses on regulation structures and re-
sults demonstrate this: F1 for regulations goes up by
nearly 2 points. While the impact of joint modeling
relative to weaker local baselines has been shown
shown by Poon and Vanderwende (2010) and Riedel
et al (2009), our findings here provide evidence that
it remains effective even when the baseline system
is very competitive.
With Model 3 our focus is extended to binding
events, improving F1 for such events by at least 5 F1.
This also has a positive effect on regulation events,
as regulations of binding events can now be more
accurately extracted. In total we see a 1.1 F1 in-
crease over the best results reported so far (Miwa et
al., 2010b). Crucially, this is achieved using only a
single parse tree per sentence, as opposed to three
SVT BIND REG TOT
McClosky 68.3 46.9 33.3 48.6
Poon 69.5 42.5 37.5 50.0
Bjoerne 70.2 44.4 40.1 52.0
Miwa 72.1 50.6 45.3 56.3
M1 71.0 42.1 41.9 53.4
M2 70.5 41.3 43.6 53.7
M3 71.1 52.9 45.2 55.8
M3+enju 72.6 52.6 46.9 57.4
Table 2: F1 scores for the test set of Task 1 of the BioNLP
2009 shared task.
used by Miwa et al (2010a).
Table 2 shows results for the test set. Here with
Model 1 we again already outperform all but the re-
sults of Miwa et al (2010a). Model 2 improves F1
for regulations, while Model 3 again increases F1
for both regulations and binding events. This yields
the best binding event results reported so far. No-
tably, not only are we able to resolve binding am-
biguity better. Binding attachments themselves also
improve, as we increase attachment F1 from 61.4 to
62.7 when going from Model 2 to Model 3.
Miwa et al (2010b) use two parsers to generate
their input features. For fairer comparison we aug-
ment Model 3 with syntactic features based on the
enju parser (Miyao et al, 2009). With these features
(M3+enju) we achieve the best results on this dataset
reported so far, and outperform Miwa et al (2010b)
by 1.1 F1 in total, 1.6 F1 on regulation events and
2.0 F1 on binding events.
We also apply Model 3, with slight modifications,
to the BioNLP 2009 task 2 which requires cellu-
lar locations to be extracted as well. With 53.0 F1
we fall 2 points short of the results of Miwa et al
(2010b) but still substantially outperform any other
reported results on the dataset. More parse trees may
again substantially improve results, as well as task-
specific constraint and feature sets.
5.4 Shared Task 2011
We entered the Shared Task 2011 with Model 3,
primarily focusing on Genia track (task 1), and the
Infectious Diseases track. The Genia track differs
from the 2009 task by including both abstracts and
full text articles. In total 908 training, 259 develop-
ment and 347 test documents are provided.
9
Genia Task 1 Infectious Diseases
System TOT System TOT
M3+Stanford 56.0 M3+Stanford 55.6
M3 55.2 M3 53.4
UTurku 53.3 Stanford 50.6
MSR-NLP 51.5 UTurku 44.2
ConcordU 50.3 PNNL 42.6
Table 3: F1 scores for the test sets of two tracks in the
BioNLP 2011 Shared Task.
The top five entries are shown in table 3. Model
3 is the best-performing system that does not use
model combination, only outperformed by a version
of Model 3 that includes Stanford predictions (Mc-
Closky et al, 2011b) as input features (Riedel et al,
2011). Not shown in the table are results for full pa-
pers only. Here M3 ranks first with 53.1 F1, while
M3+Stanford comes in second with 52.7 F1.
The Infectious Diseases (ID) track of the 2011
task has 152 train, 46 development and 118 test
documents. Relative to Genia it provides less data
and introduces more types of entities as well as
the biological process event type. Incorporating
these changes into our models is straightforward,
and hence we omit details for brevity.
Table 3 shows the top five entries for the Infec-
tious Diseases track. Again Model 3 is the best-
performing system that does not use model combi-
nation, outperformed only by Model 3 with Stanford
predictions as features. We should point out that
the feature sets and learning parameters were kept
constant when moving from Genia to ID data. The
strong results we observe without any tuning to the
domain indicate the robustness of joint modeling.
5.5 Runtime Behavior
Table 4 shows the asymptotic complexity of our
three models with respect to m = |Trig (x)|, n =
|Cand (x)| and p = |Prot (x)|. We also show the
number of iterations needed on average, the average
time in milliseconds per sentence,3 and the fraction
of sentences we get certificates of optimality for.
As expected, Model 1 is most efficient, both
asymptotically and on average. Given that its ac-
curacy is already good, it can serve as a basis for
3Measured without preprocessing and feature extraction.
Complexity Iter. Time Exact
M1 O (nm) 1.0 60ms 100%
M2 O (Rnm) 10.4 183ms 96%
M3 O (Rnm + Rp2m) 11.7 297ms 94%
Table 4: Complexity and Runtime Behavior.
large-scale extraction tasks. Models 2 and 3 re-
quire several iterations and more time, while pro-
viding slightly less certificates. However, given the
improvement in F1 they deliver, and the fact prepro-
cessing steps such as parsing would still dominate
the average time, this seems like a reasonable price
to pay.
6 Conclusion
We presented three joint models for biomedical
event extraction. Model 1 reaches near-state-of-the-
art results, outperforms all previous joint models
and has quadratic runtime guarantees. By explicitly
capturing regulation events (Model 2), and binding
events (Model 3) we achieve the best results reported
so far on several event extraction tasks. The runtime
penalty we pay is kept minimal by using dual de-
composition. We also show how dual decomposition
can be used for constraints that go beyond coupling
equalities.
We use joint models, a decomposition technique
and supervised online learning. This recipe can be
successful in many settings, but requires expensive
manual annotation. In the future we want to inte-
grate weak supervision techniques to train extractors
with existing biomedical databases, such as KEGG,
and only minimal amounts of annotated text.
Acknowledgements
This work was supported in part by the Center
for Intelligent Information Retrieval. The Univer-
sity of Massachusetts gratefully acknowledges the
support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0181. Any opinions, find-
ings, and conclusion or recommendations expressed
in this material are those of the authors and do not
necessarily reflect the view of the DARPA, AFRL,
or the US government.
10
References
Jari Bj?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings of the Natural Language
Processing in Biomedicine NAACL 2009 Workshop
(BioNLP ?09), pages 10?18, Morristown, NJ, USA.
Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL ?05),
pages 173?180.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951?991.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proceed-
ings of the 39th Annual Meeting of the Association for
Computational Linguistics (ACL ?01), pages 228?235.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of bionlp?09 shared task on event extraction. In
Proceedings of the Natural Language Processing in
Biomedicine NAACL 2009 Workshop (BioNLP ?09).
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011. Overview of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Nikos Komodakis, Nikos Paragios, and Georgios Tziri-
tas. 2007. Mrf optimization via dual decomposition:
Message-passing revisited. In Proceedings of the 11st
IEEE International Conference on Computer Vision
(ICCV ?07).
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with nonprojective head automata. In
Proceedings of the Conference on Empirical methods
in natural language processing (EMNLP ?10).
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of the
46th Annual Meeting of the Association for Computa-
tional Linguistics (ACL ?08).
David McClosky, Mihai Surdeanu, and Chris Manning.
2011a. Event extraction as dependency parsing. In
Proceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ?11), Port-
land, Oregon, June.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011b. Event extraction as dependency
parsing in bionlp 2011. In BioNLP 2011 Shared Task.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proceedings of the 11th Conference of the European
Chapter of the ACL (EACL ?06), pages 81?88.
Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly
identifying predicates, arguments and senses using
markov logic. In Joint Human Language Technol-
ogy Conference/Annual Meeting of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics (HLT-NAACL ?09).
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010a. A comparative study of syn-
tactic parsers for event extraction. In Proceedings of
the 2010 Workshop on Biomedical Natural Language
Processing, BioNLP ?10, pages 37?45, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010b. Evaluating dependency rep-
resentation for event extraction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics, COLING ?10, pages 779?787, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Makoto Miwa, Rune Saetre, Jin-Dong D. Kim, and
Jun?ichi Tsujii. 2010c. Event extraction with com-
plex event classification using rich features. Journal of
bioinformatics and computational biology, 8(1):131?
146, February.
Yusuke Miyao, Kenji Sagae, Rune S?tre, Takuya Mat-
suzaki, and Jun ichi Tsujii. 2009. Evaluating contribu-
tions of natural language parsers to protein-protein in-
teraction extraction. Bioinformatics/computer Appli-
cations in The Biosciences, 25:394?400.
Hoifung Poon and Lucy Vanderwende. 2010. Joint Infer-
ence for Knowledge Extraction from Biomedical Lit-
erature. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
813?821, Los Angeles, California, June. Association
for Computational Linguistics.
Vasin Punyakanok, Dan Roth, Wen tau Yih, and Dav Zi-
mak. 2004. Semantic role labeling via integer linear
programming inference. In Proceedings of the 20th in-
ternational conference on Computational Linguistics
(COLING ?04), pages 1346?1352, Morristown, NJ,
USA. Association for Computational Linguistics.
Sebastian Riedel and James Clarke. 2006. Incremen-
tal integer linear programming for non-projective de-
pendency parsing. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ?06), pages 129?137.
Sebastian Riedel and Andrew McCallum. 2011. Robust
biomedical event extraction with dual decomposition
and minimal domain adaptation. In Proceedings of the
11
Natural Language Processing in Biomedicine NAACL
2011 Workshop (BioNLP ?11), June.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A markov logic approach to
bio-molecular event extraction. In Proceedings of the
Natural Language Processing in Biomedicine NAACL
2009 Workshop (BioNLP ?09), pages 41?49.
Sebastian Riedel, David McClosky, Mihai Surdeanu,
Christopher D. Manning, and Andrew McCallum.
2011. Model combination for event extraction in
BioNLP 2011. In Proceedings of the Natural Lan-
guage Processing in Biomedicine NAACL 2011 Work-
shop (BioNLP ?11), June.
Sebastian Riedel. 2008. Improving the accuracy and ef-
ficiency of MAP inference for markov logic. In Pro-
ceedings of the 24th Annual Conference on Uncer-
tainty in AI (UAI ?08), pages 468?475.
D. Roth andW. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Proceedings of the 8th Conference on Computational
Natural Language Learning (CoNLL? 04), pages 1?8.
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition
and linear programming relaxations for natural lan-
guage processing. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ?10).
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic role
labeling. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics (ACL
?05), pages 589?596, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
12
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 262?272,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Optimizing Semantic Coherence in Topic Models
David Mimno
Princeton University
Princeton, NJ 08540
mimno@cs.princeton.edu
Hanna M. Wallach
University of Massachusetts, Amherst
Amherst, MA 01003
wallach@cs.umass.edu
Edmund Talley Miriam Leenders
National Institutes of Health
Bethesda, MD 20892
{talleye,leenderm}@ninds.nih.gov
Andrew McCallum
University of Massachusetts, Amherst
Amherst, MA 01003
mccallum@cs.umass.edu
Abstract
Latent variable models have the potential
to add value to large document collections
by discovering interpretable, low-dimensional
subspaces. In order for people to use such
models, however, they must trust them. Un-
fortunately, typical dimensionality reduction
methods for text, such as latent Dirichlet al
location, often produce low-dimensional sub-
spaces (topics) that are obviously flawed to
human domain experts. The contributions of
this paper are threefold: (1) An analysis of the
ways in which topics can be flawed; (2) an au-
tomated evaluation metric for identifying such
topics that does not rely on human annotators
or reference collections outside the training
data; (3) a novel statistical topic model based
on this metric that significantly improves topic
quality in a large-scale document collection
from the National Institutes of Health (NIH).
1 Introduction
Statistical topic models such as latent Dirichlet al
location (LDA) (Blei et al, 2003) provide a pow-
erful framework for representing and summarizing
the contents of large document collections. In our
experience, however, the primary obstacle to accep-
tance of statistical topic models by users the outside
machine learning community is the presence of poor
quality topics. Topics that mix unrelated or loosely-
related concepts substantially reduce users? confi-
dence in the utility of such automated systems.
In general, users prefer models with larger num-
bers of topics because such models have greater res-
olution and are able to support finer-grained distinc-
tions. Unfortunately, we have observed that there
is a strong relationship between the size of topics
and the probability of topics being nonsensical as
judged by domain experts: as the number of topics
increases, the smallest topics (number of word to-
kens assigned to each topic) are almost always poor
quality. The common practice of displaying only a
small number of example topics hides the fact that as
many as 10% of topics may be so bad that they can-
not be shown without reducing users? confidence.
The evaluation of statistical topic models has tra-
ditionally been dominated by either extrinsic meth-
ods (i.e., using the inferred topics to perform some
external task such as information retrieval (Wei
and Croft, 2006)) or quantitative intrinsic methods,
such as computing the probability of held-out doc-
uments (Wallach et al, 2009). Recent work has
focused on evaluation of topics as semantically-
coherent concepts. For example, Chang et al (2009)
found that the probability of held-out documents is
not always a good predictor of human judgments.
Newman et al (2010) showed that an automated
evaluation metric based on word co-occurrence
statistics gathered from Wikipedia could predict hu-
man evaluations of topic quality. AlSumait et al
(2009) used differences between topic-specific dis-
tributions over words and the corpus-wide distribu-
tion over words to identify overly-general ?vacuous?
topics. Finally, Andrzejewski et al (2009) devel-
oped semi-supervised methods that avoid specific
user-labeled semantic coherence problems.
The contributions of this paper are threefold: (1)
To identify distinct classes of low-quality topics,
some of which are not flagged by existing evalua-
tion methods; (2) to introduce a new topic ?coher-
ence? score that corresponds well with human co-
herence judgments and makes it possible to identify
262
specific semantic problems in topic models without
human evaluations or external reference corpora; (3)
to present an example of a new topic model that
learns latent topics by directly optimizing a metric
of topic coherence. With little additional computa-
tional cost beyond that of LDA, this model exhibits
significant gains in average topic coherence score.
Although the model does not result in a statistically-
significant reduction in the number of topics marked
?bad?, the model consistently improves the topic co-
herence score of the ten lowest-scoring topics (i.e.,
results in bad topics that are ?less bad? than those
found using LDA) while retaining the ability to iden-
tify low-quality topics without human interaction.
2 Latent Dirichlet Allocation
LDA is a generative probabilistic model for docu-
mentsW = {w(1),w(2), . . . ,w(D)}. To generate a
word token w(d)n in document d, we draw a discrete
topic assignment z(d)n from a document-specific dis-
tribution over the T topics ?d (which is itself drawn
from a Dirichlet prior with hyperparameter ?), and
then draw a word type for that token from the topic-
specific distribution over the vocabulary ?z(d)n . Theinference task in topic models is generally cast as in-
ferring the document?topic proportions {?1, ...,?D}
and the topic-specific distributions {?1 . . . ,?T }.
The multinomial topic distributions are usually
drawn from a shared symmetric Dirichlet prior with
hyperparameter ?, such that conditioned on {?t}Tt=1
and the topic assignments {z(1), z(2), . . . ,z(D)},
the word tokens are independent. In practice, how-
ever, it is common to deal directly with the ?col-
lapsed? distributions that result from integrating
over the topic-specific multinomial parameters. The
resulting distribution over words for a topic t is then
a function of the hyperparameter ? and the number
of words of each type assigned to that topic, Nw|t.
This distribution, known as the Dirichlet compound
multinomial (DCM) or Po?lya distribution (Doyle
and Elkan, 2009), breaks the assumption of condi-
tional independence between word tokens given top-
ics, but is useful during inference because the con-
ditional probability of a word w given topic t takes
a very simple form: P (w | t, ?) = Nw|t+?Nt+|V|? , where
Nt =
?
w? Nw?|t and |V| is the vocabulary size.
The process for generating a sequence of words
from such a model is known as the simple Po?lya urn
model (Mahmoud, 2008), in which the initial prob-
ability of word type w in topic t is proportional to
?, while the probability of each subsequent occur-
rence of w in topic t is proportional to the number
of times w has been drawn in that topic plus ?. Note
that this unnormalized weight for each word type de-
pends only on the count of that word type, and is in-
dependent of the count of any other word type w?.
Thus, in the DCM/Po?lya distribution, drawing word
type w must decrease the probability of seeing all
other word types w? 6= w. In a later section, we will
introduce a topic model that substitutes a general-
ized Po?lya urn model for the DCM/Po?lya distribu-
tion, allowing a draw of word type w to increase the
probability of seeing certain other word types.
For real-world data, documents W are observed,
while the corresponding topic assignments Z are
unobserved and may be inferred using either vari-
ational methods (Blei et al, 2003; Teh et al, 2006)
or MCMC methods (Griffiths and Steyvers, 2004).
Here, we use MCMC methods?specifically Gibbs
sampling (Geman and Geman, 1984), which in-
volves sequentially resampling each topic assign-
ment z(d)n from its conditional posterior given the
documents W , the hyperparameters ? and ?, and
Z\d,n (the current topic assignments for all tokens
other than the token at position n in document d).
3 Expert Opinions of Topic Quality
Concentrating on 300,000 grant and related jour-
nal paper abstracts from the National Institutes of
Health (NIH), we worked with two experts from
the National Institute of Neurological Disorders and
Stroke (NINDS) to collaboratively design an expert-
driven topic annotation study. The goal of this study
was to develop an annotated set of baseline topics,
along with their salient characteristics, as a first step
towards automatically identifying and inferring the
kinds of topics desired by domain experts.1
3.1 Expert-Driven Annotation Protocol
In order to ensure that the topics selected for anno-
tation were within the NINDS experts? area of ex-
pertise, they selected 148 topics (out of 500), all as-
sociated with areas funded by NINDS. Each topic
1All evaluated models will be released publicly.
263
t was presented to the experts as a list of the thirty
most probable words for that topic, in descending or-
der of their topic-specific ?collapsed? probabilities,
Nw|t+?
Nt+|V|? . In addition to the most probable words,the experts were also given metadata for each topic:
The most common sequences of two or more con-
secutive words assigned to that topic, the four topics
that most often co-occurred with that topic, the most
common IDF-weighted words from titles of grants,
thesaurus terms, NIH institutes, journal titles, and
finally a list of the highest probability grants and
PubMed papers for that topic.
The experts first categorized each topic as one
of three types: ?research?, ?grant mechanisms and
publication types? or ?general?.2 The quality of
each topic (?good?, ?intermediate?, or ?bad?) was
then evaluated using criteria specific to the type
of topic. In general, topics were only annotated
as ?good? if they contained words that could be
grouped together as a single coherent concept. Addi-
tionally, each ?research? topic was only considered
to be ?good? if, in addition to representing a sin-
gle coherent concept, the aggregate content of the
set of documents with appreciable allocations to that
topic clearly contained text referring to the concept
inferred from the topic words. Finally, for each topic
marked as being either ?intermediate? or ?bad?, one
or more of the following problems (defined by the
domain experts) was identified, as appropriate:
? Chained: every word is connected to every
other word through some pairwise word chain,
but not all word pairs make sense. For exam-
ple, a topic whose top three words are ?acids?,
?fatty? and ?nucleic? consists of two distinct
concepts (i.e., acids produced when fats are
broken down versus the building blocks of
DNA and RNA) chained via the word ?acids?.
? Intruded: either two or more unrelated sets
of related words, joined arbitrarily, or an oth-
erwise good topic with a few ?intruder? words.
? Random: no clear, sensical connections be-
tween more than a few pairs of words.
? Unbalanced: the top words are all logically
connected to each other, but the topic combines
very general and specific terms (e.g., ?signal
2Equivalent to ?vacuous topics? of AlSumait et al (2009).
transduction? versus ?notch signaling?).
Examples of a good general topic, a good research
topic, and a chained research topic are in Table 1.
3.2 Annotation Results
The experts annotated the topics independently and
then aggregated their results. Interestingly, no top-
ics were ever considered ?good? by one expert and
?bad? by the other?when there was disagreement
between the experts, one expert always believed the
topic to be ?intermediate.? In such cases, the ex-
perts discussed the reasons for their decisions and
came to a consensus. Of the 148 topics selected for
annotation, 90 were labeled as ?good,? 21 as ?inter-
mediate,? and 37 as ?bad.? Of the topics labeled as
?bad? or ?intermediate,? 23 were ?chained,? 21 were
?intruded,? 3 were ?random,? and 15 were ?unbal-
anced?. (The annotators were permitted to assign
more than one problem to any given topic.)
4 Automated Metrics for Predicting
Expert Annotations
The ultimate goal of this paper is to develop meth-
ods for building models with large numbers of spe-
cific, high-quality topics from domain-specific cor-
pora. We therefore explore the extent to which in-
formation already contained in the documents being
modeled can be used to assess topic quality.
In this section we evaluate several methods for
ranking the quality of topics and compare these
rankings to human annotations. No method is likely
to perfectly predict human judgments, as individual
annotators may disagree on particular topics. For
an application involving removing low quality top-
ics we recommend using a weighted combination of
metrics, with a threshold determined by users.
4.1 Topic Size
As a simple baseline, we considered the extent to
which topic ?size? (as measured by the number of
tokens assigned to each topic via Gibbs sampling) is
a good metric for assessing topic quality. Figure 1
(top) displays the topic size (number of tokens as-
signed to that topic) and expert annotations (?good?,
?intermediate?, ?bad?) for the 148 topics manually
labeled by annotators as described above. This fig-
ure suggests that topic size is a reasonable predic-
264
lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll
40000 60000 80000 120000 160000
Tokens
goo
d
inte
r
bad
lllllllllllllllllllllllllllllllllllllllllllllllllll
?600 ?500 ?400 ?300 ?200
Coherence
goo
d
inte
r
bad
Figure 1: Topic size is a good indicator of quality; the
new coherence metric is better. Top shows expert-rated
topics ranked by topic size (AP 0.89, AUC 0.79), bottom
shows same topics ranked by coherence (AP 0.94, AUC
0.87). Random jitter is added to the y-axis for clarity.
tor of topic quality. Although there is some overlap,
?bad? topics are generally smaller than ?good? top-
ics. Unfortunately, this observation conflicts with
the goal of building highly specialized, domain-
specific topic models with many high-quality, fine-
grained topics?in such models the majority of top-
ics will have relatively few tokens assigned to them.
4.2 Topic Coherence
When displaying topics to users, each topic t is gen-
erally represented as a list of theM=5, . . . , 20 most
probable words for that topic, in descending order
of their topic-specific ?collapsed? probabilities. Al-
though there has been previous work on automated
generation of labels or headings for topics (Mei et
al., 2007), we choose to work only with the ordered
list representation. Labels may obscure or detract
from fundamental problems with topic coherence,
and better labels don?t make bad topics good.
The expert-driven annotation study described in
section 3 suggests that three of the four types of
poor-quality topics (?chained,? ?intruded? and ?ran-
dom?) could be detected using a metric based on
the co-occurrence of words within the documents
being modeled. For ?chained? and ?intruded? top-
ics, it is likely that although pairs of words belong-
ing to a single concept will co-occur within a single
document (e.g., ?nucleic? and ?acids? in documents
about DNA), word pairs belonging to different con-
cepts (e.g., ?fatty? and ?nucleic?) will not. For ran-
dom topics, it is likely that few words will co-occur.
This insight can be used to design a new metric
for assessing topic quality. Letting D(v) be the doc-
ument frequency of word type v (i.e., the number
of documents with least one token of type v) and
D(v, v?) be co-document frequency of word types v
and v? (i.e., the number of documents containing one
or more tokens of type v and at least one token of
type v?), we define topic coherence as
C(t;V (t)) =
M?
m=2
m?1?
l=1
log D(v
(t)
m , v(t)l ) + 1
D(v(t)l )
, (1)
where V (t) =(v(t)1 , . . . , v(t)M ) is a list of the M most
probable words in topic t. A smoothing count of 1
is included to avoid taking the logarithm of zero.
Figure 1 shows the association between the expert
annotations and both topic size (top) and our coher-
ence metric (bottom). We evaluate these results us-
ing standard ranking metrics, average precision and
the area under the ROC curve. Treating ?good? top-
ics as positive and ?intermediate? or ?bad? topics as
negative, we get average precision values of 0.89 for
topic size vs. 0.94 for coherence and AUC 0.79 for
topic size vs. 0.87 for coherence. We performed a
logistic regression analysis on the binary variable ?is
this topic bad?. Using topic size alone as a predic-
tor gives AIC (a measure of model fit) 152.5. Co-
herence alone has AIC 113.8 (substantially better).
Both predictors combined have AIC 115.8: the sim-
pler coherence alone model provides the best perfor-
mance. We tried weighting the terms in equation 1
by their corresponding topic?word probabilities and
and by their position in the sorted list of the M most
probable words for that topic, but we found that a
uniform weighting better predicted topic quality.
Our topic coherence metric also exhibits good
qualitative behavior: of the 20 best-scoring topics,
18 are labeled as ?good,? one is ?intermediate? (?un-
balanced?), and one is ?bad? (combining ?cortex?
and ?fmri?, words that commonly co-occur, but are
conceptually distinct). Of the 20 worst scoring top-
ics, 15 are ?bad,? 4 are ?intermediate,? and only one
(with the 19th worst coherence score) is ?good.?
265
Our coherence metric relies only upon word co-
occurrence statistics gathered from the corpus being
modeled, and does not depend on an external ref-
erence corpus. Ideally, all such co-occurrence infor-
mation would already be accounted for in the model.
We believe that one of the main contributions of our
work is demonstrating that standard topic models
do not fully utilize available co-occurrence informa-
tion, and that a held-out reference corpus is therefore
not required for purposes of topic evaluation.
Equation 1 is very similar to pointwise mutual in-
formation (PMI), but is more closely associated with
our expert annotations than PMI (which achieves
AUC 0.64 and AIC 170.51). PMI has a long history
in language technology (Church and Hanks, 1990),
and was recently used by Newman et al (2010) to
evaluate topic models. When expressed in terms of
count variables as in equation 1, PMI includes an
additional term for D(v(t)m ). The improved perfor-
mance of our metric over PMI implies that what mat-
ters is not the difference between the joint probabil-
ity of words m and l and the product of marginals,
but the conditional probability of each word given
the each of the higher-ranked words in the topic.
In order to provide intuition for the behavior of
our topic coherence metric, table 1 shows three
example topics and their topic coherence scores.
The first topic, related to grant-funded training pro-
grams, is one of the best-scoring topics. All pairs
of words have high co-document frequencies. The
second topic, on neurons, is more typical of qual-
ity ?research? topics. Overall, these words occur
less frequently, but generally occur moderately in-
terchangeably: there is little structure to their co-
variance. The last topic is one of the lowest-scoring
topics. Its co-document frequency matrix is shown
in table 2. The top two words are closely related:
487 documents include ?aging? at least once, 122
include ?lifespan?, and 55 include both. Meanwhile,
the third word ?globin? occurs with only one of the
top seven words?the common word ?human?.
4.3 Comparison to word intrusion
As an additional check for both our expert annota-
tions and our automated metric, we replicated the
?word intrusion? evaluation originally introduced by
Chang et al (2009). In this task, one of the top ten
most probable words in a topic is replaced with a
l lll l lll ll l lll ll l lll lll l lll ll lll l l ll l llll
l
ll l lll l lll ll l lll
l
l ll ll lll ll
l
l l
l
l
ll l
40000 60000 80000 120000 160000
0
4
8
Comparison of Topic Size to Intrusion Detection
Tokens assigned to topic
Cor
rect
 Gu
ess
es
ll ll ll ll l ll ll ll l ll ll llll l lll llll ll ll ll lll l
l
l lll lllll ll l ll
l
lll ll ll ll
l
l l
l
l
ll l
?600 ?500 ?400 ?300 ?200
0
4
8
Comparison of Coherence to Intrusion Detection
Coherence
Cor
rect
 Gu
ess
es
Good Topics
Correct Guesses
Fre
que
ncy
0 2 4 6 8 10
0
15
35
Bad Topics
Correct Guesses
Fre
que
ncy
0 2 4 6 8 10
0
15
35
Figure 2: Top: results of the intruder selection task rel-
ative to two topic quality metrics. Bottom: marginal in-
truder accuracy frequencies of good and bad topics.
another word, selected at random from the corpus.
The resulting set of words is presented, in a random
order, to users, who are asked to identify the ?in-
truder? word. It is very unlikely that a randomly-
chosen word will be semantically related to any of
the original words in the topic, so if a topic is a
high quality representation of a semantically coher-
ent concept, it should be easy for users to select the
intruder word. If the topic is not coherent, there may
be words in the topic that are also not semantically
related to any other word, thus causing users to se-
lect ?correct? words instead of the real intruder.
We recruited ten additional expert annotators
from NINDS, not including our original annotators,
and presented them with the intruder selection task,
using the set of previously evaluated topics. Re-
sults are shown in figure 2. In the first two plots,
the x-axis is one of our two automated quality met-
266
Table 1: Example topics (good/general, good/research, chained/research) with different coherence scores (numbers
closer to zero indicate higher coherence). The chained topic combines words related to aging (indicated in plain text)
and words describing blood and blood-related diseases (bold). The only connection is the common word human.
-167.1 students, program, summer, biomedical, training, experience, undergraduate, career, minority, student, ca-
reers, underrepresented, medical students, week, science
-252.1 neurons, neuronal, brain, axon, neuron, guidance, nervous system, cns, axons, neural, axonal, cortical,
survival, disorders, motor
-357.2 aging, lifespan, globin, age related, longevity, human, age, erythroid, sickle cell, beta globin, hb, senes-
cence, adult, older, lcr
Table 2: Co-document frequency matrix for the top words in a low-quality topic (according to our coherence metric),
shaded to highlight zeros. The diagonal (light gray) shows the overall document frequency for each word w. The
column on the right is Nw|t. Note that ?globin? and ?erythroid? do not co-occur with any of the aging-related words.
aging 487 53 0 65 42 0 51 0 138 0 914
lifespan 53 122 0 15 28 0 15 0 44 0 205
globin 0 0 39 0 0 19 0 15 27 3 200
age related 65 15 0 119 12 0 25 0 37 0 160
longevity 42 28 0 12 73 0 6 0 20 1 159
erythroid 0 0 19 0 0 69 0 8 23 1 110
age 51 15 0 25 6 0 245 1 82 0 103
sickle cell 0 0 15 0 0 8 1 43 16 2 93
human 138 44 27 37 20 23 82 16 4347 157 91
hb 0 0 3 0 1 1 0 2 5 15 73
267
rics (topic size and coherence) and the y-axis is the
number of annotators that correctly identified the
true intruder word (accuracy). The histograms be-
low these plots show the number of topics with each
level of annotator accuracy for good and bad top-
ics. For good topics (green circles), the annotators
were generally able to detect the intruder word with
high accuracy. Bad topics (red diamonds) had more
uniform accuracies. These results suggest that top-
ics with low intruder detection accuracy tend to be
bad, but some bad topics can have a high accuracy.
For example, spotting an intruder word in a chained
topic can be easy. The low-quality topic recep-
tors, cannabinoid, cannabinoids, ligands, cannabis,
endocannabinoid, cxcr4, [virus], receptor, sdf1, is
a typical ?chained? topic, with CXCR4 linked to
cannabinoids only through receptors, and otherwise
unrelated. Eight out of ten annotators correctly iden-
tified ?virus? as the correct intruder. Repeating the
logistic regression experiment using intruder detec-
tion accuracy as input, the AIC value is 163.18?
much worse than either topic size or coherence.
5 Generalized Po?lya Urn Models
Although the topic coherence metric defined above
provides an accurate way of assessing topic quality,
preventing poor quality topics from occurring in the
first place is preferable. Our results in the previous
section show that we can identify low-quality top-
ics without making use of external supervision; the
training data by itself contains sufficient information
at least to reject poor combinations of words.
In this section, we describe a new topic model that
incorporates the corpus-specific word co-occurrence
information used in our coherence metric directly
into the statistical topic modeling framework. It
is important to note that simply disallowing words
that never co-occur from being assigned to the same
topic is not sufficient. Due to the power-law charac-
teristics of language, most words are rare and will
not co-occur with most other words regardless of
their semantic similarity. It is rather the degree
to which the most prominent words in a topic do
not co-occur with the other most prominent words
in that topic that is an indicator of topic incoher-
ence. We therefore desire models that guide topics
towards semantic similarity without imposing hard
constraints.
As an example of such a model, we present a new
topic model in which the occurrence of word type w
in topic t increases not only the probability of seeing
that word type again, but also increases the probabil-
ity of seeing other related words (as determined by
co-document frequencies for the corpus being mod-
eled). This new topic model retains the document?
topic component of standard LDA, but replaces the
usual Po?lya urn topic?word component with a gen-
eralized Po?lya urn framework (Mahmoud, 2008).
A sequence of i.i.d. samples from a discrete dis-
tribution can be imagined as arising by repeatedly
drawing a random ball from an urn, where the num-
ber of balls of each color is proportional to the prob-
ability of that color, replacing the selected ball af-
ter each draw. In a Po?lya urn, each ball is replaced
along with another ball of the same color. Samples
from this model exhibit the ?burstiness? property:
the probability of drawing a ball of colorw increases
each time a ball of that color is drawn. This process
represents the marginal distribution of a hierarchical
model with a Dirichlet prior and a multinomial like-
lihood, and is used as the distribution over words
for each topic in almost all previous topic models.
In a generalized Po?lya urn model, having drawn a
ball of color w, Avw additional balls of each color
v ? {1, . . . ,W} are returned to the urn. Given W
and Z , the conditional posterior probability of word
w in topic t implied by this generalized model is
P (w | t,W,Z, ?,A) =
?
vNv|tAvw + ?
Nt + |V|?
, (2)
where A is a W ? W real-valued matrix, known
as the addition matrix or schema. The simple Po?lya
urn model (and hence the conditional posterior prob-
ability of word w in topic t under LDA) can be re-
covered by setting the schema A to the identity ma-
trix. Unlike the simple Po?lya distribution, we do not
know of a representation of the generalized Po?lya
urn distribution that can be expressed using a con-
cise set of conditional independence assumptions. A
standard graphical model with plate notation would
therefore not be helpful in highlighting the differ-
ences between the two models, and is not shown.
Algorithm 1 shows pseudocode for a single Gibbs
sweep over the latent variables Z in standard LDA.
Algorithm 2 shows the modifications necessary to
268
1: for d ? D do
2: for wn ? w(d) do
3: Nzi|di ? Nzi|di ? 1
4: Nwi|zi ? Nwi|zi ? 1
5: sample zi ? (Nz|di + ?z)
Nwi|z+??
z? (Nwi|z?+?)6: Nzi|di ? Nzi|di + 1
7: Nwi|zi ? Nwi|zi + 1
8: end for
9: end for
Algorithm 1: One sweep of LDA Gibbs sampling.
1: for d ? D do
2: for wn ? w(d) do
3: Nzi|di ? Nzi|di ? 1
4: for all v do
5: Nv|zi ? Nv|zi ?Avwi
6: end for
7: sample zi ? (Nz|di + ?z)
Nwi|z+??
z? (Nwi|z?+?)8: Nzi|di ? Nzi|di + 1
9: for all v do
10: Nv|zi ? Nv|zi +Avwi
11: end for
12: end for
13: end for
Algorithm 2: One sweep of gen. Po?lya Gibbs sam-
pling, with differences from LDA highlighted in red.
support a generalized Po?lya urn model: rather than
subtracting exactly one from the count of the word
given the old topic, sampling, and then adding one
to the count of the word given the new topic, we sub-
tract a column of the schema matrix from the entire
count vector over words for the old topic, sample,
and add the same column to the count vector for the
new topic. As long as A is sparse, this operation
adds only a constant factor to the computation.
Another property of the generalized Po?lya urn
model is that it is nonexchangeable?the joint prob-
ability of the tokens in any given topic is not invari-
ant to permutation of those tokens. Inference of Z
givenW via Gibbs sampling involves repeatedly cy-
cling through the tokens in W and, for each one,
resampling its topic assignment conditioned on W
and the current topic assignments for all tokens other
than the token of interest. For LDA, the sampling
distribution for each topic assignment is simply the
product of two predictive probabilities, obtained by
treating the token of interest as if it were the last.
For a topic model with a generalized Po?lya urn for
the topic?word component, the sampling distribu-
tion is more complicated. Specifically, the topic?
word component of the sampling distribution is no
longer a simple predictive distribution?when sam-
pling a new value for z(d)n , the implication of each
possible value for subsequent tokens and their topic
assignments must be considered. Unfortunately, this
can be very computationally expensive, particularly
for large corpora. There are several ways around this
problem. The first is to use sequential Monte Carlo
methods, which have been successfully applied to
topic models previously (Canini et al, 2009). The
second approach is to approximate the true Gibbs
sampling distribution by treating each token as if it
were the last, ignoring implications for subsequent
tokens and their topic assignments. We find that
this approximate method performs well empirically.
5.1 Setting the Schema A
Inspired by our evaluation metric, we define A as
Avv ? ?vD(v) (3)
Avw ? ?vD(w, v)
where each element is scaled by a row-specific
weight ?v and each column is normalized to sum
to 1. Normalizing columns makes comparison to
standard LDA simpler, because the relative effect of
smoothing parameter ?=0.01 is equivalent. We set
?v = log (D/D(v)), the standard IDF weight used
in information retrieval, which is larger for less fre-
quent words. The column for word type w can be
interpreted as word types with significant associa-
tion with w. The IDF weighting therefore has the
effect of increasing the strength of association for
rare word types. We also found empirically that it is
helpful to remove off-diagonal elements for the most
common types, such as those that occur in more than
5% of documents (IDF < 3.0). Including nonzero
off-diagonal values in A for very frequent types
causes the model to disperse those types over many
topics, which leads to large numbers of extremely
similar topics. To measure this effect, we calcu-
lated the Jensen-Shannon divergence between all
pairs of topic?word distributions in a given model.
For a model using off-diagonal weights for all word
269
?
29
0
?
26
0
100 Topics
Co
he
ren
ce
50 300 550 800
?
29
0
?
26
0
200 Topics
Co
he
ren
ce
50 300 550 800
?
29
0
?
26
0
300 Topics
Co
he
ren
ce
50 300 550 800
?
29
0
?
26
0
400 Topics
Co
he
ren
ce
50 300 550 800
?
40
0
?
34
0
10
 W
or
st 
Co
he
r
50 300 550 800
?
40
0
?
34
0
10
 W
or
st 
Co
he
r
50 300 550 800
?
40
0
?
34
0
10
 W
or
st 
Co
he
r
50 300 550 800
?
40
0
?
34
0
10
 W
or
st 
Co
he
r
50 300 550 800
?
17
00
?
16
60
Iteration
HO
LP
50 300 550 800
?
17
00
?
16
60
Iteration
HO
LP
50 300 550 800
?
17
00
?
16
60
Iteration
HO
LP
50 300 550 800
?
17
00
?
16
60
Iteration
HO
LP
50 300 550 800
Figure 3: Po?lya urn topics (blue) have higher average coherence and converge much faster than LDA topics
(red). The top plots show topic coherence (averaged over 15 runs) over 1000 iterations of Gibbs sampling. Error bars
are not visible in this plot. The middle plot shows the average coherence of the 10 lowest scoring topics. The bottom
plots show held-out log probability (in thousands) for the same models (three runs each of 5-fold cross-validation).
Name Docs Avg. Tok. Tokens Vocab
NIH 18756 114.64 ? 30.41 2150172 28702
Table 3: Data set statistics.
types, the mean of the 100 lowest divergences was
0.29 ? .05 (a divergence of 1.0 represents distribu-
tions with no shared support) at T =200. The aver-
age divergence of the 100 most similar pairs of top-
ics for standard LDA (i.e.,A = I) is 0.67?.05. The
same statistic for the generalized Po?lya urn model
without off-diagonal elements for word types with
high document frequency is 0.822? 0.09.
Setting the off-diagonal elements of the schema
A to zero for the most common word types also has
the fortunate effect of substantially reducing prepro-
cessing time. We find that Gibbs sampling for the
generalized Po?lya model takes roughly two to three
times longer than for standard LDA, depending on
the sparsity of the schema, due to additional book-
keeping needed before and after sampling topics.
5.2 Experimental Results
We evaluated the new model on a corpus of NIH
grant abstracts. Details are given in table 3. Figure 3
shows the performance of the generalized Po?lya urn
model relative to LDA. Two metrics?our new topic
coherence metric and the log probability of held-out
documents?are shown over 1000 iterations at 50 it-
eration intervals. Each model was run over five folds
of cross validation, each with three random initial-
izations. For each model we calculated an overall
coherence score by calculating the topic coherence
for each topic individually and then averaging these
values. We report the average over all 15 models in
each plot. Held-out probabilities were calculated us-
ing the left-to-right method of Wallach et al (2009),
with each cross-validation fold using its own schema
A. The generalized Po?lya model performs very well
in average topic coherence, reaching levels within
the first 50 iterations that match the final score. This
model has an early advantage for held-out proba-
bility as well, but is eventually overtaken by LDA.
This trend is consistent with Chang et al?s observa-
tion that held-out probabilities are not always good
predictors of human judgments (Chang et al, 2009).
Results are consistent over T ? {100, 200, 300}.
In section 4.2, we demonstrated that our topic co-
herence metric correlates with expert opinions of
topic quality for standard LDA. The generalized
270
Po?lya urn model was therefore designed with the
goal of directly optimizing that metric. It is pos-
sible, however, that optimizing for coherence di-
rectly could break the association between coher-
ence metric and topic quality. We therefore repeated
the expert-driven evaluation protocol described in
section 3.1. We trained one standard LDA model
and one generalized Po?lya urn model, each with
T = 200, and randomly shuffled the 400 resulting
topics. The topics were then presented to the experts
from NINDS, with no indication as to the identity of
the model from which each topic came. As these
evaluations are time consuming, the experts evalu-
ated the only the first 200 topics, which consisted of
103 generalized Po?lya urn topics and 97 LDA top-
ics. AUC values predicting bad topics given coher-
ence were 0.83 and 0.80, respectively. Coherence
effectively predicts topic quality in both models.
Although we were able to improve the average
overall quality of topics and the average quality of
the ten lowest-scoring topics, we found that the gen-
eralized Po?lya urn model was less successful reduc-
ing the overall number of bad topics. Ignoring one
?unbalanced? topic from each model, 16.5% of the
LDA topics and 13.5% from the generalized Po?lya
urn model were marked as ?bad.? While this result
is an improvement, it is not significant at p = 0.05.
6 Discussion
We have demonstrated the following:
? There is a class of low-quality topics that can-
not be detected using existing word-intrusion
tests, but that can be identified reliably using a
metric based on word co-occurrence statistics.
? It is possible to improve the coherence score
of topics, both overall and for the ten worst,
while retaining the ability to flag bad topics, all
without requiring semi-supervised data or ad-
ditional reference corpora. Although additional
information may be useful, it is not necessary.
? Such models achieve better performance with
substantially fewer Gibbs iterations than LDA.
We believe that the most important challenges in fu-
ture topic modeling research are improving the se-
mantic quality of topics, particularly at the low end,
and scaling to ever-larger data sets while ensuring
high-quality topics. Our results provide critical in-
sight into these problems. We found that it should be
possible to construct unsupervised topic models that
do not produce bad topics. We also found that Gibbs
sampling mixes faster for models that use word co-
occurrence information, suggesting that such meth-
ods may also be useful in guiding online stochastic
variational inference (Hoffman et al, 2010).
Acknowledgements
This work was supported in part by the Center
for Intelligent Information Retrieval, in part by the
CIA, the NSA and the NSF under NSF grant # IIS-
0326249, in part by NIH:HHSN271200900640P,
and in part by NSF # number SBE-0965436. Any
opinions, findings and conclusions or recommenda-
tions expressed in this material are the authors? and
do not necessarily reflect those of the sponsor.
References
Loulwah AlSumait, Daniel Barbara, James Gentle, and
Carlotta Domeniconi. 2009. Topic significance rank-
ing of LDA generative models. In ECML.
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic
modeling via Dirichlet forest priors. In Proceedings of
the 26th Annual International Conference on Machine
Learning, pages 25?32.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022, January.
K.R. Canini, L. Shi, and T.L. Griffiths. 2009. Online
inference of topics with latent Dirichlet alocation. In
Proceedings of the 12th International Conference on
Artificial Intelligence and Statistics.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang,
Sean Gerrish, and David M. Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Ad-
vances in Neural Information Processing Systems 22,
pages 288?296.
Kenneth Church and Patrick Hanks. 1990. Word asso-
ciation norms, mutual information, and lexicography.
Computational Linguistics, 6(1):22?29.
Gabriel Doyle and Charles Elkan. 2009. Accounting for
burstiness in topic models. In ICML.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of
images. IEEE Transaction on Pattern Analysis and
Machine Intelligence 6, pages 721?741.
271
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(suppl. 1):5228?5235.
Matthew Hoffman, David Blei, and Francis Bach. 2010.
Online learning for latent dirichlet alocation. In NIPS.
Hosan Mahmoud. 2008. Po?lya Urn Models. Chapman
& Hall/CRC Texts in Statistical Science.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic mod-
els. In Proceedings of the 13th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data
Mining, pages 490?499.
David Newman, Jey Han Lau, Karl Grieser, and Timothy
Baldwin. 2010. Automatic evaluation of topic coher-
ence. In Human Language Technologies: The Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics.
Yee Whye Teh, Dave Newman, and Max Welling. 2006.
A collapsed variational Bayesian inference algorithm
for lat ent Dirichlet alocation. In Advances in Neural
Information Processing Systems 18.
Hanna Wallach, Iain Murray, Ruslan Salakhutdinov, and
David Mimno. 2009. Evaluation methods for topic
models. In Proceedings of the 26th Interational Con-
ference on Machine Learning.
Xing Wei and Bruce Croft. 2006. LDA-based document
models for ad-hoc retrival. In Proceedings of the 29th
Annual International SIGIR Conference.
272
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1456?1466,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Structured Relation Discovery using Generative Models
Limin Yao? Aria Haghighi+ Sebastian Riedel? Andrew McCallum?
? Department of Computer Science, University of Massachusetts at Amherst
+ CSAIL, Massachusetts Institute of Technology
{lmyao,riedel,mccallum}@cs.umass.edu
{aria42}@csail.mit.edu
Abstract
We explore unsupervised approaches to rela-
tion extraction between two named entities;
for instance, the semantic bornIn relation be-
tween a person and location entity. Con-
cretely, we propose a series of generative
probabilistic models, broadly similar to topic
models, each which generates a corpus of ob-
served triples of entity mention pairs and the
surface syntactic dependency path between
them. The output of each model is a cluster-
ing of observed relation tuples and their as-
sociated textual expressions to underlying se-
mantic relation types. Our proposed models
exploit entity type constraints within a relation
as well as features on the dependency path be-
tween entity mentions. We examine effective-
ness of our approach via multiple evaluations
and demonstrate 12% error reduction in preci-
sion over a state-of-the-art weakly supervised
baseline.
1 Introduction
Many NLP applications would benefit from large
knowledge bases of relational information about
entities. For instance, knowing that the entity
Steve Balmer bears the leaderOf relation to the
entity Microsoft, would facilitate question answer-
ing (Ravichandran and Hovy, 2002), data mining,
and a host of other end-user applications. Due to
these many potential applications, relation extrac-
tion has gained much attention in information ex-
traction (Kambhatla, 2004; Culotta and Sorensen,
2004; Mintz et al, 2009; Riedel et al, 2010; Yao et
al., 2010). We propose a series of generative prob-
abilistic models, broadly similar to standard topic
models, which generate a corpus of observed triples
of entity mention pairs and the surface syntactic de-
pendency path between them. Our proposed mod-
els exploit entity type constraints within a relation
as well as features on the dependency path between
entity mentions. The output of our approach is a
clustering over observed relation paths (e.g. ?X was
born in Y? and ?X is from Y?) such that expressions
in the same cluster bear the same semantic relation
type between entities.
Past work has shown that standard supervised
techniques can yield high-performance relation de-
tection when abundant labeled data exists for a
fixed inventory of individual relation types (e.g.
leaderOf ) (Kambhatla, 2004; Culotta and Sorensen,
2004; Roth and tau Yih, 2002). However, less ex-
plored are open-domain approaches where the set
of possible relation types are not fixed and little to
no labeled is given for each relation type (Banko et
al., 2007; Banko and Etzioni, 2008). A more re-
lated line of research has explored inducing rela-
tion types via clustering. For example, DIRT (Lin
and Pantel, 2001) aims to discover different repre-
sentations of the same semantic relation using dis-
tributional similarity of dependency paths. Poon
and Domingos (2008) present an Unsupervised se-
mantic parsing (USP) approach to partition depen-
dency trees into meaningful fragments (or ?parts?
to use their terminology). The combinatorial nature
of this dependency partition model makes it difficult
for USP to scale to large data sets despite several
necessary approximations during learning and infer-
1456
ence. Our work is similar to DIRT and USP in that
we induce relation types from observed dependency
paths, but our approach is a straightforward and
principled generative model which can be efficiently
learned. As we show empirically, our approach out-
performs these related works when trained with the
same amount of data and further gains are observed
when trained with more data.
We evaluate our approach using ?intrinsic? clus-
tering evaluation and ?extrinsic? evaluation settings.1
The former evaluation is performed using subset of
induced clusters against Freebase relations, a large
manually-built entity and relational database. We
also show some clusters which are not included as
Freebase relations, as well as some entity clusters
found by our approach. The latter evaluation uses
the clustering induced by our models as features for
relation extraction in distant supervision framework.
Empirical results show that we can find coherent
clusters. In relation extraction, we can achieve 12%
error reduction in precision over a state-of-the-art
weakly supervised baseline and we show that using
features from our proposed models can find more
facts for a relation without significant accuracy loss.
2 Problem and Experimental Setup
The task of relation extraction is mapping surface
textual relations to underlying semantic relations.
For instance, the textual expression ?X was born in
Y? indicates a semantic relation bornIn between en-
tities ?X? and ?Y?. This relation can be expressed
textually in several ways: for instance, ?X, a native
of Y? or ?X grew up in Y?. There are several com-
ponents to a coherent relation type, including a tight
small number of textual expressions as well as con-
straints on the entities involved in the relation. For
instance, in the bornIn relation ?X? must be a person
entity and ?Y? a location (typically a city or nation).
In this work, we present an unsupervised probabilis-
tic generative model for inducing clusters of relation
types and recognizing their textual expressions. The
set of relation types is not pre-specified but induced
from observed unlabeled data. See Table 4 for ex-
amples of learned semantic relations.
Our observed data consists of a corpus of docu-
ments and each document is represented by a bag
1See Section 4 for a fuller discussion of evaluation.
of relation tuples. Each tuple represents an ob-
served syntactic relationship between two Named
Entities (NE) and consists of three components: the
dependency path between two NE mentions, the
source argument NE, and the destination argument
NE. A dependency path is a concatenation of depen-
dency relations (edges) and words (nodes) along a
path in a dependency tree. For instance, the sentence
?John Lennnon was born in Liverpool? would yield
the relation tuple (Lennon, [? ?nsubjpass, born, ?
?in], Liverpool). This relation tuple reflects a se-
mantic bornIn relation between the John Lennon and
Liverpool entities. The dependency path in this ex-
ample corresponds to the ?X was born in Y? textual
expression given earlier. Note that for the above ex-
ample, the bornIn relation can only occur between a
person and a location. The relation tuple is the pri-
mary observed random variable in our model and we
construct our models (see Section 3) so that clusters
consist of textual expressions representing the same
underlying relation type.
3 Models
We propose three generative models for modeling
tuples of entity mention pairs and the syntactic de-
pendency path between them (see Section 2). The
first two models, Rel-LDA and Rel-LDA1 are sim-
ple extensions of the standard LDA model (Blei et
al., 2003). At the document level, our model is iden-
tical to standard LDA; a multinomial distribution
is drawn over a fixed number of relation types R.
Changes lie in the observations. In standard LDA,
the atomic observation is a word drawn from a la-
tent topic distribution determined by a latent topic
indicator variable for that word position. In our ap-
proach, a document consists of an exchangeable set
of relation tuples. Each relation tuple is drawn from
a relation type ?topic? distribution selected by a la-
tent relation type indicator variable. Relation tuples
are generated using a collection of independent fea-
tures drawn from the underlying relation type distri-
bution. These changes to standard LDA are intended
to have the effect that instead of representing seman-
tically related words, the ?topic? latent variable rep-
resents a relation type.
Our third model exploits entity type constraints
within a relation and induces clusters of relations
1457
and entities jointly. For each tuple, a set of rela-
tion level features and two latent entity type indica-
tors are drawn independently from the relation type
distribution; a collection of entity mention features
for each argument is drawn independently from the
entity type distribution selected by the entity type
indicator.
Path X, made by Y
Source Gamma Knife
Dest Elekta
Trigger make
Lex , made by the Swedish
medical technology firm
POS , VBN IN DT JJ JJ NN NN
NER pair MISC-ORG
Sync pair partmod-pobj
Table 1: The features of tuple ?(Gamma Knife, made
by, Elekta)? in sentence ?Gamma Knife, made by the
Swedish medical technology firm Elekta, focuses low-
dosage gamma radiation ...?
3.1 Rel-LDA Model
This model is an extension to the standard LDA
model. At the document level, a multinomial dis-
tribution over relations ?doc is drawn from a prior
Dir(?). To generate a relation tuple, we first draw a
relation ?topic? r from Multi(?). Then we generate
each feature f of a tuple independently from a multi-
nomial distribution Multi(?rf ) selected by r. In this
model, each tuple has three features, i.e. its three
components, shown in the first three rows in Table 1.
Figure 1 shows the graphical representation of Rel-
LDA. Table 2 lists all the notation used in describing
our models.
The learning process of the models is an EM pro-
cess. The procedure is similar to that used by the
standard topic model. In the variational E-step (in-
ference), we sample the relation type indicator for
each tuple using p(r|f):
P (r|f(p, s, d)) ? p(r)?f p(f |r)
? (?r + nr|d)
?
f
?f+nf |rP
f ? (?f ?+nf ?|r)
|R| Number of relations
|D| Number of documents
r A relation
doc A document
p, s, d Dep path, source and dest args
f A feature/feature type
T Entity type of one argument
? Dirichlet prior for ?doc
?x Dirichlet prior for ?rx
? Dirichlet prior for ?t
?doc p(r|doc)
?rx p(x|r)
?t p(fs|T ), p(fd|T )
Table 2: The notation used in our models
      
  
             |R|  
      
  
  
......
                                        N 
r
f
?
?
rf
?
?
f
f
      
  
  
                           
                           
                                             |D|  
Figure 1: Rel-LDA model. Shaded circles are observa-
tions, and unshaded ones are hidden variables. A docu-
ment consists of N tuples. Each tuple has a set of fea-
tures. Each feature of a tuple is generated independently
from a hidden relation variable r.
p(r) and p(f |r) are estimated in the M-step:
?doc =
?+ nr|doc?
r?(?+ nr?|doc)
?rf =
?f + nf |r?
f ?(?f ? + nf ?|r)
where nf |r indicates the number of times a feature f
is assigned with r.
3.2 Rel-LDA1
Looking at results of Rel-LDA, we find the clus-
ters sometimes are in need of refinement, and we
can address this by adding more features. For in-
stance, adding trigger features can encourage spar-
sity over dependency paths. We define trigger words
as all the words on the dependency path except stop
words. For example, from path ?X, based in Y?,
?base? is extracted as a trigger word. The intuition
1458
for using trigger words is that paths sharing the same
set of trigger words should go to one cluster. Adding
named entity tag pair can refine the cluster too. For
example, a cluster found by Rel-LDA contains ?X
was born in Y? and ?X lives in Y?; but it also con-
tains ?X, a company in Y?. In this scenario, adding
features ?PER-LOC? and ?ORG-LOC? can push the
model to split the clusters into two and put the third
case into a new cluster.
Hence we propose Rel-LDA1. It is similar to
Rel-LDA, except that each tuple is represented with
more features. Besides p, s, and d, we introduce
trigger words, lexical pattern, POS tag pattern, the
named entity pair and the syntactic category pair fea-
tures for each tuple. Lexical pattern is the word se-
quence between the two arguments of a tuple and
POS tag pattern is the POS tag sequence of the lexi-
cal pattern. See Table 1 as an example.
Following typical EM learning(Charniak and El-
sner, 2009), we start with a much simpler genera-
tive model, expose the model to fewer features first,
and iteratively add more features. First, we train a
Rel-LDA model, i.e. the model only generates the
dependency path, source and destination arguments.
After each interval of 10 iterations, we introduce one
additional feature. We add the features in the order
of trigger, lexical pattern, POS, NER pair, and syn-
tactic pair.
3.3 Type-LDA model
We know that relations can only hold between
certain entity types, known as selectional prefer-
ences (Ritter et al, 2010; Seaghdha, 2010; Kozareva
and Hovy, 2010). Hence we propose Type-LDA
model. This model can capture the selectional pref-
erences of relations to their arguments. In the mean
time, it clusters tuples into relational clusters, and
arguments into different entity clusters. The entity
clusters could be interesting in many ways, for ex-
ample, defining fine-grained entity types and finding
new concepts.
We split the features of a tuple into relation level
features and entity level features. Relation level fea-
tures include the dependency path, trigger, lex and
POS features; entity level features include the entity
mention itself and its named entity tag.
The generative storyline is as follows. At the doc-
ument level, a multinomial distribution over rela-
      
  
  
                                        N   
      
  
  
        
                      
                                                   |D|  
      
  
             |R|  
r
f
f
s
?
?
rf
?
t
f
d
     
  
              |R| 
?
rt2
?
?
t2
?
?
f
T
1
T
2
     
  
              |T| 
     
  
              |R| 
?
rt1
?
t1
Figure 2: Type-LDA model. Each document consists of
N tuples. Each tuple has a set of features, relation level
features f and entity level features of source argument fs
and destination argument fd. Relation level features and
two hidden entity types T1 and T2 are generated from
hidden relation variable r independently. Source entity
features are generated from T1 and destination features
are generated from T2.
tions ?doc is drawn from a Dirichlet prior. A doc-
ument consists of N relation tuples. Each tuple is
represented by relation level features (f ) and entity
level features of source argument (fs) and destina-
tion argument (fd). For each tuple, a relation r is
drawn from Multi(?doc). The relation level features
and two hidden entity types T1 and T2 are indepen-
dently generated from r. Features fs are generated
from T1 and fd from T2. Figure 2 shows the graphi-
cal representation of this model.
At inference time, we sample r, T1 and T2 for
each tuple. For efficient inference, we first initialize
the model without T1 and T2, i.e. all the features are
generated directly from r. Here the model degener-
ates to Rel-LDA1. After some iterations, we intro-
duce T1 and T2. We sample the relation variable (r)
and two mention types variables (T1,T2) iteratively
for each tuple. We can sample them together, but
this is not very efficient. In addition, we found that
it does not improve performance.
4 Experiments
Our experiments are carried out on New York Times
articles from year 2000 to 2007 (Sandhaus, 2008).
We filter out some noisy documents, for example,
1459
obituary content, lists and so on. Obituary arti-
cles often contain syntax that diverges from stan-
dard newswire text. This leads to parse errors with
WSJ-trained parsers and in turn, makes extraction
harder. We also filter out documents that contain
lists or tables of items (such as books, movies) be-
cause this semi-structured information is not the fo-
cus of our current work. After filtering we are left
with approximately 428K documents. They are pre-
processed in several steps. First we employ Stanford
tools to tokenize, sentence-split and Part-Of-Speech
tag (Toutanova et al, 2003) a document. Next we
recognize named entities (Finkel et al, 2005) by
labelling tokens with PERSON, ORGANIZATION,
LOCATION, MISC and NONE tags. Consecutive
tokens which share the same category are assembled
into entity mentions. They serve as source and des-
tination arguments of the tuples we seek to model.
Finally we parse each sentence of a document using
MaltParser (Nivre et al, 2004) and extract depen-
dency paths for each pair of named entity mentions
in one sentence.
Following DIRT (Lin and Pantel, 2001), we fil-
ter out tuples that do not satisfy the following con-
straints. First, the path needs to be shorter than
10 edges, since longer paths occur less frequently.
Second, the dependency relations in the path should
connect two content words, i.e. nouns, verbs, ad-
jectives and adverbs. For example, in phrase ?solve
a problem?, ?obj(solve, problem)? is kept, while
?det(problem, a)? is discarded. Finally, the de-
pendency labels on the path must not be: ?conj?,
?ccomp?, ?parataxis?, ?xcomp?, ?pcomp?, ?advcl?,
?punct?, and ?infmod?. This selection is based on the
observation that most of the times the corresponding
dependency relations do not explicitly state a rela-
tion between two candidate arguments.
After all entity mentions are generated and paths
are extracted, we have nearly 2.5M tuples. After
clustering (inference), each of these tuple will be-
long to one cluster/relation and is associated with its
clusterID.
We experimented with the number of clusters and
find that in a range of 50-200 the performance does
not vary significantly with different numbers. In our
experiments, we cluster the tuples into 100 relation
clusters for all three models. For Type-LDA model,
we use 50 entity clusters.
We evaluate our models in two ways. The first
aims at measuring the clustering quality by mapping
clusters to Freebase relations. The second seeks to
assess the utility of our predicted clusters as features
for relation extraction.
4.1 Relations discovered by different models
Looking closely at the clusters we predict, we find
that some of them can be mapped to Freebase rela-
tions. We discover clusters that roughly correspond
to the parentCom (parent company relation), filmDi-
rector, authorOf, comBase (base of a company rela-
tion) and dieIn relations in Freebase. We treat Free-
base annotations as ground truth and measure recall.
We count each tuple in a cluster as true positive if
Freebase states the corresponding relation between
its argument pair. We find that precision numbers
against Freebase are low, below 10%. However,
these numbers are not reliable mainly because many
correct instances found by our models are missing
in Freebase. One reason why our predictions are
missing in Freebase is coreference. For example,
we predict parentCom relation between ?Linksys?
and ?Cisco?, while Freebase only considers ?Cisco
Systems, Inc.? as the parent company of ?Linksys?.
It does not corefer ?Cisco? to ?Cisco Systems, Inc.?.
Incorporating coreference in our model may fix this
problem and is a focus of future work. Instead of
measuring precision against Freebase, we ask hu-
mans to label 50 instances for each cluster and report
precision according to this annotated data. Table 3
shows the scores.
We can see that in most cases Rel-LDA1 and
Type-LDA substantially outperform the Rel-LDA
model. This is due to the fact that both models can
exploit more features to make clustering decisions.
For example, in Rel-LDA1 model, the NER pair fea-
ture restricts the entity types the two arguments can
take.
In the following, we take parentCom relation as
an example to analyze the behaviors of different
models. Rel-LDA includes spurious instances such
as ?A is the chief executive of B?, while Rel-LDA1
has fewer such instances due to the NER pair fea-
ture. Similarly, by explicitly modeling entity type
constraints, Type-LDA makes fewer such errors. All
our models make mistakes when sentences have co-
ordination structures on which the parser has failed.
1460
Rel. Sys. Rec. Prec.
parentCom
Rel-LDA 51.4 76.0
Rel-LDA1 49.5 78.0
Type-LDA 55.3 72.0
filmDirector
Rel-LDA 42.5 32.0
Rel-LDA1 70.5 40.0
Type-LDA 74.2 26.0
comBase
Rel-LDA 31.5 12.0
Rel-LDA1 54.2 22.0
Type-LDA 57.1 30.0
authorOf
Rel-LDA 25.2 84.0
Rel-LDA1 46.9 86.0
Type-LDA 20.2 68.0
dieIn
Rel-LDA 26.5 34.0
Rel-LDA1 55.9 40.0
Type-LDA 50.2 28.0
Table 3: Clustering quality evaluation (%), Rec. is mea-
sured against Freebase, Prec. is measured according to
human annotators
For example, when a sentence has the following pat-
tern ?The winners are A, a part of B; C, a part of
D; E, a part of F?, our models may predict parent-
Com(A,F), because the parser connects A with F via
the pattern ?a part of?.
Some clusters found by our models cannot be
mapped to Freebase relations. Consider the Free-
base relation worksFor as one example. This re-
lation subsumes all types of employment relation-
ships, irrespective of the role the employee plays for
the employer. By contrast, our models discover clus-
ters such as leaderOf, editorOf that correspond to
more specific roles an employee can have. We show
some example relations in Table 4. In the table, the
2nd row shows a cluster of employees of news media
companies; the 3rd row shows leaders of companies;
the last one shows birth and death places of persons.
We can see that the last cluster is noisy since we
do not handle antonyms in our models. The argu-
ments of the clusters have noise too. For example,
?New York? occurs as a destination argument in the
2nd cluster. This is because ?New York? has high
frequency in the corpus and it brings noise to the
clustering results. In Table 5 some entity clusters
found by Type-LDA are shown. We find different
types of companies, such as financial companies and
news companies. We also find subclasses of person,
for example, reviewer and politician, because these
different entity classes participate in different rela-
tions. The last cluster shown in the table is a mix-
ture of news companies and government agencies.
This may be because this entity cluster is affected
by many relations.
4.2 Distant Supervision based Relation
Extraction
Our generative models detect clusters of dependency
paths and their arguments. Such clusters are inter-
esting in their own right, but we claim that they can
also be used to help a supervised relation extractor.
We validate this hypothesis in the context of relation
extraction with distant supervision using predicted
clusters as features.
Following previous work (Mintz et al, 2009), we
use Freebase as our distant supervision source, and
align related entity pairs to the New York Times arti-
cles discussed earlier. Our training and test instances
are pairs of entities for which both arguments appear
in at least one sentence together. Features of each
instance are extracted from all sentences in which
both entities appear together. The gold label for each
instance comes from Freebase. If a pair of entities
is not related according to Freebase, we consider it
a negative example. Note that this tends to create
some amount of noise: some pairs may be related,
but their relationships are not yet covered in Free-
base.
After filtering out relations with fewer than 10 in-
stances we have 65 relations and an additional ?O?
label for unrelated pairs of entities. We call related
instances positive examples and unrelated instances
negative examples.
We train supervised classifiers using maximum
entropy. The baseline classifier employs features
that Mintz et al (2009) used. To extract features
from the generative models we proceed as follows.
For each pair of entities, we collect all tuples asso-
ciated with it. For each of these tuples we extract its
clusterID, and use this ID as a binary feature.
The baseline system without generative model
features is called Distant. The classifiers with ad-
ditional features from generative models are named
after the generative models. Thus we have Rel-LDA,
Rel-LDA1 and Type-LDA classifiers. We compare
1461
Source New York, Euro RSCG Worldwide, BBDO Worldwide, American, DDB Worldwide
Path X, a part of Y; X, a unit of Y; X unit of Y; X, a division of Y; X is a part of Y
Dest Omnicom Group, Interpublic Group of Companies, WPP Group, Publicis Groupe
Source Supreme Court, Anna Wintour, William Kristol, Bill Keller, Charles McGrath
Path X, an editor of Y; X, a publisher of Y; X, an editor at Y; X, an editor in chief of Y; X is an editor of Y;
Dest The Times, The New York Times, Vogue, Vanity Fair, New York
Source Kenneth L. Lay, L. Dennis Kozlowski, Bernard J. Ebbers, Thomas R. Suozzi, Bill Gates
Path X, the executive of Y; X, Y?s executive; X, Y executive; X, the chairman of Y; X, Y?s chairman
Dest Enron, Microsoft, WorldCom, Citigroup, Nassau County
Source Paul J. Browne, John McArdle, Tom Cocola, Claire Buchan, Steve Schmidt
Path X, a spokesman for Y; X, a spokeswoman for Y; X, Y spokesman; X, Y spokeswoman; X, a commissioner of Y
Dest White House, Justice Department, Pentagon, United States, State Department
Source United Nations, Microsoft, Intel, Internet, M. D. Anderson
Path X, based in Y; X, which is based in Y; X, a company in Y; X, a company based in Y; X, a consultant in Y
Dest New York, Washington, Manhattan, Chicago, London
Source Army, Shiite, Navy, John, David
Path X was born in Y; X die at home in Y; X die in Y; X, son of Y; X die at Y
Dest Manhattan, World War II, Brooklyn, Los Angeles, New York
Table 4: The path, source and destination arguments of some relations found by Rel-LDA1.
Company Microsoft, Enron, NBC, CBS, Disney
FinanceCom Merrill Lynch, Morgan Stanley, Goldman Sachs, Lehman Brothers, Credit Suisse First Boston
News Notebook, New Yorker, Vogue, Vanity Fair, Newsweek
SportsTeam Yankees, Mets, Giants, Knicks, Jets
University University of California, Harvard, Columbia University, New York University, University of Penn.
Art Reviewer Stephen Holden, Ken Johnson, Roberta Smith, Anthony Tommasini, Grace Glueck
Games World Series, Olympic, World Cup, Super Bowl, Olympics
Politician Eliot Spitzer, Ari Fleischer, Kofi Annan, Scott McClellan, Karl Rove
Gov. Agency Congress, European Union, NATO, Federal Reserve, United States Court of Appeals
News/Agency The New York Times, The Times, Supreme Court, Security Council, Book Review
Table 5: The entity clusters found by Type-LDA
these against Distant and the DIRT database. For
the latter we parse our data using Minipar (Lin,
1998) and extract dependency paths between pairs
of named entity mentions. For each path, the top 3
similar paths are extracted from DIRT database. The
Minipar path and the similar paths are used as addi-
tional features.
For held-out evaluation, we construct the training
data from half of the positive examples and half of
the negative examples. The remaining examples are
used as test data. Note that the number of negative
instances is more than 10 times larger than the num-
ber of positive instances. At test time, we rank the
predictions by the conditional probabilities obtained
from the Maximum Entropy classifier. We report
precision of top ranked 50 instances for each relation
in table 6. From the table we can see that all systems
using additional features outperform the Distant sys-
tem. In average, our best model achieves 4.1%
improvement over the distant supervision baseline,
12% error reduction. The precision of bornIn is low
because in most cases we predict bornIn instances
as liveIn.
We expect systems using generative model fea-
tures to have higher recall than the baseline. This
is difficult to measure, but precision in the high re-
call area is a signal. We look at top ranked 1000
instances of each system and show the precision in
the last row of the table. We can see that our best
model Type-LDA outperforms the distant supervi-
sion baseline by 4.5%.
Why do generative model features help to im-
1462
Relation Dist Rel Rel1 Type DIRT
worksFor 80.0 92.0 86.0 90.0 84.0
authorOf 98.0 98.0 98.0 98.0 98.0
containedBy 92.0 96.0 96.0 92.0 96.0
bornIn 16.0 18.0 22.0 24.0 10.0
dieIn 28.0 30.0 28.0 24.0 24.0
liveIn 50.0 52.0 54.0 54.0 56.0
nationality 92.0 94.0 90.0 90.0 94.0
parentCom 94.0 96.0 96.0 96.0 90.0
founder 65.2 76.3 61.2 64.0 68.3
parent 52.0 54.0 50.0 52.0 52.0
filmDirector 54.0 60.0 60.0 64.0 62.0
Avg 65.6 69.7 67.4 68.0 66.8
Prec@1K 82.8 85.8 85.3 87.3 82.8
Table 6: Precision (%) of some frequent relations
prove relation extraction? One reason is that gen-
erative models can transfer information from known
patterns to unseen patterns. For example, given
?Sidney Mintz, the great food anthropologist at
Johns Hopkins University?, we want to predict the
relation between ?Sidney Mintz? and ?Johns Hopkins
University?. The distant supervision system incor-
rectly predicts the pair as ?O? since it has not seen
the path ?X, the anthropologist at Y? in the training
data. By contrast, Rel-LDA can predict this pair cor-
rectly as worksFor because the dependency path of
this pair is in a cluster which contains the path ?X, a
professor at Y?.
In addition to held-out evaluation we also carry
out manual evaluation. To this end, we use all the
positive examples and randomly select five times
the number of positive examples as negative ex-
amples to train a classifier. The remaining nega-
tive examples are candidate instances. We rank the
predicted instances according to their classification
scores. For each relation, we ask human annotators
to judge its top ranked 50 instances.
Table 7 lists the manual evaluation results for
some frequent relations. We also list how many in-
stances are found for each relation. For almost all
the relations, systems using generative model fea-
tures find more instances. In terms of precision, our
models perform comparatively to the baseline, even
better for some relations.
We also notice that clustering quality is not con-
sistent with distant supervision performance. Rel-
LDA1 can find better clusters than Rel-LDA but it
has lower precision in held-out evaluation. Type-
LDA underperforms Rel-LDA in average precision
but it gets higher precision in a higher recall area, i.e.
precision at 1K. One possible reason for the incon-
sistency is that the baseline distant supervision sys-
tem already employs features that are used in Rel-
LDA1. Another reason may be that the clusters do
not overlap with Freebase relations very well, see
section 4.1.
4.3 Comparing against USP
We also try to compare against USP (Poon and
Domingos, 2008). Due to memory requirements of
USP, we are only able to run it on a smaller data
set consisting of 1,000 NYT documents; this is three
times the amount of data Poon and Domingos (2008)
used to train USP.2 For distant supervision based re-
lation extraction, we only match about 500 Freebase
instances to this small data set.
USP provides a parse tree for each sentence and
for each mention pair we can extract a path from
the tree. Since USP provides clusters of words and
phrases, we use the USP clusterID associated with
the words on the path as binary features in the clas-
sifier.
All models are less accurate when trained on this
smaller dataset; we can do as well as USP does,
even a little better. USP achieves 8.6% in F1, Rel-
LDA 8.7%, Rel-LDA1 10.3%, Type-LDA 8.9% and
Distant 10.3%. Of course, given larger datasets,
the performance of Rel-LDA, Rel-LDA1, and Type-
LDA improves considerably. In summary, compar-
ing against USP, our approach scales much more
easily to large data.
5 Related Work
Many approaches have been explored in relation ex-
traction, including bootstrapping, supervised classi-
fication, distant supervision, and unsupervised ap-
proaches.
Bootstrapping employs a few labeled examples
for each relation, iteratively extracts patterns from
the labeled seeds, and uses the patterns to extract
2Using the publicly released USP code, training a model
with 1,000 documents resulted in about 45 gigabytes of heap
space in the JVM.
1463
Relation Top 50 (%) #InstancesDist Rel Type Dist Rel Type
worksFor 100.0 100.0 100.0 314 349 349
authorOf 94.0 94.0 96.0 185 208 229
containedBy 98.0 98.0 98.0 670 714 804
bornIn 82.6 88.2 88.0 46 36 56
dieIn 100.0 100.0 100.0 167 176 231
liveIn 98.0 98.0 94.0 77 86 109
nationality 78.0 82.0 76.0 84 92 114
parentCom 79.2 77.4 85.7 24 31 28
founder 80.0 80.0 50.0 5 5 14
parent 97.0 92.3 94.7 33 39 38
filmDirector 92.6 96.9 97.1 27 32 34
Table 7: Manual evaluation, Precision and recall of some frequent relations
more seeds (Brin, 1998). This approach may suffer
from low recall since the patterns can be too specific.
Supervised learning can discover more general
patterns (Kambhatla, 2004; Culotta and Sorensen,
2004). However, this approach requires labeled data,
and most work only carry out experiments on small
data set.
Distant supervision for relation extraction re-
quires no labeled data. The approach takes some
existing knowledge base as supervision source,
matches its relational instances against the text cor-
pus to build the training data, and extracts new in-
stances using the trained classifiers (Mintz et al,
2009; Bunescu and Mooney, 2007; Riedel et al,
2010; Yao et al, 2010).
All these approaches can not discover new rela-
tions and classify instances which do not belong to
any of the predefined relations. Other past work has
explored inducing relations using unsupervised ap-
proaches.
For example, DIRT (Lin and Pantel, 2001) aims
to discover different representations of the same se-
mantic relation, i.e. similar dependency paths. They
employ the distributional similarity based approach
while we use generative models. Both DIRT and our
approach take advantage of the arguments of depen-
dency paths to find semantic relations. Moreover,
our approach can cluster the arguments into differ-
ent types.
Unsupervised semantic parsing (USP) (Poon and
Domingos, 2008) discovers relations by merging
predicates which have similar meanings; it proceeds
to recursively cluster dependency tree fragments (or
?parts?) to best explain the observed sentence. It is
not focused on capturing any particular kind of re-
lation between sentence constituents, but to capture
repeated patterns. Our approach differs in that we
are focused on capturing a narrow range of binary
relations between named entities; some of our mod-
els (see Section 3) utilize entity type information to
constraint relation type induction. Also, our models
are built to be scalable and trained on a very large
corpus. In addition, we use a distant supervision
framework for evaluation.
Relation duality (Bollegala et al, 2010) employs
co-clustering to find clusters of entity pairs and pat-
terns. They identify each cluster of entity pairs as a
relation by selecting representative patterns for that
relation. This approach is related to our models,
however, it does not identify any entity clusters.
Generative probabilistic models are widely em-
ployed in relation extraction. For example, they are
used for in-domain relation discovery while incorpo-
rating constraints via posterior regularization (Chen
et al, 2011). We are focusing on open domain re-
lation discovery. Generative models are also ap-
plied to selectional preference discovery (Ritter et
al., 2010; Seaghdha, 2010). In this scenario, the
authors assume relation labels are given while we
automatically discover relations. Generative models
are also used in unsupervised coreference (Haghighi
and Klein, 2010).
1464
Clustering is also employed in relation extraction.
Hasegawa et al (2004) cluster pairs of named en-
tities according to the similarity of context words
intervening between them. Their approach is not
probabilistic. Researchers also use topic models to
perform dimension reduction on features when they
cluster relations (Hachey, 2009). However, they do
not explicitly model entity types.
Open information extraction aims to discover re-
lations independent of specific domains and rela-
tions (Banko et al, 2007; Banko and Etzioni, 2008).
A self-learner is employed to extract relation in-
stances but the systems do not cluster the instances
into relations. Yates and Etzioni (2009) present RE-
SOLVER for discovering relational synonyms as a
post processing step. Our approach integrates entity
and relation discovery in a probabilistic model.
6 Conclusion
We have presented an unsupervised probabilistic
generative approach to relation extraction between
two named entities. Our proposed models exploit
entity type constraints within a relation as well
as features on the dependency path between entity
mentions to cluster equivalent textual expressions.
We demonstrate the effectiveness of this approach
by comparing induced relation clusters against a
large knowledge base. We also show that using clus-
ters of our models as features in distant supervised
framework yields 12% error reduction in precision
over a weakly supervised baseline and outperforms
other state-of-the art relation extraction techniques.
Acknowledgments
This work was supported in part by the Center
for Intelligent Information Retrieval and the Uni-
versity of Massachusetts gratefully acknowledges
the support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program un-
der Air Force Research Laboratory (AFRL) prime
contract no. FA8750-09-C-0181, ITR#1, and NSF
MALLET. Any opinions, findings, and conclusion
or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect
the view of the DARPA, AFRL, or the US govern-
ment. Any opinions, findings and conclusions or
recommendations expressed in this material are the
authors? and do not necessarily reflect those of the
sponsor.
References
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT.
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI2007.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022, January.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2010. Relational duality: Unsupervised ex-
traction of semantic relations between entities on the
web. In Proceedings of WWW.
Sergey Brin. 1998. Extracting patterns and relations
from the world wide web. In Proc. of WebDB Work-
shop at 6th International Conference on Extending
Database Technology.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using min-
imal supervision. In Proceedings of the 45rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?07).
Eugene Charniak and Micha Elsner. 2009. Em works for
pronoun anaphora resolution. In Proceedings of ACL.
Harr Chen, Edward Benson, Tahira Naseem, and Regina
Barzilay. 2011. In-domain relation discovery with
meta-constraints via posterior regularization. In Pro-
ceedings of ACL.
Aron Culotta and Jeffery Sorensen. 2004. Dependency
tree kernels for relation extraction. In 42nd Annual
Meeting of the Association for Computational Linguis-
tics, Barcelona, Spain.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL
?05), pages 363?370, June.
Benjamin Hachey. 2009. Towards Generic Relation Ex-
traction. Ph.D. thesis, University of Edinburgh.
Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Proceed-
ings of HLT-NAACL.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In ACL.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy models
for extracting relations. In Proceedings of ACL.
1465
Zornitsa Kozareva and Eduard Hovy. 2010. Learning
arguments and supertypes of semantic relations using
recursive patterns. In Proceedings of ACL 10.
Dekang Lin and Patrick Pantel. 2001. Dirt - discovery of
inference rules from text. In Proceedings of KDD.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on the Eval-
uation of Parsing Systems.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In ACL-IJCNLP.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of CoNLL, pages
49?56.
Hoifung Poon and Pedro Domingos. 2008. Unsuper-
vised semantic parsing. In Proceedings of the Confer-
ence on Empirical methods in natural language pro-
cessing (EMNLP).
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD ?10).
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet alocation method for selectional preferences.
In Proceedings of ACL10.
Dan Roth and Wen tau Yih. 2002. Probabilistic reason-
ing for entity and relation recognition. In Proceedings
of Coling.
Evan Sandhaus, 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia.
Diarmuid O Seaghdha. 2010. Latent variable models of
selectional preference. In Proceedings of ACL 10.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In HLT-
NAACL, pages 252?259.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1013?1023, Cambridge, MA, Oc-
tober. Association for Computational Linguistics.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
1466
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 732?743, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Parse, Price and Cut?Delayed Column and Row Generation for Graph
Based Parsers
Sebastian Riedel David Smith Andrew McCallum
Department of Computer Science
University of Massachusetts, Amherst
{riedel,dasmith,mccallum}@cs.umass.edu
Abstract
Graph-based dependency parsers suffer from
the sheer number of higher order edges they
need to (a) score and (b) consider during opti-
mization. Here we show that when working
with LP relaxations, large fractions of these
edges can be pruned before they are fully
scored?without any loss of optimality guar-
antees and, hence, accuracy. This is achieved
by iteratively parsing with a subset of higher-
order edges, adding higher-order edges that
may improve the score of the current solu-
tion, and adding higher-order edges that are
implied by the current best first order edges.
This amounts to delayed column and row gen-
eration in the LP relaxation and is guaranteed
to provide the optimal LP solution. For second
order grandparent models, our method consid-
ers, or scores, no more than 6?13% of the sec-
ond order edges of the full model. This yields
up to an eightfold parsing speedup, while pro-
viding the same empirical accuracy and cer-
tificates of optimality as working with the full
LP relaxation. We also provide a tighter LP
formulation for grandparent models that leads
to a smaller integrality gap and higher speed.
1 Introduction
Many problems in NLP, and structured prediction in
general, can be cast as finding high-scoring struc-
tures based on a large set of candidate parts. For
example, in second order graph-based dependency
parsing (K?bler et al2009) we have to choose a
quadratic number of first order and a cubic number
of second order edges such that the graph is both
high-scoring and a tree. In coreference, we have
to select high-scoring clusters of mentions from an
exponential number of candidate clusters, such that
each mention is in exactly one cluster (Culotta et
al., 2007). In segmentation of citation strings, we
need to consider a quadratic number of possible seg-
ments such that every token is part of exactly one
segment (Poon and Domingos, 2007).
What makes such problems challenging is the
large number of possible parts to consider. This
number not only affects the cost of search or opti-
mization but also slows down the process of scor-
ing parts before they enter the optimization prob-
lem. For example, the cubic grandparent edges in
second-order dependency parsing slow down dy-
namic programs (McDonald and Pereira, 2006), be-
lief propagation (Smith and Eisner, 2008) and LP
solvers (Martins et al2009), since there are more
value functions to evaluate, more messages to pass,
or more variables to consider. But to even calculate
the score for each part we need a cubic number of
operations that usually involve expensive feature ex-
traction. This step often becomes a major bottleneck
in parsing, and structured prediction in general.
Candidate parts can often be heuristically pruned.
In the case of dependency parsing, previous work
has used coarse-to-fine strategies where simpler first
order models are used to prune unlikely first or-
der edges, and hence all corresponding higher or-
der edges (Koo and Collins, 2010; Martins et al
2009; Riedel and Clarke, 2006). While such meth-
ods can be effective, they are more convoluted, often
require training of addition models as well as tuning
of thresholding hyper-parameters, and usually pro-
vide no guarantees of optimality.
We present an approach that can solve problems
with large sets of candidate parts without consider-
ing all of these parts in either optimization or scor-
732
ing. And in contrast to most pruning heuristics, our
algorithm can give certificates of optimality before
having optimized over, or even scored, all parts. It
does so without the need of auxiliary models or tun-
ing of threshold parameters. This is achieved by a
delayed column and row generation algorithm that
iteratively solves an LP relaxation over a small sub-
set of current candidate parts, and then finds new
candidates that score highly and can be inserted into
the current optimal solution without removing high
scoring existing structure. The latter step subtracts
from the cost of a part the price of resources the part
requires, and is often referred as pricing. Sometimes
parts may score highly after pricing, but are neces-
sary in order to make the current solution feasible.
We add such parts in a step that roughly amounts to
violated cuts to the LP.
We illustrate our approach in terms of a second-
order grandparent model for dependency parsing.
We solve these models by iteratively parsing, pric-
ing, and cutting. To this end we use a variant of the
LP relaxation formulated by Martins et al2009).
Our variant of this LP is designed to be amenable to
column generation. It also turns out to be a tighter
outer bound that leads to fewer fractional solutions
and faster runtimes. To find high scoring grandpar-
ent edges without explicitly enumerating all of them,
we prune out a large fraction using factorized upper
bounds on grandparent scores.
Our parse, price and cut algorithm is evaluated
using a non-projective grandparent model on three
languages. Compared to a brute force approach of
solving the full LP, we only score about 10% of the
grandparent edges, consider only 8% in optimiza-
tion, and so observe an increase in parsing speed of
up to 750%. This is possible without loss of opti-
mality, and hence accuracy. We also find that our
extended LP formulation leads to a 15% reduction
of fractional solutions, up to 12 times higher speed,
and generally higher accuracy when compared to the
grandparent formulation of Martins et al2009).
2 Graph-Based Dependency Parsing
Dependency trees are representations of the syntac-
tic structure of a sentence (Nivre et al2007). They
determine, for each token of a sentence, the syntac-
tic head the token is modifying. As a lightweight al-
ternative to phrase-based constituency trees, depen-
dency representations have by now seen widespread
use in the community in various domains such as
question answering, machine translation, and infor-
mation extraction.
To simplify further exposition, we now formalize
the task, and mostly follow the notation of Martins et
al. (2009). Consider a sentence x = ?t0, t1, . . . , tn?
where t1, . . . , tn correspond to the n tokens of the
sentence, and t0 is an artificial root token. Let
V , {0, . . . , n} be a set of vertices corresponding
to the tokens in x, and C ? V ?V a set of candidate
directed edges. Then a directed graph y ? C is a
legal dependency parse if and only if it is a tree over
V rooted at vertex 0. Given a sentence x, we use Y
to denote the set of its legal parses. Note that all of
the above definitions depend on x, but for simplicity
we omit this dependency in our notation.
2.1 Arc-Factored Models
Graph-based models define parametrized scoring
functions that are trained to discriminate between
correct and incorrect parse trees. So called arc-
factored or first order models are the most basic
variant of such functions: they assess the quality of a
tree by scoring each edge in isolation (McDonald et
al., 2005b; McDonald et al2005a). Formally, arc-
factored models are scoring functions of the form
s (y;x,w) =
?
?h,m??y
s?h,m? (x,w) (1)
where w is a weight vector and s?h,m? (x,w) scores
the edge ?h,m? with respect to sentence x and
weightsw. From here on we will omit both x andw
from our notation if they are clear from the context.
Given such a scoring function, parsing amounts to
solving:
maximize
y
?
?h,m??y
s?h,m?
subject to y ? Y.
(2)
2.2 Higher Order Models
Arc-factored models cannot capture higher order de-
pendencies between two or more edges. Higher
order models remedy this by introducing scores
for larger configurations of edges appearing in the
733
tree (McDonald and Pereira, 2006). For example,
in grandparent models, the score of a tree also in-
cludes a score sgp?g,p,c? for each grandparent-parent-
child triple ?g, p, c?:
s (y) =
?
?h,m??y
s?h,m? +
?
?g,p??y,?p,c??y
sgp?g,p,c? (3)
There are other variants of higher order models
that include, in addition to grandparent triples, pairs
of siblings (adjacent or not) or third order edges.
However, to illustrate our approach we will focus
on grandparent models and note that most of what
we present can be generalized to other higher order
models.
2.3 Feature Templates
For our later exposition the factored and
parametrized nature of the scoring functions
will be crucial. In the following we therefore
illustrate this property in more detail.
The scoring functions for arcs or higher order
edges usually decompose into a sum of feature tem-
plate scores. For example, the grandparent edge
score sgp?g,p,c? is defined as
sgp?g,p,c? ,
?
t?T gp
sgp,t?g,p,c? (4)
where T gp is the set of grandparent templates, and
each template t ? T gp defines a scoring func-
tion sgp,t?g,p,c? to assess a specific property of the
grandparent-parent-child edge ?g, p, c?.
The template scores again decompose. Consider-
ing grandparent scores, we get
st?g,p,c? , w
>
t f
t (htg, h
t
p, h
t
c, d
t
g,p,c
)
(5)
where hti is an attribute of token ti, say h
101
i =
Part-of-Speech (ti). The term dtg,p,c corresponds to
a representation of the relation between tokens cor-
responding to g, p and g. For example, for template
101 it could return their relative positions to each
other:
d101g,p,c , ?I [g > p] , I [g > c] , I [p > c]? . (6)
The feature function f t maps the representations
of g, p and c into a vector space. For the purposes of
our work this mapping is not important, and hence
we omit details.
2.4 Learning
The scoring functions we consider are parametrized
by a family of per-template weight vectors w =
?wt?t?T . During learning we need to estimate w
such that our scoring functions learns to differenti-
ate between correct and incorrect parse trees. This
can be achieved in many ways: large margin train-
ing, maximizing conditional likelihood, or variants
in between. In this work we follow Smith and Eis-
ner (2008) and train the models with stochastic gra-
dient descent on the conditional log-likelihood of the
training data, using belief propagation in order to
calculate approximate gradients.
3 LP and ILP Formulations
Riedel and Clarke (2006) showed that dependency
parsing can be framed as Integer Linear Pro-
gram (ILP), and efficiently solved using an off-the-
shelf optimizer if a cutting plane approach is used.1
Compared to tailor made dynamic programs, such
generic solvers give the practitioner more modeling
flexibility (Martins et al2009), albeit at the cost
of efficiency. Likewise, compared to approximate
solvers, ILP and Linear Program (LP) formulations
can give strong guarantees of optimality. The study
of Linear LP relaxations of dependency parsing has
also lead to effective alternative methods for parsing,
such as dual decomposition (Koo et al2010; Rush
et al2010). As we see later, the capability of LP
solvers to calculate dual solutions is also crucial for
efficient and exact pruning. Note, however, that dy-
namic programs provide dual solutions as well (see
section 4.5 for more details).
3.1 Arc-Factored Models
To represent a parse y ? Y we first introduce an
vector of variables z , ?za?a where za is 1 if a ? y
and 0 otherwise. With this representation parsing
amounts to finding a vector z that corresponds to a
legal parse tree and that maximizes
?
a zasa. One
way to achieve this is to search through the convex
hull of all legal incidence vectors, knowing that any
linear objectives would take on its maximum on one
of the hull?s vertices. We will use Z to denote this
convex hull of incidence vectors of legal parse trees,
1Such as the highly efficient and free-for-academic-use
Gurobi solver.
734
and callZ the arborescence polytope (Martins et al
2009). The Minkowski-Weyl theorem tells us thatZ
can be represented as an intersection of halfspaces,
or constraints, Z = {z|Az ? b}. Hence optimal
dependency parsing, in theory, can be addressed us-
ing LPs.
However, it is difficult to describe Z with a com-
pact number of constraints and variables that lend
themselves to efficient optimization. In general we
therefore work with relaxations, or outer bounds, on
Z . Such outer bounds are designed to cut off all
illegal integer solutions of the problem, but still al-
low for fractional solutions. In case the optimum is
achieved at an integer vertex of the outer bound, it
is clear that we have found the optimal solution to
the original problem. In case we find a fractional
point, we need to map it onto Z (e.g., by projection
or rounding). Alternatively, we can use the outer
bound together with 0/1 constraints on z, and then
employ an ILP solver (say, branch-and-bound) to
find the true optimum. Given the NP-hardness of
ILP, this will generally be slow.
In the following we will present the outer bound
Z? ? Z proposed by Martins et al2009).
Compared to the representation Riedel and Clarke
(2006), this bound has the benefit a small polyno-
mial number of constraints. Note, however, that of-
ten exponentially many constraints can be efficiently
handled if polynomial separation algorithms exists,
and that such representations can lead to tighter
outer bounds.
The constraints we employ are:
No Head For Root In a dependency tree the root
node never has a head. While this could be captured
through linear constraints, it is easier to simply re-
strict the candidate set C to never contain edges of
the form ??, 0?.
Exactly One Head for Non-Roots Any non-root
token has to have exactly one head token. We can
enforce this property through the set of constraints:
m > 0 :
?
h
z?h,m? = 1. (OneHead)
No Cycles A parse tree cannot have cycles. This is
equivalent, together with the head constraints above,
to enforcing that the tree be fully connected. Mar-
tins et al2009) capture this connectivity constraint
using a single commodity flow formulation. This
requires the introduction of flow variables ? ,
??a?a?C . By enforcing that token 0 has n outgoing
flow, ?
m>0
??0,m? = n, (Source)
that any other token consumes one unit of flow,
t > 0 :
?
h
??h,t? ?
?
m>0
??t,m? = 1 (Consume)
and that flow is zero on disabled arcs
??h,m? ? nz?h,m?, (NoFlow)
connectivity can be ensured.
Assuming we have such a representation, parsing
with an LP relaxation amounts to solving
maximize
z?0
?
a?A
zasa
subject to A
[
z
?
]
? b.
(7)
3.2 Higher Order Models
The 1st-Order LP can be easily extended to capture
second (or higher) order models. For for the case
of grandparent models, this amounts to introduc-
ing another class of variables, zgpg,p,c, that indicate if
the parse contains both the edge ?g, p? and the edge
?p, c?. With the help of the indicators zgp we can rep-
resent the second order objective as a linear function.
We now need an outer bound on the convex hull of
vectors ?z, zgp? where z is a legal parse tree and zgp
is a consistent set of grandparent indicators. We will
refer to this convex hull as the grandparent polytope
Zgp.
We can re-use the constraints A of section 3.1 to
ensure that z is in Z . To make sure zgp is consistent
with z, Martins et al2009) linearize the equiva-
lence zgpg,p,c ? zg,p ? zp,c we know to hold for legal
incidence vectors, yielding
g, p, c : z?g,p? + z?p,c? ? z
gp
?g,p,c? ? 1 (ArcGP)
and
g, p, c : z?g,p? ? z
gp
?g,p,c?, z?p,c? ? z
gp
?g,p,c? (GPArc)
There are additional constraints we know to hold in
Zgp. First, we know that for any active edge ?p, c? ?
735
y with p > 0 there is exactly one grandparent edge
?g, p, c?. Likewise, for an inactive edge ?p, c? /? y
there must be no grandparent edge ?g, p, c?. This
can be captured through the constraint:
p > 0, c :
?
g
zgp?g,p,c? = z?p,c?. (OneGP)
We also know that if an edge ?g, p? in inactive,
there must not be any grandparent edge ?g, p, c? that
goes through ?g, p?:
g, p :
?
c
zgp?g,p,c? ? nz?g,p?. (NoGP)
It can be easily shown that for integer solu-
tions the constraints ArcGP and GPArc of Martins
et al2009) are sufficient conditions for consis-
tency between z and zgp. It can equally be shown
that the same holds for the constraints OneGP and
NoGP. However, when working with LP relax-
ations, the two polytopes have different fractional
vertices. Hence, by combining both constraint sets,
we can get a tighter outer bound on the grandparent
polytope Zgp. In section 6 we show empirically that
this combined polytope in fact leads to fewer frac-
tional solutions. Note that when using the union of
all four types of constraints, the NoGP constraint is
implied by the constraint GPArc (left) by summing
over c on both sides, and can hence be omitted.
4 Parse, Price and Cut
We now introduce our parsing algorithm. To this
end, we first give a general description of column
and row generation for LPs; then, we illustrate how
these techniques can be applied to dependency pars-
ing.
4.1 Column and Row Generation
LPs often have too many variables and constraints
to be efficiently solved. In such cases delayed
column and row generation can substantially re-
duce runtime by lazily adding variables only when
needed (Gilmore and Gomory, 1961; L?bbecke and
Desrosiers, 2004).
To illustrate column and row generation let us
consider the following general primal LP and its cor-
responding dual problem:
Primal
maximize
z?0
s?z
subject to Az ? b
Dual
minimize
??0
??b
subject to A?? ? s.
Say you are given a primal feasible z? and a dual fea-
sible ?? for which complementary slackness holds:
for all variables i we have z?i > 0? si =
?
j ?
?
jai,j
and for all constraints j we have ??j > 0 ? bj =?
i z
?
iai,j . In this case it is easy to show that z
? is
an optimal primal solution, ?? and optimal dual so-
lution, and that both objectives meet at these val-
ues (Bertsekas, 1999).
The idea behind delayed column and row gener-
ation is to only consider a small subset of variables
(or columns) I and subset of constraints (or rows) J .
Optimizing over this restricted problem, either with
an off-the-shelf solver or a more specialized method,
yields the pair
(
z?I ,?
?
J
)
of partial primal and dual
solutions. This pair is feasible and complementary
with respect to variables I and constraints J . We
can extend it to a solution (z?,y?) over all variables
and constraints by heuristically setting the remain-
ing primal and dual variables. If it so happens that
(z?,y?) is feasible and complementary for all vari-
ables and constraints, we have found the optimal so-
lution. If not, we add the constraints and variables
for which feasibility and slackness are violated, and
resolve the new partial problem.
In practice, the uninstantiated primal and dual
variables are often set to 0. In this case complemen-
tary slackness holds trivially, and we only need to
find violated primal and dual constraints. For primal
constraints,
?
i ziai,j ? bi, searching for violating
constraints j is the well-known separation step in
cutting plane algorithms. For the dual constraints,
?
j ?jai,j ? si, the same problem is referred to
as pricing. Pricing is often framed as searching for
all, or some, variables i with positive reduced cost
ri , si?
?
j ?jai,j . Note that while these problems
are, naturally, dual to each other, they can have very
different flavors. When we assess dual constraints
we need to calculate a cost si for variable i, and
usually this cost would be different for different i.
For primal constraints the corresponding right-hand-
sides are usually much more homogenous.
736
Algorithm 1 Parse, Price and Cut.
Require: Initial candidate edges and hyperedges P .
Ensure: The optimal z.
1: repeat
2: z,? ? parse(P )
3: N ? price(?)
4: M ? cut(z)
5: P ? P ?N ?M
6: until N = ? ?M = ?
7: return z
The reduced cost ri = si ?
?
j ?jai,j has sev-
eral interesting interpretations. First, intuitively it
measures the score we could gain by setting zi = 1,
and subtracts an estimate of what we would loose
because zi = 1 may compete with other variables
for shared resources (constraints). Second, it cor-
responds to the coefficient of zi in the Lagrangian
L (?, z) , s?z + ? [b?Az]. For any ?, Uzi=k =
maxz?0,zi=k L (?, z) is an upper bound on the best
possible primal objective with zi = k. This means
that ri = Uzi=1 ? Uzi=0 is the difference between
an upper bound that considers zi = 1, and one that
considers zi = 0. The tighter the bound Uzi=0 is,
the closer ri is to an upper bound on the maximal
increase we can get for setting zi to 1. At conver-
gence of column generation, complementary slack-
ness guarantees that Uzi=0 is tight for all z
?
i = 0, and
hence ri is a true an upper bound.
4.2 Application to Dependency Parsing
The grandparent formulation in section 3.2 has a cu-
bic number of variables z?g,p,c? as well as a cubic
number of constraints. For longer sentences this
number can slow us down in two ways. First, the
optimizer works with a large search space, and will
naturally become slower. Second, for every grand-
parent edge we need to calculate the score s?g,p,c?,
and this calculation can often be a major bottleneck,
in particular when using complex feature functions.
To overcome this bottleneck, our parse, price and cut
algorithm, as shown in algorithm 1, uses column and
row generation. In particular, it lazily instantiates
the grandparent edge variables zgp?g,p,c?, and the cor-
responding cubic number of constraints. All unin-
stantiated variables are implicitly set to 0.
The algorithm requires some initial set of vari-
ables to start with. In our case this set P contains all
first-order edges ?h,m? in the candidate set C, and
for each of these one grandparent edge ?0, h,m?.
The primary purpose of these grandparent edges is
to ensure feasibility of the OneGP constraints.
In step 2, the algorithm parses with the current
set of candidates P by solving the corresponding LP
relaxation. The LP contains all columns and con-
straints that involve the edges and grandparent edges
of P . The solver returns both the best primal solu-
tion z (for both edges and grandparents), and a com-
plementary dual solution ?.
In step 3 the dual variables? are used to find unin-
stantiated grandparent edges ?g, p, c? with positive
reduced cost. The price routine returns such edges
in N . In step 4 the primal solution is inspected for
violations of constraint ArcGP. The cut routine per-
forms this operation, and returns M , the set of edges
?g, p, c? that violate ArcGP.
In step 5 the algorithm converges if no more con-
straint violations, or promising new columns, can
be found. If there have been violations (M 6= ?)
or promising columns (N 6= ?), steps 2 to 4 are
repeated, with the newly found parts added to the
problem. Note that LP solvers can be efficiently
warm-started after columns and rows have been
added, and hence the cost of calls to the solver in
step 2 is substantially reduced after the first itera-
tion.
4.3 Pricing
In the pricing step we need to efficiently find a
set of grandparent edge variables zgp?g,p,c? with posi-
tive reduced cost, or the empty set if no such vari-
ables exist. Let ?OneGP?p,c? be the dual variables for
the OneGP constraints and ?NoGP?g,p? the duals for con-
straints NoGP. Then for the reduced cost of zgp?g,p,c?
we know that:
r?g,p,c? = s?g,p,c? ? ?
OneGP
?p,c? ? ?
NoGP
?g,p? . (8)
Notice that the duals for the remaining two con-
straints ArcGP and GPArc do not appear in this
equation. This is valid because we can safely set
their duals to zero without violating dual feasibility
or complementary slackness of the solution returned
by the solver.
737
4.3.1 Upper Bounds for Efficient Pricing
A naive pricing implementation would exhaus-
tively iterate over all ?g, p, c? and evaluate r?g,p,c?
for each. In this case we can still substantially re-
duce the number of grandparent variables that en-
ter the LP, provided many of these variables have
non-positive reduced cost. However, we still need to
calculate the score s?g,p,c? for each ?g, p, c?, an ex-
pensive operation we hope to avoid. In the follow-
ing we present an upper bound on the reduced cost,
r?gp?g,p,c? ? r
gp
?g,p,c?, which decomposes in a way that
allows for more efficient search. Using this bound,
we find all new grandparent edges N? for which this
upper bound is positive:
N? ?
{
?g, p, c? |r?gp?g,p,c? > 0
}
. (9)
Next we prune away all but the grandparent edges
for which the exact reduced cost is positive:
N ? N? \ {e : rgpe > 0} . (10)
Our bound r?gp?g,p,c? on the reduced cost of ?g, p, c?
is based on an upper bound s?gp?g,p,?? ? maxc s
gp
?g,p,c?
on the grandparent score involving ?g, p? as grand-
parent and parent, and the bound s?gp??,p,c? ?
maxg s
gp
?g,p,c? on the grandparent score involving
?p, c? as parent and child. Concretely, we have
r?gp?g,p,c? , min
(
s?gp?g,p,??, s?
gp
??,p,c?
)
? ?OneGP?p,c? ? ?
NoGP
?g,p? .
(11)
To find edges ?g, p, c? for which this bound is
positive, we can filter out all edges ?p, c? such that
sgp??,p,c?? ?
OneGP
?p,c? is non-positive. This is possible be-
cause NoGP is a? constraint and therefore ?NoGP?g,p? ?
0.2 Hence r?gp?g,p,c? is at most s?
gp
??,p,c? ? ?
OneGP
?p,c? . This
filtering step cuts off a substantial number of edges,
and is the main reason why can avoid scoring all
edges.
Next we filter, for each remaining ?p, c?, all pos-
sible grandparents g according to the definition of
r?gp?g,p,c?. This again allows us to avoid calling the
2Notice that in section 4.1 we discussed the LP dual in
case were all constraints are inequalities. When equality con-
straints are used, the corresponding dual variables have no sign
constraints. Hence we could not make the same argument for
?OneGP?p,c? .
grandparent scoring function on ?g, p, c?, and yields
the candidate set N? . Only if r?gp?g,p,c? is positive do we
have to evaluate the exact reduced cost and score.
4.3.2 Upper Bounds on Scores
What remains to be done is the calculation of up-
per bounds s?gp?g,p,?? and s?
gp
??,p,c?. Our bounds factor
into per-template bounds according to the definitions
in section 2.3. In particular, we have
s?gp??,p,c? ,
?
t?T gp
s?gp,t??,p,c? (12)
where s?t??,p,c? is a per-template upper bound defined
as
s?gp,t??,p,c? , max
v?range(ht)
e?range
`
dt
?
w>t f
t (v, htp, h
t
c, e
)
. (13)
That is, we maximize over all possible attribute val-
ues v any token g could have, and any possible rela-
tion e a token g can have to p and c.
Notice that these bounds can be calculated offline,
and hence amortize after deployment of the parser.
4.3.3 Tightening Duals
To price variables, we use the duals returned by
the solver. This is a valid default strategy, but may
lead to ? with overcautious reduced costs. Note,
however, that we can arbitrary alter ? to minimize
reduced costs of uninstantiated variables, as long as
we ensure that feasibility and complementary slack-
ness are maintained for the instantiated problem.
We use this flexibility for increasing ?OneGP?p,c? , and
hence lowering reduced costs zgp?g,p,c? for all tokens c.
Assume that z?p,c? = 0 and let r?p,c? = ?
OneGP
?p,c? + K
be the current reduced cost for z?p,c? in the instanti-
ated problem. Here K is a value depending on s?p,c?
and the remaining constraints z?p,c? is involved in.
We know that r?p,c? ? 0 due to dual feasibility
and hence r?p,c? may be 0, but note that r?p,c? < 0 in
many cases. In such cases we can increase ?OneGP?p,c?
to ?K and get r?p,c? = 0. With respect to z?p,c? this
maintains dual feasibility (because r?p,c? ? 0) and
complementary slackness (because z?p,c? = 0). Fur-
thermore, with respect to the zgp?g,p,c? for all tokens c
this also maintains feasibility (because the increased
?OneGP?p,c? appears with negative sign in 8) and com-
plementary slackness (because zgp?g,p,c? = 0 due to
z?p,c? = 0).
738
4.4 Separation
What happens if both z?g,p? and z?p,c? are active
while zgp?g,p,c? is still implicitly set to 0? In this case
we violate constraint ArcGP. We could remedy this
by adding the cut z?g,p? + z?p,c? ? 1, resolve the
LP, and then use the dual variable corresponding to
this constraint to get an updated reduced cost r?g,p,c?.
However, in practice we found this does not happen
as often, and when it does, it is cheaper for us to add
the corresponding column r?g,p,c? right away instead
of waiting to the next iteration to price it.
To find all pairs of variables for z?g,p? + z?p,c? ? 1
is violated, we first filter out all edges ?h,m? for
which z?h,m? = 0 as these automatically satisfy
any ArcGP constraint they appear in. Now for each
z?g,p? > 0 all z?p,c? > 0 are found, and if their sum
is larger than 1, the corresponding grandparent edge
?g, p, c? is returned in the result set.
4.5 Column Generation in Dynamic Programs
Column and Row Generation can substantially re-
duce the runtime of an off-the-shelf LP solver, as
we will find in section 6. Perhaps somewhat sur-
prisingly, it can also be applied in the context of dy-
namic programs. It is well known that for each dy-
namic program there is an equivalent polynomial LP
formulation (Martin et al1990). Roughly speak-
ing, in this formulation primal variables correspond
to state transitions, and dual variables to value func-
tions (e.g., the forward scores in the Viterbi algo-
rithm).
In pilot studies we have already used DCG to
speed up (exact) Viterbi on linear chains (Belanger
et al2012). We believe it could be equally applied
to dynamic programs for higher order dependency
parsing.
5 Related Work
Our work is most similar in spirit to the relaxation
method presented by Riedel and Smith (2010) that
incrementally adds second order edges to a graphi-
cal model based on a gain measure?the analog of
our reduced cost. However, they always score every
higher order edge, and also provide no certificates of
optimality.
Several works in parsing, and in MAP inference
in general, perform some variant of row genera-
tion (Riedel and Clarke, 2006; Tromble and Eis-
ner, 2006; Sontag and Jaakkola, 2007; Sontag et al
2008). However, none of the corresponding methods
lazily add columns, too. The cutting plane method
of Riedel (2008) can omit columns, but only if their
coefficient is negative. By using the notion of re-
duced costs we can also omit columns with positive
coefficient. Niepert (2010) applies column gener-
ation, but his method is limited to the case of k-
Bounded MAP Inference.
Several ILP and LP formulations of dependency
parsing have been proposed. Our formulation is in-
spired by Martins et al2009), and hence uses fewer
constraints than Riedel and Clarke (2006). For the
case of grandparent edges, our formulation also im-
proves upon the outer bound of Martins et al2009)
in terms of speed, tightness, and utility for column
generation. Other recent LP relaxations are based
on dual decomposition (Rush et al2010; Koo et
al., 2010; Martins et al2011). These relaxations
allow the practitioner to utilize tailor-made dynamic
programs for tractable substructure, but still every
edge needs to be scored. Given that column gener-
ation can also be applied in dynamic programs (see
section 4.5), our algorithm could in fact accelerate
dual decomposition parsing as well.
Pruning methods are a major part of many struc-
tured prediction algorithms in general, and of pars-
ing algorithms in particular (Charniak and Johnson,
2005; Martins et al2009; Koo and Collins, 2010;
Rush and Petrov, 2012). Generally these meth-
ods follow a coarse-to-fine scheme in which sim-
pler models filter out large fractions of edges. Such
methods are effective, but require tuning of thresh-
old parameters, training of additional models, and
generally lead to more complex pipelines that are
harder to analyze and have fewer theoretical guar-
antees.
A* search (Ahuja et al1993) has been used
to search for optimal parse trees, for example by
Klein and Manning (2003) or, for dependency pars-
ing, by Dienes et al2003). There is a direct rela-
tion between both A* and Column Generation based
on an LP formulation of the shortest path problem.
Roughly speaking, in this formulation any feasible
dual assignments correspond to a consistent (and
thus admissible) heuristic, and the corresponding re-
duced costs can be used as edge weights. Run-
739
ning Dijkstra?s algorithm with these weights then
amounts to A*. Column generation for the shortest
path problem can then be understood as a method to
lazily construct a consistent heuristic. In every step
this method finds edges for which consistency is vi-
olated, and updates the heuristic such that all these
edges are consistent.
6 Experiments
We claim that LP relaxations for higher order pars-
ing can be solved without considering, and scoring,
all candidate higher order edges. In practice, how
many grandparent edges do we need to score, and
how many do we need to add to the optimization
problem? And what kind of reduction in runtime
does this reduction in edges lead to?
We have also pointed out that our outer bound on
the grandparent polytope of legal edge and grand-
parent vectors is tighter than the one presented by
Martins et al2009). What effect does this bound
have on the number of fractional solutions and the
overall accuracy?
To answer these questions we will focus on a set
of non-projective grandparent models, but point out
that our method and formulation can be easily ex-
tended to projective parsing as well as other types
of higher order edges. We use the Danish test data
of Buchholz and Marsi (2006) and the Italian and
Hungarian test datasets of Nivre et al2007).
6.1 Impact of Price and Cut
Table 1 compares brute force optimization (BF) with
the full model, in spirit of Martins et al2009),
to running parse, price and cut (PPC) on the same
model. This model contains all constraints presented
in 3.2. The table shows the average number of
parsed sentences per second, the average objective,
number of grandparent edges scored and added, all
relative to the brute force approach. We also present
the average unlabeled accuracy, and the percentage
of sentences with integer solutions. This number
shows us how often we not only found the optimal
solution to the LP relaxation, but also the optimal
solution to the full ILP.
We first note that both systems achieve the same
objective, and therefore, also the same accuracy.
This is expected, given that column and row gen-
eration are known to yield optimal solutions. Next
we see that the number of grandparent edges scored
and added to the problem is reduced to 5?13% of the
full model. This leads to up to 760% improvement
in speed. This improvement comes for free, without
any sacrifice in optimality or guarantees. We also
notice that in all cases at least 97% of the sentences
have no fractional solutions, and are therefore opti-
mal even with respect to the ILP. Table 1 also shows
that our bounds on reduced costs are relatively tight.
For example, in the case of Italian we score only
one percent more grandparent edges than we actu-
ally need to add.
Our fastest PCC parser processes about one sen-
tence per second. This speed falls below the reported
numbers of Martins et al2009) of about 0.6 sec-
onds per sentence. Crucially, however, in contrast to
their work, our speed is achieved without any first-
order pruning. In addition, we expect further im-
provements in runtime by optimizing the implemen-
tation of our pricing algorithm.
6.2 Tighter Grandparent Polytope
To investigate how the additional grandparent con-
straints in section 3.2 help, we compare three mod-
els, this time without PPC. The first model follows
Martins et al2009) and uses constraints ArcGP and
GPArc only. The second model uses only constraints
OneGP and NoGP. The final model incorporates all
four constraints.
Table 2 shows speed relative to the baseline model
with constraints ArcGP and GPArc, as well as the
percentage of integer solutions and the average un-
labeled accuracy?all for the Italian and Hungarian
datasets. We notice that the full model has less frac-
tional solutions than the partial models, and either
substantially (Italian) or slightly (Hungarian) faster
runtimes than ArcGP+GPArc. Interestingly, both
sets of constraints in isolation perform worse, in par-
ticular the OneGP and NoGP model.
7 Conclusion
We have presented a novel method for parsing in
second order grandparent models, and a general
blueprint for more efficient and optimal structured
prediction. Our method lazily instantiates candidate
parts based on their reduced cost, and on constraint
740
Italian Hungarian Danish
BF PPC BF PPC BF PPC
Sent./sec. relative to BF 100% 760% 100% 380% 100% 390%
GPs Scored relative to BF 100% 6% 100% 12% 100% 13%
GPs Added relative to BF 100% 5% 100% 7% 100% 7%
Objective rel. to BF 100% 100% 100% 100% 100% 100%
% of Integer Solutions 98% 98% 97% 97% 97% 97%
Unlabeled Acc. 88% 88% 81% 81% 88% 88%
Table 1: Parse, Price and Cut (PPC) vs Brute Force (BF). Speed is the number of sentences per second,
relative to the speed of BF. Objective, GPs scored and added are also relative to BF.
GPArc+ OneGP+
Constraints ArcGP NoGP All
Sent./sec. 100% 1000% 1200%
% Integer 77% 9% 98%
Unlabeled Acc. 87% 85% 88%
(a) Italian
GPArc+ OneGP+
Constraints ArcGP NoGP All
Sent./sec. 100% 162% 105%
% Integer 71% 3% 97%
Unlabeled Acc. 80% 77% 81%
(b) Hungarian
Table 2: Different outer bounds on the grandpar-
ent polytope, for nonprojective parsing of Italian and
Danish.
violations. This allows us to discard a large fraction
of parts during both scoring and optimization, lead-
ing to nearly 800% speed-ups without loss of accu-
racy and certificates. We also present a tighter bound
on the grandparent polytope that is useful in its own
right.
Delayed column and row generation is very useful
when solving large LPs with off-the-shelf solvers.
Given the multitude of work in NLP that uses LPs
and ILPs in this way (Roth and Yih, 2004; Clarke
and Lapata, 2007), we hope that our approach will
prove itself useful for other applications. We stress
that this approach can also be used when working
with dynamic programs, as pointed out in section
4.5, and therefore also in the context of dual de-
composition. This suggests even wider applicabil-
ity, and usefulness in various structured prediction
problems.
The underlying paradigm could also be useful for
more approximate methods. In this paradigm, al-
gorithms maintain an estimate of the cost of certain
resources (duals), and use these estimates to guide
search and the propose new structures. For exam-
ple, a local-search based dependency parser could
estimate how contested certain tokens, or edges, are,
and then use these estimates to choose better next
proposals. The notion of reduced cost can give guid-
ance on what such estimates should look like.
Acknowledgements
This work was supported in part by the Center for
Intelligent Information Retrieval and the Univer-
sity of Massachusetts and in part by UPenn NSF
medium IIS-0803847. We gratefully acknowledge
the support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0181. Any opinions, find-
ings, and conclusion or recommendations expressed
in this material are those of the authors and do not
necessarily reflect the view of DARPA, AFRL, or
the US government.
References
Ravindra K. Ahuja, Thomas L. Magnanti, and James B.
Orlin. 1993. Network Flows: Theory, Algorithms, and
Applications. Prentice Hall, 1 edition, February.
David Belanger, Alexandre Passos, Sebastian Riedel, and
Andrew McCallum. 2012. A column generation ap-
proach to connecting regularization and map infer-
ence. In Inferning: Interactions between Inference
and Learning, ICML 2012 Workshop.
741
Dimitri P. Bertsekas. 1999. Nonlinear Programming.
Athena Scientific, 2nd edition, September.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the 10th Conference on Computational Natu-
ral Language Learning (CoNLL? 06), CoNLL-X ?06,
pages 149?164, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL ?05),
pages 173?180.
James Clarke and Mirella Lapata. 2007. Modelling
compression with discourse constraints. In Proceed-
ings of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-CoNLL
?07), pages 1?11.
A. Culotta, M. Wick, R. Hall, and A. McCallum. 2007.
First-order probabilistic models for coreference reso-
lution. In Joint Human Language Technology Con-
ference/Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL ?07), pages 81?88.
Peter Dienes, Alexander Koller, and Marco Kuhlmann.
2003. Statistical a-star dependency parsing. In Pro-
ceedings of the workshop on Prospects and Advances
of the Syntax/Semantics Interface, Nancy, 2003, pp.85-
89.
P.C. Gilmore and R.E. Gomory. 1961. A linear program-
ming approach to the cutting-stock problem. Opera-
tions research, pages 849?859.
Dan Klein and Christopher D. Manning. 2003. A* pars-
ing: Fast exact viterbi parse selection. In Proceedings
of the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL ?03), pages 119?126.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics (ACL ?11).
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with nonprojective head automata. In
Proceedings of the Conference on Empirical methods
in natural language processing (EMNLP ?10).
Sandra K?bler, Ryan T. McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures on
Human Language Technologies. Morgan & Claypool
Publishers.
Marco L?bbecke and Jacques Desrosiers. 2004. Selected
topics in column generation. Operations Research,
53:1007?1023.
R. Kipp Martin, Ronald L. Rardin, and Brian A. Camp-
bell. 1990. Polyhedral characterization of discrete
dynamic programming. Oper. Res., 38(1):127?138,
February.
Andr? F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP (ACL
?09), pages 342?350, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Andr? F. T. Martins, Noah A. Smith, Pedro M. Q. Aguiar,
and M?rio A. T. Figueiredo. 2011. Dual decomposi-
tion with many overlapping components. In Proceed-
ings of the Conference on Empirical methods in natu-
ral language processing (EMNLP ?11), EMNLP ?11,
pages 238?249, Stroudsburg, PA, USA. Association
for Computational Linguistics.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proceedings of the 11th Conference of the European
Chapter of the ACL (EACL ?06), pages 81?88.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ?05), pages
91?98.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In HLT-EMNLP, 2005.
Mathias Niepert. 2010. A delayed column generation
strategy for exact k-bounded map inference in markov
logic networks. In Proceedings of the 26th Annual
Conference on Uncertainty in AI (UAI ?10), pages
384?391, Corvallis, Oregon. AUAI Press.
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The conll 2007 shared
task on dependency parsing. In Conference on Em-
pirical Methods in Natural Language Processing and
Natural Language Learning, pages 915?932.
Hoifung Poon and Pedro Domingos. 2007. Joint infer-
ence in information extraction. In Proceedings of the
22nd AAAI Conference on Artificial Intelligence (AAAI
?07), pages 913?918.
Sebastian Riedel and James Clarke. 2006. Incremen-
tal integer linear programming for non-projective de-
pendency parsing. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ?06), pages 129?137.
Sebastian Riedel and David A. Smith. 2010. Relaxed
marginal inference and its application to dependency
742
parsing. In Joint Human Language Technology Con-
ference/Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL ?10), pages 760?768, Los Angeles, Cal-
ifornia, June. Association for Computational Linguis-
tics.
Sebastian Riedel. 2008. Improving the accuracy and ef-
ficiency of MAP inference for markov logic. In Pro-
ceedings of the 24th Annual Conference on Uncer-
tainty in AI (UAI ?08), pages 468?475.
D. Roth andW. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Proceedings of the 8th Conference on Computational
Natural Language Learning (CoNLL? 04), pages 1?8.
Alexander Rush and Slav Petrov. 2012. Vine pruning for
efficient multi-pass dependency parsing. In Joint Hu-
man Language Technology Conference/Annual Meet-
ing of the North American Chapter of the Association
for Computational Linguistics (HLT-NAACL ?12).
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition
and linear programming relaxations for natural lan-
guage processing. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ?10).
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 145?156, Hon-
olulu, October.
D. Sontag and T. Jaakkola. 2007. New outer bounds on
the marginal polytope. In Advances in Neural Infor-
mation Processing Systems (NIPS ?07), pages 1393?
1400.
David Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and
Y. Weiss. 2008. Tightening LP relaxations for MAP
using message passing. In Proceedings of the 24th An-
nual Conference on Uncertainty in AI (UAI ?08).
Roy W. Tromble and Jason Eisner. 2006. A fast
finite-state relaxation method for enforcing global con-
straints on sequence decoding. In Joint Human Lan-
guage Technology Conference/Annual Meeting of the
North American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL ?06), pages 423?
430.
743
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1104?1113, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Monte Carlo MCMC: Efficient Inference by Approximate Sampling
Sameer Singh
University of Massachusetts
140 Governor?s Drive
Amherst MA
sameer@cs.umass.edu
Michael Wick
University of Massachsetts
140 Governor?s Drive
Amherst, MA
mwick@cs.umass.edu
Andrew McCallum
University of Massachusetts
140 Governor?s Drive
Amherst MA
mccallum@cs.umass.edu
Abstract
Conditional random fields and other graphi-
cal models have achieved state of the art re-
sults in a variety of tasks such as coreference,
relation extraction, data integration, and pars-
ing. Increasingly, practitioners are using mod-
els with more complex structure?higher tree-
width, larger fan-out, more features, and more
data?rendering even approximate inference
methods such as MCMC inefficient. In this
paper we propose an alternative MCMC sam-
pling scheme in which transition probabilities
are approximated by sampling from the set
of relevant factors. We demonstrate that our
method converges more quickly than a tradi-
tional MCMC sampler for both marginal and
MAP inference. In an author coreference task
with over 5 million mentions, we achieve a 13
times speedup over regular MCMC inference.
1 Introduction
Conditional random fields and other graphical mod-
els are at the forefront of many natural language
processing (NLP) and information extraction (IE)
tasks because they provide a framework for discrim-
inative modeling while succinctly representing de-
pendencies among many related output variables.
Previously, most applications of graphical models
were limited to structures where exact inference is
possible, for example linear-chain CRFs (Lafferty
et al 2001). More recently, there has been a de-
sire to include more factors, longer range depen-
dencies, and more sophisticated features; these in-
clude skip-chain CRFs for named entity recogni-
tion (Sutton and McCallum, 2004), probabilistic
DBs (Wick et al 2010), higher-order models for
dependency parsing (Carreras, 2007), entity-wise
models for coreference (Culotta et al 2007; Wick
et al 2009), and global models of relations (Hoff-
mann et al 2011). The increasing sophistication of
these individual NLP components compounded with
the community?s desire to model these tasks jointly
across cross-document considerations has resulted
in graphical models for which inference is compu-
tationally intractable. Even popular approximate in-
ference techniques such as loopy belief propagation
and Markov chain Monte Carlo (MCMC) may be
prohibitively slow.
MCMC algorithms such as Metropolis-Hastings
are usually efficient for graphical models because
the only factors needed to score a proposal are those
touching the changed variables. However, MCMC
is slowed in situations where a) the model exhibits
variables that have a high-degree (neighbor many
factors), b) proposals modify a substantial subset of
the variables to satisfy domain constraints (such as
transitivity in coreference), or c) evaluating a single
factor is expensive, for example when features are
based on string-similarity. For example, the seem-
ingly innocuous proposal changing the entity type of
a single entity requires examining all its mentions,
i.e. scoring a linear number of factors (in the num-
ber of mentions of that entity). Similarly, evaluating
coreference of a mention to an entity also requires
scoring factors to all the mentions of the entity. Of-
ten, however, the factors are somewhat redundant,
for example, not all mentions of the ?USA? entity
need to be examined to confidently conclude that it
is a COUNTRY, or that it is coreferent with ?United
1104
States of America?.
In this paper we propose an approximate MCMC
framework that facilitates efficient inference in high-
degree graphical models. In particular, we approx-
imate the acceptance ratio in the Metropolis Hast-
ings algorithm by replacing the exact model score
with a stochastic approximation that samples from
the set of relevant factors. We explore two sampling
strategies, a fixed proportion approach that samples
the factors uniformly, and a dynamic alternative that
samples factors until the method is confident about
its estimate of the model score.
We evaluate our method empirically on both syn-
thetic and real-world data. On synthetic classi-
fication data, our approximate MCMC procedure
obtains the true marginals faster than a traditional
MCMC sampler. On real-world tasks, our method
achieves 7 times speedup on citation matching, and
13 times speedup on large-scale author disambigua-
tion.
2 Background
2.1 Graphical Models
Factor graphs (Kschischang et al 2001) succinctly
represent the joint distribution over random vari-
ables by a product of factors that make the depen-
dencies between the random variables explicit. A
factor graph is a bipartite graph between the vari-
ables and factors, where each (log) factor f ? F is
a function that maps an assignment of its neighbor-
ing variables to a real number. For example, in a
linear-chain model of part-of-speech tagging, transi-
tion factors score compatibilities between consecu-
tive labels, while emission factors score compatibil-
ities between a label and its observed token.
The probability distribution expressed by the fac-
tor graph is given as a normalized product of the fac-
tors, which we rewrite as an exponentiated sum:
p(y) =
exp?(y)
Z
(1)
?(y) =
?
f?F
f(yf ) (2)
Z =
?
y?Y
exp?(y) (3)
Intuitively, the model favors assignments to the ran-
dom variables that yield higher factor scores and will
assign higher probabilities to such configurations.
The two common inference problems for graphi-
cal models in NLP are maximum a posterior (MAP)
and marginal inference. For models without latent
variables, the MAP estimate is the setting to the
variables that has the highest probability under the
model:
yMAP = argmax
y
p(y) (4)
Marginal inference is the problem of finding
marginal distributions over subsets of the variables,
used primarily in maximum likelihood gradients and
for max marginal inference.
2.2 Markov chain Monte Carlo (MCMC)
Often, computing marginal estimates of a model is
computationally intractable due to the normalization
constant Z, while maximum a posteriori (MAP) is
prohibitive due to the search space of possible con-
figurations. Markov chain Monte Carlo (MCMC) is
important tool for performing sample- and search-
based inference in these models. A particularly suc-
cessful MCMC method for graphical model infer-
ence is Metropolis-Hastings (MH). Since sampling
from the true model p(y) is intractable, MH instead
uses a simpler distribution q(y?|y) that conditions
on a current state y and proposes a new state y? by
modifying a few variables. This new assignment is
then accepted with probability ?:
? = min
(
1,
p(y?)
p(y)
q(y|y?)
q(y?|y)
)
(5)
Computing this acceptance probability is often
highly efficient because the partition function can-
cels, as do all the factors in the model that do not
neighbor the modified variables. MH can be used
for both MAP and marginal inference.
2.2.1 Marginal Inference
To compute marginals with MH, the variables are
initialized to an arbitrary assignment (i.e., randomly
or with some heuristic), and sampling is run until the
samples {yi|i = 0, ? ? ? , n} become independent of
the initial assignment. The ergodic theorem provides
the MCMC analog to the law-of-large-numbers, jus-
tifying the use of the generated samples to compute
the desired statistics (such as feature expectations or
variable marginals).
1105
2.2.2 MAP Inference
Since MCMC can efficiently explore the high
density regions for a given distribution, the distri-
bution p can be modified such that the high-density
region of the new distribution represents the MAP
configuration of p. This is achieved by adding a tem-
perature term ? to the distribution p, resulting in the
following MH acceptance probability:
? = min
(
1,
(
p(y?)
p(y)
) 1
?
)
(6)
Note that as ? ? 0, MH will sample closer to the
MAP configuration. If a cooling schedule is imple-
mented for ? then the MH sampler for MAP infer-
ence can be seen as an instance of simulated anneal-
ing (Bertsimas and Tsitsiklis, 1993).
3 Monte Carlo MCMC
In this section we introduce our approach for ap-
proximating the acceptance ratio of Metropolis-
Hastings that samples the factors, and describe two
sampling strategies.
3.1 Stochastic Proposal Evaluation
Although one of the benefits of MCMC lies in its
ability to leverage the locality of the proposal, for
some information extraction tasks this can become a
crucial bottleneck. In particular, evaluation of each
sample requires computing the score of all the fac-
tors that are involved in the change, i.e. all fac-
tors that neighbor any variable in the set that has
changed. This evaluation becomes a bottleneck for
tasks in which a large number of variables is in-
volved in each proposal, or in which the model con-
tains a number of high-degree variables, resulting in
a large number of factors, or in which computing
the factor score involves an expensive computation,
such as string similarity between mention text.
Instead of evaluating the log-score ? of the model
exactly, this paper proposes a Monte-Carlo estima-
tion of the log-score. In particular, if the set of fac-
tors for a given proposal y? y? is F(y,y?), we use
a sampled subset of the factors S ? F(y,y?) as an
approximation of the model score. In the following
we use F as an abbreviation for F(y,y?). Formally,
?(y) =
?
f?F
f(yf ) = |F| ? EF [f(yf )]
?S(y) = |F| ? ES [f(yf )] (7)
We use the sample log-score (?S) in the acceptance
probability ? to evaluate the samples. Since we are
using a stochastic approximation to the model score,
in general we need to take more MCMC samples
before we converge, however, since evaluating each
sample will be much faster (O(|S|) as opposed to
O(|F|)), we expect overall sampling to be faster.
In the next sections we describe several alternative
strategies for sampling the set of factors S. The pri-
mary restriction on the set of samples S is that their
mean should be an unbiased estimator ofEF[f ]. Fur-
ther, time taken to obtain the set of samples should
be negligible when compared to scoring all the fac-
tors in F. Note that there is an implicit minimum of
1 to the number of the sampled factors.
3.2 Uniform Sampling
The most direct approach for subsampling the set
of F is to perform uniform sampling. In particular,
given a proportion parameter 0 < p ? 1, we select a
random subset Sp ? F such that |Sp| = p ? |F|. Since
this approach is agnostic as to the actual factors
scores, ES[f ] ? EF[f ]. A low p leads to fast evalua-
tion, however it may require a large number of sam-
ples due to the substantial approximation. On the
other hand, although a higher p will converge with
fewer samples, evaluating each sample is slower.
3.3 Confidence-Based Sampling
Selecting the best value for p is difficult, requiring
analysis of the graph structure, and statistics on the
distribution of the factors scores; often a difficult
task in real-world applications. Further, the same
value for p can result in different levels of approxi-
mation for different proposals, either unnecessarily
accurate or problematically noisy. We would prefer
a strategy that adapts to the distribution of the scores
in F.
Instead of sampling a fixed proportion of factors,
we can sample until we are confident that the cur-
rent set of samples Sc is an accurate estimate of the
true mean of F. In particular, we maintain a run-
ning count of the sample mean ESc [f ] and variance
1106
?Sc , using them to compute a confidence interval IS
around our estimate of the mean. Since the num-
ber of sampled factors S could be a substantial frac-
tion of the set of factors F,1 we also incorporate fi-
nite population control (fpc) in our sample variance
computation. We compute the confidence interval as
follows:
?2S =
1
|S| ? 1
?
f?S
(f ? ES [f ])
2 (8)
IS = 2z
?S
?
|S|
?
|F| ? |S|
|F| ? 1
(9)
where we set the z to 1.96, i.e. the 95% confidence
interval. This approach starts with an empty set of
samples, S = {}, and iteratively samples factors
without replacement to add to S, until the confidence
interval around the estimated mean falls below a user
specified maximum interval width threshold i. As a
result, for proposals that contain high-variance fac-
tors, this strategy examines a large number of fac-
tors, while proposals that involve similar factors will
result in fewer samples. Note that this user-specified
threshold is agnostic to the graph structure and the
number of factors, and instead directly reflects the
score distribution of the relevant factors.
4 Experiments
In this section we evaluate our approach for both
marginal and MAP inference.
4.1 Marginal Inference on Synthetic Data
Consider the task of classifying entities into a set of
types, for example, POLITICIAN, VEHICLE, CITY,
GOVERMENT-ORG, etc. For knowledge base con-
struction, this prediction often takes place on the
entity-level, as opposed to the mention-level com-
mon in traditional NLP. To evaluate the type at the
entity-level, the scored factors examine features of
all the entity mentions of the entity, along with the
labels of all relation mentions for which it is an ar-
gument. See Yao et al(2010) and Hoffmann et al
(2011) for examples of such models. Since a sub-
set of the mentions can be sufficiently informative
for the model, we expect our stochastic MCMC ap-
proach to work well.
1Specifically, the fraction may be higher than > 5%
Label
(a) Binary Classification
Model (n = 100)
-4.8 -4 -3.2 -2.4 -1.6 -0.8 0 0.8 1.6 2.4 3.2 4 4.8 5.6 6.4 7.2
-0.4
-0.3
-0.2
-0.1
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Label 1Label 0
(b) Distribution of Factor scores
Figure 1: Synthetic Model for Classification
1 0 2 0 3 0 100 200 1000 10000 100000 1000000Number of Factors Examined
0.0000.025
0.0500.075
0.1000.125
0.1500.175
0.2000.225
0.2500.275
0.3000.325
0.3500.375
0.4000.425
0.450
Erro
r in M
argin
al
p:1. p:0.75 p:0.5 p:0.2p:0.1 i:0.1 i:0.05 i:0.01i:0.005 i:0.001
Figure 2: Marginal Inference Error for Classification
on Synthetic Data
We use synthetic data for such a model to evaluate
the quality of marginals returned by the Gibbs sam-
pling form of MCMC. Since the Gibbs algorithm
samples each variable using a fixed assignment of
its neighborhood, we represent generating a single
sample as classification. We create star-shaped mod-
els with a single unobserved variable (entity type)
that neighbors many unary factors, each represent-
ing a single entity- or a relation-mention factor (See
Figure 1a for an example). We generate a synthetic
dataset for this model, creating 100 variables con-
sisting of 100 factors each. The scores of the fac-
tors are generated from gaussians, N(0.5, 1) for the
positive label, and N(?0.5, 1) for the negative label
(note the overlap between the weights in Figure 1b).
Although each structure contains only a single vari-
able, and no cycles, it is a valid benchmark to test
our sampling approach since the effects of the set-
ting of burn-in period and the thinning samples are
not a concern.
We perform standard Gibbs sampling, and com-
1107
pare the marginals obtained during sampling with
the true marginals, computed exactly. We evalu-
ate the previously described uniform sampling and
confidence-based sampling, with several parameter
values, and plot the L1 error to the true marginals
as more factors are examined. Note that here, and
in the rest of the evaluation, we shall use the num-
ber of factors scored as a proxy for running time,
since the effects of the rest of the steps of sam-
pling are relatively negligible. The error in compar-
ison to regular MCMC (p = 1) is shown in Fig-
ure 2, with standard error bars averaging over 100
models. Initially, as the sampling approach is made
more stochastic (lowering p or increasing i), we see
a steady improvement in the running time needed
to obtain the same error tolerance. However, the
amount of relative improvements slows as stochas-
ticity is increased further; in fact for extreme values
(i = 0.05, p = 0.1) the chains perform worse than
regular MCMC.
4.2 Entity Resolution in Citation Data
To evaluate our approach on a real world dataset,
we apply stochastic MCMC for MAP inference on
the task of citation matching. Given a large number
of citations (that appear at the end of research pa-
pers, for example), the task is to group together the
citations that refer to the same paper. The citation
matching problem is an instance of entity resolution,
in which observed mentions need to be partitioned
such that mentions in a set refer to the same under-
lying entity. Note that neither the identities, or the
number of underlying entities is known.
In this paper, the graphical model of entity reso-
lution consists of observed mentions (mi), and pair-
wise binary variables between all pairs of mentions
(yij) which represent whether the corresponding ob-
served mentions are coreferent. There is a local
factor for each coreference variable yij that has a
high score if the underlying mentions mi and mj
are similar. For the sake of efficiency, we only in-
stantiate and incorporate the variables and factors
when the variable is true, i.e. if yij = 1. Thus,
?(y) =
?
e
?
mi,mj?e
f(yij). The set of possible
worlds consists of all settings of the y variables that
are consistent with transitivity, i.e. the binary vari-
ables directly represent a valid clustering over the
mentions. An example of the model defined over 5
m2
m1
m3
m5
m4
1
1
1
1
y
12
y
23
y
13
y
45
Figure 3: Graphical Model for Entity Resolution:
defined over 5 mentions, with the setting of the vari-
ables resulting in 2 entities. For the sake of brevity,
we?ve only included variables set to 1; binary vari-
ables between mentions that are not coreferent have
been omitted.
mentions is given in Figure 3. This representation
is equivalent to Model 2 as introduced in McCal-
lum and Wellner (2004). As opposed to belief prop-
agation and other approximate inference techniques,
MCMC is especially appropriate for the task as it
can directly enforce transitivity.
When performing MCMC, each sample is a set-
ting to all the y variables that is consistent with tran-
sitivity. To maintain transitivity during sampling,
Metropolis Hastings is used to change the binary
variables in a way that is consistent with moving in-
dividual mentions. Our proposal function selects a
random mention, and moves it to a random entity,
changing all the pairwise variables with mentions in
its old entity, and the pairwise variables with men-
tions in its new entity. Thus, evaluation of such a
proposal function requires scoring a number of fac-
tors linear in the size of the entities, which, for large
datasets, can be a significant bottleneck. In prac-
tice, however, these set of factors are often highly
redundant, as many of the mentions that refer to the
same entity contain redundant information and fea-
tures, and entity membership may be efficiently de-
termined by observing a subset of its mentions.
We evaluate on the Cora dataset (McCallum et
al., 1999), used previously to evaluate a number
of information extraction approaches (Pasula et al
2003), including MCMC based inference (Poon and
Domingos, 2007; Singh et al 2009). The dataset
1108
10000 100000 1000000 10000000 100000000Number of Factors Examined
0.000.05
0.100.15
0.200.25
0.300.35
0.400.45
0.500.55
0.600.65
0.700.75
0.800.85
0.900.95
1.001.05
BCu
bed 
F1
p:1. p:0.5 p:0.2 p:0.1i:20. i:2. i:1. i:0.5 i:0.1
Figure 4: Citation Resolution Accuracy Plot for uni-
form and variance-based sampling compared to reg-
ular MCMC (p = 1)
consists of 1295 mentions, that refer to 134 true un-
derlying entities. We use the same features for our
model as (Poon and Domingos, 2007), using true
author, title, and venue segmentation for features.
Since our focus is on evaluating scalability of in-
ference, we combine all the three folds of the data,
and train the model using Samplerank (Wick et al
2011).
We run MCMC on the entity resolution model us-
ing the proposal function described above, running
our approach with different parameter values. Since
we are interested in the MAP configuration, we use
a temperature term for annealing. As inference pro-
gresses, we compute BCubed2 F1 of the current
sample, and plot it against the number of scored fac-
tors in Figure 4. We observe consistent speed im-
provements as stochasticity is improved, with uni-
form sampling and confidence-based sampling per-
forming competitively. To compute the speedup, we
measure the number of factors scored to obtain a de-
sired level of accuracy (90% F1), shown for a di-
verse set of parameters in Table 1. With a very
large confidence interval threshold (i = 20) and
small proportion (p = 0.1), we obtain up to 7 times
speedup over regular MCMC. Since the average en-
tity size in this data set is < 10, using a small pro-
portion (and a wide interval) is equivalent to picking
a single mention to compare against.
2B3 is a coreference evaluation metric, introduced by Bagga
and Baldwin (1998)
Method Factors Examined Speedup
Baseline 57,292,700 1x
Uniform Sampling
p = 0.75 34,803,972 1.64x
p = 0.5 28,143,323 2.04x
p = 0.3 17,778,891 3.22x
p = 0.2 12,892,079 4.44x
p = 0.1 7,855,686 7.29x
Variance-Based Sampling
i = 0.001 52,522,728 1.09x
i = 0.01 51,547,000 1.11x
i = 0.1 47,165,038 1.21x
i = 0.5 32,828,823 1.74x
i = 1 18,938,791 3.02x
i = 2 11,134,267 5.14x
i = 5 9,827,498 5.83x
i = 10 8,675,833 6.60x
i = 20 8,295,587 6.90x
Table 1: Speedups on Cora to obtain 90% B3 F1
4.3 Large-Scale Author Coreference
As the body of published scientific work continues
to grow, author coreference, the problem of clus-
tering mentions of research paper authors into the
real-world authors to which they refer, is becoming
an increasingly important step for performing mean-
ingful bibliometric analysis. However, scaling typi-
cal pairwise models of coreference (e.g., McCallum
and Wellner (2004)) is difficult because the number
of factors in the model grows quadratically with the
number of mentions (research papers) and the num-
ber of factors evaluated for every MCMC proposal
scales linearly in the size of the clusters. For author
coreference, the number of author mentions and the
number of references to an author entity can often be
in the millions, making the evaluation of the MCMC
proposals computationally expensive.
We use the publicly available DBLP dataset3 of
BibTex entries as our unlabeled set of mentions,
which contains nearly 5 million authors. For eval-
uation of accuracy, we also include author mentions
from the Rexa corpus4 that contains 2, 833 mentions
3http://www.informatik.uni-trier.de/
?ley/db/
4http://www2.selu.edu/Academics/Faculty/
aculotta/data/rexa.html
1109
10000000 100000000 1000000000 10000000000Number of Factors Examined
0.000.05
0.100.15
0.200.25
0.300.35
0.400.45
0.500.55
0.600.65
0.700.75
0.800.85
0.900.95
1.001.05
BCub
ed F1
p:1. p:0.5 p:0.2 p:0.1p:0.01 i:10. i:1. i:0.1
(a) Accuracy versus Number of Factors scored
10000000 100000000Number of Samples
0.000.05
0.100.15
0.200.25
0.300.35
0.400.45
0.500.55
0.600.65
0.700.75
0.800.85
0.900.95
1.001.05
BCub
ed F1
p:1. p:0.5 p:0.2 p:0.1p:0.01 i:10. i:1. i:0.1
(b) Accuracy versus Number of Samples
Figure 5: Performance of Different Sampling Strategies and Parameters for coreference over 5 million
mentions. Plot with p refer to uniform sampling with proportion p of factors picked, while plots with i
sample till confidence intervals are narrower than i.
labeled for coreference.
We use the same Metropolis-Hastings scheme that
we employ in the problem of citation matching. As
before, we initialize to the singleton configuration
and run the experiments for a fixed number of sam-
ples, plotting accuracy versus the number of factors
evaluated (Figure 5a) as well as accuracy versus the
number of samples generated (Figure 5b). We also
tabulate the relative speedups to obtain the desired
accuracy level in Table 2. Our proposed method
achieves substantial savings on this task: speedups
of 13.16 using the variance sampler and speedups
of 9.78 using the uniform sampler. As expected,
when we compare the performance using the num-
ber of generated samples, the approximate MCMC
chains appear to converge more slowly; however, the
overall convergence for our approach is substantially
faster because evaluation of each sample is signif-
icantly cheaper. We also present results on using
extreme approximations (for example, p = 0.01),
resulting in convergence to a low accuracy.
5 Discussion and Related Work
MCMC is a popular method for inference amongst
researchers that work with large and dense graphi-
cal models (Richardson and Domingos, 2006; Poon
and Domingos, 2006; Poon et al 2008; Singh et al
2009; Wick et al 2009). Some of the probabilistic
Method Factors Examined Speedup
Baseline 1,395,330,603 1x
Uniform
p = 0.5 689,254,134 2.02x
p = 0.2 327,616,794 4.26x
p = 0.1 206,157,705 6.77x
p = 0.05 152,069,987 9.17x
p = 0.02 142,689,770 9.78x
Variance
i = 0.00001 1,442,091,344 0.96x
i = 0.0001 1,419,110,724 0.98x
i = 0.001 1,374,667,077 1.01x
i = 0.1 1,012,321,830 1.38x
i = 1 265,327,983 5.26x
i = 10 179,701,896 7.76x
i = 100 106,850,725 13.16x
Table 2: Speedups on DBLP to reach 80% B3 F1
programming packages popular amongst NLP prac-
titioners also rely on MCMC for inference and learn-
ing (Richardson and Domingos, 2006; McCallum et
al., 2009). Although most of these methods apply
MCMC directly, the rate of convergence of MCMC
has become a concern as larger and more densely-
factored models are being considered, motivating
the need for more efficient sampling that uses par-
allelism (Singh et al 2011; Gonzalez et al 2011)
1110
and domain knowledge for blocking (Singh et al
2010). Thus we feel providing a method to speed up
MCMC inference can have a significant impact.
There has also been recent work in designing
scalable approximate inference techniques. Belief
propagation has, in particular, has gained some re-
cent interest. Similar to our approach, a number
of researchers propose modifications to BP that per-
form inference without visiting all the factors. Re-
cent work introduces dynamic schedules to priori-
tize amongst the factors (Coughlan and Shen, 2007;
Sutton and McCallum, 2007) that has been used to
only visit a small fraction of the factors (Riedel and
Smith, 2010). Gonzalez et al(2009) utilize these
schedules to facilitate parallelization.
A number of existing approaches in statistics
are also related to our contribution. Leskovec and
Faloutsos (2006) propose techniques to sample a
graph to compute certain graph statistics with asso-
ciated confidence. Christen and Fox (2005) also pro-
pose an approach to efficiently evaluate a proposal,
however, once accepted, they score all the factors.
Murray and Ghahramani (2004) propose an approx-
imate MCMC technique for Bayesian models that
estimates the partition function instead of comput-
ing it exactly.
Related work has also applied such ideas for
robust learning, for example Kok and Domingos
(2005), based on earlier work by Hulten and Domin-
gos (2002), uniformly sample the groundings of an
MLN to estimate the likelihood.
6 Conclusions and Future Work
Motivated by the need for an efficient inference tech-
nique that can scale to large, densely-factored mod-
els, this paper considers a simple extension to the
Markov chain Monto Carlo algorithm. By observ-
ing that many graphical models contain substantial
redundancy among the factors, we propose stochas-
tic evaluation of proposals that subsamples the fac-
tors to be scored. Using two proposed sampling
strategies, we demonstrate improved convergence
for marginal inference on synthetic data. Further,
we evaluate our approach on two real-world entity
resolution datasets, obtaining a 13 times speedup on
a dataset containing 5 million mentions.
Based on the ideas presented in the paper, we will
consider additional sampling strategies. In partic-
ular, we will explore dynamic sampling, in which
we sample fewer factors during the initial, burn-
in phase, but sample more factors as we get close
to convergence. Motivated by our positive results,
we will also study the application of this approach
to other approximate inference techniques, such as
belief propagation and variational inference. Since
training is often a huge bottleneck for information
extraction, we will also explore its applications to
parameter estimation.
Acknowledgements
This work was supported in part by the Center for
Intelligent Information Retrieval, in part by ARFL
under prime contract number is FA8650-10-C-7059,
and the University of Massachusetts gratefully ac-
knowledges the support of Defense Advanced Re-
search Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181.
The U.S. Government is authorized to reproduce
and distribute reprint for Governmental purposes
notwithstanding any copyright annotation thereon.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect those of
the sponsor.
References
[Bagga and Baldwin1998] Amit Bagga and Breck Bald-
win. 1998. Algorithms for scoring coreference
chains. In International Conference on Language Re-
sources and Evaluation (LREC) Workshop on Linguis-
tics Coreference, pages 563?566.
[Bertsimas and Tsitsiklis1993] D. Bertsimas and J. Tsit-
siklis. 1993. Simulated annealing. Statistical Science,
pages 10?15.
[Carreras2007] Xavier Carreras. 2007. Experiments
with a higher-order projective dependency parser. In
Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 957?961.
[Christen and Fox2005] J. Andre?s Christen and Colin
Fox. 2005. Markov chain monte carlo using an ap-
proximation. Journal of Computational and Graphi-
cal Statistics, 14(4):pp. 795?810.
[Coughlan and Shen2007] James Coughlan and Huiying
Shen. 2007. Dynamic quantization for belief propa-
1111
gation in sparse spaces. Computer Vision and Image
Understanding, 106:47?58, April.
[Culotta et al007] Aron Culotta, Michael Wick, and An-
drew McCallum. 2007. First-order probabilistic mod-
els for coreference resolution. In North American
Chapter of the Association for Computational Linguis-
tics - Human Language Technologies (NAACL HLT).
[Gonzalez et al009] Joseph Gonzalez, Yucheng Low,
and Carlos Guestrin. 2009. Residual splash for op-
timally parallelizing belief propagation. In Artificial
Intelligence and Statistics (AISTATS).
[Gonzalez et al011] Joseph Gonzalez, Yucheng Low,
Arthur Gretton, and Carlos Guestrin. 2011. Paral-
lel gibbs sampling: From colored fields to thin junc-
tion trees. In Artificial Intelligence and Statistics (AIS-
TATS), Ft. Lauderdale, FL, May.
[Hoffmann et al011] Raphael Hoffmann, Congle
Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S.
Weld. 2011. Knowledge-based weak supervision
for information extraction of overlapping relations.
In Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 541?550, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
[Hulten and Domingos2002] Geoff Hulten and Pedro
Domingos. 2002. Mining complex models from ar-
bitrarily large databases in constant time. In Interna-
tional Conference on Knowledge Discovery and Data
Mining (KDD), pages 525?531, New York, NY, USA.
ACM.
[Kok and Domingos2005] Stanley Kok and Pedro
Domingos. 2005. Learning the structure of markov
logic networks. In International Conference on
Machine Learning (ICML), pages 441?448, New
York, NY, USA. ACM.
[Kschischang et al001] Frank R. Kschischang, Bren-
dan J. Frey, and Hans Andrea Loeliger. 2001. Factor
graphs and the sum-product algorithm. IEEE Transac-
tions of Information Theory, 47(2):498?519, Feb.
[Lafferty et al001] John D. Lafferty, Andrew McCal-
lum, and Fernando Pereira. 2001. Conditional ran-
dom fields: Probabilistic models for segmenting and
labeling sequence data. In International Conference
on Machine Learning (ICML).
[Leskovec and Faloutsos2006] Jure Leskovec and Chris-
tos Faloutsos. 2006. Sampling from large graphs.
In International Conference on Knowledge Discovery
and Data Mining (KDD), pages 631?636, New York,
NY, USA. ACM.
[McCallum and Wellner2004] Andrew McCallum and
Ben Wellner. 2004. Conditional models of identity
uncertainty with application to noun coreference. In
Neural Information Processing Systems (NIPS).
[McCallum et al999] Andrew McCallum, Kamal
Nigam, Jason Rennie, and Kristie Seymore. 1999.
A machine learning approach to building domain-
specific search engines. In International Joint
Conference on Artificial Intelligence (IJCAI).
[McCallum et al009] Andrew McCallum, Karl Schultz,
and Sameer Singh. 2009. FACTORIE: Probabilistic
programming via imperatively defined factor graphs.
In Neural Information Processing Systems (NIPS).
[Murray and Ghahramani2004] Iain Murray and Zoubin
Ghahramani. 2004. Bayesian learning in undirected
graphical models: Approximate MCMC algorithms.
In Uncertainty in Artificial Intelligence (UAI).
[Pasula et al003] H. Pasula, B. Marthi, B. Milch,
S. Russell, and I. Shpitser. 2003. Identity uncertainty
and citation matching. In Neural Information Process-
ing Systems (NIPS).
[Poon and Domingos2006] Hoifung Poon and Pedro
Domingos. 2006. Sound and efficient inference with
probabilistic and deterministic dependencies. In AAAI
Conference on Artificial Intelligence.
[Poon and Domingos2007] Hoifung Poon and Pedro
Domingos. 2007. Joint inference in informa-
tion extraction. In AAAI Conference on Artificial
Intelligence, pages 913?918.
[Poon et al008] Hoifung Poon, Pedro Domingos, and
Marc Sumner. 2008. A general method for reduc-
ing the complexity of relational inference and its ap-
plication to MCMC. In AAAI Conference on Artificial
Intelligence.
[Richardson and Domingos2006] Matthew Richardson
and Pedro Domingos. 2006. Markov logic networks.
Machine Learning, 62(1-2):107?136.
[Riedel and Smith2010] Sebastian Riedel and David A.
Smith. 2010. Relaxed marginal inference and its ap-
plication to dependency parsing. In North American
Chapter of the Association for Computational Linguis-
tics - Human Language Technologies (NAACL HLT),
pages 760?768.
[Singh et al009] Sameer Singh, Karl Schultz, and An-
drew McCallum. 2009. Bi-directional joint in-
ference for entity resolution and segmentation us-
ing imperatively-defined factor graphs. In Machine
Learning and Knowledge Discovery in Databases
(Lecture Notes in Computer Science) and European
Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases
(ECML PKDD), pages 414?429.
[Singh et al010] Sameer Singh, Michael L. Wick, and
Andrew McCallum. 2010. Distantly labeling data for
large scale cross-document coreference. Computing
Research Repository (CoRR), abs/1005.4298.
[Singh et al011] Sameer Singh, Amarnag Subramanya,
Fernando Pereira, and Andrew McCallum. 2011.
1112
Large-scale cross-document coreference using dis-
tributed inference and hierarchical models. In Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL HLT).
[Sutton and McCallum2004] Charles Sutton and Andrew
McCallum. 2004. Collective segmentation and label-
ing of distant entities in information extraction. Tech-
nical Report TR#04-49, University of Massachusetts,
July.
[Sutton and McCallum2007] Charles Sutton and Andrew
McCallum. 2007. Improved dynamic schedules for
belief propagation. In Uncertainty in Artificial Intelli-
gence (UAI).
[Wick et al009] Michael Wick, Aron Culotta, Khasha-
yar Rohanimanesh, and Andrew McCallum. 2009.
An entity-based model for coreference resolution.
In SIAM International Conference on Data Mining
(SDM).
[Wick et al010] Michael Wick, Andrew McCallum, and
Gerome Miklau. 2010. Scalable probabilistic
databases with factor graphs and mcmc. International
Conference on Very Large Databases (VLDB), 3:794?
804, September.
[Wick et al011] Michael Wick, Khashayar Rohani-
manesh, Kedar Bellare, Aron Culotta, and Andrew
McCallum. 2011. Samplerank: Training factor graphs
with atomic gradients. In International Conference on
Machine Learning (ICML).
[Yao et al010] Limin Yao, Sebastian Riedel, and An-
drew McCallum. 2010. Collective cross-document
relation extraction without labelled data. In Empirical
Methods in Natural Language Processing (EMNLP).
1113
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1059?1069,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Efficient Non-parametric Estimation of
Multiple Embeddings per Word in Vector Space
Arvind Neelakantan
*
, Jeevan Shankar
*
, Alexandre Passos, Andrew McCallum
Department of Computer Science
University of Massachusetts, Amherst
Amherst, MA, 01003
{arvind,jshankar,apassos,mccallum}@cs.umass.edu
Abstract
There is rising interest in vector-space
word embeddings and their use in NLP,
especially given recent methods for their
fast estimation at very large scale. Nearly
all this work, however, assumes a sin-
gle vector per word type?ignoring poly-
semy and thus jeopardizing their useful-
ness for downstream tasks. We present
an extension to the Skip-gram model that
efficiently learns multiple embeddings per
word type. It differs from recent related
work by jointly performing word sense
discrimination and embedding learning,
by non-parametrically estimating the num-
ber of senses per word type, and by its ef-
ficiency and scalability. We present new
state-of-the-art results in the word similar-
ity in context task and demonstrate its scal-
ability by training with one machine on a
corpus of nearly 1 billion tokens in less
than 6 hours.
1 Introduction
Representing words by dense, real-valued vector
embeddings, also commonly called ?distributed
representations,? helps address the curse of di-
mensionality and improve generalization because
they can place near each other words having sim-
ilar semantic and syntactic roles. This has been
shown dramatically in state-of-the-art results on
language modeling (Bengio et al, 2003; Mnih and
Hinton, 2007) as well as improvements in other
natural language processing tasks (Collobert and
Weston, 2008; Turian et al, 2010). Substantial
benefit arises when embeddings can be trained on
large volumes of data. Hence the recent consider-
able interest in the CBOW and Skip-gram models
*
The first two authors contributed equally to this paper.
of Mikolov et al (2013a); Mikolov et al (2013b)?
relatively simple log-linear models that can be
trained to produce high-quality word embeddings
on the entirety of English Wikipedia text in less
than half a day on one machine.
There is rising enthusiasm for applying these
models to improve accuracy in natural language
processing, much like Brown clusters (Brown et
al, 1992) have become common input features
for many tasks, such as named entity extraction
(Miller et al, 2004; Ratinov and Roth, 2009) and
parsing (Koo et al, 2008; T?ackstr?om et al, 2012).
In comparison to Brown clusters, the vector em-
beddings have the advantages of substantially bet-
ter scalability in their training, and intriguing po-
tential for their continuous and multi-dimensional
interrelations. In fact, Passos et al (2014) present
new state-of-the-art results in CoNLL 2003 named
entity extraction by directly inputting continuous
vector embeddings obtained by a version of Skip-
gram that injects supervision with lexicons. Sim-
ilarly Bansal et al (2014) show results in depen-
dency parsing using Skip-gram embeddings. They
have also recently been applied to machine trans-
lation (Zou et al, 2013; Mikolov et al, 2013c).
A notable deficiency in this prior work is that
each word type (e.g. the word string plant) has
only one vector representation?polysemy and
hononymy are ignored. This results in the word
plant having an embedding that is approximately
the average of its different contextual seman-
tics relating to biology, placement, manufactur-
ing and power generation. In moderately high-
dimensional spaces a vector can be relatively
?close? to multiple regions at a time, but this does
not negate the unfortunate influence of the triangle
inequality
2
here: words that are not synonyms but
are synonymous with different senses of the same
word will be pulled together. For example, pollen
and refinery will be inappropriately pulled to a dis-
2
For distance d, d(a, c) ? d(a, b) + d(b, c).
1059
tance not more than the sum of the distances plant?
pollen and plant?refinery. Fitting the constraints of
legitimate continuous gradations of semantics are
challenge enough without the additional encum-
brance of these illegitimate triangle inequalities.
Discovering embeddings for multiple senses per
word type is the focus of work by Reisinger and
Mooney (2010a) and Huang et al (2012). They
both pre-cluster the contexts of a word type?s to-
kens into discriminated senses, use the clusters to
re-label the corpus? tokens according to sense, and
then learn embeddings for these re-labeled words.
The second paper improves upon the first by em-
ploying an earlier pass of non-discriminated em-
bedding learning to obtain vectors used to rep-
resent the contexts. Note that by pre-clustering,
these methods lose the opportunity to jointly learn
the sense-discriminated vectors and the cluster-
ing. Other weaknesses include their fixed num-
ber of sense per word type, and the computational
expense of the two-step process?the Huang et
al (2012) method took one week of computation
to learn multiple embeddings for a 6,000 subset
of the 30,000 vocabulary on a corpus containing
close to billion tokens.
3
This paper presents a new method for learn-
ing vector-space embeddings for multiple senses
per word type, designed to provide several ad-
vantages over previous approaches. (1) Sense-
discriminated vectors are learned jointly with the
assignment of token contexts to senses; thus we
can use the emerging sense representation to more
accurately perform the clustering. (2) A non-
parametric variant of our method automatically
discovers a varying number of senses per word
type. (3) Efficient online joint training makes
it fast and scalable. We refer to our method as
Multiple-sense Skip-gram, or MSSG, and its non-
parametric counterpart as NP-MSSG.
Our method builds on the Skip-gram model
(Mikolov et al, 2013a), but maintains multiple
vectors per word type. During online training
with a particular token, we use the average of its
context words? vectors to select the token?s sense
that is closest, and perform a gradient update on
that sense. In the non-parametric version of our
method, we build on facility location (Meyerson,
2001): a new cluster is created with probability
proportional to the distance from the context to the
3
Personal communication with authors Eric H. Huang and
Richard Socher.
nearest sense.
We present experimental results demonstrating
the benefits of our approach. We show quali-
tative improvements over single-sense Skip-gram
and Huang et al (2012), comparing against word
neighbors from our parametric and non-parametric
methods. We present quantitative results in three
tasks. On both the SCWS and WordSim353 data
sets our methods surpass the previous state-of-
the-art. The Google Analogy task is not espe-
cially well-suited for word-sense evaluation since
its lack of context makes selecting the sense dif-
ficult; however our method dramatically outper-
forms Huang et al (2012) on this task. Finally
we also demonstrate scalabilty, learning multiple
senses, training on nearly a billion tokens in less
than 6 hours?a 27x improvement on Huang et al.
2 Related Work
Much prior work has focused on learning vector
representations of words; here we will describe
only those most relevant to understanding this pa-
per. Our work is based on neural language mod-
els, proposed by Bengio et al (2003), which extend
the traditional idea of n-gram language models by
replacing the conditional probability table with a
neural network, representing each word token by
a small vector instead of an indicator variable, and
estimating the parameters of the neural network
and these vectors jointly. Since the Bengio et al
(2003) model is quite expensive to train, much re-
search has focused on optimizing it. Collobert and
Weston (2008) replaces the max-likelihood char-
acter of the model with a max-margin approach,
where the network is encouraged to score the cor-
rect n-grams higher than randomly chosen incor-
rect n-grams. Mnih and Hinton (2007) replaces
the global normalization of the Bengio model with
a tree-structured probability distribution, and also
considers multiple positions for each word in the
tree.
More relevantly, Mikolov et al (2013a) and
Mikolov et al (2013b) propose extremely com-
putationally efficient log-linear neural language
models by removing the hidden layers of the neu-
ral networks and training from larger context win-
dows with very aggressive subsampling. The
goal of the models in Mikolov et al (2013a) and
Mikolov et al (2013b) is not so much obtain-
ing a low-perplexity language model as learn-
ing word representations which will be useful in
1060
downstream tasks. Neural networks or log-linear
models also do not appear to be necessary to
learn high-quality word embeddings, as Dhillon
and Ungar (2011) estimate word vector repre-
sentations using Canonical Correlation Analysis
(CCA).
Word vector representations or embeddings
have been used in various NLP tasks such
as named entity recognition (Neelakantan and
Collins, 2014; Passos et al, 2014; Turian et al,
2010), dependency parsing (Bansal et al, 2014),
chunking (Turian et al, 2010; Dhillon and Ungar,
2011), sentiment analysis (Maas et al, 2011), para-
phrase detection (Socher et al, 2011) and learning
representations of paragraphs and documents (Le
and Mikolov, 2014). The word clusters obtained
from Brown clustering (Brown et al, 1992) have
similarly been used as features in named entity
recognition (Miller et al, 2004; Ratinov and Roth,
2009) and dependency parsing (Koo et al, 2008),
among other tasks.
There is considerably less prior work on learn-
ing multiple vector representations for the same
word type. Reisinger and Mooney (2010a) intro-
duce a method for constructing multiple sparse,
high-dimensional vector representations of words.
Huang et al (2012) extends this approach incor-
porating global document context to learn mul-
tiple dense, low-dimensional embeddings by us-
ing recursive neural networks. Both the meth-
ods perform word sense discrimination as a pre-
processing step by clustering contexts for each
word type, making training more expensive.
While methods such as those described in Dhillon
and Ungar (2011) and Reddy et al (2011) use
token-specific representations of words as part
of the learning algorithm, the final outputs are
still one-to-one mappings between word types and
word embeddings.
3 Background: Skip-gram model
The Skip-gram model learns word embeddings
such that they are useful in predicting the sur-
rounding words in a sentence. In the Skip-gram
model, v(w) ? R
d
is the vector representation of
the word w ? W , where W is the words vocabu-
lary and d is the embedding dimensionality.
Given a pair of words (w
t
, c), the probability
that the word c is observed in the context of word
w
t
is given by,
P (D = 1|v(w
t
), v(c)) =
1
1 + e
?v(w
t
)
T
v(c)
(1)
The probability of not observing word c in the con-
text of w
t
is given by,
P (D = 0|v(w
t
), v(c)) =
1? P (D = 1|v(w
t
), v(c))
Given a training set containing the sequence of
word types w
1
, w
2
, . . . , w
T
, the word embeddings
are learned by maximizing the following objective
function:
J(?) =
?
(w
t
,c
t
)?D
+
?
c?c
t
logP (D = 1|v(w
t
), v(c))
+
?
(w
t
,c
?
t
)?D
?
?
c
?
?c
?
t
logP (D = 0|v(w
t
), v(c
?
))
where w
t
is the t
th
word in the training set, c
t
is the set of observed context words of word w
t
and c
?
t
is the set of randomly sampled, noisy con-
text words for the word w
t
. D
+
consists of
the set of all observed word-context pairs (w
t
, c
t
)
(t = 1, 2 . . . , T ). D
?
consists of pairs (w
t
, c
?
t
)
(t = 1, 2 . . . , T ) where c
?
t
is the set of randomly
sampled, noisy context words for the word w
t
.
For each training word w
t
, the set of context
words c
t
= {w
t?R
t
, . . . , w
t?1
, w
t+1
, . . . , w
t+R
t
}
includesR
t
words to the left and right of the given
word as shown in Figure 1. R
t
is the window size
considered for the word w
t
uniformly randomly
sampled from the set {1, 2, . . . , N}, where N is
the maximum context window size.
The set of noisy context words c
?
t
for the word
w
t
is constructed by randomly sampling S noisy
context words for each word in the context c
t
. The
noisy context words are randomly sampled from
the following distribution,
P (w) =
p
unigram
(w)
3/4
Z
(2)
where p
unigram
(w) is the unigram distribution of
the words and Z is the normalization constant.
4 Multi-Sense Skip-gram (MSSG) model
To extend the Skip-gram model to learn multiple
embeddings per word we follow previous work
(Huang et al, 2012; Reisinger and Mooney, 2010a)
1061
Word 
Vector
word w
t
v(w
t+2
)
Context   
Vectors
v(w
t+1
)
v(w
t-1
)
v(w
t-2
)
v(w
t
)
Figure 1: Architecture of the Skip-gram model
with window size R
t
= 2. Context c
t
of word
w
t
consists of w
t?1
, w
t?2
, w
t+1
, w
t+2
.
and let each sense of word have its own embed-
ding, and induce the senses by clustering the em-
beddings of the context words around each token.
The vector representation of the context is the av-
erage of its context words? vectors. For every word
type, we maintain clusters of its contexts and the
sense of a word token is predicted as the cluster
that is closest to its context representation. After
predicting the sense of a word token, we perform
a gradient update on the embedding of that sense.
The crucial difference from previous approaches
is that word sense discrimination and learning em-
beddings are performed jointly by predicting the
sense of the word using the current parameter es-
timates.
In the MSSG model, each word w ? W is
associated with a global vector v
g
(w) and each
sense of the word has an embedding (sense vec-
tor) v
s
(w, k) (k = 1, 2, . . . ,K) and a context clus-
ter with center ?(w, k) (k = 1, 2, . . . ,K). The K
sense vectors and the global vectors are of dimen-
sion d and K is a hyperparameter.
Consider the word w
t
and let c
t
=
{w
t?R
t
, . . . , w
t?1
, w
t+1
, . . . , w
t+R
t
} be the
set of observed context words. The vector repre-
sentation of the context is defined as the average
of the global vector representation of the words in
the context. Let v
context
(c
t
) =
1
2?R
t
?
c?c
t
v
g
(c)
be the vector representation of the context c
t
. We
use the global vectors of the context words instead
of its sense vectors to avoid the computational
complexity associated with predicting the sense
of the context words. We predict s
t
, the sense
Word 6ense 
Vectors
v(w
t
2)
v
J
(w
t+2
)
Context   
Vectors
v
J
(w
t+1
)
 v
J
(w
t-1
)
v
J
(w
t-2
)
$verDJe Context 
Vector
Context COXster 
Centers
v(w
t
1)
v(w
t
)
3redLcted 
6ense s
t
?(w
t
1)
v
context
(c
t
)
 
 
 
?(w
t
2)
?(w
t
)
 
Context   
Vectors
v
J
(w
t+2
)
v
J
(w
t+1
)
v
J
(w
t-1
)
v
J
(w
t-2
)
Figure 2: Architecture of Multi-Sense Skip-gram
(MSSG) model with window size R
t
= 2 and
K = 3. Context c
t
of word w
t
consists of
w
t?1
, w
t?2
, w
t+1
, w
t+2
. The sense is predicted by
finding the cluster center of the context that is clos-
est to the average of the context vectors.
of word w
t
when observed with context c
t
as
the context cluster membership of the vector
v
context
(c
t
) as shown in Figure 2. More formally,
s
t
= argmax
k=1,2,...,K
sim(?(w
t
, k), v
context
(c
t
)) (3)
The hard cluster assignment is similar to the k-
means algorithm. The cluster center is the aver-
age of the vector representations of all the contexts
which belong to that cluster. For sim we use co-
sine similarity in our experiments.
Here, the probability that the word c is observed
in the context of word w
t
given the sense of the
word w
t
is,
P (D = 1|s
t
,v
s
(w
t
, 1), . . . , v
s
(w
t
,K), v
g
(c))
= P (D = 1|v
s
(w
t
, s
t
), v
g
(c))
=
1
1 + e
?v
s
(w
t
,s
t
)
T
v
g
(c)
The probability of not observing word c in the con-
text of w
t
given the sense of the word w
t
is,
P (D = 0|s
t
,v
s
(w
t
, 1), . . . , v
s
(w
t
,K), v
g
(c))
= P (D = 0|v
s
(w
t
, s
t
), v
g
(c))
= 1? P (D = 1|v
s
(w
t
, s
t
), v
g
(c))
Given a training set containing the sequence of
word types w
1
, w
2
, ..., w
T
, the word embeddings
are learned by maximizing the following objective
1062
Algorithm 1 Training Algorithm of MSSG model
1: Input: w
1
, w
2
, ..., w
T
, d, K, N .
2: Initialize v
s
(w, k) and v
g
(w), ?w ? W,k ?
{1, . . . ,K} randomly, ?(w, k) ?w ? W,k ?
{1, . . . ,K} to 0.
3: for t = 1, 2, . . . , T do
4: R
t
? {1, . . . , N}
5: c
t
= {w
t?R
t
, . . . , w
t?1
, w
t+1
, . . . , w
t+R
t
}
6: v
context
(c
t
) =
1
2?R
t
?
c?c
t
v
g
(c)
7: s
t
= argmax
k=1,2,...,K
{
sim(?(w
t
, k), v
context
(c
t
))}
8: Update context cluster center ?(w
t
, s
t
)
since context c
t
is added to context cluster s
t
of word w
t
.
9: c
?
t
= Noisy Samples(c
t
)
10: Gradient update on v
s
(w
t
, s
t
), global vec-
tors of words in c
t
and c
?
t
.
11: end for
12: Output: v
s
(w, k), v
g
(w) and context cluster
centers ?(w, k), ?w ?W,k ? {1, . . . ,K}
function:
J(?) =
?
(w
t
,c
t
)?D
+
?
c?c
t
logP (D = 1|v
s
(w
t
, s
t
), v
g
(c))+
?
(w
t
,c
?
t
)?D
?
?
c
?
?c
?
t
logP (D = 0|v
s
(w
t
, s
t
), v
g
(c
?
))
where w
t
is the t
th
word in the sequence, c
t
is the
set of observed context words and c
?
t
is the set of
noisy context words for the word w
t
. D
+
and D
?
are constructed in the same way as in the Skip-
gram model.
After predicting the sense of word w
t
, we up-
date the embedding of the predicted sense for
the word w
t
(v
s
(w
t
, s
t
)), the global vector of the
words in the context and the global vector of the
randomly sampled, noisy context words. The con-
text cluster center of cluster s
t
for the word w
t
(?(w
t
, s
t
)) is updated since context c
t
is added to
the cluster s
t
.
5 Non-Parametric MSSG model
(NP-MSSG)
The MSSG model learns a fixed number of senses
per word type. In this section, we describe a
non-parametric version of MSSG, the NP-MSSG
model, which learns varying number of senses per
word type. Our approach is closely related to
the online non-parametric clustering procedure de-
scribed in Meyerson (2001). We create a new clus-
ter (sense) for a word type with probability propor-
tional to the distance of its context to the nearest
cluster (sense).
Each wordw ?W is associated with sense vec-
tors, context clusters and a global vector v
g
(w) as
in the MSSG model. The number of senses for a
word is unknown and is learned during training.
Initially, the words do not have sense vectors and
context clusters. We create the first sense vector
and context cluster for each word on its first occur-
rence in the training data. After creating the first
context cluster for a word, a new context cluster
and a sense vector are created online during train-
ing when the word is observed with a context were
the similarity between the vector representation of
the context with every existing cluster center of the
word is less than ?, where ? is a hyperparameter
of the model.
Consider the word w
t
and let c
t
=
{w
t?R
t
, . . . , w
t?1
, w
t+1
, . . . , w
t+R
t
} be the
set of observed context words. The vector repre-
sentation of the context is defined as the average
of the global vector representation of the words in
the context. Let v
context
(c
t
) =
1
2?R
t
?
c?c
t
v
g
(c)
be the vector representation of the context c
t
. Let
k(w
t
) be the number of context clusters or the
number of senses currently associated with word
w
t
. s
t
, the sense of word w
t
when k(w
t
) > 0 is
given by
s
t
=
?
?
?
?
?
k(w
t
) + 1, ifmax
k=1,2,...,k(w
t
)
{sim
(?(w
t
, k), v
context
(c
t
))} < ?
k
max
, otherwise
(4)
where ?(w
t
, k) is the cluster center of
the k
th
cluster of word w
t
and k
max
=
argmax
k=1,2,...,k(w
t
)
sim(?(w
t
, k), v
context
(c
t
)).
The cluster center is the average of the vector
representations of all the contexts which belong to
that cluster. If s
t
= k(w
t
) + 1, a new context
cluster and a new sense vector are created for the
word w
t
.
The NP-MSSG model and the MSSG model
described previously differ only in the way word
sense discrimination is performed. The objec-
tive function and the probabilistic model associ-
ated with observing a (word, context) pair given
the sense of the word remain the same.
1063
Model Time (in hours)
Huang et al 168
MSSG 50d 1
MSSG-300d 6
NP-MSSG-50d 1.83
NP-MSSG-300d 5
Skip-gram-50d 0.33
Skip-gram-300d 1.5
Table 1: Training Time Results. First five model
reported in the table are capable of learning mul-
tiple embeddings for each word and Skip-gram
is capable of learning only single embedding for
each word.
6 Experiments
To evaluate our algorithms we train embeddings
using the same corpus and vocabulary as used in
Huang et al (2012), which is the April 2010 snap-
shot of the Wikipedia corpus (Shaoul and West-
bury, 2010). It contains approximately 2 million
articles and 990 million tokens. In all our experi-
ments we remove all the words with less than 20
occurrences and use a maximum context window
(N ) of length 5 (5 words before and after the word
occurrence). We fix the number of senses (K) to
be 3 for the MSSG model unless otherwise speci-
fied. Our hyperparameter values were selected by
a small amount of manual exploration on a vali-
dation set. In NP-MSSG we set ? to -0.5. The
Skip-gram model, MSSG and NP-MSSG models
sample one noisy context word (S) for each of the
observed context words. We train our models us-
ing AdaGrad stochastic gradient decent (Duchi et
al, 2011) with initial learning rate set to 0.025.
Similarly to Huang et al (2012), we don?t use a
regularization penalty.
Below we describe qualitative results, display-
ing the embeddings and the nearest neighbors of
each word sense, and quantitative experiments in
two benchmark word similarity tasks.
Table 1 shows time to train our models, com-
pared with other models from previous work. All
these times are from single-machine implementa-
tions running on similar-sized corpora. We see
that our model shows significant improvement in
the training time over the model in Huang et
al (2012), being within well within an order-of-
magnitude of the training time for Skip-gram mod-
els.
APPLE
Skip-gram blackberry, macintosh, acorn, pear, plum
MSSG
pear, honey, pumpkin, potato, nut
microsoft, activision, sony, retail, gamestop
macintosh, pc, ibm, iigs, chipsets
NP-MSSG
apricot, blackberry, cabbage, blackberries, pear
microsoft, ibm, wordperfect, amiga, trs-80
FOX
Skip-gram abc, nbc, soapnet, espn, kttv
MSSG
beaver, wolf, moose, otter, swan
nbc, espn, cbs, ctv, pbs
dexter, myers, sawyer, kelly, griffith
NP-MSSG
rabbit, squirrel, wolf, badger, stoat
cbs,abc, nbc, wnyw, abc-tv
NET
Skip-gram profit, dividends, pegged, profits, nets
MSSG
snap, sideline, ball, game-trying, scoring
negative, offset, constant, hence, potential
pre-tax, billion, revenue, annualized, us$
NP-MSSG
negative, total, transfer, minimizes, loop
pre-tax, taxable, per, billion, us$, income
ball, yard, fouled, bounced, 50-yard
wnet, tvontorio, cable, tv, tv-5
ROCK
Skip-gram glam, indie, punk, band, pop
MSSG
rocks, basalt, boulders, sand, quartzite
alternative, progressive, roll, indie, blues-rock
rocks, pine, rocky, butte, deer
NP-MSSG
granite, basalt, outcropping, rocks, quartzite
alternative, indie, pop/rock, rock/metal, blues-rock
RUN
Skip-gram running, ran, runs, afoul, amok
MSSG
running, stretch, ran, pinch-hit, runs
operated , running, runs, operate, managed
running, runs, operate, drivers, configure
NP-MSSG
two-run, walk-off, runs, three-runs, starts
operated, runs, serviced, links, walk
running, operating, ran, go, configure
re-election, reelection, re-elect, unseat, term-limited
helmed, longest-running, mtv, promoted, produced
Table 2: Nearest neighbors of each sense of each
word, by cosine similarity, for different algo-
rithms. Note that the different senses closely cor-
respond to intuitions regarding the senses of the
given word types.
6.1 Nearest Neighbors
Table 2 shows qualitatively the results of dis-
covering multiple senses by presenting the near-
est neighbors associated with various embeddings.
The nearest neighbors of a word are computed by
comparing the cosine similarity between the em-
bedding for each sense of the word and the context
embeddings of all other words in the vocabulary.
Note that each of the discovered senses are indeed
semantically coherent, and that a reasonable num-
ber of senses are created by the non-parametric
method. Table 3 shows the nearest neighbors of
the word plant for Skip-gram, MSSG , NP-MSSG
and Haung?s model (Huang et al, 2012).
1064
Skip-
gram
plants, flowering, weed, fungus, biomass
MS
-SG
plants, tubers, soil, seed, biomass
refinery, reactor, coal-fired, factory, smelter
asteraceae, fabaceae, arecaceae, lamiaceae, eri-
caceae
NP
MS
-SG
plants, seeds, pollen, fungal, fungus
factory, manufacturing, refinery, bottling, steel
fabaceae, legume, asteraceae, apiaceae, flowering
power, coal-fired, hydro-power, hydroelectric, re-
finery
Hua
-ng
et al
insect, capable, food, solanaceous, subsurface
robust, belong, pitcher, comprises, eagles
food, animal, catching, catch, ecology, fly
seafood, equipment, oil, dairy, manufacturer
facility, expansion, corporation, camp, co.
treatment, skin, mechanism, sugar, drug
facility, theater, platform, structure, storage
natural, blast, energy, hurl, power
matter, physical, certain, expression, agents
vine, mute, chalcedony, quandong, excrete
Table 3: Nearest Neighbors of the word plant
for different models. We see that the discovered
senses in both our models are more semantically
coherent than Huang et al (2012) and NP-MSSG
is able to learn reasonable number of senses.
6.2 Word Similarity
We evaluate our embeddings on two related
datasets: the WordSim-353 (Finkelstein et al,
2001) dataset and the Contextual Word Similari-
ties (SCWS) dataset Huang et al (2012).
WordSim-353 is a standard dataset for evaluat-
ing word vector representations. It consists of a
list of pairs of word types, the similarity of which
is rated in an integral scale from 1 to 10. Pairs
include both monosemic and polysemic words.
These scores to each word pairs are given with-
out any contextual information, which makes them
tricky to interpret.
To overcome this issue, Stanford?s Contextual
Word Similarities (SCWS) dataset was developed
by Huang et al (2012). The dataset consists of
2003 word pairs and their sentential contexts. It
consists of 1328 noun-noun pairs, 399 verb-verb
pairs, 140 verb-noun, 97 adjective-adjective, 30
noun-adjective, 9 verb-adjective, and 241 same-
word pairs. We evaluate and compare our embed-
dings on both WordSim-353 and SCWS word sim-
ilarity corpus.
Since it is not trivial to deal with multiple em-
beddings per word, we consider the following sim-
ilarity measures between words w and w
?
given
their respective contexts c and c
?
, where P (w, c, k)
is the probability that w takes the k
th
sense given
the context c, and d(v
s
(w, i), v
s
(w
?
, j)) is the sim-
ilarity measure between the given embeddings
v
s
(w, i) and v
s
(w
?
, j).
The avgSim metric,
avgSim(w,w
?
)
=
1
K
2
K
?
i=1
K
?
j=1
d (v
s
(w, i), v
s
(w
?
, j)) ,
computes the average similarity over all embed-
dings for each word, ignoring information from
the context.
To address this, the avgSimC metric,
avgSimC(w,w
?
) =
K
?
j=1
K
?
i=1
P (w, c, i)P (w
?
, c
?
, j)
? d (v
s
(w, i), v
s
(w
?
, j))
weighs the similarity between each pair of senses
by how well does each sense fit the context at
hand.
The globalSim metric uses each word?s global
context vector, ignoring the many senses:
globalSim(w,w
?
) = d (v
g
(w), v
g
(w
?
)) .
Finally, localSim metric selects a single sense
for each word based independently on its context
and computes the similarity by
localSim(w,w
?
) = d (v
s
(w, k), v
s
(w
?
, k
?
)) ,
where k = argmax
i
P (w, c, i) and k
?
=
argmax
j
P (w
?
, c
?
, j) and P (w, c, i) is the prob-
ability that w takes the i
th
sense given context c.
The probability of being in a cluster is calculated
as the inverse of the cosine distance to the cluster
center (Huang et al, 2012).
We report the Spearman correlation between a
model?s similarity scores and the human judge-
ments in the datasets.
Table 5 shows the results on WordSim-353
task. C&W refers to the language model by Col-
lobert and Weston (2008) and HLBL model is the
method described in Mnih and Hinton (2007). On
WordSim-353 task, we see that our model per-
forms significantly better than the previous neural
network model for learning multi-representations
per word (Huang et al, 2012). Among the meth-
ods that learn low-dimensional and dense repre-
sentations, our model performs slightly better than
Skip-gram. Table 4 shows the results for the
SCWS task. In this task, when the words are
1065
Model globalSim avgSim avgSimC localSim
TF-IDF 26.3 - - -
Collobort & Weston-50d 57.0 - - -
Skip-gram-50d 63.4 - - -
Skip-gram-300d 65.2 - - -
Pruned TF-IDF 62.5 60.4 60.5 -
Huang et al-50d 58.6 62.8 65.7 26.1
MSSG-50d 62.1 64.2 66.9 49.17
MSSG-300d 65.3 67.2 69.3 57.26
NP-MSSG-50d 62.3 64.0 66.1 50.27
NP-MSSG-300d 65.5 67.3 69.1 59.80
Table 4: Experimental results in the SCWS task. The numbers are Spearmans correlation ? ? 100
between each model?s similarity judgments and the human judgments, in context. First three models
learn only a single embedding per model and hence, avgSim, avgSimC and localSim are not reported
for these models, as they?d be identical to globalSim. Both our parametric and non-parametric models
outperform the baseline models, and our best model achieves a score of 69.3 in this task. NP-MSSG
achieves the best results when globalSim, avgSim and localSim similarity measures are used. The best
results according to each metric are in bold face.
Model ?? 100
HLBL 33.2
C&W 55.3
Skip-gram-300d 70.4
Huang et al-G 22.8
Huang et al-M 64.2
MSSG 50d-G 60.6
MSSG 50d-M 63.2
MSSG 300d-G 69.2
MSSG 300d-M 70.9
NP-MSSG 50d-G 61.5
NP-MSSG 50d-M 62.4
NP-MSSG 300d-G 69.1
NP-MSSG 300d-M 68.6
Pruned TF-IDF 73.4
ESA 75
Tiered TF-IDF 76.9
Table 5: Results on the WordSim-353 dataset.
The table shows the Spearmans correlation ? be-
tween the model?s similarities and human judg-
ments. G indicates the globalSim similarity mea-
sure and M indicates avgSim measure.The best
results among models that learn low-dimensional
and dense representations are in bold face. Pruned
TF-IDF (Reisinger and Mooney, 2010a), ESA
(Gabrilovich and Markovitch, 2007) and Tiered
TF-IDF (Reisinger and Mooney, 2010b) construct
spare, high-dimensional representations.
Figure 3: The plot shows the distribution of num-
ber of senses learned per word type in NP-MSSG
model
given with their context, our model achieves new
state-of-the-art results on SCWS as shown in the
Table-4. The previous state-of-art model (Huang
et al, 2012) on this task achieves 65.7% using
the avgSimC measure, while the MSSG model
achieves the best score of 69.3% on this task. The
results on the other metrics are similar. For a
fixed embedding dimension, the model by Huang
et al (2012) has more parameters than our model
since it uses a hidden layer. The results show
that our model performs better than Huang et al
(2012) even when both the models use 50 dimen-
sional vectors and the performance of our model
improves as we increase the number of dimensions
to 300.
We evaluate the models in a word analogy task
1066
(a) (b)
Figure 4: Figures (a) and (b) show the effect of varying embedding dimensionality and number of senses
respectively of the MSSG Model on the SCWS task.
Model Task Sim ?? 100
Skip-gram WS-353 globalSim 70.4
MSSG WS-353 globalSim 68.4
MSSG WS-353 avgSim 71.2
NP MSSG WS-353 globalSim 68.3
NP MSSG WS-353 avgSim 69.66
MSSG SCWS localSim 59.3
MSSG SCWS globalSim 64.7
MSSG SCWS avgSim 67.2
MSSG SCWS avgSimC 69.2
NP MSSG SCWS localSim 60.11
NP MSSG SCWS globalSim 65.3
NP MSSG SCWS avgSim 67
NP MSSG SCWS avgSimC 68.6
Table 6: Experiment results on WordSim-353 and
SCWS Task. Multiple Embeddings are learned for
top 30,000 most frequent words in the vocabulary.
The embedding dimension size is 300 for all the
models for this task. The number of senses for
MSSG model is 3.
introduced by Mikolov et al (2013a) where both
MSSG and NP-MSSG models achieve 64% accu-
racy compared to 12% accuracy by Huang et al
(2012). Skip-gram which is the state-of-art model
for this task achieves 67% accuracy.
Figure 3 shows the distribution of number of
senses learned per word type in the NP-MSSG
model. We learn the multiple embeddings for the
same set of approximately 6000 words that were
used in Huang et al (2012) for all our experiments
to ensure fair comparision. These approximately
6000 words were choosen by Huang et al. mainly
from the top 30,00 frequent words in the vocab-
ulary. This selection was likely made to avoid
the noise of learning multiple senses for infre-
quent words. However, our method is robust to
noise, which can be seen by the good performance
of our model that learns multiple embeddings for
the top 30,000 most frequent words. We found
that even by learning multiple embeddings for the
top 30,000 most frequent words in the vocubu-
lary, MSSG model still achieves state-of-art result
on SCWS task with an avgSimC score of 69.2 as
shown in Table 6.
7 Conclusion
We present an extension to the Skip-gram model
that efficiently learns multiple embeddings per
word type. The model jointly performs word
sense discrimination and embedding learning, and
non-parametrically estimates the number of senses
per word type. Our method achieves new state-
of-the-art results in the word similarity in con-
text task and learns multiple senses, training on
close to billion tokens in less than 6 hours. The
global vectors, sense vectors and cluster centers of
our model and code for learning them are avail-
able at https://people.cs.umass.edu/
?
arvind/emnlp2014wordvectors. In fu-
ture work we plan to use the multiple embeddings
per word type in downstream NLP tasks.
1067
Acknowledgments
This work was supported in part by the Center
for Intelligent Information Retrieval and in part by
DARPA under agreement number FA8750-13-2-
0020. The U.S. Government is authorized to re-
produce and distribute reprints for Governmental
purposes notwithstanding any copyright notation
thereon. Any opinions, findings and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
those of the sponsor.
References
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring Continuous Word Representations
for Dependency Parsing. Association for Computa-
tional Linguistics (ACL).
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search (JMLR).
Peter F. Brown, Peter V. Desouza, Robert L. Mercer,
Vincent J. Della Pietra, and Jenifer C. Lai. 1992.
Class-based N-gram models of natural language
Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A Uni-
fied Architecture for Natural Language Process-
ing: Deep Neural Networks with Multitask Learn-
ing. International Conference on Machine learning
(ICML).
Paramveer S. Dhillon, Dean Foster, and Lyle Ungar.
2011. Multi-View Learning of Word Embeddings via
CCA. Advances in Neural Information Processing
Systems (NIPS).
John Duchi, Elad Hazan, and Yoram Singer 2011.
Adaptive sub- gradient methods for online learn-
ing and stochastic optimization. Journal of Machine
Learning Research (JMLR).
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: the con-
cept revisited. International Conference on World
Wide Web (WWW).
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. International Joint
Conference on Artificial Intelligence (IJCAI).
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. Association of Computational
Linguistics (ACL).
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple Semi-supervised Dependency Parsing.
Association for Computational Linguistics (ACL).
Quoc V. Le and Tomas Mikolov. 2014 Distributed
Representations of Sentences and Documents. Inter-
national Conference on Machine Learning (ICML)
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011 Learning Word Vectors for Sentiment Analysis
Association for Computational Linguistics (ACL)
Adam Meyerson. 2001 IEEE Symposium on Foun-
dations of Computer Science. International Confer-
ence on Machine Learning (ICML)
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient Estimation of Word
Representations in Vector Space. Workshop at In-
ternational Conference on Learning Representations
(ICLR).
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013b. Distributed Repre-
sentations of Words and Phrases and their Composi-
tionality. Advances in Neural Information Process-
ing Systems (NIPS).
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013c. Exploiting Similarities among Languages
for Machine Translation. arXiv.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies (NAACL-HLT).
Andriy Mnih and Geoffrey Hinton. 2007. Three
new graphical models for statistical language mod-
elling. International Conference on Machine learn-
ing (ICML).
Arvind Neelakantan and Michael Collins. 2014.
Learning Dictionaries for Named Entity Recogni-
tion using Minimal Supervision. European Chap-
ter of the Association for Computational Linguistics
(EACL).
Alexandre Passos, Vineet Kumar, and Andrew McCal-
lum. 2014. Lexicon Infused Phrase Embeddings for
Named Entity Resolution. Conference on Natural
Language Learning (CoNLL).
Lev Ratinov and Dan Roth. 2009. Design Chal-
lenges and Misconceptions in Named Entity Recog-
nition. Conference on Natural Language Learning
(CoNLL).
Siva Reddy, Ioannis P. Klapaftis, and Diana McCarthy.
2011. Dynamic and Static Prototype Vectors for Se-
mantic Composition. International Joint Conference
on Artificial Intelligence (IJCNLP).
1068
Joseph Reisinger and Raymond J. Mooney. 2010a.
Multi-prototype vector-space models of word mean-
ing. North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (NAACL-HLT)
Joseph Reisinger and Raymond Mooney. 2010b. A
mixture model with sharing for lexical semantics.
Empirical Methods in Natural Language Processing
(EMNLP).
Cyrus Shaoul and Chris Westbury. 2010. The Westbury
lab wikipedia corpus.
Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011
Dynamic Pooling and Unfolding Recursive Autoen-
coders for Paraphrase Detection. Advances in Neu-
ral Information Processing Systems (NIPS).
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszkor-
eit. 2012. Cross-lingual Word Clusters for Direct
Transfer of Linguistic Structure. North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-Supervised Learning. Association
for Computational Linguistics (ACL).
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual Word Embed-
dings for Phrase-Based Machine Translation. Em-
pirical Methods in Natural Language Processing.
1069
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 729?732,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Constraint-Driven Rank-Based Learning for Information Extraction
Sameer Singh Limin Yao Sebastian Riedel Andrew McCallum
Dept. of Computer Science
University of Massachusetts
Amherst MA 01003
{sameer,lmyao,riedel,mccallum}@cs.umass.edu
Abstract
Most learning algorithms for undirected
graphical models require complete inference
over at least one instance before parameter up-
dates can be made. SampleRank is a rank-
based learning framework that alleviates this
problem by updating the parameters during in-
ference. Most semi-supervised learning algo-
rithms also perform full inference on at least
one instance before each parameter update.
We extend SampleRank to semi-supervised
learning in order to circumvent this compu-
tational bottleneck. Different approaches to
incorporate unlabeled data and prior knowl-
edge into this framework are explored. When
evaluated on a standard information extraction
dataset, our method significantly outperforms
the supervised method, and matches results of
a competing state-of-the-art semi-supervised
learning approach.
1 Introduction
Most supervised learning algorithms for undirected
graphical models require full inference over the
dataset (e.g., gradient descent), small subsets of the
dataset (e.g., stochastic gradient descent), or at least
a single instance (e.g., perceptron, Collins (2002))
before parameter updates are made. Often this is the
main computational bottleneck during training.
SampleRank (Wick et al, 2009) is a rank-based
learning framework that alleviates this problem by
performing parameter updates within inference. Ev-
ery pair of samples generated during inference is
ranked according to the model and the ground truth,
and the parameters are updated when the rankings
disagree. SampleRank has enabled efficient learn-
ing for massive information extraction tasks (Culotta
et al, 2007; Singh et al, 2009).
The problem of requiring a complete inference it-
eration before parameters are updated also exists in
the semi-supervised learning scenario. Here the sit-
uation is often considerably worse since inference
has to be applied to potentially very large unlabeled
datasets. Most semi-supervised learning algorithms
rely on marginals (GE, Mann and McCallum, 2008)
or MAP assignments (CODL, Chang et al, 2007).
Calculating these is computationally inexpensive for
many simple tasks (such as classification and re-
gression). However, marginal and MAP inference
tends to be expensive for complex structured pre-
diction models (such as the joint information extrac-
tion models of Singh et al (2009)), making semi-
supervised learning intractable.
In this work we employ a fast rank-based learning
algorithm for semi-supervised learning to circum-
vent the inference bottleneck. The ranking function
is extended to capture both the preference expressed
by the labeled data, and the preference of the domain
expert when the labels are not available. This allows
us to perform SampleRank as is, without sacrificing
its scalability, which is crucial for future large scale
applications of semi-supervised learning.
We applied our method to a standard information
extraction dataset used for semi-supervised learning.
Empirically we demonstrate improvements over the
supervised model, and closely match the results of a
competing state-of-the-art semi-supervised learner.
2 Background
Conditional random fields (Lafferty et al, 2001) are
undirected graphical models represented as factor
729
graphs. A factor graph G = {?i} defines a prob-
ability distribution over assignments y to a set of
output variables, conditioned on an observation x.
A factor ?i computes the inner product between
the vector of sufficient statistics f(xi,yi) and pa-
rameters ?. Let Z(x) be the data-dependent par-
tition function used for normalization. The proba-
bility distribution defined by the graph is:
p(y|x,?) =
1
Z(x)
?
?i?G
e??f(xi,yi)
2.1 Rank-Based Learning
SampleRank (Wick et al, 2009) is a rank-based
learning framework for that performs parameter up-
dates within MCMC inference. Every pair of con-
secutive samples in the MCMC chain is ranked ac-
cording to the model and the ground truth, and the
parameters are updated when the rankings disagree.
This allows the learner to acquire more supervision
per sample, and has led to efficient training of mod-
els for which inference is very expensive (Singh
et al, 2009).
SampleRank considers two ranking functions: (1)
the unnormalized conditional probability (model
ranking), and (2) a truth function F(y) (objective
ranking) which is defined as ?L(y,yL), the neg-
ative loss between the possible assignment y and
the true assignment yL. The truth function can take
different forms, such as tokenwise accuracy or F1-
measure with respect to some labeled data.
In order to learn the parameters for which model
rankings are consistent with objective rankings,
SampleRank performs the following update for each
consecutive pair of samples ya and yb of the MCMC
chain. Let ? be the learning rate, and ? =
f(xi,yai )? f(xi,y
b
i ), then ? is updated as follows:
?
+
?
?
??
??
?? if p(y
a|x)
p(yb|x) < 1 ? F(y
a) > F(yb)
??? if p(y
a|x)
p(yb|x) > 1 ? F(y
a) < F(yb)
0 otherwise.
This update is usually fast: in order to calculate
the required model ratio, only factors that touch
changed variables have to be taken into account.
SampleRank has been incorporated into the FAC-
TORIE toolkit for probabilistic programming with
imperatively-defined factor graphs (McCallum et al,
2009).
3 Semi-Supervised Rank-Based Learning
To apply SampleRank to the semi-supervised set-
ting, we need to specify the truth function F over
both labeled and unlabeled data. For labeled data
YL, we can use the true labels. These are not avail-
able for unlabeled data YU , and we present alterna-
tive ways of defining a truth function FU : YU ? <
for this case.
3.1 Self-Training
Self-training, which uses predictions as truth, fits di-
rectly into our SampleRank framework. After per-
forming SampleRank on training data (using FL),
MAP inference is performed on the unlabeled data.
The prediction y?U is used as the ground truth for
the unlabeled data. Thus the self-training objective
function Fs over the unlabeled data can be defined
as Fs(y) = ?L(y, y?U ).
3.2 Encoding Constraints
Constraint-driven semi-supervised learning uses
constraints to incorporate external domain knowl-
edge when labels are missing (Chang et al, 2007;
Mann and McCallum, 2008; Bellare et al, 2009).
Constraints prefer certain label configurations over
others. For example, one constraint may be that oc-
currences of the word ?California? are preferred to
have the label ?location?.
We can encode constraints directly into the objec-
tive function FU . Let a constraint i be specified as
?pi, ci?, where ci(y) denotes whether assignment y
satisfies the constraint i (+1), violates it (?1), or the
constraint does not apply (0), and pi is the constraint
strength. Then the objective function is:
Fc(y) =
?
i
pici(y)
3.3 Incorporating Model Predictions
When the objective function Fc is used, every pre-
diction on unlabeled data is ranked only according to
the constraints, and thus the model is trained to sat-
isfy all the constraints. This is a problem when the
constraints prefer a wrong solution while the model
favors the correct solution, resulting in SampleR-
ank updating the model away from the true solution.
To avoid this, the ranking function needs to balance
preferences of the constraints and the current model.
730
One option is to incorporate the self-training ob-
jective function Fs. A new objective function that
combines self-training with constraints can be de-
fined as:
Fsc(y) = Fs(y) + ?sFc(y)
= ?L(y, y?U ) + ?s
?
i
pici(y)
This objective function has at least two limita-
tions. First, self-training involves a complete infer-
ence step to obtain y?U . Second, the model might
have low confidence in its prediction (this is the case
when the underlying marginals are almost uniform),
but the self-training objective des not take this into
account. Hence, we also propose an objective func-
tion that incorporates the model score directly, i.e.
Fmc(y) = log p(y|x,?) + logZ(x) + ?mFc(y)
=
?
?i
? ? f(xi,yi) + ?m
?
i
pici(y)
This objective does not require inference, and also
takes into account model confidence.
In both objective functions Fsc and Fmc, ? con-
trols the relative contribution of the constraint pref-
erences to the objective function. With higher ?,
SampleRank will make updates that never try to vi-
olate constraints, while with low ?, SampleRank
trusts the model more. ? corresponds to constraint
satisfaction weights ? used in (Chang et al, 2007).
4 Related Work
Chang et al propose constraint-driven learn-
ing (CODL, Chang et al, 2007) which can be in-
terpreted as a variation of self-training: Instances
are selected for supervision based not only on the
model?s prediction, but also on their consistency
with a set of user-defined constraints. By directly in-
corporating the model score and the constraints (as
inFmc in Section 3.3) we follow the same approach,
but avoid the expensive ?Top-K? inference step.
Generalized expectation criterion (GE, Mann and
McCallum, 2008) and Alternating Projections (AP,
Bellare et al, 2009) encode preferences by speci-
fying constraints on feature expectations, which re-
quire expensive inference. Although AP can use on-
line training, it still involves full inference over each
instance. Furthermore, these methods only support
constraints that factorize according to the model.
Li (2009) incorporates prior knowledge into con-
ditional random fields as variables. They require full
inference during learning, restricting the application
to simple models. Furthermore, higher-order con-
straints are specified using large cliques in the graph,
which slow down inference. Our approach directly
incorporates these constraints into the ranking func-
tion, with no impact on inference time.
5 Experiments
We carried out experiments on the Cora citation
dataset. The task is to segment each citation into
different fields, such as ?author? and ?title?. We use
300 instances as training data, 100 instances as de-
velopment data, and 100 instances as test data. Some
instances from the training data are selected as la-
beled instances, and the remaining data (including
development) as unlabeled. We use the same token-
label constraints as Chang et al (2007).
We use the objective functions defined in Sec-
tion 3, specifically self-training (Self:Fs), direct
constraints (Cons:Fc), the combination of the two
(Self+Cons:Fsc), and combination of the model
score and the constraints (Model+Cons:Fmc). We
set pi = 1.0, ? = 1.0, ?s = 10, and ?m = 0.0001.
Average token accuracy for 5 runs is reported and
compared with CODL1 in Table 1. We also report
supervised results from (Chang et al, 2007) and
SampleRank. All of our methods show vast im-
provement over the supervised method for smaller
training sizes, but this difference decreases as the
training size increases. When the complete training
data is used, additional unlabeled data hurts our per-
formance. This is not observed in CODL since they
use more unlabeled data, which may also explain
their slightly higher accuracy. Note that Self+Cons
performs better than Self or Cons individually.
Model+Cons also performs competitively, and
may potentially outperform other methods if a bet-
ter ?m is chosen. Note, however, that ?m is much
harder to tune than ?s since ?m weighs the contri-
bution of the unnormalized model score, the range
1We report inference without constraints results from
CODL. Their results that incorporated constraints were higher,
but we do not implement this alternative due to the difficulty in
balancing the model score and constraint weights.
731
Method 5 10 15 20 25 300
Sup. (CODL) 55.1 64.6 68.7 70.1 72.7 86.1
SampleRank 66.5 74.6 75.6 77.6 79.5 90.7
CODL 71 76.7 79.4 79.4 82 88.2
Self 67.6 75.1 75.8 78.6 80.4 88
Cons 67.2 75.3 77.5 78.6 79.4 88.3
Self+Cons 71.3 77 77.5 79.5 81.1 87.4
Model+Cons 69.8 75.4 75.7 79.3 79.3 90.6
Table 1: Tokenwise Accuracy: for different methods as we vary the size of the labeled data
of which depends on many different factors such as
properties of the data, the learning rate, number of
samples, proposal function, etc. For self+cons (?s),
the ranges of the predictions and constraint penalties
are fixed and known, making the task simpler.
Self training takes 90 minutes to run on average,
while Self+Cons and Model+Cons need 100 min-
utes. Since the Cons method skips the inference
step over unlabeled data, it takes only 30 minutes
to run. As the size of the model and unlabeled data
set grows, this saving will become more significant.
Running time of CODL was not reported.
6 Conclusion
This work extends the rank-based learning frame-
work to semi-supervised learning. By integrating
the two paradigms, we retain the computational effi-
ciency provided by parameter updates within infer-
ence, while utilizing unlabeled data and prior knowl-
edge. We demonstrate accuracy improvements on a
real-word information extraction dataset.
We believe that the method will be of greater ben-
efit to learning in complex factor graphs such as
joint models over multiple extraction tasks. In future
work we will investigate our approach in such set-
tings. Additionally, various sensitivity, convergence,
and robustness properties of the method need to be
analyzed.
Acknowledgments
This work was supported in part by the Center for In-
telligent Information Retrieval, in part by SRI Inter-
national subcontract #27-001338 and ARFL prime
contract #FA8750-09-C-0181, and in part by The
Central Intelligence Agency, the National Secu-
rity Agency and National Science Foundation under
NSF grant #IIS-0326249. Any opinions, findings
and conclusions or recommendations expressed in
this material are the authors? and do not necessarily
reflect those of the sponsor.
References
Kedar Bellare, Gregory Druck, and Andrew McCallum.
Alternating projections for learning with expectation
constraints. In UAI, 2009.
Mingwei Chang, Lev Ratinov, and Dan Roth. Guiding
semi-supervision with constraint-driven learning. In
ACL, 2007.
Michael Collins. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithm. In ACL, 2002.
Aron Culotta, Michael Wick, and Andrew McCallum.
First-order probabilistic models for coreference reso-
lution. In NAACL/HLT, 2007.
John Lafferty, Andrew McCallum, and Fernando Pereira.
Conditional random fields: probabilistic models for
segmenting and labeling sequence data. In ICML,
2001.
Xiao Li. On the use of virtual evidence in conditional
random fields. In EMNLP, 2009.
Gideon S. Mann and Andrew McCallum. Generalized ex-
pectation criteria for semi-supervised learning of con-
ditional random fields. In ACL, 2008.
Andrew McCallum, Karl Schultz, and Sameer Singh.
FACTORIE: probabilistic programming via impera-
tively defined factor graphs. In NIPS, 2009.
Sameer Singh, Karl Schultz, and Andrew McCallum.
Bi-directional joint inference for entity resolution
and segmentation using imperatively-defined factor
graphs. In ECML/PKDD, 2009.
Michael Wick, Khashayar Rohanimanesh, Aron Culotta,
and Andrew McCallum. SampleRank: Learning pref-
erences from atomic gradients. In NIPS Workshop on
Advances in Ranking, 2009.
732
Proceedings of NAACL-HLT 2013, pages 74?84,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Relation Extraction with Matrix Factorization and Universal Schemas
Sebastian Riedel
Department of Computer Science
University College London
s.riedel@ucl.ac.uk
Limin Yao, Andrew McCallum, Benjamin M. Marlin
Department of Computer Science
University of Massachusetts at Amherst
{lmyao,mccallum,marlin}@cs.umass.edu
Abstract
Traditional relation extraction predicts rela-
tions within some fixed and finite target
schema. Machine learning approaches to this
task require either manual annotation or, in
the case of distant supervision, existing struc-
tured sources of the same schema. The need
for existing datasets can be avoided by us-
ing a universal schema: the union of all in-
volved schemas (surface form predicates as in
OpenIE, and relations in the schemas of pre-
existing databases). This schema has an al-
most unlimited set of relations (due to surface
forms), and supports integration with existing
structured data (through the relation types of
existing databases). To populate a database of
such schema we present matrix factorization
models that learn latent feature vectors for en-
tity tuples and relations. We show that such
latent models achieve substantially higher ac-
curacy than a traditional classification ap-
proach. More importantly, by operating simul-
taneously on relations observed in text and in
pre-existing structured DBs such as Freebase,
we are able to reason about unstructured and
structured data in mutually-supporting ways.
By doing so our approach outperforms state-
of-the-art distant supervision.
1 Introduction
Most previous work in relation extraction uses a pre-
defined, finite and fixed schema of relation types
(such as born-in or employed-by). Usually some tex-
tual data is labeled according to this schema, and
this labeling is then used in supervised training of
an automated relation extractor, e.g. Culotta and
Sorensen (2004). However, labeling textual rela-
tions is time-consuming and difficult, leading to sig-
nificant recent interest in distantly-supervised learn-
ing. Here one aligns existing database records with
the sentences in which these records have been ?ren-
dered???effectively labeling the text?and from this
labeling we can train a machine learning system as
before (Craven and Kumlien, 1999; Mintz et al,
2009; Bunescu and Mooney, 2007; Riedel et al,
2010). However, this method relies on the availabil-
ity of a large database that has the desired schema.
The need for pre-existing datasets can be avoided
by using language itself as the source of the schema.
This is the approach taken by OpenIE (Etzioni et al,
2008). Here surface patterns between mentions of
concepts serve as relations. This approach requires
no supervision and has tremendous flexibility, but
lacks the ability to generalize. For example, Ope-
nIE may find FERGUSON?historian-at?HARVARD
but does not know FERGUSON?is-a-professor-at?
HARVARD. OpenIE has traditionally relied on a
large diversity of textual expressions to provide good
coverage. But this diversity is not always available,
and, in any case, the lack of generalization greatly
inhibits the ability to support reasoning.
One way to gain generalization is to cluster tex-
tual surface forms that have similar meaning (Lin
and Pantel, 2001; Pantel et al, 2007; Yates and
Etzioni, 2009; Yao et al, 2011). While the clus-
ters discovered by all these methods usually contain
semantically related items, closer inspection invari-
ably shows that they do not provide reliable impli-
cature. For example, a typical representative clus-
ter may include historian-at, professor-at, scientist-
at, worked-at. Although these relation types are in-
deed semantically related, note that scientist-at does
not necessarily imply professor-at, and worked-at
74
certainly does not imply scientist-at. In fact, we
contend that any relational schema would inherently
be brittle and ill-defined??having ambiguities, prob-
lematic boundary cases, and incompleteness.1 For
example, Freebase, in spite of its extensive effort to-
wards high coverage, has no critized nor scientist-at
relation.
In response to this problem, we present a new ap-
proach: implicature with universal schemas. Here
we embrace the diversity and ambiguity of original
inputs; we avoid forcing textual meaning into pre-
defined boxes. This is accomplished by defining
our schema to be the union of all source schemas:
original input forms, e.g. variants of surface pat-
terns similarly to OpenIE, as well as relations in
the schemas of many available pre-existing struc-
tured databases. But then, unlike OpenIE, our fo-
cus lies on learning asymmetric implicature among
relations. This allows us to probabilistically ?fill
in? inferred unobserved entity-entity relations in
this union. For example, after observing FERGU-
SON?historian-at?HARVARD our system infers that
FERGUSON?professor-at?HARVARD, but not vice
versa.
At the heart of our approach is the hypothesis that
we should concentrate on predicting source data??a
relatively well defined task that can be evaluated and
optimized??as opposed to modeling semantic equiv-
alence, which we believe will always be illusive.
Note that by operating simultaneously on rela-
tions observed in text and in pre-existing structured
databases such as Freebase, we are able to reason
about unstructured and structured data in mutually-
supporting ways. For example, we can predict sur-
face pattern relations that effectively serve as addi-
tional features when predicting Freebase relations,
hence improving generalization. Also notice that
users of our system will not have to study and un-
derstand the complexities of a particular schema in
order to issue queries; they can ask in whatever form
naturally occurs to them, and our system will likely
already have that relation in our universal schema.
Our technical approach is based on extensions
to probabilistic models of matrix factorization and
1At NAACL 2012 Lucy Vanderwende asked ?Where do the
relation types come from?? There was no satisfying answer. At
the same meeting, and in line with Brachman (1983), Ed Hovy
stated ?We don?t even know what is-a means.?
collaborative filtering (Collins et al, 2001; Koren,
2008; Rendle et al, 2009). We represent the prob-
abilistic knowledge base as a matrix with entity-
entity pairs in the rows and relations in the columns
(see figure 1). The rows come from running cross-
document entity resolution across pre-existing struc-
tured databases and textual corpora. The columns
come from the union of surface forms and DB rela-
tions. We present a series of models that learn lower
dimensional manifolds for tuples, relations and enti-
ties, and a set of weights that capture direct correla-
tions between relations. Weights and lower dimen-
sional representations act, through dot products, as
the natural parameters of a single log-linear model
to derive per-cell probabilities.
In experiments we show that our models can ac-
curately predict surface patterns relationships which
do not appear explicitly in text, and that learning la-
tent representations of entities, tuples and relations
substantially improves results over a traditional clas-
sifier approach. Moreover, we can improve accu-
racy by simultaneously operating on relations ob-
served in the New York Times corpus and in Free-
base. In particular, our model outperforms the cur-
rent state-of-the-art distant supervision method (Sur-
deanu et al, 2012) by 10% points Mean Average
Precision through joint implicature among surface
patterns and Freebase relations.
2 Model
Before we present our approach in more detail, we
briefly introduce some notation. We use R to de-
note the set of relations we seek to predict (such as
works-written in Freebase, or the X?historian-at?Y
pattern), and T to denote the set of input tuples. For
simplicity we assume each relation to be binary, al-
though our approach can be easily generalized to the
n-ary case. Given a relation r ? R and a tuple t ? T
the pair ?r, t? is a fact, or relation instance. The in-
put to our model is a set of observed facts O, and
the observed facts for a given tuple is denoted by
Ot := {?r, t? ? O}.
Our goal is a model that can estimate, for a
given relation r (such as X?historian-at?Y) and a
given tuple t (such as <FERGUSON,HARVARD>),
the probability p (yr,t = 1) where yr,t is a binary
random variable that is true iff t is in relation r. We
75
Tr
a
i
n
0.95
T
e
s
t
Surface Patterns KB Relations
X-professor-at-Y
1
1
0.05
X-historian-at-Y employee(X,Y) member(X,Y)
1 1
1
1
0.97
Rel. Extraction
1 0.93
0.97
Cluster Align
Reasoning with Universal Schema
F
e
r
g
u
s
o
n
,
H
a
r
v
a
r
d
O
m
a
n
,
O
x
f
o
r
d
F
i
r
t
h
,
O
x
f
o
r
d
G
?
d
e
l
,
P
r
i
n
c
e
t
o
n
0.95
Figure 1: Filling up a database of universal schema.
Dark circles are observed facts, shaded circles are in-
ferred facts. Relation Extraction (RE) maps surface pat-
tern relations (and other features) to structured relations.
Surface form clustering models correlations between pat-
terns, and can be fed into RE (Yao et al, 2011). Database
alignment and integration models correlations between
structured relations (not done in this work). Reasoning
with the universal schema incorporates these tasks in a
joint fashion.
introduce a series of exponential family models that
estimate this probability using a natural parameter
?r,t and the logistic function:
p (yr,t = 1|?r,t) := ? (?r,t) =
1
1 + exp (??r,t)
.
We will first describe our models through differ-
ent definitions of the natural parameter ?r,t. In each
case ?r,t will be a function of r, t and a set of weights
and/or latent feature vectors. In section 2.5 we will
then show how these weights and vectors can be es-
timated based on the observed facts O.
Notice that we can interpret p (yr,t = 1) as the
probability that a customer t likes product r. This
analogy allows us to draw from a large body of work
in collaborative filtering, such as work in probabilis-
tic matrix factorization and implicit feedback.
2.1 Latent Feature Model
One way to define ?r,t is through a latent feature
model F. Here we measure compatibility between
relation r and tuple t as dot product of two latent
feature representations of size KF: ar for relation r,
and vt for tuple t. This gives:
?Fr,t :=
KF?
k
ar,kvt,k.
This corresponds to generalized PCA (Collins et al,
2001), a model were the matrix ? = (?r,t) of natural
parameters is defined as the low rank factorization
AV.
Notice that we intentionally omit any per-relation
bias-terms. In section 4 we evaluate ranked answers
to queries on a per-relation basis, and a per-relation
bias term will have no effect on ranking facts of the
same relation. Also consider that such latent feature
models can capture asymmetry by assigning more
peaked vectors to specific relations, and more uni-
form vectors to general relations.
2.2 Neighborhood Model
We can interpolate the confidence for a given tuple
and relation based on the trueness of other similar
relations for the same tuple. In collaborative filter-
ing this is referred to as a neighborhood-based ap-
proach (Koren, 2008). In terms of our natural pa-
rameter, we implement a neighborhood model N via
a set of weights wr,r? , where each corresponds to a
directed association strength between relations r and
r?. For a given tuple t and relation r we then sum
up the weights corresponding to all relations r? that
have been observed for tuple t:
?Nr,t :=
?
(r?,t)?O\{(r,t)}
wr,r? .
Notice that the neighborhood model amounts to
a collection of local log-linear classifiers, one for
each relation r with feature functions fr,r? (t) =
I [r? 6= r ? (r?, t) ? O] and weights wr. This means
that in contrast to model F, this model cannot har-
ness any synergies between textual and pre-existing
DB relations.
76
2.3 Entity Model
Relations have selectional preferences: they allow
only certain types in their argument slots. While
knowledge bases such as Freebase or DBPedia have
extensive ontologies of types of entities, these are of-
ten not sufficiently fine to allow relations to discrim-
inate (Yao et al, 2012b). Hence, instead of using a
predetermined set of entity types, in our entity model
E we learn a latent entity representation from data.
More concretely, for each entity e we introduce a la-
tent feature vector te of dimension KE. In addition,
for each relation r and argument slot i we introduce
a feature vector di of the same dimension. For ex-
ample, binary relations have feature representations
d1 for argument 1, and d2 for argument 2. Mea-
suring compatibility of an entity tuple and relation
amounts to measuring, and summing up, compati-
bility between each argument slot representation and
the corresponding entity representation. This leads
to:
?Er,t :=
arity(r)?
i=1
KE?
k
di,ktti,k.
Note that due to entity resolution, tuples may
share entities, and hence parameters are shared
across rows.
2.4 Combined Model
In practice all the above models can capture impor-
tant aspects of the data. Hence we also use various
combinations, such as:
?NFEr,t := ?
N
r,t + ?
F
r,t + ?
E
r,t.
2.5 Parameter Estimation
Our models are parametrized through weights and
latent component vectors. We could estimate these
parameters by maximizing the loglikelihood of the
observed data akin to Collins et al (2001). How-
ever, as we do not have access to negative facts, the
model would simply learn to predict all facts to be
true. In our initial attempt to overcome this issue
we sampled a set of unobserved facts as designated
negative facts, as is done in related distant supervi-
sion approaches. However, we found that (a) our
results were sensitive to the choice of negative data
and (b) runtime was increased substantially because
of a large number of required negative facts.
In collaborative filtering positive-only data is also
known as implicit feedback. This type of feedback
arises, for example, when users buy but not rate
items. One successful approach to learning with im-
plicit feedback is based on the observation that the
actual task is not necessarily one of prediction (here:
to predict a number between 0 and 1) but one of
(generally simpler) ranking: to give true ?user-item?
cells higher scores than false ones. Bayesian Person-
alized Ranking (BPR) uses a variant of this ranking:
giving observed true facts higher scores than unob-
served (true or false) facts (Rendle et al, 2009). This
relaxed constraint is to be contrasted with the log-
likelihood setting that essentially requires (randomly
sampled) negative facts to score below a globally de-
fined threshold.
2.5.1 Objective
We first create a dataset of ranked pairs: for each
relation r and each observed fact f+ := ?r, t+? ? O
we choose all tuples t? such that f? := ?r, t?? /?
O?that is, tuples we have not observed to be in
relation r. For each pair of facts f+ and f? we
want p (f+) > p (f?) and hence ?f+ > ?f? . In
BPR this is achieved by maximizing a sum terms of
the form Objf+,f? := log
(
?
(
?f+ ? ?f?
))
, one for
each ranked pair:
Obj :=
?
?r,t+??O
?
?r,t??/?O
Obj?r,t+?,?r,t??. (1)
Notice that this objective differs slightly from the
one used by Rendle et al (2009). Consider tuples
as users and items as relations. We rank different
users with respect to the same item, while BPR ranks
items with respect to the same user. Also notice that
the BPR objective is an approximation to the per-
relation AUC (area under the ROC curve), and hence
directly correlated to what we want to achieve: well-
ranked tuples per relation.
Note that all parameters are regularized with
quadratic penalty which we omit here for brevity.
2.5.2 Optimization
To maximize the objective2 in equation 1 we fol-
low Rendle et al (2009) and employ Stochastic Gra-
dient Descent (SGD). In particular, in each epoch
2This objective is non-convex for all models excluding the
N model.
77
we sample |O| facts with replacement from O. For
each sampled fact ?r, t+? we then sample a tuple
t? ? T such that ?r, t?? /? O is not an observed
fact. This gives us |O| fact pairs ?f+, f??, and for
each pair we do an SGD update using the corre-
sponding gradients of Objf+,f? . For the F model
the gradients correspond to those presented by Ren-
dle et al (2009). The remaining gradients are easy
to derive; we omit details for brevity.
3 Related Work
This work extends a previous workshop paper (Yao
et al, 2012a) by introducing the neighborhood and
entity model, by working with the BPR objective,
and by more extensive experiments.
Relational Clustering There is a large body of
work aiming to discover latent relations by clus-
tering surface patterns (Hasegawa et al, 2004;
Shinyama and Sekine, 2006; Kok and Domingos,
2008; Yao et al, 2011; Takamatsu et al, 2011), or
by inducing synonymy relationships between pat-
terns independently of the entities (Yates and Et-
zioni, 2009; Pantel et al, 2007; Lin and Pantel,
2001). Our approach has a fundamentally different
objective: we are not (primarily) interested in clus-
ters of patterns or their semantic representation, but
in predicting patterns where they are not observed.
Moreover, these related methods rely on a symmetric
notion of synonymy in which clustered patterns are
assumed to have the same meaning. Our approach
rejects this assumption in favor of a model which
learns that certain patterns, or combinations thereof,
entail others in one direction, but not necessarily the
other. This is similar in spirit to work on learning
entailment rules (Szpektor et al, 2004; Zanzotto et
al., 2006; Szpektor and Dagan, 2008). However, for
us even entailment rules are just a by-product of our
goal to improve prediction, and it is this goal we di-
rectly optimize for and evaluate.
Matrix Factorization Our approach is also re-
lated to work on factorizing YAGO to predict new
links (Nickel et al, 2012). The primary differences
are that we include surface patterns in our schema,
use a ranking objective, and learn latent vectors for
entities and tuples. Likewise, matrix factorization in
various flavors has received significant attention in
the lexical semantics community, from LSA to re-
cent work on non-negative sparse embeddings (Mur-
phy et al, 2012). In our problem columns corre-
spond to relations, and rows correspond to entity tu-
ples. By contrast, there columns are words, and rows
are contextual features such as ?words in a local win-
dow.? Consequently, our objective is to complete
the matrix, whereas their objective is to learn better
latent embeddings of words (which by themselves
again cannot capture any sense of asymmetry).
OpenIE Open IE (Etzioni et al, 2008) extracts
facts mentioned in text, but does not predict poten-
tial facts not mentioned in text. Finding answers
requires explicit mentions, and hence suffers from
lower recall for not-so-frequently mentioned facts.
Methods that learn rules between textual patterns in
OpenIE aim at a similar goal as our proposed ap-
proach (Schoenmackers et al, 2008; Schoenmack-
ers et al, 2010). However, their approach is sub-
stantially more complex, requires a categorization
of entities into fine grained entity types, and needs
inference in high tree-width Markov Networks. By
contrast, our approach is based on a single unified
model, requires no entity types, and for us inferring
a fact amounts to not more than a few dot products.
In addition, in our Universal Schema approach Ope-
nIE surface patterns are just one kind of relations,
and our aim is populate relations of all kinds. In the
future we may even include relations between enti-
ties and continuous attributes (say, gene expression
measurements).
Distant Supervision In Distant Supervision (DS)
a set of facts from pre-existing structured sources
is aligned with surface patterns mentioned in
text (Bunescu and Mooney, 2007; Mintz et al, 2009;
Riedel et al, 2010; Hoffmann et al, 2011; Surdeanu
et al, 2012), and this alignment is then used to train
a relation extractor. A core difference to our ap-
proach is the number of target relations: In DS it
is the relatively small schema size of the knowledge
base, while we also include surface patterns. This
allows us to answer more expressive queries. More-
over, by learning from surface-pattern correlations,
our latent models induce feature representations for
patterns that do not appear in the DS training set. As
we will see in section 4, this allows us to outperform
state-of-the-art DS models.
78
Never-Ending Learning and Bootstrapping Our
latent feature models are capable of never-ending
learning (Carlson et al, 2010). That is, we can con-
tinue to train these models with incoming data, even
if no structured annotation is available. In bootstrap-
ping approaches the current model is used to predict
new relations, and these hypothesized relations are
used as new supervision targets (i.e. self-training).
By contrast, our model only strengthens the correla-
tions between incoming co-occurring observations.
This has the advantage that wrong predictions are
less likely be reinforced, hence reducing the risk of
semantic drift.
4 Experiments
How accurately can we fill a database of Universal
Schema, and does reasoning jointly across a uni-
versal schema help to improve over more isolated
approaches? In the following we seek to answer
this question empirically. To this end we train our
models on observed facts in a newswire corpus and
Freebase, and then manually evaluate ranked predic-
tions: first for structured relations and then for sur-
face form relations.
4.1 Data
Following previous work (Riedel et al, 2010),
our documents are taken from the NYTimes cor-
pus (Sandhaus, 2008). Articles after 2000 are used
as training corpus, articles from 1990 to 1999 as
test corpus. We also split Freebase facts 50/50 into
train and test facts, and their corresponding tuples
into train and test tuples. Then we align training tu-
ples with the training corpus, and test tuples with the
test corpus. This alignment relies on a preprocessing
step that links NER mentions in text with entities in
Freebase. In our case we use a simple string-match
heuristic to find this linking. Now we align an entity
tuple ?t1, t2? with a pair of mentions ?m1,m2? in
the same sentence if m1 is linked to t1 and m2 to t2.
Based on this alignment we filter out all relations for
which we find fewer than 10 tuples with mentions in
text.
The above alignment and filtering process reduces
the total number of tuples related according to Free-
base to 16k: approximately 8k tuples with facts
mentioned in the training set, and approximately 8k
such tuples for the test set. In addition we have a
set of approximately 200k training tuples for which
both arguments appear in the same sentence and
both can be linked to Freebase entities, but for which
no Freebase fact is recorded. This can either be be-
cause they are not related, or simply because Free-
base does not contain the relationship yet. We also
have about 200k such tuples in the test set. To sim-
plify evaluation, we create a subsampled test set by
randomly choosing 10k of the original test set tuples.
The above alignment allows us to determine, for
each tuple t, the observed facts Ot as follows. To
find the surface pattern facts OPATt for the tuple t =
?t1, t2? we extract, for each mention m = ?m1,m2?
of t, the lexicalized dependency path p between m1
and m2. Then we add ?p, t? to OPATt . For example,
we get ?<-subj<-head->obj->? for ?M1 heads M2.?
Filtering out patterns with fewer than 10 mentions
in text yields approximately 4k patterns. For train-
ing tuples we add as Freebase facts OFBt all facts
?r, t? that appear in Freebase, and for which r has
not been filtered out beforehand. For the test setOFBt
remains empty. The total set of observed facts Ot is
OFBt ?O
PAT
t , and their union over all tuples forms the
set of observed facts O.
4.2 Evaluation
For evaluation we use collections of relations: sur-
face patterns in one experiment and Freebase re-
lations in the other. In either case we compare
the competing systems with respect to their ranked
results for each relation in the collection. Given
this ranking task, our evaluation is inspired by the
TREC competitions and work in information re-
trieval (Manning et al, 2008). That is, we treat
each relation as query and receive the top 1000 (run
depth) entity pairs from each system. Then we pool
the top 100 (pool depth) answers from each system
and manually judge their relevance or ?truth.? This
gives a set of relevant results that we can use to cal-
culate recall and precision measures. In particular,
we can use these annotations to measure an average
precision across the precision-recall curve, and an
aggregate mean average precision (MAP) across all
relations. This metric has shown to be very robust
and stable (Manning et al, 2008). In addition we
also present a weighted version of MAP (weighted
MAP) in which the average precision for each re-
79
Relation # MI09 YA11 SU12 N F NF NFE
person/company 103 0.67 0.64 0.70 0.73 0.75 0.76 0.79
location/containedby 74 0.48 0.51 0.54 0.43 0.68 0.67 0.69
author/works_written 29 0.50 0.51 0.52 0.45 0.61 0.63 0.69
person/nationality 28 0.14 0.40 0.13 0.13 0.19 0.18 0.21
parent/child 19 0.14 0.25 0.62 0.46 0.76 0.78 0.76
person/place_of_death 19 0.79 0.79 0.86 0.89 0.83 0.85 0.86
person/place_of_birth 18 0.78 0.75 0.82 0.50 0.83 0.81 0.89
neighborhood/neighborhood_of 12 0.00 0.00 0.08 0.43 0.65 0.66 0.72
person/parents 7 0.24 0.27 0.58 0.56 0.53 0.58 0.39
company/founders 4 0.25 0.25 0.53 0.24 0.77 0.80 0.68
film/directed_by 4 0.06 0.15 0.25 0.09 0.26 0.26 0.30
sports_team/league 4 0.00 0.43 0.18 0.21 0.59 0.70 0.63
team/arena_stadium 3 0.00 0.06 0.06 0.03 0.08 0.09 0.08
team_owner/teams_owned 2 0.00 0.50 0.70 0.55 0.38 0.61 0.75
roadcast/area_served 2 1.00 0.50 1.00 0.58 0.58 0.83 1.00
structure/architect 2 0.00 0.00 1.00 0.27 1.00 1.00 1.00
composer/compositions 2 0.00 0.00 0.00 0.50 0.67 0.83 0.12
person/religion 1 0.00 1.00 1.00 0.50 1.00 1.00 1.00
film/produced_by 1 1.00 1.00 1.00 1.00 0.50 0.50 0.33
MAP 0.32 0.42 0.56 0.45 0.61 0.66 0.63
Weighted MAP 0.48 0.52 0.57 0.52 0.66 0.67 0.69
Table 1: Average and (weighted) Mean Average Precisions for Freebase relations based on pooled results. The #
column shows the number of true facts in the pool. NFE is statistically different to all but NF and F according to the
sign test. Bold faced are winners per relation, italics indicate ties.
lation is weighted by the relation?s number of true
facts.
Notice that we deviate from previous work in dis-
tant supervision that (a) combines the results from
several relations in a single precision recall curve,
and (b) uses held-out evaluation to measure how
well the predictions match existing Freebase facts.
This has several benefits. First, when aggregating
across relations results are often dominated by a few
very frequent relations, such as containedby, provid-
ing little information about how the models perform
across the board. Second, evaluating with Freebase
held-out data is biased. For example, we find that
frequently mentioned entity pairs are more likely to
have relations in Freebase. Systems that rank such
tuples higher receives higher precision than those
that do not have such bias, regardless of how cor-
rect their predictions are. Third, we can aggregate
per-relation comparisons to establish statistical sig-
nificance, for example via the sign test.
Also note that while we run our models on the
complete training and test set, evaluation is re-
stricted to the subsampled test set.
4.3 Predicting Freebase Relations
Table 1 shows our results for Freebase relations,
omitting those for which none of the systems can
find any relevant facts. Our first baseline is MI09,
a distantly supervised classifier based on the work
of Mintz et al (2009). This classifier only learns
from observed pattern-relation pairs in the training
set (of which we only have about 8k). By contrast,
our latent feature models can learn pattern-pattern
correlations both on the unlabeled training and test
set (comparable to bootstrapping). We hence also
compare against YA11, a version of MI09 that uses
preprocessed cluster features according to Yao et al
(2011). The third baseline is SU12, the state-of-the-
art Multi-Instance Multi-Label system by Surdeanu
et al (2012).
The remaining systems are our neighborhood
80
 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
 0.9 1
 0  0.2  0.4  0.6  0.8  1Precision Recall
Averaged 11-point Precision/Recall MI09YA11SU12NFNFNFE
Figure 2: Averaged 11-point precision recall curve for
Freebase relations in table 1.
model (N), the factorized model (F), their combi-
nation (NF) and the combined model with a latent
entity representation (NFE). For all our models we
use the same number of components when applica-
ble (KF = KE = 100), 1000 epochs, and 0.01 as
regularizer for component weights and 0.1 for neigh-
borhood weights.
Table 1 shows that adding pattern cluster features
(and hence incorporating more data) helps YA11
to improve over MI09. Likewise, we see that the
factorized model F improves over N, again learn-
ing from unlabeled data. This improvement is big-
ger than the corresponding change between MI09
and YA11, possibly indicating that our latent rep-
resentations are optimized directly towards improv-
ing prediction performance. The combination of N,
F and E outperforms all other models in terms of
weighted MAP, indicating the power of selectional
preferences learned from data. Note that NFE is
significantly different (p  0.05 in sign test) to all
but the NF and F models. In terms of MAP the NF
model outperforms NFE, indicating that it does not
do as well for frequent relations, but better for infre-
quent ones.
Figure 2 shows an averaged 11-point precision re-
call graph (Manning et al, 2008) for Freebase re-
lations. We notice that our latent models outper-
form all remaining models across all recall levels,
and that combining neighborhood and latent models
is helpful. This finding is consistent with our MAP
results. Figure 3 shows the recall-precision curve for
the works_written relation with respect to our three
baselines and the NFE model. Observe how preci-
 0 0.2 0.4 0.6 0.8
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1Precision Recall
Recall/Precision MI09YA11SU12NFE
Figure 3: Precision and recall for works_written(X,Y).
Relation # N F NF NFE
visit 80 0.19 0.68 0.49 0.42
attend 69 0.23 0.10 0.07 0.10
base 61 0.46 0.87 0.81 0.68
head 38 0.47 0.67 0.70 0.68
scientist 36 0.25 0.84 0.79 0.73
support 18 0.16 0.29 0.32 0.38
adviser 11 0.19 0.15 0.19 0.28
criticize 9 0.09 0.60 0.67 0.64
praise 4 0.01 0.03 0.05 0.10
vote 3 0.18 0.18 0.34 0.34
MAP 0.22 0.44 0.44 0.43
Weighted MAP 0.28 0.56 0.50 0.46
Table 2: Average and (weighted) Mean Average Preci-
sions for surface patterns.2
sion drops for both MI09 and SU12 at about 50%
recall. At this point the remaining unretrieved facts
have patterns that have not been seen together with
works_written in the training set. By using cluster
features, YA11 can overcome this problem partly,
but not as dramatically as NFE?a pattern we ob-
serve for many relations.
All our models are fast to train. The slowest
model trains in just 45 minutes. By contrast, training
the topic model in YA11 alone takes 4 hours. Train-
ing SU12 takes two hours (on less data). Also notice
that our models not only learn to predict Freebase
relations, but also approximately 4k surface pattern
relations.
4.4 Predicting Surface Patterns
Table 2 presents a comparison of our models with re-
spect to 10 surface pattern relations. These relations
81
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
 0.9 1
 0  0.2  0.4  0.6  0.8  1Precision Recall
Averaged 11-point Precision/Recall NFNFNFE
Figure 4: Averaged 11-point precision recall curve for
surface pattern relations in table 2.
were chosen according to what we believe are inter-
esting questions not currently captured in Freebase.
We again see that learning a latent representation (F,
NF and NFE) from additional data helps quite sub-
stantially over the N model. For in the weighted
MAP metric we note that incorporating entity rep-
resentations (in the NFE model) in fact hurts total
performance.3 One reason may be the fact that Free-
base relations are typed?they require very specific
types of entities as arguments. By contrast, for a
surface pattern like ?X visits Y? X could be a person
or organization, and Y could be a location, organi-
zation or person. However, in terms of MAP score
this time there is no obvious winner among the la-
tent models. This is also confirmed by the averaged
11-point precision recall curve in figure 4.
Notice that we can accurately predict the X?
scientist-at?Y surface pattern relation in table 2,
as well as the more general person/company (em-
ployedBy) relation in table 1. This indicates that
our models can capture asymmetry?a symmetric
model would either over-predict X?scientist-at?Y
or under-predict person/company.
5 Conclusion
We present relation extraction into universal
schemas. Such schemas contain surface patterns
as relations, as well as relations from structured
sources. By predicting missing tuples for surface
pattern relations we can populate a database with-
out any labelled data, and answer questions not sup-
3Due to the small set of relations only N is significantly dif-
ferent to F, NF and NFE (p 0.05 in sign test).
ported by the structured schema alone. By predict-
ing missing tuples in the structured schema we can
expand a knowledge base of fixed schema, and only
require a set of existing facts from this schema. Cru-
cially, by predicting and modeling both surface pat-
terns and structured relations simultaneously we can
improve performance. We show this experimentally
by contrasting a series of the popular weakly super-
vised models to our collaborative filtering models
that learn latent feature representations across sur-
face patterns and structured relations. Moreover, our
models are computationally efficient, requiring less
time than comparable methods, while learning more
relations.
Reasoning with universal schemas is not merely a
tool for information extraction. It can also serve as
a framework for various data integration tasks. For
example, we could integrate facts from one schema
(say, Freebase) into another (say, the TAC KBP
schema) by adding both sets of relations to the set
of surface patterns. Reasoning with this schema
will mean populating each database with facts from
the other, and would leverage information in surface
patterns to improve integration. In future work we
also plan to integrate universal entity types and at-
tributes into the model.
The source code of our system, its output, and
all data annotations are available at http://www.
riedelcastro.org/uschema.
Acknowledgments
We thank the reviewers for very helpful comments.
This work was supported in part by the Center for In-
telligent Information Retrieval and the University of
Massachusetts, in part by UPenn NSF medium IIS-
0803847, in part by DARPA under agreement num-
ber FA8750-13-2-0020 and FA8750-09-C-0181, and
in part by an award from Google. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the authors
and do not necessarily reflect the view of DARPA,
AFRL, or the US government.
References
Ronald J. Brachman. 1983. What is-a is and isn t:
An analysis of taxonomic links in semantic networks.
IEEE Computer, 16(10):30?36.
82
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using min-
imal supervision. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?07).
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka, and Tom M. Mitchell.
2010. Toward an architecture for never-ending lan-
guage learning. In Proceedings of the 25th AAAI Con-
ference on Artificial Intelligence (AAAI ?10).
Michael Collins, Sanjoy Dasgupta, and Robert E.
Schapire. 2001. A generalization of principal com-
ponent analysis to the exponential family. In Proceed-
ings of NIPS.
M. Craven and J. Kumlien. 1999. Constructing biolog-
ical knowledge-bases by extracting information from
text sources. In Proceedings of the Seventh Interna-
tional Conference on Intelligent Systems for Molecular
Biology, pages 77?86, Germany.
Aron Culotta and Jeffery Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
ACL, Barcelona, Spain.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extraction
from the web. Commun. ACM, 51(12):68?74.
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Dis-
covering Relations among Named Entities from Large
Corpora. Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics (ACL
?04), pages 415?422.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Proceedings of ACL.
Stanley Kok and Pedro Domingos. 2008. Extracting Se-
mantic Networks from Text Via Relational Clustering.
In ECML.
Yehuda Koren. 2008. Factorization meets the neighbor-
hood: a multifaceted collaborative filtering model. In
Proceedings of the 14th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, KDD ?08, pages 426?434, New York, NY, USA.
ACM.
Dekang Lin and Patrick Pantel. 2001. DIRT - discovery
of inference rules from text. In Knowledge Discovery
and Data Mining, pages 323?328.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Sch?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, Cambridge, UK.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP (ACL ?09),
pages 1003?1011. Association for Computational Lin-
guistics.
Brian Murphy, Partha Pratim Talukdar, and Tom
Mitchell. 2012. Learning effective and interpretable
semantic models using non-negative sparse embed-
ding. In COLING, pages 1933?1950.
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2012. Factorizing yago: scalable machine
learning for linked data. In Proceedings of the 21st
international conference on World Wide Web, WWW
?12, pages 271?280, New York, NY, USA. ACM.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning Inferential Selectional Preferences. In Pro-
ceedings of NAACL HLT.
Steffen Rendle, Christoph Freudenthaler, Zeno Gantner,
and Lars Schmidt-Thieme. 2009. Bpr: Bayesian per-
sonalized ranking from implicit feedback. In Proceed-
ings of the Twenty-Fifth Conference on Uncertainty in
Artificial Intelligence, UAI ?09, pages 452?461, Ar-
lington, Virginia, United States. AUAI Press.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD ?10).
Evan Sandhaus, 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia.
Stefan Schoenmackers, Oren Etzioni, and Daniel S.
Weld. 2008. Scaling textual inference to the web.
In EMNLP ?08: Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 79?88, Morristown, NJ, USA. Association for
Computational Linguistics.
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,
and Jesse Davis. 2010. Learning first-order horn
clauses from web text. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?10, pages 1088?1098,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, HLT-NAACL ?06, pages 304?
311, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and
Christopher D. Manning. 2012. Multi-instance multi-
label learning for relation extraction. In Proceedings
83
of the Conference on Empirical methods in natural
language processing (EMNLP ?12), pages 455?465.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
the 22nd International Conference on Computational
Linguistics - Volume 1, COLING ?08, pages 849?856,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2011. Probabilistic matrix factorization leveraging
contexts for unsupervised relation discovery. In Pro-
ceedings of PAKDD.
Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew
McCallum. 2011. Structured relation discovery using
generative models. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ?11), July.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2012a. Probabilistic databases of universal schema.
In Proceedings of the AKBC-WEKEX Workshop at
NAACL 2012, June.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2012b. Unsupervised relation discovery with sense
disambiguation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?12), July.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using selec-
tional preferences. In Proceedings of the 44th Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?06).
84
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 793?803,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Large-Scale Cross-Document Coreference Using
Distributed Inference and Hierarchical Models
Sameer Singh? Amarnag Subramanya? Fernando Pereira? Andrew McCallum?
? Department of Computer Science, University of Massachusetts, Amherst MA 01002
? Google Research, Mountain View CA 94043
sameer@cs.umass.edu, asubram@google.com, pereira@google.com, mccallum@cs.umass.edu
Abstract
Cross-document coreference, the task of
grouping all the mentions of each entity in a
document collection, arises in information ex-
traction and automated knowledge base con-
struction. For large collections, it is clearly
impractical to consider all possible groupings
of mentions into distinct entities. To solve
the problem we propose two ideas: (a) a dis-
tributed inference technique that uses paral-
lelism to enable large scale processing, and
(b) a hierarchical model of coreference that
represents uncertainty over multiple granular-
ities of entities to facilitate more effective ap-
proximate inference. To evaluate these ideas,
we constructed a labeled corpus of 1.5 million
disambiguated mentions in Web pages by se-
lecting link anchors referring to Wikipedia en-
tities. We show that the combination of the
hierarchical model with distributed inference
quickly obtains high accuracy (with error re-
duction of 38%) on this large dataset, demon-
strating the scalability of our approach.
1 Introduction
Given a collection of mentions of entities extracted
from a body of text, coreference or entity resolu-
tion consists of clustering the mentions such that
two mentions belong to the same cluster if and
only if they refer to the same entity. Solutions to
this problem are important in semantic analysis and
knowledge discovery tasks (Blume, 2005; Mayfield
et al, 2009). While significant progress has been
made in within-document coreference (Ng, 2005;
Culotta et al, 2007; Haghighi and Klein, 2007;
Bengston and Roth, 2008; Haghighi and Klein,
2009; Haghighi and Klein, 2010), the larger prob-
lem of cross-document coreference has not received
as much attention.
Unlike inference in other language processing
tasks that scales linearly in the size of the corpus,
the hypothesis space for coreference grows super-
exponentially with the number of mentions. Conse-
quently, most of the current approaches are devel-
oped on small datasets containing a few thousand
mentions. We believe that cross-document coref-
erence resolution is most useful when applied to a
very large set of documents, such as all the news ar-
ticles published during the last 20 years. Such a cor-
pus would have billions of mentions. In this paper
we propose a model and inference algorithms that
can scale the cross-document coreference problem
to corpora of that size.
Much of the previous work in cross-document
coreference (Bagga and Baldwin, 1998; Ravin and
Kazi, 1999; Gooi and Allan, 2004; Pedersen et al,
2006; Rao et al, 2010) groups mentions into entities
with some form of greedy clustering using a pair-
wise mention similarity or distance function based
on mention text, context, and document-level statis-
tics. Such methods have not been shown to scale up,
and they cannot exploit cluster features that cannot
be expressed in terms of mention pairs. We provide
a detailed survey of related work in Section 6.
Other previous work attempts to address some of
the above concerns by mapping coreference to in-
ference on an undirected graphical model (Culotta
et al, 2007; Poon et al, 2008; Wellner et al, 2004;
Wick et al, 2009a). These models contain pair-
wise factors between all pairs of mentions captur-
ing similarity between them. Many of these mod-
els also enforce transitivity and enable features over
793
Filmmaker
Rapper
BEIJING, Feb. 21? Kevin Smith, who played the god of war in the "Xena"...
... The Physiological Basis of Politics,? by Kevin B. Smith, Douglas Oxley, Matthew Hibbing...
The filmmaker Kevin Smith returns to the role of Silent Bob...
Like Back in 2008, the Lions drafted Kevin Smith, even though Smith was badly...
Firefighter Kevin Smith spent almost 20 years preparing for Sept. 11. When he...
...shorthanded backfield in the wake of Kevin Smith's knee injury, and the addition of Haynesworth...
...were coming,'' said Dallas cornerback Kevin Smith. ''We just didn't know when...
...during the late 60's and early 70's, Kevin Smith worked with several local...
...the term hip-hop is attributed to Lovebug Starski. What does it actually mean...
Nothing could be more irrelevant to Kevin Smith's audacious ''Dogma'' than ticking off...
Cornerback
Firefighter
Actor
Running back
Author
Figure 1: Cross-Document Coreference Problem: Example mentions of ?Kevin Smith? from New York
Times articles, with the true entities shown on the right.
entities by including set-valued variables. Exact in-
ference in these models is intractable and a number
of approximate inference schemes (McCallum et al,
2009; Rush et al, 2010; Martins et al, 2010) may
be used. In particular, Markov chain Monte Carlo
(MCMC) based inference has been found to work
well in practice. However as the number of men-
tions grows to Web scale, as in our problem of cross-
document coreference, even these inference tech-
niques become infeasible, motivating the need for
a scalable, parallelizable solution.
In this work we first distribute MCMC-based in-
ference for the graphical model representation of
coreference. Entities are distributed across the ma-
chines such that the parallel MCMC chains on the
different machines use only local proposal distribu-
tions. After a fixed number of samples on each ma-
chine, we redistribute the entities among machines
to enable proposals across entities that were pre-
viously on different machines. In comparison to
the greedy approaches used in related work, our
MCMC-based inference provides better robustness
properties.
As the number of mentions becomes large, high-
quality samples for MCMC become scarce. To
facilitate better proposals, we present a hierarchi-
cal model. We add sub-entity variables that repre-
sent clusters of similar mentions that are likely to
be coreferent; these are used to propose composite
jumps that move multiple mentions together. We
also introduce super-entity variables that represent
clusters of similar entities; these are used to dis-
tribute entities among the machines such that similar
entities are assigned to the same machine. These ad-
ditional levels of hierarchy dramatically increase the
probability of beneficial proposals even with a large
number of entities and mentions.
To create a large corpus for evaluation, we iden-
tify pages that have hyperlinks to Wikipedia, and ex-
tract the anchor text and the context around the link.
We treat the anchor text as the mention, the con-
text as the document, and the title of the Wikipedia
page as the entity label. Using this approach, 1.5
million mentions were annotated with 43k entity la-
bels. On this dataset, our proposed model yields a
B3 (Bagga and Baldwin, 1998) F1 score of 73.7%,
improving over the baseline by 16% absolute (corre-
sponding to 38% error reduction). Our experimen-
tal results also show that our proposed hierarchical
model converges much faster even though it contains
many more variables.
2 Cross-document Coreference
The problem of coreference is to identify the sets of
mention strings that refer to the same underlying en-
tity. The identities and the number of the underlying
entities is not known. In within-document corefer-
ence, the mentions occur in a single document. The
number of mentions (and entities) in each document
is usually in the hundreds. The difficulty of the task
arises from a large hypothesis space (exponential in
the number of mentions) and challenge in resolv-
ing nominal and pronominal mentions to the correct
named mentions. In most cases, named mentions
794
are not ambiguous within a document. In cross-
document coreference, the number of mentions and
entities is in the millions, making the combinatorics
even more daunting. Furthermore, naming ambigu-
ity is much more common as the same string can
refer to multiple entities in different documents, and
distinct strings may refer to the same entity in differ-
ent documents.
We show examples of ambiguities in Figure 1.
Resolving the identity of individuals with the same
name is a common problem in cross-document
coreference. This problem is further complicated
by the fact that in some situations, these individ-
uals may belong to the same field. Another com-
mon ambiguity is that of alternate names, in which
the same entity is referred to by different names or
aliases (e.g. ?Bill? is often used as a substitute for
?William?). The figure also shows an example of
the renaming ambiguity ? ?Lovebug Starski? refers
to ?Kevin Smith?, and this is an extreme form of al-
ternate names. Rare singleton entities (like the fire-
fighter) that may appear only once in the whole cor-
pus are also often difficult to isolate.
2.1 Pairwise Factor Model
Factor graphs are a convenient representation for a
probability distribution over a vector of output vari-
ables given observed variables. The model that we
use for coreference represents mentions (M) and en-
tities (E) as random variables. Each mention can
take an entity as its value, and each entity takes a set
of mentions as its value. Each mention also has a
feature vector extracted from the observed text men-
tion and its context. More precisely, the probability
of a configuration E = e is defined by
p(e) ? exp
?
e?e
{?
m,n?e,n 6=m ?a(m,n)
+
?
m?e,n/?e ?r(m,n)
}
where factor ?a represents affinity between men-
tions that are coreferent according to e, and factor
?r represents repulsion between mentions that are
not coreferent. Different factors are instantiated for
different predicted configurations. Figure 2 shows
the model instantiated with five mentions over a two-
entity hypothesis.
For the factor potentials, we use cosine sim-
ilarity of mention context pairs (?mn) such that
m1
m2
m3
m4
m5
e1
e2
Figure 2: Pairwise Coreference Model: Factor
graph for a 2-entity configuration of 5 mentions.
Affinity factors are shown with solid lines, and re-
pulsion factors with dashed lines.
?a(m,n) = ?mn ? b and ?r(m,n) = ?(?mn ? b),
where b is the bias. While one can certainly make
use of a more sophisticated feature set, we leave this
for future work as our focus is to scale up inference.
However, it should be noted that this approach is
agnostic to the particular set of features used. As
we will note in the next section, we do not need to
calculate features between all pairs of mentions (as
would be prohibitively expensive for large datasets);
instead we only compute the features as and when
required.
2.2 MCMC-based Inference
Given the above model of coreference, we seek the
maximum a posteriori (MAP) configuration:
e? = argmaxe p(e)
= argmaxe
?
e?e
{?
m,n?e,n 6=m ?a(m,n)
+
?
m?e,n/?e ?r(m,n)
}
Computing e? exactly is intractable due to the
large space of possible configurations.1 Instead,
we employ MCMC-based optimization to discover
the MAP configuration. A proposal function q is
used to propose a change e? to the current config-
uration e. This jump is accepted with the following
Metropolis-Hastings acceptance probability:
?(e, e?) = min
(
1,
(
p(e?)
p(e)
)1/t q(e)
q(e?)
)
(1)
1Number of possible entities is Bell(n) in the number of
mentions, i.e. number of partitions of n items
795
where t is the annealing temperature parameter.
MCMC chains efficiently explore the high-
density regions of the probability distribution. By
slowly reducing the temperature, we can decrease
the entropy of the distribution to encourage con-
vergence to the MAP configuration. MCMC has
been used for optimization in a number of related
work (McCallum et al, 2009; Goldwater and Grif-
fiths, 2007; Changhe et al, 2004).
The proposal function moves a randomly chosen
mention l from its current entity es to a randomly
chosen entity et. For such a proposal, the log-model
ratio is:
log
p(e?)
p(e)
=
?
m?et
?a(l,m) +
?
n?es
?r(l, n)
?
?
n?es
?a(l, n)?
?
m?et
?r(l,m) (2)
Note that since only the factors between mention l
and mentions in es and et are involved in this com-
putation, the acceptance probability of each proposal
is calculated efficiently.
In general, the model may contain arbitrarily
complex set of features over pairs of mentions, with
parameters associated with them. Given labeled
data, these parameters can be learned by Percep-
tron (Collins, 2002), which uses the MAP config-
uration according to the model (e?). There also exist
more efficient training algorithms such as SampleR-
ank (McCallum et al, 2009; Wick et al, 2009b) that
update parameters during inference. However, we
only focus on inference in this work, and the only
parameter that we set manually is the bias b, which
indirectly influences the number of entities in e?. Un-
less specified otherwise, in this work the initial con-
figuration for MCMC is the singleton configuration,
i.e. all entities have a size of 1.
This MCMC inference technique, which has been
used in McCallum and Wellner (2004), offers sev-
eral advantages over other inference techniques: (a)
unlike message-passing-methods, it does not require
the full ground graph, (b) we only have to exam-
ine the factors that lie within the changed entities
to evaluate a proposal, and (c) inference may be
stopped at any point to obtain the current best con-
figuration. However, the super exponential nature of
the hypothesis space in cross-doc coreference ren-
ders this algorithm computationally unsuitable for
large scale coreference tasks. In particular, fruit-
ful proposals (that increase the model score) are ex-
tremely rare, resulting in a large number of propos-
als that are not accepted. We describe methods to
speed up inference by 1) evaluating multiple pro-
posal simultaneously (Section 3), and 2) by aug-
menting our model with hierarchical variables that
enable better proposal distributions (Section 4).
3 Distributed MAP Inference
The key observation that enables distribution is that
the acceptance probability computation of a pro-
posal only examines a few factors that are not com-
mon to the previous and next configurations (Eq. 2).
Consider a pair of proposals, one that moves men-
tion l from entity es to entity et, and the other that
moves mention l? from entity e?s to entity e
?
t. The
set of factors to compute acceptance of the first pro-
posal are factors between l and mentions in es and
et, while the set of factors required to compute ac-
ceptance of the second proposal lie between l? and
mentions in e?s and e
?
t. Since these set of factors
are completely disjoint from each other, and the re-
sulting configurations do not depend on each other,
these two proposals are mutually-exclusive. Differ-
ent orders of evaluating such proposals are equiv-
alent, and in fact, these proposals can be proposed
and evaluated concurrently. This mutual-exclusivity
is not restricted only to pairs of proposals; a set of
proposals are mutually-exclusive if no two propos-
als require the same factor for evaluation.
Using this insight, we introduce the following ap-
proach to distributed cross-document coreference.
We divide the mentions and entities among multiple
machines, and propose moves of mentions between
entities assigned to the same machine. These jumps
are evaluated exactly and accepted without commu-
nication between machines. Since acceptance of a
mention?s move requires examining factors that lie
between other mentions in its entity, we ensure that
all mentions of an entity are assigned the same ma-
chine. Unless specified otherwise, the distribution is
performed randomly. To enable exploration of the
complete configuration space, rounds of sampling
are interleaved by redistribution stages, in which the
entities are redistributed among the machines (see
Figure 3). We use MapReduce (Dean and Ghe-
796
Distributor
Inference
Inference
Figure 3: Distributed MCMC-based Inference:
Distributor divides the entities among the machines,
and the machines run inference. The process is re-
peated by the redistributing the entities.
mawat, 2004) to manage the distributed computa-
tion.
This approach to distribution is equivalent to in-
ference with all mentions and entities on a single
machine with a restricted proposer, but is faster
since it exploits independencies to propose multiple
jumps simultaneously. By restricting the jumps as
described above, the acceptance probability calcu-
lation is exact. Partitioning the entities and propos-
ing local jumps are restrictions to the single-machine
proposal distribution; redistribution stages ensure
the equivalent Markov chains are still irreducible.
See Singh et al (2010) for more details.
4 Hierarchical Coreference Model
The proposal function for MCMC-based MAP infer-
ence presents changes to the current entities. Since
we use MCMC to reach high-scoring regions of the
hypothesis space, we are interested in the changes
that improve the current configuration. But as the
number of mentions and entities increases, these
fruitful samples become extremely rare due to the
blowup in the possible space of configurations, re-
sulting in rejection of a large number of proposals.
By distributing as described in the previous section,
we propose samples in parallel, improving chances
of finding changes that result in better configura-
tions. However, due to random redistribution and a
naive proposal function within each machine, a large
fraction of proposals are still wasted. We address
these concerns by adding hierarchy to the model.
4.1 Sub-Entities
Consider the task of proposing moves of mentions
(within a machine). Given the large number of
mentions and entities, the probability that a ran-
domly picked mention that is moved to a random
entity results in a better configuration is extremely
small. If such a move is accepted, this gives us ev-
idence that the mention did not belong to the pre-
vious entity, and we should also move similar men-
tions from the previous entity simultaneously to the
same entity. Since the proposer moves only a sin-
gle mention at a time, a large number of samples
may be required to discover these fruitful moves.
To enable block proposals that move similar men-
tions simultaneously, we introduce latent sub-entity
variables that represent groups of similar mentions
within an entity, where the similarity is defined by
the model. For inference, we have stages of sam-
pling sub-entities (moving individual mentions) in-
terleaved with stages of entity sampling (moving all
mentions within a sub-entity). Even though our con-
figuration space has become larger due to these ex-
tra variables, the proposal distribution has also im-
proved since it proposes composite moves.
4.2 Super-Entities
Another issue faced during distributed inference is
that random redistribution is often wasteful. For ex-
ample, if dissimilar entities are assigned to a ma-
chine, none of the proposals may be accepted. For a
large number of entities and machines, the probabil-
ity that similar entities will be assigned to the same
machine is extremely small, leading to a larger num-
ber of wasted proposals. To alleviate this problem,
we introduce super-entities that represent groups of
similar entities. During redistribution, we ensure all
entities in the same super-entity are assigned to the
same machine. As for sub-entities above, inference
switches between regular sampling of entities and
sampling of super-entities (by moving entities). Al-
though these extra variables have made the config-
uration space larger, they also allow more efficient
distribution of entities, leading to useful proposals.
4.3 Combined Hierarchical Model
Each of the described levels of the hierarchy are sim-
ilar to the initial model (Section 2.1): mentions/sub-
entities have the same structure as the entities/super-
entities, and are modeled using similar factors. To
represent the ?context? of a sub-entity we take the
union of the bags-of-words of the constituent men-
tion contexts. Similarly, we take the union of sub-
797
Super-Entities
Entities
Mentions
Sub-Entities
Figure 4: Combined Hierarchical Model with factors instantiated for a hypothesis containing 2 super-
entities, 4 entities, and 8 sub-entities, shown as colored circles, over 16 mentions. Dotted lines represent
repulsion factors and solid lines represent affinity factors (the color denotes the type of variable that the
factor touches). The boxes on factors were excluded for clarity.
entity contexts to represent the context of an entity.
The factors are instantiated in the same manner as
Section 2.1 except that we change the bias factor
b for each level (increasing it for sub-entities, and
decreasing it for super-entities). The exact values
of these biases indirectly determines the number of
predicted sub-entities and super-entities.
Since these two levels of hierarchy operate at
separate granularities from each other, we combine
them into a single hierarchical model that contains
both sub- and super-entities. We illustrate this hi-
erarchical structure in Figure 4. Inference for this
model takes a round-robin approach by fixing two
of the levels of the hierarchy and sampling the third,
cycling through these three levels. Unless specified
otherwise, the initial configuration is the singleton
configuration, in which all sub-entities, entities, and
super-entities are of size 1.
5 Experiments
We evaluate our models and algorithms on a number
of datasets. First, we compare performance on the
small, publicly-available ?John Smith? dataset. Sec-
ond, we run the automated Person-X evaluation to
obtain thousands of mentions that we use to demon-
strate accuracy and scalability improvements. Most
importantly, we create a large labeled corpus using
links to Wikipedia to explore the performance in the
large-scale setting.
5.1 John Smith Corpus
To compare with related work, we run an evalua-
tion on the ?John Smith? corpus (Bagga and Bald-
win, 1998), containing 197 mentions of the name
?John Smith? from New York Times articles (la-
beled to obtain 35 true entities). The bias b for
our approach is set to result in the correct number
of entities. Our model achieves B3 F1 accuracy of
66.4% on this dataset. In comparison, Rao et al
(2010) obtains 61.8% using the model most similar
to ours, while their best model (which uses sophis-
ticated topic-model features that do not scale easily)
achieves 69.7%. It is encouraging to note that our
approach, using only a subset of the features, per-
forms competitively with related work. However,
due to the small size of the dataset, we require fur-
ther evaluation before reaching any conclusions.
5.2 Person-X Evaluation
There is a severe lack of labeled corpora for cross-
document coreference due to the effort required
to evaluate the coreference decisions. Related
approaches have used automated Person-X evalu-
ation (Gooi and Allan, 2004), in which unique
person-name strings are treated as the true entity
labels for the mentions. Every mention string is
replaced with an ?X? for the coreference system.
We use this evaluation methodology on 25k person-
name mentions from the New York Times cor-
pus (Sandhaus, 2008) each with one of 50 unique
strings. As before, we set the bias b to achieve the
same number of entities. We use 1 million samples
in each round of inference, followed by random re-
distribution in the flat model, and super-entities in
the hierarchical model. Results are averaged over
five runs.
798
Figure 5: Person-X Evaluation of Pairwise model:
Performance as number of machines is varied, aver-
aged over 5 runs.
Number of Entities 43,928
Number of Mentions 1,567,028
Size of Largest Entity 6,096
Average Mentions per Entity 35.7
Variance of Mentions per Entity 5191.7
Table 1: Wikipedia Link Corpus Statistics. Size
of an entity is the number of mentions of that entity.
Figure 5 shows accuracy compared to relative
wallclock running time for distributed inference on
the flat, pairwise model. Speed and accuracy im-
prove as additional machines are added, but larger
number of machines lead to diminishing returns for
this small dataset. Distributed inference on our hi-
erarchical model is evaluated in Figure 6 against in-
ference on the pairwise model from Figure 5. We
see that the individual hierarchical models perform
much better than the pairwise model; they achieve
the same accuracy as the pairwise model in approx-
imately 10% of the time. Moreover, distributed in-
ference on the combined hierarchical model is both
faster and more accurate than the individual hierar-
chical models.
5.3 Wikipedia Link Corpus
To explore the application of the proposed approach
to a larger, realistic dataset, we construct a corpus
based on the insight that links to Wikipedia that ap-
pear on webpages can be treated as mentions, and
since the links were added manually by the page au-
thor, we use the destination Wikipedia page as the
Figure 6: Person-X Evaluation of Hierarchical
Models: Performance of inference on hierarchical
models compared to the pairwise model. Experi-
ments were run using 50 machines.
entity the link refers to.
The dataset is created as follows: First, we crawl
the web and select hyperlinks on webpages that link
to an English Wikipedia page.2 The anchors of
these links form our set of mentions, with the sur-
rounding block of clean text (obtained after remov-
ing markup, etc.) around each link being its con-
text. We assign the title of the linked Wikipedia
page as the entity label of that link. Since this set
of mentions and labels can be noisy, we use the
following filtering steps. All links that have less
than 36 words in their block, or whose anchor text
has a large string edit distance from the title of the
Wikipedia page, are discarded. While this results in
cases in which ?President? is discarded when linked
to the ?Barack Obama? Wikipedia page, it was nec-
essary to reduce noise. Further, we also discard
links to Wikipedia pages that are concepts (such as
?public_domain?) rather than entities. All enti-
ties with less than 6 links to them are also discarded.
Table 1 shows some statistics about our automat-
ically generated data set. We randomly sampled 5%
of the entities to create a development set, treating
the remaining entities as the test set. Unlike the
John Smith and Person-X evaluation, this data set
also contains non-person entities such as organiza-
tions and locations.
For our models, we augment the factor potentials
with mention-string similarity:
2e.g. http://en.wikipedia.org/Hillary_Clinton
799
?a/r(m,n) = ? (?mn ? b+ wSTREQ(m,n))
where STREQ is 1 if mentions m and n are string
identical (0 otherwise), and w is the weight to this
feature.3 In our experiments we found that setting
w = 0.8 and b = 1e? 4 gave the best results on the
development set.
Due to the large size of the corpus, existing cross-
document coreference approaches could not be ap-
plied to this dataset. However, since a majority
of related work consists of using clustering after
defining a similarity function (Section 6), we pro-
vide a baseline evaluation of clustering with Sub-
Square (Bshouty and Long, 2010), a scalable, dis-
tributed clustering method. Subsquare takes as in-
put a weighted graph with mentions as nodes and
similarity between mentions used as edge weights.
Subsquare works by stochastically assigning a ver-
tex to the cluster of one its neighbors if they have
significant neighborhood overlap. This algorithm
is an efficient form of approximate spectral cluster-
ing (Bshouty and Long, 2010), and since it is given
the same distances between mentions as our models,
we expect it to get similar accuracy. We also gen-
erate another baseline clustering by assigning men-
tions with identical strings to the same entity. This
mention-string clustering is also used as the initial
configuration of our inference.
Figure 7: Wikipedia Link Evaluation: Perfor-
mance of inference for different number of machines
(N = 100, 500). Mention-string match clustering is
used as the initial configuration.
3Note that we do not use mention-string similarity for John
Smith or Person-X as the mention strings are all identical.
Method
Pairwise B3 Score
P/ R F1 P/ R F1
String-Match 30.0 / 66.7 41.5 82.7 / 43.8 57.3
Subsquare 38.2 / 49.1 43.0 87.6 / 51.4 64.8
Our Model 44.2 / 61.4 51.4 89.4 / 62.5 73.7
Table 2: F1 Scores on the Wikipedia Link Data.
The results are significant at the 0.0001 level over
Subsquare according to the difference of proportions
significance test.
Inference is run for 20 rounds of 10 million sam-
ples each, distributed over N machines. We use
N = 100, 500 and the B3 F1 score results obtained
set for each case are shown in Figure 7. It can
be seen that N = 500 converges to a better solu-
tion faster, showing effective use of parallelism. Ta-
ble 2 compares the results of our approach (at con-
vergence for N = 500), the baseline mention-string
match and the Subsquare algorithm. Our approach
significantly outperforms the competitors.
6 Related Work
Although the cross-document coreference problem
is challenging and lacks large labeled datasets, its
ubiquitous role as a key component of many knowl-
edge discovery tasks has inspired several efforts.
A number of previous techniques use scoring
functions between pairs of contexts, which are then
used for clustering. One of the first approaches
to cross-document coreference (Bagga and Bald-
win, 1998) uses an idf-based cosine-distance scor-
ing function for pairs of contexts, similar to the one
we use. Ravin and Kazi (1999) extend this work to
be somewhat scalable by comparing pairs of con-
texts only if the mentions are deemed ?ambiguous?
using a heuristic. Others have explored multiple
methods of context similarity, and concluded that
agglomerative clustering provides effective means
of inference (Gooi and Allan, 2004). Pedersen et
al. (2006) and Purandare and Pedersen (2004) inte-
grate second-order co-occurrence of words into the
similarity function. Mann and Yarowsky (2003) use
biographical facts from the Web as features for clus-
tering. Niu et al (2004) incorporate information ex-
traction into the context similarity model, and anno-
tate a small dataset to learn the parameters. A num-
ber of other approaches include various forms of
800
hand-tuned weights, dictionaries, and heuristics to
define similarity for name disambiguation (Blume,
2005; Baron and Freedman, 2008; Popescu et al,
2008). These approaches are greedy and differ in the
choice of the distance function and the clustering al-
gorithm used. Daume? III and Marcu (2005) propose
a generative approach to supervised clustering, and
Haghighi and Klein (2010) use entity profiles to as-
sist within-document coreference.
Since many related methods use clustering, there
are a number of distributed clustering algorithms
that may help scale these approaches. Datta et
al. (2006) propose an algorithm for distributed k-
means. Chen et al (2010) describe a parallel spectral
clustering algorithm. We use the Subsquare algo-
rithm (Bshouty and Long, 2010) as baseline because
it works well in practice. Mocian (2009) presents a
survey of distributed clustering algorithms.
Rao et al (2010) have proposed an online deter-
ministic method that uses a stream of input mentions
and assigns them greedily to entities. Although it
can resolve mentions from non-trivial sized datasets,
the method is restricted to a single machine, which
is not scalable to the very large number of mentions
that are encountered in practice.
Our representation of the problem as an undi-
rected graphical model, and performing distributed
inference on it, provides a combination of advan-
tages not available in any of these approaches. First,
most of the methods will not scale to the hundreds
of millions of mentions that are present in real-world
applications. By utilizing parallelism across ma-
chines, our method can run on very large datasets
simply by increasing the number of machines used.
Second, approaches that use clustering are limited
to using pairwise distance functions for which ad-
ditional supervision and features are difficult to in-
corporate. In addition to representing features from
all of the related work, graphical models can also
use more complex entity-wide features (Culotta et
al., 2007; Wick et al, 2009a), and parameters can
be learned using supervised (Collins, 2002) or semi-
supervised techniques (Mann and McCallum, 2008).
Finally, the inference for most of the related ap-
proaches is greedy, and earlier decisions are not re-
visited. Our technique is based on MCMC inference
and simulated annealing, which are able to escape
local maxima.
7 Conclusions
Motivated by the problem of solving the corefer-
ence problem on billions of mentions from all of the
newswire documents from the past few decades, we
make the following contributions. First, we intro-
duce distributed version of MCMC-based inference
technique that can utilize parallelism to enable scal-
ability. Second, we augment the model with hierar-
chical variables that facilitate fruitful proposal distri-
butions. As an additional contribution, we use links
to Wikipedia pages to obtain a high-quality cross-
document corpus. Scalability and accuracy gains of
our method are evaluated on multiple datasets.
There are a number of avenues for future work.
Although we demonstrate scalability to more than a
million mentions, we plan to explore performance
on datasets in the billions. We also plan to examine
inference on complex coreference models (such as
with entity-wide factors). Another possible avenue
for future work is that of learning the factors. Since
our approach supports parameter estimation, we ex-
pect significant accuracy gains with additional fea-
tures and supervised data. Our work enables cross-
document coreference on very large corpora, and we
would like to explore the downstream applications
that can benefit from it.
Acknowledgments
This work was done when the first author was an
intern at Google Research. The authors would
like to thank Mark Dredze, Sebastian Riedel, and
anonymous reviewers for their valuable feedback.
This work was supported in part by the Center
for Intelligent Information Retrieval, the Univer-
sity of Massachusetts gratefully acknowledges the
support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0181., in part by an award
from Google, in part by The Central Intelligence
Agency, the National Security Agency and National
Science Foundation under NSF grant #IIS-0326249,
in part by NSF grant #CNS-0958392, and in part
by UPenn NSF medium IIS-0803847. Any opin-
ions, findings and conclusions or recommendations
expressed in this material are those of the authors
and do not necessarily reflect those of the sponsor.
801
References
Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector space
model. In International Conference on Computational
Linguistics, pages 79?85.
A. Baron and M. Freedman. 2008. Who is who and what
is what: experiments in cross-document co-reference.
In Empirical Methods in Natural Language Process-
ing (EMNLP), pages 274?283.
Eric Bengston and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Empirical Methods in Natural Language Processing
(EMNLP).
Matthias Blume. 2005. Automatic entity disambigua-
tion: Benefits to NER, relation extraction, link anal-
ysis, and inference. In International Conference on
Intelligence Analysis (ICIA).
Nader H. Bshouty and Philip M. Long. 2010. Find-
ing planted partitions in nearly linear time using ar-
rested spectral clustering. In Johannes Fu?rnkranz
and Thorsten Joachims, editors, Proceedings of the
27th International Conference on Machine Learning
(ICML-10), pages 135?142, Haifa, Israel, June. Omni-
press.
Yuan Changhe, Lu Tsai-Ching, and Druzdzel Marek.
2004. Annealed MAP. In Uncertainty in Artificial In-
telligence (UAI), pages 628?635, Arlington , Virginia.
AUAI Press.
Wen-Yen Chen, Yangqiu Song, Hongjie Bai, Chih-Jen
Lin, and Edward Y. Chang. 2010. Parallel spectral
clustering in distributed systems. IEEE Transactions
on Pattern Analysis and Machine Intelligence.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithm. In Annual Meeting of the
Association for Computational Linguistics (ACL).
Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for coreference
resolution. In North American Chapter of the Associa-
tion for Computational Linguistics - Human Language
Technologies (NAACL HLT).
S. Datta, C. Giannella, and H. Kargupta. 2006. K-Means
Clustering over a Large, Dynamic Network. In SIAM
Data Mining Conference (SDM).
Hal Daume? III and Daniel Marcu. 2005. A Bayesian
model for supervised clustering with the Dirichlet pro-
cess prior. Journal of Machine Learning Research
(JMLR), 6:1551?1577.
Jeffrey Dean and Sanjay Ghemawat. 2004. Mapreduce:
Simplified data processing on large clusters. Sympo-
sium on Operating Systems Design & Implementation
(OSDI).
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 744?751.
Chung Heong Gooi and James Allan. 2004. Cross-
document coreference on a large scale corpus. In
North American Chapter of the Association for Com-
putational Linguistics - Human Language Technolo-
gies (NAACL HLT), pages 9?16.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 848?855.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Empirical Methods in Natural Language Processing
(EMNLP), pages 1152?1161.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies
(NAACL HLT), pages 385?393.
Gideon S. Mann and Andrew McCallum. 2008. General-
ized expectation criteria for semi-supervised learning
of conditional random fields. In Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 870?878.
Gideon S. Mann and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In North Amer-
ican Chapter of the Association for Computational
Linguistics - Human Language Technologies (NAACL
HLT), pages 33?40.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference.
In Empirical Methods in Natural Language Process-
ing (EMNLP), pages 34?44, Cambridge, MA, October.
Association for Computational Linguistics.
J. Mayfield, D. Alexander, B. Dorr, J. Eisner, T. Elsayed,
T. Finin, C. Fink, M. Freedman, N. Garera, P. Mc-
Namee, et al 2009. Cross-document coreference res-
olution: A key technology for learning by reading. In
AAAI Spring Symposium on Learning by Reading and
Learning to Read.
Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In Neural Information Processing Sys-
tems (NIPS).
Andrew McCallum, Karl Schultz, and Sameer Singh.
2009. FACTORIE: Probabilistic programming via im-
peratively defined factor graphs. In Neural Informa-
tion Processing Systems (NIPS).
Horatiu Mocian. 2009. Survey of Distributed Clustering
Techniques. Ph.D. thesis, Imperial College of London.
802
Vincent Ng. 2005. Machine learning for coreference res-
olution: From local classification to global ranking. In
Annual Meeting of the Association for Computational
Linguistics (ACL).
Cheng Niu, Wei Li, and Rohini K. Srihari. 2004. Weakly
supervised learning for cross-document person name
disambiguation supported by information extraction.
In Annual Meeting of the Association for Computa-
tional Linguistics (ACL), page 597.
Ted Pedersen, Anagha Kulkarni, Roxana Angheluta, Zor-
nitsa Kozareva, and Thamar Solorio. 2006. An
unsupervised language independent method of name
discrimination using second order co-occurrence fea-
tures. In International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing),
pages 208?222.
Hoifung Poon, Pedro Domingos, and Marc Sumner.
2008. A general method for reducing the complexity
of relational inference and its application to MCMC.
In AAAI Conference on Artificial Intelligence.
Octavian Popescu, Christian Girardi, Emanuele Pianta,
and Bernardo Magnini. 2008. Improving cross-
document coreference. Journe?es Internationales
d?Analyse statistique des Donne?es Textuelles, 9:961?
969.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and simi-
larity spaces. In Conference on Computational Natu-
ral Language Learning (CoNLL), pages 41?48.
Delip Rao, Paul McNamee, and Mark Dredze. 2010.
Streaming cross document entity coreference reso-
lution. In International Conference on Computa-
tional Linguistics (COLING), pages 1050?1058, Bei-
jing, China, August. Coling 2010 Organizing Commit-
tee.
Yael Ravin and Zunaid Kazi. 1999. Is Hillary Rodham
Clinton the president? disambiguating names across
documents. In Annual Meeting of the Association for
Computational Linguistics (ACL), pages 9?16.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1?11, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Evan Sandhaus. 2008. The New York Times annotated
corpus. Linguistic Data Consortium.
Sameer Singh, Amarnag Subramanya, Fernando Pereira,
and Andrew McCallum. 2010. Distributed map in-
ference for undirected graphical models. In Neural
Information Processing Systems (NIPS), Workshop on
Learning on Cores, Clusters and Clouds.
Ben Wellner, Andrew McCallum, Fuchun Peng, and
Michael Hay. 2004. An integrated, conditional model
of information extraction and coreference with appli-
cation to citation matching. In Uncertainty in Artificial
Intelligence (UAI), pages 593?601.
Michael Wick, Aron Culotta, Khashayar Rohanimanesh,
and Andrew McCallum. 2009a. An entity-based
model for coreference resolution. In SIAM Interna-
tional Conference on Data Mining (SDM).
Michael Wick, Khashayar Rohanimanesh, Aron Culotta,
and Andrew McCallum. 2009b. Samplerank: Learn-
ing preferences from atomic gradients. In Neural In-
formation Processing Systems (NIPS), Workshop on
Advances in Ranking.
803
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379?388,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Discriminative Hierarchical Model for Fast Coreference at Large Scale
Michael Wick
University of Massachsetts
140 Governor?s Drive
Amherst, MA
mwick@cs.umass.edu
Sameer Singh
University of Massachusetts
140 Governor?s Drive
Amherst, MA
sameer@cs.umass.edu
Andrew McCallum
University of Massachusetts
140 Governor?s Drive
Amherst, MA
mccallum@cs.umass.edu
Abstract
Methods that measure compatibility between
mention pairs are currently the dominant ap-
proach to coreference. However, they suffer
from a number of drawbacks including diffi-
culties scaling to large numbers of mentions
and limited representational power. As these
drawbacks become increasingly restrictive,
the need to replace the pairwise approaches
with a more expressive, highly scalable al-
ternative is becoming urgent. In this paper
we propose a novel discriminative hierarchical
model that recursively partitions entities into
trees of latent sub-entities. These trees suc-
cinctly summarize the mentions providing a
highly compact, information-rich structure for
reasoning about entities and coreference un-
certainty at massive scales. We demonstrate
that the hierarchical model is several orders
of magnitude faster than pairwise, allowing us
to perform coreference on six million author
mentions in under four hours on a single CPU.
1 Introduction
Coreference resolution, the task of clustering men-
tions into partitions representing their underlying
real-world entities, is fundamental for high-level in-
formation extraction and data integration, including
semantic search, question answering, and knowl-
edge base construction. For example, coreference
is vital for determining author publication lists in
bibliographic knowledge bases such as CiteSeer and
Google Scholar, where the repository must know
if the ?R. Hamming? who authored ?Error detect-
ing and error correcting codes? is the same? ?R.
Hamming? who authored ?The unreasonable effec-
tiveness of mathematics.? Features of the mentions
(e.g., bags-of-words in titles, contextual snippets
and co-author lists) provide evidence for resolving
such entities.
Over the years, various machine learning tech-
niques have been applied to different variations of
the coreference problem. A commonality in many
of these approaches is that they model the prob-
lem of entity coreference as a collection of deci-
sions between mention pairs (Bagga and Baldwin,
1999; Soon et al, 2001; McCallum and Wellner,
2004; Singla and Domingos, 2005; Bengston and
Roth, 2008). That is, coreference is solved by an-
swering a quadratic number of questions of the form
?does mention A refer to the same entity as mention
B?? with a compatibility function that indicates how
likely A and B are coreferent. While these models
have been successful in some domains, they also ex-
hibit several undesirable characteristics. The first is
that pairwise models lack the expressivity required
to represent aggregate properties of the entities. Re-
cent work has shown that these entity-level prop-
erties allow systems to correct coreference errors
made from myopic pairwise decisions (Ng, 2005;
Culotta et al, 2007; Yang et al, 2008; Rahman and
Ng, 2009; Wick et al, 2009), and can even provide
a strong signal for unsupervised coreference (Bhat-
tacharya and Getoor, 2006; Haghighi and Klein,
2007; Haghighi and Klein, 2010).
A second problem, that has received significantly
less attention in the literature, is that the pair-
wise coreference models scale poorly to large col-
lections of mentions especially when the expected
379
Name:,Jamie,Callan,Ins(tu(ons:-CMU,LTI.,Topics:{WWW,,IR,,SIGIR},
Name:Jamie,Callan,Ins(tu(ons:,Topics:-IR,
Name:,J.,Callan,Ins(tu(ons:-CMU,LTI,Topics:-WWW,
Name:,J.,Callan,Ins(tu(ons:-LTI,Topics:-WWW,
Name:,James,Callan,Ins(tu(ons:-CMU,Topics:{WWW,,IR,,largeIscale},
Coref?-
Jamie,Callan,Topics:-IR,
J.,Callan,Inst:-LTI, J.,Callan,Topic:-WWW,
J.,Callan,Inst:-CMU,Jamie,Callan,Topics:-IR, J.,Callan,Inst:-CMU, James,Callan,Topics:-WWW,Inst:CMU,
J.,Callan,Topics:-IR,Inst:-CMU,
J.,Callan,Topics:-LIS,
Figure 1: Discriminative hierarchical factor graph for coreference: Latent entity nodes (white boxes)
summarize subtrees. Pairwise factors (black squares) measure compatibilities between child and parent
nodes, avoiding quadratic blow-up. Corresponding decision variables (open circles) indicate whether one
node is the child of another. Mentions (gray boxes) are leaves. Deciding whether to merge these two entities
requires evaluating just a single factor (red square), corresponding to the new child-parent relationship.
number of mentions in each entity cluster is also
large. Current systems cope with this by either
dividing the data into blocks to reduce the search
space (Herna?ndez and Stolfo, 1995; McCallum et
al., 2000; Bilenko et al, 2006), using fixed heuris-
tics to greedily compress the mentions (Ravin and
Kazi, 1999; Rao et al, 2010), employing special-
ized Markov chain Monte Carlo procedures (Milch
et al, 2006; Richardson and Domingos, 2006; Singh
et al, 2010), or introducing shallow hierarchies of
sub-entities for MCMC block moves and super-
entities for adaptive distributed inference (Singh et
al., 2011). However, while these methods help man-
age the search space for medium-scale data, eval-
uating each coreference decision in many of these
systems still scales linearly with the number of men-
tions in an entity, resulting in prohibitive computa-
tional costs associated with large datasets. This scal-
ing with the number of mentions per entity seems
particularly wasteful because although it is common
for an entity to be referenced by a large number
of mentions, many of these coreferent mentions are
highly similar to each other. For example, in author
coreference the two most common strings that refer
to Richard Hamming might have the form ?R. Ham-
ming? and ?Richard Hamming.? In newswire coref-
erence, a prominent entity like Barack Obama may
have millions of ?Obama? mentions (many occur-
ring in similar semantic contexts). Deciding whether
a mention belongs to this entity need not involve
comparisons to all contextually similar ?Obama?
mentions; rather we prefer a more compact repre-
sentation in order to efficiently reason about them.
In this paper we propose a novel hierarchical dis-
criminative factor graph for coreference resolution
that recursively structures each entity as a tree of la-
tent sub-entities with mentions at the leaves. Our
hierarchical model avoids the aforementioned prob-
lems of the pairwise approach: not only can it jointly
reason about attributes of entire entities (using the
power of discriminative conditional random fields),
but it is also able to scale to datasets with enor-
mous numbers of mentions because scoring enti-
ties does not require computing a quadratic number
of compatibility functions. The key insight is that
each node in the tree functions as a highly compact
information-rich summary of its children. Thus, a
small handful of upper-level nodes may summarize
millions of mentions (for example, a single node
may summarize all contextually similar ?R. Ham-
ming? mentions). Although inferring the structure
of the entities requires reasoning over a larger state-
space, the latent trees are actually beneficial to in-
ference (as shown for shallow trees in Singh et
al. (2011)), resulting in rapid progress toward high
probability regions, and mirroring known benefits
of auxiliary variable methods in statistical physics
(such as Swendsen and Wang (1987)). Moreover,
380
each step of inference is computationally efficient
because evaluating the cost of attaching (or detach-
ing) sub-trees requires computing just a single com-
patibility function (as seen in Figure 1). Further,
our hierarchical approach provides a number of ad-
ditional advantages. First, the recursive nature of the
tree (arbitrary depth and width) allows the model to
adapt to different types of data and effectively com-
press entities of different scales (e.g., entities with
more mentions may require a deeper hierarchy to
compress). Second, the model contains compatibil-
ity functions at all levels of the tree enabling it to si-
multaneously reason at multiple granularities of en-
tity compression. Third, the trees can provide split
points for finer-grained entities by placing contex-
tually similar mentions under the same subtree. Fi-
nally, if memory is limited, redundant mentions can
be pruned by replacing subtrees with their roots.
Empirically, we demonstrate that our model is
several orders of magnitude faster than a pairwise
model, allowing us to perform efficient coreference
on nearly six million author mentions in under four
hours using a single CPU.
2 Background: Pairwise Coreference
Coreference is the problem of clustering mentions
such that mentions in the same set refer to the same
real-world entity; it is also known as entity disam-
biguation, record linkage, and de-duplication. For
example, in author coreference, each mention might
be represented as a record extracted from the author
field of a textual citation or BibTeX record. The
mention record may contain attributes for the first,
middle, and last name of the author, as well as con-
textual information occurring in the citation string,
co-authors, titles, topics, and institutions. The goal
is to cluster these mention records into sets, each
containing all the mentions of the author to which
they refer; we use this task as a running pedagogical
example.
Let M be the space of observed mention records;
then the traditional pairwise coreference approach
scores candidate coreference solutions with a com-
patibility function ? : M ? M ? < that mea-
sures how likely it is that the two mentions re-
fer to the same entity.1 In discriminative log-
1We can also include an incompatibility function for when
linear models, the function ? takes the form of
weights ? on features ?(mi,mj), i.e., ?(mi,mj) =
exp (? ? ?(mi,mj)). For example, in author coref-
erence, the feature functions ? might test whether
the name fields for two author mentions are string
identical, or compute cosine similarity between the
two mentions? bags-of-words, each representing a
mention?s context. The corresponding real-valued
weights ? determine the impact of these features on
the overall pairwise score.
Coreference can be solved by introducing a set of
binary coreference decision variables for each men-
tion pair and predicting a setting to their values that
maximizes the sum of pairwise compatibility func-
tions. While it is possible to independently make
pairwise decisions and enforce transitivity post hoc,
this can lead to poor accuracy because the decisions
are tightly coupled. For higher accuracy, a graphi-
cal model such as a conditional random field (CRF)
is constructed from the compatibility functions to
jointly reason about the pairwise decisions (McCal-
lum and Wellner, 2004). We now describe the pair-
wise CRF for coreference as a factor graph.
2.1 Pairwise Conditional Random Field
Each mention mi ? M is an observed variable, and
for each mention pair (mi,mj) we have a binary
coreference decision variable yij whose value de-
termines whether mi and mj refer to the same en-
tity (i.e., 1 means they are coreferent and 0 means
they are not coreferent). The pairwise compatibility
functions become the factors in the graphical model.
Each factor examines the properties of its mention
pair as well as the setting to the coreference decision
variable and outputs a score indicating how likely
the setting of that coreference variable is. The joint
probability distribution over all possible settings to
the coreference decision variables (y) is given as a
product of all the pairwise compatibility factors:
Pr(y|m) ?
n?
i=1
n?
j=1
?(mi,mj , yij) (1)
Given the pairwise CRF, the problem of coreference
is then solved by searching for the setting of the
coreference decision variables that has the highest
probability according to Equation 1 subject to the
the mentions are not coreferent, e.g., ? :M?M?{0, 1} ? <
381
Jamie,Callan, Jamie,Callan,
J.,Callan,
J.,Callan, J.,Callan,
J.,Callan, Jamie,Callan, Jamie,Callan,
v,Jamie,Callan,
J.,Callan,
v,v,v,
J.,Callan,J.,Callan, J.,Callan,
J.,Callan,
Jamie,Callan,
Figure 2: Pairwise model on six mentions: Open
circles are the binary coreference decision variables,
shaded circles are the observed mentions, and the
black boxes are the factors of the graphical model
that encode the pairwise compatibility functions.
constraint that the setting to the coreference vari-
ables obey transitivity;2 this is the maximum proba-
bility estimate (MPE) setting. However, the solution
to this problem is intractable, and even approximate
inference methods such as loopy belief propagation
can be difficult due to the cubic number of determin-
istic transitivity constraints.
2.2 Approximate Inference
An approximate inference framework that has suc-
cessfully been used for coreference models is
Metropolis-Hastings (MH) (Milch et al (2006), Cu-
lotta and McCallum (2006), Poon and Domingos
(2007), amongst others), a Markov chain Monte
Carlo algorithm traditionally used for marginal in-
ference, but which can also be tuned for MPE in-
ference. MH is a flexible framework for specify-
ing customized local-search transition functions and
provides a principled way of deciding which local
search moves to accept. A proposal function q takes
the current coreference hypothesis and proposes a
new hypothesis by modifying a subset of the de-
cision variables. The proposed change is accepted
with probability ?:
? = min
(
1,
P r(y?)
Pr(y)
q(y|y?)
q(y?|y)
)
(2)
2We say that a full assignment to the coreference variables
y obeys transitivity if ? ijk yij = 1 ? yjk = 1 =? yik = 1
When using MH for MPE inference, the second term
q(y|y?)/q(y?|y) is optional, and usually omitted.
Moves that reduce model score m y be accepted and
an optional temperature can be used for annealing.
The primary advantages of MH for coreference are
(1) only the compatibility functions of the changed
decision variables need to be evaluated to ccept a
move, and (2) the proposal function can enforce the
transitivity constraint by exploring only variable set-
tings that result in valid coreference partitionings.
A commonly used propos l distribution for coref-
erence is the following: (1) randomly select two
mentions (mi,mj), (2) if the mentions (mi,mj) are
in the same entity cluster according to y then move
one mention into a singleton cluster (by setting the
necessary decision variables to 0), otherwise, move
mention mi so it is in the same cluster as mj (by
setting the necessary decision variables). Typically,
MH is employed by first initializing to a singleton
configuration (all entities have one mention), and
then executing the MH for a certain number of steps
(or until the predicted coreference hypothesis stops
changing).
This proposal distribution always moves a sin-
gle mention m from some entity ei to another en-
tity ej and thus the configuration y and y? only dif-
fer by the setting of decision variables governing to
which entity m refers. In order to guarantee transi-
tivity and a valid coreference equivalence relation,
we must properly remove m from ei by untethering
m from each mention in ei (this requires computing
|ei| ? 1 pairwise factors). Similarly?again, for the
sake of transitivity?in order to complete the move
into ej we must coref m to each mention in ej (this
requires computing |ej | pairwise factors). Clearly,
all the other coreference decision variables are in-
dependent and so their corresponding factors can-
cel because they yield the same scores under y and
y?. Thus, evaluating each proposal for the pairwise
model scales linearly with the number of mentions
assigned to the entities, requiring the evaluation of
2(|ei|+ |ej | ? 1) compatibility functions (factors).
3 Hierarchical Coreference
Instead of only capturing a single coreference clus-
tering between mention pairs, we can imagine mul-
tiple levels of coreference decisions over different
382
granularities. For example, mentions of an author
may be further partitioned into semantically similar
sets, such that mentions from each set have topically
similar papers. This partitioning can be recursive,
i.e., each of these sets can be further partitioned, cap-
turing candidate splits for an entity that can facilitate
inference. In this section, we describe a model that
captures arbitrarily deep hierarchies over such lay-
ers of coreference decisions, enabling efficient in-
ference and rich entity representations.
3.1 Discriminative Hierarchical Model
In contrast to the pairwise model, where each en-
tity is a flat cluster of mentions, our proposed model
structures each entity recursively as a tree. The
leaves of the tree are the observed mentions with
a set of attribute values. Each internal node of the
tree is latent and contains a set of unobserved at-
tributes; recursively, these node records summarize
the attributes of their child nodes (see Figure 1), for
example, they may aggregate the bags of context
words of the children. The root of each tree repre-
sents the entire entity, with the leaves containing its
mentions. Formally, the coreference decision vari-
ables in the hierarchical model no longer represent
pairwise decisions directly. Instead, a decision vari-
able yri,rj = 1 indicates that node-record rj is the
parent of node-record ri. We say a node-record ex-
ists if either it is a mention, has a parent, or has at
least one child. Let R be the set of all existing node
records, let rp denote the parent for node r, that is
yr,rp = 1, and ?r? 6= rp, yr,r? = 0. As we describe
in more detail later, the structure of the tree and the
values of the unobserved attributes are determined
during inference.
In order to represent our recursive model of coref-
erence, we include two types of factors: pairwise
factors ?pw that measure compatibility between a
child node-record and its parent, and unit-wise fac-
tors ?rw that measure compatibilities of the node-
records themselves. For efficiency we enforce that
parent-child factors only produce a non-zero score
when the corresponding decision variable is 1. The
unit-wise factors can examine compatibility of set-
tings to the attribute variables for a particular node
(for example, the set of topics may be too diverse
to represent just a single entity), as well as enforce
priors over the tree?s breadth and depth. Our recur-
sive hierarchical model defines the probability of a
configuration as:
Pr(y, R|m) ?
?
r?R
?rw(r)?pw(r, r
p) (3)
3.2 MCMC Inference for Hierarchical models
The state space of our hierarchical model is substan-
tially larger (theoretically infinite) than the pairwise
model due to the arbitrarily deep (and wide) latent
structure of the cluster trees. Inference must simul-
taneously determine the structure of the tree, the la-
tent node-record values, as well as the coreference
decisions themselves.
While this may seem daunting, the structures be-
ing inferred are actually beneficial to inference. In-
deed, despite the enlarged state space, inference
in the hierarchical model is substantially faster
than a pairwise model with a smaller state space.
One explanatory intuition comes from the statisti-
cal physics community: we can view the latent tree
as auxiliary variables in a data-augmentation sam-
pling scheme that guide MCMC through the state
space more efficiently. There is a large body of lit-
erature in the statistics community describing how
these auxiliary variables can lead to faster conver-
gence despite the enlarged state space (classic exam-
ples include Swendsen and Wang (1987) and slice
samplers (Neal, 2000)).
Further, evaluating each proposal during infer-
ence in the hierarchical model is substantially faster
than in the pairwise model. Indeed, we can replace
the linear number of factor evaluations (as in the
pairwise model) with a constant number of factor
evaluations for most proposals (for example, adding
a subtree requires re-evaluating only a single parent-
child factor between the subtree and the attachment
point, and a single node-wise factor).
Since inference must determine the structure of
the entity trees in addition to coreference, it is ad-
vantageous to consider multiple MH proposals per
sample. Therefore, we employ a modified variant
of MH that is similar to multi-try Metropolis (Liu
et al, 2000). Our modified MH algorithm makes k
proposals and samples one according to its model
ratio score (the first term in Equation 2) normalized
across all k. More specificaly, for each MH step, we
first randomly select two subtrees headed by node-
383
records ri and rj from the current coreference hy-
pothesis. If ri and rj are in different clusters, we
propose several alternate merge operations: (also in
Figure 3):
? Merge Left - merges the entire subtree of rj into
node ri by making rj a child of ri
?Merge Entity Left - merges rj with ri?s root
?Merge Left and Collapse - merges rj into ri then
performs a collapse on rj (see below).
? Merge Up - merges node ri with node rj by cre-
ating a new parent node-record variable rp with ri
and rj as the children. The attribute fields of rp are
selected from ri and rj .
Otherwise ri and rj are subtrees in the same entity
tree, then the following proposals are used instead:
? Split Right - Make the subtree rj the root of a new
entity by detaching it from its parent
? Collapse - If ri has a parent, then move ri?s chil-
dren to ri?s parent and then delete ri.
? Sample attribute - Pick a new value for an at-
tribute of ri from its children.
Computing the model ratio for all of coreference
proposals requires only a constant number of com-
patibility functions. On the other hand, evaluating
proposals in the pairwise model requires evaluat-
ing a number of compatibility functions equal to the
number of mentions in the clusters being modified.
Note that changes to the attribute values of the
node-record and collapsing still require evaluating
a linear number of factors, but this is only linear in
the number of child nodes, not linear in the number
of mentions referring to the entity. Further, attribute
values rarely change once the entities stabilize. Fi-
nally, we incrementally update bags during corefer-
ence to reflect the aggregates of their children.
4 Experiments: Author Coreference
Author coreference is a tremendously important
task, enabling improved search and mining of sci-
entific papers by researchers, funding agencies, and
governments. The problem is extremely difficult due
to the wide variations of names, limited contextual
evidence, misspellings, people with common names,
lack of standard citation formats, and large numbers
of mentions.
For this task we use a publicly available collec-
tion of 4,394 BibTeX files containing 817,193 en-
tries.3 We extract 1,322,985 author mentions, each
containing first, middle, last names, bags-of-words
of paper titles, topics in paper titles (by running la-
tent Dirichlet alocation (Blei et al, 2003)), and last
names of co-authors. In addition we include 2,833
mentions from the REXA dataset4 labeled for coref-
erence, in order to assess accuracy. We also include
?5 million mentions from DBLP.
4.1 Models and Inference
Due to the paucity of labeled training data, we did
not estimate parameters from data, but rather set
the compatibility functions manually by specifying
their log scores. The pairwise compatibility func-
tions punish a string difference in first, middle, and
last name, (?8); reward a match (+2); and reward
matching initials (+1). Additionally, we use the co-
sine similarity (shifted and scaled between ?4 and
4) between the bags-of-words containing title to-
kens, topics, and co-author last names. These com-
patibility functions define the scores of the factors
in the pairwise model and the parent-child factors
in the hierarchical model. Additionally, we include
priors over the model structure. We encourage each
node to have eight children using a per node factor
having score 1/(|number of children?8|+1), manage
tree depth by placing a cost on the creation of inter-
mediate tree nodes ?8 and encourage clustering by
placing a cost on the creation of root-level entities
?7. These weights were determined by just a few
hours of tuning on a development set.
We initialize the MCMC procedures to the single-
ton configuration (each entity consists of one men-
tion) for each model, and run the MH algorithm de-
scribed in Section 2.2 for the pairwise model and
multi-try MH (described in Section 3.2) for the hi-
erarchical model. We augment these samplers us-
ing canopies constructed by concatenating the first
initial and last name: that is, mentions are only
selected from within the same canopy (or block)
to reduce the search space (Bilenko et al, 2006).
During the course of MCMC inference, we record
the pairwise F1 scores of the labeled subset. The
source code for our model is available as part of the
FACTORIE package (McCallum et al, 2009, http:
3http://www.iesl.cs.umass.edu/data/bibtex
4http://www2.selu.edu/Academics/Faculty/
aculotta/data/rexa.html
384
!"#!$#
!%#
!"#!$# !"#!$# !$#
&"#
!"#!$#
&"#&"#
!"#$%&'()%)*' +*,-*'.*/' +*,-*'0"$)1'.*/' +*,-*'23' +*,-*'.*/'%"4'56&&%3(*'
!"#
!"#$%&'7)%)*'
&"#&$#
!'"#
&"#&$#
!'"#
73&#)8#-9)'
&$# !'"#
56&&%3(*'
Figure 3: Example coreference proposals for the case where ri and rj are initially in different clusters.
//factorie.cs.umass.edu/).
4.2 Comparison to Pairwise Model
In Figure 4a we plot the number of samples com-
pleted over time for a 145k subset of the data. Re-
call that we initialized to the singleton configuration
and that as the size of the entities grows, the cost of
evaluating the entities in MCMC becomes more ex-
pensive. The pairwise model struggles with the large
cluster sizes while the hierarchical model is hardly
affected. Even though the hierarchical model is eval-
uating up to four proposals for each sample, it is still
able to sample much faster than the pairwise model;
this is expected because the cost of evaluating a pro-
posal requires evaluating fewer factors. Next, we
plot coreference F1 accuracy over time and show in
Figure 5a that the prolific sampling rate of the hierar-
chical model results in faster coreference. Using the
plot, we can compare running times for any desired
level of accuracy. For example, on the 145k men-
tion dataset, at a 60% accuracy level the hierarchical
model is 19 times faster and at 90% accuracy it is
31 times faster. These performance improvements
are even more profound on larger datasets: the hi-
erarchical model achieves a 60% level of accuracy
72 times faster than the pairwise model on the 1.3
million mention dataset, reaching 90% in just 2,350
seconds. Note, however, that the hierarchical model
requires more samples to reach a similar level of ac-
curacy due to the larger state space (Figure 4b).
4.3 Large Scale Experiments
In order to demonstrate the scalability of the hierar-
chical model, we run it on nearly 5 million author
mentions from DBLP. In under two hours (6,700
seconds), we achieve an accuracy of 80%, and in
under three hours (10,600 seconds), we achieve an
accuracy of over 90%. Finally, we combine DBLP
with BibTeX data to produce a dataset with almost 6
million mentions (5,803,811). Our performance on
this dataset is similar to DBLP, taking just 13,500
seconds to reach a 90% accuracy.
5 Related Work
Singh et al (2011) introduce a hierarchical model
for coreference that treats entities as a two-tiered
structure, by introducing the concept of sub-entities
and super-entities. Super-entities reduce the search
space in order to propose fruitful jumps. Sub-
entities provide a tighter granularity of coreference
and can be used to perform larger block moves dur-
ing MCMC. However, the hierarchy is fixed and
shallow. In contrast, our model can be arbitrarily
deep and wide. Even more importantly, their model
has pairwise factors and suffers from the quadratic
curse, which they address by distributing inference.
The work of Rao et al (2010) uses streaming
clustering for large-scale coreference. However, the
greedy nature of the approach does not allow errors
to be revisited. Further, they compress entities by
averaging their mentions? features. We are able to
provide richer entity compression, the ability to re-
visit errors, and scale to larger data.
Our hierarchical model provides the advantages
of recently proposed entity-based coreference sys-
tems that are known to provide higher accuracy
(Haghighi and Klein, 2007; Culotta et al, 2007;
Yang et al, 2008; Wick et al, 2009; Haghighi and
Klein, 2010). However, these systems reason over a
single layer of entities and do not scale well.
Techniques such as lifted inference (Singla and
Domingos, 2008) for graphical models exploit re-
dundancy in the data, but typically do not achieve
any significant compression on coreference data be-
385
Samples versus Time
0 500 1,000 1,500 2,000
Running time (s)
0
50,000
100,000
150,000
200,000
250,000
300,000
350,000
400,000
Nu
mb
er o
f Sa
mp
les
Hierar Pairwise
(a) Sampling Performance
Accuracy versus Samples
0 50,000 100,000 150,000 200,000
Number of Samples
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
F1 
Acc
ura
cy
Hierar Pairwise
(b) Accuracy vs. samples (convergence accuracy as dashes)
Figure 4: Sampling Performance Plots for 145k mentions
Accuracy versus Time
0 250 500 750 1,000 1,250 1,500 1,750 2,000
Running time (s)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
F1 
Ac
cur
acy
Hierar Pairwise
(a) Accuracy vs. time (145k mentions)
Accuracy versus Time
0 10,000 20,000 30,000 40,000 50,000 60,000
Running time (s)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
F1 
Acc
ura
cy
Hierar Pairwise
(b) Accuracy vs. time (1.3 million mentions)
Figure 5: Runtime performance on two datasets
cause the observations usually violate any symmetry
assumptions. On the other hand, our model is able
to compress similar (but potentially different) obser-
vations together in order to make inference fast even
in the presence of asymmetric observed data.
6 Conclusion
In this paper we present a new hierarchical model
for large scale coreference and demonstrate it on
the problem of author disambiguation. Our model
recursively defines an entity as a summary of its
children nodes, allowing succinct representations of
millions of mentions. Indeed, inference in the hier-
archy is orders of magnitude faster than a pairwise
CRF, allowing us to infer accurate coreference on
six million mentions on one CPU in just 4 hours.
7 Acknowledgments
We would like to thank Veselin Stoyanov for his feed-
back. This work was supported in part by the CIIR, in
part by ARFL under prime contract #FA8650-10-C-7059,
in part by DARPA under AFRL prime contract #FA8750-
09-C-0181, and in part by IARPA via DoI/NBC contract
#D11PC20152. The U.S. Government is authorized to
reproduce and distribute reprints for Governmental pur-
poses notwithstanding any copyright annotation thereon.
Any opinions, findings and conclusions or recommenda-
tions expressed in this material are those of the authors
and do not necessarily reflect those of the sponsor.
386
References
Amit Bagga and Breck Baldwin. 1999. Cross-document
event coreference: annotations, experiments, and ob-
servations. In Proceedings of the Workshop on Coref-
erence and its Applications, CorefApp ?99, pages 1?8,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Eric Bengston and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Empirical Methods in Natural Language Processing
(EMNLP).
Indrajit Bhattacharya and Lise Getoor. 2006. A latent
Dirichlet model for unsupervised entity resolution. In
SDM.
Mikhail Bilenko, Beena Kamath, and Raymond J.
Mooney. 2006. Adaptive blocking: Learning to scale
up record linkage. In Proceedings of the Sixth Interna-
tional Conference on Data Mining, ICDM ?06, pages
87?96, Washington, DC, USA. IEEE Computer Soci-
ety.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal on Machine
Learning Research, 3:993?1022.
Aron Culotta and Andrew McCallum. 2006. Prac-
tical Markov logic containing first-order quantifiers
with application to identity uncertainty. In Human
Language Technology Workshop on Computationally
Hard Problems and Joint Inference in Speech and Lan-
guage Processing (HLT/NAACL), June.
Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for coreference
resolution. In North American Chapter of the Associa-
tion for Computational Linguistics - Human Language
Technologies (NAACL HLT).
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 848?855.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies
(NAACL HLT), pages 385?393.
Mauricio A. Herna?ndez and Salvatore J. Stolfo. 1995.
The merge/purge problem for large databases. In Pro-
ceedings of the 1995 ACM SIGMOD international
conference on Management of data, SIGMOD ?95,
pages 127?138, New York, NY, USA. ACM.
Jun S. Liu, Faming Liang, and Wing Hung Wong. 2000.
The multiple-try method and local optimization in
metropolis sampling. Journal of the American Statis-
tical Association, 96(449):121?134.
Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In Neural Information Processing Sys-
tems (NIPS).
Andrew McCallum, Kamal Nigam, and Lyle Ungar.
2000. Efficient clustering of high-dimensional data
sets with application to reference matching. In In-
ternational Conference on Knowledge Discovery and
Data Mining (KDD), pages 169?178.
Andrew McCallum, Karl Schultz, and Sameer Singh.
2009. FACTORIE: Probabilistic programming via im-
peratively defined factor graphs. In Neural Informa-
tion Processing Systems (NIPS).
Brian Milch, Bhaskara Marthi, and Stuart Russell. 2006.
BLOG: Relational Modeling with Unknown Objects.
Ph.D. thesis, University of California, Berkeley.
Radford Neal. 2000. Slice sampling. Annals of Statis-
tics, 31:705?767.
Vincent Ng. 2005. Machine learning for coreference res-
olution: From local classification to global ranking. In
Annual Meeting of the Association for Computational
Linguistics (ACL).
Hoifung Poon and Pedro Domingos. 2007. Joint infer-
ence in information extraction. In AAAI Conference
on Artificial Intelligence, pages 913?918.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 2 - Volume 2, EMNLP
?09, pages 968?977, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Delip Rao, Paul McNamee, and Mark Dredze. 2010.
Streaming cross document entity coreference reso-
lution. In International Conference on Computa-
tional Linguistics (COLING), pages 1050?1058, Bei-
jing, China, August. Coling 2010 Organizing Commit-
tee.
Yael Ravin and Zunaid Kazi. 1999. Is Hillary Rodham
Clinton the president? disambiguating names across
documents. In Annual Meeting of the Association for
Computational Linguistics (ACL), pages 9?16.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62(1-
2):107?136.
Sameer Singh, Michael L. Wick, and Andrew McCallum.
2010. Distantly labeling data for large scale cross-
document coreference. Computing Research Reposi-
tory (CoRR), abs/1005.4298.
Sameer Singh, Amarnag Subramanya, Fernando Pereira,
and Andrew McCallum. 2011. Large-scale cross-
document coreference using distributed inference and
hierarchical models. In Association for Computa-
tional Linguistics: Human Language Technologies
(ACL HLT).
387
Parag Singla and Pedro Domingos. 2005. Discrimina-
tive training of Markov logic networks. In AAAI, Pitts-
burgh, PA.
Parag Singla and Pedro Domingos. 2008. Lifted first-
order belief propagation. In Proceedings of the 23rd
national conference on Artificial intelligence - Volume
2, AAAI?08, pages 1094?1099. AAAI Press.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to coref-
erence resolution of noun phrases. Comput. Linguist.,
27(4):521?544.
R.H. Swendsen and J.S. Wang. 1987. Nonuniversal crit-
ical dynamics in MC simulations. Phys. Rev. Lett.,
58(2):68?88.
Michael Wick, Aron Culotta, Khashayar Rohanimanesh,
and Andrew McCallum. 2009. An entity-based model
for coreference resolution. In SIAM International
Conference on Data Mining (SDM).
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, Ting
Liu, and Sheng Li. 2008. An entity-mention model for
coreference resolution with inductive logic program-
ming. In Association for Computational Linguistics,
pages 843?851.
388
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 712?720,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Unsupervised Relation Discovery with Sense Disambiguation
Limin Yao Sebastian Riedel Andrew McCallum
Department of Computer Science
University of Massachusetts, Amherst
{lmyao,riedel,mccallum}@cs.umass.edu
Abstract
To discover relation types from text, most
methods cluster shallow or syntactic patterns
of relation mentions, but consider only one
possible sense per pattern. In practice this
assumption is often violated. In this paper
we overcome this issue by inducing clusters
of pattern senses from feature representations
of patterns. In particular, we employ a topic
model to partition entity pairs associated with
patterns into sense clusters using local and
global features. We merge these sense clus-
ters into semantic relations using hierarchical
agglomerative clustering. We compare against
several baselines: a generative latent-variable
model, a clustering method that does not dis-
ambiguate between path senses, and our own
approach but with only local features. Exper-
imental results show our proposed approach
discovers dramatically more accurate clusters
than models without sense disambiguation,
and that incorporating global features, such as
the document theme, is crucial.
1 Introduction
Relation extraction (RE) is the task of determin-
ing semantic relations between entities mentioned in
text. RE is an essential part of information extraction
and is useful for question answering (Ravichandran
and Hovy, 2002), textual entailment (Szpektor et al,
2004) and many other applications.
A common approach to RE is to assume that rela-
tions to be extracted are part of a predefined ontol-
ogy. For example, the relations are given in knowl-
edge bases such as Freebase (Bollacker et al, 2008)
or DBpedia (Bizer et al, 2009). However, in many
applications, ontologies do not yet exist or have low
coverage. Even when they do exist, their mainte-
nance and extension are considered to be a substan-
tial bottleneck. This has led to considerable inter-
est in unsupervised relation discovery (Hasegawa et
al., 2004; Banko and Etzioni, 2008; Lin and Pantel,
2001; Bollegala et al, 2010; Yao et al, 2011). Here,
the relation extractor simultaneously discovers facts
expressed in natural language, and the ontology into
which they are assigned.
Many relation discovery methods rely exclusively
on the notion of either shallow or syntactic patterns
that appear between two named entities (Bollegala et
al., 2010; Lin and Pantel, 2001). Such patterns could
be sequences of lemmas and Part-of-Speech tags, or
lexicalized dependency paths. Generally speaking,
relation discovery attempts to cluster such patterns
into sets of equivalent or similar meaning. Whether
we use sequences or dependency paths, we will en-
counter the problem of polysemy. For example, a
pattern such as ?A beat B? can mean that person A
wins over B in competing for a political position,
as pair ?(Hillary Rodham Clinton, Jonathan Tasini)?
in ?Sen Hillary Rodham Clinton beats rival Jonathan
Tasini for Senate.? It can also indicate that an athlete
A beat B in a sports match, as pair ?(Dmitry Tur-
sunov, Andy Roddick)? in ?Dmitry Tursunov beat
the best American player Andy Roddick.? More-
over, it can mean ?physically beat? as pair ?(Mr.
Harris, Mr. Simon)? in ?On Sept. 7, 1999, Mr. Har-
ris fatally beat Mr. Simon.? This is known as poly-
semy. If we work with patterns alone, our extractor
will not be able to differentiate between these cases.
Most previous approaches do not explicitly ad-
dress this problem. Lin and Pantel (2001) assumes
only one sense per path. In (Pantel et al, 2007),
they augment each relation with its selectional pref-
712
erences, i.e. fine-grained entity types of two ar-
guments, to handle polysemy. However, such fine
grained entity types come at a high cost. It is difficult
to discover a high-quality set of fine-grained entity
types due to unknown criteria for developing such
a set. In particular, the optimal granularity of en-
tity types depends on the particular pattern we con-
sider. For example, a pattern like ?A beat B? could
refer to A winning a sports competition against B, or
a political election. To differentiate between these
senses we need types such as ?Politician? or ?Ath-
lete?. However, for ?A, the parent of B? we only
need to distinguish between persons and organiza-
tions (for the case of the sub-organization relation).
In addition, there are senses that just cannot be de-
termined by entity types alone: Take the meaning
of ?A beat B? where A and B are both persons; this
could mean A physically beats B, or it could mean
that A defeated B in a competition.
In this paper we address the problem of polysemy,
while we circumvent the problem of finding fine-
grained entity types. Instead of mapping entities to
fine-grained types, we directly induce pattern senses
by clustering feature representations of pattern con-
texts, i.e. the entity pairs associated with a pattern.
This allows us to employ not only local features such
as words, but also global features such as the docu-
ment and sentence themes.
To cluster the entity pairs of a single relation pat-
tern into senses, we develop a simple extension to
Latent Dirichlet Allocation (Blei et al, 2003). Once
we have our pattern senses, we merge them into
clusters of different patterns with a similar sense.
We employ hierarchical agglomerative clustering
with a similarity metric that considers features such
as the entity arguments, and the document and sen-
tence themes.
We perform experiments on New York Times ar-
ticles and consider lexicalized dependency paths as
patterns in our data. In the following we shall use
the term path and pattern exchangeably. We com-
pare our approach with several baseline systems, in-
cluding a generative model approach, a clustering
method that does not disambiguate between senses,
and our approach with different features. We per-
form both automatic and manual evaluations. For
automatic evaluation, we use relation instances in
Freebase as ground truth, and employ two clustering
metrics, pairwise F-score and B3 (as used in cofer-
ence). Experimental results show that our approach
improves over the baselines, and that using global
features achieves better performance than using en-
tity type based features. For manual evaluation, we
employ a set intrusion method (Chang et al, 2009).
The results also show that our approach discovers re-
lation clusters that human evaluators find coherent.
2 Our Approach
We induce pattern senses by clustering the entity
pairs associated with a pattern, and discover seman-
tic relations by clustering these sense clusters. We
represent each pattern as a list of entity pairs and
employ a topic model to partition them into different
sense clusters using local and global features. We
take each sense cluster of a pattern as an atomic clus-
ter, and use hierarchical agglomerative clustering to
organize them into semantic relations. Therefore, a
semantic relation comprises a set of sense clusters of
patterns. Note that one pattern can fall into different
semantic relations when it has multiple senses.
2.1 Sense Disambiguation
In this section, we discuss the details of how we dis-
cover senses of a pattern. For each pattern, we form
a clustering task by collecting all entity pairs the pat-
tern connects. Our goal is to partition these entity
pairs into sense clusters. We represent each pair by
the following features.
Entity names: We use the surface string of the en-
tity pair as features. For example, for pattern ?A play
B?, pairs which contain B argument ?Mozart? could
be in one sense, whereas pairs which have ?Mets?
could be in another sense.
Words: The words between and around the two
entity arguments can disambiguate the sense of a
path. For example, ?A?s parent company B? is dif-
ferent from ?A?s largest company B? although they
share the same path ?A?s company B?. The former
describes the sub-organization relationship between
two companies, while the latter describes B as the
largest company in a location A. The two words to
the left of the source argument, and to the right of the
destination argument also help sense discovery. For
example, in ?Mazurkas played by Anna Kijanowska,
pianist?, ?pianist? tells us pattern ?A played by B?
713
takes the ?music? sense.
Document theme: Sometimes, the same pattern
can express different relations in different docu-
ments, depending on the document?s theme. For
instance, in a document about politics, ?A defeated
B? is perhaps about a politician that won an elec-
tion against another politician. While in a document
about sports, it could be a team that won against an-
other team in a game, or an athlete that defeated an-
other athlete. In our experiments, we use the meta-
descriptors of a document as side information and
train a standard LDA model to find the theme of a
document. See Section 3.1 for details.
Sentence theme: A document may cover several
themes. Moreover, sometimes the theme of a doc-
ument is too general to disambiguate senses. We
therefore also extract the theme of a sentence as a
feature. Details are in 3.1.
We call entity name and word features local, and
the two theme features global.
We employ a topic model to discover senses for
each path. Each path pi forms a document, and it
contains a list of entity pairs co-occurring with the
path in the tuples. Each entity pair is represented
by a list of features fk as we described. For each
path, we draw a multinomial distribution ? over top-
ics/senses. For each feature of an entity pair, we
draw a topic/sense from ?pi . Formally, the gener-
ative process is as follows:
?pi ? Dirichlet(?)
?z ? Dirichlet(?)
ze ? Multinomial(?pi)
fk ? Multinomial(?ze)
Assume we have m paths and l entity pairs for each
path. We denote each entity pair of a path as e(pi) =
(f1, . . . , fn). Hence we have:
P (e1(pi), e2(pi), . . . , el(pi)|z1, z2, . . . , zl)
=
l?
j=1
n?
k=1
p(fk|zj)p(zj)
We assume the features are conditionally indepen-
dent given the topic assignments. Each feature is
generated from a multinomial distribution ?. We
use Dirichlet priors on ? and ?. Figure 1 shows the
graphical representation of this model.
S
p
?
e(p)
f
?
?
z
?
n
Figure 1: Sense-LDA model.
This model is a minor variation on standard LDA
and the difference is that instead of drawing an ob-
servation from a hidden topic variable, we draw
multiple observations from a hidden topic variable.
Gibbs sampling is used for inference. After infer-
ence, each entity pair of a path is assigned to one
topic. One topic is one sense. Entity pairs which
share the same topic assignments form one sense
cluster.
2.2 Hierarchical Agglomerative Clustering
After discovering sense clusters of paths, we employ
hierarchical agglomerative clustering (HAC) to dis-
cover semantic relations from these sense clusters.
We apply the complete linkage strategy and take co-
sine similarity as the distance function. The cutting
threshold is set to 0.1.
We represent each sense cluster as one vector by
summing up features from each entity pair in the
cluster. The weight of a feature indicates how many
entity pairs in the cluster have the feature. Some
features may get larger weights and dominate the co-
sine similarity. We down-weigh these features. For
example, we use binary features for word ?defeat?
in sense clusters of pattern ?A defeat B?. The two
theme features are extracted from generative mod-
els, and each is a topic number.
Our approach produces sense clusters for each
path and semantic relation clusters of the whole data.
Table 1 and 2 show some example output.
3 Experiments
We carry out experiments on New York Times ar-
ticles from years 2000 to 2007 (Sandhaus, 2008).
Following (Yao et al, 2011), we filter out noisy doc-
uments and use natural language packages to anno-
tate the documents, including NER tagging (Finkel
et al, 2005) and dependency parsing (Nivre et al,
2004). We extract dependency paths for each pair of
named entities in one sentence. We use their lemmas
714
Path 20:sports 30:entertainment 25:music/art
A play B
Americans, Ireland Jean-Pierre Bacri, Jacques Daniel Barenboim, recital of Mozart
Yankees, Angels Rita Benton, Gay Head Dance Mr. Rose, Ballade
Ecuador, England Jeanie, Scrabble Gil Shaham, Violin Romance
Redskins, Detroit Meryl Streep, Leilah Ms. Golabek, Steinways
Red Bulls, F.C. Barcelona Kevin Kline, Douglas Fairbanks Bruce Springsteen, Saints
doc theme sports music books television music theater
sen theme game yankees theater production book film show music reviews opera
lexical words beat victory num-num won played plays directed artistic director conducted production
entity names - r:theater r:theater r:hall r:york l:opera
Table 1: Example sense clusters produced by sense disambiguation. For each sense, we randomly sample 5 entity
pairs. We also show top features for each sense. Each row shows one feature type, where ?num? stands for digital
numbers, and prefix ?l:? for source argument, prefix ?r:? for destination argument. Some features overlap with each
other. We manually label each sense for easy understanding. We can see the last two senses are close to each other.
For two theme features, we replace the theme number with the top words. For example, the document theme of the
first sense is Topic30, and Topic30 has top words ?sports?.
relation paths
entertainment A, who play B:30; A play B:30; star A as B:30
sports
lead A to victory over B:20; A play to B:20; A play B:20; A?s loss to B:20; A beat B:20; A trail B:20;
A face B:26; A hold B:26; A play B:26; A acquire (X) from B:26; A send (X) to B:26;
politics
A nominate B:39; A name B:39; A select B:39; A name B:42; A select B:42;
A ask B:42; A choose B:42; A nominate B:42; A turn to B:42;
law A charge B:39; A file against B:39; A accuse B:39; A sue B:39
Table 2: Example semantic relation clusters produced by our approach. For each cluster, we list the top paths in it,
and each is followed by ?:number?, indicating its sense obtained from sense disambiguation. They are ranked by the
number of entity pairs they take. The column on the left shows sense of each relation. They are added manually by
looking at the sense numbers associated with each path.
for words on the dependency paths. Each entity pair
and the dependency path which connects them form
a tuple.
We filter out paths which occur fewer than 200
times and use some heuristic rules to filter out paths
which are unlikely to represent a relation, for exam-
ple, paths in with both arguments take the syntac-
tic role ?dobj? (direct objective) in the dependency
path. In such cases both arguments are often part
of a coordination structure, and it is unlikely that
they are related. In summary, we collect about one
million tuples, 1300 patterns and half million named
entities. In terms of named entities, the data is very
sparse. On average one named entity occurs four
times.
3.1 Feature Extraction
For the entity name features, we split each entity
string of a tuple into tokens. Each token is a fea-
ture. The source argument tokens are augmented
with prefix ?l:?, and the destination argument tokens
with prefix ?r:?. We use tokens to encourage overlap
between different entities.
For the word features, we extract all the words be-
tween the two arguments, removing stopwords and
the words with capital letters. Words with capital
letters are usually named entities, and they do not
tend to indicate relations. We also extract neigh-
boring words of source and destination arguments.
The two words to the left of the source argument are
added with prefix ?lc:?. Similarly the two words to
the right of the destination arguments are added with
prefix ?rc:?.
Each document in the NYT corpus is associated
with many descriptors, indicating the topic of the
document. For example, some documents are la-
beled as ?Sports?, ?Dallas Cowboys?, ?New York
Giants?, ?Pro Football? and so on. Some are labeled
715
as ?Politics and Government?, and ?Elections?. We
shall extract a theme feature for each document from
these descriptors. To this end we interpret the de-
scriptors as words in documents, and train a standard
LDA model based on these documents. We pick the
most frequent topic as the theme of a document.
We also train a standard LDA model to obtain
the theme of a sentence. We use a bag-of-words
representation for a document and ignore sentences
from which we do not extract any tuples. The LDA
model assigns each word to a topic. We count the
occurrences of all topics in one sentence and pick
the most frequent one as its theme. This feature
captures the intuition that different words can indi-
cate the same sense, for example, ?film??, ?show?,
?series? and ?television? are about ?entertainment?,
while ?coach?, ?game?, ?jets?, ?giants? and ?sea-
son? are about ?sports?.
3.2 Sense clusters and relation clusters
For the sense disambiguation model, we set the
number of topics (senses) to 50. We experimented
with other numbers, but this setting yielded the best
results based on our automatic evaluation measures.
Note that a path has a multinomial distribution over
50 senses but only a few senses have non-zero prob-
abilities.
We look at some sense clusters of paths. For
path ?A play B?, we examine the top three senses,
as shown in Table 1. The last two senses ?enter-
tainment? and ?music? are close. Randomly sam-
pling some entity pairs from each of them, we find
that the two sense clusters are precise. Only 1% of
pairs from the sense cluster ?entertainment? should
be assigned to the ?music? sense. For the path ?play
A in B? we discover two senses which take the
most probabilities: ?sports? and ?art?. Both clus-
ters are precise. However, the ?sports? sense may
still be split into more fine-grained sense clusters. In
?sports?, 67% pairs mean ?play another team in a
location? while 33% mean ?play another team in a
game?.
We also closely investigate some relation clusters,
shown in Table 2. Both the first and second relation
contain path ?A play B? but with different senses.
For the second relation, most paths state ?play? re-
lations between two teams, while a few of them
express relations of teams acquiring players from
other teams. For example, the entity pair ?(Atlanta
Hawks, Dallas Mavericks)? mentioned in sentence
?The Atlanta Hawks acquired point guard Anthony
Johnson from the Dallas Mavericks.? This is due to
that they share many entity pairs of team-team.
3.3 Baselines
We compare our approach against several baseline
systems, including a generative model approach and
variations of our own approach.
Rel-LDA: Generative models have been suc-
cessfully applied to unsupervised relation extrac-
tion (Rink and Harabagiu, 2011; Yao et al, 2011).
We compare against one such model: An extension
to standard LDA that falls into the framework pre-
sented by Yao et al (2011). Each document con-
sists of a list of tuples. Each tuple is represented by
features of the entity pair, as listed in 2.1, and the
path. For each document, we draw a multinomial
distribution over relations. For each tuple, we draw
a relation topic and independently generate all the
features. The intuition is that each document dis-
cusses one domain, and has a particular distribution
over relations.
In our experiments, we test different numbers of
relation topics. As the number goes up, precision in-
creases whereas recall drops. We report results with
300 and 1000 relation topics.
One sense per path (HAC): This system uses
only hierarchical clustering to discover relations,
skipping sense disambiguation. This is similar to
DIRT (Lin and Pantel, 2001). In DIRT, each path
is represented by its entity arguments. DIRT cal-
culates distributional similarities between different
paths to find paths which bear the same semantic re-
lation. It does not employ global topic model fea-
tures extracted from documents and sentences.
Local: This system uses our approach (both sense
clustering with topic models and hierarchical clus-
tering), but without global features.
Local+Type This system adds entity type features to
the previous system. This allows us to compare per-
formance of using global features against entity type
features. To determine entity types, we link named
entities to Wikipedia pages using the Wikifier (Rati-
nov et al, 2011) package and extract categories from
the Wikipedia page. Generally Wikipedia provides
many types for one entity. For example, ?Mozart? is
716
a person, musician, pianist, composer, and catholic.
As we argued in Section 1, it is difficult to determine
the right granularity of the entity types to use. In our
experiments, we use all of them as features. In hier-
archical clustering, for each sense cluster of a path,
we pick the most frequent entity type as a feature.
This approach can be seen as a proxy to ISP (Pantel
et al, 2007), since selectional preferences are one
way of distinguishing multiple senses of a path.
Our Approach+Type This system adds Wikipedia
entity type features to our approach. The Wikipedia
feature is the same as used in the previous system.
4 Evaluations
4.1 Automatic Evaluation against Freebase
We evaluate relation clusters discovered by all ap-
proaches against Freebase. Freebase comprises a
large collection of entities and relations which come
from varieties of data sources, including Wikipedia
infoboxes. Many users also contribute to Freebase
by annotating relation instances. We use coreference
evaluation metrics: pairwise F-score and B3 (Bagga
and Baldwin, 1998). Pairwise metrics measure how
often two tuples which are clustered in one seman-
tic relation are labeled with the same Freebase label.
We evaluate approximately 10,000 tuples which oc-
cur in both our data and Freebase. Since our sys-
tem predicts fine-grained clusters comparing against
Freebase relations, the measure of recall is underes-
timated. The precision measure is more reliable and
we employ F-0.5 measure, which places more em-
phasis on precision.
Matthews correlation coefficient (MCC) (Baldi et
al., 2000) is another measure used in machine learn-
ing, which takes into account true and false positives
and negatives and is generally regarded as a bal-
anced measure which can be used when the classes
are of very different sizes. In our case, the true nega-
tive number is 100 times larger than the true positive
number. Therefor we also employ MCC, calculated
as
MCC = TP?TN?FP?FN?
(TP+FP )(TP+FN)(TN+FP )(TN+FN)
The MCC score is between -1 and 1. The larger the
better. In perfect predictions, FP and FN are 0, and
the MCC score is 1. A random prediction results in
score 0.
Table 3 shows the results of all systems. Our ap-
proach achieves the best performance in most mea-
sures. Without using sense disambiguation, the per-
formance of hierarchical clustering decreases signif-
icantly, losing 17% in precision in the pairwise mea-
sure, and 15% in terms ofB3. The generative model
approach with 300 topics achieves similar precision
to the hierarchical clustering approach. With more
topics, the precision increases, however, the recall
of the generative model is much lower than those
of other approaches. We also show the results of
our approach without global document and sentence
theme features (Local). In this case, both precision
and recall decrease. We compare global features
(Our approach) against Wikipedia entity type fea-
tures (Local+Type). We see that using global fea-
tures achieves better performance than using entity
type based features. When we add entity type fea-
tures to our approach, the performance does not in-
crease. The entity type features do not help much
is due to that we cannot determine which particular
type to choose for an entity pair. Take pair ?(Hillary
Rodham Clinton, Jonathan Tasini)? as an example,
choosing politician for both arguments instead of
person will help.
We should note that these measures provide com-
parison between different systems although they
are not accurate. One reason is the following:
some relation instances should have multiple la-
bels but they have only one label in Freebase.
For example, instances of a relation that a per-
son ?was born in? a country could be labeled
as ?/people/person/place of birth? and as ?/peo-
ple/person/nationality?. This decreases the pairwise
precision. Further discussion is in Section 4.3.
4.2 Path Intrusion
We also evaluate coherence of relation clusters pro-
duced by different approaches by creating path in-
trusion tasks (Chang et al, 2009). In each task, some
paths from one cluster and an intruding path from
another are shown, and the annotator?s job is to iden-
tify one single path which is out of place. For each
path, we also show the annotators one example sen-
tence. Three graduate students in natural language
processing annotate intruding paths. For disagree-
ments, we use majority voting. Table 4 shows one
example intrusion task.
717
System
Pairwise B3
Prec. Rec. F-0.5 MCC Prec. Rec. F-0.5
Rel-LDA/300 0.593 0.077 0.254 0.191 0.558 0.183 0.396
Rel-LDA/1000 0.638 0.061 0.220 0.177 0.626 0.160 0.396
HAC 0.567 0.152 0.367 0.261 0.523 0.248 0.428
Local 0.625 0.136 0.364 0.264 0.626 0.225 0.462
Local+Type 0.718 0.115 0.350 0.265 0.704 0.201 0.469
Our Approach 0.736 0.156 0.422 0.314 0.677 0.233 0.490
Our Approach+Type 0.682 0.110 0.334 0.250 0.687 0.199 0.460
Table 3: Pairwise and B3 evaluation for various systems. Since our systems predict more fine-grained clusters than
Freebase, the recall measure is underestimated.
Path Example sentence
A beat B Dmitry Tursunov beat the best American player, Andy Roddick
A, who lose to B Sluman, Loren Roberts (who lost a 1994 Open playoff to Ernie Els at Oakmont ...
A, who beat B ... offender seems to be the Russian Mariya Sharapova, who beat Jelena Dokic
A, a broker at B Robert Bewkes, a broker at UBS for 12 years
A meet B Howell will meet Geoff Ogilvy, Harrington will face Davis Love III
Table 4: A path intrusion task. We show 5 paths and ask the annotator to identify one path which does not belong to
the cluster. And we show one example sentence for each path. The entities (As and Bs) in the sentences are bold. And
the italic row here indicates the intruder.
System Correct
Rel-LDA/300 0.737
Rel-LDA/1000 0.821
HAC 0.852
Local+Type 0.773
Our approach 0.887
Table 5: Results of intruding tasks of all systems.
From Table 5, we see that our approach achieves
the best performance. We concentrate on some in-
trusion tasks and compare the clusters produced by
different systems.
The clusters produced by HAC (without sense dis-
ambiguation) is coherent if all the paths in one rela-
tion take a particular sense. For example, one task
contains paths ?A, director at B?, ?A, specialist at
B?, ?A, researcher at B?, ?A, B professor? and ?A?s
program B?. It is easy to identify ?A?s program B?
as an intruder when the annotators realize that the
other four paths state the relation that people work
in an educational institution. The generative model
approach produces more coherent clusters when the
number of relation topics increases.
The system which employs local and entity type
features (Local+Type) produces clusters with low
coherence because the system puts high weight on
types. For example, (United States, A talk with B,
Syria) and (Canada, A defeat B, United States) are
clustered into one relation since they share the argu-
ment types ?country?-?country?. Our approach us-
ing the global theme features can correct such errors.
4.3 Error Analysis
We also closely analyze the pairwise errors that we
encounter when comparing against Freebase labels.
Some errors arise because one instance can have
multiple labels, as we explained in Section 4.1. One
example is the following: Our approach predicts that
(News Corporation, buy, MySpace) and (Dow Jones
& Company, the parent of, The Wall Street Journal)
are in one relation. In Freebase, one is labeled as
?/organization/parent/child?, the other is labeled as
?/book/newspaper owner/newspapers owned?. The
latter is a sub-relation of the former. We can over-
come this issue by introducing hierarchies in relation
labels.
Some errors are caused by selecting the incorrect
sense for an entity pair of a path. For instance, we
put (Kenny Smith, who grew up in, Queens) and
(Phil Jackson, return to, Los Angeles Lakers) into
718
the ?/people/person/place of birth? relation cluster
since we do not detect the ?sports? sense for the en-
tity pair ?(Phil Jackson, Los Angeles Lakers)?.
5 Related Work
There has been considerable interest in unsupervised
relation discovery, including clustering approach,
generative models and many other approaches.
Our work is closely related to DIRT (Lin and Pan-
tel, 2001). Both DIRT and our approach represent
dependency paths using their arguments. Both use
distributional similarity to find patterns representing
similar semantic relations. Based on DIRT, Pantel
et al (2007) addresses the issue of multiple senses
per path by automatically learning admissible argu-
ment types where two paths are similar. They cluster
arguments to fine-grained entity types and rank the
associations of a relation with these entity types to
discover selectional preferences. Selectional prefer-
ences discovery (Ritter et al, 2010; Seaghdha, 2010)
can help path sense disambiguation, however, we
show that using global features performs better than
entity type features.
Our approach is also related to feature parti-
tioning in cross-cutting model of lexical seman-
tics (Reisinger and Mooney, 2011). And our sense
disambiguation model is inspired by this work.
There they partition features of words into views and
cluster words inside each view. In our case, each
sense of a path can be seen as one view. However,
we allow different views to be merged since some
views overlap with each other.
Hasegawa et al (2004) cluster pairs of named en-
tities according to the similarity of context words in-
tervening between them. Hachey (2009) uses topic
models to perform dimensionality reduction on fea-
tures when clustering entity pairs into relations. Bol-
legala et al (2010) employ co-clustering to find clus-
ters of entity pairs and patterns jointly. All the ap-
proaches above neither deal with polysemy nor in-
corporate global features, such as sentence and doc-
ument themes.
Open information extraction aims to discover re-
lations independent of specific domains (Banko et
al., 2007; Banko and Etzioni, 2008). They employ
a self-learner to extract relation instances, but no
attempt is made to cluster instances into relations.
Yates and Etzioni (2009) present RESOLVER for
discovering relational synonyms as a post process-
ing step. Our approach falls into the same category.
Moreover, we explore path senses and global fea-
tures for relation discovery.
Many generative probabilistic models have been
applied to relation extraction. For example, vari-
eties of topic models are employed for both open
domain (Yao et al, 2011) and in-domain relation
discovery (Chen et al, 2011; Rink and Harabagiu,
2011). Our approach employs generative models
for path sense disambiguation, which achieves better
performance than directly applying generative mod-
els to unsupervised relation discovery.
6 Conclusion
We explore senses of paths to discover semantic re-
lations. We employ a topic model to partition en-
tity pairs of a path into different sense clusters and
use hierarchical agglomerative clustering to merge
senses into semantic relations. Experimental results
show our approach discovers precise relation clus-
ters and outperforms a generative model approach
and a clustering method which does not address
sense disambiguation. We also show that using
global features improves the performance of unsu-
pervised relation discovery over using entity type
based features.
Acknowledgments
This work was supported in part by the Center
for Intelligent Information Retrieval and the Uni-
versity of Massachusetts gratefully acknowledges
the support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0181. Any opinions, find-
ings, and conclusion or recommendations expressed
in this material are those of the authors and do not
necessarily reflect the view of DARPA, AFRL, or
the US government.
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In The First International
Conference on Language Resources and Evaluation
Workshop on Linguistics Coreference.
719
Pierre Baldi, S?ren Brunak, Yves Chauvin, Claus A. F.
Andersen, and Henrik Nielsen. 2000. Assessing the
accuracy of prediction algorithms for classification: an
overview. Bioinformatics, 16:412?424.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT.
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI2007.
Christian Bizer, Jens Lehmann, Georgi Kobilarov, So?ren
Auer, Christian Becker, Richard Cyganiak, and Se-
bastian Hellmann. 2009. DBpedia - a crystallization
point for the web of data. Journal of Web Semantics:
Science, Services and Agents on the World Wide Web,
pages 154?165.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet Allocation. Journal of Machine Learn-
ing Research, 3:993?1022, January.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collabo-
ratively created graph database for structuring human
knowledge. In SIGMOD ?08: Proceedings of the 2008
ACM SIGMOD international conference on Manage-
ment of data, pages 1247?1250, New York, NY, USA.
ACM.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2010. Relational duality: Unsupervised ex-
traction of semantic relations between entities on the
web. In Proceedings of WWW.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang,
Sean Gerrish, and David Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Pro-
ceedings of NIPS.
Harr Chen, Edward Benson, Tahira Naseem, and Regina
Barzilay. 2011. In-domain relation discovery with
meta-constraints via posterior regularization. In Pro-
ceedings of ACL.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL
?05), pages 363?370, June.
Benjamin Hachey. 2009. Towards Generic Relation Ex-
traction. Ph.D. thesis, University of Edinburgh.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In ACL.
Dekang Lin and Patrick Pantel. 2001. DIRT - Discovery
of Inference Rules from Text. In Proceedings of KDD.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of CoNLL, pages
49?56.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning Inferential Selectional Preferences. In Pro-
ceedings of NAACL HLT.
Lev Ratinov, Dan Roth, Doug Downey, and Mike Ander-
son. 2011. Local and global algorithms for disam-
biguation to Wikipedia. In Proceedings of ACL.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Joseph Reisinger and Raymond J. Mooney. 2011. Cross-
cutting models of lexical semantics. In Proceedings of
EMNLP.
Bryan Rink and Sanda Harabagiu. 2011. A generative
model for unsupervised discovery of relations and ar-
gument classes from clinical texts. In Proceedings of
EMNLP.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A La-
tent Dirichlet Allocation method for Selectional Pref-
erences. In Proceedings of ACL10.
Evan Sandhaus, 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia.
Diarmuid O Seaghdha. 2010. Latent variable models of
selectional preference. In Proceedings of ACL 10.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP.
Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew
McCallum. 2011. Structured relation discovery using
generative models. In Proceedings of EMNLP.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
720
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1052?1062,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Transition-based Dependency Parsing with Selectional Branching
Jinho D. Choi
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA, 01003, USA
jdchoi@cs.umass.edu
Andrew McCallum
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA, 01003, USA
mccallum@cs.umass.edu
Abstract
We present a novel approach, called selec-
tional branching, which uses confidence es-
timates to decide when to employ a beam,
providing the accuracy of beam search at
speeds close to a greedy transition-based
dependency parsing approach. Selectional
branching is guaranteed to perform a fewer
number of transitions than beam search yet
performs as accurately. We also present a
new transition-based dependency parsing
algorithm that gives a complexity of O(n)
for projective parsing and an expected lin-
ear time speed for non-projective parsing.
With the standard setup, our parser shows
an unlabeled attachment score of 92.96%
and a parsing speed of 9 milliseconds per
sentence, which is faster and more accurate
than the current state-of-the-art transition-
based parser that uses beam search.
1 Introduction
Transition-based dependency parsing has gained
considerable interest because it runs fast and per-
forms accurately. Transition-based parsing gives
complexities as low as O(n) and O(n2) for projec-
tive and non-projective parsing, respectively (Nivre,
2008).1 The complexity is lower for projective pars-
ing because a parser can deterministically skip to-
kens violating projectivity, while this property is
not assumed for non-projective parsing. Nonethe-
less, it is possible to perform non-projective parsing
in expected linear time because the amount of non-
projective dependencies is notably smaller (Nivre
and Nilsson, 2005) so a parser can assume projec-
tivity for most cases while recognizing ones for
which projectivity should not be assumed (Nivre,
2009; Choi and Palmer, 2011).
1We refer parsing approaches that produce only projective
dependency trees as projective parsing and both projective and
non-projective dependency trees as non-projective parsing.
Greedy transition-based dependency parsing has
been widely deployed because of its speed (Cer et
al., 2010); however, state-of-the-art accuracies have
been achieved by globally optimized parsers using
beam search (Zhang and Clark, 2008; Huang and
Sagae, 2010; Zhang and Nivre, 2011; Bohnet and
Nivre, 2012). These approaches generate multiple
transition sequences given a sentence, and pick one
with the highest confidence. Coupled with dynamic
programming, transition-based dependency parsing
with beam search can be done very efficiently and
gives significant improvement to parsing accuracy.
One downside of beam search is that it always
uses a fixed size of beam even when a smaller size
of beam is sufficient for good results. In our exper-
iments, a greedy parser performs as accurately as a
parser that uses beam search for about 64% of time.
Thus, it is preferred if the beam size is not fixed but
proportional to the number of low confidence pre-
dictions that a greedy parser makes, in which case,
fewer transition sequences need to be explored to
produce the same or similar parse output.
We first present a new transition-based parsing
algorithm that gives a complexity of O(n) for pro-
jective parsing and an expected linear time speed
for non-projective parsing. We then introduce se-
lectional branching that uses confidence estimates
to decide when to employ a beam. With our new ap-
proach, we achieve a higher parsing accuracy than
the current state-of-the-art transition-based parser
that uses beam search and a much faster speed.
2 Transition-based dependency parsing
We introduce a transition-based dependency pars-
ing algorithm that is a hybrid between Nivre?s arc-
eager and list-based algorithms (Nivre, 2003; Nivre,
2008). Nivre?s arc-eager is a projective parsing al-
gorithm showing a complexity of O(n). Nivre?s
list-based algorithm is a non-projective parsing al-
gorithm showing a complexity of O(n2). Table 1
shows transitions in our algorithm. The top 4 and
1052
Transition Current state ? Resulting state
LEFTl-REDUCE ( [?|i], ?, [j|?], A ) ? ( ?, ?, [j|?], A ? {i l? j} )
RIGHTl-SHIFT ( [?|i], ?, [j|?], A ) ? ( [?|i|?|j], [ ], ?, A ? {i l? j} )
NO-SHIFT ( [?|i], ?, [j|?], A ) ? ( [?|i|?|j], [ ], ?, A )
NO-REDUCE ( [?|i], ?, [j|?], A ) ? ( ?, ?, [j|?], A )
LEFTl-PASS ( [?|i], ?, [j|?], A ) ? ( ?, [i|?], [j|?], A ? {i l? j} )
RIGHTl-PASS ( [?|i], ?, [j|?], A ) ? ( ?, [i|?], [j|?], A ? {i l? j} )
NO-PASS ( [?|i], ?, [j|?], A ) ? ( ?, [i|?], [j|?], A )
Table 1: Transitions in our dependency parsing algorithm.
Transition Preconditions
LEFTl-? [i 6= 0] ? ?[?k. (i? k) ? A] ? ?[(i?? j) ? A]
RIGHTl-? ?[?k. (k ? j) ? A] ? ?[(i ?? j) ? A]
?-SHIFT ?[?k ? ?. (k 6= i) ? ((k ? j) ? (k ? j))]
?-REDUCE [?h. (h? i) ? A] ? ?[?k ? ?. (i? k)]
Table 2: Preconditions of the transitions in Table 1 (? is a wildcard representing any transition).
the bottom 3 transitions are inherited from Nivre?s
arc-eager and list-based algorithms, respectively.2
Each parsing state is represented as a tuple (?,
?, ?, A), where ? is a stack containing processed
tokens, ? is a deque containing tokens popped out
of ? but will be pushed back into ? in later parsing
states to handle non-projectivity, and ? is a buffer
containing unprocessed tokens. A is a set of labeled
arcs. (i, j) represent indices of their corresponding
tokens (wi, wj), l is a dependency label, and the 0
identifier corresponds to w0, introduced as the root
of a tree. The initial state is ([0], [ ], [1, . . . , n], ?),
and the final state is (?, ?, [ ], A). At any parsing
state, a decision is made by comparing the top of
?, wi, and the first element of ?, wj . This decision
is consulted by gold-standard trees during training
and a classifier during decoding.
LEFTl-? and RIGHTl-? are performed when wj
is the head ofwi with a dependency label l, and vice
versa. After LEFTl-? or RIGHTl-?, an arc is added
to A. NO-? is performed when no dependency is
found for wi and wj . ?-SHIFT is performed when
no dependency is found for wj and any token in
? other than wi. After ?-SHIFT, all tokens in ?
as well as wj are pushed into ?. ?-REDUCE is
performed when wi already has the head, and wi is
not the head of any token in ?. After ?-REDUCE,
wi is popped out of ?. ?-PASS is performed when
neither ?-SHIFT nor ?-REDUCE can be performed.
After ?-PASS, wi is moved to the front of ? so it
2The parsing complexity of a transition-based dependency
parsing algorithm is determined by the number of transitions
performed with respect to the number of tokens in a sentence,
say n (K?bler et al, 2009).
can be compared to other tokens in ? later. Each
transition needs to satisfy certain preconditions to
ensure the properties of a well-formed dependency
graph (Nivre, 2008); they are described in Table 2.
(i? j) and (i ?? j) indicate that wj is the head
and an ancestor of wi with any label, respectively.
When a parser is trained on only projective trees,
our algorithm learns only the top 4 transitions and
produces only projective trees during decoding. In
this case, it performs at most 2n ? 1 transitions
per sentence so the complexity is O(n). When a
parser is trained on a mixture of projective and non-
projective trees, our algorithm learns all transitions
and produces both kinds of trees during decoding.
In this case, it performs at most n(n+1)2 transitionsso the complexity is O(n2). However, because of
the presence of ?-SHIFT and ?-REDUCE, our al-
gorithm is capable of skipping or removing tokens
during non-projective parsing, which allows it to
show a linear time parsing speed in practice.
700 10 20 30 40 50 60
130
0
20
40
60
80
100
Sentence length
Tra
nsit
ions
Figure 1: The # of transitions performed during
training with respect to sentence lengths for Dutch.
1053
Transition ? ? ? A
0 Initialization [0] [ ] [1|?] ?
1 NO-SHIFT [?|1] [ ] [2|?]
2 NO-SHIFT [?|2] [ ] [3|?]
3 NO-SHIFT [?|3] [ ] [4|?]
4 LEFT-REDUCE [?|2] [ ] [4|?] A ? {3?NSUBJ? 4}
5 NO-PASS [?|1] [2] [4|?]
6 RIGHT-SHIFT [?|4] [ ] [5|?] A ? {1 ?RCMOD? 4}
7 NO-SHIFT [?|5] [ ] [6|?]
8 LEFT-REDUCE [?|4] [ ] [6|?] A ? {5?AUX? 6}
9 RIGHT-PASS [?|2] [4] [6|?] A ? {4 ?XCOMP? 6}
10 LEFT-REDUCE [?|1] [4] [6|?] A ? {2?DOBJ? 6}
11 NO-SHIFT [?|6] [ ] [7|?]
12 NO-REDUCE [?|4] [ ] [7|?]
13 NO-REDUCE [?|1] [ ] [7|?]
14 LEFT-REDUCE [0] [ ] [7|?] A ? {1?NSUBJ? 7}
15 RIGHT-SHIFT [?|7] [ ] [8] A ? {0 ?ROOT? 7}
16 RIGHT-SHIFT [?|8] [ ] [ ] A ? {7 ?ADV? 8}
Table 3: A transition sequence generated by our parsing algorithm using gold-standard decisions.
Figure 1 shows the total number of transitions per-
formed during training with respect to sentence
lengths for Dutch. Among all languages distributed
by the CoNLL-X shared task (Buchholz and Marsi,
2006), Dutch consists of the highest number of
non-projective dependencies (5.4% in arcs, 36.4%
in trees). Even with such a high number of non-
projective dependencies, our parsing algorithm still
shows a linear growth in transitions.
Table 3 shows a transition sequence generated
by our parsing algorithm using gold-standard deci-
sions. Afterw3 andw4 are compared, w3 is popped
out of ? (state 4) so it is not compared to any other
token in ? (states 9 and 13). After w2 and w4 are
compared, w2 is moved to ? (state 5) so it can be
compared to other tokens in ? (state 10). After w4
and w6 are compared, RIGHT-PASS is performed
(state 9) because there is a dependency between
w6 and w2 in ? (state 10). After w6 and w7 are
compared, w6 is popped out of ? (state 12) because
it is not needed for later parsing states.
3 Selectional branching
3.1 Motivation
For transition-based parsing, state-of-the-art accu-
racies have been achieved by parsers optimized on
multiple transition sequences using beam search,
which can be done very efficiently when it is cou-
pled with dynamic programming (Zhang and Clark,
2008; Huang and Sagae, 2010; Zhang and Nivre,
2011; Huang et al, 2012; Bohnet and Nivre, 2012).
Despite all the benefits, there is one downside of
this approach; it generates a fixed number of tran-
sition sequences no matter how confident the one-
best sequence is.3 If every prediction leading to
the one-best sequence is confident, it may not be
necessary to explore more sequences to get the best
output. Thus, it is preferred if the beam size is not
fixed but proportional to the number of low confi-
dence predictions made for the one-best sequence.
The selectional branching method presented here
performs at most d ? t? e transitions, where t is the
maximum number of transitions performed to gen-
erate a transition sequence, d = min(b, |?|+1), b is
the beam size, |?| is the number of low confidence
predictions made for the one-best sequence, and
e = d(d?1)2 . Compared to beam search that alwaysperforms b ? t transitions, selectional branching is
guaranteed to perform fewer transitions given the
same beam size because d ? b and e > 0 except for
d = 1, in which case, no branching happens. With
selectional branching, our parser shows slightly
3The ?one-best sequence? is a transition sequence gener-
ated by taking only the best prediction at each parsing state.
1054
higher parsing accuracy than the current state-of-
the-art transition-based parser using beam search,
and performs about 3 times faster.
3.2 Branching strategy
Figure 2 shows an overview of our branching strat-
egy. sij represents a parsing state, where i is the
index of the current transition sequence and j is
the index of the current parsing state (e.g., s12 rep-
resents the 2nd parsing state in the 1st transition
sequence). pkj represents the k?th best prediction
(in our case, it is a predicted transition) given s1j
(e.g., p21 is the 2nd-best prediction given s11).
s
11
s
12
p
11
s
22
? ? s
1t
p
12
? ? s
2t
p
21
s
33
p
22
? s
3t
s
dt
?
? ?
p
2j
T
1
 =
T
2
 =
T
3
 =
T
d
 =
p
1j
Figure 2: An overview of our branching strategy.
Each sequence Ti>1 branches from T1.
Initially, the one-best sequence T1 = [s11, ... , s1t]
is generated by a greedy parser. While generating
T1, the parser adds tuples (s1j , p2j), ... , (s1j , pkj)
to a list ? for each low confidence prediction p1j
given s1j .4 Then, new transition sequences are gen-
erated by using the b highest scoring predictions in
?, where b is the beam size. If |?| < b, all predic-
tions in ? are used. The same greedy parser is used
to generate these new sequences although it now
starts with s1j instead of an initial parsing state,
applies pkj to s1j , and performs further transitions.
Once all transition sequences are generated, a parse
tree is built from a sequence with the highest score.
For our experiments, we set k = 2, which gave
noticeably more accurate results than k = 1. We
also experimented with k > 2, which did not show
significant improvement over k = 2. Note that as-
signing a greater k may increase |?| but not the total
number of transition sequences generated, which
is restricted by the beam size, b. Since each se-
quence Ti>1 branches from T1, selectional branch-
ing performs fewer transitions than beam search:
at least d(d?1)2 transitions are inherited from T1,
4? is initially empty, which is hidden in Figure 2.
where d = min(b, |?| + 1); thus, it performs that
many transitions less than beam search (see the
left lower triangle in Figure 2). Furthermore, se-
lectional branching generates a d number of se-
quences, where d is proportional to the number of
low confidence predictions made by T1. To sum up,
selectional branching generates the same or fewer
transition sequences than beam search and each
sequence Ti>1 performs fewer transitions than T1;
thus, it performs faster than beam search in general
given the same beam size.
3.3 Finding low confidence predictions
For each parsing state sij , a prediction is made by
generating a feature vector xij ? X , feeding it into
a classifier C1 that uses a feature map ?(x, y) and
a weight vector w to measure a score for each label
y ? Y , and choosing a label with the highest score.
When there is a tie between labels with the highest
score, the first one is chosen. This can be expressed
as a logistic regression:
C1(x) = arg max
y?Y
{f(x, y)}
f(x, y) = exp(w ? ?(x, y))?
y??Y exp(w ? ?(x, y?))
To find low confidence predictions, we use the mar-
gins (score differences) between the best prediction
and the other predictions. If all margins are greater
than a threshold, the best prediction is considered
highly confident; otherwise, it is not. Given this
analogy, the k-best predictions can be found as
follows (m ? 0 is a margin threshold):
Ck(x,m) = K arg max
y?Y
{f(x, y)}
s.t. f(x,C1(x))? f(x, y) ? m
?K arg max? returns a set of k? labels whose mar-
gins to C1(x) are smaller than any other label?s
margin to C1(x) and also ? m, where k? ? k.
When m = 0, it returns a set of the highest scoring
labels only, including C1(x). When m = 1, it re-
turns a set of all labels. Given this, a prediction is
considered not confident if |Ck(x,m)| > 1.
3.4 Finding the best transition sequence
Let Pi be a list of all predictions that lead to gen-
erate a transition sequence Ti. The predictions in
Pi are either inherited from T1 or made specifi-
cally for Ti. In Figure 2, P3 consists of p11 as its
first prediction, p22 as its second prediction, and
1055
further predictions made specifically for T3. The
score of each prediction is measured by f(x, y) in
Section 3.3. Then, the score of Ti is measured by
averaging scores of all predictions in Pi.
score(Ti) =
?
p?Pi score(p)
|Pi|
Unlike Zhang and Clark (2008), we take the av-
erage instead of the sum of all prediction scores.
This is because our algorithm does not guarantee
the same number of transitions for every sequence,
so the sum of all scores would weigh more on se-
quences with more transitions. We experimented
with both the sum and the average, and taking the
average led to slightly higher parsing accuracy.
3.5 Bootstrapping transition sequences
During training, a training instance is generated
for each parsing state sij by taking a feature vec-
tor xij and its true label yij . To generate multiple
transition sequences during training, the bootstrap-
ping technique of Choi and Palmer (2011) is used,
which is described in Algorithm 1.5
Algorithm 1 Bootstrapping
Input: Dt: training set, Dd: development set.
Output: A model M .
1: r ? 0
2: I ? getTrainingInstances(Dt)
3: M0 ? buildModel(I)
4: S0 ? getScore(Dd,M0)
5: while (r = 0) or (Sr?1 < Sr) do
6: r ? r + 1
7: I ? getTrainingInstances(Dt,Mr?1)
8: Mr ? buildModel(I)
9: Sr ? getScore(Dd,Mr)
10: return Mr?1
First, an initial model M0 is trained on all data by
taking the one-best sequences, and its score is mea-
sured by testing on a development set (lines 2-4).
Then, the next model Mr is trained on all data but
this time, Mr?1 is used to generate multiple tran-
sition sequences (line 7-8). Among all transition
sequences generated by Mr?1, training instances
from only T1 and Tg are used to trainMr, where T1
is the one-best sequence and Tg is a sequence giv-
ing the most accurate parse output compared to the
gold-standard tree. The score of Mr is measured
(line 9), and repeat the procedure if Sr?1 < Sr;
otherwise, return the previous model Mr?1.
5Alternatively, the dynamic oracle approach of Goldberg
and Nivre (2012) can be used to generate multiple transition
sequences, which is expected to show similar results.
3.6 Adaptive subgradient algorithm
To build each model during bootstrapping, we use
a stochastic adaptive subgradient algorithm called
ADAGRAD that uses per-coordinate learning rates
to exploit rarely seen features while remaining scal-
able (Duchi et al, 2011).This is suitable for NLP
tasks where rarely seen features often play an im-
portant role and training data consists of a large
number of instances with high dimensional features.
Algorithm 2 shows our adaptation of ADAGRAD
with logistic regression for multi-class classifica-
tion. Note that when used with logistic regression,
ADAGRAD takes a regular gradient instead of a sub-
gradient method for updating weights. For our ex-
periments, ADAGRAD slightly outperformed learn-
ing algorithms such as average perceptron (Collins,
2002) or Liblinear SVM (Hsieh et al, 2008).
Algorithm 2 ADAGRAD + logistic regression
Input: D = {(xi, yi)}ni=1 s.t. xi ? X , yi ? Y
?(x, y) ? Rd s.t. d = dimension(X )? |Y|
T : iterations, ?: learning rate, ?: ridge
Output: A weight vector w ? Rd.
1: w? 0, where w ? Rd
2: G? 0, where G ? Rd
3: for t? 1 . . . T do
4: for i? 1 . . . n do
5: Q?y?Y ? I(yi, y)? f(xi, y), s.t. Q ? R|Y|
6: ? ??y?Y(?(xi, y) ?Qy)
7: G? G + ? ? ?
8: for j ? 1 . . . d do
9: wj ? wj + ? ? 1?+?Gj ? ?j
I(y, y?) =
{
1 y = y?
0 otherwise
The algorithm takes three hyper-parameters; T is
the number of iterations, ? is the learning rate, and
? is the ridge (T > 0, ? > 0, ? ? 0). G is our run-
ning estimate of a diagonal covariance matrix for
the gradients (per-coordinate learning rates). For
each instance, scores for all labels are measured
by the logistic regression function f(x, y) in Sec-
tion 3.3. These scores are subtracted from an output
of the indicator function I(y, y?), which forces our
model to keep learning this instance until the pre-
diction is 100% confident (in other words, until
the score of yi becomes 1). Then, a subgradient
is measured by taking all feature vectors together
weighted by Q (line 6). This subgradient is used to
update G and w, where ? is the Hadamard product
(lines 7-9). ? is a ridge term to keep the inverse
covariance well-conditioned.
1056
4 Experiments
4.1 Corpora
For projective parsing experiments, the Penn En-
glish Treebank (Marcus et al, 1993) is used with
the standard split: sections 2-21 for training, 22 for
development, and 23 for evaluation. All constituent
trees are converted with the head-finding rules of
Yamada and Matsumoto (2003) and the labeling
rules of Nivre (2006). For non-projective pars-
ing experiments, four languages from the CoNLL-
X shared task are used: Danish, Dutch, Slovene,
and Swedish (Buchholz and Marsi, 2006). These
languages are selected because they contain non-
projective trees and are publicly available from the
CoNLL-X webpage.6 Since the CoNLL-X data we
have does not come with development sets, the last
10% of each training set is used for development.
4.2 Feature engineering
For English, we mostly adapt features from Zhang
and Nivre (2011) who have shown state-of-the-art
parsing accuracy for transition-based dependency
parsing. Their distance features are not included
in our approach because they do not seem to show
meaningful improvement. Feature selection is done
on the English development set.
For the other languages, the same features are
used with the addition of morphological features
provided by CoNLL-X; specifically, morphological
features from the top of ? and the front of ? are
added as unigram features. Moreover, all POS tag
features from English are duplicated with coarse-
grained POS tags provided by CoNLL-X. No more
feature engineering is done for these languages; it
is possible to achieve higher performance by using
different features, especially when these languages
contain non-projective dependencies whereas En-
glish does not, which we will explore in the future.
4.3 Development
Several parameters need to be optimized during de-
velopment. For ADAGRAD, T , ?, and ? need to be
tuned (Section 3.6). For bootstrapping, the number
of iterations, say r, needs to be tuned (Section 3.5).
For selectional branching, the margin threshold m
and the beam size b need to be tuned (Section 3.3).
First, all parameters are tuned on the English devel-
opment set by using grid search on T = [1, . . . , 10],
? = [0, 01, 0, 02], ? = [0.1, 0.2], r = [1, 2, 3],
6http://ilk.uvt.nl/conll/
m = [0.83, . . . , 0.92], and b = [16, 32, 64, 80].
As a result, the following parameters are found:
? = 0.02, ? = 0.1, m = 0.88, and b = 64|80. For
this development set, the beam size of 64 and 80
gave the exact same result, so we kept the one with
a larger beam size (b = 80).
0.920.83 0.86 0.88 0.9
91.2
91
91.04
91.08
91.12
91.16
Margin
Acc
urac
y
64|803216
b =
Figure 3: Parsing accuracies with respect to mar-
gins and beam sizes on the English development set.
b = 64|80: the black solid line with solid circles,
b = 32: the blue dotted line with hollow circles,
b = 16: the red dotted line with solid circles.
Figure 3 shows parsing accuracies with respect to
different margins and beam sizes on the English de-
velopment set. These parameters need to be tuned
jointly because different margins prefer different
beam sizes. For instance, m = 0.85 gives the high-
est accuracy with b = 32, but m = 0.88 gives the
highest accuracy with b = 64|80.
140 2 4 6 8 10 12
92
88.5
89
89.5
90
90.5
91
91.5
Iteration
Acc
urac
y
UAS
LAS
Figure 4: Parsing accuracies with respect to ADA-
GRAD and bootstrap iterations on the English de-
velopment set when ? = 0.02, ? = 0.1, m = 0.88,
and b = 64|80. UAS: unlabeled attachment score,
LAS: labeled attachment score.
Figure 4 shows parsing accuracies with respect to
ADAGRAD and bootstrap iterations on the English
development set. The range 1-5 shows results of
5 ADAGRAD iterations before bootstrapping, the
range 6-9 shows results of 4 iterations during the
1057
first bootstrapping, and the range 10-14 shows re-
sults of 5 iterations during the second bootstrap-
ping. Thus, the number of bootstrap iteration is
2 where each bootstrapping takes a different num-
ber of ADAGRAD iterations. Using an Intel Xeon
2.57GHz machine, it takes less than 40 minutes
to train the entire Penn Treebank, which includes
times for IO, feature extraction and bootstrapping.
800 10 20 30 40 50 60 70
1,200,000
0
200,000
400,000
600,000
800,000
1,000,000
Beam size = 1, 2, 4, 8, 16, 32, 64, 80
Tra
nsit
ions
Figure 5: The total number of transitions performed
during decoding with respect to beam sizes on the
English development set.
Figure 5 shows the total number of transitions per-
formed during decoding with respect to beam sizes
on the English development set (1,700 sentences,
40,117 tokens). With selectional branching, the
number of transitions grows logarithmically as the
beam size increases whereas it would have grown
linearly if beam search were used. We also checked
how often the one best sequence is chosen as the
final sequence during decoding. Out of 1,700 sen-
tences, the one best sequences are chosen for 1,095
sentences. This implies that about 64% of time,
our greedy parser performs as accurately as our
non-greedy parser using selectional branching.
For the other languages, we use the same values
as English for ?, ?, m, and b; only the ADAGRAD
and bootstrap iterations are tuned on the develop-
ment sets of the other languages.
4.4 Projective parsing experiments
Before parsing, POS tags were assigned to the train-
ing set by using 20-way jackknifing. For the auto-
matic generation of POS tags, we used the domain-
specific model of Choi and Palmer (2012a)?s tagger,
which gave 97.5% accuracy on the English evalua-
tion set (0.2% higher than Collins (2002)?s tagger).
Table 4 shows comparison between past and cur-
rent state-of-the-art parsers and our approach. The
first block shows results from transition-based de-
pendency parsers using beam search. The second
block shows results from other kinds of parsing
approaches (e.g., graph-based parsing, ensemble
parsing, linear programming, dual decomposition).
The third block shows results from parsers using
external data. The last block shows results from
our approach. The Time column show how many
seconds per sentence each parser takes.7
Approach UAS LAS Time
Zhang and Clark (2008) 92.1
Huang and Sagae (2010) 92.1 0.04
Zhang and Nivre (2011) 92.9 91.8 0.03
Bohnet and Nivre (2012) 93.38 92.44 0.4
McDonald et al (2005) 90.9
Mcdonald and Pereira (2006) 91.5
Sagae and Lavie (2006) 92.7
Koo and Collins (2010) 93.04
Zhang and McDonald (2012) 93.06 91.86
Martins et al (2010) 93.26
Rush et al (2010) 93.8
Koo et al (2008) 93.16
Carreras et al (2008) 93.54
Bohnet and Nivre (2012) 93.67 92.68
Suzuki et al (2009) 93.79
bt = 80, bd = 80, m = 0.88 92.96 91.93 0.009
bt = 80, bd = 64, m = 0.88 92.96 91.93 0.009
bt = 80, bd = 32, m = 0.88 92.96 91.94 0.009
bt = 80, bd = 16, m = 0.88 92.96 91.94 0.008
bt = 80, bd = 8, m = 0.88 92.89 91.87 0.006
bt = 80, bd = 4, m = 0.88 92.76 91.76 0.004
bt = 80, bd = 2, m = 0.88 92.56 91.54 0.003
bt = 80, bd = 1, m = 0.88 92.26 91.25 0.002
bt = 1, bd = 1, m = 0.88 92.06 91.05 0.002
Table 4: Parsing accuracies and speeds on the En-
glish evaluation set, excluding tokens containing
only punctuation. bt and bd indicate the beam sizes
used during training and decoding, respectively.
UAS: unlabeled attachment score, LAS: labeled
attachment score, Time: seconds per sentence.
For evaluation, we use the model trained with b =
80 and m = 0.88, which is the best setting found
during development. Our parser shows higher ac-
curacy than Zhang and Nivre (2011), which is
the current state-of-the-art transition-based parser
that uses beam search. Bohnet and Nivre (2012)?s
transition-based system jointly performs POS tag-
ging and dependency parsing, which shows higher
accuracy than ours. Our parser gives a comparative
accuracy to Koo and Collins (2010) that is a 3rd-
order graph-based parsing approach. In terms of
speed, our parser outperforms all other transition-
based parsers; it takes about 9 milliseconds per
7Dhillon et al (2012) and Rush and Petrov (2012) also
have shown good results on this data but they are excluded
from our comparison because they use different kinds of
constituent-to-dependency conversion methods.
1058
Approach Danish Dutch Slovene SwedishLAS UAS LAS UAS LAS UAS LAS UAS
Nivre et al (2006) 84.77 89.80 78.59 81.35 70.30 78.72 84.58 89.50
McDonald et al (2006) 84.79 90.58 79.19 83.57 73.44 83.17 82.55 88.93
Nivre (2009) 84.2 - - - 75.2 - - -
F.-Gonz?lez and G.-Rodr?guez (2012) 85.17 90.10 - - - - 83.55 89.30
Nivre and McDonald (2008) 86.67 - 81.63 - 75.94 84.66
Martins et al (2010) - 91.50 - 84.91 - 85.53 - 89.80
bt = 80, bd = 1, m = 0.88 86.75 91.04 80.75 83.59 75.66 83.29 86.32 91.12
bt = 80, bd = 80, m = 0.88 87.27 91.36 82.45 85.33 77.46 84.65 86.80 91.36
Table 5: Parsing accuracies on four languages with non-projective dependencies, excluding punctuation.
sentence using the beam size of 80. Our parser is
implemented in Java and tested on an Intel Xeon
2.57GHz. Note that we do not include input/output
time for our speed comparison.
For a proof of concept, we run the same model,
trained with bt = 80, but decode with different
beam sizes using the same margin. Surprisingly,
our parser gives the same accuracy (0.01% higher
for labeled attachment score) on this data even with
bd = 16. More importantly, bd = 16 shows about
the same parsing speed as bd = 80, which indicates
that selectional branching automatically reduced
down the beam size by estimating low confidence
predictions, so even if we assigned a larger beam
size for decoding, it would have performed as effi-
ciently. This implies that we no longer need to be
so conscious about the beam size during decoding.
Another interesting part is that (bt = 80, bd = 1)
shows higher accuracy than (bt = 1, bd = 1); this
implies that our training method of bootstrapping
transition sequences can improve even a greedy
parser. Notice that our greedy parser shows higher
accuracy than many other greedy parsers (Hall et
al., 2006; Goldberg and Elhadad, 2010) because
it uses the non-local features of Zhang and Nivre
(2011) and the bootstrapping technique of Choi
and Palmer (2011) that had not been used for most
other greedy parsing approaches.
4.5 Non-projective parsing experiments
Table 5 shows comparison between state-of-the-art
parsers and our approach for four languages with
non-projective dependencies. Nivre et al (2006)
uses a pseudo-projective transition-based parsing
approach. McDonald et al (2006) uses a 2nd-order
maximum spanning tree approach. Nivre (2009)
and Fern?ndez-Gonz?lez and G?mez-Rodr?guez
(2012) use different non-projective transition-based
parsing approaches. Nivre and McDonald (2008)
uses an ensemble model between transition-based
and graph-based parsing approaches. Martins et
al. (2010) uses integer linear programming for the
optimization of their parsing model.
Some of these approaches use greedy parsers, so
we include our results from models using (bt = 80,
bd = 1, m = 0.88), which finds only the one-best
sequences during decoding although it is trained on
multiple transition sequences (see Section 4.4). Our
parser shows higher accuracies for most languages
except for unlabeled attachment scores in Danish
and Slovene. Our greedy approach outperforms
both Nivre (2009) and Fern?ndez-Gonz?lez and
G?mez-Rodr?guez (2012) who use different non-
projective parsing algorithms.
600 10 20 30 40 50
130
0
20
40
60
80
100
Sentence length
Tra
nsit
ions
Figure 6: The # of transitions performed during de-
coding with respect to sentence lengths for Dutch.
Figure 6 shows the number of transitions performed
during decoding with respect to sentence lengths
for Dutch using bd = 1. Our parser still shows a
linear growth in transition during decoding.
5 Related work
Our parsing algorithm is most similar to Choi and
Palmer (2011) who integrated our LEFT-REDUCE
transition into Nivre?s list-based algorithm. Our
algorithm is distinguished from theirs because ours
gives different parsing complexities of O(n) and
O(n2) for projective and non-projective parsing,
respectively, whereas their algorithm gives O(n2)
1059
for both cases; this is possible because of the new
integration of the RIGHT-SHIFT and NO-REDUCE
transitions. There are other transition-based de-
pendency parsing algorithms that take a similar ap-
proach; Nivre (2009) integrated a SWAP transition
into Nivre?s arc-standard algorithm (Nivre, 2004)
and Fern?ndez-Gonz?lez and G?mez-Rodr?guez
(2012) integrated a buffer transition into Nivre?s
arc-eager algorithm to handle non-projectivity.
Our selectional branching method is most rele-
vant to Zhang and Clark (2008) who introduced
a transition-based dependency parsing model that
uses beam search. Huang and Sagae (2010) later
applied dynamic programming to this approach
and showed improved efficiency. Zhang and Nivre
(2011) added non-local features to this approach
and showed improved parsing accuracy. Bohnet
and Nivre (2012) introduced a transition-based sys-
tem that jointly performed POS tagging and de-
pendency parsing. Our work is distinguished from
theirs because we use selectional branching instead.
6 Conclusion
We present selectional branching that uses confi-
dence estimates to decide when to employ a beam.
Coupled with our new hybrid parsing algorithm,
ADAGRAD, rich non-local features, and bootstrap-
ping, our parser gives higher parsing accuracy than
most other transition-based dependency parsers in
multiple languages and shows faster parsing speed.
It is interesting to see that our greedy parser out-
performed most other greedy dependency parsers.
This is because our parser used both bootstrapping
and Zhang and Nivre (2011)?s non-local features,
which had not been used by other greedy parsers.
In the future, we will experiment with more ad-
vanced dependency representations (de Marneffe
and Manning, 2008; Choi and Palmer, 2012b) to
show robustness of our approach. Furthermore, we
will evaluate individual methods of our approach
separately to show impact of each method on pars-
ing performance. We also plan to implement the
typical beam search approach to make a direct com-
parison to our selectional branching.8
Acknowledgments
Special thanks are due to Luke Vilnis of the Uni-
versity of Massachusetts Amherst for insights on
8Our parser is publicly available under an open source
project, ClearNLP (clearnlp.googlecode.com).
the ADAGRAD derivation. We gratefully acknowl-
edge a grant from the Defense Advanced Research
Projects Agency (DARPA) under the DEFT project,
solicitation #: DARPA-BAA-12-47.
References
Bernd Bohnet and Joakim Nivre. 2012. A Transition-
Based System for Joint Part-of-Speech Tagging and
Labeled Non-Projective Dependency Parsing. In
Proceedings of the 2012 Joint Conference on Em-
pirical Methods in Natural Language Processing
and Computational Natural Language Learning,
EMNLP?12, pages 1455?1465.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning, CoNLL?06,
pages 149?164.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, Dynamic Programming, and the Perceptron
for Efficient, Feature-rich Parsing. In Proceedings
of the 12th Conference on Computational Natural
Language Learning, CoNLL?08, pages 9?16.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Ju-
rafsky, and Christopher D. Manning. 2010. Pars-
ing to Stanford Dependencies: Trade-offs between
speed and accuracy. In Proceedings of the 7th In-
ternational Conference on Language Resources and
Evaluation, LREC?10.
Jinho D. Choi and Martha Palmer. 2011. Getting the
Most out of Transition-based Dependency Parsing.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, ACL:HLT?11, pages 687?
692.
Jinho D. Choi and Martha Palmer. 2012a. Fast and Ro-
bust Part-of-Speech Tagging Using Dynamic Model
Selection. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics,
ACL?12, pages 363?367.
Jinho D. Choi and Martha Palmer. 2012b. Guidelines
for the Clear Style Constituent to Dependency Con-
version. Technical Report 01-12, University of Col-
orado Boulder.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In Proceedings
of the conference on Empirical methods in natural
language processing, EMNLP?02, pages 1?8.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies
representation. In Proceedings of the COLING
workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
1060
Paramveer S. Dhillon, Jordan Rodu, Michael Collins,
Dean P. Foster, and Lyle H. Ungar. 2012. Spectral
Dependency Parsing with Latent Variables. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP?12, pages
205?213.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. The Journal of Ma-
chine Learning Research, 12(39):2121?2159.
Daniel Fern?ndez-Gonz?lez and Carlos G?mez-
Rodr?guez. 2012. Improving Transition-Based
Dependency Parsing with Buffer Transitions. In
Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning,
EMNLP?12, pages 308?319.
Yoav Goldberg and Michael Elhadad. 2010. An Effi-
cient Algorithm for Easy-First Non-Directional De-
pendency Parsing. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT:NAACL?10, pages 742?750.
Yoav Goldberg and Joakim Nivre. 2012. A Dynamic
Oracle for Arc-Eager Dependency Parsing. In Pro-
ceedings of the 24th International Conference on
Computational Linguistics, COLING?12.
Johan Hall, Joakim Nivre, and Jens Nilsson. 2006.
Discriminative Classifiers for Deterministic Depen-
dency Parsing. In In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, COLING-ACL?06, pages 316?
323.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin,
S. Sathiya Keerthi, and S. Sundararajan. 2008. A
Dual Coordinate Descent Method for Large-scale
Linear SVM. In Proceedings of the 25th interna-
tional conference on Machine learning, ICML?08,
pages 408?415.
Liang Huang and Kenji Sagae. 2010. Dynamic Pro-
gramming for Linear-Time Incremental Parsing. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, ACL?10.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured Perceptron with Inexact Search. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL-HLT?12, pages 142?151.
Terry Koo and Michael Collins. 2010. Efficient Third-
order Dependency Parsers. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, ACL?10.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple Semi-supervised Dependency Parsing. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, ACL:HLT?08,
pages 595?603.
Sandra K?bler, Ryan T. McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures
on Human Language Technologies. Morgan & Clay-
pool Publishers.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a Large Anno-
tated Corpus of English: The Penn Treebank. Com-
putational Linguistics, 19(2):313?330.
Andr? F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M. Q. Aguiar, and M?rio A. T. Figueiredo. 2010.
Turbo Parsers: Dependency Parsing by Approximate
Variational Inference. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP?10, pages 34?44.
Ryan Mcdonald and Fernando Pereira. 2006. Online
Learning of Approximate Dependency Parsing Algo-
rithms. In Proceedings of the Annual Meeting of the
European American Chapter of the Association for
Computational Linguistics, EACL?06, pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online Large-Margin Training of
Dependency Parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 91?98.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual Dependency Analysis with a
Two-Stage Discriminative Parser. In Proceedings
of the Tenth Conference on Computational Natural
Language Learning, CoNLL?06, pages 216?220.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing Graph-based and Transition-based Dependency
Parsers. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, ACL:HLT?08,
pages 950?958.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
Projective Dependency Parsing. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, ACL?05, pages 99?106.
Joakim Nivre, Johan Hall, Jens Nilsson, G?ls?en Eryig?it,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vec-
tor machines. In Proceedings of the 10th Confer-
ence on Computational Natural Language Learning,
CoNLL?06, pages 221?225.
Joakim Nivre. 2003. An Efficient Algorithm for Pro-
jective Dependency Parsing. In Proceedings of the
8th International Workshop on Parsing Technolo-
gies, IWPT?03, pages 149?160.
1061
Joakim Nivre. 2004. Incrementality in Deterministic
Dependency Parsing. In Proceedings of the ACL?04
Workshop on Incremental Parsing: Bringing Engi-
neering and Cognition Together, pages 50?57.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513?553.
Joakim Nivre. 2009. Non-Projective Dependency Pars-
ing in Expected Linear Time. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, ACL-
IJCNLP?09, pages 351?359.
Alexander M. Rush and Slav Petrov. 2012. Vine Prun-
ing for Efficient Multi-Pass Dependency Parsing. In
Proceedings of the 12th Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, NAACL:HLT?12.
Alexander M. Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On Dual Decomposi-
tion and Linear Programming Relaxations for Nat-
ural Language Processing. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP?10, pages 1?11.
Kenji Sagae and Alon Lavie. 2006. Parser Combina-
tion by Reparsing. In In Proceedings of the Human
Language Technology Conference of the NAACL,
NAACL?06, pages 129?132.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An Empirical Study of
Semi-supervised Structured Conditional Models for
Dependency Parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP?09, pages 551?560.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chine. In Proceedings of the 8th International Work-
shop on Parsing Technologies, IWPT?03, pages 195?
206.
Yue Zhang and Stephen Clark. 2008. A Tale of
Two Parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP?08, pages 562?571.
Hao Zhang and Ryan McDonald. 2012. Generalized
Higher-Order Dependency Parsing with Cube Prun-
ing. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL?12, pages 320?331.
Yue Zhang and Joakim Nivre. 2011. Transition-based
Dependency Parsing with Rich Non-local Features.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, ACL?11, pages 188?193.
1062
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 593?602,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning Soft Linear Constraints with Application to Citation Field
Extraction
Sam Anzaroot Alexandre Passos David Belanger Andrew McCallum
Department of Computer Science
University of Massachusetts, Amherst
{anzaroot, apassos, belanger, mccallum}@cs.umass.edu
Abstract
Accurately segmenting a citation string
into fields for authors, titles, etc. is a chal-
lenging task because the output typically
obeys various global constraints. Previous
work has shown that modeling soft con-
straints, where the model is encouraged,
but not require to obey the constraints, can
substantially improve segmentation per-
formance. On the other hand, for impos-
ing hard constraints, dual decomposition
is a popular technique for efficient predic-
tion given existing algorithms for uncon-
strained inference. We extend dual decom-
position to perform prediction subject to
soft constraints. Moreover, with a tech-
nique for performing inference given soft
constraints, it is easy to automatically gen-
erate large families of constraints and learn
their costs with a simple convex optimiza-
tion problem during training. This allows
us to obtain substantial gains in accuracy
on a new, challenging citation extraction
dataset.
1 Introduction
Citation field extraction, an instance of informa-
tion extraction, is the task of segmenting and la-
beling research paper citation strings into their
constituent parts, including authors, editors, year,
journal, volume, conference venue, etc. This task
is important because citation data is often pro-
vided only in plain text; however, having an ac-
curate structured database of bibliographic infor-
mation is necessary for many scientometric tasks,
such as mapping scientific sub-communities, dis-
covering research trends, and analyzing networks
of researchers. Automated citation field extrac-
tion needs further research because it has not yet
reached a level of accuracy at which it can be prac-
tically deployed in real-world systems.
Hidden Markov models and linear-chain condi-
tional random fields (CRFs) have previously been
applied to citation extraction (Hetzner, 2008; Peng
and McCallum, 2004) . These models support ef-
ficient dynamic-programming inference, but only
model local dependencies in the output label se-
quence. However citations have strong global reg-
ularities not captured by these models. For exam-
ple many book citations contain both an author
section and an editor section, but none have two
disjoint author sections. Since linear-chain mod-
els are unable to capture more than Markov depen-
dencies, the models sometimes mislabel the editor
as a second author. If we could enforce the global
constraint that there should be only one author
section, accuracy could be improved.
One framework for adding such global con-
straints into tractable models is constrained infer-
ence, in which at inference time the original model
is augmented with restrictions on the outputs such
that they obey certain global regularities. When
hard constraints can be encoded as linear equa-
tions on the output variables, and the underlying
model?s inference task can be posed as linear opti-
mization, one can formulate this constrained infer-
ence problem as an integer linear program (ILP)
(Roth and Yih, 2004). Alternatively, one can em-
ploy dual decomposition (Rush et al, 2010). Dual
decompositions?s advantage over ILP is is that it
can leverage existing inference algorithms for the
original model as a black box. Such a modular
algorithm is easy to implement, and works quite
well in practice, providing certificates of optimal-
ity for most examples.
The above two approaches have previously been
applied to impose hard constraints on a model?s
output. On the other hand, recent work has demon-
strated improvements in citation field extraction
by imposing soft constraints (Chang et al, 2012).
Here, the model is not required obey the global
constraints, but merely pays a penalty for their vi-
593
4 .
ref-marker
[ J.
first
D.
middle
Monk ,
last person
]
authors
[ Cardinal Functions on Boolean Algebra , ]
title
[ Lectures in Mathematics
, ETH Zurich ,
series
Birkhause Verlag ,
publisher
Basel , Boston , Berlin ,
address
1990 .
year date
]
venue
Figure 1: Example labeled citation
olation.
This paper introduces a novel method for im-
posing soft constraints via dual decomposition.
We also propose a method for learning the penal-
ties the prediction problem incurs for violating
these soft constraints. Because our learning
method drives many penalties to zero, it allows
practitioners to perform ?constraint selection,? in
which a large number of automatically-generated
candidate global constraints can be considered and
automatically culled to a smaller set of useful con-
straints, which can be run quickly at test time.
Using our new method, we are able to incor-
porate not only all the soft global constraints of
Chang et al (2012), but also far more com-
plex data-driven constraints, while also provid-
ing stronger optimality certificates than their beam
search technique. On a new, more broadly rep-
resentative, and challenging citation field extrac-
tion data set, we show that our methods achieve a
17.9% reduction in error versus a linear-chain con-
ditional random field. Furthermore, we demon-
strate that our inference technique can use and
benefit from the constraints of Chang et al (2012),
but that including our data-driven constraints on
top of these is beneficial. While this paper fo-
cusses on an application to citation field extrac-
tion, the novel methods introduced here would
easily generalize to many problems with global
output regularities.
2 Background
2.1 Structured Linear Models
The overall modeling technique we employ is to
add soft constraints to a simple model for which
we have an existing efficient prediction algorithm.
For this underlying model, we employ a chain-
structured conditional random field (CRF), since
CRFs have been shown to perform better than
other simple unconstrained models like hidden
markov models for citation extraction (Peng and
McCallum, 2004). We produce a prediction by
performing MAP inference (Koller and Friedman,
2009).
The MAP inference task in a CRF be can ex-
pressed as an optimization problem with a lin-
ear objective (Sontag, 2010; Sontag et al, 2011).
Here, we define a binary indicator variable for
each candidate setting of each factor in the graph-
ical model. Each of these indicator variables is
associated with the score that the factor takes on
when it has the indictor variable?s corresponding
value. Since the log probability of some y in the
CRF is proportional to sum of the scores of all the
factors, we can concatenate the indicator variables
as a vector y and the scores as a vectorw and write
the MAP problem as
max. ?w, y?
s.t. y ? U ,
(1)
where the set U represents the set of valid config-
urations of the indicator variables. Here, the con-
straints are that all neighboring factors agree on
the components of y in their overlap.
Structured Linear Models are the general fam-
ily of models where prediction requires solving a
problem of the form (1), and they do not always
correspond to a probabilistic model. The algo-
rithms we present in later sections for handling
soft global constraints and for learning the penal-
ties of these constraints can be applied to gen-
eral structured linear models, not just CRFs, pro-
vided we have an available algorithm for perform-
ing MAP inference.
2.2 Dual Decomposition for Global
Constraints
In order to perform prediction subject to various
global constraints, we may need to augment the
problem (1) with additional constraints. Dual De-
composition is a popular method for performing
MAP inference in this scenario, since it lever-
ages known algorithms for MAP in the base prob-
lem where these extra constraints have not been
added (Komodakis et al, 2007; Sontag et al,
2011; Rush and Collins, 2012). In this case, the
MAP problem can be formulated as a structured
linear model similar to equation (1), for which we
have a MAP algorithm, but where we have im-
posed some additional constraints Ay ? b that
no longer allow us to use the algorithm. In other
594
Algorithm 1 DD: projected subgradient for dual
decomposition with hard constraints
1: while has not converged do
2: y
(t)
= argmax
y?U
?
w +A
T
?, y
?
3: ?
(t)
= ?
0??
[
?
(t?1)
? ?
(t)
(Ay ? b)
]
words, we consider the problem
max. ?w, y?
s.t. y ? U
Ay ? b,
(2)
for an arbitrary matrix A and vector b. We can
write the Lagrangian of this problem as
L(y, ?) = ?w, y?+ ?
T
(Ay ? b). (3)
Regrouping terms and maximizing over the primal
variables, we have the dual problem
min.
?
D(?) = max
y?U
?
w +A
T
?, y
?
? ?
T
b. (4)
For any ?, we can evaluate the dual objective
D(?), since the maximization in (4) is of the same
form as the original problem (1), and we assumed
we had a method for performing MAP in this. Fur-
thermore, a subgradient ofD(?) isAy
?
?b, for an
y
?
which maximizes this inner optimization prob-
lem. Therefore, we can minimize D(?) with the
projected subgradient method (Boyd and Vanden-
berghe, 2004), and the optimal y can be obtained
when evaluating D(?
?
). Note that the subgradient
of D(?) is the amount by which each constraint is
violated by ? when maximizing over y.
Algorithm 1 depicts the basic projected subgra-
dient descent algorithm for dual decomposition.
The projection operator ? consists of truncating
all negative coordinates of ? to 0. This is neces-
sary because ? is a vector of dual variables for in-
equality constraints. The algorithm has converged
when each constraint is either satisfied by y
(t)
with
equality or its corresponding component of ? is 0,
due to complimentary slackness (Boyd and Van-
denberghe, 2004).
3 Soft Constraints in Dual
Decomposition
We now introduce an extension of Algorithm 1
to handle soft constraints. In our formulation, a
soft-constrained model imposes a penalty for each
unsatisfied constraint, proportional to the amount
by which it is violated. Therefore, our derivation
parallels how soft-margin SVMs are derived from
hard-margin SVMs by introducing auxiliary slack
variables (Cortes and Vapnik, 1995). Note that
when performing MAP subject to soft constraints,
optimal solutions might not satisfy some con-
straints, since doing so would reduce the model?s
score by too much.
Consider the optimization problems of the
form:
max. ?w, y? ? ?c, z?
s.t. y ? U
Ay ? b ? z
?z ? 0,
(5)
For positive c
i
, it is clear that an optimal z
i
will
be equal to the degree to which a
T
i
y ? b
i
is vio-
lated. Therefore, we pay a cost c
i
times the degree
to which the ith constraint is violated, which mir-
rors how slack variables are used to represent the
hinge loss for SVMs. Note that c
i
has to be pos-
itive, otherwise this linear program is unbounded
and an optimal value can be obtained by setting z
i
to infinity.
Using a similar construction as in section 2.2 we
write the Lagrangian as:
(6)
L(y, z, ?, ?) = ?w, y? ? ?c, z?
+ ?
T
(Ay ? b? z) + ?
T
(?z).
The optimality constraints with respect to z tell us
that ?c? ?? ? = 0, hence ? = ?c? ?. Substi-
tuting, we have
L(y, ?) = ?w, y?+ ?
T
(Ay ? b), (7)
except the constraint that ? = ?c? ? implies that
for ? to be positive ? ? c.
Since this Lagrangian has the same form as
equation (3), we can also derive a dual problem,
which is the same as in equation (4), with the ad-
ditional constraint that each ?
i
can not be bigger
than its cost c
i
. In other words, the dual problem
can not penalize the violation of a constraint more
than the soft constraint model in the primal would
penalize you if you violated it.
This optimization problem can still be solved
with projected subgradient descent and is depicted
in Algorithm 2. The only modifications to Al-
gorithm 1 are replacing the coordinate-wise pro-
jection ?
0??
with ?
0???c
and how we check for
convergence. Now, we check for the KKT con-
ditions of (5), where for every constraint i, either
the constraint is satisfied with equality, ?
i
= 0, or
?
i
= c
i
.
595
Algorithm 2 Soft-DD: projected subgradient for
dual decomposition with soft constraints
1: while has not converged do
2: y
(t)
= argmax
y?U
?
w +A
T
?, y
?
3: ?
(t)
= ?
0???c
[
?
(t?1)
? ?
(t)
(Ay ? b)
]
Therefore, implementing soft-constrained dual
decomposition is as easy as implementing hard-
constrained dual decomposition, and the per-
iteration complexity is the same. We encourage
further applications of soft-constraint dual decom-
position to existing and new NLP problems.
3.1 Learning Penalties
One consideration when using soft v.s. hard con-
straints is that soft constraints present a new train-
ing problem, since we need to choose the vector
c, the penalties for violating the constraints. An
important property of problem (5) in the previous
section is that it corresponds to a structured lin-
ear model over y and z. Therefore, we can apply
known training algorithms for estimating the pa-
rameters of structured linear models to choose c.
All we need to employ the structured perceptron
algorithm (Collins, 2002) or the structured SVM
algorithm (Tsochantaridis et al, 2004) is a black-
box procedure for performing MAP inference in
the structured linear model given an arbitrary cost
vector. Fortunately, the MAP problem for (5) can
be solved using Soft-DD, in Algorithm 2.
Each penalty c
i
has to be non-negative; other-
wise, the optimization problem in equation (5) is
ill-defined. This can be ensured by simple mod-
ifications of the perceptron and subgradient de-
scent optimization of the structured SVM objec-
tive simply by truncating c coordinate-wise to be
non-negative at every learning iteration.
Intuitively, the perceptron update increases the
penalty for a constraint if it is satisfied in the
ground truth and not in an inferred prediction, and
decreases the penalty if the constraint is satisfied
in the prediction and not the ground truth. Since
we truncate penalties at 0, this suggests that we
will learn a penalty of 0 for constraints in three cat-
egories: constraints that do not hold in the ground
truth, constraints that hold in the ground truth but
are satisfied in practice by performing inference
in the base CRF model, and constraints that are
satisfied in practice as a side-effect of imposing
non-zero penalties on some other constraints . A
similar analysis holds for the structured SVM ap-
proach.
Therefore, we can view learning the values of
the penalties not just as parameter tuning, but as a
means to perform ?constraint selection,? since con-
straints that have a penalty of 0 can be ignored.
This property allows us to consider large families
of constraints, from which the useful ones are au-
tomatically identified.
We found it beneficial, though it is not theoreti-
cally necessary, to learn the constraints on a held-
out development set, separately from the other
model parameters, as during training most con-
straints are satisfied due to overfitting, which leads
to an underestimation of the relevant penalties.
4 Citation Extraction Data
We consider the UMass citation dataset, first intro-
duced in Anzaroot and McCallum (2013). It has
over 1800 citation from many academic fields, ex-
tracted from the arXiv. This dataset contains both
coarse-grained and fine-grained labels; for exam-
ple it contains labels for the segment of all authors,
segments for each individual author, and for the
first and last name of each author. There are 660
citations in the development set and 367 citation
in the test set.
The labels in the UMass dataset are a con-
catenation of labels from a hierarchically-defined
schema. For example, a first name of an author is
tagged as: authors/person/first. In addition, indi-
vidual tokens are labeled using a BIO label schema
for each level in the hierarchy. BIO is a commonly
used labeling schema for information extraction
tasks. BIO labeling allows individual labels on
tokens to label segmentation information as well
as labels for the segments. In this schema, labels
that begin segments are prepended with a B, la-
bels that continue a segment are prepended with
an I, and tokens that don?t have a labeling in this
schema are given an O label. For example, in a hi-
erarchical BIO label schema the first token in the
first name for the second author may be labeled as:
I-authors/B-person/B-first.
An example labeled citation in this dataset can
be viewed in figure 1.
5 Global Constraints for Citation
Extraction
5.1 Constraint Templates
We now describe the families of global constraints
we consider for citation extraction. Note these
596
constraints are all linear, since they depend only
on the counts of each possible conditional ran-
dom field label. Moreover, since our labels are
BIO-encoded, it is possible, by counting B tags,
to count how often each citation tag itself appears
in a sentence. The first two families of constraints
that we describe are general to any sequence la-
beling task while the last is specific to hierarchical
labeling such as available in the UMass dataset.
Our sequence output is denoted as y and an ele-
ment of this sequence is y
k
.
We denote [[y
k
= i]] as the function that outputs
1 if y
k
has a 1 at index i and 0 otherwise. Here, y
k
represents an output tag of the CRF, so if [[y
k
= i]]
= 1, then we have that y
k
was given a label with
index i.
5.2 Singleton Constraints
Singleton constraints ensure that each label can
appear at most once in a citation. These are same
global constraints that were used for citation field
extraction in Chang et al (2012). We define s(i)
to be the number of times the label with index i is
predicted in a citation, formally:
s(i) =
?
y
k
?y
[[y
k
= i]]
The constraint that each label can appear at
most once takes the form:
s(i) <= 1
5.3 Pairwise Constraints
Pairwise constraints are constraints on the counts
of two labels in a citation. We define z
1
(i, j) to be
z
1
(i, j) =
?
y
k
?y
[[y
k
= i]] +
?
y
k
?y
[[y
k
= j]]
and z
2
(i, j) to be
z
2
(i, j) =
?
y
k
?y
[[y
k
= i]]?
?
y
k
?y
[[y
k
= j]]
We consider all constraints of the forms:
z(i, j) ? 0, 1, 2, 3 and z(i, j) ? 0, 1, 2, 3.
Note that some pairs of these constraints are re-
dundant or logically incompatible. However, we
are using them as soft constraints, so these con-
straints will not necessarily be satisfied by the out-
put of the model, which eliminates concern over
enforcing logically impossible outputs. Further-
more, in section 3.1 we described how our proce-
dure for learning penalties will drive some penal-
ties to 0, which effectively removes them from our
set of constraints we consider. It can be shown, for
example, that we will never learn non-zero penal-
ties for certain pairs of logically incompatible con-
straints using the perceptron-style algorithm de-
scribed in section 3.1 .
5.4 Hierarchical Equality Constraints
The labels in the citation dataset are hierarchical
labels. This means that the labels are the concate-
nation of all the levels in the hierarchy. We can
create constraints that are dependent on only one
or couple of elements in the hierarchy.
We define C(x, i) as the function that returns 1
if the output x contains the label i in the hierarchy
and 0 otherwise. We define e(i, j) to be
e(i, j) =
?
y
k
?y
[[C(y
k
, i)]]?
?
y
k
?y
[[C(y
k
, j)]]
Hierarchical equality constraints take the forms:
e(i, j) ? 0 (8)
e(i, j) ? 0 (9)
5.5 Local constraints
We constrain the output labeling of the chain-
structured CRF to be a valid BIO encoding.
This both improves performance of the underly-
ing model when used without global constraints,
as well as ensures the validity of the global con-
straints we impose, since they operate only on
B labels. The constraint that the labeling is
valid BIO can be expressed as a collection of
pairwise constraints on adjacent labels in the se-
quence. Rather than enforcing these constraints
using dual decomposition, they can be enforced
directly when performing MAP inference in the
CRF by modifying the dynamic program of the
Viterbi algorithm to only allow valid pairs of adja-
cent labels.
5.6 Constraint Pruning
While the techniques from section 3.1 can easily
cope with a large numbers of constraints at train-
ing time, this can be computationally costly, spe-
cially if one is considering very large constraint
families. This is problematic because the size
597
Unconstrained
[17]
ref-marker
[ D.
first
Sivia ,
last person
J.
first
Skilling ,
last person
]
authors
[ Data Analysis : A Bayesian Tutorial
,
booktitle
Oxford University Press ,
publisher
2006
year date
]
venue
Constrained
[17]
ref-marker
[ D.
first
Sivia ,
last person
J.
first
Skilling ,
last person
]
authors
Data Analysis : A Bayesian Tutorial
,
title
[ Oxford University Press ,
publisher
2006
year date
]
venue
Unconstrained
[ Sobol? ,
last
I.
first
M.
middle person
]
authors
[ (1990) .
year
]
date
[On sensitivity estimation for nonlinear mathe-
matical models .]
title
[ Matematicheskoe Modelirovanie ,
journal
2
volume
(1) :
number
112?118 .
pages
( In Russian
) .
status
]
venue
Constrained
[ Sobol? ,
last
I.
first
M.
middle person
]
authors
[ (1990) .
year
]
date
[On sensitivity estimation for nonlinear mathe-
matical models .]
title
[ Matematicheskoe Modelirovanie ,
journal
2
volume
(1) :
number
112?118 .
pages
( In Russian
) .
language
]
venue
Figure 2: Two examples where imposing soft global constraints improves field extraction errors. Soft-
DD converged in 1 iteration on the first example, and 7 iterations on the second. When a reference is
citing a book and not a section of the book, the correct labeling of the name of the book is title. In
the first example, the baseline CRF incorrectly outputs booktitle, but this is fixed by Soft-DD, which
penalizes outputs based on the constraint that booktitle should co-occur with an address label. In the
second example, the unconstrained CRF output violates the constraint that title and status labels should
not co-occur. The ground truth labeling also violates a constraint that title and language labels should
not co-occur. At convergence of the Soft-DD algorithm, the correct labeling of language is predicted,
which is possible because of the use of soft constraints.
Constraints F1 score Sparsity # of cons
Baseline 94.44
Only-one 94.62 0% 3
Hierarchical 94.55 56.25% 16
Pairwise 95.23 43.19% 609
All 95.39 32.96% 628
All DD 94.60 0% 628
Table 1: Set of constraints learned and F1 scores.
The last row depicts the result of inference using
all constraints as hard constraints.
of some constraint families we consider grows
quadratically with the number of candidate labels,
and there are about 100 in the UMass dataset.
Such a family consists of constraints that the sum
of the counts of two different label types has to
be bounded (a useful example is that there can?t
be more than one out of ?phd thesis? and ?jour-
nal?). Therefore, quickly pruning bad constraints
can save a substantial amount of training time, and
can lead to better generalization.
To do so, we calculate a score that estimates
how useful each constraint is expected to be. Our
score compares how often the constraint is vio-
lated in the ground truth examples versus our pre-
dictions. Here, prediction is done with respect to
the base chain-structured CRF tagger and does not
include global constraints. Note that it may make
sense to consider a constraint that is sometimes vi-
olated in the ground truth, as the penalty learning
algorithm can learn a small penalty for it, which
will allow it to be violated some of the time. Our
importance score is defined as, for each constraint
c on labeled set D,
imp(c) =
?
d?D
[[max
y
w
T
d
y]]
c
?
d?D
[[y
d
]]
c
, (10)
where [[y]]
c
is 1 if the constraint is violated on out-
put y and 0 otherwise. Here, y
d
denotes the ground
truth labeling and w
d
is the vector of scores for the
CRF tagger.
We prune constraints by picking a cutoff value
for imp(c). A value of imp(c) above 1 implies
that the constraint is more violated on the pre-
dicted examples than on the ground truth, and
hence that we might want to keep it.
We also find that the constraints that have the
largest imp values are semantically interesting.
6 Related Work
There are multiple previous examples of augment-
ing chain-structured sequence models with terms
capturing global relationships by expanding the
chain to a more complex graphical model with
non-local dependencies between the outputs. In-
ference in these models can be performed, for
example, with loopy belief propagation (Bunescu
and Mooney, 2004; Sutton and McCallum, 2004)
or Gibbs sampling (Finkel et al, 2005). Be-
lief propagation is prohibitively expensive in our
598
model due to the high cardinalities of the out-
put variables and of the global factors, which in-
volve all output variables simultaneously. There
are various methods for exploiting the combi-
natorial structure of these factors, but perfor-
mance would still have higher complexity than our
method. While Gibbs sampling has been shown
to work well tasks such as named entity recogni-
tion (Finkel et al, 2005), our previous experiments
show that it does not work well for citation extrac-
tion, where it found only low-quality solutions in
practice because the sampling did not mix well,
even on a simple chain-structured CRF.
Recently, dual decomposition has become a
popular method for solving complex structured
prediction problems in NLP (Koo et al, 2010;
Rush et al, 2010; Rush and Collins, 2012; Paul
and Eisner, 2012; Chieu and Teow, 2012). Soft
constraints can be implemented inefficiently using
hard constraints and dual decomposition? by in-
troducing copies of output variables and an aux-
iliary graphical model, as in Rush et al (2012).
However, at every iteration of dual decomposition,
MAP must be run in this auxiliary model. Further-
more the copying of variables doubles the num-
ber of iterations needed for information to flow
between output variables, and thus slows conver-
gence. On the other hand, our approach to soft
constraints has identical per-iteration complexity
as for hard constraints, and is a very easy modifi-
cation to existing hard constraint code.
Initial work in machine learning for citation ex-
traction used Markov models with no global con-
straints. Hidden Markov models (HMMs), were
originally employed for automatically extracting
information from research papers on the CORA
dataset (Seymore et al, 1999; Hetzner, 2008).
Later, CRFs were shown to perform better on
CORA, improving the results from the Hmm?s
token-level F1 of 86.6 to 91.5 with a CRF(Peng
and McCallum, 2004).
Recent work on globally-constrained inference
in citation extraction used an HMM
CCM
, which is
an HMM with the addition of global features that
are restricted to have positive weights (Chang et
al., 2012). Approximate inference is performed
using beam search. This method increased the
HMM token-level accuracy from 86.69 to 93.92
on a test set of 100 citations from the CORA
dataset. The global constraints added into the
model are simply that each label only occurs
once per citation. This approach is limited in its
use of an HMM as an underlying model, as it
has been shown that CRFs perform significantly
better, achieving 95.37 token-level accuracy on
CORA (Peng and McCallum, 2004). In our ex-
periments, we demonstrate that the specific global
constraints used by Chang et al (2012) help on the
UMass dataset as well.
7 Experimental Results
Our baseline is the one used in Anzaroot and
McCallum (2013), with some labeling errors re-
moved. This is a chain-structured CRF trained
to maximize the conditional likelihood using L-
BFGS with L2 regularization.
We use the same features as Anzaroot and Mc-
Callum (2013), which include word type, capital-
ization, binned location in citation, regular expres-
sion matches, and matches into lexicons. In addi-
tion, we use a rule-based segmenter that segments
the citation string based on punctuation as well as
probable start or end segment words (e.g. ?in? and
?volume?). We add a binary feature to tokens that
correspond to the start of a segment in the output
of this simple segmenter. This final feature im-
proves the F1 score on the cleaned test set from
94.0 F1 to 94.44 F1, which we use as a baseline
score.
We then use the development set to learn the
penalties for the soft constraints, using the percep-
tron algorithm described in section 3.1. MAP in-
ference in the model with soft constraints is per-
formed using Soft-DD, shown in Algorithm 2.
We instantiate constraints from each template in
section 5.1, iterating over all possible labels that
contain a B prefix at any level in the hierarchy and
pruning all constraints with imp(c) < 2.75 cal-
culated on the development set. We asses perfor-
mance in terms of field-level F1 score, which is
the harmonic mean of precision and recall for pre-
dicted segments.
Table 1 shows how each type of constraint fam-
ily improved the F1 score on the dataset. Learning
all the constraints jointly provides the largest im-
provement in F1 at 95.39. This improvement in F1
over the baseline CRF as well as the improvement
in F1 over using only-one constraints was shown
to be statistically significant using the Wilcoxon
signed rank test with p-values < 0.05. In the
all-constraints settings, 32.96% of the constraints
have a learned parameter of 0, and therefore only
599
Stop F1 score Convergence Avg Iterations
1 94.44 76.29% 1.0
2 95.07 83.38% 1.24
5 95.12 95.91% 1.61
10 95.39 99.18% 1.73
Table 2: Performance from terminating Soft-DD
early. Column 1 is the number of iterations we
allow each example. Column 3 is the % of test
examples that converged. Column 4 is the aver-
age number of necessary iterations, a surrogate for
the slowdown over performing unconstrained in-
ference.
421 constraints are active. Soft-DD converges,
and thus solves the constrained inference prob-
lem exactly, for all test set examples after at most
41 iterations. Running Soft-DD to convergence
requires 1.83 iterations on average per example.
Since performing inference in the CRF is by far
the most computationally intensive step in the iter-
ative algorithm, this means our procedure requires
approximately twice as much work as running the
baseline CRF on the dataset. On examples where
unconstrained inference does not satisfy the con-
straints, Soft-DD converges after 4.52 iterations
on average. For 11.99% of the examples, the
Soft-DD algorithm satisfies constraints that were
not satisfied during unconstrained inference, while
in the remaining 11.72% Soft-DD converges with
some constraints left unsatisfied, which is possible
since we are imposing them as soft constraints.
We could have enforced these constraints as
hard constraints rather than soft ones. This exper-
iment is shown in the last row of Table 1, where
F1 only improves to 94.6. In addition, running
the DD algorithm with these constraints takes 5.21
iterations on average per example, which is 2.8
times slower than Soft-DD with learned penalties.
In Figure 2, we analyze the performance of
Soft-DD when we don?t necessarily run it to con-
vergence, but stop after a fixed number of itera-
tions on each test set example. We find that a large
portion of our gain in accuracy can be obtained
when we allow ourselves as few as 2 dual decom-
position iterations. However, this only amounts to
1.24 times as much work as running the baseline
CRF on the dataset, since the constraints are satis-
fied immediately for many examples.
In Figure 2 we consider two applications of our
Soft-DD algorithm, and provide analysis in the
caption.
We train and evaluate on the UMass dataset in-
stead of CORA, because it is significantly larger,
has a useful finer-grained labeling schema, and its
annotation is more consistent. We were able to ob-
tain better performance on CORA using our base-
line CRF than the HMM
CCM
results presented
in Chang et al (2012), which include soft con-
straints. Given this high performance of our base
model on CORA, we did not apply our Soft-DD
algorithm to the dataset. Furthermore, since the
dataset is so small, learning the penalties for our
large collection of constraints is difficult, and test
set results are unreliable. Rather than compare our
work to Chang et al (2012) via results on CORA,
we apply their constraints on the UMass data us-
ing Soft-DD and demonstrate accuracy gains, as
discussed above.
7.1 Examples of learned constraints
We now describe a number of the useful con-
straints that receive non-zero learned penalties
and have high importance scores, defined in Sec-
tion 5.6. The importance score of a constraint pro-
vides information about how often it is violated
by the CRF, but holds in the ground truth, and a
non-zero penalty implies we enforce it as a soft
constraint at test time.
The two singleton constraints with highest im-
portance score are that there should only be at
most one title segment in a citation and that there
should be at most one author segment in a cita-
tion. The only one author constraint is particu-
larly useful for correctly labeling editor segments
in cases where unconstrained inference mislabels
them as author segments. As can be seen in Table
3, editor fields are among the most improved with
our new method, largely due to this constraint.
The two hierarchical constraints with the high-
est importance scores with non-zero learned
penalties constrain the output such that number
of person segments does not exceed the number
of first segments and vice-versa. Together, these
constraints penalize outputs in which the number
of person segments do not equal the number of
first segments, i.e., every author should have a first
name.
One important pairwise constraint penalizes
outputs in which thesis segments don?t co-occur
with school segments. School segments label the
name of the university that the thesis was submit-
ted to. The application of this constraint increases
the performance of the model on school segments
600
Label U C +
venue/series 35.29 66.67 31.37
venue/editor/person/first 66.67 94.74 28.07
venue/school 40.00 66.67 26.67
venue/editor/person/last 75.00 94.74 19.74
venue/editor 77.78 90.00 12.22
venue/editor/person/middle 81.82 91.67 9.85
Table 3: Labels with highest improvement in F1.
U is in unconstrained inference. C is the results of
constrained inference. + is the improvement in F1.
dramatically, as can be seen in table 3.
An interesting form of pairwise constraints pe-
nalize outputs in which some labels do not co-
occur with other labels. Some examples of con-
straints in this form enforce that journal segments
should co-occur with pages segments and that
booktitle segments should co-occur with address
segments. An example of the latter constraint be-
ing employed during inference is the first example
in Figure 2. Here, the constrained inference pe-
nalizes output which contains a booktitle segment
but no address segment. This penalization leads
allows the constrained inference to correctly label
the booktitle segment as a title segment.
The above example constraints are almost al-
ways satisfied on the ground truth, and would be
useful to enforce as hard constraints. However,
there are a number of learned constraints that are
often violated on the ground truth but are still use-
ful as soft constraints. Take, for example, the con-
straint that the number of number segments does
not exceed the number of booktitle segments, as
well as the constraint that it does not exceed the
number of journal segments. These constraints
are moderately violated on ground truth examples,
however. For example, when booktitle segments
co-occur with number segments but not with jour-
nal segments, the second constraint is violated. It
is still useful to impose these soft constraints, as
strong evidence from the CRF allows us to violate
them, and they can guide the model to good pre-
dictions when the CRF is unconfident.
8 Conclusion
We introduce a novel modification to the stan-
dard projected subgradient dual decomposition al-
gorithm for performing MAP inference subject to
hard constraints to one for performing MAP in the
presence of soft constraints. In addition, we offer
an easy-to-implement procedure for learning the
penalties on soft constraints. This method drives
many penalties to zero, which allows users to auto-
matically discover discriminative constraints from
large families of candidates.
We show via experiments on a recent substantial
dataset that using soft constraints, and selecting
which constraints to use with our penalty-learning
procedure, can lead to significant gains in accu-
racy. We achieve a 17% gain in accuracy over
a chain-structured CRF model, while only need-
ing to run MAP in the CRF an average of less
than 2 times per example. This minor incremen-
tal cost over Viterbi, plus the fact that we obtain
certificates of optimality on 100% of our test ex-
amples in practice, suggests the usefulness of our
algorithm for large-scale applications. We encour-
age further use of our Soft-DD procedure for other
structured prediction problems.
Acknowledgments
This work was supported in part by the Cen-
ter for Intelligent Information Retrieval, in part
by DARPA under agreement number FA8750-13-
2-0020, in part by NSF grant #CNS-0958392,
and in part by IARPA via DoI/NBC contract
#D11PC20152. The U.S. Government is autho-
rized to reproduce and distribute reprint for Gov-
ernmental purposes notwithstanding any copy-
right annotation thereon. Any opinions, findings
and conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect those of the sponsor.
References
Sam Anzaroot and Andrew McCallum. 2013. A new
dataset for fine-grained citation field extraction. In
ICML Workshop on Peer Reviewing and Publishing
Models.
Stephen Poythress Boyd and Lieven Vandenberghe.
2004. Convex optimization. Cambridge university
press.
Razvan Bunescu and Raymond J Mooney. 2004.
Collective information extraction with relational
markov networks. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Lin-
guistics, page 438. Association for Computational
Linguistics.
M. Chang, L. Ratinov, and D. Roth. 2012. Structured
learning with constrained conditional models. Ma-
chine Learning, 88(3):399?431, 6.
Hai Leong Chieu and Loo-Nin Teow. 2012. Com-
bining local and non-local information with dual de-
composition for named entity recognition from text.
601
In Information Fusion (FUSION), 2012 15th Inter-
national Conference on, pages 231?238. IEEE.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing-Volume 10, pages 1?8.
Association for Computational Linguistics.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273?297.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363?370. Association for Computational Lin-
guistics.
Erik Hetzner. 2008. A simple method for citation
metadata extraction using hidden markov models. In
Proceedings of the 8th ACM/IEEE-CS joint confer-
ence on Digital libraries, pages 280?284. ACM.
Daphne Koller and Nir Friedman. 2009. Probabilistic
graphical models: principles and techniques. The
MIT Press.
Nikos Komodakis, Nikos Paragios, and Georgios Tzir-
itas. 2007. Mrf optimization via dual decomposi-
tion: Message-passing revisited. In Computer Vi-
sion, 2007. ICCV 2007. IEEE 11th International
Conference on, pages 1?8. IEEE.
Terry Koo, Alexander M Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1288?1298. Association for Compu-
tational Linguistics.
Michael J Paul and Jason Eisner. 2012. Implicitly in-
tersecting weighted automata using dual decompo-
sition. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 232?242. Association for Computa-
tional Linguistics.
Fuchun Peng and Andrew McCallum. 2004. Accu-
rate information extraction from research papers us-
ing conditional random fields. In Daniel Marcu Su-
san Dumais and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings, pages 329?336, Boston,
Massachusetts, USA, May 2 - May 7. Association
for Computational Linguistics.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. Defense Technical Information Center.
Alexander M. Rush and Michael Collins. 2012. A tu-
torial on dual decomposition and lagrangian relax-
ation for inference in natural language processing.
J. Artif. Intell. Res. (JAIR), 45:305?362.
Alexander M Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposition
and linear programming relaxations for natural lan-
guage processing. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1?11. Association for Computa-
tional Linguistics.
Alexander M Rush, Roi Reichart, Michael Collins, and
Amir Globerson. 2012. Improved parsing and pos
tagging using inter-sentence consistency constraints.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 1434?1444.
Kristie Seymore, Andrew McCallum, Roni Rosenfeld,
et al 1999. Learning hidden markov model struc-
ture for information extraction. In AAAI-99 Work-
shop on Machine Learning for Information Extrac-
tion, pages 37?42.
David Sontag, Amir Globerson, and Tommi Jaakkola.
2011. Introduction to dual decomposition for in-
ference. In Suvrit Sra, Sebastian Nowozin, and
Stephen J. Wright, editors, Optimization for Ma-
chine Learning. MIT Press.
David Sontag. 2010. Approximate Inference in Graph-
ical Models using LP Relaxations. Ph.D. thesis,
Massachusetts Institute of Technology, Department
of Electrical Engineering and Computer Science.
Charles Sutton and Andrew McCallum. 2004. Col-
lective segmentation and labeling of distant entities
in information extraction. Technical report, DTIC
Document.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vector
machine learning for interdependent and structured
output spaces. In Proceedings of the twenty-first in-
ternational conference on Machine learning, page
104. ACM.
602
Proceedings of BioNLP Shared Task 2011 Workshop, pages 46?50,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Robust Biomedical Event Extraction with Dual Decomposition and Minimal
Domain Adaptation
Sebastian Riedel Andrew McCallum
Department of Computer Science
University of Massachusetts, Amherst
{riedel,mccallum}@cs.umass.edu
Abstract
We present a joint model for biomedical event
extraction and apply it to four tracks of the
BioNLP 2011 Shared Task. Our model de-
composes into three sub-models that concern
(a) event triggers and outgoing arguments, (b)
event triggers and incoming arguments and
(c) protein-protein bindings. For efficient de-
coding we employ dual decomposition. Our
results are very competitive: With minimal
adaptation of our model we come in second
for two of the tasks?right behind a version
of the system presented here that includes pre-
dictions of the Stanford event extractor as fea-
tures. We also show that for the Infectious
Diseases task using data from the Genia track
is a very effective way to improve accuracy.
1 Introduction
This paper presents the UMass entry to the BioNLP
2011 shared task (Kim et al, 2011a). We introduce
a simple joint model for the extraction of biomedical
events, and show competitive results for four tracks
of the competition. Our model subsumes three
tractable sub-models, one for extracting event trig-
gers and outgoing edges, one for event triggers and
incoming edges and one for protein-protein bind-
ings. Fast and accurate joint inference is provided by
combining optimizing methods for these three sub-
models via dual decomposition (Komodakis et al,
2007; Rush et al, 2010). Notably, our model con-
stitutes the first joint approach that explicitly pre-
dicts which protein should share the same binding
event. So far this has either been done through post-
processing heuristics (Bj?rne et al, 2009; Riedel et
al., 2009; Poon and Vanderwende, 2010), or through
a local classifier at the end of a pipeline (Miwa et al,
2010).
Our model is very competitive. For Genia (GE)
Task 1 (Kim et al, 2011b) we achieve the second-
best results. In addition, the best-performing FAUST
system (Riedel et al, 2011) is a variant of the model
presented here. Its advantage stems from the fact
that it uses predictions of the Stanford system (Mc-
Closky et al, 2011a; McClosky et al, 2011b), and
hence performs model combination. The same holds
for the Infectious Diseases (ID) track (Pyysalo et al,
2011), where we come in as second right behind
the FAUST system. For the Epigenetics and Post-
translational Modifications (EPI) track (Ohta et al,
2011) we achieve the 4th rank, partly because we did
not aim to extract speculations, negations or cellular
locations. Finally, for Genia Task 2 we rank 3rd?
with the 1st rank achieved by the FAUST system.
In the following we will briefly describe our
model and inference algorithm, as far as this is pos-
sible in limited space. Then we show our results on
the three tasks and conclude. Note we will assume
familiarity with the task, and refer the reader to the
shared task overview paper for more details.
2 Biomedical Event Extraction
Our goal is to extract biomedical events as shown
in figure 1a). To formulate the search for such
structures as an optimization problem, we represent
structures through a set of binary variables. Our rep-
resentation is inspired by previous work (Riedel et
al., 2009; Bj?rne et al, 2009) and based on a projec-
tion of events to a labelled graph over tokens in the
46
... phosphorylation of TRAF2 inhibits binding to the CD40 ...
Phosphorylation
Regulation
Binding
Theme
Cause
Theme
Theme
Theme
Regulation BindingPhosphorylation
Theme
Cause
Theme
Theme
Theme
Same Binding
 2 3
4 5 6 7 8 9
b
4,9
e
2,Phos.
a
6,9,Theme
(a)
(b)
Figure 1: (a) sentence with target event structure; (b) pro-
jection to labelled graph.
sentence, as seen figure 1b).
We will first present some basic notation to sim-
plify our exposition. For each sentence x we have
a set candidate trigger words Trig (x), and a set of
candidate proteins Prot (x). We will generally use
the indices i and l to denote members of Trig (x), the
indices p, q for members of Prot (x) and the index j
for members of Cand (x) def= Trig (x) ? Prot (x).
We label each candidate trigger i with an event
Type t ? T (with None ? T ), and use the binary
variable ei,t to indicate this labeling. We use binary
variables ai,l,r to indicate that between i and l there
is an edge labelled r ? R (with None ? R).
The representation so far has been used in previ-
ous work (Riedel et al, 2009; Bj?rne et al, 2009).
Its shortcoming is that it does not capture whether
two proteins are arguments of the same binding
event, or arguments of two binding events with the
same trigger. To overcome this problem, we intro-
duce binary ?same Binding? variables bp,q that are
active whenever there is a binding event that has
both p and q as arguments. Our inference algorithm
will also need, for each trigger i and protein pair p, q,
a binary variable ti,p,q that indicates that at i there is
a binding event with arguments p and q. All ti,p,q are
summarized in t.
Constructing events from solutions (e,a,b) can
be done almost exactly as described by Bj?rne et al
(2009). However, while Bj?rne et al (2009) group
arguments according to ad-hoc rules based on de-
pendency paths from trigger to argument, we simply
query the variables bp,q.
3 Model
We use the following objective to score the struc-
tures we like to extract:
s (e,a,b) def=
?
ei,t=1
sT (i, t) +
?
ai,j,r=1
sR (i, j, r)+
?
bp,q=1
sB (p, q)
with local scoring functions sT (i, t)
def=
?wT, fT (i, t)?, sR (i, j, r)
def= ?wR, fR (i, j, r)?
and sB (p, q)
def= ?wB, fB (p, q)?.
Our model scores all parts of the structure in isola-
tion. It is a joint model due to the three types of con-
straints we enforce. The first type acts on trigger la-
bels and their outgoing edges. It includes constraints
such as ?an active label at trigger i requires at least
one active outgoing Theme argument?. The second
type enforces consistency between trigger labels and
their incoming edges. That is, if an incoming edge
has a label that is not None, the trigger must not be
labelled None either. The third type of constraints
ensures that when two proteins p and q are part of
the same binding (as indicated by bp,q = 1), there
needs to be a binding event at some trigger i that
has p and q as arguments. We will denote the set of
structures (e,a,b) that satisfy all above constraints
as Y .
To learn w we choose the passive-aggressive
online learning algorithm (Crammer and Singer,
2003). As loss function we apply a weighted sum of
false positives and false negative labels and edges.
The weighting scheme penalizes false negatives 3.8
times more than false positives.
3.1 Features
For feature vector fT (i, t) we use a collection of
representations for the token i: word-form, lemma,
POS tag, syntactic heads, syntactic children; mem-
bership in two dictionaries used by Riedel et al
(2009).For fR (a; i, j, r) we use representations of
the token pair (i, j) inspired by Miwa et al (2010) .
They contain: labelled and unlabeled n-gram depen-
dency paths; edge and vertex walk features (Miwa et
al., 2010), argument and trigger modifiers and heads,
words in between (for close distance i and j). For
fB (b; p, q) we use a small subset of the token pair
representations in fR.
47
Algorithm 1 Dual Decomposition.
require:
R: max. iteration, ?t: stepsizes
t? 0 ?? 0?? 0
repeat
(e?, a?)? bestIncoming (??)
(e,a)? bestOutgoing (cout (?,?))
(b, t)? bestBinding
(
cbind (?)
)
?i,t ? ?i,t ? ?t (ei,t ? e?i,t)
?i,j,r ? ?i,j,r ? ?t (ai,j,r ? a?i,j,r)
?trigi,j,k ?
[
?trigi,j,k ? ?t (ei,Bind ? ti,j,k)
]
+
?arg1i,j,k ?
[
?arg1i,j,k ? ?t (ai,j,Theme ? ti,j,k)
]
+
?arg2i,j,k ?
[
?arg2i,j,k ? ?t (ai,k,Theme ? ti,j,k)
]
+
t ? t + 1
until no ?, ? changed or t > R
return(e,a,b)
3.2 Inference
Inference in our model amounts to solving
arg max
(e,a,b)?Y
s (e,a,b) . (1)
Our approach to finding the maximizer is dual de-
composition (Komodakis et al, 2007; Rush et al,
2010), a technique that allows us to exploit effi-
cient search algorithms for tractable substructures
of our problem. We divide the problem into three
sub-problems: (1) finding the highest-scoring trig-
ger labels and edges (e,a) such that constraints on
triggers and their outgoing edges are fulfilled; (2)
finding the highest-scoring trigger labels and edges
(e?, a?) such that constraints on triggers and their in-
coming edges are fulfilled; (3) finding the highest-
scoring pairs of proteins b to appear in the same
binding, and make binding event trigger decisions
t for these. Due to space constraints we only state
that the first two problems can be solved exactly in
O
(
n2 + nm
)
time while the last needs O
(
m2n
)
.
Here n is the number of trigger candidates and m
the number of proteins.
The subroutines to solve these three sub-problems
are combined in algorithm 1?an instantiation of
subgradient descent on the dual of an LP relaxation
of problem 1. In the first three steps in the main
loop of this algorithm, the individual sub-problems
are solved. Note that to each subroutine a parame-
ter is passed. For example, when finding the struc-
ture (e?, a?) that maximizes the objective under the
incoming edge constraints, we pass the parameter
??. This parameter represents a set of penalties to
be added to the objective used for the subproblem.
In this case we have penalties ??i,e to be added to
the scores of trigger-label pairs (i, e), and penalties
??i,j,r to be added for labelled edges i
r
? j.
One way to understand dual decomposition is as
iterative tuning of the penalties such that eventu-
ally all individual solutions are consistent with each
other. In our case this would mean, among other
things, that the solutions (e,a) and (e?, a?) are iden-
tical. This tuning happens in the second part of the
main loop which updates the dual variables? and?.
We see, for example, how the penalties ?i,e are de-
creased by ei,e? e?i,e scaled by a step-size ?t. Effec-
tively this change to ?i,e will decrease the score of
e?i,e within bestIn (??) by ?t if e?i,e was true while
ei,e was false in the current solutions.1 If e?i,e was
false but ei,e was true, the score is increased by ?t.
If both agree, no change is needed.
Consistency between solutions also means that
the binding decisions in b and t are consistent
with the rest of the solution. This is achieved in
algorithm 1 through tuning of the dual variables
? but we omit details for brevity. For complete-
ness we state how the penalties used for solving
the other subproblems are set based on the dual
variables ? and ?. We set couti,t (?,?)
def= ?i,t +
?t,Bind
?
p,q ?
trig
i,p,q; for the case that j ? Prot (x) we
get couti,j,r (?,?)
def= ?i,j,r +
?
p ?
arg1
i,j,p +
?
q ?
arg2
i,q,j ,
otherwise couti,j,r (?,?)
def= ?i,j,r . For bestBind (c)
we set cbindi,p,q (?) = ??
trig
i,p,q ? ?
arg1
i,,p,q ? ?
arg2
i,,p,q.
3.3 Preprocessing
After basic tokenization and sentence segmentation,
we generate a set of protein head tokens Prot (x)
for each sentence x based on protein span defi-
nitions from the shared task. To ensure tokens
contain not more than one protein we split them
at protein boundaries. Parsing is performed using
the Charniak-Johnson parser (Charniak and John-
son, 2005) with the self-trained biomedical parsing
1We refer to Koo et al (2010) for details on how to set ?t.
48
SVT BIND REG TOT
Task 1 73.5 48.8 43.8 55.2
Task 1 (abst.) 71.5 50.8 45.5 56.1
Task 1 (full) 79.2 44.4 40.1 53.1
Task 2 71.4 38.6 39.1 51.0
Table 1: Results for the GE track, task 1 and 2;
abst.=abstract; full=full text.
model of McClosky and Charniak (2008). Finally,
based on the set of trigger words in the training data,
we generate a set of candidate triggers Trig (x).
4 Results
We apply the same model to the GE, ID and EPI
tracks, with minor modifications in order to deal
with the different event type sets T and role sets R
of each track. Training and testing together took be-
tween 30 (EPI) to 120 (GE) minutes using a single-
core implementation.
4.1 Genia
Our results for GE task 1 and 2 can be seen in table
1. We also show results for abstracts only (abst.),
and for full text only (full). Note that binding events
(BIND) and general regulation events (REG) seem
to be harder to extract in full text. Somewhat surpris-
ingly, for simple events (SVT) the opposite holds.
We also like to point out that for full text extrac-
tion we rank first?the second best FAUST system
achieves an F1 score of 52.67.
4.2 Infectious Diseases
The Infectious Diseases track differs from the Genia
track in two important ways. First, it introduces the
event type Process that is allowed to have no ar-
guments at all. Second, it comes with significantly
less training data (152 vs 908 documents). We can
accommodate the first difference by making simple
changes in our inference algorithms. For example,
for Process events we do not force the algorithm to
pick a Theme argument.
To compensate for the lack of training data we
simply add data from the GE track. This is reason-
able because annotations overlap quite significantly.
In table 2 we show the impact of mixing different
amounts of ID data (I) and GE data (G) into the
training set. We point out that adding the ID training
I/G BIND REG PRO TOT
DEV 1/0 18.6 27.1 34.3 41.5
DEV 0/1 18.2 26.8 0.00 35.5
DEV 1/1 20.0 33.1 49.3 47.2
DEV 2/1 20.0 34.5 52.0 48.5
TEST 2/1 34.6 46.4 62.3 53.4
Table 2: ID results for different amounts of ID (I) and (G)
training data.
set twice, and the GENIA set once, leads to the best
performance (I/G=2/1). Remarkably, the F1 score
for Process increases by including data, although
this data does not include any such events. This may
stem from a shared model of None arguments that is
improved with more data.
4.3 Epigenetics and Post-translational
Modifications
For this track a different set of events is to be pre-
dicted. However, it is straightforward to adapt our
model and algorithms to this setting. For brevity we
only report our total results here and omit a table
with details. The first metric (ALL) includes nega-
tion, speculation and cellular location targets. We
omitted these in our model and hence our result of
33.52 F1 is relatively weak. For the metric that ne-
glects these aspects (CORE), we achieve 64.15 F1
and come in 4th. Note that in this metric the FAUST
system, based on the model presented here, comes
in as very close second.
5 Conclusion
We have presented a robust joint model for event
extraction from biomedical text that performs well
across all tasks. Remarkably, no feature set or pa-
rameter tuning was necessary to achieve this. We
also show substantial improvements for the ID task
by adding GENIA data into the training set.
Acknowledgements
This work was supported in part by the Center for Intelli-
gent Information Retrieval. The University of Massachusetts
gratefully acknowledges the support of Defense Advanced Re-
search Projects Agency (DARPA) Machine Reading Program
under Air Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0181. Any opinions, findings, and conclu-
sion or recommendations expressed in this material are those
of the authors and do not necessarily reflect the view of the
DARPA, AFRL, or the US government.
49
References
Jari Bj?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings of the Natural Language
Processing in Biomedicine NAACL 2009 Workshop
(BioNLP ?09), pages 10?18, Morristown, NJ, USA.
Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL ?05),
pages 173?180.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951?991.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview
of BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Nikos Komodakis, Nikos Paragios, and Georgios Tziri-
tas. 2007. Mrf optimization via dual decomposition:
Message-passing revisited. In In ICCV.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decompo-
sition for parsing with non-projective head automata.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP).
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of the
46rd Annual Meeting of the Association for Computa-
tional Linguistics (ACL ?08).
David McClosky, Mihai Surdeanu, and Chris Manning.
2011a. Event extraction as dependency parsing. In
Proceedings of the Association for Computational Lin-
guistics: Human Language Technologies 2011 Con-
ference (ACL-HLT?11), Main Conference (to appear),
Portland, Oregon, June.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011b. Event extraction as dependency
parsing in BioNLP 2011. In BioNLP 2011 Shared
Task.
Makoto Miwa, Rune Saetre, Jin-Dong D. Kim, and
Jun?ichi Tsujii. 2010. Event extraction with com-
plex event classification using rich features. Journal of
bioinformatics and computational biology, 8(1):131?
146, February.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Hoifung Poon and Lucy Vanderwende. 2010. Joint Infer-
ence for Knowledge Extraction from Biomedical Lit-
erature. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
813?821, Los Angeles, California, June. Association
for Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A markov logic approach to
bio-molecular event extraction. In Proceedings of the
Natural Language Processing in Biomedicine NAACL
2009 Workshop (BioNLP ?09), pages 41?49.
Sebastian Riedel, David McClosky, Mihai Surdeanu,
Christopher D. Manning, and Andrew McCallum.
2011. Model combination for event extraction in
BioNLP 2011. In BioNLP 2011 Shared Task.
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In In Proc. EMNLP.
50
Proceedings of BioNLP Shared Task 2011 Workshop, pages 51?55,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Model Combination for Event Extraction in BioNLP 2011
Sebastian Riedela, David McCloskyb, Mihai Surdeanub,
Andrew McCalluma, and Christopher D. Manningb
a Department of Computer Science, University of Massachusetts at Amherst
b Department of Computer Science, Stanford University
{riedel,mccallum}@cs.umass.edu
{mcclosky,mihais,manning}@stanford.edu
Abstract
We describe the FAUST entry to the BioNLP
2011 shared task on biomolecular event ex-
traction. The FAUST system explores sev-
eral stacking models for combination using
as base models the UMass dual decomposi-
tion (Riedel and McCallum, 2011) and Stan-
ford event parsing (McClosky et al, 2011b)
approaches. We show that using stacking is
a straightforward way to improving perfor-
mance for event extraction and find that it is
most effective when using a small set of stack-
ing features and the base models use slightly
different representations of the input data. The
FAUST system obtained 1st place in three out
of four tasks: 1st place in Genia Task 1 (56.0%
f-score) and Task 2 (53.9%), 2nd place in the
Epigenetics and Post-translational Modifica-
tions track (35.0%), and 1st place in the In-
fectious Diseases track (55.6%).
1 Introduction
To date, most approaches to the BioNLP event ex-
traction task (Kim et al, 2011a) use a single model
to produce their output. However, model combina-
tion techniques such as voting, stacking, and rerank-
ing have been shown to consistently produce higher
performing systems by taking advantage of multi-
ple views of the same data. The Netflix Prize (Ben-
nett et al, 2007) is a prime example of this. System
combination essentially allows systems to regular-
ize each other, smoothing over the artifacts of each
(c.f. Nivre and McDonald (2008), Surdeanu and
Manning (2010)). To our knowledge, the only previ-
ous example of model combination for the BioNLP
shared task was performed by Kim et al (2009). Us-
ing a weighted voting scheme to combine the out-
puts from the top six systems, they obtained a 4%
absolute f-score improvement over the best individ-
ual system.
This paper shows that using a straightforward
model combination strategy on two competitive
systems produces a new system with substantially
higher accuracy. This is achieved with the frame-
work of stacking: a stacking model uses the output
of a stacked model as additional features.
While we initially considered voting and rerank-
ing model combination strategies, it seemed that
given the performance gap between the UMass and
Stanford systems that the best option was to in-
clude the predictions from the Stanford system into
the UMass system (e.g., as in Nivre and McDon-
ald (2008)). This has the advantage that one model
(Umass) determines how to integrate the outputs of
the other model (Stanford) into its own structure,
whereas in reranking, for example, the combined
model is required to output a complete structure pro-
duced by only one of the input models.
2 Approach
In the following we briefly present both the stacking
and the stacked model and some possible ways of
integrating the stacked information.
2.1 Stacking Model
As our stacking model, we employ the UMass ex-
tractor (Riedel and McCallum, 2011). It is based on
a discriminatively trained model that jointly predicts
trigger labels, event arguments and protein pairs in
51
binding. We will briefly describe this model but first
introduce three types of binary variables that will
represent events in a given sentence. Variables ei,t
are active if and only if the token at position i has
the label t. Variables ai,j,r are active if and only if
there is an event with trigger i that has an argument
with role r grounded at token j. In the case of an
entity mention this means that the mention?s head is
j. In the case of an event j is the position of its trig-
ger. Finally, variables bp,q indicate whether or not
two entity mentions at p and q appear as arguments
in the same binding event.
Two parts form our model: a scoring function, and
a set of constraints. The scoring function over the
trigger variables e, argument variables a and binding
pair variables b is
s (e,a,b) def=
?
ei,t=1
sT (i, t) +
?
ai,j,r=1
sR (i, j, r)+
?
bp,q=1
sB (p, q)
with local scoring functions sT (i, t)
def=
?wT, fT (i, t)?, sR (i, j, r)
def= ?wR, fR (i, j, r)?
and sB (p, q)
def= ?wB, fB (p, q)?.
Our model scores all parts of the structure in iso-
lation. It is a joint model due to the nature of the
constraints we enforce: First, we require that each
active event trigger must have at least one Theme ar-
gument; second, only regulation events (or Catalysis
events for the EPI track) are allowed to have Cause
arguments; third, any trigger that is itself an argu-
ment of another event has to be labelled active, too;
finally, if we decide that two entities p and q are part
of the same binding (as indicated by bp,q = 1), there
needs to be a binding event at some trigger i that
has p and q as arguments. We will denote the set of
structures (e,a,b) that satisfy these constraints as
Y .
Stacking with this model is simple: we only
need to augment the local feature functions fT (i, t),
fR (i, j, r) and fB (p, q) to include predictions from
the systems to be stacked. For example, for every
system S to be stacked and every pair of event types
(t?, tS) we add the features
fS,t? ,tS (i, t) =
{
1 hS (i) = tS ? t? = t
0 otherwise
to fT (i, t). Here hS (i) is the event label given to to-
ken i according to S. These features allow different
weights to be given to each possible combination of
type t? that we want to assign, and type tS that S
predicts.
Inference in this model amounts to maximizing
s (e,a,b) over Y . Our approach to solving this
problem is dual decomposition (Komodakis et al,
2007; Rush et al, 2010). We divide the problem into
three subproblems: (1) finding the best trigger label
and set of outgoing edges for each candidate trigger;
(2) finding the best trigger label and set of incoming
edges for each candidate trigger; (3) finding the best
pairs of entities to appear in the same binding. Due
to space limitations we refer the reader to Riedel and
McCallum (2011) for further details.
2.2 Stacked Model
For the stacked model, we use a system based on an
event parsing framework (McClosky et al, 2011a)
referred to as the Stanford model in this paper. This
model converts event structures to dependency trees
which are parsed using MSTParser (McDonald et
al., 2005).1 Once parsed, the resulting dependency
tree is converted back to event structures. Using the
Stanford model as the stacked model is helpful since
it captures tree structure which is not the focus in
the UMass model. Of course, this is also a limita-
tion since actual BioNLP event graphs are DAGs,
but the model does well considering these restric-
tions. Additionally, this constraint encourages the
Stanford model to provide different (and thus more
useful for stacking) results.
Of particular interest to this paper are the four
possible decoders in MSTParser. These four de-
coders come from combinations of feature order
(first or second) and whether the resulting depen-
dency tree is required to be projective.2 Each de-
coder presents a slightly different view of the data
and thus has different model combination proper-
ties. Projectivity constraints are not captured in the
UMass model so these decoders incorporate novel
information.
To produce stacking output from the Stanford sys-
tem, we need its predictions on the training, devel-
1http://sourceforge.net/projects/mstparser/
2For brevity, the second-order non-projective decoder is ab-
breviated as 2N, first-order projective as 1P, etc.
52
UMass FAUST+All
R P F1 R P F1
GE T1 48.5 64.1 55.2 49.4 64.8 56.0
GE T2 43.9 60.9 51.0 46.7 63.8 53.9
EPI (F) 28.1 41.6 33.5 28.9 44.5 35.0
EPI (C) 57.0 73.3 64.2 59.9 80.3 68.6
ID (F) 46.9 62.0 53.4 48.0 66.0 55.6
ID (C) 49.5 62.1 55.1 50.6 66.1 57.3
Table 1: Results on test sets of all tasks we submitted to.
T1 and T2 stand for task 1 and 2, respectively. C stands
for CORE metric, F for FULL metric.
opment and test sets. For predictions on test and de-
velopment sets we used models learned from the the
complete training set. Predictions over training data
were produced using crossvalidation. This helps to
avoid a scenario where the stacking model learns to
rely on high accuracy at training time that cannot be
matched at test time.
Note that, unlike Stanford?s individual submission
in this shared task, the stacked models in this paper
do not include the Stanford reranker. This is because
it would have required making a reranker model for
each crossvalidation fold.
We made 19 crossvalidation training folds for Ge-
nia (GE) (Kim et al, 2011b), 12 for Epigenetics
(EPI), and 17 for Infectious Diseases (ID) (Kim et
al., 2011b; Ohta et al, 2011; Pyysalo et al, 2011,
respectively). Note that while ID is the smallest and
would seem like it would have the fewest folds, we
combined the training data of ID with the training
and development data from GE. To produce predic-
tions over the test data, we combined the training
folds with 6 development folds for GE, 4 for EPI,
and 1 for ID.
3 Experiments
Table 1 gives an overview of our results on the test
sets for all four tasks we submitted to. Note that
for the EPI and ID tasks we show the CORE metric
next to the official FULL metric. The former is suit-
able for our purposes because it does not measure
performance for negations, speculations and cellular
locations?all of these we did not attempt to predict.
We compare the UMass standalone system to the
FAUST+All system which stacks the Stanford 1N,
1P, 2N and 2P predictions. For all four tasks we
System SVT BIND REG TOTAL
UMass 74.7 47.7 42.8 54.8
Stanford 1N 71.4 38.6 32.8 47.8
Stanford 1P 70.8 35.9 31.1 46.5
Stanford 2N 69.1 35.0 27.8 44.3
Stanford 2P 72.0 36.2 32.2 47.4
FAUST+All 76.9 43.5 44.0 55.9
FAUST+1N 76.4 45.1 43.8 55.6
FAUST+1P 75.8 43.1 44.6 55.7
FAUST+2N 74.9 42.8 43.8 54.9
FAUST+2P 75.7 46.0 44.1 55.7
FAUST+All 76.4 41.2 43.1 54.9
(triggers)
FAUST+All 76.1 41.7 43.6 55.1
(arguments)
Table 2: BioNLP f-scores on the development section of
the Genia track (task 1) for several event categories.
observe substantial improvements due to stacking.
The increase is particular striking for the EPI track,
where stacking improves f-score by more than 4.0
points on the CORE metric.
To analyze the impact of stacking further, Ta-
ble 2 shows a breakdown of our results on the Ge-
nia development set. Presented are f-scores for sim-
ple events (SVT), binding events (BIND), regulation
events (REG) and the set of all event types (TOTAL).
We compare the UMass standalone system, various
Stanford-standalone models and stacked versions of
these (FAUST+X).
Remarkably, while there is a 7 point gap between
the best individual Stanford system and the stand-
alone UMass systems, integrating the Stanford pre-
diction still leads to an f-score improvement of 1.
This can be seen when comparing the UMass, Stan-
ford 1N and FAUST+All results, where the latter
stacks 1N, 1P, 2N and 2P. We also note that stack-
ing the projective 1P and 2P systems helps almost
as much as stacking all Stanford systems. Notably,
both 1P and 2P do not do as well in isolation when
compared to the 1N system. When stacked, how-
ever, they do slightly better. This suggests that pro-
jectivity is a missing aspect in the UMass standalone
system.
The FAUST+All (triggers) and FAUST+All (ar-
guments) lines represent experiments to determine
whether it is useful to incorporate only portions of
53
the stacking information from the Stanford system.
Given the small gains over the original UMass sys-
tem, it is clear that stacking information is only use-
ful when attached to triggers and arguments. Our
theory is that most of our gains come from when the
UMass and Stanford systems disagree on triggers
and the Stanford system provides not only its trig-
gers but also their attached arguments to the UMass
system. This is supported by a pilot experiment
where we trained the Stanford model to use the
UMass triggers and saw no benefit from stacking
(even when both triggers and arguments were used).
Table 3 shows our results on the development set
of the ID task, this time in terms of recall, precision
and f-score. Here the gap between Stanford-only
results, and the UMass results, is much smaller. This
seems to lead to more substantial improvements for
stacking: FAUST+All obtains a f-score 2.2 points
larger than the standalone UMass system. Also note
that, similarly to the previous table, the projective
systems do worse on their own, but are more useful
when stacked.
Another possible approach to stacking conjoins
all the original features of the stacking model with
the predicted features of the stacked model. The
hope is that this allows the learner to give differ-
ent weights to the stacked predictions in different
contexts. However, incorporating Stanford predic-
tions by conjoining them with all features of the
UMass standalone system (FAUST+2P-Conj in Ta-
ble 3) does not help here.
We note that for our results on the ID task we
augment the training data with events from the GE
training set. Merging both training sets is reasonable
since there is a significant overlap between both in
terms of events as well as lexical and syntactic pat-
terns to express these. When building our training
set we add each training document from GE once,
and each ID training document twice?this lead to
substantially better results than including ID data
only once.
4 Discussion
Generally stacking has led to substantial improve-
ments across the board. There are, however, some
exceptions. One is binding events for the GE task.
Here the UMass model still outperforms the best
System Rec Prec F1
UMass 46.2 51.1 48.5
Stanford 1N 43.1 49.1 45.9
Stanford 1P 40.8 46.7 43.5
Stanford 2N 41.6 53.9 46.9
Stanford 2P 42.8 48.1 45.3
FAUST+All 47.6 54.3 50.7
FAUST+1N 45.8 51.6 48.5
FAUST+1P 47.6 52.8 50.0
FAUST+2N 45.4 52.4 48.6
FAUST+2P 49.1 52.6 50.7
FAUST+2P-Conj 48.0 53.2 50.4
Table 3: Results on the development set for the ID track.
stacked system (see Table 2). Likewise, for full pa-
pers in the Genia test set, the UMass model still does
slightly better with 53.1 f-score compared to 52.7
f-score. This suggests that a more informed com-
bination of our systems (e.g., metaclassifiers) could
lead to better performance.
5 Conclusion
We have presented the FAUST entry to the BioNLP
2011 shared task on biomolecular event extraction.
It is based on stacking, a simple approach for model
combination. By using the predictions of the Stan-
ford entry as features of the UMass model, we sub-
stantially improved upon both systems in isolation.
This helped us to rank 1st in three of the four tasks
we submitted results to. Remarkably, in some cases
we observed improvements despite a 7.0 f-score
margin between the models we combined.
In the future we would like to investigate alter-
native means for model combination such as rerank-
ing, union, intersection, and other voting techniques.
We also plan to use dual decomposition to encourage
models to agree. In particular, we will seek to incor-
porate an MST component into the dual decomposi-
tion algorithm used by the UMass system.
Acknowledgments
We thank the BioNLP shared task organizers for setting this
up and their quick responses to questions. This work was sup-
ported in part by the Center for Intelligent Information Re-
trieval. We gratefully acknowledge the support of the Defense
Advanced Research Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181.
54
References
James Bennett, Stan Lanning, and Netflix. 2007. The
netflix prize. In KDD Cup and Workshop in conjunc-
tion with KDD.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of the Workshop on BioNLP: Shared Task,
pages 1?9. Association for Computational Linguistics.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview
of BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Nikos Komodakis, Nikos Paragios, and Georgios Tziri-
tas. 2007. MRF optimization via dual decomposition:
Message-passing revisited. In ICCV.
David McClosky, Mihai Surdeanu, and Chris Manning.
2011a. Event extraction as dependency parsing. In
Proceedings of the Association for Computational Lin-
guistics: Human Language Technologies 2011 Confer-
ence (ACL-HLT?11), Main Conference, Portland, Ore-
gon, June.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011b. Event extraction as dependency
parsing in BioNLP 2011. In BioNLP 2011 Shared
Task.
Ryan T. McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT/EMNLP. The Association for Computational
Linguistics.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT, pages 950?958,
Columbus, Ohio, June. Association for Computational
Linguistics.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sebastian Riedel and Andrew McCallum. 2011. Ro-
bust biomedical event extraction with dual decomposi-
tion and minimal domain adaptation. In BioNLP 2011
Shared Task.
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proc. EMNLP.
Mihai Surdeanu and Christopher D. Manning. 2010. En-
semble models for dependency parsing: Cheap and
good? In Proceedings of the North American Chapter
of the Association for Computational Linguistics Con-
ference (NAACL-2010), Los Angeles, CA, June.
55
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 153?162,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Dynamic Knowledge-Base Alignment for Coreference Resolution
Jiaping Zheng Luke Vilnis Sameer Singh Jinho D. Choi Andrew McCallum
School of Computer Science
University of Massachusetts
Amherst MA 01003
{jzheng,luke,sameer,jdchoi,mccallum}@cs.umass.edu
Abstract
Coreference resolution systems can benefit
greatly from inclusion of global context,
and a number of recent approaches have
demonstrated improvements when precom-
puting an alignment to external knowledge
sources. However, since alignment itself
is a challenging task and is often noisy, ex-
isting systems either align conservatively,
resulting in very few links, or combine the
attributes of multiple candidates, leading
to a conflation of entities. Our approach
instead performs joint inference between
within-document coreference and entity
linking, maintaining ranked lists of candi-
date entities that are dynamically merged
and reranked during inference. Further, we
incorporate a large set of surface string vari-
ations for each entity by using anchor texts
from the web that link to the entity. These
forms of global context enables our system
to improve classifier-based coreference by
1.09 B3 F1 points, and improve over the
previous state-of-art by 0.41 points, thus
introducing a new state-of-art result on the
ACE 2004 data.
1 Introduction
Coreference resolution is the task of identifying
sets of noun phrase mentions from a document
that refer to the same real-world entities. For ex-
ample, in the following excerpt: ?The Chicago
suburb of Arlington Heights is the first stop for
?George W. Bush?1 today. ?The Texas governor?2
stops in ?Gore?s home state?3 of ?Tennessee?4 this
afternoon. . . ?, (m1,m2) and (m3,m4) define the
coreferent pairs. Coreference resolution forms an
important component for natural language process-
ing and information extraction pipelines due to its
utility in relation extraction, cross-document coref-
erence, text summarization, and question answer-
ing. The task of coreference is challenging for
automated systems as the local information con-
tained in the document is often not enough to accu-
rately disambiguate mentions, for example, corefer-
encing (m1,m2) requires identifying that George
W. Bush (m1) is the governor of Texas (m2), and
similarly for (m3,m4). External knowledge-bases
such as FrameNet (Baker et al, 1998), Wikipedia,
Yago (Suchanek et al, 2007), and Freebase (Bol-
lacker et al, 2008), can be used to provide global
context, and there is a strong need for coreference
resolution systems to accurately use such sources
for disambiguation.
Incorporating external knowledge bases into
coreference has been the subject of active recent
research. Ponzetto and Strube (2006) and Ratinov
and Roth (2012) precompute a fixed alignment of
the mentions to the knowledge base entities. The
attributes of these entities are used during corefer-
ence by incorporating them in the mention features.
Since alignment of mentions to the external enti-
ties is itself a difficult task, these systems favor
high-precision linking. Unfortunately, this results
in fewer alignments, and improvements are only
shown on mentions that are easier to align and core-
fer (such as the non-transcript documents in Rati-
nov and Roth (2012)). Alternatively, Rahman and
Ng (2011) link each mention to multiple entities in
the knowledge base, improving recall at the cost
of lower precision; the attributes of all the linked
entities are aggregated as features. Although this
approach is more robust to noise in the documents,
the features of a mention merge the different as-
pects of the entities, for example a ?Michael Jordan?
mention will contain features for both the scientist
and basketball personas.
Instead of fixing the alignment of the mentions to
the knowledge base, our proposed approach main-
tains a ranked list of candidate entities for each
mention. To expand the set of surface strings that
153
may be used to refer to each entity, the attributes
of each candidate contain anchor texts (the visible
text) of the links on the web that refer to that entity
candidate. When mentions are compared during
inference, we use the features computed from the
top ranked entity candidate of the antecedent men-
tion. As mentions are merged, the ranked lists of
candidate entities are also merged and reranked, of-
ten changing the top-ranked entity candidate used
in subsequent comparisons. The large set of sur-
face string variations and constant reranking of the
entity candidates during inference allows our ap-
proach to correct mistakes in alignment and makes
external information applicable to a wider variety
of mentions.
Our paper provides the following contributions:
(1) an approach that jointly reasons about both
within-doc entities and their alignment to KB-
entities by dynamically adjusting a ranked list of
candidate alignments, during coreference, (2) Uti-
lization of a larger set of surface string variations
for each entity candidate by using links that appear
all over the web (Spitkovsky and Chang, 2012), (3)
A combination of these approaches that improves
upon a competitive baseline without a knowledge
base by 1.09 B3 F1 points on the ACE 2004 data,
and outperforms the state-of-the-art coreference
system (Stoyanov and Eisner, 2012) by 0.41 B3
F1 points, and (4) Accurate predictions on docu-
ments that are difficult for coreference, such as the
transcript documents that were omitted from the
evaluation in Ratinov and Roth (2012), and docu-
ments that contain a large number of mentions.
2 Baseline Pairwise System
In this section we describe a variant of a commonly-
used coreference resolution system that does not
utilize external knowledge sources. This widely
adopted model casts the problem as a series of
binary classifications (Soon et al, 2001; Ng and
Cardie, 2002; Ponzetto and Strube, 2006; Bengston
and Roth, 2008; Stoyanov et al, 2010). Given
a document with its mentions, the system itera-
tively checks each mention mj for coreference with
preceding mentions using a classifier. A corefer-
ence link may be created between mj and one of
these preceding mentions using one of the follow-
ing strategies. The CLOSESTLINK (Soon et al,
2001) method picks the closest mention to mj that
is positively classified, while the BESTLINK (Ng
and Cardie, 2002) method links mj to the preced-
Types Features
String-
Similarity
mention string match, head string match,
head substring match, head word pair, men-
tion substring match, acronym
Syntax number match, gender match, apposition,
relative pronoun, mention type, modifier
match, head word POS tags
Semantic synonym, antonym, hypernym, modifier re-
lations, both mentions are surrounded by a
verb meaning ?to say?, demonym match
Other predicted entity type, predicted entity type
match, both mentions in same sentence, sen-
tence/token distance, capitalization
Table 1: Features of the baseline model. Extensions
to Bengston and Roth (2008) are italicized.
ing mention that was scored the highest. If none
of the preceding mentions are classified as positive
(for CLOSESTLINK), or are above a threshold (for
BESTLINK), then mj is left unlinked. After all the
mentions have been processed, the links are used
to generate a transitive closure that corresponds to
the recognized entities in the document.
2.1 Pairwise Mention Features
The features used to train our classifier are similar
to those in Bengston and Roth (2008), including
lexical, syntactical, semantic, predicted NER types,
etc., with the exclusion of their ?learned features?
that require additional classifiers. Further, we in-
clude features that compare the mention strings, the
distance between the two mentions in terms of the
number of sentences and tokens, and the POS tags
of the head words. We also use the conjunctions of
these features as in Bengston and Roth (2008), as
well as the BESTLINK approach. The complete set
of features are listed in Table 1.
The training for our system is similar
to Bengston and Roth (2008). The positive train-
ing examples are generated from mentions and
their immediate preceding antecedent. The neg-
ative examples are generated from mentions and
all their preceding non-coreferent mentions. If the
mention is not a pronoun, preceding pronouns are
not used to create training examples, and they are
also excluded during inference. In contrast to aver-
aged perceptron used in Bengston and Roth (2008),
our baseline system is trained using hinge-loss, `2-
regularized SVM.
2.2 Merging Pairwise Features
When a mention mj is compared against a preced-
ing mention mi, information from other mentions
154
that are already coreferent with mi may be helpful
in disambiguating mj as they may contain infor-
mation that is not available from mi. Let M be
the mentions between mi and mj that are coref-
erent with mi. Let mq ? M be the mention that
is closest to mj . All the features from the pair
(mq,mj), except those that characterize one men-
tion (for example, mention type of mj), are added
to the features between (mi,mj). This extends a
similar approach by Lee et al (2011) that merges
only the attributes of mentions (such as gender, but
not all pairwise features).
2.3 Pruning Comparisons During Training
A potential drawback of including all the negative
examples as in Bengston and Roth (2008) is that
the negative instances far outnumber the positive
ones, which is challenging for training a classifier.
In their system, the positive training examples only
constitute 1.6% of the total training instances. By
contrast, Soon et al (2001) reduce the number of
negative instances by using only mentions between
the mention and its closest coreferent pair as neg-
ative examples. Instead of just using the closest
coreferent mention, we extend this approach to
use the k closest of coreferent preceding mentions,
where k is tuned using the development data.
3 Dynamic Linking to Knowledge-Base
In this section, we describe our approach to coref-
erence resolution that incorporates external knowl-
edge sources. The approach is an extension of the
pairwise model described earlier, with the inclusion
of a ranked list of entities, and using a larger set of
surface string variations.
3.1 Algorithm
We describe our overall approach in Algorithm 1.
The system assumes that the data is annotated with
true mention boundaries and mention types. We
additionally tokenize the document text and tag the
tokens with their parts of speech for use as features.
First, an empty entity candidate list is created for
each mention in the document. For each proper
noun mention, we query a knowledge base for an
ordered list of Wikipedia articles that may refer
to it, and add these to the mention?s candidate list.
Other mentions? candidates lists are left empty.
After this pre-processing, each mention mi
is compared against its preceding mentions
m1 . . .mi?1 and their top-ranked entity candi-
Algorithm 1 Dynamic Linking to Wikipedia
1: Input: Mentions {mj}
2: Initialize blank entity lists {Em} . Section 3.2
3: for m ? Proper Noun Mentions do
4: LINKWIKIPEDIA(m, Em) . Section 3.2
5: POPULATEENTITYATTRS(Em) . Section 3.3
6: end for
7: for mi ?Mentions do
8: Antecedents? {m1...mi?1}
9: for m? ? Antecedents do
10: t? TOPRANKEDATTRS(Em?) . Section 3.4
11: s? SCORE(m?, mi, t) . Section 3.4
12: Scoresm?? s
13: end for
14: m? ? argmaxm? Scoresm?15: if Scoresm? > threshold then
16: MARKCOREFERENT(m?, mi)
17: MERGEENTITYLISTS(Em? , Emi ) . Section 3.418: end if
19: end for
20: return Coreferent mention clusters
date using a classifier. Amongst antecedents
m1 . . .mi?1 that score above a threshold, the
highest-scoring one mj is marked as coreferent
with mi and the two candidate lists that correspond
to mi and mj are merged. Merging two mentions
results in the merging and reranking of their respec-
tive entity candidate lists, described below. If no
antecedents score above a threshold, we leave the
mention in its singleton cluster.
3.2 Linking to Wikipedia
To create the initial entity candidate lists for
proper noun mentions, we query a knowledge base
searcher (Dalton and Dietz, 2013) with the text
of these mentions. These queries return scored,
ranked lists of entity candidates (Wikipedia arti-
cles), which we associate with each proper noun
mention, leaving the rest of the candidate lists
empty. Linking is often noisy, so only selecting the
high-precision links as in Ratinov and Roth (2012)
results in too few matches, while picking an aggre-
gation of all links results in more noise due to lower
precision (Rahman and Ng, 2011). Additionally,
since linking is often performed in pre-processing,
two mentions that are determined coreferent dur-
ing inference could still be linked to different KB
entities. To avoid these problems, we keep a list of
candidate links for each mention, merging the lists
when two mentions are determined coreferent, and
rerank this list during inference.
3.3 Populating Entity Attributes
After linking to Wikipedia, we have a list of can-
didate KB entities for each mention. Each entity
155
has access to external information keyed on the
Wikipedia article, but this information could more
generally come from any knowledge base. Given
these entities, there are many possible features that
may be used for disambiguation of the mentions,
such as gender and fine-grained Wikipedia cate-
gories as used by Ratinov and Roth (2012), how-
ever most of these features may not be relevant to
the task of within-document coreference. Instead,
an important resource for linking non-proper men-
tions of an entity is to identify the possible name
variations of the entity. For example, it would be
useful to know that Massachusetts is also referred
to as ?The 6th State?, however this information is
not readily available from Wikipedia.1
We instead use the corpus described
in Spitkovsky and Chang (2012) that con-
sists of anchor texts of links to Wikipedia that
appear on web pages. This collection of anchor
texts is sufficiently extensive to cover many
common misspellings of entity names, as well as
many name variations missing from Wikipedia.
For example, for the entity ?Massachusetts?, our
anchor texts include misspellings like ?Massachus-
setts? and ?Messuchusetts?, and the (debatably)
affectionate nickname of ?Taxachusetts??none of
which are found in Wikipedia. Using these anchor
texts, each entity candidate provides a rich set of
name variations that we use for disambiguation, as
described in the next section.
3.4 Inference with Dynamic Linking
The input to our inference algorithm consists of a
number of mentions, a list of ranked entity candi-
dates for the proper noun mentions that are present
in the KB, and a list of attributes (in this case, name
variations) for each entity candidate.
Scoring: Our underlying model is a pairwise
classification approach as described in Section 2.
Similar to existing coreference systems such as
Bengston and Roth (2008) and Rahman and Ng
(2011), we perform coreference resolution using
greedy left-to-right pairwise mention classification,
clustering each mention with its highest-scoring
antecedent (or leaving it as a singleton temporarily
if no score is above a threshold). We add the same
additional features and perform feature merging
operation (Section 2.2) as in our baseline system.
1Some of this information is available as redirects and
from links within Wikipedia, however these do not accurately
reflect all the variations of the name.
The top-ranked entity candidate of the an-
tecedent mention is used during coreference to
provide additional features for the pairwise classi-
fier. Only using the top-ranked entity candidate al-
lows the system to maintain a consistent one entity
per cluster hypothesis, reducing the noise resulting
from conflated entities. The attributes for this top-
ranked entity consist of name variations. We add a
binary feature, and conjunctions of this with other
features, if the text of the right mention matches
one of these name variations.
Entity List Merging: Once a mention pair is
scored as coreferent, their corresponding entity can-
didates are merged. Merging is performed by sim-
ply combining the two lists of candidates. Note that
there is only one candidate list for a given group of
coreferent mentions at any point in inference: if m1
and m2 have been previously marked as coreferent,
and m3 is marked as coreferent with m2, m1?s en-
tity candidates will then contain those from m3 for
future classification decisions.
Re-Ranking: After the two entity candidate lists
are merged, we rerank the candidates to identify
the top-ranked one. We sort the new list of candi-
date entities by the number of times each candidate
occurs in the list, breaking ties by their original
relevance from the KB. For example, if two men-
tions disagree on the top-ranked KB search result,
but agree on the second one, after being clustered
they will both use the second search result when
creating feature vectors for future coreference de-
cisions. Even though other candidates besides the
top-ranked one are ignored for a single classifica-
tion decision, they may become top-ranked after
merging with later candidate sets.
This approach allows our system to use the inter-
mediate results of coreference resolution to re-link
mentions to KB entities, reducing the noise and
contradictory features from incorrect links. Addi-
tionally, features from the KB are added to non-
proper noun mentions once those mentions are
linked with a populated entity, allowing the results
of coreference to enrich non-proper noun mentions
with KB-based features. The initial proper noun
queries effectively seed the linking process, and
KB data is then dynamically spread to the other
mentions through coreference.
3.5 Example
We describe a run of our approach on an exam-
ple in Figure 1. Consider three mentions, each
156
?about navigation charts that he had 
ordered from a company based in the 
state of Washington. He assumed ?
?opened one of them to discover the 
absentee ballot of Steven H. Forrester 
of Bellevue, Wash?.
...were not meaningful because 
counting in Washington State has 
been completed...
(a) Example Excerpts with Mentions
Washington, DC
Washington State
...
Car Wash
The Wash
...
Washington State
Washington State
...
Washington
Wash
Washington 
State
(b) Initial Alignment (top-ranked in bold)
Washington State
Washington, DC
Car Wash
The Wash
...
Washington State
...
Washington
Wash
Washington 
State
(c) Merged and Reranked Alignment
Figure 1: Example of Dynamic Alignment
paired with a top-ranked KB candidate: ?Washing-
ton?, ?Wash?, and ?Washington State?. For the
first two mentions, clearly the top entity candidate
is incorrect; hence approaches that rely on a fixed
alignment will perform poorly. In particular, since
?Washington State? mention is not compatible with
the top-ranked entities of the first two mentions
(Washington, D.C. and Car Wash respectively), ap-
proaches that do not modify the ranking during
inference may not resolve them. However, the cor-
rect candidate Washington State does appear in the
candidate entities of the first two mentions, albeit
with a lower rank. In our approach, clustering
the first two mentions causes the shared candidate
Washington State to move to the top of the list. The
coreference system is now able to easily identify
that the ?Washington State? mention is compati-
ble with the Washington State entity formed by the
previous two mentions, providing evidence that the
final mention should be clustered with either of
them in subsequent comparisons.
4 Experiments
4.1 Setup
We evaluate our system on the ACE 2004 anno-
tated dataset (Doddington et al, 2004). Following
the setup in Bengston and Roth (2008), we split
the corpus into training, development, and test sets,
resulting in 268 documents in the train set, 107
documents in the test set, and 68 documents in the
development set. The data is processed using stan-
dard open source tools to segment the sentences
and tokenize the corpus, and using the OpenNLP2
tagger to obtain the POS tags. The hyperparame-
ters of our system, such as regularization, initial
number of candidates, and the number of compar-
2http://opennlp.apache.org/
isons during training (k in Section 2.3) are tuned
on the development data when trained on the train
set. The models we use to evaluate on the test data
set are trained on the training and development sets,
following the standard evaluation for coreference
first used by Culotta et al (2007).
To provide the initial ranked list of entity candi-
dates from Wikipedia, we query the KB Bridge sys-
tem (Dalton and Dietz, 2013) with the proper name
mentions. KB Bridge is an information-retrieval-
based entity linking system that connects the query
mentions to Wikipedia entities using a sequential
dependence model. This system has been shown to
match or outperform the top performing systems in
the 2012 TAC KBP entity linking task.
4.2 Methods
Our experiments investigate a number of baselines
that are similar or identical to existing approaches.
Wikipedia Linking: As a simple baseline, we
directly evaluate the quality of the alignment for
coreference by merging all pairs of proper noun
mentions that share at least one common candi-
date, as per KB bridge. Further, the non-pronoun
mentions are linked to these proper nouns if the
mention string matches any of the entity titles or
anchor texts.
Bengston and Roth (2008): A pairwise corefer-
ence model containing a rich set of features, as de-
scribed and evaluated in Bengston and Roth (2008).
Baseline: Our implementation of a pairwise
model that is similar to the approach in Bengston
and Roth (2008) with the differences described in
Section 2. This is our baseline system that performs
coreference without the use of external knowledge.
Incidentally, it outperforms Bengston and Roth
(2008).
Dynamic linking: This is our complete system as
157
described in Section 3, in which the list of candi-
dates associated with each mention is reranked and
modified during inference.
Static linking: Identical to dynamic linking ex-
cept that entity candidate lists are not merged dur-
ing inference (i.e., Algorithm 1 without line 17).
This approach is comparable to the fixed alignment
model, as in the approaches of Ponzetto and Strube
(2006) and Ratinov and Roth (2012).
4.3 Results
As in Bengston and Roth (2008), we evaluate our
system primarily using the B3 metric (Bagga and
Baldwin, 1998), but also include pairwise, MUC
and CEAF(m) metrics. The performance of our
systems on the test data set is shown in Table 2.
These results use true mentions provided in the
dataset, since, as suggested by Ng (2010), corefer-
ence resolvers that use different mention detectors
(extraction from parse tree, detector trained from
gold boundaries, etc.) should not be compared.
Our baseline system outperforms Bengston and
Roth (2008) by 0.32 B3 F1 points on this data set.
Incorporating Wikipedia and anchor text informa-
tion from the web with a fixed alignment (static
linking) further improves our performance by 0.54
B3 F1 points. Using dynamic linking, which im-
proves the alignment during inference, achieves
another 0.55 F1 point improvement, which is 1.09
F1 above our baseline, 1.41 F1 above the current
best pairwise classification system (corresponding
to an error reduction of 7.4%), and 0.4 F1 above the
current state-of-art on this dataset (Stoyanov and
Eisner, 2012). The improvement of the dynamic
linking approach over our baselines is consistent
across the various evaluation metrics.
5 Discussion
We also explore our system?s performance on sub-
sets of the ACE dataset, and on the OntoNotes
dataset.
5.1 Document Length
Coreference becomes more difficult as the number
of mentions is increased since the number of pair-
wise comparisons increases quadratically with the
number of mentions. We observe this phenomenon
in our dataset: the performance on the smallest
third of the documents (when sorted according to
number of mentions) is 8.5-10% higher than on the
largest third of the documents, as per the B3 metric.
55   10 15 20 25 30 35 40 45 50
   
   
0.6
0.8
1
1.2
1.4
1.6
1.8
Top X% of Docs by Number of Mentions
Im
pro
vem
en
t o
ver
 Ba
sel
ine
Dynamic Linking
Static Linking
Figure 2: Improvements on the top X% of docu-
ments ranked by the number of mentions.
Method Non-Transcripts Transcripts
Baseline 82.50 79.77
RR 2012 83.03 -
Static Linking 83.06 80.25
Dynamic Linking 83.32 81.13
Table 3: B3 F1 accuracy on transcripts and non-
transcripts from the ACE test data. RR 2012 only
evaluate on non-transcripts.
However, we expect dynamic linking of entities to
be more beneficial on these larger documents as
our system can use the information from a larger
number of mentions to improve the alignment dur-
ing inference. Static linking, on the other hand, is
unlikely to obtain higher improvements with the
larger number of mentions in the document as the
alignment is fixed.
We perform the following experiment to analyze
the performance with varying numbers of mentions.
We sort all the documents in the test set according
to their number of mentions, and evaluate on the top
X% of this list (where X is 10, 33, 40, 50). As the
results demonstrate in Figure 2, the improvement
of the static linking approach stays fairly even as
X is varied. Even though the experiments suggest
that the larger documents are tougher to corefer-
ence,3 dynamic linking provides higher improve-
ments when the documents contain a larger number
of mentions.
5.2 Performance on Transcripts
The quality of alignment and the coreference pre-
dictions for a document is influenced by the quality
of the mentions in the document. In particular,
3i.e., the absolute values are lower for these splits. The
baseline system obtains 83.08, 79.29, 79.64, and 79.77 respec-
tively for X = 10, 33, 40, 50.
158
Method Pairwise MUC CEAF B
3
P / R F1 P / R F1 P / R F1 P / R F1
Culotta et al (2007) - - - - - - 86.7 73.2 79.3
Raghunathan et al (2010) 71.6 46.2 56.1 80.4 71.8 75.8 - - 86.3 75.4 80.4
Stoyanov and Eisner (2012) - - - 80.1 - - - 81.8
Wiki-linking 64.15 14.99 24.30 74.41 28.39 41.10 58.54 58.4 58.47 92.89 57.21 70.81
Bengston and Roth (2008) - - 82.7 69.9 75.8 - - 88.3 74.5 80.8
Baseline 66.56 47.07 55.14 82.84 72.02 77.05 75.58 75.40 75.49 87.02 75.97 81.12
Static Linking 82.53 40.80 54.61 88.39 66.93 76.18 75.33 75.35 75.44 93.10 72.72 81.66
Dynamic Linking 72.20 47.40 57.23 85.07 72.02 78.01 76.55 76.37 76.46 89.37 76.12 82.21
Table 2: Evaluation on the ACE test data, with the system trained on the train and development sets.
   
   
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Im
pro
vem
en
t o
ver
 Ba
sel
ine
non-trans
non-trans     
transcripts
Static Linking Dynamic Linking
trans
Figure 3: Comparison on the transcripts data.
ACE contains a large number of broadcast news
documents, many of which consist of transcribed
data containing noise in the form of incomplete
sentences and disfluencies. Since these transcripts
provide an additional challenge for alignment and
coreference, Ratinov and Roth (2012) only use the
set of non-transcripts for their evaluation.
Using dynamic linking and a large set of surface
string variations, our approach may be able to pro-
vide an improvement even on the transcripts. To
identify the transcripts in the test set, we use the
approximation from Ratinov and Roth (2012) that
considers a document to be non-transcribed if it
contains proper noun mentions and at least a third
of those start with a capital letter. The performance
is shown in Table 3, while the improvement over
our baseline is shown in Figure 3.
Our static linking matches the performance of
Ratinov and Roth (2012) on the non-transcripts.
Further, the improvement of static linking on the
transcripts over the baseline is lower than that on
the non-transcript data, suggesting that noisy men-
tions and text result in poor quality alignment. Dy-
namic linking, on the other hand, not only outper-
forms all other systems, but also shows a higher im-
provement over the baseline on the transcripts than
on non-transcripts. This indicates that dynamic
linking approach is robust to noise, and its wider
variety of surface strings and flexible alignments
are especially useful for transcripts.
5.3 OntoNotes
We also run our systems on the OntoNotes dataset,
which was used for evaluation in CoNLL 2011
Shared Task (Pradhan et al, 2011). The dataset
consists of 2083 documents from a much larger va-
riety of genres, such as conversations, magazines,
web text, etc. Further, the dataset alo consists of
mentions that refer to events, most of which do not
appear as Wikipedia pages. Since only the non-
singleton mentions are annotated in the training set,
we also include additional noun phrase mentions
during training. We obtain B3 F1 of 65.3, 67.6, and
67.7 for our baseline, static linking, and dynamic
linking respectively.4 When compared to the par-
ticipants of the closed task, the dynamic linking
system outperforms all but two on this metric, sug-
gesting that dynamic alignment is beneficial even
when the features have not been engineered for
events or for different genres.
6 Related Work
Within-document coreference has been well-
studied for a number of years. A variety of ap-
proaches incorporate linguistic knowledge as rules
iteratively applied to identify the chains, such
as Haghighi and Klein (2009), Raghunathan et
al. (2010), Stoyanov et al (2010). Alternatively
(and similar to our approach), others represent this
knowledge as features in a machine learning model.
Early applications of such models include Soon et
al. (2001), Ng and Cardie (2002) and (Bengston
and Roth, 2008). There are also a number of tech-
niques that represent entities explicitly (Culotta et
4with MUC 46.1, 49.9 & 50.1, and CEAF(m) 47.9, 49.6 &
49.8, respectively for baseline, static and dynamic linking.
159
al., 2007; Wick et al, 2009; Haghighi and Klein,
2010; Stoyanov and Eisner, 2012).
This work is an extension of recent approaches
that incorporate external knowledge sources to im-
prove within-document coreference. Ponzetto and
Strube (2006) identify Wikipedia candidates for
each mention as a preprocessing step, and incor-
porate them as features in a pairwise model. Our
method differs in that we draw such features from
entity candidates during inference, and also main-
tain and update a set of candidate entity links
instead of selecting only one. Rahman and Ng
(2011) introduce similar features from a more ex-
tensive set of knowledge sources (such as YAGO
and FrameNet) into a cluster-based model whose
features change as inference proceeds. However,
the features for each cluster come from a combina-
tion of all entities aligned to the cluster mentions.
We improve upon this approach by maintaining a
list of the candidate entities for each mention clus-
ter, modifying this list during the course of infer-
ence, and using features from only the top-ranked
candidate at any time. Further, they do not provide
a comparison on a standard dataset.
Ratinov and Roth (2012) extend the multi-sieve
coreference model (Raghunathan et al, 2010) by
identifying at most a single candidate for each men-
tion, and incorporating high-precision attributes
extracted from Wikipedia. The high-precision
mention-candidate pairings are precomputed and
fixed; additionally, the features for an entity are
based on the predictions of the previous sieves, thus
fixed while a sieve is applied. With these restric-
tions, they show improvements over the state-of-
the-art on a subset of ACE mentions that are more
easily aligned to Wikipedia, while our approach
demonstrates improvements on the complete set of
mentions including the tougher to link mentions
from the transcripts.
There are a number of approaches that provide
an alignment from mentions in a document to
Wikipedia. Wikifier (Ratinov et al, 2011) analyzes
the context around the mentions and the entities
jointly, and was used to align mentions for corefer-
ence in Ratinov and Roth (2012). Dalton and Dietz
(2013) introduce an approximation to the above ap-
proach, but incorporate retrieval-based supervised
reranking that provides multiple candidates and
scores; this approach performed competitively on
previous TAC-KBP entity linking benchmarks (Di-
etz and Dalton, 2012). Alignment to an external
knowledge-base has improved performance for a
number of NLP and information extraction tasks,
such as named-entity recognition (Cucerzan, 2007;
Han and Zhao, 2009), cross-document corefer-
ence (Finin et al, 2009; Singh et al, 2010), and
relation-extraction (Riedel et al, 2010; Hoffmann
et al, 2011).
7 Conclusions
In this paper, we incorporate external knowledge to
improve within-document coreference. Instead of
fixing the alignment a priori, our approach main-
tains a ranked list of candidate entities for each
mention, and merges and reranks the list during
inference. Further, we consider a large set of sur-
face string variations for each entity by using an-
chor texts from the web. These external sources
allow our system to achieve a new state-of-the-art
on the ACE data. We also demonstrate improve-
ments on documents that are difficult for alignment
and coreference, such as transcripts and documents
containing a large number of mentions.
A number of possible avenues for future study
are apparent. First, our alignment to a knowledge-
base can benefit from more document-aware link-
ing to entities, such as the Wikifier (Ratinov et al,
2011). Second, we would like to augment mention
features with additional information available from
the knowledge base, such as Wikipedia categoriza-
tion and gender attributes. We also want to investi-
gate a cluster ranking model, as used in (Rahman
and Ng, 2011; Stoyanov and Eisner, 2012), to ag-
gregate the features of all the coreferent mentions
as inference progresses.
Acknowledgments
This work was supported in part by the Center
for Intelligent Information Retrieval, in part by
DARPA under agreement number FA8750-13-2-
0020, in part by NSF medium IIS-0803847 and
in part by an award from Google. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprint for Governmental purposes notwithstanding
any copyright annotation thereon. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are the authors? and neces-
sarily those of the sponsor.
160
References
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In International
Conference on Language Resources and Evaluation
(LREC) Workshop on Linguistics Coreference, pages
563?566.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics -
Volume 1, ACL ?98, pages 86?90, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Eric Bengston and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Empirical Methods in Natural Language Processing
(EMNLP).
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, SIGMOD ?08, pages 1247?1250, New York,
NY, USA. ACM.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Empirical
Methods in Natural Language Processing (EMNLP),
pages 708?716.
Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for corefer-
ence resolution. In North American Chapter of the
Association for Computational Linguistics - Human
Language Technologies (NAACL HLT).
Jeffrey Dalton and Laura Dietz. 2013. A neighbor-
hood relevance model for entity linking. In Open
Research Areas in Information Retrieval (OAIR).
Laura Dietz and Jeffrey Dalton. 2012. Across-
document neighborhood expansion: UMass at TAC
KBP 2012 entity linking. In Text Analysis Confer-
ence (TAC).
G. Doddington, A. Mitchell, M. Przybocki,
L. Ramshaw, S. Strassel, and R. Weischedel.
2004. The Automatic Content Extraction (ACE)
program?tasks, data, and evaluation. In Pro-
ceedings of LREC, volume 4, pages 837?840.
Citeseer.
Tim Finin, Zareen Syed, James Mayfield, Paul Mc-
Namee, and Christine Piatko. 2009. Using Wiki-
tology for cross-document entity coreference resolu-
tion. In AAAI Spring Symposium on Learning by
Reading and Learning to Read.
Aria Haghighi and Dan Klein. 2009. Simple corefer-
ence resolution with rich syntactic and semantic fea-
tures. In Empirical Methods in Natural Language
Processing (EMNLP), pages 1152?1161.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies
(NAACL HLT), pages 385?393.
Xianpei Han and Jun Zhao. 2009. Named entity disam-
biguation by leveraging Wikipedia semantic knowl-
edge. In Conference on Information and Knowledge
Management (CIKM), pages 215?224.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), pages 541?550, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan
Jurafsky. 2011. Stanford?s multi-pass sieve
coreference resolution system at the CoNLL-2011
shared task. In Conference on Computational
Natural Language Learning (CoNLL), pages 28?34.
Association for Computational Linguistics.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 104?111.
Vincent Ng. 2010. Supervised noun phrase corefer-
ence research: the first fifteen years. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, ACL ?10, pages 1396?
1411, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies
(NAACL HLT), pages 192?199.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 shared task: Modeling
unrestricted coreference in ontonotes. In Confer-
ence on Computational Natural Language Learning
(CoNLL), pages 1?27.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Empirical
Methods in Natural Language Processing (EMNLP),
pages 492?501. Association for Computational Lin-
guistics.
Altaf Rahman and Vincent Ng. 2011. Coreference
resolution with world knowledge. In Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), pages 814?824, Portland, Oregon, USA,
June.
161
L. Ratinov and D. Roth. 2012. Learning-based multi-
sieve co-reference resolution with knowledge. In
Empirical Methods in Natural Language Processing
(EMNLP).
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambigua-
tion to wikipedia. In Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Empirical Methods in Nat-
ural Language Processing (EMNLP).
Sameer Singh, Michael L. Wick, and Andrew McCal-
lum. 2010. Distantly labeling data for large scale
cross-document coreference. Computing Research
Repository (CoRR), abs/1005.4298.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544, Dec.
Valentin I. Spitkovsky and Angel X. Chang. 2012. A
cross-lingual dictionary for english wikipedia con-
cepts. In International Conference on Language Re-
sources and Evaluation (LREC).
Veselin Stoyanov and Jason Eisner. 2012. Easy-first
coreference resolution. In Computational Linguis-
tics (COLING).
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010.
Coreference resolution with reconcile. In Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 156?161, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th international con-
ference on World Wide Web, WWW ?07, pages 697?
706, New York, NY, USA. ACM.
Michael Wick, Aron Culotta, Khashayar Rohani-
manesh, and Andrew McCallum. 2009. An entity-
based model for coreference resolution. In SIAM In-
ternational Conference on Data Mining (SDM).
162
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 78?86,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Lexicon Infused Phrase Embeddings for Named Entity Resolution
Alexandre Passos, Vineet Kumar, Andrew McCallum
School of Computer Science
University of Massachusetts, Amherst
{apassos,vineet,mccallum}@cs.umass.edu
Abstract
Most state-of-the-art approaches for
named-entity recognition (NER) use semi
supervised information in the form of
word clusters and lexicons. Recently
neural network-based language models
have been explored, as they as a byprod-
uct generate highly informative vector
representations for words, known as word
embeddings. In this paper we present
two contributions: a new form of learn-
ing word embeddings that can leverage
information from relevant lexicons to
improve the representations, and the first
system to use neural word embeddings
to achieve state-of-the-art results on
named-entity recognition in both CoNLL
and Ontonotes NER. Our system achieves
an F1 score of 90.90 on the test set for
CoNLL 2003?significantly better than
any previous system trained on public
data, and matching a system employing
massive private industrial query-log data.
1 Introduction
In many natural language processing tasks, such
as named-entity recognition or coreference reso-
lution, syntax alone is not enough to build a high
performance system; some external source of in-
formation is required. In most state-of-the-art
systems for named-entity recognition (NER) this
knowledge comes in two forms: domain-specific
lexicons (lists of word types related to the de-
sired named entity types) and word representa-
tions (either clusterings or vectorial representa-
tions of word types which capture some of their
syntactic and semantic behavior and allow gener-
alizing to unseen word types).
Current state-of-the-art named entity recogni-
tion systems use Brown clusters as the form of
word representation (Ratinov and Roth, 2009;
Turian et al., 2010; Miller et al., 2004; Brown et
al., 1992), or other cluster-based representations
computed from private data (Lin and Wu, 2009).
While very attractive due to their simplicity, gen-
erality, and hierarchical structure, Brown clusters
are limited because the computational complex-
ity of fitting a model scales quadratically with the
number of words in the corpus, or the number of
?base clusters? in some efficient implementations,
making it infeasible to train it on large corpora or
with millions of word types.
Although some attempts have been made to
train named-entity recognition systems with other
forms of word representations, most notably those
obtained from training neural language models
(Turian et al., 2010; Collobert and Weston, 2008),
these systems have historically underperformed
simple applications of Brown clusters. A disad-
vantage of neural language models is that, while
they are inherently more scalable than Brown clus-
ters, training large neural networks is still often
expensive; for example, Turian et al (2010) re-
port that some models took multiple days or weeks
to produce acceptable representations. Moreover,
language embeddings learned from neural net-
works tend to behave in a ?nonlinear? fashion, as
they are trained to encourage a many-layered neu-
ral network to assign high probability to the data.
These neural networks can detect nonlinear rela-
tionships between the embeddings, which is not
possible in a log-linear model such as a condi-
tional random field, and therefore limiting how
much information from the embeddings can be ac-
tually leveraged.
Recently Mikolov et al (Mikolov et al., 2013a;
78
Mikolov et al., 2013b) proposed two simple log-
linear language models, the CBOW model and the
Skip-Gram model, that are simplifications of neu-
ral language models, and which can be very effi-
ciently trained on large amounts of data. For ex-
ample it is possible to train a Skip-gram model
over more than a billion tokens with a single ma-
chine in less than half a day. These embeddings
can also be trained on phrases instead of individual
word types, allowing for fine granularity of mean-
ing.
In this paper we make the following contribu-
tions. (1) We show how to extend the Skip-Gram
language model by injecting supervisory train-
ing signal from a collection of curated lexicons?
effectively encouraging training to learn similar
embeddings for phrases which occur in the same
lexicons. (2) We demonstrate that this method
outperforms a simple application of the Skip-
Gram model on the semantic similarity task on
which it was originally tested. (3) We show that
a linear-chain CRF is able to successfully use
these log-linearly-trained embeddings better than
the other neural-network-trained embeddings. (4)
We show that lexicon-infused embeddings let us
easily build a new highest-performing named en-
tity recognition system on CoNLL 2003 data
(Tjong Kim Sang and De Meulder, 2003) which
is trained using only publicly available data. (5)
We also present results on the relatively under-
studied Ontonotes NER task (Weischedel et al.,
2011), where we show that our embeddings out-
perform Brown clusters.
2 Background and Related Work
2.1 Language models and word embeddings
A statistical language model is a way to assign
probabilities to all possible documents in a given
language. Most such models can be classified
in one of two categories: they can directly as-
sign probabilities to sequences of word types, such
as is done in n-gram models, or they can oper-
ate in a lower-dimensional latent space, to which
word types are mapped. While most state-of-
the-art language models are n-gram models, the
representations used in models of the latter cate-
gory, henceforth referred to as ?embeddings,? have
been found to be useful in many NLP applications
which don?t actually need a language model. The
underlying intuition is that when language models
compress the information about the word types in
a latent space they capture much of the common-
alities and differences between word types. Hence
features extracted from these models then can gen-
eralize better than features derived from the word
types themselves.
One simple language model that discovers use-
ful embeddings is known as Brown clustering
(Brown et al., 1992). A Brown clustering is a
class-based bigram model in which (1) the prob-
ability of a document is the product of the proba-
bilities of its bigrams, (2) the probability of each
bigram is the product of the probability of a bi-
gram model over latent classes and the probability
of each class generating the actual word types in
the bigram, and (3) each word type has non-zero
probability only on a single class. Given a one-to-
one assignment of word types to classes, then, and
a corpus of text, it is easy to estimate these proba-
bilities with maximum likelihood by counting the
frequencies of the different class bigrams and the
frequencies of word tokens of each type in the cor-
pus. The Brown clustering algorithm works by
starting with an initial assignment of word types
to classes (which is usually either one unique class
per type or a small number of seed classes corre-
sponding to the most frequent types in the corpus),
and then iteratively selecting the pair of classes to
merge that would lead to the highest post-merge
log-likelihood, doing so until all classes have been
merged. This process produces a hierarchical clus-
tering of the word types in the corpus, and these
clusterings have been found useful in many appli-
cations (Ratinov and Roth, 2009; Koo et al., 2008;
Miller et al., 2004). There are other similar models
of distributional clustering of English words which
can be similarly effective (Pereira et al., 1993).
One limitation of Brown clusters is their com-
putational complexity, as training takes O(kV
2
+
N)x time to train, where k is the number of base
clusters, V size of vocabulary, and N number of
tokens. This is infeasible for large corpora with
millions of word types.
Another family of language models that pro-
duces embeddings is the neural language mod-
els. Neural language models generally work by
mapping each word type to a vector in a low-
dimensional vector space and assigning probabil-
ities to n-grams by processing their embeddings
in a neural network. Many different neural lan-
guage models have been proposed (Bengio et al.,
2003; Morin and Bengio, 2005; Bengio, 2008;
79
Mnih and Hinton, 2008; Collobert and Weston,
2008; Mikolov et al., 2010). While they can cap-
ture the semantics of word types, and often gen-
eralize better than n-gram models in terms of per-
plexity, applying them to NLP tasks has generally
been less successful than Brown clusters (Turian
et al., 2010).
Finally, there are algorithms for computing
word embeddings which do not use language mod-
els at all. A popular example is the CCA family of
word embeddings (Dhillon et al., 2012; Dhillon et
al., 2011), which work by choosing embeddings
for a word type that capture the correlations be-
tween the embeddings of word types which occur
before and after this type.
2.2 The Skip-gram Model
A main limitation of neural language models is
that they often have many parameters and slow
training times. To mitigate this, Mikolov et
al. (2013a; 2013b) recently proposed a family
of log-linear language models inspired by neu-
ral language models but designed for efficiency.
These models operate on the assumption that, even
though they are trained as language models, users
will only look at their embeddings, and hence all
they need is to produce good embeddings, and not
high-accuracy language models.
The most successful of these models is
the skip-gram model, which computes the
probability of each n-gram as the product of
the conditional probabilities of each context
word in the n-gram conditioned on its central
word. For example, the probability for the n-
gram ?the cat ate my homework? is represented as
P (the|ate)P (cat|ate)P (my|ate)P (homework|ate).
To compute these conditional probabilities the
model assigns an embedding to each word type
and defines a binary tree of logistic regression
classifiers with each word type as a leaf. Each
classifier takes a word embedding as input and
produces a probability for a binary decision cor-
responding to a branch in the tree. Each leaf in the
tree has a unique path from the root, which can be
interpreted as a set of (classifier,label) pairs. The
skip-gram model then computes a probability of a
context word given a target word as the product of
the probabilities, given the target word?s embed-
dings, of all decisions on a path from the root to
the leaf corresponding to the context word. Figure
1 shows such a tree structured model.
...
...
A An San Diego New York City
...
...
Figure 1: A binary Huffman tree. Circles repre-
sent binary classifiers. Rectangles represent to-
kens, which can be multi-word.
The likelihood of the data, then, given a set N
of n-grams, with m
n
being n-gram n?s middle-
word, c
n
each context word, w
c
n
i
the parameters
of the i-th classifier in the path from the root to
c
n
in the tree, l
c
n
i
its label (either 1 or ?1), e
f
the
embedding of word type f , and ? is the logistic
sigmoid function, is
?
n?N
?
c
n
?n
?
i
?(l
c
n
i
w
c
n
i
T
e
m
n
). (1)
Given a tree, then, choosing embeddings e
m
n
and classifier parameters w
c
n
i
to maximize equa-
tion (1) is a non-convex optimization problem
which can be solved with stochastic gradient de-
scent.
The binary tree used in the model is com-
monly estimated by computing a Huffman coding
tree (Huffman, 1952) of the word types and their
frequencies. We experimented with other tree esti-
mation schemes but found no perceptible improve-
ment in the quality of the embeddings.
It is possible to extend these embeddings to
model phrases as well as tokens. To do so,
Mikolov et al (2013b) use a phrase-building cri-
terion based on the pointwise mutual information
of bigrams. They perform multiple passes over
a corpus to estimate trigrams and higher-order
phrases. We instead consider candidate trigrams
for all pairs of bigrams which have a high PMI
and share a token.
2.3 Named Entity Recognition
Named Entity Recognition (NER) is the task of
finding all instances of explicitly named entities
and their types in a given document. While
80
detecting named entities is superficially simple,
since most sequences of capitalized words are
named entities (excluding headlines, sentence be-
ginnings, and a few other exceptions), finding all
entities is non trivial, and determining the correct
named entity type can sometimes be surprisingly
hard. Performing the task well often requires ex-
ternal knowledge of some form.
In this paper we evaluate our system on two
labeled datasets for NER: CoNLL 2003 (Tjong
Kim Sang and De Meulder, 2003) and Ontonotes
(Weischedel et al., 2011). The CoNLL dataset
has approximately 320k tokens, divided into 220k
tokens for training, 55k tokens for development,
and 50k tokens for testing. While the training and
development sets are quite similar, the test set is
substantially different, and performance on it de-
pends strongly on how much external knowledge
the systems have. The CoNLL dataset has four
entity types: PERSON, LOCATION, ORGANIZA-
TION, AND MISCELLANEOUS. The Ontonotes
dataset is substantially larger: it has 1.6M tokens
total, with 1.4M for training, 100K for develop-
ment, and 130k for testing. It also has eighteen
entity types, a much larger set than the CoNLL
dataset, including works of art, dates, cardinal
numbers, languages, and events.
The performance of NER systems is commonly
measured in terms of precision, recall, and F1 on
the sets of entities in the ground truth and returned
by the system.
2.3.1 Baseline System
In this section we describe in detail the baseline
NER system we use. It is inspired by the system
described in Ratinov and Roth (2009).
Because NER annotations are commonly not
nested (for example, in the text ?the US Army?,
?US Army? is treated as a single entity, instead
of the location ?US? and the organization ?US
Army?) it is possible to treat NER as a sequence
labeling problem, where each token in the sen-
tence receives a label which depends on which en-
tity type it belongs to and its position in the en-
tity. Following Ratinov and Roth (2009) we use
the BILOU encoding, where each token can either
BEGIN an entity, be INSIDE an entity, be the LAST
token in an entity, be OUTSIDE an entity, or be the
single UNIQUE token in an entity.
Our baseline architecture is a stacked linear-
chain CRF (Lafferty et al., 2001) system: we train
two CRFs, where the second CRF can condition
on the predictions made by the first CRF as well as
features of the data. Both CRFs, following Zhang
and Johnson (2003), have roughly similar features.
While local features capture a lot of the clues
used in text to highlight named entities, they can-
not necessarily disambiguate entity types or detect
named entities in special positions, such as the first
tokens in a sentence. To solve these problems most
NER systems incorporate some form of external
knowledge. In our baseline system we use lexi-
cons of months, days, person names, companies,
job titles, places, events, organizations, books,
films, and some minor others. These lexicons were
gathered from US Census data, Wikipedia cate-
gory pages, and Wikipedia redirects (and will be
made publicly available upon publication).
Following Ratinov and Roth (2009), we also
compare the performance of our system with a
system using features based on the Brown clusters
of the word types in a document. Since, as seen
in section 2.1, Brown clusters are hierarchical, we
use features corresponding to prefixes of the path
from the root to the leaf for each word type.
More specifically, the feature templates of the
baseline system are as follows. First for each token
we compute:
? its word type;
? word type, after excluding digits and lower-
casing it;
? its capitalization pattern;
? whether it is punctuation;
? 4-character prefixes and suffixes;
? character n-grams from length 2 to 5;
? whether it is in a wikipedia-extracted lexicon
of person names (first, last, and honorifics),
dates (months, years), place names (country,
US state, city, place suffixes, general location
words), organizations, and man-made things;
? whether it is a demonym.
For each token?s label we have feature templates
considering all token?s features, all neighboring
token?s features (up to distance 2), and bags of
words of features of tokens in a window of size
8 around each token. We also add a feature mark-
ing whether a token is the first occurrence of its
word type in a document.
When using Brown clusters we add as token
features all prefixes of lengths 4, 6, 10, and 20,
of its brown cluster.
For the second-layer model we use all these fea-
tures, as well as the label predicted for each token
81
Figure 2: Chain CRF model for a NER system
with three tokens. Filled rectangles represent fac-
tors. Circles at top represent labels, circles at bot-
tom represent binary token based features. Filled
circles indicate the phrase embeddings for each to-
ken.
by the first-layer model.
As seen in the Experiments Section, our base-
line system is competitive with state-of-the-art
systems which use similar forms of information.
We train this system with stochastic gradient as-
cent, using the AdaGrad RDA algorithm (Duchi et
al., 2011), with both `
1
and `
2
regularization, au-
tomatically tuned for each experimental setting by
measuring performance on the development set.
2.4 NER with Phrase Embeddings
In this section we describe how to extend our base-
line NER system to use word embeddings as fea-
tures.
First we group the tokens into phrases, assign-
ing to each token a single phrase greedily. We
prefer shorter phrases over longer ones, sinceour
embeddings are often more reliable for the shorter
phrases, and since the longer phrases in our dic-
tionary are mostly extracted from Wikipedia page
titles, which are not always semantically meaning-
ful when seen in free text. We then add factors
connecting each token?s label with the embedding
for its phrase.
Figure 2 shows how phrase embeddings are
plugged into a chain-CRF based NER system.
Following Turian (2010), we scale the embed-
ding vector by a real number, which is a hyper-
parameter tuned on the development data. Con-
necting tokens to phrase embeddings of their
neighboring tokens did not improve performance
for phrase embeddings, but it was mildly benefi-
cial for token embeddings.
3 Lexicon-infused Skip-gram Models
The Skip-gram model as defined in Section 2.2 is
fundamentally trained in unsupervised fashion us-
ing simply words and their n-gram contexts. In-
jecting some NER-specific supervision into the
embeddings can make them more relevant to the
NER task.
Lexicons are a simple yet powerful way to pro-
vide task-specific supervisory information to the
model without the burden of labeling additional
data. However, while lexicons have proven use-
ful in various NLP tasks, a small amount of noise
in a lexicon can severely impair the its usefulness
as a feature in log-linear models. For example,
even legitimate data, such as the Chinese last name
?He? occurring in a lexicon of person last names,
can cause the lexicon feature to fire spuriously
for many training tokens that are labeled PERSON,
and then this lexicon feature may be given low or
even negative weight.
We propose to address both these problems by
employing lexicons as part of the word embedding
training. The skip-gram model can be trained to
predict not only neighboring words but also lexi-
con membership of the central word (or phrase).
The resulting embedding training will thus be
somewhat supervised by tending to bring together
the vectors of words sharing a lexicon member-
ship. Furthermore, this type of training can effec-
tively ?clean? the influence of noisy lexicons be-
cause even if ?He? appears in the PERSON lexicon,
it will have a sufficiently different context distribu-
tion than labeled named person entities (e.g. a lack
of preceding honorifics, etc) that the presence of
this noise in the lexicon will not be as problematic
as it was previously.
Furthermore, while Skip-gram models can be
trained on billions of tokens to learn word em-
beddings for over a million word types in a sin-
gle day, this might not be enough data to cap-
ture reliable embeddings of all relevant named en-
tity phrases. Certain sets of word types, such as
names of famous scientists, can occur infrequently
enough that the Skip-gram model will not have
enough contextual examples to learn embeddings
that highlight their relevant similarities.
In this section we describe how to extend the
Skip-gram model to incorporate auxiliary infor-
mation from lexicons, or lists of related words, en-
couraging the model to assign similar embeddings
to word types in similar lexicons.
82
New YorkThe ofstate is often referred
...
...
...
...
stateThe
...
New York
US-STATE
WIKI-LOCATION
BUSINESS
Figure 3: A Semi supervised Skip-gram Model.
?New York? predicts the word ?state?. With
lexicon-infusion, ?New York? also predicts its lex-
icon classes: US-State, Wiki-location
.
In the basic Skip-gram model, as seen in Sec-
tion 2.2, the likelihood is, for each n-gram, a prod-
uct of the probability of the embedding associated
with the middle word conditioned on each context
word. We can inject supervision in this model by
also predicting, given the embedding of the mid-
dle word, whether it is a member of each lexicon.
Figure 3 shows an example, where the word ?New
York? predicts ?state?, and also its lexicon classes:
Business, US-State and Wiki-Location.
Hence, with subscript s iterating over each lex-
icon (or set of related words), and l
m
n
s
being a la-
bel for whether each word is in the set, and w
s
indicating the parameters of its classifier, the full
likelihood of the model is
(2)
?
n ?N
(
?
c
n
?n
?
i
?(l
c
n
i
w
c
n
i
T
e
m
n
)
)
(
?
s
?(l
m
n
s
w
T
s
e
m
n
)
)
.
This is a simple modification to equation (1) that
also predicts the lexicon memberships. Note that
the parameters w
s
of the auxiliary per-lexicon
classifiers are also learned. The lexicons are not
inserted in the binary tree with the words; instead,
each lexicon gets its own binary classifier.
Algorithm 1 Generating the training examples for
lexicon-infused embeddings
1: for all n-gram n with middle word m
n
do
2: for all Context-word c
n
do
3: for all Classifier, label pair (w
c
n
i
,l
c
n
i
)
in the tree do
4: Add training example
e
m
n
, w
c
n
i
, l
c
n
5: end for
6: end for
7: for all Lexicon s, with label l
m
n
s
do
8: Add training example e
m
n
, w
s
, l
m
n
s
9: end for
10: end for
In practice, a very small fraction of words are
present in a lexicon-class and this creates skewed
training data, with overwhelmingly many negative
examples. We address this issue by aggressively
sub-sampling negative training data for each lex-
icon class. We do so by randomly selecting only
1% of the possible negative lexicons for each to-
ken.
A Skip-gram model has V binary classifiers. A
lexicon-infused Skip-gram model predicts an ad-
ditional K classes, and thus has V + K binary
classifiers. If number of classes K is large, we can
induce a tree over the classes, similarly to what is
done over words in the vocabulary. In our trained
models, however, we have one million words in
the vocabulary and twenty-two lexicons, so this is
not necessary.
4 Experiments
Our phrase embeddings are learned on the combi-
nation of English Wikipedia and the RCV1 Cor-
pus (Lewis et al., 2004). Wikipedia contains 8M
articles, and RCV1 contains 946K. To get candi-
date phrases we first select bigrams which have
a pointwise mutual information score larger than
1000. We discard bigrams with stopwords from a
manually selected list. If two bigrams share a to-
ken we add its corresponding trigram to our phrase
list. We further add page titles from the English
Wikipedia to the list of candidate phrases, as well
as all word types. We get a total of about 10M
phrases. We restrict the vocabulary to the most fre-
quent 1M phrases. All our reported experiments
are on 50-dimensional embeddings. Longer em-
beddings, while performing better on the semantic
similarity task, as seen in Mikolov et al (2013a;
83
Model Accuracy
Skip-Gram 29.89
Lex-0.05 30.37
Lex-0.01 30.72
Table 1: Accuracy for Semantic-Syntactic task,
when restricted to Top 30K words. Lex-0.01 refers
to a model trained with lexicons, where 0.01% of
negative examples were used for training.
2013b), did not perform as well on NER.
To train phrase embeddings, we use a con-
text of length 21. We use lexicons derived from
Wikipedia categories and data from the US Cen-
sus, totaling K = 22 lexicon classes. We use a
randomly selected 0.01% of negative training ex-
amples for lexicons.
We perform two sets of experiments. First, we
validate our lexicon-infused phrase embeddings
on a semantic similarity task, similar to Mikolov et
al (Mikolov et al., 2013a). Then we evaluate their
utility on two named-entity recognition tasks.
For the NER Experiments, we use the base-
line system as described in Section 2.3.1. NER
systems marked as ?Skip-gram? consider phrase
embeddings; ?LexEmb? consider lexicon-infused
embeddings; ?Brown? use Brown clusters, and
?Gaz? use our lexicons as features.
4.1 Syntactic and Semantic Similarity
Mikolov et al. (2013a) introduce a test set to mea-
sure syntactic and semantic regularities for words.
This set contains 8869 semantic and 10675 syn-
tactic questions. Each question consists of four
words, such as big, biggest, small, smallest. It
asks questions of the form ?What is the word that
is similar to small in the same sense as biggest is
similar to big?. To test this, we compute the vec-
tor X = vector(?biggest?) ? vector(?big?) +
vector(?small?). Next, we search for the word
closest to X in terms of cosine distance (exclud-
ing ?biggest?, ?small?, and ?big?). This question
is considered correctly answered only if the clos-
est word found is ?smallest?. As in Mikolov et
al (Mikolov et al., 2013a), we only search over
words which are among the 30K most frequent
words in the vocabulary.
Table 1 depicts the accuracy on Semantic Syn-
tactic Task for models trained with 50 dimensions.
We find that lexicon-infused embeddings perform
better than Skip-gram. Further, lex-0.01 performs
System Dev Test
Baseline 92.22 87.93
Baseline + Brown 93.39 90.05
Baseline + Skip-gram 93.68 89.68
Baseline + LexEmb 93.81 89.56
Baseline + Gaz 93.69 89.27
Baseline + Gaz + Brown 93.88 90.67
Baseline + Gaz + Skip-gram 94.23 90.33
Baseline + Gaz + LexEmb 94.46 90.90
Ando and Zhang (2005) 93.15 89.31
Suzuki and Isozaki (2008) 94.48 89.92
Ratinov and Roth (2009) 93.50 90.57
Lin and Wu (2009) - 90.90
Table 2: Final NER F1 scores for the CoNLL 2003
shared task. On the top are the systems presented
in this paper, and on the bottom we have base-
line systems. The best results within each area are
highlighted in bold. Lin and Wu 2009 use massive
private industrial query-log data in training.
the best, and we use this model for further NER
experiments. There was no perceptible difference
in computation cost from learning lexicon-infused
embeddings versus learning standard Skip-gram
embeddings.
4.2 CoNLL 2003 NER
We applied our models on CoNLL 2003 NER data
set. All hyperparameters were tuned by training
on training set, and evaluating on the development
set. Then the best hyperparameter values were
trained on the combination of training and devel-
opment data and applied on the test set, to obtain
the final results.
Table 2 shows the phrase F1 scores of all sys-
tems we implemented, as well as state-of-the-
art results from the literature. Note that us-
ing traditional unsupervised Skip-gram embed-
dings is worse than Brown clusters. In contrast,
our lexicon-infused phrase embeddings Lex-0.01
achieves 90.90?a state-of-the-art F1 score for the
test set. This result matches the highest F1 previ-
ously reported, in Lin and Wu (2009), but is the
first system to do so without using massive private
data. Our result is signficantly better than the pre-
vious best using public data.
4.3 Ontonotes 5.0 NER
Similarly to the CoNLL NER setup, we tuned the
hyperparameters on the development set. We use
84
System Dev Test
Baseline 79.04 79.85
Baseline + Brown 79.95 81.38
Baseline + Skip-gram 80.59 81.91
Baseline + LexEmbd 80.65 81.82
Baseline + Gaz 79.85 81.31
Baseline + Gaz + Brown 80.53 82.05
Baseline + Gaz + Skip-gram 80.70 82.30
Baseline + Gaz + LexEmb 80.81 82.24
Table 3: Final NER F1 scores for Ontonotes 5.0
dataset. The results in bold face are the best on
each evaluation set.
the same list of lexicons as for CoNLL NER.
Table 3 summarize our results. We found that
both Skip-gram and Lexicon infused embeddings
give better results than using Brown Clusters as
features. However, in this case Skip-gram embed-
dings give marginally better results. (So as not to
jeopardize our ability to fairly do further research
on this task, we did not analyze the test set errors
that may explain this.) These are, to the best of our
knowledge, the first published performance num-
bers on the Ontonotes NER task.
5 Conclusions
We have shown how to inject external supervision
to a Skip-gram model to learn better phrase em-
beddings. We demonstrate the quality of phrase
embeddings on three tasks: Syntactic-semantic
similarity, CoNLL 2003 NER, and Ontonotes 5.0
NER. In the process, we provide a new public
state-of-the-art NER system for the widely con-
tested CoNLL 2003 shared task.
We demonstrate how we can plug phrase em-
beddings into an existing log-linear CRF System.
This work demonstrates that it is possible to
learn high-quality phrase embeddings and fine-
tune them with external supervision from billions
of tokens within one day computation time. We
further demonstrate that learning embeddings is
important and key to improve NLP Tasks such as
NER.
In future, we want to explore employing embed-
dings to other NLP tasks such as dependency pars-
ing and coreference resolution. We also want to
explore improving embeddings using error gradi-
ents from NER.
References
Rie Kubota Ando and Tong Zhang. 2005. A high-
performance semi-supervised learning method for
text chunking. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 1?9. Association for Computational Lin-
guistics.
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137?1155.
Yoshua Bengio. 2008. Neural net language models.
Scholarpedia, 3(1):3881.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160?167. ACM.
Paramveer Dhillon, Dean P Foster, and Lyle H Ungar.
2011. Multi-view learning of word embeddings via
cca. In Advances in Neural Information Processing
Systems, pages 199?207.
Paramveer Dhillon, Jordan Rodu, Dean Foster, and
Lyle Ungar. 2012. Two step cca: A new spec-
tral method for estimating vector models of words.
arXiv preprint arXiv:1206.6403.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 999999:2121?2159.
David A Huffman. 1952. A method for the construc-
tion of minimum-redundancy codes. Proceedings of
the IRE, 40(9):1098?1101.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.
David D Lewis, Yiming Yang, Tony G Rose, and Fan
Li. 2004. Rcv1: A new benchmark collection for
text categorization research. The Journal of Ma-
chine Learning Research, 5:361?397.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1030?1038. Association for
Computational Linguistics.
85
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013b. Distributed repre-
sentations of words and phrases and their composi-
tionality. arXiv preprint arXiv:1310.4546.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In HLT-NAACL, volume 4, pages
337?342. Citeseer.
Andriy Mnih and Geoffrey E Hinton. 2008. A scal-
able hierarchical distributed language model. In
Advances in neural information processing systems,
pages 1081?1088.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international workshop on artifi-
cial intelligence and statistics, pages 246?252.
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
Proceedings of the 31st annual meeting on Associa-
tion for Computational Linguistics, pages 183?190.
Association for Computational Linguistics.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning, pages 147?
155. Association for Computational Linguistics.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In ACL, pages 665?673.
Citeseer.
Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 142?147. Association for Computational Lin-
guistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Ralph Weischedel, Martha Palmer, Mitchell Marcus,
Eduard Hovy, Sameer Pradhan, Lance Ramshaw,
Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, et al. 2011. OntoNotes Release 4.0. Lin-
guistic Data Consortium.
Tong Zhang and David Johnson. 2003. A robust
risk minimization based named entity recognition
system. In Proceedings of the seventh conference
on Natural language learning at HLT-NAACL 2003-
Volume 4, pages 204?207. Association for Compu-
tational Linguistics.
86
