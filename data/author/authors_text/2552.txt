Experiments on Sentence Boundary Detection 
Mark  Stevenson  and  Rober t  Ga izauskas  
Depar tment  of  Computer  Science, 
Un ivers i ty  of  Sheff ield 
Regent  Cour t ,  211 Por tobe l lo  St reet ,  
Sheff ield 
S1 4DP Un i ted  K ingdom 
{marks, robertg}@dcs, shef. ac.uk 
Abst ract  
This paper explores the problem of identifying sen- 
tence boundaries in the transcriptions produced by 
automatic speech recognition systems. An experi- 
ment which determines the level of human perform- 
ance for this task is described as well as a memory- 
based computational pproach to the problem. 
1 The  Prob lem 
This paper addresses the problem of identifying sen- 
tence boundaries in the transcriptions produced by 
automatic speech recognition (ASR) systems. This 
is unusual in the field of text processing which has 
generally dealt with well-punctuated text: some of 
the most commonly used texts in NLP are machine 
readable versions of highly edited documents uch 
as newspaper articles or novels. However, there are 
many types of text which are not so-edited and the 
example which we concentrate on in this paper is 
the output from ASR systems. These differ from 
the sort of texts normally used in NLP in a number 
of ways; the text is generally in single case (usually 
upper), unpunctuated and may contain transcrip- 
tion errors. 1 Figure 1 compares a short text in the 
format which would be produced by an ASR system 
with a fully punctuated version which includes case 
information. For the remainder of this paper error- 
free texts such as newspaper articles or novels shall 
be referred to as "standard text" and the output 
from a speech recognition system as "ASR text". 
There are many possible situations in which an 
NLP system may be required to process ASR text. 
The most obvious examples are NLP systems which 
take speech input (eg. Moore et al (1997)). Also, 
dictation software programs do not punctuate or 
capitalise their output but, if this information could 
be added to ASR text, the results would be far more 
usable. One of the most important pieces of inform- 
1 Speech recognition systems are often evaluated in terms 
of word error ate (WER), the percentage oftokens which are 
wrongly transcribed. For large vocabulary tasks and speaker- 
independent systems, WER varies between 7% and 50%, de- 
pending upon the quality of the recording being recognised. 
See, e.g., Cole (1996). 
G00D EVENING GIANNI VERSACE ONE OF THE 
WORLDS LEADING FASHION DESIGNERS HAS 
BEEN MURDERED IN MIAMI POLICE SAY IT WAS 
A PLANNED KILLING CARRIED OUT LIKE AN 
EXECUTION SCHOOLS INSPECTIONS ARE GOING 
TO BE TOUGHER TO FORCE BAD TEACHERS OUT 
AND THE FOUR THOUSAND COUPLES WH0 SHARED 
THE QUEENS GOLDEN DAY 
Good evening. Gi~nni Versace, one of 
the world's leading fashion designers, 
has been murdered in Miami. Police say 
it was a planned killing carried out 
like an execution. Schools inspections 
are going to be tougher to force bad 
teachers out. And the four thousand 
couples who shared the Queen's golden 
day. 
Figure 1: Example text shown in standard and ASR 
format 
ation which is not available in ASR output is sen- 
tence boundary information. However, knowledge of 
sentence boundaries i required by many NLP tech- 
nologies. Part of speech taggers typically require 
input in the format of a single sentence per line (for 
example Brill's tagger (Brill, 1992)) and parsers gen- 
erally aim to produce a tree spanning each sentence. 
Only the most trivial linguistic analysis can be car- 
ried out on text which is not split into sentences. 
It is worth mentioning that not all transcribed 
speech can be sensibly divided into sentences. It has 
been argued by Gotoh and Renals (2000) that the 
main unit in spoken language is the phrase rather 
than the sentence. However, there are situations 
in which it is appropriate to consider spoken lan- 
guage to be made up from sentences. One example 
is broadcast news: radio and television news pro- 
grams. The DARPA HUB4 broadcast news evalu- 
ation (Chinchor et al, 1998) focussed on informa- 
tion extraction from ASR text from news programs. 
Although news programs are scripted there are of- 
ten deviations from the script and they cannot be 
relied upon as accurate transcriptions of the news 
84 
program. The spoken portion of the British National 
Corpus (Burnard, 1995) contains 10 million words 
and was manually marked with sentence boundar- 
ies. A technology which identifies entence boundar- 
ies could be used to speed up the process of creating 
any future corpus of this type. 
It is important o distinguish the problem just 
mentioned and another problem sometimes called 
"sentence splitting". This problem aims to identify 
sentence boundaries in standard text but since this 
includes punctuation the problem is effectively re- 
duced to deciding which of the symbols which poten- 
tially denote sentence boundaries ( . ,  !,  ?) actually 
do. This problem is not trivial since these punc- 
tuation symbols do not always occur at the end of 
sentences. For example in the sentence "Dr. Jones 
l ec tures  at  U.C.L.A." only the final full stop de- 
notes the end of a sentence. For the sake of clarity 
we shall refer to the process of discovering sentence 
boundaries in standard punctuated text as "punc- 
tuation disambiguation" and that of finding them 
in unpunctuated ASR text as "sentence boundary 
detection". 
2 Related Work 
Despite the potential application of technology 
which can carry out the sentence boundary detec- 
tion task, there has been little research into the 
area. However, there has been work in the re- 
lated field of punctuation disambiguation. Palmer 
and Hearst (1994) applied a neural network to the 
problem. They used the Brown Corpus for training 
and evaluation, noting that 90% of the full stops in 
this text indicate sentence boundaries. They used 
the part of speech information for the words sur- 
rounding a punctuation symbol as the input to a 
feed-forward neural network. But, as we mentioned, 
most part of speech taggers require sentence bound- 
aries to be pre-determined and this potential cir- 
cularity is avoided by using the prior probabilities 
for each token, determined from the Brown corpus 
markup. The network was trained on 573 potential 
sentence nding marks from the Wall Street Journal 
and tested on 27,294 items from the same corpus. 
98.5% of punctuation marks were correctly disam- 
biguated. 
Reynar and Ratnaparkhi (1997) applied a max- 
imum entropy approach to the problem. Their 
system considered only the first word to the left 
and right of any potential sentence boundary and 
claimed that examining wider context did not help. 
For both these words the prefix, suffix, presence of 
particular characters in the prefix or suffix, whether 
the candidate is honorific (Mr., Dr. etc.) and 
whether the candidate is a corporate designator (eg. 
Corp.) are features that are considered. This sys- 
tem was tested on the same corpus as Palmer and 
Hearst's system and correctly identified 98.8% of 
sentence boundaries. Mikheev (1998) optimised this 
approach and evaluated it on the same test corpus. 
An accuracy of 99.2477% was reported, to our know- 
ledge this is the highest quoted result for this test 
set. 
These three systems achieve very high results 
for the punctuation disambiguation task. It would 
seem, then, that this problem has largely been 
solved. However, it is not clear that these techniques 
will be as successful for ASR text. We now go on to 
describe a system which attempts a task similar to 
sentence boundary detection of ASR text. 
Beeferman et al (1998) produced a system, "CY- 
BERPUNC", which added intra-sentence punctu- 
ation (i.e. commas) to the output of an ASR system. 
They mention that the comma is the most frequently 
used punctuation symbol and its correct insertion 
can make a text far more legible. CYBERPUNC 
operated by augmenting a standard trigram speech 
recognition model with information about commas; 
it accesses only lexical information. CYBERPUNC 
was tested by separating the trigram model from 
the ASR system and applying it to 2,317 sentences 
from the Wall Street Journal. The system achieved 
a precision of 75.6% and recall of 65.6% compared 
against he original punctuation i  the text. 2 A fur- 
ther qualitative valuation was carried out using 100 
randomly-drawn output sentences from the system 
and 100 from the Wall Street Journal. Six human 
judges blindly marked each sentence as either ac- 
ceptable or unacceptable. It was found that the 
Penn TreeBank sentences were 86% correct and the 
system output 66% correct. It is interesting that the 
human judges do not agree completely on the ac- 
ceptability of many sentences from the Wall Street 
Journal. 
In the next section we go on to describe exper- 
iments which quantify the level of agreement that 
can be expected when humans carry out sentence 
boundary detection. Section 4 goes on to describe a
computational pproach to the problem. 
3 Determining Human Abi l i ty 
Beeferman et. al.'s experiments demonstrated that 
humans do not always agree on the acceptability of 
comma insertion and therefore it may be useful to 
determine how often they agree on the placing of 
sentence boundaries. To do this we carried out ex- 
periments using transcriptions ofnews programmes, 
specifically the transcriptions of two editions of the 
~Precision and recall are complementary  evaluation met- 
rics commonly  used in Information Retrieval (van Rijsbergen, 
1979). In this case precision is the percentage of commas pro- 
posed by the system which are correct while recall is the per- 
centage of the commas occurring in the test corpus which the 
system identified. 
R~ 
BBC television program "The Nine O'Clock News" .3 
The transcriptions consisted of punctuated mixed 
case text with sentences boundaries marked using a 
reserved character ("; "). These texts were produced 
by trained transcribers listening to the original pro- 
gram broadcast. 
Six experimental subjects were recruited. All sub- 
jects were educated to at least Bachelor's degree 
level and are either native English speakers or flu- 
ent second language speakers. Each subject was 
presented with the same text from which the sen- 
tence boundaries had been removed. The texts were 
transcriptions of two editions of the news program 
from 1997, containing 534 sentences and represented 
around 50 minutes of broadcast news. The subjects 
were randomly split into two groups. The subjects 
in the first group (subjects 1-3) were presented with 
the text stripped of punctuation and converted to 
upper case. This text simulated ASR text with no 
errors in the transcription. The remaining three sub- 
jects (4-6) were presented with the same text with 
punctuation removed but case information retained 
(i.e. mixed case text). This simulated unpunctuated 
standard text. All subjects were asked to add sen- 
tence boundaries to the text whenever they thought 
they occurred. 
The process of determining human ability at some 
linguistic task is generally made difficult by the lack 
of an appropriate reference. Often all we have to 
compare one person's judgement with is that of an- 
other. For example, there have been attempts to 
determine the level of performance which can be ex- 
pected when humans perform word sense disambig- 
uation (Fellbaum et al, 1998) but these have simply 
compared some human judgements against others 
with one being chosen as the "expert". We have 
already seen, in Section 2, that there is a signific- 
ant degree of human disagreement over the accept- 
ability of intra-sentential punctuation. The human 
transcribers ofthe "Nine O'Clock News" have access 
to the original news story which contains more in- 
formation than just the transcription. Under these 
conditions it is reasonable to consider their opinion 
as expert. 
Table 1 shows the performance of the human sub- 
jects compared to the reference transcripts. 4 
An algorithm was implemented to provide a 
baseline tagging of the text. The average length of 
sentences in our text is 19 words and the baseline al- 
gorithm randomly assigns a sentence break at each 
word boundary with a probability of ~ .  The two 
annotators labelled "random" show the results when 
this algorithm is applied. This method produced a
3This is a 25 minute long television ews program broad- 
cast in the United Kingdom on Monday to Friday evenings. 
4F-measure (F) is a weighted harmonic ombining preci- 
sion (P) and recall (R) via the formula 2PR 
PTR " 
very low result in comparison to the expert annota- 
tion. 
1 Upper 84 68 76 
2 Upper 93 78 85 
3 Upper 90 76 82 
4 Mixed 97 90 94 
5 Mixed 96 89 92 
6 Mixed 97 67 79 
Random Upper 5 5 5 
Random Mixed 5 5 5 
Table 1: Results from Human Annotation Experi- 
ment 
The performance of the human annotators on the 
upper case text is quite significantly lower than 
the reported performance of the algorithms which 
performed punctuation disambiguation on standard 
text as described in Section 2. This suggests that 
the performance which may be obtained for this task 
may be lower than has been achieved for standard 
text. 
~Sarther insight into the task can be gained from 
determining the degree to which the subjects agreed. 
Carletta (1996) argues that the kappa statistic (a) 
should be adopted to judge annotator consistency 
for classification tasks in the area of discourse and 
dialogue analysis. It is worth noting that the prob- 
lem of sentence boundary detection presented so far 
in this paper has been formulated as a classification 
task in which each token boundary has to be clas- 
sifted as either being a sentence boundary or not. 
Carletta argues that several incompatible measures 
of annotator agreement have been used in discourse 
analysis, making comparison impossible. Her solu- 
tion is to look to the field of content analysis, which 
has already experienced these problems, and adopt 
their solution of using the kappa statistic. This de- 
termines the difference between the observed agree- 
ment for a linguistic task and that which would be 
expected by chance. It is calculated according to for- 
mula 1, where Pr(A) is the proportion of times the 
annotators agree and Pr(E) the proportion which 
would be expected by chance. Detailed instructions 
on calculating these probabilities are described by 
Siegel and Castellan (1988). 
Pr(A) - Pr(E)  
= (1) 
1 - Pr(E) 
The value of the kappa statistic ranges between 
1 (perfect agreement) and 0 (the level which would 
be expected by chance). It has been claimed that 
content analysis researchers usually regard a > .8 to 
demonstrate good reliability and .67 < ~ < .8 al- 
f16 
lows tentative conclusions to be drawn (see Carletta 
(1996)). 
We began to analyse the data by computing the 
kappa statistic for both sets of annotators. Among 
the two annotators who marked the mixed case (sub- 
jects 4 and 5) there was an observed kappa value of 
0.98, while there was a measure of 0.91 for the three 
subjects who annotated the single case text. These 
values are high and suggest a strong level of agree- 
ment between the annotators. However, manual 
analysis of the annotated texts suggested that the 
subjects did not agree on many cases. We then ad- 
ded the texts annotated by the "random" annotation 
algorithm and calculated the new ~ values. It was 
found that the mixed case test produced a kappa 
value of 0.92 and the upper case text 0.91. These 
values would still suggest a high level of agreement 
although the sentences produced by our random al- 
gorithm were nonsensical. 
The problem seems to be that most word bound- 
aries in a text are not sentence boundaries. There- 
fore we could compare the subjects' annotations 
who had not agreed on any sentence boundaries but 
find that they agreed most word boundaries were 
not sentence boundaries. The same problem will 
effect other standard measures of inter-annotator 
agreement such as the Cramer, Phi and Kendall 
coefficients (see Siegel and Castellan (1988)). Car- 
letta mentions this problem, asking what the dif- 
ference would be if the kappa statistic were com- 
puted across "clause boundaries, transcribed word 
boundaries, and transcribed phoneme boundaries" 
(Carletta, 1996, p. 252) rather than the sentence 
boundaries he suggested. It seems likely that more 
meaningful ~ values would be obtained if we restric- 
ted to the boundaries between clauses rather than 
all token boundaries. However, it is difficult to ima- 
gine how clauses could be identified without parsing 
and most parsers require part of speech tagged input 
text. But, as we already mentioned, part of speech 
taggers often require input text split into sentences. 
Consequently, there is a lack of available systems for 
splitting ASR text into grammatical clauses. 
4 A Computat iona l  Approach  to  
Sentence  Boundary  Detect ion  
The remainder of this paper describes an implemen- 
ted program which attempts entence boundary de- 
tection. The approach is based around the Timbl 
memory-based learning algorithm (Daelemans et al, 
1999) which we previously found to be very success- 
ful when applied to the word sense disambiguation 
problem (Stevenson and Wilks, 1999). 
Memory-based learning, also known as case-based 
and lazy learning, operates by memorising a set of 
training examples and categorising new cases by as- 
signing them the class of the most similar learned 
example. We apply this methodology to the sen- 
tence boundary detection task by presenting Timbl 
with examples of word boundaries from a train- 
ing text, each of which is categorised as either 
sentence_boundary or no_boundary. Unseen ex- 
amples are then compared and categorised with the 
class of the most similar example. We shall not 
discuss the method by which Timbl determines the 
most similar training example which is described by 
Daelemans et al (1999). 
Following the work done on punctuation disambig- 
uation and that of Beeferman et. al. on comma in- 
sertion (Section 2), we used the Wall Street Journal 
text for this experiment. These texts are reliably 
part of speech tagged 5 and sentence boundaries can 
be easily derived from the corpus. This text was 
initially altered so as to remove all punctuation and 
map all characters into upper case. 90% of the cor- 
pus, containing 965 sentence breaks, was used as a 
training corpus with the remainder, which contained 
107 sentence breaks, being held-back as unseen test 
data. The first stage was to extract some statistics 
from the training corpus. We examined the training 
corpus and computed, for each word in the text, the 
probability that it started a sentence and the prob- 
ability that it ended a sentence. In addition, for each 
part of speech tag we also computed the probability 
that it is assigned to the first word in a sentence and 
the probability that it is assigned to the last word. 6 
Each word boundary in the corpus was translated to 
a feature-vector representation consisting of 13 ele- 
ments, shown in Table 2. Vectors in the test corpus 
are in a similar format, the difference being that the 
classification (feature 13) is not included. 
The results obtained are shown in the top row of 
Table 3. Both precision and recall are quite prom- 
ising under these conditions. However, this text is 
different from ASR text in one important way: the 
text is mixed case. The experimented was repeated 
with capitalisation information removed; that is, 
features 6 and 12 were removed from the feature- 
vectors. The results form this experiment are shown 
in the bottom row of Table 3. It can be seen that 
the recorded performance is far lower when capital- 
isation information is not used, indicating that this 
is an important feature for the task. 
These experiments have shown that it is much 
easier to add sentence boundary information to 
mixed case test, which is essentially standard text 
with punctuation removed, than ASR text, even as- 
5Applying a priori tag probability distributions could have 
been used rather than the tagging in the corpus as such re- 
liable annotations may not be available for the output of an 
ASR system. Thus, the current experiments should be viewed 
as making an optimistic assumption. 
eWe attempted to smooth these probabilities using Good- 
Turing frequency estimation (Gale and Sampson, 1996) but 
found that it had no effect on the final results. 
87 
Position Feature 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
Preceding word 
Probability preceding word ends a sentence 
Part of speech tag assigned to preceding word 
Probability that part of speech tag (feature 3) is assigned to last word in a sentence 
Flag indicating whether preceding word is a stop word 
Flag indicating whether preceding word is capitalised 
Following word 
Probability following word begins a sentence 
Part of speech tag assigned to following word 
Probability that part of speech (feature 9) is assigned to first word in a sentence 
Flag indicating whether following word is a stop word 
Flag indicating whether following word is capitalised word 
sentence_boundary or no_boundary 
Table 2: Features used in Timbl representation 
Case information \[I P I R I F 
Applied I 78 \[ 75 \[ 76 
Not applied 36 35 35 
Table 3: Results of the sentence boundary detection 
program 
suming a zero word error rate. This result is in 
agreement with the results from the human annota- 
tion experiments described in Section 3. However, 
there is a far greater difference between the auto- 
matic system's performance on standard and ASR 
text than the human annotators. 
Reynar and Ratnaparkhi (1997) (Section 2) ar- 
gued that a context of one word either side is suf- 
ficient for the punctuation disambiguation problem. 
However, the results of our system suggest that this 
may be insufficient for the sentence boundary detec- 
tion problem even assuming reliable part of speech 
tags (cf note 5). 
These experiments do not make use of prosodic in- 
formation which may be included as part of the ASR 
output. Such information includes pause length, 
pre-pausal lengthening and pitch declination. If this 
information was made available in the form of extra 
features to a machine learning algorithm then it is 
possible that the results will improve. 
5 Conc lus ion  
This paper has introduced the problem of sentence 
boundary detection on the text produced by an ASR 
system as an area of application for NLP technology. 
An attempt was made to determine the level of 
human performance which could be expected for the 
task. It was found that there was a noticeable dif- 
ference between the observed performance for mixed 
and upper case text. It was found that the kappa 
statistic, a commonly used method for calculating 
inter-annotator agreement, could not be applied dir- 
ectly in this situation. 
A memory-based system for identifying sentence 
boundaries in ASR text was implemented. There 
was a noticeable difference when the same system 
was applied to text which included case information 
demonstrating that this is an important feature for 
the problem. 
This paper does not propose to offer a solution to 
the sentence boundary detection problem for ASR 
transcripts. However, our aim has been to high- 
light the problem as one worthy of further explor- 
ation within the field of NLP and to establish some 
baselines (human and algorithmic) against which 
further work may be compared. 
Acknowledgements  
The authors would like to thank Steve Renals and 
Yoshihiko Gotoh for providing the data for human 
annotation experiments and for several useful con- 
versations. They are also grateful to the following 
people who took part in the annotation experiment: 
Paul Clough, George Demetriou, Lisa Ferry, Michael 
Oakes and Andrea Setzer. 
References  
D. Beeferman, A. Berger, and J. Lafferty. 1998. CY- 
BERPUNC: A lightweight punctuation annota- 
tion system for speech. In Proceedings of the IEEE 
International Conference on Acoustics, Speech 
and Signal Processing, pages 689-692, Seattle, 
WA. 
E. Brill. 1992. A simple rule-based part of speech 
tagger. In Proceeding of the Third Conference on 
Applied Natural Language Processing (ANLP-92), 
pages 152-155, Trento, Italy. 
R~ 88
L. Burnard, 1995. Users Reference Guide for the 
British National Corpus. Oxford University Com- 
puting Services. 
J. Carletta. 1996. Assessing agreement on classific- 
ation tasks: the kappa statistic. Computational 
Linguistics, 22(2):249-254. 
N. Chinchor, P. Robinson, and E. Brown. 
1998. HUB-4 Named Entity Task Defini- 
tion (version 4.8). Technical report, SAIC. 
http ://www. nist. gov/speech/hub4-98. 
R. Cole, editor. 1996. Survey of the State of the 
Art in Human Language Technology. Available at: 
http://cslu.cse.ogi.edu/HLTsurvey/HLTsurvey.html. 
Site visited 17/11/99. 
W. Daelemans, J. Zavrel, K. van der Sloot, 
and A. van den Bosch. 1999. TiMBL: Tilburg 
memory based learner version 2.0, reference guide. 
Technical report, ILK Technical Report 98-03. 
ILK Reference Report 99-01, Available from 
http ://ilk. kub. nl/" ilk/papers/ilk9901, ps. gz. 
C. Fellbaum, J. Grabowski, S. Landes, and A. Ban- 
mann. 1998. Matching words to senses in Word- 
Net: Naive vs. expert differentiation of senses. In 
C. Fellbaum, editor, WordNet: An electronic lex- 
ieal database and some applications. MIT Press, 
Cambridge, MA. 
W. Gale and G. Sampson. 1996. Good-Turing 
frequency estimation without tears. Journal of 
Quantitave Linguistics, 2(3):217-37. 
Y. Gotoh and S. Renals. 2000. Information extrac- 
tion from broadcast news. Philosophical Trans- 
actions of the Royal Society of London, series A: 
Mathematical, Physical and Engineering Sciences. 
(to appear). 
A. Mikheev. 1998. Feature lattices for maximum en- 
tropy modelling. In Proceedings of the 36th Meet- 
ing of the Association for Computational Linguist- 
ics (COLING-ACL-98), pages 848-854, Montreal, 
Canada. 
R. Moore, J. Dowding, H. Bratt, J. Gawron, 
Y. Gorfu, and A. Cheyer. 1997. CommandTalk: 
A Spokcaa-Language Interface to Battlefield Simu- 
lations. In Proceedings of the Fifth Conference on 
Applied Natural Language Processing, pages 1-7, 
Washington, DC. 
D. Palmer and M. Hearst. 1994. Adaptive sen- 
tence boundary disambiguation. In Proceedings of 
the 1994 Conference on Applied Natural Language 
Processing, pages 78-83, Stutgart, Germany. 
J. Reynar and A. Ratnaparkhi. 1997. A max- 
imum entropy approach to identifying sentence 
boundries. In Proceedings of the Fifth Conference 
on Applied Natural Language Processing, pages 
16-19, Washington, D.C. 
S. Siegel and N. Castellan. 1988. Nonparametrie 
Statistics for the Behavioural Sciences. McGraw- 
Hill, second edition. 
M. Stevenson and Y. Wilks. 1999. Combining weak 
knowledge sources for sense disambiguation. In 
Proceedings of the Sixteenth International Joint 
Conference on Artificial Intelligence, pages 884- 
889. Stockholm, Sweden. 
C. van Rijsbergen. 1979. Information Retrieval. 
Butterworths, London. 
89 
Using Corpus-derived Name Lists for Named Entity Recognition 
Mark  Stevenson  and Rober t  Ga izauskas  
Depar tment  of  Computer  Science, 
Un ivers i ty  of  Sheff ield 
Regent  Cour t ,  211 Por tobe l lo  S t reet ,  
Sheff ield 
S1 4DP Un i ted  K ingdom 
{marks, robertg}~dcs, shef. ac.uk 
Abst ract  
This paper describes experiments to establish the 
performance of a named entity recognition system 
which builds categorized lists of names from manu- 
ally annotated training data. Names in text are then 
identified using only these lists. This approach does 
not perform as well as state-of-the-art named en- 
tity recognition systems. However, we then show 
that by using simple filtering techniques for improv- 
ing the automatically acquired lists, substantial per- 
formance benefits can be achieved, with resulting F- 
measure scores of 87% on a standard test set. These 
results provide a baseline against which the con- 
tribution of more sophisticated supervised learning 
techniques for NE recognition should be measured. 
1 In t roduct ion  
Named entity (NE) recognition is the process of 
identifying and categorising names in text. Systems 
which have attempted the NE task have, in general, 
made use of lists of common ames to provide clues. 
Name lists provide an extremely efficient way of re- 
cognising names, as the only processing required is 
to match the name pattern in the list against the 
text and no expensive advanced processing such as 
full text parsing is required. However, name lists are 
a naive method for recognising names. McDonald 
(1996) defines internal and external evidence in the 
NE task. The first is found within the name string 
itself, while the second is gathered from its context. 
For example, in the sentence "President Washington 
chopped the tree" the word "President" is clear ex- 
ternal evidence that "Washington" denotes a person. 
In this case internal evidence from the name cannot 
conclusively tell us whether "Washington" is a per- 
son or a location ("Washington, DC"). A NE sys- 
tem based solely on lists of names makes use of only 
internal evidence and examples uch as this demon- 
strate the limitations of this knowledge source. 
Despite these limitations, many NE systems use 
extensive lists of names. Krupke and Hausman 
(1998) made extensive use of name lists in their sys- 
tem. They found that reducing their size by more 
than 90% had little effect on performance, conversely 
adding just 42 entries led to improved results. This 
implies that the quality of list entries is a more im- 
portant factor in their effectiveness than the total 
number of entries. Mikheev et al (1999) experi- 
mented with different ypes of lists in an NE system 
entered for MUC7 (MUC, 1998). They concluded 
that small lists of carefully selected names are as 
effective as more complete lists, a result consistent 
with Krupke and Hausman. However, both studies 
altered name lists within a larger NE system and it 
is difficult to tell whether the consistency of perform- 
ance is due to the changes in lists or extra, external, 
evidence being used to balance against the loss of 
internal evidence. 
In this paper a NE system which uses only the in- 
ternal evidence contained in lists of names is presen- 
ted. Section 3 explains how such lists can be auto- 
matically generated from annotated text. Sections 
4 and 5 describe xperiments in which these corpus- 
generated lists are applied and their performance 
compared against hand-crafted lists. In the next sec- 
tion the NE task is described in further detail. 
2 NE background 
2.1 NE Recognition of Broadcast News 
The NE task itself was first introduced as part of 
the MUC6 (MUC, 1995) evaluation exercise and was 
continued in MUC7 (MUC, 1998). This formulation 
of the NE task defines seven types of NE: PERSON, 
ORGANIZATION, LOCATION, DATE, TIME, MONEY and 
PERCENT. Figure 1 shows a short text marked up 
in SGML with NEs in the MUC style. 
The task was duplicated for the DARPA/N IST  
HUB4 evaluation exercise (Chinchor et al, 1998) 
but this time the corpus to be processed consisted 
of single case transcribed speech, rather than mixed 
case newswire text. Participants were asked to carry 
out NE recognition on North American broadcast 
news stories recorded from radio and television and 
processed by automatic speech recognition (ASR) 
software. The participants were provided with a 
training corpus consisting of around 32,000 words 
of transcribed broadcast news stories from 1997 an- 
notated with NEs. Participants used these text to 
290 
"It's a chance to think about first-level questions," said Ms. <enamex 
type="PERS0N">Cohn<enamex>, a partner in the <enamex type="0RGANIZATION">McGlashan 
Sarrail<enamex> firm in <enamex type="L0CATION">San Mateo<enamex>, <enamex 
type="L0CATION">Calif.<enamex> 
Figure 1: Text with MUC-style NE's marked 
develop their systems and were then provided with 
new, unannotated texts, consisting of transcribed 
broadcast news from 1998 which they were given a 
short time to annotate using their systems and re- 
turn. Participants are not given access to the eval- 
uation data while developing their systems. 
After the evaluation, BBN, one of the parti- 
cipants, released a corpus of 1 million words which 
they had manually annotated to provide their sys- 
tem with more training data. Through the re- 
mainder of this paper we refer to the HUB4 training 
data provided by DARPA/NIST as the SNORT_TRAIN 
corpus and the union of this with the BBN data as 
the LONG_TRAIN corpus. The data used for the 1998 
HUB4 evaluation was kept blind, we did not exam- 
ine the text themselves, and shall be referred to as 
the TEST corpus. 
The systems were evaluated in terms of the com- 
plementary precision (P) and recall (R) metrics. 
Briefly, precision is the proportion of names pro- 
posed by a system which are true names while recall 
is the proportion of the true names which are actu- 
ally identified. These metrics are often combined 
using a weighted harmonic called the F-measure 
(F) calculated according to formula 1 where fl is a 
weighting constant often set to 1. A full explana- 
tion of these metrics is provided by van Rijsbergen 
(1979). 
F= ( f~+l)  xPxR 
(fl ? P) + R (1) 
The best performing system in the MUC7 exercise 
was produced by the Language Technology Group of 
Edinburgh University (Mikheev et al, 1999). This 
achieved an F-measure of 93.39% (broken down as 
a precision of 95% and 92% recall). In HUB4 BBN 
(Miller et al, 1999) produced the best scoring sys- 
tem which achieved an F-measure of 90.56% (preci- 
sion 91%, recall 90%) on the manually transcribed 
test data. 
2.2 A Full NE  sys tem 
The NE system used in this paper is based on Shef- 
field's LaSIE system (Wakao et al, 1996), versions 
of which have participated in MUC and HUB4 eval- 
uation exercises (Renals et al, 1999). The system 
identifies names using a process consisting of four 
main modules: 
List Lookup This module consults several ists of 
likely names and name cues, marking each oc- 
currence in the input text. The name lists in- 
clude lists of organisations, locations and per- 
son first names and the name cue lists of titles 
(eg. "Mister", "Lord"), which are likely to pre- 
cede person names, and company designators 
(eg. "Limited" or "Incorporated"), which are 
likely to follow company names. 
Par t  of speech tagger  The text is the part of 
speech tagged using the Brill tagger (Brill, 
1992). This tags some tokens as "proper name" 
but does not attempt o assign them to a NE 
class (eg. PERSON, LOCATION). 
Name pars ing  Next the text is parsed using a col- 
lection of specialised NE grammars. The gram- 
mar rules identify sequences of part of speech 
tags as added by the List Lookup and Par t  
of  speech tagger  modules. For example, there 
is a rule which says that a phrase consisting 
of a person first name followed by a word part 
of speech tagged as a proper noun is a person 
name. 
Namematch ing  The names identified so far in the 
text are compared against all unidentified se- 
quences of proper nouns produced by the part of 
speech tagger. Such sequences form candidate 
NEs and a set of heuristics is used to determ- 
ine whether any such candidate names match 
any of those already identified. For example one 
such heuristics ays that if a person is identified 
with a title (eg. "President Clinton") then any 
occurrences without the title are also likely to 
be person names '(so "Clinton" on it own would 
also be tagged as a person name). 
For the experiments described in this paper a re- 
stricted version of the system which used only the 
List Lookup module was constructed. The list 
lookup mechanism arks all words contained in any 
of the name lists and each is proposed as a NE. Any 
string occurring in more than one list is assigned the 
category form the first list in which it was found, al- 
though this did not occur in any of the sets of lists 
used in the experiments described here. 
3 L i s t  Generat ion  
The List Lookup module uses a set of hand- 
crafted lists originally created for the MUC6 eval- 
uation. They consisted of lists of names from the 
gazetteers provided for that competition, supple- 
mented by manually added entries. These lists 
291 
evolved for the MUC7 competition with new entries 
and lists being added. For HUB4 we used a se- 
lection of these lists, again manually supplementing 
them where necessary. These lists included lists of 
companies, organisations (such as government de- 
partments), countries and continents, cities, regions 
(such as US states) and person first names as well as 
company designators and person titles. We specu- 
late that this ad hoc, evolutionary, approach to cre- 
ating name lists is quite common amongst systems 
which perform the NE task. 
In order to compare this approach against a simple 
system which gathers together all the names occur- 
ring in NE annotated training text, a program was 
implemented to analyse text annotated in the MUC 
SGML style (see Figure 1) and create lists for each 
NE type found. For example, given the NE <enamex 
type="LOCATION">SAN MATE0<enamex> an entry 
SAN MATE0 would be added a list of locations. 
This simple approach is certainly acceptable for 
the LOCATION, ORGANIZATION and, to a more lim- 
ited extent, PERSON classes. It is less applicable to 
the remaining classes of names (DATE, TIME, MONEY 
and PERCENT) because these are most easily recog- 
nised by their grammatical structure. For example, 
there is a rule in the NE grammar which says a num- 
ber followed by a currency unit is as instance of the 
MONEY name class- eg. FIFTY THREE DOLLARS, FIVE 
MILLION ECU. According to Przbocki et al (1999) 
88% of names occurring in broadcast news text fall 
into one of the LOCATION, ORGANIZATION and PERSON 
categories. 
Two sets of lists were derived, one from 
the SHORT_TRAIN corpus and a second from the 
LONG_TRAIN texts. The lengths of the lists produced 
are shown in Table 1. 
Corpus 
Category SHORT_TRAIN LONG_TRAIN 
ORGANIZATION 245 2,157 
PERSON 252 3,947 
LOCATION 230 1,489 
Table 1: Lengths of lists derived from SHORT_TRAIN 
and LONG_TRAIN corpora 
4 L i s t  App l i ca t ion  
The SHORT_TRAIN and LONG_TRAIN lists were each 
applied in two ways, alone and appended to the ori- 
ginal, manually-created, lists. In addition, we com- 
puted the performance obtained using only the ori- 
ginal lists for comparison. Although both sets of lists 
were derived using the SHORT_TRAIN data (since the 
LONG_TRAIN corpus includes SHORT_TRAIN), we still 
compute the performance of the SHORT_TRAIN lists 
on that corpus since this provides some insight into 
the best possible performance which can be expected 
from NE recognition using a simple list lookup mech- 
anism. No scores were computed for the LONG_TRAIN 
lists against he SHORT_TRAIN corpus since this is un- 
likely to provide more information. 
Table 2 shows the results obtained when the 
SHORT_TRAIN lists were applied to that corpus. This 
first experiment was designed to determine how 
well the list lookup approach would perform given 
lists compiled directly from the corpus to which 
they are being applied. Only PERSON, LOCATION 
and ORGANIZATION name classes are considered since 
they form the majority of names occurring in the 
HUB4 text. As was mentioned previously, the re- 
maining categories of name are more easily recog- 
nised using the NE parser. For each configuration of 
lists the precision, recall and F-measure are calcu- 
lated for the each name class both individually and 
together. 
We can see that the original ists performed reas- 
onably well, scoring an F-measure of 79% overall. 
However, the corpus-based lists performed far bet- 
ter achieving high precision and perfect recall. We 
would expect he system to recognise very name in 
the text, since they are all in the lists, but perfect 
precision is unlikely as this would require that no 
word appeared as both a name and non-name or in 
more than one name class. Even bearing this in mind 
the calculated precision for the ORGANIZATION class 
of names is quite low. Analysis of the output showed 
that several words occurred as names a few times in 
the text but also as non-names more frequently. For 
example, "police" appeared 35 times but only once 
as an organisation; similarly "finance" and "repub- 
lican" occur frequently but only as a name a few 
times. In fact, these three list entries account for 61 
spuriously generated names, from a total of 86 for 
the ORGANIZATION class. The original lists do not 
include words which are likely to generate spurious 
entries and names like "police" would only be recog- 
nised when there was further evidence. 
The SHORT_TRAIN lists contain all the names oc- 
curring in that text. When these lists are combined 
with the original system lists the observed recall re- 
mains 100% while the precision drops. The original 
system lists introduce more spurious entries, leading 
to a drop of 3% F-measure. 
The results of applying the corpus-derived lists to 
the texts from which they were obtained show that, 
even under these circumstances, perfect results can- 
not be obtained. Table 3 shows a more meaningful 
evaluation; the SHORT_TRAIN lists are applied to the 
TEST corpus, an unseen text. The original system 
lists achieve an F-measure of 83% on this text and 
the corpus-derived lists perform 8% worse. However, 
the configuration of lists which performs best is the 
union of the original ists with those derived from the 
292 
Lists Original SHORT_TRAIN Combination 
Name Type P R F P R F P R F 
ALL 86 73 79 94 100 97 88 100 94 
ORGANIZATION 84 49 62 83 100 90 79 100 88 
PERSON 78 71 74 99 100 99 88 100 94 
LOCATION 92 88 90 98 100 99 95 100 97 
Table 2: SHORT_TRAIN lists applied to SHORT_TRAIN corpus 
corpus. This out-performs each set of lists taken in 
isolation both overall and for each name category in- 
dividually. This is clear evidence that the lists used 
by the system described could be improved with the 
addition of lists derived from annotated text. 
It is worth commenting on some of the results for 
individual classes of names in this experiment. We 
can see that the performance for the ORGANIZATION 
class actually increases when the corpus-based lists 
are used. This is partially because names which are 
made up from initials (eg. "C. N. N." and "B. B. C. ") 
are not generally recognised by the list lookup mech- 
anism in our system, but are captured by the 
parser and so were not included in the original lists. 
However, it is also likely that the organisation list is 
lacking, at least to some level. More interestingly, 
there is a very noticeable drop in the performance 
for the PERSON class. The SHORT_TRAIN lists achieved 
an F-measure of 99% on that text but only 48% on 
the TEST text. In Section 2.1 we mentioned that the 
HUB4 training data consists of news stories from 
1997, while the test data contains tories from 1998. 
We therefore suggest hat the decrease in perform- 
ance for the PERSON category demonstrates a general 
property of broadcast news: many person names 
mentioned are specific to a particular time period 
(eg. "Monica Lewinksi" and "Rodney King"). In 
contrast, the locations and organisations mentioned 
are more stable over time. 
Table 4 shows the performance obtained when the 
lists derived from LONG_TRAIN were applied to the 
TEST corpus. The corpus-derived lists perform sig- 
nificantly worse than the original system lists, show- 
ing a large drop in precision. This is to be expec- 
ted since the lists derived from LONG_TRAIN contain 
all the names occurring in a large body of text and 
therefore contain many words and phrases which are 
not names in this text, but spuriously match non- 
names. Although the F-measure result is worse than 
when the SHORT_TRAIN lists were used, the recall 
is higher showing that a higher proportion of the 
true names can be found by analysing a larger body 
of text. Combining the original and corpus-derived 
lists leads to a 1% improvement. Recall is noticeably 
improved compared with the original lists, however 
precision is lowered and this shows that the corpus- 
derived lists introduce a large number of spurious 
names. 
From this first set of experiments it can be seen 
that perfect results will not be obtained even using 
lists contain all and only the names in a particular 
text, thus demonstrating the limitations of this na- 
ive approach to named entity recognition. We have 
also demonstrated that it is possible for the addi- 
tion of corpus-derived lists to improve the perform- 
ance of a NE recognition system based on gazetteers. 
However, this is not guaranteed and it appears that 
adding too many names without any restriction may 
actually lead to poorer results, as happened when 
the LONG_TRAIN lists were applied. 
5 F i l te r ing  L is ts  
The results from our first set of experiments led us to 
question whether it is possible to restrict the entries 
being added to the lists in order to avoid those likely 
to generate spurious names. We now go on to de- 
scribe some methods which can be used to identify 
and remove list entries which may generate spurious 
names. 
Method 1: Dic t ionary  F i l te r ing  The derived 
lists can be improved by removing items in the 
list which also occur as entries in a dictionary. 
We began by taking the Longman Dictionary of 
Contemporary Englisb (LDOCE) (Procter, 1978) and 
extracting a list of words it contained including all 
derived forms, for example pluralisation of nouns 
and different verb forms. This produced a list of 
52,576 tokens which could be used to filter name 
lists. 
Method 2: Probability F i l te r ing  The lists can 
be improved by removing names which occur 
more frequently in the corpus as non-names 
than names. 
Another method for filtering lists was imple- 
mented, this time using the relative frequencies of 
phrases occurring as names and non-names. We can 
extract the probability that a phrase occurs as a 
name in the training corpus by dividing the num- 
ber of times it occurs as a name by the total number 
of corpus occurrences. If this probability estimate is 
an accurate reflection of the name's behaviour in a 
293 
Lists Original SHORT_TI~IN Combination 
Name Type P R F P R F P R F 
ALL 86 79 83 90 65 75 83 86 84 
ORGANIZATION 82 57 67 76 66 71 79 81 80 
PERSON 77 80 78 93 32 48 79 83 81 
LOCATION 93 89 91 97 81 88 92 94 93 
Table 3: SHORT_TRAIN \]ists applied to TEST corpus 
Lists Original LONG_TRAIN Combination 
Name Type P R F P R F P R F 
ALL 86 79 83 64 86 73 62 91 74 
ORGANIZATION 82 57 67 44 85 58 43 88 58 
PERSON 77 80 78 55 75 63 53 86 66 
LOCATION 93 89 91 87 92 89 84 94 89 
Table 4: LONG_TRAIN lists applied to TEST corpus 
new text we can use it to estimate the accuracy of 
adding that name to the list. Adding a name to a 
list will lead to a recall score of 1 for that name and 
a precision of Pr  (where Pr  is the probability value 
estimated from the training corpus) which implies an 
F-measure of ~.2Pr 1 Therefore the probabilities can 
be used to filter out candidate list items which imply 
low F-measure scores. We chose names whose cor- 
pus probabilities produced an F-measure lower than 
the overall score for the list. The LONG_TRAIN lists 
scored an F-measure of 73% on the unseen, TEST, 
data (see Table 4). Hence a filtering probability of 
73% was used for these lists, with the corpus stat- 
istics gathered from LONG_TRAIN. 
Method  3: Combin ing  F i l ters  These filtering 
strategies can be improved by combining them. 
We also combined these two filtering strategies in 
two ways. Firstly, all names which appeared in the 
lexicon or whose corpus probability is below the fil- 
tering probability are removed from the lists. This is 
dubbed the "or combination". The second combin- 
ation strategy removes any names which appear in 
the lexicon and occur with a corpus frequency below 
the filtering probability are removed. This second 
strategy is called the "and combination". 
These filtering strategies were applied to the 
LONG_TRAIN lists. The lengths of the lists produced 
are shown in Table 5. 
The strategies were evaluated by applying the 
filtered LONG_TRAIN lists to the TEST corpus, the res- 
ults of which are shown in Table 6. There is an 
1Analysis of the behaviour of the function f (P r )  -- 2P~ l+Pr  
shows that it does not deviate too far from the value of Pr  (ie. 
. f (P r )  ~ Pr )  and so there is an argument for simply filtering 
the lists using the raw probabilities. 
improvement in performance of 4% F-measure when 
lists filtered using the "and" combination are used 
compared to the original, hand-crafted, lists. Al- 
though this approach removes only 108 items from 
all the lists there is a 14% F-measure improvement 
over the un-filtered lists. Each filtering strategy used 
individually demonstrates a lower level of improve- 
ment: the dictionary filtered lists 12% and the prob- 
ability filtered 10%. 
The "and" combination is more successful be- 
cause filtering lists using the dictionary alone re- 
moves many names we would like to keep (eg. coun- 
try names are listed in LDOCE) but many of these 
are retained since both filters must agree. These 
experiments demonstrate hat appropriately filtered 
corpus-derived lists can be more effective for NE re- 
cognition than hand-crafted lists. The difference 
between the observed performance of our simple 
method and those reported for the best-performing 
HUB4 system is perhaps lower that one may ex- 
pect. The BBN system achieved 90.56% overall, 
and about 92% when only the PERSON, LOCATION 
and ORGANIZATION name classes are considered, 5% 
more than the method reported here. This difference 
is perhaps lower than we might expect given that 
name lists use only internal evidence (in the sense 
of Section 1). This indicates that simple application 
of the information contained in manually annotated 
NE training data can contribute massively to the 
overall performance of a system. They also provide 
a baseline against which the contribution of more 
sophisticated supervised learning techniques for NE 
recognition should be measured. 
294 
NE 
Category 
ORGANIZATION 
PERSON 
LOCATION 
Un-Filtered Dictionary Probability 
List Filtered Filtered 
2,157 1,978 2,000 
3,947 3,769 3,235 
1,489 1,412 1,364 
Or 
Combined 
1,964 
3,522 
1,382 
And 
Combined 
2,049 
3,809 
1,449 
Table 5: Lengths of corpus-derived lists 
Original t Un-Filtered Dictionary I Probability Or And 
Lists Lists Filtered Filtered Combination Combination 
Name Type P R F P R F P R F P R F P R F P R F 
ALL 
ORGANIZATION 
PERSON 
LOCATION 
86 79 83 
82 57 67 
77 80 78 
93 89 91 
64 86 73 
44 85 58 
55 75 63 
87 92 89 
95 79 85 
86 72 78 
96 66 78 
98 89 93 
96 73 83 
85 74 79 
96 40 56 
97 90 93 
95 73 83 
84 60 70 
100 49 66 
98 90 94 
93 81 87 
84 76 80 
94 66 78 
97 92 94 
Table 6: Filtered and un-filtered LONG_TRAIN lists applied to TEST corpus 
6 Conclusion 
This paper explored the role of lists of names in 
NE recognition, comparing hand-crafted and corpus- 
derived lists. It was shown that, under certain condi- 
tions, corpus-derived lists outperform hand-crafted 
ones. Also, supplementing hand-crafted lists with 
corpus-based ones often improves their performance. 
The reported method was more effective for the 
ORGANIZATION and LOCATION classes of names than 
for PERSON, which was attributed to the fact that 
reportage of these names does not change as much 
over time in broadcast news. 
The method reported here achieves 87% F- 
measure, 5% less than the best performing system 
in the HUB4 evaluation. However, it should be re- 
membered that this technique uses only a simple ap- 
plication of internal evidence. 
References 
E. Brill. 1992. A simple rule-based part of speech 
tagger. In Proceeding of the Third Conference on 
Applied Natural Language Processing (ANLP-92), 
pages 152-155, Trento, Italy. 
N. Chinchor, P. Robinson, and E. Brown. 
1998. Hub-4 named entity task defini- 
tion (version 4.8). Technical report, SAIC. 
http ://www. nist. gov/speech/hub4_98. 
G. Krupke and K. Hausman. 1998. Isoquest Inc: 
description of the NetOwl(TM) extractor system 
as used for MUC-7. In Message Understanding 
Conference Proceedings: MUC 7. Available from 
http ://www.muc. saic. com. 
D. McDonald. 1996. Internal and external evid- 
ence in the identification and semantic ategor- 
ization of proper names. In B. Boguraev and 
J. Pustejovsky, editors, Corpus Processing for 
Lexical Aquisition, chapter 2, pages 21-39. MIT 
Press, Cambridge, MA. 
A. Mikheev, M. Moens, and C. Grovel 1999. 
Named entity recognition without gazeteers. In 
Proceedings of the Ninth Conference of the 
European Chapter of the Association for Compu- 
tational Linguistics, pages 1-8, Bergen, Norway. 
D. Miller, R. Schwartz, R. Weischedel, and R. Stone. 
1999. Named entity extraction from broadcast 
news. In Proceedings of the DARPA Broadcast 
News Workshop, ages 37-40, I-Ierndon, Virginia. 
MUC. 1995. Proceedings of the Sixth Message Un- 
derstanding Conference (MUC-6}, San Mateo, 
CA. Morgan Kaufmann. 
1998. Message Understanding Conference Proceed- 
ings: MUC7.  http ://www.muc. sale com. 
P. Procter, editor. 1978. Longman Dictionary of 
Contemporary English. Longman Group, Essex, 
UK. 
M. Przbocki, J. Fiscus, J. Garofolo, and D. Pallett. 
1999. 1998 HUB4 Information Extraction Eval- 
uation. In Proceedings of the DARPA Broadcast 
News Workshop, ages 13-18, Herndon, Virginia. 
S. Renals, Y. Gotoh, R. Gaizausaks, and M. Steven- 
son. 1999. Baseline IE-NE Experimants Using the 
SPRACH/LASIE System. In Proceedings of the 
DAPRA Broadcast News Workshop, ages 47-50, 
Herndon, Virginia. 
C. van Rijsbergen. 1979. Information Retrieval. 
Butterworths, London. 
T. Wakao, R. Gaizauskas, and K. Humphreys. 1996. 
Evaluation of an algorithm for the recognition and 
classification of proper names. In Proceedings of 
the 16th International Conference on Computa- 
tional Linguistics (COLING-96), pages 418-423, 
Copenhagen, Denmark. 
295 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 809?816
Manchester, August 2008
Acquiring Sense Tagged Examples using Relevance Feedback
Mark Stevenson, Yikun Guo and Robert Gaizauskas
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield, S1 4DP
United Kingdom
inital.surname@dcs.shef.ac.uk
Abstract
Supervised approaches to Word Sense Dis-
ambiguation (WSD) have been shown to
outperform other approaches but are ham-
pered by reliance on labeled training ex-
amples (the data acquisition bottleneck).
This paper presents a novel approach to the
automatic acquisition of labeled examples
for WSD which makes use of the Informa-
tion Retrieval technique of relevance feed-
back. This semi-supervised method gener-
ates additional labeled examples based on
existing annotated data. Our approach is
applied to a set of ambiguous terms from
biomedical journal articles and found to
significantly improve the performance of a
state-of-the-art WSD system.
1 Introduction
The resolution of lexical ambiguities has long been
considered an important part of the process of
understanding natural language. Supervised ap-
proaches to Word Sense Disambiguation (WSD)
have been shown to perform better than unsuper-
vised ones (Agirre and Edmonds, 2007) but require
examples of ambiguous words used in context an-
notated with the appropriate sense (labeled exam-
ples). However these often prove difficult to obtain
since manual sense annotation of text is a complex
and time consuming process. In fact, Ng (1997)
estimated that 16 person years of manual effort
would be required to create enough labeled exam-
ples to train a wide-coverage WSD system. This
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
limitation is commonly referred to as the data ac-
quisition bottleneck. It is particularly acute in spe-
cific domains, such as biomedicine, where terms
may have technical usages which only domain ex-
perts are likely to be aware of. For example, pos-
sible meanings of the term ?ganglion? in UMLS
(Humphreys et al, 1998) include ?neural structure?
or ?benign mucinous tumour?, although only the
first meaning is listed in WordNet. These domain-
specific semantic distinctions make manual sense
annotation all the more difficult.
One approach to the data acquisition bottleneck
is to generate labeled training examples automat-
ically. Others, such as Leacock et al (1998) and
Agirre and Mart??nez (2004b), used information
from WordNet to construct queries which were
used to retrieve training examples. This paper
presents a novel approach to this problem. Rele-
vance feedback, a technique used in Information
Retrieval (IR) to improve search results, is adapted
to identify further examples for each sense of am-
biguous terms. These examples are then used
to train a semi-supervised WSD system either by
combining them with existing annotated data or
using them alone. The approach is applied to a set
of ambiguous terms in biomedical texts, a domain
for which existing resources containing labeled ex-
amples, such as the NLM-WSD data set (Weeber
et al, 2001), are limited.
The next section outlines previous techniques
which have been used to avoid the data acquisi-
tion bottleneck. Section 3 describes our approach
based on relevance feedback. The WSD system we
use is described in Section 4. Section 5 describes
experiments carried out to determine the useful-
ness of the automatically retrieved examples. The
final section summarises conclusions which can be
drawn from this work and outlines future work.
809
2 Previous Approaches
A variety of approaches to the data acquisition bot-
tleneck have been proposed. One is to use un-
supervised algorithms, which do not require la-
beled training data. Examples include Lesk (1986)
who disambiguated ambiguous words by examin-
ing their dictionary definitions and selecting the
sense whose definition overlapped most with def-
initions of words in the ambiguous word?s con-
text. Leroy and Rindflesch (2005) presented an
unsupervised approach to WSD in the biomedi-
cal domain using information derived from UMLS
(Humphreys et al, 1998).
However, results from SemEval (Agirre et al,
2007) and its predecessors have shown that su-
pervised approaches to WSD generally outperform
unsupervised ones. It has also been shown that re-
sults obtained from supervised methods improve
with access to additional labeled data for training
(Ng, 1997). Consequently various techniques for
automatically generating training data have been
developed.
One approach makes use of the fact that differ-
ent senses of ambiguous words often have different
translations (e.g. Ng et al (2003)). Parallel text is
used as training data with the alternative transla-
tions serving as sense labels. However, disadvan-
tages of this approach are that the alternative trans-
lations do not always correspond to the sense dis-
tinctions in the original language and parallel text
is not always available.
Another approach, developed by Leacock et
al. (1998) and extended by Agirre and Mart??nez
(2004b), is to examine a lexical resource, Word-
Net in both cases, to identify unambiguous terms
which are closely related to each of the senses of an
ambiguous term. These ?monosemous relatives?
are used to as query terms for a search engine and
the examples returned used as additional training
data.
In the biomedical domain, Humphrey et al
(2006) use journal descriptors to train models
based on the terms which are likely to co-occur
with each sense. Liu et al (2002) used informa-
tion in UMLS to disambiguate automatically re-
trieved examples which were then used as labeled
training data. The meanings of 35 ambiguous ab-
breviations were identified by examining the close-
ness of concepts in the same abstract in UMLS.
Widdows et al (2003) employ a similar approach,
although their method also makes use of parallel
corpora when available.
All of these approaches rely on the existence of
an external resource (e.g. parallel text or a domain
ontology). In this paper we present a novel ap-
proach, inspired by the relevance feedback tech-
nique used in IR, which automatically identifes ad-
ditional training examples using existing labeled
data.
3 Generating Examples using Relevance
Feedback
The aim of relevance feedback is to generate im-
proved search queries based on manual analysis of
a set of retrieved documents which has been shown
to improve search precision (Salton, 1971; Robert-
son and Spark Jones, 1976). Variations of rele-
vance feedback have been developed for a range of
IR models including Vector Space and probabilis-
tic models. The formulation of relevance feedback
for the Vector Space Model is most pertinent to our
approach.
Given a collection of documents, C, containing
a set of terms, C
terms
, a basic premise of the Vec-
tor Space Model is that documents and queries can
be represented by vectors whose dimensions repre-
sent the C
terms
. Relevance feedback assumes that
a retrieval system returns a set of documents, D,
for some query, q. It is also assumed that a user
has examined D and identified some of the docu-
ments as relevant to q and others as not relevant.
Relevant documents are denoted by D
+q
and the
irrelevant as D
?q
, where D
+q
? D, D
?q
? D
and D
+q
?D
?q
= ?. This information is used to
create a modified query, q
m
, which should be more
accurate than q. A standard approach to construct-
ing q
m
was described by Rocchio (1971):
q
m
= ?q+
?
|D
+q
|
?
?d?D
+q
d ?
?
|D
?q
|
?
?d?D
?q
d (1)
where the parameters ?, ? and ? are set for partic-
ular applications. Rocchio (1971) set ? to 1.
Our scenario is similar to the relevance feedback
problem since the sense tagged examples provide
information about the documents in which a par-
ticular meaning of an ambiguous term is likely to
be found. By identifying the features which dis-
tinguish the documents containing one sense from
the others we can create queries which can then be
used to retrieve further examples of the ambiguous
words used in the same sense. However, unlike
810
score(t, s) = idf(t)?
?
?
?
|D
+s
|
?
?d?D
+s
count(t, d)?
?
|D
?s
|
?
?d?D
?s
count(t, d)
?
?
(2)
the relevance feedback scenario there is no origi-
nal query to modify. Consequently we start with
a query containing just the ambiguous term and
use relevance feedback to generate queries which
aim to retrieve documents where that term is being
used in a particular sense.
The remainder of this section describes how this
approach is applied in more detail.
3.1 Corpus Analysis
The first stage of our process is to analyse the la-
beled examples and identify good search terms.
For each sense of an ambiguous term, s, the la-
beled examples are divided into two sets: those
annotated with the sense in question and the re-
mainder (annotated with another sense). In rele-
vance feedback terminology the documents anno-
tated with the sense in question are considered to
be relevant and the remainder irrelevant. These ex-
amples are denoted by D
+s
and D
?s
respectively.
At its core relevance feedback, as outlined
above, aims to discover how accurately each term
in the collection discriminates between relevant
and irrelevant documents. This approach was used
to inspire a technique for identifying terms which
are likely to indicate the sense in which an am-
biguous word is being used. We compute a single
score for each term, reflecting its indicativeness of
that sense, using the formula in equation 2, where
count(t, d) is the number of times term t occurs in
document d and idf(t) is the inverse document fre-
quency term weighting function commonly used in
IR. We compute idf as follows:
idf(t) = log
|C|
df(t)
(3)
where D is the set of all annotated examples (i.e.
D = D
+s
? D
?s
) and df(t) the number of docu-
ments in C which contain t.
1
In our experiments the ? and ? parameters in
equation 2 are set to 1. Documents are lemma-
tised and stopwords removed before computing
relevance scores.
1
Our computation of idf(t) is based on only information
from the labeled examples, i.e. we assume C = D
+s
?D
?s
.
Alternatively idf could be computed over a larger corpus of
labeled and unlabeled examples.
Table 1 shows the ten terms with the highest
relevance score for two senses of the term ?cul-
ture? in UMLS: ?laboratory culture? (?In periph-
eral blood mononuclear cell culture streptococcal
erythrogenic toxins are able to stimulate trypto-
phan degradation in humans?) and ?anthropolog-
ical culture? (?The aim of this paper is to de-
scribe the origins, initial steps and strategy, cur-
rent progress and main accomplishments of intro-
ducing a quality management culture within the
healthcare system in Poland.?).
?anthropological culture? ?laboratory culture?
cultural 26.17 suggest 6.32
recommendation 14.82 protein 6.13
force 14.80 presence 5.86
ethnic 14.79 demonstrate 5.86
practice 14.76 analysis 5.78
man 14.76 gene 5.58
problem 13.04 compare 5.47
assessment 12.94 level 5.36
experience 11.60 response 5.35
consider 11.58 data 5.35
Table 1: Relevant terms for two senses of ?culture?
3.2 Query Generation
Unlike the traditional formulation of relevance
feedback there is no initial query. To create a
query designed to retrieve examples of each sense
we simply combine the ambiguous term and the
n terms with the highest relevance scores. We
found that using the three highest ranked terms
provided good results. So, for example, the queries
generated for the two senses of culture shown
in Table 1 would be ?culture cultural
recommendation force? and ?culture
suggest protein presence?.
3.3 Example Collection
The next stage is to collect a set of examples using
the generated queries. We use the Entrez retrieval
system (http://www.ncbi.nlm.nih.gov/
sites/gquery) which provides an online in-
terface for carrying out boolean queries over the
PubMed database of biomedical journal abstracts.
Agirre and Mart??nez (2004b) showed that it is
important to preserve the bias of the original cor-
pus when automatically retrieving examples and
811
consequently the number retrieved for each sense
is kept in proportion to the original corpus. For
example, if our existing labeled examples contain
75 usages of ?culture? in the ?laboratoy culture?
sense and 25 meaning ?anthropological culture? we
would ensure that 75% of the examples returned
would refer to the first sense and 25% to the sec-
ond.
Unsurprisingly, we found that the most useful
abstracts for a particular sense are the ones which
contain more of the relevant terms identified using
the process in Section 3.1. However, if too many
terms are included Entrez may not return any ab-
stracts. To ensure that a sufficient number of ab-
stracts are returned we implemented a process of
query relaxation which begins by querying Entrez
with the most specific query for set of terms. If that
query matches enough abstracts these are retrieved
and the search for labeled examples for the rele-
vant sense considered complete. However, if that
query does not match enough abstracts it is relaxed
and Entrez queried again. This process is repeated
until enough examples can be retrieved for a par-
ticular sense.
The process of relaxing queries is carried out as
follows. Assume we have an ambiguous term, a,
and a set of terms T identified using the process
in Section 3.1. The first, most specific query,
is formed from the conjunction of all terms in
a ? T , i.e. ?a and t
1
AND t
2
AND ... t
|T |
?.
This is referred to as the level |T | query. If
this query does not return enough abstracts the
more relaxed level |T | ? 1 query is formed.
This query returns documents which include the
ambiguous word and all but one of the terms in T :
?a AND ((t
1
AND t
2
AND ... AND t
n?1
) OR
(t
1
AND t
2
AND ... t
n?2
AND t
n
) OR ... OR
(t
2
AND t
3
... AND t
n
))?. Similarly, level
|T | ? 2 queries return documents containing the
ambiguous term and all but two of the terms
in T . Level 1 queries, the most relaxed, return
documents containing the ambiguous term and
one of the terms in T . We do not use just the
ambiguous term as the query since this does not
contain any information which could discriminate
between the possible meanings. Figure 1 shows
the queries which are formed for the ambigu-
ous term ?culture? and the three most salient
terms identified for the ?anthropological culture?
sense. The ?matches? column lists the number
of PubMed abstracts the query matches. It can
be seen that there are no matches for the level 3
query and 83 for the more relaxed level 2 query.
For this sense, abstracts returned by the level 2
query would be used if 83 or fewer examples were
required, otherwise abstracts returned by the level
1 query would be used.
Note that the queries submitted to Entrez are re-
stricted so the terms only match against the title
and abstract of the PubMed articles. This avoids
spurious matches against other parts of the records
including metadata and authors? names.
4 WSD System
The basis of our WSD system was developed by
Agirre and Mart??nez (2004a) and participated in
the Senseval-3 challenge (Mihalcea et al, 2004)
with a performance which was close to the best
system for the English and Basque lexical sample
tasks. The system has been adapted to the biomed-
ical domain (Stevenson et al, 2008) and has the
best reported results over the NLM-WSD corpus
(Weeber et al, 2001), a standard data set for eval-
uation of WSD algorithms in this domain.
The system uses a wide range of features which
are commonly employed for WSD:
Local collocations: A total of 41 features which
extensively describe the context of the ambiguous
word and fall into two main types: (1) bigrams
and trigrams containing the ambiguous word con-
structed from lemmas, word forms or PoS tags,
and (2) preceding/following lemma/word-form of
the content words (adjective, adverb, noun and
verb) in the same sentence with the target word.
Syntactic Dependencies: This feature mod-
els longer-distance dependencies of the ambiguous
words than can be represented by the local colloca-
tions. Five relations are extracted: object, subject,
noun-modifier, preposition and sibling. These are
identified using heuristic patterns and regular ex-
pressions applied to PoS tag sequences around the
ambiguous word (Agirre and Mart??nez, 2004a).
Salient bigrams: Salient bigrams within the ab-
stract with high log-likelihood scores, as described
by Pedersen (2001).
Unigrams: Lemmas of all content words
(nouns, verbs, adjectives, adverbs) in the target
word?s sentence and, as a separate feature, lem-
mas of all content words within a 4-word window
around the target word, excluding those in a list
of corpus-specific stopwords (e.g. ?ABSTRACT?,
?CONCLUSION?). In addition, the lemmas of any
812
Level Matches Query
3 0 culture AND (cultural AND recommendation AND force)
2 83 culture AND ((cultural AND recommendation) OR (cultural AND force) OR
(recommendation AND force))
1 6,358 culture AND (cultural OR recommendation OR force)
Figure 1: Examples of various query levels
unigrams which appear at least twice in the en-
tire corpus which are found in the abstract are also
included as features. This feature was not used
by Agirre and Mart??nez (2004a), but Joshi et al
(2005) found them to be useful for this task.
Features are combined using the Vector Space
Model, a memory-based learning algorithm (see
Agirre and Mart??nez (2004a)). Each occurrence
of an ambiguous word is represented as a binary
vector in which each position indicates the oc-
currence/absence of a feature. A single centroid
vector is generated for each sense during training.
These centroids are compared with the vectors that
represent new examples using the cosine metric to
compute similarity. The sense assigned to a new
example is that of the closest centroid.
5 Experiments
5.1 Setup
The NLM-WSD corpus Weeber et al (2001) was
used for evaluation. It contains 100 examples of 50
ambiguous terms which occur frequently in MED-
LINE. Each example consists of the abstract from
a biomedical journal article which contains an in-
stance of the ambiguous terms which has been
manually annotated with a UMLS concept.
The 50 ambiguous terms which form the NLM-
WSD data set represent a range of challenges for
WSD systems. Various researchers (Liu et al,
2004; Leroy and Rindflesch, 2005; Joshi et al,
2005; McInnes et al, 2007) chose to exclude some
of the terms (generally those with highly skewed
sense distributions or low inter-annotator agree-
ment) and evaluated their systems against a subset
of the terms. The number of terms in these subsets
range between 9 and 28. The Most Frequent Sense
(MFS) heuristic has become a standard baseline in
WSD (McCarthy et al, 2004) and is simply the ac-
curacy which would be obtained by assigning the
most common meaning of a term to all of its in-
stances in a corpus. The MFS for the whole NLM-
WSD corpus is 78% and ranges between 69.9%
and 54.9% for the various subsets. We report re-
sults across the NLM-WSD corpus and four sub-
sets from the literature for completeness.
The approach described in Section 3 was ap-
plied to the NLM-WSD data set. 10-fold cross
validation is used for all experiments. Conse-
quently 10 instances of each ambiguous term were
held back for testing during each fold and addi-
tional examples generated by examining the 90 re-
maining instances. Three sets of labeled examples
were generated for each fold, containing 90, 180
and 270 examples for each ambiguous term. The
NLM-WSD corpus represents the only reliably la-
beled data to which we have access and is used to
evaluate all approaches (that is, systems trained on
combinations of the NLM-WSD corpus and/or the
automatically generated examples).
5.2 Results
Various WSD systems were created. The ?basic?
system was trained using only the NLM-WSD data
set and was used as a benchmark. Three systems,
?+90?, ?+180? and ?+270? were trained using the
combination of the NLM-WSD data set and, re-
spectively, the 90, 180 and 270 automatically re-
trieved examples for each term. A further three
systems, ?90?, ?180? and ?270? were trained us-
ing only the automatically retrieved examples.
The performance of our system is shown in Ta-
ble 2. The part of the table labeled ?Subsets prop-
erties? lists the number of terms in each subset of
the NLM-WSD corpus and the relevant MFS base-
line.
Adding the first 90 automatically retrieved ex-
amples (?+90? column) significantly improves per-
formance of our system from 87.2%, over all
words, to 88.5% (Wilcoxon Signed Ranks Test,
p < 0.01). Improvements are observed over
all subsets of the NLM-WSD corpus. Although
the improvements may seem modest they should
be understood in the context of the WSD system
we are using which has exceeded previously re-
ported performance figures and therefore repre-
sents a high baseline.
Table 2 also shows that adding more auto-
matically retrieved examples (?+180? and ?+270?
columns) causes a drop in performance and re-
813
Subset Properties Combined New only
Subset Terms MFS
basic
+90 +180 +270 90 180 270
All words 50 78.0 87.2 88.5 87.0 86.1 85.6 84.5 82.7
Joshi et. al. 28 66.9 82.3 83.8 81.6 80.9 79.8 78.0 76.3
Liu et. al. 22 69.9 77.8 79.6 76.9 76.1 74.9 72.0 70.9
Leroy 15 55.3 84.3 85.9 84.4 83.6 81.2 80.0 78.0
McInnes et. al. 9 54.9 79.6 81.8 80.4 79.4 75.2 73.0 71.4
Table 2: Performance of system using a variety of combinations of training examples
sults using these examples are worse than using the
NLM-WSD corpus alone. The query relaxation
process, outlined in Section 3.3, uses less discrim-
inating queries when more examples are required
and it is likely that this is leading to noise in the
training examples.
The rightmost portion of Table 2 shows perfor-
mance when the system is trained using only the
automatically generated examples which is con-
sistently worse than using the NLM-WSD corpus
alone. Performance also decreases as more exam-
ples are added. However, results obtained using
only the automatically generated training exam-
ples are consistently better than the relevant base-
line.
Table 3 shows the performance of the sys-
tem trained on the NLM-WSD data set compared
against training using only the 90 automatically
generated examples for each ambiguous term in
the NLM-WSD corpus. It can be seen that there
is a wide variation between the performance of
the additional examples compared with the origi-
nal corpus. For 11 terms training using the addi-
tional examples alone is more effective than using
the NLM-WSD corpus. However, there are several
words for which the performance using the auto-
matically acquired examples is considerably worse
than using the NLM-WSD corpus.
Information about the performance of a system
trained using only the 90 automatically acquired
examples can be used to boost WSD performance
further. In this scenario, which we refer to as ex-
ample filtering, the system has a choice whether
to make use of the additional training data or not.
For each word, performance of the WSD system
trained using only the 90 automatically acquired
examples is compared against the one trained on
the NLM-WSD data set (i.e. results shown in Ta-
ble 3). If the performance is as good, or better,
then the additional examples are used, otherwise
only examples in the NLM-WSD corpus are used
as training data. Since the annotated examples in
the NLM-WSD corpus have already been exam-
ined to generate the additional examples, example
filtering does not require any more labeled data.
Results obtained when example filtering is used
are shown in Table 4. The columns ?+90(f)?,
?+180(f)? and ?+270(f)? show performance when
the relevant set of examples is filtered. (Note
that all three sets of examples are filtered against
the performance of the first 90 examples, i.e. re-
sults shown in Table 3.) This table shows that
example filtering improves performance when the
WSD system is trained using the automatically re-
trieved examples. Performance using the first 90
filtered examples (?+90(f)? column) is 89%, over
all words, compared with 88.5% when filtering is
not used. While performance decreases as larger
sets of examples are used, results using each of the
three sets of filtered examples is signifcantly bet-
ter than the basic system (Wilcoxon Signed Ranks
Test, p < 0.01 for ?+90(f)? and ?+180(f)?, p <
0.05 for ?+270(f)?).
6 Conclusion and Future Work
This paper has presented a novel approach to the
data acquisition bottleneck for WSD. Our tech-
nique is inspired by the relevance feedback tech-
nique from IR. This is a semi-supervised approach
which generates labeled examples using available
sense annotated data and, unlike previously pub-
lished approaches, does not rely on external re-
sources such as parallel text or an ontology. Eval-
uation was carried out on a WSD task from the
biomedical domain for which the number of la-
beled examples available for each ambiguous term
is limited. The automatically acquired examples
improve the performance of a WSD system which
has already been shown to exceed previously pub-
lished results.
The approach presented in this paper could be
extended in several ways. Our experiments focus
814
basic +90(f) +180(f) +270(f)
All words 87.2 89.0 88.2 87.9
Joshi et. al. 82.3 84.6 83.5 83.3
Liu et. al. 84.3 86.6 85.7 85.5
Leroy 77.8 80.3 79.1 78.5
McInnes et. al. 79.6 82.4 81.6 80.8
Table 4: Performance using example filtering
on the biomedical domain. The relevance feedback
approach could be applied to other lexical ambi-
guities found in biomedical texts, such as abbre-
viations with multiple expansions (e.g. Liu et al
(2002)), or to WSD of general text, possibly using
the SemEval data for evaluation.
Future work will explore alternative methods for
generating query terms including other types of
relevance feedback and lexical association mea-
sures (e.g. Chi-squared and mutual information).
Experiments described here rely on a boolean IR
engine (Entrez). It is possible that an IR sys-
tem which takes term weights into account could
lead to the retrieval of more useful MEDLINE ab-
stracts. Finally, it would be interesting to explore
the relation between query relaxation and the use-
fulness of the retrieved abstracts.
Acknowledgments
The authors are grateful to David Martinez for the
use of his WSD system for these experiments and
to feedback provided by three anonymous review-
ers. This work was funded by the UK Engineer-
ing and Physical Sciences Research Council, grant
number EP/E004350/1.
References
E. Agirre and P. Edmonds, editors. 2007. Word
Sense Disambiguation: Algorithms and Applica-
tions. Text, Speech and Language Technology.
Springer.
E. Agirre and D. Mart??nez. 2004a. The Basque Coun-
try University system: English and Basque tasks. In
Rada Mihalcea and Phil Edmonds, editors, Senseval-
3: Third International Workshop on the Evaluation
of Systems for the Semantic Analysis of Text, pages
44?48, Barcelona, Spain, July.
E. Agirre and D. Mart??nez. 2004b. Unsupervised WSD
based on automatically retrieved examples: The im-
portance of bias. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-04), Barcelona, Spain.
E. Agirre, L. Marquez, and R. Wicentowski, editors.
2007. SemEval 2007: Proceedings of the 4th
International Workshop on Semantic Evaluations,
Prague, Czech Republic.
S. Humphrey, W. Rogers, H. Kilicoglu, D. Demner-
Fushman, and T. Rindflesch. 2006. Word Sense
Disambiguation by selecting the best semantic type
based on Journal Descriptor Indexing: Preliminary
experiment. Journal of the American Society for In-
formation Science and Technology, 57(5):96?113.
L. Humphreys, D. Lindberg, H. Schoolman, and
G. Barnett. 1998. The Unified Medical Language
System: An Informatics Research Collaboration.
Journal of the American Medical Informatics Asso-
ciation, 1(5):1?11.
M. Joshi, T. Pedersen, and R. Maclin. 2005. A Com-
parative Study of Support Vector Machines Applied
to the Word Sense Disambiguation Problem for the
Medical Domain. In Proceedings of the Second In-
dian Conference on Artificial Intelligence (IICAI-
05), pages 3449?3468, Pune, India.
C. Leacock, M. Chodorow, and G. Miller. 1998.
Using corpus statistics and WordNet relations for
sense identification. Computational Linguistics,
24(1):147?165.
G. Leroy and T. Rindflesch. 2005. Effects of Infor-
mation and Machine Learning algorithms on Word
Sense Disambiguation with small datasets. Interna-
tional Journal of Medical Informatics, 74(7-8):573?
585.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proceedings of
ACM SIGDOC Conference, pages 24?26, Toronto,
Canada.
H. Liu, S. Johnson, and C. Friedman. 2002. Automatic
Resolution of Ambiguous Terms Based on Machine
Learning and Conceptual Relations in the UMLS.
Journal of the American Medical Informatics Asso-
ciation, 9(6):621?636.
H. Liu, V. Teller, and C. Friedman. 2004. A Multi-
aspect Comparison Study of Supervised Word Sense
Disambiguation. Journal of the American Medical
Informatics Association, 11(4):320?331.
815
word basic 90 ?
adjustment 71 70 -1
association 100 100 0
blood pressure 48 50 2
cold 88 86 -2
condition 89 90 1
culture 96 91 -5
degree 96 86 -10
depression 88 85 -3
determination 87 82 -5
discharge 95 92 -3
energy 98 99 1
evaluation 76 75 -1
extraction 85 82 -3
failure 66 71 5
fat 85 83 -2
fit 87 85 -2
fluid 100 100 0
frequency 95 94 -1
ganglion 97 95 -2
glucose 91 92 1
growth 70 67 -3
immunosuppression 79 79 0
implantation 90 88 -2
inhibition 98 98 0
japanese 73 75 2
lead 91 90 -1
man 87 82 -5
mole 95 84 -11
mosaic 87 83 -4
nutrition 53 43 -10
pathology 85 85 0
pressure 94 96 2
radiation 84 82 -2
reduction 89 90 1
repair 87 86 -1
resistance 98 97 -1
scale 86 79 -7
secretion 99 99 0
sensitivity 93 91 -2
sex 87 84 -3
single 99 99 0
strains 92 92 0
support 86 89 3
surgery 97 98 1
transient 99 99 0
transport 93 93 0
ultrasound 87 85 -2
variation 94 89 -5
weight 77 77 0
white 73 74 1
Average 87.2 85.6 -1.58
Table 3: Comparison of performance using orig-
inal training data and 90 automatically generated
examples
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Finding Predominant Senses in Untagged
Text. In Proceedings of the 42nd Annual Meeting of
the Association for Computational Lingusitics (ACL-
2004), pages 280?287, Barcelona, Spain.
B. McInnes, T. Pedersen, and J. Carlis. 2007. Us-
ing UMLS Concept Unique Identifiers (CUIs) for
Word Sense Disambiguation in the Biomedical Do-
main. In Proceedings of the Annual Symposium
of the American Medical Informatics Association,
pages 533?537, Chicago, IL.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004.
The Senseval-3 English lexical sample task. In
Proceedings of Senseval-3: The Third International
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text, Barcelona, Spain.
H. Ng, B. Wang, and S. Chan. 2003. Exploiting Paral-
lel Texts for Word Sense Disambiguation: an Empir-
ical Study. In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics
(ACL-03), pages 455?462, Sapporo, Japan.
H. Ng. 1997. Getting serious about Word Sense Dis-
ambiguation. In Proceedings of the SIGLEX Work-
shop ?Tagging Text with Lexical Semantics: What,
why and how??, pages 1?7, Washington, DC.
T. Pedersen. 2001. A Decision Tree of Bigrams is an
Accurate Predictor of Word Sense. In Proceedings
of the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-01), pages 79?86, Pittsburgh, PA., June.
S. Robertson and K. Spark Jones. 1976. Relevance
weighting of search terms. Journal of the Ameri-
can Society for Information Science and Technology,
27(3):129?146.
J. Rocchio. 1971. Relevance feedback in Information
Retrieval. In G. Salton, editor, The SMART Retrieval
System ? Experiments in Automatic Document Pro-
cessing. Prentice Hall, Englewood Cliffs, NJ.
G. Salton. 1971. The SMART Retrieval System ? Ex-
periments in Automatic Document Processing. Pren-
tice Hall Inc., Englewood Cliffs, NJ.
M. Stevenson, Y. Guo, R. Gaizauskas, and D. Martinez.
2008. Knowledge Sources for Word Sense Disam-
biguation of Biomedical Text. In Proceedings of the
Workshop on Current Trends in Biomedical Natural
Language Processing at ACL 2008, pages 80?87.
M. Weeber, J. Mork, and A. Aronson. 2001. Devel-
oping a Test Collection for Biomedical Word Sense
Disambiguation. In Proceedings of AMAI Sympo-
sium, pages 746?50, Washington, DC.
D. Widdows, S. Peters, S. Cedernerg, C. Chan, D. Stef-
fen, and P. Buitelaar. 2003. Unsupervised Mono-
lingual and Bilingual Word-sense Disambiguation of
Medical Documents using UMLS. In Workshop on
?Natural Langauge Processing in Biomedicine? at
ACL 2003, pages 9?16, Sapporo, Japan.
816
Intelligent Access to Text: Integrating Information
Extraction Technology into Text Browsers
Robert Gaizauskas   Patrick Herring   Michael Oakes  
Michelline Beaulieu  Peter Willett  Helene Fowkes  Anna Jonsson 
  Department of Computer Science /  Department of Information Studies
University of Sheffield
Regent Court, Portobello Road
Sheffield S1 4DP UK

initial.surname  @sheffield.ac.uk
ABSTRACT
In this paper we show how two standard outputs from information
extraction (IE) systems ? named entity annotations and scenario
templates ? can be used to enhance access to text collections via
a standard text browser. We describe how this information is used
in a prototype system designed to support information workers? ac-
cess to a pharmaceutical news archive as part of their ?industry
watch? function. We also report results of a preliminary, qualita-
tive user evaluation of the system, which while broadly positive in-
dicates further work needs to be done on the interface to make users
aware of the increased potential of IE-enhanced text browsers.
1. INTRODUCTION
Information extraction (IE) technology, as promoted and defined
by the DARPA Message Understanding Conferences [4, 5] and
the current ACE component of TIDES [1], has resulted in impres-
sive new abilities to extract structured information from texts, and
complements more traditional information retrieval (IR) technol-
ogy which retrieves documents or passages of relevance from text
collections and leaves information seekers to browse the retrieved
sub-collection (e.g. [2]). However, while IR technology has been
readily incorporated into end-user applications (e.g. web search
engines), IE technology has not yet been as successfully deployed
in end-user systems as its proponents had hoped. There are several
reasons for this, including:
1. Porting cost. Moving IE systems to new domains requires
considerable expenditure of time and expertise, either to cre-
ate/modify domain-specific resources and rule bases, or to
annotate texts for supervised machine learning approaches.
2. Sensitivity to inaccuracies in extracted data. IE holds out
the promise of being able to construct structured databases
from text sources automatically, but extraction results are by
no means perfect. Thus, the technology is only appropriate
.
for applications where some error is tolerable and readily de-
tectable by end users.
3. Complexity of integration into end-user systems. IE sys-
tems produce results (named entity tagged texts, filled tem-
plates) which must be incorporated into larger, more sophis-
ticated application systems if end users are to gain benefit
from them.
In this paper we present the approach taken in the TRESTLE
project (Text Retrieval Extraction and Summarisation Technolo-
gies for Large Enterprises) which addresses the second and third of
these problems; and also preliminary results from the user testing
evaluation of the TRESTLE interface. The goal of the TRESTLE
project is to develop an advanced text access facility to support
information workers at GlaxoSmithKline (GSK), a large pharma-
ceutical corporation. Specifically, the project aims to provide en-
hanced access to Scrip1, the largest circulation pharmaceutical in-
dustry newsletter, in order to increase the effectiveness of employ-
ees in their ?industry watch? function, which involves both broad
current awareness and tracking of people, companies and products,
particularly the progress of new drugs through the clinical trial and
regulatory approval process.
2. IE AND INFORMATION SEEKING IN
LARGE ENTERPRISES
While TRESTLE aims to support information workers in the
pharmaceutical industry, most of the functionality it embodies is
required in any large enterprise. Our analysis of user requirements
at GlaxoSmithKline has led us to distinguish various categories
of information seeking. At the highest level we must distinguish
requirements for current awareness from those for retrospective
search. Current awareness requirements can be further split into
general updating (what?s happened in the industry news today/this
week) and entity or event-based tracking (e.g. what?s happened
concerning a specific drug or what regulatory decisions have been
made).
Retrospective search tends to break down into historical tracking
of entities or events of interest (e.g. where has a specific person
been reported before, what is the clinical trial history of a particular
drug) and search for a specific event or a remembered context in
which a specific entity played a role.

Scrip is the trademark of PJB Publications Ltd. See
http://www.pjbpub.co.uk.
Scenario
Summary Writer
Indexer
LaSIE
Information
System
Web Server
Dynamic
Page Creator
Web Browser
Off-line System
ExtractionScrip
Internet
Scenario
Templates
User
Information
Seeking
NE Tagged Texts
Scenario Summaries
Entity/
Indices
Figure 1: TRESTLE Architecture
Notice that both types of information seeking require the identi-
fication of entities and events in the news ? precisely the function-
ality that IE systems are intended to deliver.
3. THE TRESTLE SYSTEM
The overall archictecture of the TRESTLE system is shown in
Figure 1. The system comprises an on-line and an off-line com-
ponent. The off-line component runs automatically whenever a
new electronic delivery of Scrip takes place. It runs an IE System
(the LaSIE system, developed for participation in the MUC evalu-
ations [6]), which yields as output Named Entity (NE) tagged texts
and Scenario Templates. To address the domain of interest, the
MUC-7 NE categories of person, location and organisation have
been retained and the categories of drug names and diseases have
been added. The system generates three scenario templates: person
tracking (a minor modification of the MUC-6 management succes-
sion scenario), clinical trials experimental results (drug, phase of
trial, experimental parameters/outcomes) and regulatory announce-
ments (drugs approved, rejected by various agencies).
After the IE system outputs the NE tagged texts and scenario
templates, an indexing process is run to update indices which are
keyed by entity type (person, drug, disease, etc.) and date, and by
scenario type and date.
The on-line component of TRESTLE is a dynamic web page cre-
ation process which responds to the users? information seeking be-
haviour, expressed as clicks on hypertext links in a browser-based
interface, by generating web pages from the information held in
the indexed IE results and the original Scrip texts. A basic Infor-
mation Retrieval component has also been plugged in to TRESTLE
to provide users with seamless access to query Scrip texts, i.e., not
confined to the pre-defined named entities in the index.
3.1 Interface Overview
The interface allows four ways of accessing Scrip: by headline,
by named entity category, by scenario summary, and by freetext
search. For the three first access routes the date range of Scrip
articles accessible may be set to the current day, previous day, last
week, last four weeks or full archive.
The interface is a browser whose main window is divided into
three independently scrollable frames (see Figure 2). An additional
frame (the ?head frame?) is located at the top displaying the date
range options, as well as information about where the user cur-
rently is in the system. Down the full length of the left side of
the window is the ?access frame?, in which text access options are
specified. The remainder of the main window is split horizonally
into two frames, the upper of which is used to display the auto-
matically generated index information (the ?index frame?) and the
lower of which is used to present the Scrip articles themselves (the
?text frame?).
Headline access is the traditional way GSK Scrip users access
text, and is retained as the initial default presentation in TRESTLE.
In the index frame a list of Scrip headlines is presented in reverse
chronological order. Each headline is a clickable link to full text of
the article; clicking on one displays the full text in the text frame
(like Figure 2, only without the second column in the index frame).
Named entity and scenario access are the novel IE-based tech-
niques TRESTLE supports.
3.2 NEAT: Named Entity Access to Text
From the access frame a user selects a category, for example,
drugs. The index frame then displays an alphabetically ordered list
of drug names extracted from the Scrip texts by the IE engine (Fig-
ure 2). To the right of each drug name is the title of the article from
which the name was extracted (if a name occurs in multiple texts,
there are multiple lines in the index frame). Once again the title is
a hyperlink to the text and if followed the full text is displayed in
the text frame.
When a text is displayed in the text frame, every occurrence of
every name which has been identified as a named entity of any
category is displayed as a clickable link; furthermore, each name
category is displayed in a different colour. Clicking on a name, say
a company name (e.g. Warner-Lambert in Figure 2) occurring in
a text which was accessed initially via the drug index, updates the
index frame with the subset of entries from the index for that name
only ? in our example, all entries for the selected company.
In addition to listing the full drug index alphabetically, the user
may also enter a specific drug name in the Index Look-up box
Figure 2: TRESTLE Interface: NEAT
in the access frame, and the index frame will then list the titles of
all articles containing that drug name.
NEAT allows rapid text navigation by named entity. A user with
a watching brief on, say diabetes, can start by reviewing recent ar-
ticles mentioning diabetes, but then follow up all recent references
to companies or drugs mentioned in these articles, extending the
search back in time as necessary, and at any point branching off to
pursue related entities.
3.3 SCAT: Scenario Access to Text
While NEAT allows navigation by named entity, the user still
derives information by reading the original Scrip texts. Scenario
access to text (SCAT) utilises summaries generated from templates
extracted by the scenario template filling component of the IE sys-
tem to provide access to the source texts. It is based on the obser-
vation that many scenarios of interest can be expressed via single
sentence summaries. For example, regulatory announcements in
the pharmaceutical industry can be captured in a template and sum-
marised via one or more simple sentence schemas such as ?Agency
approves/rejects/considers Company?s Drug for Disease in Juris-
diction?.
To use SCAT a user selects one of the tracking options (keep-
ing track) from the access frame of the interface. A list of one
line summaries, one per extracted scenario, is then presented in
the index frame. Along with each summary is a link to the source
text, which allows the user to confirm the correctness of the sum-
mary, or to follow up for more detail/context. Clicking on this link
causes the source text to appear in the text frame (see Figure 3).
The presence of a summary in a Scrip article is also presented to
the user through coloured tracking flags next to the article head-
line (see Figure 2). This feature can be viewed as a shortcut to the
summary facility; clicking the flag gives the generated summary in
the text frame together with the link to the source. Of course suffi-
cient information may have been gleaned from the summary alone,
obviating the need to read the full text.
4. PRELIMINARY USER EVALUATION
Although input from users has informed each stage of the de-
sign process from the conceptual non-interactive mock-ups to the
development of the web-based prototype, this section reports on a
preliminary evaluation of user testing of the first fully functional
prototype. The aim was to elicit feedback on the presentation and
usability of NEAT and SCAT and the overall interface design. The
objectives were two-fold. Firstly, and more broadly, to assess to
what extent the interface conformed to principles of good usability
design such as simplicity, consistency, predictability, and flexibil-
ity [7]. Secondly, and more importantly, to focus on the interaction
issues presented by NEAT and SCAT:
  procedurally, in terms of users? ability to move between dif-
ferent search options in a logical and seamless fashion; and
  conceptually, in terms of users? awareness and understanding
of the respective functions for exploiting current and retro-
spective Scrip headlines and full text.
4.1 Evaluation Methodology
A group of eight users consisting of postgraduate students and
research staff were recruited from the Department of Information
Studies at the University of Sheffield. The subjects had different
subject backgrounds and all had experience of using web based
Figure 3: TRESTLE Interface: SCAT
interfaces, searching for information online and some knowledge
of alerting/current awareness bulletins.
The focus of the exercise was to observe user-system interactions
?real time? to gain insight into:
  ease of use and learnability of the system;
  preferred strategies for accessing text;
  problems in interpreting the interface.
In addition, user perceptions of the interface were also elicited to
provide further explanations on searcher behaviour. A combination
of instruments was thus used including a usability questionnaire,
verbal protocols and observational notes. Note that this evaluation
was a purely quantitative exercise aimed at gaining an understand-
ing of how the users responded to the novel functions offerred by
the interface. A further evaluation will take place in an operational
setting with real end users from GSK.
After a brief introduction to the purpose of the evaluation and
a brief overview of the system, users were asked to explore the
system in an undirected manner, asking questions and providing
comments as they proceeded. Following this, they were asked to
carry out a number of tasks from a set of tasks that simulated typical
information needs characteristic of real end-users at GSK and were
instructed to identify a ?relevant? article for each task. The tasks
were designed to exploit the full functionality of the prototype; an
example of the task is given below:
You?ve heard that one of your colleagues, Mr Garcia,
has recently accepted an appointment at another phar-
maceutical company. You want to find out which com-
pany he will be moving to and what post he has taken
up.
The number of tasks completed by each subject varied according
to the subject?s thoroughness in exploring the system. The order in
which the tasks were assigned was random.
4.2 Access Strategies
Access to named entities was made available in three ways:
1. by clicking directly on a list of four categories;
2. through the index look up query box;
3. through the free-text keyword search option.
The optimal strategy differed for the different assigned tasks. Most
subjects tended to use the index look-up as a first attempt irrespec-
tive of its appropriateness for the task in hand. Preference for the
use of the index look-up as opposed to selecting more general entity
categories may be explained by the fact that users knew what they
were looking for (i.e. an artefact of the assigned task). Moreover
the query box for the index look-up option may have been a more
familiar feature which encouraged searchers to adopt a searching
strategy as opposed to browsing named entities. The preference for
using the index look-up option over free text searching may have
been influenced by the order of presentation as well as the promi-
nence of the text entry box in the access frame. In addition for
assigned tasks where the choice was between any of the three en-
tity access strategies, or using the tracking options, the majority of
users opted for the entity access via the index look-up. The novelty
of the tracking options appeared to be a contributory factor.
4.3 User Perceptions
4.3.0.1 Colour Coding.
The colour coding of the named entities was highly noticeable,
although there was some disagreement on its usefulness. Of those
subjects that found the colour coding unhelpful, it was the choice of
colours that they objected to rather than the function of the colour
per se. Although subjects claimed that coloured entity links were
distracting when reading full news items, the majority indicated
that the linking to previous Scrip items was very useful. The dis-
traction often had a positive effect in leading to useful and re-
lated articles. The overall integration of the current awareness and
retrospective searching functions through named entities was thus
widely appreciated.
4.3.0.2 Index Look-up.
All subjects except one found the index look-up function use-
ful, once they discovered that it was a quick way of accessing pre-
defined named entity categories. The fact that the approach only
provided exact string matching was judged to be limiting.
4.3.0.3 Scenario Tracking.
The keeping track option was not as easily understood as
the named entity options. The label ?keeping track? was misinter-
preted by some subjects as a search history function or an alerting
service based on user profiles. After having used the tracking fa-
cility half of the subjects did, however, correctly understand the
function. One problem that arose was the differentiation between
summaries presented in SCAT and the actual Scrip headlines. Al-
though the header informed searchers that they were viewing Scrip
summaries, the display of the summaries in the same frame where
the headlines were normally presented as well as the similarity in
content led to confusion.
The coloured flags next to the headlines, which were meant to
serve as a tracking label to allow users to move seamlessly from
headlines to scenario summaries, raised another problem. Not only
was the meaning of the flag symbol poorly understood, but also
subjects did not realise that they could click on it. Moreover when
they clicked on the flag they expected to see a full news item rather
than a summary. Hence, the scenario access was both procedurally
and conceptually confusing.
5. CONCLUSIONS
To date IE has largely been a ?technology push? activity, with
language engineers working to develop core technologies. For the
technology to become usable, and for its further development to be
influenced by end user requirements (?user pull?), prototype end-
user application systems must be built which exploit the signifi-
cant achievement of the technology to date, while acknowledging
its limitations. In this paper we have described such a prototype,
the TRESTLE system, which exploits named entity and scenario
template IE technology to offer users novel ways to access textual
information.
Our preliminary evaluation has revealed that although search op-
tions initially selected from the access frame were not always op-
timal for undertaking set tasks, the colour coded textual and iconic
cues embedded in the headline index and full text frames on the
whole enabled users to exploit the different functions seamlessly.
Whilst the TRESTLE interface appeared to support interaction at a
procedural level, at the conceptual level however, searchers did not
necessarily gain sufficient understanding of the underlying func-
tionality, particularly in respect to the scenario access. For exam-
ple the inability to distinguish between the original headlines and
the system generated summaries for SCAT was problematic and
requires further investigation. Other studies have reported simi-
lar issues in introducing more complex interactive search functions
[3, 8]. More meaningful labelling may in part address some of the
difficulties encountered. A more extensive evaluation in a work set-
ting will follow to assess to what extent the integration of new and
established conventions can support users with domain knowledge
and greater familiarity with alerting systems to adopt new searching
and awareness approaches effectively.
6. ACKNOWLEDGEMENTS
The authors would like the acknowledge the financial support
of GlaxoSmithKline which has made this work possible, and in
addition the helpful comments and insights of many staff at GSK,
in particular Peter McMeekin, Charlie Hodgman, David Pearson
and Derek Black.
7. REFERENCES
[1] ACE: Automatic Content Extraction.
http://www.itl.nist.gov/iaui/894.01/tests/ace/. Site visited
08/01/01.
[2] R. Baeza-Yates and B. Ribiero-Neto. Modern Information
Retrieval. ACM Press Books, 1999.
[3] M. Beaulieu and S. Jones. Interactive searching and interface
issues in the Okapi best match probabilistic retrieval system.
Interacting with Computers, 10:237?248, 1998.
[4] Defense Advanced Research Projects Agency. Proceedings of
the Sixth Message Understanding Conference (MUC-6).
Morgan Kaufmann, 1995.
[5] Defense Advanced Research Projects Agency. Proceedings of
the Seventh Message Understanding Conference (MUC-7),
1998. Available at http://www.saic.com.
[6] K. Humphreys, R. Gaizauskas, S. Azzam, C Huyck,
B. Mitchell, H. Cunningham, and Y. Wilks. Description of the
LaSIE-II system as used for MUC-7. In MUC-7 [5]. Available
at http://www.saic.com.
[7] J. Nielson. Designing Web Usability: The Practice of
Simplicity. New Riders, 2000.
[8] A. Sutcliffe. Evaluating the effectiveness of visual user
interfaces for information retrieval. International Journal
Human-Computer Studies, 53:741?763, 1982.
METER: MEasuring TExt Reuse
Paul Clough and Robert Gaizauskas and Scott S.L. Piao and Yorick Wilks
Department of Computer Science
University of She?eld
Regent Court, 211 Portobello Street,
She?eld, England, S1 4DP
finitial.surname@dcs.shef.ac.ukg
Abstract
In this paper we present results from
the METER (MEasuring TExt Reuse)
project whose aim is to explore issues
pertaining to text reuse and derivation,
especially in the context of newspapers
using newswire sources. Although the
reuse of text by journalists has been
studied in linguistics, we are not aware
of any investigation using existing com-
putational methods for this particular
task. We investigate the classication
of newspaper articles according to their
degree of dependence upon, or deriva-
tion from, a newswire source using a
simple 3-level scheme designed by jour-
nalists. Three approaches to measur-
ing text similarity are considered: n-
gram overlap, Greedy String Tiling,
and sentence alignment. Measured
against a manually annotated corpus of
source and derived news text, we show
that a combined classier with fea-
tures automatically selected performs
best overall for the ternary classica-
tion achieving an average F
1
-measure
score of 0.664 across all three cate-
gories.
1 Introduction
A topic of considerable theoretical and practical
interest is that of text reuse: the reuse of existing
written sources in the creation of a new text. Of
course, reusing language is as old as the retelling
of stories, but current technologies for creating,
copying and disseminating electronic text, make
it easier than ever before to take some or all of
any number of existing text sources and reuse
them verbatim or with varying degrees of mod-
ication.
One form of unacceptable text reuse, plagia-
rism, has received considerable attention and
software for automatic plagiarism detection is
now available (see, e.g. (Clough, 2000) for a re-
cent review). But in this paper we present a
benign and acceptable form of text reuse that
is encountered virtually every day: the reuse of
news agency text (called copy) in the produc-
tion of daily newspapers. The question is not
just whether agency copy has been reused, but
to what extent and subject to what transforma-
tions. Using existing approaches from computa-
tional text analysis, we investigate their ability
to classify newspapers articles into categories in-
dicating their dependency on agency copy.
2 Journalistic reuse of a newswire
The process of gathering, editing and publish-
ing newspaper stories is a complex and spe-
cialised task often operating within specic pub-
lishing constraints such as: 1) short deadlines;
2) prescriptive writing practice (see, e.g. Evans
(1972)); 3) limits of physical size; 4) readability
and audience comprehension, e.g. a tabloid's
vocabulary limitations; 5) journalistic bias, e.g.
political and 6) a newspaper's house style. Of-
ten newsworkers, such as the reporter and edi-
tor, will rely upon news agency copy as the basis
of a news story or to verify facts and assess the
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 152-159.
                         Proceedings of the 40th Annual Meeting of the Association for
importance of a story in the context of all those
appearing on the newswire. Because of the na-
ture of journalistic text reuse, dierences will
arise between reused news agency copy and the
original text. For example consider the follow-
ing:
Original (news agency) A drink-driver who
ran into the Queen Mother's o?cial Daim-
ler was ned $700 and banned from driving
for two years.
Rewrite (tabloid) A DRUNK driver who
ploughed into the Queen Mother's limo was
ned $700 and banned for two years yes-
terday.
This simple example illustrates the types of
rewrite that can occur even in a very short
sentence. The rewrite makes use of slang and
exaggeration to capture its readers' attention
(e.g. DRUNK, limo, ploughed). Deletion (e.g.
from driving) has also been used and the addi-
tion of yesterday indicates when the event oc-
curred. Many of the transformations we ob-
served between moving from news agency copy
to the newspaper version have also been re-
ported by the summarisation community (see,
e.g., McKeown and Jing (1999)).
Given the value of the information news agen-
cies supply, the ease with which text can be
reused and commercial pressures, it would be
benecial to be able to identify those news sto-
ries appearing in the newspapers that have relied
upon agency copy in their production. Potential
uses include: 1) monitoring take-up of agency
copy; 2) identifying the most reused stories ; 3)
determining customer dependency upon agency
copy and 4) new methods for charging customers
based upon the amount of copy reused. Given
the large volume of news agency copy output
each day, it would be infeasible to identify and
quantify reuse manually; therefore an automatic
method is required.
3 A conceptual framework
To begin to get a handle on measuring text
reuse, we have developed a document-level clas-
sication scheme, indicating the level at which
a newspaper story as a whole is derived from
agency copy, and a lexical-level classication
scheme, indicating the level at which individ-
ual word sequences within a newspaper story
are derived from agency copy. This framework
rests upon the intuitions of trained journalists
to judge text reuse, and not on an explicit lex-
ical/syntactic denition of reuse (which would
presuppose what we are setting out to discover).
At the document level, newspaper stories
are assigned to one of three possible categories
coarsely reecting the amount of text reused
from the news agency and the dependency of
the newspaper story upon news agency copy
for the provision of \facts". The categories in-
dicate whether a trained journalist can iden-
tify text rewritten from the news agency in
a candidate derived newspaper article. They
are: 1) wholly-derived (WD): all text in
the newspaper article is rewritten only from
news agency copy; 2) partially-derived (PD):
some text is derived from the news agency, but
other sources have also been used; and 3) non-
derived (ND): news agency has not been used
as the source of the article; although words may
still co-occur between the newspaper article and
news agency copy on the same topic, the jour-
nalist is condent the news agency has not been
used.
At the lexical or word sequence level, individ-
ual words and phrases within a newspaper story
are classied as to whether they are used to ex-
press the same information as words in news
agency copy (i.e. paraphrases) and or used to
express information not found in agency copy.
Once again, three categories are used, based on
the judgement of a trained journalist: 1) verba-
tim: text appearing word-for-word to express
the same information; 2) rewrite: text para-
phrased to create a dierent surface appearance,
but express the same information and 3) new:
text used to express information not appearing
in agency copy (can include verbatim/rewritten
text, but being used in a dierent context).
3.1 The METER corpus
Based on this conceptual framework, we have
constructed a small annotated corpus of news
texts using the UK Press Association (PA) as
the news agency source and nine British daily
newspapers
1
who subscribe to the PA as candi-
date reusers. The METER corpus (Gaizauskas
et al, 2001) is a collection of 1716 texts (over
500,000 words) carefully selected from a 12
month period from the areas of law and court
reporting (769 stories) and showbusiness (175
stories). 772 of these texts are PA copy and 944
from the nine newspapers. These texts cover 265
dierent stories from July 1999 to June 2000 and
all newspaper stories have been manually classi-
ed at the document-level. They include 300
wholly-derived, 438 partially-derived and 206
non-derived (i.e. 77% are thought to have used
PA in some way). In addition, 355 have been
classied according to the lexical-level scheme.
4 Approaches to measuring text
similarity
Many problems in computational text analy-
sis involve the measurement of similarity. For
example, the retrieval of documents to full a
user information need, clustering documents ac-
cording to some criterion, multi-document sum-
marisation, aligning sentences from one lan-
guage with those in another, detecting exact and
near duplicates of documents, plagiarism detec-
tion, routing documents according to their style
and identifying authorship attribution. Meth-
ods typically vary depending upon the match-
ing method, e.g. exact or partial, the degree
to which natural language processing techniques
are used and the type of problem, e.g. search-
ing, clustering, aligning etc. We have not had
time to investigate all of these techniques, nor
is there space here to review them. We have
concentrated on just three: ngram overlap mea-
sures, Greedy String Tiling, and sentence align-
ment. The rst was investigated because it of-
fers perhaps the simplest approach to the prob-
lem. The second was investigated because it has
been successfully used in plagiarism detection, a
problem which at least supercially is quite close
1
The newspapers include ve popular papers (e.g. The
Sun, The Daily Mail, Daily Star, Daily Mirror) and four
quality papers (e.g. Daily Telegraph, The Guardian, The
Independent and The Times).
to the text reuse issues we are investigating. Fi-
nally, alignment (treating the derived text as a
\translation" of the rst) seemed an intriguing
idea, and contrasts, certainly with the ngram ap-
proach, by focusing more on local, as opposed to
global measures of similarity.
4.1 Ngram Overlap
An initial, straightforward approach to assessing
the reuse between two texts is to measure the
number of shared word ngrams. This method
underlies many of the approaches used in copy
detection including the approach taken by Lyon
et al (2001).
They measure similarity using the set-
theoretic measures of containment and resem-
blance of shared trigrams to separate texts writ-
ten independently and those with su?cient sim-
ilarity to indicate some form of copying.
We treat each document as a set of overlap-
ping n-word sequences (initially considering only
n-word types) and compute a similarity score
from this. Given two sets of ngrams, we use
the set-theoretic containment score to measure
similarity between the documents for ngrams of
length 1 to 10 words. For a source text A and
a possibly derived text B represented by sets of
ngrams S
n
(A) and S
n
(B) respectively, the pro-
portion of ngrams in B also in A, the ngram con-
tainment C
n
(A;B), is given by:
C
n
(A;B) =
j S
n
(A) \ S
n
(B) j
j S
n
(B) j
(1)
Informally containment measures the number
of matches between the elements of ngram sets
S
n
(A) and S
n
(B), scaled by the size of S
n
(B).
In other words we measure the proportion of
unique n-grams in B that are found in A. The
score ranges from 0 to 1, indicating none to all
newspaper copy shared with PA respectively.
We also compare texts by counting only those
ngrams with low frequency, in particular those
occurring once. For 1-grams, this is the same as
comparing the hapax legomena which has been
shown to discriminate plagiarised texts from
those written independently even when lexical
overlap between the texts is already high (e.g.
70%) (Finlay, 1999). Unlike Finlay's work, we
nd that repetition in PA copy
2
drastically re-
duces the number of shared hapax legomena
thereby inhibiting classication of derived and
non-derived texts. Therefore we compute the
containment of hapax legomena (hapax contain-
ment) by comparing words occurring once in the
newspaper, i.e. those 1-grams in S
1
(B) that oc-
cur once with all 1-grams in PA copy, S
1
(A).
This containment score represents the number
of newspaper hapax legomena also appearing at
least once in PA copy.
4.2 Greedy String-Tiling
Greedy String-Tiling (GST) is a substring
matching algorithm which computes the degree
of similarity between two strings, for exam-
ple software code, free text or biological subse-
quences (Wise, 1996). Compared with previous
algorithms for computing string similarity, such
as the Longest Common Subsequence or
Levenshtein distance, GST is able to deal with
transposition of tokens (in earlier approaches
transposition is seen as a number of single inser-
tions/deletions rather than a single block move).
The GST algorithm performs a 1:1 matching
of tokens between two strings so that as much of
one token stream is covered with maximal length
substrings from the other (called tiles). In our
problem, we consider how much newspaper text
can be maximally covered by words from PA
copy. A minimum match length (MML) can be
used to avoid spurious matches (e.g. of 1 or
2 tokens) and the resulting similarity between
the strings can be expressed as a quantitative
similarity match or a qualitative list of common
substrings. Figure 1 shows the result of GST for
the example in Section 2.
Figure 1: Example GST results (MML=3)
2
As stories unfold, PA release copy with new, as well
as previous versions of the story
Given PA copy A, a newspaper text B and a
set of maximal matches, tiles, of a given length
between A and B, the similarity, gstsim(A,B),
is expressed as:
gstsim(A;B) =
P
i2tiles
length
i
j B j
(2)
4.3 Sentence alignment
In the past decade, various alignment algorithms
have been suggested for aligning multilingual
parallel corpora (Wu, 2000). These algorithms
have been used to map translation equivalents
across dierent languages. In this specic case,
we investigate whether alignment can map de-
rived texts (or parts of them) to their source
texts. PA copy may be subject to various
changes during text reuse, e.g. a single sen-
tence may derive from parts of several source
sentences. Therefore, strong correlations of sen-
tence length between the derived and source
sentences cannot be guaranteed. As a result,
sentence-length based statistical alignment al-
gorithms (Brown et al, 1991; Gale and Church,
1993) are not appropriate for this case. On the
other hand, cognate-based algorithms (Simard
et al, 1992; Melamed, 1999) are more e?cient
for coping with change of text format. There-
fore, a cognate-based approach is adopted for
the METER task. Here cognates are dened as
pairs of terms that are identical, share the same
stems, or are substitutable in the given context.
The algorithm consists of two principal com-
ponents: a comparison strategy and a scoring
function. In brief, the comparison works as fol-
lows (more details may be found in Piao (2001)).
For each sentence in the candidate derived text
DT the sentences in the candidate source text
ST are compared in order to nd the best match.
A DT sentence is allowed to match up to three
possibly non-consecutive ST sentences. The
candidate pair with the highest score (see be-
low) above a threshold is accepted as a true
alignment. If no such candidate is found, the
DT sentence is assumed to be independent of
the ST. Based on individual DT sentence align-
ments, the overall possibility of derivation for
the DT is estimated with a score ranging be-
tween 0 and 1. This score reects the propor-
tion of aligned sentences in the newspaper text.
Note that not only may multiple sentences in
the ST be aligned with a single sentence in the
DT, but also multiple sentences in the DT may
be aligned with one sentence in the ST.
Given a candidate derived sentence DS and
a proposed (set of) source sentence(s) SS, the
scoring function works as follows. Three basic
measures are computed for each pair of candi-
date DS and SS: SNG is the sum of lengths
of the maximum length non-overlapping shared
n-grams with n  2; SWD is the number of
matched words sharing stems not in an n-gram
guring in SNG; and SUB is the number of
substitutable terms (mainly synonyms) not g-
uring in SNG or SWD. Let L
1
be the length of
the candidate DS and L
2
the length of candidate
SS. Then, three scores PD, PS (Dice score) and
PV S are calculated as follows:
PSD =
SWD + SNG + SUB
L
1
PS =
2(SWD + SNG + SUB)
L
1
+ L
2
PSNG =
SNG
SWD + SNG + SUB
These three scores reect dierent aspects of
relations between the candidate DS and SS:
1. PSD: The proportion of the DS which is
shared material.
2. PS: The proportion of shared terms in DS
and SS. This measure prefers SS's which not
only contain many terms in the DS, but also
do not contain many additional terms.
3. PSNG: The proportion of matching n-
grams amongst the shared terms. This
measure captures the intuition that sen-
tences sharing not only words, but word se-
quences are more likely to be related.
These three scores are weighted and combined
together to provide an alignment metric WS
(weighted score), which is calculated as follows:
WS = ?
1
PSD+ ?
2
PS + ?
3
PSNG
where ?
1
+?
2
+?
3
= 1. The three weighting vari-
ables ?
i
(i = 1; 2; 3) have been determined empir-
ically and are currently set to: ?
1
= 0:85; ?
2
=
0:05; ?
3
= 0:1.
5 Reuse Classiers
To evaluate the previous approaches for measur-
ing text reuse at the document-level, we cast the
problem into one of a supervised learning task.
5.1 Experimental Setup
We used similarity scores as attributes for a ma-
chine learning algorithm and used the Weka 3.2
software (Witten and Frank, 2000). Because of
the small number of examples, we used tenfold
cross-validation repeated 10 times (i.e. 10 runs)
and combined this with stratication to ensure
approximately the same proportion of samples
from each class were used in each fold of the
cross-validation. All 769 newspaper texts from
the courts domain were used for evaluation and
randomly permuted to generate 10 sets. For
each newspaper text, we compared PA source
texts from the same story to create results in
the form: newspaper; class; score. These results
were ordered according to each set to create the
same 10 datasets for each approach thereby en-
abling comparison.
Using this data we rst trained ve single-
feature Naive Bayes classiers to do the ternary
classication task. The feature in each case was
a variant of one of the three similarity measures
described in Section 4, computed between the
two texts in the training set. The target classi-
cation value was the reuse classication category
from the corpus. A Naive Bayes classier was
used because of its success in previous classi-
cation tasks, however we are aware of its naive
assumptions that attributes are assumed inde-
pendent and data to be normally distributed.
We evaluated results using the F
1
-measure
(harmonic mean of precision and recall given
equal weighting). For each run, we calculated
the average F
1
score across the classes. The
overall average F
1
-measure scores were com-
puted from the 10 runs for each class (a single
accuracy measure would su?ce but the Weka
package outputs F
1
-measures). For the 10 runs,
the standard deviation of F
1
scores was com-
puted for each class and F
1
scores between all
approaches were tested for statistical signi-
cance using 1-way analysis of variance at a 99%
condence-level. Statistical dierences between
results were identied using Bonferroni analy-
sis
3
.
After examining the results of these single fea-
ture classiers, we also trained a \combined"
classier using a correlation-based lter ap-
proach (Hall and Smith, 1999) to select the com-
bination of features giving the highest classica-
tion score ( correlation-based ltering evaluates
all possible combinations of features). Feature
selection was carried for each fold during cross-
validation and features used in all 10 folds were
chosen as candidates. Those which occurred in
at least 5 of the 10 runs formed the nal selec-
tion.
We also tried splitting the training data into
various binary partitions (e.g. WD/PD vs. ND)
and training binary classiers, using feature se-
lection, to see how well binary classication
could be performed. Eskin and Bogosian (1998)
have observed that using cascaded binary clas-
siers, each of which splits the data well, may
work better on n-ary classication problems
than a single n-way classier. We then com-
puted how well such a cascaded classier should
perform using the best binary classier results.
5.2 Results
Table 1 shows the results of the single ternary
classiers. The baseline F
1
measure is based
upon the prior probability of a document falling
into one of the classes. The gures in parenthe-
sis are the standard deviations for the F
1
scores
across the ten evaluation runs. The nal row
shows the results for combining features selected
using the correlation-based lter.
Table 2 shows the result of training binary
classiers using feature selection to select the
most discriminating features for various binary
splits of the training data.
For both ternary and binary classiers feature
selection produced better results than using all
3
Using SPSS v10.0 for Windows.
Approach Category Avg F-measure
Baseline WD 0.340 (0.000)
PD 0.444 (0.000)
ND 0.216 (0.000)
total 0.333 (0.000)
3-gram WD 0.631 (0.004)
containment PD 0.624 (0.004)
ND 0.549 (0.005)
total 0.601 (0.003)
GST Sim WD 0.669 (0.004)
MML = 3 PD 0.633 (0.003)
ND 0.556 (0.004)
total 0.620 (0.002)
GST Sim WD 0.681 (0.003)
MML = 1 PD 0.634 (0.003)
ND 0.559 (0.008)
total 0.625 (0.004)
1-gram WD 0.718 (0.003)
containment PD 0.643 (0.003)
ND 0.551 (0.006)
total 0.638 (0.003)
Alignment WD 0.774 (0.003)
PD 0.624 (0.005)
ND 0.537 (0.007)
total 0.645 (0.004)
hapax WD 0.736 (0.003)
containment PD 0.654 (0.003)
ND 0.549 (0.010)
total 0.646 (0.004)
hapax cont. WD 0.756 (0.002)
1-gram cont. PD 0.599 (0.006)
alignment ND 0.629 (0.008)
(\combined") total 0.664 (0.004)
Table 1: A summary of classication results
possible features, with the one exception of the
binary classication between PD and ND.
5.3 Discussion
From Table 1, we nd that all classier results
are signicantly higher than the baseline (at
p < 0:01) and all dierences are signicant ex-
cept between hapax containment and alignment.
The highest F-measure for the 3-class problem
is 0.664 for the \combined" classier, which is
signicantly greater than 0.651 obtained with-
out. We notice that highest WD classication
is with alignment at 0.774, highest PD classi-
cation is 0.654 with hapax containment and
highest ND classication is 0.629 with combined
features. Using hapax containment gives higher
results than 1-gram containment alone and in
fact provides results as good as or better than
the more complex sentence alignment and GST
approaches.
Previous research by (Lyon et al, 2001) and
(Wise, 1996) had shown derived texts could be
distinguished using trigram overlap and tiling
with a match length of 3 or more, respectively.
Attributes Category Avg F
1
Correlation- alignment WD 0.942 (0.008)
based ND 0.909 (0.011)
lter total 0.926 (0.010)
alignment PD/ND 0.870 (0.003)
WD 0.770 (0.003)
total 0.820 (0.002)
alignment WD 0.778 (0.003)
PD 0.812 (0.002)
total 0.789 (0.002)
hapax cont. WD/PD 0.882 (0.002)
alignment ND 0.649 (0.007)
1-gram cont. total 0.763 (0.002)
1-gram PD 0.802 (0.002)
GST mml 3 ND 0.638 (0.007)
GST mml 1 total 0.720 (0.004)
alignment
GST mml 1 WD/ND 0.672 (0.002)
alignment PD 0.662 (0.003)
total 0.668 (0.003)
Table 2: Binary Classiers with feature selection
However, our results run counter to this be-
cause the highest classication scores are ob-
tained with 1-grams and an MML of 1, i.e. as
n or MML length increases, the F
1
scores de-
crease. We believe this results from two factors
which are characteristic of reuse in journalism.
First, since even ND texts are thematically sim-
ilar (same events being described) there is high
likelihood of coincidental overlap of ngrams of
length 3 or more (e.g. quoted speech). Secondly,
when journalists rewrite it is rare for them not
to vary the source.
For the intended application { helping the PA
to monitor text reuse { the cost of dierent mis-
classications is not equal. If the classier makes
a mistake, it is better that WD and ND texts are
mis-classied as PD, and PD as WD. Given the
dierence in distribution of documents across
classes where PD contains the most documents,
the classier will be biased towards this class
anyway as required. Table 3 shows the confu-
sion matrix for the combined ternary classier.
WD PD ND
WD 203 55 4
PD 79 192 70
ND 3 53 109
Table 3: Confusion matrix for combined ternary
classier
Although the overall F
1
-measure score is low
(0.664), mis-classication of both WD as ND
and ND as WD is also very low, as most mis-
classications are as PD. Note the high mis-
classication of PD as both WD and ND, re-
ecting the di?culty of separating this class.
From Table 2, we nd alignment is a selected
feature for each binary partition of the data.
The highest binary classication is achieved be-
tween the WD and ND classes using alignment
only, and the highest three scores show WD is
the easiest class to separate from the others.
The PD class is the hardest to isolate, reect-
ing the mis-classications seen in Table 3.
To predict how well a cascaded binary classi-
er will perform we can reason as follows. From
the preceding discussion we see that WD can
be separated most accurately; hence we choose
WD versus PD/ND as the rst binary classier.
This forces the second classier to be PD versus
ND. From the results in Table 2 and the follow-
ing equation to compute the F
1
measure for a
two-stage binary classier
WD + (PD=ND)(
PD+ND
2
)
2
we obtain an overall F
1
measure for ternary clas-
sication of 0.703, which is signicantly higher
than the best single stage ternary classier.
6 Conclusions
In this paper we have investigated text reuse in
the context of the reuse of news agency copy, an
area of theoretical and practical interest. We
present a conceptual framework in which we
measure reuse and based on which the METER
corpus has been constructed. We have presented
the results of using similarity scores, computed
using n-gram containment, Greedy String Tiling
and an alignment algorithm, as attributes for
a supervised learning algorithm faced with the
task of learning how to classify newspaper sto-
ries as to whether they are wholly, partially or
non-derived from a news agency source. We
show that the best single feature ternary clas-
sier uses either alignment or simple hapax con-
tainment measures and that a cascaded binary
classier using a combination of features can
outperform this.
The results are lower than one might like,
and reect the problems of measuring journalis-
tic reuse, stemming from complex editing trans-
formations and the high amount of verbatim
text overlapping as a result of thematic simi-
larity and \expected" similarity due to, e.g., di-
rect/indirect quotes. Given the relative close-
ness of results obtained by all approaches we
have considered, we speculate that any compar-
ison method based upon lexical similarity will
probably not improve classication results by
much. Perhaps improved performance at this
task may possible by using more advanced nat-
ural language processing techniques, e.g. better
modeling of the lexical variation and syntactic
transformation that goes on in journalistic reuse.
Nevertheless the results we have obtained are
strong enough in some cases (e.g. wholly derived
texts can be identied with > 80% accuracy) to
begin to be exploited.
In summary measuring text reuse is an excit-
ing new area that will have a number of appli-
cations, in particular, but not limited to, mon-
itoring and controlling the copy produced by a
newswire.
7 Future work
We are adapting the GST algorithm to deal with
simple rewrites (e.g. synonym substitution) and
to observe the eects of rewriting upon nding
longest common substrings. We are also experi-
menting using the more detailed METER corpus
lexical-level annotations to investigate how well
the GST and ngrams approaches can identify
reuse at this level.
A prototype browser-based demo of both the
GST algorithm and alignment program, allow-
ing users to test arbitrary text pairs for simi-
larity, is now available
4
and will continue to be
enhanced.
Acknowledgements
The authors would like to acknowledge the
UK Engineering and Physical Sciences Re-
search Council for funding the METER project
(GR/M34041). Thanks also to Mark Hepple for
helpful comments on earlier drafts.
4
See http://www.dcs.shef.ac.uk/nlp/meter.
References
P.F. Brown, J.C. Lai, and R.L. Mercer. 1991. Aligning
sentences in parallel corpora. In Proceedings of the
29th Annual Meeting of the Assoc. for Computational
Linguistics, pages 169{176, Berkeley, CA, USA.
P Clough. 2000. Plagiarism in natural and programming
languages: An overview of current tools and technolo-
gies. Technical Report CS-00-05, Dept. of Computer
Science, University of She?eld, UK.
E. Eskin and M. Bogosian. 1998. Classifying text docu-
ments using modular categories and linguistically mo-
tivated indicators. In AAAI-98 Workshop on Learning
for Text Classication.
H. Evans. 1972. Essential English for Journalists, Edi-
tors and Writers. Pimlico, London.
S. Finlay. 1999. Copycatch. Master's thesis, Dept. of
English. University of Birmingham.
R. Gaizauskas, J. Foster, Y. Wilks, J. Arundel,
P. Clough, and S. Piao. 2001. The meter corpus:
A corpus for analysing journalistic text reuse. In Pro-
ceedings of the Corpus Linguistics 2001 Conference,
pages 214|223.
W.A. Gale and K.W. Church. 1993. A program for align-
ing sentences in bilingual corpus. Computational Lin-
guistics, 19:75{102.
M.A. Hall and L.A. Smith. 1999. Feature selection for
machine learning: Comparing a correlation-based l-
ter approach to the wrapper. In Proceedings of the
Florida Articial Intelligence Symposium (FLAIRS-
99), pages 235{239.
C. Lyon, J. Malcolm, and B. Dickerson. 2001. Detecting
short passages of similar text in large document collec-
tions. In Conference on Empirical Methods in Natural
Language Processing (EMNLP2001), pages 118{125.
K. McKeown and H. Jing. 1999. The decomposition of
human-written summary sentences. In SIGIR 1999,
pages 129{136.
I. Dan Melamed. 1999. Bitext maps and alignment via
pattern recognition. Computational Linguistics, pages
107{130.
Scott S.L. Piao. 2001. Detecting and measuring text
reuse via aligning texts. Research Memorandum
CS-01-15, Dept. of Computer Science, University of
She?eld.
M. Simard, G. Foster, and P. Isabelle. 1992. Using
cognates to align sentences in bilingual corpora. In
Proceedings of the 4th Int. Conf. on Theoretical and
Methodological Issues in Machine Translation, pages
67{81, Montreal, Canada.
M. Wise. 1996. Yap3: Improved detection of similarities
in computer programs and other texts. In Proceedings
of SIGCSE'96, pages 130{134, Philadelphia, USA.
I.H. Witten and E. Frank. 2000. Datamining - practi-
cal machine learning tools and techniques with Java
implementations. Morgan Kaufmann.
D. Wu. 2000. Alignment. In R. Dale and H. Moisl and
H. Somers (eds.), A Handbook of Natural Language
Processing, pages 415{458. New York: Marcel Dekker.
Abstract
AKT is a major research project
applying a variety of technologies to
knowledge management. Knowledge
is a dynamic, ubiquitous resource,
which is to be found equally in an
expert's head, under terabytes of data,
or explicitly stated in manuals. AKT
will extend knowledge management
technologies to exploit the potential
of the semantic web, covering the use
of knowledge over its entire lifecycle,
from acquisition to maintenance and
deletion. In this paper we discuss how
HLT will be used in AKT and how
the use of HLT will affect different
areas of KM, such as knowledge
acquisition, retrieval and publishing.
1 Introduction
As globalisation reduces the competitive
advantage existing between companies, the role
of proprietary information and its appropriate
management becomes all-important. A
company?s value depends more and more on
?intangible assets?1 which exist in the minds of
employees, in databases, in files and in a
multitude of documents. It is the goal of
knowledge management (KM) technologies to
make computer systems which provide access to
this intangible knowledge present in a company
or organisation.  The system must make it
possible to share, store and retrieve the
collective expertise of all the people in an
organization. At present, many companies spend
                                                     
1 A term coined by Karl-Erik Sveiby
considerable resources on knowledge
management; estimates range between 7 and
10% of revenues (Davenport 1998).
In developing a knowledge management
system, the knowledge must first be captured or
acquired in some form which is usable by a
computer. The knowledge acquisition
bottleneck, so well-known in AI, is just as
important in knowledge management. The
acquisition of knowledge does not become less
difficult in a business environment and often
requires a sea-change in company culture in
order to persuade users to accommodate to the
technology adopted, precisely because
knowledge acquisition is so difficult.
Once knowledge has been acquired, it must be
managed, i.e. modelled, updated and published.
Modelling means representing information in a
way that is both manageable and easy to
integrate with the rest of the company?s
knowledge. Updating is necessary because
knowledge is dynamic. Part of its importance
for a company or individual lies in the fact that
knowledge is ever changing and keeping up
with the change is a crucial dimension in
knowledge management. Publishing is the
process that allows sharing the knowledge
across the company. These needs have
crystallised in efforts to develop the so-called
Semantic Web. It is envisaged that in the future,
the content currently available on the Web (both
Internets and Intranets) as raw data will be
automatically annotated with machine-readable
semantic information.  In such a case, we will
no longer speak of information retrieval but
rather of Knowledge Retrieval because instead
of obtaining thousands of potentially relevant or
irrelevant documents, only the dozen or so
documents that are truly needed by the user will
be presented to them.
Using HLT for Acquiring, Retrieving and Publishing Knowledge in AKT:
Position Paper
K. Bontcheva, C. Brewster, F. Ciravegna, H. Cunningham,
L. Guthrie, R. Gaizauskas, Y. Wilks
Department of Computer Science, the University of Sheffield,
Regent Court, 211 Portobello Street, S1 4DP Sheffield, UK
Email: N.Surname@dcs.shef.ac.uk
 In this paper we present the way Human
Language Technology (HLT) is used to address
several facets of the KM problem:  acquiring,
retrieving, and publishing knowledge. The work
presented in this paper is supported by the AKT
project (Advanced Knowledge Technologies), a
multimillion pound six year research project
funded by the EPSRC in the UK. AKT, started
in 2000, involves the University of
Southampton, the Open University, the
University of Edinburgh, the University of
Aberdeen, and the University of Sheffield
together with a large number of major UK
companies. Its objectives are to develop
technologies to cope with the six main
challenges of knowledge management:
? acquisition ? reuse
? modelling ? publication
? retrieval/extraction ? maintenance
These challenges will be addressed by the
University of Sheffield in the context of AKT
by the application of a variety of human
language technologies. Here, we consider only
the contribution of HLT to the acquisition of
knowledge, its retrieval and extraction, its
publication, and finally the role of appropriate
HLT infrastructure to the completion of these
goals.
2 Knowledge Acquisition
Knowledge acquisition (KA) is concerned with
the process of turning data into coherent
knowledge for a computer program.  The need
for effective KA methods increases as the
quantity of data available electronically
increases year by year, and the importance it
plays in our society is more and more
recognised. The challenge, we believe, lies in
designing effective techniques for acquiring the
vast amounts of (largely) tacit knowledge. KA is
a complex process, which traditionally is
extremely time consuming.
Existing KA methodologies are varied but
almost always require a great deal of manual
input. One methodology, often used in Expert
Systems, involves the time-consuming process
of structured interviews (?protocols?), which are
then analysed by knowledge engineers in order
to codify and model the knowledge of an expert
in a particular domain. Even if a complex expert
system is not required, all forms of KA are very
labour intensive. Yahoo currently employs over
100 people to keep its category hierarchy up to
date (Dom 1999). Some methodologies have
started to appear to automate this process,
although still limited to some steps in the KA
process. They depend on replacing the
introspection of knowledge engineers or the
extended elicitations of the protocol methods
(Ericsson and Simon 1984) by using Human
Language Technologies, more specifically
Information Extraction, Natural Language
Processing and Information Retrieval.
 Although knowledge acquisition produces
data (knowledge) for use by a computer
program, the form and content of that
knowledge is often debated in the research
community.  Ontologies have emerged as one of
the most popular means of modelling the
knowledge of a domain.  The meaning of this
word varies somewhat in the literature, but
minimally it is a hierarchical taxonomy of
categories, concepts or words. Ontologies can
act as an index to the memory of an organisation
and facilitate semantic searches and the retrieval
of knowledge from the corporate memory as it
is embodied in documents and other archives.
Repeated research has shown their usefulness,
especially for specific domains (J?rvelin and
Kek?l?inen 2000). The process of ontology
construction is illustrated in the rest of this
section.
2.1 Taxonomy construction
We propose to introduce automation in the stage
of taxonomy construction mainly in order to
eliminate or reduce the need for extensive
elicitation of data.  In the literature approaches
to construction of taxonomies of concepts have
been proposed (Brown et al 1992, McMahon
and Smith 1996, Sanderson and Croft 1999).
Such approaches either use a large collection of
documents as their sole data source, or they can
attempt to use existing concepts to extend the
taxonomy (Agirre et al2000, Scott 1998).  We
intend to develop a semi-automatic method that,
starting from a seed ontology sketched by the
user, produces the final ontology via a cycle of
refinements by eliciting knowledge from a
collection of texts. In this approach the role of
the user should only be that of proposing an
initial ontology and validating/changing the
different versions proposed by the system.
We intend to integrate a methodology for
automatic hierarchy definition (such as
(Sanderson and Croft 1999)) with a method for
the identification of terms related to a concept
in a hierarchy (such as (Scott 1998)). The
advantage of this integration is that, as
knowledge is continually changing, we can
reconstruct an appropriate domain specific
ontology very rapidly. This does not preclude
incorporating an existing ontology and using the
tools to extend and update it on the basis of
appropriate texts. Finally an ontology defined in
this way has the particular advantage that it
overcomes the well-known ?Tennis problem?
associated with many predefined ontologies
such as WordNet, i.e where terms closely
related in a given domain are structurally very
distant such as ball and court, for example.
In addition we intend to employ classic
Information Extraction techniques (described
below) such as named entity recognition
(Humphreys et al 1998) in order to pre-process
the text, as the identification of complex terms
such as proper names, dates, numbers, etc,
allows to reduce data sparseness in learning
(Ciravegna 2000).
We plan to introduce many cycles of ontology
learning and validation. At each stage the
defined ontology can be: i) validated/corrected
by a user/expert; ii) used to retrieve a larger set
of appropriate documents to be used for further
refinement (J?rvelin and Kek?l?inen 2000); iii)
passed on to the next development stage.
2.2 Learning Other Relations
This stage proceeds to build on the skeletal
ontology in order to specify, as much as
possible without human intervention, relations
among concepts in the ontology, other than
ISAs. In order to flesh the concept relations, we
need to identify relations such as synonymy,
meronymy, antonymy and other relations. We
plan to integrate a variety of methods existing in
the literature, e.g. by using recurrences in verb
subcategorisation as a symptom of general
relations (Basili et al 1998), by using Morin?s
user-guided approach to identify the correct
lexico/syntactic environment (Morin 1999), and
by using methods such as (Hays 1997) to locate
specific cases of synonymy.
3 Knowledge Extraction
Assuming that the shape of knowledge has been
acquired and adequately modelled, it will have
to be stored in a repository from which it is
retrieved as and when needed. On the one hand
there is the problem of retrieving instances in
order to populate the resulting knowledge base.
On the other hand, considering that repositories
could become very substantial in size, there is
the necessity to navigate the repository in order
to extract the knowledge when needed. In this
section we focus on the problem of knowledge
base population, as it is in our opinion the most
challenging from the HLT point of view.
3.1 Knowledge Base  Population
Instance identification for Knowledge Base
population can be performed by HLT-based
document analysis. With the term documents,
we mean a wide variety of types of texts such as
plain texts, web pages, knowledge elicitation
interview transcriptions (protocols), etc.  For the
sake of this paper we limit our analysis to
language related tasks only, ignoring the
problem of multi-media information. As a first
step instance identification requires the
identification of relevant documents containing
citation of the interesting information
(document classification). Then it requires the
ability to identify and extract information from
documents (Information Extraction from text).
3.2 Document Classification
Text classification for IE purposes has been
explored both in the MUC conferences as well
as in some commercially oriented projects
(Ciravegna et al 2000). In concrete terms
classification is used in order to identify the
scenario to apply to a specific set of texts, while
IE will identify (i.e. index) the instances in the
texts.  In most cases of application document
classification is quite straightforward, being
limited to the Boolean classification of a
document between relevant/irrelevant (single
scenario application as in the MUC
conferences). In cases in which knowledge may
be distributed along a number of different
detailed scenarios, full document classification
is then needed. In such cases, two main
characteristics are relevant for the classification
approach: flexibility and refinability (Ciravegna
et al 1999). Flexibility is needed with respect
to both the number of the categories and the
granularity of the classification to be coped
with. Three main types of classification can be
identified: coarse-grained, fine-grained, and
content-based. Coarse-grained classification is
performed among a relatively small number of
classes (e.g., some dozens) that are sharply
different (e.g., sport vs finance). This can be
obtained reliably and efficiently by the
application of statistical classifiers. Fine-
grained classification is performed over a
usually larger number of classes that can be
very similar (e.g., discriminating between news
about private bond issues and news about public
bond issues). This type of classification
generally requires some more knowledge-
oriented approaches such as pattern-based
classification. Sometimes categories are so
similar that classification needs to be content-
based, i.e. it can be performed only by
extracting the news content (e.g., finding news
articles issued by English financial institutions
referring to amounts in excess of 100,000 Euro).
In this case some forms of shallow adaptive
Information Extraction can be used (see next
section). Refinability concerns the possibility
of performing classification in a sequence of
steps, each one providing a more precise
classification (from coarse-grained to content-
based). In the current technological situation
coarse-grained classification can be performed
quickly, while the systems available for more
fine-grained classification are much slower and
less general purpose. When the amount of
textual material is large an incremental
approach, based on some level of coarse-grained
classification further refined by successive
analysis, proves to be very effective. A refinable
classification is generally performed over a
hierarchy of classes. A refinement may revise
the categories assigned to specific texts with
more specialised classes from the hierarchy.
More complex techniques are invoked only
when needed and, in any case, within an already
detected context (Ciravegna et al 1999).
We plan to produce a number of solutions for
text classification, adaptable to different
scenarios and situations, following the criteria
mentioned above.
3.3 Information Extraction
Information extraction from text (IE) is the
process of mapping of texts into fixed format
output (templates) representing the key
information (Gaizauskas 1997). In using IE for
KM, templates represent an intermediate format
for mapping the information in the texts into
ontology instances. Templates can be semi-
automatically derived from the ontology. We
plan to use IE for a number of passes: on the
one hand, we plan to populate a knowledge base
with instances as mentioned above. On the other
hand, IE can be used to monitor relevant
changes in the information, providing a
fundamental contribution to the problem of
knowledge updating. We have a long experience
in IE from texts, Sheffield having actively
participated in the MUC conferences and in the
TIPSTER project, activities that historically
have made a fundamental contribution to
making IE as we now know it.  The new
challenge we are currently addressing is
adaptivity. Adaptivity is a major goal for
Information Extraction, especially in the case of
its application to knowledge management, as
KM is a process that has to be distributed
throughout companies. The real value of IE will
become apparent when it can be adapted to new
applications and scenarios directly by the final
user without the intervention of IE experts. The
goal for research in adaptive IE is to create
systems adaptable to new applications/domains
by using only an analyst?s knowledge, i.e.
knowledge about the domain/scenario.
There are two directions of research in
adaptive IE, both involving the use of Machine
Learning. On the one hand machine learning is
used to automate as much as possible the tasks
an IE expert would perform in application
development (Cardie 1997) (Yangarber et al
2000). The goal here is to reduce the porting
time to a new application (and hence the cost).
This area of research comes mainly from the
MUC community. Currently, the technology
makes use mainly of NLP-intensive
technologies and the type of texts addressed are
mainly journal articles.
On the other hand, there is an attempt to make
IE systems adaptable to new
domains/applications by using only an analyst?s
knowledge, i.e. knowledge about the
domain/scenario only (Kushmerick et al 1997),
(Califf 1998), (Muslea et al 1998), (Freitag and
McCallum 1999), (Soderland 1999), (Freitag
and Kushmerick 2000), (Ciravegna 2001a).
Most research has so far focused on Web-
related texts (e.g. web pages, email, etc.)
Successful commercial products have been
created and there is an increasing interest on IE
in the Web-related market.  Current adaptive
technologies make no use of natural language
processing in the web context, as extra linguistic
structures (e.g. HTML tags, document
formatting, and ungrammatical stereotypical
language) are the elements used to identify
information. Linguistically intensive approaches
are difficult or unnecessary in such cases. When
these non-linguistic approaches are used on
texts with a reduced (or no) structure, they tend
to be ineffective.
There is a technological gap between adaptive
IE on free texts and adaptive IE on web-related
texts. For the purposes of KM, such a gap has to
be bridged so to create a set of technologies able
to cover the whole range of potential
applications for different kinds of texts, as the
type of texts to be analysed for KM may vary
dramatically from case to case. We plan to
bridge this gap via the use of lazy natural
language processing. We intend to use an
approach where the system starts with a range
of potential methodologies (from shallow to
linguistically intensive) and learns from a
training corpus which is the most effective
approach for the particular case under
consideration. A number of factors can
influence the choice: from the type of texts to be
analysed to the type of information the user is
able to provide in adapting the system. In the
first case the system will have to identify what
type of task is under consideration and select the
correct level of analysis  (e.g. language based
for free texts). Formally in this case the level of
language analysis is one of the parameters the
learner will have to learn. Concerning the type
of tagging the user is able to provide: different
users are able to provide different levels of
information in training the system: IE-trained
users are able to provide sophisticated tagging,
maybe inclusive of syntactic, semantic or
pragmatic information. Na?ve users on the other
hand are only able to provide some basic
information (e.g. to spot the relevant
information in the texts and highlight it in
different colours). We plan to develop a system
able to cope with a wide of variety of situations
by starting from the (LP)2 algorithm and
enhancing its learning capabilities on free texts
(Ciravegna 2001) and developing a powerful
human-computer interface for system adaptation
(Ciravegna and Petrelli 2001).
4 Knowledge Publishing
Knowledge is only effective if it is delivered in
the right form, at the right place, to the right
person at the right time. Knowledge publishing
is the process that allows getting knowledge to
the people who need it in a form that they can
use. As a matter of fact, different users need to
see knowledge presented and visualised in quite
different ways. The dynamic construction of
appropriate perspectives is a challenge which, in
AKT, we will address from the perspective of
generating automatically such presentations
from the ontologies acquired by the KA and KE
methods, discussed in the previous sections.
Natural Language Generation (NLG) systems
automatically produce language output (ranging
from a single sentence to an entire document)
from computer-accessible data, usually encoded
in a knowledge or data base (Reiter 2000). NLG
techniques have already been used successfully
in a number of application domains, the most
relevant of which is automatic production of
technical documentation (Reiter et al 1995),
(Paris et al 1996). In the context of KM and
knowledge publishing in particular, NLG is
needed for knowledge diffusion and
documenting ontologies. The first task is
concerned with personalised presentation of
knowledge, in the form needed by each specific
user and tailored to the correct language type
and the correct level of details. The latter is a
very important issue, because as discussed
earlier, knowledge is dynamic and needs to be
updated frequently. Consequently, the
accompanying documentation which is vital for
the understanding and successful use of the
acquired knowledge, needs to be updated in
sync. The use of NLG simplifies the ontology
maintenance and update tasks, so that the
knowledge engineer can concentrate on   the
knowledge itself, because the documentation is
automatically updated as the ontology changes.
The NLG-based knowledge publishing tools
will also utilise the ontology instances extracted
from documents using the IE approaches
discussed in Section 3.3. The dynamically
generated documentation will not only include
these instances, as soon as they get extracted,
but it will also provide examples of their
occurrence in the documents, thus facilitating
users? understanding and use of the ontology.
Our approach to knowledge publishing is based
on an existing framework for generation of user-
adapted hypertext explanations (Bontcheva
2001), (Bontcheva and Wilks 2001). The
framework incorporates a powerful agent
modelling module, which is used to tailor the
explanations to the user?s knowledge, task, and
preferences. We are now also extending the
personalisation techniques to account for user
interests. The main challenge for NLG will be
to develop robust and efficient techniques for
knowledge publishing which can operate on
large-scale knowledge resources and support the
personalised presentation of diverse
information, such as speech, video, text,
graphics (see (Maybury 2001)).
The other challenge in using NLG for
knowledge publishing is to develop tools and
techniques that will enable knowledge
engineers, instead of linguists, to create and
customise the linguistic resources (e.g., domain
lexicon) at the same time as they create and edit
the ontology.  In order to allow such inter-
operability with the KA tools, we will integrate
the NLG tools in the GATE infrastructure,
discussed next.
5 HLT Infrastructure
The range and complexity of the task of
knowledge management make imperative the
need for standardisation. While there has been
much talk about the re-use of knowledge
components such ontologies, much less has
been undertaken to standardise the
infrastructure for tools and their development.
The types of data structures typically involved
are large and complex, and without good tools
to manage and allow succinct viewing of the
data we will continue to work below our
potential. The University of Sheffield has
pioneered in the Gate and Gate 2 projects the
development of an architecture for text
engineering (Cunningham et al 1997),
(Cunningham et al 2000). Given the modular
architecture and component structure of Gate, it
is natural to build on this basis to extend the
capabilities of Gate so as to provide the most
suitable possible environment for tool
development, implementation and evaluation in
AKT. The system will provide a single
interaction and deployment point for the roll-out
of HLT in Knowledge Management. We expect
Gate2 to act as the skeleton for a large range of
knowledge management activities within AKT
and plan to extend its capabilities within the life
of the AKT project by integrating with suitable
ontological and lexical databases in order to
permit the use of  the Gate system with large
bodies of heterogeneous data
6 Conclusion and Future Work
We have presented how we plan to use HLT for
helping KM in AKT. We believe that HLT can
make a substantial contribution to the following
issues in  KM:
? Cost reduction: KM is an expensive task,
especially in the acquisition phase. HLT can
aid in automating both the acquisition of the
structure of the ontology to be learnt and in
populating such ontology with instances. It
will also provide support for automatic
knowledge documentation.
? Time reduction: KM is a slow task: HLT
can help in making it more efficient by
reducing the need for the human effort;
? Subjectivity reduction: this is a main
problem in knowledge identification and
selection. Subjective knowledge is difficult
to integrate with the rest of the company?s
knowledge and its use is somehow difficult.
KM constitutes a challenge for HLT as it
provides a number of fields of application and
in particular it challenges the integration of a set
of techniques for a common goal.
Acknowledgement
This work is supported under the Advanced
Knowledge Technologies (AKT)
Interdisciplinary Research Collaboration (IRC),
which is sponsored by the UK Engineering and
Physical Sciences Research Council under grant
number GR/N15764/01. The AKT IRC
comprises the Universities of Aberdeen,
Edinburgh, Sheffield, Southampton and the
Open University.
References
Agirre, E. O. Ansa, E. Hovy, and  D.
Mart?nez 2000. Enriching very large ontologies
using the WWW, Proceedings of the ECAI 2000
workshop ?Ontology Learning?.
Basili, R., R. Catizone, M. Stevenson, P.
Velardi, M. Vindigni, and Y. Wilks. 1998. ?An
Empirical Approach to Lexical Tuning?.
Proceedings of the Adapting Lexical and
Corpus Resources to Sublanguages and
Applications Workshop, held jointly with 1st
LREC Granada, Spain.
Bontcheva, K. 2001. Generating adaptive
hypertext explainations with a nested agent
model.  Ph. D. Thesis, University of Sheffield.
Bontcheva, K. and Wilks, Y. 2001. Dealing
with Dependencies between Content Planning
and Surface Realisation in a Pipeline
Generation Architecture. Proceedings of the 17th
International Joint Conference on Artificial
Intelligence (IJCAI2001), Seattle.
Brown, P.F., Peter F., V. J. Della Pietra, P.
V. DeSouza, J. C. Lai, and R. L. Mercer. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18, 467-479.
 Califf, M. E. 1998. Relational Learning
Techniques for Natural Language Information
Extraction. Ph.D. thesis, Univ. Texas, Austin,
www/cs/utexas.edu/users/mecaliff
C. Cardie, `Empirical methods in
information extraction', AI Journal,18(4), 65-
79, (1997).
F. Ciravegna, A. Lavelli, N. Mana, J.
Matiasek, L. Gilardoni, S. Mazza, M. Ferraro,
W. J. Black F. Rinaldi, and D. Mowatt.
FACILE: Classifying Texts Integrating Pattern
Matching and Information Extraction. In
Proceedings of the 16th International Joint
Conference On Artificial Intelligence
(IJCAI99), Stockholm, Sweden, 1999.
F. Ciravegna, A. Lavelli,, L. Gilardoni, S.
Mazza, W. J. Black, M. Ferraro, N. Mana, J.
Matiasek, F. Rinaldi. Flexible Text
Classification for Financial
Applications: The FACILE System. In
Proceedings of Prestigious Applications sub-
conference (PAIS2000) sub-conference of the
14th European Conference On Artificial
Intelligence (ECAI2000), Berlin, Germany,
August, 2000.
 Ciravegna, F. 2001. Adaptive Information
Extraction from Text by Rule Induction and
Generalisation. Proceedings of the 17th
International Joint Conference on Artificial
Intelligence (IJCAI2001), Seattle.
Ciravegna, F. and D. Petrelli. 2001. User
Involvement in customizing Adaptive
Information Extraction from Texts: Position
Paper. Proceedings of the IJCAI01 Workshop on
Adaptive Text Extraction and Mining, Seattle.
Cunningham, H., K. Humphreys, R.
Gaizauskas and Y. Wilks. 1997. Software
Infrastructure for Natural Language Processing.
Proceedings of the Fifth Conference on Applied
Natural Language Processing (ANLP-97).
Cunningham H., K. Bontcheva, V. Tablan
and Y. Wilks. 2000. Software Infrastructure for
Language Resources: a Taxonomy of Previous
Work and a Requirements Analysis.
Proceedings of the Second Conference on
Language Resources Evaluation, Athens.
Dom, B. 1999. Automatically finding the
best pages on the World Wide Web (CLEVER).
Search Engines and Beyond: Developing
efficient knowledge management systems.
Boston, MA.
Ericsson, K. A. and H. A. Simon. 1984.
Protocol Analysis: verbal reports as data. MIT
Press, Cambridge, Mass.
Freitag, D. and A. McCallum. 1999
Information Extraction with HMMs and
Shrinkage. AAAI-99 Workshop on Machine
Learning for Information Extraction, Orlando,
FL. (www.isi.edu/~muslea/RISE/ML4IE/)
Freitag, D. and N. Kushmerick. 2000.
Boosted wrapper induction. F. Ciravegna, R.
Basili, R. Gaizauskas, ECAI2000 Workshop on
Machine Learning for Information Extraction,
Berlin, 2000, (www.dcs.shef.ac.uk/~fabio/ecai-
workshop.html)
Hays, P. R. 1997. Collocational Similarity:
Emergent Patterns in Lexical Environments,
PhD. Thesis. School of English, University of
Birmingham
Humphreys, K., R. Gaizauskas, S. Azzam, C.
Huyck, B. Mitchell, H. Cunningham and  Y.
Wilks. 1998. Description of the University of
Sheffield LaSIE-II System as used for MUC-7.
Proceedings of the 7th Message Understanding
Conference.
J?rvelin, K.  and J. Kek?l?inen. 2000. IR
evaluation methods for retrieving highly
relevant documents. Proceedings of the 23rd
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval, , Athens.
Kushmerick, N., D. Weld, and R.
Doorenbos. 1997. Wrapper induction for
information extraction. Proceedings of 15th
International Conference on Artificial
Intelligence, IJCAI-97.
Manchester, P. 1999. Survey ? Knowledge
Management. Financial Times, 28.04.99.
Maybury, M.. 2001. Human Language
Technologies for Knowledge Management:
Challenges and Opportunities. Workshop on
Human Language Technology and Knowledge
Management. Toulouse, France.
McMahon, J. G. and  F. J. Smith. 1996
Improving Statistical Language Models
Performance with Automatically  Generated
Word Hierarchies. Computational Linguistics,
22(2), 217-247, ACL/MIT.
Morin, E. 1999. Using Lexico-Syntactic
patterns to Extract Semantic Relations between
Terms from Technical Corpus, TKE 99,
Innsbruck, Austria.
Muslea, I., S. Minton, and C. Knoblock.
1998. Wrapper induction for semi-structured,
web-based information sources. Proceedings of
the Conference on Autonomous Learning and
Discovery CONALD-98.
Paris, C. , K. Vander Linden. 1996.
DRAFTER: An interactive support tool for
writing multilingual instructions, IEEE
Computer, Special Issue on Interactive NLP.
Reiter, E. 1995. NLG vs. Templates.
Proceedings of the 5th European workshop on
natural language generation, (ENLG-95),
Leiden.
Reiter, E. , C. Mellish and J. Levine. 1995
Automatic generation of technical
documentation. Journal of Applied Artificial
Intelligence,  9(3) 259-287, 1995
Sanderson, M.  and B. Croft. 1999. Deriving
concept hierarchies from text. Proceedings of
the 22nd ACM SIGIR Conference, 206-213.
Scott, M. 1998. Focusing on the Text and Its
Key Words. TALC 98 Proceedings, Oxford,
Humanities Computing Unit, Oxford University.
Soderland, S. 1999. Learning information
extraction rules for semi-structured and free
text. Machine Learning, (1), 1-44.
Yangarber, R., R. Grishman, P. Tapanainen
and S. Huttunen. 2000. Automatic Acquisition
of Domain Knowledge for Information
Extraction. Proceedings of COLING 2000: The
18th International Conference on
Computational Linguistics, Saarbr?cken.
  	
 Utilizing Text Mining Results: The PastaWeb System
G. Demetriou and R. Gaizauskas
Department of Computer Science
University of Sheffield
Western Bank
Sheffield S10 2TU UK
 
g.demetriou, r.gaizauskas  @dcs.shef.ac.uk
Abstract
Information Extraction (IE), defined as
the activity to extract structured knowl-
edge from unstructured text sources, of-
fers new opportunities for the exploitation
of biological information contained in the
vast amounts of scientific literature. But
while IE technology has received increas-
ing attention in the area of molecular bi-
ology, there have not been many exam-
ples of IE systems successfully deployed
in end-user applications. We describe
the development of PASTAWeb, a WWW-
based interface to the extraction output of
PASTA, an IE system that extracts protein
structure information from MEDLINE ab-
stracts. Key characteristics of PASTAWeb
are the seamless integration of the PASTA
extraction results (templates) with WWW-
based technology, the dynamic generation
of WWW content from ?static? data and
the fusion of information extracted from
multiple documents.
1 Introduction
The rapidly growing volume of scientific literature,
a by-product of intensive research in molecular bi-
ology and bioinformatics, necessitates efficient and
effective information access to the published text
sources. Information retrieval (IR) techniques em-
ployed in WWW interfaces such as PubMed and
Entrez are very useful in browsing bibliographic
databases and in facilitating the linking between
protein or genome sequences and related refer-
ences. But the jobs of automatically locating and
extracting specific information within the texts re-
quire more specialised Natural Language Processing
(NLP) techniques and have been the object of work
in information extraction (IE) or text mining ((Cowie
and Lehnert, 1996)).
While current work on IE in biology has concen-
trated by and large on the refinement of IE tech-
niques and improving their accuracy, the incorpo-
ration of an IE system?s output into effective inter-
faces that genuinely assist the biological researcher
in his/her work is equally important, and has been
neglected to date. Of course improving IE tech-
niques, their accuracy and cross-domain portability
are important research objectives for language tech-
nology researchers. But given that the techniques
will remain imperfect for the foreseeable future, we
must also ask how biologists can benefit today from
the limited capabilities of existing IE technology.
In this paper we describe an approach to pro-
viding effective access to the results of the Protein
Active Site Template Acquisition (PASTA) system
(Humphreys et al, 2000; Demetriou et al, 2002),
an IE system that extracts information about amino
acid residues in protein structures and their roles in
protein active sites directly from the published lit-
erature. To experiment with a mechanism for de-
livering PASTA results to biologist end-users, we
have developed the PASTAWeb interface, a WWW-
based interface that offers search and browsing facil-
ities to the extracted protein structure information,
as well as to the original text sources (MEDLINE
                                            Association for Computational Linguistics.
                            the Biomedical Domain, Philadelphia, July 2002, pp. 77-84.
                         Proceedings of the Workshop on Natural Language Processing in
abstracts). PASTAWeb provides transparent text ac-
cess and advanced navigation capabilities to enable
users to track and display the relevant information
from text to text. The PASTAWeb facilities enable
users to find answers to implicit questions such as
What are the important residues for trypsin? or Is
serine found in the active site of amylase? and to
track the flow of information for specific classes of
biological entities (residues, proteins, species) from
text to text.
Given the performance limitations of current IE
technology, it is to be expected that some of the ex-
tracted information may only be partially correct,
missing or spurious. The PASTAWeb interface com-
pensates for the loss of information by supporting
rapid, easy verification by scientists of the extracted
information against the source texts.
2 IE and its Application to Biomedical
Texts
Perhaps not surprisingly, the identification of
biomedical terms in scientific texts has proved to be
the easiest extraction task and has demonstrated ac-
ceptable levels of performance, not too far from the
best results achieved in the NE task in the MUC
competitions, despite differences between the do-
mains (i.e. names of persons, organisations etc. in
MUC vs. terms identifying proteins, genes, drugs
etc. in biomedical domains). The techniques used
for this task vary from rule-based methods (Fukuda
et al, 1998; Humphreys et al, 2000), to statistical
methods (Collier et al, 2000) and statistical-rule-
based hybrids (Proux et al, 1998).
More complex IE tasks involving the extraction
of relational information have also been addressed
by the bioinformatics community. These include
protein or gene interactions (Sekimizu et al, 1998;
Thomas et al, 2000; Pustejovsky et al, 2002),
relations between genes and drugs (Rindflesh et
al., 2000) and identification of metabolic pathways
(Humphreys et al, 2000). The range of techniques
used in these systems varies considerably, but in
most cases requires the application of more sophis-
ticated NLP methods including part-of-speech tag-
ging, phrasal or syntactic parsing and (for some sys-
tems) semantic analysis and discourse processing.
To date IE researchers working on biological texts
have concentrated on building or porting systems to
work in biological domains. This paper addresses
the issue of utilising the IE results, after describing,
in the next section, the underlying PASTA extraction
system ? what it is designed to extract, how it works,
and how well it fares in blind evaluation using con-
ventional evaluation metrics.
<RESIDUE-134> :=
NAME: SERINE NO: 87
SITE/FUNCTION: "catalytic"
"calcium-binding"
"active-site"
SEC_STRUCT: "helical"
QUAT_STRUCT: <not specified>
REGION: "lid"
INTERACTION: <not specified>
<IN_PROTEIN> :=
RESIDUE: <RESIDUE-134>
PROTEIN: <PROTEIN-2>
<IN_SPECIES> :=
PROTEIN: <PROTEIN-2>
SPECIES: <SPECIES-5>
<PROTEIN-2> :=
NAME: "triacylglycerol lipase"
<SPECIES-5> :=
NAME: "Pseudomonas cepacia"
Figure 1: PASTA template example
3 The PASTA System
The overall aim of the PASTA system is to extract
information about the roles of residues in protein
molecules, specifically to assist in identifying ac-
tive sites and binding sites. We do not describe the
system in great detail here, as this is described else-
where (Demetriou et al, 2002).
3.1 PASTA Extraction Tasks
3.1.1 Terminological Tagging
A key component of PASTA, and of various other
IE systems operating in the biomedical domain is the
identification and classification of textual references
(terms) to key entity types in the domain. We have
identified 12 significant classes of technical terms
in the PASTA domain: protein, species, residue,
site, region, secondary structure, supersecondary
structure, quaternary structure, base, atom (ele-
ment), non-protein compound, interaction. Guide-
lines defining the scope of the term classes were
written, and an SGML-based markup scheme spec-
ified to allow instances of the term classes to be
tagged in texts1.
3.1.2 PASTA Template Filling Tasks
The PASTA template conforms to the MUC tem-
plate specification and is object oriented. Slot fillers
are of three types: (1) string fill ? a string excised
directly from the text (e.g. Pseudomonas cepa-
cia); (2) set fill ? a normalised form selected from a
predefined set (e.g. the expressions Ser or serine
are mapped to SERINE, one of a set of normalised
forms that represent the 20 standard amino acids);
(3) pointer fill ? a pointer to another template object,
used, e.g., for indicating relations between objects.
To meet the objectives of PASTA, three tem-
plate elements and two template relations were iden-
tified. The elements are RESIDUE, PROTEIN
and SPECIES; the two relations are IN PROTEIN,
holding between a residue and the protein in which
it occurs, and IN SPECIES, holding between a pro-
tein and the species in which it occurs.
An example of a template produced by PASTA
for a Medline abstract is shown in Figure 1, which
illustrates the three template element objects and
two template relation objects. As can be seen from
the figure, the <RESIDUE> template object con-
tains slots for the residue name and the residue
number in the sequence (NO). Secondary and qua-
ternary structural arrangements of the part of the
structure in which the residue is found are stored
in the SEC STRUCT and QUAT STRUCT slots re-
spectively. The SITE/FUNCTION slot is filled with
widely recognizable descriptions that indicate that
this residue is important for the structure?s activation
(e.g. active-site) or functional characteristics
(e.g. catalytic). The REGION slot is about
the more general geographical areas of the structure
(e.g. lid) in which this particular residue is found2.
The INTERACTION slot captures textual references
to hydrogen bonds, disulphide bonds or other types
of atomic contacts. At this point the only attributes
1The term class annotation guidelines are available at:
http://www.dcs.shef.ac.uk/nlp/pasta.
2A residue may belong to more than one region
extracted for protein and species objects are their
names.
3.2 System Architecture
The PASTA system has been adapted from an
IE system called LaSIE (Large Scale Information
Extraction), originally developed for participation in
the MUC competitions (Humphreys et al, 1998).
The PASTA system is a pipeline of processing com-
ponents that perform the following major tasks: text
preprocessing, terminological processing, syntactic
and semantic analysis, discourse interpretation, and
template extraction.
Text Preprocessing The text preprocessing phase
aims at low-level text processing tasks including the
analysis of the structure of the MEDLINE abstracts
in terms of separate sections (e.g. the title, au-
thor names, abstract etc.), tokenisation and sentence
boundary identification. With respect to tokenisa-
tion, tokens are identified at the subword level result-
ing in the splitting of biochemical compound terms
into their constituents which need to be matched
separately during the lexical lookup phase. For ex-
ample, the term Cys128 is split to the three-letter
residue abbreviation Cys and the numeral 128.
Terminological Processing The aim of the 3-
stage terminological processing phase is to identify
and correctly classify instances of the term classes
described above in section 3.1.1. During the mor-
phological analysis stage individual tokens are anal-
ysed to see if they contain interesting biochemical
affixes such as -ase or -in that indicate candidate
protein names respectively.
During the lexical lookup stage the previously to-
kenised terms are matched against terminological
lexicons which have been compiled from biologi-
cal databases such as CATH3 and SCOP4 and have
been augmented with terms produced by corpus
processing techniques (Demetriou and Gaizauskas,
2000). Additional subcategorisation information is
provided for multi-token terms by splitting the terms
into their constituents and placing the constituents
into subclasses whose combination is determined by
grammar rules.
3http://www.biochem.ucl.ac.uk/bsm/cath/index.html
4http://scop.mrc-lmb.cam.ac.uk/scop/
Development Interannotator Blind
Recall Precision Recall Precision Recall Precision
Terminology recognition 88 94 92 86 82 84
Template extraction 69 79 78 80 69 64
Table 1: Summary evaluation results for term recognition/classification and template extraction.
Finally, in a terminology parsing stage, a rule-
based parser is used to analyse the tokenisation in-
formation and the morphological and lexical proper-
ties of component terms and to combine them into a
single multi-token unit.
Syntactic and Semantic Processing Terms clas-
sified during the previous stages (proteins, species,
residues etc.) are passed to the syntactic process-
ing modules as non-decomposable noun phrases and
a part-of-speech tagger assigns syntactic labels to
the remaining text tokens. With the application of
phrasal grammar rules, the phrase structure of each
sentence is derived and this is used to build a seman-
tic representation via compositional semantic rules.
Discourse Processing During the discourse pro-
cessing stage, the semantic representation of each
sentence is added to a predefined domain model
which provides a conceptualisation of the knowl-
edge of the domain. The domain model consists of
a concept hierarchy (ontology) together with inheri-
table properties and inference rules for the concepts.
Instances of concepts are gradually added to the hi-
erarchy in order to construct a complete discourse
model of the input text.
Template Extraction A template writing module
scans the final discourse model for any instances that
are relevant to the template filling task, ensures that
it has all necessary information to generate a tem-
plate and fill its slots, and then formats and outputs
the templates.
3.3 Development and Evaluation
Following standard IE system development method-
ology, a corpus of texts relevant to the study of
protein structure was assembled. The corpus con-
sists of 1513 Medline abstracts from 20 major scien-
tific journals that publish new macromolecular struc-
tures. Of these abstracts, 113 were manually tagged
for the 12 term classes mentioned above and 55 had
associated templates filled manually. These anno-
tated data were divided into distinct training and test
sets.
The corpus and annotated data assisted in the re-
finement of the extraction task definitions, supported
system development and permitted final blind eval-
uation of the system. Detailed results of the evalu-
ation, for each term class, and for each slot in the
templates, can be found in Demetriou et al (2002).
In Table 1 we present the summary totals for the de-
velopment corpus, the unseen final evaluation cor-
pus (Blind) and the human interannotator agreement
where one annotator is taken to be the gold standard
and the other scored against him/her. The evaluation
metrics are the well known measures of precision
and recall.
4 The PastaWeb Interface
The PASTAWeb interface5 is aimed at providing
quick access and navigation facilities through the
database of the PASTA tagged texts and their asso-
ciated templates. PASTAWeb has borrowed ideas
from the interface component of the TRESTLE6
system Gaizauskas et al (2001) developed to sup-
port information workers in the pharmaceutical in-
dustry. Key characteristics of PASTAWeb are the
seamless integration between the PASTA IE results
and WWW-based browsing technology, the dynamic
generation of WWW pages from ?static? content
and the fusion of information relating to proteins and
amino acid residues when found in different sources.
4.1 PASTAWeb Architecture
The PASTAWeb architecture is illustrated in Fig 2.
5Accessible at http://www.gate.ac.uk/cgi-bin/
pasta.cgi?source=start or via
the PASTA project home page at
http://www.dcs.shef.ac.uk/nlp/pasta/
6TRESTLE:Text Retrieval Extraction and Summarisation
Technologies for large Enterprises
PASTA IE 
NE Tagged Texts
 Templates
Indexer
Entity/Template
    Indices
Dynamic Page
   Generation
Web Server
  Medline
Information Seeking User
Figure 2: The PASTAWeb Architecture
Access Frame Header Frame Document Index Frame
Template Flags
Template Flag
(multiple templates)
(single template)
Colur Index to
Tagged Entities
Template in
Tabular Format
Tagged Text
   Frame
Figure 3: The PASTAWeb Interface
Initially, MEDLINE abstracts are fed through the
PASTA IE system which produces two kinds of out-
put: (i) texts annotated with SGML tags describing
term class information for protein, residues, species,
regions, and (ii) templates which are used as the
main stores of information about residues including
relational information between proteins and residues
and between proteins and species.
Once PASTA has run, a separate indexing process
creates three indices. The first associates with each
processed document the terminology tagged version
of the text and any templates extracted from the text.
The second is a relational table between each doc-
ument and each of the instances of the main term
classes (i.e. proteins, residues or species) mentioned
in the document. This index also points to the title
of the document, because the title can provide vital
clues about the content of the text.
The final index is used to assist the ?fusion? of
the information in templates generated from multi-
ple texts for the same protein. This index provides
information about those proteins for which there are
templates generated from multiple documents. Due
to variations in the expression of the same protein
name from text to text, the identification of suit-
able templates for fusion is not trivial. The prob-
lem of matching variant expressions of the same
term in different databases is a well known prob-
lem in bioinformatics. The current implementation
of the indexing addresses this problem using sim-
ple heuristic rules. Simply put, two templates are
considered suitable for fusion if the protein names
either match exactly (ignoring case sensitivity) or
they include the same ?head term?. The applica-
bility of the heuristic for finding a ?head term? is
limited to constituent terms ending in -ase or -in
(to exclude common words, such as ?protein?, ?do-
main? etc.). For example, the protein terms ?scor-
pion toxin?, ?diphtheria toxin? and ?toxins? would
match with each other because they all include the
head term ?toxin?. Consequently, the corresponding
template information about the residues occurring in
these proteins would be merged into a single table,
though information about which slot fillers belong
to which term variant is retained.
The decision to do the matching of variant names
at the index level and not at the interface level is
simply due to operational issues. Matching the pro-
tein names from multiple texts involves the pair-
wise string comparisons between all proteins in the
PASTA templates. The number of these compar-
isons increases very rapidly as new texts and tem-
plates are added to the database and it was found
that is causes considerable delay to the operation of
the PASTAWeb interface.
Since information seeking tasks of molecular bi-
ologists may require complex navigation capabil-
ities, the storing of the results in ?static? HTML
pages would have been unsuitable both practically
(more difficult to implement pointers between dif-
ferent pieces of information and to alter and main-
tain pages) and economically (requires more disk
space). We therefore opted for a dynamic page cre-
ator that is triggered by the users? requests expressed
as choices over hypertext links. The dynamic page
creator compiles the information from the indices
and the associated databases (texts and templates)
and sends the results to the WWW browser via a
Web server. In the dynamically created pages, each
hypertext link encodes the current frame, the infor-
mation to be displayed when the link is selected,
and the frame in which this information is to be dis-
played. For example, the hypertext link for a title
of a document encodes information about the docu-
ment id of the document as well as about the target
frame in which the text will be displayed. Click-
ing on this link expresses a request to PASTAWeb
for displaying that particular text in the target frame.
The whole operation of PASTAWeb loosely resem-
bles the operation of a finite-state automaton.
4.2 Interface Overview
PASTAWeb offers a number of ways of access-
ing the protein structure information extracted by
PASTA. As shown in Fig 3 the interface layout can
be split into four main areas (frames). On the left
side of the page we find the ?Access Frame? which
allows the user to select amongst text access options.
These options include browsing the contents of the
text databases via either the protein, the residue or
the species indices or via a text search option over
these indexed terms.
The right hand side of the screen is split into
three frames. The top frame, so called ?Header
Frame?(see Fig 3), is used to generate an alphabeti-
cal index for protein or species names whenever the
user has chosen the protein or species access modes
for navigation. For residues, rather than an alphabet-
ical index, a list of residue names is displayed in the
?Header Frame?. This is because while the number
of protein names and their variants is probably inde-
terminable, the number of residues remains constant
(i.e the 20 standard amino acids).
Just below the ?Header Frame? is the ?Document
Index Frame? which initially serves to display the
automatically generated indices together with docu-
ment information. The ?Index Frame? is split into
two columns, the left of which is used to present an
alphabetically sorted list of the chosen type of index
(i.e. protein, residue, species). The right column
occupies more space because it displays the list of
corresponding document titles (as extracted by the
PASTA IE system). These titles are presented as
clickable hyperlinks to the full texts each of which
can be displayed in the ?Tagged Text Frame? below.
A second use of the ?Index Frame? is for display-
ing template results, explained in more detail below.
4.3 Term Access to Texts
A typical interaction session with PASTAWeb re-
quires the user to select one of the three term cate-
gories in the ?Access Frame?, i.e. proteins, residues
or species. The ?Header Frame? then displays a list
of alphabetical indices (for proteins and species) or
a list of residue names. Selecting any of these in-
dices, e.g. ?M? for proteins, activates the dynamic
generation of a list of protein terms that are indexed
by ?M? (on the left) of the ?Index Frame? and their
corresponding document titles (on the right). Differ-
ent font colours are used to distinguish between the
two different kinds of information.
The selection of any of the title links causes the
system to dynamically transform the PASTA-tagged
text from SGML to HTML and display it in the bot-
tom ?Tagged Text Frame? with the recognised term
types highlighted in different colours. The colour in-
dex for the term categories can be viewed in a frame
just below the ?Access Frame? (the ?Colour Index
Frame?). Each tagged protein, species or residue
term is itself a hyperlink which can be used to dy-
namically fetch the indices of the texts in which this
term occurs and display them in the ?Index Frame?.
Using this functionality, the user can navigate
through a succession of texts following a single term
or at any point branching off this chain by select-
ing another term and following its occurrences in the
text collection.
4.4 Web-based Access to Templates
Unfortunately, although the type of object-oriented
template produced by PASTA (Fig 1) is an efficient
data structure for storing complex information, it is
not suitable for displaying to end-users. For this
reason, the templates are dynamically converted to
a format that can be readily accommodated to the
screen?s layout while being at the same time eas-
ily accessible. The format chosen for displaying the
PASTA templates is tabular and is implemented as
an HTML table (see background picture in Fig 3).
Access to the templates produced by PASTA is
facilitated by special template ?icons? or ?flags?
which are displayed next to text titles or protein
terms in the ?Index Frame?.
When a ?single? template icon is displayed to the
right of a title, this serves to flag that a template for
this text is available and can be accessed by click-
ing on the icon. On the other hand, when a ?double?
template icon is displayed next to a protein name in
the left column of the ?index frame?, this indicates
that there are multiple templates (i.e. templates ex-
tracted from different texts) for this protein. Click-
ing on either of these icons will trigger PASTAWeb
to scan the corresponding object-oriented templates,
analyse their structures and convert them into tab-
ular format. In the case of fused templates the in-
formation is assimilated into a single template. The
template information is then displayed in the ?Index
Frame? together a hyperlink to the title of the origi-
nal text which, when selected, displays the (tagged)
text in the ?Tagged Text Frame?. This enables the
user to retrieve more detailed information from the
text if needed, or to inspect and verify the correct-
ness of the extracted information.
PASTAWeb offers a simple and easy to use mech-
anism for the tracking of information for a specific
entity from text to text, but can also assist in the
linking of information between different entities in
multiple documents. Starting with a specific pro-
tein in mind for example, a molecular biologist may
want to investigate structural similarities between
that and other proteins with respect to what has been
described in the literature.
5 Conclusions and Future Work
We have described PASTAWeb, a prototype Web-
based interface to the IE results of the PASTA sys-
tem. Scientists in the area of molecular biology can
benefit from the novel navigation and information
tracking capabilities of PASTAWeb and use it as a
tool for fast access to specialised information.
At the time of writing, the database of processed
texts accessible through PASTAWeb is rather small
(975 texts in total). The rate at which new articles
appear on MEDLINE and the limited resources de-
voted to PASTA make it prohibitive to develop and
maintain for PASTAWeb a text database of size com-
parable to MEDLINE. Nevertheless, PASTAWeb of-
fers the core technology for the development of a
fully automated IE system whose input can be based
on automated updates (?autoalerts?) from MED-
LINE without human intervention. Current work
concentrates on the development of such an auto-
mated software component and on the feasibility of
expanding the system?s navigation capabilities to al-
low users to link together information provided by
PASTAWeb and by related servers such as the Pro-
tein Data Bank or SWISSPROT.
Finally, the utility of an interface such as
PASTAWeb can only be truly assessed by user evalu-
ation. Usability evaluation should be carried out us-
ing both qualitative and quantitative methods. Qual-
itative evaluation should be used to review the users?
perceptions about the design, their preferred strate-
gies for accessing information and whether they find
the system easy to use and useful for performing
their tasks. Quantitative evaluation should focus on
measures of activity time, efficiency in tracking rele-
vant information and on analysing the effect ?noise?
in the IE results has on user satisfaction. However,
while this evaluation remains to be done, we believe
that the work presented here provides concrete, con-
structive ideas about how to effectively utilise the
output of IE systems in the biology domain.
References
N. Collier, C. Nobata, and J. Tsujii. 2000. Extracting
the names of genes and gene products with a hidden
markov model. In Proc. of the 18th Int. Conf. on Com-
putational Linguistics (COLING-2000), pp. 201?207.
J. Cowie and W. Lehnert. 1996. Information extraction.
Communications of the ACM, 39(1):80?91.
G. Demetriou and R. Gaizauskas. 2000. Automat-
ically augmenting terminological lexicons from un-
tagged text. In Proc. of the 2nd Int. Conf. on Language
Resources and Evaluation (LREC-2000), pp. 861?867,
Athens, May-June.
G. Demetriou, R. Gaizauskas, P. Artymiuk, and P. Wil-
lett. 2002. Protein structures and information extrac-
tion from biological texts: The PASTA system. Bioin-
formatics. Accepted for publication.
K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi.
1998. Information extraction: Identifying protein
names from biological papers. In Proc. of the Pacific
Symp. on Biocomputing ?98 (PSB?98), pp. 707?718,
Hawaii, January.
R. Gaizauskas, P. Herring, M. Oakes, M. Beaulieu,
P. Willett, H. Fowkes, and A. Jonsson. 2001. Intelli-
gent access to text: Integrating information extraction
technology into text browsers. In Proc. of the Human
Language Technology Conf. (HLT 2001), pp. 189?193,
San Diego.
K. Humphreys, R. Gaizauskas, S. Azzam, C Huyck,
B. Mitchell, H. Cunningham, and Y. Wilks.
1998. Description of the LaSIE-II system as
used for MUC-7. In Proc. of the 7th Mes-
sage Understanding Conf. (MUC-7). Available at
http://www.itl.nist.gov/iaui/894.02/related projects/muc/.
K. Humphreys, G. Demetriou, and R. Gaizauskas. 2000.
Two applications of information extraction to bio-
logical science journal articles: Enzyme interactions
and protein structures. In Proc. of the Pacific Symp.
on Biocomputing ?2000 (PSB?2000), pp. 505?516,
Hawaii, January.
D. Proux, F. Rechenmann, L. Julliard, V. Pillet, and
B. Jacq. 1998. Detecting gene symbols and names
in biological texts. In Proc. of the 9th Workshop on
Genome Informatics, pp. 72?80.
J. Pustejovsky, J. Castano, J. Zhang, M. Kotecki, and
B. Cochran. 2002. Robust relational parsing over
biomedical literature: Extracting inhibit relations. In
Proc. of the Pacific Symp. on Biocomputing 2002
(PSB?2002), pp. 362?373, Hawaii, January.
T. Rindflesh, L. Tanabe, J. Weinstein, and L. Hunter.
2000. Edgar: Extraction of drugs, genes and relations
from the biomedical literature. In Proc. of the Pacific
Symp. on Biocomputing ?2000 (PSB?2000), pp. 517?
528, Hawaii, January.
T. Sekimizu, H. S. Park, and J. Tsujii. 1998. Identify-
ing the interactions between genes and gene products
based on frequently seen verbs in Medline abstracts.
In Proc. of Genome Informatics, pp. 62?71, Tokyo.
J. Thomas, D. Milward, C. Ouzounis, S. Pulman, and
M. Carroll. 2000. Automatic extraction of protein
interactions from scientific abstracts. In Proc. of the
Pacific Symp. on Biocomputing ?2000 (PSB?2000), pp.
541?551, Hawaii, January.
A Large Scale Terminology Resource for Biomedical Text Processing
Henk Harkema, Robert Gaizauskas, Mark Hepple, Angus Roberts,
Ian Roberts, Neil Davis, Yikun Guo
Department of Computer Science, University of Sheffield, UK
biomed@dcs.shef.ac.uk
Abstract
In this paper we discuss the design, implemen-
tation, and use of Termino, a large scale termi-
nological resource for text processing. Dealing
with terminology is a difficult but unavoidable
task for language processing applications, such
as Information Extraction in technical domains.
Complex, heterogeneous information must be
stored about large numbers of terms. At the
same time term recognition must be performed
in realistic times. Termino attempts to recon-
cile this tension by maintaining a flexible, ex-
tensible relational database for storing termino-
logical information and compiling finite state
machines from this database to do term look-
up. While Termino has been developed for
biomedical applications, its general design al-
lows it to be used for term processing in any
domain.
1 Introduction
It has been widely recognized that the biomedical litera-
ture is now so large, and growing so quickly, that it is be-
coming increasingly difficult for researchers to access the
published results that are relevant to their research. Con-
sequently, any technology that can facilitate this access
should help to increase research productivity. This has
led to an increased interest in the application of natural
language processing techniques for the automatic capture
of biomedical content from journal abstracts, complete
papers, and other textual documents (Gaizauskas et al,
2003; Hahn et al, 2002; Pustejovsky et al, 2002; Rind-
flesch et al, 2000).
An essential processing step in these applications is
the identification and semantic classification of techni-
cal terms in text, since these terms often point to enti-
ties about which information should be extracted. Proper
semantic classification of terms also helps in resolving
anaphora and extracting relations whose arguments are
restricted semantically.
1.1 Challenge
Any technical domain generates very large numbers of
terms ? single or multiword expressions that have some
specialised use or meaning in that domain. For exam-
ple, the UMLS Metathesaurus (Humphreys et al, 1998),
which provides a semantic classification of terms from a
wide range of vocabularies in the clinical and biomedical
domain, currently contains well over 2 million distinct
English terms.
For a variety of reasons, recognizing these terms in
text is not a trivial task. First of all, terms are often
long multi-token sequences, e.g. 3-methyladenine-DNA
glycosylase I. Moreover, since terms are referred to re-
peatedly in discourses there is a benefit in their being
short and unambiguous, so they are frequently abbre-
viated and acronymized, e.g. CvL for chromobacterium
viscosum lipase. However, abbreviations may not al-
ways occur together with their full forms in a text, the
method of abbreviation is not predictable in all cases, and
many three letter abbreviations are highly overloaded.
Terms are also subject to a high degree of orthographic
variation as a result of the representation of non-Latin
characters, e.g. a-helix vs. alpha-helix, capitalization,
e.g. DNA vs. dna, hyphenation, e.g. anti-histamine vs. an-
tihistamine, and British and American spelling variants,
e.g. tumour vs. tumor. Furthermore, biomedical science
is a dynamic field: new terms are constantly being in-
troduced while old ones fall into disuse. Finally, certain
classes of biomedical terms exhibit metonomy, e.g. when
a protein is referred to by the gene that expresses it.
To begin to address these issues in term recognition, we
are building a large-scale resource for storing and recog-
nizing technical terminology, called Termino. This re-
source must store complex, heterogeneous information
about large numbers of terms. At the same time term
recognition must be performed in realistic times. Ter-
mino attempts to reconcile this tension by maintaining a
                                            Association for Computational Linguistics.
                   Linking Biological Literature, Ontologies and Databases, pp. 53-60.
                                                HLT-NAACL 2004 Workshop: Biolink 2004,
flexible, extensible relational database for storing termi-
nological information and compiling finite state machines
from this database to do term look-up.
1.2 Context
Termino is being developed in the context of two ongoing
projects: CLEF, for Clinical E-Science Framework (Rec-
tor et al, 2003) and myGrid (Goble et al, 2003). Both
these projects involve an Information Extraction compo-
nent. Information Extraction is the activity of identifying
pre-defined classes of entities and relationships in natural
language texts and storing this information in a structured
format enabling rapid and effective access to the informa-
tion, e.g. Gaizauskas and Wilks (1998), Grishman (1997).
The goal of the CLEF project is to extract information
from patient records regarding the treatment of cancer.
The treatment of cancer patients may extend over several
years and the resulting clinical record may include many
documents, such as clinic letters, case notes, lab reports,
discharge summaries, etc. These documents are gener-
ally full of medical terms naming entities such as body
parts, drugs, problems (i.e. symptoms and diseases), in-
vestigations and interventions. Some of these terms are
particular to the hospital from which the document origi-
nates. We aim to identify these classes of entities, as well
as relationships between such entities, e.g. that an investi-
gation has indicated a particular problem, which, in turn,
has been treated with a particular intervention. The infor-
mation extracted from the patient records is potentially of
value for immediate patient care, but can also be used to
support longitudinal and epidemiological medical stud-
ies, and to assist policy makers and health care managers
in regard to planning and clinical governance.
The myGrid project aims to present research biolo-
gists with a unified workbench through which component
bioinformatic services can be accessed using a workflow
model. These services may be remotely located from the
user and will be exploited via grid or web-service chan-
nels. A text extraction service will form one of these ser-
vices and will facilitate access to information in the sci-
entific literature. This text service comprises an off-line
and an on-line component. The off-line component in-
volves pre-processing a large biological sciences corpus,
in this case the contents of Medline, in order to identify
various biological entities such as genes, enzymes, and
proteins, and relationships between them such as struc-
tural and locative relations. These entities and relation-
ships are referred to in Medline abstracts by a very large
number of technical terms and expressions, which con-
tributes to the complexity of processing these texts. The
on-line component supports access to the extracted infor-
mation, as well as to the raw texts, via a SOAP interface
to an SQL database.
Despite the different objectives for text extraction
within the CLEF and myGrid projects, many of the tech-
nical challenges they face are the same, such as the
need for extensive capabilities to recognize and classify
biomedical entities as described using complex techni-
cal terminology in text. As a consequence we are con-
structing a general framework for the extraction of infor-
mation from biomedical text: AMBIT, a system for ac-
quiring medical and biological information from text. An
overview of the AMBIT logical architecture is shown in
figure 1.
The AMBIT system contains several engines, of which
Termino is one. The Information Extraction Engine pulls
selected information out of natural language text and
pushes this information into a set of pre-defined tem-
plates. These are structured objects which consists of one
or more slots for holding the extracted entities and rela-
tions. The Query Engine allows users to access informa-
tion through traditional free text search and search based
on the structured information produced by the Informa-
tion Extraction Engine, so that queries may refer to spe-
cific entities and classes of entities, and specific kinds of
relations that are recognised to hold between them. The
Text Indexing Engine is used to index text and extracted,
structured information for the purposes of information re-
trieval. The AMBIT system contains two further compo-
nents: an interface layer, which provides a web or grid
channel to allow user and program access to the system;
and a database which holds free text and structured infor-
mation that can be searched through the Query Engine.
Termino interacts with the Query Engine and the Text
Indexing Engine to provide terminological support for
query formulation and text indexation. It also provides
knowledge for the Information Extraction Engine to use
in identifying and classifying biomedical entities in text.
The Terminology Engine can furthermore be called by
users and remote programs to access information from
the various lexical resources that are integrated in the ter-
minological database.
2 Related Work
Since identification and classification of technical terms
in biomedical text is an essential step in information
extraction and other natural language processing tasks,
most natural language processing systems contain a
terminological resource of some sort. Some systems
make use of existing terminological resources, notably
the UMLS Metathesaurus, e.g. Rindflesch et al (2000),
Pustejovski et al (2002); other systems rely on re-
sources that have been specifically built for the applica-
tion, e.g. Humphreys et al (2000), Thomas et al (2000).
The UMLS Metathesaurus provides a semantic classi-
fication of terms drawn from a wide range of vocabularies
in the clinical and biomedical domain (Humphreys et al,
1998). It does so by grouping strings from the source vo-
from Hospital 1
Clinical Records
Journals
On?line
Abstracts
Medline
Literature
Biomedical
Engine
Indexing
Text
Engine
Extraction
...
(Termino)
Engine
Terminology
Ambit
from Hospital 2
Clinical Records
Web GRID
Interface layer
Raw text
(entities / relations)
Structured InfoFree text
  search
Engine
Query
Information
SOAP /
     HTTP
& Annotations
Structured Info
Figure 1: AMBIT Architecture
cabularies that are judged to have the same meaning into
concepts, and mapping these concepts onto nodes or se-
mantic types in a semantic network. Although the UMLS
Metathesaurus is used in a number of biomedical natural
language processing applications, we have decided not to
adopt the UMLS Metathesaurus as the primary terminol-
ogy resource in AMBIT for a variety of reasons.
One of the reasons for this decision is that the Metathe-
saurus is a closed system: strings are classified in terms
of the concepts and the semantic types that are present
in the Metathesaurus and the semantic network, whereas
we would like to be able to link our terms into multi-
ple ontologies, including in-house ontologies that do not
figure in any of the Metathesaurus? source vocabularies
and hence are not available through the Metathesaurus.
Moreover, we would also like to be able to have access to
additional terminological information that is not present
in the Metathesaurus, such as, for example, the annota-
tions in the Gene Ontology (The Gene Ontology Con-
sortium, 2001) assigned to a given human protein term.
While the terms making up the the tripartite Gene On-
tology are present in the UMLS Metathesaurus, assign-
ments of these terms to gene products are not recorded
in the Metathesaurus. Furthermore, as new terms appear
constantly in the biomedical field we would like to be
able to instantly add these to our terminological resource
and not have to wait until they have been included in the
UMLS Metathesaurus. Additionally, some medical terms
appearing in patient notes are hospital-specific and are
unlikely to be included in the Metathesaurus at all.
With regard to systems that do not use the UMLS
Metathesaurus, but rather depend on terminological re-
sources that have been specifically built for an applica-
tion, we note that these terminological resources tend to
be limited in the following two respects. First, the struc-
ture of these resources is often fixed and in some cases
amounts to simple gazetteer lists. Secondly, because of
their fixed structure, these resources are usually popu-
lated with content from just a few sources, leaving out
many other potentially interesting sources of terminolog-
ical information.
Instead, we intend for Termino to be an exten-
sible resource that can hold diverse kinds of termi-
nological information. The information in Termino
is either imported from existing, outside knowledge
sources, e.g. the Enzyme Nomenclature (http://www.
chem.qmw.ac.uk/iubmb/enzyme/), the Structural Classi-
fication of Proteins database (Murzin et al, 1995), and
the UMLS Metathesaurus, or it is induced from on-line
raw text resources, e.g. Medline abstracts. Termino thus
provides uniform access to terminological information
aggregated across many sources. Using Termino re-
moves the need for multiple, source-specific terminolog-
ical components within text processing systems that em-
ploy multiple terminological resources.
3 Architecture
Termino consists of two components: a database holding
terminological information and a compiler for generating
term recognizers from the contents of the database. These
two components will be discussed in the following two
sections.
STRINGS
string str id
. . . . . .
neurofibromin str728
abdomen str056
mammectomy str176
mastectomy str183
. . . . . .
TERMOID STRINGS
trm id str id
. . . . . .
trm023 str056
trm656 str056
trm924 str728
trm369 str728
trm278 str176
trm627 str183
. . . . . .
PART OF SPEECH
trm id pos
. . . . . .
trm023 N
. . . . . .
SYNONYMY
syn id trm id scl id
. . . . . . . . .
syn866 trm278 syn006
syn435 trm627 syn006
. . . . . . . . .
GO ANNOTATIONS
trm id annotation version
. . . . . . . . .
trm924 GO:0004857 9/2003
trm369 GO:0008285 9/2003
. . . . . . . . .
UMLS
trm id cui lui sui version
. . . . . . . . . . . . . . .
trm278 C0024881 L0024669 S0059711 2003AC
trm656 C0000726 L0000726 S0414154 2003AC
. . . . . . . . . . . . . . .
Figure 2: Structure of the terminological database
3.1 Terminological Database
The terminological database is designed to meet three re-
quirements. First of all, it must be capable of storing large
numbers of terms. As we have seen, the UMLS Metathe-
saurus contains over 2 million distinct terms. However,
as UMLS is just one of many resources whose terms may
need to be stored, many millions of terms may need to
be stored in total. Secondly, Termino?s database must
also be flexible enough to hold a variety of information
about terms, including information of a morpho-syntactic
nature, such as part of speech and morphological class;
information of a semantic nature, such as quasi-logical
form and links to concepts in ontologies; and provenance
information, such as the sources of the information in the
database. The database will also contain links to connect
synonyms and morphological and orthographic variants
to one another and to connect abbreviations and acronyms
to their full forms. Finally, the database must be orga-
nized in such a way that it allows for fast and efficient
recognition of terms in text.
As mentioned above, the information in Termino?s
database is either imported from existing, outside knowl-
edge sources or induced from text corpora. Since these
sources are heterogeneous in both information content
and format, Termino?s database is ?extensional?: it stores
strings and information about strings. Higher-order con-
cepts such as ?term? emerge as the result of interconnec-
tions between strings and information in the database.
The database is organized as a set of relational tables,
each storing one of the types of information mentioned
above. In this way, new information can easily be in-
cluded in the database without any global changes to the
structure of the database.
Terminological information about any given string is
usually gathered from multiple sources. As information
about a string accumulates in the database, we must make
sure that co-dependencies between various pieces of in-
formation about the string are preserved. This considera-
tion leads to the fundamental element of the terminologi-
cal database, a termoid. A termoid consists of a string to-
gether with associated information of various kinds about
the string. Information in one termoid holds conjunc-
tively for the termoid?s string, while multiple termoids
for the same string express disjunctive alternatives.
For instance, taking an example from UMLS, we may
learn from one source that the string cold as an adjective
refers to a temperature, whereas another source may tell
us that cold as a noun refers to a disease. This informa-
tion is stored in the database as two termoids: abstractly,
?cold, adjective, temperature? and ?cold, noun, disease?.
A single termoid ?cold, adjective, noun, temperature, dis-
ease? would not capture the co-dependency between the
part of speech and the ?meaning? of cold.1 This example
illustrates that a string can be in more than one termoid.
1Note that the UMLS Metathesaurus has no mechanism for
storing this co-dependency between grammatical and semantic
information.
Each termoid, however, has one and only one string.
Figure 2 provides a detailed example of part of the
structure of the terminological database. In the table
STRINGS every unique string is assigned a string iden-
tifier (str id). In the table TERMOID STRINGS each string
identifier is associated with one or more termoid iden-
tifiers (trm id). These termoid identifiers then serve as
keys into the tables holding terminological information.
Thus, in this particular example, the database includes
the information that in the Gene Ontology the string
neurofibromin has been assigned the terms with identi-
fiers GO:0004857 and GO:0008285. Furthermore, in the
UMLS Metathesaurus version 2003AC, the string mam-
mectomy has been assigned the concept-unique identifier
C0024881 (CUI), the lemma-unique identifier L0024669
(LUI), and the string-unique identifier S0059711 (SUI).
Connections between termoids such as those arising
from synonymy and orthographic variation are recorded
in another set of tables. For example, the table SYN-
ONYMY in figure 2 indicates that termoids 278 and
627 are synonymous, since they have the same syn-
onymy class identifier (scl id).2 The synonymy identifier
(syn id) identifies the assignment of a termoid to a partic-
ular synonymy class. This identifier is used to record the
source on which the assignment is based. This can be a
reference to a knowledge source from which synonymy
information has been imported into Termino, or a refer-
ence to both an algorithm by which and a corpus from
which synonyms have been extracted. Similarly there are
tables containing provenance information for strings, in-
dexed by str id, and termoids, indexed by trm id. These
tables are not shown in he example.
With regard to the first requirement for the design of
the terminological database mentioned at the beginning
of this section ? scalability ?, an implementation of Ter-
mino in MySQL has been loaded with 427,000 termoids
for 363,000 strings (see section 4 for more details). In it
the largest table, STRINGS, measures just 16MB, which is
nowhere near the default limit of 4GB that MySQL im-
poses on the size of tables. Hence, storing a large num-
ber of terms in Termino is not a problem size-wise. The
second requirement, flexibility of the database, is met by
distributing terminological information over a set of rela-
tively small tables and linking the contents of these tables
to strings via termoid identifiers. In this way we avoid the
strictures of any one fixed representational scheme, thus
making it possible for the database to hold information
from disparate sources. The third requirement on the de-
sign of the database, efficient recognition of terms, will
2The function of synonymy class identifiers in Termino is
similar to the function of CUIs in the UMLS Metathesaurus.
However, since we are not bound to a classification into UMLS
CUIs, we can assert synonymy between terms coming from ar-
bitrary sources.
be addressed in the next section.
3.2 Term Recognition
To ensure fast term recognition with Termino?s vast ter-
minological database, the system comes equipped with
a compiler for generating finite state machines from the
strings in the terminological database discussed in the
previous section. Direct look-up of strings in the database
is not an option, because it is unknown in advance at
which positions in a text terms will start and end. In order
to be complete, one would have to look up all sequences
of words or tokens in the text, which is very inefficient.
Compilation of a finite state recognizer proceeds in
the following way. First, each string in the database is
broken into tokens, where a token is either a contigu-
ous sequence of alpha-numeric characters or a punctu-
ation symbol. Next, starting from a single initial state, a
path through the machine is constructed, using the tokens
of the string to label transitions. For example, for the
string Graves? disease the machine will include a path
with transitions on Graves, ?, and disease. New states are
only created when necessary. The state reached on the fi-
nal token of a string will be labeled final and is associated
with the identifiers of the termoids for that string.
To recognize terms in text, the text is tokenized and the
finite state machine is run over the text, starting from the
initial state at each token in the text. For each sequence
of tokens leading to a final state, the termoid identifiers
associated with that state are returned. These identifiers
are then used to access the terminological database and
retrieve the information contained in the termoids. Where
appropriate the machine will produce multiple termoid
identifiers for strings. It will also recognize overlapping
and embedded strings.
Figure 3 shows a small terminological database and a
finite state recognizer derived from it. Running this rec-
ognizer over the phrase . . . thyroid dysfunction, such as
Graves? disease . . . produces four annotations: thyroid
is assigned the termoid identifiers trm1 and trm2; thyroid
dysfunction, trm3; and Graves? disease, trm4.
It should be emphasised at this point that term recog-
nition as performed by Termino is in fact term look-up
and not the end point of term processing. Term look-up
might return multiple possible terms for a given string,
or for overlapping strings, and subsequent processes may
apply to filter these alternatives down to the single option
that seems most likely to be correct in the given context.
Furthermore, more flexible processes of term recognition
might apply over the results of look-up. For example, a
term grammar might be provided for a given domain, al-
lowing longer terms to be built from shorter terms that
have been identified by term look-up.
The compiler can be parameterized to produce finite
state machines that match exact strings only, or that ab-
STRINGS
string str id
thyroid str12
thyroid disfunction str15
Graves? disease str25
TERMOID STRINGS
trm id str id
trm1 str12
trm2 str12
trm3 str15
trm4 str25
? trm4disease
thyroid
Graves
trm3
trm2
trm1
disfunction
Figure 3: Sample terminological database and finite state term recognizer
stract away from morphological and orthographical vari-
ation. At the moment, morphological information about
strings is supplied by a component outside Termino. In
our current term recognition system, this component ap-
plies to a text before the recognition process and asso-
ciates all verbs and nouns with their base form. Similarly,
the morphological component applies to the strings in the
terminological database before the compilation process.
The set-up in which term recognizers are compiled
from the contents of the terminological database turns
Termino into a general terminological resource which is
not restricted to any single domain or application. The
database can be loaded with terms from multiple domains
and compilation can be restricted to particular subsets of
strings by selecting termoids from the database based on
their source, for example. In this way one can produce
term recognizers that are tailored towards specific do-
mains or specific applications within domains.
4 Implementation & Performance
A first version of Termino has been implemented. It uses
a database implemented in MySQL and currently con-
tains over 427,000 termoids for around 363,000 strings.
Content has been imported from various sources by
means of source-specific scripts for extracting relevant
information from sources and a general script for load-
ing this extracted information into Termino. More specif-
ically, to support information extraction from patient
records, we have included in Termino strings from the
UMLS Metathesaurus falling under the following seman-
tic types: pharmacologic substances, anatomical struc-
tures, therapeutic procedure, diagnostic procedure, and
several others. We have also loaded a list of hu-
man proteins and their assignments to the Gene Ontol-
ogy as produced by the European Bioinformatics Insti-
tute (http://www.ebi.ac.uk/GOA/) into Termino. Further-
more, we have included several gazetteer lists containing
terms in the fields of molecular biology and pharmacol-
ogy that were assembled for previous information extrac-
tion projects in our NLP group. A web services (SOAP)
API to the database is under development. We plan to
make the resource available to researchers as a web ser-
vice or in downloadable form.3
The compiler to construct finite state recognizers from
the database is fully implemented, tested, and integrated
into AMBIT. The compiled recognizer for the 363,000
strings of Termino has 1.2 million states and an on-disk
size of around 80MB. Loading the matcher from disk
into memory requires about 70 seconds (on an UltraSparc
900MHz), but once loaded recognition is a very fast pro-
cess. We have been able to annotate a corpus of 114,200
documents, drawn from electronic patient records from
the Royal Marsden NHS Trust in London and each ap-
proximately 1kB of text, in approximately 44 hours ? an
average rate of 1.4 seconds per document, or 42 docu-
ments per minute. On average, about 30 terms falling un-
der the UMLS ?clinical? semantic types mentioned above
were recognized in each document. We are currently an-
notating a bench-mark corpus in order to obtain precision
and recall figures. We are also planning to compile rec-
ognizers for differently sized subsets of the terminologi-
cal database and measure their recognition speed over a
given collection of texts. This will provide some indica-
tion as to the scalability of the system.
Since Termino currently contains many terms imported
from the UMLS Metathesaurus, it is interesting to com-
pare its term recognition performance against the per-
formance of MetaMap. MetaMap is a program avail-
able from at the National Library of Medicine ? the de-
velopers of UMLS ? specifically designed to discover
UMLS Metathesaurus concepts referred to in text (Aron-
son, 2001). An impressionistic comparison of the per-
formance of Termino and MetaMap on the CLEF patient
records shows that the results differ in two ways. First,
MetaMap recognizes more terms than Termino. This
is simply because MetaMap draws on a comprehensive
version of UMLS, whereas Termino just contains a se-
lected subset of the strings in the Metathesaurus. Sec-
ondly, MetaMap is able to recognize variants of terms,
e.g. it will map the verb to treat and its inflectional forms
onto the term treatment, whereas Termino currently does
not do this. To recognize term variants MetaMap re-
lies on UMLS?s SPECIALIST lexicon, which provides
3Users may have to sign license agreements with third par-
ties in order to be able to use restricted resources that have been
integrated into Termino.
syntactic, morphological, and orthographic information
for many of the terms occurring in the Metathesaurus.
While the performance of both systems differs in favor
of MetaMap, it is important to note that the source of
these differences is unrelated to the actual design of Ter-
mino?s terminological database or Termino?s use of fi-
nite state machines to do term recognition. Rather, the
divergence in performance follows from a difference in
breadth of content of both systems at the moment. With
regard to practical matters, the comparison showed that
term recognition with Termino is much faster than with
MetaMap. Also, compiling a finite state recognizer from
the terminological database in Termino is a matter of min-
utes, whereas setting up MetaMap can take several hours.
However, since MetaMap?s processing is more involved
than Termino?s, e.g. MetaMap parses the input first, and
hence requires more resources, these remarks should be
backed up with a more rigorous comparison between Ter-
mino and MetaMap, which is currently underway.
The advantage of term recognition with Termino over
MetaMap and UMLS or any other recognizer with a sin-
gle source, is that it provides immediate entry points
into a variety of outside ontologies and other knowledge
sources, making the information in these sources avail-
able to processing steps subsequent to term recognition.
For example, for a gene or protein name recognized in a
text, Termino will return the database identifiers of this
term in the HUGO Nomenclature database (Wain et al,
2002) and the OMIM database (Online Mendelian Inher-
itance in Man, OMIM (TM), 2000). These identifiers
give access to the information stored in these databases
about the gene or protein, including alternative names,
gene map locus, related disorders, and references to rele-
vant papers.
5 Conclusions & Future Work
Dealing with terminology is an essential step in natural
language processing in technical domains. In this paper
we have described the design, implementation, and use of
Termino, a large scale terminology resource for biomedi-
cal language processing.
Termino includes a relational database which is de-
signed to store a large number of terms together with
complex, heterogeneous information about these terms,
such as morpho-syntactic information, links to concepts
in ontologies, and other kinds of annotations. The
database is also designed to be extensible: it is easy to
include terms and information about terms found in out-
side biological databases and ontologies. Term look-up
in text is done via finite state machines that are compiled
from the contents of the database. This approach allows
the database to be very rich without sacrificing speed at
look-up time. These three features make Termino a flexi-
ble tool for inclusion in a biomedical text processing sys-
tem.
As noted in section 3.2, Termino has not been designed
to be used as a stand-alone term recognition system but
rather as the first component, the lexical look-up com-
ponent, in a multi-component term processing system.
Since Termino may return multiple terms for a given
string, or for overlapping strings, some post-filtering of
these alternatives is necessary. Secondly, it is likely that
better term recognition performance will be obtained by
supplementing Termino look-up with a term parser which
uses a grammar to give a term recognizer the generative
capacity to recognize previously unseen terms. For ex-
ample, many terms for chemical compounds conform to
grammars that allow complex terms to be built out of sim-
pler terms prefixed or suffixed with numerals separated
from the simpler term with hyphens. It does not make
sense to attempt to store in Termino all of these variants.
Termino provides a firm basis on which to build large-
scale biomedical text processing applications. However,
there are a number of directions where further work can
be done. First, as noted in 3.2, morphological informa-
tion is currently not held in Termino, but rather resides
in an external morphological analyzer. We are working
to extend the Termino data model to enable information
about morphological variation to be stored in Termino,
so that Termino serves as a single source of information
for the terms it contains. Secondly, we are working to
build term induction modules to allow Termino content
to be automatically acquired from corpora, in addition
to deriving it from manually created resources such as
UMLS. Finally, while we have already incorporated Ter-
mino into the AMBIT system where it collaborates with
a term parser to perform more complete term recogni-
tion, more work can be done to with respect to such an
integration. For example, probabilities could be incorpo-
rated into Termino to assist with probabilistic parsing of
terms; or, issues of trade-off between what should be in
the term lexicon versus the term grammar could be fur-
ther explored by looking to see which compound terms
in the lexicon contain other terms as substrings and at-
tempt to abstract away from these to grammar rules. For
example, in the example thyroid disfunction above, both
thyroid and disfunction are terms, the first of class ?body
part?, the second of class ?problem?. Their combination
thyroid disfunction is a term of class ?problem?, suggest-
ing a rule of the form ?problem?   ?body part? ?problem?.
References
A.R. Aronson. 2001. Effective mapping of biomedical
text to the UMLS Metathesaurus: the MetaMap pro-
gram. In Proceedings of the American Medical Infor-
matics Association Symposium, pages 17?21.
R. Gaizauskas and Y. Wilks. 1998. Information extrac-
tion: Beyond document retrieval. Journal of Docu-
mentation, 54(1):70?105.
R. Gaizauskas, G. Demetriou, P. Artymiuk, and P. Wil-
lett. 2003. Protein structures and information extrac-
tion from biological texts: The PASTA system. Jour-
nal of Bioinformatics, 19(1):135?143.
C.A. Goble, C.J. Wroe, R. Stevens, and the my-
Grid consortium. 2003. The myGrid project:
Services, architecture and demonstrator. In
S. Cox, editor, Proceedings of UK e-Science
All Hands Meeting 2003, Nottingham, UK.
http://www.nesc.ac.uk/events/ahm2003/AHMCD/.
R. Grishman. 1997. Information extraction: Techniques
and challenges. In Maria Teresa Pazienza, editor, In-
formation Extraction, pages 10?27. Springer Verlag.
U. Hahn, M. Romacker, and S. Schulz. 2002. Creating
knowledge repositories from biomedical reports: the
medSynDiKATe text mining system. In Proceedings
of the Pacific Symposium on Biocomputing, pages 338?
349.
L. Humphreys, D.A.B. Lindberg, H.M. Schoolman, and
G.O. Barnett. 1998. The Unified Medical Language
System: An informatics research collaboration. Jour-
nal of the American Medical Informatics Association,
1(5):1?13.
K. Humphreys, G. Demetriou, and R. Gaizauskas. 2000.
Two applications of information extraction to biolog-
ical science journal articles: Enzyme interactions and
protein structures. In Proceedings of the Pacific Sym-
posium on Biocomputing, pages 505?516.
A.G. Murzin, S.E. Brenner, T. Hubbard, and C. Chothia.
1995. SCOP: A structural classification of proteins
database for the investigation of sequences and struc-
tures. Journal of Molecular Biology, (247):536?540.
(http://scop.mrc-lmb.cam.ac.uk/scop/).
Online Mendelian Inheritance in Man, OMIM (TM).
2000. McKusick-Nathans Institute for Genetic
Medicine, Johns Hopkins University (Baltimore, MD)
and National Center for Biotechnology Informa-
tion, National Library of Medicine (Bethesda, MD).
http://www.ncbi.nlm.nih.gov/omim/.
J. Pustejovsky, J. Castan?o, R. Saur??, A. Rumshisky,
J. Zhang, and W. Luo. 2002. Medstract: Creat-
ing large-scale information servers for biomedical li-
braries. In Proceedings of the Workshop on Natural
Language Processing in the Biomedical Domain, As-
sociation for Computational Linguistics 40th Anniver-
sary Meeting (ACL-02), pages 85?92.
A. Rector, J. Rogers, A. Taweel, D. Ingram, D. Kalra,
J. Milan, R. Gaizauskas, M. Hepple, D. Scott,
and R. Power. 2003. Joining up health care
with clinical and post-genomic research. In
S. Cox, editor, Proceedings of UK e-Science
All Hands Meeting 2003, Nottingham, UK.
http://www.nesc.ac.uk/events/ahm2003/AHMCD/.
C.T. Rindflesch, J.V. Rajan, and L. Hunter. 2000. Ex-
tracting molecular binding relationships from biomed-
ical text. In Proceedings of the 6th Applied Natu-
ral Language Processing conference / North American
chapter of the Association for Computational Linguis-
tics annual meeting, pages 188?915.
The Gene Ontology Consortium. 2001. Creating the
gene ontology resource: design and implementation.
Genome Research, 11(8):1425?1433.
J. Thomas, D. Milward, C. Ouzounis, and S. Pulman.
2000. Automatic extraction of protein interactions
from scientific abstracts. In Proceedings of the Pacific
Symposium on Biocomputing, pages 538?549.
H.M. Wain, M. Lush, F. Ducluzeau, and S. Povey.
2002. Genew: The human nomenclature
database. Nucleic Acids Research, 30(1):169?171.
(http://www.gene.ucl.ac.uk/nomenclature/).
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 57?64,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
A hybrid approach to align sentences and words  
in English-Hindi parallel corpora 
 
Niraj Aswani Robert Gaizauskas 
Department of Computer Science Department of Computer Science 
University of Sheffield University of Sheffield 
Regent Court 211, Portobello Street Regent Court 211, Portobello Street 
Sheffield S1 4DP, UK Sheffield S1 4DP, UK 
N.Aswani@dcs.shef.ac.uk R.Gaizauskas@dcs.shef.ac.uk 
 
 
 
Abstract 
 
In this paper we describe an alignment 
system that aligns English-Hindi texts 
at the sentence and word level in 
parallel corpora. We describe a simple 
sentence length approach to sentence 
alignment and a hybrid, multi-feature 
approach to perform word alignment. 
We use regression techniques in order 
to learn parameters which characterise 
the relationship between the lengths of 
two sentences in parallel text.  We use 
a multi-feature approach with 
dictionary lookup as a primary 
technique and other methods such as 
local word grouping, transliteration 
similarity (edit-distance) and a nearest 
aligned neighbours approach to deal 
with many-to-many word alignment.  
Our experiments are based on the 
EMILLE (Enabling Minority Language 
Engineering) corpus.  We obtained 
99.09% accuracy for many-to-many 
sentence alignment and 77% precision 
and 67.79% recall for many-to-many 
word alignment. 
 
1 Introduction 
 
Text alignment is not only used for the tasks such 
as bilingual lexicography or machine translation 
but also in other language processing applications 
such as multilingual information retrieval and word 
sense disambiguation.  Whilst resources like 
bilingual dictionaries and parallel grammars help 
to improve Machine Translation (MT) quality, text 
alignment, by aligning two texts at various levels 
(i.e. documents, sections, paragraphs, sentences 
and words), helps in the creation of such lexical 
resources (Manning & Sch?tze, 2003).   
 
In this paper, we describe a system that aligns 
English-Hindi texts at the sentence and word level.  
Our system is motivated by the desire to develop 
for the research community an alignment system 
for the English and Hindi languages.  Building on 
this, alignment results can be used in the creation 
of other Hindi language processing resources (e.g. 
part-of-speech taggers).  We present a simple 
sentence length approach to align English-Hindi 
sentences and a hybrid approach with local word 
grouping and dictionary lookup as the primary 
techniques to align words.   
 
2 Sentence Alignment 
 
Sentence alignment techniques vary from simple 
character-length or word-length techniques to more 
sophisticated techniques which involve lexical 
constraints and correlations or even cognates (Wu 
2000). Examples of such alignment techniques are 
Brown et al (1991), Kay and Roscheisen (1993), 
Warwick et al (1989), and the ?align? programme 
by Gale and Church (1993).   
 
2.1 Length-based methods 
 
Length-based approaches are computationally 
better, while lexical methods are more resource 
57
hungry. Brown et al (1991) and Gale and Church 
(1993) are amongst the most cited works in text 
alignment work.  Purely length-based techniques 
have no concern with word identity or meaning 
and as such are considered knowledge-poor 
approaches.  The method used by Brown et al 
(1991) measures sentence length in number of 
words.  Their approach is based on matching 
sentences with the nearest length. Gale and Church 
(1993) used a similar algorithm, but measured 
sentence length in number of characters.  Their 
method performed well on the Union Bank of 
Switzerland (UBS) corpus giving a 2% error rate 
for 1:1 alignment.   
 
2.2 Lexical methods 
 
Moving towards knowledge-rich methods, lexical 
information can be vital in cases where a string 
with the same length appears in two languages. 
Kay and Roscheisen (1993) tried lexical methods 
for sentence alignment.  In their algorithm, they 
consider the most reliable pair of source and target 
sentences, i.e. those that contain many possible 
lexical correspondences. They achieved 96% 
coverage on Scientific American articles after four 
passes of the algorithm. Other examples of lexical 
methods are Warwick et al (1989), Mayers et al 
(1998), Chen (1993) and Haruno and Yamazaki 
(1996).   
 
Warwick et al (1989) calculate the probability of 
word pairings on the basis of frequency of source 
word and the number of possible translations 
appearing in target segments.  They suggest using 
a bilingual dictionary to build word-pairs. Mayers 
et al (1998) propose a method that is based on a 
machine readable dictionary.  Since bilingual 
dictionaries contain base forms, they pre-process 
the text to find the base form for each word. They 
tried this method in an English-Japanese alignment 
system and got accuracy of about 89.5% for 1-to-1 
and 42.9% for 2-to-1 sentence alignments. Chen 
(1993) constructs a simple word-to-word 
translation model and then takes the alignment that 
maximizes the likelihood of generating the corpus 
given the translation model. Haruno and Yamazaki 
(1996) use a POS tagger for source and target 
languages and use an online dictionary to find 
matching word pairs.  Haruno and Yamazaki 
(1996) pointed out that though dictionaries cannot 
capture context dependent keywords in the corpus, 
they can be very useful to obtain information about 
words that appear only once in the corpus.  Lexical 
methods for sentence alignment may also result in 
partial word alignment.  Given that lexical methods 
can be computationally expensive, our idea was to 
try a simple length-based approach similar to that 
of Brown et al (1991) for sentence alignment and 
then use lexical methods to align words within 
aligned sentences. 
 
2.3 Algorithm 
 
We use English-Hindi parallel data from the 
EMILLE corpus for our experiments.  EMILLE is 
a 63 Million word electronic corpus of South Asian 
languages, especially those spoken as minority 
languages in UK.  It has around 120,000 words of 
parallel data in each of English, Hindi, Urdu, 
Punjabi, Bengali, Gujarati, Sinhala and Tamil 
(Baker et al, 2004).   
 
 
Figure 2.1 Sentence Alignment Parameter  
Learning algorithm 
58
Table 2.1 Rules for the Sentence Alignment Algorithm 
Rule If Hindi:English Alignment 
H1 |hi| - (|ej| + |ej+1|) < 0.17 * |hi| 1-To-2 
H2 |hi| - ( |ej| + |ej+1| + |ej+2| ) < 0.17 * |hi| 1-To-3 
E1 |ej| - ( |hi| + |hi+1| ) < 0.17 * |ej | 2-To-1 
E2 |ej| - ( |hi| + |hi+1| + |hi+2| ) < 0.14 * |ej| 3-To-1 
Default ( |ej| = |hi| )  ||  (Rule H1 and E1 Fails) 1-To-1 
Examining the data, we observe that it is possible 
to align one English sentence with one or more 
Hindi sentences or vice-versa.  In the method 
described below, sentence length is calculated in 
number of words. We define our task as that of 
learning rules that characterise the relationship 
between the lengths of two sentences in parallel 
texts.  We used 60 manually aligned paragraphs 
from the EMILLE corpus, each with an average of 
3 sentences, as a dataset for our learning task.  
Initially we derived minimum and maximum 
length differences in percentages for each of the 
one-to-one, one-to-two and one-to-three parallel 
sentence pairs. Later we used these values as input 
to our algorithm to learn new rules that maximize 
the probability of aligning sentences.   
 
Learning: Let T = [1:1, 1:2, 1:3, 2:1, 3:1], a set of 
possible alignment types between the English and 
Hindi sentences.  For each alignment type t ? T, 
minimum and maximum length differences in 
number of words, normalized to percentages, can 
be described as mint and maxt.   For each alignment 
type t ? T, a constant parameter ?t, where ?t ? 
[mint , mint + 0.01, mint + 0.02, ?, maxt ] was 
learned using an algorithm described in figure 2.1.    
?t is a value that describes the length relationship 
between the sentences of a pair of type t. For 
example, given a pair of one Hindi and two 
English sentences and a value ?t, where t = 1:2, it 
is possible to check if these sentences can be 
aligned with each other.  Suppose for a given pair 
of parallel sentences that consist of hi (Hindi 
sentence at ith position) and ej and ej+1 (English 
sentences at jth and j+1th positions), let |hi|, |ej| and 
|ej+1| be the lengths of Hindi and English sentences. 
hi, ej and ej+1 are said to have 1:2 alignment if |hi| - 
(|ej| + |ej+1|) < 0.17 * |hi|, i.e. the difference 
between the length of the Hindi sentence and the 
length of the two consecutive English sentences is 
less than (?t=1:2 = 0.17) times the length of the 
Hindi sentence.  Table 2.1 lists rules for different 
possible alignments. Before we decide on the final 
alignment, we check each possibility of one Hindi 
sentence being aligned with one, two or three 
consecutive English sentences and vice-versa.  We 
use rules H1 and H2 to check the possibility of one 
Hindi sentence being aligned with two or three 
consecutive English sentences.  Similarly, rules E1 
and E2 are used to check the possibility of one 
English sentence being aligned with two or three 
consecutive Hindi sentences. If none of the rules 
from H1, H2, E1 and E2 return true, we consider 
the default alignment (1-To-1) between the English 
and Hindi sentences.  We give preference to the 
higher alignment over the possible lower 
alignments, i.e. given 1-To-2 and 1-To-3 possible 
alignment mappings, we consider 1-To-3 mapping.  
We tested our algorithm on parallel texts with total 
of 3441 English-Hindi sentence pairs and obtained 
an accuracy of 99.09%; i.e., the correctly aligned 
pairs were 3410. 
 
3 Word Alignment 
 
Extending sentence alignment to word alignment is 
a process of locating corresponding word pairs in 
two languages.  In some cases, a word is not 
translated, or is translated by several words.  A 
word can also be a part of an expression that is 
translated as a whole, and therefore the entire 
expression must be translated as a whole (Manning 
& Sch?tze, 2003).  We present a hybrid method for 
many-to-many word alignment.  Hindi is a partial 
free order language where the order of word 
groups in a Hindi sentence is not fixed, but the 
order of words within groups is fixed (Ray et al, 
2003).  According to Ray et al (2003), fixed order 
word group extraction is essential for decreasing 
the load on the free word order parser.  The word 
alignment algorithm takes as input a pair of aligned 
sentences and groups words in sentences of both 
languages.  We have observed a few facts about 
the Hindi language. For example, there are no 
59
articles in Hindi (Bal Anand, 2001).  Since there 
are no articles in Hindi, articles are aligned to null.   
 
3.1 Local word grouping 
 
A separate group is created for each token in the 
English text.  Every English word has one property 
associated with it: the lemma of the word. This is 
necessary because a dictionary lookup approach is 
at the heart of our word alignment algorithm.  
Verbs are used in different inflected forms in 
different sentences.  For a verb, it is common not 
to find all inflected forms listed in a dictionary, i.e. 
most dictionaries contain verbs only in their base 
forms. Therefore we use a morphological analyzer 
to find the lemma of each English word.   
 
Word groups in Hindi are created using two 
resources: a Hindi gazetteer list that contains a 
large set of named entities (NE) and a rule file that 
contains more than 250 rules. The gazetteer list is 
available as a part of Hindi Gazetteer Processing 
Resource in GATE (Maynard et al, 2003).  For 
each rule in the rule file, it contains the following 
information:  
1. Hindi Regular Expression (RE) for a word 
or phrase.  This must match one or more 
words in the Hindi sentence. 
2. Group name or a part-of-speech category. 
3. Expected English word(s) (EEW) that this 
Hindi word group may align to. 
4. Expected Number of English words (NW) 
that the Hindi group may align to. 
5. In case a group of one or more English 
words aligns with a group of one or more 
Hindi words, information about the key 
words (KW) in both groups.  Key words 
must match each other in order to align 
English-Hindi groups. 
6. A rule to convert the Hindi word into its 
base form (BF). 
Rules in the rule file identify verbs, postpositions, 
noun phrases and also a set of words, whose 
translation is expected to occur in the same order 
as the English words in the English sentence.  The 
local word grouping algorithm considers one rule 
at a time and tries to match the regular expression 
in the Hindi sentence.  If the expression is 
matched, a separate group for each found pattern is 
created.  When a Hindi group is created, based on 
its pattern type, one of the following categories is 
assigned to that group: 
 
proper-noun city job-title location 
country number day-unit date-unit 
month-unit verb auxiliary pronoun 
post-position other   
 
These rules have been obtained mainly through 
consulting Hindi grammar material (Bal Anand, 
2001 and Ta, 2002) and by observing the EMILLE 
corpus. For example, consider the following rules:  
 
No RE Cat EEW NW KW BF 
1 ???? num fifty two 2   
2 (.)+ ??? verb   1  
3 (.)+ ? ??? verb   1 1,?? = ?? 
4 (.)+ ?? ???? prep for (.)+ 2 1-2  
5 ??? ??? other different 1   
i) ???? ?, ?????, ????? are used to indicate the progressive tense.  They 
can be seen as analogous to the English (-ing) ending. 
ii) ????, ????, and ???? are used as verb endings to indicate the habitual 
tense.  They must agree with subject number and gender. 
iii) ???? is a past tense conjunction of the verb ??????. 
 
In the first rule, if we find a word ?????? (bavan) 
in Hindi, we mark it as a ?Number? and search for 
the English string with two words that is equal to 
the expected string ?fifty two?.  In the second rule, 
we locate a string where the second word is ????? 
(raha). ?1? in the fifth column specifies that the first 
word is the keyword. We use the dictionary to 
locate the word in the English sentence that 
matches with the key word. If the English word is 
located, we align ?(.)+ ???? with the English word 
found. In the third rule, if we find a Hindi string 
with two words where the first word ends with ???? 
(te) and the second word is ???? (the), we group them 
as a verb.  As specified in the sixth column, we 
replace the characters ???? with ???? (na) to convert 
the first word into its base form (e.g. ?????? (gaate) 
into ?????? (gaana)). In the fourth rule, we align ?X 
?? ????? with ?For X?, where ?For? = ??? ?????. As 
specified in the fifth column, we align the first 
word in Hindi with the second word in English. In 
the final example, we group two words that are 
identical to each other.  For example: "??? ???" 
(alag alag) which means ?different? in English.  
Such bigrams are used to stress the importance of a 
word/activity in a sentence. 
60
 Figure 3.1 Dictionary Lookup Approach 
 
example, in rule 3 and 4 if the word ends with 
either of ??, ? ?or ?? followed by (PH), it is assumed 
that the word is a verb.  The formula for finding 
the lemma of any Hindi verb is: infinitive = root 
verb + ????.  Sometimes it is possible to predict 
the corresponding English translation. For 
example, for the postposition ??? ??????, one is 
likely to find the preposition ?in front of? in the 
English sentence.  We store this information as an 
expected English word(s) in Hindi Word Groups 
(HWGs) and search for it in the English sentence.  
In the case of rules 4 and 5, though the HWG 
contains more than one word, only one is the actual 
verb (key word) that is expected to be available in 
a dictionary.  We specify the index of this key 
word in the HWG, so as to consider only the word 
at the specified index to compare with key word in 
English word group.  If they match, the full HWG 
is aligned to the word in English sentence.   
 
3.2 Alignment Algorithm 
 
After applying the local word grouping rules to the 
Hindi sentence(s), based on their categories of 
HWGs, we use four methods to process and align 
HWGs with their respective English Word Groups. 
   
1. Dictionary lookup approach (DL) 
2. Transliteration similarity approach (TS) 
3. Expected English words approach (EEW) 
4. Nearest aligned neighbour approach 
 
Whilst the verbs and other groups are processed 
with DL approach, HWGs with categories such as 
proper nouns, city, job-title, location, and country 
are processed with TS approach. HWGs such as 
number, day-unit, date-unit, month-unit, auxiliary, 
pronoun and postpositions, where the expected 
English words are specified, are processed with 
EEW approach.  Sometimes the combination of 
DL and TS is also used to identify the proper 
alignment.  At the end, nearest aligned neighbour 
approach is used to align the unaligned HWGs. 
 
Dictionary Lookup 
 
The corpus we used in our experiments is encoded 
in Unicode and therefore the word matching 
process requires dictionary entries to be in Unicode 
encoding. The only English-Hindi dictionary we 
found is called, ?shabdakoSha? and is freely 
available from (WWW2).  In this dictionary, the 
ITRANS transliteration system is followed, i.e. 
Hindi entries are not written in the Devanagari 
script, but in the Roman script. This dictionary has 
around 15,000 English words, each with an 
average of 4 relevant Hindi words. Following  
61
 Figure 3.2 Nearest Aligned Neighbours Approach 
 
ITRANS conventions, a parser was developed to 
convert all these entries into Unicode.  Given a set 
of English and Hindi words, the algorithm 
presented in figure 3.1 is executed to search for the 
best translation among the English words.  
 
Transliteration Similarity 
 
A transliteration system maintains a consistent 
correspondence between the alphabets of two 
languages, irrespective of sound (Manning & 
Sch?tze, 2003).  Given two words, each from a 
different language, we define ?transliteration 
similarity? as the measure of likeness between 
them.  This could exist due to the word in one 
language being inherited or adopted by the other 
language, or because the word is a proper noun. 
Named entities such as city, job-title, location, 
country and proper nouns, all recognized by the 
local word grouping algorithm are compared using 
a transliteration similarity approach. This likeness 
is counted using a table that lists letter 
correspondences between the alphabets of two 
languages.  For the English and Hindi languages, it 
is possible to come up with a table that defines 
letter correspondence between the alphabets of two 
languages.  For example,  
 
A  ?, B  ?, Bh  ?, Ch  ?,  
D  ?,  Dh  ? and so on? 
 
A bidirectional mapping is established between 
each character in the English and Hindi alphabets.  
When DL is not able to find any specific English 
word in dictionary, this approach is used to find the 
transliteration similarity between the unaligned 
words. Sometimes because the words in a Hindi 
sentence are not spelled correctly, when DL issues 
a query to dictionary, none of the Hindi words 
appearing in a Hindi sentence match with the 
words returned from dictionary.  We use a 
dynamic programming algorithm ?edit-distance? to 
calculate similarity between these words 
(WWW3).  According to WWW3, ?The edit 
distance of two strings, s1 and s2, is defined as the 
minimum number of point mutations required to 
change s1 into s2, where a point mutation is one 
of:  change a letter, insert a letter or delete a 
letter.? The lower the distance, the greater the 
similarity. From our experiments of 100 proper 
noun pairs, we found that if the similarity is greater 
than 75%, the words can be reliably aligned with 
each other.  We consider a pair with the highest 
similarity.  E.g.: Aswani  ???????.  Here we 
remove vowels in both strings, except those that 
appear at the start of words.  After the removal of 
vowels from the English and Hindi texts, the 
resulting text would be: Aswn  ????.  The 
Hindi text is then converted into English text using 
the transliteration table:  Aswn  Aswn. The two 
texts are then compared using an ?edit-distance? 
algorithm.  
 
Expected English word(s) 
 
For HWGs which are categorised as numbers, job-
titles or postpositions, it is possible to specify the 
expected English word or words that can be found 
in the parallel English text. The algorithm retrieves 
expected English word(s) from the HWGs and tries 
to locate them in the English sentence. This 
approach can be useful to locate one or more 
English words that align with one or more Hindi 
words.  For example, the number ???????? whose 
equivalent translation in English is ?forty two? has 
two words in English, and the postposition ??? 
??????, whose equivalent translation in English is 
?in front of?, has three words in English.  These 
are examples of many-to-many word alignment. 
62
Nearest Aligned Neighbours 
 
At the end of the first three stages of the word 
alignment process, many words remain unaligned.  
Here we introduce a new approach, called the 
?Nearest Aligned Neighbours approach?.  In 
certain cases, words in English-Hindi phrases 
follow a similar order.  The Nearest Aligned 
Neighbours approach works on this principle and 
aligns one or more words with one of the English 
words. A local word grouping algorithm, explained 
in section 3.1, groups such phrases and tags them 
as ?group?. Considering one HWG at a time, we 
find the nearest Hindi word that is already aligned 
with one or more English word(s).  We assume that 
the words in English-Hindi phrases follow a 
similar order and align the rest words in that group 
accordingly.  An example of alignment using the 
Nearest Aligned Neighbours approach is given in 
Figure 3.2. Word H4 is already aligned with E5, 
and H3, H5, H6 and H7 are yet to be aligned.  The 
local word grouping algorithm has tagged a 
sequence of H4, H5, H6 and H7 as a single group.  
At the same time, H6 and H7 are also grouped as a 
single group.  The algorithm searches for the 
aligned Hindi word, which, in this case, is H4 and 
aligns H5 with E6 and the group of H6 and H7 
with E7.   
 
4 Results 
 
 
Figure 4.1 Word Alignment Results 
 
We performed manual evaluation of our word 
alignment algorithm on a set of parallel data 
aligned at the sentence level.  The parallel texts 
consist of 3954 English and 5361 Hindi words 
taken from the EMILLE Corpus.  We calculate our 
results in terms of the number of aligned English 
word groups. The precision is calculated as the 
ratio of the number of correctly aligned English 
word groups to the total number of English word 
groups aligned by the system, and recall is 
calculated as the ratio of the number of correctly 
aligned English word groups to the total number of 
English word groups created by the system. We 
obtained 77% precision and 67.79% recall for 
many-to-many word alignment.  Figure 4.1 shows 
an example of the word alignment results.  
 
5 Future works 
 
It would be useful to evaluate separate stages (i.e. 
DL, TS, EEW and Nearest Aligned Neighbours 
approach) in the word alignment algorithm 
separately.  We aim to do this as part of a failure 
analysis of the algorithm in future. We also aim to 
improve our alignment results by using Part-of-
Speech information for the English texts. We aim 
to implement or use local word grouping rules for 
the English text and improve our existing word 
grouping rules for the Hindi texts.  The Nearest 
Aligned Neighbours approach suggests possible 
alignments, but we are trying to integrate some 
statistical ranking algorithms in order to suggest 
more reliable pairs of alignment. Yarowsky et al 
(2001) introduced a new method for developing a 
Part-of-Speech tagger by projecting tags across 
aligned corpora.  They used this technique to 
supply data for a supervised learning technique to 
acquire a French part-of-speech tagger. We aim to 
use our English-Hindi word alignment results to 
bootstrap a Part-of-Speech tagger for the Hindi 
language.   
 
References 
 
Bal Anand, 2001, Hindi Grammar Books for 
standard 5 to standard 10, Navneet Press, India.   
 
Baker P., Bontcheva K., Cunningham H., 
Gaizauskas R., Hamza O., Hardie A., Jayaram 
B.D., Leisher M., McEnery A.M., Maynard D., 
Tablan V., Ursu C., Xiao Z., 2004, Corpus 
linguistics and South Asian languages: Corpus 
creation and tool development, Literary and 
Linguistic Computing, 19(4), pp. 509-524. 
 
63
Brown, P., Lai, J. C., and Mercer, R., 1991, 
Aligning Sentences in Parallel Corpora,  
In Proceedings of ACL-91, Berkeley CA. 
 
Chen S., 1993, Aligning sentences in bilingual 
corpora using lexical information, Proceedings 
of the 31st conference on Association for   
Computational Linguistics, pp. 9 ? 16, 
Columbus, Ohio. 
 
Gale W., and Church K., 1993, A program for 
aligning sentences in bilingual corpora, 
Proceedings of the 29th conference of the 
Association for Computational Linguistics, 
pp.177-184, June 18-21, 1991, Berkeley, 
California. 
 
Haruno M. and Yamazaki T., 1996, High-
performance bilingual text alignment using 
statistical and dictionary information, 
Proceedings of the 34th conference of the 
Association for Computational Linguistics, pp. 
131 ? 138, Santa Cruz, California. 
 
Kay M. and Roscheisen M., 1993, Text translation 
alignment, Computational Linguistics, 19(1):75--
102. 
 
Manning C. and Sch?tze H., 2003, Foundations of 
Statistical Natural Language Processing, MIT 
Press, Cambridge, Massachusetts. 
 
Mark D., 2004, Technical Report on Unicode 
Standard Annex #29 - Text Boundaries, Version 
4.0.1, Unicode Inc.,  
 http://www.unicode.org/reports/tr29/ [22/11/04]. 
 
Mayers A., Grishman R., Kosaka M., 1998, A 
Multilingual Procedure for Dictionary-Based 
Sentence Alignment, Proceedings of the Third 
Conference of the Association for Machine 
Translation in the Americas on Machine 
Translation and the Information Soup.   
 
Maynard D., Tablan V., Bontcheva K., 
Cunningham H., 2003, Rapid customisation of 
an Information Extraction system for surprise 
languages, ACM Transactions on Asian 
Language Information Processing, Special issue 
on Rapid Development of Language 
Capabilities: The Surprise Languages. 
Ray, P, Harish V., Sarkar, S., and Basu, A., 2003, 
Part of Speech Tagging and Local Word 
Grouping Techniques for Natural Language 
Parsing in Hindi, Proceedings of the 1st 
International Conference on Natural Language 
Processing (ICON 2003); Mysore. 
 
Simard M. and Pierre P., 1996, Bilingual Sentence 
Alignment: Balancing Robustness and Accuracy, 
Proceedings of the First Conference of the 
Association for Machine Translation in the 
Americas (AMTA-96), pp. 135-144, Montreal, 
Quebec, Canada. 
 
Ta A., 2002, A Door into Hindi, NC State 
University, 
http://www.ncsu.edu/project/hindi_lessons/lesso
ns.html [22/11/04] 
 
Warwick S., Catizone, R., and Graham R., 1989, 
Deriving Translation Data from Bilingual Texts, 
in Proceedings of the First International Lexical 
Acquisition Workshop, Detroit.   
 
WU D., Jul 2000, Alignment, In Robert DALE, 
Hermann MOISL, and Harold SOMERS 
(editors), Handbook of Natural Language 
Processing. pp. 415-458. New York: Marcel 
Dekker. ISBN 0-8247-9000-6.  
 
WWW1, Devanagari Unicode Chart, the Unicode 
Standard, Version 4.0, Unicode 
Inc.,http://www.unicode.org/charts/PDF/U0900.
pdf [22/03/05]. 
 
WWW2, English-Hindi dictionary source, 
http://sanskrit.gde.to/hindi/dict/eng-hin_guj.itx 
[22/03/05]. 
 
WWW3, Dynamic Programming Algorithm (DPA) 
for Edit-Distance, 
http://www.csse.monash.edu.au/~lloyd/tildeAlg
DS/Dynamic/Edit/ [22/03/05] 
 
Yarowsky, D., G. Ngai and R. Wicentowski, 2001, 
Inducing Multilingual Text Analysis Tools via 
Robust Projection across Aligned Corpora, In 
Proceedings of HLT 2001, First International 
Conference on Human Language Technology 
Research. 
64
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 115?118,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Aligning words in English-Hindi parallel corpora 
 
Niraj Aswani Robert Gaizauskas 
Department of Computer Science Department of Computer Science 
University of Sheffield University of Sheffield 
Regent Court 211, Portobello Street Regent Court 211, Portobello Street 
Sheffield S1 4DP, UK Sheffield S1 4DP, UK 
N.Aswani@dcs.shef.ac.uk R.Gaizauskas@dcs.shef.ac.uk 
 
Abstract 
 
In this paper, we describe a word 
alignment algorithm for English-Hindi 
parallel data. The system was developed 
to participate in the shared task on word 
alignment for languages with scarce 
resources at the ACL 2005 workshop, on 
?Building and using parallel texts: data 
driven machine translation and beyond?.  
Our word alignment algorithm is based on 
a hybrid method which performs local 
word grouping on Hindi sentences and 
uses other methods such as dictionary 
lookup, transliteration similarity, expected 
English words and nearest aligned 
neighbours. We trained our system on the 
training data provided to obtain a list of 
named entities and cognates and to collect 
rules for local word grouping in Hindi 
sentences. The system scored 77.03% 
precision and 60.68% recall on the shared 
task unseen test data. 
 
1 Introduction 
 
This paper describes a word alignment system 
developed as a part of shared task on word 
alignment for languages with scarce resources at 
the ACL 2005 workshop on ?building and using 
parallel texts: data driven machine translation and 
beyond?.  Participants in the shared task were 
provided with common sets of training data, 
consisting of English-Inuktitut, Romanian-English, 
and English-Hindi parallel texts and the 
participating teams could choose to evaluate their 
system on one, two, or all three language pairs.  
Our system is for aligning English-Hindi parallel 
data at the word level.  The word-alignment 
algorithm described here is based on a hybrid ? 
multi-feature approach, which groups Hindi words 
locally within a Hindi sentence and uses dictionary 
lookup (DL) as the main method of aligning words 
along with other methods such as Transliteration 
Similarity (TS), Expected English Words (EEW) 
and Nearest Aligned Neighbors (NAN).  We used 
the training data supplied to derive rules for local 
word grouping in Hindi sentences and to find 
Named Entities (NE) and cognates using our TS 
approach.  In the following sections we briefly 
describe our approach. 
 
2 Training Data 
 
The training data set was composed of 
approximately 3441 English-Hindi parallel 
sentence pairs drawn from the EMILLE (Enabling 
Minority Language Engineering) corpus (Baker et 
al., 2004).  The data was pre-tokenized. For the 
English data, a token was a sequence of characters 
that matches any of the ?Dr.?, ?Mr.?, ?Hon.?, 
?Mrs.?, ?Ms.?, ?etc.?, ?i.e.?, ?e.g.?, ?[a-zA-Z0-
9]+?, words ending with apostrophe and all special 
characters except the currency symbols ? and $.  
Similarly for the Hindi, a token consisted of a 
sequence of characters with spaces on both ends 
and all special characters except the currency 
symbols ? and $.  
 
3 Word Alignment 
 
Given a pair of parallel sentences, the task of word 
alignment can be described as finding one-to-one, 
one-to-many, and many-to-many correspondences 
115
between the words of source and target sentences.  
It becomes more complicated when aligning 
phrases of one language with the corresponding 
words or phrases in the target language.  For some 
words, it is also possible not to find any translation 
in the target language. Such words are aligned to 
null.   
 
The algorithm presented in this paper, is a blend of 
various methods. We categorize words of a Hindi 
sentence into one of four different categories and 
use different techniques to deal with each of them. 
These categories include: 1) NEs and cognates 2) 
Hindi words for which it is possible to predict their 
corresponding English words 3) Hindi words that 
match certain pre-specified regular expression 
patterns specified in a rule file (explained in 
section 3.3.) and finally 4) words which do not fit 
in any of the above categories.  In the following 
sections we explain different methods to deal with 
words from each of these categories. 
 
3.1 Named Entities and Cognates 
 
According to WWW1, the Named Entity Task is 
the process of annotating expressions in the text 
that are ?unique identifiers? of entities (e.g. 
Organization, Person, Location etc.).  For example: 
?Mr. Niraj Aswani?, ?United Kingdom?, and 
?Microsoft? are examples of NEs. In most text 
processing systems, this task is achieved by using 
local pattern-matching techniques e.g. a word that 
is in upper initial orthography or a Title followed 
by the two adjacent words that are in upper initial 
or in all upper case.  We use a Hindi gazetteer list 
that contains a large set of NEs.  This gazetteer list 
is distributed as a part of Hindi Gazetteer 
processing resource in GATE (Maynard et al, 
2003).  The Gazetteer list contains various NEs 
including person names, locations, organizations 
etc.  It also contains other entities such as time 
units ? months, dates, and number expressions.   
Cognates can be defined as two words having a 
common etymology and thus are similar or 
identical.  In most cases they are pronounced in a 
similar way or with a minor change. For example 
?Bungalow? in English is derived from the word 
??????? in Hindi, which means a house in the 
Bengali style (WWW2).  We use our TS method to 
locate such words.  Section 3.2 describes the TS 
approach. 
 
3.2 Transliteration Similarity 
 
For the English-Hindi alphabets, it is possible to 
come up with a table consisting of 
correspondences between the letters of the two 
alphabets.  This table is generated based on the 
various sounds that each letter can produce. For 
example a letter ?c? can be mapped to two letters 
in Hindi, ??? and ???. This mapping is not 
restricted to one-to-one but also includes many-to-
many correspondences.  It is also possible to map a 
sequence of two or more characters to a single 
character or to a sequence two or more characters.  
For example ?tio? and ?sh? in English correspond 
to the character ??? in Hindi.   
 
Prior to executing our word alignment algorithm, 
we use the TS approach to build a table of NEs and 
cognates. We consider one pair of parallel 
sentences at a time and for each word in a Hindi 
sentence, we generate different English words 
using our TS table.  We found that before 
comparing words of two languages, it is more 
accurate to eliminate vowels from the words 
except those that appear at the start of words.  We 
use a dynamic programming algorithm called 
?edit-distance? to measure the similarity between 
these words (WWW3). We calculate the similarity 
measure for each word in a Hindi sentence by 
comparing it with each and every word of an 
English sentence.  We come up with an m x n 
matrix, where m and n refer to the number of 
words in Hindi and English respectively. This 
matrix contains a similarity measure for each word 
in a Hindi sentence corresponding to each word in 
a parallel English sentence.  From our experiments 
of comparing more than 100 NE and cognate pairs, 
we found that the word pairs should be considered 
valid matches only if the similarity is greater than 
75%.  Therefore, we consider only those pairs 
which have the highest similarity among the other 
pairs with similarity greater than 75%.  The 
following example shows how TS is used to 
compare a pair of English-Hindi words. For 
example consider a pair ?aswani  ???????? and 
the TS table entries as shown below:  
116
A?,  S?,  SS?,  V?,  W? and N?  
 
We remove vowels from both words: ?aswn  
?????, and then convert the Hindi word into 
possible English words. This gives four different 
combinations: ?asvn?, ?assvn?, ?aswn? and 
?asswn?. These words are then compared with the 
actual English word ?aswn?.  Since we are able to 
locate at least one word with similarity greater than 
75%, we consider ?aswani  ???????? as a NE. 
Once a list of NEs and cognates is ready, we 
switch to our next step: local word grouping, 
where all words in Hindi sentences, either those 
available in the gazetteer list or in the list derived 
using TS approach, are aligned using TS approach.  
 
3.3 Local Word Grouping 
 
Hindi is a partially free order language (i.e. the 
order of the words in a Hindi sentence is not fixed 
but the order of words in a group/phrase is fixed).  
Unlike English where the verbs are used in 
different inflected forms to indicate different 
tenses, Hindi uses one or two extra words after the 
verb to indicate the tense.  Therefore, if the English 
verb is not in its base form, it needs to be aligned 
with one or more words in a parallel Hindi 
sentence.  Sometimes a phrase is aligned with 
another phrase. For example ?customer benefits? 
aligns with ????? ?? ??????.  In this example the 
first word ?customer? aligns with the first word 
?????? and the second word ?benefits? aligns with 
the third word ???????. Considering ?customer 
satisfaction? and ????? ?? ?????? as phrases to be 
aligned with each other, ???? is the word that 
indicates the relation between the two words 
?????? and ???????, which means the ?benefits of 
customer? in English.  These words in a phrase 
need to be grouped together in order to align them 
correctly. In the case of certain prepositions, 
pronouns and auxiliaries, it is possible to predict 
the respective Hindi postpositions, pronouns and 
other words. We derived a set of more than 250 
rules to group such patterns by consulting the 
provided training data and other grammar 
resources such as Bal Anand (2001).  The rule file 
contains the following information for each rule: 
1) Hindi Regular Expression for a word or 
phrase.  This must match one or more words in 
the Hindi sentence. 
2) Group name or a part-of-speech category. 
3) Expected English word(s) that this Hindi word 
group may align to. 
4) In case a group of one or more English words 
aligns with a group of one or more Hindi 
words, information about the key words in 
both groups.  Key words must match each 
other in order to align English-Hindi groups. 
5) A rule to convert Hindi word into its base 
form. 
 
We list some of the derived rules below: 
1) Group a sequence of [X + Postposition], where 
X can be any category in the above list except 
postposition or verb. For example: ?For X? = 
?X ?? ?????, where ?For? = ??? ?????.  
2) Root Verb + (???, ??? or ??)? + (PH).  Present 
continuous tense.  We use ?PH? as an 
abbreviation to refer to the present/past tense 
conjunction of the verb ?????? - ?,? ??, ?,? ??, etc. 
3) Group two words that are identical to each 
other.  For example: "??? ???", which means 
?different? in English. Such bi-grams are 
common in Hindi and are used to stress the 
importance of a word/activity in a sentence. 
 
Once the words are grouped in a Hindi sentence, 
we identify those word groups which do not fit in 
any of the TS and EEW categories.  Such words 
are then aligned using the DL approach. 
 
3.3 Dictionary lookup 
 
Since the most dictionaries contain verbs in their 
base forms, we use a morphological analyzer to 
convert verbs in their base forms. The English-
Hindi dictionary is obtained from (WWW4).  The 
dictionary returns, on average, two to four Hindi 
words referring to a particular English word.  The 
formula for finding the lemma of any Hindi verb 
is: infinitive = root verb + ????.  Since in most 
cases, our dictionary contains Hindi verbs in their 
infinitive forms, prior to comparing the word with 
the unaligned words, we remove the word ???? 
from the end of it.  Due to minor spelling mistakes 
it is also possible that the word returned from 
dictionary does not match with any of the words in 
117
a Hindi sentence.  In this case, we use edit-distance 
algorithm to obtain similarity between the two 
words.  If the similarity is greater than 75%, we 
consider them similar.  We use EEW approach for 
the words which remain unaligned after the DL 
approach. 
 
3.4 Expected English words 
 
Candidates for the EEW approach are the Hindi 
word groups (HWG) that are created by our Hindi 
local word grouping algorithm (explained in 
section 3.3).  The HWGs such as postpositions, 
number expressions, month-units, day-units etc. 
are aligned using the EEW approach.  For 
example, for the Hind word ?????? in a Hindi 
sentence, which means ?fifty two? in English, the 
algorithm tries to locate ?fifty two? in its parallel 
English sentence and aligns them if found. For the 
remaining unaligned Hindi words we use the NAN 
approach.  
 
3.5 Nearest Aligned Neighbors 
 
In certain cases, words in English-Hindi phrases 
follow a similar order.  The NAN approach works 
on this principle and aligns one or more words 
with one of the English words. Considering one 
HWG at a time, we find the nearest Hindi word 
that is already aligned with one or more English 
word(s). Aligning a phrase ?customer benefits? 
with ????? ?? ?????? (example explained in section 
3.3) is an example of NAN approach.  Similarly 
consider a phrase ?tougher controls?, where for its 
equivalent Hindi phrase ????? ???????, the 
dictionary returns a correct pair ?controls  
???????, but fails to locate ?tougher  ?????. For 
aligning the word ?tougher?, NAN searches for the 
nearest aligned word, which, in this case, is 
?controls?. Since the word ?controls? is already 
aligned with the word ????????, the NAN method 
aligns the word ?tougher? with the nearest 
unaligned word ??????. 
 
4 Test Data results 
 
We executed our algorithm on the test data 
consisting of 90 English-Hindi sentence pairs. We 
obtained the following results for non-null 
alignment pairs.  
 
Word Alignment Evaluation 
Evaluation of SURE alignments 
Precision = 0.7703 
Recall    = 0.6068 
F-measure = 0.6788 
Evaluation of PROBABLE alignments 
Precision = 0.7703 
Recall    = 0.6068 
F-measure = 0.6788 
AER       = 0.3212 
 
References 
 
Bal Anand, 2001, Hindi Grammar Books for 
standard 5 to standard 10, Navneet Press, India. 
 
Baker P., Bontcheva K., Cunningham H., 
Gaizauskas R., Hamza O., Hardie A., Jayaram 
B.D., Leisher M., McEnery A.M., Maynard D., 
Tablan V., Ursu C., Xiao Z., 2004, Corpus 
linguistics and South Asian languages: Corpus 
creation and tool development, Literary and 
Linguistic Computing, 19(4), pp. 509-524. 
 
Maynard D., Tablan V., Bontcheva K., 
Cunningham H., 2003, Rapid customisation of 
an Information Extraction system for surprise 
languages, ACM Transactions on Asian 
Language Information Processing, Special issue 
on Rapid Development of Language 
Capabilities: The Surprise Languages. 
 
WWW1, Named Entity Task Definition,  
http://www.cs.nyu.edu/cs/faculty/grishman/NEta
sk20.book_2.html#HEADING1 [15/04/2005] 
 
WWW2, Britannica Online Encyclopaedia, 
http://www.britannica.com/eb/article?tocId=901
8081 [15/04/2005] 
 
WWW3, Dynamic Programming Algorithm (DPA) 
for Edit-Distance,  
http://www.csse.monash.edu.au/~lloyd/tildeAlg
DS/Dynamic/Edit/ [22/03/05] 
 
WWW4, English-Hindi dictionary source,  
http://sanskrit.gde.to/hindi/dict/eng-hin_guj.itx 
[22/03/05]. 
118
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 200?201,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
SUPPLE: A Practical Parser for Natural Language Engineering
Applications
Robert Gaizauskas, Mark Hepple, Horacio Saggion,
Mark A. Greenwood and Kevin Humphreys?
Department of Computer Science
University of Sheffield, Sheffield, UK
{robertg|hepple|saggion|m.greenwood|-}@dcs.shef.ac.uk
Abstract
We describe SUPPLE, a freely-available,
open source natural language parsing sys-
tem, implemented in Prolog, and designed
for practical use in language engineering
(LE) applications. SUPPLE can be run as
a stand-alone application, or as a compo-
nent within the GATE General Architec-
ture for Text Engineering. SUPPLE is dis-
tributed with an example grammar that has
been developed over a number of years
across several LE projects. This paper de-
scribes the key characteristics of the parser
and the distributed grammar.
1 Introduction
In this paper we describe SUPPLE1 ? the Sheffield
University Prolog Parser for Language Engineering
? a general purpose parser that produces both syn-
tactic and semantic representations for input sen-
tences, which is well-suited for a range of LE ap-
plications. SUPPLE is freely available, and is dis-
tributed with an example grammar for English that
was developed across a number of LE projects. We
will describe key characteristics of the parser and the
grammar in turn.
2 The SUPPLE Parser
SUPPLE is a general purpose bottom-up chart parser
for feature-based context free phrase structure gram-
?At Microsoft Corporation since 2000 (Speech and Natural
Language Group). Email: kevinhum@microsoft.com.
1In previous published materials and in the current GATE
release the parser is referred to as buChart. This is name is now
deprecated.
mars (CF-PSGs), written in Prolog, that has a num-
ber of characteristics making it well-suited for use
in LE applications. It is available both as a language
processing resource within the GATE General Ar-
chitecture for Text Engineering (Cunningham et al,
2002) and as a standalone program requiring vari-
ous preprocessing steps to be applied to the input.
We will here list some of its key characteristics.
Firstly, the parser allows multiword units identi-
fied by earlier processing components, e.g. named
entity recognisers (NERs), gazetteers, etc, to be
treated as non-decomposable units for syntactic pro-
cessing. This is important as the identification of
such items is an essential part of analyzing real text
in many domains.
The parser allows a layered parsing process, with
a number of separate grammars being applied in se-
ries, one on top of the other, with a ?best parse? se-
lection process between stages so that only a sub-
set of the constituents constructed at each stage is
passed forward to the next. While this may make
the parsing process incomplete with respect to the
total set of analyses licensed by the grammar rules,
it makes the parsing process much more efficient and
allows a modular development of sub-grammars.
Facilities are provided to simplify handling
feature-based grammars. The grammar representa-
tion uses flat, i.e. non-embedded, feature represen-
tations which are combined used Prolog term uni-
fication for efficiency. Features are predefined and
source grammars compiled into a full form repre-
sentation, allowing grammar writers to include only
relevant features in any rule, and to ignore feature or-
dering. The formalism also permits disjunctive and
optional right-hand-side constituents.
The chart parsing algorithm is simple but very
200
efficient, exploiting the characteristics of Prolog to
avoid the need for active edges or an agenda. In in-
formal testing, this approach was roughly ten times
faster than a related Prolog implementation of stan-
dard bottom-up active chart parsing.
The parser does not fail if full sentential parses
cannot be found, but instead outputs partial anal-
yses as syntactic and semantic fragments for user-
selectable syntactic categories. This makes the
parser robust in applications which deal with large
volumes of real text.
3 The Sample Grammar
The sample grammar distributed with SUPPLE has
been developed over several years, across a number
LE projects. We here list some key characteristics.
The morpho-syntactic and semantic information
required for individual lexical items is minimal ?
inflectional root and word class only, where the word
class inventory is basically the PTB tagset.
A conservative philosophy is adopted regarding
identification of verbal arguments and attachment of
nominal and verbal post-modifiers, such as preposi-
tional phrases and relative clauses. Rather than pro-
ducing all possible analyses or using probabilities to
generate the most likely analysis, the preference is to
offer a single analysis that spans the input sentence
only if it can be relied on to be correct, so that in
many cases only partial analyses are produced. The
philosophy is that it is more useful to produce par-
tial analyses that are correct than full analyses which
may well be wrong or highly disjunctive. Output
from the parser can be passed to further processing
components which may bring additional information
to bear in resolving attachments.
An analysis of verb phrases is adopted in which
a core verb cluster consisting of verbal head plus
auxiliaries and adverbials is identified before any at-
tempt to attach any post-verbal arguments. This con-
trasts with analyses where complements are attached
to the verbal head at a lower level than auxiliaries
and adverbials, e.g. as in the Penn TreeBank. This
decision is again motivated by practical concerns: it
is relatively easy to recognise verbal clusters, much
harder to correctly attach complements.
A semantic analysis, or simplified quasi-logical
form (SQLF), is produced for each phrasal con-
stituent, in which tensed verbs are interpreted as re-
ferring to unique events, and noun phrases as refer-
ring to unique objects. Where relations between syn-
tactic constituents are identified in parsing, semantic
relations between associated objects and events are
asserted in the SQLF.
While linguistically richer grammatical theories
could be implemented in the grammar formalism
of SUPPLE, the emphasis in our work has been on
building robust wide-coverage tools ? hence the re-
quirement for only minimal lexical morphosyntac-
tic and semantic information. As a consequence the
combination of parser and grammars developed to
date results in a tool that, although capable of return-
ing full sentence analyses, more commonly returns
results that include chunks of analysis with some,
but not all, attachment relations determined.
4 Downloading SUPPLE Resources
SUPPLE resources, including source code and the
sample grammar, and also a longer paper providing
a more detailed account of both the parser and gram-
mar, are available from the supple homepage at:
http://nlp.shef.ac.uk/research/supple
5 Conclusion
The SUPPLE parser has served as a component in
numerous LE research projects, and is currently in
use in a Question Answering system which partic-
ipated in recent TREC/QA evaluations. We hope
its availability as a GATE component will facilitate
its broader use by NLP researchers, and by others
building applications exploiting NL technology.
Acknowledgements
The authors would like to acknowledge the sup-
port of the UK EPSRC under grants R91465 and
K25267, and also the contributions of Chris Huyck
and Sam Scott to the parser code and grammars.
References
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. GATE: A framework and graphical devel-
opment environment for robust NLP tools and applica-
tions. Proceedings of the 40th Anniversary Meeting of
the Association for Computational Linguistics, 2002.
201
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 75?80,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 15: TempEval Temporal Relation Identification
Marc Verhagen?, Robert Gaizauskas?, Frank Schilder?, Mark Hepple?,
Graham Katz? and James Pustejovsky?
? Brandeis University, {marc,jamesp}@cs.brandeis.edu
? University of Sheffield, {r.gaizauskas,m.hepple}@dcs.shef.ac.uk
? Thomson Legal & Regulatory, frank.schilder@thomson.com,
? Stanford University, egkatz@stanford.edu
Abstract
The TempEval task proposes a simple way
to evaluate automatic extraction of temporal
relations. It avoids the pitfalls of evaluat-
ing a graph of inter-related labels by defin-
ing three sub tasks that allow pairwise eval-
uation of temporal relations. The task not
only allows straightforward evaluation, it
also avoids the complexities of full tempo-
ral parsing.
1 Introduction
Newspaper texts, narratives and other texts describe
events that occur in time and specify the temporal
location and order of these events. Text comprehen-
sion, amongst other capabilities, clearly requires the
capability to identify the events described in a text
and locate these in time. This capability is crucial to
a wide range of NLP applications, from document
summarization and question answering to machine
translation.
Recent work on the annotation of events and tem-
poral relations has resulted in both a de-facto stan-
dard for expressing these relations and a hand-built
gold standard of annotated texts. TimeML (Puste-
jovsky et al, 2003a) is an emerging ISO standard
for annotation of events, temporal expressions and
the anchoring and ordering relations between them.
TimeBank (Pustejovsky et al, 2003b; Boguraev et
al., forthcoming) was originally conceived of as a
proof of concept that illustrates the TimeML lan-
guage, but has since gone through several rounds of
revisions and can now be considered a gold standard
for temporal information. TimeML and TimeBank
have already been used as the basis for automatic
time, event and temporal relation annotation tasks in
a number of research projects in recent years (Mani
et al, 2006; Boguraev et al, forthcoming).
An open evaluation challenge in the area of tem-
poral annotation should serve to drive research for-
ward, as it has in other areas of NLP. The auto-
matic identification of all temporal referring expres-
sions, events and temporal relations within a text is
the ultimate aim of research in this area. However,
addressing this aim in a first evaluation challenge
was judged to be too difficult, both for organizers
and participants, and a staged approach was deemed
more effective. Thus we here present an initial eval-
uation exercise based on three limited tasks that we
believe are realistic both from the perspective of as-
sembling resources for development and testing and
from the perspective of developing systems capable
of addressing the tasks. They are also tasks, which
should they be performable automatically, have ap-
plication potential.
2 Task Description
The tasks as originally proposed were modified
slightly during the course of resource development
for the evaluation exercise due to constraints on data
and annotator availability. In the following we de-
scribe the tasks as they were ultimately realized in
the evaluation.
There were three tasks ? A, B and C. For all
three tasks the data provided for testing and train-
ing includes annotations identifying: (1) sentence
boundaries; (2) all temporal referring expression as
75
specified by TIMEX3; (3) all events as specified
in TimeML; (4) selected instances of temporal re-
lations, as relevant to the given task. For tasks A and
B a restricted set of event terms were identified ?
those whose stems occurred twenty times or more in
TimeBank. This set is referred to as the Event Target
List or ETL.
TASK A This task addresses only the temporal re-
lations holding between time and event expressions
that occur within the same sentence. Furthermore
only event expressions that occur within the ETL are
considered. In the training and test data, TLINK an-
notations for these temporal relations are provided,
the difference being that in the test data the relation
type is withheld. The task is to supply this label.
TASK B This task addresses only the temporal
relations holding between the Document Creation
Time (DCT) and event expressions. Again only
event expressions that occur within the ETL are con-
sidered. As in Task A, TLINK annotations for these
temporal relations are provided in both training and
test data, and again the relation type is withheld in
the test data and the task is to supply this label.
TASK C Task C relies upon the idea of their being
a main event within a sentence, typically the syn-
tactically dominant verb. The aim is to assign the
temporal relation between the main events of adja-
cent sentences. In both training and test data the
main events are identified (via an attribute in the
event annotation) and TLINKs between these main
events are supplied. As for Tasks A and B, the task
here is to supply the correct relation label for these
TLINKs.
3 Data Description and Data Preparation
The TempEval annotation language is a simplified
version of TimeML 1. For TempEval, we use the fol-
lowing five tags: TempEval, s, TIMEX3, EVENT,
and TLINK. TempEval is the document root and s
marks sentence boundaries. All sentence tags in the
TempEval data are automatically created using the
Alembic Natural Language processing tools. The
other three tags are discussed here in more detail:
1See http://www.timeml.org for language specifica-
tions and annotation guidelines
? TIMEX3. Tags the time expressions in the text.
It is identical to the TIMEX3 tag in TimeML.
See the TimeML specifications and guidelines
for further details on this tag and its attributes.
Each document has one special TIMEX3 tag,
the Document Creation Time, which is inter-
preted as an interval that spans a whole day.
? EVENT. Tags the event expressions in the text.
The interpretation of what an event is is taken
from TimeML where an event is a cover term
for predicates describing situations that happen
or occur as well as some, but not all, stative
predicates. Events can be denoted by verbs,
nouns or adjectives. The TempEval event an-
notation scheme is somewhat simpler than that
used in TimeML, whose complexity was de-
signed to handle event expressions that intro-
duced multiple event instances (consider, e.g.
He taught on Wednesday and Friday). This
complication was not necessary for the Tem-
pEval data. The most salient attributes encode
tense, aspect, modality and polarity informa-
tion. For TempEval task C, one extra attribute
is added: mainevent, with possible values
YES and NO.
? TLINK. This is a simplified version of the
TimeML TLINK tag. The relation types for the
TimeML version form a fine-grained set based
on James Allen?s interval logic (Allen, 1983).
For TempEval, we use only six relation types
including the three core relations BEFORE, AF-
TER, and OVERLAP, the two less specific re-
lations BEFORE-OR-OVERLAP and OVERLAP-
OR-AFTER for ambiguous cases, and finally the
relation VAGUE for those cases where no partic-
ular relation can be established.
As stated above the TLINKs of concern for each
task are explicitly included in the training and in the
test data. However, in the latter the relType at-
tribute of each TLINK is set to UNKNOWN. For each
task the system must replace the UNKNOWN values
with one of the six allowed values listed above.
The EVENT and TIMEX3 annotations were taken
verbatim from TimeBank version 1.2.2 The annota-
2TimeBank 1.2 is available for free through the Linguistic
Data Consortium, see http://www.timeml.org for more
76
tion procedure for TLINK tags involved dual anno-
tation by seven annotators using a web-based anno-
tation interface. After this phase, three experienced
annotators looked at all occurrences where two an-
notators differed as to what relation type to select
and decided on the best option. For task C, there
was an extra annotation phase where the main events
were marked up. Main events are those events that
are syntactically dominant in the sentences.
It should be noted that annotation of temporal re-
lations is not an easy task for humans due to ram-
pant temporal vagueness in natural language. As a
result, inter-annotator agreement scores are well be-
low the often kicked-around threshold of 90%, both
for the TimeML relation set as well as the TempEval
relation set. For TimeML temporal links, an inter-
annotator agreement of 0.77 was reported, where
agreement was measured by the average of preci-
sion and recall. The numbers for TempEval are even
lower, with an agreement of 0.72 for anchorings of
events to times (tasks A and B) and an agreement of
0.65 for event orderings (task C). Obviously, num-
bers like this temper the expectations for automatic
temporal linking.
The lower number for TempEval came a bit as
a surprise because, after all, there were fewer rela-
tions to choose form. However, the TempEval an-
notation task is different in the sense that it did not
give the annotator the option to ignore certain pairs
of events and made it therefore impossible to skip
hard-to-classify temporal relations.
4 Evaluating Temporal Relations
In full temporal annotation, evaluation of temporal
annotation runs into the same issues as evaluation of
anaphora chains: simple pairwise comparisons may
not be the best way to evaluate. In temporal annota-
tion, for example, one may wonder how the response
in (1) should be evaluated given the key in (2).
(1) {A before B, A before C, B equals C}
(2) {A after B, A after C, B equals C}
Scoring (1) at 0.33 precision misses the interde-
pendence between the temporal relations. What we
need to compare is not individual judgements but
two partial orders.
details.
For TempEval however, the tasks are defined in
a such a way that a simple pairwise comparison is
possible since we do not aim to create a full temporal
graph and judgements are made in isolation.
Recall that there are three basic temporal relations
(BEFORE, OVERLAP, and AFTER) as well as three
disjunctions over this set (BEFORE-OR-OVERLAP,
OVERLAP-OR-AFTER and VAGUE). The addition
of these disjunctions raises the question of how to
score a response of, for example, BEFORE given a
key of BEFORE-OR-OVERLAP. We use two scor-
ing schemes: strict and relaxed. The strict scoring
scheme only counts exact matches as success. For
example, if the key is OVERLAP and the response
BEFORE-OR-OVERLAP than this is counted as fail-
ure. We can use standard definitions of precision
and recall
Precision = Rc/R
Recall = Rc/K
where Rc is number of correct answers in the re-
sponse, R the total number of answers in the re-
sponse, and K the total number of answers in the
key. For the relaxed scoring scheme, precision and
recall are defined as
Precision = Rcw/R
Recall = Rcw/K
where Rcw reflects the weighted number of correct
answers. A response is not simply counted as 1 (cor-
rect) or 0 (incorrect), but is assigned one of the val-
ues in table 1.
B O A B-O O-A V
B 1 0 0 0.5 0 0.33
O 0 1 0 0.5 0.5 0.33
A 0 0 1 0 0.5 0.33
B-O 0.5 0.5 0 1 0.5 0.67
O-A 0 0.5 0.5 0.5 1 0.67
V 0.33 0.33 0.33 0.67 0.67 1
Table 1: Evaluation weights
This scheme gives partial credit for disjunctions,
but not so much that non-commitment edges out pre-
cise assignments. For example, assigning VAGUE as
the relation type for every temporal relation results
in a precision of 0.33.
77
5 Participants
Six teams participated in the TempEval tasks. Three
of the teams used statistics exclusively, one used a
rule-based system and the other two employed a hy-
brid approach. This section gives a short description
of the participating systems.
CU-TMP trained three support vector machine
(SVM) models, one for each task. All models used
the gold-standard TimeBank features for events and
times as well as syntactic features derived from the
text. Additionally, the relation types obtained by
running the task B system on the training data for
Task A and Task C, were added as a feature to the
two latter systems. A subset of features was selected
using cross-validations on the training data, dis-
carding features whose removal improved the cross-
validation F-score. When applied to the test data,
the Task B system was run first in order to supply
the necessary features to the Task A and Task C sys-
tems.
LCC-TE automatically identifies temporal refer-
ring expressions, events and temporal relations in
text using a hybrid approach, leveraging various
NLP tools and linguistic resources at LCC. For tem-
poral expression labeling and normalization, they
used a syntactic pattern matching tool that deploys a
large set of hand-crafted finite state rules. For event
detection, they used a small set of heuristics as well
as a lexicon to determine whether or not a token is
an event, based on the lemma, part of speech and
WordNet senses. For temporal relation discovery,
LCC-TE used a large set of syntactic and semantic
features as input to a machine learning components.
NAIST-japan defined the temporal relation iden-
tification task as a sequence labeling model, in
which the target pairs ? a TIMEX3 and an EVENT
? are linearly ordered in the document. For analyz-
ing the relative positions, they used features from
dependency trees which are obtained from a depen-
dency parser. The relative position between the tar-
get EVENT and a word in the target TIMEX3 is used
as a feature for a machine learning based relation
identifier. The relative positions between a word in
the target entities and another word are also intro-
duced.
The USFD system uses an off-the-shelf Machine
Learning suite(WEKA), treating the assignment of
temporal relations as a simple classification task.
The features used were the ones provided in the
TempEval data annotation together with a few fea-
tures straightforwardly computed from the docu-
ment without any deeper NLP analysis.
WVALI?s approach for discovering intra-
sentence temporal relations relies on sentence-level
syntactic tree generation, bottom-up propaga-
tion of the temporal relations between syntactic
constituents, a temporal reasoning mechanism
that relates the two targeted temporal entities to
their closest ancestor and then to each other, and
on conflict resolution heuristics. In establishing
the temporal relation between an event and the
Document Creation Time (DCT), the temporal ex-
pressions directly or indirectly linked to that event
are first analyzed and, if no relation is detected,
the temporal relation with the DCT is propagated
top-down in the syntactic tree. Inter-sentence tem-
poral relations are discovered by applying several
heuristics and by using statistical data extracted
from the training corpus.
XRCE-T used a rule-based system that relies on
a deep syntactic analyzer that was extended to treat
temporal expressions. Temporal processing is inte-
grated into a more generic tool, a general purpose
linguistic analyzer, and is thus a complement for a
better general purpose text understanding system.
Temporal analysis is intertwined with syntactico-
semantic text processing like deep syntactic analy-
sis and determination of thematic roles. TempEval-
specific treatment is performed in a post-processing
stage.
6 Results
The results for the six teams are presented in tables
2, 3, and 4.
team strict relaxed
P R F P R F
CU-TMP 0.61 0.61 0.61 0.63 0.63 0.63
LCC-TE 0.59 0.57 0.58 0.61 0.60 0.60
NAIST 0.61 0.61 0.61 0.63 0.63 0.63
USFD* 0.59 0.59 0.59 0.60 0.60 0.60
WVALI 0.62 0.62 0.62 0.64 0.64 0.64
XRCE-T 0.53 0.25 0.34 0.63 0.30 0.41
average 0.59 0.54 0.56 0.62 0.57 0.59
stddev 0.03 0.13 0.10 0.01 0.12 0.08
Table 2: Results for Task A
78
team strict relaxed
P R F P R F
CU-TMP 0.75 0.75 0.75 0.76 0.76 0.76
LCC-TE 0.75 0.71 0.73 0.76 0.72 0.74
NAIST 0.75 0.75 0.75 0.76 0.76 0.76
USFD* 0.73 0.73 0.73 0.74 0.74 0.74
WVALI 0.80 0.80 0.80 0.81 0.81 0.81
XRCE-T 0.78 0.57 0.66 0.84 0.62 0.71
average 0.76 0.72 0.74 0.78 0.74 0.75
stddev 0.03 0.08 0.05 0.03 0.06 0.03
Table 3: Results for Task B
team strict relaxed
P R F P R F
CU-TMP 0.54 0.54 0.54 0.58 0.58 0.58
LCC-TE 0.55 0.55 0.55 0.58 0.58 0.58
NAIST 0.49 0.49 0.49 0.53 0.53 0.53
USFD* 0.54 0.54 0.54 0.57 0.57 0.57
WVALI 0.54 0.54 0.54 0.64 0.64 0.64
XRCE-T 0.42 0.42 0.42 0.58 0.58 0.58
average 0.51 0.51 0.51 0.58 0.58 0.58
stddev 0.05 0.05 0.05 0.04 0.04 0.04
Table 4: Results for Task C
All tables give precision, recall and f-measure for
both the strict and the relaxed scoring scheme, as
well as averages and standard deviation on the pre-
cision, recall and f-measure numbers. The entry for
USFD is starred because the system developers are
co-organizers of the TempEval task.3
For task A, the f-measure scores range from 0.34
to 0.62 for the strict scheme and from 0.41 to 0.63
for the relaxed scheme. For task B, the scores range
from 0.66 to 0.80 (strict) and 0.71 to 0.81 (relaxed).
Finally, task C scores range from 0.42 to 0.55 (strict)
and from 0.56 to 0.66 (relaxed).
The differences between the systems is not spec-
tacular. WVALI?s hybrid approach outperforms the
other systems in task B and, using relaxed scoring,
in task C as well. But for task A, the winners barely
edge out the rest of the field. Similarly, for task C
using strict scoring, there is no system that clearly
separates itself from the field.
It should be noted that for task A, and in lesser ex-
tent for task B, the XRCE-T system has recall scores
that are far below all other systems. This seems
mostly due to a choice by the developers to not as-
sign a temporal relation if the syntactic analyzer did
not find a clear syntactic relation between the two
3There was a strict separation between people assisting in
the annotation of the evaluation corpus and people involved in
system development.
elements that needed to be linked for the TempEval
task.
7 Conclusion: the Future of Temporal
Evaluation
The evaluation approach of TempEval avoids the in-
terdependencies that are inherent to a network of
temporal relations, where relations in one part of the
network may constrain relations in any other part of
the network. To accomplish that, TempEval delib-
erately focused on subtasks of the larger problem of
automatic temporal annotation.
One thing we may want to change to the present
TempEval is the definition of task A. Currently, it
instructs to temporally link all events in a sentence
to all time expressions in the same sentence. In the
future we may consider splitting this into two tasks,
where one subtask focuses on those anchorings that
are very local, like ?...White House spokesman Mar-
lin Fitzwater [said] [late yesterday] that...?. We ex-
pect both inter-annotator agreement and system per-
formance to be higher on this subtask.
There are two research avenues that loom beyond
the current TempEval: (1) definition of other sub-
tasks with the ultimate goal of establishing a hierar-
chy of subtasks ranked on performance of automatic
taggers, and (2) an approach to evaluate entire time-
lines.
Some other temporal linking tasks that can be
considered are ordering of consecutive events in a
sentence, ordering of events that occur in syntactic
subordination relations, ordering events in coordi-
nations, and temporal linking of reporting events to
the document creation time. Once enough temporal
links from all these subtasks are added to the en-
tire temporal graph, it becomes possible to let confi-
dence scores from the separate subtasks drive a con-
straint propagation algorithm as proposed in (Allen,
1983), in effect using high-precision relations to
constrain lower-precision relations elsewhere in the
graph.
With this more complete temporal annotation it
is no longer possible to simply evaluate the entire
graph by scoring pairwise comparisons. Instead
the entire timeline must be evaluated. Initial ideas
regarding this focus on transforming the temporal
graph of a document into a set of partial orders built
79
around precedence and inclusion relations and then
evaluating each of these partial orders using some
kind of edit distance measure.4
We hope to have taken the first baby steps with
the three TempEval tasks.
8 Acknowledgements
We would like to thank all the people who helped
prepare the data for TempEval, listed here in no
particular order: Amber Stubbs, Jessica Littman,
Hongyuan Qiu, Emin Mimaroglu, Emma Barker,
Catherine Havasi, Yonit Boussany, Roser Saur??, and
Anna Rumshisky.
Thanks also to all participants to this new task:
Steven Bethard and James Martin (University of
Colorado at Boulder), Congmin Min, Munirath-
nam Srikanth and Abraham Fowler (Language Com-
puter Corporation), Yuchang Cheng, Masayuki Asa-
hara and Yuji Matsumoto (Nara Institute of Science
and Technology), Mark Hepple, Andrea Setzer and
Rob Gaizauskas (University of Sheffield), Caroline
Hageg`e and Xavier Tannier (XEROX Research Cen-
tre Europe), and Georgiana Pus?cas?u (University of
Wolverhampton and University of Alicante).
Part of the work in this paper was funded by
the DTO/AQUAINT program under grant num-
ber N61339-06-C-0140 and part funded by the EU
VIKEF project (IST- 2002-507173).
References
James Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Bran Boguraev, James Pustejovsky, Rie Ando, and Marc
Verhagen. forthcoming. Timebank evolution as a
community resource for timeml parsing. Language
Resources and Evaluation.
Inderjeet Mani, BenWellner, Marc Verhagen, ChongMin
Lee, and James Pustejovsky. 2006. Machine learn-
ing of temporal relations. In Proceedings of the 44th
Annual Meeting of the Association for Computational
Linguistics, Sydney, Australia. ACL.
James Pustejovsky, Jose? Castan?o, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Gra-
ham Katz. 2003a. TimeML: Robust specification of
4Edit distance was proposed by Ben Wellner as a way to
evaluate partial orders of precedence relations (personal com-
munication).
event and temporal expressions in text. In Proceedings
of the Fifth International Workshop on Computational
Semantics (IWCS-5), Tilburg, January.
James Pustejovsky, Patrick Hanks, Roser Saur??, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro, and
Marcia Lazo. 2003b. The TIMEBANK corpus. In
Proceedings of Corpus Linguistics 2003, pages 647?
656, Lancaster, March.
80
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 438?441,
Prague, June 2007. c?2007 Association for Computational Linguistics
USFD: Preliminary Exploration of Features
and Classifiers for the TempEval-2007 Tasks
Mark Hepple
Dept of Computer Science
University of Sheffield
Regent Court
211 Portobello Street
Sheffield S1 4DP, UK
hepple@dcs.shef.ac.uk
Andrea Setzer
Dept of Computer Science
University of Sheffield
Regent Court
211 Portobello Street
Sheffield S1 4DP, UK
andrea@dcs.shef.ac.uk
Rob Gaizauskas
Dept of Computer Science
University of Sheffield
Regent Court
211 Portobello Street
Sheffield S1 4DP, UK
robertg@dcs.shef.ac.uk
Abstract
We describe the Sheffield system used
in TempEval-2007. Our system takes
a machine-learning (ML) based approach,
treating temporal relation assignment as a
simple classification task and using features
easily derived from the TempEval data, i.e.
which do not require ?deeper? NLP analy-
sis. We aimed to explore three questions:
(1) How well would a ?lite? approach of
this kind perform? (2) Which features con-
tribute positively to system performance?
(3) Which ML algorithm is better suited for
the TempEval tasks? We used the Weka
ML workbench to facilitate experimenting
with different ML algorithms. The paper de-
scribes our system and supplies preliminary
answers to the above questions.
1 Introduction
The Sheffield team were involved in TempEval as
co-proposers/co-organisers of the task.1 For our par-
ticipation in the task, we decided to pursue an ML-
based approach, the benefits of which have been ex-
plored elsewhere (Boguraev and Ando, 2005; Mani
et al, 2006). For the TempEval tasks, this is easily
done by treating the assignment of temporal relation
types as a simple classification task, using readily
available information for the instance features. More
specifically, the features used were ones provided as
1We maintained a strict separation between persons assisting
in annotation of the test corpus and those involved in system
development.
attributes in the TempEval data annotation for the
events/times being related, plus some additional fea-
tures that could be straightforwardly computed from
documents, i.e. without the use of more heavily ?en-
gineered? NLP components. The aims of this work
were three-fold. First, we wanted to see whether a
?lite? approach of this kind could yield reasonable
performance, before pursuing possibilities that re-
lied on using ?deeper? NLP analysis methods. Sec-
ondly, we were interested to see which of the fea-
tures considered would contribute positively to sys-
tem performance. Thirdly, rather than selecting a
single ML approach (e.g. one of those currently in
vogue within NLP), we wanted to look across ML
algorithms to see if any approach was better suited
to the TempEval tasks than any other, and conse-
quently we used the Weka workbench (Witten and
Frank, 2005) in our ML experiments.
In what follows, we will first describe how our
system was constructed, before going on to discuss
our main observations around the key aims men-
tioned above. For example, in regard to our ?lite? ap-
proach, we would observe (c.f. the results reported
in the Task Description paper) that although some
other systems scored more highly, the score differ-
ences were relatively small. Regarding features, we
found for example that the system performed better
for Task A when, surprisingly, the tense attribute
of EVENTs was excluded. Regarding ML algo-
rithms, we found not only that there was substantial
variation between the effectiveness of different algo-
rithms for assigning relations (as one might expect),
but also that there was considerable differences in
the relative effectiveness of algorithms across tasks,
438
i.e. so that an algorithm performing well on one task
(compared to the alternatives), might perform rather
poorly on another task. The paper closes with some
comments about future research directions.
2 System Description
The TempEval training and test data is marked up
to identify all event and time expressions occurring
within documents, and also to record the TLINK re-
lations that are relevant for each task (except that
TLINK relation types are absent in the test data).
These annotations provide additional information
about these entities in the form of XML attributes,
e.g. for EVENT annotations we find attributes such
as tense, aspect, part-of-speech and so on.
Our system consists of a suite of Perl scripts
that create the input files required for Weka, and
handle its output. These include firstly an ?ex-
traction? script, which extracts information about
EVENT, TIMEXs and TLINKs from the data files, and
secondly a ?feature selection/reformatting? script,
which allows the information that is to be supplied
to Weka to be selected, and recasts it into the format
that Weka requires for its training/test files. A final
script takes Weka?s output over the test files and con-
nects it back to the original test documents to pro-
duce the final output files required for scoring.
The information that the first extraction script ex-
tracts for each EVENT, TIMEX and TLINK largely
corresponds to attributes/values associated with the
annotations of these items in the initial data files
(although not all such attributes are of use for ma-
chine learning purposes). In addition, the script de-
termines for each EVENT expression whether it is
one deemed relevant by the Event Target List (ETL)
for Tasks A and B. This script also maps EVENTs
and TIMEXs into sequential order ? intra-sentential
order for task A and inter-sentential order for task
C. This information can be used to compute various
?order? features, such as:
event-first: do a related EVENT and TIMEX
(for Task A) appear with the EVENT before or after
the TIMEX?
adjacent: do a related EVENT and TIMEX (again
for Task A) appear adjacently in the sequence of
temporal entities or not? (Note that this allows an
EVENT and TIMEX to be adjacent if there tokens
Task
Type Attribute A B C
EVENT aspect X X X
EVENT polarity X X ?
EVENT POS X X X
EVENT stem X ? ?
EVENT string ? ? ?
EVENT class ? X X
EVENT tense ? X X
ORDER adjacent X N/A N/A
ORDER event-first X N/A N/A
ORDER event-between ? N/A N/A
ORDER timex-between ? N/A N/A
TIMEX3 mod X ? N/A
TIMEX3 type X ? N/A
TLINK reltype X X X
Table 1: Features
that intervene, but not any other temporal entities.)
event-between: for a related EVENT/TIMEX
pair, do any other events appear between them?
timex-between: for a related EVENT/TIMEX
pair, do any other timexes appear between them?
Table 1 lists all the features that we tried using
for any of the three tasks. Aside from the OR-
DER features (as designated in the leftmost col-
umn), which were computed as just described, and
the EVENT string feature (which is the literal
tagged expression from the text), all other features
correspond to annotation attributes. Note that the
TLINKreltype is extracted from the training data
to provide the target attribute for training (a dummy
value is provided for this in test data).
The output of the extraction script is converted to
a format suitable for use by Weka by a second script.
This script also allows a manual selection to be made
as to the features that are included. For each of the
three tasks, a rough-and-ready process was followed
to find a ?good? set of features for use with that
task, which proceeded as follows. Firstly, the maxi-
mal set of features considered for the task was tried
with a few ML algorithms in Weka (using a 10-fold
cross-validation over the training data) to find one
that seemed to work quite well for the task. Then
using only that algorithm, we checked whether the
string feature could be dropped (since this fea-
439
ture?s value set was always of quite high cardinality),
i.e. if its omission improved performance, which for
all three tasks was the case. Next, we tried dropping
each of the remaining features in turn, to identify
those whose exclusion improved performance, and
then for those features so identified, tried dropping
them in combination to arrive at a final ?optimal? fea-
ture set. Table 1 shows for each of the tasks which
of the features were considered for inclusion (those
marked N/A were not), and which of these remained
in the final optimal feature set (X).
Having determined the set of features for use with
each task, we tried out a range of ML algorithms
(again with a 10-fold cross-validation over the train-
ing data), to arrive at the final feature-set/ML algo-
rithm combination that was used for the task in the
competitive evaluation. This was trained over the
entire training data and applied to the test data to
produce the final submitted results.
3 Discussion
Looking to Table 1, and the features that were con-
sidered for each task and then included in the final
set, various observations can be made. First, note
that the string feature was omitted for all tasks,
which is perhaps not surprising, since its values will
be sparsely distributed, so that there will be very few
training instances for most of its individual values.
However, the stem feature was found to be use-
ful for Task A, which can be interpreted as evidence
for a ?lexical effect? on local event-timex relations,
e.g. perhaps with different verbs displaying different
trends in how they relate to timexes. No correspond-
ing effects were observed for Tasks B and C.
The use of ORDER features for Task A was found
to be useful ? specifically the features indicating
whether the event or timex appeared linearly first in
the sentence and whether the two were adjacent or
not. The more elaborate ORDER features, address-
ing more specific cases of what might intervene be-
tween the related timex and event expression, were
not found to be helpful.
Perhaps the most striking observation to be made
regarding the table is that it was found beneficial to
exclude the feature tense for Task A, whilst the
feature aspect was retained. We have no expla-
nation to offer for this result. Likewise, the event
Task
Algorithm A B C
baseline 49.8 62.1 42.0
lazy.KStar 58.2 76.7 54.0
rules.DecisionTable 53.3 79.0 52.9
functions.SMO (svm) 55.1 78.1 55.5
rules.JRip 50.7 78.6 53.4
bayes.NaiveBayes 56.3 76.2 50.7
Table 2: Comparing different algorithms (%-acc.
scores, from cross-validation over training data)
class feature, which distinguishes e.g. perception
vs. reporting vs. aspectual etc verbs, was excluded
for Task A, although it was retained for Task B.
In regard to the use of different ML algorithms for
the classification tasks addressed in TempEval, we
observed considerable variation between algorithms
as to their performance, and this was not unexpected.
However, given the seemingly high similarity of the
three tasks, we were rather more surprised to see that
there was considerable variation between the perfor-
mance of algorithms across tasks, i.e. so that an al-
gorithm performing well on one task (compared to
the alternatives), might perform rather poorly on an-
other task. This is illustrated by the results in Table 2
for a selected subset of the algorithms considered,
which shows %-accuracy scores that were computed
by cross-validation over the training data, using the
feature set chosen as ?optimal? for each task.2 The
algorithm names in the left-hand column are the
ones used in WEKA (of which functions.SMO
is the WEKA implementation of support-vector ma-
chines or SVM). The first row of results give a ?base-
line? for performance, corresponding to the assign-
ment of the most common label for the task. (These
were produced using WEKA?s rules.ZeroR al-
gorithm, which does exactly that.)
The best results observed for each task are shown
in bold in the table. These best performing al-
gorithms were used for the corresponding tasks in
the competition. Observe that the lazy.KStar
2These scores are computed under the ?strict? requirement
that key and response labels should be identical. The TempE-
val competition also uses a ?relaxed? metric which gives par-
tial credit when one (or both) label is disjunctive and there is a
partial match, e.g. between labels AFTER and OVERLAP-OR-
AFTER. See (Verhagen et al, 2007) for details.
440
Task A Task B Task C
FS FR FS FR FS FR
USFD 0.59 0.60 0.73 0.74 0.54 0.59
ave. 0.56 0.59 0.74 0.75 0.51 0.60
max. 0.62 0.64 0.80 0.81 0.55 0.66
Table 3: Competition task scores for Sheffield sys-
tem (USFD), plus average/max scores across all
competing systems
method, which gives the best performance for Task
A, gives a rather ?middling? performance for Task
B. Similarly, the SVM method that gives the best
results for Task C falls quite a way below the per-
formance of KStar on Task A. A more extreme
case is seen with the results for rules.JRip
(Weka?s implementation of the RIPPER algorithm),
whose score for Task B is close to that of the best-
performing system, but which scores only slightly
above baseline on Task A.
The competition scores for our system are given
in Table 3, shown as (harmonic) F-measures under
both strict (FS) and relaxed (FR) metrics (see foot-
note 2). The table also shows the average score for
each task/metric across all systems taking part in the
competition, as well as the maximum score returned
by any system. See (Verhagen et al, 2007) for a full
tabulation of results for all systems.3
4 Future Directions
SIGNALs and SLINKs are possible candidates as
additional features ? signals obviously so, whereas
the benefits of exploiting subordination information
are less clear. Our initial exploratory efforts in
this direction involved pulling information regard-
ing SIGNALs and SLINKs across from TimeBank4
(Pustejovsky et al, 2003) so as to make this avail-
3The TempEval test data identifies precisely the temporal
entity pairs to which a relation label must be assigned. When
a fixed set of items is classified, the scores for precision, recall
and F-measure will be identical, being the same as the score for
simple accuracy. However, not all the participating systems fol-
low this pattern of assigning labels to ?all and only? the entity
pairs identified in the test data, i.e. some systems decide which
entity pairs to label, as well as which label to assign. Accord-
ingly, the performance results given in (Verhagen et al, 2007)
are reported using metrics of precision, recall and F-measure.
4This was possible because both the trial and training data
were derived from TimeBank.
able for use with the TempEval tasks, in the hope
that this would allow us to determine if this informa-
tion would be useful without first facing the cost of
developing SIGNAL and SLINK recognisers. Re-
garding SIGNALs, however, we ran into the prob-
lem that there are many TLINKs in the TempEval
data for which no corresponding TLINK appears
in TimeBank, and hence for which SIGNAL infor-
mation could not be imported. We were unable to
progress this work sufficiently in the time available
for there to be any useful results to report here.
5 Conclusion
We have explored using a ML-based approach to
the TempEval tasks, which does not rely on the use
of deeper NLP-analysis components. We observe
that although some other systems in the competi-
tion have produced higher scores for the tasks, the
score differences are relatively small. In the course
of this work, we have made some interesting ob-
servations regarding the performance variability of
different ML algorithms when applied to the diffent
TempEval tasks, and regarding the features that con-
tribute to the system?s performance.
References
B. Boguraev and R. Kubota Ando. 2005. TimeML-
Compliant Text Analysis for Temporal Reasoning. In
Proceedings of IJCAI-05, pages 997?1003.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine Learning
of Temporal Relations. In ACL ?06: Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the ACL,
pages 753?760, Morristown, NJ, USA. Association for
Computational Linguistics.
J. Pustejovsky, D. Day, L. Ferro, R. Gaizauskas, P. Hanks,
M. Lazo, D. Radev, R. Saur??, A. See, A. Setzer, and
B. Sundheim. 2003. The TIMEBANK Corpus. In
Proceedings of Corpus Linguistics 2003, pages 647?
656, Lancaster, March.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,
G. Katz, and J. Pustejovsky. 2007. SemEval-2007
Task 15: TempEval Temporal Relation Identification.
In Proceedings of SemEval-2007: 4th International
Workshop on Semantic Evaluations.
I.H. Witten and E. Frank, editors. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, San Francisco, second edition.
441
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 10?18,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Extracting Clinical Relationships from Patient Narratives
Angus Roberts, Robert Gaizauskas, Mark Hepple
Department of Computer Science, University of Sheffield,
Regent Court, 211 Portobello, Sheffield S1 4DP
{initial.surname}@dcs.shef.ac.uk
Abstract
The Clinical E-Science Framework (CLEF)
project has built a system to extract clin-
ically significant information from the tex-
tual component of medical records, for clin-
ical research, evidence-based healthcare and
genotype-meets-phenotype informatics. One
part of this system is the identification of rela-
tionships between clinically important entities
in the text. Typical approaches to relationship
extraction in this domain have used full parses,
domain-specific grammars, and large knowl-
edge bases encoding domain knowledge. In
other areas of biomedical NLP, statistical ma-
chine learning approaches are now routinely
applied to relationship extraction. We report
on the novel application of these statistical
techniques to clinical relationships.
We describe a supervised machine learning
system, trained with a corpus of oncology nar-
ratives hand-annotated with clinically impor-
tant relationships. Various shallow features
are extracted from these texts, and used to
train statistical classifiers. We compare the
suitability of these features for clinical re-
lationship extraction, how extraction varies
between inter- and intra-sentential relation-
ships, and examine the amount of training data
needed to learn various relationships.
1 Introduction
The application of Natural Language Processing
(NLP) is widespread in biomedicine. Typically, it
is applied to improve access to the ever-burgeoning
research literature. Increasingly, biomedical re-
searchers need to relate this literature to pheno-
typic data: both to populations, and to individ-
ual clinical subjects. The computer applications
used in biomedical research, including NLP appli-
cations, therefore need to support genotype-meets-
phenotype informatics and the move towards trans-
lational biology. Such support will undoubtedly in-
clude linkage to the information held in individual
medical records: both the structured portion, and the
unstructured textual portion.
The Clinical E-Science Framework (CLEF)
project (Rector et al, 2003) is building a frame-
work for the capture, integration and presentation of
this clinical information, for research and evidence-
based health care. The project?s data resource is a
repository of the full clinical records for over 20000
cancer patients from the Royal Marsden Hospital,
Europe?s largest oncology centre. These records
combine structured information, clinical narratives,
and free text investigation reports. CLEF uses infor-
mation extraction (IE) technology to make informa-
tion from the textual portion of the medical record
available for integration with the structured record,
and thus available for clinical care and research. The
CLEF IE system analyses the textual records to ex-
tract entities, events and the relationships between
them. These relationships give information that is
often not available in the structured record. Why
was a drug given? What were the results of a physi-
cal examination? What problems were not present?
We have previously reported entity extraction in the
CLEF IE system (Roberts et al, 2008b). This paper
examines relationship extraction.
Extraction of relationships from clinical text is
usually carried out as part of a full clinical IE sys-
tem. Several such systems have been described.
They generally use a syntactic parse with domain-
specific grammar rules. The Linguistic String
project (Sager et al, 1994) used a full syntactic and
10
clinical sublanguage parse to fill template data struc-
tures corresponding to medical statements. These
were mapped to a database model incorporating
medical facts and the relationships between them.
MedLEE (Friedman et al, 1994), and more recently
BioMedLEE (Lussier et al, 2006) used a semantic
lexicon and grammar of domain-specific semantic
patterns. The patterns encode the possible relation-
ships between entities, allowing both entities and the
relationships between them to be directly matched
in the text. Other systems have incorporated large-
scale domain-specific knowledge bases. MEDSYN-
DIKATE (Hahn et al, 2002) employed a rich dis-
course model of entities and their relationships, built
using a dependency parse of texts and a descrip-
tion logic knowledge base re-engineered from exist-
ing terminologies. MENELAS (Zweigenbaum et al,
1995) also used a full parse, a conceptual represen-
tation of the text, and a large scale knowledge base.
In other applications of biomedical NLP, a sec-
ond paradigm has become widespread: the appli-
cation of statistical machine learning techniques to
feature-based models of the text. Such approaches
have typically been applied to journal texts. They
have been used both for entity recognition and ex-
traction of various relations, such as protein-protein
interactions (see, for example, Grover et al(2007)).
This follows on from the success of these methods
in general NLP (see for example Zhou et al(2005)).
Statistical machine learning has also been applied to
clinical text, but its use has generally been limited
to entity recognition. The Mayo Clinic text analysis
system (Pakhomov et al, 2005), for example, uses a
combination of dictionary lookup and a Na??ve Bayes
classifier to identify entities for information retrieval
applications. To the best of our knowledge, statisti-
cal methods have not been previously applied to ex-
traction of clinical relationships from text.
This paper describes experiments in the statistical
machine learning of relationships from a novel text
type: oncology narratives. The set of relationships
extracted are considered to be of interest for clinical
and research applications down line of IE, such as
querying to support clinical research. We apply Sup-
port Vector Machine (SVM) classifiers to learn these
relationships. The classifiers are trained and eval-
uated using novel data: a gold standard corpus of
clinical text, hand-annotated with semantic entities
and relationships. In order to test the applicability
of this method to the clinical domain, we train clas-
sifiers using a number of comparatively simple text
features, and look at the contribution of these fea-
tures to system performance. Clinically interesting
relationships may span several sentences, and so we
compare classifiers trained for both intra- and inter-
sentential relationships (spanning one or more sen-
tence boundaries). We also examine the influence of
training corpus size on performance, as hand anno-
tation of training data is the major expense in super-
vised machine learning.
2 Relationship Schema
Relationship Argument 1 Argument 2
has target Investigation Locus
Intervention Locus
has finding Investigation Condition
Investigation Result
has indication Drug or device Condition
Intervention Condition
Investigation Condition
has location Condition Locus
negation modifies Negation modifier Condition
laterality modifies Laterality modifier Intervention
Laterality modifier Locus
sub-location modifies Sub-location modifier Locus
Table 1: Relationship types and their argument type con-
straints.
The CLEF application extracts entities, relation-
ships and modifiers from text. By entity, we mean
some real-world thing, event or state referred to in
the text: the drugs that are mentioned, the tests that
were carried out, etc. Modifiers are words that qual-
ify an entity in some way, referring e.g. to the lat-
erality of an anatomical locus, or the negation of a
condition (?no sign of inflammation?). Entities are
connected to each other and to modifiers by rela-
tionships: e.g. linking a drug entity to the condition
entity for which it is indicated, linking an investiga-
tion to its results, or linking a negating phrase to a
condition.
The entities, modifiers, and relationships are de-
scribed by both a formal XML schema, and by a
set of detailed definitions. These were developed by
a group of clinical experts through an iterative pro-
cess, until acceptable agreement was reached. Entity
types are mapped to types from the UMLS seman-
tic network (Lindberg et al, 1993), each CLEF en-
11
tity type covering several UMLS types. Relationship
types are those that were felt necessary to capture the
essential clinical dependencies between entities re-
ferred to in patient documents, and to support CLEF
end user applications.
Each relationship type is constrained to exist be-
tween limited pairs of entity types. For example,
the has location relationship can only exist be-
tween a Condition entity and a Locus entity.
Some relationships can exist between multiple type
pairs. The full set of relationships and their argu-
ment type constraints are shown in Table 1. Ex-
amples of each relationship are given in Roberts et
al (2008a).
Some of the relationships considered important
by the clinical experts were not obvious without do-
main knowledge. For example,
He is suffering from nausea and severe
headaches. Dolasteron was prescribed.
Without domain knowledge, it is not clear that there
is a has indication relationship between the
?Dolasteron? Drug or device entity and the
?nausea? Condition entity. As in this example,
many of this type of relationship are intra-sentential.
A single real-world entity may be referred to sev-
eral times in the same text. Each of these co-
referring expressions is a mention of the entity. The
gold standard includes annotation of co-reference
between different textual mentions of the same en-
tity. For the work reported in this paper, however,
co-reference is not considered. Each entity is as-
sumed to have a single mention. Relationships be-
tween entities can be considered, by extension, as
relationships between the single mentions of those
entities. The implications of this are discussed fur-
ther below.
3 Gold Standard Corpus
The schema and definitions were used to hand-
annotate the entities and relationships in 77 oncol-
ogy narratives, to provide a gold standard for sys-
tem training and evaluation. Corpora of this size
are typical in supervised machine learning, and re-
flect the expense of hand annotation. Narratives
were carefully selected and annotated according to
a best practice methodology, as described in Roberts
et al(2008a). Narratives were annotated by two in-
dependent, clinically trained, annotators, and a con-
sensus created by a third. We will refer to this corpus
as C77.
Annotators were asked to first mark the mentions
of entities and modifiers, and then to go through
each of these in turn, deciding if any had relation-
ships with mentions of other entities. Although the
annotators were marking co-reference between men-
tions of the same entity, they were asked to ignore
this with respect to relationship annotation. Both
the annotation tool that they were using and their
annotation guidelines, enforced the creation of rela-
tionships between mentions, and not between enti-
ties. The gold standard is thus analogous to the style
of relationship extraction reported here, in which
we extract relations between single mention entities,
and do not consider co-reference. Annotators were
further told that relationships could span multiple
sentences, and that it was acceptable to use clini-
cal domain knowledge to infer that a relationship
existed between two mentions. Counts of all rela-
tionships annotated in C77 are shown in Table 2,
sub-divided by the number of sentence boundaries
spanned by a relationship.
4 Relationship Extraction
The system we have built uses the GATE NLP
toolkit (Cunningham et al, 2002) 1. The system is
shown in Figure 1, and is described below.
Narratives are first pre-processed using standard
GATE modules. Narratives were tokenised, sen-
tences found with a regular expression-based sen-
tence splitter, part-of-speech (POS) tagged, and
morphological roots found for tokens. Each to-
ken was also labelled with a generalised POS tag,
the first two characters of the full POS tag. This
takes advantage of the Penn Treebank tagset used
by GATE?s POS tagger, in which related POS tags
share the first two characters. For example, all six
verb POS tags start with the letters ?VB?.
After pre-processing, mentions of entities within
the text are annotated. In the experiments reported,
we assume perfect entity recognition, as given by
the entities in the human annotated gold standard
1We used a development build of GATE 4.0, downloadable
from http://gate.ac.uk
12
Sentence boundaries between arguments
0 1 2 3 4 5 6 7 8 9 >9 Total
has finding 265 46 25 7 5 4 3 2 2 2 0 361
has indication 139 85 35 32 14 11 6 4 5 5 12 348
has location 360 4 1 1 1 1 1 0 0 0 4 373
has target 122 14 4 2 2 4 3 1 0 1 0 153
laterality modifies 128 0 0 0 0 0 0 0 0 0 0 128
negation modifies 100 1 0 0 0 0 0 0 0 0 0 101
sub location modifies 76 0 0 0 0 0 0 0 0 0 0 76
Total 1190 150 65 42 22 20 13 7 7 8 16 1540
Cumulative total 1190 1340 1405 1447 1469 1489 1502 1509 1516 1524 1540
Table 2: Count of relationships in 77 gold standard documents.
described above. Our results are therefore higher
than would be expected in a system with automatic
entity recognition. It is useful and usual to fix en-
tity recognition in this way, to allow tuning specific
to relationship extraction, and to allow the isolation
of relation-specific problems. We accept, however,
that ultimately, relation extraction does depend on
the quality of entity recognition. The relation extrac-
tion described here is used as part of an operational
IE system in which clinical entity recognition is per-
formed by a combination of lexical lookup and su-
pervised machine learning. We have described our
entity extraction system elsewhere (Roberts et al,
2008b).
4.1 Classification
We treat clinical relationship extraction as a classi-
fication task, training classifiers to assign a relation-
ship type to an entity pair. An entity pair is a pairing
of entities that may or may not be the arguments of
a relation. For a given document, we create all pos-
sible entity pairs within two constraints. First, en-
tities that are paired must be within n sentences of
each other. For all of the work reported here, unless
stated, n ? 1 (crossing 0 or 1 sentence boundaries).
Second, we can constrain the entity pairs created
by argument type (Rindflesch and Fiszman, 2003).
For example, there is little point in creating an en-
tity pair between a Drug or device entity and
a Result entity, as no relationships, as specified
by the schema, exist between entities of these types.
Entity pairing is carried out by a GATE component
developed specifically for clinical relationship ex-
traction. In addition to pairing entities according to
the above constraints, this component also assigns
features to each pair that characterise its lexical and
syntactic qualities (described further in Section 4.2).
Entity pairs correspond to classifier training and
test instances. In classifier training, if an entity
pair corresponds to the arguments of a relationship
present in the gold standard, then it is assigned a
class of that relationship type. If it does not corre-
spond to such a relation, then it is assigned the class
null. The classifier builds a model of these entity
pair training instances, from their features. In classi-
fier application, entity pairs are created from unseen
text, under the above constraints. The classifier as-
signs one of our seven relationship types, or null,
to each entity pair.
We use Support Vector machines (SVMs) as train-
able classifiers, as these have proved to be robust and
efficient for a range of NLP tasks, including relation
extraction. We use an SVM implementation devel-
oped within our own group, and provided as part
of the GATE toolkit. This is a variant on the orig-
inal SVM algorithm, SVM with uneven margins, in
which classification may be biased towards positive
training examples. This is particularly suited to NLP
applications, in which positive training examples are
often rare. Full details of the classifier are given in
Li et al(2005). We used the implementation ?out of
the box?, with default parameters as determined in
experiments with other data sets.
SVMs are binary classifiers: the multi-class prob-
lem of classifying entity pairs must therefore be
mapped to a number of binary classification prob-
lems. There are several ways in which a multi-
class problem can be recast as binary problems. The
commonest are one-against-one in which one classi-
fier is trained for every possible pair of classes, and
one-against-all in which a classifier is trained for
a binary decision between each class and all other
13
classes, including null, combined. We have car-
ried out extensive experiments (not reported here),
with these two strategies, and have found little dif-
ference between them for our data. We have chosen
to use one-against-all, as it needs fewer classifiers
(for an n class problem, it needs n classifiers, as op-
posed to (n?1)!2 for one-against-one).
The resultant class assignments by multiple bi-
nary classifiers must be post-processed to deal with
ambiguity. In application to unseen text, it is possi-
ble that several classifiers assign different classes to
an entity pair (test instance). To disambiguate these
cases, the output of each one-against-all classifier is
transformed into a probability, and the class with
the highest probability is assigned. Re-casting the
multi-class relation problem as a number of binary
problems, and post-processing to resolve ambigui-
ties, is handled by the GATE Learning API.
Figure 1: The relationship extraction system.
4.2 Features for Classification
The SVM classification model is built from lexical
and syntactic features assigned to tokens and en-
tity pairs prior to classification. We use features
developed in part from those described in Zhou et
al (2005) and Wang et al(2006). These features are
split into 11 sets, as described in Table 3.
The tokN features are POS and surface string
taken from a window of N tokens on each side of
each paired entity?s mention. For N = 6, this
gives 48 features. The rationale behind these sim-
ple features is that there is useful information in the
words surrounding two mentions, that helps deter-
mine any relationship between them. The gentokN
features generalise tokN to use morphological root
and generalised POS. The str features are a set
of 14 surface string features, encoding the full sur-
face strings of both entity mentions, their heads,
their heads combined, the surface strings of the first,
last and other tokens between the mentions, and
of the two tokens immediately before and after the
leftmost and rightmost mentions respectively. The
pos, root, and genpos feature sets are similarly
constructed from the POS tags, roots, and gener-
alised POS tags of the entity mentions and their sur-
rounding tokens. These four feature sets differ from
tokN and gentokN, in that they provide more fine-
grained information about the position of features
relative to the paired entity mentions.
For the event feature set, the main entities
were divided into events (Investigation and
Intervention) and non-events (all others). Fea-
tures record whether the entity pair consists of two
events, two non-events, one of each, and whether
there are any intervening events and non-events.
This feature set gives similar information to atype
(semantic types of arguments) and inter (inter-
vening entities), but at a coarser level of typing.
5 Evaluation
We used a standard ten-fold cross validation
methodology and standard evaluation metrics. Met-
rics are defined in terms of true positive, false pos-
itive and false negative matches between relation-
ships in a system annotated response document and
a gold standard key document. A response relation-
ship is a true positive if a relationship of the same
type, and with the exact same arguments, exists in
the key. Corresponding definitions apply for false
positive and false negative. Counts of these matches
are used to calculate standard metrics of Recall (R),
Precision (P ) and F1 measure.
The metrics do not say how hard relationship ex-
traction is. We therefore provide a comparison with
Inter Annotator Agreement (IAA) scores from the
gold standard. The IAA score gives the agreement
between the two independent double annotators. It
is equivalent to scoring one annotator against the
other using the F1 metric. IAA scores are not di-
rectly comparable here, as relationship annotation is
14
Feature set Size Description
tokN 8N Surface string and POS of tokens surrounding the arguments, windowed ?N to +N , N = 6 by default
gentokN 8N Root and gerenalised POS of tokens surrounding the argument entities, windowed ?N to +N , N = 6 by default
atype 1 Concatenated semantic type of arguments, in arg1-arg2 order
dir 1 Direction: linear text order of the arguments (is arg1 before arg2, or vice versa?)
dist 2 Distance: absolute number of sentence and paragraph boundaries between arguments
str 14 Surface string features based on Zhou et al(2005), see text for full description
pos 14 POS features, as above
root 14 Root features, as above
genpos 14 Generalised POS features, as above
inter 11 Intervening mentions: numbers and types of intervening entity mentions between arguments
event 5 Events: are any of the arguments, or intevening entities, events?
allgen 96 All features in root and generalised POS forms, i.e. gentok6+atype+dir+dist+root+genpos+inter+event
notok 48 All except tokN features, others in string and POS forms, i.e. atype+dir+dist+str+pos+inter+event
Table 3: Feature sets used for learning relationships. The size of a set is the number of features in that set.
a slightly different task for the human annotators.
The relationship extraction system is given entities,
and finds relationships between them. Human an-
notators must find both the entities and the relation-
ships. Therefore, were one human annotator to fail
to find a particular entity, they could never find rela-
tionships with that entity. The raw IAA score does
not take this into account: if an annotator fails to
find an entity, then they will also be penalised for
all relationships with that entity. We therefore give a
Corrected IAA, CIAA, in which annotators are only
compared on those relations for which they have
both found the entities involved. Both forms of IAA
are shown in Table 4. It is clear that it is hard for
annotators to reach agreement on relationships, and
that this is compounded massively by lack of perfect
agreement on entities. Note that the gold standard
used in training and evaluation reflects a further con-
sensus annotation, to correct this poor agreement.
6 Results
6.1 Feature Selection
The first group of experiments reported looks at the
performance of relation extraction with various fea-
ture sets. We followed an additive strategy for fea-
ture selection. Starting with basic features, we added
further features one set at a time. We measured the
performance of the resulting classifier each time we
added a new feature set. Results are shown in Ta-
ble 4. The initial classifier used a tok6+atype
feature set. Addition of both dir and dist fea-
tures give significant improvements in all metrics, of
around 10% F1 overall, in each case. This suggests
that the linear text order of arguments, and whether
relations are intra- or inter-sentential is important to
classification. Addition of the str features also give
good improvement in most metrics, again 10% F1
overall. Addition of part-of-speech information, in
the form of pos features, however, leads to a drop
in some metrics, overall F1 dropping by 1%. Unex-
pectedly, POS seems to provide little extra informa-
tion above that in surface string. Errors in POS tag-
ging cannot be dismissed, and could be the cause of
this. The existence of intervening entities, as coded
in feature set inter, provides a small benefit. The
inclusion of information about events, in the event
feature set, is less clear-cut.
We were interested to see if generalising features
could improve performance, as this had benefited
our previous work in entity extraction. We replaced
all surface string features with their root form, and
POS features with their generalised POS form. This
gave the results shown in column allgen. Results
are not clear cut, in some cases better and in some
worse than the previous best. Overall, there is no
difference in F1. There is a slight increase in over-
all recall, and a corresponding drop in precision ?
as might be expected.
Both the tokN, and the str and pos feature sets
provide surface string and POS information about
tokens surrounding and between relationship argu-
ments. The former gives features from a window
around each argument. The latter two give a greater
amount of positional information. Do these two pro-
vide enough information on their own, without the
windowed features? To test this, we removed the
tokN features from the full cumulative feature set,
from column +event. Results are given in column
15
Relation Metric tok6+atype +dir +dist +str +pos +inter +event allgen notok IAA CIAA
has finding P 44 49 58 63 62 64 65 63 63
R 39 63 78 80 80 81 81 82 82
F1 39 54 66 70 69 71 72 71 71 46 80
has indication P 37 23 38 42 40 41 42 37 44
R 14 14 46 44 44 47 47 45 47
F1 18 16 39 39 38 41 42 38 41 26 50
has location P 36 36 50 68 71 72 72 73 73
R 28 28 74 79 79 81 81 83 83
F1 30 30 58 72 74 76 75 77 76 55 80
has target P 9 9 32 63 57 60 62 60 59
R 11 11 51 68 67 67 66 68 68
F1 9 9 38 64 60 63 63 63 62 42 63
laterality modifies P 21 38 73 84 83 84 84 86 86
R 9 55 82 89 86 88 88 87 89
F1 12 44 76 85 83 84 84 84 85 73 94
negation modifies P 19 54 85 81 80 79 79 77 81
R 12 82 97 98 93 92 93 93 93
F1 13 63 89 88 85 84 85 83 85 66 93
sub location modifies P 2 2 55 88 86 86 88 88 87
R 1 1 62 94 92 95 95 95 95
F1 1 1 56 90 86 89 91 91 90 49 96
Overall P 33 38 50 63 62 64 65 64 64
R 22 36 70 74 73 75 75 76 76
F1 26 37 58 68 67 69 69 69 70 47 75
Table 4: Variation in performance by feature set. Features sets are abbreviated as in Table 3. For the first seven
columns, features were added cumulatively to each other. The next two columns, allgen and notok, are as de-
scribed in Table 3. The final two columns give inter annotator agreement and corrected inter annotator agreement, for
comparison.
notok. There is no clear change in performance,
some relationships improving, and some worsening.
Overall, there is a 1% improvement in F1.
It appears that the bulk of performance is attained
through entity type and distance features, with some
contribution from positional surface string informa-
tion. Performance is between 1% and 9% lower than
CIAA for the same relationship, with a best overall
F1 of 70%, compared to a CIAA of 75%.
6.2 Sentences Spanned
Table 2 shows that although most relationships are
intra-sentential, 23% are inter-sentential, 10% of all
relationships being between arguments in adjacent
sentences. If we consider a relationship to cross n
sentence boundaries, then the classifiers described in
the previous section were all trained on relationships
crossing n ? 1 sentence boundaries, i.e. with argu-
ments in the same or adjacent sentences. What effect
does including more distant relationships have on
performance? We trained classifiers on only intra-
sentential relationships, and on relationships span-
ning up to n sentence boundaries, for n ? {1...5}.
We also trained a classifier on relationships with
1 ? n ? 5, comprising 85% of all inter-sentential
relationships. In each case, the cumulative feature
set +event from Table 4 was used. Results are
shown in Table 5. It is clear from the results that
the feature sets used do not perform well on inter-
sentential relationships. There is a 6% drop in over-
all F1 when including relationships with n = 1 to-
gether with n < 1. Performance continues to drop as
more inter-sentential relationships are included, and
is very poor for just inter-sentential relationships.
A preliminary error analysis suggests that the
more distant relationship arguments are from each
other, the more likely clinical knowledge is required
to extract the relationship. This raises additional dif-
ficulties for extraction, which the simple features de-
scribed here are unable to address.
6.3 Size of Training Corpus
The provision of sufficient training data for super-
vised learning algorithms is a limitation on their use.
We examined the effect of training corpus size on
relationship extraction. The C77 corpus, compris-
16
Number of sentence boundaries between arguments
inter- intra- inter- and intra-sentential Corpus size
Relation Metric 1 ? n ? 5 n < 1 n ? 1 n ? 2 n ? 3 n ? 4 n ? 5 C25 C50 C77
has finding P 24 68 65 62 60 61 61 66 63 65
R 18 89 81 79 78 78 77 74 74 81
F1 18 76 72 69 67 68 67 67 67 72
has indication P 18 49 42 42 36 32 30 22 25 42
R 17 59 47 42 42 39 38 30 31 47
F1 16 51 42 39 37 34 33 23 25 42
has location P 0 74 72 73 72 72 72 72 71 72
R 0 83 81 81 81 82 82 76 80 81
F1 0 77 75 76 75 76 76 73 74 75
has target P 3 64 62 59 60 59 58 65 49 62
R 1 75 66 64 62 61 61 60 65 66
F1 2 68 63 61 60 60 59 59 54 63
laterality modifies P 0 86 84 86 86 86 87 77 78 84
R 0 89 88 88 88 87 88 69 68 88
F1 0 85 84 85 86 85 86 72 69 84
negation modifies P 0 80 79 79 80 80 80 78 79 79
R 0 94 93 91 93 93 93 80 93 93
F1 0 86 85 84 85 86 85 78 84 85
sub location modifies P 0 89 88 88 89 89 89 64 91 88
R 0 95 95 95 95 95 95 64 85 95
F1 0 91 91 91 91 91 91 64 86 91
Overall P 22 69 65 64 62 61 60 62 63 65
R 17 83 75 73 71 70 70 65 71 75
F1 19 75 69 68 66 65 65 63 66 69
Table 5: Variation in performance, by number of sentence boundaries (n), and by training corpus size.
ing 77 narratives and used in the previous experi-
ments, was subsetted to give corpora of 25 and 50
narratives, which will be referred to as C25 and C50
respectively. We trained two further classifiers on
these new corpora. Again, the cumulative feature
set +event from Table 4 was used. Results are
shown in Table 5. Overall, performance improves as
training corpus size increases (F1 rising from 63%
to 69%). We were struck however, by the fact that
increasing from 50 to 77 documents has little effect
on a few relationships (negation modifies and
has location). It may well be that the amount
of training data required has plateaued for those re-
lationships.
7 Conclusion
We have shown that it is possible to extract clini-
cal relationships from text, using shallow features,
and supervised statistical machine learning. Judg-
ing from poor inter annotator agreement, the task
is hard. Our system achieves a reasonable perfor-
mance, with an overall F1 just 5% below a cor-
rected inter annotator agreement. This performance
is reached largely by using features of the text that
encode entity type, distance between arguments, and
some surface string information. Performance does,
however, vary with the number of sentences spanned
by the relationships. Learning inter-sentential rela-
tionships does not seem amenable to this approach,
and may require the use of domain knowledge.
A major concern when using supervised learning
algorithms is the expense and availability of training
data. We have shown that while this concern is jus-
tified in some cases, larger training corpora may not
improve performance for all relationships.
The technology used has proved scalable. The
full CLEF IE system, including automatic entity
recognition, is able to process a document in sub-
second time on a commodity workstation. We
have used the system to extract 6 million relations
from over half a million patient documents, for use
in downstream CLEF applications (Roberts et al,
2008a). Our future work on relationship extrac-
tion in CLEF includes integration of a dependency
parse into the feature set, further analysis to deter-
mine what knowledge may be required to learn inter-
sentential relations, and integration of relationship
extraction with a co-reference algorithm.
17
Availability All of the software described here
is open source and can be downloaded as part of
GATE, with the exception of the entity pairing com-
ponent, which will be released shortly. We are cur-
rently preparing a UK research ethics committee ap-
plication, requesting permission to release our anno-
tated corpus.
Acknowledgements
CLEF is funded by the UK Medical Research Coun-
cil. We would like to thank the Royal Marsden
Hospital for providing the corpus, and our clinical
partners in CLEF for assistance in developing the
schema, and for gold standard annotation.
References
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graphi-
cal development environment for robust NLP tools and
applications. In Proceedings of the 40th Anniversary
Meeting of the Association for Computational Linguis-
tics, pages 168?175, Philadelphia, PA, USA, July.
C. Friedman, P. Alderson, J. Austin, J. Cimino, and
S. Johnson. 1994. A general natural-language text
processor for clinical radiology. Journal of the Amer-
ican Medical Informatics Association, 1(2):161?174,
March.
C. Grover, B. Haddow, E. Klein, M. Matthews,
L. Nielsen, R. Tobin, and X. Wang. 2007. Adapting
a relation extraction pipeline for the BioCreAtIvE II
task. In Proceedings of the BioCreAtIvE II Workshop
2007, Madrid, Spain.
U. Hahn, M. Romacker, and S. Schulz. 2002. MEDSYN-
DIKATE ? a natural language system for the ex-
traction of medical information from findings reports.
International Journal of Medical Informatics, 67(1?
3):63?74, December.
Y. Li, K. Bontcheva, and H. Cunningham. 2005.
SVM based learning system for information extrac-
tion. In Deterministic and statistical methods in ma-
chine learning: first international workshop, number
3635 in Lecture Notes in Computer Science, pages
319?339. Springer.
D. Lindberg, B. Humphreys, and A. McCray. 1993. The
Unified Medical Language System. Methods of Infor-
mation in Medicine, 32(4):281?291.
Y. Lussier, T. Borlawsky, D. Rappaport, Y. Liu, and
C. Friedman. 2006. PhenoGO: Assigning phenotypic
context to Gene Ontology annotations with natural lan-
guage processing. In Biocomputing 2006, Proceed-
ings of the Pacific Symposium, pages 64?75, Hawaii,
USA, January.
S. Pakhomov, J. Buntrock, and P. Duffy. 2005. High
throughput modularized NLP system for clinical text.
In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
interactive poster and demonstration sessions, pages
25?28, Ann Arbor, MI, USA, June.
A. Rector, J. Rogers, A. Taweel, D. Ingram, D. Kalra,
J. Milan, P. Singleton, R. Gaizauskas, M. Hepple,
D. Scott, and R. Power. 2003. CLEF ? joining up
healthcare with clinical and post-genomic research. In
Proceedings of UK e-Science All Hands Meeting 2003,
pages 264?267, Nottingham, UK.
T. Rindflesch and M. Fiszman. 2003. The interaction of
domain knowledge and linguistic structure in natural
language processing: interpreting hypernymic propo-
sitions in biomedical text. Journal of Biomedical In-
formatics, 36(6):462?477.
A. Roberts, R. Gaizauskas, M. Hepple, G. Demetriou,
Y. Guo, A. Setzer, and I. Roberts. 2008a. Seman-
tic annotation of clinical text: The CLEF corpus. In
Proceedings of Building and evaluating resources for
biomedical text mining: workshop at LREC 2008,
Marrakech, Morocco, May. In press.
A. Roberts, R. Gaizauskas, M. Hepple, and Y. Guo.
2008b. Combining terminology resources and statis-
tical methods for entity recognition: an evaluation.
In Proceedings of the Sixth International Conference
on Language Resources and Evaluation, LREC 2008,
Marrakech, Morocco, May. In press.
N. Sager, M. Lyman, C. Bucknall, N. Nhan, and L. Tick.
1994. Natural language processing and the representa-
tion of clinical data. Journal of the American Medical
Informatics Association, 1(2):142?160, March-April.
T. Wang, Y. Li, K. Bontcheva, H. Cunningham, and
J. Wang. 2006. Automatic extraction of hierarchical
relations from text. In The Semantic Web: Research
and Applications. 3rd European Semantic Web Con-
ference, ESWC 2006, number 4011 in Lecture Notes
in Computer Science, pages 215?229. Springer.
G. Zhou, J. Su, J. Zhang, and M. Zhang. 2005. Ex-
ploring Various Knowledge in Relation Extraction. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05), pages
427?434, Ann Arbor, MI, USA, June.
P. Zweigenbaum, B. Bachimont, J. Bouaud, J. Charlet,
and J-F. Boisvieux. 1995. A multi-lingual architec-
ture for building a normalised conceptual representa-
tion from medical language. In Proceedings of the An-
nual Symposium on Computer Applications in Medical
Care, pages 357?361, New York, NY, USA.
18
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 80?87,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Knowledge Sources for Word Sense
Disambiguation of Biomedical Text
Mark Stevenson, Yikun Guo
and Robert Gaizauskas
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello Street
Sheffield, S1 4DP
United Kingdom
{inital.surname}@dcs.shef.ac.uk
David Martinez
Department of Computer Science
& Software Engineering
University of Melbourne
Victoria 3010
Australia
davidm@csse.unimelb.edu.au
Abstract
Like text in other domains, biomedical doc-
uments contain a range of terms with more
than one possible meaning. These ambigu-
ities form a significant obstacle to the auto-
matic processing of biomedical texts. Previ-
ous approaches to resolving this problem have
made use of a variety of knowledge sources in-
cluding linguistic information (from the con-
text in which the ambiguous term is used) and
domain-specific resources (such as UMLS). In
this paper we compare a range of knowledge
sources which have been previously used and
introduce a novel one: MeSH terms. The best
performance is obtained using linguistic fea-
tures in combination with MeSH terms. Re-
sults from our system outperform published
results for previously reported systems on a
standard test set (the NLM-WSD corpus).
1 Introduction
The number of documents discussing biomedical
science is growing at an ever increasing rate, making
it difficult to keep track of recent developments. Au-
tomated methods for cataloging, searching and nav-
igating these documents would be of great benefit
to researchers working in this area, as well as hav-
ing potential benefits to medicine and other branches
of science. Lexical ambiguity, the linguistic phe-
nomena where a word or phrase has more than
one potential meaning, makes the automatic pro-
cessing of text difficult. For example, ?cold? has
six possible meanings in the Unified Medical Lan-
guage System (UMLS) Metathesaurus (Humphreys
et al, 1998) including ?common cold?, ?cold sen-
sation? and ?Chronic Obstructive Airway Disease
(COLD)?. The NLM Indexing Initiative (Aronson et
al., 2000) attempted to automatically index biomedi-
cal journals with concepts from the UMLS Metathe-
saurus and concluded that lexical ambiguity was the
biggest challenge in the automation of the indexing
process. Weeber et al (2001) analysed MEDLINE
abstracts and found that 11.7% of phrases were am-
biguous relative to the UMLS Metathesaurus.
Word Sense Disambiguation (WSD) is the pro-
cess of resolving lexical ambiguities. Previous re-
searchers have used a variety of approaches for
WSD of biomedical text. Some of them have taken
techniques proven to be effective for WSD of gen-
eral text and applied them to ambiguities in the
biomedical domain, while others have created sys-
tems using domain-specific biomedical resources.
However, there has been no direct comparison of
which knowledge sources are the most useful or
whether combining a variety of knowledge sources,
a strategy which has been shown to be successful for
WSD in the general domain (Stevenson and Wilks,
2001), improves results.
This paper compares the effectiveness of a vari-
ety of knowledge sources for WSD in the biomed-
ical domain. These include features which have
been commonly used for WSD of general text as
well as information derived from domain-specific
resources. One of these features is MeSH terms,
which we find to be particularly effective when com-
bined with generic features.
The next section provides an overview of various
approaches to WSD in the biomedical domain. Sec-
80
tion 3 outlines our approach, paying particular atten-
tion to the range of knowledge sources used by our
system. An evaluation of this system is presented
in Section 4. Section 5 summarises this paper and
provides suggestions for future work.
2 Previous Work
WSD has been actively researched since the 1950s
and is regarded as an important part of the process
of understanding natural language texts.
2.1 The NLM-WSD data set
Research on WSD for general text in the last decade
has been driven by the SemEval evaluation frame-
works1 which provide a set of standard evaluation
materials for a variety of semantic evaluation tasks.
At this point there is no specific collection for the
biomedical domain in SemEval, but a test collection
for WSD in biomedicine was developed by Wee-
ber et al (2001), and has been used as a benchmark
by many independent groups. The UMLS Metathe-
saurus was used to provide a set of possible mean-
ings for terms in biomedical text. 50 ambiguous
terms which occur frequently in MEDLINE were
chosen for inclusion in the test set. 100 instances
of each term were selected from citations added to
the MEDLINE database in 1998 and manually dis-
ambiguated by 11 annotators. Twelve terms were
flagged as ?problematic? due to substantial disagree-
ment between the annotators. There are an average
of 2.64 possible meanings per ambiguous term and
the most ambiguous term, ?cold? has five possible
meanings. In addition to the meanings defined in
UMLS, annotators had the option of assigning a spe-
cial tag (?none?) when none of the UMLS meanings
seemed appropriate.
Various researchers have chosen to evaluate their
systems against subsets of this data set. Liu et al
(2004) excluded the 12 terms identified as problem-
atic by Weeber et al (2001) in addition to 16 for
which the majority (most frequent) sense accounted
for more than 90% of the instances, leaving 22 terms
against which their system was evaluated. Leroy and
Rindflesch (2005) used a set of 15 terms for which
the majority sense accounted for less than 65% of
the instances. Joshi et al (2005) evaluated against
1http://www.senseval.org
the set union of those two sets, providing 28 am-
biguous terms. McInnes et al (2007) used the set
intersection of the two sets (dubbed the ?common
subset?) which contained 9 terms. The terms which
form these various subsets are shown in Figure 1.
The 50 terms which form the NLM-WSD data set
represent a range of challenges for WSD systems.
The Most Frequent Sense (MFS) heuristic has be-
come a standard baseline in WSD (McCarthy et al,
2004) and is simply the accuracy which would be
obtained by assigning the most common meaning of
a term to all of its instances in a corpus. Despite its
simplicity, the MFS heuristic is a hard baseline to
beat, particularly for unsupervised systems, because
it uses hand-tagged data to determine which sense
is the most frequent. Analysis of the NLM-WSD
data set showed that the MFS over all 50 ambigu-
ous terms is 78%. The different subsets have lower
MFS, indicating that the terms they contain are more
difficult to disambiguate. The 22 terms used by (Liu
et al, 2004) have a MFS of 69.9% while the set
used by (Leroy and Rindflesch, 2005) has an MFS
of 55.3%. The union and intersection of these sets
have MFS of 66.9% and 54.9% respectively.
adjustment
blood pressure
evaluation
immunosuppression
radiation
sensitivity
degree
growth
man
mosaic
nutrition
cold
depression
discharge
extraction
fat
implantation
association
condition
culture
determination
energy
failure
fit
fluid
frequency
ganglion
glucose
inhibition 
pressure 
resistance
secretion
single
strains
support
surgery
transient
transport
variation
repair
scale
weight
white
japanese
lead
mole
pathology
reduction
sex
ultrasound
NLM-WSD data set
Liu et. al. (2004)
Leroy and Rindflesch (2005)
Figure 1: The NLM-WSD test set and some of its sub-
sets. Note that the test set used by (Joshi et al, 2005)
comprises the set union of the terms used by (Liu et al,
2004) and (Leroy and Rindflesch, 2005) while the ?com-
mon subset? is formed from their intersection.
2.2 WSD of Biomedical Text
A standard approach to WSD is to make use of
supervised machine learning systems which are
trained on examples of ambiguous words in con-
text along with the correct sense for that usage. The
81
models created are then applied to new examples of
that word to determine the sense being used.
Approaches which are adapted from WSD of gen-
eral text include Liu et al (2004). Their technique
uses a supervised learning algorithm with a vari-
ety of features consisting of a range of collocations
of the ambiguous word and all words in the ab-
stract. They compared a variety of supervised ma-
chine learning algorithms and found that a decision
list worked best. Their best system correctly dis-
ambiguated 78% the occurrences of 22 ambiguous
terms in the NLM-WSD data set (see Section 2.1).
Joshi et al (2005) also use collocations as features
and experimented with five supervised learning al-
gorithms: Support Vector Machines, Naive Bayes,
decision trees, decision lists and boosting. The Sup-
port Vector Machine performed scoring 82.5% on
a set of 28 words (see Section 2.1) and 84.9% on
the 22 terms used by Liu et al (2004). Performance
of the Naive Bayes classifier was comparable to the
Support Vector Machine, while the other algorithms
were hampered by the large number of features.
Examples of approaches which have made use of
knowledge sources specific to the biomedical do-
main include Leroy and Rindflesch (2005), who re-
lied on information from the UMLS Metathesaurus
assigned by MetaMap (Aronson, 2001). Their sys-
tem used information about whether the ambigu-
ous word is the head word of a phrase identified by
MetaMap, the ambiguous word?s part of speech, se-
mantic relations between the ambiguous words and
surrounding words from UMLS as well as semantic
types of the ambiguous word and surrounding word.
Naive Bayes was used as a learning algorithm. This
approach correctly disambiguated 65.6% of word in-
stances from a set of 15 terms (see Section 2.1).
Humphrey et al (2006) presented an unsupervised
system that also used semantic types. They con-
structed semantic type vectors for each word from
a large collection of MEDLINE abstracts. This al-
lowed their method to perform disambiguation at a
coarser level, without the need for labeled training
examples. In most cases the semantic types can be
mapped to the UMLS concepts but not for five of the
terms in the NLM-WSD data set. Humphrey et al
(2006) reported 78.6% accuracy over the remaining
45. However, their approach could not be applied
to all instances of ambiguous terms and, in particu-
lar, is unable to model the ?none? tag. Their system
could only assign senses to an average of 54% of the
instances of each ambiguous term.
McInnes et al (2007) made use of Concept
Unique Identifiers (CUIs) from UMLS which are
also assigned by MetaMap. The information con-
tained in CUIs is more specific than in the semantic
types applied by Leroy and Rindflesch (2005). For
example, there are two CUIs for the term ?culture?
in UMLS: ?C0010453: Anthropological Culture?
and ?C0430400: Laboratory Culture?. The seman-
tic type for the first of these is ?Idea or Concept? and
?Laboratory Procedure? for the second. McInnes et
al. (2007) were interested in exploring whether the
more specific information contained in CUIs was
more effective than UMLS semantic types. Their
best result was reported for a system which repre-
sented each sense by all CUIs which occurred at
least twice in the abstract surrounding the ambigu-
ous word. They used a Naive Bayes classifier as the
learning algorithm. McInnes et al (2007) reported
an accuracy of 74.5% on the set of ambiguous terms
tested by Leroy and Rindflesch (2005) and 80.0% on
the set used by Joshi et al (2005). They concluded
that CUIs are more useful for WSD than UMLS se-
mantic types but that they are not as robust as fea-
tures which are known to work in general English,
such as unigrams and bigrams.
3 Approach
Our approach is to adapt a state-of-the-art WSD sys-
tem to the biomedical domain by augmenting it with
additional domain-specific and domain-independent
knowledge sources. Our basic system (Agirre and
Mart??nez, 2004) participated in the Senseval-3 chal-
lenge (Mihalcea et al, 2004) with a performance
close to the best system for the English and Basque
lexical sample tasks. The system is based on a su-
pervised learning approach. The features used by
Agirre and Mart??nez (2004) are derived from text
around the ambiguous word and are domain inde-
pendent. We refer to these as linguistic features.
This feature set has been adapted for the disam-
biguation of biomedical text by adding further lin-
guistic features and two different types of domain-
specific features: CUIs (as used by (McInnes et al,
2007)) and Medical Subject Heading (MeSH) terms.
82
3.1 Features
Our feature set contains a number of parameters
which were set empirically (e.g. threshold for un-
igram frequency in the linguistic features). In addi-
tion, we use the entire abstract as the context of the
ambiguous term for relevant features rather than just
the sentence containing the term. Effects of varying
these parameters are consistent with previous results
(Liu et al, 2004; Joshi et al, 2005; McInnes et al,
2007) and are not reported in this paper.
Linguistic features: The system uses a wide
range of domain-independent features which are
commonly used for WSD.
? Local collocations: A total of 41 features which
extensively describe the context of the am-
biguous word and fall into two main types:
(1) bigrams and trigrams containing the am-
biguous word constructed from lemmas, word
forms or PoS tags2 and (2) preceding/following
lemma/word-form of the content words (adjec-
tive, adverb, noun and verb) in the same sen-
tence with the target word. For example, con-
sider the sentence below with the target word
adjustment.
?Body surface area adjustments of
initial heparin dosing...?
The features would include the following: left-
content-word-lemma ?area adjustment?, right-
function-word-lemma ?adjustment of ?, left-
POS ?NN NNS?, right-POS ?NNS IN?, left-
content-word-form ?area adjustments?, right-
function-word-form ?adjustment of ?, etc.
? Syntactic Dependencies: These features model
longer-distance dependencies of the ambigu-
ous words than can be represented by the lo-
cal collocations. Five relations are extracted:
object, subject, noun-modifier, preposition and
sibling. These are identified using heuristic pat-
terns and regular expressions applied to PoS tag
sequences around the ambiguous word. In the
above example, ?heparin? is noun-modifier fea-
ture of ?adjustment?.
2A maximum-entropy-based part of speech tagger was used
(Ratnaparkhi, 1996) without the adaptation to the biomedical
domain.
? Salient bigrams: Salient bigrams within the ab-
stract with high log-likelihood scores, as de-
scribed by Pedersen (2001).
? Unigrams: Lemmas of unigrams which appear
more frequently than a predefined threshold in
the entire corpus, excluding those in a list of
stopwords. We empirically set the threshold
to 1. This feature was not used by Agirre and
Mart??nez (2004), but Joshi et al (2005) found
them to be useful for this task.
Concept Unique Identifiers (CUIs): We follow
the approach presented by McInnes et al (2007) to
generate features based on UMLS Concept Unique
Identifiers (CUIs). The MetaMap program (Aron-
son, 2001) identifies all words and terms in a
text which could be mapped onto a UMLS CUI.
MetaMap does not disambiguate the senses of the
concepts, instead it enumerates all the possible com-
binations of the concept names found. For exam-
ple, MetaMap will segment the phrase ?Body sur-
face area adjustments of initial heparin dosing ...?
into two chunks: ?Body surface area adjustments?
and ?of initial heparin dosing?. The first chunk
will be mapped onto four CUIs with the concept
name ?Body Surface Area?: ?C0005902: Diag-
nostic Procedure? and ?C1261466: Organism At-
tribute? and a further pair with the name ?Adjust-
ments?: ?C0456081: Health Care Activity? and
?C0871291: Individual Adjustment?. The final re-
sults from MetaMap for the first chunk will be eight
combinations of those concept names, e.g. first four
by second two concept names. CUIs which occur
more than three times in the abstract containing the
ambiguous word are included as features.
Medical Subject Headings (MeSH): The fi-
nal feature is also specific to the biomedical do-
main. Medical Subject Headings (MeSH) (Nelson
et al, 2002) is a controlled vocabulary for index-
ing biomedical and health-related information and
documents. MeSH terms are manually assigned to
abstracts by human indexers. The latest version of
MeSH contains over 24,000 terms organised into an
11 level hierarchy.
The terms assigned to the abstract in which
each ambiguous word occurs are used as fea-
tures. For example, the abstract containing our
example phrase has been assigned 16 MeSH
83
terms including ?M01.060.116.100: Aged?,
?M01.060.116.100.080: Aged, 80 and over?,
?D27.505.954.502.119: Anticoagulants? and
?G09.188.261.560.150: Blood Coagulation?. To
our knowledge MeSH terms have not been pre-
viously used as a feature for WSD of biomedical
documents.
3.2 Learning Algorithms
We compared three machine leaning algorithms
which have previously been shown to be effective
for WSD tasks.
The Vector Space Model is a memory-based
learning algorithm which was used by (Agirre and
Mart??nez, 2004). Each occurrence of an ambiguous
word is represented as a binary vector in which each
position indicates the occurrence/absence of a fea-
ture. A single centroid vector is generated for each
sense during training. These centroids are compared
with the vectors that represent new examples using
the cosine metric to compute similarity. The sense
assigned to a new example is that of the closest cen-
troid.
The Naive Bayes classifier is based on a proba-
bilistic model which assumes conditional indepen-
dence of features given the target classification. It
calculates the posterior probability that an instance
belongs to a particular class given the prior proba-
bilities of the class and the conditional probability
of each feature given the target class.
Support Vector Machines have been widely
used in classification tasks. SVMs map feature vec-
tors onto a high dimensional space and construct a
classifier by searching for the hyperplane that gives
the greatest separation between the classes.
We used our own implementation of the Vector
Space Model and Weka implementations (Witten
and Frank, 2005) of the other two algorithms.
4 Results
This system was applied to the NLM-WSD data set.
Experiments were carried out using each of the three
types of features (linguistic, CUI and MeSH) both
alone and in combination. Ten-fold cross valida-
tion was used, and the figures we report are averaged
across all ten runs.
Results from this experiment are shown in Table
1 which lists the performance using combinations of
learning algorithm and features. The figure shown
for each configuration represents the percentage of
instances of ambiguous terms which are correctly
disambiguated.
These results show that each of the three types
of knowledge (linguistic, CUIs and MeSH) can be
used to create a classifier which achieves a reason-
able level of disambiguation since performance ex-
ceeds the relevant baseline score. This suggests that
each of the knowledge sources can contribute to the
disambiguation of ambiguous terms in biomedical
text.
The best performance is obtained using a combi-
nation of the linguistic and MeSH features, a pattern
observed across all test sets and machine learning
algorithms. Although the increase in performance
gained from using both the linguistic and MeSH
features compared to only the linguistic features is
modest it is statistically significant, as is the differ-
ence between using both linguistic and MeSH fea-
tures compared with using the MeSH features alone
(Wilcoxon Signed Ranks Test, p < 0.01).
Combining MeSH terms with other features gen-
erally improves performance, suggesting that the
information contained in MeSH terms is distinct
from the other knowledge sources. However, the
inclusion of CUIs as features does not always im-
prove performance and, in several cases, causes it to
fall. This is consistent with McInnes et al (2007)
who concluded that CUIs were a useful informa-
tion source for disambiguation of biomedical text
but that they were not as robust as a linguistic knowl-
edge source (unigrams) which they had used for a
previous system. The most likely reason for this is
that our approach relies on automatically assigned
CUIs, provided by MetaMap, while the MeSH terms
are assigned manually. We do not have access to a
reliable assignment of CUIs to text; if we had WSD
would not be necessary. On the other hand, reli-
ably assigned MeSH terms are readily available in
Medline. The CUIs assigned by MetaMap are noisy
while the MeSH terms are more reliable and prove
to be a more useful knowledge source for WSD.
The Vector Space Model learning algorithm per-
forms significantly better than both Support Vector
Machine and Naive Bayes (Wilcoxon Signed Ranks
Test, p < 0.01). This pattern is observed regardless
84
Features
CUI+ Linguistic Linguistic Linguistic+Data sets Linguistic CUI MeSH
MeSH +MeSH +CUI MeSH+CUI
Vector space model
All words 87.2 85.8 81.9 86.9 87.8 87.3 87.6
Joshi subset 82.3 79.6 76.6 81.4 83.3 82.4 82.6
Leroy subset 77.8 74.4 70.4 75.8 79.0 78.0 77.8
Liu subset 84.3 81.3 78.3 83.4 85.1 84.3 84.5
Common subset 79.6 75.1 70.4 76.9 80.8 79.6 79.2
Naive Bayes
All words 86.2 81.2 85.7 81.1 86.4 81.4 81.5
Joshi subset 80.6 73.4 80.1 73.3 80.9 73.7 73.8
Leroy subset 76.4 66.1 74.6 65.9 76.8 66.3 66.3
Liu subset 81.9 75.4 81.7 75.3 82.2 75.5 75.6
Common subset 76.7 66.1 74.7 65.8 77.2 65.9 65.9
Support Vector Machine
All words 85.6 83.5 85.3 84.5 86.1 85.3 85.6
Joshi subset 79.8 76.4 79.5 78.0 80.6 79.1 79.8
Leroy subset 75.1 69.7 72.6 72.0 76.3 74.2 74.9
Liu subset 81.3 78.2 81.0 80.0 82.0 80.6 81.2
Common subset 75.7 69.8 71.6 73.0 76.8 74.7 75.2
Previous Approaches
MFS Liu et. al. Leroy and Joshi et. McInnes et.
baseline (2004) Rindflesch (2005) al. (2005) al. (2007)
All words 78.0 ? ? ? 85.3
Joshi subset 66.9 ? ? 82.5 80.0
Leroy subset 55.3 ? 65.5 77.4 74.5
Liu subset 69.9 78.0 ? 84.9 82.0
Common subset 54.9 ? 68.8 79.8 75.7
Table 1: Results from WSD system applied to various sections of the NLM-WSD data set using a variety of fea-
tures and machine learning algorithms. Results from baseline and previously published approaches are included for
comparison.
of which set of features are used, and it is consis-
tent of the results in Senseval data from (Agirre and
Mart??nez, 2004).
4.1 Per-Word Analysis
Table 2 shows the results of our best performing sys-
tem (combination of linguistic and MeSH features
using the Vector Space Model learning algorithm).
Comparable results for previous supervised systems
are also reported where available.3 The MFS base-
line for each term is shown in the leftmost column.
The performance of Leroy and Rindflesch?s sys-
3It is not possible to directly compare our results with Liu
et al (2004) or Humphrey et al (2006). The first report only
optimal configuration for each term (combination of feature sets
and learning algorithm) while the second do not assign senses
to all of the instances of each ambiguous term (see Section 2).
tem is always lower than the best result for each
word. The systems reported by Joshi et al (2005)
and McInnes et al (2007) are better than, or the
same as, all other systems for 14 and 12 words re-
spectively. The system reported here achieves re-
sults equal to or better than previously reported sys-
tems for 33 terms.
There are seven terms for which the performance
of our approach is actually lower than the MFS base-
line (shown in italics) in Table 2. (In fact, the base-
line outperforms all systems for four of these terms.)
The performance of our system is within 1% of the
baseline for five of these terms. The remaining pair,
?blood pressure? and ?failure?, are included in the
set of problematic words identified by (Weeber et
al., 2001). Examination of the possible senses show
that they include pairs with similar meanings. For
85
MFS Leroy and Joshi et. McInnes et. Reported
baseline Rindflesch (2005) al. (2005) al. (2007) system
adjustment 62 57 71 70 74
association 100 - - 97 100
blood pressure 54 46 53 46 46
cold 86 - 90 89 88
condition 90 - - 89 89
culture 89 - - 94 95
degree 63 68 89 79 95
depression 85 - 86 81 88
determination 79 - - 81 87
discharge 74 - 95 96 95
energy 99 - - 99 98
evaluation 50 57 69 73 81
extraction 82 - 84 86 85
failure 71 - - 73 67
fat 71 - 84 77 84
fit 82 - - 87 88
fluid 100 - - 99 100
frequency 94 - - 94 94
ganglion 93 - - 94 96
glucose 91 - - 90 91
growth 63 62 71 69 68
immunosuppression 59 61 80 75 80
implantation 81 - 94 92 93
inhibition 98 - - 98 98
japanese 73 - 77 76 75
lead 71 - 89 90 94
man 58 80 89 80 90
mole 83 - 95 87 93
mosaic 52 66 87 75 87
nutrition 45 48 52 49 54
pathology 85 - 85 84 85
pressure 96 - - 93 95
radiation 61 72 82 81 84
reduction 89 - 91 92 89
repair 52 81 87 93 88
resistance 97 - - 96 98
scale 65 84 81 83 88
secretion 99 - - 99 99
sensitivity 49 70 88 92 93
sex 80 - 88 87 87
single 99 - - 98 99
strains 92 - - 92 93
support 90 - - 91 89
surgery 98 - - 94 97
transient 99 - - 98 99
transport 93 - - 93 93
ultrasound 84 - 92 85 90
variation 80 - - 91 95
weight 47 68 83 79 81
white 49 62 79 74 76
Table 2: Per-word performance of best reported systems.
example, the two senses which account for 98% of
the instances of ?blood pressure?, which refer to the
blood pressure within an organism and the result ob-
tained from measuring this quantity, are very closely
related semantically.
5 Conclusion
This paper has compared a variety of knowledge
sources for WSD of ambiguous biomedical terms
and reported results which exceed the performance
of previously published approaches. We found that
accurate results can be achieved using a combina-
tion of linguistic features commonly used for WSD
86
of general text and manually assigned MeSH terms.
While CUIs are a useful source of information for
disambiguation, they do not improve the perfor-
mance of other features when used in combination
with them. Our approach uses manually assigned
MeSH terms while the CUIs are obtained automati-
cally using MetaMap.
The linguistic knowledge sources used in this pa-
per comprise a wide variety of features including
n-grams and syntactic dependencies. We have not
explored the effectiveness of these individually and
this is a topic for further work.
In addition, our approach does not make use of
the fact that MeSH terms are organised into a hierar-
chy. It would be interesting to discover whether this
information could be used to improve WSD perfor-
mance. Others have developed techniques to make
use of hierarchical information in WordNet for WSD
(see Budanitsky and Hirst (2006)) which could be
adapted to MeSH.
References
E. Agirre and D. Mart??nez. 2004. The Basque Coun-
try University system: English and Basque tasks. In
Rada Mihalcea and Phil Edmonds, editors, Senseval-
3: Third International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text, pages 44?
48, Barcelona, Spain, July.
A. Aronson, O. Bodenreider, H. Chang, S. Humphrey,
J. Mork, S. Nelson, T. Rindflesch, and W. Wilbur.
2000. The NLM Indexing Initiative. In Proceedings
of the AMIA Symposium.
A. Aronson. 2001. Effective mapping of biomedical text
to the UMLS Metathesaurus: the MetaMap program.
In Proceedings of the American Medical Informatics
Association (AMIA), pages 17?21.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of semantic distance. Computational
Linguistics, 32(1):13?47.
S. Humphrey, W. Rogers, H. Kilicoglu, D. Demner-
Fushman, and T. Rindflesch. 2006. Word Sense Dis-
ambiguation by selecting the best semantic type based
on Journal Descriptor Indexing: Preliminary experi-
ment. Journal of the American Society for Information
Science and Technology, 57(5):96?113.
L. Humphreys, D. Lindberg, H. Schoolman, and G. Bar-
nett. 1998. The Unified Medical Language System:
An Informatics Research Collaboration. Journal of the
American Medical Informatics Association, 1(5):1?11.
M. Joshi, T. Pedersen, and R. Maclin. 2005. A Compara-
tive Study of Support Vector Machines Applied to the
Word Sense Disambiguation Problem for the Medical
Domain. In Proceedings of the Second Indian Confer-
ence on Artificial Intelligence (IICAI-05), pages 3449?
3468, Pune, India.
G. Leroy and T. Rindflesch. 2005. Effects of Information
and Machine Learning algorithms on Word Sense Dis-
ambiguation with small datasets. International Jour-
nal of Medical Informatics, 74(7-8):573?585.
H. Liu, V. Teller, and C. Friedman. 2004. A Multi-aspect
Comparison Study of Supervised Word Sense Disam-
biguation. Journal of the American Medical Informat-
ics Association, 11(4):320?331.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004.
Finding predominant senses in untagged text. In Pro-
ceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Lingusitics (ACL-2004), pages
280?287, Barcelona, Spain.
B. McInnes, T. Pedersen, and J. Carlis. 2007. Using
UMLS Concept Unique Identifiers (CUIs) for Word
Sense Disambiguation in the Biomedical Domain. In
Proceedings of the Annual Symposium of the Ameri-
can Medical Informatics Association, pages 533?537,
Chicago, IL.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004. The
Senseval-3 English lexical sample task. In Proceed-
ings of Senseval-3: The Third International Workshop
on the Evaluation of Systems for the Semantic Analysis
of Text, Barcelona, Spain.
S. Nelson, T. Powell, and B. Humphreys. 2002. The
Unified Medical Language System (UMLS) Project.
In Allen Kent and Carolyn M. Hall, editors, Ency-
clopedia of Library and Information Science. Marcel
Dekker, Inc.
T. Pedersen. 2001. A Decision Tree of Bigrams is an
Accurate Predictor of Word Sense. In Proceedings
of the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-01), pages 79?86, Pittsburgh, PA., June.
A. Ratnaparkhi. 1996. A Maximum Entropy Model for
Part-of-Speech Tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 133?142.
M. Stevenson and Y. Wilks. 2001. The Interaction of
Knowledge Sources in Word Sense Disambiguation.
Computational Linguistics, 27(3):321?350.
M. Weeber, J. Mork, and A. Aronson. 2001. Developing
a Test Collection for Biomedical Word Sense Disam-
biguation. In Proceedings of AMAI Symposium, pages
746?50, Washington, DC.
I. Witten and E. Frank. 2005. Data Mining: Practical
machine learning tools and techniques. Morgan Kauf-
mann, San Francisco.
87
Proceedings of the Workshop on BioNLP, pages 71?79,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Disambiguation of Biomedical Abbreviations
Mark Stevenson1, Yikun Guo2, Abdulaziz Al Amri3 and Robert Gaizauskas4
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield, S1 4DP
United Kingdom
1,2,4{initial.surname}@dcs.shef.ac.uk, 3abdulazizmail@gmail.com
Abstract
Abbreviations are common in biomedical doc-
uments and many are ambiguous in the sense
that they have several potential expansions.
Identifying the correct expansion is necessary
for language understanding and important for
applications such as document retrieval. Iden-
tifying the correct expansion can be viewed as
a Word Sense Disambiguation (WSD) prob-
lem. A WSD system that uses a variety of
knowledge sources, including two types of in-
formation specific to the biomedical domain,
is also described. This system was tested on a
corpus of ambiguous abbreviations, created by
automatically identifying the correct expan-
sion in Medline abstracts, and found to iden-
tify the correct expansion with up to 99% ac-
curacy.
1 Introduction
Many abbreviations are ambiguous in the sense that
they have more than one possible expansion. For
example, expansions for ?NLP? include ?Neuro-
linguistic Programming? as well as ?Natural Lan-
guage Processing?. Ambiguous abbreviations form
a challenge to language understanding since iden-
tification of the correct expansion is often impor-
tant. The query ?NLP?, for example, returns pages
which refer to ?Neuro-linguistic programming? for
most web search engines, pages which are of lim-
ited value to those interested in Natural Language
Processing. In some cases this problem could be
obviated by altering the query terms, for example
including ?Natural?, ?Language? and ?Processing?.
However, this will not help when the abbreviation?s
expansion does not occur within the document. Fred
and Cheng (1999) point out that this is often the case
in biomedical documents, in this domain ubiquitous
abbreviations (such as DNA and mRNA) often ap-
pear without an expansion.
It has been reported that misinterpretation of ab-
breviations in biomedical documents has lead to
medical practitioners making fatal errors (Fred and
Cheng, 1999). However, identifying the correct ex-
pansion is not a straightforward task since an ab-
breviation may have several possible expansions.
Chang et al (2002) reported that abbreviations in
biomedical journal articles consisting of six charac-
ters or less have an average of 4.61 possible mean-
ings and Pustejovsky et al (2002) mention that the
simple abbreviation ?AC? is associated with at least
10 strings in different biomedical documents includ-
ing ?atrioventricular connection?, ?anterior colpor-
rhaphy procedure?, ?auditory cortex? and ?atypical
carcinoid?.
The problem of identifying the correct expansion
of an ambiguous abbreviation can be viewed as a
Word Sense Disambiguation (WSD) task where the
various expansions are the ?senses? of the abbrevia-
tion. In this paper we approach the problem in this
way by applying a WSD system which has previ-
ously been applied to biomedical text (Stevenson et
al., 2008). The WSD system uses a variety of infor-
mation sources, including those traditionally applied
to the WSD problem in addition to two knowledge
sources that are specific to the biomedical domain.
Evaluation of systems for disambiguating am-
biguous abbreviations has been hindered by the fact
71
that there is no freely available benchmark corpus
against which approaches can be compared. We de-
scribe a process whereby such a corpus can be cre-
ated by automatically mining abstracts from Med-
line. This corpus is being made publicly available
to encourage comparative research in this area. Our
abbreviation disambiguation system was evaluated
against this corpus and found to identify the correct
abbreviation with up to 99% accuracy.
The remainder of this paper is organised as fol-
lows. The next section describes relevant previous
work on disambiguation of abbreviations. Section
3 describes a supervised learning WSD system tai-
lored specifically to the biomedical domain. Section
4 describes the automatic creation of a corpus of am-
biguous abbreviations designed specifically for the
training and evaluation of abbreviation disambigua-
tion systems. Section 5 describes the evaluation of
our system on this corpus. Our conclusions are pre-
sented in Section 6.
2 Previous Work
Gaudan et al (2005) distinguish two types of abbre-
viations: global and local. Global abbreviations are
those found in documents without the expansion ex-
plicitly stated, while local abbreviations are defined
in the same document in which the abbreviation oc-
curs. Our work is concerned with the problem of
disambiguating global abbreviations. Gaudan et al
(2005) point out that global abbreviations are often
ambiguous.
Various researchers have explored the problem
of disambiguating global abbreviations in biomed-
ical documents. Liu et al (2001)(2002) used sev-
eral domain-specific knowledge sources to identify
terms which are semantically related to each possi-
ble expansion but which have only one sense them-
selves. Instances of these terms were identified in
a corpus of biomedical journal abstracts and used
as training data. Their learning algorithm uses a
variety of features including all words in the ab-
stract and collocations of the ambiguous abbrevia-
tion. They report an accuracy of 97% on a small set
of abbreviations. Liu et al (2004) present a fully
supervised approach. They compared a variety of
supervised machine learning algorithms and found
that the best performance over a set of 15 ambigu-
ous abbreviations, 98.6%, was obtained using Naive
Bayes. Gaudan et al (2005) use a Support Vector
Machine trained on a bag-of-words model and re-
port an accuracy of 98.5%. Yu et al (2006) exper-
imented with two supervised learning algorithms:
Naive Bayes and Support Vector Machines. They
extracted a corpus containing examples of 60 ab-
breviations from a set of biomedical journal articles
which was split so that abstracts in which the abbre-
viations were defined were used as training data and
those in which no definition is found as test data.
Abbreviations in the test portion were manually dis-
ambiguated. They report 79% coverage and 80%
precision using a Naive Bayes classifier. Pakho-
mov (2002) applied a maximum entropy model to
identify the meanings of ambiguous abbreviations in
10,000 rheumatology notes with around 89% accu-
racy. Joshi et al (2006) disambiguated abbreviations
in clinical notes using three supervised learning al-
gorithms (Naive Bayes, decision trees and Support
Vector Machines). They used a range of features and
found that the best performance was obtained when
these were combined. Unfortunately direct compari-
son of these methods is made difficult by the fact that
various researchers have evaluated their approaches
on different data sets.
A variety of approaches have also been proposed
for the problem of disambiguating local abbrevia-
tions in biomedical documents. This task is equiv-
alent to identifying the abbreviation?s expansion in
the document. The problem is relatively straight-
forward for abbreviations which are created by se-
lecting the first character from each word in the ex-
pansion, such as ?angiotensin converting enzyme
(ACE)?, but is more difficult when this convention
is not followed, for example ?acetylchlinesterase
(ACE)?, ?antisocial personality (ASP)? and ?cata-
lase (CAT)?. Okazaki et al (2008) recently pro-
posed an approach to this problem based on dis-
criminative alignment that has been shown to per-
form well. However, the most common solutions
are based on heuristic approaches, for example
Adar (2004) and Zhou et al (2006). Pustejovsky
et al (2002) used hand-built regular expressions.
Schwartz and Hearst (2003) describe an approach
which starts by identifying the set of candidate ex-
pansions in the same sentence as an abbreviation.
The most likely one is identified by searching for the
72
shortest candidate which contains all the characters
in the abbreviation in the correct order.
3 Abbreviation Disambiguation System
Our abbreviation disambiguation system is based on
a state-of-the-art WSD system that has been adapted
to the biomedical domain by augmenting it with ad-
ditional knowledge sources. The system on which
our approach is based (Agirre and Mart??nez, 2004)
participated in the Senseval-3 challenge (Mihalcea
et al, 2004) with a performance close to the best
system for the lexical sample tasks in two languages
while the version adapted to the biomedical domain
has achieved the best recorded results (Stevenson et
al., 2008) on a standard test set consisting of am-
biguous terms (Weeber et al, 2001).
This system is based on a supervised learning ap-
proach with features derived from text around the
ambiguous word that are domain independent. We
refer to these as general features. This feature set
has been adapted for the disambiguation of biomed-
ical text by adding further linguistic features and two
different types of domain-specific features: CUIs (as
used by McInnes et al (2007)) and Medical Sub-
ject Heading (MeSH) terms. This set of features is
more diverse than have been explored by previous
approaches to abbreviation disambiguation.
3.1 Features
Our feature set contains a number of parameters
(e.g. thresholds for unigram and CUI frequencies).
These parameters were set to the same values that
were used when the system was applied to gen-
eral biomedical terms (Stevenson et al, 2008) since
these were found to perform well. We also use the
entire abstract as the context of the ambiguous term
for relevant features rather than just the sentence
containing the term. Effects of altering these vari-
ables are consistent with previous results (Liu et al,
2004; Joshi et al, 2005; McInnes et al, 2007) and
are not reported here.
General features: The system uses a wide range
of domain-independent features that are commonly
employed for WSD.
? Local collocations: A total of 41 features which
extensively describe the context of the am-
biguous word and fall into two main types:
(1) bigrams and trigrams containing the am-
biguous word constructed from lemmas, word
forms or PoS tags and (2) preceding/following
lemma/word-form of the content words (adjec-
tive, adverb, noun and verb) in the same sen-
tence as the ambiguous abbreviation. For ex-
ample, consider the sentence below with the
target abreviation BSA.
?Lean BSA was obtained from height
and lean body weight ...?
The features would include the following:
left-content-word-lemma ?lean BSA?, right-
function-word-lemma ?BSA be?, left-POS ?JJ
NNP?, right-POS ?NNP VBD?, left-content-
word-form ?Lean BSA?, right-function-word-
form ?BSA was?, etc.
? Salient bigrams: Salient bigrams within the ab-
stract with high log-likelihood scores, as de-
scribed by Pedersen (2001).
? Unigrams: Lemmas of all content words in the
abstract and words within a ?4-word window
around the target word, excluding those in a list
of stopwords. In addition, the lemmas of any
unigrams appearing at least twice in the entire
corpus and which are found in the abstract are
also included as features.
Concept Unique Identifiers (CUIs): We follow
the approach presented by McInnes et al (2007) to
generate features based on UMLS Concept Unique
Identifiers (CUIs). The MetaMap program (Aron-
son, 2001) identifies all words and terms in a
text which could be mapped onto a UMLS CUI.
MetaMap does not disambiguate the senses of the
concepts, instead it enumerates likely candidate con-
cepts. For example, MetaMap will segment the
phrase ?Lean BSA was obtained from height and
lean body weight ...? into four chunks: ?Lean
BSA?, ?obtained?, ?from height? and ?lean body
weight?. The first chunk will be mapped onto
three CUIs: ?C1261466: BSA (Body surface area)?,
?C1511233: BSA (NCI Board of Scientific Ad-
visors)? and ?C0036774: BSA (Serum Albumin,
Bovine)?. The chunk ?lean body weight? is mapped
onto two concepts: ?C0005910: Body Weight?
73
and ?C1305866: Body Weight (Weighing patient)?1.
CUIs occurring more than twice in an abstract are in-
cluded as features. CUIs have been used for various
disambiguation tasks in the biomedical domain, in-
cluding disambiguation of ambiguous general terms
(McInnes et al, 2007) and gene symbol disambigua-
tion (Xu et al, 2007), but not, to our knowledge, for
abbreviation disambiguation.
Medical Subject Headings (MeSH): The fi-
nal feature is also specific to the biomedical do-
main. Medical Subject Headings (MeSH) (Nelson
et al, 2002) is a controlled vocabulary for index-
ing biomedical and health-related information and
documents. MeSH terms are manually assigned to
abstracts by human indexers. The latest version of
MeSH (2009) contains over 25,000 terms organised
into an 11 level hierarchy.
The MeSH terms assigned to the abstract in which
each ambiguous word occurs are used as features.
For example, the abstract containing our example
phrase has been assigned 16 terms including ?Body
Surface Area?, ?Body Weight?, ?Humans? and ?Or-
gan Size? . MeSH terms have previously been used
for abbreviation disambiguation by Yu et al (2006).
3.2 Learning Algorithms
We compared three machine leaning algorithms
which have previously been shown to be effective
for WSD tasks.
The Vector Space Model (VSM) is a memory-
based learning algorithm which was used by Agirre
and Mart??nez (2004). Each occurrence of an
ambiguous word is represented as a binary vec-
tor in which each position indicates the occur-
rence/absence of a feature. A single centroid vector
is generated for each sense during training. These
centroids are compared with the vectors that repre-
sent new examples using the cosine metric to com-
pute similarity. The sense assigned to a new example
is that of the closest centroid.
The Naive Bayes (NB) classifier is based on a
probabilistic model which assumes conditional in-
dependence of features given the target classifica-
tion. It calculates the posterior probability that an
1The first of these, C0005910, refers to the weight of
a patient as a property of that individual while the second,
C1305866, refers to the process of weighing a patient as part
of a diagnostic procedure.
instance belongs to a particular class given the prior
probabilities of the class and the conditional proba-
bility of each feature given the target class.
Support Vector Machines (SVM) have been
widely used in classification tasks. SVMs map
feature vectors onto a high dimensional space and
construct a classifier by searching for the hyper-
plane that gives the greatest separation between the
classes.
We used our own implementation of the Vector
Space Model and Weka implementations (Witten
and Frank, 2005) of the other two algorithms.
4 Evaluation Corpus
The most common method for generating corpora
to train and test WSD systems is to manually an-
notate instances of ambiguous terms found in text
with the appropriate meaning. However, this process
is both time-consuming and difficult (Artstein and
Poesio, 2008). An alternative to manual tagging is
to find a way of automatically creating sense tagged
corpora. For the translation of ambiguous English
words Ng et al (2003) made use of the fact that the
various senses are often translated differently. For
example when ?bank? is used in the ?financial insti-
tution? sense it is translated to French as ?banque?
and ?bord? when it is used to mean ?edge of river?.
However, a disadvantage of this approach is that it
relies on the existence of parallel text which may
not be available. In the biomedical domain Liu et al
(2001)(2002) created a corpus using unambiguous
related terms (see Section 2) although they found
that it was not always possible to identify suitable
related terms.
4.1 Corpus Creation
Liu et al (2001) also made use of the fact that
when abbreviations are introduced they are often ac-
companied by their expansion, for example ?BSA
(bovine serum albumin)?. This phenomenon was
exploited to automatically generate a corpus of ab-
breviations and associated definitions by replacing
the abbreviation and expansion with the abbrevia-
tion alone. For example, the sentence ?The adsorp-
tion behavior of bovine serum albumin (BSA) on
a Sepharose based hydrophobic interaction support
has been studied.? becomes ?The adsorption behav-
74
?BSA? AND ?body surface area? NOT ?bovine serum albumin?
?BSA? AND ?bovine serum albumin? NOT ?body surface area?
Figure 1: Example queries for abbreviation ?BSA?
ior of BSA on a Sepharose based hydrophobic inter-
action support has been studied.?
We used this approach to create a corpus of sense
tagged abbreviations in biomedical documents using
a set of 21 three letter abbreviations used in previ-
ous research on abbreviation disambiguation (Liu et
al., 2001; Liu et al, 2002; Liu et al, 2004). Pos-
sible expansions for the majority of these abbrevi-
ations were listed in these papers. For the few re-
maining ones possible expansions were taken from
the Medstract database (Pustejovsky et al, 2002).
We searched for instances of these abbreviations in
Medline, a database containing more than 18 mil-
lion abstracts from publications in biomedicine and
the life sciences. For each abbreviation we queried
Medline, using the Entrez interface, to identify doc-
uments containing one of its meanings. For exam-
ple the abbreviation ?BSA? has two possible expan-
sions: ?body surface area? and ?bovine serum alu-
min?. Medline is searched to identify documents
that contain each possible expansion of the abbre-
viation using the queries shown in Figure 1. Each
query matches documents containing the abbrevia-
tion and relevant expansion and no mentions of the
other possible expansion(s).
The retrieved documents are then processed to
remove the expansions of each abbreviation. The
Schwartz and Hearst (2003) algorithm for identi-
fying abbreviations and the relevant expansion (see
Section 2) is then run over each of the retrieved ab-
stracts to identify the correct expansion. The expan-
sion is removed from the document and stored sep-
arately, effectively creating a sense tagged corpus.
For convenience the abstracts are converted into a
format similar to the one used for the NLM-WSD
corpus (Weeber et al, 2001).
The resulting corpus consists of 55,655 docu-
ments. For each abbreviation Table 1 shows the
number of abstracts retrieved from Medline (in the
column labeled ?Abstracts?) and the number of ex-
pansions (?Count? column). The column labelled
?Rare? lists the number of expansions that account
for fewer than 1% of the occurrences of an abbre-
viation and ?Frequent? lists the percentage of occu-
rances represented by the most frequent expansion.
It can be seen that there is a wide variation between
the number of abstracts retrieved for each abbrevi-
ation. CSF occurs in 14,871 abstracts and ASP in
just 71. There is also a wide variation between the
frequency of the most common expansion with over
99% of the occurrences of ?CSF? representing one
expansion (?cerebrospinal fluid?) while for ?ASP?
two of the five possible expansions (?antisocial per-
sonality? and ?aspartate?) each account for almost
34% of the documents. In addition, several abbrevi-
ations have expansions which occur only rarely. For
example, two of the expansions of ?APC? (?atrial
pressure complexes? and ?aphidicholin?) each have
only a single document and account for just 0.03%
of the instances of that abbreviation.
4.2 Corpus Reduction
Given the diversity of the abbreviations which were
downloaded from Medline, both in terms of num-
ber of documents and distribution of senses, sub-
sets of this corpus that are more suitable for WSD
experiments were created. Corpora containing 100,
200 and 300 randomly selected examples of each ab-
breviation were generated and these are referred to
as Corpus.100, Corpus.200 and Corpus.300 respec-
tively.
Some of the 21 abbreviations were not suitable
for inclusion in these corpora. Abbreviations were
not included in the relevant corpus if an insufficient
number of examples were retrieved from Medline.
For example, only 71 abstracts containing ?ASP?
were retrieved and it is is not included in any of the
three corpora. Similarly, ?ANA? and ?FDP? are not
included in Corpus.200 or Corpus.300 and ?DIP?
not included in Corpus.300. In addition, rare senses,
those which represent fewer than 1% of the occur-
rences of an abbreviation in all retrieved abstracts,
were discarded. Finally, two abbreviations (?ACE?
and ?CSF?) have only one sense that is not ?Rare?
75
Expansions
Abstracts Count Rare Frequent
ACE 3105 3 2 98.7
ANA 100 3 0 58.0
APC 3146 5 2 39.4
ASP 71 5 0 33.8
BPD 1841 3 0 46.7
BSA 5373 2 0 86.4
CAT 4636 3 1 55.2
CML 2234 4 2 91.7
CMV 7665 2 0 96.7
CSF 14871 3 2 99.1
DIP 209 2 0 75.1
EMG 2052 2 0 88.4
FDP 130 4 0 78.5
LAM 325 4 1 48.3
MAC 955 5 1 64.3
MCP 815 5 1 50.2
PCA 2442 5 1 68.9
PCP 1642 2 0 57.8
PEG 607 2 0 94.1
PVC 234 2 2 78.2
RSV 3202 2 0 76.7
Average 2650 3.2 0.6 70.8
Table 1: Properties of abbreviations corpus retrieved
from Medline
(see Table 1) and these were also excluded from the
reduced corpora.
Consequently, Corpus.100 contains 18 abbrevia-
tions (?ACE?, ?ASP? and ?CSF? are excluded), Cor-
pus.200 contains 16 (?ANA? and ?FDP? are also
excluded) and Corpus.300 contains 14 (?DIP? and
?PVC? also excluded). Where an abbreviation is in-
cluded in more than one corpus, all the examples in
the smaller corpus are included in the larger one(s).
For example, the 100 examples of ?APC? in Cor-
pus.100 are also included in Corpus.200 and Cor-
pus.300.
5 Experiments
Various combinations of learning algorithms and
features were applied to the three reduced corpora
described in Section 4.2. Performance of the WSD
system is measured in terms of the proportion of ab-
breviation instances for which the correct expansion
is identified. 10-fold cross validation was used for
all experiments and all quoted results refer to the av-
erage performance across the 10 folds. Results are
shown in Table 2. The baseline figures, based on
selecting the most frequent expansion for each ab-
breviation, are shown for each corpus. Note that
these figures vary slightly across the three corpora
because of the different abbreviations each contains
(see Section 4.2).
A first observation is that performance of the
WSD system is consistently better than the base-
line for the relevant corpus and, with a few excep-
tions, above 90%. As might be expected, perfor-
mance improves as additional training examples are
added. However, even when the number of exam-
ples is relatively low, just 100, performance of the
best configuration (VSM learning algorithm with all
three types of feature) is 97.4%.
The best result, 99% (300 training examples,
VSM learning algorithm with all feature types), ex-
ceeds reported performance of previous abbreviation
disambiguation systems (see Section 2). Although
these results are not directly comparable, since these
studies used different evaluation corpora, the set
of ambiguous abbreviations used in this study and
methodology for corpus creation are similar to those
used by Liu et al (2001)(2002)(2004).
The best performance for each learning algorithm
is obtained when all three types of features are com-
bined. The difference between performance ob-
tained using all three feature types and using only
the MeSH or CUI features is statistically significant
(Wilcoxon Signed Ranks test, p < 0.01) although
the difference between this and performance using
just the linguistic features is not.
The VSM learning algorithm generally performs
better than either the SVM or Naive Bayes learning
algorithms. The difference between performance of
VSM and the other algorithms is statistically signif-
icant for Corpus.100 but not for the other two, sug-
gesting that this learning algorithm is better able to
cope with small number of training examples than
Naive Bayes and Support Vector Machines. Strong
performance of the VSM algorithm is consistent
with previous work which has shown that this algo-
rithm performs well on the disambiguation of am-
biguous terms in both biomedical and general text
(Agirre and Mart??nez, 2004; Stevenson et al, 2008).
76
Features
Algorithm Linguistic Linguistic CUI+ Linguistic+Linguistic CUI MeSH +CUI +MeSH MeSH MeSH+CUI
Corpus.100 (Baseline = 69.0%)
SVM 0.934 0.900 0.949 0.947 0.946 0.938 0.954
NB 0.940 0.917 0.949 0.951 0.947 0.944 0.958
VSM 0.968 0.937 0.888 0.970 0.971 0.939 0.974
Corpus.200 (Baseline = 69.1%)
SVM 0.957 0.911 0.964 0.964 0.964 0.947 0.965
NB 0.966 0.926 0.962 0.969 0.971 0.955 0.972
VSM 0.979 0.930 0.894 0.982 0.981 0.947 0.984
Corpus.300 (Baseline = 68.7%)
SVM 0.966 0.914 0.970 0.968 0.974 0.954 0.975
NB 0.971 0.933 0.960 0.971 0.976 0.960 0.978
VSM 0.981 0.938 0.894 0.987 0.985 0.957 0.990
Table 2: Performance of WSD system using various combinations of learning algorithms and features.
Performance of our system on this task is higher
than would be expected for most WSD tasks sug-
gesting that the problem of abbreviation disam-
biguation is simpler than the disambiguation of gen-
eral terms. The most probable reason for this is that
the various expansions of abbreviations in our cor-
pus are more distinct and better defined than senses
for general terms. For example, the three possi-
ble expansions for ?ANA? in our corpus are a pro-
fessional body (?American Nurses Association?), a
type of medical test (?antinuclear?) and a neuro-
transmitter (?Anandamide?). It is likely that these
diverse meanings will tend to occur in very differ-
ent contexts and in documents with different topics.
On the other hand it is widely accepted that distinc-
tions between possible meanings of words in natu-
ral language are often vague (Kilgarriff, 1993). It
is likely that clearer distinctions between possible
expansions of abbreviations make the task of iden-
tifying the correct one more straightforward than
identifying meanings of ambiguous words. In ad-
dition, the creation of annotated data for WSD is of-
ten hampered by the difficulty in obtaining sufficient
agreement between annotators (Artstein and Poesio,
2008; Weeber et al, 2001) and this problem does not
apply to our automatically-generated corpus.
Results in Table 2 indicate that CUIs are use-
ful features in the disambiguation of abbreviations.
This is in contrast with previous experiments on am-
biguous terms in biomedical documents (Stevenson
et al, 2008) in which it was found that the best
performance as obtained using only linguistic and
MeSH features. It is likely that the clear distinction
between expansions of abbreviations is the reason
behind this difference. CUIs are assigned automat-
ically by the MetaMap program (Aronson, 2001).
However, this assignment is very noisy. It is likely
that the various expansions of abbreviations are dis-
tinct enough for this noise to be tolerated by the
learning algorithms while it causes problems when
the meanings are closer together, such as in the case
of ambiguous terms.
5.1 Performance of Individual Abbreviations
Table 3 shows the performance of the best WSD sys-
tem (VSM learning algorithm with all features) for
each abbreviation in the three subsets of our corpus.
Our system performs well for all abbreviations. Ac-
curacy is no lower than 92% for any abbreviation
using Corpus.100 and no lower than 97% for Cor-
pus.300, demonstrating that the approach is robust.
In fact, the approach still performs well for abbre-
viations with low baseline scores, such as ?APC?,
?BPD? and ?LAM?.
It is interesting to note that the abbreviations with
the lowest performance tend to have expansions that
are closely related. For example, the two expansions
of ?EMG? are ?electromyography? and ?electromyo-
77
Corpus
100 200 300
ANA 0.980 - -
APC 0.980 1.000 1.000
BPD 1.000 1.000 1.000
BSA 0.970 0.970 0.982
CAT 0.990 0.990 1.000
CML 0.960 0.963 0.978
CMV 0.970 0.970 0.970
DIP 1.000 1.000 -
EMG 0.920 0.960 0.980
FDP 0.970 - -
LAM 0.960 0.980 0.980
MAC 0.970 0.990 0.989
MCP 0.980 0.978 1.000
PCA 0.960 0.987 0.992
PCP 0.990 1.000 1.000
PEG 0.980 0.982 1.000
PVC 0.990 1.000 -
RSV 0.960 0.972 0.978
Overall 0.974 0.984 0.990
Table 3: Performance of WSD system over individual ab-
breviations in three reduced corpora
gram? while for ?LAM? one expansion (?Lymphan-
gioleiomyomatosis?) is a rare lung disease and the
other (?Lipoarabinomannan?) a molecule associated
with another lung disease (tuberculosis). On the
other hand, abbreviations that are more accurately
disambiguated tend to have expansions with more
distinct meanings. For example, ?BPD? can be an
acronym for ?borderline personality disorder? (a psy-
chiatric diagnosis), ?bronchopulmonary dysplasia?
(a lung disease) or ?biparietal diameter? (diameter of
a foetus? head in an ultrasound) and the expansions
of ?DIP? are ?desquamative interstitial pneumonia?
(a lung disease) and ?distal interphalangeal joints?
(types of joints in the human hand and foot).
6 Conclusions
This paper has presented an approach to the disam-
biguation of ambiguous abbreviations in biomedi-
cal documents. We treat this problem as a form
of WSD and apply a system that combines a wider
range of features than have been previously applied,
including those which are commonly used within
WSD systems in addition to information from two
domain-specific knowledge sources. The approach
is evaluated using a corpus of abbreviations auto-
matically mined from Medline and found to iden-
tify the correct expansion with accuracy of up to
99%. This figure is higher than previously reported
results for abbreviation disambiguation systems, al-
though direct comparison is difficult due to the use
of different data sets. It was also found that best per-
formance could be obtained using a simple machine
learning algorithm and a diverse range of knowledge
sources. Performance of our system is higher than is
normally achieved by WSD systems when applied
to general terms and we suggest that the reason for
this is that the various expansions of abbreviations
are better defined and more distinct than the senses
of ambiguous words.
This study has been limited to the disambiguation
of abbreviations consisting of exactly three letters.
Possibilities for future work include experimenting
with abbreviations of various lengths.
Data
The corpus described in Section 4 has been
made freely available for research and may
be obtained from http://nlp.shef.ac.uk/
BioWSD/downloads/abbreviationdata/.
Acknowledgments
We are grateful to the anonymous reviewers of this
paper for their valuable feedback.
References
E. Adar. 2004. SaRAD: A simple and robust abbrevia-
tion dictionary. Bioinformatics, 20(4):527?533.
E. Agirre and D. Mart??nez. 2004. The Basque Coun-
try University system: English and Basque tasks. In
Rada Mihalcea and Phil Edmonds, editors, Senseval-
3: Third International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text, pages 44?
48, Barcelona, Spain, July.
A. Aronson. 2001. Effective mapping of biomedical text
to the UMLS Metathesaurus: the MetaMap program.
In Proceedings of the American Medical Informatics
Association (AMIA), pages 17?21.
R. Artstein and M. Poesio. 2008. Inter-coder agreement
for computational linguistics. Computational Linguis-
tics, 34(4):555?596.
78
J. Chang, H. Schu?tze, and R. Altman. 2002. Creating an
Online Dictionary of Abbreviations from MEDLINE.
The Journal of the American Medical Informatics As-
sociation, 9(6):612?620.
H. Fred and T. Cheng. 1999. Acronymesis: the explod-
ing misuse of acronyms. Texas Heart Institute Jour-
nal, 30:255?257.
S. Gaudan, H. Kirsch, and D. Rebholz-Schuhmann.
2005. Resolving abbreviations to their senses in Med-
line. Bioinformatics, 21(18):3658?3664.
M. Joshi, T. Pedersen, and R. Maclin. 2005. A Compara-
tive Study of Support Vector Machines Applied to the
Word Sense Disambiguation Problem for the Medical
Domain. In Proceedings of the Second Indian Confer-
ence on Artificial Intelligence (IICAI-05), pages 3449?
3468, Pune, India.
M. Joshi, S. Pakhomov, T. Pedersen, and C. Chute. 2006.
A comparative study of supervised learning as applied
to acronym expansion in clinical reports. In Proceed-
ings of the Annual Symposium of the American Medi-
cal Informatics Association, pages 399?403, Washing-
ton, DC.
A. Kilgarriff. 1993. Dictionary word sense distinctions:
An enquiry into their nature. Computers and the Hu-
manities, 26:356?387.
H. Liu, Y. Lussier, and C. Friedman. 2001. Disam-
biguating ambiguous biomedical terms in biomedical
narrative text: An unsupervised method. Journal of
Biomedical Informatics, 34:249?261.
H. Liu, S. Johnson, and C. Friedman. 2002. Au-
tomatic Resolution of Ambiguous Terms Based on
Machine Learning and Conceptual Relations in the
UMLS. Journal of the American Medical Informatics
Association, 9(6):621?636.
H. Liu, V. Teller, and C. Friedman. 2004. A Multi-aspect
Comparison Study of Supervised Word Sense Disam-
biguation. Journal of the American Medical Informat-
ics Association, 11(4):320?331.
B. McInnes, T. Pedersen, and J. Carlis. 2007. Using
UMLS Concept Unique Identifiers (CUIs) for Word
Sense Disambiguation in the Biomedical Domain. In
Proceedings of the Annual Symposium of the Ameri-
can Medical Informatics Association, pages 533?537,
Chicago, IL.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004. The
Senseval-3 English lexical sample task. In Proceed-
ings of Senseval-3: The Third International Workshop
on the Evaluation of Systems for the Semantic Analysis
of Text, Barcelona, Spain.
S. Nelson, T. Powell, and B. Humphreys. 2002. The
Unified Medical Language System (UMLS) Project.
In Allen Kent and Carolyn M. Hall, editors, Ency-
clopedia of Library and Information Science. Marcel
Dekker, Inc.
H. Ng, B. Wang, and S. Chan. 2003. Exploiting Parallel
Texts for Word Sense Disambiguation: an Empirical
Study. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics (ACL-
03), pages 455?462, Sapporo, Japan.
N. Okazaki, S. Ananiadou, and J. Tsujii. 2008. A dis-
criminative alignment model for abbreviation recogni-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 657?664, Manchester, UK.
S. Pakhomov. 2002. Semi-supervised maximum entropy
based approach to acronym and abbreviation normal-
ization in medical texts. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 160?167, Philadelphia, PA.
T. Pedersen. 2001. A Decision Tree of Bigrams is an
Accurate Predictor of Word Sense. In Proceedings
of the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-01), pages 79?86, Pittsburgh, PA.
J. Pustejovsky, J. Castano, R. Saur, A. Rumshisky,
J. Zhang, and W. Luo. 2002. Medstract: Creating
Large-scale Information Servers for Biomedical Li-
braries. In ACL 2002 Workshop on Natural Language
Processing in the Biomedical Domain.
A. Schwartz and M. Hearst. 2003. A simple algorithm
for identifying abbreviation definitions in biomedical
text. In Proceedings of the Pacific Symposium on Bio-
computing, Kauai.
M. Stevenson, Y. Guo, R. Gaizauskas, and D. Martinez.
2008. Disambiguation of biomedical text using di-
verse sources of information. BMC Bioinformatics,
9(Suppl 11):S7.
M. Weeber, J. Mork, and A. Aronson. 2001. Developing
a Test Collection for Biomedical Word Sense Disam-
biguation. In Proceedings of AMAI Symposium, pages
746?50, Washington, DC.
I. Witten and E. Frank. 2005. Data Mining: Practical
machine learning tools and techniques. Morgan Kauf-
mann, San Francisco.
H. Xu, J. Fan, G. Hripcsak, E. Mendonc?a, Markatou M.,
and Friedman C. 2007. Gene symbol disambigua-
tion using knowledge-based profiles. Bioinformatics,
23(8):1015?22.
H. Yu, W. Kim, V. Hatzivassiloglou, and J. Wilbur. 2006.
A large scale, corpus-based approach for automati-
cally disambigutaing biomedical abbreviations. ACM
Transactions on Information Systems, 24(3):380?404.
W. Zhou, I. Vetle, and N. Smalheiser. 2006. ADAM: an-
other database of abbreviations in MEDLINE. Bioin-
formatics, 22(22):2813?2818.
79
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1544?1555, Dublin, Ireland, August 23-29 2014.
Collective Named Entity Disambiguation using Graph Ranking and
Clique Partitioning Approaches
Ayman Alhelbawy
??
and Robert Gaizauskas
?
?
The University of Sheffield, Regent Court, 211 Portobello Street, Sheffield, S1 4DP, U.K
?
Faculty of Computers and Information, Fayoum University, Fayoum, Egypt
ayman,R.Gaizauskas@dcs.shef.ac.uk
Abstract
Disambiguating named entities (NE) in running text to their correct interpretations in a specific
knowledge base (KB) is an important problem in NLP. This paper presents two collective disam-
biguation approaches using a graph representation where possible KB candidates for NE textual
mentions are represented as nodes and the coherence relations between different NE candidates
are represented by edges. Each node has a local confidence score and each edge has a weight.
The first approach uses Page-Rank (PR) to rank all nodes and selects a candidate based on PR
score combined with local confidence score. The second approach uses an adapted Clique Par-
titioning technique to find the most weighted clique and expands this clique until all NE textual
mentions are disambiguated. Experiments on 27,819 NE textual mentions show the effectiveness
of both approaches, outperforming both baseline and state-of-the-art approaches.
1 Introduction
Named entities (NEs) have received a lot of attention from the NLP community over the last two decades
(see, e.g. Nadeau and Sekine (2007)). Most of this work has focussed on the task of recognizing the
boundaries of NE mentions in text and classifying them into one of several classes, such as Person,
Organization or Location. However, references to entities in the real world are often ambiguous: there is
a many-to-many relation between NE mentions in text and the entities denoted by these mentions in the
world. For example, the same NE mention ?Norfolk? may refer to a person, ?Peter Norfolk, a wheelchair
tennis player?, a place in the United Kingdom, ?Norfolk County?, or a place in the United States like
?Norfolk, Massachusetts?; conversely, one entity many be known by many names, such as ?Cat Stevens?,
?Yusuf Islam? and ?Steven Georgiou?. The task of named entity disambiguation (NED) is to establish
a correct mapping between each NE mention in a document and the entity it denotes in the real world.
Following most researchers in this area, we treat entries in a large knowledge base (KB) as surrogates for
real world entities when carrying out NED and, in particular, use Wikipedia as the reference KB against
which to disambiguate NE mentions. NED is important for tasks like KB population, where we want to
extract new information from text about an entity and add it to a pre-existing entry for that entity in a
KB, or for information retrieval where we may want to cluster or filter results for different entities with
the same textual mentions.
The main hypothesis underlying this work is that different NEs in a document help to disambiguate
each other. However, other textual mentions in the document are also ambiguous. So, what is needed is
a collective disambiguation approach that jointly disambiguates all NE textual mentions.
In our approaches we model each possible candidate for every NE mention in a document as a distinct
node in a graph and model candidate coherence by links between the nodes. Figure 1 shows an example
of the disambiguation graph for three mentions ?A?, ?B?, and ?C? found in a document, where the
candidate entities for each mention are referred to using the lower case form of the mention?s letter
together with a distinguishing subscript. The goal of disambiguation is to find a set of nodes where only
one candidate is selected from the set of entities associated with each mention, e.g. a
3
, b
2
, c
2
.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1544
Figure 1: Example of solution graph
We propose two different approaches to find the best disam-
biguation candidates in the graph. The first approach starts by
finding the most confident and coherent set of disambiguation
entities and iteratively expands this set until all NE textual men-
tions are disambiguated. The second approach ranks all nodes
in the solution graph using the Page-Rank algorithm, then re-
ranks all nodes by combining the initial confidence and graph
ranking scores. We consider several different measures for com-
puting the initial confidence assigned to each node and several
measures for determining and weighting the graph edges. Node
linking relies on the fact that the textual portion of KB entries
typically contains mentions of other NEs. When these mentions
are hyper-linked to KB entries, we can infer that there is some relation between the real world entities
corresponding to the KB entries, i.e. that they should be linked in our solution graph. These links also
allow us to build up statistical co-occurrence counts between entities that occur in the same context,
which may be used to weight edges in our graph.
We evaluate our approaches on the AIDA dataset (Hoffart et al., 2011). Comparison with the baseline
and some state-of-the-art approaches shows our proposed approaches offers substantial improvements in
disambiguation accuracy.
The rest of the paper is organized as follows: Section 2 discusses related work. Section 3 discusses
selection of NE candidate entities from the Wikipedia knowledge base and the assignment of confidence
scores to each candidate. Formulation of the NE disambiguation problem in terms of a graph model is
presented in section 4. Sections 5 and 6 describe the clique partitioning and ranking disambiguation ap-
proaches for collective NED. The experimental dataset and experimental results are presented in Section
7. Section 8 concludes the paper and presents some suggestions for future work to improve the results.
2 Related Work
Named Entity Disambiguation has received a lot attention in the past few years. Perhaps the best known
related work is the Entity Linking (EL) shared task challenge first proposed by the National Institute of
Standards and Technology (NIST) as part of the Knowledge Base Population (KBP) track within the Text
Analysis Conference (TAC) in 2009 (McNamee and Dang, 2009). EL is a similar but broader task than
NED: NED is concerned with disambiguating a textual NE mention where the correct entity is known to
be one of the KB entries, while EL also requires systems to deal with the case where there is no entry for
the NE in the reference KB. Ji et al. (2011) group and summarise the different approaches to EL taken
by participating systems.
In general, there are two main lines of approach to the NED problem. The first, single entity dis-
ambiguation approaches (SNED), disambiguates one entity at a time without considering the effect of
other NEs. These approaches use local context textual features of the mention and compare them to the
textual features of NE candidate documents in the KB, and link to the most similar. The first approach in
this line was Bunescu and Pasca (2006), who measure similarity between the textual context of the NE
mention and the Wikipedia categories of the candidate. More similarity features were added by Cucerzan
(2007) who realized that topical coherence between a candidate entity and other entities in the context
will improve NED accuracy by calculating the nodes? coherence based on the their incoming links in
Wikipedia and the overlaps in Wikipedia categories. Milne and Witten (2008) improve Cucerzan?s work
by calculating the topical coherence using Normalized Google Distance and restrict the context entities
to the unambiguous entities. Different query expansion approaches are incorporated into this framework,
such as using context term expansion (Gottipati and Jiang, 2011) and acronym expansion (Zhang et al.,
2011). Sen (2012) proposed a latent topic model to learn the context entity association. Machine learning
is widely used in SNED as some approaches deal with the problem as a search result ranking problem.
Supervised learn-to-rank models are used to re-rank the ambiguous candidate set (Zheng et al., 2010;
Dredze et al., 2010; Alhelbawy and Gaizauskas, 2012; Nebhi, 2013).
1545
The second line of approach is collective named entity disambiguation (CNED), where all mentions of
entities in the document are disambiguated jointly. These approaches try to model the interdependence
between the different candidate entities for different NE mentions in the query document, and reformulate
the problem of NED as a global optimization problem whose aim is to find the best set of entities. As
this new formulation is NP-hard, many approximations have been proposed. Kulkarni et. al. (2009)
presents a collective approach for entity linking that models the coherence between all pairs of entity
candidates for different mentions as a probabilistic factor graph. They present two approximations to
solve this optimization problem where the interdependence between decisions is modelled as the sum
of the pairs? dependencies. Alhelbawy and Gaizauskas (2013) proposed a sequence dependency model
using HMMs to model NE interdependency. Another approximation uses a mixture of local and global
features to train the coefficients of a linear ranking SVM to rank different NE candidates (Ratinov et al.,
2011). Shirakawa et al. (2011) cluster related textual mentions and assign a concept to each cluster using
a probabilistic taxonomy. The concept associated with a mention is used in selecting the correct entity
from the Freebase KB.
Graph models are widely used in collective disambiguation approaches. All these approaches model
NE interdependencies, while different methods may be used for disambiguation. Han (2011) uses local
dependency between NE mention and the candidate entity, and semantic relatedness between candidate
entities to construct a referent graph, proposing a collective inference algorithm to infer the correct
reference node in the graph. Hoffert (2011) poses the problem as one of finding a dense sub-graph,
which is infeasible in a huge graph. So, an algorithm originally used to find strongly interconnected,
size-limited groups in social media is adapted to prune the graph, and then a greedy algorithm is used to
find the densest graph.
The word sense disambiguation (WSD) task has many similarities to NED, since in both cases the
goal is to determine which of a set of predefined senses or reference entities is the correct interpretation
of a surface string in context. Many researchers have used graph-based approaches successfully for
the WSD task. Sinha and Michalecea (2007) proposed using four different graph centrality algorithms
? Indegree, PageRank, Closeness and Betweenness for WSD. We propose to use a clique partitioning
algorithm, originally proposed by Born et al. (1973), for NED. Clique algorithms have been successfully
used for WSD problems. Guti?errez et al. (2011; 2012), for example, use an N-cliques graph partitioning
technique to identify sets of highly related senses. However, this approach has not been used for NED.
Our second proposed model uses the Page-Rank algorithm (PR), which to our knowledge has also not
previously been applied to NED. PR was proposed by Page et al. (1999) to produce a global rank for
web pages based on the hyperlink structure of the web. Xing and Ghorbani (2004) adapted PR to take
into account the weights of links and the nodes? importance. PR and Personalized PR algorithms have
been used successfully in WSD (e.g. Sinha and Mihalcea (2007), Agirre and Soroa (2008; 2009)).
3 Named Entity Candidates Selection
Given an input document D containing a set of pre-tagged NE textual mentions M =
{m
1
,m
2
,m
3
. . .m
k
}, we need to select all possible candidate interpretations for each m
i
from the
knowledge base. I.e. for each NE textual mention m
i
? M we select a set of candidates E
i
=
{e
i,1
, e
i,2
, e
i,3
. . . e
i,j
} from the KB. The NE textual mention m
i
is used to search the KB entry ti-
tles using Lucene
1
to find entries with titles that fully or partially contain the NE textual mention.
The following example shows the possible candidates for the textual mention ?Sheffield?: ?Sheffield,
New Zealand,?, ?University of Sheffield?, ?Sheffield United F.C.?, ?Sheffield, Massachusetts?, ?Fred
Sheffield?, ?Sheffield, Alabama?, etc. The result of this search is quite large and this increases the
likelihood of the correct entry occurring somewhere in the list, i.e. it improves recall. However, the
challenge now moves to the disambiguation step. In this step, we need to assign a confidence score to
each candidate, as shown in the following section.
1
https://lucene.apache.org/
1546
3.1 Candidate Confidence Score
For each candidate e
i,j
, a set of initial confidence scores IConf(e
i,j
) is assigned. These scores are
calculated for each NE candidate independent of other candidates or the candidates for other NE textual
mentions in the document. Three scores are calculated locally using the NE textual mention context.
There is also one global confidence score, entity popularity (EP), which is calculated globally indepen-
dent of the document or the textual mention context by using the Freebase KB (Bollacker et al., 2008).
The four confidence scores to be calculated for each NE candidate as follows:
? Cos: The cosine similarity between the NE textual mention and the KB entry title.
? JwSim: While the cosine similarity between a textual mention in the document and the candidate
NE title in the KB is widely used in NED, this similarity is a misleading feature. For example,
the textual mention ?Essex? may refer to either of the following candidates ?Essex County Cricket
Club? or ?Danbury, Essex?, both of which are returned by the candidate generation process. The
cosine similarity between ?Essex? and ?Danbury, Essex? is higher than that between ?Essex? and
?Essex County Cricket Club?, which is not helpful in the NED setting. We adopted a new mention-
candidate similarity function, jwSim, which uses Jaro-Winkler similarity as a first estimate of the
initial confidence value for each candidate. This function considers all terms found in the candidate
entity KB entry title, but not in the textual mention as disambiguation terms. The percentage of
disambiguation terms found in the query document is used to boost in the initial jwSim value, in
addition to an acronym check (whether the NE textual mention could be an acronym for a specific
candidate entity title). Experiments show that jwSim performs much better than the standard cosine
similarity.
? Ctxt: The cosine similarity between the sentence containing the NE mention in the query document
and the textual description of the candidate NE in the KB (we use the first section of the Wikipedia
article as the candidate entity description).
? EP: Entity popularity refers to connectivity to this entity. It has been used successfully as a dis-
criminative feature for NED (Nebhi, 2013). Freebase provides an API interface to get an entity?s
popularity score, which is computed during Freebase data indexing. This score is a function of the
entity?s inbound and outbound link counts in Freebase and Wikipedia
2
.
Initial confidence scores are calculated independently for each candidate entity for an NE mention. How-
ever, after the initial calculation, initial confidence scores for all candidates for a single NE mention are
normalized to sum to 1.
4 Disambiguation Graph Model
In this section we discuss the graph model we use for NED. All candidate entities for the different NE
textual mentions in the document are represented as an undirected graph G = (V,D) where V is the
node set of all possible candidate entities for different NE textual mentions in the input document and D
is the set of edges between nodes. Because the same entity may be found multiple times as a candidate
for different textual mentions and each occurrence must be evaluated independently, each node is formed
as an ordered pair of textual mention m
i
and candidate entity e
i,j
. So, the graph nodes are formulated as
a set V = {(m
i
, e
i,j
) | ?e
i,j
? E
i
,?m
i
?M}.
A set of entities is coherent if real world relations hold between them. We use such relations to link
candidate entities for different NE textual mentions in our graph model. Edges are not drawn between
different nodes for the same mention. However, they are drawn between two entities when there is a
relation between them. Different approaches to determine and weight entity coherence relations are
presented in the following section.
2
https://developers.google.com/freebase/v1/search
1547
4.1 Entity Coherence
Entity coherence refers to the real world relatedness of different entities which are candidate interpreta-
tions of different textual mentions in the document. Such relatedness is not based on documentcontext, so
the relatedness of two candidate entities is always the same regardless of the query document. Coherence
is represented as an edge between nodes in the graph. We used two measures for coherence:
? Entity Reference Relation (Ref): This is a boolean relation between two entities e
1
and e
2
. The Ref
relation holds if the Wikipedia document for either entity has a link to the other. Since the Wikipedia
hyperlinks are directed, this relation is implicitly directed. However, we assume an inverse relation
also exists and represented the relation as undirected.
Ref(e
i
, e
j
) =
{
1, if e
i
or e
j
refers to the other
0, otherwise
(1)
? Entity Co-occurrence (Jprob): An estimate of the probability of both entities appearing in the same
sentence. Wikipedia documents are used to estimate this probability, as shown in (2), where S(e) is
the set of all sentences that contain a hyperlink reference to the entity e and S is the set of sentences
containing any such entity references.
Jprob(e
i
, e
j
) =
|S(e
i
)
?
S(e
j
)|
|S|
(2)
5 Cliques Partitioning Disambiguation
The clique model originated in social network studies when Luce and Perry (1949) defined a clique as a
set of two or more people who are mutual friends. In graph theory, this pattern is known as a complete
sub-graph. Assuming that NEs that appear in the same document can be split into groups of highly
cohesive entities, we adopt the clique partitioning technique to find the biggest clique in terms of size
and weight. Given an undirected graph G(V,D) where V is the set of all nodes and D is the set of
all edges, G
s
= (V
s
, D
s
) is a sub-graph of G where V
s
? V and D
s
? D. G
s
is called complete
sub-graph or clique if and only if each node in V
s
has a link in D
s
to all other nodes in V
s
. The clique
partitioning algorithm aims to find all possible complete sub-graphs G
s
in an undirected graph G. Our
approach iteratively finds the ?best? clique, deletes all ?wrong? candidate entities for textual mentions that
are disambiguated by the selected clique and converts the selected clique to a node in the graph to be
used in the next iteration. The details are shown in algorithm 1. Figure 2 shows an exampler of the clique
partitioning disambiguation algorithm given a graph of candidate entities for six NE textual mentions,
?A?,?B?,?C?,?D?,?E?,?F?. Candidate entities are coded with the lower case letter of the NE textual mention
plus an index subscript, e.g., ?a1?, ?a2?, ?a3?, etc. Cliques are shown with bold links in different colours.
As described in section 4, one of the properties of the disambiguation graph is that there are no links
between candidates of the same NE textual mention. Because of this property, we can guarantee that
there is no more than one candidate for each textual mention in any clique.
Data: Undirected graph G(V,E) and for each node v ? V an associated IConf score
Result: Solution sub-graph
while not all textual mentions are disambiguated do
1- clique-List = find cliques(G);
2- weight each clique by summing the IConf scores of all nodes in the clique;
3- select the highest scoring clique and use its nodes as disambiguation candidates;
4- remove all wrong candidates for any mention disambiguated in step 3;
5- merge all nodes in the selected clique into one node with IConf score of the
new node = sum of the IConf scores of the merged nodes;
end
Algorithm 1: Clique Disambiguation Algorithm
This approach does not use an entity coherence weighting (e.g. Jprob). Rather it just uses the entity
1548
Figure 2: Example of Clique Partitioning Disambiguation
links to find the cliques regardless the relation strength. Because of the huge number of nodes, the clique
finder algorithm is not fast. To speed-up the disambiguation, we filtered the nodes with low confidence
in the graph, keeping only the top confidence scored 50 NE candidates for each NE textual mention.
6 Graph Ranking Disambiguation
The clique approach disambiguates different NE textual mentions iteratively, where in each iteration
one or more NE mentions are disambiguated taking into account the disambiguated mentions from the
previous iteration. The graph Ranking approach iteratively ranks all graph nodes depending on the links.
So, all NE candidates of all NE textual mentions in the text are ranked together without ignoring any of
them. Hence, a selection algorithm is used to combine the initial confidence and the graph rank score,
and select the most appropriate NE candidate.
Graph Ranking: The links between different candidates in the graph represent real world relations.
These relations are used to reliably boost relevant candidates. In some setups, the weight of these links
are set to 1 and in some others they are set to the entities? coherence score. All nodes in the graph
are ranked according to these relations using Page-Rank. We adapted a version of the PR algorithm
with normalization term to rank the different NE candidates according to entity coherence as shown in
equation 3, where N is the number of nodes in the graph, coh(e
i
) is the set of nodes that cohere with
node e
i
and W (e
i
, e
j
) is the weight of the edge between e
i
and e
j
nodes. The original PR uses a directed
graph while our graph is an undirected graph; so all links are treated as bidirectional.
PR(e
i
) =
(1? d)
N
+
d
F (e
i
)
?
e
j
?coh(e
i
)
PR(e
j
)?W (e
i
, e
j
) (3)
F (e
i
) =
?
e
j
?coh(e
i
)
W (e
i
, e
j
) (4)
The standard PR algorithm assumes the initial rank of all nodes is uniformly equal, while in our
approach we used the initial confidence as an initial weight for the candidate nodes. A problem with
Page-Rank for our purposes is the dissipation of initial node weight (confidence) over all linked nodes.
The final rank of a node is based solely on the importance of linked nodes and the initial confidence plays
no further role. In our case this is not appropriate, so the final rank for each mention is calculated after
1549
graph ranking, by combining the graph rank with the initial confidence score. Let us refer to the graph
rank of a candidate as PR(e
i
). We used two different combination schemes R
s
and R
m
as described in
equations 6 and 5.
R
m
(e
i,j
) = IConf(e
i,j
)? PR(e
i,j
) (5) R
s
(e
i,j
) = IConf(e
i,j
) + PR(e
i,j
) (6)
Data: E
i
is a candidate list of one NE textual
mention m
i
Result: The best disambiguation NE candidate e?
g
i
R1 = {(R
m
(e
i,j
), e
i,j
) | ?e
i,j
? E
i
};
R2 = {(R
s
(e
i,j
), e
i,j
) | ?e
i,j
? E
i
};
Sort R1 in descending order ;
Sort R2 in descending order ;
R1diff = R1[0]-R1[1];
R2diff = R2[0]-R2[1];
if R1diff > R2diff then
return highest rank scored entity of R1, (R1[0])
else
return highest rank scored entity of R2, (R2[0])
end
Algorithm 2: Selection Algorithm
Decision Making: Selecting the proper
candidate is the final phase in the disam-
biguation process. The simplest approach
is to select the highest ranked entity in the
list for each mention m
i
according to equa-
tion 7 or 8, which correpond to the rank
combining schemes expressed in equations
5 and 6. Experiments show that overall
using the R
m
combining scheme is better
than the R
s
scheme. However, the high-
est rank, after combining graph rank score
and initial confidence score, is not always
correct. So we developed a dynamic selec-
tion algorithm which uses both combina-
tion schemes to pick the best disambigua-
tion candidate. We found that a dynamic
choice between the re-ranking schemes,
based on the difference between the top
two candidates, as described in Algorithm 2, works best. The selected candidate entity is referred to
as e? with the superscript showing the selection scheme.
e?
m
i
= argmax
e
i,j
R
m
(e
i,j
) (7) e?
s
i
= argmax
e
i,j
R
s
(e
i,j
) (8)
7 Experiments and Results
7.1 Dataset
NIST has released a dataset for use in the TAC KBP entity linking task (EL). But, the task of named entity
disambiguation is different from entity linking task, as noted above in Section 2. Also, the NIST dataset
is not suitable for evaluating the collective NE disambiguation task because only one NE mention is an-
notated and disambiguated per query document while we need all mentions of NEs in the document to be
annotated and disambiguated to evaluate the performance of the collective named entity disambiguation
technique. Another dataset manually annotated for NED is reported in (Kulkarni et al., 2009), but it uses
an old version of Wikipedia and it is quite small. We have used another dataset, the AIDA dataset, which
is based on the CoNLL 2003 data for NER tagging and in which most tagged NE mentions have been
manually disambiguated against Wikipedia (Hoffart et al., 2011). This dataset contains 1393 documents,
and 34,965 annotated mentions, where 7136 mention are not linked to Wikipedia
3
.
We compare our results to Hoffart?s work ? Accurate Online Disambiguation of Named Entities
(AIDA). For fair comparison, we only considered NE mentions with an entry in the Wikipedia KB,
ignoring the 20% of query mentions without a link to the KB, as Hoffart did.
7.2 Evaluation Metric
We use accuracy as the evaluation metric. Micro-averaged accuracy is used as the official metric for the
disambiguation task and has been used in much previous and related work. Micro-averaged accuracy
3
AIDA dataset is available on the web to download http://www.mpi-inf.mpg.de/yago-naga/aida/
1550
corresponds to the percentage of the correctly disambiguated textual mentions and it is calculated as
shown in equation 9.
A
micro
=
#correctly disambiguated mentions
Number of NE Mentions
(9)
Macro-averaged accuracy is used to calculate the average percentage of correctly identified named
entities. Macro-averaged accuracy is calculated as shown in equation 10.
A
macro
=
?
num
i
Num Correct(E
i
)
Num Queries(E
i
)
# of unique entities
(10)
7.3 Results
In addition to the state-of-the-art, we used two strong baselines to evaluate the performance of the pro-
posed approaches. The first baseline is a setup where the IConf scores only are used to disambiguate
the NE textual mention. In this setup a ranking based on Entity Popularity (EP) does best, with micro-
and macro-averaged accuracy scores of 80.55% and 78.09% respectively. This high baseline is close to
the state-of-the-art. A summary of the first baseline is shown in Table 1. The second baseline is the basic
PR algorithm, where both IConf scores and link weights are ignored. Links between nodes are created
wherever any non-zero entity coherence relation, REF or JProb, is found. Micro- and macro-averaged
accuracy scores of 70.60% and 60.91% respectively were obtained with this baseline.
Baseline1 Cliques PR
I
e?
g
IConf A
micro
A
macro
A
micro
A
macro
A
micro
A
macro
A
micro
A
macro
cos 38.44 45.68 71.59 64.83 70.6 60.83 78.41 72.35
jwSim 61.01 58.81 72.26 69.53 70.61 60.94 83.16 78.28
ctxt 24.58 21.44 58.06 57.37 70.61 60.83 75.45 65.22
EP 80.55 78.09 86.10 81.79 71.78 81.07 87.59 84.19
Table 1: Results using different IConf scores with different approaches
The clique partitioning disambiguation algorithm experiments are setup so a link between nodes is
created whenever a non-zero coherence relation is found between nodes regardless its weight. We used
different settings for the candidates filter. In the case where no candidates filter is applied, all nodes are
considered to find the best initial clique. So, bigger cliques with nodes that have lower confidence may
be selected in the first iteration. This approach is very sensitive to the results of the first iteration. Conse-
quently, the accuracy goes down. Also, because of the huge graph size, the clique partitioning algorithm
takes a long time. At the other extreme, if we use only a small number of candidates with the highest
confidence scores, then the accuracy also goes down because in most cases the correct disambiguation
entity is filtered out of the graph. We used the highest 50 candidates in the graph and all other nodes
are deleted. Table 1 shows the results of using different initial confidence scores in clique partitioning
disambiguation.
Graph ranking disambiguation experiments were setup in three different settings in order to evaluate
the contribution of different features like initial confidence and link weights. For all setups, we used
different decision making approaches e?
m
, e?
s
and e?
g
. The results when using e?
g
are better than e?
m
and e?
s
for all setups. So, we report the results of e?
g
only. Different setups are as follows:
? PR
I
: In this setup, the IConf scores are used to be the initial rank for Page-Rank while the links
between nodes are uniformly weighted to one. As in the PR baseline, links are created wherever
Ref or Jprob are not zero. Table 1 shows the results both without IConf combination, i.e. using
only the PR score for ranking, and after combining the initial confidence score using dynamic
decision making (indicated by e?
g
) When comparing these results to the PR baseline, we notice a
slight positive effect of using the initial confidence as an initial rank instead of uniform ranking. The
major improvement comes by combining the initial confidence with the PR score. All combining
1551
methods improve the results over the baseline results when using the the same confidence score
while the dynamic selection algorithm overcomes other basic methods, i.e. e?
m
and e?
s
.
? PR
C
: In the second setup, entity coherence features are tested by setting the edge weights to the
coherence score and the initial node rank is set to be uniform when running the PR algorithm. So,
initial confidence scores are not considered in graph ranking but just considered in disambiguation
decision making. This setting is intended to evaluate the contribution of different coherence rela-
tions. We compared Jprob and Ref edge weighting approaches, where for each approach edges
were created only where the coherence score according to the approach was non-zero. We also in-
vestigated a variant, called Jprob+Ref , in which the Ref edge weights are normalized to sum to
1 over the whole graph and then added to the JProb edge weights (here edges result wherever Jprob
and Ref scores are non-zero). Results in Table 2 show the JProb feature seems to be more dis-
criminative than the Ref feature but the combined Jprob+Ref feature performs better than each
separately, just outperforming the baseline. We used the best IConf score, i.e. EP, for re-ranking.
Again, combining the IConf with the PR score improves the results.
? PR
IC
: This setup uses different combinations of IConf and entity coherence scores in PR. Table
3 shows the accuracy when using different combinations of all entity coherence scores and some
selected (i.e. the best) IConf scores. Here the Jprob + Ref combination does not add any value
over Jprob alone. Interestingly using IConf score with differentially weighted edges does not
show any benefit over using IConf score and uniformly weighted edges (Table 1).
PR e?
g
Edge Weight A
micro
A
macro
A
micro
A
macro
Jprob 66.52 55.83 83.31 80.38
Ref 67.48 59.76 81.80 78.53
Jprob+ ref 72.69 65.71 83.46 80.69
Table 2: Results using weighted edges (PR
C
)
e?
g
IConf Edge Weight A
micro
A
macro
jwSim Jprob 82.56 76.16
jwSim Ref 78.61 71.12
jwSim Jprob+Ref 81.97 75.63
EP Jprob 86.29 82.77
EP Ref 83.16 80.01
EP Jprob+Ref 86.10 82.80
Table 3: Results using initial confidence and
weighted edges (PR
IC
)
To compare our results with the state-of-the-art, we report Hoffart et al.?s (2011) results as they re-
implemented two other systems and ran them over the AIDA dataset which we used to evaluate our
approach. We also compare with Alhelbawy and Gaizauskas (2013) and Shirakawa et al. (2011) who
carried out their experiments using the same dataset. Table 4 shows a comparison between the results
of our proposed approaches and the state-of-the-art. Both proposed approaches exceed the results of the
state-of-the-art. However our approaches are very simple and direct to apply, unlike Hoffart et al.?s and
Shirakawa et al.?s which are considerably more complex. Also, our approaches do not need any kind of
training, unlike the Alhelbawy approach.
7.4 Discussion
The Page-Rank algorithm was originally designed for directed graphs while our coherence features are
undirected. So, the node rank depends on both incoming and outgoing links (when converting the undi-
rected graph to a directed graph). That explains the little improvement over basic PR when using the
B1 B2 Cliques PR
C
PR
I
PR
CI
Cucerzan Kulkarni Hoffart Shirakawa Alhelbawy
A
macro
78.09 60.91 81.79 80.98 84.19 82.80 43.74 76.74 81.91 83.02 74.18
A
micro
80.55 70.60 86.11 83.59 87.59 86.10 51.03 72.87 81.82 82.29 78.49
Table 4: Summary of Presented Approaches and State-of-the-art Results. B1 and B2 are baselines.
1552
initial confidence as an initial rank before using PR (see Table 1). However, when comparing PR results
in Tables 2 and 1, we can see that the PR algorithm is more sensitive to the links than to initial ranks.
The combined coherence approach (Jprob + Ref ) actually has a value other than the different weight-
ing it supplies; the approach results in more edges than either of the combined approaches do alone. In
all PR results wherever edge weights are applied, the result of using the combined coherence measures
outperforms either of them singly.
Informal failure analysis was carried out to determine reasons for disambiguation failure. Reasons
identified include:
1. The correct NE candidate does not exist in the graph. In such cases the disambiguation approach
selected is irrelevant and what is needed is improved candidate selection.
2. Lack of edges. When there are no edges between any of the query NE mention candidate entities
and other mentions? candidates. In this case the decision depends only on the IConf score.
3. Where the Freebase popularity score (EP) is used, whenever this score for the correct NE candidate
is 0, which means the selection process is based on the PR score.
Table 5 shows an example of the highest three NE candidates for three NE mentions taken from
a document (overall the document contains textual mentions for ten different NEs). The first one is
?Ford? and is disambiguated correctly to ?Ford Motor Company?, where the PR and popularity scores
are higher than any of the other candidates. The second one is ,?Magna?, disambiguated correctly, where
the first two NE candidates have the same PR score but the popularity score discriminates between them.
The third, ?Markham?, is disambiguated to ?Clements Markham? while it should be disambiguated to
?Markham, Ontario?. The problem in this case is that all NE candidates for the mention ?Markham?
are not linked to any entity candidates for any other NE mentions in the document (problem 2 above).
Therefore, the popularity score dominates the final rank score.
NE Candidate PR score FB Rank our Rank
?10
?3
?10
?3
?10
?3
Ford
Ford Motor Company 21.37 62.12 1.32
Ford Galaxie 4.59 10.94 0.05
Ford GT 2.83 11.43 0.03
Magna
Magna International 2.65 4.78 0.013
Magna Powertrain 2.65 2.18 0.005
Germania 0.83 3.46 0.003
Markham
Clements Markham 0.83 4.42 0.004
Markham Waxers 0.83 3.67 0.003
Edwin Markham 0.83 2.89 0.002
Table 5: Example show the first three NE candidates for three NE mentions with scores
8 Conclusion
Our results show that graph ranking and cliques partitioning approaches in conjunction with the candi-
date confidence scores and entity coherence across a disambiguation graph can be used as an effective
approach to collectively disambiguate named entity textual mentions in a document. Our proposed fea-
tures are very simple and easy to extract, and work well when employed in PR or clique partitioning
algorithms. Also, entity coherence is a discriminative feature when using graph models for NED. In
future work we plan to explore enriching the edges between nodes, by incorporating semantic relations
extracted from an ontology, and extending the scope of entity co-occurrence to be the document instead
of the sentence. Also, it is worth investigating whether using the entity coherence score can help when
evaluating clique weight in the clique partitioning algorithm.
1553
References
Eneko Agirre and Aitor Soroa. 2008. Using the multilingual central repository for graph-based word sense
disambiguation. In LREC.
Eneko Agirre and Aitor Soroa. 2009. Personalizing pagerank for word sense disambiguation. In Proceedings of
the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 33?41.
Association for Computational Linguistics.
Ayman Alhelbawy and Rob Gaizauskas. 2012. Named entity based document similarity with svm-based re-
ranking for entity linking. In Advanced Machine Learning Technologies and Applications, volume 322 of
Communications in Computer and Information Science, pages 379?388. Springer Berlin Heidelberg.
Ayman Alhelbawy and Robert Gaizauskas. 2013. Named entity disambiguation using hmms. In Web Intelli-
gence (WI) and Intelligent Agent Technologies (IAT), 2013 IEEE/WIC/ACM International Joint Conferences on,
volume 3, pages 159?162.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A collaboratively
created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD Interna-
tional Conference on Management of Data, SIGMOD ?08, pages 1247?1250, New York, NY, USA. ACM.
Coen Bron and Joep Kerbosch. 1973. Algorithm 457: finding all cliques of an undirected graph. Communications
of the ACM, 16(9):575?577.
Razvan C. Bunescu and Marius Pasca. 2006. Using encyclopedic knowledge for named entity disambiguation. In
EACL. The Association for Computer Linguistics.
S. Cucerzan. 2007. Large-scale named entity disambiguation based on wikipedia data. In Proceedings of EMNLP-
CoNLL, volume 6, pages 708?716.
M. Dredze, P. McNamee, D. Rao, A. Gerber, and T. Finin. 2010. Entity disambiguation for knowledge base
population. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 277?
285. Association for Computational Linguistics.
S. Gottipati and J. Jiang. 2011. Linking entities to a knowledge base with query expansion. In Proceedings
of the Conference on Empirical Methods in Natural Language Processing, pages 804?813. Association for
Computational Linguistics.
Yoan Guti?errez, Sonia V?azquez, and Andr?es Montoyo. 2011. Word sense disambiguation: a graph-based approach
using n-cliques partitioning technique. In Natural Language Processing and Information Systems, pages 112?
124. Springer.
Yoan Guti?errez, Sonia V?azquez, and Andr?es Montoyo. 2012. A graph-based approach to wsd using relevant
semantic trees and n-cliques model. In Computational Linguistics and Intelligent Text Processing, pages 225?
237. Springer.
X. Han, L. Sun, and J. Zhao. 2011. Collective entity linking in web text: a graph-based method. In Proceedings
of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages
765?774. ACM.
J. Hoffart, M.A. Yosef, I. Bordino, H. F?urstenau, M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and G. Weikum.
2011. Robust disambiguation of named entities in text. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 782?792. Association for Computational Linguistics.
H. Ji and R. Grishman. 2011. Knowledge base population: successful approaches and challenges. In Proceedings
of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-
Volume 1, pages 1148?1158. Association for Computational Linguistics.
S. Kulkarni, A. Singh, G. Ramakrishnan, and S. Chakrabarti. 2009. Collective annotation of wikipedia entities in
web text. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data
mining, pages 457?466. ACM.
R.Duncan Luce and AlbertD. Perry. 1949. A method of matrix analysis of group structure. Psychometrika,
14(2):95?116.
P. McNamee and H.T. Dang. 2009. Overview of the tac 2009 knowledge base population track. In Text Analysis
Conference (TAC).
1554
D. Milne and I.H. Witten. 2008. Learning to link with wikipedia. In Proceeding of the 17th ACM conference on
Information and knowledge management, pages 509?518. ACM.
D. Nadeau and S. Sekine. 2007. A survey of named entity recognition and classification. Lingvisticae Investiga-
tiones, 30(1):3?26.
Kamel Nebhi. 2013. Named entity disambiguation using freebase and syntactic parsing. In CEUR-WS.org, editor,
Proceedings of the First International Workshop on Linked Data for Information Extraction (LD4IE 2013) co-
located with the 12th International Semantic Web Conference (ISWC 2013). Gentile, A.L. ; Zhang, Z. ; d?Amato,
C. & Paulheim, H.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The pagerank citation ranking: Bring-
ing order to the web. Technical Report 1999-66, November.
L. Ratinov, D. Roth, D. Downey, and M. Anderson. 2011. Local and global algorithms for disambiguation to
wikipedia. In Proceedings of the Annual Meeting of the Association of Computational Linguistics (ACL).
P. Sen. 2012. Collective context-aware topic models for entity disambiguation. In Proceedings of the 21st inter-
national conference on World Wide Web, pages 729?738. ACM.
Masumi Shirakawa, Haixun Wang, Yangqiu Song, Zhongyuan Wang, Kotaro Nakayama, Takahiro Hara, and Sho-
jiro Nishio. 2011. Entity disambiguation based on a. Technical report, Technical report, Technical Report
MSR-TR-2011-125, Microsoft Research.
Ravi Sinha and Rada Mihalcea. 2007. Unsupervised graph-basedword sense disambiguation using measures
of word semantic similarity. In Semantic Computing, 2007. ICSC 2007. International Conference on, pages
363?369. IEEE.
Wenpu Xing and Ali Ghorbani. 2004. Weighted pagerank algorithm. In Communication Networks and Services
Research, 2004. Proceedings. Second Annual Conference on, pages 305?314. IEEE.
Wei Zhang, Yan Chuan Sim, Jian Su, and Chew Lim Tan. 2011. Entity linking with effective acronym expansion,
instance selection and topic modeling. In Proceedings of the Twenty-Second international joint conference on
Artificial Intelligence-Volume Volume Three, pages 1909?1914. AAAI Press.
Z. Zheng, F. Li, M. Huang, and X. Zhu. 2010. Learning to link entities with knowledge base. In Human
Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for
Computational Linguistics, HLT ?10, pages 483?491. Association for Computational Linguistics.
1555
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 482?491,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Multi-document summarization using A* search and discriminative training
Ahmet Aker Trevor Cohn
Department of Computer Science
University of Sheffield, Sheffield, S1 4DP, UK
{a.aker, t.cohn, r.gaizauskas}@dcs.shef.ac.uk
Robert Gaizauskas
Abstract
In this paper we address two key challenges
for extractive multi-document summarization:
the search problem of finding the best scoring
summary and the training problem of learn-
ing the best model parameters. We propose an
A* search algorithm to find the best extractive
summary up to a given length, which is both
optimal and efficient to run. Further, we pro-
pose a discriminative training algorithm which
directly maximises the quality of the best sum-
mary, rather than assuming a sentence-level
decomposition as in earlier work. Our ap-
proach leads to significantly better results than
earlier techniques across a number of evalua-
tion metrics.
1 Introduction
Multi-document summarization aims to present
multiple documents in form of a short summary.
This short summary can be used as a replacement
for the original documents to reduce, for instance,
the time a reader would spend if she were to read
the original documents. Following dominant trends
in summarization research (Mani, 2001), we focus
solely on extractive summarization which simplifies
the summarization task to the problem of identify-
ing a subset of units from the document collection
(here sentences) which are concatenated to form the
summary.
Most multi-document summarization systems de-
fine a model which assigns a score to a candidate
summary based on the features of the sentences in-
cluded in the summary. The research challenges are
then twofold: 1) the search problem of finding the
best scoring summary for a given document set, and
2) the training problem of learning the model pa-
rameters to best describe a training set consisting of
pairs of document sets with model or reference sum-
maries ? typically human authored extractive or ab-
stractive summaries.
Search is typically performed by a greedy al-
gorithm which selects each sentence in decreasing
order of model score until the desired summary
length is reached (see, e.g., Saggion (2005)) or us-
ing heuristic strategies based on position in docu-
ment or lexical clues (Edmundson, 1969; Brandow
et al, 1995; Hearst, 1997; Ouyang et al, 2010).1
We show in this paper that the search problem can
be solved optimally and efficiently using A* search
(Russell et al, 1995). Assuming the model only uses
features local to each sentence in the summary, our
algorithm finds the best scoring extractive summary
up to a given length in words.
Framing summarization as search suggests that
many of the popular training techniques are max-
imising the wrong objective. These approaches train
a classifier, regression or ranking model to distin-
guish between good and bad sentences under an
evaluation metric, e.g., ROUGE (Lin, 2004). The
model is then used during search to find a summary
composed of high scoring (?good?) sentences (see
for a review Ouyang et al (2010)). However, there
is a disconnect between the model used for training
and the model used for prediction. In this paper we
present a solution to this disconnect in the form of
a training algorithm that optimises the full predic-
tion model directly with the search algorithm intact.
The training algorithm learns parameters such that
1Genetic algorithms have also been devised for solving the
search problem (see, e.g., Riedhammer et al (2008)), however
these approaches do not guarantee optimality, nor are they effi-
cient enough to be practicable for large datasets.
482
the best scoring whole summary under the model
has a high score under the evaluation metric. We
demonstrate that this leads to significantly better test
performance than a competitive baseline, to the tune
of 3% absolute increase for ROUGE-1, -2 and -SU4.
The paper is structured as follows. Section 2
presents the summarization model. Next in sec-
tion 3 we present an A* search algorithm for finding
the best scoring (argmax) summary under the model
with a constraint on the maximum summary length.
We show that this algorithm performs search effi-
ciently, even for very large document sets composed
of many sentences. The second contribution of the
paper is a new training method which directly opti-
mises the summarization system, and is presented in
section 4. This uses the minimum error-rate training
(MERT) technique from machine translation (Och,
2003) to optimise the summariser?s output to an ar-
bitrary evaluation metric. Section 5 describes our
experimental setup and section 6 the results. Finally
we conclude in section 7.
2 Summarization Model
Extractive multi-document summarization aims to
find the most important sentences from a set of doc-
uments, which are then collated and presented to
the user in form of a short summary. Following
the predominant approach to data-driven summari-
sation, we define a linear model which scores sum-
maries as the weighted sum of their features,
s(y|x) = ?(x,y) ? ? , (1)
where x is the document set, composed of k sen-
tences, y ? {1 . . . k} are the set of selected sen-
tence indices, ?(?, ?) is a feature function which re-
turns a vector of features for the candidate summary
and ? are the model parameters. We further assume
that the features decompose with the sentences in
the summary, ?(x,y) =
?
i?y ?(xi), and there-
fore the scoring function also decomposes along the
same lines,
s(y|x) =
?
i?y
?(xi) ? ? . (2)
While this assumption greatly simplifies inference, it
does constrain the representative power of the model
by disallowing global features, e.g., those which
measure duplication in the summary.2 Under this
model, the search problem is to solve
y? = arg max
y
s(y|x) , (3)
for which we develop a best-first algorithm using A*
search, as described in section 3. The training chal-
lenge is to find the parameters, ?, to best model the
training set. This is achieved by finding ? such that
y? is similar to the gold standard summary accord-
ing to an automatic evaluation metric, as described
in section 4.
3 A* Search
The prediction problem is to find the best scoring
extractive summary (see Equation 3) up to a given
length, L. At first glance, this appears to be a sim-
ple problem that might be solved efficiently with a
greedy algorithm, say by taking the sentences in or-
der of decreasing score and stopping just before the
summary exceeds the length threshold. However,
the greedy algorithm cannot be guaranteed to find
the best summary; to do so requires arbitrary back-
tracking to revise previous incorrect decisions.
The problem of constructing the summary can be
considered a search problem in which we start with
an empty summary and incrementally enlarge the
summary by concatenating a sentence from our doc-
ument set. The search graph starts with an empty
summary (the starting state) and each outgoing edge
adds a sentence to produce a subsequent state, and
is assigned a score under the model. A goal state is
any state with no more words than the given thresh-
old. The summarisation problem is then equivalent
to finding the best scoring path (summed over the
edge scores) between the start state and a goal state.
The novel insight in our work is to use A* search
(Russell et al, 1995) to solve the prediction prob-
lem. A* is a best-first search algorithm which can
efficiently find the best scoring path or the n-best
paths (unlike the greedy algorithm which is not op-
timal, and the backtracking variant which is not ef-
ficient). The search procedure requires a scoring
function for each state, here s(y|x) from (2), and
2Our approach could be adapted to support global features,
which would require changes to the heuristic for A* search to
bound the score obtainable from the global features. This may
incur an additional computational cost over a purely local fea-
ture model and perhaps also necessitate using beam search.
483
a heuristic function which estimates the additional
score to get from a given state to a goal state. For
the search to be optimal ? guaranteed to find the best
scoring path as the first solution ? the heuristic must
be admissible, meaning that it bounds from above
the score for reaching a goal state. We present three
different admissible heuristics later in this section,
which bound the score with differing tightness and
consequently different search cost.
Algorithm 1 presents A* search for our extractive
summarisation model. Given a set of sentences to
summary, a scoring and a heuristic function, it finds
the best scoring summary. This is achieved by build-
ing the search graph incrementally, and storing each
frontier state in a priority queue (line 1) which is
sorted by the sum of the state?s score and its heuris-
tic. These states are popped off the queue (line 3)
and expanded by adding a sentence, which is then
added to the schedule (lines 8?14). We designate
special finishing states using a boolean variable (the
last entry in the tuple in lines 1, 7 and 12). Fin-
ishing states (with value T) denote ceasing to ex-
pand the summary, and consequently their scores
do not include the heuristic component. When-
ever one of these states is popped in line 2, we
know that it outscores all competing hypotheses and
therefore represents the optimal summary (because
the heuristic is guaranteed to never underestimate
the cost to a goal state from an unfinished state).3
Note that in algorithm 1 we create the summary
by building a list of sentence indices in sorted or-
der to avoid spurious ambiguity which would un-
necessarily expand the search space. The function
length(y,x) =
?
n?y length(xn) returns the length
of sentences specified.
We now return to the problem of defining the
heuristic function, h(y;x, l) which provides an up-
per bound on the additional score achievable in
reaching a goal state from state y. We present three
different variants of increasing fidelity, that is, that
bound the cost to a goal state more tightly. Algo-
rithm 2 is the simplest, which simply finds the max-
imum score per word from the set of unused sen-
3To improve the efficiency of Algorithm 1 we make a small
modification to avoid expanding every possible edge in step 8,
of which there are O(k) options. Instead we expand a small
number (here, 3) at a time and defer the remaining items until
later by inserting a special node into the schedule. These special
nodes are represented using a third ?to-be-continued? state into
the done flag.
Algorithm 1 A* search for extractive summarization.
Require: set of sentences, x = x1, . . . , xk
Require: scoring function s(?)
Require: heuristic function h(?)
Require: summary length limit L
1: schedule = [(0, ?, F)] {priority queue of triples}
{(A* score, sentence indices, done flag)}
2: while schedule 6= [] do
3: v,y, f ? pop(schedule)
4: if f = T then
5: return y {success}
6: else
7: push(schedule, (s(y|x),y,T))
8: for y ? [max(y) + 1, k] do
9: y? ? y ? y
10: if length(y?,x) ? L then
11: v? ? s(y?|x) + h(y?;x, l)
12: push(schedule, (v?,y?, F))
13: end if
14: end for
15: end if
16: end while
tences and then extrapolates this out over the re-
maining words available to the length threshold. In
the algorithm, we use the shorthand sn = ?(xn) ? ?
for sentence n?s score, ln = length(xn) for its length
and ly =
?
n?y ln for the total length of the current
state (unfinished summary).
Algorithm 2 Uniform heuristic, h1(y;x, L)
Require: x sorted in order of score/length
1: n? max(y) + 1
2: return (L? ly)max
(
sn
ln
, 0
)
The h1 heuristic is overly simple in that it assumes
we can ?reuse? a high scoring short sentence many
times despite this being disallowed by the model.
For this reason we develop an improved bound, h2,
in Algorithm 3. This incrementally adds each sen-
tence in order of its score-per-word until the length
limit is reached. If the limit is to be exceeded,
the heuristic scales down the final sentence?s score
based on the fraction of words than can be used to
reach the limit.
The fractional usage of the final sentence in h2
could be considered overly optimistic, especially
when the state has length just shy of the limit L. If
the next best ranked sentence is a long one, then it
will be used in the heuristic to over-estimate of the
state. This is complicated to correct, and doing so
exactly would require full backtracking which is in-
tractable and would obviate the entire point of using
A* search. Instead we use a subtle modification in
h3 (Alg. 4) which is equivalent to h2 except in the
484
Algorithm 3 Aggregated heuristic, h2(y;x, L)
Require: x sorted in order of score/length
1: v ? 0
2: l? ? ly
3: for n ? [max(y) + 1, k] do
4: if sn ? 0 then
5: return v
6: end if
7: if l? + ln ? L then
8: l? ? l? + ln
9: v ? v + sn
10: else
11: return v + lnL?l? sn
12: end if
13: end for
14: return v
instance where the next best score/word sentence is
too long, where it skips over these sentences until
it finds the best scoring sentence that does fit. This
helps to address the overestimate of h2 and should
therefore lead to a smaller search graph and faster
runtime due to its early elimination of dead-ends.
Algorithm 4 Agg.+final heuristic, h3(y;x, L)
Require: x sorted in order of score/length
1: n? max(y) + 1
2: if n ? k ? sn > 0 then
3: if ly + ln ? L then
4: return h2(y;x, L)
5: else
6: form ? [n+ 1, k] do
7: if ly + lm ? L then
8: return sm
L?ly
lm
9: end if
10: end for
11: end if
12: end if
13: return 0
The search process is illustrated in figure 1. When
a node is visited in the search, if it satisfied the
length constraint then the all its child nodes are
added to the schedule. These nodes are scored with
the score for the summary thus far plus a heuristic
term. For example, the value of 4+1.5=5.5 for the
{1} node arises from a score of 4 plus a heuristic of
(7? 5) ? 34 = 1.5, reflecting the additional score that
would arise if it were to use half of the next sentence
to finish the summary. Note that in finding the best
two summaries the search process did not need to
instantiate the full search graph.
To test the efficacy of A* search with each of the
different heuristic functions, we now present empir-
ical runtime results. We used the training data as
described in Section 5.2 and for each document set
start
(4+1.5,{1},F)
+1 (3+2,{2},F)+2 (2+2,{3},F)+3
(1+0,{4},F)+4
(0,{},T)
finish
(7+0,{1,2},F)+2 (6+0,{1,3},F)+3
(5+0,{1,4},F)+4
(5+0,{2,3},F)+3
(4+0,{2,4},F)+4
(5,{1,4},T)finish
(6+0,{2,3,4},F)+4
(5,{2,3},T)finish
Figure 1: Example of the A* search graph created to find
the two top scoring summaries of length ? 7 when sum-
marising four sentences with scores of 4, 3, 2 and 1 re-
spectively and lengths of 5, 4, 3 and 1 respectively. The
h1 heuristic was used and the score and heuristic scores
are shown separately for clarity. Bold nodes were visited
while dashed nodes were visited but found to exceed the
length constraint.
generated the 100-best summaries with word limit
L = 200. Figure 2 shows the number of nodes
and edges visited by A* search, reflecting the space
and time cost of the algorithm, as a function of the
number of sentences in the document set being sum-
marised. All three heuristics shown an empirical
increase in complexity that is roughly linear in the
document size, although there are some notable out-
liers, particularly for the uniform heuristic. Surpris-
ingly the aggregated heuristic, h2, is not consider-
ably more efficient than the uniform heuristic h1,
despite bounding the cost more precisely. However,
the aggregated+final heuristic, h3, consistently out-
performs the other two methods. For this reason we
have used h3 in all subsequent experimentation.
4 Training
We frame the training problem as one of finding
model parameters, ?, such that the predicted out-
put, y? closely matches the gold standard, r.4 The
quality of the match is measured using an automatic
evaluation metric. We adopt the standard machine
learning terminology of loss functions, which mea-
sure the degree of error in the prediction, ?(y?, r).
In our case the accuracy is measured by the ROUGE
4The gold standard is typically an abstractive summary, and
as such it is usually impossible for an extractive summarizer to
match it exactly.
485
ll
l
l
l
l
l
l l
l
l
l
l
l
l
ll
l
l
l
l
lll
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
lll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l l
l
l
l
l
l
l
l
l
l l l
l
ll l
l l
l
l
l
l
l
l
l
l
l l
ll
l l
l
ll
l
l
l
ll
l
l
ll
l
l
l
ll
ll
l l
l
l
l
l
l
l l
l
l
ll
l
l
l
l
l
l
l
l
ll
l
l
5 10 20 50 100 200 500 1000 2000
1e+
02
1e+
03
1e+
04
1e+
05
1e+
06
sentences in document set
tota
l edg
es a
nd n
odes
l uniformaggregatedaggregated+final
Figure 2: Efficiency of A* search search is roughly linear
in the number of sentences in the document set. The y
axis measures the search graph size in terms of the num-
ber of edges in the schedule and the number of nodes
visited. Measured with the final parameters after training
to optimise ROUGE-2 with the three different heuristics
and expanding five nodes in each step.
score, R, and the loss is simply 1 - R. The training
problem is to solve
?? = arg min
?
?(y?, r) , (4)
where with a slight abuse of notation, y? and r are
taken to range over the corpus of many document-
sets and summaries.
To optimise the weights we use the minimum er-
ror rate training (MERT) technique (Och, 2003), as
used for training statistical machine translation sys-
tems. This approach is a first order optimization
method using Powell search to find the parameters
which minimise the loss on the training data. MERT
requires n-best lists which it uses to approximate
the full space of possible outcomes. We use the
A* search algorithm to construct these n-best lists,5
and use MERT to optimise the ROUGE score on the
training set for the R-1, R-2 and R-SU4 variants of
the metric.
5We used n = 100 in our experiments.
5 Experimental settings
In this section we describe the features for which we
learn weights. We also describe the input data used
in training and testing.
5.1 Summarization system
The summarizer we use is an extractive, query-based
multi-document summarization system. It is given
two inputs: a query (place name) associated with an
image and a set of documents. The summarizer uses
the following features, as reported in previous work
(Edmundson, 1969; Brandow et al, 1995; Radev et
al., 2001; Conroy et al, 2005; Aker and Gaizauskas,
2009; Aker and Gaizauskas, 2010a):
? querySimilarity: Sentence similarity to the
query (cosine similarity over the vector repre-
sentation of the sentence and the query).
? centroidSimilarity: Sentence similarity to the
centroid. The centroid is composed of the 100
most frequently occurring non stop words in
the document collection (cosine similarity over
the vector representation of the sentence and
the centroid). For each word/term in the vec-
tor we store a value which is the product of
the term frequency in the document and the in-
verse document frequency, a measurement of
the term?s distribution over the set of docu-
ments (Salton and Buckley, 1988).
? sentencePosition: Position of the sentence
within its document. The first sentence in the
document gets the score 1 and the last one gets
1
n where n is the number of sentences in the
document.
? inFirst5: Binary feature indicating whether the
sentence occurs is one of the first 5 sentences
of the document.
? isStarter: A sentence gets a binary score if it
starts with the query term (e.g. Westminster
Abbey, The Westminster Abbey, The Westmin-
ster or The Abbey) or with the object type, e.g.
The church. We also allow gaps (up to four
words) between the and the query/object type
to capture cases such as The most magnificent
abbey, etc.
? LMProb: The probability of the sentence un-
der a unigram language model. We trained
a separate language model on Wikipedia arti-
cles about locations for each object type, e.g.,
486
church, bridge, etc. When we generate a sum-
mary about a location of type church, for in-
stance, then we apply the church language
model on the related input documents related
to the location.6
? sentenceCount: Each sentence gets assigned a
value of 1. This feature is used to learn whether
summaries with many sentences are better than
summaries with few sentences or vice versa.
? wordCount: Number of words in the summary,
to decide whether the model should favor long
summaries or short ones.
5.2 Data
For training and testing we use the freely avail-
able image description corpus described in Aker and
Gaizauskas (2010b). The corpus is based around
289 images of static located objects (e.g Eiffel
Tower, Mont Blanc) each with a manually assigned
place name and object type category (e.g. church,
mountain). For each place name there are up to
four model summaries that were created manually
after reading existing image descriptions taken from
the VirtualTourist travel community web-site. Each
summary contains a minimum of 190 and a maxi-
mum of 210 words. We divide this set of 289 place
names into training and testing sets. Both sets are
described in the following subsections.
Training We use 184 place names from the 289
set for training feature weights. For each train-
ing place name we gather all descriptions associ-
ated with it from VirtualTourist. We compute for
each sentence in each description a ROUGE score
by comparing the sentence to those included in the
model summaries for that particular place name and
retaining the highest score. Table 1 gives some de-
tails about this training data.
We use ROUGE as a metric to maximize be-
cause it is also used in DUC7 and TAC.8 How-
ever, it should be noted that any automatic metric
could be used instead of ROUGE. In particular we
use ROUGE 1 (R-1), ROUGE 2 (R-2) and ROUGE
SU4 (R-SU4). R-1 and R-2 compute the number
6For our training and testing sets we manually assigned each
location to its corresponding object type (Aker and Gaizauskas,
2009).
7http://duc.nist.gov/
8http://www.nist.gov/tac/
Max Min Avg
Sentences/place 1724 3 260
Words/sentence 37 3 17
Table 1: The training input data contains 184 place
names with 42333 sentences in total. The numbers in
the columns give detail about the number of sentences
for each place and the lengths of the sentences.
Max Min Avg
Documents/place 20 5 12
Sentences/place 1716 15 132
Sentences/document 275 1 10
Words/sentence 211 1 20
Table 2: In domain test data. The numbers in the columns
give detail about the number of documents (descriptions)
for each place, number of sentences for each place and
document (description) and the lengths of the sentences.
of uni-gram and bi-gram overlaps, respectively, be-
tween the automatic and model summaries. R-SU4
allows bi-grams to be composed of non-contiguous
words, with a maximum of four words between the
bi-grams.
Testing For testing purposes we use the rest of
the place names (105) from the 289 place name
set. For each place name we use a set of input
documents, generate a summary from these docu-
ments using our summarizer and compare the results
against model summaries of that place name using
ROUGE. We experimented with two different input
document types: out of domain and in domain.
The in domain documents are the VirtualTourist
original image descriptions from which the model
summaries were derived. As with the training set
we take all place name descriptions for a particular
place and use them as input documents to our sum-
marizer. Table 2 summarizes these input documents.
The out of domain documents are retrieved from
the web. Compared to the in domain documents
these documents should more challenging to sum-
marize because they will contain different kinds
of documents to those seen in training. For each
place name we retrieved the top ten related web-
documents using the Yahoo! search engine with the
place name as a query. The text from these docu-
ments is extracted using an HTML parser and passed
to the summarizer. Table 3 gives an overview of this
data.
487
Max Min Avg
Sentences/place 1773 55 328
Sentence/document 874 1 32
Words/sentence 236 1 21
Table 3: Out of domain test data. The numbers in the
columns give detail about the number of sentences for
each place and document and the lengths of the sentences.
6 Results
To evaluate our approach we used two different as-
sessment methods: ROUGE (Lin, 2004) and manual
readability. In the following we present the results
of each assessment.
6.1 Automatic Evaluation using ROUGE
We report results for training and testing. In
both training and testing we distinguish between
three different summaries: wordLimit, sentence-
Limit and regression. WordLimit and sentenceLimit
summaries are the ones generated using the model
trained by MERT. As described in section 4 we
trained the summariser using the A* search decoder
to maximise the ROUGE score of the best scoring
summaries. We used the heuristic function h3 in
A* search because it is the best performing heuris-
tic, and 100-best lists. To experiment with differ-
ent summary length conditions we differentiate be-
tween summaries with a word limit (wordLimit, set
to 200 words) and summaries containing N number
of sentences (sentenceLimit) as stop condition in A*
search. We set N so that in both wordLimit and sen-
tenceLimit summaries we obtain more or less the
same number of words (because our training data
contains on average 17 words for each word we set
N to 12, 12*17=194). However, this is only the case
in the training. In the testing for both wordLimit and
sentenceLimit we generate summaries with the same
word limit constraint which allows us to have a fair
comparison between the ROUGE recall scores.
The regression summaries are our baseline. In
these summaries the sentences are ranked based on
the weighted features produced by Support Vec-
tor Regression (SVR).9 Ouyang et al (2010) use
multi-document summarization and linear regres-
sion methods to rank sentences in the documents.
As regression model they used SVR and showed
9We use the term regression to refer to SVR.
Type metric R-1 R-2 R-SU4
wordLimit
R-1 0.5792 0.3176 0.3580
R-2 0.5656 0.3208 0.3510
R-SU4 0.5688 0.3197 0.3585
sentenceLimit
R-1 0.5915 0.3507 0.3881
R-2 0.5783 0.3601 0.3890
R-SU4 0.5870 0.3546 0.3929
regression
R-1 0.4993 0.1946 0.2448
R-2 0.4833 0.1949 0.2413
R-SU4 0.5009 0.2031 0.2562
Table 4: ROUGE scores obtained on the training data.
that it out-performed classification and Learning To
Rank methods on the DUC 2005 to 2007 data. For
comparison purpose we use SVR as a baseline sys-
tem for learning feature weights. It should be noted
that these weights are learned based on single sen-
tences. However, to have a fair comparison between
all our summary types we use these weights to gen-
erate summaries using the A* search with the word
limit as constraint. We do this for reporting both for
training and testing results.
The results for training are shown in Table 4. The
table shows ROUGE recall numbers obtained by
comparing model summaries against automatically
generated summaries on the training data. Because
in training we used three different metrics (R-1, R-2,
R-SU4) to train weights we report results for each of
these three different ROUGE metrics.
In Table 4 we can see that the scores for wordLimit
and sentenceLimit type summaries are always at
maximum on the metric they were trained on (this
can be observed by following the main diagonal of
the result matrix). This confirms that MERT is max-
imizing the metric for which it was trained. How-
ever, this is not the case for regression results. The
scores obtained with R-SU4 metric trained weights
achieve higher scores on R-1 and R-2 compared to
the scores obtained using weights trained on those
metrics. This is most likely due to SVR being
trained on sentences rather than over entire sum-
maries, and thereby not adequately optimising the
metric used for evaluation.
The results for testing are shown in Tables 5 and
6. As with the training setting we report ROUGE re-
call scores. We use the testing data described in sec-
tion 5.2 for this setting. However, because we have
two different input document sets we report sepa-
rate results for each of these (Table 5 shows result
for in domain data and Table 6 shows result for out
488
Type metric R-1 R-2 R-SU4
wordLimit
R-1 0.3733 0.0842 0.1399
R-2 0.3731 0.0842 0.1402
R-SU4 0.3627 0.0794 0.1340
sentenceLimit
R-1 0.3664 0.0774 0.1321
R-2 0.3559 0.0717 0.1251
R-SU4 0.3629 0.0778 0.1312
regression
R-1 0.3431 0.0669 0.1229
R-2 0.2934 0.0560 0.1043
R-SU4 0.3417 0.0668 0.1226
Table 5: ROUGE scores obtained on the testing data. The
automated summaries are generated using the in domain
input documents.
Type metric R-1 R-2 R-SU4
wordLimit
R-1 0.3758 0.0882 0.1421
R-2 0.3755 0.0895 0.1423
R-SU4 0.369 0.0812 0.137
sentenceLimit
R-1 0.3541 0.0693 0.1226
R-2 0.3426 0.0638 0.1157
R-SU4 0.3573 0.073 0.1251
regression
R-1 0.3392 0.0611 0.1179
R-2 0.3422 0.0606 0.1164
R-SU4 0.3413 0.0606 0.1176
Table 6: ROUGE scores obtained on the testing data. The
automated summaries are generated using the out of do-
main input documents.
of domain data). Again as with the training setting
we report results for the different metrics (R-1, R-2,
R-SU4) separately.
From Table 5 we can see that the wordLimit sum-
maries score highest compared to the other two types
of summaries. This is different from the train-
ing results where sentenceLimit summary type sum-
maries are the top scoring ones. As mentioned ear-
lier the sentenceLimit summaries contain exactly 12
sentences, where on average each sentence in the
training data has 17 words. We picked 12 sen-
tences to achieve roughly the same word limit con-
straint (12 ? 17 = 204) so they can be compared
to the wordLimit and regression type summaries.
However, these sentenceLimit summaries have an
average of 221 words, which explains the higher
ROUGE recall scores seen in training compared to
testing (where a 200 word limit was imposed).
The wordLimit summaries are significantly better
than the scores from the other summary types ir-
respective of the evaluation metric.10 It should be
10Significance is reported at level p < 0.001. We used
Wilcoxson signed ranked test to perform significance.
noted that these summaries are the only ones where
the training and testing had the same condition in
A* search concerning the summary word limit con-
straint. The scores in sentenceLimit type summaries
are significantly lower than wordLimit summaries,
despite using MERT to learn the weights. This
shows that training the true model is critical for
getting good accuracy. The regression type sum-
maries achieved the worst ROUGE metric scores.
The weights used to generate these summaries were
trained on single sentences using SVR. These results
indicate that if the goal is to generate high scoring
summaries under a length limit in testing, then the
same constraint should also be used in training.
From Table 5 and 6 we can see that the summaries
obtained from VirtualTourist captions (in domain
data) score roughly the same as the summaries gen-
erated using web-documents (out of domain data) as
input. A possible explanation is that in many cases
the VirtualTourist original captions contain text from
Wikipedia articles, which are also returned as results
from the web search. Therefore the web-document
sets included similar content to the VirtualTourist
captions.
6.2 Manual Evaluation
We also evaluated our summaries using a readabil-
ity assessment as in DUC and TAC. DUC and TAC
manually assess the quality of automatically gener-
ated summaries by asking human subjects to score
each summary using five criteria ? grammaticality,
redundancy, clarity, focus and coherence criteria.
Each criterion is scored on a five point scale with
high scores indicating a better result (Dang, 2005).
For this evaluation we used the best scoring sum-
maries from the wordLimit summary type (R-1, R-2
and R-SU4) generated using web-documents (out of
domain documents) as input. We also evaluate the
regression summary types generated using the same
input documents to investigate the correlation be-
tween high and low ROUGE metric scores to man-
ual evaluation ones. From the regression summary
type we only use summaries under the R2 and RSU4
trained models.
In total we evaluated five different summary types
(three from wordLimit and two from regression).
For each type we randomly selected 30 place names
and asked three people to assess the summaries for
these place names. Each person was shown all 150
489
Criterion wordLimit regression
R1 R2 RSU4 R2 RSU4
clarity 4.03 3.92 3.99 3.00 2.92
coherence 3.31 3.06 2.99 2.12 1.88
focus 3.79 3.56 3.54 2.44 2.29
grammaticality 4.21 4.13 4.13 3.93 3.87
redundancy 4.19 4.33 4.41 4.47 4.44
Table 7: Manual evaluation results for the wordLimit (R1,
R2, RSU4) and regression (R2, RSU4) summary types.
The numbers in the columns are the average scores.
summaries (30 from each summary type) in a ran-
dom way and was asked to assess them according to
the DUC and TAC manual assessment scheme. The
results are shown in Table 7.11
From Table 7 we can see that overall the
wordLimit type summaries perform better than the
regression ones. For each metric in regression sum-
mary types (R-2 and R-SU4) we compute the sig-
nificance of the difference with the same metrics
in wordLimit summary types.12 The results for the
clarity, coherence and focus criteria in wordLimit
summaries are significantly better than in regression
ones (p<0.001) irrespective of the training metric.
These results concur with the automatic evaluation
results as described in section 6.1. However, this
is not the case for the grammaticality and redun-
dancy criteria. Although in regression type sum-
maries the scores for the grammaticality criterion
are lower than those in wordLimit summaries the
difference is not significant. Furthermore, we can
see that the redundancy scores for regression sum-
maries are slightly higher than those for wordLimit
summaries.
One reason for these differences might be the
way we trained feature weights for wordLimit and
regression summaries. As mentioned above, fea-
ture weights for wordLimit summaries are trained
using summaries with a specific word limit con-
straint, whereas the weights for the regression sum-
maries are learned using single sentences. Maxi-
mizing the ROUGE metrics using ?final or output
11We computed the agreement between the users using intra
class correlation with Cronbach?s Alpha where the correlation
coefficient ranges between 0 and 1. Numbers close to 1 indicate
high correlation and numbers close to 0 indicate low correlation.
For the clarity criterion the assessors? correlation coefficient is
0.547, for coherence 0.687, for focus 0.688, for grammaticality
0.232 and for redundancy 0.453.
12We compute significance test for the manual evaluation re-
sults using ? square.
like summaries? will lead to a higher content agree-
ment between the training and the model summaries
whereas this is not guaranteed with single sentences.
With single sentences we have only a guarantee for
high content overlap between single training and
model sentences. However, when these sentences
are combined into summaries it is not guaranteed
that these summaries will also have high content
overlap with the entire model ones. Therefore we
believe if there is a high content agreement between
the training and model summaries this could lead to
more readable summaries. However, as we can see
from Table 7 this hypothesis does not hold for all
criteria. In case of the redundancy criterion we have
compared to wordLimit summary type high scores
in regression summaries although wordLimit sum-
maries are significantly better than regression ones
when it concerns the ROUGE scores. Thus it is
likely that by aggressively optimising the ROUGE
metric the model learns to game the metric, which
does not penalise redundancy in the summaries.
As such it may no longer possible to extrapolate
trends from earlier correlation studies against human
judgements (Lin, 2004).
To minimize redundancy in summaries it is nec-
essary to also take into consideration global features
addressing the linguistic aspects of the summaries.
Furthermore, instead of ROUGE recall scores which
do not take the repetition of information into consid-
eration, ROUGE precision scores could be used as a
metric in order to minimize the redundant content in
the summaries.
7 Conclusion
In this paper we have proposed an A* search ap-
proach for generating a summary from a ranked list
of sentences and learning feature weights for a fea-
ture based extractive multi-document summariza-
tion system. We developed an algorithm to learn
optimize an arbitrary metric and showed that our
approach significantly outperforms state of the art
techniques. Furthermore, we highlighted the impor-
tance of uniformity in training and testing and ar-
gued that if the goal is to generate high scoring sum-
maries under a length limit in testing, then the same
constraint should also be used in training.
In this paper we experimented with sentence-local
features. In the future we plan to expand this fea-
ture set with global features, especially ones mea-
490
suring lexical diversity in the summaries to reduce
the redundancy in them. We will investigate vari-
ous ways of incorporating these global features into
our A* search. However this will incur an additional
computational cost over a purely local feature model
and therefore may necessitate using an approximate
beam search. We also plan to investigate using other
metrics in training in order to reduce redundant in-
formation in the summaries. Finally, we have made
our summarizer publicly available as open-source
software.13
References
A. Aker and R. Gaizauskas. 2009. Summary Gener-
ation for Toponym-Referenced Images using Object
Type Language Models. International Conference
on Recent Advances in Natural Language Processing
(RANLP) September 14-16, 2009, Borovets, Bulgaria.
A. Aker and R. Gaizauskas. 2010a. Generating im-
age descriptions using dependency relational patterns.
Proc. of the ACL 2010, Upsala, Sweden.
A. Aker and R. Gaizauskas. 2010b. Model Summaries
for Location-related Images. In Proc. of the LREC-
2010 Conference.
R. Brandow, K. Mitze, and L.F. Rau. 1995. Automatic
condensation of electronic publications by sentence
selection* 1. Information Processing & Management,
31(5):675?685.
J.M. Conroy, J.D. Schlesinger, and J.G. Stewart. 2005.
CLASSY query-based multi-document summariza-
tion. Proc. of the 2005 Document Understanding
Workshop, Boston.
H.T. Dang. 2005. Overview of DUC 2005. DUC 05
Workshop at HLT/EMNLP.
H. Edmundson, P. 1969. New Methods in Automatic
Extracting. Journal of the Association for Computing
Machinery, 16:264?285.
M.A. Hearst. 1997. TextTiling: segmenting text into
multi-paragraph subtopic passages. Computational
linguistics, 23(1):33?64.
C-Y. Lin. 2004. Rouge: A package for automatic evalua-
tion of summaries. Text Summarization Branches Out:
Proc. of the ACL-04 Workshop, pages 74?81.
I. Mani. 2001. Automatic Summarization. John Ben-
jamins Publishing Company.
F.J. Och. 2003. Minimum error rate training in statistical
machine translation. Proc. of the 41st Annual Meeting
on Association for Computational Linguistics-Volume
1, page 167.
13Available from http://www.dcs.shef.ac.uk/
?tcohn/a-star
Y. Ouyang, W. Li, S. Li, and Q. Lu. 2010. Applying
regression models to query-focused multi-document
summarization. Information Processing & Manage-
ment.
D.R. Radev, S. Blair-Goldensohn, and Z. Zhang. 2001.
Experiments in single and multi-document summa-
rization using MEAD. Document Understanding Con-
ference.
K. Riedhammer, D. Gillick, B. Favre, and D. Hakkani-T
?ur. 2008. Packing the meeting summarization knap-
sack. Proc. Interspeech, Brisbane, Australia.
S.J. Russell, P. Norvig, J.F. Canny, J. Malik, and D.D.
Edwards. 1995. Artificial intelligence: a modern ap-
proach. Prentice hall Englewood Cliffs, NJ.
H. Saggion. 2005. Topic-based Summarization at
DUC 2005. Document Understanding Conference
(DUC05).
G. Salton and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information Pro-
cessing and Management: an International Journal,
24(5):513?523.
491
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1250?1258,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Generating image descriptions using dependency relational patterns
Ahmet Aker
University of Sheffield
a.aker@dcs.shef.ac.uk
Robert Gaizauskas
University of Sheffield
r.gaizauskas@dcs.shef.ac.uk
Abstract
This paper presents a novel approach
to automatic captioning of geo-tagged
images by summarizing multiple web-
documents that contain information re-
lated to an image?s location. The summa-
rizer is biased by dependency pattern mod-
els towards sentences which contain fea-
tures typically provided for different scene
types such as those of churches, bridges,
etc. Our results show that summaries bi-
ased by dependency pattern models lead
to significantly higher ROUGE scores than
both n-gram language models reported in
previous work and also Wikipedia base-
line summaries. Summaries generated us-
ing dependency patterns also lead to more
readable summaries than those generated
without dependency patterns.
1 Introduction
The number of images tagged with location infor-
mation on the web is growing rapidly, facilitated
by the availability of GPS (Global Position Sys-
tem) equipped cameras and phones, as well as by
the widespread use of online social sites. The ma-
jority of these images are indexed with GPS coor-
dinates (latitude and longitude) only and/or have
minimal captions. This typically small amount of
textual information associated with the image is of
limited usefulness for image indexing, organiza-
tion and search. Therefore methods which could
automatically supplement the information avail-
able for image indexing and lead to improved im-
age retrieval would be extremely useful.
Following the general approach proposed by
Aker and Gaizauskas (2009), in this paper we
describe a method for automatic image caption-
ing or caption enhancement starting with only a
scene or subject type and a set of place names per-
taining to an image ? for example ?church, {St.
Paul?s,London}?. Scene type and place names can
be obtained automatically given GPS coordinates
and compass information using techniques such as
those described in Xin et al (2010) ? that task is
not the focus of this paper.
Our method applies only to images of static fea-
tures of the built or natural landscape, i.e. objects
with persistent geo-coordinates, such as buildings
and mountains, and not to images of objects which
move about in such landscapes, e.g. people, cars,
clouds, etc. However, our technique is suitable not
only for image captioning but in any application
context that requires summary descriptions of in-
stances of object classes, where the instance is to
be characterized in terms of the features typically
mentioned in describing members of the class.
Aker and Gaizauskas (2009) have argued that
humans appear to have a conceptual model of
what is salient regarding a certain object type (e.g.
church, bridge, etc.) and that this model informs
their choice of what to say when describing an in-
stance of this type. They also experimented with
representing such conceptual models using n-gram
language models derived from corpora consisting
of collections of descriptions of instances of spe-
cific object types (e.g. a corpus of descriptions of
churches, a corpus of bridge descriptions, and so
on) and reported results showing that incorporat-
ing such n-gram language models as a feature in a
feature-based extractive summarizer improves the
quality of automatically generated summaries.
The main weakness of n-gram language mod-
els is that they only capture very local information
about short term sequences and cannot model long
distance dependencies between terms. For exam-
ple one common and important feature of object
descriptions is the simple specification of the ob-
ject type, e.g. the information that the object Lon-
don Bridge is a bridge or that the Rhine is a river.
If this information is expressed as in the first line
of Table 1, n-gram language models are likely to
1250
Table 1: Example of sentences which express the type of an object.
London Bridge is a bridge...
The Rhine (German: Rhein; Dutch: Rijn; French: Rhin; Romansh: Rain;
Italian: Reno; Latin: Rhenus West Frisian Ryn) is one of the longest and
most important rivers in Europe...
reflect it, since one would expect the tri-gram is a
bridge to occur with high frequency in a corpus of
bridge descriptions. However, if the type predica-
tion occurs with less commonly seen local context,
as is the case for the object Rhine in the second
row of Table 1 ? most important rivers ? n-gram
language models may well be unable to identify it.
Intuitively, what is important in both these cases
is that there is a predication whose subject is the
object instance of interest and the head of whose
complement is the object type: London Bridge ...
is ... bridge and Rhine ... is ... river. Sentences
matching such patterns are likely to be important
ones to include in a summary. This intuition sug-
gests that rather than representing object type con-
ceptual models via corpus-derived language mod-
els as do Aker and Gaizauskas (2009), we do so in-
stead using corpus-derived dependency patterns.
We pursue this idea in this paper, our hy-
pothesis being that information that is important
for describing objects of a given type will fre-
quently be realized linguistically via expressions
with the same dependency structure. We explore
this hypothesis by developing a method for deriv-
ing common dependency patterns from object type
corpora (Section 2) and then incorporating these
patterns into an extractive summarization system
(Section 3). In Section 4 we evaluate the approach
both by scoring against model summaries and via
a readability assessment. Since our work aims to
extend the work of Aker and Gaizauskas (2009)
we reproduce their experiments with n-gram lan-
guage models in the current setting so as to permit
accurate comparison.
Multi-document summarizers face the problem
of avoiding redundancy: often, important infor-
mation which must be included in the summary
is repeated several times across the document set,
but must be included in the summary only once.
We can use the dependency pattern approach to
address this problem in a novel way. The com-
mon approach to avoiding redundancy is to use a
text similarity measure to block the addition of a
further sentence to the summary if it is too simi-
lar to one already included. Instead, since specific
dependency patterns express specific types of in-
Table 2: Object types and the number of articles in each object type cor-
pus. Object types which are bold are covered by the evaluation image set.
village 39970, school 15794, city 14233, organization 9393, university
7101, area 6934, district 6565, airport 6493, island 6400, railway station
5905, river 5851, company 5734, mountain 5290, park 3754, college 3749,
stadium 3665, lake 3649, road 3421, country 3186, church 3005, way
2508, museum 2320, railway 2093, house 2018, arena 1829, field 1731,
club 1708, shopping centre 1509, highway 1464, bridge 1383, street 1352,
theatre 1330, bank 1310, property 1261, hill 1072, castle 1022, forest 995,
court 949, hospital 937, peak 906, bay 899, skyscraper 843, valley 763, ho-
tel 741, garden 739, building 722, market 712, monument 679, port 651,
sea 645, temple 625, beach 614, square 605, store 547, campus 525, palace
516, tower 496, cemetery 457, volcano 426, cathedral 402, glacier 392,
residence 371, dam 363, waterfall 355, gallery 349, prison 348, cave 341,
canal 332, restaurant 329, path 312, observatory 303, zoo 302, coast 298,
statue 283, venue 269, parliament 258, shrine 256, desert 248, synagogue
236, bar 229, ski resort 227, arch 223, landscape 220, avenue 202, casino
179, farm 179, seaside 173, waterway 167, tunnel 167, ruin 166, chapel 165,
observation wheel 158, basilica 157, woodland 154, wetland 151, cinema
144, gate 142, aquarium 136, entrance 136, opera house 134, spa 125,
shop 124, abbey 108, boulevard 108, pub 92, bookstore 76, mosque 56
formation we can group the patterns into groups
expressing the same type of information and then,
during sentence selection, ensure that sentences
matching patterns from different groups are se-
lected in order to guarantee broad, non-redundant
coverage of information relevant for inclusion in
the summary. We report work experimenting with
this idea too.
2 Representing conceptual models
2.1 Object type corpora
We derive n-gram language and dependency pat-
tern models using object type corpora made avail-
able to us by Aker and Gaizauskas. Aker and
Gaizauskas (2009) define an object type corpus as
a collection of texts about a specific static object
type such as church, bridge, etc. Objects can be
named locations such as Eiffel Tower. To refer to
such names they use the term toponym. To build
such object type corpora the authors categorized
Wikipedia articles places by object type. The ob-
ject type of each article was identified automati-
cally by running Is-A patterns over the first five
sentences of the article. The authors report 91%
accuracy for their categorization process. The
most populated of the categories identified (in to-
tal 107 containing articles about places around the
world) are shown in Table 2.
2.2 N-gram language models
Aker and Gaizauskas (2009) experimented with
uni-gram and bi-gram language models to capture
the features commonly used when describing an
object type and used these to bias the sentence se-
lection of the summarizer towards the sentences
that contain these features. As in Song and Croft
(1999) they used their language models in a gener-
1251
ative way, i.e. they calculate the probability that a
sentence is generated based on a n-gram language
model. They showed that summarizer biased with
bi-gram language models produced better results
than those biased with uni-gram models. We repli-
cate the experiments of Aker and Gaizauskas and
generate a bi-gram language model for each object
type corpus. In later sections we use LM to refer
to these models.
2.3 Dependency patterns
We use the same object type corpora to derive
dependency patterns. Our patterns are derived
from dependency trees which are obtained using
the Stanford parser1. Each article in each ob-
ject type corpus was pre-processed by sentence
splitting and named entity tagging2. Then each
sentence was parsed by the Stanford dependency
parser to obtain relational patterns. As with the
chain model introduced by Sudo et al (2001) our
relational patterns are concentrated on the verbs
in the sentences and contain n+1 words (the verb
and n words in direct or indirect relation with the
verb). The number n is experimentally set to two
words.
For illustration consider the sentence shown in
Table 3 that is taken from an article in the bridge
corpus. The first two rows of the table show the
original sentence and its form after named entity
tagging. The next step in processing is to replace
any occurrence of a string denoting the object type
by the term ?OBJECTTYPE? as shown in the third
row of Table 3. The final two rows of the table
show the output of the Stanford dependency parser
and the relational patterns identified for this ex-
ample. To obtain the relational patterns from the
parser output we first identified the verbs in the
output. For each such verb we extracted two fur-
ther words being in direct or indirect relation to the
current verb. Two words are directly related if they
occur in the same relational term. The verb built-4,
for instance, is directly related to DATE-6 because
they both are in the same relational term prep-
in(built-4, DATE-6). Two words are indirectly re-
lated if they occur in two different terms but are
linked by a word that occurs in those two terms.
The verb was-3 is, for instance, indirectly related
to OBJECTTYPE-2 because they are both in dif-
ferent terms but linked with built-4 that occurs in
1http://nlp.stanford.edu/software/lex-parser.shtml
2For performing shallow text analysis the OpenNLP tools
(http://opennlp.sourceforge.net/) were used.
Table 3: Example sentence for dependency pattern.
Original sentence: The bridge was built in 1876 by W. W.
After NE tagging: The bridge was built in DATE by W. W.
Input to the parser: The OBJECTTYPE was built in DATE by W. W.
Output of the parser: det(OBJECTTYPE-2, The-1), nsubjpass(built-
4, OBJECTTYPE-2), auxpass(built-4, was-3), prep-in(built-4, DATE-6),
nn(W-10, W-8), agent(built-4, W-10)
Patterns: The OBJECTTYPE built, OBJECTTYPE was built, OBJECT-
TYPE built DATE, OBJECTTYPE built W, was built DATE, was built W
both terms. E.g. for the term nsubjpass(built-4,
OBJECTTYPE-2) we use the verb built and ex-
tract patterns based on this. OBJECTTYPE is in
direct relation to built and The is in indirect rela-
tion to built through OBJECTTYPE. So a pattern
from these relations is The OBJECTTYPE built.
The next pattern extracted from this term is OB-
JECTTYPE was built. This pattern is based on di-
rect relations. The verb built is in direct relation
to OBJECTTYPE and also to was. We continue
this until we cover all direct relations with built re-
sulting in two more patterns (OBJECTTYPE built
DATE and OBJECTTYPE built W). It should be
noted that we consider all direct and indirect rela-
tions while generating the patterns.
Following these steps we extracted relational
patterns for each object type corpus along with the
frequency of occurrence of the pattern in the en-
tire corpus. The frequency values are used by the
summarizer to score the sentences. In the follow-
ing sections we will use the term DpM to refer to
these dependency pattern models.
2.3.1 Pattern categorization
In addition to using dependency patterns as mod-
els for biasing sentence selection, we can also use
them to control the kind of information to be in-
cluded in the final summary (see Section 3.2). We
may want to ensure that the summary contains
a sentence describing the object type of the ob-
ject, its location and some background informa-
tion. For example, for the object Eiffel Tower we
aim to say that it is a tower, located in Paris, de-
signed by Gustave Eiffel, etc. To be able to do
so, we categorize dependency patterns according
to the type of information they express.
We manually analyzed human written descrip-
tions about instances of different object types and
recorded for each sentence in the descriptions the
kind of information it contained about the object.
We analyzed descriptions of 310 different objects
where each object had up to four different human
written descriptions (Section 4.1). We categorized
the information contained in the descriptions into
1252
the following categories:
? type: sentences containing the ?type? information of
the object such as XXX is a bridge
? year: sentences containing information about when the
object was built or in case of mountains, for instance,
when it was first climbed
? location: sentences containing information about
where the object is located
? background: sentences containing some specific in-
formation about the object
? surrounding: sentences containing information about
what other objects are close to the main object
? visiting: sentences containing information about e.g.
visiting times, etc.
We also manually assigned each dependency
pattern in each corpus-derived model to one of the
above categories, provided it occurred five or more
times in the object type corpora. The patterns ex-
tracted for our example sentence shown in Table 3,
for instance, are all categorized by year category
because all of them contain information about the
foundation date of an object.
3 Summarizer
We adopted the same overall approach to sum-
marization used by Aker and Gaizauskas (2009)
to generate the image descriptions. The summa-
rizer is an extractive, query-based multi-document
summarization system. It is given two inputs: a
toponym associated with an image and a set of
documents to be summarized which have been re-
trieved from the web using the toponym as a query.
The summarizer creates image descriptions in a
three step process. First, it applies shallow text
analysis, including sentence detection, tokeniza-
tion, lemmatization and POS-tagging to the given
input documents. Then it extracts features from
the document sentences. Finally, it combines the
features using a linear weighting scheme to com-
pute the final score for each sentence and to cre-
ate the final summary. We modified the approach
to feature extraction and the way the summarizer
acquires the weights for feature combination. The
following subsections describe how feature extrac-
tion/combination is done in more detail.
3.1 Feature Extraction
The original summarizer reported in Aker and
Gaizauskas (2009) uses the following features to
score the sentences:
? querySimilarity: Sentence similarity to the query (to-
ponym) (cosine similarity over the vector representa-
tion of the sentence and the query).
? centroidSimilarity: Sentence similarity to the centroid.
The centroid is composed of the 100 most frequently
occurring non stop words in the document collection
(cosine similarity over the vector representation of the
sentence and the centroid).
? sentencePosition: Position of the sentence within its
document. The first sentence in the document gets the
score 1 and the last one gets 1n where n is the number
of sentences in the document.
? starterSimilarity: A sentence gets a binary score if it
starts with the query term (e.g. Westminster Abbey, The
Westminster Abbey, The Westminster or The Abbey) or
with the object type, e.g. The church. We also allow
gaps (up to four words) between the and the query to
capture cases such as The most magnificent Abbey, etc.
? LMSim3: The similarity of a sentence S to an n-gram
language model LM (the probability that the sentence
S is generated by LM).
In our experiments we extend this feature set by
two dependency pattern related features: DpMSim
and DepCat.
DpMSim is computed in a similar fashion to
LMSim feature. We assign each sentence a depen-
dency similarity score. To compute this score, we
first parse the sentence on the fly with the Stan-
ford parser and obtain the dependency patterns for
the sentence. We then associate each dependency
pattern of the sentence with the occurrence fre-
quency of that pattern in the dependency pattern
model (DpM). DpMSim is then computed as given
in Equation 1. It is a sum of all occurrence fre-
quencies of the dependency patterns detected in a
sentence S that are also contained in the DpM.
DpMSim(S,DpM) =
?
p?S
fDpM (p) (1)
The second feature, DepCat, uses dependency
patterns to categorize the sentences rather than
ranking them. It can be used independently from
other features to categorize each sentence by one
of the categories described in Section 2.3.1. To do
this, we obtain the relational patterns for the cur-
rent sentence, check whether for each such pattern
whether it is included in the DpM, and, if so, we
add to the sentence the category the pattern was
manually associated with. It should be noted that
a sentence can have more than one category. This
can occur, for instance, if the sentence contains in-
formation about when something was built and at
the same time where it is located. It is also impor-
tant to mention that assigning sentences categories
does not change the order in the ranked list.
We use DepCat to generate an automated sum-
mary by first including sentences containing the
category ?type?, then ?year? and so on until the
3In Aker and Gaizauskas (2009) this feature is called mod-
elSimilarity.
1253
summary length is violated. The sentences are se-
lected according to the order in which they occur
in the ranked list. From each of the first three cat-
egories (?type?, ?year? and ?location?) we take a
single sentence to avoid redundancy. The same is
applied to the final two categories (?surrounding?
and ?visiting?). Then, if length limit is not vio-
lated, we fill the summary with sentences from the
?background? category until the word limit of 200
words is reached. Here the number of added sen-
tences is not limited. Finally, we order the sen-
tences by first adding the sentences from the first
three categories to the summary, then the ?back-
ground? related sentences and finally the last two
sentences from the ?surrounding? and ?visiting?
categories. However, in cases where we have not
reached the summary word limit because of un-
covered categories, i.e. there were not, for in-
stance, sentences about ?location?, we add to the
end of the summary the next top sentence from the
ranked list that was not taken.
3.2 Sentence Selection
To compute the final score for each sentence Aker
and Gaizauskas (2009) use a linear function with
weighted features:
Sscore = (
n?
i=1
featurei ? weighti) (2)
We use the same approach, but whereas the fea-
ture weights they use are experimentally set rather
than learned, we learn the weights using linear re-
gression instead. We used 23 of the 310 images
from our image set (see Section 4.1) to train the
weights. The image descriptions from this data set
are used as model summaries.
Our training data contains for each image a
set of image descriptions taken from the Virtual-
Tourist travel community web-site 4. From this
web-site we took all existing image descriptions
about a particular image or object. Note that some
of these descriptions about a particular object were
used to derive the model summaries for that ob-
ject (see Section 4.1). Assuming that model sum-
maries contain the most relevant sentences about
an object we perform ROUGE comparisons be-
tween the sentences in all the image descriptions
and the model summaries, i.e. we pair each sen-
tence from all image descriptions about a particu-
lar place with every sentence from all the model
4www.virtualtourist.com
summaries for that particular object. Sentences
which are exactly the same or have common parts
will score higher in ROUGE than sentences which
do not have anything in common. In this way, we
have for each sentence from all existing image de-
scriptions about an object a ROUGE score5 indi-
cating its relevance. We also ran the summarizer
for each of these sentences to compute the values
for the different features. This gives information
about each feature?s value for each sentence. Then
the ROUGE scores and feature score values for ev-
ery sentence were input to the linear regression al-
gorithm to train the weights.
Given the weights, Equation 2 is used to com-
pute the final score for each sentence. The final
sentence scores are used to sort the sentences in
the descending order. This sorted list is then used
by the summarizer to generate the final summary
as described in Aker and Gaizauskas (2009).
4 Evaluation
To evaluate our approach we used two different as-
sessment methods: ROUGE (Lin, 2004) and man-
ual readability. In the following we first describe
the data sets used in each of these evaluations, and
then we present the results of each assessment.
4.1 Data sets
For evaluation we use the image collection de-
scribed in Aker and Gaizauskas (2010). The image
collection contains 310 different images with man-
ually assigned toponyms. The images cover 60
of the 107 object types identified from Wikipedia
(see Table 2). For each image there are up to
four short descriptions or model summaries. The
model summaries were created manually based on
image descriptions taken from VirtualTourist and
contain a minimum of 190 and a maximum of 210
words. An example model summary about the Eif-
fel Tower is shown in Table 4. 23 of this image
collection was used to train the weights and the
remaining 13 (105 images) for evaluation.
To generate automatic captions for the im-
ages we automatically retrieved the top 30 related
web-documents for each image using the Yahoo!
search engine and the toponym associated with the
image as a query. The text from these documents
was extracted using an HTML parser and passed
to the summarizer. The set of documents we used
to generate our summaries excluded any Virtual-
Tourist related sites, as these were used to generate
5We used ROUGE 1.
1254
Table 4: Model, Wikipedia baseline and starterSimilarity+LMSim+DepCat summary for Eiffel Tower.
Model Summary Wikipedia baseline summary starterSimilarity+LMSim+DepCat summary
The Eiffel Tower is the most famous place in Paris. It
is made of 15,000 pieces fitted together by 2,500,000
rivets. It?s of 324 m (1070 ft) high structure and
weighs about 7,000 tones. This world famous land-
mark was built in 1889 and was named after its de-
signer, engineer Gustave Alexandre Eiffel. It is now
one of the world?s biggest tourist places which is vis-
ited by around 6,5 million people yearly. There are
three levels to visit: Stages 1 and 2 which can be
reached by either taking the steps (680 stairs) or the
lift, which also has a restaurant ?Altitude 95? and a
Souvenir shop on the first floor. The second floor also
has a restaurant ?Jules Verne?. Stage 3, which is at
the top of the tower can only be reached by using the
lift. But there were times in the history when Tour Eif-
fel was not at all popular, when the Parisians thought
it looked ugly and wanted to pull it down. The Eif-
fel Tower can be reached by using the Mtro through
Trocadro, Ecole Militaire, or Bir-Hakeim stops. The
address is: Champ de Mars-Tour Eiffel.
The Eiffel Tower (French: Tour Eiffel, [tur efel])
is a 19th century iron lattice tower located on the
Champ de Mars in Paris that has become both a
global icon of France and one of the most recog-
nizable structures in the world. The Eiffel Tower,
which is the tallest building in Paris, is the single
most visited paid monument in the world; millions
of people ascend it every year. Named after its de-
signer, engineer Gustave Eiffel, the tower was built
as the entrance arch for the 1889 World?s Fair. The
tower stands at 324 m (1,063 ft) tall, about the
same height as an 81-story building. It was the
tallest structure in the world from its completion
until 1930, when it was eclipsed by the Chrysler
Building in New York City. Not including broad-
cast antennas, it is the second-tallest structure in
France, behind the Millau Viaduct, completed in
2004. The tower has three levels for visitors. Tick-
ets can be purchased to ascend either on stairs or
lifts to the first and second levels.
The Eiffel Tower, which is the tallest building in
Paris, is the single most visited paid monument in the
world; millions of people ascend it every year. The
tower is located on the Left Bank of the Seine River,
at the northwestern extreme of the Parc du Champ
de Mars, a park in front of the Ecole Militaire that
used to be a military parade ground. The tower was
met with much criticism from the public when it was
built, with many calling it an eyesore. Counting from
the ground, there are 347 steps to the first level, 674
steps to the second level, and 1,710 steps to the small
platform on the top of the tower. Although it was
the world?s tallest structure when completed in 1889,
the Eiffel Tower has since lost its standing both as
the tallest lattice tower and as the tallest structure in
France. The tower has two restaurants: Altitude 95,
on the first floor 311ft (95m) above sea level; and
the Jules Verne, an expensive gastronomical restau-
rant on the second floor, with a private lift.
Table 5: ROUGE scores for each single feature and Wikipedia baseline.
Recall centroidSimilarity sentencePosition querySimilarity starterSimilarity LMSim DpMSim*** Wiki
R2 .0734 .066 .0774 .0869 .0895 .093 .097
RSU4 .12 .11 .12 .137 .142 .145 .14
the model summaries.
4.2 ROUGE assessment
In the first assessment we compared the automat-
ically generated summaries against model sum-
maries written by humans using ROUGE (Lin,
2004). Following the Document Understanding
Conference (DUC) evaluation standards we used
ROUGE 2 (R2) and ROUGE SU4 (RSU4) as eval-
uation metrics (Dang, 2006) . ROUGE 2 gives re-
call scores for bi-gram overlap between the auto-
matically generated summaries and the reference
ones. ROUGE SU4 allows bi-grams to be com-
posed of non-contiguous words, with a maximum
of four words between the bi-grams.
As baselines for evaluation we used two dif-
ferent summary types. Firstly, we generated
summaries for each image using the top-ranked
non Wikipedia document retrieved in the Yahoo!
search results for the given toponyms. From this
document we create a baseline summary by select-
ing sentences from the beginning until the sum-
mary reaches a length of 200 words. As a second
baseline we use the Wikipedia article for a given
toponym from which we again select sentences
from the beginning until the summary length limit
is reached.
First, we compared the baseline summaries
against the VirtualTourist model summaries. The
comparison shows that the Wikipedia baseline
ROUGE scores (R2 .097***, RSU4 .14***) are
significantly higher than the first document ones
(R2 0.042, RSU4 .079) 6. Thus, we will focus
on the Wikipedia baseline summaries to draw con-
clusions about our automatic summaries. Table 4
shows the Wikipedia baseline summary about the
Eiffel Tower.
Secondly, we separately ran the summarizer
over the top ten documents for each single feature
and compared the automated summaries against
the model ones. The results of this comparison
are shown in Table 5.
Table 5 shows that the dependency model fea-
ture (DpMSim) contributes most to the summary
quality according to the ROUGE metrics. It is also
significantly better than all other feature scores
except the LMSim feature. Compared to LMSim
ROUGE scores the DpMSim feature offers only a
moderate improvement. The same moderate im-
provement we can see between the DpMSim RSU4
and the Wiki RSU4. The lowest ROUGE scores
are obtained if only sentence position (sentecePo-
sition) is used.
To see how the ROUGE scores change when
features are combined with each other we per-
formed different combinations of the features,
ran the summarizer for each combination and
compared the automated summaries against the
model ones. In the different combinations we
6To assess the statistical significance of ROUGE score
differences between multiple summarization results we per-
formed a pairwise Wilcoxon signed-rank test. We use the
following conventions for indicating significance level in the
tables: *** = p < .0001, ** = p < .001, * = p < .05 and no
star indicates non-significance.
1255
Table 6: ROUGE scores of feature combinations which score moderately
or significantly higher than dependency pattern model (DpMSim) feature and
Wikipedia baseline.
Recall starterSimilarity
+ LMSim
starterSimilarity
+ LMSim + Dep-
Cat***
DpmSim Wiki
R2 .095 .102 .093 .097
RSU4 .145 .155 .145 .14
also included the dependency pattern categoriza-
tion (DepCat) feature explained in Section 3.1.
Table 6 shows the results of feature combinations
which score moderately or significantly higher
than the dependency pattern model (DpMSim) fea-
ture score shown in Table 5.
The results showed that combining DpMSim
with other features did not lead to higher ROUGE
scores than those produced by that feature alone.
The summaries categorized by dependency pat-
terns (starterSimilarity+LMSim+DepCat) achieve
significantly higher ROUGE scores than the
Wikipedia baseline. For both ROUGE R2 and
ROUGE SU4 the significance is at level p <
.0001. Table 4 shows a summary about the
Eiffel Tower obtained using this starterSimilar-
ity+LMSim+DepCat feature. Table 5 also shows
the ROUGE scores of the feature combination
starterSimilarity and LMSim used without the de-
pendency categorization (DepCat) feature. It can
be seen that this combination without the depen-
dency patterns lead to lower ROUGE scores in
ROUGE 2 and only moderate improvement in
ROUGE SU4 if compared with Wikipedia base-
line ROUGE scores.
4.3 Readability assessment
We also evaluated our summaries using a read-
ability assessment as in DUC and TAC. DUC and
TAC manually assess the quality of automatically
generated summaries by asking human subjects to
score each summary using five criteria ? gram-
maticality, redundancy, clarity, focus and structure
criteria. Each criterion is scored on a five point
scale with high scores indicating a better result
(Dang, 2005).
For this evaluation we used the same 105 im-
ages as in the ROUGE evaluation. As the ROUGE
evaluation showed that the dependency pattern
categorization (DepCat) renders the best results
when used in feature combination starterSimilar-
ity + LMSim + DepCat, we further investigated
the contribution of dependency pattern categoriza-
tion by performing a readability assessment on
summaries generated using this feature combina-
tion. For comparison we also evaluated sum-
maries which were not structured by dependency
patterns (starterSimilarity + LMSim) and also the
Wikipedia baseline summaries.
We asked four people to assess the summaries.
Each person was shown all 315 summaries (105
from each summary type) in a random way and
was asked to assess them according to the DUC
and TAC manual assessment scheme. The results
are shown in Table 7.
We see from Table 7 that using dependency pat-
terns to categorize the sentences and produce a
structured summary helps to obtain better readable
summaries. Looking at the 5 and 4 scores the ta-
ble shows that the dependency pattern categorized
summaries (SLMD) have better clarity (85% of the
summaries), are more coherent (74% of the sum-
maries), contain less redundant information (83%
of the summaries) and have better grammar (92%
of the summaries) than the ones without depen-
dency categorization (80%, 70%, 60%, 84%).
The scores of our automated summaries were
better than the Wikipedia baseline summaries in
the grammar feature. However, in other features
the Wikipedia baseline summaries obtained better
scores than our automated summaries. This com-
parison show that there is a gap to fill in order to
obtain better readable summaries.
5 Related Work
Our approach has an advantage over related work
in automatic image captioning in that it requires
only GPS information associated with the image in
order to generate captions. Other attempts towards
automatic generation of image captions generate
captions based on the immediate textual context of
the image with or without consideration of image
related features such as colour, shape or texture
(Deschacht and Moens, 2007; Mori et al, 2000;
Barnard and Forsyth, 2001; Duygulu et al, 2002;
Barnard et al, 2003; Pan et al, 2004; Feng and La-
pata, 2008; Satoh et al, 1999; Berg et al, 2005).
However, Marsch & White (2003) argue that the
content of an image and its immediate text have
little semantic agreement and this can, according
to Purves et al (2008), be misleading to image
retrieval. Furthermore, these approaches assume
that the image has been obtained from a document.
In cases where there is no document associated
with the image, which is the scenario we are prin-
cipally concerned with, these techniques are not
applicable.
1256
Table 7: Readability evaluation results: Each cell shows the percentage of summaries scoring the ranking score heading the column for each criterion in the
row as produced by the summary method indicated by the subcolumn heading ? Wikipedia baseline (W), starterSimilarity + LMSim (SLM) and starterSimilarity +
LMSim + DepCat (SLMD). The numbers indicate the percentage values averaged over the four people.
5 4 3 2 1
Criterion W SLM SLMD W SLM SLMD W SLM SLMD W SLM SLMD W SLM SLMD
clarity 72.6 50.5 53.6 21.7 30.0 31.4 1.2 6.7 5.7 4.0 10.2 6.0 0.5 2.6 3.3
focus 72.1 49.3 51.2 20.5 26.0 25.2 3.8 10.0 10.7 3.3 10.0 10.5 0.2 4.8 2.4
coherence 67.1 39.0 48.3 23.6 31.4 26.9 4.8 12.4 11.9 3.3 10.2 9.8 1.2 6.9 3.1
redundancy 69.8 42.9 55.0 21.7 17.4 28.8 2.4 4.5 4.3 5.0 27.1 8.8 1.2 8.1 3.1
grammar 48.6 55.7 62.9 32.9 29.0 30.0 5.0 3.1 1.9 11.7 12.1 5.2 1.9 0 0
Dependency patterns have been exploited in
various language processing applications. In in-
formation extraction, for instance, dependency
patterns have been used to extract relevant in-
formation from text resources (Yangarber et al,
2000; Sudo et al, 2001; Culotta and Sorensen,
2004; Stevenson and Greenwood, 2005; Bunescu
and Mooney, 2005; Stevenson and Greenwood,
2009). However, dependency patterns have not
been used extensively in summarization tasks. We
are aware only of the work described in Nobata et
al. (2002) who used dependency patterns in com-
bination with other features to generate extracts in
a single document summarization task. The au-
thors found that when learning weights in a simple
feature weigthing scheme, the weight assigned to
dependency patterns was lower than that assigned
to other features. The small contribution of the de-
pendency patterns may have been due to the small
number of documents they used to derive their
dependency patterns ? they gathered dependency
patterns from only ten domain specific documents
which are unlikely to be sufficient to capture re-
peated features in a domain.
6 Discussion and Conclusion
We have proposed a method by which dependency
patterns extracted from corpora of descriptions of
instances of particular object types can be used in a
multi-document summarizer to automatically gen-
erate image descriptions. Our evaluations show
that such an approach yields summaries which
score more highly than an approach which uses a
simpler representation of an object type model in
the form of a n-gram language model.
When used as the sole feature for sentence rank-
ing, dependency pattern models (DpMSim) pro-
duced summaries with higher ROUGE scores than
those obtained using the features reported in Aker
and Gaizauskas (2009). These dependency pat-
tern models also achieved a modest improvement
over Wikipedia baseline ROUGE SU4. Further-
more, we showed that using dependency patterns
in combination with features reported in Aker and
Gaizauskas to produce a structured summary led
to significantly better results than Wikipedia base-
line summaries as assessed by ROUGE. However,
human assessed readability showed that there is
still scope for improvement.
These results indicate that dependency patterns
are worth investigating for object focused auto-
mated summarization tasks. Such investigations
should in particular concentrate on how depen-
dency patterns can be used to structure informa-
tion within the summary, as our best results were
achieved when dependency patterns were used for
this purpose.
There are a number of avenues to pursue in fu-
ture work. One is to explore how dependency pat-
terns could be used to produce generative sum-
maries and/or perform sentence trimming. An-
other is to investigate how dependency patterns
might be automatically clustered into groups ex-
pressing similar or related facts, rather than rely-
ing on manual categorization of dependency pat-
terns into categories such as ?type?, ?year?, etc.
as was done here. Evaluation should be extended
to investigate the utility of the automatically gen-
erated image descriptions for image retrieval. Fi-
nally, we also plan to analyze automated ways for
learning information structures (e.g. what is the
flow of facts to describe a location) from existing
image descriptions to produce better summaries.
7 Acknowlegment
The research reported was funded by the TRIPOD
project supported by the European Commission
under the contract No. 045335. We would like
to thank Emina Kurtic, Mesude Bicak, Edina Kur-
tic and Olga Nesic for participating in our manual
evaluation. We also would like to thank Trevor
Cohn and Mark Hepple for discussions and com-
ments.
References
A. Aker and R. Gaizauskas. 2009. Summary Gener-
ation for Toponym-Referenced Images using Object
1257
Type Language Models. International Conference
on Recent Advances in Natural Language Process-
ing (RANLP),2009.
A. Aker and R. Gaizauskas. 2010. Model Summaries
for Location-related Images. In Proc. of the LREC-
2010 Conference.
K. Barnard and D. Forsyth. 2001. Learning the seman-
tics of words and pictures. In International Confer-
ence on Computer Vision, volume 2, pages 408?415.
Vancouver: IEEE.
K. Barnard, P. Duygulu, D. Forsyth, N. de Freitas,
D.M. Blei, and M.I. Jordan. 2003. Matching words
and pictures. The Journal of Machine Learning Re-
search, 3:1107?1135.
T.L. Berg, A.C. Berg, J. Edwards, and DA Forsyth.
2005. Whos in the Picture? In Advances in Neural
Information Processing Systems 17: Proc. Of The
2004 Conference. MIT Press.
R.C. Bunescu and R.J. Mooney. 2005. A shortest
path dependency kernel for relation extraction. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 724?731. Association for
Computational Linguistics Morristown, NJ, USA.
A. Culotta and J. Sorensen. 2004. Dependency Tree
Kernels for Relation Extraction. In Proceedings of
the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL?04), Main Volume, pages
423?429, Barcelona, Spain, July.
H.T. Dang. 2005. Overview of DUC 2005. DUC 05
Workshop at HLT/EMNLP.
H.T. Dang. 2006. Overview of DUC 2006. National
Institute of Standards and Technology.
K. Deschacht and M.F. Moens. 2007. Text Analy-
sis for Automatic Image Annotation. Proc. of the
45th Annual Meeting of the Association for Compu-
tational Linguistics. East Stroudsburg: ACL.
P. Duygulu, K. Barnard, JFG de Freitas, and D.A.
Forsyth. 2002. Object Recognition as Machine
Translation: Learning a Lexicon for a Fixed Im-
age Vocabulary. In Seventh European Conference
on Computer Vision (ECCV), 4:97?112.
X. Fan, A. Aker, M. Tomko, P. Smart, M Sanderson,
and R. Gaizauskas. 2010. Automatic Image Cap-
tioning From the Web For GPS Photographs. In
Proc. of the 11th ACM SIGMM International Con-
ference on Multimedia Information Retrieval, Na-
tional Constitution Center, Philadelphia, Pennsylva-
nia.
Y. Feng and M. Lapata. 2008. Automatic Image An-
notation Using Auxiliary Text Information. Proc.
of Association for Computational Linguistics (ACL)
2008, Columbus, Ohio, USA.
C.Y. Lin. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. Proc. of the Workshop
on Text Summarization Branches Out (WAS 2004),
pages 25?26.
E.E. Marsh and M.D. White. 2003. A taxonomy of
relationships between images and text. Journal of
Documentation, 59:647?672.
Y. Mori, H. Takahashi, and R. Oka. 2000. Automatic
word assignment to images based on image division
and vector quantization. In Proc. of RIAO 2000:
Content-Based Multimedia Information Access.
C. Nobata, S. Sekine, H. Isahara, and R. Grishman.
2002. Summarization system integrated with named
entity tagging and ie pattern discovery. In Proc. of
the LREC-2002 Conference, pages 1742?1745.
J.Y. Pan, H.J. Yang, P. Duygulu, and C. Faloutsos.
2004. Automatic image captioning. In Multime-
dia and Expo, 2004. ICME?04. IEEE International
Conference on, volume 3.
RS Purves, A. Edwardes, and M. Sanderson. 2008.
Describing the where?improving image annotation
and search through geography. 1st Intl. Workshop
on Metadata Mining for Image Understanding, Fun-
chal, Madeira-Portugal.
S. Satoh, Y. Nakamura, and T. Kanade. 1999. Name-It:
naming and detecting faces in news videos. Multi-
media, IEEE, 6(1):22?35.
F. Song and W.B. Croft. 1999. A general language
model for information retrieval. In Proc. of the
eighth international conference on Information and
knowledge management, pages 316?321. ACM New
York, NY, USA.
M. Stevenson and M.A. Greenwood. 2005. A seman-
tic approach to IE pattern induction. In Proc. of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 379?386. Association for
Computational Linguistics Morristown, NJ, USA.
M. Stevenson and M. Greenwood. 2009. Depen-
dency Pattern Models for Information Extraction.
Research on Language and Computation, 7(1):13?
39.
K. Sudo, S. Sekine, and R. Grishman. 2001. Auto-
matic pattern acquisition for Japanese information
extraction. In Proc. of the first international con-
ference on Human language technology research,
page 7. Association for Computational Linguistics.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic acquisition of domain
knowledge for information extraction. In Proc. of
the 18th International Conference on Computational
Linguistics (COLING 2000), pages 940?946. Saar-
briicken, Germany, August.
1258
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 402?411,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Extracting bilingual terminologies from comparable corpora
Ahmet Aker, Monica Paramita, Robert Gaizauskas
University of Sheffield
ahmet.aker, m.paramita, r.gaizauskas@sheffield.ac.uk
Abstract
In this paper we present a method for extracting
bilingual terminologies from comparable corpora.
In our approach we treat bilingual term extrac-
tion as a classification problem. For classification
we use an SVM binary classifier and training data
taken from the EUROVOC thesaurus. We test our
approach on a held-out test set from EUROVOC
and perform precision, recall and f-measure eval-
uations for 20 European language pairs. The per-
formance of our classifier reaches the 100% pre-
cision level for many language pairs. We also
perform manual evaluation on bilingual terms ex-
tracted from English-German term-tagged compa-
rable corpora. The results of this manual evalu-
ation showed 60-83% of the term pairs generated
are exact translations and over 90% exact or partial
translations.
1 Introduction
Bilingual terminologies are important for various
applications of human language technologies, in-
cluding cross-language information search and re-
trieval, statistical machine translation (SMT) in
narrow domains and computer-aided assistance
to human translators. Automatic construction of
bilingual terminology mappings has been investi-
gated in many earlier studies and various methods
have been applied to this task. These methods may
be distinguished by whether they work on parallel
or comparable corpora, by whether they assume
monolingual term recognition in source and target
languages (what Moore (2003) calls symmetrical
approaches) or only in the source (asymmetric ap-
proaches), and by the extent to which they rely on
linguistic knowledge as opposed to simply statis-
tical techniques.
We focus on techniques for bilingual term ex-
traction from comparable corpora ? collections of
source-target language document pairs that are not
direct translations but are topically related. We
choose to focus on comparable corpora because
for many less widely spoken languages and for
technical domains where new terminology is con-
stantly being introduced, parallel corpora are sim-
ply not available. Techniques that can exploit such
corpora to deliver bilingual terminologies are of
significant practical interest in these cases.
The rest of the paper is structured as follows.
In Section 2 we outline our method. In Section
3 we review related work on bilingual term ex-
traction. Section 4 describes feature extraction for
term pair classification. In Section 5 we present
the data used in our evaluations and discuss our
results. Section 6 concludes the paper.
2 Method
The method we present below for bilingual term
extraction is a symmetric approach, i.e. it assumes
a method exists for monolingual term extraction in
both source and target languages. We do not pre-
scribe what a term must be. In particular we do not
place any particular syntactic restrictions on what
constitutes an allowable term, beyond the require-
ment that terms must be contiguous sequences of
words in both source and target languages.
Our method works by first pairing each term ex-
tracted from a source language document S with
each term extracted from a target language doc-
ument T aligned with S in the comparable cor-
pus. We then treat term alignment as a binary
classification task, i.e. we extract features for each
source-target language potential term pair and de-
cide whether to classify the pair as a term equiv-
alent or not. For classification purposes we use
an SVM binary classifier. The training data for
the classifier is derived from EUROVOC (Stein-
berger et al, 2002), a term thesaurus covering
the activities of the EU and the European Parlia-
ment. We have run our approach on the 21 official
EU languages covered by EUROVOC, construct-
ing 20 language pairs with English as the source
402
language. Considering all these languages allows
us to directly compare our method?s performance
on resource-rich (e.g. German, French, Spanish)
and under-resourced languages (e.g. Latvian, Bul-
garian, Estonian). We perform two different tests.
First, we evaluate the performance of the classifier
on a held-out term-pair list from EUROVOC us-
ing the standard measures of recall, precision and
F-measure. We run this evaluation on all 20 lan-
guage pairs. Secondly, we test the system?s per-
formance on obtaining bilingual terms from com-
parable corpora. This second test simulates the
situation of using the term alignment system in a
real world scenario. For this evaluation we col-
lected English-German comparable corpora from
Wikipedia, performed monolingual term tagging
and ran our tool over the term tagged corpora to
extract bilingual terms.
3 Related Work
Previous studies have investigated the extraction
of bilingual terms from parallel and comparable
corpora. For instance, Kupiec (1993) uses statisti-
cal techniques and extracts bilingual noun phrases
from parallel corpora tagged with terms. Daille
et al (1994), Fan et al (2009) and Okita et
al. (2010) also apply statistical methods to extract
terms/phrases from parallel corpora. In addition
to statistical methods Daille et al use word trans-
lation information between two words within the
extracted terms as a further indicator of the correct
alignment. More recently, Bouamor et al (2012)
use vector space models to align terms. The en-
tries in the vectors are co-occurrence statistics be-
tween the terms computed over the entire corpus.
Bilingual term alignment methods that work on
comparable corpora use essentially three sorts of
information: (1) cognate information, typically es-
timated using some sort of transliteration similar-
ity measure (2) context congruence, a measure of
the extent to which the words that the source term
co-occurs with have the same sort of distribution
and co-occur with words with the same sort dis-
tribution as do those words that co-occur with the
candidate term and (3) translation of component
words in the term and/or in context words, where
some limited dictionary exists. For example, in
Rapp (1995), Fung and McKeown (1997), Morin
et. al. (2007), Cao and Li (2002) and Ismail
and Manandhar (2010) the context of text units
is used to identify term mappings. Transliteration
and cognate-based information is exploited in Al-
Onaizan and Knight (2002), Knight and Graehl
(1998), Udupa et. al. (2008) and Aswani and
Gaizauskas (2010).
Very few approaches have treated term align-
ment as a classification problem suitable for ma-
chine learning (ML) techniques. So far as we
are aware, only Cao and Li (2002), who treat
only base noun phrase (NP) mapping, consider the
problem this way. However, it naturally lends it-
self to being viewed as a classification task, as-
suming a symmetric approach, since the differ-
ent information sources mentioned above can be
treated as features and each source-target language
potential term pairing can be treated as an in-
stance to be fed to a binary classifier which decides
whether to align them or not. Our work differs
from that of Cao and Li (2002) in several ways.
First they consider only terms consisting of noun-
noun pairs. Secondly for a given source language
term ?N1, N2?, target language candidate terms
are proposed by composing all translations (given
by a bilingual dictionary) ofN1 into the target lan-
guage with all translations ofN2. We remove both
these restrictions. By considering all terms pro-
posed by monolingual term extractors we consider
terms that are syntactically much richer than noun-
noun pairs. In addition, the term pairs we align are
not constrained by an assumption that their com-
ponent words must be translations of each other as
found in a particular dictionary resource.
4 Feature extraction
To align or map source and target terms we use an
SVM binary classifier (Joachims, 2002) with a lin-
ear kernel and the trade-off between training error
and margin parameter c = 10. Within the classi-
fier we use language dependent and independent
features described in the following sections.
4.1 Dictionary based features
The dictionary based features are language depen-
dent and are computed using bilingual dictionar-
ies which are created with GIZA++ (Och and Ney,
2000; Och and Ney, 2003). The DGT-TM par-
allel data (Steinberger et al, 2012) was input to
GIZA++ to obtain the dictionaries. Dictionary en-
tries have the form ?s, ti, pi?, where s is a source
word, ti is the i-th translation of s in the dictio-
nary and pi is the probability that s is translated
by ti, the pi?s summing to 1 for each s in the dic-
tionary. From the dictionaries we removed all en-
tries with pi < 0.05. In addition we also removed
403
every entry from the dictionary where the source
word was less than four characters and the target
word more than five characters in length and vice
versa. This step is performed to try to eliminate
translation pairs where a stop word is translated
into a non-stop word. After performing these fil-
tering steps we use the dictionaries to extract the
following language dependent features:
? isFirstWordTranslated is a binary feature in-
dicating whether the first word in the source
term is a translation of the first word in the
target term. To address the issue of com-
pounding, e.g. for languages like German
where what is a multi-word term in En-
glish may be expressed as a single com-
pound word, we check whether the com-
pound source term has an initial prefix that
matches the translation of the first target
word, provided that translation is at least 5
character in length.
? isLastWordTranslated is a binary feature in-
dicating whether the last word in the source
term is a translation of the last word in the
target term. As with the previous feature in
case of compound terms we check whether
the source term ends with the translation of
the target last word.
? percentageOfTranslatedWords returns the
percentage of words in the source term which
have their translations in the target term. To
address compound terms we check for each
source word translation whether it appears
anywhere within the target term.
? percentageOfNotTranslatedWords returns
the percentage of words of the source term
which have no translations in the target term.
? longestTranslatedUnitInPercentage returns
the ratio of the number of words within the
longest contiguous sequence of source words
which has a translation in the target term to
the length of the source term, expressed as a
percentage. For compound terms we proceed
as with percentageOfTranslatedWords.
? longestNotTranslatedUnitInPercentage re-
turns the percentage of the number of words
within the longest sequence of source words
which have no translations in the target term.
These six features are direction-dependent and
are computed in both directions, reversing which
language is taken as the source and which as
the target. We also compute another feature av-
eragePercentageOfTranslatedWords which builds
the average between the feature values of percent-
ageOfTranslatedWords from source to target and
target to source. Thus in total we have 13 dic-
tionary based features. Note for non-compound
terms if we compare two words for equality we do
not perform string match but rather use the Lev-
enshtein Distance (see Section 4.2) between the
two words and treat them as equal if the Leven-
shtein Distance returns >= 0.95. This is per-
formed to capture words with morphological dif-
ferences. We set 0.95 experimentally.
4.2 Cognate based features
Dictionaries mostly fail to return translation en-
tries for named entities (NEs) or specialized termi-
nology. Because of this we also use cognate based
methods to perform the mapping between source
and target words or vice versa. Aker et al (2012)
have applied (1) Longest Common Subsequence
Ratio, (2) Longest Common Substring Ratio, (3)
Dice Similarity, (4) Needleman-Wunsch Distance
and (5) Levenshtein Distance in order to extract
parallel phrases from comparable corpora. We
adopt these measures within our classifier. Each
of them returns a score between 0 and 1.
? Longest Common Subsequence Ratio
(LCSR): The longest common subsequence
(LCS) measure measures the longest com-
mon non-consecutive sequence of characters
between two strings. For instance, the words
?dollars? and ?dolari? share a sequence of
5 non-consecutive characters in the same
ordering. We make use of dynamic program-
ming (Cormen et al, 2001) to implement
LCS, so that its computation is efficient and
can be applied to a large number of possible
term pairs quickly. We normalize relative to
the length of the longest term:
LCSR(X,Y ) = len[LCS(X,Y )]max[len(X), len(Y )]
where LCS is the longest common subse-
quence between two strings and characters
in this subsequence need not be contiguous.
The shorthand len stands for length.
? Longest Common Substring Ratio (LC-
STR): The longest common substring
(LCST) measure is similar to the LCS
measure, but measures the longest common
404
consecutive string of characters that two
strings have in common. I.e. given two terms
we need to find the longest character n-gram
the terms share. The formula we use for the
LCSTR measure is a ratio as in the previous
measure:
LCSTR(X,Y ) = len[LCST (X,Y )]max[len(X), len(Y )]
? Dice Similarity:
dice = 2 ? LCSTlen(X) + len(Y )
? Needlemann Wunsch Distance (NWD):
NWD = LCSTmin[len(X) + len(Y )]
? Levenshtein Distance (LD): This method
computes the minimum number of operations
necessary to transform one string into an-
other. The allowable operations are insertion,
deletion, and substitution. Compared to the
previous methods, which all return scores be-
tween 0 and 1, this method returns a score s
that lies between 0 and n. The number n rep-
resents the maximum number of operations
to convert an arbitrarily dissimilar string to a
given string. To have a uniform score across
all cognate methods we normalize s so that
it lies between 0 and 1, subtracting from 1 to
convert it from a distance measure to a simi-
larty measure:
LDnormalized = 1?
LD
max[len(X), len(Y )]
4.3 Cognate based features with term
matching
The cognate methods assume that the source and
target language strings being compared are drawn
from the same character set and fail to capture
the corresponding terms if this is not the case.
For instance, the cognate methods are not directly
applicable to the English-Bulgarian and English-
Greek language pairs, as both the Bulgarian and
Greek alphabets, which are Cyrillic-based, differ
from the English Latin-based alphabet. However,
the use of distinct alphabets is not the only prob-
lem when comparing source and target terms. Al-
though most EU languages use the Latin alpha-
bet, the occurrence of special characters and di-
acritics, as well spelling and phonetic variations,
are further challenges which are faced by term or
entity mapping methods, especially in determin-
ing the variants of the same mention of the entity
(Snae, 2007; Karimi et al, 2011).1 We address this
problem by mapping a source term to the target
language writing system or vice versa. For map-
ping we use simple character mappings between
the writing systems, such as ? ? a, ? ? ph,
etc., from Greek to English. The rules allow one
character on the lefthand side (source language) to
map onto one or more characters on the righthand
side (target language). We created our rules man-
ually based on sound similarity between source
and target language characters. We created map-
ping rules for 20 EU language pairs using primar-
ily Wikipedia as a resource for describing phonetic
mappings to English.
After mapping a term from source to target lan-
guage we apply the cognate metrics described in
4.2 to the resulting mapped term and the original
term in the other language. Since we perform both
target to source and source to target mapping, the
number of cognate feature scores on the mapped
terms is 10 ? 5 due to source to target mapping
and 5 due to target to source mapping.
4.4 Combined features
We also combined dictionary and cognate based
features. The combined features are as follows:
? isFirstWordCovered is a binary feature indi-
cating whether the first word in the source
term has a translation (i.e. has a translation
entry in the dictionary regardless of the score)
or transliteration (i.e. if one of the cognate
metric scores is above 0.72) in the target term.
The threshold 0.7 for transliteration similar-
ity is set experimentally using the training
data. To do this we iteratively ran feature
extraction, trained the classifier and recorded
precision on the training data using a thresh-
old value chosen from the interval [0, 1] in
steps of 0.1. We selected as final threshold
value, the lowest value for which the preci-
sion score was the same as when the thresh-
old value was set to 1.
? isLastWordCovered is similar to the previ-
ous feature one but indicates whether the last
word in the source term has a translation or
1Assuming the terms are correctly spelled, otherwise the
misspelling is another problem.
2Note that we use the cognate scores obtained on the char-
acter mapped terms.
405
transliteration in the target term. If this is the
case, 1 is returned otherwise 0.
? percentageOfCoverage returns the percent-
age of source term words which have a trans-
lation or transliteration in the target term.
? percentageOfNonCoverage returns the per-
centage of source term words which have nei-
ther a translation nor transliteration in the tar-
get term.
? difBetweenCoverageAndNonCoverage
returns the difference between the last two
features.
Like the dictionary based features, these five
features are direction-dependent and are computed
in both directions ? source to target and target to
source, resulting in 10 combined features.
In total we have 38 features ? 13 features based
on dictionary translation as described in Section
4.1, 5 cognate related features as outlined in Sec-
tion 4.2, 10 cognate related features derived from
character mappings over terms as described in
Section 4.3 and 10 combined features.
5 Experiments
5.1 Data Sources
In our experiments we use two different data re-
sources: EUROVOC terms and comparable cor-
pora collected from Wikipedia.
5.1.1 EUROVOC terms
EUROVOC is a term thesaurus covering the ac-
tivities of the EU and the European Parliament in
particular. It contains 6797 term entries in 24 dif-
ferent languages including 22 EU languages and
Croatian and Serbian (Steinberger et al, 2002).
5.1.2 Comparable Corpora
We also built comparable corpora in the infor-
mation technology (IT) and automotive domains
by gathering documents from Wikipedia for the
English-German language pair. First, we man-
ually chose one seed document in English as a
starting point for crawling in each domain3. We
then identified all articles to which the seed doc-
ument is linked and added them to the crawling
queue. This process is performed recursively for
each document in the queue. Since our aim is to
build a comparable corpus, we only added English
3http://en.wikipedia.org/wiki/Information technology for
IT and http://en.wikipedia.org/wiki/Automotive industry for
automotive domain.
documents which have an inter-language link in
Wikipedia to a German document. We set a max-
imum depth of 3 in the recursion to limit size of
the crawling set, i.e. documents are crawled only
if they are within 3 clicks of the seed documents.
A score is then calculated to represent the impor-
tance of each document di in this domain:
scoredi =
n?
j=1
freqdij
depthdj
where n is the total number of documents in the
queue, freqdij is 1 if di is linked to dj , or 0 other-
wise, and depthdj is the number of clicks between
dj and the seed document. After all documents in
the queue were assigned a score, we gathered the
top 1000 documents and used inter-language link
information to extract the corresponding article in
the target language.
We pre-processed each Wikipedia article by
performing monolingual term tagging using
TWSC (Pinnis et al, 2012). TWSC is a term ex-
traction tool which identifies terms ranging from
one to four tokens in length. First, it POS-tags
each document. For German POS-tagging we
use TreeTagger (Schmid, 1995). Next, it uses
term grammar rules, in the form of sequences of
POS tags or non-stop words, to identify candidate
terms. Finally, it filters the candidate terms us-
ing various statistical measures, such as pointwise
mutual information and TF*IDF.
5.2 Performance test of the classifier
To test the classifier?s performance we evaluated it
against a list of positive and negative examples of
bilingual term pairs using the measures of preci-
sion, recall and F -measure. We used 21 EU offi-
cial languages, including English, and paired each
non-English language with English, leading to 20
language pairs.4 In the evaluation we used 600
positive term pairs taken randomly from the EU-
ROVOC term list. We also created around 1.3M
negative term pairs by pairing a source term with
200 randomly chosen distinct target terms. We
select such a large number to simulate the real
application scenario where the classifier will be
confronted with a huge number of negative cases
4Note that we do not use the Maltese-English language
pair, as for this pair we found that 5861 out of 6797 term
pairs were identical, i.e. the English and the Maltese terms
were the same. Excluding Maltese, the average number of
identical terms between a non-English language and English
in the EUROVOC data is 37.7 (out of a possible 6797).
406
Table 1: Wikipedia term pairs processed and judged as pos-
itive by the classifier.
Processed Positive
DE IT 11597K 3249
DE Automotive 12307K 1772
and a relatively small number of positive pairs.
The 600 positive examples contain 200 single term
pairs (i.e. single word on both sides), 200 term
pairs with a single word on only one side (either
source or target) and 200 term pairs with more
than one word on each side. For training we took
the remaining 6200 positive term pairs from EU-
ROVOC and constructed another 6200 term pairs
as negative examples, leading to total of 12400
term pairs. To construct the 6200 negative exam-
ples we used the 6200 terms on the source side
and paired each source term with an incorrect tar-
get term. Note that we ensure that in both train-
ing and testing the set of negative and positive
examples do not overlap. Furthermore, we per-
formed data selection for each language pair sep-
arately. This means that the same pairs found
in, e.g., English-German are not necessarily the
same as in English-Italian. The reason for this is
that the translation lengths, in number of words,
vary between language pairs. For instance adult
education is translated into Erwachsenenbildung
in German and contains just a single word (al-
though compound). The same term is translated
into istruzione degli adulti in Italian and contains
three words. For this reason we carry out the data
preparation process separately for each language
pair in order to obtain the three term pair sets con-
sisting of term pairs with only a single word on
each side, term pairs with a single word on just
one side and term pairs with multiple words on
both sides.
5.3 Manual evaluation
For this evaluation we used the Wikipedia com-
parable corpora collected for the English-German
(EN-DE) language pair. For each pair of
Wikipedia articles we used the terms tagged by
TWSC and aligned each source term with every
target term. This means if both source and target
articles contain 100 terms then this leads to 10K
term pairs. We extracted features for each pair
of terms and ran the classifier to decide whether
the pair is positive or negative. Table 1 shows the
number of term pairs processed and the count of
pairs classified as positive. Table 2 shows five
positive term pairs extracted from the English-
German comparable corpora for each of the IT and
automotive domains. We manually assessed a sub-
set of the positive examples. We asked human as-
sessors to categorize each term pair into one of the
following categories:
1. Equivalence: The terms are exact transla-
tions/transliterations of each other.
2. Inclusion: Not an exact transla-
tion/transliteration, but an exact transla-
tion/transliteration of one term is entirely
contained within the term in the other lan-
guage, e.g: ?F1 car racing? vs ?Autorennen
(car racing)?.
3. Overlap: Not category 1 or 2, but the terms
share at least one translated/transliterated
word, e.g: ?hybrid electric vehicles? vs ?hy-
bride bauteile (hybrid components)?.
4. Unrelated: No word in either term is a trans-
lation/transliteration of a word in the other.
In the evaluation we randomly selected 300
pairs for each domain and showed them to two
German native speakers who were fluent in En-
glish. We asked the assessors to place each of the
term pair into one of the categories 1 to 4.
5.4 Results and Discussion
5.4.1 Performance test of the classifier
The results of the classifier evaluation are shown
in Table 3. The results show that the overall per-
formance of the classifier is very good. In many
cases the precision scores reach 100%. The low-
est precision score is obtained for Lithuanian (LT)
with 67%. For this language we performed an er-
ror analysis. In total there are 221 negative ex-
amples classified as positive. All these terms are
multi-term, i.e. each term pair contains at least
two words on each side. For the majority of the
misclassified terms ? 209 in total ? 50% or more
of the words on one side are either translations or
cognates of words on the other side. Of these, 187
contained 50% or more translation due to cognate
words ? examples of such cases are capital in-
crease ? kapitalo eksportas or Arab organisation
? Arabu lyga with the cognates capital ? kapitalo
and Arab ? Arabu respectively. For the remain-
der, 50% or more of the words on one side are
dictionary translations of words on the other side.
In order to understand the reason why the classi-
fier treats such cases as positive we examined the
407
Table 2: Example positive pairs for English-German.
IT Automotive
chromatographic technique ? chromatographie methode distribution infrastructure ? versorgungsinfrastruktur
electrolytic capacitor ? elektrolytkondensatoren ambient temperature ? au?enlufttemperatur
natural user interfaces ? natu?rliche benutzerschnittstellen higher cetane number ? erho?hter cetanzahl
anode voltage ? anodenspannung fuel tank ? kraftstoffpumpe
digital subscriber loop ? digitaler teilnehmeranschluss hydrogen powered vehicle ? wasserstoff fahrzeug
Table 3: Classifier performance results on EUROVOC data (P stands for precision, R for recall and F for F -measure). Each
language is paired with English. The test set contains 600 positive and 1359400 negative examples.
ET HU NL DA SV DE LV FI PT SL FR IT LT SK CS RO PL ES EL BG
P 1 1 .98 1 1 .98 1 1 .7 1 1 1 .67 .81 1 1 1 1 1 1
R .67 .72 .82 .69 .81 .77 .78 .65 .82 .66 .66 .7 .77 .84 .72 .78 .69 .8 .78 .79
F .80 .83 .89 .81 .89 .86 .87 .78 .75 .79 .79 .82 .71 .91 .83 .87 .81 .88 .87 .88
training data and found 467 positive pairs which
had the same characteristics as the negative exam-
ples in the testing set classified. We removed these
467 entries from the training set and re-trained the
classifier. The results with the new classifier are
99% precision, 68% recall and 80% F score.
In addition to Lithuanian, two further lan-
guages, Portuguese (PT) and Slovak (SK), also
had substantially lower precision scores. For these
languages we also removed positive entries falling
into the same problem categories as the LT ones
and trained new classifiers with the filtered train-
ing data. The precision results increased substan-
tially for both PT and SK ? 95% precision, 76%
recall, 84% F score for PT and 94% precision,
72% recall, 81% F score for SK. The recall scores
are lower than the precision scores, ranging from
65% to 84%. We have investigated the recall prob-
lem for FI, which has the lowest recall score at
65%. We observed that all the missing term pairs
were not cognates. Thus, the only way these terms
could be recognized as positive is if they are found
in the GIZA++ dictionaries. However, due to data
sparsity in these dictionaries this did not happen in
these cases. For these term pairs either the source
or target terms were not found in the dictionar-
ies. For instance, for the term pair offshoring ?
uudelleensijoittautuminen the GIZA++ dictionary
contains the entry offshoring but according to the
dictionary it is not translated into uudelleensijoit-
tautuminen, which is the matching term in EU-
ROVOC.
5.4.2 Manual evaluation
The results of the manual evaluation are shown in
Table 4. From the results we can see that both as-
sessors judge above 80% of the IT domain terms
as category 1 ? the category containing equivalent
Table 4: Results of the EN-DE manual evaluation by two
annotators. Numbers reported per category are percentages.
Domain Ann. 1 2 3 4
IT P1 81 6 6 7
P2 83 7 7 3
Automotive P1 66 12 16 6
P2 60 15 16 9
term pairs. Only a small proportion of the term
pairs are judged as belonging to category 4 (3?7%)
? the category containing unrelated term pairs. For
the automotive domain the proportion of equiva-
lent term pairs varies between 60 and 66%. For
unrelated term pairs this is below 10% for both as-
sessors.
We investigated the inter-annotator agreement.
Across the four classes the percentage agreement
was 83% for the automotive domain term pairs and
86% for the IT domain term pairs. The kappa
statistic, ?, was .69 for the automotive domain
pairs and .52 for the IT domain. We also consid-
ered two class agreement where we treated term
pairs within categories 2 and 3 as belonging to
category 4 (i.e. as ?incorrect? translations). In
this case, for the automotive domain the percent-
age agreement was 90% and ? = 0.72 and for the
IT domain percentage agreement was 89% with
? = 0.55. The agreement in the automotive do-
main is higher than in the IT one although both
judges were computer scientists. We analyzed
the differences and found that they differ in cases
where the German and the English term are both in
English. One of the annotators treated such cases
as correct translation, whereas the other did not.
We also checked to ensure our technique was
not simply rediscovering our dictionaries. Since
the GIZA++ dictionaries contain only single
word?single word mappings, we examined the
408
newly aligned term pairs that consisted of one
word on both source and target sides. Taking both
the IT and automotive domains together, our al-
gorithm proposed 5021 term pairs of which 2751
(55%) were word-word term pairs. 462 of these
(i.e. 17% of the word-word term pairs or 9% of
the overall set of aligned term pairs) were already
in either the EN-DE or DE-EN GIZA++ dictionar-
ies. Thus, of our newly extracted term pairs a rela-
tively small proportion are rediscovered dictionary
entries. We also checked our evaluation data to see
what proportion of the assessed term pairs were
already to be found in the GIZA++ dictionaries.
A total of 600 term pairs were put in front of the
judges of which 198 (33%) were word-word term
pairs. Of these 15 (less than 8% of the word-word
pairs and less then 3% of the overall assessed set of
assessed term pairs) were word-word pairs already
in the dictionaries. We conclude that our evalua-
tion results are not unduly affected by assessing
term pairs which were given to the algorithm.
Error analysis For both domains we performed
an error analysis for the unrelated, i.e. category
4 term pairs. We found that in both domains the
main source of errors is due to terms with different
meanings but similar spellings such as the follow-
ing example (1).
(1) accelerator ? decelerator
For this example the cognate methods, e.g. the
Levenshtein similarity measure, returns a score of
0.81. This problem could be addressed in different
ways. First, it could be resolved by applying a very
high threshold for the cognate methods. Any cog-
nate score below that threshold could be regarded
as zero ? as we did for the combined features (cf.
Section 4.4). However, setting a similarity thresh-
old higher than 0.9 ? to filter out cases as in (1)
? will cause real cognates with greater variation
in the spellings to be missed. This will, in par-
ticular, affect languages with a lot of inflection,
such as Latvian. Another approach to address this
problem would be to take the contextual or dis-
tributional properties of the terms into considera-
tion. To achieve this, training data consisting of
term pairs along with contextual information is re-
quired. However, such training data does not cur-
rently exist (i.e. resources like EUROVOC do not
contain contextual information) and it would need
to be collected as a first step towards applying this
approach to the problem.
Partial Translation The assessors assigned 6 ?
7% of the term pairs in the IT domain and 12 ?
16% in the automotive domain to categories 2 and
3. In both categories the term pairs share transla-
tions or cognates.
Clearly, if humans such as professional transla-
tors are the end users of these terms, then it could
be helpful for them to find some translation units
within the terms. In category 2 this will be the en-
tire translation of one term in the other such as the
following examples.5
(2) visible graphical interface ? grafische be-
nutzerschnittstelle
(3) modern turbocharger systems ? moderne
turbolader
In example (3) the a translation of the German
term is to be found entirely within in the English
term but the English term has the additional word
visible, a translation of which is not found in the
German term. In example (4), again the transla-
tion of the German term is entirely found in the
English term, but as in the previous example, one
of the English words ? systems ? in this case, has
no match within the German term. In category 3
there are only single word translation overlaps be-
tween the terms as shown in the following exam-
ples.
(4) national standard language ?
niederla?ndischen standardsprache
(5) thermoplastic material ? thermoplastische
elastomere
In example (5) standard language is translated
to standardsprache and in example (6) thermo-
plastic to thermoplastische. The other words
within the terms are not translations of each other.
Another application of the extracted term pairs
is to use them to enhance existing parallel corpora
to train SMT systems. In this case, including the
partially correct terms may introduce noise. This
is especially the case for the terms within category
3. However, the usefulness of terms in both these
scenarios requires further investigation, which we
aim to do in future work.
5In our data it is always the case that the target term is
entirely translated within the English one and the other way
round.
409
6 Conclusion
In this paper we presented an approach to align
terms identified by a monolingual term extractor in
bilingual comparable corpora using a binary clas-
sifier. We trained the classifier using data from
the EUROVOC thesaurus. Each candidate term
pair was pre-processed to extract various features
which are cognate-based or dictionary-based. We
measured the performance of our classifier using
Information Retrieval (IR) metrics and a manual
evaluation. In the IR evaluation we tested the per-
formance of the classifier on a held out test set
taken from EUROVOC. We used 20 EU language
pairs with English being always the source lan-
guage. The performance of our classifier in this
evaluation reached the 100% precision level for
many language pairs. In the manual evaluation
we had our algorithm extract pairs of terms from
Wikipedia articles ? articles forming comparable
corpora in the IT and automotive domains ? and
asked native speakers to categorize a selection of
the term pairs into categories reflecting the level
of translation of the terms. In the manual evalu-
ation we used the English-German language pair
and showed that over 80% of the extracted term
pairs were exact translations in the IT domain and
over 60% in the automotive domain. For both do-
mains over 90% of the extracted term pairs were
either exact or partial translations.
We also performed an error analysis and high-
lighted problem cases, which we plan to address
in future work. Exploring ways to add contextual
or distributional features to our term representa-
tions is also an avenue for future work, though it
clearly significantly complicates the approach, one
of whose advantages is its simplicitiy. Further-
more, we aim to extend the existing dictionaries
and possibly our training data with terms extracted
from comparable corpora. Finally, we plan to in-
vestigate the usefulness of the terms in different
application scenarios, including computer assisted
translation and machine translation.
Acknowledgements
The research reported was funded by the TaaS
project, European Union Seventh Framework Pro-
gramme, grant agreement no. 296312. The au-
thors would like to thank the manual annotators
for their helpful contributions. We would also like
to thank partners at Tilde SIA and at the University
of Zagreb for supplying the TWSC term extraction
tool, developed within the EU funded project AC-
CURAT.
References
A. Aker, Y. Feng, and R. Gaizauskas. 2012. Auto-
matic bilingual phrase extraction from comparable
corpora. In 24th International Conference on Com-
putational Linguistics (COLING 2012), IIT Bom-
bay, Mumbai, India, 2012. Association for Compu-
tational Linguistics.
Y. Al-Onaizan and K. Knight. 2002. Machine translit-
eration of names in arabic text. In Proceedings of
the ACL-02 workshop on Computational approaches
to semitic languages, pages 1?13. Association for
Computational Linguistics.
N. Aswani and R. Gaizauskas. 2010. English-hindi
transliteration using multiple similarity metrics. In
Proceedings of the Seventh International Confer-
ence on Language Resources and Evaluation (LREC
2010), Valetta, Malta.
D. Bouamor, N. Semmar, and P. Zweigenbaum. 2012.
Identifying bilingual multi-word expressions for sta-
tistical machine translation. In LREC 2012, Eigth
International Conference on Language Resources
and Evaluation, pages 674-679, Istanbul, Turkey,
2012. ELRA.
Y. Cao and H. Li. 2002. Base noun phrase translation
using web data and the em algorithm. In Proceed-
ings of the 19th international conference on Com-
putational linguistics-Volume 1, pages 1?7. Associ-
ation for Computational Linguistics.
T. H. Cormen, C. E. Leiserson, R. L. Rivest, and
C. Stein. 2001. Introduction to Algorithms. The
MIT Press, 2nd revised edition, September.
B. Daille, E?. Gaussier, and J.M. Lange?. 1994. Towards
automatic extraction of monolingual and bilingual
terminology. In Proceedings of the 15th conference
on Computational linguistics-Volume 1, pages 515?
521. Association for Computational Linguistics.
X. Fan, N. Shimizu, and H. Nakagawa. 2009. Auto-
matic extraction of bilingual terms from a chinese-
japanese parallel corpus. In Proceedings of the
3rd International Universal Communication Sympo-
sium, pages 41?45. ACM.
P. Fung and K. McKeown. 1997. Finding terminol-
ogy translations from non-parallel corpora. In Pro-
ceedings of the 5th Annual Workshop on Very Large
Corpora, pages 192?202.
A. Ismail and S. Manandhar. 2010. Bilingual lexi-
con extraction from comparable corpora using in-
domain terms. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, pages 481?489. Association for Computa-
tional Linguistics.
410
T. Joachims. 2002. Learning to classify text using sup-
port vector machines: Methods, theory and algo-
rithms, volume 186. Kluwer Academic Publishers
Norwell, MA, USA:.
S. Karimi, F. Scholer, and A. Turpin. 2011. Ma-
chine transliteration survey. ACM Computing Sur-
veys (CSUR), 43(3):17.
K. Knight and J. Graehl. 1998. Machine translitera-
tion. Computational Linguistics, 24(4):599?612.
J. Kupiec. 1993. An algorithm for finding noun phrase
correspondences in bilingual corpora. In Proceed-
ings of the 31st annual meeting on Association for
Computational Linguistics, pages 17?22. Associa-
tion for Computational Linguistics.
R. Moore. 2003. Learning translations of named-
entity phrases from parallel corpora. In In Proceed-
ings of the tenth conference on European chapter
of the Association for Computational Linguistics-
Volume 1, pages 259266. Association for Compu-
tational Linguistics.
E. Morin, B. Daille, K. Takeuchi, and K. Kageura.
2007. Bilingual terminology mining - using brain,
not brawn comparable corpora. In Proceedings
of the 45th Annual Meeting of the Association of
Computational Linguistics, pages 664?671, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
F. J. Och and H. Ney. 2000. A comparison of align-
ment models for statistical machine translation. In
Proceedings of the 18th conference on Computa-
tional linguistics, pages 1086?1090, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
F. J. Och Och and H. Ney. 2003. A systematic compar-
ison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51.
T. Okita, A. Maldonado Guerra, Y. Graham, and
A. Way. 2010. Multi-word expression-sensitive
word alignment. Association for Computational
Linguistics.
Ma?rcis Pinnis, Nikola Ljubes?ic?, Dan S?tefa?nescu, In-
guna Skadin?a, Marko Tadic?, and Tatiana Gornostay.
2012. Term extraction, tagging, and mapping tools
for under-resourced languages. In Proc. of the 10th
Conference on Terminology and Knowledge Engi-
neering (TKE 2012), June, pages 20?21.
R. Rapp. 1995. Identifying word translations in non-
parallel texts. In Proceedings of the 33rd annual
meeting on Association for Computational Linguis-
tics, pages 320?322. Association for Computational
Linguistics.
Helmut Schmid. 1995. Treetagger? a lan-
guage independent part-of-speech tagger. Insti-
tut fu?r Maschinelle Sprachverarbeitung, Universita?t
Stuttgart, page 43.
C. Snae. 2007. A comparison and analysis of
name matching algorithms. International Journal
of Applied Science. Engineering and Technology,
4(1):252?257.
R. Steinberger, B. Pouliquen, and J. Hagman. 2002.
Cross-lingual document similarity calculation using
the multilingual thesaurus eurovoc. Computational
Linguistics and Intelligent Text Processing, pages
101?121.
R. Steinberger, A. Eisele, S. Klocek, S. Pilos, and
P. Schlter. 2012. Dgt-tm: A freely available trans-
lation memory in 22 languages. In Proceedings of
LREC, pages 454?459.
R. Udupa, K. Saravanan, A. Kumaran, and J. Jagarla-
mudi. 2008. Mining named entity transliteration
equivalents from comparable corpora. In Proceed-
ing of the 17th ACM conference on Information and
knowledge management, pages 1423?1424. ACM.
411
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 645?650,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Temporal Signals Help Label Temporal Relations
Leon Derczynski and Robert Gaizauskas
Natural Language Processing Group
Department of Computer Science
University of Sheffield
211 Portobello, S1 4DP, Sheffield, UK
{leon,robertg}@dcs.shef.ac.uk
Abstract
Automatically determining the temporal order
of events and times in a text is difficult, though
humans can readily perform this task. Some-
times events and times are related through use
of an explicit co-ordination which gives infor-
mation about the temporal relation: expres-
sions like ?before? and ?as soon as?. We in-
vestigate the ro?le that these co-ordinating tem-
poral signals have in determining the type of
temporal relations in discourse. Using ma-
chine learning, we improve upon prior ap-
proaches to the problem, achieving over 80%
accuracy at labelling the types of temporal re-
lation between events and times that are re-
lated by temporal signals.
1 Introduction
It is important to understand time in language. The
ability to express and comprehend expressions of time
enables us to plan, to tell stories, and to discuss change
in the world around us.
When we automatically extract temporal informa-
tion, we are often concerned with events and times ? re-
ferred to collectively as temporal intervals. We might
ask, for example, ?Who is the current President of the
USA?.? In order to extract an answer to this question
from a document collection, we need to identify events
related to persons becoming president and the times of
those events. Crucially, however, we also need to iden-
tify the temporal relations between these events and
times, perhaps, for example, by recognizing a tempo-
ral relation type from a set such as that of Allen (1983).
This last task, temporal relation typing, is challeng-
ing, and is the focus of this paper.
Temporal signals are words or phrases that act as
discourse markers that co-ordinate a pair of events or
times and explicitly state the nature of the temporal re-
lation that holds between them. For example, in ?The
parade reached the town hall before noon?, the word
before is a temporal signal, co-ordinating the event
reached with the time noon. Intuitively, these signal
words act as discourse contain temporal ordering infor-
mation that human readers can readily access, and in-
deed this hypothesis is borne out empirically (Bestgen
and Vonk, 1999). In this paper, we present an in-depth
examination into the role temporal signals can play in
machine learning for temporal relation typing, within
the framework of TimeML (Pustejovsky et al, 2005).
2 Related Work
Temporal relation typing is not a new problem. Clas-
sical work using TimeML is that of Boguraev and
Ando (2005), Mani et al (2007) and Yoshikawa et al
(2009). The TempEval challenge series features re-
lation typing as a key task (Verhagen et al, 2009).
The take-home message from all this work is that tem-
poral relation typing is a hard problem, even using
advanced techniques and extensive engineering ? ap-
proaches rarely achieve over 60% on typing relations
between two events or over 75% accuracy for those be-
tween an event and a time. Recent attempts to include
more linguistically sophisticated features representing
discourse, syntactic and semantic role information have
yielded but marginal improvements, e.g. Llorens et al
(2010); Mirroshandel et al (2011).
Although we focus solely on determining the types
of temporal relations, one must also identify which
pairs of temporal intervals should be temporally re-
lated. Previous work has covered the tasks of identi-
fying and typing temporal relations jointly with some
success (Denis and Muller, 2011; Do et al, 2012). The
TempEval3 challenge addresses exactly this task (Uz-
Zaman et al, 2013).
Investigations into using signals for temporal rela-
tion typing have had promising results. Lapata and
Lascarides (2006) learn temporal structure according
to these explicit signals, then predict temporal order-
ings in sentences without signals. As part of an early
TempEval system, Min et al (2007) automatically an-
notate signals and associate them with temporal rela-
tions. They then include the signal text as a feature
for a relation type classifier. Their definition of sig-
nals varies somewhat from the traditional TimeML sig-
645
Event-event relations Event-time relations
Non-signalled Signalled Overall Non-signalled Signalled Overall
Baseline most-common-class 41.4% 57.4% 43.0% 49.2% 51.6% 49.6%
Maxent classifier 57.7% 58.6% 57.8% 81.4% 59.6% 77.3%
Error reduction 27.8% 2.74% 25.4% 64.5% 16.4% 55.5%
Sample size (number of relations) 3 179 343 3 522 2 299 529 2 828
Table 1: Relation typing performance using the base feature set, for relations with and without a temporal signal.
nal definition, as they include words such as reporting
which would otherwise be annotated as an event. The
system achieves a 22% error reduction on a simplified
set of temporal relation types.
Later, Derczynski and Gaizauskas (2010) saw a 50%
error reduction in assignment of relation types on sig-
nalled relation instances from introducing simple fea-
tures describing a temporal signal?s interaction with the
events or times that it co-ordinates. The features for de-
scribing signals included the signal text itself and the
signal?s position in the document relative to the inter-
vals it co-ordinated. This led to a large increase in re-
lation typing accuracy to 82.19% for signalled event-
event relations, using a maximum entropy classifier.
Previous work has attempted to linguistically charac-
terise temporal signals (Bre?e et al, 1993; Derczynski
and Gaizauskas, 2011). Signal phrases typically fall
into one of three categories: monosemous as temporal
signals (e.g. ?during?, ?when?); bisemous as temporal
or spatial signals (e.g. ?before?); or polysemous with
the temporal sense a minority class (e.g. ?in?, ?fol-
lowing?). Further, a signal phrase may take two argu-
ments, though its arguments need not be in the imme-
diate content and may be anaphoric. We leave the task
of automatic signal annotation to future work, instead
focusing on the impact that signals have on temporal
relation typing.
Our work builds on previous work by expanding the
study to include relations other than just event-event
relations, by extending the feature set, by doing tem-
poral relation labelling over a more carefully curated
version of the TimeBank corpus (see below), and by
providing detailed analysis of the performance of a set
of labelling techniques when using temporal signals.
3 Experimental Setup
We only approach the relation typing task, and we use
existing signal annotations ? that is, we do not attempt
to automatically identify temporal signals.
The corpus used is the signal-curated version of
TimeBank (Pustejovsky et al, 2003). This corpus, TB-
sig,1 adds extra events, times and relations to Time-
Bank, in an effort to correct signal under-annotation in
the original corpus (Derczynski and Gaizauskas, 2011).
Like the original TimeBank corpus, it comprises 183
documents. In these, we are interested only in the tem-
poral relations that use a signal. There are 851 signals
annotated in the corpus, co-ordinating 886 temporal re-
1See http://derczynski.com/sheffield/resources/tb sig.tar.bz2
lations (13.7% of all). For comparison, TimeBank has
688 signal annotations which co-ordinate 718 temporal
relations (11.2%).
When evaluating classifiers, we performed 10-fold
cross-validation, keeping splits at document level.
There are only 14 signalled time-time relations in this
corpus, which is not enough to support any generaliza-
tions, and so we disregard this interval type pairing.
As is common with statistical approaches to tempo-
ral relation typing, we also perform relation folding;
that is, to reduce the number of possible classes, we
sometimes invert argument order and relation type. For
example, A BEFORE B and B AFTER A convey the
same temporal relation, and so we can remove all AF-
TER-type relations by swapping their argument order
and converting them to BEFORE relations. This loss-
less process condenses the labels that our classifier has
to distinguish between, though classification remains a
multi-class problem.
We adopt the base feature set of Mani et al (2007),
which consists mainly of TimeML event and time
annotation surface attributes. These are, for events:
class, aspect, modality, tense, polarity, part
of speech; and, for times: value, type, function
in document, mod, quant. To these are added
same-tense and same-aspect features, as well as
the string values of events/times.
The feature groups we use here are:
? Base ? The attributes of TimeML annotations in-
volved (includes tense, aspect, polarity and so on
as above), as with previous approaches.
? Argument Ordering ? Two features: a boolean
set if both arguments are in the same sentence (as
in Chambers et al (2007)), and the text order of
argument intervals (as in Hepple et al (2007)).
? Signal Ordering ? Textual ordering is important
with temporal signals; compare ?You walk before
you run? and ?Before you walk you run?. We
add features accounting for relative textual posi-
tion of signal and arguments as per Derczynski
and Gaizauskas (2010). To these we add a feature
reporting whether the signal occurs in first, last,
or mid-sentence position, and features to indicate
whether each interval is in the same sentence as
the signal.
? Syntactic ? We add syntactic features: fol-
lowing Bethard et al (2007), the lowest com-
mon constituent label between each argument and
646
Features Classifier Event-event accuracy Event-time accuracy
N/A Baseline most-common-class 57.4% 51.6%
Base Baseline maximum entropy 58.6% 59.6%
Maximum entropy 72.6% 72.4%DG2010 Random forest 76.7% 78.6%
All
Adaptive boosting 70.4% 73.0%
Na??ve Bayes 73.8% 71.5%
Maximum entropy 75.5% 78.1%
Linear SVC / Crammer-Singer 79.3% 75.6%
Linear SVC 80.7% 77.1%
Random forest 80.8% 80.3%
Table 2: Results at temporal relation typing over TB-sig, for relations that use a temporal signal
the signal; following Swampillai and Stevenson
(2011), the syntactic path from each argument
to the signal, using a top-level ROOT node for
cross-sentence paths; and three features indicat-
ing whether there is a temporal function tag (-TMP
between each of the intervals or the signal to the
root note. These features are generated using the
Stanford parser (Klein and Manning, 2003) and a
function tagger (Blaheta and Charniak, 2000).
? Signal Text ? We add the signal?s raw string, as
well as its lower-case version and its lemma.
? DCT ? For event-time relations, whether the time
expression also functions as the document?s cre-
ation timestamp.
Collectively, these feature groups comprise the All
feature set. For comparison, the feature set we reported
in previous work (Derczynski and Gaizauskas, 2010)
is also included, labeled DG2010. This set contains the
base and the signal ordering feature groups only, plus a
single signal feature for the signal raw string.
Using these feature representations we trained multi-
nomial na??ve Bayes (Rennie et al, 2003), maximum
entropy (Daume? III, 2008), adaptive boosting (Fre-
und and Schapire, 1997; Zhu et al, 2009), multi-class
SVM (Crammer and Singer, 2002; Chang and Lin,
2011) and random forest2 (Breiman, 2001) classifiers
via Scikit-learn (Pedregosa et al, 2011).
We use two baselines: most-common-class and a
model trained with no signal features. We also in-
troduce two measures replicating earlier work: one
using the DG2010 features and the classifier used in
that work (maximum entropy), and another using the
DG2010 features with the best-performing classifier
under our All feature set, in order to see if performance
changes are due to features or classifier.
Classifiers were evaluated by determining if the class
they output matched the relation type in TB-sig. Re-
sults are given in Table 2. For comparison with the
general case, i.e. for both signalled and non-signalled
temporal relation instances, we list performance with
a maximum entropy classifier and the base feature set
2With nestimators = 200, a minimum of one sample per
node, and no maximum depth.
Figure 1: Effect of training data size on relation typing
performance.
on TB-sig?s temporal relations. Results are in Table 1.
These are split into those that use a signal and those that
do not, though no features relaying signal information
are included.
In order to assess the adequacy of the dataset in
terms of size, we also examined performance using a
maximum entropy classifier learned from varying sub-
proportions of the training data. This was measured
over event-event relations, using all features. Results
are given in Figure 1. That performance appears to sta-
bilise and level off indicates that the training set is of
sufficient size for these experiments.
4 Analysis
The results in Table 2 echo earlier findings and intu-
ition: temporal signals are useful in temporal relation
typing. Results support that signals are not only helpful
in event-event relation typing but also event-time typ-
ing. For comparison, inter-annotator agreement across
all temporal relation labels, i.e. signalled and non-
signalled relations, in TimeBank is 77%.
Using the maximum entropy classifier, our approach
gives a 2.9% absolute performance increase over the
DG2010 feature set for event-event relations (10.6% er-
ror reduction) and a 5.7% absolute increase for event-
time relations (20.7% error reduction). Random forests
647
Feature sets Evt-evt Evt-time
All 80.8% 80.3%
All-argument order 80.8% 78.3%
All-signal order 79.0% 77.5%
All-syntax 79.2% 79.6%
All-signal text 70.8% 72.7%
All-DCT 79.9% 79.4%
Base 54.2% 53.9%
Base+argument order 56.8% 60.1%
Base+signal order 59.7% 65.0%
Base+syntax 70.0% 71.0%
Base+signal text 75.5% 66.3%
Base+DCT 54.2% 53.9%
Base+signal text+signal order 80.4% 76.9%
Base+signal text+syntax 79.0% 74.1%
Base+arg order+signal order 77.8% 75.2%
Table 3: Relation typing accuracy based on various fea-
ture combinations, using random forests. Bold figures
indicate the largest performance change.
offer better performance under both feature sets, with
the extended features achieving notable error reduction
over DG2010 ? 17.6% for event-event, 7.9% for event-
time relations. Linear support vector classification pro-
vided rapid labelling and comparable performance for
event-event relations but was accuracy was not as good
as random forests for event-time relation labelling.
Note, figures reported earlier in Derczynski and
Gaizauskas (2010) are not directly comparable to the
DG2010 figures reported here, as here we are using the
better-annotated TB-sig corpus, which contains a larger
and more varied set of temporal signal annotations.
Although we are only examining the 13.7% of tem-
poral relations that are co-ordinated with a signal, it
is important to note the performance of conventional
classification approaches on this subset of temporal
relations. Specifically, the error reduction relative to
the baseline that is achieved without signal features is
much lower on relations that use signals than on non-
signalled relations (Table 1). Thus, temporal relations
that use a signal appear to be more difficult to clas-
sify than other relations, unless signal information is
present in the features. This may be due to differences
in how signals are used by authors. One explanation
is that signals may be used in the stead of temporal or-
dering information in surrounding discourse, such as
modulations of dominant tense or aspect (Derczynski
and Gaizauskas, 2013).
Unlike earlier work using maxent, we experiment
with a variety of classifiers, and find a consistent im-
provement in temporal relation typing using signal fea-
tures. With the notable exception of adaptive boost-
ing, classifiers with preference bias (Liu et al, 2002)
? AdaBoost, random trees and SVC ? performed best
in this task. Conversely, those tending toward the in-
dependence assumption (na??ve Bayes and maxent) did
not capitalise as effectively on the training data.
Features Evt-evt Evt-time
All 80.8% 80.3%
All-signal text 70.8% 72.7%
All-signal text-argument order 70.7% 72.2%
All-signal text-signal order 69.5% 71.2%
All-signal text-syntax 59.5% 69.0%
All-signal text-DCT 70.8% 72.8%
Table 4: Feature ablation without signal text features.
Bold figures indicate largest performance change.
We also investigated the impact of each feature
group on the best-performing classifier (random forests
with n = 200) through feature ablation. Results are
given in Table 3. Ablation suggested that the signal text
features (signal string, lower case string, head word and
lemma) had most impact in event-event relation typing,
though were second to syntax features in event-time re-
lations. Removing other feature groups gave only mi-
nor performance decreases.
We also experimented with adding feature groups to
the base set one-by-one. All but DCT features gave
above-baseline improvement, though argument order-
ing features were not very helpful for event-event re-
lation typing. Signal text features gave the strongest
improvement over baseline for event-event relations,
but syntax gave a larger improvement for event-time
relations. Accordingly, it may be useful to distinguish
between event-event and event-time relations when ex-
tracting temporal information using syntax (c.f. the ap-
proach of Wang et al (2010)).
A strong above-baseline performance was still ob-
tained even when signal text features were removed,
which included the signal text itself. This was interest-
ing, as signal phrases can indicate quite different tem-
poral orderings (e.g. ?Open the box while it rains? vs.
?Open the box before it rains?, and the words used are
typically critical to correct interpretation of the tempo-
ral relation. Further, the model is able to generalise
beyond particular signal phrase choices. To investigate
further, we examined the performance impact of each
group sans ?signal text? features (Table 4). In this case,
removing the syntactic features had the greatest (neg-
ative) impact on performance, though the absolute im-
pact on event-event relations (a drop of 11.3%) was far
lower than that on event-time relations (3.7%).
To examine helpful features, we trained a max-
ent classifier on the entire dataset and collected fea-
ture:value pairs. These were then ranked by their
weight. The ten largest-weighted pairings for event-
event relations (the hardest problem in overall temporal
relation typing) are given in Table 5. Prefixes of 1- and
2- correspond to the two interval arguments (events).
Negative values are those where the presence of a par-
ticular feature:value pair suggests the mentioned class
is not applicable.
648
Weight Feature Value Class
9.346 2-polarity POS ENDS
-8.713 1-2-same-sent True BEGINS
-7.861 2-aspect NONE BEGINS
-7.256 1-aspect NONE INCLUDES
6.564 2-sig-synt-path NN-NP-IN INCLUDES
6.519 signal-lower before ENDS
-6.294 2-tense NONE BEGINS
-5.908 2-modality None ENDS
5.643 2-text took BEGINS
-5.580 1-modality None ENDS
Table 5: Top ten largest-weighted feature:value pairs.
It can be seen that BEGINS and INCLUDES rela-
tionships are not indicated if the arguments have no
TimeML aspect assigned; this is what one might ex-
pect, given how aspect is used in English, with these
temporal relation types corresponding to event starts
and the progressive. Also, notice how a particular syn-
tactic path, connecting adjacent nominalised event and
the word in acting as a signal, indicate a temporal inclu-
sion relationship. Temporal polysemy, where a word
has more than one possible temporal interpretation,
is also observable here (Derczynski and Gaizauskas
(2011) examine this polysemy in depth). This is vis-
ible in how the temporal signal phrase ?before? is not,
as one might expect, a strong indicator of a BEFORE or
even AFTER relation, but of an ENDS relationship.
5 Conclusion
This paper set out to investigate the ro?le of temporal
signals in predicting the type of temporal relation be-
tween two intervals. The paper demonstrated the util-
ity of temporal signals in this task, and identified ap-
proaches for using the information these signals con-
tain, which performed consistently better than the state-
of-the-art across a range of machine learning classi-
fiers. Further, it identified the impact that signal text,
signal order and syntax features had in temporal rela-
tion typing of signalled relations.
Two directions of future work are indicated. Firstly,
the utility of signals prompts investigation into detect-
ing which words in a given text occur as temporal sig-
nals. Secondly, it is intuitive that temporal signals ex-
plicitly indicate related pairs of intervals (i.e. events or
times). So, the task of deciding which interval pair(s) a
temporal signal co-ordinates must be approached.
Although we have found a method for achieving
good temporal relation typing performance on a subset
of temporal relations, the greater problem of general
temporal relation typing remains. A better understand-
ing of the semantics of events, times, signals and how
they are related together through syntax may provide
further insights into the temporal relation typing task.
Finally, Bethard et al (2007) reached high temporal
relation typing performance on one a subset of relations
(events and times in the same sentence); we reach high
temporal relation typing performance on another subset
of relations ? those using a temporal signal. Identify-
ing further explicit sources of temporal information ap-
plicable to new sets of relations may reveal promising
paths for investigation.
Acknowledgements
The first author was supported by UK EPSRC grant
EP/K017896/1, uComp (http://www.ucomp.eu/).
References
J. Allen. 1983. Maintaining knowledge about temporal
intervals. Communications of the ACM, 26(11):832?
843.
Y. Bestgen and W. Vonk. 1999. Temporal adverbials as
segmentation markers in discourse comprehension.
Journal of Memory and Language, 42(1):74?87.
S. Bethard, J. Martin, and S. Klingenstein. 2007.
Timelines from text: Identification of syntactic tem-
poral relations. In Proceedings of the International
Conference on Semantic Computing, pages 11?18.
D. Blaheta and E. Charniak. 2000. Assigning function
tags to parsed text. In Proceedings of the meeting
of the North American chapter of the Association for
Computational Linguistics, pages 234?240. ACL.
B. Boguraev and R. K. Ando. 2005. TimeBank-Driven
TimeML Analysis. In G. Katz, J. Pustejovsky, and
F. Schilder, editors, Annotating, Extracting and Rea-
soning about Time and Events, number 05151 in
Dagstuhl Seminar Proceedings, Dagstuhl, Germany.
Internationales Begegnungs- und Forschungszen-
trum fu?r Informatik (IBFI), Schloss Dagstuhl, Ger-
many.
D. Bre?e, A. Feddag, and I. Pratt. 1993. Towards a for-
malization of the semantics of some temporal prepo-
sitions. Time & Society, 2(2):219.
L. Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5?32.
N. Chambers, S. Wang, and D. Jurafsky. 2007. Clas-
sifying temporal relations between events. In Pro-
ceedings of the 45th meeting of the Association for
Computational Linguistics, pages 173?176. ACL.
C.-C. Chang and C.-J. Lin. 2011. LIBSVM: a library
for support vector machines. ACM Transactions on
Intelligent Systems and Technology, 2(3):27.
K. Crammer and Y. Singer. 2002. On the algorith-
mic implementation of multiclass kernel-based vec-
tor machines. The Journal of Machine Learning Re-
search, 2:265?292.
H. Daume? III. 2008. MegaM: Maximum entropy
model optimization package. ACL Data and Code
Repository, ADCR2008C003, 50.
649
P. Denis and P. Muller. 2011. Predicting globally-
coherent temporal structures from texts via endpoint
inference and graph decomposition. In Proceedings
of the International Joint Conference on Artificial In-
telligence, pages 1788?1793. AAAI Press.
L. Derczynski and R. Gaizauskas. 2010. Using Sig-
nals to Improve Automatic Classification of Tempo-
ral Relations. In Proceedings of 15th Student Ses-
sion of the European Summer School for Logic, Lan-
guage and Information, pages 224?231. FoLLI.
L. Derczynski and R. Gaizauskas. 2011. A Corpus-
based Study of Temporal Signals. In Proceedings of
the Corpus Linguistics Conference.
L. Derczynski and R. Gaizauskas. 2013. Empirical
Validation of Reichenbach?s Tense Framework. In
Proceedings of the 10th International Conference on
Computational Semantics, pages 71?82. ACL.
Q. X. Do, W. Lu, and D. Roth. 2012. Joint infer-
ence for event timeline construction. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 677?687. ACL.
Y. Freund and R. E. Schapire. 1997. A decision-
theoretic generalization of on-line learning and an
application to boosting. Journal of Computer and
System Sciences, 55(1):119?139.
M. Hepple, A. Setzer, and R. Gaizauskas. 2007.
USFD: preliminary exploration of features and clas-
sifiers for the TempEval-2007 tasks. In Proceedings
of the 4th International Workshop on Semantic Eval-
uations, pages 438?441. ACL.
D. Klein and C. D. Manning. 2003. Accurate unlex-
icalized parsing. In Proceedings of the 41st meet-
ing of the Association for Computational Linguistics,
pages 423?430. ACL.
M. Lapata and A. Lascarides. 2006. Learning
sentence-internal temporal relations. Journal of Ar-
tificial Intelligence Research, 27(1):85?117.
Y. Liu, Y. Yang, and J. Carbonell. 2002. Boosting to
correct inductive bias in text classification. In Pro-
ceedings of the 11th international Conference on In-
formation and Knowledge Management, pages 348?
355. ACM.
H. Llorens, E. Saquete, and B. Navarro. 2010. TIPSem
(English and Spanish): Evaluating CRFs and Se-
mantic Roles in TempEval-2. In Proceedings of
SemEval-2010. ACL.
I. Mani, B. Wellner, M. Verhagen, and J. Pustejovsky.
2007. Three approaches to learning TLINKS in
TimeML. Technical report, CS-07-268, Brandeis
University.
C. Min, M. Srikanth, and A. Fowler. 2007. LCC-TE:
A hybrid approach to temporal relation identification
in news text. In Proceedings of the 4th International
Workshop on Semantic Evaluations, pages 219?222.
ACL.
S. A. Mirroshandel, G. Ghassem-Sani, and
M. Khayyamian. 2011. Using syntactic-based
kernels for classifying temporal relations. Journal
of Computer Science and Technology, 26(1):68?80.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, et al 2011. Scikit-learn: Ma-
chine learning in Python. The Journal of Machine
Learning Research, 12:2825?2830.
J. Pustejovsky, R. Sauri, R. Gaizauskas, A. Setzer,
L. Ferro, et al 2003. The TimeBank Corpus. In
Proceedings of the Corpus Linguistics Conference,
pages 647?656.
J. Pustejovsky, J. Castano, R. Ingria, R. Saur??,
R. Gaizauskas, A. Setzer, G. Katz, and D. Radev.
2005. TimeML: Robust specification of event and
temporal expressions in text. In I. Mani, J. Puste-
jovsky, and R. Gaizauskas, editors, The language of
time: a reader. Oxford University Press.
J. D. Rennie, L. Shih, J. Teevan, and D. Karger. 2003.
Tackling the Poor Assumptions of Naive Bayes Text
Classifiers. In Proceedings of the International Con-
ference on Machine Learning. AAAI Press.
K. Swampillai and M. Stevenson. 2011. Extracting re-
lations within and across sentences. In Proceedings
of the International Conference Recent Advances in
Natural Language Processing, pages 25?32. ACL.
N. UzZaman, H. Llorens, L. Derczynski, M. Verhagen,
J. F. Allen, and J. Pustejovsky. 2013. SemEval-2013
Task 1: TempEval-3: Evaluating Time Expressions,
Events, and Temporal Relations. In Proceedings of
the 7th International Workshop on Semantic Evalu-
ations.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hep-
ple, J. Moszkowicz, and J. Pustejovsky. 2009.
The TempEval challenge: identifying temporal re-
lations in text. Language Resources and Evaluation,
43(2):161?179.
W. Wang, J. Su, and C. L. Tan. 2010. Kernel based
discourse relation recognition with temporal order-
ing information. In Proceedings of the 48th meet-
ing of the Association for Computational Linguistics,
pages 710?719. ACL.
K. Yoshikawa, S. Riedel, M. Asahara, and Y. Mat-
sumoto. 2009. Jointly identifying temporal relations
with Markov logic. In Proceedings of the Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 405?413. ACL.
J. Zhu, H. Zou, S. Rosset, and T. Hastie. 2009. Multi-
class AdaBoost. Statistics and Its Interface, 2:349?
360.
650
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 75?80,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Graph Ranking for Collective Named Entity Disambiguation
Ayman Alhelbawy
1,2
and Robert Gaizauskas
1
1
The University of Sheffield, Regent Court, 211 Portobello Street, Sheffield, S1 4DP, U.K
2
Faculty of Computers and Information, Fayoum University, Fayoum, Egypt
ayman,R.Gaizauskas@dcs.shef.ac.uk
Abstract
Named Entity Disambiguation (NED)
refers to the task of mapping different
named entity mentions in running text to
their correct interpretations in a specific
knowledge base (KB). This paper presents
a collective disambiguation approach us-
ing a graph model. All possible NE candi-
dates are represented as nodes in the graph
and associations between different candi-
dates are represented by edges between the
nodes. Each node has an initial confidence
score, e.g. entity popularity. Page-Rank
is used to rank nodes and the final rank
is combined with the initial confidence
for candidate selection. Experiments on
27,819 NE textual mentions show the ef-
fectiveness of using Page-Rank in con-
junction with initial confidence: 87% ac-
curacy is achieved, outperforming both
baseline and state-of-the-art approaches.
1 Introduction
Named entities (NEs) have received much atten-
tion over the last two decades (Nadeau and Sekine,
2007), mostly focused on recognizing the bound-
aries of textual NE mentions and classifying them
as, e.g., Person, Organization or Location. How-
ever, references to entities in the real world are of-
ten ambiguous: there is a many-to-many relation
between NE mentions and the entities they denote
in the real world. For example, Norfolk may refer
to a person, ?Peter Norfolk, a wheelchair tennis
player?, a place in the UK, ?Norfolk County?, or
in the US, ?Norfolk, Massachusetts?; conversely,
one entity may be known by many names, such as
?Cat Stevens?, ?Yusuf Islam? and ?Steven Geor-
giou?. The NED task is to establish a correct map-
ping between each NE mention in a document and
the real world entity it denotes. Following most re-
searchers in this area, we treat entries in a large
Figure 1: Example of solution graph
knowledge base (KB) as surrogates for real world
entities when carrying out NED and, in particu-
lar, use Wikipedia as the reference KB for dis-
ambiguating NE mentions. NED is important for
tasks like KB population, where we want to ex-
tract new information from text about an entity and
add this to a pre-existing entry in a KB; or for in-
formation retrieval, where we may want to cluster
or filter results for different entities with the same
textual mentions.
The main hypothesis in this work is that differ-
ent NEs in a document help to disambiguate each
other. The problem is that other textual mentions
in the document are also ambiguous. So, what is
needed is a collective disambiguation approach
that jointly disambiguates all NE textual mentions.
In our approach we model each possible can-
didate for every NE mention in a document as a
distinct node in a graph and model candidate co-
herence by links between the nodes. We call such
graphs solution graphs. Figure 1 shows an exam-
ple of the solution graph for three mentions ?A?,
?B?, and ?C? found in a document, where the can-
didate entities for each mention are referred to us-
ing the lower case form of the mention?s letter to-
gether with a distinguishing subscript. The goal of
disambiguation is to find a set of nodes where only
one candidate is selected from the set of entities
associated with each mention, e.g. a
3
, b
2
, c
2
.
Our approach first ranks all nodes in the solu-
tion graph using the Page-Rank algorithm, then re-
75
ranks all nodes by combining the initial confidence
and graph ranking scores. We consider several dif-
ferent measures for computing the initial confi-
dence assigned to each node and several measures
for determining and weighting the graph edges.
Node linking relies on the fact that the textual por-
tion of KB entries typically contains mentions of
other NEs. When these mentions are hyper-linked
to KB entries, we can infer that there is some rela-
tion between the real world entities corresponding
to the KB entries, i.e. that they should be linked
in our solution graph. These links also allow us to
build up statistical co-occurrence counts between
entities that occur in the same context which may
be used to weight links in our graph.
We evaluate our approach on the AIDA dataset
(Hoffart et al, 2011). Comparison with the
baseline approach and some state-of-the-art ap-
proaches shows our approach offers substantial
improvements in disambiguation accuracy.
2 Related Work
In 2009, NIST proposed the shared task challenge
of Entity Linking (EL) (McNamee and Dang,
2009). EL is a similar but broader task than NED
because NED is concerned with disambiguating
a textual NE mention where the correct entity is
known to be one of the KB entries, while EL also
requires systems to deal with the case where there
is no entry for the NE in the reference KB. Ji et
al. (2011) group and summarise the different ap-
proaches to EL taken by participating systems.
In general, there are two main lines of approach
to the NED problem. Single entity disambigua-
tion approaches (SNED), disambiguate one entity
at a time without considering the effect of other
NEs. These approaches use local context textual
features of the mention and compare them to the
textual features of NE candidate documents in the
KB, and link to the most similar. The first ap-
proach in this line was Bunescu and Pasca (2006),
who measure similarity between the textual con-
text of the NE mention and the Wikipedia cate-
gories of the candidate. More similarity features
were added by Cucerzan (2007) who realized that
topical coherence between a candidate entity and
other entities in the context will improve NED ac-
curacy and by Milne and Witten (2008) who built
on Cucerzan?s work. Han and Sun (2011) combine
different forms of disambiguation knowledge us-
ing evidence from mention-entity associations and
entity popularity in the KB, and context similarity.
The second line of approach is collective named
entity disambiguation (CNED), where all men-
tions of entities in the document are disambiguated
jointly. These approaches try to model the interde-
pendence between the different candidate entities
for different NE mentions in the query document,
and reformulate the problem of NED as a global
optimization problem whose aim is to find the best
set of entities. As this new formulation is NP-
hard, many approximations have been proposed.
Alhelbawy and Gaizauskas (2013) proposed a se-
quence dependency model using HMMs to model
NE interdependency. Another approximation uses
a mixture of local and global features to train the
coefficients of a linear ranking SVM to rank dif-
ferent NE candidates (Ratinov et al, 2011). Shi-
rakawa et al (2011) cluster related textual men-
tions and assign a concept to each cluster using
a probabilistic taxonomy. The concept associated
with a mention is used in selecting the correct en-
tity from the Freebase KB.
Graph models are widely used in collective ap-
proaches
1
. All these approaches model NE in-
terdependencies, while different methods may be
used for disambiguation. Han (2011) uses local
dependency between NE mention and the can-
didate entity, and semantic relatedness between
candidate entities to construct a referent graph,
proposing a collective inference algorithm to in-
fer the correct reference node in the graph. Hoffert
(2011) poses the problem as one of finding a dense
sub-graph, which is infeasible in a huge graph. So,
an algorithm originally used to find strongly inter-
connected, size-limited groups in social media is
adopted to prune the graph, and then a greedy al-
gorithm is used to find the densest graph.
Our proposed model uses the Page-Rank (PR)
algorithm (Page et al, 1999), which to our knowl-
edge has not previously been applied to NED.
Xing and Ghorbani (2004) adopted PR to consider
the weights of links and the nodes? importance. PR
and Personalized PR algorithms have been used
successfully in WSD (Sinha and Mihalcea, 2007;
Agirre and Soroa, 2009).
3 Solution Graph
In this section we discuss the construction of
a graph representation that we call the solution
1
Graph models are also widely used in Word Sense Dis-
ambiguation (WSD), which has lots of similarities to NED
(Guti?errez et al, 2011; Guti?errez et al, 2012).
76
graph. The input is a document containing pre-
tagged NE textual mentions. The solution graph
is an undirected graph G = (V,D) where V is the
node set of all possible NE candidates for differ-
ent textual mentions in the input document and D
is the set of edges between nodes. Edges are not
drawn between different nodes for the same men-
tion. They are drawn between two entities when
there is a relation between them, as described be-
low. Each candidate has associated with it an ini-
tial confidence score, also detailed below.
Assume the input document D has a set of
mentions M = {m
1
,m
2
,m
3
, ...,m
k
}. For each
m
i
? M , we rank each candidate entity, where
the list of candidates for a mention m
i
is E
i
=
{e
i,1
, e
i,2
, ..., e
i,j
}. The graph nodes are formu-
lated as a set V = {(m
i
, e
i,j
) | ?e
i,j
? E
i
, ?m
i
?
M}. Nodes are represented as ordered pairs of
textual mentions and candidate entities, since the
same entity may be found multiple times as a can-
didate for different textual mentions and each oc-
currence must be evaluated independently.
3.1 NE Candidate Generation
The first step in constructing a solution graph is to
find all possible candidates for each NE mention
in the query document. For each such mention the
KB entry titles are searched to find all entries to
which the mention could refer. This includes en-
tries with titles that fully or partially contain the
query mention and those that could be an acronym
of the query mention. These candidate entries are
paired with their textual mentions in the document
to become nodes in the solution graph.
3.2 Initial Confidence
Initial confidence IConf(e
i,j
) is an independent
feature of the NE candidate regardless of other
candidates in the document. This confidence may
be calculated locally using the local mention con-
text, or globally using, e.g., the Freebase popular-
ity score for the KB entry (Bollacker et al, 2008).
Local NE Candidate Confidence: The local
confidence is computed by a similarity measure
between the NE mention in the query document
and the KB entry of the candidate entity. We pro-
pose four different measures to be used in the dis-
ambiguation phase.
cos: The cosine similarity between the named en-
tity textual mention and the KB entry title.
jwSim: While the cosine similarity between a tex-
tual mention in the document and the candidate
NE title in the KB is widely used in NED, this
similarity is a misleading feature. For example,
the textual mention ?Essex? may refer to either
of the following candidates ?Essex County Cricket
Club? or ?Danbury, Essex?, both of which are re-
turned by the candidate generation process. The
cosine similarity between ?Essex? and ?Danbury,
Essex? is higher than that between ?Essex? and
?Essex County Cricket Club?, which is not helpful
in the NED setting. We adopted a new mention-
candidate similarity function, jwSim, which uses
Jaro-Winkler similarity as a first estimate of the
initial confidence value for each candidate. This
function considers all terms found in the candidate
entity KB entry title, but not in the textual mention
as disambiguation terms. The percentage of dis-
ambiguation terms found in the query document is
used to boost in the initial jwSim value, in addi-
tion to an acronym check (whether the NE textual
mention could be an acronym for a specific can-
didate entity title). Experiments show that jwSim
performs much better than cos.
ctxt: The cosine similarity between the sentence
containing the NE mention in the query document
and the textual description of the candidate NE in
the KB (we use the first section of the Wikipedia
article as the candidate entity description).
Global NE Candidate Confidence: Global
confidence is a measure of the global importance
of the candidate entity. Entity popularity has been
used successfully as a discriminative feature for
NED (Nebhi, 2013). Freebase provides an API
to get an entity?s popularity score (FB), which is
computed during Freebase indexing. This score is
a function of the entity?s inbound and outbound
link counts in Freebase and Wikipedia
2
. The initial
confidence is not normalized across all NEs be-
cause each score is calculated independently. Ini-
tial confidence scores of all candidates for a single
NE mention are normalized to sum to 1.
3.3 Entity Coherence
Entity coherence refers to the real world related-
ness of different entities which are candidate inter-
pretations of different textual mentions in the doc-
ument. It is not based on context, so it is always
the same regardless of the query document. Co-
herence is represented as an edge between nodes
in the solution graph. We used two measures for
coherence, described as follows:
2
https://developers.google.com/freebase/v1/search
77
Ref: Uses the Wikipedia documents for both en-
tity candidates to check if either document has a
link to the other. This relation is directed, but we
assume an inverse relation also exists; so this rela-
tion is represented as undirected.
Ref(e
i
, e
j
) =
{
1, if e
i
or e
j
refers to the other
0, otherwise
(1)
JProb: An estimate of the probability of both
entities appearing in the same sentence. Wikipedia
documents are used to estimate this probability, as
shown in (2), where S(e) is the set of all sentences
that contain the entity e and S the set of sentences
containing any entity references.
JProb(e
i
, e
j
) =
|S(e
i
)
?
S(e
j
)|
|S|
(2)
4 Disambiguation
The solution graph contains all possible candi-
dates for each NE mention in the document. Each
candidate has an initial confidence, with some
connected by association relations. The disam-
biguation phase ranks all nodes in the solution
graph and selects the best from the candidate list
for each NE textual mention. The process of dis-
ambiguation consists of three steps. The first step
is initial graph ranking, where all nodes are ranked
according to the link structure. The second step is
to re-rank the nodes by combining the graph rank
with the initial confidence. The highest rank is not
always correct, so in the third step a selection al-
gorithm is used to choose the best candidate.
Graph Ranking: The links between different
candidates in the solution graph represent real
world relations. These relations may be used to re-
liably boost relevant candidates. All nodes in the
graph are ranked according to these relations using
PR. Initial confidence is used as an initial rank for
the graph nodes, while entities? coherence mea-
sures are used as link weights which play a role in
distributing a node?s rank over its outgoing nodes.
Candidate Re-ranking: A problem with Page-
Rank for our purposes is the dissipation of initial
node weight (confidence) over all outgoing nodes.
The final rank of a node is based solely on the im-
portance of incoming nodes and the initial confi-
dence play no further role. In our case this is not
appropriate, so the final rank for each mention is
determined after graph ranking, by combining the
graph rank with the initial confidence.
Let us refer to the graph rank of a candidate as
PR(e
i
). Two combination schemes are used:
R
s
(e
i,j
) = IConf(e
i,j
) + PR(e
i,j
) (3)
R
m
(e
i,j
) = IConf(e
i,j
)? PR(e
i,j
) (4)
Named Entity Selection: The simplest ap-
proach is to select the highest ranked entity in the
list for each mention m
i
according to equation
5, where R could refer to R
m
or R
s
. However,
we found that a dynamic choice between the re-
ranking schemes, based on the difference between
the top two candidates, as described in algorithm
1 and indicated by e
g
,works best. The underlying
intuition of this algorithm is that a greater differ-
ence between the top ranks reflects more confident
discrimination between candidates. So, the two
combination schemes assign different ranks to the
candidates and the algorithm selects the scheme
which appears more discriminative.
e?
i
= argmax
e
i,j
R(e
i,j
) (5)
Data: Two lists, R1 and R2, of candidates E
i
, where R1
is ranked using R
s
, and R2 is ranked using R
m
Result: One NE e
g
i
Sort R1 and R2 in descending order;
R1diff = R1[0]-R1[1];
R2diff = R2[0]-R2[1];
if R1diff > R2diff then
return highest rank scored entity of R1
else
return highest rank scored entity of R2
end
Algorithm 1: Selection Algorithm
5 Experiments and Results
We used AIDA dataset
3
, which is based on the
CoNLL 2003 data for NER tagging. All mentions
are manually disambiguated against Wikipedia
(Hoffart et al, 2011). This dataset contains 1393
documents and 34,965 annotated mentions. We
only consider NE mentions with an entry in the
Wikipedia KB, ignoring the 20% of query men-
tions (7136) without a link to the KB, as Hoffart
did. Micro-averaged and macro-averaged accu-
racy are used for evaluation. In this context micro-
averaged accuracy corresponds to the propor-
tion of textual mentions correctly disambiguated
while macro-averaged accuracy corresponds to the
proportion of textual mentions correctly disam-
biguated per entity, averaged over all entities.
5.1 Results
Initially, we evaluated the performance of two
baselines. One is a setup where a ranking based
solely on different initial confidence scores is used
3
http://www.mpi-inf.mpg.de/yago-naga/aida/
78
IConf PR
C
PR
I
PR
IC
Cucerzan Kulkarni Hoffart Shirakawa Alhelbawy
A
macro
78.09 80.98 84.19 82.80 43.74 76.74 81.91 83.02 74.18
A
micro
80.55 83.59 87.59 86.10 51.03 72.87 81.82 82.29 78.49
Table 1: Results comparison between Proposed Approach and State-of-the-art
PR e
g
IConf A
micro
A
macro
A
micro
A
macro
cos 70.6 60.83 78.41 72.35
jwSim 70.61 60.94 83.16 78.28
ctxt 70.61 60.83 75.45 65.22
freebase 71.78 81.07 87.59 84.19
Table 2: Results using initial confidence (PR
I
)
PR e
g
Edge Weight A
micro
A
macro
A
micro
A
macro
Jprob 66.52 55.83 83.31 80.38
Ref 67.48 59.76 81.80 78.53
prob+ refs 72.69 65.71 83.46 80.69
Table 3: Results using weighted edges (PR
C
)
for candidate selection, i.e. without using PR. In
this setup a ranking based on Freebase popularity
does best, with micro- and macro-averaged accu-
racy scores of 80.55% and 78.09% respectively.
This is a high baseline, close to the state-of-the-
art. Our second baseline is the basic PR algorithm,
where weights of nodes and edges are uniform (i.e.
initial node and edge weights set to 1, edges be-
ing created wherever REF or JProb are not zero).
Micro and macro accuracy scores of 70.60% and
60.91% were obtained with this baseline.
To study graph ranking using PR, and the con-
tributions of the initial confidence and entity co-
herence, experiments were carried out using PR in
different modes and with different selection tech-
niques. In the first experiment, referred to as PR
I
,
initial confidence is used as an initial node rank for
PR and edge weights are uniform, edges, as in the
PR baseline, being created wherever REF or JProb
are not zero. Table 2 shows the results both before
re-ranking, i.e. using only the PR score for rank-
ing, and after re-ranking using the dynamic selec-
tion scheme e
g
. When comparing these results to
the PR baseline we notice a slight positive effect
when using the initial confidence as an initial rank
instead of uniform ranking. The major improve-
ment comes from re-ranking nodes by combining
initial confidence with PR score.
In our second experiment, PR
C
, entity coher-
ence features are tested by setting the edge weights
to the coherence score and using uniform ini-
tial node weights. We compared JProb and Ref
e
g
(jwSim) e
g
(freebase)
Edge Weight A
micro
A
macro
A
micro
A
macro
Jprob 82.56 76.16 86.29 82.77
Ref 78.61 71.12 83.16 80.01
Jprob+Ref 81.97 75.63 86.10 82.80
Table 4: Results using IConf and weighted edges PR
IC
edge weighting approaches, where for each ap-
proach edges were created only where the coher-
ence score according to the approach was non-
zero. We also investigated a variant, called JProb +
Ref, in which the Ref edge weights are normalized
to sum to 1 over the whole graph and then added
to the JProb edge weights (here edges result wher-
ever JProb or Ref scores are non-zero). Results in
Table 3 show the JProb feature seems to be more
discriminative than the Ref feature but the com-
bined Jprob + Ref feature performs better than
each separately, just outperforming the baseline.
We used the best initial confidence score (Free-
base) for re-ranking. Again, combining the initial
confidence with the PR score improves the results.
Finally, Table 4 shows the accuracy when using
different combinations of initial confidence and
entity coherence scores just in the case when re-
ranking is applied. Here the jprob + refs com-
bination does not add any value over jprob alone.
Interestingly using initial confidence with differ-
entially weighted edges does not show any ben-
efit over using initial confidence and uniformly
weighted edges (Table 2).
To compare our results with the state-of-the-art,
we report Hoffart et al?s (2011) results as they re-
implemented two other systems and also ran them
over the AIDA dataset. We also compare with Al-
helbawy and Gaizauskas (2013) and Shirakawa et
al. (2011) who carried out their experiments using
the same dataset. Table 1 presents a comparison
between our approach and the state-of-the-art and
shows our approach exceeds the state-of-the-art.
Futhermore our approach is very simple and direct
to apply, unlike Hoffart et al?s and Shirakawa et
al.?s which are considerably more complex. Also,
our approach does not need any kind of training,
as does the Alhelbawy approach.
6 Conclusion
Our results show that Page-Rank in conjunction
with re-ranking by initial confidence score can be
used as an effective approach to collectively dis-
ambiguate named entity textual mentions in a doc-
ument. Our proposed features are very simple and
easy to extract, and work well when employed in
PR. In future work we plan to explore enriching
the edges between nodes by incorporating seman-
tic relations extracted from an ontology.
79
References
Eneko Agirre and Aitor Soroa. 2009. Personaliz-
ing pagerank for word sense disambiguation. In
Proceedings of the 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 33?41. Association for Computa-
tional Linguistics.
Ayman Alhelbawy and Robert Gaizauskas. 2013.
Named entity disambiguation using hmms. In
Web Intelligence (WI) and Intelligent Agent Tech-
nologies (IAT), 2013 IEEE/WIC/ACM International
Joint Conferences on, volume 3, pages 159?162.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008
ACM SIGMOD International Conference on Man-
agement of Data, SIGMOD ?08, pages 1247?1250,
New York, NY, USA. ACM.
Razvan C. Bunescu and Marius Pasca. 2006. Us-
ing encyclopedic knowledge for named entity dis-
ambiguation. In EACL. The Association for Com-
puter Linguistics.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In Proceed-
ings of EMNLP-CoNLL, volume 6, pages 708?716.
Yoan Guti?errez, Sonia V?azquez, and Andr?es Montoyo.
2011. Word sense disambiguation: a graph-based
approach using n-cliques partitioning technique. In
Natural Language Processing and Information Sys-
tems, pages 112?124. Springer.
Yoan Guti?errez, Sonia V?azquez, and Andr?es Montoyo.
2012. A graph-based approach to wsd using rele-
vant semantic trees and n-cliques model. In Compu-
tational Linguistics and Intelligent Text Processing,
pages 225?237. Springer.
Xianpei Han and Le Sun. 2011. A generative entity-
mention model for linking entities with knowledge
base. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1, pages 945?
954. Association for Computational Linguistics.
Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective
entity linking in web text: a graph-based method. In
Proceedings of the 34th international ACM SIGIR
conference on Research and development in Infor-
mation Retrieval, pages 765?774. ACM.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen F?urstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named
entities in text. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 782?792. Association for Computational
Linguistics.
Heng Ji and Ralph Grishman. 2011. Knowledge
base population: successful approaches and chal-
lenges. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies-Volume 1, pages
1148?1158. Association for Computational Linguis-
tics.
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the tac 2009 knowledge base population track. In
Text Analysis Conference (TAC), volume 17, pages
111?113.
David Milne and Ian H Witten. 2008. Learning to link
with wikipedia. In Proceeding of the 17th ACM con-
ference on Information and knowledge management,
pages 509?518. ACM.
David Nadeau and Satoshi Sekine. 2007. A sur-
vey of named entity recognition and classification.
Lingvisticae Investigationes, 30(1):3?26.
Kamel Nebhi. 2013. Named entity disambiguation
using freebase and syntactic parsing. In CEUR-
WS.org, editor, Proceedings of the First Interna-
tional Workshop on Linked Data for Information Ex-
traction (LD4IE 2013) co-located with the 12th In-
ternational Semantic Web Conference (ISWC 2013).
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web. Technical Report
1999-66, Stanford InfoLab, November. Previous
number = SIDL-WP-1999-0120.
Lev Ratinov, Dan Roth, Doug Downey, and Mike
Anderson. 2011. Local and global algorithms
for disambiguation to wikipedia. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 1375?1384. Associ-
ation for Computational Linguistics.
Masumi Shirakawa, Haixun Wang, Yangqiu Song,
Zhongyuan Wang, Kotaro Nakayama, Takahiro
Hara, and Shojiro Nishio. 2011. Entity disam-
biguation based on a. Technical report, Technical
report, Technical Report MSR-TR-2011-125, Mi-
crosoft Research.
Ravi Sinha and Rada Mihalcea. 2007. Unsupervised
graph-basedword sense disambiguation using mea-
sures of word semantic similarity. In Semantic Com-
puting, 2007. ICSC 2007. International Conference
on, pages 363?369. IEEE.
Wenpu Xing and Ali Ghorbani. 2004. Weighted pager-
ank algorithm. In Communication Networks and
Services Research, 2004. Proceedings. Second An-
nual Conference on, pages 305?314. IEEE.
80
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 337?340,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
USFD2: Annotating Temporal Expresions and TLINKs for TempEval-2
Leon Derczynski
Dept of Computer Science
University of Sheffield
Regent Court
211 Portobello
Sheffield S1 4DP, UK
leon@dcs.shef.ac.uk
Robert Gaizauskas
Dept of Computer Science
University of Sheffield
Regent Court
211 Portobello
Sheffield S1 4DP, UK
robertg@dcs.shef.ac.uk
Abstract
We describe the University of Sheffield
system used in the TempEval-2 challenge,
USFD2. The challenge requires the au-
tomatic identification of temporal entities
and relations in text.
USFD2 identifies and anchors temporal
expressions, and also attempts two of the
four temporal relation assignment tasks.
A rule-based system picks out and an-
chors temporal expressions, and a max-
imum entropy classifier assigns temporal
link labels, based on features that include
descriptions of associated temporal signal
words. USFD2 identified temporal expres-
sions successfully, and correctly classified
their type in 90% of cases. Determin-
ing the relation between an event and time
expression in the same sentence was per-
formed at 63% accuracy, the second high-
est score in this part of the challenge.
1 Introduction
The TempEval-2 (Pustejovsky and Verhagen,
2009) challenge proposes six tasks. Our system
tackles three of these: task A ? identifying time ex-
pressions, assigning TIMEX3 attribute values, and
anchoring them; task C ? determining the tempo-
ral relation between an event and time in the same
sentence; and task E ? determining the temporal
relation between two main events in consecutive
sentences. For our participation in the task, we
decided to employ both rule- and ML-classifier-
based approaches. Temporal expressions are dealt
with by sets of rules and regular expressions, and
relation labelling performed by NLTK?s1 maxi-
mum entropy classifier with rule-based processing
applied during feature generation. The features
(described in full in Section 2) included attributes
1See http://www.nltk.org/ .
from the TempEval-2 training data annotation,
augmented by features that can be directly derived
from the annotated texts. There are two main aims
of this work: (1) to create a rule-based tempo-
ral expression annotator that includes knowledge
from work published since GUTime (Mani and
Wilson, 2000) and measure its performance, and
(2) to measure the performance of a classifier that
includes features based on temporal signals.
Our entry to the challenge, USFD2, is a succes-
sor to USFD (Hepple et al, 2007). In the rest of
this paper, we will describe how USFD2 is con-
structed (Section 2), and then go on to discuss
its overall performance and the impact of some
internal parameters on specific TempEval tasks.
Regarding classifiers, we found that despite us-
ing identical feature sets across relation classifi-
cation tasks, performance varied significantly. We
also found that USFD2 performance trends with
TempEval-2 did not match those seen when clas-
sifiers were trained on other data while perform-
ing similar tasks. The paper closes with comments
about future work.
2 System Description
The TempEval-2 training and test sets are parti-
tioned into data for entity recognition and descrip-
tion, and data for temporal relation classification.
We will first discuss our approach for temporal ex-
pression recognition, description and anchoring,
and then discuss our approach to two of the re-
lation labelling tasks.
2.1 Identifying, describing and anchoring
temporal expressions
Task A of TempEval-2 requires the identification
of temporal expressions (or timexes) by defining
a start and end boundary for each expression, and
assigning an ID to it. After this, systems should
attempt to describe the temporal expression, de-
termining its type and value (described below).
337
Our timex recogniser works by building a set of
n-grams from the data to be annotated (1 ? n ?
5), and comparing each n-gram against a hand-
crafted set of regular expressions. This approach
has been shown to achieve high precision, with re-
call increasing in proportion to ruleset size (Han
et al, 2006; Mani and Wilson, 2000; Ahn et al,
2005). The recogniser chooses the largest possible
sequence of words that could be a single temporal
expression, discarding any sub-parts that indepen-
dently match any of our set of regular expressions.
The result is a set of boundary-pairs that describe
temporal expression locations within documents.
This part of the system achieved 0.84 precision
and 0.79 recall, for a balanced f1-measure of 0.82.
The next part of the task is to assign a type
to each temporal expression. These can be one
of TIME, DATE, DURATION, or SET. USFD2
only distinguishes between DATE and DURATION
timexes. If the words for or during occur in the
three words before the timex, the timex ends with
an s (such as in seven years), or the timex is a bi-
gram whose first token is a (e.g. in a month), then
the timex is deemed to be of type DURATION; oth-
erwise it is a DATE. These three rules for deter-
mining type were created based on observation of
output over the test data, and are correct 90% of
the time with the evaluation data.
The final part of task A is to provide a value
for the timex. As we only annotate DATEs
and DURATIONs, these will be either a fixed
calendrical reference in the format YYYY-MM-
DD, or a duration in according to the TIMEX2
standard (Ferro et al, 2005). Timex strings of
today or now were assigned the special value
PRESENT REF, which assumes that today is be-
ing used in a literal and not figurative manner, an
assumption which holds around 90% of the time
in newswire text (Ahn et al, 2005) such as that
provided for TempEval-2. In an effort to calcu-
late a temporal distance from the document cre-
ation time (DCT), USFD2 then checks to see if
numeric words (e.g. one, seven hundred) are in
the timex, as well as words like last or next which
determine temporal offset direction. This distance
figure supplies either the second parameter to a
DURATION value, or helps calculate DCT offset.
Strings that describe an imprecise amount, such as
few, are represented in duration values with an X,
as per the TIMEX2 standard. We next search the
timex for temporal unit strings (e.g. quarter, day).
Table 1: Features used by USFD2 to train a tem-
poral relation classifier.
Feature Type
For events
Tense String
Aspect String
Polarity pos or neg
Modality String
For timexes
Type Timex type
Value String
Describing signals
Signal text String
Signal hint Relation type
Arg 1 before signal? Boolean
Signal before Arg 2? Boolean
For every relation
Arguments are same tense Boolean
Arguments are same aspect Boolean
Arg 1 before Arg 2? Boolean
For every interval
Token number in sentence / 5 Integer
Text annotated String
Interval type event or timex
This helps build either a duration length or an off-
set. If we are anchoring a date, the offset is applied
to DCT, and date granularity adjusted according to
the coarsest temporal primitive present ? for ex-
ample, if DCT is 1997-06-12 and our timex is six
months ago, a value of 1997-01 is assigned, as it is
unlikely that the temporal expression refers to the
day precisely six months ago, unless followed by
the word today.
Where weekday names are found, we used
Baldwin?s 7-day window (Baldwin, 2002) to an-
chor these to a calendrical timeline. This tech-
nique has been found to be accurate over 94%
of the time with newswire text (Mazur and Dale,
2008). Where dates are found that do not specify
a year or a clear temporal direction marker (e.g.,
April 17 vs. last July), our algorithm counts the
number of days between DCT and the next oc-
currence of that date. If this is over a limit f ,
then the date is assumed to be last year. This is
a very general rule and does not take into account
the tendency of very-precisely-described dates to
be closer to DCT, and far off dates to be loosely
specified. An f of 14 days gives the highest per-
formance based on the TempEval-2 training data.
Anchoring dates / specifying duration lengths
was the most complex part of task A and our na??ve
rule set was correct only 17% of the time.
338
Table 2: A sample of signals and the TempEval-2
temporal relation they suggest.
Signal phrase Suggested relation
previous AFTER
ahead of BEFORE
so far OVERLAP
thereafter BEFORE
in anticipation of BEFORE
follows AFTER
since then BEFORE
soon after AFTER
as of OVERLAP-OR-AFTER
throughout OVERLAP
2.2 Labelling temporal relations
Our approach for labelling temporal relations (or
TLINKs) is based on NLTK?s maximum en-
tropy classifier, using the feature sets initially pro-
posed in Mani et al (2006). Features that de-
scribe temporal signals have been shown to give
a 30% performance boost in TLINKs that em-
ploy a signal (Derczynski and Gaizauskas, 2010).
Thus, the features in Mani et al (2006) are aug-
mented with those used to describe signals de-
tailed in Derczynski and Gaizauskas (2010), with
some slight changes. Firstly, as there are no spe-
cific TLINK/signal associations in the TempEval-
2 data (unlike TimeBank (Pustejovsky et al,
2003)), USFD2 needs to perform signal identifi-
cation and then associate signals with a temporal
relation between two events or timexes. Secondly,
a look-up list is used to provide TLINK label hints
based on a signal word. A list of features em-
ployed by USFD2 is in Table 1.
We used a simplified version of the approach
in Cheng et al (2007) to identify signal words.
This involved the creation of a list of signal
phrases that occur in TimeBank with a frequency
of 2 or more, and associating a signal from this list
with a temporal entity if it is in the same sentence
and clause. The textually nearest signal is chosen
in the case of conflict.
As this list of signal phrases only contained 42
entries, we also decided to define a ?most-likely?
temporal relation for each signal. This was done
by imagining a short sentence of the form event1
? signal ? event2, and describing the type of re-
lation between event 1 and event 2. An excerpt
from these entries is shown in Table 2. The hint
from this table was included as a feature. Deter-
mining whether or not to invert the suggested rela-
tion type based on word order was left to the clas-
sifier, which is already provided with word order
features. It would be possible to build these sug-
gestions from data such as TimeBank, but a num-
ber of problems stand in the way; the TimeML and
TempEval-2 relation types are not identical, word
order often affects the actual relationship type sug-
gested by a signal (e.g. compare He ran home
before he showered and Before he ran home, he
showered), and noise in mined data is a problem
with the low corpus occurrence frequency of most
signals.
This approach was used for both the intra-
sentence timex/event TLINK labelling task and
also the task of labelling relations between main
events in adjacent sentences.
3 Discussion
USFD2?s rule-based element for timex identifica-
tion and description performs well, even achieving
above-average recall despite a much smaller rule
set than comparable and more complex systems.
However, the temporal anchoring component per-
forms less strongly. The ?all-or-nothing? metric
employed for evaluating the annotation of timex
values gives non-strict matches a zero score (e.g.
if the expected answer is 1990-05-14, no reward is
given for 1990-05) even if values are close, which
many were.
In previous approaches that used a maxi-
mum entropy classifier and comparable feature
set (Mani et al, 2006; Derczynski and Gaizauskas,
2010), the accuracy of event-event relation classi-
fication was higher than that of event-timex clas-
sification. Contrary to this, USFD2?s event-event
classification of relations between main events
of successive sentences (Task E) was less accu-
rate than the classification of event-timex rela-
tions between events and timexes in the same sen-
tence (Task C). Accuracy in Task C was good
(63%), despite the lack of explicit signal/TLINK
associations and the absence of a sophisticated
signal recognition and association mechanism.
This is higher than USFD2?s accuracy in Task
E (45%) though the latter is a harder task, as
most TempEval-2 systems performed significantly
worse at this task than event/timex relation classi-
fication.
Signal information was not relied on by many
TempEval 2007 systems (Min et al (2007) dis-
339
cusses signals to some extent but the system de-
scribed only includes a single feature ? the sig-
nal text), and certainly no processing of this data
was performed for that challenge. USFD2 begins
to leverage this information, and gives very com-
petitive performance at event/timex classification.
In this case, the signals provided an increase from
61.5% to 63.1% predictive accuracy in task C. The
small size of the improvement might be due to the
crude and unevaluated signal identification and as-
sociation system that we implemented.
The performance of classifier based approaches
to temporal link labelling seems to be levelling
off ? the 60%-70% relation labelling accuracy of
work such as Mani et al (2006) has not been
greatly exceeded. This performance level is still
the peak of the current generation of systems. Re-
cent improvements, while employing novel ap-
proaches to the task that rely on constraints be-
tween temporal link types or on complex linguistic
information beyond that describable by TimeML
attributes, still yield marginal improvements (e.g.
Yoshikawa et al (2009)). It seems that to break
through this performance ?wall?, we need to con-
tinue to innovate with and discuss temporal re-
lation labelling, using information and knowl-
edge from many sources to build practical high-
performance systems.
4 Conclusion
In this paper, we have presented USFD2, a novel
system that annotates temporal expressions and
temporal links in text. The system relies on
new hand-crafted rules, existing rule sets, machine
learning and temporal signal information to make
its decisions. Although some of the TempEval-2
tasks are difficult, USFD2 manages to create good
and useful annotations of temporal information.
USFD2 is available via Google Code2.
Acknowledgments
Both authors are grateful for the efforts of the
TempEval-2 team and appreciate their hard work.
The first author would like to acknowledge the
UK Engineering and Physical Science Research
Council for support in the form of a doctoral stu-
dentship.
2See http://code.google.com/p/usfd2/ .
References
D. Ahn, S.F. Adafre, and MD Rijke. 2005. Towards
task-based temporal extraction and recognition. In
Dagstuhl Seminar Proceedings, volume 5151.
J.A. Baldwin. 2002. Learning temporal annotation of
French news. Ph.D. thesis, Georgetown University.
Y. Cheng, M. Asahara, and Y. Matsumoto. 2007.
Temporal relation identification using dependency
parsed tree. In Proceedings of the 4th International
Workshop on Semantic Evaluations, pages 245?248.
L. Derczynski and R. Gaizauskas. 2010. Using sig-
nals to improve automatic classification of temporal
relations. In Proceedings of the ESSLLI StuS. Sub-
mitted.
L. Ferro, L. Gerber, I. Mani, B. Sundheim, and G. Wil-
son. 2005. TIDES 2005 standard for the annotation
of temporal expressions. Technical report, MITRE.
B. Han, D. Gates, and L. Levin. 2006. From language
to time: A temporal expression anchorer. In Tem-
poral Representation and Reasoning (TIME), pages
196?203.
M. Hepple, A. Setzer, and R. Gaizauskas. 2007.
USFD: preliminary exploration of features and clas-
sifiers for the TempEval-2007 tasks. In Proceedings
of SemEval-2007, pages 438?441.
I. Mani and G. Wilson. 2000. Robust temporal pro-
cessing of news. In Proceedings of the 38th Annual
Meeting on ACL, pages 69?76. ACL.
I. Mani, M. Verhagen, B. Wellner, C.M. Lee, and
J. Pustejovsky. 2006. Machine learning of tem-
poral relations. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics,
page 760. ACL.
P. Mazur and R. Dale. 2008. Whats the date? High
accuracy interpretation of weekday. In 22nd Inter-
national Conference on Computational Linguistics
(Coling 2008), Manchester, UK, pages 553?560.
C. Min, M. Srikanth, and A. Fowler. 2007. LCC-TE:
a hybrid approach to temporal relation identification
in news text. In Proceedings of the 4th International
Workshop on Semantic Evaluations, pages 219?222.
J. Pustejovsky and M. Verhagen. 2009. SemEval-2010
task 13: evaluating events, time expressions, and
temporal relations (TempEval-2). In Proceedings of
the Workshop on Semantic Evaluations, pages 112?
116. ACL.
J. Pustejovsky, P. Hanks, R. Sauri, A. See,
R. Gaizauskas, A. Setzer, D. Radev, D. Day,
L. Ferro, et al 2003. The Timebank Corpus. In
Corpus Linguistics, volume 2003, page 40.
K. Yoshikawa, S. Riedel, M. Asahara, and Y. Mat-
sumoto. 2009. Jointly identifying temporal rela-
tions with markov logic. In IJCNLP: Proceedings
of 47th Annual Meeting of the ACL, pages 405?413.
340
Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, page 1
Manchester, August 2008
Generating Image Captions using Topic Focused Multi-document
Summarization
Robert Gaizauskas
Natural Language Processing Group
Department of Computer Science, University of Sheffield
Regent Court, 211 Portobello, Sheffield, S1 4DP, UK
R.Gaizauskas@sheffield.ac.uk
In the near future digital cameras will come
standardly equipped with GPS and compass and
will automatically add global position and direc-
tion information to the metadata of every picture
taken. Can we use this information, together with
information from geographical information sys-
tems and the Web more generally, to caption im-
ages automatically?
This challenge is being pursued in the TRIPOD
project (http://tripod.shef.ac.uk/) and in this talk
I will address one of the subchallenges this topic
raises: given a set of toponyms automatically gen-
erated from geo-data associated with an image, can
we use these toponyms to retrieve documents from
the Web and to generate an appropriate caption for
the image?
We begin assuming the toponyms name the prin-
cipal objects or scene contents in the image. Using
web resources (e.g. Wikipedia) we attempt to de-
termine the types of these things ? is this a picture
of church? a mountain? a city? We have con-
structed a taxonomy of such image content types
using on-line collections of captioned images and
for each type in the taxonomy we have constructed
several collections of texts describing that type.
For example, we have a collection of captions de-
scribing churches and a collection of Wiki pages
describing churches. The intuition here is that
these collections are examples of, e.g. the sorts
of things people say in captions or in descriptions
of churches. These collections can then be used to
derive models of objects or scene types which in
turn can be used to bias or focus multi-document
summaries of new images of things of the same
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
type.
In the talk I report results of work we have
carried out to explore the hypothesis underlying
this approach, namely that brief multi-document
summaries generated as image captions by using
models of object/scene types to bias or focus con-
tent selection will be superior to generic multi-
document summaries generated for this purpose.
I describe how we have constructed an image con-
tent taxonomy, how we have derived text collec-
tions for object/scene types, how we have derived
object/scene type models from these collections
and how these have been used in multi-document
summarization. I also discuss the issue of how to
evaluate the resulting captions and present prelim-
inary results from one sort of evaluation.
1
Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 41?48
Manchester, August 2008
Evaluating automatically generated user-focused multi-document
summaries for geo-referenced images
Ahmet Aker
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, UK
A.Aker@dcs.shef.ac.uk
Robert Gaizauskas
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, UK
R.Gaizauskas@dcs.shef.ac.uk
Abstract
This paper reports an initial study that aims
to assess the viability of a state-of-the-art
multi-document summarizer for automatic
captioning of geo-referenced images. The
automatic captioning procedure requires
summarizing multiple web documents that
contain information related to images? lo-
cation. We use SUMMA (Saggion and
Gaizauskas, 2005) to generate generic and
query-based multi-document summaries
and evaluate them using ROUGE evalua-
tion metrics (Lin, 2004) relative to human
generated summaries. Results show that,
even though query-based summaries per-
form better than generic ones, they are still
not selecting the information that human
participants do. In particular, the areas
of interest that human summaries display
(history, travel information, etc.) are not
contained in the query-based summaries.
For our future work in automatic image
captioning this result suggests that devel-
oping the query-based summarizer further
and biasing it to account for user-specific
requirements will prove worthwhile.
1 Introduction
Retrieving textual information related to a loca-
tion shown in an image has many potential appli-
cations. It could help users gain quick access to
the information they seek about a place of inter-
est just by taking its picture. Such textual informa-
tion could also, for instance, be used by a journalist
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
who is planning to write an article about a building,
or by a tourist who seeks further interesting places
to visit nearby. In this paper we aim to generate
such textual information automatically by utilizing
multi-document summarization techniques, where
documents to be summarized are web documents
that contain information related to the image con-
tent. We focus on geo-referenced images, i.e. im-
ages tagged with coordinates (latitude and longi-
tude) and compass information, that show things
with fixed locations (e.g. buildings, mountains,
etc.).
Attempts towards automatic generation of
image-related textual information or captions have
been previously reported. Deschacht and Moens
(2007) and Mori et al (2000) generate image
captions automatically by analyzing image-related
text from the immediate context of the image,
i.e. existing image captions, surrounding text in
HTML documents, text contained in the image,
etc. The authors identify named entities and other
noun phrases in the image-related text and assign
these to the image as captions. Other approaches
create image captions by taking into considera-
tion image features as well as image-related text
(Westerveld, 2000; Barnard et al, 2003; Pan et
al., 2004). These approaches can address all kinds
of images, but focus mostly on images of people.
They analyze only the immediate textual context of
the image on the web and are concerned with de-
scribing what is in the image only. Consequently,
background information about the objects in the
image is not provided. Our aim, however, is to
have captions that inform users? specific interests
about a location, which clearly includes more than
just image content description. Multi-document
summarization techniques offer the possibility to
include image-related information from multiple
41
documents, however, the challenge lies in being
able to summarize unrestricted web documents.
Various multi-document summarization tools
have been developed: SUMMA (Saggion and
Gaizauskas, 2005), MEAD (Radev et al, 2004),
CLASSY (Conroy et al, 2005), CATS (Farzin-
der et al, 2005) and the system of Boros et al
(2001), to name just a few. These systems generate
either generic or query-based summaries or both.
Generic summaries address a broad readership
whereas query-based summaries are preferred by
specific groups of people aiming for quick knowl-
edge gain about specific topics (Mani, 2001).
SUMMA and MEAD generate both generic and
query-based multi-document summaries. Boros
et al (2001) create only generic summaries,
while CLASSY and CATS create only query-based
summaries from multiple documents. The perfor-
mance of these tools has been reported for DUC
tasks
1
. As Sekine and Nobata (2003) note, al-
though DUC tasks provide a common evaluation
standard, they are restricted in topic and are some-
what idealized. For our purposes the summarizer
needs to create summaries from unrestricted web
input, for which there are no previous performance
reports.
For this reason we evaluate the performance of
both a generic and a query-based summarizer and
use SUMMA which provides both summarization
modes. We hypothesize that a query-based sum-
marizer will better address the problem of creating
summaries tailored to users? needs. This is because
the query itself may contain important hints as to
what the user is interested in. A generic summa-
rizer generates summaries based on the topics it
observes from the documents and cannot take user
specific input into consideration. Using SUMMA,
we generate both generic and query-based multi-
document summaries of image-related documents
obtained from the web. In an online data collection
procedure we presented a set of images with re-
lated web documents to human subjects and asked
them to select from these documents the infor-
mation that best describes the image. Based on
this user information we created model summaries
against which we evaluated the automatically gen-
erated ones.
Section 2 in this paper describes how image-
related documents were collected from the web.
In section 3 SUMMA is described in detail. In
1
http://www-nlpir.nist.gov/projects/duc/index.html
section 4 we explain how the human image de-
scriptions were collected. Section 5 discusses the
results, and section 6 concludes the paper and out-
lines directions for future work and improvements.
2 Web Document Collection
For web document collection we used geo-
referenced images of locations in London such as
Westminster Abbey, London Eye, etc. The images
were taken with a digital SLR camera with a Geo-
tagger plugged-in to its flash slot. The Geotagger
helped us to identify the location by means of co-
ordinates of the position where the photographer
stands, as well as the direction the camera is point-
ing (compass information). Based on the coordi-
nates and compass information for each image, we
carried out the following steps to collect related
documents from the web:
? identify a set of toponyms (terms that denote
locations or associate names with locations,
e.g. Westminster Abbey) that can be passed to
a search engine as query terms for document
search;
? use a search engine to retrieve HTML docu-
ments to be summarized;
? extract the pure text out of the HTML docu-
ments.
2.1 Toponym Collection
In order to create the web queries a set of to-
ponyms were collected semi-automatically. We
implemented an application (cf. Figure 1) that
suggests a list of toponyms close to the photogra-
pher?s location. The application uses Microsoft?s
MapPoint
2
service which allows users to query
location-related information. For example, a user
can query for tourist attractions (interesting build-
ings, museums, art galleries etc.) close to a loca-
tion that is identified by its address or its coordi-
nates.
Based on the coordinates (latitude and longi-
tude), important toponyms for a particular image
can be queried from the MapPoint database. In
order to facilitate this, MapPoint returns a met-
ric that measures the importance of each toponym.
A value close to zero means that the returned to-
ponym is closer to the specified coordinates than
a toponym with a higher value. For instance for
2
http://www.microsoft.com/mappoint/
42
Figure 1: Image Toponym Collector: Westminster
Abbey, Lat: 51.50024 Lon: -0.128138333: Direc-
tion: 137.1
the image of Westminster Abbey shown in the Im-
age box of Figure 1 the following toponyms are
collected:
Queen Elizabeth II Conf. Centre: 0.059
Parliament Square: 0.067
Westminster Abbey: 0.067
The photographer?s location is shown with a black
dot on the first map in the Maps box of Figure 1.
The application suggests the toponyms shown in
the Suggested Terms list.
Knowing the direction the photographer was
facing helps us to select the correct toponyms from
the list of suggested toponyms. The current Map-
Point implementation does not allow an arrow to
be drawn on the map which would be the best in-
dication of the direction the photographer is facing.
To overcome this problem we create a second map
(cf. Maps box of Figure 1) that shows another dot
moved 50 meters in the compass direction. By fol-
lowing the dot from the first map to the second map
we can determine the direction the photographer is
facing. When the direction is known, it is certain
that the image shows Westminster Abbey and not
the Queen Elizabeth II Conf. Centre or Parliament
Square. The Queen Elizabeth II Conf. Centre is
behind the photographer and Parliament Square is
on the left hand side.
Consequently in this example the toponym
Westminster Abbey is selected manually for the
web search. In order to avoid ambiguities, the
city name and the country name (also generated
by MapPoint) are added manually to the selected
toponyms. Hence, for Westminster Abbey, Lon-
don and United Kingdom are added to the toponym
list. Finally the terms in the toponym list are sim-
ply separated by a boolean AND operator to form
the web query. Then, the query is passed to the
search engine as described in the next section.
2.2 Document Query and Text Extraction
The web queries were passed to the Google Search
engine and the 20 best search results were re-
trieved, from which only 11 were taken for the
summarization process. We ensure that these 20
search results are healthy hyperlinks, i.e. that the
content of the hyperlink is accessible. In addition
to this, multiple hyperlinks belonging to the same
domain are ignored as it is assumed that the con-
tent obtained from the same domain would be sim-
ilar. Each remaining search result is crawled to ob-
tain its content.
The web-crawler downloads only the content of
the document residing under the hyperlink, which
was previously found as a search result, and does
not follow any other hyperlinks within the docu-
ment. The content obtained by the web-crawler
encapsulates an HTML structured document. We
further process this using an HTML parser
3
to se-
lect the pure text, i.e. text consisting of sentences.
The HTML parser removes advertisements,
menu items, tables, java scripts etc. from the
HTML documents and keeps sentences which con-
tain at least 4 words. This number was chosen after
several experiments. The resulting data is passed
on to the multi-document summarizer which is de-
scribed in the next section.
3 SUMMA
SUMMA
4
is a set of language and processing re-
sources to create and evaluate summarization sys-
tems (single document, multi-document, multi-
lingual). The components can be used within
GATE
5
to produce ready summarization applica-
tions. SUMMA has been used in this work to
create an extractive multi-document summarizer:
both generic and query-based.
In the case of generic summarization SUMMA
uses a single cluster approach to summarize n re-
lated documents which are given as input. Using
GATE, SUMMA first applies sentence detection
and sentence tokenisation to the given documents.
Then each sentence in the documents is repre-
sented as a vector in a vector space model (Salton,
1988), where each vector position contains a term
3
http://htmlparser.sourceforge.net/
4
http://www.dcs.shef.ac.uk/ saggion/summa/default.htm
5
http://gate.ac.uk
43
(word) and a value which is a product of the term
frequency in the document and the inverse docu-
ment frequency (IDF), a measurement of the term?s
distribution over the set of documents (Salton and
Buckley, 1988). Furthermore, SUMMA enhances
the sentence vector representation with further fea-
tures such as the sentence position in its document
and the sentence similarity to the lead-part in its
document. In addition to computing the vector rep-
resentation for all sentences in the document col-
lection the centroid of this sentence representation
is also computed.
In the sentence selection process, each sentence
in the collection is ranked individually, and the top
sentences are chosen to build up the final summary.
The ranking of a sentence depends on its distance
to the centroid, its absolute position in its docu-
ment and its similarity to the lead-part of its doc-
ument. For calculating vector similarities, the co-
sine similarity measure is used (Salton and Lesk,
1968).
In the case of the query-based approach,
SUMMA adds an additional feature to the sentence
vector representation as computed for generic
summarization. For each sentence, cosine simi-
larity to the given query is computed and added
to the sentence vector representation. Finally, the
sentences are scored by summing all features in the
vector space model according to the following for-
mula:
Sentence
score
=
n
?
i=1
feature
i
? weight
i
After the scoring process, SUMMA starts selecting
sentences for summary generation. In both generic
and query-based summarization, the summary is
constructed by first selecting the sentence that has
the highest score, followed by the next sentence
with the second highest score until the compres-
sion rate is reached. However, before a sentence
is selected a similarity metric for redundancy de-
tection is applied to each sentence which decides
whether a sentence is distinct enough from already
selected sentences to be included in the summary
or not. SUMMA uses the following formula to
compute the similarity between two sentences:
NGramSim(S
1
, S
2
, n) =
n
?
j=1
w
j
?
grams(S
1
, j)
?
grams(S
2
, j)
grams(S
1
, j)
?
grams(S
2
, j)
where n specifies maximum size of the n-grams to
be considered, grams(S
X
, j) is the set of j-grams in
sentence X and w
j
is the weight associated with
j-gram similarity. Two sentences are similar if
NGramSim(S
1
, S
2
, n) > ?. In this work n is set
to 4 and ? to 0.1. For j-gram similarity weights
w
1
= 0.1, w
2
= 0.2, w
3
= 0.3 and w
4
= 0.4 are
selected. These values are coded in SUMMA as
defaults.
Using SUMMA, generic and query-based sum-
maries are generated for the image-related docu-
ments obtained from the web. Each summary con-
tains a maximum of 200 words. The queries used
in the query-based mode are toponyms collected as
described in section 2.1.
4 Creating Model Summaries
For evaluating automatically generated summaries
as image captions, information that people asso-
ciate with images is collected. For this purpose, an
online data collection procedure was set up. Par-
ticipants were provided with a set of 24 images.
Each image had a detailed map showing the loca-
tion where it was taken, along with URLs to 11
related documents which were used for the auto-
mated summarization. Figure 2 shows an example
of an image and Table 2 contains the correspond-
ing related information.
Each participant was asked to familiarize him-
or herself with the location of the image by an-
alyzing the map and going through all 11 URLs.
Then each participant decided on up to 5 different
pieces of information he/she would like to know if
he/she sees the image or information about some-
thing he/she relates with the image. The informa-
tion we collected in this way is similar to ?infor-
mation nuggets? (Voorhees, 2003). Information
nuggets are facts which help us assess automatic
summaries by checking whether the summary con-
tains the fact or not. In addition to this, each par-
ticipant was asked to collect the information only
from the given documents, ignoring any other links
in these documents.
Eleven students participated in this survey, sim-
ulating the scenario in which tourists look for in-
formation about an image of a popular sight. The
number of images annotated by each participant is
shown in Table 1.
The participants selected the information from
original HTML documents on the web and not
from the documents which were preprocessed for
the multi-document summarization task. We found
44
Table 1: Number of images annotated by each particant
User1 User2 User3 User4 User5 User6 User7 User8 User9 User10 User11
24 7 24 24 18 24 8 4 16 12 24
Figure 2: Example image
Table 2: Information related to Figure 2
1. Westminster Abbey is the place of the coronation, mar-
riage and burial of British monarchs, except Edward V and
Edward VIII since 1066
2. the parish church of the Royal Family
3. the centrepiece to the City of Westminster
4. first church on the site is believed to have been con-
structed around the year 700
5. The history and the monuments, crypts and memorials
are not to be missed.
out that in some cases the participants selected in-
formation that did not occur in the preprocessed
documents. To ensure that the information selected
by the participants also occurs in the preprocessed
documents, we retained only the information se-
lected by the participants that could also be found
in these documents, i.e. that was available to the
summarizer. Out of 807 nuggets selected by partic-
ipants 21 (2.6%) were not found in the documents
available to the summarizer and were removed.
Furthermore, as the example above shows (cf.
Table 2), not all the items of information se-
lected by the participants were in form of full sen-
tences. They vary from phrases to whole sen-
tences. The participants were free to select any
text unit from the documents that they related to
the image content. However, SUMMA works
extractively and its summaries contain only sen-
tences selected from the given input documents.
The user selected information was normalized to
sentences in order to have comparable summaries
for evaluation. This was achieved by selecting
the sentence(s) from the documents in which the
participant-selected information was found and re-
placing the participant-selected phrases or clauses
with the full sentence(s). In this way model sum-
maries were obtained.
5 Results
The model summaries were compared against
24 summaries generated automatically using
SUMMA by calculating ROUGE-1 to ROUGE-
4, ROUGE-L and ROUGE-W-1.2 recall metrics
(Lin, 2004). For all these metrics ROUGE com-
pares each automatically generated summary s
pairwise to every model summary m
i
from the set
of M model summaries and takes the maximum
ROUGE
Score
value among all pairwise compar-
isons as the best ROUGE
Score
score:
ROUGE
Score
= argmax
i
ROUGE
Score
(m
i
, s)
ROUGE repeats this comparisonM times. In each
iteration it applies the Jackknife method and takes
one model summary from theM model summaries
away and compares the automatically generated
summary s against the M ? 1 model summaries.
In each iteration one best ROUGE
Score
is calcu-
lated. The final ROUGE
Score
is then the average
of all best scores calculated in M iterations.
In this way each generic and query-based sum-
mary was compared with the corresponding model
summaries. The results are given in the first two
columns of Table 3. We also collected the com-
mon information all participants selected for a par-
ticular image and compared this to the correspond-
ing query-based summary. The common informa-
tion is the intersection set of the sets of information
each of the participants selected for a particular im-
age. The results for this comparison are shown in
column QueryToCPOfModel of Table 3.
The model summaries were also compared
against each other in order to assess the agreement
between the participants. To achieve this, the im-
age information selected by each participant was
compared against the rest. The corresponding re-
sults are shown in column UserToUser of Table
4. We applied the same pairwise comparison we
used for our model summaries to the model sum-
maries of task 5 in DUC 2004 in order to mea-
45
Table 3: Comparison: Automatically generated summaries against model summaries. The column GenericToModel for
example shows ROUGE results for generic summaries relative to model summaries. CP stands for common part, i.e. common
information selected by all participants.
Recall GenericToModel QueryToModel QueryToCPOfModel QueryToModelInDUC
R-1 0.38293 0.39655 0.22084 0.3341
R-2 0.14760 0.17266 0.09894 0.0723
R-3 0.09286 0.11196 0.06222 0.0279
R-4 0.07450 0.09219 0.04971 0.0131
R-L 0.34437 0.35837 0.20913 0.3320
R-W-1.2 0.11821 0.12606 0.06350 0.1130
Table 4: Comparison: Model summaries against each other
Recall UserToUser UserToUserInDUC
R-1 0.42765 0.45407
R-2 0.30091 0.13820
R-3 0.26338 0.05870
R-4 0.24964 0.02950
R-L 0.40403 0.41594
R-W-1.2 0.15846 0.13973
sure the agreements between the participants on
this standard task. This gives us a benchmark rel-
ative to which we can assess how well users agree
on what information should be related to images.
The results for this comparison are shown in col-
umn UserToUserInDUC of Table 4.
All ROUGE metrics except R-1 and R-L in-
dicate higher agreement in human image-related
summaries than in DUC document summaries.
The ROUGE metrics most indicative of agreement
between human summaries are those that best cap-
ture words occurring in longer sequences of words
immediately following each other (R-2, R-3, R-4
and R-W). If long word sequences are identical
in two summaries it is more likely that they be-
long to the same sentence than if only single words
are common, as captured by R-1, or sequences of
words that do not immediately follow each other,
as captured by R-L. In R-L gaps in word sequences
are ignored so that for instance A B C D G and
A E B F C K D have the common sequence A B
C D according to R-L. R-W considers the gaps in
words sequences so that this sequence would not
be recognized as common. Therefore the agree-
ment on our image-related human summaries is
substantially higher than agreement on DUC doc-
ument human summaries.
The results in Table 3 support our hypothesis
that query-based summaries will perform better
than generic ones on image-related summaries. All
ROUGE results of the query-based summaries are
greater than the generic summary scores. This
reinforces our decision to focus on query-based
summaries in order to create image-related sum-
maries which also satisfy the users? needs. How-
ever, even though the query-based summaries are
more appropriate for our purposes, they are not
completely satisfactory. The query-based sum-
maries cover only 39% of the unigrams (ROUGE
1) in the model summaries and only 17% of the
bigrams (ROUGE 2), while the model summaries
have 42% agreement in unigrams and 30% agree-
ment in bigrams (cf. column UserToUser in Table
4). The agreement between the query-based and
model summaries gets lower for ROUGE-3 and
ROUGE-4 indicating that the query-based sum-
maries contain very little information in common
with the participants? results. This indication is
supported by the ROUGE-L (35%) and the low
ROUGE-W (12%) agreement which are substan-
tially lower compared to the UserToUser ROUGE-
L (40%) and ROUGE-W (15%) and the low
ROUGE scores in column QueryToCPOfModel.
For comparison with automated summaries in a
different domain, we include ROUGE scores of
query based SUMMA used in DUC 2004 (Sag-
gion and Gaizauskas, 2005) as shown in the last
column of Table 3. All scores are lower than our
QueryToModel results which might be due to low
agreement between human generated summaries
for the DUC task (cf. UserToUserInDUC column
in Table 4) or maybe because image captioning is
an easier task. The possibility that our summariza-
tion task is easier than DUC due to the summa-
rizer having fewer documents to summarize or due
to the documents being shorter than those in the
DUC task can be excluded. In the DUC task the
multi-document clusters contain 10 documents on
average while our summarizer works with 11 doc-
uments. The mean length in documents in DUC
46
Table 5: Query-based summary for Westminster Abbey and information selected by participants
Query-based summary Information selected by participants
The City of London has St Pauls, but Westminster Abbey
is the centrepiece to the City of Westminster. Westmin-
ster Abbey should be at the top of any London traveler?s
list. Westminster Abbey, however, lacks the clear lines of
a Rayonnant church,... I loved Westminster Abbey on my
trip to London. Westminster Abbey was rebuilt after
1245 by Henry III?s order, and in 1258 the remodeling
of the east end of St. Paul?s Cathedral began. He was in-
terred in Westminster Abbey. From 1674 to 1678 he tuned
the organ at Westminster Abbey and was employed there
in 1675-76 to copy organ parts of anthems. The architec-
tural carving found at Westminster Abbey (mainly of the
1250s) has much of the daintiness of contemporary French
work, although the drapery is still more like that of the early
Chartres or Wells sculpture than that of the Joseph Master.
Nevertheless, Westminster Abbey is something to see if you
have not seen it before. I happened upon the Westminster
Abbey on an outing to Parliament and Big Ben.
1.(3) Westminster Abbey is the place of the coronation,
marriage and burial of British monarchs, except Edward
V and Edward VIII since 1066. 2.(1) What is unknown,
however is just how old it is. The first church on the
site is believed to have been constructed around the year
700. 3.(2) Standing as it does between Westminster Abbey
and the Houses of Parliament, and commonly called ?the
parish church of the House of Commons?, St Margaret?s has
witnessed many important events in the life of this coun-
try. 4.(1) In addition, the Abbey is the parish church of
the Royal Family, when in residence at Buckingham Palace.
5.(1) The history and the monuments, crypts and memorials
are not to be missed. 6.(1) For almost one thousand years,
Westminister Abbey has been the setting for much of Lon-
don?s ceremonies such as Royal Weddings, Coronations,
and Funeral Services. 7.(1) It is also where many visitors
pay pilgrimage to The Tomb of the Unknown Soldier. 8.(1)
The City of London has St Pauls, but Westminster Abbey is
the centrepiece to the City of Westminster.
is 23 sentences while our documents have 44 sen-
tences on average.
Table 5 shows an example query-based sum-
mary for the image of Westminster Abbey and the
information participants selected for this particu-
lar image. Jointly the participants have selected 8
different pieces of information as indicated by the
bold numbers in the table. The numbers in paren-
theses show the number of times that a particular
information unit was selected. By comparing the
two sides it can be seen that the query-based sum-
mary does not cover most of the information from
the list with the exception of item 2. The item 2 is
semantically related to the sentence in bold on the
summary side as it addresses the year the abbey
was built, but the information contained in the two
descriptions is different.
Our results have confirmed our hypothesis that
query-based summaries will better address the aim
of this research, which is to get summaries tai-
lored to users? needs. A generic summary does not
take the user query into consideration and gener-
ates summaries based on the topics it observes. For
a set of documents containing mainly historical
and little location-related information, a generic
summary will probably contain a higher number
of history-related than location-related sentences.
This might satisfy a group of people seeking his-
torical information, however, it might not be inter-
esting for a group who want to look for location-
related information. Therefore using a query-
based multi-document summarizer is more appro-
priate for image-related summaries than a generic
one. However, the results of the query-based sum-
maries show that even so they only cover a small
part of the information the users select. One reason
for this is that the query-based summarizer takes
relevant sentences according to the query given to
it and does not take into more general consider-
ation the information likely to be relevant to the
user. However, we can assume that users will have
shared interests in some of the information they
would like to get about a particular type of object
in an image (e.g. a bridge, church etc.). This as-
sumption is supported by the high agreement be-
tween participants? performances in our online sur-
vey (cf. column UserToUser of Table 4).
Therefore, one way to improve the performance
of the query-based summarizer is to give the sum-
marizer the information that users typically asso-
ciate with a particular object type as input and bias
the multi-document summarizer towards this in-
formation. To do this we plan to build models of
user preferences for different object types from the
large number of existing image captions from web
resources, which we believe will improve the qual-
ity of automatically generated captions.
6 Conclusion
In this work we showed that query-based summa-
rizers perform slightly better than generic sum-
marizers on an image captioning task. However,
their output is not completely satisfactory when
compared to what human participants indicated as
important in our data collection study. Our fu-
ture work will concentrate on extending the query-
47
based summarizer to improve its performance in
generating captions that match user expectations
regarding specific image types. This will include
collecting a large number of existing captions from
web sources and applying machine learning tech-
niques for building models of the kinds of informa-
tion that people use for captioning. Further work
also needs to be carried out on improving the read-
ability of the extractive caption summaries.
7 Acknowledgement
This work is supported by the EU-funded TRIPOD
project
6
. We would like to thank Horacio Saggion
for his support with SUMMA. We are also grateful
to Emina Kurtic, Mark Sanderson, Mesude Bicak
and Dilan Paranavithana for comments on the pre-
vious versions of this paper.
References
Barnard, Kobus and Duygulu, Pinar and Forsyth, David
and de Freitas, Nando and Blei M, David and Jor-
dan I, Michael. 2003. Matching words and pic-
tures. The Journal of Machine Learning Research,
MIT Press Cambridge, MA, USA, 3: 1107?1135.
Boros, Endre and Kantor B, Paul and Neu j, David.
2001. A Clustering Based Approach to Creating
Multi-Document Summaries. Proc. of the 24th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval.
Conroy M, John and Schlesinger D, Judith and Stew-
art G, Jade 2005. CLASSY query-based multi-
document summarization. Proc. of the 2005 Doc-
ument Understanding Workshop, Boston.
Deschacht, Koen andMoens F, Marie. 2007. Text Anal-
ysis for Automatic Image Annotation. Proc. of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, Prague.
Farzindar, Atefeh and Rozon, Frederik and Lapalme,
Guy. 2005. CATS a topic-oriented multi-
document summarization system at DUC 2005.
Proc. of the 2005 Document Understanding Work-
shop (DUC2005).
Lin, Chin-Yew 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. Proc. of the Work-
shop on Text Summarization Branches Out (WAS
2004).
Mani, Inderjeet. 2001. Automatic Summarization.
John Benjamins Publishing Company.
Mori, Yasuhide and Takahashi, Hironobu and Oka,
Ryuichi. 2000. Automatic word assignment to im-
ages based on image division and vector quantiza-
tion. Proc. of RIAO 2000: Content-Based Multime-
dia Information Access.
6
http://tripod.shef.ac.uk/
Pan, Jia-Yu. and Yang, Hyung-Jeong and Duygulu,
Pinar and Faloutsos, Christos. 2004. Automatic
image captioning. Multimedia and Expo, 2004.
ICME?04. 2004 IEEE International Conference on.
Radev R, Dragomir. and Jing, Hongyan and Sty?s, Mal-
gorzata and Tam, Daniel. 2004. Centroid-based
summarization of multiple documents. Information
Processing and Management,40(6): 919?938.
Saggion, Horacio and Gaizauskas, Robert 2004. Multi-
document summarization by cluster/profile relevance
and redundancy removal. Document Understanding
Conference (DUC04).
Salton, Gerhard 1988. Automatic text process-
ing. Addison-Wesley Longman Publishing Co., Inc.
Boston, MA, USA.
Salton, Gerhard and Buckley, Chris 1988. Term-
weighting approaches in automatic text retrieval.
Pergamon Press, Inc. Tarrytown, NY, USA.
Salton, Gerhard and Lesk E., Michael 1968. Computer
Evaluation of Indexing and Text Processing. Journal
of the ACM,15(1):8?36.
Sekine, Satoshi and Nobata, Chikashi. 2003. A Sur-
vey for Multi-Document Summarization. Associa-
tion for Computational Linguistics Morristown, NJ,
USA, Proc. of the HLT-NAACL 03 on Text summa-
rization workshop-Volume 5.
Voorhees M, Ellen. 2003. Overview of the TREC 2003
Question Answering Track. Proc. of the Twelfth Text
REtrieval Conference (TREC 2003).
Westerveld, Thijs. 2000. Image retrieval: Content ver-
sus context. Content-Based Multimedia Information
Access, RIAO 2000 Conference.
48
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 34?41
Manchester, UK. August 2008
A Data Driven Approach to Query Expansion in Question Answering
Leon Derczynski, Jun Wang, Robert Gaizauskas and Mark A. Greenwood
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield S1 4DP UK
{aca00lad, acp07jw}@shef.ac.uk
{r.gaizauskas, m.greenwood}@dcs.shef.ac.uk
Abstract
Automated answering of natural language
questions is an interesting and useful prob-
lem to solve. Question answering (QA)
systems often perform information re-
trieval at an initial stage. Information re-
trieval (IR) performance, provided by en-
gines such as Lucene, places a bound on
overall system performance. For example,
no answer bearing documents are retrieved
at low ranks for almost 40% of questions.
In this paper, answer texts from previous
QA evaluations held as part of the Text
REtrieval Conferences (TREC) are paired
with queries and analysed in an attempt
to identify performance-enhancing words.
These words are then used to evaluate the
performance of a query expansion method.
Data driven extension words were found
to help in over 70% of difficult questions.
These words can be used to improve and
evaluate query expansion methods. Sim-
ple blind relevance feedback (RF) was cor-
rectly predicted as unlikely to help overall
performance, and an possible explanation
is provided for its low value in IR for QA.
1 Introduction
The task of supplying an answer to a question,
given some background knowledge, is often con-
sidered fairly trivial from a human point of view,
as long as the question is clear and the answer is
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
known. The aim of an automated question answer-
ing system is to provide a single, unambiguous re-
sponse to a natural language question, given a text
collection as a knowledge source, within a certain
amount of time. Since 1999, the Text Retrieval
Conferences have included a task to evaluate such
systems, based on a large pre-defined corpus (such
as AQUAINT, containing around a million news
articles in English) and a set of unseen questions.
Many information retrieval systems perform
document retrieval, giving a list of potentially rel-
evant documents when queried ? Google?s and Ya-
hoo!?s search products are examples of this type of
application. Users formulate a query using a few
keywords that represent the task they are trying to
perform; for example, one might search for ?eif-
fel tower height? to determine how tall the Eiffel
tower is. IR engines then return a set of references
to potentially relevant documents.
In contrast, QA systems must return an exact an-
swer to the question. They should be confident
that the answer has been correctly selected; it is
no longer down to the user to research a set of doc-
ument references in order to discover the informa-
tion themselves. Further, the system takes a natural
language question as input, instead of a few user-
selected key terms.
Once a QA system has been provided with a
question, its processing steps can be described in
three parts - Question Pre-Processing, Text Re-
trieval and Answer Extraction:
1. Question Pre-Processing TREC questions
are grouped into series which relate to a given
target. For example, the target may be ?Hinden-
burg disaster? with questions such as ?What type
of craft was the Hindenburg?? or ?How fast could
it travel??. Questions may include pronouns ref-
34
erencing the target or even previous answers, and
as such require processing before they are suitable
for use.
2. Text Retrieval An IR component will return
a ranked set of texts, based on query terms. At-
tempting to understand and extract data from an
entire corpus is too resource intensive, and so an IR
engine defines a limited subset of the corpus that
is likely to contain answers. The question should
have been pre-processed correctly for a useful set
of texts to be retrieved ? including anaphora reso-
lution.
3. Answer Extraction (AE) Given knowledge
about the question and a set of texts, the AE sys-
tem attempts to identify answers. It should be clear
that only answers within texts returned by the IR
component have any chance of being found.
Reduced performance at any stage will have a
knock-on effect, capping the performance of later
stages. If questions are left unprocessed and full
of pronouns (e.g.,?When did it sink??) the IR com-
ponent has very little chance of working correctly
? in this case, the desired action is to retrieve
documents related to the Kursk submarine, which
would be impossible.
IR performance with a search engine such as
Lucene returns no useful documents for at least
35% of all questions ? when looking at the top
20 returned texts. This caps the AE component
at 65% question ?coverage?. We will measure the
performance of different IR component configura-
tions, to rule out problems with a default Lucene
setup.
For each question, answers are provided in the
form of regular expressions that match answer text,
and a list of documents containing these answers
in a correct context. As references to correct doc-
uments are available, it is possible to explore a
data-driven approach to query analysis. We deter-
mine which questions are hardest then concentrate
on identifying helpful terms found in correct doc-
uments, with a view to building a system than can
automatically extract these helpful terms from un-
seen questions and supporting corpus. The avail-
ability and usefulness of these terms will provide
an estimate of performance for query expansion
techniques.
There are at least two approaches which could
make use of these term sets to perform query ex-
pansion. They may occur in terms selected for
blind RF (non-blind RF is not applicable to the
TREC QA task). It is also possible to build a cata-
logue of terms known to be useful according to cer-
tain question types, thus leading to a dictionary of
(known useful) expansions that can be applied to
previously unseen questions. We will evaluate and
also test blind relevance feedback in IR for QA.
2 Background and Related Work
The performance of an IR system can be quanti-
fied in many ways. We choose and define mea-
sures pertinent to IR for QA. Work has been done
on relevance feedback specific to IR for QA, where
it is has usually be found to be unhelpful. We out-
line the methods used in the past, extend them, and
provide and test means of validating QA relevance
feedback.
2.1 Measuring QA Performance
This paper uses two principle measures to describe
the performance of the IR component. Coverage
is defined as the proportion of questions where at
least one answer bearing text appears in the re-
trieved set. Redundancy is the average number
of answer bearing texts retrieved for each ques-
tion (Roberts and Gaizauskas, 2004).
Both these measures have a fixed limit n on the
number of texts retrieved by a search engine for a
query. As redundancy counts the number of texts
containing correct answers, and not instances of
the answer itself, it can never be greater than the
number of texts retrieved.
The TREC reference answers provide two ways
of finding a correct text, with both a regular expres-
sion and a document ID. Lenient hits (retrievals of
answer bearing documents) are those where the re-
trieved text matches the regular expression; strict
hits occur when the document ID of the retrieved
text matches that declared by TREC as correct and
the text matches the regular expression. Some doc-
uments will match the regular expression but not
be deemed as containing a correct answer (this
is common with numbers and dates (Baeza-Yates
and Ribeiro-Neto, 1999)), in which case a lenient
match is found, but not a strict one.
The answer lists as defined by TREC do not in-
clude every answer-bearing document ? only those
returned by previous systems and marked as cor-
rect. Thus, false negatives are a risk, and strict
measures place an approximate lower bound on
the system?s actual performance. Similarly, lenient
35
matches can occur out of context, without a sup-
porting document; performance based on lenient
matches can be viewed as an approximate upper
bound (Lin and Katz, 2005).
2.2 Relevance Feedback
Relevance feedback is a widely explored technique
for query expansion. It is often done using a spe-
cific measure to select terms using a limited set of
ranked documents of size r; using a larger set will
bring term distribution closer to values over the
whole corpus, and away from ones in documents
relevant to query terms. Techniques are used to
identify phrases relevant to a query topic, in or-
der to reduce noise (such as terms with a low cor-
pus frequency that relate to only a single article)
and query drift (Roussinov and Fan, 2005; Allan,
1996).
In the context of QA, Pizzato (2006) employs
blind RF using the AQUAINT corpus in an attempt
to improve performance when answering factoid
questions on personal names. This is a similar ap-
proach to some content in this paper, though lim-
ited to the study of named entities, and does not
attempt to examine extensions from the existing
answer data.
Monz (2003) finds a negative result when apply-
ing blind feedback for QA in TREC 9, 10 and 11,
and a neutral result for TREC 7 and 8?s ad hoc re-
trieval tasks. Monz?s experiment, using r = 10
and standard Rocchio term weighting, also found
a further reduction in performance when r was
reduced (from 10 to 5). This is an isolated ex-
periment using just one measure on a limited set
of questions, with no use of the available answer
texts.
Robertson (1992) notes that there are issues
when using a whole document for feedback, as
opposed to just a single relevant passage; as men-
tioned in Section 3.1, passage- and document-level
retrieval sets must also be compared for their per-
formance at providing feedback. Critically, we
will survey the intersection between words known
to be helpful and blind RF terms based on initial
retrieval, thus showing exactly how likely an RF
method is to succeed.
3 Methodology
We first investigated the possibility of an IR-
component specific failure leading to impaired
coverage by testing a variety of IR engines and
configurations. Then, difficult questions were
identified, using various performance thresholds.
Next, answer bearing texts for these harder ques-
tions were checked for words that yielded a per-
formance increase when used for query expansion.
After this, we evaluated how likely a RF-based ap-
proach was to succeed. Finally, blind RF was ap-
plied to the whole question set. IR performance
was measured, and terms used for RF compared to
those which had proven to be helpful as extension
words.
3.1 IR Engines
A QA framework (Greenwood, 2004a) was origi-
nally used to construct a QA system based on run-
ning a default Lucene installation. As this only
covers one IR engine in one configuration, it is
prudent to examine alternatives. Other IR engines
should be tested, using different configurations.
The chosen additional engines were: Indri, based
on the mature INQUERY engine and the Lemur
toolkit (Allan et al, 2003); and Terrier, a newer en-
gine designed to deal with corpora in the terabyte
range and to back applications entered into TREC
conferences (Ounis et al, 2005).
We also looked at both passage-level and
document-level retrieval. Passages can be de-
fined in a number of ways, such as a sentence,
a sliding window of k terms centred on the tar-
get term(s), parts of a document of fixed (and
equal) lengths, or a paragraph. In this case,
the documents in the AQUAINT corpus contain
paragraph markers which were used as passage-
level boundaries, thus making ?passage-level?
and ?paragraph-level? equivalent in this paper.
Passage-level retrieval may be preferable for AE,
as the number of potential distracters is some-
what reduced when compared to document-level
retrieval (Roberts and Gaizauskas, 2004).
The initial IR component configuration was with
Lucene indexing the AQUAINT corpus at passage-
level, with a Porter stemmer (Porter, 1980) and an
augmented version of the CACM (Jones and van
Rijsbergen, 1976) stopword list.
Indri natively supports document-level indexing
of TREC format corpora. Passage-level retrieval
was done using the paragraph tags defined in the
corpus as delimiters; this allows both passage- and
document-level retrieval from the same index, ac-
cording to the query.
All the IR engines were unified to use the Porter
36
Coverage Redundancy
Year Len. Strict Len. Strict
Lucene
2004 0.686 0.636 2.884 1.624
2005 0.703 0.566 2.780 1.155
2006 0.665 0.568 2.417 1.181
Indri
2004 0.690 0.554 3.849 1.527
2005 0.694 0.512 3.908 1.056
2006 0.691 0.552 3.373 1.152
Terrier
2004 - - - -
2005 - - - -
2006 0.638 0.493 2.520 1.000
Table 1: Performance of Lucene, Indri and Terrier at para-
graph level, over top 20 documents. This clearly shows the
limitations of the engines.
stemmer and the same CACM-derived stopword
list.
The top n documents for each question in the
TREC2004, TREC2005 and TREC2006 sets were
retrieved using every combination of engine, and
configuration1 . The questions and targets were
processed to produce IR queries as per the default
configuration for the QA framework. Examining
the top 200 documents gave a good compromise
between the time taken to run experiments (be-
tween 30 and 240 minutes each) and the amount
one can mine into the data. Tabulated results are
shown in Table 1 and Table 2. Queries have had
anaphora resolution performed in the context of
their series by the QA framework. AE compo-
nents begin to fail due to excess noise when pre-
sented with over 20 texts, so this value is enough to
encompass typical operating parameters and leave
space for discovery (Greenwood et al, 2006).
A failure analysis (FA) tool, an early version
of which is described by (Sanka, 2005), provided
reporting and analysis of IR component perfor-
mance. In this experiment, it provided high level
comparison of all engines, measuring coverage
and redundancy as the number of documents re-
trieved, n, varies. This is measured because a per-
fect engine will return the most useful documents
first, followed by others; thus, coverage will be
higher for that engine with low values of n.
3.2 Identification of Difficult Questions
Once the performance of an IR configuration over
a question set is known, it?s possible to produce
a simple report listing redundancy for each ques-
tion. A performance reporting script accesses the
1Save Terrier / TREC2004 / passage-level retrieval;
passage-level retrieval with Terrier was very slow using our
configuration, and could not be reliably performed using the
same Terrier instance as document-level retrieval.
Coverage Redundancy
Year Len. Strict Len. Strict
Indri
2004 0.926 0.837 7.841 2.663
2005 0.935 0.735 7.573 1.969
2006 0.882 0.741 6.872 1.958
Terrier
2004 0.919 0.806 7.186 2.380
2005 0.928 0.766 7.620 2.130
2006 0.983 0.783 6.339 2.067
Table 2: Performance of Indri and Terrier at document level
IR over the AQUAINT corpus, with n = 20
FA tool?s database and lists all the questions in
a particular set with the strict and lenient redun-
dancy for selected engines and configurations. En-
gines may use passage- or document-level config-
urations.
Data on the performance of the three engines is
described in Table 2. As can be seen, the cover-
age with passage-level retrieval (which was often
favoured, as the AE component performs best with
reduced amounts of text) languishes between 51%
and 71%, depending on the measurement method.
Failed anaphora resolution may contribute to this
figure, though no deficiencies were found upon vi-
sual inspection.
Not all documents containing answers are noted,
only those checked by the NIST judges (Bilotti
et al, 2004). Match judgements are incomplete,
leading to the potential generation of false nega-
tives, where a correct answer is found with com-
plete supporting information, but as the informa-
tion has not been manually flagged, the system will
mark this as a failure. Assessment methods are
fully detailed in Dang et al (2006). Factoid per-
formance is still relatively poor, although as only
1.95 documents match per question, this may be an
effect of such false negatives (Voorhees and Buck-
land, 2003). Work has been done into creating
synthetic corpora that include exhaustive answer
sets (Bilotti, 2004; Tellex et al, 2003; Lin and
Katz, 2005), but for the sake of consistency, and
easy comparison with both parallel work and prior
local results, the TREC judgements will be used to
evaluate systems in this paper.
Mean redundancy is also calculated for a num-
ber of IR engines. Difficult questions were those
for which no answer bearing texts were found by
either strict or lenient matches in any of the top n
documents, using a variety of engines. As soon as
one answer bearing document was found by an en-
gine using any measure, that question was deemed
non-difficult. Questions with mean redundancy of
37
zero are marked difficult, and subjected to further
analysis. Reducing the question set to just diffi-
cult questions produces a TREC-format file for re-
testing the IR component.
3.3 Extension of Difficult Questions
The documents deemed relevant by TREC must
contain some useful text that can help IR engine
performance. Such words should be revealed by
a gain in redundancy when used to extend an ini-
tially difficult query, usually signified by a change
from zero to a non-zero value (signifying that rele-
vant documents have been found where none were
before). In an attempt to identify where the use-
ful text is, the relevant documents for each difficult
question were retrieved, and passages matching the
answer regular expression identified. A script is
then used to build a list of terms from each passage,
removing words in the question or its target, words
that occur in the answer, and stopwords (based on
both the indexing stopword list, and a set of stems
common within the corpus). In later runs, num-
bers are also stripped out of the term list, as their
value is just as often confusing as useful (Baeza-
Yates and Ribeiro-Neto, 1999). Of course, answer
terms provide an obvious advantage that would not
be reproducible for questions where the answer is
unknown, and one of our goals is to help query ex-
pansion for unseen questions. This approach may
provide insights that will enable appropriate query
expansion where answers are not known.
Performance has been measured with both the
question followed by an extension (Q+E), as well
as the question followed by the target and then
extension candidates (Q+T+E). Runs were also
executed with just Q and Q+T, to provide non-
extended reference performance data points. Ad-
dition of the target often leads to gains in perfor-
mance (Roussinov et al, 2005), and may also aid
in cases where anaphora resolution has failed.
Some words are retained, such as titles, as in-
cluding these can be inferred from question or tar-
get terms and they will not unfairly boost redun-
dancy scores; for example, when searching for a
?Who? question containing the word ?military?,
one may want to preserve appellations such as
?Lt.? or ?Col.?, even if this term appears in the an-
swer.
This filtered list of extensions is then used to cre-
ate a revised query file, containing the base ques-
tion (with and without the target suffixed) as well
as new questions created by appending a candidate
extension word.
Results of retrievals with these new question are
loaded into the FA database and a report describ-
ing any performance changes is generated. The
extension generation process also creates custom
answer specifications, which replicate the informa-
tion found in the answers defined by TREC.
This whole process can be repeated with vary-
ing question difficulty thresholds, as well as alter-
native n values (typically from 5 to 100), different
engines, and various question sets.
3.4 Relevance Feedback Performance
Now that we can find the helpful extension words
(HEWs) described earlier, we?re equipped to eval-
uate query expansion methods. One simplistic ap-
proach could use blind RF to determine candidate
extensions, and be considered potentially success-
ful should these words be found in the set of HEWs
for a query. For this, term frequencies can be
measured given the top r documents retrieved us-
ing anaphora-resolved query Q. After stopword
and question word removal, frequent terms are ap-
pended to Q, which is then re-evaluated. This
has been previously attempted for factoid ques-
tions (Roussinov et al, 2005) and with a limited
range of r values (Monz, 2003) but not validated
using a set of data-driven terms.
We investigated how likely term frequency (TF)
based RF is to discover HEWs. To do this, the
proportion of HEWs that occurred in initially re-
trieved texts was measured, as well as the propor-
tion of these texts containing at least one HEW.
Also, to see how effective an expansion method is,
suggested expansion terms can be checked against
the HEW list.
We used both the top 5 and the top 50 documents
in formulation of extension terms, with TF as a
ranking measure; 50 is significantly larger than the
optimal number of documents for AE (20), without
overly diluting term frequencies.
Problems have been found with using entire
documents for RF, as the topic may not be the
same throughout the entire discourse (Robertson
et al, 1992). Limiting the texts used for RF to
paragraphs may reduce noise; both document- and
paragraph-level terms should be checked.
38
Engine
Year Lucene
Para
Indri
Para
Indri
Doc
Terrier
Doc
2004 76 72 37 42
2005 87 98 37 35
2006 108 118 59 53
Table 3: Number of difficult questions, as defined by those
which have zero redundancy over both strict and lenient mea-
sures, at n = 20. Questions seem to get harder each year.
Document retrieval yields fewer difficult questions, as more
text is returned for potential matching.
Engine
Lucene Indri Terrier
Paragraph 226 221 -
Document - 121 109
Table 4: Number of difficult questions in the 2006 task, as de-
fined above, this time with n = 5. Questions become harder
as fewer chances are given to provide relevant documents.
4 Results
Once we have HEWs, we can determine if these
are going to be of significant help when chosen as
query extensions. We can also determine if a query
expansion method is likely to be fruitful. Blind RF
was applied, and assessed using the helpful words
list, as well as RF?s effect on coverage.
4.1 Difficult Question Analysis
The number of difficult questions found at n =
20 is shown in Table 3. Document-level retrieval
gave many fewer difficult questions, as the amount
of text retrieved gave a higher chance of finding
lenient matches. A comparison of strict and lenient
matching is in Table 5.
Extensions were then applied to difficult ques-
tions, with or without the target. The performance
of these extensions is shown in Table 6. Results
show a significant proportion (74.4%) of difficult
questions can benefit from being extended with
non-answer words found in answer bearing texts.
4.2 Applying Relevance Feedback
Identifying HEWs provides a set of words that
are useful for evaluating potential expansion terms.
Match type
Strict Lenient
Year
2004 39 49
2005 56 66
2006 53 49
Table 5: Common difficult questions (over all three engines
mentioned above) by year and match type; n = 20.
Difficult questions used 118
Variations tested 6683
Questions that benefited 87 (74.4%)
Helpful extension words (strict) 4973
Mean helpful words per question 42.144
Mean redundancy increase 3.958
Table 6: Using Terrier Passage / strict matching, retrieving 20
docs, with TREC2006 questions / AQUAINT. Difficult ques-
tions are those where no strict matches are found in the top 20
IRT from just one engine.
2004 2005 2006
HEW found in IRT 4.17% 18.58% 8.94%
IRT containing HEW 10.00% 33.33% 34.29%
RF words in HEW 1.25% 1.67% 5.71%
Table 7: ?Helpful extension words?: the set of extensions that,
when added to the query, move redundancy above zero. r =
5, n = 20, using Indri at passage level.
Using simple TF based feedback (see Section 3.4),
5 terms were chosen per query. These words had
some intersection (see Table 7) with the exten-
sion words set, indicating that this RF may lead to
performance increases for previously unseen ques-
tions. Only a small number of the HEWs occur in
the initially retrieved texts (IRTs), although a no-
ticeable proportion of IRTs (up to 34.29%) contain
at least one HEW. However, these terms are prob-
ably not very frequent in the documents and un-
likely to be selected with TF-based blind RF. The
mean proportion of RF selected terms that were
HEWs was only 2.88%. Blind RF for question an-
swering fails here due to this low proportion. Strict
measures are used for evaluation as we are inter-
ested in finding documents which were not pre-
viously being retrieved rather than changes in the
distribution of keywords in IRT.
Document and passage based RF term selection
is used, to explore the effect of noise on terms, and
document based term selection proved marginally
superior. Choosing RF terms from a small set of
documents (r = 5) was found to be marginally
better than choosing from a larger set (r = 50).
In support of the suggestion that RF would be un-
r
5 50 Baseline
Rank Doc Para Doc Para
5 0.253 0.251 0.240 0.179 0.312
10 0.331 0.347 0.331 0.284 0.434
20 0.438 0.444 0.438 0.398 0.553
50 0.583 0.577 0.577 0.552 0.634
Table 8: Coverage (strict) using blind RF. Both document-
and paragraph-level retrieval used to determine RF terms.
39
Question:
Who was the nominal leader after the overthrow?
Target: Pakistani government overthrown in 1999
Extension word Redundancy
Kashmir 4
Pakistan 4
Islamabad 2.5
Question: Where did he play in college?
Target: Warren Moon
Extension word Redundancy
NFL 2.5
football 1
Question: Who have commanded the division?
Target: 82nd Airborne division
Extension word Redundancy
Gen 3
Col 2
decimated 2
officer 1
Table 9: Queries with extensions, and their mean redundancy
using Indri at document level with n = 20. Without exten-
sions, redundancy is zero.
likely to locate HEWs, applying blind RF consis-
tently hampered overall coverage (Table 8).
5 Discussion
HEWs are often found in answer bearing texts,
though these are hard to identify through sim-
ple TF-based RF. A majority of difficult questions
can be made accessible through addition of HEWs
present in answer bearing texts, and work to deter-
mine a relationship between words found in initial
retrieval and these HEWs can lead to coverage in-
creases. HEWs also provide an effective means
of evaluating other RF methods, which can be de-
veloped into a generic rapid testing tool for query
expansion techniques. TF-based RF, while finding
some HEWs, is not effective at discovering exten-
sions, and reduces overall IR performance.
There was not a large performance change
between engines and configurations. Strict
paragraph-level coverage never topped 65%, leav-
ing a significant number of questions where no
useful information could be provided for AE.
The original sets of difficult questions for in-
dividual engines were small ? often less than the
35% suggested when looking at the coverage fig-
ures. Possible causes could include:
Difficult questions being defined as those for
which average redundancy is zero: This limit
may be too low. To remedy this, we could increase
the redundancy limit to specify an arbitrary num-
ber of difficult questions out of the whole set.
The use of both strict and lenient measures: It
is possible to get a lenient match (thus marking a
question as non-difficult) when the answer text oc-
curs out of context.
Reducing n from 20 to 5 (Table 4) increased
the number of difficult questions produced. From
this we can hypothesise that although many search
engines are succeeding in returning useful docu-
ments (where available), the distribution of these
documents over the available ranks is not one that
bunches high ranking documents up as those im-
mediately retrieved (unlike a perfect engine; see
Section 3.1), but rather suggests a more even dis-
tribution of such documents over the returned set.
The number of candidate extension words for
queries (even after filtering) is often in the range
of hundreds to thousands. Each of these words
creates a separate query, and there are two varia-
tions, depending on whether the target is included
in the search terms or not. Thus, a large number
of extended queries need to be executed for each
question run. Passage-level retrieval returns less
text, which has two advantages: firstly, it reduces
the scope for false positives in lenient matching;
secondly, it is easier to scan result by eye and de-
termine why the engine selected a result.
Proper nouns are often helpful as extensions.
We noticed that these cropped up fairly regularly
for some kinds of question (e.g. ?Who?). Espe-
cially useful were proper nouns associated with
locations - for example, adding ?Pakistani? to
a query containing the word Pakistan lifted re-
dundancy above zero for a question on President
Musharraf, as in Table 9. This reconfirms work
done by Greenwood (2004b).
6 Conclusion and Future Work
IR engines find some questions very difficult and
consistently fail to retrieve useful texts even with
high values of n. This behaviour is common over
many engines. Paragraph level retrieval seems to
give a better idea of which questions are hard-
est, although the possibility of false negatives is
present from answer lists and anaphora resolution.
Relationships exist between query words and
helpful words from answer documents (e.g. with
a military leadership themes in a query, adding the
term ?general? or ?gen? helps). Identification of
HEWs has potential use in query expansion. They
could be used to evaluate RF approaches, or asso-
ciated with question words and used as extensions.
Previous work has ruled out relevance feedback
40
in particular circumstances using a single ranking
measure, though this has not been based on analy-
sis of answer bearing texts. The presence of HEWs
in IRT for difficult questions shows that guided RF
may work, but this will be difficult to pursue. Blind
RF based on term frequencies does not increase IR
performance. However, there is an intersection be-
tween words in initially retrieved texts and words
data driven analysis defines as helpful, showing
promise for alternative RF methods (e.g. based on
TFIDF). These extension words form a basis for
indicating the usefulness of RF and query expan-
sion techniques.
In this paper, we have chosen to explore only
one branch of query expansion. An alternative data
driven approach would be to build associations be-
tween recurrently useful terms given question con-
tent. Question texts could be stripped of stopwords
and proper nouns, and a list of HEWs associated
with each remaining term. To reduce noise, the
number of times a particular extension has helped
a word would be counted. Given sufficient sample
data, this would provide a reference body of HEWs
to be used as an aid to query expansion.
References
Allan, J., J. Callan, K. Collins-Thompson, B. Croft,
F. Feng, D. Fisher, J. Lafferty, L. Larkey, TN Truong,
P. Ogilvie, et al 2003. The Lemur Toolkit for Lan-
guage Modeling and Information Retrieval.
Allan, J. 1996. Incremental Relevance Feedback for
Information Filtering. In Research and Development
in IR, pages 270?278.
Baeza-Yates, R. and B. Ribeiro-Neto. 1999. Modern
Information Retrieval. Addison Wesley.
Bilotti, M.W., B. Katz, and J. Lin. 2004. What Works
Better for Question Answering: Stemming or Mor-
phological Query Expansion. Proc. IR for QA Work-
shop at SIGIR 2004.
Bilotti, M.W. 2004. Query Expansion Techniques for
Question Answering. Master?s thesis, Massachusetts
Institute of Technology.
Dang, H.T., J. Lin, and D. Kelly. 2006. Overview of
the TREC 2006 QA track. Proc. 15th Text REtrieval
Conf..
Greenwood, M.A., M. Stevenson, and R. Gaizauskas.
2006. The University of Sheffield?s TREC 2006
Q&A Experiments. In Proc. 15th Text REtrieval
Conference
Greenwood, M.A. 2004a. AnswerFinder: Question
Answering from your Desktop. In Proc. 7th Annual
Colloquium for the UK SIG for Computational Lin-
guistics (CLUK ?04).
Greenwood, M.A. 2004b. Using Pertainyms to Im-
prove Passage Retrieval for Questions Requesting
Information about a Location. In Proc. Workshop
on IR for QA (SIGIR 2004).
Jones, K.S. and C.J. van Rijsbergen. 1976. IR Test
Collections. J. of Documentation, 32(1):59?75.
Lin, J. and B. Katz. 2005. Building a Reusable Test
Collection for Question Answering. J. American So-
ciety for Information Science and Technology.
Monz, C. 2003. From Document Retrieval to Question
Answering. ILLC Dissertation Series 2003, 4.
Ounis, I., G. Amati, V. Plachouras, B. He, C. Macdon-
ald, and D. Johnson. 2005. Terrier IR Platform.
Proc. 27th European Conf. on IR (ECIR 05), San-
tiago de Compostela, Spain, pages 517?519.
Pizzato, L.A., D. Molla, and C. Paris. 2006. Pseudo-
Relevance Feedback using Named Entities for Ques-
tion Answering. Australasian Language Technology
Workshop (ALTW2006), pages 83?90.
Porter, M. 1980. An Algorithm for Suffix Stripping
Program. Program, 14(3):130?137.
Roberts, I and R Gaizauskas. 2004. Evaluating Passage
Retrieval Approaches for Question Answering. In
Proc. 26th European Conf. on IR.
Robertson, S.E., S. Walker, M. Hancock-Beaulieu,
A. Gull, and M. Lau. 1992. Okapi at TREC. In
Text REtrieval Conf., pages 21?30.
Roussinov, D. and W. Fan. 2005. Discretization Based
Learning Approach to Information Retrieval. In
Proc. 2005 Conf. on Human Language Technologies.
Roussinov, D., M. Chau, E. Filatova, and J.A. Robles-
Flores. 2005. Building on Redundancy: Fac-
toid Question Answering, Robust Retrieval and the
?Other?. In Proc. 14th Text REtrieval Conf.
Sanka, Atheesh. 2005. Passage Retrieval for Question
Answering. Master?s thesis, University of Sheffield.
Tellex, S., B. Katz, J. Lin, A. Fernandes, and G. Marton.
2003. Quantitative Evaluation of Passage Retrieval
Algorithms for Question Answering. Proc. 26th An-
nual Int?l ACM SIGIR Conf. on R&D in IR, pages
41?47.
Voorhees, E. and L. P. Buckland, editors. 2003. Proc.
12th Text REtrieval Conference.
41
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 58?65
Manchester, UK. August 2008
Evaluation of Automatically Reformulated Questions in Question Series
Richard Shaw, Ben Solway, Robert Gaizauskas and Mark A. Greenwood
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield S1 4DP UK
{aca04rcs, aca04bs}@shef.ac.uk
{r.gaizauskas, m.greenwood}@dcs.shef.ac.uk
Abstract
Having gold standards allows us to evalu-
ate new methods and approaches against a
common benchmark. In this paper we de-
scribe a set of gold standard question re-
formulations and associated reformulation
guidelines that we have created to support
research into automatic interpretation of
questions in TREC question series, where
questions may refer anaphorically to the
target of the series or to answers to pre-
vious questions. We also assess various
string comparison metrics for their utility
as evaluation measures of the proximity of
an automated system?s reformulations to
the gold standard. Finally we show how
we have used this approach to assess the
question processing capability of our own
QA system and to pinpoint areas for im-
provement.
1 Introduction
The development of computational systems which
can answer natural language questions using large
text collections as knowledge sources is widely
seen as both intellectually challenging and prac-
tically useful. To stimulate research and devel-
opment in this area the US National Institute of
Standards and Technology (NIST) has organized a
shared task evaluation as one track at the annual
TExt Retrieval Conference (TREC) since 19991.
These evaluations began by considering factoid-
type questions only (e.g. How many calories are
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1http://trec.nist.gov/
there in a Big Mac?) each of which was asked in
isolation to any of the others. However, in an effort
to move the challenge towards a long term vision
of interactive, dialogue-based question answer-
ing to support information analysts (Burger et al,
2002), the track introduced the notion of question
targets and related question series in TREC2004
(Voorhees, 2005), and this approach to question
presentation has remained central in each of the
subsequent TRECs. In this simulated task, ques-
tions are grouped into series where each series has
a target of a definition associated with it (see Fig-
ure 1). Each question in the series asks for some
information about the target and there is a final
?other? question which is to be interpreted as ?Pro-
vide any other interesting details about the target
that has not already been asked for explicitly?. In
this way ?each series is a (limited) abstraction of
an information dialogue in which the user is trying
to define the target. The target and earlier ques-
tions in a series provide the context for the current
question.? (Voorhees, 2005).
One consequence of putting questions into se-
ries in this way is that questions may not make
much sense when removed from the context their
series provides. For example, the question When
was he born? cannot be sensibly interpreted with-
out knowledge of the antecedent of he provided
by the context (target or prior questions). Inter-
preting questions in question series, therefore, be-
comes a critical component within a QA systems.
Many QA systems have an initial document re-
trieval stage that takes the question and derives a
query from it which is then passed to a search en-
gine whose task is to retrieve candidate answering
bearing documents for processing by the rest of
the system. Clearly a question such as When was
he born? is unlikely to retrieve documents rele-
58
Target 136: Shiite
Q136.1 Who was the first Imam of the Shiite
sect of Islam?
Q136.2 Where is his tomb?
Q136.3 What was this persons relationship to
the Prophet Mohammad?
Q136.4 Who was the third Imam of Shiite
Muslims?
Q136.5 When did he die?
Figure 1: An Example Question Series
vant to answering a question about Kafka?s date
of birth if passed directly to a search engine. This
problem can be addressed in a naive way by sim-
ply appending the target to every question. How-
ever, this has several disadvantages: (1) in some
cases co-reference in a question series is to the
answer of a previous question and not to the tar-
get, so blindly substituting the target is not ap-
propriate; (2) some approaches to query formula-
tion and to answer extraction from retrieved docu-
ments may require syntactically well-formed ques-
tions and may be able to take advantage of the extra
information, such as syntactic dependencies, pro-
vided in a fully de-referenced, syntactically correct
question.
Thus, it is helpful in general if systems can auto-
matically interpret a question in context so as to re-
solve co-references appropriately, and indeed most
TREC QA systems do this to at least a limited ex-
tent as part of their question pre-processing. Ide-
ally one would like a system to be able to reformu-
late a question as a human would if they were to re-
express the question so as to make it independent
of the context of the preceding portion of the ques-
tion series. To support the development of such
systems it would useful if there were a collection
of ?gold standard? reformulated questions against
which systems? outputs could be compared. How-
ever, to the best of our knowledge no such resource
exists.
In this paper we describe the creation of such a
corpus of manually reformulated questions, mea-
sures we have investigated for comparing system
generated reformulations against the gold stan-
dard, and experiments we have carried out com-
paring our TREC system?s automatic question re-
formulator against the gold standard and insights
we have obtained therefrom.
2 The Gold Standard Corpus
Our aim was to take the questions in a TREC
question series and re-express them as questions
that would naturally be asked by a human ask-
ing them as a single, stand-alone question outside
the context of the question series. Our intuition
was that most adult native speakers would agree
on a small number of variant forms these refor-
mulated questions would take. We explored this
intuition by having two persons iteratively refor-
mulate some questions independently, compare re-
sults and evolve a small set of guidelines for the
process.
2.1 Creating the Gold Standard
Ten question sets were randomly selected from
sets available at http://trec.nist.gov/
data/qa/t2007_qadata.html. These were
reformulated separately by two people and results
compared. From this an initial set of guidelines
was drawn up. Using these guidelines another 10
question sets from the TREC 2007 QA set were in-
dependently reformulated and then the guidelines
refined.
At this point the reformulators? outputs were
sufficiently close to each other and the guidelines
sufficiently stable that, given limited resources, it
was decided reformulation could proceed singly.
Using the guidelines, therefore, a further 48 ques-
tion sets from 2007 were reformulated, where
this time each question set was only reformulated
by a single person. Each question set contained
between 5 and 7 individual questions therefore
around 406 questions were reformulated, creating
one or more gold standard forms for each question.
In total there are approximately 448 individual re-
formulations, with a maximum number of 3 refor-
mulations for any single question and a mean of
1.103 reformulations per question.
2.2 Guidelines
Using the above method we derived a set of simple
guidelines which anyone should be able to follow
to create a set of reformulated questions.
Context independence and readability: The
reformulation of questions should be understand-
able outside of the question series context. The re-
formulation should be written as a native speaker
would naturally express it; this means, for exam-
ple, that stop words are included.
Example: ?How many people were killed 1991
59
eruption of Mount Pinatubo?? vs ?How many
people were killed in the 1991 eruption of Mount
Pinatubo?. The latter is preferred as it more read-
able due to the inclusion of stop words ?in the?.
Reformulate questions so as to maximise
search results:
Example: ?Who was William Shakespeare??
vs ?Who was Shakespeare??. William should be
added to the phrase as it adds extra information
which could allow more results to be found.
Target matches a sub-string of the question:
If the target string matches a sub-string of the ques-
tion the target string should substitute the entirety
of the substring. Stop-words should not be used
when determining if strings and target match but
should usually be substituted along with the rest of
the target.
Example: Target: ?Sony Pictures Entertainment
(SPE)?; Question: ?What U.S. company did Sony
purchase to form SPE??; Gold Standard: ?What
U.S. company did Sony purchase to form Sony Pic-
tures Entertainment (SPE)??
Rephrasing: A Question should not be unnec-
essarily rephrased.
Example: Target: ?Nissan Corp?; Question:
?What was Nissan formerly known as??; ?What
was Nissan Corp. formerly known as?? is pre-
ferred over the other possible reformulation ?Nis-
san Corp. was formerly known as what??.
Previous Questions and Answers: Questions
which include a reference to a previous ques-
tion should be reformulated to include a PREVI-
OUS ANSWER variable. Another reformulation
should also be provided should a system know it
needs the answer to the previous question but has
not found one. This should be a reformulation of
the previous question within the current question.
Example: Target: ?Harriet Miers withdraws
nomination to Supreme Court?; Question: ?What
criterion did this person cite in nominating
Miers??; Gold Standard 1: ?What criterion did
PREVIOUS ANSWER cite in nominating Harriet
Miers??; Gold Standard 2: ?What criterion did
this person who nominated Harriet Miers for the
post cite in nominating Harriet Miers??
Targets that contain brackets: Brackets in tar-
get should be dealt with in the following way. The
full target should be substituted into the question
in the correct place as one of the Gold Standards.
The target without the bracketed word and with it
should also be included in the Gold Standard.
Example: Target: ?Church of Jesus Christ
of Latter-day Saints (Mormons)?; Question:?Who
founded the Church of Jesus Christ of Latter-day
Saints??; Gold Standard 1: ?Who founded the
Church of Jesus Christ of Latter-day Saints (Mor-
mons)??; Gold Standard 2: ?Who founded the
Church of Jesus Christ of Latter-day Saints??;
Gold Standard 3 ?Who founded the Mormons??
Stemming and Synonyms: Words should not
be stemmed and synonyms should not be used un-
less they are found in the target or the current ques-
tion series. If they are found then both should be
used in the Gold Standard.
Example: Target: ?Chunnel?; Question:?How
long is the Chunnel??; Gold Standard: ?How long
is the Chunnel??; Incorrect reformulation: ?How
long is the Channel Tunnel??
As the term ?Channel Tunnel? is not referenced
in this section or hard-coded into the QA engine it
cannot be substituted for ?Chunnel?, even though
doing so may increase the probability of finding
the correct answer.
It: The word it should be interpreted as referring
to either the answer of the previous question of that
set or if no answer available to the target itself.
Example:Target: ?1980 Mount St. Helens erup-
tion?; Question: ?How many people died when
it erupted??; Gold Standard: ?How many people
died when Mt. St. Helens? erupted in 1980??
Pronouns (1): If the pronouns he or she are
used within a question and the TARGET is of type
?Person? then substitute the TARGET string for the
pronoun. If however the PREVIOUS ANSWER
is of type ?Person? then it should be substituted in-
stead as in this case the natural interpretation of the
pronoun is to the answer of the previous question.
Example: Target: ?Jay-Z?; Question: ?When
was he born??; Gold Standard: ?When was Jay-Z
born??
Pronouns (2): If the pronouns his/hers/their
are used within a question and the TARGET is of
type ?Person? then substitute the TARGET string
for the pronoun appending the string ??s? to the
end of the substitution. If however the PREVI-
OUS ANSWER is of type ?Person? then it should
be substituted as the natural interpretation of the
pronoun is to the answer of the previous question.
Example: Target: ?Jasper Fforde?; Question:
?What year was his first book written??; Gold
Standard: ?What year was Jasper Fforde?s first
book written??
60
3 Evaluation against the Gold Standard
To assess how close a system?s reformulation of a
question in a questions series is to the gold stan-
dard requires a measure of proximity. Whatever
metric we adopt should have the property that re-
formulations that are closer to our gold standard re-
formulations get a higher score. The closest possi-
ble score is achieved by getting an identical string
to that of the gold standard. Following conven-
tional practice we will adopt a metric that gives us
a value between 0 and 1, where 1 is highest (i.e. a
score of 1 is achieved when the pre-processed re-
formulation and the gold standard are identical).
Another requirement for the metric is that the
ordering of the words in the reformulation is not
as important as the content of the reformulation.
We assume this because one key use for reformu-
lated questions in the retrieval of candidate answer
bearing documents and the presence of key content
terms in a reformulation can help to find answers
when it is used as a query, regardless of their order
Ordering does still need to be taken into account
by the metric but it should alter the score less than
the content words in the reformulation.
Related to this point, is that we would like refor-
mulations that simply append the target onto the
end of the original question to score more highly
on average than the original questions on their
own, since this is a default strategy followed by
many systems that clearly helps in many cases.
These requirement can help to guide metric selec-
tion.
3.1 Choosing a metric
There are many different systems which attempt
to measure string similarity. We considered a va-
riety of tools like ROUGE (Lin, 2004) and ME-
TEOR (Lavie and Agarwal, 2007) but decided they
were unsuitable for this task. ROUGE and ME-
TEOR were developed to compare larger stretches
of text ? they are usually used to compare para-
graphs rather than sentences. We decided develop-
ing our own metric would be simpler than trying to
adapt one of these existing tools.
To explore candidate similarity measures we
created a program which would take as input a list
of reformulations to be assessed and a list of gold
standard reformulations and compare them to each
other using a selection of different string compar-
ison metrics. To find out which of these metrics
best scored reformulations in the way which we
expected, we created a set of test reformulations to
compare against the gold standard reformulations.
Three test data sets were created: one where
the reformulation was simply the original ques-
tion, one where the reformulation included the tar-
get appended to the end, and one where the refor-
mualation was identical to the gold standard. The
idea here was that the without target question set
should score less than the with target question set
and the identical target question set should have a
score of 1 (the highest possible score).
We then had to choose a set of metrics to test and
chose to use metrics from the SimMetrics library
as it is an open source extensible library of string
similarity and distance metrics 2.
3.2 Assessing Metrics
After running the three input files against the met-
rics we could see that certain metrics gave a score
which matched our requirements more closely than
others.
Table 1 shows the metrics used and the mean
scores across the data set for the different question
sets. A description of each of these metrics can be
found in the SimMetrics library.
From these results we can see that certain met-
rics are not appropriate. SmithWaterman, Jaro and
JaroWinkler all do the opposite to what we require
them to do in that they score a reformulation with-
out the target higher than one with the target. This
could be due to over-emphasis on word ordering.
These metrics can therefore be discounted.
Levenshtein, NeedlemanWunch and QGrams-
Distance can also be discounted as the difference
between With target and Without target is not large
enough. It would be difficult to measure improve-
ments in the system if the difference is this small.
MongeElkan can also be discounted as overall its
scores are too large and for this reason it would be
difficult to measure improvements using it.
Of the five remaining metrics ? DiceSimilar-
ity, JaccardSimilarity, BlockDistance, Euclidean-
Distance and CosineSimilarity ? we decided that
we should discount EuclideanDistance as it had the
smallest gap between with target and without tar-
get. We now look at the other four metrics in more
detail3:
2http://www.dcs.shef.ac.uk/
?
sam/
simmetrics.html
3Refer to Manning and Schu?tze (2001) for more details on
these algorithms.
61
Metric Without Target With Target Identical
JaccardSim. 0.798 0.911 1.0
DiceSim. 0.872 0.948 1.0
CosineSim. 0.878 0.949 1.0
BlockDistance 0.869 0.941 1.0
EuclideanDistance 0.902 0.950 1.0
MongeElkan 0.922 0.993 1.0
Levenshtein 0.811 0.795 1.0
NeedlemanWunch 0.830 0.839 1.0
SmithWaterman 0.915 0.859 1.0
QGramsDistance 0.856 0.908 1.0
JaroWinkler 0.855 0.831 0.993
Jaro 0.644 0.589 0.984
Table 1: Mean scores across the data set for each of the different question sets.
3.2.1 Block Distance
Block Distance metric is variously named block
distance, L1 distance or city block distance. It is a
vector-based approach, where q and r are defined
in n-dimensional vector space. The L
1
or block
distance is calculated from summing the edge dis-
tances.
L
1
(q, r) =
?
y
| q(y) ? r(y)|
This can be described in two dimensions with
discrete-valued vectors. When we can picture the
set of points within a grid, the distance value is
simply the number of edges between points that
must be traversed to get from q to r within the grid.
This is the same problem as getting from corner
a to b in a rectilinear street map, hence the name
?city-block metric?.
3.2.2 Dice Similarity
This is based on Dice coefficient which is a term
based similarity measure (0-1) whereby the simi-
larity measure is defined as twice the number of
terms common to compared entities divided by the
total number of terms in both. A coefficient result
of 1 indicates identical vectors while a 0 indicates
orthogonal vectors.
Dice Coefficient =
2 ? |S
1
? S
2
|
|S
1
| + |S
2
|
3.2.3 Jaccard Similarity
This is a token based vector space similarity
measure like the cosine distance. Jaccard Sim-
ilarity uses word sets from the comparison in-
stances to evaluate similarity. The Jaccard mea-
sure penalizes a small number of shared entries
(as a portion of all non-zero entries) more than
the Dice coefficient. Each instance is represented
as a Jaccard vector similarity function. The Jac-
card similarity between two vectors X and Y is
(X ? Y )/(|X||Y | ? (X ? Y )) where (X ? Y ) is the
inner product of X and Y , and |X| = (X ? X)1/2,
i.e. the Euclidean norm of X. This can more easily
be described as (|X ? Y |)/(|X ? Y |)
3.2.4 Cosine similarity
This is a common vector based similarity mea-
sure similar to the Dice Coefficient. The input
string is transformed into vector space so that the
Euclidean cosine rule can be used to determine
similarity. The cosine similarity is often paired
with other approaches to limit the dimensionality
of the problem. For instance with simple strings a
list of stopwords is used to reduce the dimension-
ality of the comparison. In theory this problem has
as many dimensions as terms exist.
cos(q, r) =
?
y
q(y)r(y)
?
?
y
q(y)
2
?
y
r(y)
2
3.3 Using bigrams and trigrams
All four of these measures appear to value the con-
tent of the strings higher than ordering which is
what we want our metric to do. However the scores
are quite large, and as a result we considered refin-
ing the metrics to give scores that are not as close
to 1. To do this we decided to try and increase the
importance of ordering by also taking into account
shared bigrams and trigrams. As we do not want
ordering to be too important in our metric we intro-
duced a weighting mechanism into the program to
62
Metric Without Target With Target ?Gap
Dice 0.872 0.948 +0.076
Cosine 0.878 0.949 +0.071
Jaccard 0.798 0.911 +0.113
Block 0.869 0.941 +0.072
Table 2: Results for Unigram weighting
Metric Without Target With Target ?Gap
Dice 0.783 0.814 -3.6
Cosine 0.789 0.816 -3.5
Jaccard 0.698 0.748 -5.5
Block 0.782 0.811 -3.5
Table 3: U:1, B:1, T:0
allow us to used a weighted combination of shared
unigrams, bigrams and trigrams.
The results for just unigram weighting is shown
in Table 2.
We began by testing the metrics by introduc-
ing just bigrams to give us an idea of what effect
they would have. A weight ratio of U:1, B:1, T:0
was used (where U:unigram, B:bigram, T:trigram).
The results are shown in Table 3.
The ? Gap column is the increase in the differ-
ence between Without Target and With Target from
the first test run which used only unigrams.
The introduction of bigrams decreases the gap
between Without Target and With Target. It also
lowers the scores which is good as it is then eas-
ier to distinguish between perfect reformulations
and reformulations which are close but not perfect.
This means that the introduction of bigrams is al-
ways going to decrease a system?s ability to dis-
tinguish between Without Target and With Target.
We had to now find the lowest decrease in this gap
whilst still lowering the score of the with target re-
sult.
From the results of the bigrams we expected that
the introduction of trigrams would further decrease
the gap (U : 1, B : 1, T : 1). The results proved
Metric Without Target With Target ?Gap
Dice 0.725 0.735 -6.4
Cosine 0.730 0.735 -6.3
Jaccard 0.639 0.663 -9.0
Block 0.724 0.733 -6.1
Table 4: U:1, B:1, T:1
Metric Without Target With Target ?Gap
Dice 0.754 0.770 -4.8
Cosine 0.759 0.771 -4.9
Jaccard 0.664 0.694 -7.4
Block 0.753 0.767 -4.6
Table 5: U:1, B:2
Metric Without Target With Target ?Gap
Dice 0.813 0.859 -2.4
Cosine 0.819 0.860 -2.4
Jaccard 0.731 0.802 -3.7
Block 0.811 0.854 -2.2
Table 6: U:2, B:1
this and are shown in Table 4.
The introduction of trigrams has caused the gaps
to significantly drop. It has also lowered the scores
too much. From this evidence we decided trigrams
are not appropriate to use to refine these metrics.
We now had to try and find the best weighting
of unigram to bigram that would lower the With
Target score from 1.0 whilst still keeping the gap
between Without Target and With Target high.
We would expect that further increasing the bi-
gram weighting would further decrease the gap
and the With Target score. The results in Table 5
show this to be the case. However this has de-
creased the gap too much. The next step was to
look at decreasing the weighting of the bigrams.
Table 6 shows that the gap has decreased slightly
but the With Target score has decreased by around
10% on average. The Jaccard score for this run is
particularly good as it has a good gap and is not
too close to 1.0. The Without Target is also quite
low which is what we want.
U : 2, B : 1 is currently the best weighting
found with the best metric being Jaccard. Fur-
ther work in this area could be directed at further
modifying these weightings using machine learn-
ing techniques to refine the weightings using linear
regression.
4 Our system against the Metric
Our current pre-processing system takes a question
and its target and looks to replace pronouns like
?he?, ?she? and certain definite nominals with the
target and also to replace parts of the target with
the full target (Gaizauskas et al, 2005). Given
our choice of metric we would hope that this strat-
63
Figure 2: Graph of Jaccard score distribution
egy gets a better score than just adding the target
on the end, as the ordering of the words is also
taken into account by our pre-processing as it tries
to achieve natural reformulations like those of our
gold standard. We would therefore expect that it
achieves at least the same score as adding the target
on the end, which is its default strategy when no
co-reference can be determined, though of course
incorrect coreference resolutions will have a neg-
ative effect. One of the aims of creating the gold
standard and a comparison metric was to quickly
identify whether strategies such as ours are work-
ing and if not where not.
A subset of the gold standard was preprocessed
by our system then compared against the results
of doing no reformulation and of reformulating by
simply appending the target.
Tables 7 and 8 shows how our system did in
comparison. Diff shows the difference between
WithTarget and Our System. Table 7 is results for
weighting U : 1, B : 0, T : 0, Table 8 is results for
U : 2, B : 1, T : 0.
Our system does do better than just adding the
target on the end, and this difference is exaggerated
(Table 8) when bigrams are taken into account, as
expected since this weighting increases the met-
ric?s sensitivity to recognising our system?s ability
to put the target in the correct place.
Mean scores across a data set tell part of the
story, but to gain more insight we need to exam-
ine the distribution of scores and then, in order to
improve the system, we need to look at questions
which have a low score and work out what has
gone wrong. Figure 2 shows the distribution of
Jaccard scores across the test data set. Looking at
the scores from the data set using the U:2,B:1,T:0
weighting we find that the minimum Jaccard score
was 0.44 and was for the following example:
Metric Score
Dice 0.574
Cosine 0.578
Jaccard 0.441
Block 0.574
Table 9: Finding Bad Reformulations
Target: ?Hindenburg disaster?; Question:
?How many of them were killed?; Our System:
?How many of Hindenburg disaster were killed?;
Gold Standard: ?How many people were killed
during the Hindenburg disaster?.
The results of comparing our system with the
gold standard for this question for all four metrics
are shown in Table 9.
The problem here is that our system has wrongly
replaced the term ?them? with the target when in
fact its antecedent was in the previous question
in the series How many people were on board?.
Once again the low score has helped us to quickly
identify a problem: the system is only interpret-
ing pronouns as references to the target, which is
clearly insufficient. Furthermore should the pre-
processing system be altered to address a problem
like this the gold system and scoring software can
be used for regression testing to ensure no previ-
ously correct reformulations have been lost.
Another example of a poor scoring reformula-
tion is:
Target: ?Hindenburg disaster?; Question:
?What type of craft was the Hindenburg?; Our
System: ?What type of craft was the Hindenburg
disaster?; Gold Standard: ?What type of craft was
the Hindenburg?.
For this example Jaccard gave our system refor-
mulation a score of 0.61. The problem here is our
system blindly expanded a substring of the target
appearing in the question to the full target without
recognizing that in this case the substring is not an
abbreviated reference to the target (an event) but to
an entity that figured in the event.
5 Conclusions and Future Work
In this paper we have presented a Gold Standard
for question reformulation and an associated set of
guidelines which can be used to reformulate other
questions in a similar fashion. We then evaluated
metrics which can be used to assess the effective-
ness of the reformulations and validated the whole
approach by showing how it could be used to help
64
Metric Without Target With Target Our System Diff
Dice 0.776 0.901 0.931 +3.1
Cosine 0.786 0.904 0.936 +3.1
Jaccard 0.657 0.834 0.890 +5.5
Block 0.772 0.888 0.920 +4.2
Table 7: How our system compared, U:1,B:0,T:0
Metric Without Target With Target Our System Diff
Dice 0.702 0.819 0.889 +8.7
Cosine 0.742 0.822 0.893 +9.2
Jaccard 0.616 0.738 0.839 +12.3
Block 0.732 0.812 0.884 +9.1
Table 8: How our system compared, U:2,B:1,T:0
improve the question pre-processing component of
a QA system.
Further work will aim to expand the Gold Stan-
dard to at least 1000 questions, refining the guide-
lines as required. The eventual goal is to incor-
porate the approach into an evaluation tool such
that a developer would have a convenient way
of evaluating any question reformulation strategy
against a large gold standard. Of course one also
needs to develop methods for observing and mea-
suring the effect of question reformulation within
question pre-processing upon the performance of
downstream components in the QA system, such
as document retrieval.
References
Burger, J., C. Cardie, V. Chaudhri, R. Gaizauskas,
S. Harabagiu, D. Israel, C. Jacquemin, C-Y.
Lin, S. Maiorano, G. Miller, D. Moldovan,
B. Ogden, J. Prager, E. Riloff, A. Singhal,
R. Shrihari, T. Strzalkowski, E. Voorhees, and
R. Weischedel. 2002. Issues, tasks and pro-
gram structures to roadmap research in question
& answering (q&a). Technical report. www-
nlpir.nist.gov/projects/duc/papers/qa.Roadmap-
paper v2.doc.
Gaizauskas, Robert, Mark A. Greenwood, Mark Hep-
ple, Henk Harkemaa, Horacio Saggion, and Atheesh
Sanka. 2005. The University of Sheffield?s TREC
2005 Q&A Experiments. In Proceedings of the 14th
Text REtrieval Conference.
Lavie, Alon and Abhaya Agarwal. 2007. METEOR:
An automatic metric for MT evaluation with high
levels of correlation with human judgments. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 228?231, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Lin, Chin-Yew. 2004. Rouge: A package for
automatic evaluation of summaries. In Marie-
Francine Moens, Stan Szpakowicz, editor, Text Sum-
marization Branches Out: Proceedings of the ACL-
04 Workshop, pages 74?81, Barcelona, Spain, July.
Association for Computational Linguistics.
Manning, Christopher D. and Hinrich Schu?tze. 2001.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press.
Voorhees, E. 2005. Overview of the TREC 2004
question answering track. In Proceedings of the
Thirteenth Text Retrieval Conference (TREC 2004).
NIST Special Publication 500-261.
65
Proceedings of the 8th International Natural Language Generation Conference, pages 54?63,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
A Hybrid Approach to Multi-document Summarization of
Opinions in Reviews
Giuseppe Di Fabbrizio
Amazon.com?
Cambridge, MA - USA
pino@difabbrizio.com
Amanda J. Stent
Yahoo! Labs
New York, NY - USA
stent@labs.yahoo.com
Robert Gaizauskas
Department of Computer Science
University of Sheffield, Sheffield - UK
R.Gaizauskas@sheffield.ac.uk
Abstract
We present a hybrid method to gener-
ate summaries of product and services re-
views by combining natural language gen-
eration and salient sentence selection tech-
niques. Our system, STARLET-H, re-
ceives as input textual reviews with asso-
ciated rated topics, and produces as out-
put a natural language document summa-
rizing the opinions expressed in the re-
views. STARLET-H operates as a hybrid
abstractive/extractive summarizer: using
extractive summarization techniques, it se-
lects salient quotes from the input reviews
and embeds them into an automatically
generated abstractive summary to provide
evidence for, exemplify or justify posi-
tive or negative opinions. We demon-
strate that, compared to extractive meth-
ods, summaries generated with abstractive
and hybrid summarization approaches are
more readable and compact.
1 Introduction
Text summarization is a well-established area of
research. Many approaches are extractive, that
is, they select and stitch together pieces of text
from the input documents (Goldstein et al., 2000;
Radev et al., 2004). Other approaches are abstrac-
tive; they use natural language generation (NLG)
techniques to paraphrase and condense the con-
tent of the input documents (Radev and McKeown,
1998). Most summarization methods focus on dis-
tilling factual information by identifying the in-
put documents? main topics, removing redundan-
cies, and coherently ordering extracted phrases or
sentences. Summarization of sentiment-laden text
(e.g., product or service reviews) is substantially
different from the traditional text summarization
task: instead of presenting facts, the summarizer
must present the range of opinions and the con-
sensus opinion (if any), and instead of focusing
on one topic, the summarizer must present infor-
mation about multiple aspects of the target entity.
?This work was conducted when in AT&T Labs Research
In addition, traditional summarization techniques
discard redundancies, while for summarization of
sentiment-laden text, similar opinions mentioned
multiple times across documents are crucial indi-
cators of the overall strength of the sentiments ex-
pressed by the writers (Ku et al., 2006).
Extractive summaries are linguistically interest-
ing and can be both informative and concise. Ex-
tractive summarizers also require less engineer-
ing effort. On the other hand, abstractive sum-
maries tend to have better coverage for a particular
level of conciseness, and to be less redundant and
more coherent (Carenini et al., 2012). They also
can be constructed to target particular discourse
goals, such as summarization, comparison or rec-
ommendation. Although in theory, it is possible to
produce user-targeted extractive summaries, user-
specific review summarization has only been ex-
plored in the context of abstractive summarization
(Carenini et al., 2012).
Current systems for summarizing sentiment-
laden text use information about the attributes of
the target entity (or entities); the range, mean
and median of the ratings of each attribute; re-
lationships between the attributes; and links be-
tween ratings/attributes and text elements in the
input documents (Blair-Goldensohn et al., 2008).
However, there is other information that no sum-
marizer currently takes into account. This in-
cludes temporal features (in particular, depending
on how old the documents are, products and ser-
vices evaluated features may change over time)
and social features (in particular, social or demo-
graphic similarities or relationships between doc-
ument authors and the reader of the summary).
In addition, there is an essential contradiction at
the heart of current review summarization sys-
tems: the system is authoring the review, but the
opinions contained therein are really attributable
to one or more human authors, and those attribu-
tions are not retained in the review summary. For
example, consider the extractive summary gener-
ated with STARLET-E (Di Fabbrizio et al., 2013):
?Delicious. Can?t wait for my next trip to Buffalo.
GREAT WINGS. I have rearranged business trips
54
so that I could stop in and have a helping or two
of their wings?. We were seated promptly and the
staff was courteous.
The summary is generated by selecting sen-
tences from reviews to reflect topics and rating dis-
tributions contained in the input review set. Do the
two sentences about wings reflect one (repeated)
opinion from a single reviewer, or two opinions
from two separate reviewers? The ability to at-
tribute subjective statements to known sources can
make them more trustworthy; conversely, in the
absence of the ability to attribute, a reader may
become skeptical or confused about the content of
the review summary. We term this summarization
issue opinion holder attribution.
In this paper we present STARLET-H, a hybrid
review summarizer that combines the advantages
of the abstractive and extractive approaches to
summarization and implements a solution to the
opinion holder attribution problem. STARLET-H
takes as input a set of reviews, each review of
which is labeled with aspect ratings and author-
ship. It generates hybrid abstractive/extractive re-
views that: 1) are informative (achieve broad cov-
erage of the input opinions); 2) are concise and
avoid redundancy; 3) are readable and coherent (of
high linguistic quality); 4) can be targeted to the
reader; and 5) address the opinion holder attribu-
tion problem by directly referring to reviewers au-
thorship when embedding phrases from reviews.
We demonstrate through a comparative evalua-
tion of STARLET-H and other review summariz-
ers that hybrid review summarization is preferred
over extractive summarization for readability, cor-
rectness, completeness (achieving broad coverage
of the input opinions) and compactness.
2 Hybrid summarization
Most NLG research has converged around a ?con-
sensus architecture? (Reiter, 1994; Rambow and
Korelsky, 1992), a pipeline architecture including
the following modules: 1) text planning, which
determines how the presentation content is se-
lected, structured, and ordered; 2) sentence plan-
ning, which assigns content to sentences, inserts
discourse cues to communicate the structure of
the presentation, and performs sentence aggrega-
tion and optionally referring expression genera-
tion; and 3) surface realization, which performs
lexical selection, resolves syntactic issues such as
subject-verb and noun-determiner agreement, and
assigns morphological inflection to produce the fi-
nal grammatical sentence. An abstractive sum-
marizer requires the customization of these three
modules. Specifically, the text planner has to se-
lect and organize the information contained in the
input reviews to reflect the rating distributions over
the aspects discussed by the reviewers. The sen-
tence planner must perform aggregation in such a
way as to optimize summary length without con-
fusing the reader, and insert discourse cues that
reveal the discourse structure underlying the sum-
mary. And, finally, the surface realizer must select
the proper domain lexemes to express positive and
negative opinions.
Figure 1: STARLET-H hybrid review summarizer
architecture
Figure 1 shows the architecture we adopted for
our STARLET-H hybrid review summarizer. We
use a generate-and-select approach: the decisions
to be made at each stage of the NLG process just
outlined are complex, and because they are not
truly independent of each other, a generate-and-
rank approach may be best (allowing each com-
ponent to express alternative ?good? choices and
choosing the best combination of these choices
at the end). Our text planner is responsible for
analyzing the input text reviews, extracting per-
attribute rating distributions and other meta-data
from each review, and synthesizing this informa-
tion to produce one or more discourse plans. Our
sentence planner, JSPARKY ? a freely-available
toolkit (Stent and Molina, 2009) ? can produce
several candidate sentence plans and their corre-
sponding surface realizations through SimpleNLG
(Gatt and Reiter, 2009). The candidate summaries
are ranked by calculating their perplexity with a
language model trained over a large number of
sentences from additional restaurant reviews col-
lected over the Web.
2.1 Data
STARLET-H uses review data directly, as input
to summarization, and indirectly, as training data
for statistical models and for lexicons for various
stages of the summarization process.
For training data, we used two sets of la-
beled data: one for the restaurant domain and
the other for the hotel domain. Both corpora in-
clude manually created sentence-level annotations
55
that identify: 1) opinion targets ? phrases refer-
ring to domain-relevant aspects that are the tar-
gets of opinions expressed by the reviewer; 2)
opinion phrases ? phrases expressing an opinion
about an entity, and its polarity (positive or neg-
ative); and 3) opinion groups ? links between
opinion phrases and their opinion targets. Ad-
ditionally, sentences satisfying the properties of
quotable sentence mentioned in Section 3 were la-
beled as ?quotable?. Table 1 summarizes the over-
all statistics of the two corpora. The annotated cor-
pora included the following rated aspects: Atmo-
sphere, Food, Service, Value, and Overall for the
Restaurant domain, and Location, Rooms, Service,
Value, and Overall for the Hotel domain1.
Table 1: Quote-annotated dataset statistics
Dataset RQ4000 HQ4000
Domain Restaurant Hotel Total
Reviews 484 404 888
Sentences 4,007 4,013 8,020
Avg sentences / review 8.28 9.93 9.03
2.2 Text planning
Reviews present highly structured information:
each contains an (implicit or explicit) rating of one
or more aspects of a target entity, possibly with
justification or evidence in the form of examples.
The rich information represented in these ratings
? either directly expressed in reviews or extracted
by an automatic rating prediction model ? can be
exploited in several ways. Our text planner re-
ceives as input a set of text reviews with associated
per-aspect ratings, and for each review proceeds
through the following analysis steps:
Entity description Extracts basic information
to describe the reviewed entity, e.g., the name and
location of the business, number of total and recent
reviews, review dates and authors, etc.
Aspect distribution categorization Catego-
rizes the rating distribution for each aspect of the
reviewed entity as one of four types: 1) positive
? most of the ratings are positive; 2) negative ?
most of the ratings are negative; 3) bimodal ?
most of the ratings are equally distributed into
positive and negative values; 4) uniform ? ratings
are uniformly distributed across the rating scale.
1Some examples from the annotated corpus are avail-
able at the following address http://s286209735.
onlinehome.us/starlet/examples
Quote selection and attribution Classifies each
sentence from the reviews using a quote selec-
tion model (see Section 3), which assigns to
each sentence an aspect, a rating polarity (posi-
tive/negative) and a confidence score. The classi-
fied sentences are sorted by confidence score and
a candidate quote is selected for each aspect of the
target entity that is explicitly mentioned in the in-
put reviews. Each quote is stored with the name
of the reviewer for correct authorship attribution.
Note that when the quote selection module is ex-
cluded, the system is an abstractive summarizer,
which we call STARLET-A.
Lexical selection Selects a lexicon for each as-
pect based on its rating polarity and its assigned
rating distribution type. Lexicons are extracted
from the corpus of annotated opinion phrases de-
scribed in Di Fabbrizio et al. (2011).
Aspect ordering Assigns an order over aspects
using aspect ordering statistics from our training
data (see Section 2.4), and generates a discourse
plan, using a small set of rhetorical relations orga-
nized into summary templates (see below).
2.3 Sentence planning
The STARLET-H sentence planner relies on rhetor-
ical structure theory (RST) (Mann and Thomp-
son, 1989). RST is a linguistic framework that
describes the structure of natural language text
in terms of the rhetorical relationships organizing
textual units. Through a manual inspection of our
training data, we identified a subset of six RST re-
lations that are relevant to review summarization:
concession, contrast, example, justify, list, and
summary. We further identified four basic RST-
based summary templates, one for each per-aspect
rating distribution: mostly positive, mostly nega-
tive, uniform across all ratings, and bimodal (e.g.,
both positive and negative). These summary tem-
plates are composed by the text planner to build
summary discourse plans. The JSPARKY sen-
tence planner then converts input discourse plans
into sentence plans, performing sentence order-
ing, sentence aggregation, cross-sentence refer-
ence resolution, sentence tense and mode (passive
or active), discourse cue insertion, and the selec-
tion of some lexical forms from FrameNet (Baker
et al., 1998) relations.
Figure 2 illustrates a typical RST template rep-
resenting a positive review summary and corre-
sponding text output generated by JSPARKY. For
each aspect of the considered domain, the sentence
plan strategy covers a variety of opinion distribu-
56
Figure 2: Example of RST structure generated by the text planner for mostly positive restaurant reviews
tion conditions (e.g., positive, negative, bimodal,
and uniform), and provides alternative RST struc-
tures when the default relation is missing due to
lack of data (e.g., missing quotes for a specific as-
pect, missing information about review distribu-
tion over time, missing type of cuisine, and so on).
The sentence template can also manage lexical
variations by generating multiple options to qual-
ify a specific pair of aspect and opinion polarity.
For instance, in case of very positive reviews about
restaurant atmosphere, it can provide few alterna-
tive adjective phrases (e.g., great, wonderful, very
warm, terrific, etc.) that can be used to produce
more summary candidates (over-generate) during
the final surface realization stage.
2.4 Ordering aspects and polarities
The discourse structure of a typical review consists
of a summary opinion, followed by a sequence
of per-aspect ratings with supporting information
(e.g., evidence, justification, examples, and con-
cessions). The preferred sequence of aspects to
present in a summary depends on the specific re-
view domain, the overall polarity of the reviews,
and how opinion polarity is distributed across the
reviewed aspects. Looking at our training data, we
observed that when the review is overall positive,
positively-rated aspects are typically discussed at
the beginning, while negatively-rated aspects tend
to gather toward the end. The opposite order
seems predominant in the case of negative re-
views. When opinions are mixed, aspect ordering
strategies are unclear. To most accurately model
aspect ordering, we trained weighted finite state
transducers for the restaurant and hotel domains
using our training data. Weighted finite state
transducers (WFSTs) are an elegant approach to
search large feature spaces and find optimal paths
by using well-defined algebraic operations (Mohri
et al., 1996). To find the optimal ordering of rated
aspects in a domain, the text planner creates a
WFST with all the possible permutations of the
input sequence of aspects, and composes it with a
larger WFST trained from bigram sequences of as-
pects extracted from the relevant domain-specific
review corpus. The best path sequence is then de-
rived from the composed WFST by applying the
Viterbi decoding algorithm. For instance, the se-
quence of aspects and polarities represented by the
string: value-n service-p overall-n food-n
atmosphere-n2 is first permuted in all the dif-
ferent possible sequences and then converted into
a WFST. Then the permutation network is fully
composed with the larger, corpus-trained WFST.
The best path is extracted by dynamic program-
ming, producing the optimal sequence service-p
value-n overall-n atmosphere-n food-n.
2We postfix the aspect label with a ?-p? for positive and
with ?-n? for negative opinion
57
2.5 Lexical choice
It can be hard to choose the best opinion words,
especially when the summary must convey the
different nuances between ?good? and ?great? or
?bad? and ?terrible? for a particular aspect in a
particular domain. For our summarization task,
we adopted a simple approach. From our anno-
tated corpora, we mined both positive and negative
opinion phrases with their associated aspects and
rating polarities. We sorted the opinion phrases
by frequency and then manually selected from the
most likely phrases adjective phrases that may cor-
rectly express per-aspect polarities. We then split
positive and negative phrases into two levels of
polarity (i.e., strongly positive, weakly positive,
weakly negative, strongly negative) and use the
number of star ratings to select the right polarity
during content planning. For bimodal and uniform
polarity distributions, we manually defined a cus-
tomized set of terms. Sample lexical terms are re-
ported in Table 2.
3 Quote selection modeling
There are several techniques to extract salient
phrases from text, often related to summariza-
tion problems, but there is a relatively little work
on extracting quotable sentences from text (Sar-
mento and Nunes, 2009; De La Clergerie et al.,
2009) and none, to our knowledge, on extract-
ing quotes from sentiment-latent text. So, what
does make a phrase quotable? What is a proper
quote definition that applies to review summa-
rization? We define a sentiment-laden quotable
phrase as a text fragment with the following char-
acteristics: attributable ? clearly ascribable to the
author; compact and simple ? it is typically a
relatively short phrase (between two and twenty
words) which contains a statement with a simple
syntactic structure and independent clauses; self-
contained its meaning is clear and self-contained,
e.g., it does not include pronominal references to
entities outside its scope; on-topic ? it refers to
opinion targets (i.e., aspects) in a specific domain;
sentiment-laden ? it has one or two opinion tar-
gets and an unambiguous overall polarity. Exam-
ple quotable phrases are presented in Table 3.
To automatically detect quotes from reviews,
we adopted a supervised machine learning ap-
proach based on manually labeled data. The clas-
sification task consists of classifying both aspects
and polarity for the most frequent aspects defined
for each domain. Quotes for the aspect food, for
instance, are split into positive and negative classi-
Table 3: Example of quotes from restaurant and
hotel domains
?Everyone goes out of their way to make sure you
are happy with their service and food.?
?The stuffed mushrooms are the best I?ve ever had
as was the lasagna.?
?Service is friendly and attentive even during
the morning rush.?
?I?ve never slept so well away from home loved
the comfortable beds.?
?The price is high for substandard mattresses
when I pay this much for a room.?
fication labels: food-p and food-n, respectively.
We identify quotable phrases and associate them
with aspects and rating polarities all in one step,
but multi-step approaches could also be used (e.g.,
a configuration with binary classification to detect
quotable sentences followed by another classifica-
tion model for aspect and polarity detection).
3.1 Training quote selection models
We used the following features for automatic
quote selection: ngrams ? unigrams, bigrams, and
trigrams from the input phrases with frequency
higher than three; binned number of words ?
we assumed a maximum length of twenty words
per sentence and created six bins, five of them
uniformly distributed from one to twenty, and the
sixth including all the sentences of length greater
than twenty words; POS ? unigrams, bigrams, and
trigrams for part of speech tags; chunks ? uni-
grams, bigrams, and trigrams for shallow parsed
syntactic chunks; opinion phrases ? a binary fea-
ture to keep track of the presence of positive and
negative opinion phrases as defined in our anno-
tated review corpora. In our annotated data only
the most popular aspects are well represented.
For instance, food-p and overall-p are the most
popular positive aspects among the quotable sen-
tences for the restaurant domain, while quotes on
atmosphere-n and value-n are scarce. The dis-
tribution is even further skewed for the hotel do-
main; there are plenty of quotes for overall-p
and service-p and only 13 samples (0.43%) for
location-n. To compensate for the broad vari-
ation in the sample population, we used stratified
sampling methods to divide the data into more bal-
anced testing and training data We generated 10-
fold stratified training/test sets. We experimented
with three machine learning algorithms: MaxEnt,
SVMs with linear kernels, and SVMs with poly-
nomial kernels. The MaxEnt learning algorithm
produced statistically better classification results
than the other algorithms when used with uni-
58
Table 2: Summarizer lexicon for most frequent adjective phrases by aspect and polarity
Domain Restaurant Hotel
Aspect positive very positive negative very negative Aspect positive very positive negative very negative
atmosphere nice, good,
friendly, com-
fortable
great, wonder-
ful, very warm,
terrific
ordinary,
depressing
really bad location good, nice,
pleasant
amazing,
awesome,
excellent, great
bad, noisy,
gloomy
very bad, very
bleak, very
gloomy
food good, deli-
cious, pleasant,
nice, hearty,
enjoyable
great, ex-
cellent, very
good, to die
for, incredible
very basic, un-
original, unin-
teresting, unac-
ceptable, sub-
standard, poor
mediocre, ter-
rible, horrible,
absolutely hor-
rible
rooms comfortable,
decent, clean,
good
amazing,
awesome,
gorgeous
average, basic,
subpar
terrible, very
limited, very
average
overall good, quite en-
joyable, lovely
wonderful, ter-
rific, very nice
bad, unremark-
able, not so
good
absolutely ter-
rible, horrible,
pretty bad
overall great, nice,
welcoming
excellent,
superb, perfect
average, noth-
ing great, noisy
quite bad, aw-
ful, horrible
service attentive,
friendly, pleas-
ant, courteous
very atten-
tive, great,
excellent, very
friendly
inattentive,
poor, not
friendly, bad
extremely
poor, horrible,
so lousy, awful
service friendly, great,
nice, helpful,
good
very friendly,
great, ex-
cellent, very
nice
average, basic,
not that great
very bad,
dreadful
value reasonable,
fair, good
value
very reason-
able, great
not that good,
not worthy
terrible, outra-
geous
value great, nice,
good, decent
very good,
wonderful,
perfectly good
not good not very good
gram features. This confirmed a general trend we
have previously observed in other text classifica-
tion experiments: with relatively small and noisy
datasets, unigram features provide better discrimi-
native power than sparse bigrams or trigrams, and
MaxEnt methods are more robust when dealing
with noisy data.
3.2 Quote selection results
Table 4 reports precision, recall and F-measures
averaged across 10-fold cross-validated test sets
with relative standard deviation. The label nq
identifies non-quotable sentences, while the other
labels refer to the domain-specific aspects and
their polarities. For the quote selection task, pre-
cision is the most important metric: missing some
potential candidates is less important than incor-
rectly identifying the polarity of a quote or sub-
stituting one aspect with another. The text planner
in STARLET-H further prunes the quotable phrases
by considering only the quote candidates with the
highest scores.
4 Evaluation
Evaluating an abstractive review summarizer in-
volves measuring how accurately the opinion con-
tent present in the reviews is reflected in the sum-
mary and how understandable the generated con-
tent is to the reader. Traditional multi-document
summarization evaluation techniques utilize both
qualitative and quantitative metrics. The former
require human subjects to rate different evaluative
characteristics on a Likert-like scale, while the lat-
ter relies on automatic metrics such as ROUGE
(Lin, 2004), which is based on the common num-
ber of n-grams between a peer, and one or several
gold-standard reference summaries.
Table 4: Quote, aspect, and polarity classification
performances for the restaurant domain
Precision Recall F-measure
atmosphere-n 0.233 0.080 0.115
atmosphere-p 0.589 0.409 0.475
food-n 0.634 0.409 0.491
food-p 0.592 0.634 0.612
nq 0.672 0.822 0.740
overall-n 0.545 0.275 0.343
overall-p 0.555 0.491 0.518
service-n 0.699 0.393 0.498
service-p 0.716 0.563 0.626
value-n 0.100 0.033 0.050
value-p 0.437 0.225 0.286
Hotel Precision Recall F-measure
location-n - - -
location-p 0.572 0.410 0.465
nq 0.678 0.836 0.748
overall-n 0.517 0.233 0.305
overall-p 0.590 0.492 0.536
rooms-n 0.628 0.330 0.403
rooms-p 0.667 0.573 0.612
service-n 0.517 0.163 0.240
service-p 0.605 0.500 0.543
value-n - - -
value-p 0.743 0.300 0.401
4.1 Evaluation materials
To evaluate our abstractive summarizer, we used
a qualitative metric approach and compared four
review summarizers: 1) the open source MEAD
system, designed for extractive summarization of
general text (Radev et al., 2004); 2) STARLET-E,
an extractive summarizer based on KL-divergence
and language modeling features that is described
in Di Fabbrizio et al. (2011); 3) STARLET-A, the
abstractive summarizer presented in this paper,
without the quote selection module; and 4) the hy-
brid summarizer STARLET-H.
We used the Amazon Mechanical Turk3 crowd-
3http://www.mturk.com
59
sourcing system to post subjective evaluation
tasks, or HITs, for 20 restaurant summaries. Each
HIT consists of a set of ten randomly ordered re-
views for one restaurant, and four randomly or-
dered summaries of reviews for that restaurant,
each one accompanied by a set of evaluation wid-
gets for the different evaluation metrics described
below. To minimize reading order bias, both re-
views and summaries were shuffled each time a
task was presented.
4.2 Evaluation metrics
We chose to carry out a qualitative evaluation
in the first instance as n-gram metrics, such as
ROUGE, are not necessarily appropriate for as-
sessing abstractive summaries. We asked each par-
ticipant to evaluate each summary by rating (using
a Likert scale with the following rating values: 1)
Not at all; 2) Not very; 3) Somewhat; 4) Very; 5)
Absolutely) the following four summary criteria:
readability ? a summary is readable if it is easy to
read and understand; correctness ? a summary is
correct if it expresses the opinions in the reviews;
completeness ? a summary is complete if it cap-
tures the whole range of opinions in the reviews;
compactness ? a summary is compact if it does
not repeat information.
4.3 Evaluation procedure
We requested five evaluators for each HIT. To in-
crease the chances of getting accurate evaluations,
we required evaluators to be located in the USA
and have an approval rate of 90% or higher (i.e.,
have a history of 90% or more approved HITs).
Manual examinations of the evaluation responses
did not show evidence of tampered data, but statis-
tical analysis showed unusually widely spread rat-
ing ranges. We noticed that most evaluators only
evaluated one or two HITs; this may imply that
they tried a few HITs and then decided not to con-
tinue because they found the task too long or the
instructions unclear. We then re-opened the evalu-
ation and directly contacted three additional eval-
uators, explaining in detail the instructions and the
evaluation scales. For consistency, we asked these
evaluators to complete the evaluation for all HITs.
In our analysis, we only included the five evalu-
ators (two from the first round of evaluation, and
three from the second) who completed all HITs.
For each evaluation metric, the five workers eval-
uated each of the 20 summaries, for a total of 100
ratings. Table 5 shows an example output of the
four summarization methods for a single set of
restaurant review documents.
Table 5: Example of MEAD-based, extractive, ab-
stractive and hybrid summaries from the restaurant
domain
MEAD Summary
a truly fun resturant everyone who like spicy
food should try the rattoes and for a mixed drink
the worm burner really good food and a fun place
to meet your friends. We were attracted by the
great big frog on the exterior of the building
and the fun RAZZOO S logo during a trip to the
mall. it was great the waitress was excellent
very prompt and courteous and friendly to all a
real complement to razzoo ?s way of service her
name was Tabitha. The best spicy food restaurant
with great server and fast service.
Extractive summary
Eat there every chance i get. We ve been going
here for years. Their crawfish etoufee is the
BEST. And such an awesome value for under 10.
Excellent as always. Some of the best food in
the area. I use to work at Razzoo s. It was
hard to leave. The people are great and so is
the food. I still go in there and miss it more
everytime. I Love Loney. It was great. Our
server was great and very observant. Try the
Chicken Tchoupitoulas.
Abstractive summary
Razzoo?s Cajun Cafe in Concord, NC is an American
restaurant. It has nine reviews. It had three
very recent reviews. It is an awesome, American
restaurant. It has many very positive reviews.
It has an excellent atmosphere and and has always
exceptional service.
Hybrid summary
Razzoo?s Cajun Cafe in Concord, NC is an American
restaurant. It has nine reviews. It had three
very recent reviews. It is an awesome, American
restaurant. It has many very positive reviews.
First it has a great price. Angela Haithcock
says ??And such an awesome value for under 10??.
Second it has always exceptional service and for
instance Danny Benson says ??it was great the
waitress was excellent very prompt and courteous
and friendly to all a real complement to razzoo?s
way of service her name was Tabitha??. Third it
has an excellent atmosphere. Last it has amazing
food. Scott Kern says ??Some of the best food in
the area??.
4.4 Evaluation results and discussion
The evaluation results are presented in Table 6.
Each evaluation metric is considered separately.
Average values for STARLET-E, STARLET-A and
STARLET-H are better than for MEAD across the
board, suggesting a preference for summaries of
sentiment-laden text that take opinion into ac-
count. To validate this hypothesis, we first com-
puted the non-parametric Kruskal-Wallis statistic
for each evaluation metric, using a chi-square test
to establish significance. The results were not sig-
nificant for any of the metrics.
However, when we conducted pairwise
Wilcoxon signed-rank tests considering two
summarization methods at a time, we found some
significant differences (p < 0.05). As predicted,
60
Table 6: Qualitative evaluation results
MEAD Starlet-E Starlet-A Starlet-H
Readability 2.95 3.17 3.64 3.74
Completeness 2.88 3.29 3.290 3.58
Compactness 3.07 3.35 3.80 3.58
Correctness 3.26 3.48 3.59 3.72
MEAD perform substantially worse than both
STARLET-A and STARLET-H on readability,
correctness, completeness, and compactness.
STARLET-A and STARLET-H are also preferred
over STARLET-E for readability. While STARLET-
A is preferred over STARLET-E for compactness
(the average length of the abstractive reviews
was 45.05 words, and of the extractive,102.30),
STARLET-H is preferred over STARLET-E for
correctness, since the former better captures the
reviewers opinions by quoting them in the ap-
propriate context. STARLET-A and STARLET-H
achieve virtually indistinguishable performance
on all evaluation metrics. Our evaluation results
accord with those of Carenini et al. (2012); their
abstractive summarizer had superior performance
in terms of content precision and accuracy when
compared to summaries generated by an extractive
summarizer. Carenini et al. (2012) also found that
the differences between extractive and abstractive
approaches are even more significant in the case
of controversial content, where the abstractive
system is able to more effectively convey the full
range of opinions.
5 Related work
Ganesan et al. (2010) propose a method to extract
salient sentence fragments that are both highly fre-
quent and syntactically well-formed by using a
graph-based data structure to eliminate redundan-
cies. However, this approach assumes that the in-
put sentences are already selected in terms of as-
pect and with highly redundant opinion content.
Also, the generated summaries are very short and
cannot be compared to a full-length output of a
typical multi-document summarizer (e.g., 100-200
words). A similar approach is described in Gane-
san et al. (2012), where very short phrases (from
two to five words) are collated together to generate
what the authors call ultra-concise summaries.
The most complete contribution to evaluative
text summarization is described in Carenini et al.
(2012) and it closely relates to this work. Carenini
et al. (2012) compare an extractive summariza-
tion system, MEAD* ? a modified version of
the open source summarization system MEAD
(Radev et al., 2004) ? with SEA, an abstractive
summarization system, demonstrating that both
systems perform equally well. The SEA approach,
although better than traditional MEAD, has a few
drawbacks. Firstly, the sentence selection mecha-
nism only considers the most frequently discussed
aspects, leaving the decision about where to stop
the selection process to the maximum summary
length parameter. This could leave out interest-
ing opinions that do not appear with sufficient fre-
quency in the source documents. Ideally, all opin-
ions should be represented in the summary accord-
ing to the overall distribution of the input reviews.
Secondly, Carenini et al. (2012) use the absolute
value of the sum of positive and negative contri-
butions to determine the relevance of a sentence in
terms of opinion content. This flattens the aspect
distributions since sentences with very negative or
very positive polarity or with numerous opinions,
but with moderate polarity strengths, will get the
same score, regardless. Finally, it does not ad-
dress the opinion holder attribution problem leav-
ing the source of opinion undefined. In contrast,
STARLET-H follows reviews aspect rating distri-
butions both to select quotable sentences and to
summarize relevant aspects. Moreover, it explic-
itly mentions the opinion source in the embedded
quoted sentences.
6 Conclusions
In this paper, we present a hybrid summarizer for
sentiment-laden text that combines an overall ab-
stractive summarization method with an extrac-
tive summarization-based quote selection method.
This summarizer can provide the readability and
correctness of abstractive summarization, while
addressing the opinion holder attribution problem
that can lead readers to become confused or mis-
led about who is making claims that they read in
review summaries. We plan a more extensive eval-
uation of STARLET-H. Another potential area of
future research concerns the ability to personal-
ize summaries to the user?s needs. For instance,
the text planner can adapt its communicative goals
based on polarity orientation ? a user can be more
interested in exploring in detail negative reviews
? or it can focus more on specific (user-tailored)
aspects and change the order of the presentation
accordingly. Finally, it could be interesting to cus-
tomize the summarizer to provide an overview of
what is available in a specific geographic neigh-
borhood and compare and contrast the options.
61
References
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. The Berkeley FrameNet Project. In
Proceedings of the 17th International Con-
ference on Computational Linguistics - Vol-
ume 1, COLING ?98, pages 86?90, Strouds-
burg, PA, USA, 1998. Association for Com-
putational Linguistics. doi: 10.3115/980451.
980860. URL http://dx.doi.org/10.
3115/980451.980860.
Sasha Blair-Goldensohn, Kerry Hannan, Ryan
McDonald, Tyler Neylon, George Reis, and Jeff
Reynar. Building a Sentiment Summarizer for
Local Service Reviews. In NLP in the Informa-
tion Explosion Era, 2008.
Giuseppe Carenini, Jackie Chi Kit Cheung, and
Adam Pauls. Multi-Document Summarization
of Evaluative Text. Computational Intelligence,
2012.
E?ric De La Clergerie, Beno??t Sagot, Rosa Stern,
Pascal Denis, Gae?lle Recource?, and Victor
Mignot. Extracting and Visualizing Quotations
from News Wires. In Language and Technol-
ogy Conference, Poznan, Pologne, 2009. Projet
Scribo (po?le de compe?titivite? System@tic).
Giuseppe Di Fabbrizio, Ahmet Aker, and Robert
Gaizauskas. STARLET: Multi-document Sum-
marization of Service and Product Reviews with
Balanced Rating Distributions. In Proceedings
of the 2011 IEEE International Conference on
Data Mining (ICDM) Workshop on Sentiment
Elicitation from Natural Text for Information
Retrieval and Extraction (SENTIRE), Vancou-
ver, Canada, december 2011.
Giuseppe Di Fabbrizio, Ahmet Aker, and Robert
Gaizauskas. Summarizing On-line Product and
Service Reviews Using Aspect RatingDistribu-
tions and Language Modeling. Intelligent Sys-
tems, IEEE, 28(3):28?37, May 2013. ISSN
1541-1672. doi: 10.1109/MIS.2013.36.
Kavita Ganesan, ChengXiang Zhai, and Jiawei
Han. Opinosis: A Graph-Based Approach to
Abstractive Summarization of Highly Redun-
dant Opinions. In Proceedings of the 23rd Inter-
national Conference on Computational Linguis-
tics, COLING ?10, pages 340?348, Strouds-
burg, PA, USA, 2010. Association for Compu-
tational Linguistics.
Kavita Ganesan, ChengXiang Zhai, and Evelyne
Viegas. Micropinion Generation: An Unsu-
pervised Approach to Generating Ultra-concise
Summaries of Opinions. In Proceedings of the
21st international conference on World Wide
Web, WWW ?12, pages 869?878, New York,
NY, USA, 2012. ACM.
Albert Gatt and Ehud Reiter. SimpleNLG: A Re-
alisation Engine for Practical Applications. In
Proceedings of the 12th European Workshop
on Natural Language Generation, ENLG ?09,
pages 90?93, Stroudsburg, PA, USA, 2009. As-
sociation for Computational Linguistics.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell,
and Mark Kantrowitz. Multi-document Sum-
marization by Sentence Extraction. In Proceed-
ings of the 2000 NAACL-ANLP Workshop on
Automatic summarization - Volume 4, pages 40?
48, Stroudsburg, PA, USA, 2000. Association
for Computational Linguistics.
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.
Opinion Extraction, Summarization and Track-
ing in News and BlogCorpora. In Proceedings
of AAAI-2006 Spring Symposium on Computa-
tional Approaches to Analyzing Weblogs, 2006.
Chin-Yew Lin. ROUGE: A Package for Auto-
matic Evaluation of summaries. In Proc. ACL
workshop on Text Summarization Branches Out,
page 10, 2004.
William C. Mann and Sandra A. Thompson.
Rhetorical Structure Theory: A Theory of Text
Organization. In Livia Polanyi, editor, The
Structure of Discourse. Ablex, Norwood, NJ,
1989.
Mehryar Mohri, Fernando Pereira, and Michael
Riley. Weighted Automata in Text and Speech
Processing. In ECAI-96 Workshop, pages 46?
50. John Wiley and Sons, 1996.
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda C?elebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam,
Danyu Liu, Jahna Otterbacher, Hong Qi, Ho-
racio Saggion, Simone Teufel, Michael Top-
per, Adam Winkel, and Zhu Zhang. MEAD
? A Platform for Multidocument Multilingual
Text Summarization. In Conference on Lan-
guage Resources and Evaluation (LREC), Lis-
bon, Portugal, May 2004.
Dragomir R. Radev and Kathleen R. McKe-
own. Generating natural language summaries
from multiple on-line sources. Computational
Linguistiscs, 24(3):470?500, September 1998.
ISSN 0891-2017.
62
Owen Rambow and Tanya Korelsky. Applied Text
Generation. In Proceedings of the Third Confer-
ence on Applied Natural Language Processing,
pages 40?47, Trento, Italy, 1992. Association
for Computational Linguistics. 31 March - 3
April.
Ehud Reiter. Has a Consensus NL Generation Ar-
chitecture Appeared, and is it Psychologically
Plausible? In David McDonald and Marie
Meteer, editors, Proceedings of the 7th. Inter-
national Workshop on Natural Language gen-
eration (INLGW ?94), pages 163?170, Kenneb-
unkport, Maine, 1994.
Luis Sarmento and Se?rgio Nunes. Automatic Ex-
traction of Quotes and Topics from News Feeds.
In 4th Doctoral Symposium on Informatics En-
gineering (DSIE09), 2009.
Amanda Stent and Martin Molina. Evaluating Au-
tomatic Extraction of Rules for Sentence Plan
Construction. In Proceedings of the SIGDIAL
2009 Conference: The 10th Annual Meeting of
the Special Interest Group on Discourse and Di-
alogue, SIGDIAL ?09, pages 290?297, Strouds-
burg, PA, USA, 2009. Association for Compu-
tational Linguistics.
63
Proceedings of the 4th International Workshop on Computational Terminology, pages 11?21,
Dublin, Ireland, August 23 2014.
Assigning Terms to Domains by Document Classification
Robert Gaizauskas, Emma Barker, Monica Lestari Paramita and Ahmet Aker
Department of Computer Science, University of Sheffield, United Kingdom
{r.gaizauskas,e.barker,m.paramita,ahmet.aker}@sheffield.ac.uk
Abstract
In this paper we investigate a number of questions relating to the identification of the domain
of a term by domain classification of the document in which the term occurs. We propose and
evaluate a straightforward method for domain classification of documents in 24 languages that
exploits a multilingual thesaurus and Wikipedia. We investigate and provide quantitative results
about the extent to which humans agree about the domain classification of documents and terms
also the extent to which terms are likely to ?inherit? the domain of their parent document.
1 Introduction
In an increasingly interconnected world, characterised by high international mobility and globalised trade
patterns, communication across languages is ever more important. The demand for translation services
has never been higher and there is constant pressure for technological solutions, e.g., in the form of ma-
chine translation (MT) and computer-assisted translation (CAT), to increase translation throughput and
lower costs. One requirement of these technologies is bilingual lexical resources, i.e. dictionaries, partic-
ularly in specialist subject areas or domains, such as biomedicine, information techology, or aerospace.
While in theory statistical MT approaches need only parallel corpora to train their translation models,
there is never enough parallel material in technical areas or for minority languages to support high qual-
ity technical translation, so specialist bilingual terminological resources are very important. Similarly,
human translators using CAT systems need support in the form of bilingual terminological resources in
specialist areas about which they may know very little.
The EU FP-7 TaaS project has created a cloud-based terminological service, which makes available
bilingual terminological resources for all EU languages. These resources include both existing termi-
nological resources and resources derived automatically from parallel and comparable corpora available
on the web. Additionally, the service?s user community is able manually to supplement or correct these
resources. Like many other terminology resources (e.g. IATE1, Eurotermbank2), terms in TaaS have do-
mains associated with them. This is done for a number of reasons: (1) Computational Feasiblity: While
in theory a translator faced with a translation task could provide the set of documents to be translated to
a system that dynamically assembled a bespoke terminological resource specific to this task, this is not
computationally feasible, at least not in a time-frame a user is likely to accept. Much more feasible is
to collect bilingual terminology off-line and store it within a term repository with an associated domain
or domains. Then, an on-line user, having identified the domain of the document(s) to be translated,
searches for terms within that domain or may have terms from the domain into which his documents are
automatically classified made available to him. (2) Sense Disambiguation: Term expressions, or their
translations, may have multiple senses, but these are likely to be in different domains. By restricting
the domain when looking up terms, sense confusions are less likely to occur. (3) User Preference: Our
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are
added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1http://iate.europa.eu
2http://www.eurotermbank.com
Terminology questions in texts authored by patients
Noemie Elhadad
Department of Biomedical Informatics
Columbia University, USA
noemie@dbmi.columbia.edu
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
Assigning Terms to Domains by Document Classification
Robert Gaizauskas, Emma Barker, Monica Lestari Paramita and Ahmet Aker
Department of Computer Science, University of Sheffield, United Kingdom
{r.gaizauskas,e.barker,m.paramita,ahmet.aker}@sheffield.ac.uk
Abstract
In this paper we investigate a number of questions relating to the identification of the domain
of a term by domain classification of the document in which the term occurs. We propose and
evaluate a straightforward method for domain classification of documents in 24 languages that
exploits a multilingual thesaurus and Wikipedia. We investigate and provide quantitative results
about the extent to which humans agree about the domain classification of documents and terms
also the extent to which terms are likely to ?inherit? the domain of their parent document.
1 Introduction
In an increasingly interconnected world, characterised by high international mobility and globalised trade
patterns, communication across languages is ever more important. The demand for translation services
has never been higher and there is constant pressure for technological solutions, e.g., in the form of ma-
chine translation (MT) and computer-assisted translation (CAT), to increase translation throughput and
lower costs. One requirement of these technologies is bilingual lexical resources, i.e. dictionaries, partic-
ularly in specialist subject areas or domains, such as biomedicine, information techology, or aerospace.
While in theory statistical MT approaches need only parallel corpora to train their translation models,
there is never enough parallel material in technical areas or for minority languages to support high qual-
ity technical translation, so specialist bilingual terminological resources are very important. Similarly,
human translators using CAT systems need support in the form of bilingual terminological resources in
specialist areas about which they may know very little.
The EU FP-7 TaaS project has created a cloud-based terminological service, which makes available
bilingual terminological resources for all EU languages. These resources include both existing termi-
nological resources and resources derived automatically from parallel and comparable corpora available
on the web. Additionally, the service?s user community is able manually to supplement or correct these
resources. Like many other terminology resources (e.g. IATE1, Eurotermbank2), terms in TaaS have do-
mains associated with them. This is done for a number of reasons: (1) Computational Feasiblity: While
in theory a translator faced with a translation task could provide the set of documents to be translated to
a system that dynamically assembled a bespoke terminological resource specific to this task, this is not
computationally feasible, at least not in a time-frame a user is likely to accept. Much more feasible is
to collect bilingual terminology off-line and store it within a term repository with an associated domain
or domains. Then, an on-line user, having identified the domain of the document(s) to be translated,
searches for terms within that domain or may have terms from the domain into which his documents are
automatically classified made available to him. (2) Sense Disambiguation: Term expressions, or their
translations, may have multiple senses, but these are likely to be in different domains. By restricting
the domain when looking up terms, sense confusions are less likely to occur. (3) User Preference: Our
This work is licensed under a Creative Commons Attribution 4.0 International Licen . Page numbers and proceedings footer are
dde by the organisers. Licen d tails: http://creativecommons.org/licenses/ y/4.0/
1http://i te.europa.eu
2 www.eurotermbank.com
11
discussions with technical translators show they are used to and comfortable with the notion of domains
and prefer terminological resources structured by domain.
Assuming, therefore, that term resources are to be structured into domains, the question arises as to
how this is to be done automatically for automatically acquired terms. While the notion of domain is
inherent in most definitions of ?term?3, most term extraction systems identify terms using grammatical
patterns and/or statistical occurrence information applied to and gathered from corpora deemed to be
either in-domain or general/multi-domain. I.e. such tools do not have any inherent notion of domain, but
instead rely on the external provision of documents pre-selected by domain to determine the domain of
the extracted terms. But how valid is this procedure?
In this paper we explore several questions related to the assignment of terms to domains. These ques-
tions were addressed within the evaluation of that component of the TaaS platform which automatically
creates bilingual term resources (the Bilingual Term Extraction System, aka BiTES). Specifically:
1. How well can a simple vector space classifier built from a multilingual thesaurus automatically clas-
sify documents into domains prior to assigning these domains to the terms within the documents?
2. To what extent do humans agree about the assignment of terms to domains?
3. How accurate is the assumption that terms can be assigned to the domains of the documents in
which they are found?
The rest of the paper is structured as follows. Section 2 gives a brief overview of the BiTES system
as a whole and the domain classification component in somewhat more detail. In section 3 we describe
the evaluation of those parts of BiTES relevant to the questions above, detailing the evaluation tasks,
participants and data used and as well as the results of the evaluation. Section 4 provides analysis and
discussion of results. Section 5 discusses related work. We conclude in Section 6.
2 System Components
2.1 BiTES overview
The Bilingual Term Extraction System (BiTES) uses different workflows, each comprising a set of tools
run in sequence, to collect bilingual term pairs. Each new bilingual term pair found by BiTES is fed into
a database for later retrieval. The workflows consist of four different types of tools:
1. tools for collecting Web resources, such as parallel and comparable corpora from which the bilingual
terms are extracted;
2. tools for performing document classification into pre-defined categories or domains;
3. tools for extracting terms from or tagging terms in monolingual documents collected from the Web;
4. tools for bilingual alignment of tagged terms in parallel or comparable document pairs collected
from the Web.
Each workflow can be run in an offline and periodic manner and starts with document collection from the
Web followed by document classification. The output of the document classifier is passed to the mono-
lingual term extractor. Term-tagged document pairs are fed to the bilingual term alignment processor
to extract bilingual terms. The main goal of BiTES within the TaaS platform is to automatically col-
lect large numbers of bilingual term pairs off-line that are then stored in a database for later retrieval by
users. This database of automatically collected terms is consulted when other pre-existing, and presumed
higher quality, manually gathered terminological resources, such as, EuroTermBank or IATE, which are
also available in the TaaS platform, do not contain translations for terms the user seeks.
3For example Besse? et al. (1997) define term as ?a lexical unit consisting of one or more than one word which represents a
concept inside a domain?; ISO 1087-1:2000 defines term as ?verbal designation of a general concept in a specific subject field?.
12
In this section we detail only the domain classification component of BiTES as it is the component that
has the most direct implications for the research questions addressed in the paper and as the underlying
methods and performance of the other tools used in BiTES have been reported elsewhere (Aker et al.,
2012; Pinnis et al., 2012; Su and Babych, 2012; Skadin?a et al., 2012; Aker et al., 2013; Aker et al.,
2014b; Aker et al., 2014a).
2.2 Domain Classification
2.2.1 Domain classification scheme
Despite the existence of various domain classification schemes, the TaaS project has created its own do-
main classification for several reasons. First, the TaaS platform requires a suitable classification system
which is easy to use, yet provides broad coverage of the topics that are of greatest interest to users work-
ing in terminology management and machine translation. The project conducted a user study to identify
the set of required domains. Various classification systems were considered, including the Dewey Dec-
imal Classification (DDC) and Universal Decimal Classification (UDC). These schemes, however, are
too complicated to be used by terminologists (the latter uses 10 level-1 domains and more than 60,000
level-2 domains) yet still did not sufficiently cover relevant subject fields identified by our users, such
as IT, medicine and mechanical engineering. The Internal Classification for Standards (ICS) scheme
was considered next, as it covers technical subject fields, but it was lacking with respect to legal and
humanities domains. Intially, therefore, the TaaS project decided to adopt the domain structuring used
in the EuroVoc thesaurus, which includes a broad range of domains. However, with 21 level-1 domains
and 127 level-2 domains, it too is quite complex and focuses more on European Union domains than the
industry-related domains identified in our user study. Therefore, various modifications to the EuroVoc
domain scheme were performed to merge and delete various domains so as to increase the scheme?s
suitability for the project and also improve its practicality and ease of use. This resulted in what we here
refer to as the TaaS domain classification scheme, which contains 11 level-1 domains and 66 level-2 do-
mains4. A mapping from EuroVoc level-1 and -2 domains to TaaS level-1 and -2 domains was manually
established.
2.2.2 Document classifier
Many approaches to document classification have been proposed in the literature ? see Agarwal et al.
(2014) for a survey. Our domain classifier uses the well-explored vector space approach. For each
language, each domain is represented by one vector and each document to be classified by another vector.
The cosine similarity measure (Salton and Lesk, 1968) is calculated between the vector representation
of the input document and the vector representation of a domain and serves as a measure of the extent
to which the document belongs to that domain. The highest scoring domain may be chosen if hard
classification is required, or a vector of scores, one per domain, may be returned, if soft classification
is needed. The advantage of this approach in our setting is that we can exploit an existing multilingual,
domain-structured thesaurus to build our domain vector to deliver domain classifiers for 11 domains in
24 languages, without the need for collecting training data.
To create a vector representation for an input document, the document is first pre-processed and stop
words and punctuation are removed from it. The TaaS project covers 23 of the 24 official EU languages5
as well as Russian. For each of these languages we took the entire dump of Wikipedia and weighted each
word in the articles using tf ? idf (Manning et al., 2008). Any word whose idf is below a predefined
threshold is used as a stop word. Using this method we collected stop word lists for all 24 languages.
To identify punctuation we used simple rules covering the major punctuation symbols. After filtering
out stop words and punctuation, the remaining words in the input document are stemmed. We adopted
Lucene stemmers for all languages for which these resources are available in and implemented new
stemmers for Latvian, Lithuanian and Estonian. Finally, term frequency counts for the stems in the input
document are gathered, idf scores are taken from the Wikipedia dump and tf ? idf weights are computed
and stored to create the vector representation of the input document.
4A full specification of the scheme is available at: https://demo.taas-project.eu/domains.
5The omitted language is Irish, for which insufficient data was available for training our tools.
13
To create domain vectors we did the following: (1) For each domain and language, we manually
downloaded the relevant EuroVoc term file from the EuroVoc website6. (2) We used the EuroVoc-to-
TaaS mapping described in Section 2.2.1 above to map all terms belonging to a specific EuroVoc domain
(level-1 or -2) to the corresponding TaaS domain (level-1 or -2). (3) For each TaaS domain (in each
language) we built a domain-specific vector from the set of newly derived TaaS terms in the domain.
Since our vector elements correspond to single words, we convert any multi-word term in the domain
into multiple single word representations. To do this we process each multi-word by splitting it on
whitespace, removing any words that are stop words and finally stemming the remaining words. For any
single word terms we simply take their stems. Finally, all the word stems so derived are stored in a vector.
We use simple term frequency, measured across the bag of stemmed words derived from all terms in the
domain, as a weight for each stem. In the experiment below we report results only for classification into
the 11 level-1 TaaS domains ? see Table1.
Level-1 Domain Level-2 Domain
Agriculture and foodstuff Agriculture, forestry, fisheries, foodstuff, beverages and tobacco, and food technol-
ogy.
Arts Plastic arts, music, literature, and dance.
Economics Business administration, national economics, finance and accounting, trade, mar-
keting and public relations, and insurance.
Energy Energy policy, coal and mining, oil and gas, nuclear energy, and wind, water and
solar energy.
Environment Climate, and environmental protection.
Industries and technology Information and communication technology, chemical industry, iron, steel and other
metal industries, mechanical engineering, electronics and electrical engineering,
building and public works, wood industry, leather and textile industries, transporta-
tion and aeronautics, and tourism.
Law Civil law, criminal law, commercial law, public law, and international law and hu-
man rights.
Medicine and pharmacy Anatomy, ophthalmology, dentistry, otolaryngology, paediatrics, surgery, alterna-
tive treatment methods, gynaecology, veterinary medicine, pharmacy, cosmetic, and
medical engineering.
Natural sciences Astronomy, biology, chemistry, geology, geography, mathematics and physics.
Politics and administration Administration, politics, international relations and defence, and European Union.
Social sciences Education, history, communication and media, social affairs, culture and religion,
linguistics, and sports.
Table 1: TaaS Domains
3 Evaluation
To evaluate the BiTES system we devised a set of four human assessment tasks focussed on different
aspects of the system. These tasks were designed to assess the domain classifier, the extent to which
terms found in a document judged to be in a given domain were in the domain of their document, the
accuracy of the boundaries of extracted terms in context and the accuracy of system proposed bilingual
term alignments. In this paper we focus on the first two of these tasks only. As noted above the TaaS
project addressed 24 languages in total. Evaluation of all these languages and language pairs was clearly
impossible. We chose to focus on six languages ? English (EN), German (DE), Spanish (ES), Czech
(CS), Lithuanian (LT) and Latvian (LV) ? and five language pairs EN-DE, EN-ES, EN-CS, EN-LT and
EN-LV. This gave us exemplars from the Germanic, Romance, Slavic and Baltic language groups.
3.1 Human assessment tasks
3.1.1 Domain classification assessment
In the domain classification assessment task we present participants with a document and the TaaS set of
domain classes (see Table 1), and ask them to select the TaaS level-1 domain that in their judgement best
represents the document. We provide a brief set of guidelines to help them carry out this task.
6http://eurovoc.europa.eu
14
We encourage participants to select a primary domain wherever possible ? i.e. a single domain that
best represents the document. But we allow them to select multiple domains from the list provided, if they
believe the text spans more than one domain and they are unable to decide upon a primary domain. If they
do opt to select multiple domains we ask them to keep the number of selected domains to a minimum.
For example, the Wikipedia article entitled ?Hydraulic Fracturing? 7 discusses a wide range of topics,
including the process of hydraulic fracturing and its impacts in the geological, environmental, economic
and political spheres. For this document, which we use in our guidelines for the task, we recommend
assessors choose ?Energy? as a primary domain and possibly also ?Industries and Technology?, since
these two domains best represent the overall document content, which is chiefly concerned with what is
described as a ?mechanical? process in the ?industrial sector of mining?, the products being natural gas
and oil. But we would limit our selection to these two.
The aim is for participants to select domains from the list we provide. However, in the event that they
are unable to do so, we provide an option ?none of the above?, which they may select and then provide
a domain of their own. In the guidelines we ask them to spend some time reviewing potential domain
candidates, and combinations of candidates, before opting to provide an as yet unspecified domain.
I.e. they should only select the option ?none of the above? if they have genuinely exhausted all the
possibilities using one or more domains from our list.
3.1.2 Term in domain assessment
Figure 1: Judging a Term Candidate in a Domain
This is the first of two tasks assessing the (monolingual) extraction of terms. It assesses whether an
automatically extracted term candidate is a term in a proposed, automatically determined, domain. As-
suming the candidate is a term, a subsequent task assesses whether the boundaries of the term candidate,
when taken in their original document context, are correct.
In this task (see Figure 1) we present assessors with a term candidate and a domain and then ask them
to judge if the candidate is a term in the given domain or if it is a term in a different domain. If they judge
the term to be in a different domain we ask them to specify the alternate domain(s). In this question the
candidate and the domain category are assessed together but we do not provide any specific context, such
as the source sentence or source document. As with the previous task we provide a brief set of guidelines
to help assessors carry out the task.
We ask assessors to base their judgement on the entire candidate string. If the string contains a term
but also contains, additional words that are not part of the term then they should answer ?no?. For
7Aka ?fracking?, see http://en.wikipedia.org/wiki/Hydraulic_fracturing
15
example, consider the candidate ?excessive fuel emissions? and the domain ?Industries and Technology?.
Although most people would agree that ?fuel emissions? is a term, Q1.1 and Q1.2 should be answered
?no? in this case since the candidate also contains noise, i.e. the word ?excessive?. Superfluous articles,
determiners and other closed class words are also considered ?noise? in this context.
We encourage assessors to search the Internet, as translators and terminologists might do, to help
determine whether the entire candidate is indeed a term in the given domain. Web searches can provide
examples of real world uses of a candidate in different domains. We also allow assessors to consult
existing terminological or dictionary resources, online or otherwise, during the evaluation task. However,
participants are encouraged not to assume that such resources are complete or entirely correct and advised
that such resources be used with some consideration and caution.
Finally, if assessors have answered ?yes? to one of Q1.1 or Q1.2, they will also be asked to indicate
the utility of the term candidate in Q1.3, however this aspect of the assessment is not of interest here and
will not be discussed further.
3.2 Participants
We recruited experienced translators to participate in the evaluation tasks. For English and for each
language pair, three assessors carried out each of the evaluation tasks. In total our study involved 17
assessors ? one assessor took part in DE only, EN-DE and EN only tasks. All assessors had an excellent
background in translation in a wide variety of domains, with an average of 8.5 years translation experi-
ence in the relevant language pairs. All assessors who evaluated the English, Lithuanian and Latvian data
were native speakers. For each of the remaining languages (Czech, German and Spanish), 2 were native
speakers whilst 1 was a fluent speaker with over 54 years, 15 years and 12 years experience (respectively)
in using these languages as a second language.
3.3 Data
3.3.1 Domain classification
For the domain classification task, we selected a set of documents to be evaluated using the following
approach. First, we gathered all articles from the August 2013 Wikipedia dump in each of the assesment
languages and extracted the main text paragraphs, i.e. tables, images, infoboxes and URLs were filtered
out. The number of articles ranged from 50,000 (for Latvian) to 4,000,000 (for English). We then ran
our domain classifier over each document in this dataset and assigned to each document the top domain
proposed by the classifier, i.e. the domain with the highest score according to our vector space approach
(Section 2.2.2). During processing we filtered out documents whose top domain scores were below
a previously set minimum threshold and those whose document length was below a minimum length.
Finally, for each domain D, we sorted the documents classified into D based on their scores, divided this
sequence into 10 equal-size bins and selected one document from each bin. Since we were classifying
documents into one of the 11 level-1 TaaS domains, this resulted in 110 documents for each language8.
3.3.2 Term extraction
For the term in domain assessment task, we narrowed the task to focus on two domains only ? ?Industries
and Technology? and ?Politics and Administration? ? since we could not hope to assess sufficient terms
in all domains in all languages. We extracted terms from all documents contained in the top bin of the
domain classifier, i.e. the 10% of documents in the domain with the highest similarity score to the domain
vector, using TWSC as the term extractor tool (Pinnis et al., 2012). Next, we selected 200 terms from
both domains, choosing terms of different word lengths: 50 of length 1, 70 of length 2, 50 of length 3
and 30 of length 4. This distribution was chosen in order to approximate roughly the distribution of term
lengths one might expect in the data9. This process was repeated for each of our six languages.
8The Latvian set contains a slightly smaller set (i.e. 106 documents) due to a fewer number of documents found in one of
the domains (i.e. 6 documents in the ?Energy? domains).
9This distribution was chosen after analysing term lengths in the EuroVoc thesaurus and in the term extractor results, which
indicated that terms length 2 are the most common, followed by terms length 1 and 3, and terms length 4 are found to be the
least common. We boosted slightly the numbers of length 4 terms in our test to try to eliminate very small number effects.
16
3.4 Results
3.4.1 Domain classification assessment
A total of 656 documents (in 6 languages) were assessed and on average 1.2 domains were selected for
each document. Regarding human-human agreement, at least 2 assessors fully agreed on their domain
selections (including cases where more than one domain was selected) on 78% of the cases. When
considering cases where at least 2 assessors agreed on at least one domain, agreement increases to 98%.
Regarding human-system agreement, since 3 assessors participated in each assessment, we produced
two types of human judgments: majority (i.e. any domains selected by at least two assessors) and union
(i.e. any domains selected by at least one assessor). We computed the agreements between the classifier
and both the majority and the union human judgments. Results averaged over all domains and languages
show the system?s proposed top domain agreed with the majority human judgment in 45% of cases and
with the union of human judgments in 58% of cases. Broken down by language, agreement with the
majority judgment ranged from a low of 35% (EN) to a high of over 53% (DE) while agreement with the
union of judgments ranged from a low of 48% (EN) to a high of over 64% (CS). By domain, agreement
with majority judgment ranged from just over 12% (Agriculture and foodstuff) to 88% (Medicine and
pharmacy) while agreement with the union of judgments ranged from 23% (Agriculture and foodstuff)
to over 91% (Social sciences).
Recall (Section 3.3.1) that our test data includes documents from different similarity score bins. This
enables us to analyse the agreement between the assessors and the classifier in more detail. In general
we see a monotonically increasing agreement with both the majority judgement and union of judgments
as we move from the lowest to highest scoring bin. The highest agreement is achieved in bin 10 which
represents the 10% of documents ?most confidently? classified to a given domain, i.e. those documents
with the highest similarity score to the domain vector. Just under 80% of these documents (77.27%) are
included in the union of assessors data and 63% are included in the majority. I.e. for approximately 77%
of the documents most confidently classified to a domain by our classifier, at least one in three humans
will agree with the domain classification and for about 63% the majority of humans will agree.
3.4.2 Term in domain assessment
Term length Total Term in the Term in agiven domain different domain
All length 457 88% 12%
1 144 88% 12%
2 182 87% 13%
3 84 92% 8%
4 47 91% 9%
Table 2: Terms with different term length
Languages Total Term in the Term in agiven domain different domain
All languages 457 88% 12%
CS 103 86% 14%
DE 79 82% 18%
EN 80 88% 13%
ES 54 80% 20%
LT 47 98% 2%
LV 94 97% 3%
Table 3: Terms of different languages
A total of 1,200 candidate terms in 6 languages were assessed by 3 assessors and the majority judg-
ments (i.e. cases where at least two assessors agree) show that 38% terms were assessed to be candidate
terms in the given domain, 5% terms were assessed to be candidate terms in a different domain, and the
rest (57%) were deemed not to be terms.
This indicates that out of all candidate terms which were identified to be correct terms (43% of the
data), 88% were assessed to be in the same domain as the documents they were extracted from. Further
analysis showed that the 57% of candidates judged not to be terms could be further broken down into
33% which contain an overlap with a term, i.e. term boundaries were incorrectly identified, and 24%
which neither are nor overlap with a term.
Of the 43% candidate terms that were judged to be terms, we examined the variation in extent to
which they were judged to be terms in the given domain across term lengths and across languages. These
figures are shown in Tables 2 and 3. We also examined variation in the extent to which these terms were
judged to be terms in the given domain across the two domains we were investigating: in ?Industries and
17
Technology? 92% of the terms were judged to be in the given domain and 8% in another domain, while
for ?Politics and Administration? these figures were 85% and 15% respectively.
For the 43% of the term candidates that were identified as correct terms (457 terms), all three assessors
agreed about the domain of the term, i.e. they either accepted the domain proposed by the system for the
term or they agreed on an alternative(s), in 45% of the cases. In 54% of the cases there was not universal
agreement but at least two assessors agreed on at least one domain they assigned to the term. Only in 1%
of the cases was there no overlap in judgment about term domain.
4 Analysis and Discussion
Let us now return to the research questions we raised in Section 1. Our first question was: How well
can a simple vector space classifier built from a multilingual thesaurus automatically classify documents
into domains prior to assigning these domains to the terms within the documents? First, we have to view
system performance in the context of human performance. Results in the last section show that 2 out
of 3 humans agree 78% of the time on exact assignment of (possibly multiple) domains to documents
and 98% of the time if only one of the domains they assign to a document need to match. Over all
languages and domains our classifier achieves only 45% agreement with the majority judgment and
58% with the union of judgments. However, if we restrict ourselves to the highest confidence domain
assigments, then the picture is much better: 63% agreement with the majority judgment and 77% with
the union of judgments. This restriction reduces the number of documents from which terms could be
mined from if accurate domain classification is important ? but so long as there are lots of documents
to mine terms from this may not be important. Furthermore note that our classifier could easily be
used to select multiple domains, perhaps, e.g., when the differences in scores between highest scoring
domains is small. This would make the comparison with the human figures fairer (now the system can
only propose one domain per document while the humans can propose several) and could only result
in higher system figures relative to human ones. We conclude that the vector space classifier utilizing
domain representations derived from a pre-existing multingual thesaurus has much to recommend: it is
simple, it needs no training data, it is straightforwardly applicable to multiple (24 in our case) different
languages and its performance is adequate if it is suitably constrained.
Our second question was: To what extent do humans agree about the assignment of terms to domains?
Our results show that in less than half the cases do all three human assessors agree with the assignment
of a term to a particular domain. However, in 99% of the cases at least two of three assessors concur on
at least one domain to which the term belongs. This suggests that using overlap with two of three human
assessors is a good approach to measuring automatic domain assignment to terms.
Our third question was: How accurate is the assumption that terms can be assigned to the domains of
the documents in which they are found? Tables 2 and 3 show that on average 88% of terms are judged to
be in the domain of the document in which they are found. Furthermore there is relatively little variation
in this figure ? it ranges from a low of 80% (ES) to a high of 98% (LT) and a low of 87% for terms of
length 2 to a high of 92% for terms of length 3. This suggests that assigning domains to terms based
on the domain of the document the term is found in is a relatively safe thing to do, but is by no means
perfect: just over 10% of terms will have their domains incorrectly assigned by making this assumption.
5 Related Work
There has been extensive work on the development of automated techniques to extract terminology from
document collections. Such term extraction approaches can be grouped into three categories based on
the information used to extract terms: approaches using purely linguistic information, approaches using
purely statistical information and those using combinations of both. An analysis of different approaches
is given by Pazienza et al. (2005). For the most part, however, such approaches make the assumption
that domain-specific, and perhaps also non-domain-specific, collections of texts are available. Justeson
and Katz (1995), for example, assume that term frequency of a limited sort of noun phrases in domain-
specific texts is sufficicent to indicate termhood. Others such as Chung (2003) and Drouin (2004) look
at statistical contrasts between domain-specific and general comparison or reference corpus. See also
18
(Kim et al., 2009; Marciniak and Mykowiecka, 2013; Kilgariff, 2014). By contrast our approach does
not presuppose the existence of documents pre-classified by domain (though we could benefit from this).
Rather our approach starts by classifying a document into a domain and then extracting terms from it and
assigning them the domain of the document.
Utsuro et al. (2006) and Kida et al. (2007) extract terms from web-documents. The domain spec-
ification of a term is determined in two stage approach. In the first stage for a term under inspection
web-documents which mention the term are collected. Then these documents are divided into two sets:
domain relevant and domain-irrelevant documents. A document whose content similarity to a domain
specific corpora is above a predefined threshold is regarded as relevant. Any other document is regarded
as irrelevant. In the second stage a ratio of times the term occurs in the relevant and the irrelevant set is
computed. This ratio is used to determine whether the extracted term belongs to the domain in hand or
not. Again, a domain-specific corpus is assumed for this approach to proceed.
Benedictis et al. (2013) use bootstrapping to collect domain specific terms. They start with some
manually selected domain specific seed terms, perform web-search to obtain documents, extract further
terms and re-start the process with the new terms. The documents returned by the search engine are
assumed to belong to the domain in hand and so are the extracted terms. By contrast our approach does
not require manually selected terms, but instead uses an existing domain structured multingual thesaurus.
6 Conclusion
In this paper we have investigated a number of questions relating to the identification of the domain of
a term by domain classification of the document in which the term occurs. We proposed and evaluated
a straightforward method for domain classification of documents in 24 languages which uses a multilin-
gual thesaurus to construct ?domain vectors?. We investigated the extent to which humans agree about
the domain classification of documents and terms. And, we investigated the extent to which terms are
likely to ?inherit? the domain of their parent document. Our results show that the domain classification
method has significant merit, that humans generally, but by no means universally, agree about domain
classification of documents and terms, and again that terms are generally, but certainly not universally,
likely to be of the same domain as the document in which they occur.
7 Acknowledgments
The authors would like to acknowledge funding from the European Union FP-7 programme for the TaaS
project, grant number: 296312. We would also like to thank the human assessors without whose careful
work the results reported here would not have been obtained. Finally we thank our project partners in the
TaaS project for user studies with translators and terminologists, contributions to the TaaS system, and
development of the TaaS domain classication scheme and the EuroVoc-to-TaaS mapping.
19
References
Basant Agarwal and Namita Mittal. 2014. Text classification using machine learning methods-a survey. In
Proceedings of the Second International Conference on Soft Computing for Problem Solving (SocProS 2012),
December 28-30, 2012, pages 701?709. Springer.
Ahmet Aker, Evangelos Kanoulas, and Robert J Gaizauskas. 2012. A light way to collect comparable corpora from
the web. In Proceedings of Eighth International Conference on Language Resources and Evalution (LREC),
pages 15?20.
Ahmet Aker, Monica Paramita, and Robert Gaizauskas. 2013. Extracting bilingual terminologies from comparable
corpora. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL
2013).
Ahmet Aker, Monica Paramita, Emma Barker, and Robert Gaizauskas. 2014a. Bootstraping Term Extractors for
Multiple Languages. In Proceedings of the International Conference on Language Resources and Evaluation
(LREC).
Ahmet Aker, Monica Paramita, Ma?rcis Pinnis, and Robert Gaizauskas. 2014b. Bilingual dictionaries for all EU
languages. In Proceedings of the International Conference on Language Resources and Evaluation (LREC).
Teresa Mihwa Chung. 2003. A corpus comparison approach for terminology extraction. Terminology, 9(2).
Flavio De Benedictis, Stefano Faralli, Roberto Navigli, et al. 2013. Glossboot: Bootstrapping multilingual domain
glossaries from the web. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics, pages 528?538.
Bruno de Besse?, Blaise Nkwenti-Azeh, and Juan C. Sager. 1997. Glossary of terms used in terminology. Termi-
nology. International Journal of Theoretical and Applied Issues in Specialized Communication, 4:117?156(39).
Patrick Drouin. 2004. Detection of domain specific terminology using corpora comparison. In Proceedings of the
Fourth International Conference on Language Resources and Evaluation (LREC2004).
John S. Justeson and Slava M. Katz. 1995. Technical terminology: Some linguistic properties and an algorithm
for identification in text. Natural Language Engineering, 1(1):9?27.
Mitsuhiro Kida, Masatsugu Tonoike, Takehito Utsuro, and Satoshi Sato. 2007. Domain classification of technical
terms using the web. Systems and Computers in Japan, 38(14):11?19.
Adam Kilgariff. 2014. Finding terms in corpora for many languages with the Sketch Engine. 14th Conference of
the European Chapter of the Association for Computational Linguistics.
Su Nam Kim, Timothy Baldwin, and Min-Yen Kan. 2009. An unsupervised approach to domain-specific term
extraction. In Australasian Language Technology Association Workshop 2009, page 94.
Christopher D Manning, Prabhakar Raghavan, and Hinrich Schu?tze. 2008. Introduction to information retrieval,
volume 1. Cambridge university press Cambridge.
Ma?gorzata Marciniak and Agnieszka Mykowiecka. 2013. Terminology extraction from domain texts in polish.
In Intelligent Tools for Building a Scientific Information Platform, pages 171?185. Springer.
Maria Teresa Pazienza, Marco Pennacchiotti, and Fabio Massimo Zanzotto. 2005. Terminology extraction: an
analysis of linguistic and statistical approaches. In Knowledge Mining, pages 255?279. Springer.
Ma?rcis Pinnis, Nikola Ljubes?ic?, Dan S?tefa?nescu, Inguna Skadin?a, Marko Tadic?, and Tatiana Gornostay. 2012.
Term extraction, tagging, and mapping tools for under-resourced languages. In Proceedings of the 10th Confer-
ence on Terminology and Knowledge Engineering (TKE 2012), June, pages 20?21.
Gerard Salton and Michael E Lesk. 1968. Computer evaluation of indexing and text processing. Journal of the
ACM (JACM), 15(1):8?36.
Inguna Skadin?a, Ahmet Aker, Nikos Mastropavlos, Fangzhong Su, Dan Tufis, Mateja Verlic, Andrejs Vasil?jevs,
Bogdan Babych, Monica Paramita, Paul Clough, Robert Gaizauskas, and Nikos Glaros. 2012. Collecting and
using comparable corpora for statistical machine translation. In Proceedings of the 8th International Conference
on Language Resources and Evaluation (LREC), Istanbul, Turkey.
20
Fangzhong Su and Bogdan Babych. 2012. Measuring comparability of documents in non-parallel corpora for
efficient extraction of (semi-) parallel translation equivalents. In Proceedings of the Joint Workshop on Exploit-
ing Synergies between Information Retrieval and Machine Translation (ESIRMT) and Hybrid Approaches to
Machine Translation (HyTra), pages 10?19. Association for Computational Linguistics.
Takehito Utsuro, Mitsuhiro Kida, Masatsugu Tonoike, and Satoshi Sato. 2006. Collecting novel technical terms
from the web by estimating domain specificity of a term. In Computer Processing of Oriental Languages.
Beyond the Orient: The Research Challenges Ahead, pages 173?180. Springer.
21
Proceedings of the 25th International Conference on Computational Linguistics, pages 38?45,
Dublin, Ireland, August 23-29 2014.
A Poodle or a Dog? Evaluating Automatic Image Annotation Using
Human Descriptions at Different Levels of Granularity
Josiah K. Wang
1
Fei Yan
2
Ahmet Aker
1
Robert Gaizauskas
1
1
Department of Computer Science, University of Sheffield, UK
2
Centre for Vision, Speech and Signal Processing, University of Surrey, UK
{j.k.wang, ahmet.aker, r.gaizauskas}@sheffield.ac.uk f.yan@surrey.ac.uk
Abstract
Different people may describe the same object in different ways, and at varied levels of granular-
ity (?poodle?, ?dog?, ?pet? or ?animal??) In this paper, we propose the idea of ?granularity-
aware? groupings where semantically related concepts are grouped across different levels of
granularity to capture the variation in how different people describe the same image content.
The idea is demonstrated in the task of automatic image annotation, where these semantic group-
ings are used to alter the results of image annotation in a manner that affords different insights
from its initial, category-independent rankings. The semantic groupings are also incorporated
during evaluation against image descriptions written by humans. Our experiments show that se-
mantic groupings result in image annotations that are more informative and flexible than without
groupings, although being too flexible may result in image annotations that are less informative.
1 Introduction
Describing the content of an image is essential for various tasks such as image indexing and retrieval, and
the organization and browsing of large image collections. Recent years have seen substantial progress
in the field of visual object recognition, allowing systems to automatically annotate an image with a list
of terms representing concepts depicted in the image. Fueled by advances in recognition algorithms and
the availability of large scale datasets such as ImageNet (Deng et al., 2009), current systems are able to
recognize thousands of object categories with reasonable accuracy, for example achieving an error rate
of 0.11 in classifying 1, 000 categories in the ImageNet Large Scale Visual Recognition Challenge 2013
(ILSVRC13) (Russakovsky et al., 2013).
However, the ILSVRC13 classification challenge assumes each image is annotated with only one
correct label, although systems are allowed up to five guesses per image to make the correct prediction
(or rather, to match the ground truth label). The problem with this is that it becomes difficult to guess
what the ?correct? label is, especially when many other categories can equally be considered correct.
For instance, should a system label an image containing an instance of a dog (and possibly some other
objects like a ball and a couch) as ?dog?, ?poodle?, ?puppy?, ?pet?, ?domestic dog?, ?canine? or even
?animal? (in addition to ?ball?, ?tennis ball?, ?toy?, ?couch?, ?sofa?, etc.)? The problem becomes even
harder when the number of possible ways to refer to the same object instance increases, but the number
of prediction slots to fill remains limited. With so many options from which to choose, how do we know
what the ?correct? annotation is supposed to be?
In this paper, we take a human-centric view of the problem, motivated by the observation that humans
are likely to be the end-users or consumers of such linguistic image annotations. In particular, we investi-
gate the effects of grouping semantically related concepts that may refer to the same object instance in an
image. Our work is related to the idea of basic-level categories (Biederman, 1995) in Linguistics, where
most people have a natural preference to classify certain object categories at a particular level of granu-
larity, e.g. ?bird? instead of ?sparrow? or ?animal?. However, we argue that what one person considers
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are
added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
38
?basic-level? may not necessarily be ?basic-level? to another, depending on the person?s knowledge, ex-
pertise, interest, or the context of the task at hand. For example, Rorissa (2008) shows that users label
groups of images and describe individual images differently with regards to the level of abstraction. The
key idea behind our proposed ?granularity-aware? approach is to group semantically related categories
across different levels of granularity to account for how different people would describe content in an
image differently.
We demonstrate the benefits of the ?granularity-aware? approach by producing a re-ranking of visual
classifier outputs for groups of concept nodes, e.g. WordNet synsets. The concept nodes are grouped
across different levels of specificity within a semantic hierarchy (Section 3.1). This models better the
richness of the vocabulary and lexical semantic relations in natural language. In this sense these group-
ings are used to alter the results of image annotation in a manner that affords different insights from
its initial, category-independent rankings. For example, if the annotation mentions only ?dog? but not
?poodle?, a system ranking ?poodle? at 1 and ?dog? at 20 will have a lower overall score than a system
ranking ?dog? at 1, although both are equally correct. Grouping (?poodle? or ?dog?) however will allow a
fairer evaluation and comparison where both systems are now considered equally good. The ?granularity-
aware? groupings will also be used in evaluating these re-rankings using textual descriptions written by
humans, rather than a keyword-based gold-standard annotation. The hypothesis is that by modeling the
variation in granularity levels for different concepts, we can gain a more informative insight as to how
the output of image annotation systems can relate to how a person describes what he or she perceives in
an image, and consequently produce image annotation systems that are more human-centric.
Overview. The remainder of the paper is organized as follows: Section 2 discusses related work. Sec-
tion 3 describes our proposed ?granularity-aware? approach to group related concepts across different
levels of granularity. It also discusses how to apply the idea both in automatic image annotation, by
re-ranking noisy visual classifier outputs in a ?granularity-aware? manner, and in evaluation of classi-
fier outputs against human descriptions of images. The results of the proposed method are reported in
Section 4. Finally, Section 5 offers conclusions and proposes possible future work.
2 Related work
Work on automatic image annotation traditionally relies heavily on image datasets annotated with a fixed
set of labels as training data. For example, Duygulu et al. (2002) investigated learning from images
annotated with a set of keywords, posing the problem as a machine translation task between image
regions and textual labels. Gupta and Davis (2008) includes some semantic information by incorporating
prepositions and comparative adjectives, which also requires manual annotation as no such data is readily
available. Recent work has moved beyond learning image annotation from constrained text labels to
learning from real world texts, for example from news captions (Feng and Lapata, 2008) and sports
articles (Socher and Fei-Fei, 2010).
There is also recent interest in treating texts as richer sources of information than just simple bags
of keywords, for example with the use of semantic hierarchies for object recognition (Marsza?ek and
Schmid, 2008; Deng et al., 2012b) and the inclusion of attributes for a richer representation (Lampert
et al., 2009; Farhadi et al., 2009). Another line of recent work uses textual descriptions of images for
various vision tasks, for example for recognizing butterfly species from butterfly descriptions (Wang
et al., 2009) and discovering attributes from item descriptions on fashion shopping websites (Berg et
al., 2010). There has also been interest in recent years in producing systems that annotate images with
full sentences rather than just a list of terms (Kulkarni et al., 2011; Yang et al., 2011). We consider
our work to complement the work of generating full sentences, as it is important to filter and select the
most suitable object instances from noisy visual output. The shift from treating texts as mere labels to
utilizing them as human-centric, richer forms of annotations is important to gain a better understanding
of the processes underlying image and text understanding or interpretation.
Deng et al. (2012b) address the issue of granularity in a large number of object categories by allowing
classifiers to output decisions at the optimum level in terms of being accurate and being informative, for
example outputting ?mammal? rather than ?animal? while still being correct. Their work differs from
39
ours in that the semantic hierarchy is used from within the visual classifier to make a decision about
its output, rather than for evaluating existing outputs. More directly related to our work is recent work
by Ordonez et al. (2013), which incorporates the notion of basic-level categories by modeling word
?naturalness? from text corpora on the web. While their focus is on obtaining the most ?natural? basic-
level categories for different encyclopedic concepts as well as for image annotation, our emphasis is on
accommodating different levels of naturalness, not just a single basic level. We adapt their model directly
to our work, details of which will be discussed in Section 3.1.
3 Granularity-aware approach to image annotation
The proposed ?granularity-aware? approach to image annotation consists of several components. We first
define semantic groupings of concepts by considering hypernym/hyponym relations in WordNet (Fell-
baum, 1998) and also how people describe image content (Section 3.1). The groupings are then used to
re-rank the output of a set of category-specific visual classifiers (Section 3.2), and also used to produce
a grouped ?gold standard? from image captions (Section 3.3). The re-ranked output is then evaluated
against the ?gold standard?, and the initial rankings and ?granularity-aware? re-rankings are compared
to gain a different insight into the visual classifiers? performance as human-centric image annotation
systems.
3.1 Semantic grouping across different granularity levels
The goal of semantic grouping is to aggregate related concepts such that all members of the group refer
to the same instance of an object, even across different specificity levels. In particular, we exploit the
hypernym/hyponym hierarchy of WordNet (Fellbaum, 1998) for this task. WordNet is also the natural
choice as it pairs well with our visual classifiers which are trained on ImageNet (Deng et al., 2009)
categories, or synsets.
The WordNet hypernym hierarchy alone is insufficient for semantic grouping as we still need a way to
determine what constitutes a reasonable group, e.g. putting all categories into a single ?entity? group is
technically correct but uninformative. For this, we draw inspiration from previous work by Ordonez et al.
(2013), where a ?word naturalness? measure is proposed to reflect how people typically describe image
content. More specifically, we adapt for our purposes their proposed approach of mapping encyclopedic
concepts to basic-level concepts (mapping ?Grampus griseus? to the more ?natural? ?dolphin?). In this
approach, the task is defined as learning a translation function ?(v, ?) : V 7?W that best maps a node v
to a hypernym node w which optimizes a trade-off between the ?naturalness? of w (how likely a person
is to use w to describe something) and the distance between v and w (to constrain the translation from
being too general, e.g. ?entity?), with the parameter ? controlling this trade-off between naturalness and
specificity. Formally, ?(v, ?) is defined as:
?(v, ?) = arg max
w ? ?(v)
[??(w)? (1? ?)?(w, v) ] (1)
where ?(v) is the set of hypernyms for v (including v), ?(w) is naturalness measure for node w, and
?(w, v) is the number of edges separating nodes w and v in the hypernym structure of WordNet.
For our work, all synsets that map to a common hypernym w are clustered as a single semantic group
G
?
w
:
G
?
w
= {v : ?
v
?(v, ?) = w} (2)
In this sense, the parameter ? ? [0, 1] essentially also controls the average size of the groups: ? = 0
results in no groupings, while ? = 1 results in synsets being grouped with their most ?natural? hypernym,
giving the largest possible difference in the levels of granularity within each group.
Estimating the naturalness function using Flickr. Ordonez et al. (2013) use n-gram counts of the
Google IT corpus (Brants and Franz, 2006) as an estimate for term naturalness ?(w). Although large,
the corpus might not be optimal as it is a general corpus and may not necessarily mirror how people
40
describe image content. Thus, we explore a different corpus that (i) better reflects how humans describe
image content; (ii) is sufficiently large for a reasonable estimate of ?(w). The Yahoo! Webscope Yahoo
Flickr Creative Commons 100M (YFCC-100M) dataset (Yahoo! Webscope, 2014) fits these criteria with
100 million images containing image captions written by users. Hence, we compute term occurrence
statistics from the title, description, and user tags of images from this dataset. Following Ordonez et al.,
we measure ?(w) as the maximum log count of term occurrences for all terms appearing in synset w.
Internal nodes. Unlike Ordonez et al. (2013), we do not constrain v to be a leaf node, but instead
also allow for internal nodes to be translated to one of their hypernyms. We could choose to limit
visual recognition to leaf nodes and estimate the visual content of internal nodes by aggregating the
outputs from all its leaf nodes, as done by Ordonez et al. (2013). However, since the example images in
ImageNet are obtained for internal nodes pretty much in the same way as leaf nodes (by querying ?dog?
rather than by combining images from ?poodle?, ?terrier? and ?border collie?) (Deng et al., 2009), the
visual models learnt from images at internal nodes may capture different kinds of patterns than from
their hyponyms. For example, a model trained with ImageNet examples of ?dog? might capture some
higher-level information that may otherwise not be captured by merely accumulating the outputs of the
leaf nodes under it, and vice versa.
3.2 Re-ranking of visual classifier output
The visual classifier used in our experiments (Section 4.2) outputs a Platt-scaled (Platt, 2000) confidence
value for each synset estimating the probability of the synset being depicted in a given image. The
classifier outputs are then ranked in descending order of these probability values, and are treated as
image annotation labels.
As mentioned, these rankings do not take into consideration that some of these synsets are semantically
related. Thus, we aggregate classifier outputs within our semantic groupings (Section 3.1), and then re-
rank the scores of each grouped classifier. Formally, the new score of a classifier c, ?
c
(G
?
w
), for a
semantic group G
?
w
is defined as:
?
c
(G
?
w
) = max
v?G
?
w
p
c
(v) (3)
where v is a synset from the semantic groupG
?
w
, and p
c
(v) is the original probability estimate of classifier
c for synset v. I.e., the probability of the most probable synset in the group is taken as the probability of
the group.
To enable comparison of the rankings against a gold standard keyword annotation, a word label is
also generated for each semantic group. We assign as the semantic group?s label `(G
?
w
) the first term of
synset w, the common hypernym node to which members of the group best translates. Note that the term
merely acts a label for evaluation purposes and should not be treated as a word in a traditional sense. We
also merge semantic groups with the same label to account for polysemes/homonyms, again taking the
maximum of ?
c
among the semantic groups as the new score.
The semantic grouping of synsets is performed independently of visual classifier output. As such, we
only need to train each visual classifier once for each synset, without requiring re-training for different
groupings since we only aggregate the output of the visual classifiers. This allows for more flexibility
since the output for each semantic group is only aggregated at evaluation time.
3.3 Evaluation using human descriptions
The image dataset used in our experiments (Section 4.1) is annotated with five full-sentence captions
per image but not keyword labels. Although an option would be to obtain keyword annotations via
crowdsourcing, it is time consuming and expensive and also requires validating the annotation quality.
Instead, we exploit the existing full-sentence captions from the dataset to automatically generate a gold
standard keyword annotation for evaluating our ranked classifier outputs. The use of such captions is
also in line with our goal of making the evaluation of image annotation systems more human-centric.
For each caption, we extract nouns using the open source tool FreeLing (Padr?o and Stanilovsky, 2012).
41
?0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Semantic Grouping 0.3450 0.3450 0.3548 0.3735 0.4025 0.4417 0.4562 0.4702 0.4834 0.5059 0.5395
Random Grouping 0.3450 0.3450 0.3493 0.3529 0.3585 0.3689 0.3823 0.4067 0.4241 0.4359 0.4467
Number of groups 1294 1294 1237 1105 949 817 693 570 474 419 368
Table 1: Results of re-ranking with semantic groupings. The first two rows show the average NDCG
scores for the proposed groupings and the random baseline groupings, for different groupings formed by
varying ?. The bottom row shows the number of semantic groups formed for different values of ?.
For each image, each noun is assigned an individual relevance score, which is the number of captions
that mentions the noun. This upweights important objects while downweighting less important objects
(or errors from the annotator or the parser). The result is a list of nouns that humans use to describe
objects present in the image, each weighted by its relevance score. We assume nouns that appear in the
same WordNet synset (?bicycle? and ?bike?) are synonyms and that they refer to the same object instance
in the image. Hence, we group them as a single label-group, with the relevance score taken to be the
maximum relevance score among the nouns in the group.
Since there are only five captions per image, the proposed approach will result in a sparse set of
keywords. This mirrors the problem described in Section 1 where systems have to ?guess? the so-called
?correct? labels, thus allowing us to demonstrate the effectiveness of our ?granularity-aware? re-rankings.
In order to compare the annotations against the re-rankings, we will need to map the keywords to the
semantic groupings. This is done by matching the nouns to any of the terms in a semantic group, with a
corresponding label `(G
?
w
) for each group (Section 3.2). Nouns assigned the same label are merged, with
the new relevance score being the maximum relevance score among the nouns. If a noun matches more
than one semantic group (polyseme/homonym), we treat all groups as relevant and divide the relevance
score uniformly among the groups. Evaluation is then performed by matching the semantic group labels
against the image annotation output.
4 Experimental evaluation
Our proposed method is evaluated on the dataset and categories as will be described in Section 4.1, by re-
ranking the output of the visual classifiers in Section 4.2. The effects of semantic groupings are explored
using different settings of ? (see Section 3.1).
Baseline. To ensure any improvements in scores are not purely as a result of having a shorter list of
concepts to rank, we compare the results to a set of baseline groupings where synsets are grouped in a
random manner. For a fair comparison the baselines contain the same number of groups and cluster size
distributions as our semantic groupings.
4.1 Dataset and Object Categories
The Flickr8k dataset (Hodosh et al., 2013) is used in our image annotation experiments. The dataset
contains 8,091 images, each annotated with five textual descriptions. To demonstrate the notion of gran-
ularity in large-scale object hierarchies, we use as object categories synset nodes from WordNet (Fell-
baum, 1998). Ideally, we would like to be able to train visual classifiers for all synset categories in
ImageNet (Deng et al., 2009). However, we limit the categories to only synsets with terms occurring in
the textual descriptions of the Flickr8k dataset to reduce computational complexity, and regard the use
of more categories as future work. This results in a total of 1,372 synsets to be used in our experiments.
The synsets include both leaf nodes as well as internal nodes in the WordNet hierarchy.
4.2 Visual classifier
Deep learning (LeCun et al., 1989; Hinton and Salakhutdinov, 2006) based approaches have be-
come popular in visual recognition following the success of deep convolutional neural networks
42
(CNN) (Krizhevsky et al., 2012) in the ImageNet Large Scale Visual Recognition Challenge 2012
(ILSVRC12) (Deng et al., 2012a). Donahue et al. (2013) report that features extracted from the acti-
vation of a deep CNN trained in a fully supervised fashion can also be re-purposed to novel generic tasks
that differ significantly from the original task. Inspired by Donahue et al. (2013), we extract such acti-
vation as feature for ImageNet images that correspond to the 1,372 synsets, and train binary classifiers
to detect the presence of the synsets in the images of Flickr8k. More specifically, we use as our training
set the 1,571,576 ImageNet images in the 1,372 synsets, where a random sample of 5,000 images serves
as negative examples, and as our test set the 8,091 images in Flickr8k. For each image in both sets, we
extracted activation of a pre-trained CNN model as its feature. The model is a reference implementation
of the structure proposed in Krizhevsky et al. (2012) with minor modifications, and is made publicly
available through the Caffe project (Jia, 2013). It is shown in Donahue et al. (2013) that the activation
of layer 6 of the CNN performs the best for novel tasks. Our study on a toy example with 10 ImageNet
synsets however suggests that the activation of layer 7 has a small edge. Once the 4,096 dimensional
activation of layer 7 is extracted for both training and test sets, 1,372 binary classifiers are trained and
applied using LIBSVM (Chang and Lin, 2011), which give probability estimates for the test images. For
each image, the 1,372 classifiers are then ranked in order of their probability estimates.
4.3 Evaluation measure
The systems are evaluated using the Normalized Discounted Cumulative Gain (NDCG) (Wang et al.,
2013) measure. This measure is commonly used in Information Retrieval (IR) to evaluate ranked retrieval
results where each document is assigned a relevance score. This measure favours rankings where the
most relevant items are ranked ahead of less relevant items, and does not penalize irrelevant items.
The NDCG at position k, NDCG
k
, for a set of test images I is defined as:
NDCG
k
(I) =
1
|I|
|I|
?
i=1
1
IDCG
k
(i)
k
?
p=1
2
R
p
? 1
log
2
(1 + p)
(4)
where R
p
is the relevance score of the concept at position p, and IDCG
k
(i) is the ideal discounted
cumulative gain for a perfect ranking algorithm at position k, which normalizes the overall measure to
be between 0.0 to 1.0. This makes the scores comparable across rankings regardless of the number of
synset groups involved. For each grouping, we report the results of NDCG
k
for the largest possible k
(i.e. the number of synset groups), which gives the overall performance of the rankings.
4.4 Results
Table 1 shows the results of re-ranking the output of the visual classifiers (Section 4.2), with different
semantic groupings formed by varying ?. The effects of the proposed groupings is apparent when com-
pared to the random baseline groupings. As we increase the value of ? (allowing groups to have a larger
range of granularity), the NDCG scores also consistently increase. However, higher NDCG scores do
not necessarily equate to better groupings, as semantic groups with too much flexibility in granularity
levels may end up being less informative, for example by annotating a ?being? in an image. The in-
formativeness of the groupings is a subjective issue depending on the context, and makes an interesting
open question. To provide insight into the effects of our groupings, Figure 1 shows an example where at
low levels of ? (rigid flexibility), the various dog species are highly ranked but none of them is consid-
ered relevant by the evaluation system. However, at ? = 0.5 most dog species are grouped as a ?dog?
semantic group resulting in a highly relevant prediction, while at the same time allowing the ?sidewalk?
group to rise higher in the rankings. At higher levels of ?, however, the semantic groupings become less
informative when superordinate groups like ?being?, ?artifact? and ?equipment? are formed, suggesting
that higher flexibility with granularity levels may not always be more informative.
5 Conclusions and future work
We presented a ?granularity-aware? approach to grouping semantically related concepts across different
levels of granularity, taking into consideration that different people describe the same thing in different
43
dog (5)
road (2)
pavement (1)
street (1)
? = 0.0
score: 0.241
beagle
boston terrier
corgi
basset
hound
spaniel
border collie
terrier
dachshund
pup
st bernard
bulldog
springer spaniel
leash
kitten
pet
dog
sheepdog
penguin
sidewalk
? = 0.3
score: 0.480
beagle
boston terrier
dog
basset
hound
spaniel
border collie
terrier
dachshund
pup
bulldog
springer spaniel
leash
kitten
pet
penguin
sidewalk
doberman
collie
cat
? = 0.5
score: 0.941
dog
animal
leash
kitten
pet
penguin
sidewalk
cat
artifact
person
student
goat
livestock
rabbit
duck
baseball
chair
child
frisbee
spectator
? = 0.8
score: 0.943
dog
animal
leash
being
bird
sidewalk
cat
artifact
student
baseball
chair
child
frisbee
ball
slope
equipment
fabric
rug
seat
support
? = 1.0
score: 0.944
dog
animal
leash
being
bird
sidewalk
cat
artifact
sport
chair
child
device
ball
slope
equipment
fabric
rug
seat
support
stick
Figure 1: Example re-ranking of our visual classifier by semantic groupings, for selected values of
?. Words directly below the image indicate the ?gold standard? nouns extracted automatically from its
corresponding five captions. The number next to each noun indicate its relevance score. For each re-
ranking, we show the labels representing the semantic groupings. Italicized labels indicate a match with
the (grouped) ?gold standard? nouns (see Section 3.3).
ways, and at varied levels of specificity. To gain insight into the effects of our semantic groupings on
human-centric applications, the proposed idea was investigated in the context of re-ranking the output
of visual classifiers, and was also incorporated during evaluation against human descriptions. We found
that although the groupings help provide a more human-centric and flexible image annotation system,
too much flexibility may result in an uninformative image annotation system. Future work could include
(i) exploring different ways of grouping concepts; (ii) incorporating the output of visual classifiers to
improve both groupings and rankings; (iii) using information from more textual sources to improve
image annotation; (iv) taking the approach further to generate full sentence annotations. We believe that
these steps are important to bridge the semantic gap between computer vision and natural language.
Acknowledgements
The authors would like to acknowledge funding from the EU CHIST-ERA D2K programme, EPSRC
grant reference: EP/K01904X/1.
References
Tamara L. Berg, Alexander C. Berg, and Jonathan Shih. 2010. Automatic attribute discovery and characterization
from noisy web data. In Proceedings of ECCV, volume 1, pages 663?676.
Irving Biederman. 1995. Visual object recognition. In S. F. Kosslyn and D. N. Osherson, editors, An Invitation to
Cognitive Science, 2nd edition, Volume 2, Visual Cognition, pages 121?165. MIT Press.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1. In Linguistic Data Consortium.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions
on Intelligent Systems and Technology, 2(3):1?27. http://www.csie.ntu.edu.tw/
?
cjlin/libsvm.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical
image database. In Proceedings of CVPR.
Jia Deng, Alexander C. Berg, Sanjeev Satheesh, Hao Su, Aditya Khosla, and Li Fei-Fei. 2012a. ImageNet large
scale visual recognition challenge (ILSVRC) 2012. http://image-net.org/challenges/LSVRC/2012/.
Jia Deng, Jonathan Krause, Alexander C. Berg, and Li Fei-Fei. 2012b. Hedging your bets: Optimizing accuracy-
specificity trade-offs in large scale visual recognition. In Proceedings of CVPR.
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. 2013.
DeCAF: A deep convolutional activation feature for generic visual recognition. arXiv:1310.1531 [cs.CV].
44
Pinar Duygulu, Kobus Barnard, Nando de Freitas, and David A. Forsyth. 2002. Object recognition as machine
translation: Learning a lexicon for a fixed image vocabulary. In Proceedings of ECCV, pages 97?112.
Ali Farhadi, Ian Endres, Derek Hoiem, and David A. Forsyth. 2009. Describing objects by their attributes. In
Proceedings of CVPR.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.
Yansong Feng and Mirella Lapata. 2008. Automatic image annotation using auxiliary text information. In Pro-
ceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Tech-
nologies, pages 272?280. Association for Computational Linguistics.
Abhinav Gupta and Larry S. Davis. 2008. Beyond nouns: Exploiting prepositions and comparative adjectives for
learning visual classifiers. In Proceedings of ECCV, pages 16?29.
Geoffrey E. Hinton and Ruslan R. Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks.
Science, 313:504?507.
Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing image description as a ranking task: Data,
models and evaluation metrics. Journal of Artificial Intelligence Research, 47:853?899.
Yangqing Jia. 2013. Caffe: An open source convolutional architecture for fast feature embedding. http://
caffe.berkeleyvision.org.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. 2012. ImageNet classification with deep convolutional
neural networks. In Advances in Neural Information Processing Systems.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg.
2011. Baby talk: Understanding and generating simple image descriptions. In Proceedings of CVPR.
Chris H. Lampert, Hannes Nickisch, and Stefan Harmeling. 2009. Learning to detect unseen object classes by
between-class attribute transfer. In Proceedings of CVPR.
Y. LeCun, B. Boser, J. Denker, D. Henerson, R. Howard, W. Hubbard, and L. Jackel. 1989. Backpropagation
applied to handwritten zip code recognition. Neural Computation, 1(4):541?551.
Marcin Marsza?ek and Cordelia Schmid. 2008. Constructing category hierarchies for visual recognition. In David
Forsyth, Philip Torr, and Andrew Zisserman, editors, Proceedings of ECCV, volume 5305 of Lecture Notes in
Computer Science, pages 479?491. Springer Berlin Heidelberg.
Vicente Ordonez, Jia Deng, Yejin Choi, Alexander C. Berg, and Tamara L. Berg. 2013. From large scale image
categorization to entry-level categories. In Proceedings of ICCV.
Llu??s Padr?o and Evgeny Stanilovsky. 2012. Freeling 3.0: Towards wider multilinguality. In Proceedings of the
Language Resources and Evaluation Conference, LREC ?12, Istanbul, Turkey, May. ELRA.
John C. Platt. 2000. Probabilities for SV machines. Advances in Large-Margin Classifiers, pages 61?74.
Abebe Rorissa. 2008. User-generated descriptions of individual images versus labels of groups of images: A
comparison using basic level theory. Information Processing and Management, 44(5):1741?1753.
Olga Russakovsky, Jia Deng, Jonathan Krause, Alexander C. Berg, and Li Fei-Fei. 2013. ImageNet large
scale visual recognition challenge (ILSVRC) 2013. http://image-net.org/challenges/LSVRC/2013/
results.php.
Richard Socher and Li Fei-Fei. 2010. Connecting modalities: Semi-supervised segmentation and annotation of
images using unaligned text corpora. In Proceedings of CVPR, pages 966?973.
Josiah Wang, Katja Markert, and Mark Everingham. 2009. Learning models for object recognition from natural
language descriptions. In Proceedings of BMVC.
Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Wei Chen, and Tie-Yan Liu. 2013. A theoretical analysis of NDCG
ranking measures. In Proceedings of the 26th Annual Conference on Learning Theory (COLT 2013).
Yahoo! Webscope. 2014. Yahoo! Webscope dataset YFCC-100M. http://labs.yahoo.com/Academic_
Relations.
Yezhou Yang, Ching Lik Teo, Hal Daum?e, III, and Yiannis Aloimonos. 2011. Corpus-guided sentence generation
of natural images. In Proceedings of EMNLP, pages 444?454.
45
