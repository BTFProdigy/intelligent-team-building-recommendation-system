Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 297?304
Manchester, August 2008
Dependency-Based N-Gram Models for
General Purpose Sentence Realisation
Yuqing Guo
NCLT, School of Computing
Dublin City University
Dublin 9, Ireland
yguo@computing.dcu.ie
Josef van Genabith
NCLT, School of Computing
Dublin City University
IBM CAS, Dublin, Ireland
josef@computing.dcu.ie
Haifeng Wang
Toshiba (China)
Research & Development Center
Beijing, 100738, China
wanghaifeng@rdc.toshiba.com.cn
Abstract
We present dependency-based n-gram
models for general-purpose, wide-
coverage, probabilistic sentence realisa-
tion. Our method linearises unordered
dependencies in input representations
directly rather than via the application
of grammar rules, as in traditional chart-
based generators. The method is simple,
efficient, and achieves competitive accu-
racy and complete coverage on standard
English (Penn-II, 0.7440 BLEU, 0.05
sec/sent) and Chinese (CTB6, 0.7123
BLEU, 0.14 sec/sent) test data.
1 Introduction
Sentence generation,1 or surface realisation can be
described as the problem of producing syntacti-
cally, morphologically, and orthographically cor-
rect sentences from a given semantic or syntactic
representation.
Most general-purpose realisation systems de-
veloped to date transform the input into sur-
face form via the application of a set of gram-
mar rules based on particular linguistic theories,
e.g. Lexical Functional Grammar (LFG), Head-
Driven Phrase Structure Grammar (HPSG), Com-
binatory Categorial Grammar (CCG), Tree Ad-
joining Grammar (TAG) etc. These grammar rules
are either carefully handcrafted, as those used in
FUF/SURGE (Elhadad, 1991), LKB (Carroll et al,
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1In this paper, the term ?generation? is used generally for
what is more strictly referred to by the term ?tactical genera-
tion? or ?surface realisation?.
1999), OpenCCG (White, 2004) and XLE (Crouch
et al, 2007), or created semi-automatically (Belz,
2007), or fully automatically extracted from an-
notated corpora, like the HPSG (Nakanishi et
al., 2005), LFG (Cahill and van Genabith, 2006;
Hogan et al, 2007) and CCG (White et al,
2007) resources derived from the Penn-II Treebank
(PTB) (Marcus et al, 1993).
Over the last decade, probabilistic models have
become widely used in the field of natural lan-
guage generation (NLG), often in the form of a re-
alisation ranker in a two-stage generation architec-
ture. The two-stage methodology is characterised
by a separation between generation and selection,
in which rule-based methods are used to generate a
space of possible paraphrases, and statistical meth-
ods are used to select the most likely realisation
from the space. By and large, two statistical mod-
els are used in the rankers to choose output strings:
? N-gram language models over different units,
such as word-level bigram/trigram mod-
els (Bangalore and Rambow, 2000; Langk-
ilde, 2000), or factored language models inte-
grated with syntactic tags (White et al, 2007).
? Log-linear models with different syntactic
and semantic features (Velldal and Oepen,
2005; Nakanishi et al, 2005; Cahill et al,
2007).
To date, however, probabilistic models learn-
ing direct mappings from generation input to sur-
face strings, without the effort to construct a gram-
mar, have rarely been explored. An exception is
Ratnaparkhi (2000), who presents maximum en-
tropy models to learn attribute ordering and lexi-
cal choice for sentence generation from a semantic
representation of attribute-value pairs, restricted to
an air travel domain.
297
SNP
PRP
We
VP
VBP
believe
PP
IN
in
NP
NP
DT
the
NN
law
PP
IN
of
NP
NNS
averages
f
1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
PRED ?believe?
TENSE pres
SUBJ f
2
?
?
?
PRED ?pro?
PERS 1
NUM pl
?
?
?
OBL f
3
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
PFORM ?in?
OBJ f
4
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
PRED ?law?
PERS 3
NUM sg
SPEC f
5
[
DET f
6
[
PRED ?the?
]
]
ADJ
?
?
?
?
?
?
?
?
f
7
?
?
?
?
?
?
PFORM ?of?
OBJ f
8
?
?
?
PRED ?average?
PERS 3
NUM pl
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(a.) c-structure (b.) f-structure
string We believe in the law of averages
position 1 2 3 4 5 6 7
f
1
SUBJ PRED OBL
f
3
PFORM OBJ
f
4
SPEC PRED ADJ
f
7
PFORM OBJ
(c.) linearised grammatical functions / bilexical dependencies
Figure 1: C- and f-structures for the sentence We believe in the law of averages.
In this paper, we develop an efficient, wide-
coverage generator based on simple n-gram mod-
els to directly linearise dependency relations from
the input representations. Our work is aimed at
general-purpose sentence generation but couched
in the framework of Lexical Functional Grammar.
We give an overview of LFG and the dependency
representations we use in Section 2. We describe
the general idea of our dependency-based gener-
ation in Section 3 and give details of the n-gram
generation models in Section 4. Section 5 explains
the experiments and provides results for both En-
glish and Chinese data. Section 6 compares the re-
sults with previous work and between languages.
Finally we conclude with a summary and outline
future work.
2 LFG-Based Generation
2.1 Lexical Functional Grammar
Lexical Functional Grammar (Kaplan and Bres-
nan, 1982) is a constraint-based grammar for-
malism which postulates (minimally) two lev-
els of representation: c(onstituent)-structure and
f(unctional)-structure. As illustrated in Figure 1,
a c-structure is a conventional phrase structure
tree and captures surface grammatical configu-
rations. The f-structure encodes more abstract
functional relations like SUBJ(ect), OBJ(ect) and
ADJ(unct). F-structures are hierarchical attribute-
value matrix representations of bilexical labelled
dependencies, approximating to basic predicate-
argument/adjunct structures.2 Attributes in f-
structure come in two different types:
? Grammatical Functions (GFs) indicate the re-
lationship between the predicate and depen-
dents. GFs can be divided into:
? arguments are subcategorised for by the
predicate, such as SUBJ(ect), OBJ(ect),
and thus can only occur once in each lo-
cal f-structure.
? modifiers like ADJ(unct), COORD(inate)
are not subcategorised for by the predi-
cate, and can occur any number of times
in a local f-structure.
? Atomic-valued features describe linguistic
properties of the predicate, such as TENSE,
ASPECT, MOOD, PERS, NUM, CASE etc.
2.2 Generation from F-Structures
Work on generation in LFG generally assumes that
the generation task is to determine the set of strings
of the language that corresponds to a specified f-
structure, given a particular grammar (Kaplan and
Wedekind, 2000). Previous work on generation
2F-structures can be also interpreted as quasi-logical
forms (van Genabith and Crouch, 1996), which more closely
resemble inputs used by some other generators.
298
within LFG includes the XLE,3 Cahill and van
Genabith (2006), Hogan et al (2007) and Cahill et
al. (2007). The XLE generates sentences from f-
structures according to parallel handcrafted gram-
mars for English, French, German, Norwegian,
Japanese, and Urdu. Based on the German XLE
resources, Cahill et al (2007) describe a two-stage,
log-linear generation model. Cahill and van Gen-
abith (2006) and Hogan et al (2007) present a
chart generator using wide-coverage PCFG-based
LFG approximations automatically acquired from
treebanks (Cahill et al, 2004).
3 Dependency-Based Generation: the
Basic Idea
Traditional LFG generation models can be re-
garded as the reverse process of parsing, and
use bi-directional f-structure-annotated CFG rules.
In a sense, the generation process is driven by
an input dependency (or f-structure) representa-
tion, but proceeds through the ?detour? of us-
ing dependency-annotated CFG (or PCFG) gram-
mars and chart-based generators. In this paper,
we develop a simple n-gram and dependency-
based, wide-coverage, robust, probabilistic gener-
ation model, which cuts out the middle-man from
previous approaches: the CFG-component.
Our approach is data-driven: following the
methodology in (Cahill et al, 2004; Guo et al,
2007), we automatically convert the English Penn-
II treebank and the Chinese Penn Treebank (Xue
et al, 2005) into f-structure banks. F-structures
such as Figure 1(b.) are unordered, i.e. they do
not carry information on to the relative surface or-
der of local GFs. In order to generate a string
from an f-structure, we need to linearise the GFs
(at each level of embedding) in the f-structure (and
map lemmas and features to surface forms). We
do this in terms of n-gram models over GFs. In or-
der to build the n-gram models, we linearise the f-
structures automatically produced from treebanks
by associating the numerical string position (word
offset from start of the sentence) with the predicate
in each local f-structure, producing GF sequences
as in Figure 1(c.).
Even though the n-gram models are exemplified
using LFG f-structures, they are general-purpose
models and thus suitable for any bilexical labelled
dependency (Nivre, 2006) or predicate-argument
type representations, such as the labelled feature-
3http://www2.parc.com/isl/groups/nltt/xle/
value structures used in HALogen and the func-
tional descriptions in the FUF/SURGE system.
4 N-Gram Models for Dependency-Based
Generation
4.1 Basic N-Gram Model
The primary task of a sentence generator is to de-
termine the linear order of constituents and words,
represented as lemmas in predicates in f-structures.
At a particular local f-structure, the task of gen-
erating a string covered by the local f-structure
is equivalent to linearising all the GFs present at
that local f-structure. E.g. in f
4
in Figure 1, the
unordered set of local GFs {SPEC, PRED, ADJ}
generates the surface sequence ?the law of aver-
ages?. We linearise the GFs in the set by com-
puting n-gram models, similar to traditional word-
based language models, except using the names of
GFs (including PRED) instead of words. Given
a (sub-) f-structure F containing m GFs, the n-
gram model searches for the best surface sequence
S
m
1
=s
1
...s
m
generated by the GF linearisation
GF
m
1
= GF
1
...GF
m
, which maximises the prob-
ability P (GFm
1
). Using n-gram models, P (GFm
1
)
is calculated according to Eq.(1).
P (GF
m
1
) = P (GF
1
...GF
m
)
=
m
?
k=1
P (GF
k
|GF
k?1
k?n+1
) (1)
4.2 Factored N-Gram Models
In addition to the basic n-gram model over bare
GFs, we integrate contextual and fine-grained
lexical information into several factored models.
Eq.(2) additionally conditions the probability of
the n-gram on the parent GF label of the cur-
rent local f-structure f
i
, Eq.(3) on the instantiated
PRED of the local f-structure f
i
, and Eq.(4) lexi-
calises the model, where each GF is augmented
with its own predicate lemma.
P
g
(GF
m
1
) =
m
?
k=1
P (GF
k
|GF
k?1
k?n+1
, GF
i
) (2)
P
p
(GF
m
1
) =
m
?
k=1
P (GF
k
|GF
k?1
k?n+1
, P red
i
) (3)
P
l
(GF
m
1
) =
m
?
k=1
P (Lex
k
|Lex
k?1
k?n+1
) (4)
299
To avoid data sparseness, the factored n-gram
models P f are smoothed by linearly interpolating
the basic n-gram model P , as in Eq.(5).
?
P
f
(GF
m
1
) = ?P
f
(GF
m
1
) + (1? ?)P (GF
m
1
) (5)
Additionally, the lexicalised n-gram models P l
are combined with the other two models con-
ditioned on the additional parent GF P g and
PRED P p, as shown in Eqs. (6) & (7), respectively.
?
P
lg
(GF
m
1
) = ?
1
P
l
(GF
m
1
) + ?
2
P
g
(GF
m
1
)
+?
3
P (GF
m
1
) (6)
?
P
lp
(GF
m
1
) = ?
1
P
l
(GF
m
1
) + ?
2
P
p
(GF
m
1
)
+?
3
P (GF
m
1
) (7)
where
?
?
i
= 1
Table 1 exemplifies the different n-gram models
for the local f-structure f
4
in Figure 1.
Model N-grams Cond.
basic (P ) SPEC PRED ADJ
gf (P g ) SPEC PRED ADJ OBL
pred (Pp) SPEC PRED ADJ ?law?
lex (P l) SPEC PRED[?law?] ADJ[?of?]
Table 1: Examples of n-grams for f
4
in Figure 1
Besides grammatical functions, we also make
use of atomic-valued features like TENSE, PERS,
NUM (etc.) to aid linearisation. The attributes and
values of these features are integrated into the GF
n-grams for disambiguation (see Section 5.2).
4.3 Generation Algorithm
Our basic n-gram based generation model im-
plements the simplifying assumption that lineari-
sation at one sub-f-structure is independent of
linearisation at any other sub-f-structures. This
assumption is feasible for projective dependen-
cies. In most cases (at least in English and
Chinese), non-projective dependencies are only
used to account for Long-Distance Dependen-
cies (LDDs). Consider sentence (1) discussed
in Carroll et al (1999) and its corresponding f-
structure in Figure 2. In LFG f-structures, LDDs
are represented via reentrancies between ?dislo-
cated? TOPIC, TOPIC REL, FOCUS (etc.) GFs and
?source? GFs subcategorised for by local predi-
cates, but only the dislocated GFs are instantiated
in generation. Therefore traces of the source GFs
in input f-structures are removed before genera-
tion, and non-projective dependencies are trans-
formed into simple projective dependencies.
(1) How quickly did the newspapers say the ath-
lete ran?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
FOCUS
?
?
?
PRED ?quickly?
ADJ
{
[
PRED ?how?
]
}
?
?
?
1
PRED ?say?
SUBJ
?
?
PRED ?newspaper?
SPEC
[
PRED ?the?
]
?
?
COMP
?
?
?
?
?
?
?
?
PRED ?run?
SUBJ
?
?
PRED ?athlete?
SPEC
[
PRED ?the?
]
?
?
ADJ 1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 2: schematic f-structure for How quickly
did the newspapers say the athlete ran?
In summary, given an input f-structure f , the
core algorithm of the generator recursively tra-
verses f and at each sub-f-structure f
i
:
1. instantiates the local predicate at f
i
and per-
forms inflections/declensions if necessary
2. calculates the GF linearisations present at f
i
by n-gram models
3. finds the most probable GF sequence among
all possibilities by Viterbi search
4. generates the string covered by f
i
according
to the linearised GFs
5 Experiments and Evaluation
To test the performance and coverage of our n-
gram-based generation models, experiments are
carried out for both English and Chinese, two lan-
guages with distinct properties.
5.1 Experiment Design
Experiments on English data are carried out on
the WSJ portion of the PTB, using standard train-
ing/test/development splits, viz 39,832 sentences
from sections 02-21 are used for training, 2,416
sentences from section 23 for testing, while 1,700
sentences from section 22 are held out for develop-
ment. The latest version of the Penn Chinese Tree-
bank 6.0 (CTB6), excluding the portion of ACE
broadcast news, is used for experiments on Chi-
nese data.4 We follow the recommended splits (in
the list-of-file of CTB6) to divide the data into test
set, development set and training set. The training
set includes 756 files with a total of 15,663 sen-
tences. The test set includes 84 files with 1,708
4Sentences labelled as fragment are not included in our
development and test set.
300
sentences. The development set includes 50 files
with 1,116 sentences. Table 2 shows some of the
characteristics of the English and Chinese data ob-
tained from the development sets.
Development Set English Chinese
num of sent 1,700 1,116
max length of sent (#words) 110 145
ave length of sent (#words) 23 31
num of local fstr 23,289 15,847
num of local fstr per sent 13.70 14.20
max length of local fstr (#gfs) 12 16
ave length of local fstr (#gfs) 2.56 2.90
Table 2: Comparison English and Chinese data
The n-gram models are created using the
SRILM toolkit (Stolcke, 2002) with Good-Turning
smoothing for both the Chinese and English data.
For morphological realisation of English, a set of
lexical macros is automatically extracted from the
training data. This is not required for Chinese sur-
face realisation as Chinese has very little morphol-
ogy. Lexical macro examples are listed in Table 3.
lexical macro surface word
pred=law, num=sg, pers=3 law
pred=average, num=pl, pers=3 averages
pred=believe, num=pl, tense=pres believe
Table 3: Examples of lexical macros
The input to our generator are unordered f-
structures automatically derived from the develop-
ment and test set trees of our treebanks, which do
not contain any string position information. But,
due to the particulars of the automatic f-structure
annotation algorithm, the order of sub-f-structures
in set-valued GFs, such as ADJ, COORD, happens
to correspond to their surface order. To avoid un-
fairly inflating evaluation results, we lexically re-
order the GFs in each sub-f-structure of the devel-
opment and test input before the generation pro-
cess. This resembles the ?permute, no dir? type
experiment in (Langkilde, 2002).
5.2 Experimental Results
Following (Langkilde, 2002) and other work
on general-purpose generators, BLEU score (Pa-
pineni et al, 2002), average NIST simple
string accuracy (SSA) and percentage of exactly
matched sentences are adopted as evaluation met-
rics. As our system guarantees that all input f-
structures can generate a complete sentence, spe-
cial coverage-dependent evaluation (as has been
adopted in most grammar-based generation sys-
tems) is not necessary in our experiments.
Experiments are carried out on an Intel Pentium
4 server, with a 3.80GHz CPU and 3GB mem-
ory. It takes less than 2 minutes to generate all
2,416 sentences (with average sentence length of
21 words) of WSJ section 23 (average 0.05 sec per
sentence), and approximately 4 minutes to gener-
ate 1,708 sentences (with average sentence length
of 30 words) of CTB test data (average 0.14 sec
per sentence), using 4-gram models in all experi-
ments. Our evaluation results for English and Chi-
nese data are shown in Tables 4 and 5, respectively.
Different n-gram models perform nearly consis-
tently in all the experiments on both English and
Chinese data. The results show that factored n-
gram models outperform the basic n-gram models,
and in turn the combined n-gram models outper-
form single n-gram models. The combined model
interpolating n-grams over lexicalised GFs with n-
grams conditioned on PRED achieves the best re-
sults in both experiments on English (with feature
names) and Chinese (with feature names & val-
ues), with BLEU scores of 0.7440 and 0.7123 re-
spectively, and full coverage.
Lexicalisation plays an important role in both
English and Chinese, boosting the BLEU score
without features from 0.5074 to 0.6741 for En-
glish, and from 0.5752 to 0.6639 for Chinese.
Atomic-valued features play an important role
in English, and boost the BLEU score from 0.5074
in the baseline model to 0.6842 when feature
names are integrated into the n-gram models.
However, feature names in Chinese only increase
the BLEU score from 0.5752 to 0.6160. This
is likely to be the case as English has a richer
morphology than Chinese, and important func-
tion words such as ?if?, ?to?, ?that? are encoded
in atomic-valued features in English f-structures,
which helps to determine string order. However,
combined feature names and values work better on
Chinese data, but turn out to hurt the n-gram model
performance for English data. This may suggest
that the feature names in English already include
enough information, while the value of morpho-
logical features, such as TENSE, NUM does not pro-
vide any new information to help determine word
order, but aggravate data sparseness instead.
301
WSJ Sec23 Without Features Feature Names Feature Names & Values
Model ExMatch BLEU SSA ExMatch BLEU SSA ExMatch BLEU SSA
baseline 5.30% 0.5074 57.29% 15.27% 0.6842 69.48% 15.15% 0.6829 69.15%
gf 6.62% 0.5318 60.06% 16.76% 0.6969 71.51% 16.68% 0.6977 71.55%
pred 8.03% 0.5697 60.73% 16.72% 0.7035 70.12% 16.76% 0.7042 71.08%
lex 12.87% 0.6741 69.43% 19.41% 0.7384 74.76% 18.96% 0.7375 74.12%
lex+gf 12.62% 0.6611 69.41% 19.70% 0.7388 74.98% 19.74% 0.7405 75.08%
lex+pred 12.25% 0.6569 68.04% 19.83% 0.7440 75.34% 19.58% 0.7422 75.04%
Table 4: Results for English Penn-II WSJ section 23
Test Without Features Feature Names Feature Names & Values
Model ExMatch BLEU SSA ExMatch BLEU SSA ExMatch BLEU SSA
baseline 8.96% 0.5752 51.92% 11.77% 0.6160 54.64% 12.30% 0.6239 55.20%
gf 9.54% 0.6009 53.02% 12.53% 0.6391 55.78% 13.47% 0.6486 56.60%
pred 10.07% 0.6180 53.80% 13.35% 0.6608 56.72% 14.46% 0.6720 57.67%
lex 13.93% 0.6639 59.61% 15.16% 0.6770 60.44% 15.98% 0.6804 60.20%
lex+gf 14.81% 0.6773 59.92% 15.52% 0.6911 60.97% 16.80% 0.6957 61.07%
lex+pred 16.04% 0.6952 60.82% 16.22% 0.7060 61.45% 17.51% 0.7123 61.54%
Table 5: Results for Chinese CTB6 test data
WSJ Sec23 Sentence length ? 20 words All sentences
Coverage ExMatch BLEU SSA Coverage ExMatch BLEU SSA
Langkilde(2002) 82.7% 28.2% 0.757 69.6%
Callaway(2003) 98.7% 49.0% 88.84%
Nakanishi(2005) 90.75% 0.7733 83.6% 0.705
Cahill(2006) 98.65% 0.7077 73.73% 98.05% 0.6651 68.08%
Hogan(2007) 100% 0.7139 99.96% 0.6882 70.92%
White(2007) 94.3% 6.9% 0.5768
this paper 100% 35.40% 0.7625 81.09% 100% 19.83% 0.7440 75.34%
Table 6: Cross system comparison of results for English WSJ section 23
6 Discussion
6.1 Comparison to Previous Work
It is very difficult to compare sentence generators
since the information contained in the input rep-
resentation varies greatly between systems. The
most direct comparison is between our system and
those presented in Cahill and van Genabith (2006)
and Hogan et al (2007), as they also use treebank-
based automatically generated f-structures as the
generator inputs. The labelled feature-value struc-
tures used in HALogen (Langkilde, 2002) and
functional descriptions in FUF/SURGE (Callaway,
2003) also bear some broad similarities to our f-
structures. A number of systems using different
input but adopting the same evaluation metrics and
testing on the same data are listed in Table 6.
Surprisingly (or not), the best results are
achieved by a purely symbolic generation
system?FUF/SURGE (Callaway, 2003). How-
ever the approach uses handcrafted grammars
which are very time-consuming to produce and
adapt to different languages and domains. Langk-
ilde (2002) reports results for experiments with
varying levels of linguistic detail in the input
given to the generator. The type ?permute, no dir?
is most comparable to the level of information
contained in our f-structure in that the modifiers
(adjuncts, coordinates etc.) in the input are not
ordered. However her labelled feature-value
structure is more specific than our f-structure
as it also includes syntactic properties such as
part-of-speech, which might contribute to the
higher BLEU score of HALogen. And moreover,
in HALogen nearly 20% of the sentences are only
partially generated (or not at all). Nakanishi et
al. (2005) carry out experiments on sentences up
to 20 words, with BLEU scores slightly higher
than ours. However their results without sentence
length limitation (listed in the right column), for
500 sentences randomly selected from WSJ Sec22
are lower than ours, even at a lower coverage.
Overall our system is competitive, with best results
for coverage (100%), second best for BLEU and
SSA scores, and third best overall on exact match.
However, we admit that automatic metrics such as
BLEU are not fully reliable to compare different
systems, and results vary widely depending on the
coverage of the systems and the specificity of the
generation input.
302
6.2 Error Analysis and Differences Between
the Languages
Though our dependency-based n-gram models per-
form well in both the English and Chinese exper-
iments, we are surprised that experiments on En-
glish data produce better results than those for Chi-
nese. It is widely accepted that English generation
is more difficult than Chinese, due to morpholog-
ical inflections and the somewhat less predictable
word order of English compared to Chinese. This
is reflected by the results of the baseline models.
Chinese has a BLEU score of 0.5752 and 8.96%
exact match, both are higher than those of English.
However with feature augmentation and lexicali-
sation, the results for English data exceed Chinese.
This is probably because of the following reasons:
Data size of the English training set is more than
twice that of Chinese.
Grammatical functions are more fine-grained
in English f-structures than those in Chinese.
There are 32 GFs defined for English compared to
20 for Chinese in our input f-structures.
Properties of the languages and data sets are
different. For example, due to lack of inflection
and case markers, many sequences of VPs in Chi-
nese have to be treated as coordinates, whereas
their counterparts in English act as different gram-
matical functions, e.g. (2).
(2) ?? z ,?????
invest million build this construction
?invest million yuan to build the construction?
This results in a total of 7,377 coordinates (4.32
per sentence) in the Chinese development data,
compared to 2,699 (1.12 per sentence) in the En-
glish data. The most extreme case in the Chinese
data features 14 coordinates of country names in
a local f-structure. This may account for the low
SSA score for the Chinese experiments, as many
coordinates are tied in the n-gram scoring method
and can not be ordered correctly. Examining the
development data shows different types of coordi-
nation errors:
? syntactic coordinates, but not semantic coor-
dinates, as in sentence (2).
? syntactic and semantic coordinates, but usu-
ally expressed in a fixed order, e.g. (3).
(3) U? m?
reform opening-up
?reform and opening up?
? syntactic and semantic coordinates, which
can freely swap positions, e.g. (4).
(4) ? ?? ? ?$g?
plentiful energy and quick thinking
?energetic and agile?
At the current stage, our n-gram generation
model only keeps the most likely realisation for
each local f-structure. We believe that packing all
equivalent elements, like coordinates in a local f-
structure into equivalent classes, and outputing n-
best candidate realisations will greatly increase the
SSA score and may also further benefit the effi-
ciency of the algorithm.
7 Conclusions and Further Work
We have described a number of increasingly so-
phisticated n-gram models for sentence genera-
tion from labelled bilexical dependencies, in the
form of LFG f-structures. The models include
additional conditioning on parent GFs and differ-
ent degrees of lexicalisation. Our method is sim-
ple, highly efficient, broad coverage and accurate
in practice. We present experiments on English
and Chinese, showing that the method generalises
well to different languages and data sets. We are
currently exploring further combinations of con-
ditioning context and lexicalisation, application to
different languages and to dependency represen-
tations used to train state-of-the-art dependency
parsers (Nivre, 2006).
Acknowledgments
This research is funded by Science Foundation Ire-
land grant 04/IN/I527. We thank Aoife Cahill for
providing the treebank-based LFG resources for
the English data. We gratefully acknowledge the
feedback provided by our anonymous reviewers.
References
Bangalore, Srinivas and Rambow, Owen. 2000. Ex-
ploiting a Probabilistic Hierarchical Model for Gen-
eration. Proceedings of the 18th International
Conference on Computational Linguistics, 42?48.
Saarbru?cken, Germany.
Belz, Anja. 2007. Probabilistic Generation of Weather
Forecast Texts. Proceedings of the Conference of the
North American Chapter of the Association for Com-
putational Linguistics, 164?171. New York.
Cahill, Aoife, Burke, Michael, O?Donovan, Ruth, van
Genabith, Josef and Way, Andy. 2004. Long-
Distance Dependency Resolution in Automatically
303
Acquired Wide-Coverage PCFG-Based LFG Ap-
proximations. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics, 320-327. Barcelona, Spain.
Cahill, Aoife and van Genabith, Josef. 2006. Ro-
bust PCFG-Based Generation Using Automatically
Acquired LFG Approximations. Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, 1033?1040.
Sydney, Australia.
Cahill, Aoife, Forst, Martin and Rohrer, Christian.
2007. Stochastic Realisation Ranking for a Free
Word Order Language. Proceedings of the 11th Eu-
ropean Workshop on Natural Language Generation,
17?24. Schloss Dagstuhl, Germany.
Callaway, Charles B.. 2003. Evaluating Coverage for
Large Symbolic NLG Grammars. Proceedings of the
Eighteenth International Joint Conference on Artifi-
cial Intelligence, 811?817. Acapulco, Mexico.
Carroll, John, Copestake, Ann, Flickinger, Dan and
Poznanski, Victor. 1999. An efficient chart gen-
erator for (semi-)lexicalist grammars. Proceedings
of the 7th European Workshop on Natural Language
Generation, 86?95. Toulouse, France.
Crouch, Dick, Dalrymple, Mary, Kaplan, Ron, King,
Tracy, Maxwell, John and Newman, Paula. 2007.
XLE Documentation. Palo Alto Research Center,
CA.
Elhadad, Michael. 1991. FUF: The universal unifier
user manual version 5.0. Technical Report CUCS-
038-91. Dept. of Computer Science, Columbia Uni-
versity.
Guo, Yuqing and van Genabith, Josef and Wang,
Haifeng. 2007. Treebank-based Acquisition of LFG
Resources for Chinese. Proceedings of LFG07 Con-
ference, 214?232. Stanford, CA, USA.
Hogan, Deirdre Cafferkey, Conor Cahill, Aoife and van
Genabith, Josef. 2007. Exploiting Multi-Word Units
in History-Based Probabilistic Generation. Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
CoNLL, 267?276. Prague, Czech Republic.
Kaplan, Ronald and Bresnan, Joan. 1982. Lexical
Functional Grammar: a Formal System for Gram-
matical Representation. The Mental Representation
of Grammatical Relations, 173?282. MIT Press,
Cambridge.
Kaplan, Ronald and Wedekind, Jurgen. 2000. LFG
Generation Produces Context-free Languages. Pro-
ceedings of the 18th International Conference on
Computational Linguistics, 425?431. Saarbru?cken,
Germany.
Langkilde, Irene. 2000. Forest-Based Statistical Sen-
tence Generation. Proceedings of 1st Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, 170?177. Seattle, WA.
Langkilde, Irene. 2002. An Empirical Verification
of Coverage and Correctness for a General-Purpose
Sentence Generator. Proceedings of the Second In-
ternational Conference on Natural Language Gener-
ation, 17?24. New York, USA.
Marcus, Mitchell P., Santorini, Beatrice and
Marcinkiewicz, Mary Ann. 1993. Building a
large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2).
Nakanishi, Hiroko and Nakanishi, Yusuke and Tsu-
jii, Jun?ichi. 2005. Probabilistic Models for Dis-
ambiguation of an HPSG-Based Chart Generator.
Proceedings of the 9th International Workshop on
Parsing Technology, 93?102. Vancouver, British
Columbia.
Nivre, Joakim. 2006. Inductive Dependency Parsing.
Springer.
Papineni, Kishore, Roukos, Salim, Ward, Todd and
Zhu, Wei-Jing. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, 311-318. Philadelphia, USA.
Ratnaparkhi, Adwait. 2000. Trainable methods for nat-
ural language generation. Proceedings of NAACL
2000, 194?201. Seattle, WA.
Stolcke, Andreas. 2002. SRILM-An Extensible Lan-
guage Modeling Toolkit. Proceedings of Interna-
tional Conference of Spoken Language Processing.
Denver, Colorado.
van Genabith, Josef and Crouch, Dick. 1996. Di-
rect and underspecified interpretations of LFG f-
structures. Proceedings of the 16th conference on
Computational linguistics, 262?267. Copenhagen,
Denmark
Velldal, Erik and Oepen, Stephan. 2005. Maximum
entropy models for realization ranking. Proceedings
of the MTSummit ?05.
White, Michael. 2004. Reining in CCG Chart Realiza-
tion. Proceedings of the third International Natural
Language Generation Conference. Hampshire, UK.
White, Michael, Rajkumar, Rajakrishnan and Martin,
Scott. 2007. Towards Broad Coverage Surface Re-
alization with CCG. Proceedings of the MT Summit
XI Workshop, 22?30. Copenhagen, Danmark.
Xue, Nianwen, Xia, Fei, Chiou, Fu dong and Palmer,
Martha. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural
Language Engineering, 11(2): 207?238.
304
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 257?266, Prague, June 2007. c?2007 Association for Computational Linguistics
Recovering Non-Local Dependencies for Chinese
Yuqing Guo
NCLT, School of Computing
Dublin City University
Dublin 9, Ireland
yguo@computing.dcu.ie
Haifeng Wang
Toshiba (China)
Research and Development Center
Beijing, 100738, China
wanghaifeng@rdc.toshiba.com.cn
Josef van Genabith
NCLT, School of Computing
Dublin City University
IBM CAS, Dublin, Ireland
josef@computing.dcu.ie
Abstract
To date, work on Non-Local Dependencies
(NLDs) has focused almost exclusively on
English and it is an open research question
how well these approaches migrate to other
languages. This paper surveys non-local de-
pendency constructions in Chinese as repre-
sented in the Penn Chinese Treebank (CTB)
and provides an approach for generating
proper predicate-argument-modifier struc-
tures including NLDs from surface context-
free phrase structure trees. Our approach re-
covers non-local dependencies at the level
of Lexical-Functional Grammar f-structures,
using automatically acquired subcategorisa-
tion frames and f-structure paths linking an-
tecedents and traces in NLDs. Currently our
algorithm achieves 92.2% f-score for trace
insertion and 84.3% for antecedent recovery
evaluating on gold-standard CTB trees, and
64.7% and 54.7%, respectively, on CTB-
trained state-of-the-art parser output trees.
1 Introduction
A substantial number of linguistic phenomena such
as topicalisation, relativisation, coordination and
raising & control constructions, permit a constituent
in one position to bear the grammatical role asso-
ciated with another position. These relationships
are referred to Non-Local Dependencies (NLDs),
where the surface location of the constituent is
called /antecedent0, and the site where the an-
tecedent should be interpreted semantically is called
/trace0. Capturing non-local dependencies is cru-
cial to the accurate and complete determination of
semantic interpretation in the form of predicate-
argument-modifier structures or deep dependencies.
However, with few exceptions (Model 3 of
Collins, 1999; Schmid, 2006), output trees pro-
duced by state-of-the-art broad coverage statistical
parsers (Charniak, 2000; Bikel, 2004) are only sur-
face context-free phrase structure trees (CFG-trees)
without empty categories and coindexation to repre-
sent displaced constituents. Because of the impor-
tance of non-local dependencies in the proper de-
termination of predicate-argument structures, recent
years have witnessed a considerable amount of re-
search on reconstructing such hidden relationships
in CFG-trees. Three strategies have been proposed:
(i) post-processing parser output with pattern match-
ers (Johnson, 2002), linguistic principles (Campbell,
2004) or machine learning methods (Higgins, 2003;
Levy and Manning, 2004; Gabbard et al, 2006) to
recover empty nodes and identify their antecedents;1
(ii) integrating non-local dependency recovery into
the parser by enriching a simple PCFG model with
GPSG-style gap features (Collins, 1999; Schmid,
2006); (iii) pre-processing the input sentence with
a finite-state trace tagger which detects empty nodes
before parsing, and identify the antecedents on the
parser output with the gap information (Dienes and
Dubey, 2003a; Dienes and Dubey, 2003b).
In addition to CFG-oriented approaches, a num-
ber of richer treebank-based grammar acquisition
and parsing methods based on HPSG (Miyao et
al., 2003), CCG (Clark and Hockenmaier, 2002),
LFG (Riezler et al, 2002; Cahill et al, 2004) and
Dependency Grammar (Nivre and Nilsson, 2005)
incorporate non-local dependencies into their deep
syntactic or semantic representations.
A common characteristic of all these approaches
1(Jijkoun, 2003; Jijkoun and Rijke, 2004) also describe post-
processing methods to recover NLDs, which are applied to syn-
tactic dependency structures converted from CFG-trees.
257
is that, to date, the research has focused almost
entirely on English,2 despite the disparity in type
and frequency of non-local dependencies for vari-
ous languages. In this paper, we address recover-
ing non-local dependencies for Chinese, a language
drastically different from English and whose spe-
cial features such as lack of morphological inflection
make NLD recovery more challenging. Inspired by
(Cahill et al, 2004)?s methodology which was origi-
nally designed for English and Penn-II treebank, our
approach to Chinese non-local dependency recovery
is based on Lexical-Functional Grammar (LFG), a
formalism that involves both phrase structure trees
and predicate-argument structures. NLDs are re-
covered in LFG f-structures using automatically ac-
quired subcategorisation frames and finite approxi-
mations of functional uncertainty equations describ-
ing NLD paths at the level of f-structures.
The paper is structured as follows: in Section 2 we
outline the distinguishing features of Chinese non-
local dependencies compared to English. In Section
3 we review (Cahill et al, 2004)?s method for recov-
ering English NLDs in treebank-based LFG approx-
imations. In Section 4, we describe how we mod-
ify and substantially extend the previous method
to recover all types of NLDs for Chinese data.
We present experiments and provide a dependency-
based evaluation in Section 5. Finally we conclude
and summarise future work.
2 Non-Local Dependencies in Chinese
In the Penn Chinese Treebank (CTB) (Xue et al,
2002) non-local dependencies are represented in
terms of empty categories (ECs) and (for some of
them) coindexation with antecedents, as exemplified
in Figure 1. Following previous work for English
and the CTB annotation scheme, we use /non-
local dependencies0as a cover term for all miss-
ing or dislocated elements represented in the CTB
as an empty category (with or without coindexa-
tion/antecedent), and our use of the term remains ag-
nostic about fine-grained distinctions between non-
local dependencies drawn in the theoretical linguis-
tics literature.
In order to give an overview on the character-
2 (Levy and Manning, 2004) is the only approach we are
aware of that has been applied to both English and German.
(1) ? ?u? ?k d?  # ?[
not want look-for train have potential DE new writer
?(People) don?t want to look for and train new writers who
have potential.?
IP
NP-SBJ
-NONE-
*pro*
VP
ADVP
AD
?
not
VP
VV
?
want
IP-OBJ
NP-SBJ
-NONE-
*PRO*
VP
VP
VV
u?
look for
NP-OBJ
-NONE-
*RNR*-2
PU
!
VP
VV
?
train
NP-OBJ-2
CP
WHNP-1
-NONE-
*OP*
CP
IP
NP-SBJ
-NONE-
*T*-1
VP
VE
k
have
NP
NN
d?
potential
DEC

DE
ADJP
JJ
#
new
NP
NN
?[
writer
Figure 1: Example of non-local annotations in CTB,
including dropped subject (*pro*), control subject
(*PRO*), relative clause (*T*), and coordination
(*RNR*).
istics of Chinese non-local dependencies, we ex-
tracted all empty categories together with coindexed
antecedents from the Penn Chinese Treebank ver-
sion 5.1 (CTB5.1). Table 1 gives a breakdown of the
most frequent types of empty categories and their
antecedents, which account for 43,791 of the total
43,954 (99.6%) ECs in CTB5.1.3
According to their different linguistics properties,
we divide the empty nodes listed in Table 1 into
three major types: null relative pronouns, locally
mediated dependencies, and long-distance depen-
dencies.
Null Relative Pronouns (lines 2, 7) themselves
are local dependencies, and thus are not coindexed
with an antecedent. But they mediate non-local de-
pendencies by functioning as antecedents for the dis-
3An extensive description of the types of empty categories
and the use of coindexation in CTB can be found in Section VI
of the bracketing guidelines.
258
Antecedent POS Label Count Description
1 WHNP NP *T* 11670 WH trace (e.g. *OP*?I/Chinau/launch*T*/DE?(/satellite)
2 WHNP *OP* 11621 Empty relative pronouns (e.g. *OP*?I/Chinau/launch/DE?(/satellite)
3 NP *PRO* 10946 Control constructions (e.g. ?p/here?/notN/allow*PRO*??/smoke)
4 NP *pro* 7481 Pro-drop situations (e.g. *pro*?/notQ/ever?/encounter/DE?K/problem)
5 IP IP *T* 575 Topicalisation (e.g. ??/weU/canI/win?/he`/say*T*)
6 WHPP PP *T* 337 WH trace (e.g. *OP*<?/population*T*?8/dense/?/area)
7 WHPP *OP* 337 Empty relative pronouns (e.g. *OP*<?/population?8/dense/?/area)
8 NP NP * 291 Raising & passive constructions (e.g. ??/weProceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 809?816,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Dependency Based Chinese Sentence Realization 
 
 
Wei He1, Haifeng Wang2, Yuqing Guo2, Ting Liu1 
1Information Retrieval Lab, Harbin Institute of Technology, Harbin, China 
{whe,tliu}@ir.hit.edu.cn 
2Toshiba (China) Research and Development Center, Beijing, China 
{wanghaifeng,guoyuqing}@rdc.toshiba.com.cn 
 
  
Abstract 
This paper describes log-linear models for a 
general-purpose sentence realizer based on de-
pendency structures. Unlike traditional realiz-
ers using grammar rules, our method realizes 
sentences by linearizing dependency relations 
directly in two steps. First, the relative order 
between head and each dependent is deter-
mined by their dependency relation. Then the 
best linearizations compatible with the relative 
order are selected by log-linear models. The 
log-linear models incorporate three types of 
feature functions, including dependency rela-
tions, surface words and headwords. Our ap-
proach to sentence realization provides sim-
plicity, efficiency and competitive accuracy. 
Trained on 8,975 dependency structures of a 
Chinese Dependency Treebank, the realizer 
achieves a BLEU score of 0.8874. 
1 Introduction 
Sentence realization can be described as the 
process of converting the semantic and syntactic 
representation of a sentence or series of sen-
tences into meaningful, grammatically correct 
and fluent text of a particular language. 
Most previous general-purpose realization sys-
tems are developed via the application of a set of 
grammar rules based on particular linguistic 
theories, e.g. Lexical Functional Grammar (LFG), 
Head Driven Phrase Structure Grammar (HPSG), 
Combinatory Categorical Grammar (CCG), Tree 
Adjoining Grammar (TAG) etc. The grammar 
rules are either developed by hand, such as those 
used in LinGo (Carroll et al, 1999), OpenCCG 
(White, 2004) and XLE (Crouch et al, 2007), or 
extracted automatically from annotated corpora, 
like the HPSG (Nakanishi et al, 2005), LFG 
(Cahill and van Genabith, 2006; Hogan et al, 
2007) and CCG (White et al, 2007) resources 
derived from the Penn-II Treebank. 
Over the last decade, there has been a lot of in-
terest in a generate-and-select paradigm for sur-
face realization. The paradigm is characterized 
by a separation between realization and selection, 
in which rule-based methods are used to generate 
a space of possible paraphrases, and statistical 
methods are used to select the most likely reali-
zation from the space. Usually, two statistical 
models are used to rank the output candidates. 
One is n-gram model over different units, such as 
word-level bigram/trigram models (Bangalore 
and Rambow, 2000; Langkilde, 2000), or fac-
tored language models integrated with syntactic 
tags (White et al 2007). The other is log-linear 
model with different syntactic and semantic fea-
tures (Velldal and Oepen, 2005; Nakanishi et al, 
2005; Cahill et al, 2007). 
However, little work has been done on proba-
bilistic models learning direct mapping from in-
put to surface strings, without the effort to con-
struct a grammar. Guo et al (2008) develop a 
general-purpose realizer couched in the frame-
work of Lexical Functional Grammar based on 
simple n-gram models. Wan et al (2009) present 
a dependency-spanning tree algorithm for word 
ordering, which first builds dependency trees to 
decide linear precedence between heads and 
modifiers then uses an n-gram language model to 
order siblings. Compared with n-gram model, 
log-linear model is more powerful in that it is 
easy to integrate a variety of features, and to tune 
feature weights to maximize the probability. A 
few papers have presented maximum entropy 
models for word or phrase ordering (Ratnaparkhi, 
2000; Filippova and Strube, 2007). However, 
those attempts have been limited to specialized 
applications, such as air travel reservation or or-
dering constituents of a main clause in German.  
This paper presents a general-purpose realizer 
based on log-linear models for directly lineariz-
ing dependency relations given dependency 
structures. We reduce the generation space by 
809
two techniques: the first is dividing the entire 
dependency tree into one-depth sub-trees and 
solving linearization in sub-trees; the second is 
the determination of relative positions between 
dependents and heads according to dependency 
relations. Then the best linearization for each 
sub-tree is selected by the log-linear model that 
incorporates three types of feature functions, in-
cluding dependency relations, surface words and 
headwords. The evaluation shows that our realiz-
er achieves competitive generation accuracy. 
The paper is structured as follows. In Section 
2, we describe the idea of dividing the realization 
procedure for an entire dependency tree into a 
series of sub-procedures for sub-trees. We de-
scribe how to determine the relative positions 
between dependents and heads according to de-
pendency relations in Section 3. Section 4 gives 
details of the log-linear model and the feature 
functions used for sentence realization. Section 5 
explains the experiments and provides the results. 
2 Sentence Realization from Dependen-
cy Structure  
2.1 The Dependency Input  
The input to our sentence realizer is a dependen-
cy structure as represented in the HIT Chinese 
Dependency Treebank (HIT-CDT)1. In our de-
pendency tree representations, dependency rela-
tions are represented as arcs pointing from a head 
to a dependent. The types of dependency arcs 
indicate the semantic or grammatical relation-
ships between the heads and the dependents, 
which are recorded in the dependent nodes. Fig-
ure 1 gives an example of dependency tree repre-
sentation for the sentence: 
 
(1) ? ? ?? ?? 
 this is Wuhan Airline 
 ?? ?? ?? ?? 
 first time buy Boeing airliner 
?This is the first time for Airline Wuhan to buy 
Boeing airliners.? 
In a dependency structure, dependents are un-
ordered, i.e. the string position of each node is 
not recorded in the representation. Our sentence 
realizer takes such an unordered dependency tree 
as input, determines the linear order of the words 
                                                 
1 HIT-CDT (http://ir.hit.edu.cn) includes 10,000 sentences 
and 215,334 words, which are manually annotated with 
part-of-speech tags and dependency labels. (Liu et al, 
2006a) 
as encoded in the nodes of the dependency struc-
ture and produces a grammatical sentence. As the 
dependency structures input to our realizer have 
been lexicalized, lexical selection is not involved 
during the surface realization. 
2.2 Divide and Conquer Strategy for Linea-
rization 
For determining the linear order of words 
represented by nodes of the given dependency 
structure, in principle, the sentence realizer has 
to produce all possible sequences of the nodes 
from the input tree and selects the most likely 
linearization among them. If the dependency tree 
consists of a considerable number of nodes, this 
procedure would be very time-consuming.  To 
reduce the number of possible realizations, our 
generation algorithm adopts a divide-and-
conquer strategy, which divides the whole tree 
into a set of sub-trees of depth one and recursive-
ly linearizes the sub-trees in a bottom-up fashion. 
As illustrated in Figure 2, sub-trees c and d, 
which are at the bottom of the tree, are linearized 
first, then sub-tree b is processed, and finally 
sub-tree a.  
The procedure imposes a projective constraint 
on the dependency structures, viz. each head 
dominates a continuous substring of the sentence 
realization. This assumption is feasible in the 
application of the dependency-based generation, 
because: (i) it has long been observed that the 
dependency structures of a vast majority of sen-
tences in the languages of the world are projec-
tive (Igor, 1988) and (ii) non-projective depen-
dencies in Chinese, for the most part, are used to 
account for non-local dependency phenomena. 
Figure 1: The dependency tree for the sentence 
???????????????? 
??(HED)
is 
??(SBV)
this 
???(VOB) 
buy 
???(ADV)
first time 
???(VOB) 
airliner 
???(SBV)
airline 
???(ATT)
Wuhan 
???(ATT) 
Boeing 
810
Though non-local dependencies are important for 
accurate semantic analysis, they can be easily 
converted to local dependencies conforming to 
the projective constraint. In fact, we find that the 
10, 000 manually-build dependency trees of the 
HIT-CDT do not contain any non-projective de-
pendencies. 
3 Relative Position Determination 
In dependency structures, the semantic or gram-
matical roles of the nodes are indicated by types 
of dependency relations. For example, the VOB 
dependency relation, which stands for the verb-
object structure, means that the head is a verb 
and the dependent is an object of the verb; the 
ATT relation, means that the dependent is an 
attribute of the head. In languages with fairly 
rigid word order, the relative position between 
the head and dependent of a certain relation is in 
a fixed order. For example in Chinese, the object 
almost always occurs behind its dominating verb; 
the attribute modifier always occurs in front of 
its head word. Therefore, we can draw a conclu-
sion that the relative positions between head and 
dependent of VOB and ATT can be determined 
by the types of dependency relations. 
We make a statistic on the relative positions 
between head and dependent for each dependen-
cy relation type. Following (Covington, 2001), 
we call a dependent that precedes its head prede-
pendent, a dependent that follows its head post-
dependent. The corpus used to gather appropriate 
statistics is HIT-CDT. Table 1 gives the numbers 
??(HED) 
is 
??(SBV) 
this 
?   ?   ???????????? 
?  
???(VOB) 
buy 
???(ADV)
first time 
?  ?  
????   ??   ??   ???? 
???(VOB)
airliner 
???(ATT)
Boeing 
??   ??
???(SBV) 
Airline 
???(ATT) 
Wuhan 
??   ?? 
sub-tree a 
sub-tree b 
sub-tree c sub-tree d 
Figure 2: Illustration of the linearization procedure 
Relation Description Postdep. Predep.
ADV adverbial 1 25977
APP appositive 807 0
ATT attribute 0 47040
CMP complement 2931 3
CNJ conjunctive 0 2124
COO coordinate 6818 0
DC dep. clause 197 0
DE DE phrase 0 10973
DEI DEI phrase 131 3
DI DI phrase 0 400
IC indep.clause 3230 0
IS indep.structure 125 794
LAD left adjunct 0 2644
MT mood-tense 3203 0
POB prep-obj 7513 0
QUN quantity 0 6092
RAD right adjunct 1332 1
SBV subject-verb 6 16016
SIM similarity 0 44
VOB verb-object 23487 21
VV verb-verb 6570 2
Table 1: Numbers of pre/post-dependents for each 
dependency relation 
811
of predependent/postdependent for each type of 
dependency relations and its descriptions. 
Table 1 shows that 100% dependents of ATT 
relation are predependents and 23,487(99.9%) 
against 21(0.1%) VOB dependents are postde-
pendents. Almost all the dependency relations 
have a dominant dependent type?predependent 
or postdependent. Although some dependency 
relations have exceptional cases (e.g. VOB), the 
number is so small that it can be ignored. The 
only exception is the IS relation, which has 
794(86.4%) predependents and 125(13.6%) 
postdependents. The IS label is an abbreviation 
for independent structure. This type of depen-
dency relation is usually used to represent inter-
jections or comments set off by brackets, which 
usually has little grammatical connection with 
the head. Figure 3 gives an example of indepen-
dent structure. This example is from a news re-
port, and the phrase ??????? (set apart by 
brackets in the original text) is a supplementary 
explanation for the source of the news. The con-
nection between this phrase and the main clause 
is so weak that either it precedes or follows the 
head verb is acceptable in grammar. However, 
this kind of news-source-explanation is customa-
ry to place at the beginning of a sentence in Chi-
nese. This can probably explain the majority of 
the IS-tagged dependents are predependents. 
If we simply treat all the IS dependents as pre-
dependents, we can assume that every dependen-
cy relation has only one type of dependent, either 
predependent or postdependent. Therefore, the 
relative position between head and dependent 
can be determined just by the types of dependen-
cy relations. 
In the light of this assumption, all dependents 
in a sub-tree can be classified into two groups?
predependents and postdependents. The prede-
pendents must precede the head, and the postde-
pendents must follow the head. This classifica-
tion not only reduces the number of possible se-
quences, but also solves the linearization of a 
sub-tree if the sub-tree contains only one depen-
dent, or two dependents of different types, viz. 
one predependent and one postdependent. In sub-
tree c of Figure 2, the dependency relation be-
tween the only dependent and the head is ATT, 
which indicates that the dependent is a prede-
pendent. Therefore, node 7 is bound to precede 
node 5, and the only linearization result is ???
???. In sub-tree a of the same figure, the clas-
sification for SBV is predependent, and for VOB 
is postdependent, so the only linearization is 
<node 2, node 1, node 3>.  
In HIT-CDT, there are 108,086 sub-trees in 
the 10,000 sentences, 65% sub-trees have only 
one dependent, and 7% sub-trees have two de-
pendents of different types (one predependent 
and one postdependent). This means that the 
relative position classification can deterministi-
cally linearize 72% sub-trees, and only the rest 
28% sub-trees with more than one predependent 
or postdependent need to be further determined. 
4 Log-linear Models 
We use log-linear models for selecting the se-
quence with the highest probability from all the 
possible linearizations of a sub-tree. 
4.1 The Log-linear Model 
Log-linear models employ a set of feature func-
tions to describe properties of the data, and a set 
of learned weights to determine the contribution 
of each feature. In this framework, we have a set 
of M feature functions Mmtrhm ,...,1),,( = . 
For each feature function, there exists a model 
parameter Mmtrm ,...,1),,( =?  that is fitted to 
optimize the likelihood of the training data. A 
conditional log-linear model for the probability 
of a realization r given the dependency tree t, has 
the general parametric form 
)],(exp[
)(
1
)|(
1
trh
tZ
trp m
M
m
m?
=
= ?
?
?            (1) 
where )(tZ?  is a normalization factor defined as 
? ?
? =
=
)(' 1
)],'(exp[)(
tYr
m
M
m
m trhtZ ??                    (2) 
And Y(t) gives the set of all possible realizations 
of the dependency tree t. 
4.2 Feature Functions 
We use three types of feature functions for cap-
turing relations among nodes on the dependency 
tree. In order to better illustrate the feature func-
tions used in the log-linear model, we redraw 
sub-tree b of Figure 2 in Figure 4. Here we as-
sume the linearizations of sub-tree c and d have 
Figure 3: Example of independent structure 
???(HED) 
serious 
??????(IS) 
Xinhua news 
?????(SBV) 
southern snowstorm 
812
been finished, and the strings of linearizing re-
sults are recorded in nodes 5 and 6. 
The sub-tree in Figure 4 has two predepen-
dents (SBV and ADV) and one postdependent 
(VOB). As a result of this classification, the only 
two possible linearizations of the sub-tree are 
<node 4, node 6, node 3, node 5> and <node 6, 
node 4, node 3, node 5>. Then the log-linear 
model that incorporates three types of feature 
functions is used to make further selection. 
Dependency Relation Model: For a particular 
sub-tree structure, the task of generating a string 
covered by the nodes on the sub-tree is equiva-
lent to linearizing all the dependency relations in 
that sub-tree. We linearize the dependency rela-
tions by computing n-gram models, similar to 
traditional word-based language models, except 
using the names of dependency relations instead 
of words. For the two linearizations of Figure 4, 
the corresponding dependency relation sequences 
are ?ADV SBV VOB VOB? and ?SBV ADV 
VOB VOB?. The dependency relation model 
calculates the probability of dependency relation 
n-gram P(DR) according to Eq.(3). The probabil-
ity score is integrated into the log-linear model as 
a feature.  
)...()( 11 m
m DRDRPDRP =  (3) 
       )|( 1 1
1
?
+?
=
?= k nkm
k
k DRDRP  
Word Model: We integrate an n-gram word 
model into the log-linear model for capturing the 
relation between adjacent words. For a string of 
words generated from a possible sequence of 
sub-tree nodes, the word models calculate word-
based n-gram probabilities of the string. For ex-
ample, in Figure 4, the strings generated by the 
two possible sequences are ????? ?? ?
? ????? and ??? ???? ?? ???
??. The word model takes these two strings as 
input, and calculates the n-gram probabilities. 
Headword Model: 2 In dependency representa-
tions, heads usually play more important roles 
than dependents. The headword model calculates 
the n-gram probabilities of headwords, without 
regard to the words occurring at dependent nodes, 
in that dependent words are usually less impor-
tant than headwords. In Figure 4, the two possi-
ble sequences of headwords are ??? ?? ?
?  ??? and ???  ??  ??  ???. The 
headword strings are usually more generic than 
the strings including all words, and thus the 
headword model is more likely to relax the data 
sparseness. 
   Table 2 gives some examples of all the features 
used in the log-linear model. The examples listed 
in the table are features of the linearization 
<node 6, node 4, node 3, node 5>, extracted from 
the sub-tree in Figure 4. 
In this paper, all the feature functions used in 
the log-linear model are n-gram probabilities. 
However, the log-linear framework has great 
potential for including other types of features. 
4.3 Parameter Estimation 
BLEU score, a method originally proposed to 
automatically evaluate machine translation quali-
ty (Papineni et al, 2002), has been widely used 
as a metric to evaluate general-purpose sentence 
generation (Langkilde, 2002; White et al, 2007; 
Guo et al 2008, Wan et al 2009). The BLEU 
measure computes the geometric mean of the 
precision of n-grams of various lengths between 
a sentence realization and a (set of) reference(s).  
To estimate the parameters ),...,( 1 M??  for the 
feature functions ),...,( 1 Mhh , we use BLEU
3 as 
optimization objective function and adopt the 
approach of minimum error rate training 
                                                 
2 Here the term ?headword? is used to describe the word 
that occurs at head nodes in dependency trees.  
3 The BLEU scoring script is supplied by NIST Open Ma-
chine Translation Evaluation at 
ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 
Feature function Examples of features 
Dependency Relation ?SBV ADV VOB?  ?ADV VOB VOB? 
Word Model ???????? ???????? ????????????????
Headword Model ?????? ?????? ?????? 
Table 2: Examples of feature functions 
???(VOB) 
buy 
???(ADV) 
first time 
???(VOB) 
airliner 
?????? 
airliners of Boeing 
???(SBV)
Airline 
?????? 
Airline Wuhan
Figure 4: Sub-tree with multiple predependents
813
(MERT), which is popular in statistical machine 
translation (Och, 2003).   
4.4 The Realization Algorithm 
The realization algorithm is a recursive proce-
dure that starts from the root node of the depen-
dency tree, and traverses the tree by depth-first 
search. The pseudo code of the realization algo-
rithm is shown in Figure 5. 
5 Experiments 
5.1 Experimental Design 
Our experiments are carried out on HIT-CDT. 
We randomly select 526 sentences as the test set, 
and 499 sentences as the development set for 
optimizing the model parameters. The rest 8,975 
sentences of the HIT-CDT are used for training 
of the dependency relation model. For training of 
word models, we use the Xinhua News part 
(6,879,644 words) of Chinese Gigaword Second 
Edition (LDC2005T14), segmented by the Lan-
guage Technology Platform (LTP) 4 . And for 
training the headword model, we use both the 
HIT-CDT and the HIT Chinese Skeletal Depen-
dency Treebank (HIT-CSDT). HIT-CSDT is a 
                                                 
4 http://ir.hit.edu.cn/demo/ltp  
component of LTP and contains 49,991 sen-
tences in dependency structure representation 
(without dependency relation labels). 
As the input dependency representation does 
not contain punctuation information, we simply 
remove all punctuation marks in the test and de-
velopment sets. 
5.2 Evaluation Metrics 
In addition to BLEU score, percentage of exactly 
matched sentences and average NIST simple 
string accuracy (SSA) are adopted as evaluation 
metrics. The exact match measure is percentage 
of the generated string that exactly matches the 
corresponding reference sentence. The average 
NIST simple string accuracy score reflects the 
average number of insertion (I), deletion (D), and 
substitution (S) errors between the output sen-
tence and the reference sentence. Formally, SSA 
= 1 ? (I + D + S) / R, where R is the number of 
tokens in the reference sentence. 
5.3 Experimental Results 
All the evaluation results are shown in Table 3. 
The first experiment, which is a baseline experi-
ment, ignores the tree structure and randomly 
chooses position for every word. From the 
second experiment, we begin to utilize the tree 
structure and apply the realization algorithm de-
scribed in Section 4.4. In the second experiment, 
predependents are distinguished from postdepen-
dents by the relative position determination me-
thod (RPD), then the orders inside predependents 
and postdependents are chosen randomly. From 
the third experiments, the log-linear models are 
used for scoring the generated sequences, with 
the aid of three types of feature functions as de-
scribed in Section 4.2. First, the feature functions 
of trigram dependency relation model (DR), bi-
gram word model (Bi-WM), trigram word model 
(Tri-WM) (with Katz backoff) and trigram 
headword model (HW) are used separately in 
experiments 3-6. Then we combine the feature 
1:procedure SEARCH 
2:input: sub-tree T {head:H dep.:D1?Dn} 
3:  if n = 0 then return 
4:  for i := 1 to n 
5:    SEARCH(Di) 
6:  Apre := {} 
7:  Apost := {} 
8:  for i := 1 to n 
9:    if PRE-DEP(Di) then Apre:=Apre?{Di} 
10:   if POST-DEP(Di) then Apost:=Apost?{Di} 
11: for all permutations p1 of Apre 
12:   for all permutations p2 of Apost 
13:     sequence s := JOIN(p1,H,p2) 
14:     score r := LOG-LINEAR(s) 
15:     if best-score(r) then RECORD(r,s) 
Figure 5: The algorithm for linearizations of sub-
trees 
 Model BLEU ExMatch SSA 
1 Random 0.1478 0.0038 0.2044 
2 RPD + Random 0.5943 0.1274 0.6369 
3 RPD + DR 0.7204 0.2167 0.7683 
4 RPD + Bi-WM 0.8289 0.4125 0.8270 
5 RPD + Tri-WM 0.8508 0.4715 0.8415 
6 RPD + HW 0.7592 0.2909 0.7638 
7 RPD + DR + Bi-WM 0.8615 0.4810 0.8723 
8 RPD + DR + Tri-WM 0.8772 0.5247 0.8817 
9 RPD + DR + Tri-WM + HW 0.8874 0.5475 0.8920 
Table 3: BLEU, ExMatch and SSA scores on the test set 
814
functions incrementally based on the RPD and 
DR model. 
The relative position determination plays an 
important role in the realization algorithm. We 
observe that the BLEU score is boosted from 
0.1478 to 0.5943 by using the RPD method. This 
can be explained by the reason that the lineariza-
tions of 72% sub-trees can be definitely deter-
mined by the RPD method. All of the four fea-
ture functions we have tested achieve considera-
ble improvement in BLEU scores. The depen-
dency relation model achieves 0.7204, the bi-
gram word model 0.8289, the trigram word mod-
el 0.8508 and the headword model achieves 
0.7592. While the combined models perform bet-
ter than any of their individual component mod-
els. On the foundation of relative position deter-
mination method, the combination of dependen-
cy relation and bigram word model achieves a 
BLEU score of 0.8615, and the combination of 
dependency relation and trigram word model 
achieves a BLEU score of 0.8772. Finally the 
combination of dependency relation model, tri-
gram word model and headword model achieves 
the best result 0.8874.  
5.4 Discussion 
We first inspected the errors made by the relative 
position determination method. In the treebank-
tree test set, there are 7 predependents classified 
as postdependents and 3 postdependents classi-
fied as predependents by error. Among the 9,384 
dependents, the error rate of the relative position 
determination method is very small (0.1%). 
Then we make a classification on the errors in 
the experiment of dependency relation model 
(with relative position determination method). 
Table 4 shows the distribution of the errors. 
The first type of errors is caused by duplicate 
dependency relations, i.e. a head with two or 
more dependents that have the same dependency 
relations. In this situation, only using the depen-
dency relation model cannot generate the right 
linearization. However, word models, which util-
ize the word information, can make distinctions 
between the dependencies. The reason for the 
errors of SBV-ADV and ATT-QUN is probably 
because the order of these pairs of grammar roles 
is somewhat flexible. For example, the strings of 
???(ADV)/today ?(SBV)/I? and ??(SBV)/I 
??(ADV)/today? are both very common and 
acceptable in Chinese. 
The word models tend to combine the nodes 
that have strong correlation together. For exam-
ple in Figure 6, node 2 is more likely to precede 
node 3 because the words ??? /protect? and 
??? /future? have strong correlation, but the 
correct order is <node 3, node 2>. 
Headword model only consider the words oc-
cur at head nodes, which is helpful in the situa-
tion like Figure 6. In our experiments, the head-
word model gets a relatively low performance by 
itself, however, the addition of headword model 
to the combination of the other two feature func-
tions improves the result from 0.8772 to 0.8874. 
This indicates that the headword model is com-
plementary to the other feature functions. 
6 Conclusions 
We have presented a general-purpose realizer 
based on log-linear models, which directly maps 
dependency relations into surface strings. The 
linearization of a whole dependency tree is di-
vided into a series of sub-procedures on sub-trees. 
The dependents in the sub-trees are classified 
into two groups, predependents or postdepen-
dents, according to their dependency relations. 
The evaluation shows that this relative position 
determination method achieves a considerable 
result. The log-linear model, which incorporates 
three types of feature functions, including de-
pendency relation, surface words and headwords, 
successfully captures factors in sentence realiza-
tion and demonstrates competitive performance.  
 
References  
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a Probabilistic Hierarchical Model for 
Generation. In Proceedings of the 18th Interna-
tional Conference on Computational Linguistics, 
pages 42-48. Saarbr?cken, Germany. 
 Error types Proportion
1 Duplicate dependency relations 60.0% 
2 SBV-ADV 20.3% 
3 ATT-QUN 6.3% 
4 Other 13.4% 
Table 4: Error types in the RPD+DR experiment 
Figure 6: Sub-tree for ??????????? 
??? 
work
???(ATT) 
protect 
??? ??? 
?birds protecting?
??(SBV) 
of 
??? ?? 
future 
815
Aoife Cahill and Josef van Genabith. 2006. Robust 
PCFG-Based Generation Using Automatically Ac-
quired LFG Approximations. In Proceedings of the 
21st International Conference on Computational 
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1033-
1040. Sydney, Australia. 
Aoife Cahill, Martin Forst and Christian Rohrer. 2007. 
Stochastic Realisation Ranking for a Free Word 
Order language. In Proceedings of 11th European 
Workshop on Natural Language Generation, pages 
17-24. Schloss Dagstuhl, Germany. 
John Carroll, Ann Copestake, Dan Flickinger, and 
Victor Poznanski. 1999. An Efficient Chart Gene-
rator for (Semi-)Lexicalist Grammars. In Proceed-
ings of the 7th European Workshop on Natural 
Language Generation, pages 86-95, Toulouse. 
Michael A. Covington. 2001. A Fundamental Algo-
rithm for Dependency Parsing. In Proceedings of 
the 39th Annual ACM Southeast Conference, pages 
95?102. 
Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy 
King, John Maxwell, and Paula Newman. 2007. 
XLE documentation. Palo Alto Research Center, 
CA. 
Katja Filippova and Michael Strube. 2007. Generating 
Constituent Order in German Clauses. In Proceed-
ings of the 45th Annual Meeting of the Association 
of Computational Linguistics, pages 320-327. Pra-
gue, Czech Republic. 
Yuqing Guo, Haifeng Wang and Josef van Genabith. 
2008. Dependency-Based N-Gram Models for 
General Purpose Sentence Realisation. In Proceed-
ings of the 22th International Conference on Com-
putational Linguistics, pages 297-304. Manchester, 
UK. 
Deirdre Hogan, Conor Cafferkey, Aoife Cahill and 
Josef van Genabith. 2007. Exploiting Multi-Word 
Units in History-Based Probabilistic Generation. In 
Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing 
and CoNLL, pages 267-276. Prague, Czech Repub-
lic. 
Mel'?uk Igor. 1988. Dependency syntax: Theory and 
practice. In Suny Series in Linguistics. State Uni-
versity of New York Press, New York, USA. 
Irene Langkilde. 2000. Forest-Based Statistical Sen-
tence Generation. In Proceedings of 1st Meeting of 
the North American Chapter of the Association for 
Computational Linguistics, pages 170-177. Seattle, 
WA. 
Irene Langkilde. 2002. An Empirical Verification of 
Coverage and Correctness for a General-Purpose 
Sentence Generator. In Proceedings of the Second 
International Conference on Natural Language 
Generation, pages 17-24. New York, USA. 
Ting Liu, Jinshan Ma, and Sheng Li. 2006a. Building 
a Dependency Treebank for Improving Chinese 
Parser. Journal of Chinese Language and Compu-
ting, 16(4): 207-224. 
Ting Liu, Jinshan Ma, Huijia Zhu, and Sheng Li. 
2006b. Dependency Parsing Based on Dynamic 
Local Optimization. In Proceedings of CoNLL-X, 
pages 211-215, New York, USA. 
Hiroko Nakanishi, Yusuke Miyao and Jun?ichi Tsujii. 
2005. Probabilistic Models for Disambiguation of 
an HPSG-Based Chart Generator. In Proceedings 
of the 9th International Workshop on Parsing 
Technology, pages 93-102. Vancouver, British Co-
lumbia. 
Franz Josef Och. 2003. Minimum Error Rate Training 
in Statistical Machine Translation. In Proceedings 
of the 41st Annual Meeting of the Association for 
Computational Linguistics, pages 160-167, Sappo-
ro, Japan. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. In Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 311-
318. Philadelphia, PA. 
Adwait Ratnaparkhi. 2000. Trainable Methods for 
Natural Language Generation. In Proceedings of 
North American Chapter of the Association for 
Computational Linguistics, pages 194-201. Seattle, 
WA. 
Erik Velldal and Stephan Oepen. 2005. Maximum 
Entropy Models for Realization Ranking. In Pro-
ceedings of the 10th Machine Translation Summit, 
pages 109-116. Phuket, Thailand,  
Stephen Wan, Mark Dras, Robert Dale, C?cile Paris. 
2009. Improving Grammaticality in Statistical Sen-
tence Generation: Introducing a Dependency Span-
ning Tree Algorithm with an Argument Satisfac-
tion Model. In Proceedings of the 12th Conference 
of the European Chapter of the ACL, pages 852-
860. Athens, Greece. 
Michael White. 2004. Reining in CCG Chart Realiza-
tion. In Proceedings of the third International Nat-
ural Language Generation Conference, pages 182-
191. Hampshire, UK. 
Michael White, Rajakrishnan Rajkumar and Scott 
Martin. 2007. Towards Broad Coverage Surface 
Realization with CCG. In Proceedings of the Ma-
chine Translation Summit XI Workshop, pages 22-
30. Copenhagen, Danmark. 
816
Accurate and Robust LFG-Based Generation for Chinese
Yuqing Guo
NCLT, School of Computing
Dublin City University
Dublin 9, Ireland
yguo@computing.dcu.ie
Haifeng Wang
Toshiba (China)
Research and Development Center
Beijing, 100738, China
wanghaifeng@rdc.toshiba.com.cn
Josef van Genabith
NCLT, School of Computing
Dublin City University
IBM CAS, Dublin, Ireland
josef@computing.dcu.ie
Abstract
We describe three PCFG-based models for
Chinese sentence realisation from Lexical-
Functional Grammar (LFG) f-structures. Both
the lexicalised model and the history-based
model improve on the accuracy of a simple
wide-coverage PCFG model by adding lexical
and contextual information to weaken inap-
propriate independence assumptions implicit
in the PCFG models. In addition, we pro-
vide techniques for lexical smoothing and rule
smoothing to increase the generation cover-
age. Trained on 15,663 automatically LFG f-
structure annotated sentences of the Penn Chi-
nese treebank and tested on 500 sentences ran-
domly selected from the treebank test set, the
lexicalised model achieves a BLEU score of
0.7265 at 100% coverage, while the history-
based model achieves a BLEU score of 0.7245
also at 100% coverage.
1 Introduction
Sentence generation, or surface realisation can be
described as the problem of producing syntacti-
cally, morphologically, and orthographically cor-
rect sentences from a given abstract semantic /
logical representation according to some linguistic
theory, e.g. Lexical Functional Grammar (LFG),
Head-Driven Phrase Structure Grammar (HPSG),
Combinatory Categorial Grammar (CCG), Tree Ad-
joining Grammar (TAG) etc. Grammars, such as
these, are declarative formulations of the correspon-
dences between semantic and syntactic representa-
tions. Traditionally, grammar rules have been care-
fully handcrafted, such as those used in LinGo (Car-
roll et al, 1999), OpenCCG (White, 2004) and
XLE (Crouch et al, 2007). As handcrafting gram-
mar rules is time-consuming, language-dependent
and domain-specific, recent years have witnessed re-
search on extracting wide-coverage grammars auto-
matically from annotated corpora, for both parsing
and generation. FERGUS (Bangalore and Rambow,
2000) took dependency structures as inputs, and pro-
duced XTAG derivations by a stochastic tree model
automatically acquired from an annotated corpus.
Nakanishi et al (2005) presented log-linear models
for a chart generator using a HPSG grammar ac-
quired from the Penn-II Treebank. From the same
treebank, Cahill and van Genabith (2006) automati-
cally extracted wide-coverage LFG approximations
for a PCFG-based generation model.
In addition to applying statistical techniques to
automatically acquire generation grammars, over the
last decade, there has been a lot of interest in a
generate-and-select paradigm for surface realisation.
The paradigm is characterised by a separation be-
tween generation and selection, in which symbolic
or rule-based methods are used to generate a space
of possible paraphrases, and statistical methods are
used to select one or more outputs from the space.
Starting from Langkilde (2002) who used a n-gram
language model to rank generated output strings, a
substantial number of traditional handcrafted sur-
face realisers have been augmented with sophisti-
cated stochastic rankers (Velldal and Oepen, 2005;
White et al, 2007; Cahill et al, 2007).
It is interesting to note that, while the study of
how the granularity of context-free grammars (CFG)
affects the performance of a parser (e.g. in the form
86
n1:IP
[?=?]
n2:NP
[?SUBJ=?]
n4:NR
[?=?]
L?
JiangZemin
n3:VP
[?=?]
n5:VV
[?=?]
??
interview
n6:NP
[?OBJ=?]
n7:NR
[???ADJUNCT]
I
Thai
n8:NN
[?=?]
on
president
f1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
PRED ????
SUBJ f2
?
?
?
PRED ?L??
NTYPE proper
NUM sg
?
?
?
OBJ f3
?
?
?
?
?
?
?
?
?
?
?
PRED ?on?
NTYPE common
NUM sg
ADJUNCT
?
?
?
?
?
f4
?
?
?
PRED ?I?
NTYPE proper
NUM sg
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
? : N ? F
?(n1)=?(n3)=?(n5)=f1 ?(n2)=?(n4)=f2 ?(n6)=?(n8)=f3 ?(n7)=f4
Figure 1: C- and f-structures with ? links for the sentence ?L???Ion?
of grammar transforms (Johnson, 1998) and lexical-
isation (Collins, 1997)) has attracted substantial at-
tention, to our knowledge, there has been a lot less
research on this subject for surface realisation, a pro-
cess that is generally regarded as the reverse pro-
cess of parsing. Moreover, while most of the re-
search so far has concentrated on English or Euro-
pean languages, we are also interested in generation
for other languages with diverse properties, such as
Chinese which is currently a focus language in pars-
ing (Bikel, 2004; Cao et al, 2007).
In this paper, we investigate three generative
PCFG models for Chinese generation based on
wide-coverage LFG grammars automatically ex-
tracted from the Penn Chinese Treebank (CTB). Our
work is couched in the framework of Lexical Func-
tional Grammar and is implemented in a chart-style
generator. We briefly describe LFG and the basic
generation model in Section 2. We improve the
baseline PCFG model by weakening the indepen-
dence assumptions in two disambiguation models in
Section 3. Section 4 describes the smoothing algo-
rithms adopted for the chart generator and Section 5
gives the experimental details and results.
2 LFG-Based Generation
2.1 Lexical Functional Grammar
Lexical Functional Grammar (Kaplan and Bres-
nan, 1982) is a constraint-based grammar formal-
ism which postulates (minimally) two levels of rep-
resentation: c(onstituent)-structure and f(unctional)-
structure. C-structure takes the form of phrase struc-
ture trees and captures surface grammatical config-
urations. F-structure encodes more abstract gram-
matical functions (GFs) such as SUBJ(ect), OBJ(ect),
ADJUNCT and TOPIC etc., in the form of hierar-
chical attribute-value matrices. C-structures and
f-structures are related by a piecewise correspon-
dence function ? that goes from the nodes of a c-
structure tree into units of f-structure spaces (Ka-
plan, 1995). As illustrated in Figure 1, given a
c-structure node ni, the corresponding f-structure
component fj is ?(ni). Admissible c-structures
are specified by a context-free grammar. The cor-
responding f-structures are derived from functional
annotations attached to the CFG rewriting rules.
(1) a. IP ?? NP VP
[?SUBJ=?] [?=?]
b. VP ?? VV NP
[?=?] [?OBJ=?]
c. NP ?? NR NN
[?ADJ=?] [?=?]
d. NP ?? NR
[?=?]
(1) shows a miniature set of annotated CFG rules
(lexical entries omitted) which generates the c- and
f-structure in Figure 1. In the functional annotations,
(?) refers to the f-structure associated with the local
c-structure node ni, i.e. ?(ni), and (?) refers to the
87
Model Grammar Rule Conditions
PCFG VP[?=?] ? VV[?=?] NP[?OBJ=?] VP[?=?], {PRED, SUBJ, OBJ}
HB-PCFG VP[?=?] ? VV[?=?] NP[?OBJ=?] VP[?=?], {PRED, SUBJ, OBJ}, TOP
LEX-PCFG VP(??)[?=?] ? VV(??)[?=?] NP(on)[?OBJ=?] VP(??)[?=?], {PRED, SUBJ, OBJ}
Table 1: Examples of f-structure annotated CFG rules (from Figure 1) in different models
f-structure associated with the mother (M ) node of
ni, i.e. ?(M(ni)).
2.2 Generation from f-Structures
The generation task in LFG is to determine which
sentences correspond to a specified f-structure,
given a particular grammar, such as (1). Kaplan
and Wedekind (2000) proved that the set of strings
generated by an LFG grammar from fully speci-
fied f-structures is a context-free language. Based
on this theoretical cornerstone, Cahill and van Gen-
abith (2006) presented a PCFG-based chart genera-
tor using wide-coverage LFG approximations auto-
matically extracted from the Penn-II treebank. The
LFG-based statistical generation model defines the
conditional probability P (T |F ), for each candidate
functionally annotated c-structure tree T (which
fully specifies a surface realisation) given an f-
structure F . The generation model searches for the
Tbest that maximises P (T |F ) (Eq. 1). P (T |F ) is
then decomposed as the product of the probabilities
of all the functionally annotated CFG rewriting rules
X ? Y (conditioned on the left hand side (LHS) X
and local features of the corresponding f-structure
?(X)) contributing to the tree T (Eq. 2). The first
line (PCFG) of Table 1 shows the f-structure anno-
tated CFG rule to expand node n3 in Figure 1.
Tbest = argmax
T
P (T |F ) (1)
P (T |F ) =
?
X ? Y in T
Feats = {ai|ai ? ?(X)}
P (X ? Y |X,Feats) (2)
3 Disambiguation Models
The basic generation model presented in (Cahill
and van Genabith, 2006) used simple probabilis-
tic context-free grammars. However, the indepen-
dence assumptions implicit in PCFG models may
not be appropriate to best capture natural language
phenomena. Methodologies such as lexicalisa-
tion (Collins, 1997; Charniak, 2000) and tree trans-
formations (Johnson, 1998), weaken the indepen-
dence assumptions and have been applied success-
fully to parsing and shown significant improvements
over simple PCFGs. In this section we study the ef-
fect of such methods in LFG-based generation for
Chinese.
3.1 A History-Based Model
The history-based (HB) approach which incorpo-
rates more context information has worked well
in parsing (Collins, 1997; Charniak, 2000). Re-
sembling history-based models for parsing, Hogan
et al (2007) presented a history-based generation
model to overcome some of the inappropriate inde-
pendence assumptions in the basic generation model
of (Cahill and van Genabith, 2006). The history-
based model increases the context by simply includ-
ing the parent grammatical function GF of the f-
structure in addition to the local ?-linked feature set
in the conditioning context (Eq. 3). The f-structure
annotated CFG rule expanding n3 in the history-
based model is shown in the second line (HB-PCFG)
of Table 1.1
P (T |F ) =
?
X ? Y in T
Feats = {ai|ai ? ?(X)}
?f (f GF) = ?(X)
P (X ? Y |X,Feats,GF) (3)
The history-based model is motivated by English
data, for example, to generate the appropriate case
for pronouns in subject position and object position,
respectively. Though Chinese does not distinguish
cases, we expect the f-structure parent GF to help
predict grammar rule expansions more accurately in
the tree derivation than the simple PCFG model. We
will investigate how the HB model performs while
migrating it from English to Chinese data.
1The parent grammatical function of the outermost f-
structure is assumed to be a dummy GF TOP.
88
3.2 A Lexicalised Model
Compared to the HB model which includes the par-
ent grammatical function in the conditioning con-
text, lexicalised grammar rules contain more fine-
grained categorial information. To the best of our
knowledge, lexicalised parsers (Bikel, 2004) outper-
form unlexicalised parsers for Chinese. The expec-
tation is that a lexicalised PCFG model also works
better than a simple PCFG model in Chinese gen-
eration, considering e.g. prepositional phrase (PP)
modification in Chinese. Some prepositions indicat-
ing directions can occur either before or after the
main verbs, for instance both (2a) and (2b) are ac-
ceptable in Chinese. However, most PP modifiers
only act as adverbial adjuncts between the subjects
and verbal predicates. For instance ??/to? never
follows a verb as exemplified in the ungrammatical
sentence (3b).
(2) a. ? 4 ?m  ?
this CLS train run to Beijing
?The train is bound for Beijing.?
b. ? 4 ? ? m
this CLS train to Beijing run
(3) a. Ion ??I ?1 ??
Thai president to China make visit
?The Thai president paid a visit to China.?
b. *Ion ?1 ????I
Thai president make visit to China
In order to model phenomena such as these, we
head-lexicalise our grammar by associating each
non-terminal node with the head word2 in the c-
structure tree along the head-projection line. A non-
terminal node is written as X(x), where x is the lex-
ical head of X. The example generation grammar
rule in the lexicalised model is shown in the last line
(LEX-PCFG) of Table 1.
As in CKY chart parsing, generation grammars
are binarised in our chart generator. Thus all gram-
mar rules are either unary of the form X ? H or
binary X ? Y H (or X ? HY ), where H is the
head constituent and Y is the modifier. To handle the
problem of sparse data while estimating rule proba-
bilities, a back-off to baseline model is employed.
As, from a linguistic perspective, it is the modifier
2We use a mechanism similar to (Collins, 1997) but adapted
to Chinese data to find lexical heads in the treebank data.
rather than the head word which plays the main role
in determining word order, a back-off to partial lexi-
calisation on the modifier only is also used for bi-
nary rules. As a result, the probabilities of lexi-
calised unary and binary CFG rules are calculated
as in Eq. (4) and Eq. (5), respectively.
Pbk(H(h)|X(h)) = ?1P (H(h)|X(h))
+?2P (H |X) (4)
Pbk(Y (y)H(h)|X(h)) = ?1P (Y (y)H(h)|X(h))
+?2P (Y (y)H |X) + ?3P (Y H |X) (5)
where
?
i=1
?i = 1
In principle, grammars binarisation from left-to-
right (left-) or from right-to-left (right-) are equiva-
lent to represent the original grammar and the prob-
ability distributions. However the head word is the
final constituent for most phrasal categories in Chi-
nese.3 In lexicalised model, the head word imme-
diately projects to the top level in a left-binary tree,
and as a result, the intermediate NP nodes cannot
be lexicalised with the head word as illustrated in
Figure (2b). By contrast, right-binary rules are lex-
icalised and the head word is percolated from the
bottom of the tree (Figure (2c)). Therefore we adopt
the right binarisation method in our generation algo-
rithm.
4 Chart Generation and Smoothing
Algorithms
4.1 Chart Generation Algorithm
The PCFG-based generation algorithms are imple-
mented in terms of a chart generator (Kay, 1996).
In the generation algorithm, each (sub-)f-structure
indexes a (sub-)chart. Each local chart generates
the most probable trees for the local f-structure in
a bottom-up manner:
? generating lexical edges from the the local GF
PRED and some atomic features representing
function words, mood or aspect etc.
3Except for prepositional phrases, localiser and some verbal
phrases.
89
NP(m)
NR
[???ADJUNCT]
??
Shanghai
NN
[???ADJUNCT]
?
tennis
NN
[???ADJUNCT]
??
masters
NN
[?=?]
m
cup
(a.) the original tree
NP(m)
NP(null)
[?=?]
NP(null)
[?=?]
NR
[???ADJUNCT]
??
Shanghai
NN
[???ADJUNCT]
?
tennis
NN
[???ADJUNCT]
??
masters
NN
[?=?]
m
cup
NP(m)
NR
[???ADJUNCT]
??
Shanghai
NP(m)
[?=?]
NN
[???ADJUNCT]
?
tennis
NP(m)
[?=?]
NN
[???ADJUNCT]
??
masters
NN
[?=?]
m
cup
(b.) left-binarisation (c.) right-binarisation
Figure 2: Lexicalised binary trees
? applying unary rules and binary rules to gener-
ate new edges until no any new edges can be
generated in the current local chart.
? propagating compatible edges to the upper-
level chart.
For efficiency, the generation algorithm does
Viterbi-pruning for each local chart, viz. if two
edges have equivalent categories and lexical cover-
age, only the most probable one is kept.
The generation coverage is impacted on by un-
known words4 and unmatched grammar rules in
chart generation. We present a lexical smoothing
and a rule smoothing strategy in the following sub-
sections.
4.2 Lexical Smoothing
In LFG f-structure, the surface form of the lemma
is represented via lexical rules involving a particular
set of features, e.g. the lemma ?on /president? is
represented as {?PRED=?on ?, ?NTYPE=common,
?NUM=sg}. Particular lexical rules can be cap-
tured in general lexical macros abstracting away
4We use unknown words as a cover term to refer to all words
occurring in the test set but not in the training set.
from particular surface forms to lemmas, e.g. the
lexical macro encapsuling the above lexical rule is
{?PRED=$LEMMA, ?NTYPE=common, ?NUM=sg},
which generally associates to common nouns NN in
the CTB. According to the assumption that unknown
words have a probability distribution similar to ha-
pax legomenon (Baayen and Sproat, 1996), we pre-
dict the part-of-speech of unknown words from in-
frequent words in the training set by automatically
extracting lexical macros corresponding to the par-
ticular set of f-structure features. The probability of
the potential POS tag t associated to a feature set f
is estimated according to Eq. (6).
P (t|f) = count(t, f)?n
i=1 count(ti, f)
(6)
4.3 Rule Smoothing
The coverage of grammar rules increases with the
size of training data and in theory all the rules can
be fully covered by a training set, if it is big enough.
With limited training resources we have to resort to
fuzzy matching of grammar rules. Two smoothing
strategies are carried out at the level of grammar
rules.
90
Mathched Grammar Rule
Nonsmooth VP[?=?] ? VV[?=?] NP[?OBJ=?], {SUBJ, OBJ, PRED}
Feature smooth VP[?=?] ? VV[?=?] NP[?OBJ=?]
Partial match VP ? VV [?OBJ=?], {SUBJ, OBJ, PRED}
Table 2: Smoothing of CFG rules
? Reducing the conditioning f-structure features
during rule matching;
? Applying partial match during rule application.
A node in each unlexicalised grammar rule X ?
Y H includes two parts: constituent category c, such
as IP, NP, VP etc.; functional f-structure annotation
a, such as [?SUBJ=?], [?=?] etc. As a heuristic based
on linguistic experience, we define the order of im-
portance of these elements as follows:
X(c) > H(c) > Y (a) > Y (c) > X(a) > H(a)
(4) IP[?COMP=?] ? NP[?SUBJ=?] VP[?=?]
For the above example rule (4), the importance of
the elements is:
IP > VP > [?SUBJ=?] > NP > [?COMP=?] > [?=?]
The elements can be deleted from the rules in an im-
portance order from low to high.5 The partial rules
adopted in our system ignore the least important 3
elements, viz. the functional annotation of the head
node H(a), the functional annotation on LHS X(a)
and constituent category of the modifier node Y (c).
Examples of the two types of smoothed rules are
shown in Table 2.
5 Experimental Results
Our experiments are carried out on the newly
released Penn Chinese treebank version 6.0
(CTB6) (Xue et al, 2005), excluding the portion of
ACE broadcast news. We follow the recommended
splits (in the list-of-file of CTB6) to divide the
data into test set, development set and training set.
The training set includes 756 files with a total of
15,663 sentences. The CTB trees of the training set
were automatically annotated with LFG f-structure
equations following (Guo et al, 2007). Table 3
shows the number of different grammar rule types
extracted from the training set. From the test files,
5However c and a on the same node can?t be deleted at the
same time.
we randomly select 500 sentences as test data
with minimal sentence length 5 words, maximal
length 80 words, and average length 28.84 words.
The development set alo includes 500 sentences
randomly selected from the development files with
sentence length between 5 and 80 words. The
c-structure trees of the test and development data
were also automatically converted to f-structures as
input to the generator.
Type with features without features
PCFG 22,372 8,548
HB-PCFG 28,487 11,969
LEX-PCFG 325,094 286,468
Table 3: Number of rules in the training set
The generation system is evaluated against the
raw text of the test data in terms of accuracy and cov-
erage. Following (Langkilde, 2002) and other work
on general-purpose generators, we adopt BLEU
score (Papineni et al, 2002), average simple string
accuracy (SSA) and percentage of exactly matched
sentences for accuracy evaluation.6 For coverage
evaluation, we measure the percentage of input f-
structures that generate a sentence.
Table 4 reports the initial experiments on the sim-
ple PCFG, HB-based PCFG and lexicalised PCFG
models. The results in the left column evaluate all
input f-structures, the right column evaluate only
those f-structures which yield a complete sentence.
The results show that the lexicalised model outper-
forms the baseline PCFG model. The HB model is
the most accurate for complete sentences, but with
reduced coverage compared to the other two mod-
els. However the low coverage of sentences com-
pletely generated due to unknown words and un-
matched rules makes the results unusable in prac-
6We are aware of the limitations in fully automatic evalua-
tion metrics, and in an ideal scenario, we would complement the
BLEU and SSA scores by a human evaluation. Unfortunately,
this is beyond the scope of the current paper.
91
All Output Strings Complete Output Sentences
Coverage ExMatch BLEU SSA Coverage ExMatch BLEU SSA
PCFG 100% 7.2% 0.5401 0.6261 36.40% 19.78% 0.7101 0.7687
HB-PCFG 100% 8.60% 0.5474 0.6281 34.80% 24.71% 0.7513 0.8092
LEX-PCFG 100% 9.40% 0.5687 0.6537 37.00% 25.41% 0.7431 0.8024
Table 4: Results without smoothing
All Output Strings Complete Output Sentences
Coverage ExMatch BLEU SSA Coverage ExMatch BLEU SSA
PCFG 100% 11.00% 0.6894 0.7240 94.20% 11.68% 0.7047 0.7388
HB-PCFG 100% 11.80% 0.7108 0.7348 94.00% 12.55% 0.7284 0.7506
LEX-PCFG 100% 14.00% 0.7152 0.7595 94.40% 14.83% 0.7302 0.7754
Table 5: Results with lexical smoothing
Partial match Feature smooth
Complete Sentences Coverage ExMatch BLEU SSA Coverage ExMatch BLEU SSA
PCFG 97.20% 11.32% 0.7022 0.7356 100% 11.20% 0.7021 0.7330
HB-PCFG 96.20% 12.27% 0.7263 0.7458 100% 12.00% 0.7245 0.7413
LEX-PCFG 97.80% 14.31% 0.7265 0.7696 100% 14.20% 0.7265 0.7675
Table 6: Results with lexical and rule smoothing
tice.
Table 5 gives the results with lexical smoothing.
The coverage for complete sentences increases by
nearly 60% absolute for all models. The increased
coverage also improves the overall results evaluated
against all sentences. The HB model performs better
than the simple PCFG model in nearly all respects
and in turn the lexicalised model comprehensively
outperforms the HB model.
The final results with both lexical smoothing and
rule smoothing by two different strategies are tabu-
lated in Table 6. The left column provides the results
of smoothing by partial match and the right column
the results by reducing conditioning f-structure fea-
tures. All results are evaluated for completely gen-
erated sentences only. The feature smoothing re-
sults in a full coverage of 100%, while slightly de-
grading the quality of sentences generated compared
with partial match smoothing. We feel the tradeoff
at the cost of a small decrease in quality is still worth
the full coverage. Throughout the experiments, the
lexicalised model exhibits consistently better perfor-
mance than the unlexicalised models, which proves
our intuition that successful techniques in parsing
also work well in generation.
6 Conclusion and Further Work
We have presented an accurate, robust chart genera-
tor for Chinese based on treebank-based, automati-
cally acquired LFG resources. Our model improves
the baseline provided by (Cahill and van Genabith,
2006): (i) accuracy is increased by creating a lexi-
calised PCFG grammar and enriching conditioning
context with parent f-structure features; and (ii) cov-
erage is increased by providing lexical smoothing
and fuzzy matching techniques for rule smoothing.
The combinational explosion of grammar rules
encountered in the chart generator is similar to that
in parsing. In the current system, we only keep the
most probable realisation for each input f-structure.
An alternative model in line with the generate-and-
select paradigm, would pack all the locally equiva-
lent edges in a forest and re-rank all the realisations
by a separate language model. This might help us to
reduce some errors caused in our current model, for
instance, the generation of function words in fixed
phrases. As shown in ex. (5), the function word
??? is incorrectly generated as ??. This is be-
cause they share the same part-of-speech DEG in
CTB, however ?? has a much higher frequency
than ??? in Chinese text and thus has a higher prob-
ability to be generated.
92
(5) a. ? ? ?
all things DE in
?among all things?
b. *?  ?
all things DE in
Acknowledgments
The research reported in this paper is supported by
Science Foundation Ireland grant 04/IN/I527. Also,
we would like to thank Aoife Cahill for many help-
ful and insightful discussions on the work. And we
gratefully acknowledge the anonymous reviewers.
References
Harald Baayen and Richard Sproat. 1996. Estimat-
ing Lexical Priors for Low-Frequency Morphologi-
cally Ambiguous Forms. Computational Linguistics,
22(2): 155?166.
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a Probabilistic Hierarchical Model for Gen-
eration. Proceedings of the 18th International Con-
ference on Computational Linguistics, pages 42?48.
Saarbru?cken, Germany.
Daniel M. Bikel. 2004. On the Parameter Space of Gen-
erative Lexicalized Statistical Parsing Models. Ph.D.
Thesis of Department of Computer & Information Sci-
ence, University of Pennsylvania.
Aoife Cahill and Josef van Genabith. 2006. Robust
PCFG-Based Generation Using Automatically Ac-
quired LFG Approximations. Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 1033?1040. Syd-
ney, Australia.
Aoife Cahill and Martin Forst and Christian Rohrer.
2007. Stochastic Realisation Ranking for a Free Word
Order Language. Proceedings of the 11th European
Workshop on Natural Language Generation, pages
17?24. Schloss Dagstuhl, Germany.
John Carroll and Ann Copestake and Dan Flickinger and
Victor Poznanski. 1999. An efficient chart genera-
tor for (semi-)lexicalist grammars. Proceedings of the
7th European Workshop on Natural Language Gener-
ation, pages 86?95. Toulouse, France.
Hailong Cao and Yujie Zhang and Hitoshi Isahara. 2007.
Empirical study on Parsing Chinese Based on Collins?
Model. Proceedings of the 10th Conference of the Pa-
cific Association for Computational Linguistics, pages
113?119. Melbourne, Australia.
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. Proceedings of the 1st Annual Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, pages 132?139. Seattle, WA.
Michael Collins. 1997. Three Generative, Lexicalized
Models for Statistical Parsing. Proceedings of the 35th
Annual Meeting of the Association for Computational
Linguistics, pages 16?23. Madrid, Spain.
Dick Crouch and Mary Dalrymple and Ron Kaplan and
Tracy King and John Maxwell and Paula Newman.
2007. XLE Documentation. Palo Alto Research Cen-
ter, CA.
Yuqing Guo and Josef van Genabith and Haifeng Wang.
2007. Treebank-based Acquisition of LFG Resources
for Chinese. Proceedings of LFG07 Conference,
pages 214?232. Stanford, CA, USA.
Deirdre Hogan and Conor Cafferkey and Aoife Cahill
and Josef van Genabith. 2007. Exploiting Multi-Word
Units in History-Based Probabilistic Generation. Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 267?276.
Prague, Czech Republic.
Mark Johnson. 1998. PCFG Models of Linguistic Tree
Representations. Computational Linguistics, 24(4):
613?632. MIT Press, Cambridge, MA,
Ronald M. Kaplan. 1995. The formal architecture of
lexical-functional grammar. Formal Issues in Lexical-
Functional Grammar, pages 7?27. CSLI Publications,
Standford, USA.
Ronald M. Kaplan and Joan Bresnan. 1982. Lexical
Functional Grammar: a Formal System for Grammat-
ical Representation. The Mental Representation of
Grammatical Relations, pages 173-282. MIT Press,
Cambridge, MA.
Ronald M. Kaplan and Jurgen Wedekind. 2000.
LFG Generation Produces Context-free Languages.
Proceedings of the 18th International Conference
on Computational Linguistics, pages 425?431.
Saarbru?cken, Germany.
Martin Kay. 1996. Chart Generation. Proceedings of the
34th Annual Meeting of the Association for Computa-
tional Linguistics, pages 200?204. Santa Cruz, USA.
Irene Langkilde. 2000. Forest-Based Statistical Sen-
tence Generation. Proceedings of 1st Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, pages 170?177. Seattle, WA.
Langkilde, Irene. 2002. An Empirical Verification of
Coverage and Correctness for a General-Purpose Sen-
tence Generator. Proceedings of the Second Interna-
tional Conference on Natural Language Generation,
17?24. New York, USA.
93
Hiroko Nakanishi and Yusuke Nakanishi and Jun?ichi
Tsujii. 2005. Probabilistic Models for Disambigua-
tion of an HPSG-Based Chart Generator. Proceedings
of the 9th International Workshop on Parsing Technol-
ogy, pages 93?102. Vancouver, British Columbia.
Kishore Papineni and Salim Roukos and Todd Ward and
Wei-Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311-318. Philadelphia,
USA.
Erik Velldal and Stephan Oepen. 2005. Maximum en-
tropy models for realization ranking. Proceedings of
the MTSummit ?05.
Michael White. 2004. Reining in CCG Chart Realiza-
tion. Proceedings of the third International Natural
Language Generation Conference. Hampshire, UK.
Michael White and Rajakrishnan Rajkumar and Scott
Martin. 2007. Towards Broad Coverage Surface Re-
alization with CCG. Proceedings of the MT Summit
XI Workshop on Language Generation and Machine
Translation, pages 22?30. Copenhagen, Danmark.
Nianwen Xue and Fei Xia and Fu dong Chiou and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2): 207?238.
94
